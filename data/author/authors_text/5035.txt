Unknown Word Extraction for Chinese Documents 
 
Keh-Jiann Chen 
Institute of Information science,  
Academia Sinica 
kchen@iis.sinica.edu.tw 
Wei-Yun Ma 
Institute of Information science,  
Academia Sinica 
ma@iis.sinica.edu.tw 
 
Abstract  
There is no blank to mark word boundaries in 
Chinese text. As a result, identifying words is 
difficult, because of segmentation ambiguities 
and occurrences of unknown words. 
Conventionally unknown words were extracted 
by statistical methods because statistical 
methods are simple and efficient. However the 
statistical methods without using linguistic 
knowledge suffer the drawbacks of low 
precision and low recall, since character strings 
with statistical significance might be phrases or 
partial phrases instead of words and low 
frequency new words are hardly identifiable by 
statistical methods. In addition to statistical 
information, we try to use as much information 
as possible, such as morphology, syntax, 
semantics, and world knowledge. The 
identification system fully utilizes the context 
and content information of unknown words in 
the steps of detection process, extraction process, 
and verification process. A practical unknown 
word extraction system was implemented which 
online identifies new words, including low 
frequency new words, with high precision and 
high recall rates. 
1 Introduction 
One of the most prominent problems in 
computer processing of Chinese language is 
identification of the word sequences of input 
sentences. There is no blank to mark word 
boundaries in Chinese text. As a result, 
identifying words is difficult, because of 
segmentation ambiguities and occurrences of 
unknown words (i.e. out-of-vocabulary words).  
Most papers dealing with the problem of word 
segmentation focus their attention only on the 
resolution of ambiguous segmentation. The 
problem of unknown word identification is 
considered more difficult and needs to be further 
investigated. According to an inspection on the 
Sinica corpus (Chen etc., 1996), a 5 million 
word Chinese corpus with word segmented, it 
shows that 3.51% of words are not listed in the 
CKIP lexicon, a Chinese lexicon with more than 
80,000 entries. 
Identifying Chinese unknown words from a 
document is difficult; since  
 
1. There is no blank to mark word boundaries; 
2. Almost all Chinese characters and words are also 
morphemes; 
3. Morphemes are syntactic ambiguous and semantic 
ambiguous; 
4. Words with same morpho-syntactic structure might 
have different syntactic categories; 
5. No simple rules can enumerate all types of unknown 
words; 
6. Online identification from a short text is even harder, 
since low frequency unknown words are not 
identifiable by naive statistical methods. 
 
It is difficult to identify unknown words in a 
text since all Chinese characters can either be a 
morpheme or a word and there are no blank to 
mark word boundaries. Therefore without (or 
even with) syntactic or semantic checking, it is 
difficult to tell whether a character in a 
particular context is a part of an unknown word 
or whether it stands alone as a word. Compound 
words and proper names are two major types of 
unknown words. It is not possible to list all of 
the proper names and compounds neither in a 
lexicon nor enumeration by morphological rules. 
Conventionally unknown words were extracted 
by statistical methods for statistical methods are 
simple and efficient. However the statistical 
methods without using linguistic knowledge 
suffer the drawbacks of low precision and low 
recall. Because character strings with statistical 
significance might be phrases or partial phrases 
instead of words and low frequency new words 
are hardly identifiable by statistical methods. 
Common statistical features for unknown 
word extraction are mutual information (Church 
90), entropy (Tung 94), association strength 
(Smadja 93, Wang 95) and dice coefficients 
(Smadja 96) etc. Chang etc. (Chang etc. 97) 
iteratively apply the joint character association 
metric, which is derived by integrating above 
statistical features. Their performance is recall 
rate:81%, precision rate: 72% in disyllabic 
unknown word, recall rate:88%, precision rate: 
39% in trisyllabic unknown word, and recall 
rate:94%, precision rate: 56% in four-syllabic 
unknown word. 
Chang etc. (1994) used statistical methods to 
identify personal names in Chinese text which 
achieved a recall rate of 80% and a precision 
rate of 90%. Chen & Lee (1994) used 
morphological rules and contextual information 
to identify the names of organizations. Since 
organizational names are much more irregular 
than personal names in Chinese, they achieved a 
recall rate of 54.50% and a precision rate of 
61.79%. Lin etc. (1993) made a preliminary 
study of the problem of unknown word 
identification. They used 17 morphological rules 
to recognize regular compounds and a statistical 
model to deal with irregular unknown words, 
such as proper names etc.. With this unknown 
word resolution procedure, an error reduction 
rate of 78.34% was obtained for the word 
segmentation process. Since there is no standard 
reference data, the claimed accuracy rates of 
different papers vary due to different 
segmentation standards. In this paper we use the 
Sinica corpus as a standard reference data. As 
mentioned before, the Sinica corpus is a 
word-segmented corpus based on the Chinese 
word segmentation standard for information 
processing proposed by ROCLING (Huang et al 
1997). Therefore it contains both known words 
and unknown words, which are properly 
segmented. The corpus was utilized for the 
purposes of training and testing. 
From the above discussion, it is known that 
identification of unknown words is difficult and 
need to adopt different methods in identifying 
different types of unknown words. The objective 
of this research is to find methods to extract 
unknown words from a document and identify 
their syntactic and semantic categories. 
Although both processing are interrelated, for 
limiting scope of this paper, we will focus our 
discussion on the extraction process only and 
leave the topics of syntactic and semantic 
category predictions to other papers. 
2 Steps to Identify Unknown Words 
In addition to statistical information, we try to 
use as much information as possible, such as 
morphology, syntax, semantics, and world 
knowledge, to identify unknown words. The 
identification system fully utilizes the context 
and content information of unknown words in 
each three steps of processes, i.e. detection 
process, extraction process, and verification 
process. The detection process detects the 
occurrences of unknown words for better 
focusing, so that on the next step extraction 
process, it needs only focus on the places where 
unknown were detected. In addition, it also 
helps in identifying low frequency unknown 
words, which hardly can be identified by 
conventional statistical extraction methods. The 
extraction process extracts unknown words by 
applying morphological rules and statistical 
rules to match for different types of unknown 
words. As usual, tradeoff would occur between 
recall and precision. Enriching the extraction 
rules might increase recall rates, but it also 
increases the ambiguous and false extractions 
and thus lowers the precision. The final 
verification process comes to rescue. It resolves 
ambiguous and false extractions based on the 
morphological validity, syntactic validity, and 
statistical validity.  
3 Unknown Word Detection 
Conventionally a word segmentation process 
identifies the words in input text by matching 
lexical entries and resolving the ambiguous 
matching (Chen & Liu, 1992, Sproat et al 1996). 
Hence after segmentation process the unknown 
words in the text would be incorrectly 
segmented into pieces of single character word 
or shorter words. If all occurrences of 
monosyllabic words are considered as 
morphemes of unknown words, the recall rate of 
the detection will be about 99%, but the 
precision is as low as 13.4% (Chen & Bai, 1998). 
Hence the complementary problem of unknown 
word detection is the problem of monosyllabic 
known-word detection, i.e. to remove the 
monosyllabic known-words as the candidates of 
unknown morphemes. A corpus-based learning 
method is proposed to derive a set of syntactic 
discriminators for monosyllabic words and 
monosyllabic morphemes (Chen & Bai, 1998).  
The following types of rule patterns were 
generated from the training corpus. Each rule 
contains a key token within curly brackets and 
its contextual tokens without brackets. For some 
rules there may be no contextual dependencies. 
The function of each rule means that in a 
sentence, if a character and its context match the 
key token and the contextual tokens of the rule 
respectively, this character is a proper word (i.e. 
not a morpheme of an unknown word). For 
instance, the rule ?{Dfa} Vh? says that a 
character with syntactic category Dfa is a proper 
word, if it follows a word of syntactic category 
Vh. 
 
Rule type               Example 
================================= 
char   {?}  
word char  ? {?} 
char word  {?} ?? 
category   {T} 
{category} category {Dfa} Vh 
category {category} Na {Vcl} 
char category  {?} VH 
category char  Na {?} 
category category char Na Dfa {?} 
char category category {?} Vh T 
=================================== 
Table1. Rule types and Examples 
 
Rules of the 10 different types of patterns 
above were generated automatically by 
extracting each instance of monosyllabic words 
in the training corpus. Every generated rule 
pattern was checked for applicability and 
accuracy. At the initial stage, 1455633 rules 
were found. After eliminating the low 
applicability rules, i.e. frequency less than 3, 
there are 215817 rules remained. At next stage, 
the rules with accuracy greater than 98% are 
selected for better recall rate. However the 
selected rules may subsume each other. Shorter 
rule patterns are usually more general than the 
longer rules. A further screening process is 
applied to remove the redundant rules. The final 
rule sets contain 45839 rules and were used to 
detect unknown words in the experiment. It 
achieves the detection rate of 96% and the 
precision rates of 60%. Where detection rate 
96% means that for 96% of unknown words in 
the testing data, at least one of its morpheme 
was detected as part of unknown word. However 
the boundaries of unknown words are still not 
known. For more detail discussion, see (Chen & 
Bai 1998). For convenience, hereafter we use (?) 
to mark detected morphemes of unknown words 
and () to mark the words which are not detected 
as morphemes of unknown words. 
4 Unknown Word Extraction 
At detection stages, the contextual rules were 
applied to detect fragments of unknown words, 
i.e. monosyllabic morphemes. The extraction 
rules will be triggered by the detected 
morphemes only. The extraction rules are 
context, content, and statistically constrained. 
Rule-design targets for high recall rate and try to 
maintain high precision at the mean time. It is 
hard to derive a set of morphological rules, 
which exactly cover all types of unknown words. 
Our approach is that if morphological structures 
of certain types of unknown words are well 
established, their fine-grain morphological rules 
will be designed. Otherwise statistical rules are 
designed without differentiate their extracted 
word types. Redundancy is allowed to achieve 
better coverage. Both morphological rules and 
statistical rules use context, content and 
statistical information in their extraction.  
4.1  Morphological rules 
Since there are too many different types of 
unknown words, we cannot go through the detail 
extraction processes for each different type. It 
will be exemplified by the personal name 
extraction to illustrate the idea of using different 
clues in the extraction process. First of all the 
content information is used, each different type 
of unknown words has its own morphological 
structure. For instance, a typical Chinese 
personal name starts with a last name and 
followed by a given name. The set of last names 
is about one hundred. Most of them are common 
characters. Given names are usually one or two 
characters and seldom with bad meaning. Based 
on the above structure information of Chinese 
personal names, the name extraction rules are 
designed as shown in Table 2. Context 
information is used for verification and 
determining the boundary of the extracted word. 
For instance, in the last rule of Table 2, it uses 
context information and statistical information to 
resolve ambiguity of the word boundary. It is 
illustrated by the following examples.  
 
1) after detection   : ?(?) ?(?) ?() ?() ?() ?()? 
  extractnion : ??? ? ? ?? 
             Ming-Zheng Zhang want kill somebody. 
         or  ?? ? ? ? ?? 
             Ming Zhang just want kill somebody.    
     
Rule type                Constraints & Procedure 
========================================== 
(?)  (?)  (?) 21 ++ iii msmsms    combine  )2,1,( ++ iii
(?)  (?)    () 21 ++ iii msmsms    combine       )2,1,( ++ iii
(?)    ()  (?) 21 ++ iii msmsms    combine       )2,1,( ++ iii
()   (?) 1+ii dsms          combine           )1,( +ii
()  (?)  (?) 21 ++ iii psmsms                )1,( +iicombine
()  (?)  (?) 21 ++ iii msmsms      as follows: 
 ( ) 1|  12 <++ iiidocument msmsmsprobif  
    namedisyllabicaasiicombine         )1,( +
( ) 1,,  32 ?++ iicoupus wordmsNAMEfreqelsif  
namedisyllabicaasiicombine         )1,( +
( )3,  + ? coupusicoupus freqwordNAMEfreq
( ) yllabic na as a tris,ii,i 21 ++
  
     combine  
( 2, +imsNAMEelsif
me
)
)
( )
else   namedisyllabicaasiicombine         )1,( +
 
Notes: ms denotes monosyllable. ds denotes disyllable. ps 
denotes polysyllable which consists of more than one 
syllable. word denotes a word which could consist of any 
number of syllable. msi must belong to Common Chinese 
Last Name Set, such as ?, ??etc. 
========================================= 
      Table 2. Rule types of Chinese personal name 
 
In the examples 1), there are two possible 
candidates of personal names, ?? and ???. 
By context information, the bi-gram (NAME, 
?) is less freguent than (NAME, ?) in the 
corpus, so without considering statistical 
constraints, it would suggest that ??? is a 
correct extraction instead of ??. However, the 
locality of the keywords is very important clue 
for identification, since the keywords of a text 
are usually unknown words and they are very 
frequently reoccurred in the text. The statistical 
information is used here for verification. For 
instance, if an another sentence which is like ?
(?) ? (?) ? () ? () occurs in the same 
document, it suggests ??  is the correct 
extraction, since the statistical constraint 
 rejects???. ( 1| <???documentprob
4.2  Statistical Rules 
It is well known that keywords often reoccur in 
a document (Church, 2000) and very possible 
the keywords are also unknown words. 
Therefore statistical extraction methods utilize 
the locality of unknown words. The idea is that 
if two consecutive morphemes are highly 
associated then combine them to form a new 
word. Mutual information-like statistics are very 
often adopted in measuring association strength 
between two morphemes (Church & Merser, 
1993, Sproat et al 1996). However such kind of 
statistic does not work well when the sample 
size is very limited. Therefore we propose to use 
reoccurrence frequency and fan-out numbers to 
characterize words and their boundaries (Chien, 
1999). 12 statistical rules are derived to extract 
unknown words. Each rule is triggered by 
detected morphemes and executed in iteration. 
The boundaries of unknown words might extend 
during iteration until no rule could be applied. 
Following are two examples of statistical rules. 
 
Rule id       Pattern          Statistical constraint  
========================================== 
R1         Lm(?) Rm()               S1  
R2         Lm(?) Rm(?)              S2 
 ( ) ( )
( ) 2  and       
  8.0|  and  8.0| : S1
?
??
LmRmFreq
LmRmPRmLmP
( ) ( )( )
 
( )
( ) ( )( )8.0|  and  8.0|or         
2    8.0|or    8.0| : S2
??
???
LmRmPRmLmP
LmRmFreqandLmRmPRmLmP  
========================================== 
     Table 3. Two examples of statistical rules 
 
The rule R1 says that Lm and Rm will be 
combined, if both conditional probability 
P(Lm|Rm)>=0.8 and P(Rm|Lm)>=0.8 hold and 
the string LmRm occurred more than once in the 
processed document. Conditional probabilities 
constrain the fan-out number on each side of 
morpheme, i.e. the preceding morpheme of Rm 
should almost be limited to Lm only and vice 
versa. The threshold value 0.8 is adjusted 
according to the experimental results, which 
means at least four out of five times the 
preceding morpheme of Rm is Lm and vice 
versa. However the statistical constraints are 
much loose when the right morpheme Rm is also 
a detected morpheme, as exemplified in R2. You 
may notice that it also accepts the unknown 
words occurred only once in the document.  
Conventional statistical extraction methods 
are simple and efficient. However if without 
supporting linguistic evidences the precision of 
extraction is still not satisfactory, since a high 
frequency character string might be a phrase or a 
partial phrase instead of a word. In addition to 
statistical constraint, our proposed statistical 
method requires that a candidate string must 
contain detected morphemes. In other words, the 
statistical rules are triggered by detected 
morphemes only. Furthermore the 
morphological structure of extracted unknown 
word must be valid. A validation process will be 
carried out at the different stages for all 
extracted unknown words. 
5  Verification 
To verify a correct extraction depends on the 
following information. 
 
1. Structure validity: the morphological structure of a 
word should be valid. 
2. Syntactic validity: the syntactic context of an 
 identified new word should be valid. 
3. Local consistency: the identified unknown words  
should satisfy the local statistical constraints, i.e. no 
inconsistent extension on the morphological structures. 
For instance, a new word was identified by the pattern 
rules, but if it violates the statistical constraints, as 
exemplified in 1), will be rejected. 
 
Each extracted candidate will be evaluated 
according to the validity of above three criteria. 
For the candidates extracted by the statistical 
rules, their structure validity and syntactic 
validity are checked after extraction. On the 
other hand, for the unknown words extracted 
according to the morphological rules, their 
structure validity and syntactic validity are 
checked at extraction stage and their local 
statistical consistency is checked after extraction.    
To verify the structure validity and syntactic 
validity of the unknown words extracted by 
statistical methods, their syntactic categories are 
predicted first, since statistical rules do not 
classify unknown word types. The prediction 
method is adopted from (Chen, Bai & Chen, 
1997). They use the association strength 
between morpheme and syntactic category to 
predict the category of a word. The accuracy rate 
is about 80%. Once the syntactic category of an 
unknown word is known its contextual bi-gram 
will be checked. If the bi-grams of (preceding 
word/category, unknown word category) and 
(unknown word category, following 
word/category) are syntactically valid, i.e. the 
bi-gram patterns are commonly occurred in the 
corpus, the extracted word is considered to be a 
valid word. Otherwise this candidate will be 
rejected.  
5.1  Final Selection 
It is possible that the extracted candidates 
conflict each other. For instance, in the 
following example, both candidates are valid. 
????, Bennet? is extracted by name rules and 
????, lawyer-class? is extracted by suffix 
rules. 
 
name  ==>  ?? ?? ?? ??? ? ? 
      An-jan company lawyer Bennett said, 
suffix  ==>  ?? ?? ??? ? ? ? ? 
 An-jan company lawyer-class is pecial said, 
 
The extracted new words will form a word 
lattice. The selection process finds the most 
probable word sequence among word lattice as 
the final result. In the current implementation, 
we used a very simple heuristics of maximizing 
the total weights of words to pick the most 
probable word sequence. The weight of a word 
w is defined to be freq(w)*length(w), where 
freq(w) is the occurrence frequency of w in the 
document and length is the number of characters 
in w. For the above example, ????, Bennett? 
occurred 5 times and ????, lawyer-class? 
occurred twice only in the document. Therefore 
the final result is 
 
??   ??   ??  ? ? ? 
An-jan company lawyer Bennett said , 
?Bennett, the lawyer of An-jan company, said?? 
6  Experimental Results 
In the current implementation, the 
morphological rules include the rules for 
Chinese personal names, foreign transliteration 
names, and compound nouns. In addition to the 
morphological rules, twelve constrained 
statistical rules were implemented to patch the 
under coverage of the morphological rules. 
Although the current implementation is not 
complete, morphological rules of many other 
types of unknown words were not included, such 
as rules for compound verbs. The experiment 
results still show that the proposed methods 
work well and the morphological rules and the 
statistical rules complement each other in the 
extraction and verification. 
The Sinica balanced corpus provides the 
major training and testing data. The training data 
contains 8268 documents with 4.6 million words. 
We use it to train the detection rules and 
morphological rules. We randomly pick 100 
documents from rest of the corpus as the testing 
data, which contain 17585 words and 1160 
unknown word types. 
 A word is considered as an unknown word, 
if neither it is in the CKIP lexicon nor it is 
identified as foreign word (for instance English) 
or a number. The CKIP lexicon contains about 
80000 entries. 
The precision and recall rates are provided. 
The target of our approach is to extract unknown 
words from a document, so we define ?correct 
extractions? as unknown word types correctly 
identified in the document. The precision and 
recall rate formulas are as follows: 
 
idocument in  sextractioncorrect  ofnumber NCi =  
idocument            
in    typesrdunknown wo extracted ofnumber NEi =  
idocument            
in    typesrdunknown wo reference ofnumber NRi =  
 
?
?
=
=
=
== 100
1
i
100
1
i
NE
NC
ratePrecision i
i
i
i      
?
?
=
=
=
== 100
1
i
100
1
i
NR
NC
rate Recall i
i
i
i  
 
  To observe the frequency impact on our 
system, the performance evaluation on both high 
frequency and low frequency unknown word 
identifications are also provided at Table 5 & 6. 
A word occurs more than or equal to 3 times in a 
document is considered a high frequency word. 
There are only 66 high frequency unknown 
words in our testing data. It counts less than 6% 
of the total unknown words. 
 
 Correct# Extract# Precision Recall 
Morphological rules 541 590 92% 47% 
Statistical rules 455 583 78% 39% 
Total system 791 890 89% 68% 
Table 4. Experimental result of total unknown 
word types 
 Correct# Extract# Precision Recall 
Morphological rules 25 26 96% 38% 
Statistical rules 50 60 83% 76% 
Total system 54 64 84% 82% 
Table 5. The performance on the set of unknown 
words with frequency >= 3 in a document 
 
 Correct# Extract# Precision Recall 
Morphological rules 510 564 90% 47% 
Statistical rules 400 523 76% 37% 
Total system 731 826 88% 67% 
Table 6. The performance on the set of unknown 
words with frequency <3 in a document 
 
Recall rate of total unknown word types is not 
very high, because not all of the morphological 
rules were implemented and some of the word 
tokens in the testing data are arguable. The 
experiment results in Table 6 show that the 
proposed methods work well on low frequency 
unknown word identification.  
7  Conclusions and Future Works 
Unknown word extraction is a very hard task. 
In addition to statistical information, it requires 
supporting knowledge of morphological, 
syntactic, semantic, word type specific and 
common sense. One important trend is to look 
harder for sources of knowledge and managing 
knowledge that can support unknown word 
identification. A word segmented and tagged 
corpus is essential for the success of the whole 
research. The corpus provides the major training 
and testing data. It also supports plenty of 
unknown words and their contextual data to 
derive extraction rules. In this work we are 
managing to use the structure information, the 
context environment, and statistical consistency 
of the unknown words and to increase the recall 
and precision of the extraction process. The 
syntactic and semantic classifications for 
unknown words are executed in parallel with the 
extraction process. Both classification processes 
are very hard and need further researches.  
8  References 
Chang J. S.,S.D. Chen, S. J. Ker, Y. Chen, & J. 
Liu,1994 "A Multiple-Corpus Approach to 
Recognition of Proper Names in Chinese Texts", 
Computer Processing of Chinese and Oriental 
Languages, Vol. 8, No. 1, 75-85. 
Chang, Jing-Shin and Keh-Yih Su, 1997a. "An 
Unsupervised Iterative Method for Chinese New 
Lexicon Extraction", International Journal of 
Computational Linguistics & Chinese Language 
Processing, 1997. 
Chen, H.H., & J.C. Lee, 1994,"The Identification of 
Organization Names in Chinese Texts", 
Communication of COLIPS, Vol.4 No. 2, 131-142. 
Chen, K.J. & S.H. Liu, 1992,"Word Identification for 
Mandarin Chinese Sentences," Proceedings of 14th 
Coling, pp. 101-107. 
Chen, K.J., C.R. Huang, L. P. Chang & H.L. Hsu, 
1996,"SINICA CORPUS: Design Methodology for 
Balanced Corpora," Proceedings of PACLIC 11th 
Conference, pp.167-176. 
Chen, C. J., M. H. Bai, K. J. Chen, 1997, ?Category 
Guessing for Chinese Unknown Words.? 
Proceedings of the Natural Language Processing 
Pacific Rim Symposium 1997, pp. 35-40. 
NLPRS ?97 Thailand. 
Chen, K.J. & Ming-Hong Bai, 1998, ?Unknown 
Word Detection for Chinese by a Corpus-based 
Learning Method,? international Journal of 
Computational linguistics and Chinese Language 
Processing, Vol.3, #1, pp.27-44. 
Chen, K.J., Chao-Jan Chen. 1998. ?A Corpus Based 
Study on Computational Morphology for Mandarin 
Chinese?????????????????
???.? Quantitative and Computational Studies 
on the Chinese Language. Benjamin K. T?sou, 
Tom B.Y. Lai, Samuel W. K. Chan, William S-Y. 
Wang, ed. HK: City Univ. of Hong Kong. 
pp.283-306. 
Chiang, T. H., M. Y. Lin, & K. Y. Su, 1992,? 
Statistical Models for Word Segmentation and 
Unknown Word Resolution,? Proceedings of 
ROCLING V, pp. 121-146. 
Chien, Lee-feng, 1999,? PAT-tree-based Adaptive 
Keyphrase Extraction for Intelligent Chinese 
Information Retrieval,? Information Processing 
and Management, Vol. 35, pp. 501-521. 
Church, K. W., & R. L. Mercer, 1993, ?Introduction 
to the Special Issue on Computational Linguistics 
Using Large Corpora.? Computational Linguistics, 
Vol. 19, #1, pp. 1-24 
Church, Kenneth W., 2000,? Empirical Estimates of 
Adaptation: The Chance of Two Noriegas is Closer 
to p/2 than p*p?, Proceedings of Coling 2000, 
pp.180-186. 
Huang, C. R. Et al.,1995,"The Introduction of Sinica 
Corpus," Proceedings of ROCLING VIII, pp. 
81-89. 
Lin, M. Y., T. H. Chiang, &  K. Y. Su, 1993,? A 
Preliminary Study on Unknown Word Problem in 
Chinese Word Segmentation,? Proceedings of 
ROCLING VI, pp. 119-137. 
Smadja, F., 1993,? Retrieving Collocations from 
Text: Xtract,? Computational Linguistics, 
19(1),143-177. 
Smadja, F., K. McKeown, and V. Hatzivassiloglou, 
1996,?Translating Collocations for Bilingual 
Lexicons: A Statistical Approach,? Computational 
Linguistics, 22(1). 
Sproat, R., C. Shih, W. Gale, & N. Chang,1996, "A 
Stochastic Finite-State Word-Segmentation 
Algorithm for Chinese," Computational Linguistics, 
22(3),377-404. 
Sun, M. S., C.N. Huang, H.Y. Gao, & Jie Fang, 1994, 
"Identifying Chinese Names in Unrestricted Texts", 
Communication of COLIPS, Vol.4 No. 2, 113-122. 
A Bottom-up Merging Algorithm for Chinese 
Unknown Word Extraction 
Wei-Yun Ma 
Institute of Information science,  
Academia Sinica 
ma@iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information science,  
Academia Sinica 
kchen@iis.sinica.edu.tw 
 
 
 
 
 
 
 
Abstract 
Statistical methods for extracting Chinese 
unknown words usually suffer a problem 
that superfluous character strings with 
strong statistical associations are extracted 
as well. To solve this problem, this paper 
proposes to use a set of general morpho-
logical rules to broaden the coverage and 
on the other hand, the rules are appended 
with different linguistic and statistical 
constraints to increase the precision of the 
representation. To disambiguate rule ap-
plications and reduce the complexity of 
the rule matching, a bottom-up merging 
algorithm for extraction is proposed, 
which merges possible morphemes recur-
sively by consulting above the general 
rules and dynamically decides which rule 
should be applied first according to the 
priorities of the rules. Effects of different 
priority strategies are compared in our ex-
periment, and experimental results show 
that the performance of proposed method 
is very promising. 
1 Introduction and Related Work 
Chinese sentences are strings of characters with no 
delimiters to mark word boundaries. Therefore the 
initial step for Chinese processing is word 
segmentation. However, occurrences of unknown 
words, which do not listed in the dictionary, 
degraded significantly the performances of most 
word segmentation methods, so unknown word 
extraction became a key technology for Chinese 
segmentation. 
For unknown words with more regular 
morphological structures, such as personal names, 
morphological rules are commonly used for 
improving the performance by restricting the 
structures of extracted words (Chen et. al 1994, 
Sun et. al 1994, Lin et. al 1994). However, it's not 
possible to list  morphological rules for all kinds of 
unknown words, especially those words with very 
irregular structures, which have the characteristics 
of variable lengths and flexible morphological 
structures, such as proper names, abbreviations etc. 
Therefore, statistical approaches usually play 
major roles on irregular unknown word extraction 
in most previous work (Sproat & Shih 1990, 
Chiang et. al 1992, Tung and Lee 1995, Palmer 
1997, Chang et. al 1997, Sun et. al 1998, Ge et. al 
1999). 
For statistical methods, an important issue is 
how to resolve competing ambiguous extractions 
which might include erroneous extractions of 
phrases or partial phrases. They might have 
statistical significance in a corpus as well. Very 
frequently superfluous character strings with 
strong statistic associations are extracted. These 
wrong results are usually hard to be filtered out 
unless deep content and context analyses were 
performed. To solve this problem, the idea of 
unknown word detection procedure prior to 
extraction is proposed. Lin et al (1993) adopt the 
following strategy: First, they decide whether there 
is any unknown word within a detected region with 
fix size in a sentence, and then they extract the 
unknown word from the region by a statistical 
method if the previous answer is "yes". A 
limitation of this method is that it restricts at most 
one unknown word occurs in the detected region, 
so that it could not deal with occurrences of 
consecutive unknown words within a sentence. 
Chen & Ma (2002) adopt another strategy: After an 
initial segmentation process, each monosyllable is 
decided whether it is a common word or a 
morpheme of unknown word by a set of syntactic 
discriminators. The syntactic discriminators are a 
set of syntactic patterns containing monosyllabic, 
words which are learned from a large word 
segmented corpus, to discriminate between 
monosyllabic words and morphemes of unknown 
words. Then more deep analysis can be carried out 
at the detected unknown word morphemes to 
extract unknown words. 
In this paper, in order to avoid extractions of  
superfluous character strings with high frequencies, 
we proposed to use a set of general rules, which is 
formulated as a context free grammar rules of 
composing detected morphemes and their adjacent 
tokens, to match all kinds of unknown words, for 
instance which includes the rule of (UW  UW 
UW). To avoid too much superfluous extractions 
caused by the over general rules, rules are 
appended with linguistic or statistical constraints. 
To disambiguate between rule applications and 
reduce the complexity of the rule matching, a 
bottom-up merging algorithm for extraction is 
proposed, which merges possible morphemes 
recursively by consulting above general rules and 
dynamically decides which rule should be applied 
first according to the priorities of the rules. 
The paper is organized into 7 sections. In the 
next section, we provide an overview of our sys-
tem. Section 3 briefly introduce unknown word 
detection process and makes some analysis for 
helping the derivation of general rules for un-
known words. In section 4, we derive a set of gen-
eral rules to represent all kinds of unknown words, 
and then modify it by appending rules constraints 
and priorities. In section 5, a bottom-up merging 
algorithm is presented for unknown word extrac-
tion. In section 6, the evaluation of extraction is 
presented; we also compare the performances to 
different priority strategies. Finally, in section 7, 
we make the conclusion and propose some future 
works. 
2 System Overview 
The purpose to our unknown word extraction 
system is to online extract all types of unknown 
words from a Chinese text. Figure 1 illustrates the 
block diagram of the system proposed in this paper. 
Initially, the input sentence is segmented by a 
conventional word segmentation program. As a 
result, each unknown word in the sentence will be 
segmented into several adjacent tokens (known 
words or monosyllabic morphemes). At unknown 
word detection stage, every monosyllable is 
decided whether it is a word or an unknown word 
morpheme by a set of syntactic discriminators, 
which are learned from a corpus. Afterward, a 
bottom-up merging process applies the general 
rules to extract unknown word candidates. Finally, 
the input text is re-segmented by consulting the 
system dictionary and the extracted unknown word 
candidates to get the final segmented result. 
 
 
Figure 1. Flowchart of the system 
 
(1)                          
    if    can   increase   gross profit rate 
"if gross profit rate can be increased?" 
 
(2)   after first step word segmentation: 
         
 
                             
        after unknown word detection: 
         
 
               (?)     (?)    (?) 
        after unknown word extraction: 
         
 
                
                   
 
For example, the correct segmentation of (1) is 
shown, but the unknown word ? 	
 ? is 
segmented into three monosyllabic words after the 
first step of word segmentation process as shown 
in (2). The unknown word detection process will 
mark the sentence as ?
  ()   ()   ()   (?)  
(?)   (?)?, where (?) denotes the detected 
monosyllabic unknown word morpheme and () 
denotes common words. During extracting process, 
the rule matching process focuses on the 
morphemes marked with (?) only and tries to 
combine them with left/right neighbors according 
to the rules for unknown words. After that, the 
unknown word ?  ? is extracted. During the 
process, we do not need to take care of other 
superfluous combinations such as ?
 
 ? even 
though they might have strong statistical 
association or co-occurrence too. 
3 Analysis of Unknown Word Detection 
The unknown word detection method proposed by 
(Chen & Bai 1998) is applied in our system. It 
adopts a corpus-based learning algorithm to derive 
a set of syntactic discriminators, which are used to 
distinguish whether a monosyllable is a word or an 
unknown word morpheme after an initial 
segmentation process. If all occurrences of 
monosyllabic words are considered as morphemes 
of unknown words, the recall of the detection will 
be about 99%, but the precision is as low as 13.4%.  
The basic idea in (Chen & Bai 1998) is that the 
complementary problem of unknown word 
detection is the problem of monosyllabic known-
word detection, i.e. to remove the monosyllabic 
known-words as the candidates of unknown word 
morphemes. Chen and Bai (1998) adopt ten types 
of context rule patterns, as shown in table 1, to 
generate rule instances from a training corpus. The 
generated rule instances were checked for 
applicability and accuracy. Each rule contains a 
key token within curly brackets and its contextual 
tokens without brackets. For some rules there may 
be no contextual dependencies. The function of 
each rule means that in a sentence, if a character 
and its context match the key token and the 
contextual tokens of the rule respectively, this 
character is a common word (i.e. not a morpheme 
of unknown word). 
For instance, the rule ?{Dfa} Vh? says that a 
character with syntactic category Dfa is a common 
word, if it follows a word of syntactic category Vh. 
 
 
Rule type               Example 
================================= 
char   {   }  
word char     { } 
char word   {  }   
category   {T} 
{category} category  {Dfa} Vh 
category {category}  Na {Vcl} 
char category  {  } VH 
category char  Na {  } 
category category char Na Dfa { 	 } 
char category category  { 
 } Vh T 
================================= 
 
Table1. Rule types and Examples 
 
The final rule set contains 45839 rules and 
were used to detect unknown words in the ex-
periment. It achieves a detection rate of 96%, and a 
precision of 60%. Where detection rate 96% means 
that for 96% of unknown words in the testing data, 
at least one of its morphemes are detected as part 
of unknown word and the precision of 60% means 
that for 60% of detected monosyllables in the test-
ing data, are actually morphemes. Although the 
precision is not high, most of over-detecting errors 
are ?isolated?, which means there are few situa-
tions that two adjacent detected monosyllabic un-
known morphemes are both wrong at the mean 
time. These operative characteristics are very im-
portant for helping the design of general rules for 
unknown words later. 
4 Rules for Unknown Words 
Although morphological rules work well in regular 
unknown word extraction, it's difficult to induce 
morphological rules for irregular unknown words. 
In this section, we try to represent a common struc-
ture for unknown words from another point of 
view; an unknown word is regarded as the combi-
nation of morphemes which are consecutive mor-
phemes/words in context after segmentation, most 
of which are monosyllables. We adopt context free 
grammar (Chomsky 1956), which is the most 
commonly used generative grammar for modelling 
constituent structures, to express our unknown 
word structure. 
4.1 Rule Derivation 
According to the discussion in section 3, for 96% 
of unknown words, at least one of its morphemes 
are detected as part of unknown word, which 
motivates us to represent the unknown word 
structure with at least one detected morpheme. 
Taking this phenomenon into our consideration, 
the rules for modeling unknown words and an 
unknown word example are presented as follows. 
 
 
  UW     UW UW    (1) 
| ms(?) ms(?)    (2) 
| ms(?) ps()   (3) 
| ms(?) ms()  (4) 
| ps() ms(?)   (5) 
| ms() ms(?)   (6) 
| ms(?) UW   (7) 
| ms() UW (8) 
| ps() UW (9) 
| UW ms(?)  (10) 
| UW ms()   (11) 
| UW ps()   (12) 
 
Notes: There is one non-terminal symbol. ?UW? 
denotes ?unknown word? and is also the start symbol. 
There are three terminal symbols, which includes ms(?), 
which denotes the detected monosyllabic unknown 
word morpheme, ms() , which denotes the monosyllable 
that is not detected as the morpheme, and ps(), which 
denotes polysyllabic (more than one syllable) known 
word. 
 
 
Table 2. General rules for unknown words 
 
 
 
Figure 2. A possible structure for the unknown word 
?   ?(Chen Zhi Ming), which is 
segmented initially and detected as ?   (?) 
(?)  ()?, and ?  ? was marked incorrectly at 
detection stage. 
 
There are three kinds of commonly used meas-
ures applied to evaluate grammars: 1. generality 
(recall), the range of sentences the grammar ana-
lyzes correctly; 2. selectivity (precision), the range 
of non-sentences it identifies as problematic and 3. 
understandability, the simplicity of the grammar 
itself (Allen 1995). For generality, 96% unknown 
words have this kind of structure, so the grammar 
has high generality to generate unknown words. 
But for selectivity, our rules are over-generation. 
Many patterns accepted by the rules are not words. 
The main reason is that rules have to include non-
detected morphemes for high generality. Therefore 
selectivity is sacrificed momentary. In next section, 
rules would be constrained by linguistic and text-
based statistical constraints to compensate the se-
lectivity of the grammar. For understandability, 
you can find each rule in (1)-(12) consists of just 
two right-hand side symbols. The reason for using 
this kind of presentation is that it regards the un-
known word structure as a series of combinations 
of consecutive two morphemes, such that we could 
simplify the analysis of unknown word structure 
by only analyzing its combinations of consecutive 
two morphemes. 
4.2 Appending Constraints 
Since the general rules in table 2 have high 
generality and low selectivity to model unknown 
words, we append some constraints to restrict their 
applications. However, there are tradeoffs between 
generality and selectivity: higher selectivity 
usually results in lower generality. In order to keep 
high generality while assigning constraints, we 
assign different constraints on different rules 
according to their characteristics, such that it is 
only degraded generality slightly but selectivity 
being upgraded significantly. 
The rules in table 2 are classified into two kinds: 
one kind is the rules which both its right-hand side 
symbols consist of detected morphemes, i.e, (1), 
(2), (7), and (10), the others are the rules that just 
one of its right-hand side symbols consists of 
detected morphemes, i.e, (3), (4), (5), (6), (8), (9), 
(11), and (12). The former is regarded as ?strong? 
structure since they are considered to have more 
possibility to compose an unknown word or an 
unknown word morpheme and the latter is 
regarded as ?weak? structure, which means they 
are considered to have less possibility to compose 
an unknown word or an unknown word morpheme. 
The basic idea is to assign more constraint on those 
rules with weak structure and less constraint on 
those rules with strong structure. 
The constraints we applied include word length, 
linguistic and statistical constraints. For statistical 
constraints, since the target of our system is to 
extract unknown words from a text, we use text-
based statistical measure as the statistical 
constraint. It is well known that keywords often 
reoccur in a document (Church 2000) and very 
possible the keywords are also unknown words. 
Therefore the reoccurrence frequency within a 
document is adopted as the constraint. Another 
useful statistical phenomenon in a document is that 
a polysyllabic morpheme is very unlikely to be the 
morphemes of two different unknown words 
within the same text. Hence we restrict the rule 
with polysyllabic symbols by evaluating the 
conditional probability of polysyllabic symbols.  In 
addition, syntactic constraints are also utilized here. 
For most of unknown word morphemes, their 
syntactic categories belong to ?bound?, 
?verb?, ?noun?, and ?adjective? instead of 
?conjunction?, ?preposition??etc. So we restrict 
the rule with non-detected symbols by checking 
whether syntactic categories of its non-detected 
symbols belong to ?bound?, ?verb?, ?noun?, or 
?adjective?. To avoid unlimited recursive rule 
application, the length of matched unknown word 
is restricted unless very strong statistical 
association do occur between two matched tokens. 
The constraints adopted so far are presented in 
table 3. Rules might be restricted by multi-
constraints. 
 
Freqdocu(LR)>=Threshold (3) (4) (5) (6) (8) (9) (11) (12) 
Pdocu(L|R)=1 (1) (3) (7) (8) (9) (12) 
Pdocu(R|L)=1 (1) (5) (9) (10) (11) (12) 
Category(L) is bound, verb, 
noun or adjective (5) (6) (8) (9) 
Category(R) is bound, verb, 
noun or adjective (3) (4) (11) (12) 
 
Notes: L denotes left terminal of right-hand side 
            R denotes right terminal of right-hand side 
                  Threshold is a function of Length(LR) and text 
size. The basic idea is larger amount of length(LR) 
or text size matches larger amount of Threshold.       
 
 Table 3. Constraints for general rules 
4.3 Priority 
To scheduling and ranking ambiguous rule 
matching, each step of rule matching is associated 
with a measure of priority which is calculated by 
the association strength of right-hand side symbols. 
In our extracting algorithm, the priority measure is 
used to help extracting process dynamically decide 
which rule should be derived first. More detail 
discussion about ambiguity problem and complete 
disambiguation process are presented in section 5. 
We regard the possibility of a rule application as 
co-occurrence and association strength of its right-
hand side symbols within a text. In other words, a 
rule has higher priority of application while its 
right-hand side symbols are strongly associated 
with each other, or co-occur frequently in the same 
text. There have been many statistical measures 
which estimate co-occurrence and the degree of 
association in previous researches, such as mutual 
information (Church 1990, Sporat 1990), t-score 
(Church 1991), dice matrix (Smadja 1993, 1996). 
Here, we adopt four well-developed kinds of 
statistical measures as our priority individually: 
mutual information (MI), a variant of mutual 
information (VMI), t-score, and co-occurrence. 
The formulas are listed in table 4. MI mainly 
focuses on association strength, and VMI and t-
score consider both co-occurrence and association 
strength. The performances of these four measures 
are evaluated in our experiments discussed in 
section 6. 
 
==================================== 
),(),( RLfRLoccurenceco =?   
------------------------------------------------------------- 
)()(
),(log),(
RPLP
RLPRLMI =  
------------------------------------------------------------- 
),(),(),( RLMIRLfRLVMI =  
------------------------------------------------------------- 
),(
)()(),(
),(
RLf
N
RfLfRLf
RLscoret
?
=?
  
 
Notes: f(L,R) denotes the number of occurrences of L,R in the 
text; N denotes the number of occurrences of all the 
tokens in the text; length(*) denotes the length of *. 
==================================== 
 
Table 4. Formulas of 4 kinds of priority 
5 Unknown Word Extraction 
5.1 Ambiguity 
Even though the general rules are appended with 
well-designed constraints, ambiguous matchings, 
such as, overlapping and covering, are still existing. 
We take the following instance to illustrate that: 
?   ? (La Fa Yeh), a warship name, occurs 
frequently in the text and is segmented and 
detected as ?   (?)  (?)  (?)?. Although ?  
 ? could be derived as an unknown word ?((   )
 )? by rule 2 and rule 10, ?   ? and ?  ? 
might be also derived as unknown words ?(   )? 
and ?(  )? individually by the rule 2. Hence 
there are total three possible ambiguous unknown 
words and only one is actually correct. 
Several approaches on unsupervised segmenta-
tion of Chinese words were proposed to solve 
overlapping ambiguity to determine whether to 
group ?xyz? as ?xy z? or ?x yz?, where x, y, and z 
are Chinese characters. Sproat and Shih (1990) 
adopt a greedy algorithm: group the pair of adja-
cent characters with largest mutual information 
greater than some threshold within a sentence, and 
the algorithm is applied recursively to the rest of 
the sentence until no character pair satisfies the 
threshold. Sun et al (1998) use various association 
measures such as t-score besides mutual informa-
tion to improve (Sproat & Shih 1990). They devel-
oped an efficient algorithm to solve overlapping 
character pair ambiguity. 
5.2 Bottom-up Merging Algorithm 
Following the greedy strategy of (Sproat & Shih 
1990), here we present an efficient bottom-up 
merging algorithm consulting the general rules to 
extract unknown words. The basic idea is that for a 
segmented sentence, if there are many rule-
matched token pairs which also satisfy the rule 
constraints, the token pair with the highest rule 
priority within the sentence is merged first and 
forms a new token string. Same procedure is then 
applied to the updated token string recursively 
until no token pair satisfied the general rules. It is 
illustrated by the following example: 
 
====================================== 
System environment:  
Co-occurrence priority is adopted. 
Text environment: 
                                  ?    ? (Chen Zhi Qiang), an unknown word, 
occurs three times. 
? 	
 ? (take an electing activity), an unknown 
word, occurs two times. 
?   	
 ? (Chen Zhi Qiang took an electing 
activity), a sentence, occurs one time. 
Input:   	
  
After initial segmentation and detection: 
               (?)   (?)   (?)  	 (?)  
 (?) 
 
                    3        3          1         2                   priority 
After first iteration: 
          (uw)  (?)   	 (?)  
 (?) 
 
3          1            2                    priority 
After second iteration: 
   (uw)  	 (?)  
 (?) 
 
                                               2                         priority 
After third iteration: 
                       (uw) 	
 (uw) 
===================================== 
 
Figure 3. Extraction process of input ?   	
 ?. 
 
By the general rules and greedy strategy, besides 
overlapping character pair ambiguity, the 
algorithm is able to deal with more complex 
overlapping and coverage ambiguity, even which 
result from consecutive unknown words. In finger 
3, input sentence ?  ? is derived as the 
correct two unknown words ?((  )  )? and ?( 
 )? by rule (2), rule (10), and rule (2) in turn. ? 
 ? and ?  ? are not further merged. That is 
because P(  |  )<1 violates the constraint 
of rule (1). Same reason explains why ?  ? 
and ?  ? do not satisfy rule (10) in the third 
iteration. 
By this simple algorithm, unknown words with 
unlimited length all have possibilities to be ex-
tracted. Observing the extraction process of ? Introduction to CKIP Chinese Word Segmentation System for the First 
 International Chinese Word Segmentation Bakeoff 
Wei-Yun Ma 
Institute of Information science,  
Academia Sinica 
ma@iis.sinica.edu.tw 
Keh-Jiann Chen 
Institute of Information science,  
Academia Sinica 
kchen@iis.sinica.edu.tw 
 
 
 
 
 
 
 
Abstract 
In this paper, we roughly described the 
procedures of our segmentation system, 
including the methods for resolving  seg-
mentation ambiguities and identifying un-
known words. The CKIP group of 
Academia Sinica participated in testing on 
open and closed tracks of Beijing Univer-
sity (PK) and Hong Kong Cityu (HK). 
The evaluation results show our system 
performs very well in either HK open 
track or HK closed track and just accept-
able in PK tracks. Some explanations and 
analysis are presented in this paper. 
1 Introduction 
At the first international Chinese Word 
Segmentation Bakeoff, Academia Sinica 
participated in testing on open and closed tracks of 
Beijing University (PK) and Hong Kong Cityu 
(HK). The same segmentation algorithm was 
applied to process these two corpora, except that 
character code conversion from GB to BIG5 for 
PK corpus and few modifications due to different 
segmentation standards had been made. The 
difference between open and closed tracks is that 
while processing the open track, besides of the 
lexicon trained from the specific corpus, we also 
consulted the Academia Sinica lexicon to enhance 
the word collection. 
    It is well known that there are two major 
difficulties in Chinese word segmentation. One is 
resolving the ambiguous segmentation, and the 
other is identifying unknown words. 
Our earlier work mainly focused on the 
resolving of segmentation ambiguities and using 
regular expressions to handle the determinant-
measure and reduplication compounds (Chen & 
Liu 1992, Chen 1999). We adopt a variation of the 
longest matching algorithm with several heuristic 
rules to resolve the ambiguities and achieve 
99.77% of the success rate without counting the 
mistakes occurred due to the existence of unknown 
words. After that, we were paying more attention 
on the problems of extracting and identifying 
unknown words (Chen et.al 1997, Chen & Bai 
1998, Chen & Ma 2002, Tseng & Chen 2002, Ma 
& Chen 2003). The process of unknown word 
extraction could be roughly divided into two steps, 
i.e. detection process and extraction process. The 
detection process detects possible occurrences of 
unknown words (Chen & Bai 1998), so that deeper 
morphological analysis is carried out only at the 
places where unknown word morphemes were 
detected (Chen & Ma 2002). A bottom-up merging 
algorithm was proposed in (Ma & Chen 2003), 
which utilizes hybrid statistical and linguistic 
information to extract unknown words effectively. 
In addition to the bakeoff results evaluated by 
SIGHAN, we also present some other relevant 
experiment results and provide analysis on the 
system performance in the following sections. 
2 System Overview 
Figure 1 illustrates the block diagram of our 
segmentation system used in this contest. The first 
two steps of word segmentation algorithm are 
word matching and resolution for ambiguous 
matches. These two processes were performed in 
parallel. The algorithm reads the input sentences 
from left to right and matches the input character 
string with lexemes. In (Chen & Liu 1992), if an 
ambiguous segmentation does occur, the matching 
algorithm looks ahead two more words, and the 
disambiguation rules for those three word chunks 
is applied afterward. For instance, in (1), the first 
matched word could be '
 
' or '
 
'. Then the 
algorithm will look ahead to take all of the possible 
combinations of three word chunks, as shown in 
(2), into consideration. 
 
 
 
Figure 1. Flowchart of the system 
 
 (1)                             	  
complete  authenticate  report 
"complete the report about authenticating" 
 
(2)                      
 

                  
 

                	  
 
   
The disambiguation algorithm will select the 
first word of the most plausible chunks as the 
solution according to heuristic rules. The first 
heuristic rule is: 
 
Longest Matching Rule: The most plausible seg-
mentation is the three word sequence with the 
longest length. 
In the above example, the longest matched 
three-word chunk is (1). Therefore the first seg-
mented word is '  '. This heuristic rules 
achieves as high as 99.69% accuracy and a high 
applicability of 93.21%, i.e. the 93.21% of the am-
biguities were resolved by this rule. However there 
are still about 6.79% of ambiguities, i.e. the three 
word chunks with the same length but with differ-
ent segmentations, which cannot be resolved by the 
maximal matching rule. The following heuristic 
rules were used for further resolution. 
 
Word Length Rule: Pick the three-word chunk 
that has the smallest standard deviation in length of 
the three words.  
Morphemic Rules: 
(a). Pick the chunk with fewer bound morphemes. 
(b). Pick the chunk with fewer characters in com-
pound words. 
Probability Rule: 
(a). Pick the chunk with the high frequency mono-
syllabic words. 
(b). Pick the chunk with the highest probability 
value. 
 
After disambiguation process, an input sentence 
is segmented into a word sequence. Then for the 
needs of the following unknown word extraction, a 
Pos bi-gram tagging model is applied to tag Pos of 
words. 
It is clear that unknown words in the input text 
will be segmented into several adjacent tokens 
(known words or monosyllabic morphemes). Then 
at unknown word detection stage, every 
monosyllable is decided whether it is a word or an 
unknown word morpheme by a set of syntactic 
discriminators, which are trained from a word 
segmented corpus. 
 
 
(3)                               
    if    can   increase   gross profit rate 
"if gross profit rate can be increased?" 
 
(4)   after first step word segmentation: 
                                        
        after unknown word detection: 
                          (?)     (?)    (?) 
        after unknown word extraction: 
                         	  
 
For example, the correct segmentation of (3) is 
shown, but the unknown word ? 
 ? is 
segmented into three monosyllabic words after the 
first step of word segmentation process. In (4), The 
unknown word detection process will mark the 
sentence as ?   ()   ()   ()   (?)   (?)   (?)?, 
where (?) denotes the detected monosyllabic 
unknown word morpheme and () denotes common 
words. During extracting process, the rule 
matching process focuses on the morphemes 
marked with (?) only and tries to combine them 
with left/right neighbors according to the rules for 
unknown words. After that the unknown word ? 
 ? is extracted. 
We adopt a bottom-up merging algorithm (Ma & 
Chen 2003), which utilizes hybrid statistical and 
linguistic information, to extract unknown words. 
3 Adaptation for Different Tracks 
It is known that different segmentation standards 
could affect the performance of segmentation 
significantly. In this contest, due to limited 
preparing time, we mainly focused on adjusting the 
regular expressions for determinant-measure 
compounds according to the HK and PK 
segmentation standards. 
While processing the PK track, a shortcut 
method of converting GB codes to BIG5 codes was 
adopted to cope with the problem of character 
coding difference. Instead of re-design or re-
implement the GB segmentation system, we 
convert the codes of training and testing PK 
corpora into BIG5 versions and perform the 
segmentation under the BIG5 environment. The 
segmented results are then translated back to GB 
code as the final outputs. In contrast, processing of 
HK corpus is easier for us, because our system was 
designed for the BIG5 environment. 
As for the lexicons, for closed test, both PK and 
HK lexicons are derived from the word sets of 
each respective training corpus. For the open test, 
each lexicon was enhanced by adding the lexical 
entries in the CKIP lexicon. The sizes of lexicons 
are shown in table1.  
 
 HK PK 
# of lexical entries (HK/PK)for 
closed test 
22K 50K 
# of lexical entries (HK/PK join 
CKIP) for open test 
140K 156
K 
 
Note: # lexicon of (CKIP) is 133K 
 
Table 1. The sizes of lexicons 
 
Syntactic categories of a words were utilized in 
unknown word detection and extraction processes. 
We don?t have syntactic categories for words 
which are not in the CKIP lexicon. Therefore, we 
(Chen et.al 1997, Tseng & Chen 2002) use 
association strength between morphemes and 
syntactic categories to predict the category of a 
new word. The accuracy rate is about 80%. 
4 Evaluation Results 
There are several evaluation indexes provided 
by SIGHAN, i.e. test recall (R), test precision (P), 
F score2, the out-of-vocabulary (OOV) rate for the 
test corpus, the recall on OOV words (Roov), and 
the recall on in-vocabulary (Riv) words.  
Tables 2 shows the evaluation results of our sys-
tem in HK closed and open tracks. For both tracks, 
our system achieved the top ranks on F scores. 
 
 R P F OOV Roov Riv 
Closed 0.947 0.934 0.940 0.071 0.625 0.972 
Open 0.958 0.954 0.956 0.071 0.788 0.971 
 
Note: The word count of testing corpus is 34955 
                     
Table 2. Scores for HK 
 
The evaluations of our system in PK closed and 
open tracks are shown in table 3. For PK closed 
track, our system ranks 6th among 10 systems. And 
for PK open track, our system ranks 3rd among 8 
systems. 
 
 R P F OOV Roov Riv 
Closed 0.939 0.934 0.936 0.069 0.642 0.961 
Open 0.939 0.938 0.938 0.069 0.675 0.959 
 
Note: The word count of testing corpus is 34955 
                     
Table 3. Scores for PK 
 
Because Academia Sinica corpora (AS) are 
provided by us, we are not allowed to participate 
any AS track at this contest. Therefore, in this 
report, we still show the performance of our 
system evaluating AS closed track in table 4. Our 
system would have the top rank if the result was 
compared with the other  6 participants of AS 
closed track. 
 
R P F OOV Roov Riv 
0.968 0.966 0.967 0.022 0.657 0.975 
 
Note: The word count of testing corpus is 11985 
 
Table 4. Scores for AS closed 
5 Discussions and Conclusions 
The evaluation results show that our system 
performs very well in either HK closed track or 
HK open track. We think the key to the success is 
our unknown word extraction performs better than 
other participants. This could be observed by the 
results of HK closed track, the 2th and 3th system, 
which have better performance in Riv but worse 
Roov than our system, performs worse than our 
system in f score. Furthermore to have better 
performance, high precision for unknown word 
extraction is necessary, since one identification 
error may cause at least two segmentation errors. 
The performance in PK tracks are not as well as 
HK. An important  reason is that coding 
conversion may cause errors. For instance, in the 
conversion of the GB code of ?   ? (the capital 
of Brazil) to its BIG5 codes, Since GB code to 
BIG5 conversion is a one-to-many mapping, the 
above example is wrongly converted to ?  ?. 
This kind of errors do affect accuracy of the 
segmentation significantly, especially for the 
unkown word processes. To solve this problem, we 
think the best and direct solution is to re-
implement the GB segmentation version without 
any code conversion.  
Variation on the word segmentation standards is 
another reason of causing segmentation errors. 
Some of the standards were even not available to 
the public. It is better to propose a uniform word 
segmentation standard in the future.  
Regarding evaluation index, we suggest that an 
error type of crossing error should be take into 
consideration, since noncrossing errors are more or 
less related to segmentation standards and crossing 
errors are more severe. 
 
6 References 
[1]  Chen, K.J. & S.H. Liu, 1992,"Word Identification 
for Mandarin Chinese Sentences," Proceedings of 
14th Coling, pp. 101-107 
[2]  Chen, C. J., M.H. Bai, & K.J. Chen, 1997,? Cate-
gory Guessing for Chinese Unknown Words,? Pro-
ceedings of the Natural Language Processing 
Pacific Rim Symposium, 35-40, Thailand. 
[3]  Chen, K.J. & Ming-Hong Bai, 1998, ?Unknown 
Word Detection for Chinese by a Corpus-based 
Learning Method,? international Journal of Com-
putational linguistics and Chinese Language 
Processing, Vol.3, #1, pp.27-44 
[4]  Chen, Keh-jiann,1999,?Lexical Analysis for Chi-
nese- Difficulties and Possible Solutions?, Journal 
of Chinese Institute of Engineers, Vol. 22. #5, pp. 
561-571. 
[5]   Chen, K.J. & Wei-Yun Ma, 2002. Unknown Word 
Extraction for Chinese Documents. In Proceedings 
of COLING 2002, pages 169-175 
[6]   Tseng, H.H. & K.J. Chen, 2002. Design of Chinese 
Morphological Analyzer. In Proceedings of 
SIGHAN, pages 49-55 
[7]  Ma Wei-Yun & K.J. Chen, 2003. A bottom-up 
Merging Algorithm for Chinese Unknown Word 
Extraction. In Proceedings of SIGHAN 
 
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 333?336,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Where's the Verb? 
Correcting Machine Translation During Question Answering 
Wei-Yun Ma, Kathleen McKeown 
Department of Computer Science 
Columbia University 
New York, NY 10027, USA 
{ma,kathy}@cs.columbia.edu 
 
 
 
 
 
 
 
 
 
Abstract 
 
When a multi-lingual question-answering (QA) 
system provides an answer that has been 
incorrectly translated, it is very likely to be 
regarded as irrelevant. In this paper, we 
propose a novel method for correcting a 
deletion error that affects overall 
understanding of the sentence. Our post-editing 
technique uses information available at query 
time: examples drawn from related documents 
determined to be relevant to the query. Our 
results show that 4%-7% of MT sentences are 
missing the main verb and on average, 79% of 
the modified sentences are judged to be more 
comprehensible. The QA performance also 
benefits  from the improved MT: 7% of 
irrelevant response sentences become relevant. 
1. Introduction 
We are developing a multi-lingual question-
answering (QA) system that must provide 
relevant English answers for a given query, 
drawing pieces of the answer from translated 
foreign source. Relevance and translation quality 
are usually inseparable: an incorrectly translated 
sentence in the answer is very likely to be 
regarded as irrelevant even when the 
corresponding source language sentence is 
actually relevant. We use a phrase-based 
statistical machine translation system for the MT 
component and thus, for us, MT serves as a 
black box that produces the translated 
documents in our corpus; we cannot change the 
MT system itself. As MT is used in more and 
more multi-lingual applications, this situation 
will become quite common.  
We propose a novel method which uses 
redundant information available at question-
answering time to correct errors. We present a 
post-editing mechanism to both detect and 
correct errors in translated documents 
determined to be relevant for the response. In 
this paper, we focus on cases where the main 
verb of a Chinese sentence has not been 
translated. The main verb usually plays a crucial 
role in conveying the meaning of a sentence. In 
cases where only the main verb is missing, an 
MT score relying on edit distance (e.g., TER or 
Bleu) may be high, but the sentence may 
nonetheless be incomprehensible.  
Handling this problem at query time rather 
than during SMT gives us valuable information 
which was not available during SMT, namely, a 
set of related sentences and their translations 
which may contain the missing verb. By using 
translation examples of verb phrases and 
alignment information in the related documents, 
we are able to find an appropriate English verb 
and embed it in the right position as the main 
verb in order to improve MT quality. 
 A missing main verb can result in an incom-
prehensible sentence as seen here where the 
Chinese verb ???? was not translated at all. 
 
MT:          On December 13 Saddam . 
REF :        On December 13 Saddam was arrested. 
Chinese:   12?13??????? 
 
In other cases, a deleted main verb can result 
in miscommunication; below the Chinese verb 
???? should have been translated as 
?reduced?. An English native speaker could 
easily misunderstand the meaning to be ?People 
love classical music every year.? which happens 
to be the opposite of the original intended 
meaning. 
 
 
MT:          People of classical music loving every year.  
REF :        People?s love for classical music reduced every year. 
Chinese:   ??????????????? 
2. Related Work 
Post-editing has been used in full MT systems 
for tasks such as article selection (a, an, the) for 
333
English noun phrases (Knight and Chander 
1994). Simard et alin 2007 even developed a 
statistical phrase based MT system in a post-
editing task, which takes the output of a rule-
based MT system and produces post-edited 
target-language text.  Zwarts et al (2008) target 
selecting the best of a set of outputs from 
different MT systems through their 
classification-based approach. Others have also 
proposed using the question-answering context 
to detect errors in MT, showing how to correct 
names (Parton et. al 2008, Ji et. al 2008). 
3. System Overview 
The architecture of our QA system is shown in 
Figure 1. Our MT post-editing system (the bold 
block in Figure 1) runs after document retrieval 
has retrieved all potentially relevant documents 
and before the response generator selects 
sentences for the answer. It modifies any MT 
documents retrieved by the embedded 
information retrieval system that are missing a 
main verb. All MT results are provided by a 
phrase-based SMT system.  
   Post-editing includes three steps: detect a 
clause with a missing main verb, determine 
which Chinese verb should have been translated, 
and find an example sentence in the related 
documents with an appropriate sentence which 
can be used to modify the sentence in question.  
To detect clauses, we first tag the corpus using a 
Conditional Random Fields (CRF) POS tagger 
and then use manually designed regular 
expressions to identify main clauses of the 
sentence, subordinate clauses (i.e., clauses which 
are arguments to a verb) and conjunct clauses in 
a sentence with conjunction. We do not handle 
adjunct clauses. Hereafter, we simply refer to all 
of these as ?clause?. If a clause does not have 
any POS tag that can serve as a main verb (VB, 
VBD, VBP, VBZ), it is marked as missing a 
main verb.  
   MT alignment information is used to further 
ensure that these marked clauses are really 
missing main verbs.  We segment and tag the 
Chinese source sentence using the Stanford 
Chinese segmenter and the CRF Chinese POS 
tagger developed by Purdue University. If we 
find a verb phrase in the Chinese source 
sentence that was not aligned with any English 
words in the SMT alignment tables, then we 
label it as a verb translation gap (VTG) and 
confirm that the marking was correct. 
   In the following sections, we describe how we 
determine which Chinese verb should have been 
translated and how that occurs. 
Query in English
Document Retrieval
Detecting Possible Clauses 
with no Main Verb
Finding the Main Verb Position
Obtain Translation of the Main
Verb and embed it to the 
translated sentence
Corpus of translated 
English documents with
Chinese-English word 
alignment
Dynamic Verb 
Phrase Table
Static Verb 
Phrase Table 
Retrieved English docs
Modified English docs
Response Generator
Response in English  
Figure 1. The System Pipeline 
4. Finding the Main Verb Position  
Chinese ordering differs from English mainly 
in clause ordering (Wang et al, 2007) and 
within the noun phrase. But within a clause 
centered by a verb, Chinese mostly uses a SVO 
or SV structure, like English (Yamada and 
Knight 2001), and we can assume the local 
alignment centered by a verb between Chinese 
and English is a linear mapping relation. Under 
this assumption, the translation of ???? in the 
above example should be placed in the position 
between ?Saddam? and ?.?. Thus, once we find a 
VTG, its translation can be inserted into the 
corresponding position of the target sentence 
using the alignment.  
This assumes, however, that there is only one 
VTG found within a clause. In practice, more 
than one VTG may be found in a clause. If we 
choose one of them, we risk making the wrong 
choice. Instead, we insert the translations of both 
VTGs simultaneously. This strategy could result 
in more than one main verb in a clause, but it is 
more helpful than having no verb at all. 
5. Obtaining a VTG Translation 
We translate VTGs by using verb redundancy 
in related documents: if the VTG was translated 
in other places in related documents, the existing 
translations can be reused. Related documents 
are likely to use a good translation for a specific 
VTG as it is used in a similar context. A verb?s 
aspect and tense can be directly determined by 
referencing the corresponding MT examples and 
their contexts. If, unfortunately, a given VTG 
334
did not have any other translation record, then 
the VTG will not be processed. 
To do this, our system first builds verb phrase 
tables from relevant documents and then uses 
the tables to translate the VTG. We use two verb 
phrase tables: one is built from a collection of 
MT documents before any query and is called 
the ?Static Verb Phrase Table?, and the other 
one is dynamically built from the retrieved 
relevant MT documents for each query and is 
called the ?Dynamic Verb Phrase Table?.  
The construction procedure is the same for 
both. Given a set of related MT documents and 
their MT alignments, we collect all Chinese verb 
phrases and their translations along with their 
frequencies and contexts. 
One key issue is to decide appropriate 
contextual features of a verb. A number of 
researchers (Cabezas and Resnik 2005, Carpuat 
and Wu 2007) provide abundant evidence that 
rich context features are useful in MT tasks. 
Carpuat and Wu (2007) tried to integrate a 
Phrase Sense Disambiguation (PSD) model into 
their Chinese-English SMT system and they 
found that the POS tag preceding a given phrase, 
the POS tag following the phrase and bag-of-
words are the three most useful features. 
Following their approach, we use the word 
preceding and the word following a verb as the 
context features. 
The Static and Dynamic Verb Phrase Tables 
provide us with MT examples to translate a 
VTG. The system first references the Dynamic 
Verb Phrase Table as it is more likely to yield a 
good translation. If the record is not found, the 
Static one is referenced. If it is not found in 
either, the given VTG will not be processed. No 
matter which table is referenced, the following 
Naive Bayes equation is applied to obtain the 
translation of a given VTG. 
 
))|(log)|(log)((logmaxarg    
),|(maxarg'
kkk
t
k
t
tfwPtpwPtP
fwpwtPt
k
k
++=
=
 
pw, fw and tk respectively represent the 
preceding source word, the following source 
word and a translation candidate of a VTG. 
6.  Experiments 
Our test data is drawn from Chinese-English MT 
results generated by Aachen?s 2007 RWTH sys-
tem (Mauser et al, 2007), a phrase-based SMT 
system with 38.5% BLEU score on IWSLT 
2007 evaluation data.  
Newswires and blog articles are retrieved for 
five queries which served as our experimental 
test bed. The queries are open-ended and on av-
erage, answers were 30 sentences in length. 
 
Q1: Who/What is involved in Saddam Hussein's trial 
Q2: Produce a biography of Jacques Rene Chirac 
Q3: Describe arrests of person from Salafist Group for 
Preaching and Combat 
Q4: Provide information on Chen Sui Bian 
Q5: What connections are there between World Cup games and 
stock markets? 
 
We used MT documents retrieved by IR for 
each query to build the Dynamic Verb Phrase 
Table. We tested the system on 18,886 MT 
sentences from the retrieved MT documents for 
all of the five queries. Among these MT 
sentences, 1,142 sentences were detected and 
modified (6 % of all retrieved MT sentences). 
6.1 Evaluation Methodology 
For evaluation, we used human judgments of the 
modified and original MT. We did not have 
reference translations for the data used by our 
question-answering system and thus, could not 
use metrics such as TER or Bleu. Moreover, at 
best, TER or Bleu score would increase by a 
small amount and that is only if we select the 
same main verb in the same position as the 
reference. Critically, we also know that a 
missing main verb can cause major problems 
with comprehension. Thus, readers could better 
determine if the modified sentence better 
captured the meaning of the source sentence. We 
also evaluated relevance of a sentence to a query 
before and after modification. 
We recruited 13 Chinese native speakers who 
are also proficient in English to judge MT 
quality. Native English speakers cannot tell 
which translation is better since they do not 
understand the meaning of the original Chinese. 
To judge relevance to the query, we used native 
English speakers. 
Each modified sentence was evaluated by 
three people. They were shown the Chinese 
sentence and two translations, the original MT 
and the modified one. Evaluators did not know 
which MT sentence was modified. They were 
asked to decide which sentence is a better 
translation, after reading the Chinese sentence. 
An evaluator also had the option of answering 
?no difference?.  
6.2 Results and Discussion 
We used majority voting (two out of three) to 
decide the final evaluation of a sentence judged 
by three people. On average, 900 (79%) of the 
335
1142 modified sentences, which comprise 5% of 
all 18,886 retrieved MT sentences, are better 
than the original sentences based on majority 
voting. And for 629 (70%) of these 900 better 
modified sentences all three evaluators agreed 
that the modified sentence is better. 
 Furthermore, we found that for every 
individual query, the evaluators preferred more 
of the modified sentences than the original MT. 
And among these improved sentences, 81% 
sentences reference the Dynamic Verb Phrase 
Table, while only 19% sentences had to draw 
from the Static Verb Phrase Table, thus 
demonstrating that the question answering 
context is quite helpful in improving MT. 
We also evaluated the impact of post-editing 
on the 234 sentences returned by our response 
generator. In our QA task, response sentences 
were judged as ?Relevant(R)?, ?Partially 
Relevant(PR)?, ?Irrelevant(I)? and ?Too little 
information to judge(T)? sentences. With our 
post-editing technique, 7% of 141 I/T responses 
become R/PR responses and none of the R/PR 
responses become I/T responses. This means 
that R/PR response percentage has an increase of 
4%, thus demonstrating that our correction of 
MT truly improves QA performance. An 
example of a change from T to PR is: 
 
 
Question: What connections are there between World Cup games 
and stock markets? 
Original QA answer: But if winning the ball, not necessarily in 
the stock market. 
Modified QA answer: But if winning the ball, not necessarily in 
the stock market increased.  
6.3 Analysis of Different MT Systems 
In order to examine how often missing verbs 
occur in different recent MT systems, in addition 
to using Aachen?s up-to-date system ? ?RWTH-
PBT?of 2008, we also ran the detection process 
for another state-of-the-art MT system ? ?SRI-
HPBT? (Hierarchical Phrase-Based System) of 
2008 provided by SRI, which uses a grammar on 
the target side as well as reordering, and focuses 
on improving grammaticality of the target 
language. Based on a government 2008 MT 
evaluation, the systems achieve 30.3% and 
30.9% BLEU scores respectively. We used the 
same test set, which includes 94 written articles 
(953 sentences). 
Overall, 7% of sentences translated by 
RWTH-PBT are detected with missing verbs 
while 4% of sentences translated by SRI-HPBT 
are detected with missing verb. This shows that 
while MT systems improve every year, missing 
verbs remain a problem.  
7 Conclusions 
In this paper, we have presented a technique for 
detecting and correcting deletion errors in trans-
lated Chinese answers as part of a multi-lingual 
QA system. Our approach uses a regular gram-
mar and alignment information to detect missing 
verbs and draws from examples in documents 
determined to be relevant to the query to insert a 
new verb translation. Our evaluation demon-
strates that MT quality and QA performance are 
both improved. In the future, we plan to extend 
our approach to tackle other MT error types by 
using information available at query time. 
Acknowledgments 
This material is based upon work supported 
by the Defense Advanced Research Projects 
Agency under Contract No. HR0011-06-C-0023 
References 
Clara Cabezas and Philip Resnik. 2005. Using WSD 
Techniques for Lexical Selection in Statistical 
Machine, Translation Technical report CS-TR-
4736  
Marine Carpuat and Dekai Wu. 2007. Context-
Dependent Phrasal Translation Lexicons for 
Statistical Machine Translation, Machine 
Translation Summit XI, Copenhagen 
Heng Ji, Ralph Grishman and Wen Wang. 2008. 
Phonetic Name Matching For Cross-lingual 
Spoken Sentence Retrieval, IEEE-ACL SLT08. 
Goa, India 
K. Knight and I. Chander. 1994. Automated 
Postediting of Documents, AAAI 
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. 2008. Simultaneous 
multilingual search for translingual information 
retrieval,  ACM 17th CIKM 
Arne Mauser, David Vilar, Gregor Leusch, Yuqi 
Zhang, and Hermann Ney. 2007. The RWTH 
Machine Translation System for IWSLT 2007, 
IWSLT 
Michel Simard, Cyril Goutte and Pierre Isabelle. 
2007. Statistical Phrase-based Post-Editing, 
NAACL-HLT 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for 
Statistical Machine Translation, EMNLP-
CoNLL. 
Kenji Yamada , Kevin Knight. 2001. A syntax-based 
statistical translation model, ACL 
S. Zwarts and M. Dras. 2008. Choosing the Right 
Translation: A Syntactically Informed 
Approach, COLING 
336
Proceedings of NAACL-HLT 2013, pages 433?438,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using a Supertagged Dependency Language Model to Select 
a Good Translation in System Combination 
 
Wei-Yun Ma Kathleen McKeown 
Department of Computer Science Department of Computer Science 
Columbia University Columbia University 
New York, NY 10027, USA New York, NY 10027, USA 
ma@cs.columbia.edu kathy@cs.columbia.edu 
 
 
 
 
 
 
Abstract 
We present a novel, structured language 
model - Supertagged Dependency Language 
Model to model the syntactic dependencies 
between words. The goal is to identify 
ungrammatical hypotheses from a set of 
candidate translations in a MT system 
combination framework and help select the 
best translation candidates using a variety of 
sentence-level features. We use a two-step 
mechanism based on constituent parsing and 
elementary tree extraction to obtain supertags 
and their dependency relations. Our 
experiments show that the structured language 
model provides significant improvement in 
the framework of sentence-level system 
combination. 
1 Introduction 
In recent years, there has been a burgeoning 
interest in incorporating syntactic structure into 
Statistical machine translation (SMT) models (e..g, 
Galley et al, 2006; DeNeefe and Knight 2009; 
Quirk et al, 2005). In addition to modeling 
syntactic structure in the decoding process, a 
methodology for candidate translation selection 
has also emerged. This methodology first generates 
multiple candidate translations followed by 
rescoring using global sentence-level syntactic 
features to select the final translation. The 
advantage of this methodology is that it allows for 
easy integration of complex syntactic features that 
would be too expensive to use during the decoding 
process. The methodology is usually applied in two 
scenarios: one is as part of an n-best reranking 
(Och et al, 2004; Hasan et al, 2006), where n-best 
candidate translations are generated through a 
decoding process. The other is translation selection 
or reranking (Hildebrand and Vogel 2008; 
Callison-Burch et al, 2012), where candidate 
translations are generated by different decoding 
processes or different decoders.  
This paper belongs to the latter; the goal is to 
identify ungrammatical hypotheses from given 
candidate translations using grammatical 
knowledge in the target language that expresses 
syntactic dependencies between words. To achieve 
that, we propose a novel Structured Language 
Model (SLM) - Supertagged Dependency 
Language Model (SDLM) to model the syntactic 
dependencies between words. Supertag (Bangalore 
and Joshi, 1999) is an elementary syntactic 
structure based on Lexicalized Tree Adjoining 
Grammar (LTAG). Traditional supertagged n-gram 
LM predicts the next supertag based on the 
immediate words to the left with supertags, so it 
can not explicitly model long-distance dependency 
relations. In contrast, SDLM predicts the next 
supertag using the words with supertags on which 
it syntactically depend, and these words could be 
anywhere and arbitrarily far apart in a sentence. A 
candidate translation?s grammatical degree or 
?fluency? can be measured by simply calculating 
the SDLM likelihood of the supertagged 
dependency structure that spans the entire sentence. 
To obtain the supertagged dependency structure, 
the most intuitive way is through a LTAG parser 
(Schabes et al, 1988). However, this could be very 
433
slow as it has time complexity of O(n6).  Instead 
we propose an alternative mechanism in this paper: 
first we use a constituent parser1 of O(n3) ~ O(n5) 
to obtain the parse of a sentence, and then we 
extract elementary trees with dependencies from 
the parse in linear time.  Aside from the 
consideration of time complexity, another 
motivation of this two-step mechanism is that 
compared with LTAG parsing, the mechanism is 
more flexible for defining syntactic structures of 
elementary trees for our needs. Because those 
structures are defined only within the elementary 
tree extractor, we can easily adjust the definition of 
those structures within the extractor and avoid 
redesigning or retraining our constituent parser. 
We experiment with sentence-level translation 
combination of five different translation systems; 
the goal is for the system to select the best 
translation for each input source sentence among 
the translations provided by the five systems. The 
results show a significant improvement of 1.45 
Bleu score over the best single MT system and 
0.72 Bleu score over a baseline sentence-level 
combination system of using consensus and n-
gram LM. 
2 Related Work  
Och et al, (2004) investigated various syntactic 
feature functions to rerank the n-best candidate 
translations. Most features are syntactically 
motivated and based on alignment information 
between the source sentence and the target 
translation. The results are rather disappointing. 
Only the non-syntactic IBM model 1 yielded 
significant improvement. All other tree-based 
feature functions had only a very small effect on 
the performance. 
In contrast to (Och et al, 2004)?s bilingual 
syntax features, Hasan et al, (2006) focused on 
monolingual syntax features in n-best reranking. 
They also investigated the effect of directly using 
the log-likelihood of the output of a HMM-based 
supertagger, and found it did not improve 
performance significantly. It is worth noticing that 
this log-likelihood is based on supertagged n-gram 
                                                          
1  Stanford parser (http://nlp.stanford.edu/software/lex-
parser.shtml). We use its PCFG version of O(n3) for SDLM 
training of part of Gigaword in addition to Treebank and use 
its factor version of O(n5) to calculate the SDLM likelihood of 
translations. 
LM, which is one type of class-based n-gram LM, 
so it does not model explicit syntactic 
dependencies between words in contrast to the 
work we describe in this paper. Hardmeier et al, 
(2012) use tree kernels over constituency and 
dependency parse trees for either the input or 
output sentences to identify constructions that are 
difficult to translate in the source language, and 
doubtful syntactic structures in the output language. 
The tree fragments extracted by their tree kernels 
are similar to our elementary trees but they only 
regard them as the individual inputs of support 
vector machine regression while binary relations of 
our elementary trees are considered in a 
formulation of a structural language model. 
Outside the field of candidate translation 
selection, Hassan et al, (2007) proposed a phrase-
based SMT model that integrates supertags into the 
target side of the translation model and the target 
n-gram LM. Two kinds of supertags are employed: 
those from LTAG and Combinatory Categorial 
Grannar (CCG), and both yield similar 
improvements. They found that using both or 
either of the supertag-based translation model and 
supertagged LM can achieve significant 
improvement. Again, the supertagged LM is a 
class-based n-gram LM and does not model 
explicit syntactic dependencies during decoding. 
In the field of MT system combination, word-
level confusion network decoding is one of the 
most successful approaches (Matusov et al, 2006; 
Rosti et al, 2007; He et al 2008; Karakos et al 
2008; Sim et al 2007; Xu et al 2011). It is capable 
of generating brand new translations but it is 
difficult to consider more complex syntax such as 
dependency LM during decoding since it adds one 
word at a time while a dependency based LM must 
parse a complete sentence. Typically, a confusion 
network approach selects one translation as the 
best and uses this as the backbone for the 
confusion network. The work we present here 
could provide a more sophisticated mechanism for 
selecting the backbone. Alternatively, one can 
enhance confusion network models by 
collaborating with a sentence-level combination 
model which uses complex syntax to re-rank n-best 
outputs of a confusion network model. This kind of 
collaboration is one of our future works. 
 
 
434
3 LTAG and Supertag 
LTAG (Joshi et al, 1975; Schabes et al, 1988) is a 
formal tree rewriting formalism, which consists of 
a set of elementary trees, corresponding to minimal 
linguistic structures that localize dependencies, 
including long-distance dependencies, such as 
predicate-argument structure. Each elementary tree 
is associated with at least one lexical item on its 
frontier. The lexical item associated with an 
elementary tree is called the anchor in that tree; an 
elementary tree thus serves as a description of 
syntactic constraints of the anchor. The elementary 
syntactic structures of elementary trees are called 
supertags (Bangalore and Joshi, 1999), in order to 
distinguish them from the standard part-of-speech 
tags. Some examples are provided in figure 1 (b).
Elementary trees are divided into initial and 
auxiliary trees. Initial trees are those for which all 
non-terminal nodes on the frontier are substitutable. 
Auxiliary trees are defined as initial trees, except 
that exactly one frontier, non-terminal node must 
be a foot node, with the same label as the root node. 
Two operations - substitution and adjunction - are 
provided in LTAG to combine elementary trees 
into a derived tree. 
4 SDLM 
Our goal is to use SDLM to calculate the 
grammaticality of translated sentences. We do this 
by calculating the likelihood of the supertagged 
dependency structure that spans the entire sentence 
using SDLM. To obtain the supertagged 
dependency linkage, the most intuitive way is 
through a LTAG parser (Schabes et al, 1988). 
However, this could be very slow as it has time 
complexity of O(n6). Another possibility is to 
follow the procedure in (Joshi and Srinivas 1994, 
Bangalore and Joshi, 1999): use a HMM-based 
supertagger to assign words with supertags, 
followed by derivation of a shallow parse in linear 
time based on only the supertags to obtain the 
dependencies. But since this approach uses only 
the local context, in (Joshi and Srinivas 1994), they 
also proposed another greedy algorithm based on 
supertagged dependency probabilities to gradually 
select the path with the maximum path probability 
to extend to the remaining directions in the 
dependency list.  
In contrast to the LTAG parsing and 
supertagging-based approaches, we propose an 
alternative mechanism: first we use a state-of-the-
art constituent parser to obtain the parse of a 
sentence, and then we extract elementary trees with 
dependencies from the parse to assign each word 
with an elementary tree. The second step is similar 
to the approach used in extracting elementary trees 
from the TreeBank (Xia, 1999; Chen and Vijay-
Shanker, 2000).  
4.1 Elementary Tree Extraction 
We use an elementary tree extractor, a 
modification of (Chen and Vijay-Shanker, 2000), 
to serve our purpose. Heuristic rules were used to 
distinguish arguments from adjuncts, and the 
extraction process can be regarded as a process that 
gradually decomposes a constituent parse to 
multiple elementary trees and records substitutions 
and adjunctions. From elementary trees, we can 
obtain supertags by only considering syntactic 
structure and ignoring anchor words. Take the 
sentence ? ?The hungry boys ate dinner? as an 
example; the constituent parse and extracted 
supertags are shown in Figure 1. 
In Figure 1 (b), dotted lines represent the 
operations of substitution and adjunction. Note that 
each word in a translated sentence would be 
assigned exactly one elementary syntactic structure 
which is associated with a unique supertag id for 
the whole corpus. Different anchor words could 
own the same elementary syntactic structure and 
would be assigned the same supertag id, such as 
? 1?  ? for ?boys? and ?dinner?. For our corpus, 
around 1700 different elementary syntactic 
structures (1700 supertag ids) are extracted. 
 
 
 
 
Figure 1. (a) Parse of ?The hungry boys ate dinner?          
                                          
435
NP
the
NP*
boys
S
NP1? VP
ate
NP2?
NP
dinner
DT
anchor
word
NP
hungry
NP*JJ
anchor
word VB
anchor
word
NN
anchor
word
NP
NN
anchor
word
anchor
word:
elementary
syntactic
structure
(supertag):
supertag id: 1? 1?2?1? 2?
 
 
Figure 1. (b) Extracted elementary trees 
4.2 Model  
Bangalore and Joshi (1999) gave a concise 
description for dependencies between supertags: 
?A supertag is dependent on another supertag if the 
former substitutes or adjoins into the latter?. 
Following this description, for the example in 
Figure 1 (b), supertags of ?the? and ?hungry? are 
dependent on the supertag of ?boys?, and supertags 
of ?boys? and ?dinner? are dependent on the 
supertag of ?ate?. These dependencies between 
supertags also provide the dependencies between 
anchor words.  
Since the syntactic constraints for each word in 
its context are decided and described through its 
supertag, the likelihood of SDLM for a sentence 
could also be regarded as the degree of violations 
of the syntactic constraints on all words in the 
sentence. Consider a sentence S = w1 w2 ?wn with 
corresponding supertags T = t1 t2 ?tn. We use di=j 
to represent the dependency relations for words or 
supertags. For example, d3 = 5 means that w3 
depends on w5 or t3 depends on t5. We propose five 
different bigram SDLM as follows and evaluate 
their effects in section 5. 
 
 
 
 
 
 
 
 
               
 
 
SDLM model (2) is the approximation form of 
model (1); model (3) and (4) are individual terms 
of model (2); model (5) models word dependencies 
based on elementary tree dependencies. The 
estimation of the probabilities is done using 
maximum likelihood estimations with Laplace 
smoothing.  Take Figure 1 (b) as an example; if 
using model (1), the SDLM likelihood of ?The 
hungry boys ate dinner? is 
 
)|2,(*)2,|1,(
*)2,|1,(*)1,|2,(*)1,|1,(
rootatePatedinnerP
ateboysPboyshungryPboystheP
???
??????
 
In our experiment on sentence-level translation 
combination, we use a log-linear model to integrate 
all features including SDLM models. The 
corresponding weights are trained discriminatively 
for Bleu score using Minimum Error Rate Training 
(MERT). 
5 Experiment  
Our experiments are conducted and reported on the 
Chinese-English dataset from NIST 2008 
(LDC2010T01). It consists of four human 
reference translations and corresponding machine 
translations for the NIST Open MT08 test set, 
which consists of newswire and web data. The test 
set contains 105 documents with 1312 sentences 
and output from 23 machine translation systems. 
Each system provides the top one translation 
hypothesis for every sentence. We further divide 
the NIST Open MT08 test set into the tuning set 
and test set for our experiment of sentence-level 
translation combination. We divided the 1312 
sentences into tuning data of 524 sentences and the 
test set of 788 sentences. Out of 23 MT systems, 
we manually select the top five MT systems as our 
MT systems for our combination experiment. 
In terms of SDLM training, since the size of 
TreeBank-extracted elementary trees is much 
smaller compared to most practical n-gram LMs 
trained from the Gigaword corpus, we also extract 
elementary trees from automatically-generated 
parses of part of the Gigaword corpus (around one-
year newswire of ?afp_eng? in Gigaword 4) in 
addition to TreeBank-extracted elementary trees. 
5.1 Feature Functions 
For the baseline combination system, we use the 
following feature functions in the log-linear model 
to calculate the score of a system translation. 
 
z Sentence consensus based on Translation Edit 
Ratio (TER) 
z Gigaword-trained 3-gram LM and word 
penalty 
 
|? i model(5) SDLM                                                    )(
model(4) SDLM                                                       )|(
model(3) SDLM                                                       )|(
model(2) SDLM       )|()|()|(
model(1) SDLM                                             )|(
?
?
??
?
?
i
d
i
ii
i
di
ii
i
di
i
ddii
i
ddii
i
i
iii
ii
wwP
twP
ttP
twPttPtwtwP
twtwP
436
For testing SDLM, in additional to all features 
that the baseline combination system uses, we add 
single or multiple SDLM models in the log-linear 
model, and each SDLM model has its own weight. 
5.2 Result 
From table 1, we can see that the combination of 
SDLM model 3, 4 and 5 yields the best 
performance, which is better than the best MT 
system by Bleu of 1.45, TER of 0.67 and 
METEOR of 1.25, and also better than the baseline 
combination system by Bleu of 0.72, TER of 0.25 
and METEOR of 0.44. Compared with SDLM 
model 5, which represents a type of word 
dependency LM without labels, the results show 
that adding appropriate syntactic ?labels? (here, 
they are ?supertags?) on word dependencies brings 
benefits. 
 
 
Table 1. Result of Sentence-level Translation Combination 
6 Conclusion  
In this paper we presented Supertagged 
Dependency Language Model for explicitly 
modeling syntactic dependencies of the words of 
translated sentences. Our goal is to select the most 
grammatical translation from candidate translations.  
To obtain the supertagged dependency structure of 
a translation candidate, a two-step mechanism 
based on constituent parsing and elementary tree 
extraction is also proposed. SDLM shows its 
effectiveness in the scenario of translation 
selection.  
There are several avenues for future work: we 
have focused on bigram dependencies in our 
models; extension to more than two dependent 
elementary trees is straightforward. It would also 
be worth investigating the performance of using 
our sentence-level model to re-rank n-best outputs 
of a confusion network model. And in terms of 
applications, SDLM can be directly applied to 
many other NLP tasks, such as speech recognition 
and natural language generation. 
Acknowledgments  
We would like to thank Owen Rambow for 
providing the elementary tree extractor and also 
thank the anonymous reviewers for their helpful 
comments. This work is supported by the National 
Science Foundation via Grant No. 0910778 
entitled ?Richer Representations for Machine 
Translation?. All views expressed in this paper are 
those of the authors and do not necessarily 
represent the view of the National Science 
Foundation. 
References  
Srinivas Bangalore and Aravind K. Joshi. 1999. 
Supertagging: An approach to almost parsing. 
Computational Linguistics, 25(2):237?265.  
John Chen and K. Vijay-Shanker. 2000. Automated 
extraction of TAGs from the Penn treebank. In 
Proceedings of the Sixth International Workshop on 
Parsing Technologies 
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut and Lucia Specia. 2012. 
Findings of the 2012 Workshop on Statistical 
Machine Translation. In Proceedings of WMT12. 
Steve DeNeefe and Kevin Knight. 2009 Synchronous 
Tree Adjoining Machine Translation. In Proceedings 
of EMNLP 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang, Ignacio Thayer. 
2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proceedings of 
the Annual Meeting of the Association for 
Computational Linguistics 
Christian Hardmeier, Joakim Nivre and J?rg Tiedemann. 
2012. Tree Kernels for Machine Translation Quality 
Estimation. In Proceedings of WMT12 
S. Hasan, O. Bender, and H. Ney. 2006. Reranking 
translation hypotheses using structural properties. In 
Proceedings of the EACL'06 Workshop on Learning 
Structured Information in Natural Language 
Applications  
Hany Hassan , Khalil Sima'an and Andy Way. 2007. 
Supertagged Phrase-Based Statistical Machine 
Translation. In Proceedings of the Annual Meeting of 
the Association for Computational Linguistics 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, 
and Robert Moore. 2008. Indirect-hmm-based 
hypothesis alignment for computing outputs from 
machine translation systems. In Proceedings of 
EMNLP 
 Bleu TER METEOR 
Best MT system 30.16 55.45 54.43 
baseline 30.89 55.03 55.24 
baseline+ model 1 31.29 54.99 55.63 
baseline+ model 2 31.25 55.23 55.37 
baseline+ model 3 31.25 55.06 55.40 
baseline+ model 4 31.44 54.70 55.54 
baseline+ model 5 31.39 55.15 55.68 
baseline+ model 3+ 
model 4+ model 5 31.61 54.78 55.68 
437
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
Proceedings of the Eighth Conference of the 
Association for Machine Translation in the Americas 
Aravind K. Joshi and B. Srinivas. 1994. Disambiguation 
of super parts of speech (or supertags): Almost 
parsing. In Proceedings of the 15th International 
Conference on Computational Linguistics 
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 
1975. Tree Adjunct Grammars. Journal of Computer 
and System Science, 10:136?163. 
Evgeny Matusov, Nicola Ueffing, and Hermann Ney 
2006. Computing consensus translation from multiple 
machine translation systems using enhanced 
hypotheses alignment. In Proceedings of EACL 
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, and Dragomir Radev. 2004 A 
smorgasbord of features for statistical machine 
translation. In Proceedings of the Meeting of the 
North American chapter of the Association for 
Computational Linguistics 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine translation 
system combination using ITG-based alignments. In 
Proceedings of ACL-HLT 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Treelet Translation: Syntactically 
Informed Phrasal SMT, In Proceedings of the 
Association for Computational Linguistics 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007. Improved word-level system 
combination for machine translation. In Proceedings 
of ACL 
Yves Schabes, Anne Abeille and Aravind K. Joshi. 
1988. Parsing strategies with 'lexicalized' grammars: 
Application to Tree Adjoining Grammars. In 
Proceedings of the 12th International Conference on 
Computational Linguistics 
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi and P.C. 
Woodland .2007. Consensus Network Decoding for 
Statistical Machine Translation System Combination. 
In Proceedings of ICASSP 
Fei Xia. 1999. Extracting Tree Adjoining Grammars 
from Bracketed Corpora. In Proceedings of the 5th 
Natural Language Processing Pacific Rim 
Symposium (NLPRS-1999) 
Daguang Xu, Yuan Cao, Damianos Karakos. 2011. 
Description of the JHU System Combination Scheme 
for WMT 2011. In Proceedings of the Sixth 
Workshop on Statistical Machine Translation 
 
438
