 Decomposition for ISO/IEC 10646 Ideographic Characters 
LU Qin, CHAN Shiu Tong, LI Yin, LI Ngai Ling 
Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{csluqin, cstchan, csyinli, csnlli}@comp.polyu.edu.hk 
 
Abstract 
Ideograph characters are often formed by some 
smaller functional units, which we call character 
components. These character components can be 
ideograph radicals, ideographs proper, or some 
pure components which must be used with 
others to form characters. Decomposition of 
ideographs can be used in many applications. It 
is particularly important in the study of Chinese 
character formation, phonetics and semantics.  
However, the way a character is decomposed 
depends on the definition of components as well 
as the decomposition rules. The 12 Ideographic 
Description Characters (IDCs) introduced in ISO 
10646 are designed to describe characters using 
components. The Hong Kong SAR Government 
recently published two sets of glyph standards 
for ISO10646 characters. The standards, being 
the first of its kind, make use of character 
decomposition to specify a character glyph using 
its components. In this paper, we will first 
introduce the IDCs and how they can be used 
with components to describe two dimensional 
ideograph characters in a linear fashion. Next we 
will briefly discuss the basic references and 
character decomposition rules. We will then 
describe the data structure and algorithms to 
decompose Chinese characters into components 
and, vice versa. We have also implemented our 
database and algorithms as an internet 
application, called  the Chinese Character 
Search System, available at website 
http://www.iso10646hk.net/. With this tool, 
people can easily search characters and 
components in ISO 10646. 
  
Introduction 
ISO/IEC 10646 (ISO 10646) in its current 
version, contains more than 27,000 Han 
characters, or ideograph characters as it is called, 
in its basic multilingual plane and another 
40,000 in the second plane[1-2]. The complete 
set of ideograph repertoire includes Han 
characters in all national/regional standards as 
well as all characters from the Kang Xi 
Dictionary( ) and other major 
references. In almost all the current encoding 
systems including ISO 10646 and Unicode, each 
Han character is treated as a separate unique 
symbol and given a separate code point.  This 
single character encoding method has some 
serious drawbacks. Consider most of the 
alphabet-based languages, such as English, even 
though new words are created quite frequently, 
the alphabet itself is quite stable. Thus the newly 
adopted words do not have any impact on 
coding standards. When new Han characters are 
created, they must be assigned a new code point, 
thus all codesets supporting Han characters must 
leave space for extension. As there is no formal 
rule to limit the formation of new Han characters, 
the standardization process for code point 
assignment can be potentially endless. On the 
other hand, new Han characters are almost 
always be created using some existing character 
components which can be existing radicals, 
characters proper, or pure components which are 
not used alone as characters. If we can use coded 
components to describe a new character, we can 
potentially eliminate the standardization process. 
Han characters can be considered as a two 
dimensional encoding of components. The same 
set of components when used in different 
relative positions can form different characters. 
For example the two components  and  
can form two different characters:   
depending on the relative positions of the two 
components. However, the current internal code 
point assignments in no way can reveal the 
relationship of the these characters with respect 
to their component characters. Because of the 
limitation of the encoding system, people have 
to put a lot of efforts to develop different input 
methods. Searching for characters with similar 
shapes are also quite difficult. The 12 
 Ideographic Description Characters (IDCs) were 
introduced in ISO 10646 in the code range of 
2FF0 - 2FFB to describe the relative positions of 
components as shown in Table 1. Each IDC 
symbol shows a typical ideograph character 
composition structure. For example,  
(U+2FF0) indicates that a character is formed by 
two components, one on the left-hand side and 
one on the right-hand side. All IDCs except 
U+2FF2 and U+2FF3 have cardinality of two 
because the decomposition requires two 
components only.  Details of these symbols can 
be found in Annex F of ISO 10646 2nd Edition 
[1] and in John Jenkens' report [3]. 
 
 
Smbl 
 
Code 
point  
 
Name in ISO 10646 
 
Cardi- 
nality 
 
Label 
 2FF0 IDC LEFT TO RIGHT IDC2 A 
 2FF1 IDC ABOVE TO BELOW 
IDC2 B 
 2FF2 IDC LEFT TO MIDDLE AND RIGHT 
IDC3 K 
 2FF3 IDC ABOVE TO MIDDLE AND 
BELOW 
IDC3 L 
 2FF4 IDC FULL SURROUND 
IDC2 I 
 2FF5 IDC SURROUND FROM ABOVE 
IDC2 F 
 2FF6 IDC SURROUND FROM BELOW 
IDC2 G 
 2FF7 IDC SURROUND FROM LEFT 
IDC2 H 
 2FF8 IDC SURROUND FROM UPPER LEFT 
IDC2 D 
 2FF9 IDC SURROUND FROM UPPER RIGHT 
IDC2 C 
 2FFA IDC SURROUND FROM LOWER LEFT 
IDC2 E 
 2FFB IDC OVERLAID IDC2 J 
Table 1. The 12 Ideograph Description 
Characters 
 
The IDCs can be used to describe not only 
unencoded characters, but also coded characters 
to reveal their internal structures and 
relationships among components. Thus 
applications for using these structural symbols 
can be quite useful. In fact the most common 
applications are in electronic dictionaries and 
on-line education [4]. 
 
In this paper, however, we introduce a new 
application where the IDCs and components are 
used in the standardization of Han character 
glyphs. As we all know that ISO 10646 is a 
character standard, which allows different glyph 
styles for the same character and different 
regions can develop different glyph styles to suit 
their own needs. The ideographic repertoire in 
ISO 10646 has a so called Horizontal Extension, 
where each coded ideograph character is listed 
under the respective CJKV columns. The glyph 
of each character can be different under different 
columns because ISO 10646 is a character 
standard, not a glyph standard. We normally call 
these different glyphs as variants. For example, 
the character bone   can take three different 
forms(variants):  
 
   
HK Mainland Taiwan 
 
Even with the ISO 10646 horizontal extensions, 
people in Hong Kong still get confused as to 
which styles to use, as only some characters in 
the Hong Kong style deviate from both G 
column(mainland China) and T column(Taiwan). 
Consequently, the Hong Kong SAR Government 
has decided to  develop the Hong Kong glyph 
standards for ISO 10646 which can serve as a 
reference guide for font vendors when 
developing products for Hong Kong. The 
standards, being the first of its kind, makes uses 
of character decomposition to specify a 
character glyph using its components. 
 
The rest of the paper is organized as follows. 
Section 1 gives the rationale for the use of 
character components, the references and 
decomposition rules. Section 2 describes the 
data structure and algorithms to decompose 
Chinese characters into components and, vice 
versa. Section 3 discusses performance 
considerations and Section 4 is the conclusion. 
 
1. Character Decomposition Rules 
At the beginning of the glyph standardization, 
one important requirement was agreed by the 
working group, namely, extensibility. That is, the 
specifications should be easily extended by 
adding more characters into later versions of the 
ISO/IEC 10646, which we refer to as the new 
characters. The specifications should also not 
contain any internal inconsistency, or 
inconsistency in relation to the ISO/IEC 10646?s 
 source standards. In order to satisfy both 
consistency requirements, we have concluded 
that listing every character in ISO/IEC 10646 is 
not desirable. Instead, we decided to produce the 
specifications by giving the correct glyphs of 
character components based on a common 
assumption that if a component or a character is 
written in a certain way, all other characters 
using it as a component should also write it in 
the same way. For example if the character 
?bone?  (U+9AA8) is written in a certain 
way, all characters using ?bone? as a component, 
such as ? ? (U+6ED1) and ? ? (U+9ABC), 
should have the bone ? ? component follow 
the same style. In this way, the specification can 
be extended very easily for all new characters 
using bone ? ? as a component. In other words, 
we can assume that component glyphs are 
standardized for general usage. By using 
components to describe a character, we can also 
avoid inconsistency. That is, by avoid listing all 
characters with bone, ? ? as a component, we 
do not need to be concerned about producing 
inconsistent glyphs in the specifications. This is 
important because the working group does not 
have any font vendor as a member, because of 
an implicit rule that was specified by the 
Government of the HKSAR to avoid any 
potential conflict of interest. The glyph style is 
mostly based on the book published by the Hong 
Kong Institute of Education in 2000[5] 
 
In principle, for producing glyph specifications, 
we have to produce a concrete, minimal, and 
unique list of basic components. In order to 
achieve this, we need to have a set of rules to 
decompose the characters systematically. In our 
work, we have used the GF 3001-1997 [6] as our 
major component reference. The following is a 
brief description of the rules. (For a detailed 
description, please refer to the paper ?The Hong 
Kong Glyph Specifications for ISO 10646's 
Ideographic Characters?[7].) 
? Use GF 3001-1997 specifications as the 
basis to construct a set of primary 
components. Components for simplified 
Chinese are removed. The shapes are 
modified to match the glyph style for 
Hong Kong. 
? Characters are decomposed into 
components according to their structure 
and etymological origin. 
? In some cases, an ?ad-hoc? 
decomposition occurs if the 
etymological origin and its glyph shape 
are not consistent, or the etymological 
origin is not clear, or to avoid defining 
additional components. 
? Characters are not decomposed if it 
appears in GF 3001-1997 as a 
component. 
? Detached parts can be further 
decomposed. 
? Merely touched parts that do not have 
overlapping or crossing can be 
decomposed. 
? In some cases, we do not decompose 
some components to prevent the 
components from getting too small. 
? In some cases, a single component will 
be distinguished as two different 
components. This is the concept of 
variant or related component. 
This set of rules, together with 644 basic 
components and the set of intermediate 
components defined, enables us to decompose 
Chinese characters that appear in the first 
version ISO 10646 with 20,902 characters, Ext. 
A in the second version of ISO 10646[1] and 
Hong Kong Suplementary Character Set [8-9].  
The 644 basic components play a very important 
role because they form all the Chinese characters 
in our scope. 
 
In order to describe the position relationship 
amongst components in a character, we have 
used the 12 Ideographic Description Characters 
(IDC) in ISO/IEC 10646 Part1:2000 in the range 
from 2FF0 to 2FFB, and defined an extra IDC 
?M? (which indicates that a particular 
component is a basic component and will not be 
further decomposed), as shown in Table 1. Every 
character can be decomposed into up to three 
components depending on the cardinality of the 
IDC used.  
 
Each Character is decomposed according to the 
following definition: 
 
Character = IDC2 CC(1)  CC(2)  
| IDC3 CC(1) CC(2) CC(3) 
| M 
where 
IDC2 ? (2FF0 ? 2FFB)  
CC(i)  is a set of character components 
and i indicates its position in the 
sequence  
 M is a special symbol indicating 
Character will not be further 
decomposed 
 
By our definition, a CC can be formed by three 
subsets: (1) coded radicals, (2) coded 
components and ideographs proper, and (3) 
intermediate components that are not coded in 
ISO 10646. The intermediate components are 
maintained by our system only. The 
decomposition result is stored in the database. 
Conceptually, every entry in the database can be 
treated as a Chinese component, having a data 
structure described above. 
 
2. Decomposition/Formation Algorithms 
As mentioned above, the decomposition 
database only gives information on how a 
character is decomposed in a minimal way. 
However, some characters have nested 
components. For instance, the character ??? 
can be decomposed into two components: ??? 
and ???, but ??? being a character can be 
further decomposed into two components. In 
order to handle nesting and finding components 
to the most elementary form(no further 
decomposition), we have defined the 
decomposition and formation algorithms.  
There are mainly two algorithms, one for the 
decomposition of a character into a set of 
components(the algorithm is called 
Char-to-Compnt) , another one for the formation 
of a set of characters from a component ( the 
algorithm is called Compn-to-Charr). 
Let x be the seed (x = starting character for 
search); 
Stop = false 
WHILE NOT  stop DO 
      IF Struct_Symbol(CD[x]) = ?M? 
        Stop = True 
     ELSE 
LCmp ={ cc[x] ?  CC } 
ENDWHILE 
 
Figure 1. Pseudo-code of ?Char-to-Compnt? 
 
Both algorithms are very similar. They 
recursively retrieve all characters/components 
appearing in the decomposition database by 
using the characters/components themselves as a 
seed, but their directions of retrieval are opposite 
to each other. In the ?Char-to-Compnt?, the 
decomposition goes from its current level down, 
one level at a time, until no more decomposition 
can be done. Figure 1 the pseudo code of the 
algorithm for one level only and they can be 
done recursively to find all components of a 
character. Table 2 shows the entries related to 
the character ???. Notice that the number of 
components for ??? is not two, but 4 because 
one of the components ??? can be further 
decomposed into two more components. 
 
Character IDC Comp1 Comp2 Comp3 
? B ? ?  
? A ? ?  
? M    
? M    
? M    
Table 2. Component Entries of character 
??? 
 
On the other hand, the ?Compnt-to-Char? 
algorithm searches from its current level up until 
no more character can be found using the current 
component. Figure 2 shows the pseudo code of 
the upward search algorithm where x is 
considered the seed to start the search and the 
variable contains all characters formed using the 
current component x.  
 
Let x be the seed (x = starting component for 
search); 
Stop = false 
Char_List ={ x} 
WHILE NOT  stop DO 
      IF No Change to Char_List 
              Stop = True 
     ELSE 
FOR each x in Char_List 
Char_List = Char_List ?{ Char[x]} 
     ENDFOR 
 
ENDWHILE 
 
Figure 2. Pseudo-code of ?Compnt-to-Char? 
 
Character IDC Comp1 Comp2 Comp3 
? M    
? B ? ?  
? A ? ?  
? A ? ?  
? A ? ?  
?     
Table 3. Example character entries of  
component ???   
  
Table 3 shows some of the search results 
involving the component  ???.  Note that the  
result not only find the character ???, but also 
the characters using ??? as components as well.  
 
Further more, due to the fact that there are two 
IDCs with cardinality of three, the 
decomposition is not unique.  Based Han 
characters formation rules, some characters 
should be decomposed into two components first 
before considering further decomposition.  For 
instance, ??? should be decomposed into ??? 
and ??? whereas ??? should be decomposed 
into ??? and ???.  However, for upward 
search we certainly want the character ??? to 
be found if the search component is ?? ?. 
Therefore, in addition to using the most reason 
decomposition at the first level, we also 
maintain different decompositions for 
applications where character formation rule are 
less important. In other words, we also provided 
composition and decompositions independent of 
certain particular character formal rules. Again 
taking the character ??? as an examples, its 
components should not only be ??? and ???, 
but also  ???, ???, ??? as well as ??? and 
? ?.  In fact, in our system,  ?? ?  is 
decomposed into ???, ??? and ? ? as shown 
in Table 4. The ?Char-to-Compnt? algorithm 
will take the relative positions of the 
components into consideration based on the IDC 
defined in each entry to find other three possible 
components  ???, ??? and ???.  This can 
be done because the combination of ??? and 
??? will form ???; similarly ??? and ? ? 
will form ???;, and ??? and ? ?will form 
???. Note that in the first two cases of the OR 
clause, ??? and ??? will be identified. In the 
third case of the OR  clause, the character  
??? will be identified. You may argue the 
validity of the third case of the OR clause, but 
for the character ????, finding the component 
??? would be very important.    
Character IDC Comp1 Comp2 Comp3 
? K ? ?  
? A ? ?  
? A ?   
? A ?   
Table 4 An example of handling a character 
with three components 
 
The  basic principle of the algorithm, as shown 
in Figure 3,  is that if we see a character with 
an IDC {K} or {L}, or an IDC of a character 
that can be transformed to IDC {K} or {L}, we 
will try to use its components to form characters. 
 
Let x be a Chinese component (x = cc); 
Let LCsub be the list of sub-components c; 
IF x[structure] = IDC{K} THEN 
LCsub = c : c[structure] = IDC{A}AND 
c[component(1)] = x[component(1)] AND 
c[component(2)] = x[component(2)] or 
c[component(2)] = x[component(2)] AND 
c[component(3)] = x[component(3)] or 
c[component(1)] = x[component(1)] AND 
c[component(3)] = x[component(3)] 
END 
**the same algorithm works when x[structure] = 
IDC{L}, then the result c[structure] will become 
IDC{B} 
Figure 4 Pseudo-code for handling a 
character with three components 
 
Let s be the seed (s = cc); 
Let r be the result component; 
if s[structure] = IDC{A} 
if s[component(1)][ structure] = IDC{A} then 
r = IDC{K} + 
s[component(1)][component(1)] + 
s[component(1)][component(2)] + 
s[component(2)] 
else if s[component(2)][ structure] = IDC{A} 
then 
r = IDC{K} + s[component(1)] + 
s[component(2)][component(1)] + 
s[component(2)][component(2)] 
end 
end 
**this algorithm also works when s[structure] = 
IDC{B}, then the result structure will become 
IDC{L} 
Figure 4 Pseudo-code of For the Split Step 
 
In many cases, we still want to maintain the 
characters in the right decomposition, e.g, to 
decompose them into two components first and 
then further decompose if needed. Take another  
character ??? as an example. Suppose it is only 
decomposed into two components (??? and 
???). This makes the search more complex. In 
order to simplify the search, we need to go 
through an internal step which we call the Split 
Step to decompose the character into three 
 components before we allow for component to 
character search. The pseudo code for the Split 
Step is shown in Figure 4. The generated result 
is shown in Table 5.  
 
Character IDC Comp1 Comp2 Comp3 
? A ? ?  
? A ? ?  
 
? K ? ? ? 
Table 5. An example Output of the Split Step 
 
For some characters like ???, the Split Step 
must consider the component  ?? ? in the 
middle as an insertion into the character ???. 
We use similar handling to decompose  ??? 
into  ???, ??? and ???, with an IDC {K}. In 
order to find a character with the component 
??? such as ??? , we need additional algorithm 
to locate components that are potentially being 
split to the two sides with an inserted component. 
We try to decompose a component into two 
sub-components if their IDC is ?A? or ?B?. 
Once we get the two sub-components, we try to 
make different combinations to see if there are 
any characters with an IDC {K} or {L} that 
contain the two sub-components as shown in 
Figure 5. 
 
Let x be a Chinese character (x = cc); 
Let Clst be the list of results c; 
if x[structure] = IDC{A} then 
Clst = c : c[structure] = IDC{K} and 
((c[component(1)] = x[component(1)] and 
c[component(2)] = x[component(2)] ) or 
(c[component(2)] = x[component(1)] and 
c[component(3)] = x[component(2)]) or 
(c[component(1)] = x[component(1)] and  
c[component(3)] = x[component(2)])) 
end 
**this algorithm also works when x[structure] = 
IDC{B}, then the result structure will become 
IDC{L} 
Figure 5. Pseudo-code of finding inserted 
component 
 
3. Performance Evaluation 
Since the algorithms have to do excessive search 
for many combinations in many levels 
recursively, performance becomes a very 
important issue especially if we want to make 
this for public access through the internet. 
However, since the decomposition is static, it 
does not need to be done in real time. as the 
search doesn?t need to be done online, In other 
words, searching of the same data will always 
give the same result unless the decomposition 
rules or algorithms are changed. Consequently, 
we built two pre-searched tables to store the 
results of both ?Compnt-to-Char? algorithm and 
the ?Char-to-Compnt?algorithm. Once we have 
the pre-searched tables, we can totally avoid the 
recursive search. Instead, the search result can 
be directly retrieved in a single tuple. This 
results in much better performance both in terms 
of usage of CPU time and I/O usage. 
 
Character Pre-searched result 
? ? ? ? ? ? ?  
? ? ? ? ? ? 
??  
Table 6. Examples of pre-searched results of 
?Cha-to-Compnt?Algorithm 
 
Character Pre-searched result 
? ? ? ? ? ? ? ? ? (total 
5481 characters) 
? ? ? ? ? ? ? ? (total 44 
characters) 
??  
Table 7. Examples of pre-searched results of 
?Component to Character? 
Table 6 and table 7 shows some samples of the 
pre-searched tables for the downward search and 
the upward search, respectively. 
  
Although the advanced control algorithms can 
retrieve most Chinese characters correctly, they 
also return some components that do not make 
much sense. For example, the character ??? has 
a structure of IDC{B}, and components ??? 
and ?? ?. However, when it is eventually 
decomposed into  ???, ??? and ???. Using 
the algorithm ?Char-to-Compnt?, the component 
??? will also be returned, even though ??? has 
no cognate relationship with the character ???. 
We can take into consideration of only a subset 
of characters that can be split in character 
formation, such as ??? and ???. This way, the 
insertion components will only be considered for 
these characters.   
 
4. Conclusion 
In this paper, we focus on the algorithms of 
character decomposition and formation. The 
results can be used for the standardization of 
 computer fonts, glyphs, or relevant language 
resources. We have implemented a Chinese 
Character Search System based on the result of 
this standardization work. We can use this search 
system to look for character decomposition or 
formation results. The system comes with many 
handy and useful features. It provides a lot of 
useful information on Chinese characters, such 
as the code for various encodings, and 
pronunciations. A stand-alone version is also 
built. The actual implementation of these 
algorithms and of the database helps people to 
get information about Chinese characters very 
quickly. It further facilitates researchers? work in 
related areas. For more information on the 
system, please visit the website 
http://www.iso10646hk.net. 
 
Acknowledgement 
The project is partially supported by the Hong 
Kong SAR Government(Project code: ITF 
AF/212/99) and  the Hong Kong Polytechnic 
University(Project Code: Z044). 
 
References 
[1] ISO/IEC, ?ISO/IEC 10646-1 Information 
Technology-Universal Multiple-Octet 
Coded Character Set - Part 1?, ISO/IEC, 
2000 
[2] ISO/IEC, ?ISO/IEC 10646-2 Information 
Technology-Universal Multiple-Octet 
Coded Character Set - Part 1?, ISO/IEC, 
2001 
[3] John Jenkins, "New Ideographs in Unicode 
3.0 and Beyond", Proceedings of the 15th 
International Unicode Conference C15, 
San Jose, California, Sept. 1-2, 1999 
[4] Dept. of Education(Taiwan), ?Dictionary 
of Chinese Character Variants Version 2?, 
Dept. of Education, Taiwan, 2000 
[5] ???????, ????????, (?
???????), ???????
? , ????? ( LEE Hok-ming as 
Chief Editor, Common Character 
Glyph Table 2nd Edition, Hong Kong 
Institute of Education, 2000) 
[6] GF3001-1997???????????? 
?????? ?????GB13000.1?
?????????, ????????
???, 1997? 12?. 
 
[7] Lu Qin, The Hong Kong Glyph 
Specifications for ISO 10646?s Ideographic 
Characters. 21st International Unicode 
Conference, Dublin, Ireland, May 2002 
[8] Hong Kong Special Administrative Region 
Government, ?Hong Kong Supplementary 
Character Set?, HKSARG, September 28, 
1999 
[9] Hong Kong Special Administrative Region 
Government, ?Hong Kong Supplementary 
Character Set ? 2001 ? , HKSARG, 
December 31, 2001 
 
 
 The Construction of A Chinese Shallow Treebank 
Ruifeng Xu 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
Qin Lu 
Dept. Computing, 
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong  
csluqin@comp.polyu.edu.hk 
Yin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csyinli@comp.polyu.edu.hk 
Wanyin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
 
Abstract 
This paper presents the construction of a 
manually annotated Chinese shallow Treebank, 
named PolyU Treebank. Different from 
traditional Chinese Treebank based on full 
parsing, the PolyU Treebank is based on 
shallow parsing in which only partial syntactical 
structures are annotated. This Treebank can be 
used to support shallow parser training, testing 
and other natural language applications. 
Phrase-based Grammar, proposed by Peking 
University, is used to guide the design and 
implementation of the PolyU Treebank. The 
design principles include good resource sharing, 
low structural complexity, sufficient syntactic 
information and large data scale. The design 
issues, including corpus material preparation, 
standard for word segmentation and POS 
tagging, and the guideline for phrase bracketing 
and annotation, are presented in this paper. 
Well-designed workflow and effective 
semiautomatic and automatic annotation 
checking are used to ensure annotation accuracy 
and consistency. Currently, the PolyU Treebank 
has completed the annotation of a 
1-million-word corpus. The evaluation shows 
that the accuracy of annotation is higher than 
98%. 
1 Introduction 
A Treebank can be defined as a syntactically 
processed corpus. It is a language resource  
containing annotations of information at various 
linguistic levels such as words, phrases, clauses and 
sentences to form a ?bank of linguistic trees?. There 
are many Treebanks built for different languages 
such as the Penn Treebank (Marcus 1993), ICE-GB 
(Wallis 2003), and so on. The Penn Chinese 
Treebank is an important resource (Xia et al 2000; 
Xue et al 2002). Its annotation is based on 
Head-driven Phrase Structure Grammar (HPSG). 
The corpus of 100,000 Chinese words has been 
manually annotated with a strict quality assurance 
process. Another important work is the Sinica 
Treebank at the Academic Sinica, Taiwan ( Chen et 
al. 1999; Chen et al 2003). Information-based Case 
Grammar (ICG) was selected as the language 
framework. A head-driven chart parser was 
performed to do phrase bracketing and annotating. 
Then, manual post-editing was conducted. 
According to the report, The Sinica Treebank  
contains 38,725 parsed trees with 329,532 words.  
Most reported Chinese Treebanks, including the 
two above, are based on full parsing which requires 
complete syntactical analysis including determining 
syntactic categories of words, locating chunks that 
can be nested, finding relations between phrases and 
resolving the attachment ambiguities. The output of 
full parsing is a set of complete syntactic trees. 
Automatic full parsing, however, is difficult to 
achieve good performance. Shallow parsing (or 
partial parsing) is usually defined as a parsing 
process aiming to provide a limited amount of local 
syntactic information such as non-recursive noun 
phrases, V-O structures and S-V structures etc. Since 
shallow parsing can recognize the backbone of a 
sentence more effectively and accurately with lower 
cost, people has in recent years started to work using 
results from shallow parsing. A shallow parsed 
Treebank can be used to extract information for 
different applications especially for training shallow 
parsers. 
Different from full parsing, annotation to a 
shallow Treebank is only targeted at certain local 
structures in a sentence. The depth of ?shallowness?  
and the scope of annotation vary from different 
reported work. Thus, two issues in shallow Treebank 
annotation is (1) what information and (2) to what 
depths the syntactic information should be annotated. 
Generally speaking, the degree of ?shallowness? and 
the syntactical labeling are determined by the 
requirement of the serving applications. The choice 
of full parsing or shallow parsing is dependent on 
the need of the application including resources and 
 the capability of system to be developed (Xia et al 
2000; Chen et al 2000; Li et al 2003). Currently, 
there is no large-scale shallow annotated Treebank 
available as a publicly resource for training and 
testing.  
In this paper, we present a manually annotated 
shallow Treebank, called the PolyU Treebank. It is 
targeted to contain 1-million-word contemporary 
Chinese text. The whole work on the PolyU 
Treebank follows the Phrase-based Grammar 
proposed by Peking University (Yu et al 1998). In 
this language framework, a phrase, lead by a lexical 
word(or sometimes called a content word) as a head, 
is considered the basic syntactical unit in a Chinese 
sentence. The building of the PolyU Treebank was 
originally designed as training data for a shallow 
parser used for Chinese collocation extraction. From 
linguistics viewpoint, a collocation occurs only in 
words within a phrase, or between the headwords of 
related phrases (Zhang and Lin 1992). Therefore, the 
use of syntactic information is naturally considered 
an effective way to improve the performance of 
collocation extraction systems. The typical problems 
like doctor-nurse (Church and Hanks 1990) could be 
avoided by using such information. When 
employing syntactical information in collocation 
extraction, we restrict ourselves to identify the stable 
phrases in the sentences with certain levels of 
nesting. Thus it has motivated us to produce a 
shallow Treebank. 
A natural way to obtain a shallow Treebank is 
through extracting shallow structures from a fully 
parsed Treebank. Unfortunately, all the available 
fully parsed Treebank, such as the Penn Treebank 
and the Sinica Treebank, are annotated using 
different grammars than our chosen Phrase-based 
Grammar. Also, the sizes of these Treebank are 
much smaller in scale to be useful for training our 
shallow parser. 
This paper presents the most important design 
issues of the PolyU Treebank and the quality control 
mechanisms. The rest of this paper is organized as 
follows. Section 2 introduces the overview and 
design principles.  Section 3 to Section5, present 
the design issues on corpus material preparation, the 
standard for word segmentation and POS tagging, 
and the guideline for phrase bracketing and labeling, 
respectively. Section 6 discusses the quality 
assurance mechanisms including a carefully 
designed workflow, parallel annotation, and 
automatic and semi-automatic post-annotation 
checking. Section 7 gives the current progress and 
future work. 
2 Overview and Design Principles 
The objective of this project is to manually 
construct a large shallow Treebank with high 
accuracy and consistency.  
The design principles of The PolyU Treebank are: 
high resource sharing ability, low structural 
complexity, sufficient syntactic information and 
large data scale. First of all, the design and 
construction of The PolyU Treebank aims to provide 
as much a general purpose Treebank as possible so 
that different applications can make use of it as a 
NLP resource. With this objective, we chose to 
follow the well-known Phrase-based Grammar as 
the framework for annotation as this grammar is 
widely accepted by Chinese language researchers, 
and thus our work can be easily understood and 
accepted.  
Due to the lack of word delimitation in Chinese, 
word segmentation must be performed before any 
further syntactical annotation. High accuracy of 
word segmentation is very important for this project. 
In this project, we chose to use the segmented and 
tagged corpus of People Daily annotated by the 
Peking University. The annotated corpus contains 
articles appeared in the People Daily Newspaper in 
1998. The segmentation is based on the guidelines, 
given in the Chinese national standard GB13715, 
(Liu et al 1993) and the POS tagging specification 
was developed according to the ?Grammatical 
Knowledge-base of contemporary Chinese?. 
According to the report from Peking University, the 
accuracy of this annotated corpus in terms of 
segmentation and POS tagging are 99.9% and 99.5%, 
respectively (Yu et al 2001). The use of such mature 
and widely adopted resource can effectively reduce 
our cost, ensure syntactical annotation quality. With 
consistency in segmentation, POS, and syntactic 
annotation, the resulting Treebank can be readily 
shared by other researchers as a public resource. 
The second design principle is low structural 
complexity. That means, the annotation framework 
should be clear and simple, and the labeled syntactic 
and functional information should be commonly 
used and accepted. Considering the characteristics of 
shallow annotation, our project has focused on the 
annotation of phrases and headwords while the 
sentence level syntax are ignored.  
Following the framework of Phrase-based 
Grammar, a base-phrase is regarded as the smallest 
unit where a base-phrase is defined as a ?stable? and 
?simple? phrase without nesting components. Study 
on Chinese syntactical analysis suggests that phrases 
should be the fundamental unit instead of words in a 
sentence. This is because, firstly, the usage of 
Chinese words is very flexible. A word may have 
different POS tags serving for different functions in 
sentences. On the contrary, the use of Chinese 
phrases is much more stable. That is, a phrase has 
very limited functional use in a sentence. Secondly, 
the construction rules of Chinese phrases are nearly 
 the same as that of Chinese sentences. Therefore, the 
analysis of phrases can help identifying POS and 
grammatical functions of words. Naturally, it should 
be regarded as the basic syntactical unit. Usually, a 
base-phrase is driven by a lexical word as its 
headword. Examples of base-phrases include base 
NP, base VP and so on, such as the sample shown 
below. 
  
Using base-phrases as the start point, nested levels 
of phrases are then identified, until the maximum 
phrases (will be defined later) are identified. Since 
we do not intend to provide full parsing information, 
there has to be a limit on the level of nesting. For 
practical reasons, we choose to limit the nesting of 
brackets to 3 levels. That means, the depth of our 
shallow parsed Treebank will be limited to 3. This 
restriction can limit the structural complexity to a 
manageable level.  
Our nested bracketing is not strictly bottom up. 
That is we do not simply extend from base-phrase 
and move up until the 3rd level. Instead, we first 
identify the maximal-phrase which is used to 
identify the backbone of the sentence. The 
maximal-phrase provides the framework under 
which the base-phrases of up to 2 levels can be 
identified. The principles for the identification of 
scope and depth of phrase bracketing are briefly 
explained below and the operating procedure is 
indicated by the given order in which these 
principles are presented. More details is given in 
Section 5. 
Step 1: Annotation of maximal-phrase which is 
the shortest word sequence of maximally 
spanning non-overlapping edges which plays a 
distinct semantic role of a predicate. A 
maximal-phrase contains two or more lexical 
words. 
Step 2: Annotation of base-phrases within a 
maximal-phrase. In case a base-phrase and a 
maximal-phrase are identical and the 
maximal-phrase is already bracketed in Step 1, no 
bracketing is done in this step. For each identified 
base-phrase, its headword will be marked. 
Step 3: Annotation of next level of bracketing, 
called mid-phrase which is expended from a 
base-phrase. A mid-phrase is annotated only if it is 
deemed necessary. The process starts from the 
identified base-phrase. One more level of 
syntactical structure is then bracketed if it exists 
within the maximal-phrase.   
  
The third design principle is to provide sufficient 
syntactical information for natural language 
application even though shallow annotation does not 
necessarily contain complete syntactic information 
at sentence level. Some past research in Chinese 
shallow parsing were on single level base-phrases 
only (Sun 2001). However, for certain applications, 
such as for collocation extraction, identification of 
base-phrases only are not very useful. In this project, 
we have decided to annotate phrases within three 
levels of nesting within a sentence. For each phrase, 
a label is be given to indicate its syntactical 
information, and an optional semantic or structural 
label is given if applicable. Furthermore, the 
headword of a base-phrase is annotated. We believe 
these information are sufficient for many natural 
language processing research work and it is also 
manageable for this project within its working 
schedule. 
Fourthly, aiming to support practical language 
processing, a reasonably large annotated Treebank is 
expected. Studies on English have shown that 
Treebank of word size 500K to 1M is reasonable for 
syntactical structure analysis (Leech and Garside 
1996). In consideration of the resources available 
and the reference of studies on English, we have set 
out our Treebank size to be one million words. We 
hope such a reasonably large-scale data can 
effectively support some language research, such as  
collocation extraction.  
We chose to use the XML format to record the 
annotated data. Other information such as original 
article related information (author, date, etc.), 
annotator name, and other useful information are 
also given through the meta-tags provided by XML. 
All the meta-tags can be removed by a program to 
recover the original data. 
We have performed a small-scale experiment to 
compare the annotation cost of shallow annotation 
and full annotation (followed Penn Chinese 
Treebank specification) on 500 Chinese sentences 
by the same annotators. The time cost in shallow 
annotation is only 25% of that for full annotation. 
Meanwhile, due to the reduced structural complexity 
in shallow annotation, the accuracy of first pass 
shallow annotation is much higher than full 
annotation. 
3 Corpus Materials Preparation 
The People Daily corpus, developed by PKU, 
consists of more than 13k articles totaling 5M words. 
As we need one million words for our Treebank, we 
have selected articles covering different areas in 
different time span to avoid duplications due to 
short-lived events and news topics. Our selection 
takes each day?s news as one single unit, and then  
several distant dates are randomly selected among 
the whole 182 days in the entire collection.  We 
have also decided to keep the original articles? 
structures and topics indicators as they may be 
useful for some applications. 
 4 Word Segmentation and Part-of-Speech 
Tagging 
The articles selected from PKU corpus are already 
segmented into words following the guidelines 
given in GB13715. The annotated corpus has a basic 
lexicon of over 60,000 words. We simply use this 
segmentation without any change and the accuracy 
is claimed to be 99.9%.  
Each word in the PKU corpus is given a POS tag.  
In this tagging scheme, a total of 43 POS tags are 
listed (Yu et al 2001).  Our project takes the PKU 
POS tags with only notational changes explained as 
follows: 
The morphemes tags including Ag (Adjectives 
morphemes), Bg, Dg, Ng, Mg, Rg, Tg, Qg, and Ug 
are re-labeled as lowercase letters, ag, bg, dg, ng, mg, 
rg, tg, qg and ug, respectively. This modification is 
to ensure consistent labeling in our system where the 
lower cases are used to indicate word-level tags and 
upper cases are used to indicate phrase-level labels. 
5 Phrase Bracketing and Annotation 
Phrase bracketing and annotation is the core part 
of this project. Not only all the original annotated 
files are converted to XML files, results of our 
annotations are also given in XML form. The meta 
tags provided by XML are very helpful for further 
processing and searching to the annotated text. . 
Note that in our project, the basic phrasal analysis 
looks at the context of a clause, not a sentence. Here, 
the term clause refers the text string ended by some 
punctuations including comma (,), semicolon (;), 
colon (:), or period (.). Certain punctuation marks 
such as ? ?, ?<?, and ?>? are not considered clause 
separators. For example,  
  
is considered having two clauses and thus will be 
bracketed separately. It should be pointed out that he 
set of Chinese punctuation marks are different from 
that of English and their usage can also be different. 
Therefore, an English sentence and their Chinese 
translation may use different punctuation marks.  
For example, the sentence 
 
is the translation of the English ?Tom, John, and 
Jack go back to school together? , which uses ? ? 
rather than comma(,) to indicate parallel structures, 
and is thus considered one clause.   
Each clause will then be processed according to 
the principles discussed in Section 2. The symbols 
?[? and ?]? are used to indicate the left and right 
boundaries of a phrase. The right bracket is 
appended with syntactic labels as described in the 
general form of [Phrase]SS-FF, where SS is a 
mandatory syntactic label such as NP(noun phrase) 
and AP(adjective phrase), and FF is an optional label 
indicating internal structures and semantic functions 
such as BL(parallel), SB(a noun is the object of verb 
within a verb phrase). A total of  21 SS labels and 
20 FF labels are given in our phrase annotation 
specification. For example, the functional label BL 
identifies parallel components in a phrase as 
indicated in the example .  
As in another example shown below,  
 
the phrase  is a verb phrase, thus it is 
labeled as VP. Furthermore, the verb phrase can be 
further classified as a verb-complement type. Thus 
an additional SBU function label is marked. We 
should point out that since the FF labels are not 
syntactical information and are thus not expected to 
be used by any shallow parsers. The FF labels carry 
structural and/or semantic information which are of 
help in annotation. We consider it useful for other 
applications and thus decide to keep them in the 
Treebank. Appendix 1 lists all the FF labels used in 
the annotation. 
 
5.1  Identification of Maximal-phrase:  
The maximal-phrases are the main syntactical 
structures including subject, predicate, and objects in 
a clause. Again, maximal-phrase is defined as the 
phrase with the maximum spanning non-overlapping 
length, and it is a predicate playing a distinct 
semantic role and containing more than one lexical 
word. That means a maximal-phrase contains at least 
one base-phrase. As this is the first stage in the 
bracketing process, no nesting should occur. In the 
following annotated sentence, 
 (Eg.1) 
there are two separate maximal-phrases, 
, and 
. Note 
that  is considered a base-phrase, but not a 
maximal-phrase because it contains only one lexical 
word. Unlike many annotations where the object of 
a sentence is included as a part of the verb phrase, 
we treat them as separate maximal-phrases both due 
to our requirement and also for reducing nesting. 
If a clause is completely embedded in a larger 
clause, it is considered a special clause and given a 
special name called an internal clause .  We will 
bracket such an internal clause as a maximal phrase 
with the tag ?IC? as shown in the following example, 
 
 
5.2  Annotation of Base-phrases:  
A base-phrase is the phrase with stable, close and 
simple structure without nesting components. 
Normally a base-phrase contains a lexical word as 
 headword. Taking the  maximal-phrase 
in 
Eg.1 as an example,  and 
, are base-phrases in this 
maximal-phrase. Thus, the sentence is annotated as 
 
  
In fact, and are also 
base-phrases.  is not bracketed because it is a 
single lexical word as a base-phrase without any 
ambiguity and it is thus by default not being 
bracketed. is not further 
bracketed because it overlaps with a maximal-phrase. 
Our annotation principle here is that if a base-phrase 
overlaps with a maximal-phrase, it will not be 
bracketed twice.  
The identification of base-phrase is done only 
within an already identified maximal-phrase. In 
other words, if a base-phrase is identified, it must be 
nested inside a maximal-phrase or at most overlaps 
with it. It should be pointed out that the 
identification of a base-phrase is the most 
fundamental and most important goal of Treebank 
annotation. The identification of maximal-phrases 
can be considered as parsing a clause using a 
top-down approach. On the other hand, the 
identification of a base-phrase is a bottom up 
approach to find the most basic units within a 
maximal-phrase.  
 
5.3  Mid-Phrase Identification:  
Due to the fact that sometimes there may be more 
syntactic structures between the base-phrases and 
maximal-phrases, this step uses base-phrase as the 
starting point to further identify one more level of 
the syntactical structure in a maximal-phrase. Takes 
Eg.1 as an example, it is further annotated as 
 
where the underlined text shows the additional 
annotation. 
As we only limit our nesting to three levels, any 
further nested phrases will be ignored. The 
following sentence shows the result of our 
annotation with three levels of nesting:  
  
However, a full annotation should have 4 levels of 
nesting as shown below. The underlined text is the 
4th level annotation skipped by our system. 
 
 
5.4  Annotation of Headword 
In our system, a ?#? tag will be appended after a 
word to indicate that it is a headword of the 
base-phrase. Here, a headword must be a lexical 
word rather than a function word.  
In most cases, a headword stays in a fixed position 
of a base-phrase. For example, the headword of a 
noun phrase is normally the last noun in this phrase. 
Thus, we call this position the default position. If a 
headword is in the default position, annotation is not 
needed. Otherwise, a ?#? tag is used to indicate the 
headword. 
For example, in a clause, 
,  
 is a verb phrase, and the headword 
of the phrase is , which is not in the default 
position of a verb phrase. Thus, this phrase is further 
annotated as:  
  
Note that  is also a headword, but since it 
is in the default position, no explicit annotation is 
needed. 
6 Annotation and Quality Assurance 
Our research team is formed by four people at the 
Hong Kong Polytechnic University, two linguists 
from Beijing Language and Culture University and 
some research collaborators from Peking University. 
Furthermore, the annotation work has been 
conducted by four post-graduate students in 
language studies and computational linguistics from 
the Beijing Language and Culture University.  
The annotation work is conducted in 5 separate 
stages to ensure quality output of the annotation 
work. The preparation of annotation specification 
and corpus selection was done in the first stage. 
Researchers in Hong Kong invited two linguists 
from China to come to Hong Kong to prepare for the 
corpus collection and selection work. A thorough 
study on the reported work in this area was 
conducted. After the project scope was defined, the 
SS labels and the FF labels were then defined. A 
Treebank specification was then documented.  The 
Treebank was given the name PolyU Treebank to 
indicate that it is produced at the Hong Kong 
Polytechnic University. In order to validate the 
specifications drafted, all the six members first 
manually annotated 10k-word material, separately. 
The outputs were then compared, and the problems 
and ambiguities occurred were discussed and 
consolidated and named Version 1.0. Stage 1 took 
about 5 months to complete. Details of the 
specification can be downloaded from the project 
website www.comp.polyu.edu.hk/~cclab. 
In Stage 2, the annotators in Beijing were then 
involved. They had to first study the specification 
and understand the requirement of the annotation. 
Then, the annotators under the supervision of a team 
member in Stage 1 annotated 20k-word materials 
together and discussed the problems occurred. 
 During this two-month work, the annotators were 
trained to understand the specification. The 
emphasis at this stage was to train the annotators? 
good understanding of the specification as well as 
consistency by each annotator and consistency by 
different annotators. Further problems occurred in 
the actual annotation practice were then solved and 
the specification was also further refined or 
modified.  
In Stage 3, which took about 2 months, each 
annotator was  assigned 40k-word material each in 
which 5k-words material were duplicate annotated 
to all the annotators. Meanwhile, the team members 
in Hong Kong also developed a post-annotation 
checking tool to verify the annotation format, phrase 
bracketing, annotation tags, and phrase marks to 
remove ambiguities and mistakes. Furthermore, an 
evaluation tool was built to check the consistency of 
annotation output. The detected annotation errors 
were then sent back to the annotators for discussion 
and correction. Any further problems occurred were 
submitted for group discussion and minor 
modification on the specification was also done. 
In stage 4, each annotator was dispatched with one 
set of 50k-word material each time. For each 
distribution, 15k-word data in each set were 
distributed to more than two annotators in duplicates 
so that for any three annotators, there would be 5K 
duplicated materials. When the annotators finished 
the first pass annotation, we used the post-annotation 
checking tool to do format checking in order to 
remove the obvious annotation errors such as wrong 
tag annotation and cross bracketing. However, it was 
quite difficult to check the difference in annotation 
due to different interpretation of a sentence. What 
we did was to make use of the annotations done on 
the duplicate materials to compare for consistency. 
When ambiguity or differences were identified, 
discussions were conducted and a result used by the 
majority would be chosen as the accepted result. The 
re-annotated results were regarded as the Golden 
Standard to evaluate the accuracy of annotation and 
consistency between different annotators. The 
annotators were required to study this Golden 
Standard and go back to remove  similar mistakes. 
The annotated 50k data was accepted only after this. 
Then, a new 50k-word materials was distributed and 
repeated in the same way. During this stage, the 
ambiguous and out-of-tag-set phrase structures were 
marked as OT for further process. The annotation 
specification was not modified in order to avoid 
frequent revisit to already annotated data. About 4 
months were spent on this stage. 
In Stage 5, all the members and annotators were 
grouped and discuss the OT cases. Some typical new 
phrase structure and function types were appended 
in the specification and thus the final formal 
annotation specification was established. Using this 
final specification, the annotators had to go back to 
check their output, modify the mistakes and 
substitute the OT tags by the agreed tags. Currently, 
the project was already in Stage 5 with 2 months of 
work finished. A further 2 months was expected to 
complete this work. 
Since it is impossible to do all the checking and 
analysis manually, a series of checking and 
evaluating tools are established. One of the tools is 
to check the consistency between text corpus files 
and annotated XML files including checking the 
XML format, the filled XML header, and whether 
the original txt material is being altered by accident. 
This program ensures that the XML header 
information is correctly filled and during annotation 
process, no additional mistakes are introduced due to 
typing errors.  
Furthermore, we have developed and trained a 
shallow parser using the Golden Standard data. This 
shallow parser is performed on the original text data, 
and its output and manually annotated result are 
compared for verification to further remove errors 
Now, we are in the process of developing an 
effective analyzer to evaluate the accuracy and 
consistency for the whole annotated corpus. For the 
exactly matched bracketed phrases, we check 
whether the same phrase labels are given. Abnormal 
cases will be manually checked and confirmed. Our 
final goal is to ensure the bracketing can reach 99% 
accuracy and consistency. 
7 Current Progress and Future Work 
As mentioned earlier, we are now in Stage 5 of the 
annotation. The resulting annotation contains 2,639 
articles selected from PKU People Daily corpus. 
These articles contains 1, 035, 058 segmented 
Chinese words, with on average, around 394 words 
in each article. There are a total of 284, 665 
bracketed phrases including nested phrases. A 
summary of the different SS labels used are given in 
Table 1. 
 
Table 1. Statistics of annotated syntactical phrases 
 
For each bracketed phrase, if its FF label does not 
fit into the corresponding default pattern, (like for 
the noun phrase(NP), the default grammatical 
structure is that the last noun in the phrase is the 
headword and other components are the modifiers, 
using PZ tags), its FF labels should then be 
explicitly labeled. The statistics of annotated FF tags 
 are listed in Table 2.  
 
Table 2. Statistics of function and structure tags 
 
For the material annotated by multiple annotators 
as duplicates, the evaluation program has reported 
that the accuracy of phrase annotation is higher than 
99.5% and the consistency between different 
annotators is higher than 99.8%. As for other 
annotated materials, the quality evaluation program 
preliminarily reports the accuracy of phrase 
annotation is higher than 98%. Further checking and 
evaluation work are ongoing to ensure the final 
overall accuracy achieves 99%. 
Up to now, the FF labels of 5,255 phrases are 
annotated as OT. That means about 1.8% (5,255 out 
of a total of 284,665) of them do not fit into any 
patterns listed in Table 2. Most of them are proper 
noun phrase, syntactically labeled as PP. We are 
investigating these cases and trying to identify 
whether some of them can be in new function and 
structure patterns and give a new label. 
It is also our intention to further develop our tools 
to improve the automatic annotation analysis and 
evaluation program to find out the potential 
annotation error and inconsistency. Other 
visualization tools are also being developed to 
support keyword searching, context indexing, and 
annotation case searching. Once we complete Stage 
5, we intend to make the PolyU Treebank data 
available for public access.  Furthermore, we are 
developing a shallow parser and using The PolyU 
Treebank as training and testing data. 
 
Acknowledgement 
 
This project is partially supported by the Hong Kong 
Polytechnic University (Project Code A-P203) and 
CERG Grant (Project code 5087/01E) 
References  
Baoli Li, Qin Lu and Yin Li. 2003. Building a 
Chinese Shallow Parsed Treebank for Collocation 
Extraction, Proceedings of CICLing 2003: 
402-405 
Fei Xia, et al 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text Annotation 
Proceedings of LREC-2000, Greece 
Feng-yi Chen, et al 1999. Sinica Treebank, 
Computational Linguistics and Chinese Language 
Processing, 4(2):183-204 
G. N. Leech, R.Garside. 1996. Running a grammar 
factory: the production of syntactically analyzed 
corpora or ?treebanks?, Johansson and Stenstron. 
Honglin Sun, 2001. A Content Chunk Parser for 
Unrestricted Chinese Text, Ph.D Thesis, Peking 
University, 2001 
Keh-jiann Chen et al 2003. Building and Using 
Parsed Corpora (Anne Abeill? ed. s) KLUWER, 
Dordrecht 
Kenneth Church, and Patrick Hanks. 1990. Word 
association norms, mutual information, and 
lexicography, Computational Linguistics, 16(1): 
22-29 
Marcus, M. et al 1993. Building a Large Annotated 
Corpus of English: The Penn Treebank, 
Computational Linguistics, 19(1): 313-330. 
Nianwen Xue, et al 2002. Building a Large-Scale 
Annotated Chinese Corpus, Proceedings of 
COLING 2002, Taipei, Taiwan 
Sean Wallis, 2003. Building and Using Parsed 
Corpora (Anne Abeill? eds) KLUWER, Dordrecht 
Shiwen Yu, et al 1998. The Grammatical 
Knowledge- base of contemporary Chinese: a 
complete specification. Tsinghua University Press, 
Beijing, China 
Shiwen Yu, et al 2001. Guideline of People Daily 
Corpus Annotation, Technical report, Beijing 
University 
Shoukang Zhang and Xingguang Lin, 1992. 
Collocation Dictionary of Modern Chinese 
Lexical Words, Business Publisher, China 
Yuan Liu, et al 1993. Segmentation standard for 
Modern Chinese Information Processing and 
automatic segmentation methodology. Tsinghua 
University Press, Beijing, China 
  
Appendix 1 The structural and semantic FF labels  
 
 
Appendix 2 Example of an Annotated Article  
 
 
 
