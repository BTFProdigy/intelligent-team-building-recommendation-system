Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 355?364, Dublin, Ireland, August 23-29 2014.
Inducing Word Sense with Automatically Learned Hidden Concepts
Baobao Chang Wenzhe Pei Miaohong Chen
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871
{chbb,peiwenzhe,miaohong-chen}@pku.edu.cn
Abstract
Word Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from
unlabeled corpora. In this paper, we first propose a novel Bayesian parametric model to WSI.
Unlike previous work, our research introduces a layer of hidden concepts and view senses as
mixtures of concepts. We believe that concepts generalize the contexts, allowing the model to
measure the sense similarity at a more general level. The Zipf?s law of meaning is used as a
way of pre-setting the sense number for the parametric model. We further extend the parametric
model to non-parametric model which not only simplifies the problem of model selection but
also brings improved performance. We test our model on the benchmark datasets released by
Semeval-2010 and Semeval-2007. The test results show that our model outperforms state-of-the-
art systems.
1 Introduction
Word Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from unla-
beled corpora. It discriminates among meanings of a word by identifying clusters of similar contexts.
Unlike the task of Word Sense Disambiguation (WSD), which classifies polysemous words according
to a pre-existing and usually hand-crafted inventory of senses, WSI makes it attractive to researchers by
eliminating dependence on a particular sense inventory and learning word meaning distinction directly
based on the contexts as observed in corpora.
Almost all WSI work relies on the distributional hypothesis, which states that words occurring in
similar contexts will have similar meanings. To effectively discriminate among contexts, proper repre-
sentation of contexts would be a key issue. Basically, context can be represented as a vector of words
co-occurring with the target word within a fixed context window. The similarity between two contexts
of the target word can then be measured by the geometrical distance between the corresponding vectors.
To ease the sparse problem and capture more semantic content, some kinds of generalizations or abstrac-
tions are needed. For example, a context of bank including money may not share similarity with that
including cash measured at word level. However, given the conceptual relationship between money and
cash, the two contexts actually share high similarity.
One straightforward way of introducing conceptualization is to assign semantic code to context words,
where semantic codes could be derived from WordNet or other resources like thesauruses. However, two
problems remain to be tackled. The first one concerns ambiguities of context words. Context words may
have multiple semantic codes and thus word sense disambiguation to context words or other extra cost
is needed. The second one concerns the nature of WSI task. WSI actually is target-word-specific, which
means the conceptualization should be done specifically to different target words. A general purpose
conceptualization defined by a thesaurus may not well meet this requirement and may not be equally
successful in discriminating contexts of different target words.
To address these problems, we first propose a parametric Bayesian model which jointly finds concep-
tual representations of context words and the sense of the target word. We do this by introducing a layer
of target-specific conceptual representation between the target sense layer and the context words layer
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
355
Figure 1: Architecture of our model Figure 2: Graphical notation of the Basic Model
through a Bayesian framework as illustrated in Figure 1. From the generative perspective, the sense of
the target word is first sampled. Then the sense generates different conceptual configurations which in
turn generate different contexts. With a deeper architecture, our model makes it possible to induce word
senses at a more abstract level, i.e. the concept level, which is not only less sparse but also more seman-
tically oriented. Both the senses of the target word and the latent concepts are inferred automatically and
unsupervisedly with inference procedure given enough contexts involving a target word. The latent con-
cepts inferred with the model share similarities with those defined in thesauruses, as both of them cluster
semantically related words. However, since the latent concepts are inferred with regard to individual
target words, they are target-word-specific and thus fit the WSI task better than general purpose concepts
defined in thesauruses. Context words may still correspond to multiple latent concepts. However, the
disambiguation is implicitly done in the process of the word sense induction.
Setting the number of senses that the algorithm should arrive at is another problem frequently exer-
cising the minds of WSI people. Instead of trying different sense numbers on a word-by-word basis,
we propose to use Zipf?s law of meaning (Zipf, 1945) to guide the selection of the sense numbers in
this paper. With the law of meaning, sense numbers could be set on an all-word basis, rather than on a
word-by-word basis. This is not only simple but also efficient, especially in the case where there are a
large number of target words to be concerned.
We further extend the parametric model into a non-parametric model, as it allows adaptation of model
complexity to data. By extending our model to non-parametric model, the need to preset the numbers of
senses and latent concepts are totally removed and, moreover, the model performance is also improved.
We evaluate our model on the commonly used benchmark datasets released by both Semeval-2010
(Manandhar et al., 2010) and Semeval-2007 (Agirre and Soroa, 2007). The test results show that our
models perform much better than the state-of-the-art systems.
2 The parametric model
2.1 Basic Model
The main point of our work is that different senses are signaled by contexts with different concept con-
figurations, where different concepts are formally defined as different distributions over context words.
Formally, we denote by P (s) the global multinomial distribution over senses of an ambiguous word
and by P (w|z) the multinomial distributions over context words w given concept z. Context words are
generated by a mixture of different concepts whose mixture proportion is defined by P (z|s), such that:
P (w
i
) =
?
j
P (s = j)
?
k
P (z
i
= k|s = j)P (w
i
|z
i
= k)
Following the model, each context word w
i
surrounding the target word is generated as follows: First, a
sense s is sampled from P (s) for the target word. Then for each context word position i, a concept z
i
is
sampled according to mixture proportion P (z|s) and w
i
is finally sampled from P (w|z).
Figure 2 shows the model with the graphical notation, where M is the number of instances of contexts
regarding to a concerned target word and N
m
is the number of word tokens in context m. s
m
is the
sense label for target word in context m. w
m,n
is the n-th context word in context m. z
m,n
is the
concept label associated with w
m,n
. I is the total number of senses to be induced. J is the total number
356
Figure 3: Graphical notation of the non-parametric WSI model
of concepts.
~
? is the notational shorthand for the sense distribution P (s), ~?
i
is the shorthand for the
i-th sense-concept distribution P (z|s = i), and ~?
j
is the j-th concept-word distribution P (w|z = j).
Following conventional Bayesian practice,
~
?, ~?
i
and ~?
j
are assumed to be drawn from Dirichlet priors
with symmetric parameter ?, ?, ? respectively. The observed variable is represented with shaded node
and hidden variable with unshaded node.
2.2 Zipf?s law of meaning
Most of the WSI work requires that the number of senses to be induced be specified ahead of time.
One straightforward way to deal with this problem is to repeatedly try different numbers of senses on
a development set and select the best performed number. However, this should be done in principle on
a word-by-word basis, and thus could be time-consuming and prohibitive when there are lots of target
words to be concerned. A more systematic way of setting sense numbers in Bayesian models is extending
the parametric model into a non-parametric model, which will be described in detail in section 3.
To work with our parametric model, we propose in this paper that an empirical law, Zipf?s law of
meaning (Zipf, 1945), could be used to guide the sense number selection. Zipf?s law of meaning states
that the number of sense of a word is proportional to its frequency as shown in the following equation:
I = K ? f
b
(1)
where I is the number of word senses and f is the frequency of the word. K is the coefficient of
proportionality which is unknown and b is about 0.404 according to an experimental study done by
Edmonds (2006).
Certainly, Zipf?s law of meaning is not as strict as a rigorous mathematical law. However, it sketches
the distribution of the sense numbers with word frequencies of all words and allows us to estimate the
sense numbers on an all-word basis by selecting appropriate coefficient K. This is not only simple but
also efficient, especially in the case that there are a large number of target words to be concerned.
3 Non-parametric Model
A limitation of the parametric model is that the sense number I of the target word and the number J
of latent concepts need to be fixed beforehand. Bayesian non-parametric (BNP) models offer elegant
approach to the problem of model selection and adaption. Rather than comparing models that vary in
complexity, the BNP approach is to fit a single model that can adapt its complexity to the data. Unlike
the parametric approach, BNP approach assumes an infinite number of clusters, among which only a few
are active given the training data. Our basic model can be naturally extended into a BNP model as shown
in Figure 3. Instead of assuming a finite number of senses, we place a nonparametric, Dirichlet process
(DP) prior on the sense distribution as follows:
G ? DP (?,H)
s
m
? G,m = 1, 2, . . . ,M
where ? is the concentration parameter and H is the base measure of the Dirichlet process.
357
For each sense s
i
of the target words, we place a Hierarchical Dirichlet process (HDP) prior on the
mixture proportion to latent concepts shown as follows:
G
0
? DP (?,H
0
)
G
i
? DP (?,G
0
), i = 1, 2, . . .
z
m,n
? G
i
, n = 1, 2, . . . , N
m
w
m,n
? ~?
z
m,n
where ? and ? are concentration parameters to G
0
and G
i
, H
0
is the base measure of G
0
.
By using HDP priors, we make sure that the same set of concept-word distributions is shared across
all senses and all contexts of a target word, since each random measure G
i
inherits its set of concepts
from the same G
0
.
As in parametric model, ~?
j
is the j-th concept-word distribution P (w|z = j), however, there are now
an infinite number of such distributions. So is the number of senses. However, with a fixed number of
contexts of the target word, only a finite number of senses and concepts are active and they could be
inferred automatically by the inference procedure.
4 Model Inference
We use Gibbs sampling (Casella and George, 1992) for inference to both the parametric and nonpara-
metric model. As a particular Markov Chain Monte Carlo (MCMC) method, Gibbs sampling is widely
used for inference in various Bayesian models (Teh et al., 2006; Li and Li, 2013; Li and Cardie, 2014).
4.1 The Parametric Model
For the parametric model, we use collapsed Gibbs sampling, in which the sense distribution
~
?, sense-
concept distribution ~?
i
and concept-word distribution ~?
j
are integrated out. At each iteration, the sense
label s
m
of the target word in context m is sampled from conditional distribution p(s
m
|~s
?m
, ~z, ~w),
and the concept label z
m,n
for the context word w
m,n
is sampled from conditional distribution
p(z
m,n
|~s, ~z
?(m,n)
, ~w). Here ~s
?m
refers to all current sense assignments other than s
m
and ~z
?(m,n)
refers
to all current concept assignment other than z
m,n
.
The conditional distribution p(s
m
|~s
?m
, ~z, ~w) and p(z
m,n
|~s, ~z
?(m,n)
, ~w) can be derived as shown in
equation (2) and (3) respectively:
p(s
m
= i|~s
?m
, ~z, ~w;?, ?, ?) ? (c
?m
i
+ ?) ?
?
J
j=1
?
f
m,j
x=1
(c
?m
i,j
+ ? + x? 1)
?
f
m,?
x=1
(
?
J
j=1
c
?m
i,j
+ J ? ? + x? 1)
(2)
p(z
m,n
= j|~s, ~z
?(m,n)
, ~w;?, ?, ?) ? (c
?(m,n)
s
m
,j
+ ?) ?
(c
?(m,n)
j,w
m,n
+ ?)
?
V
t=1
c
?(m,n)
j,t
+ V ? ?
(3)
Here, c
?m
i
is the number of instances with sense i. c
?m
i,j
is the number of concept j in instances with sense
i. Both of them are counted without the m-th instance of the target word. c
?(m,n)
s
m
,j
is defined in a similar
way with c
?m
i,j
but without counting the word position (m,n). c
?(m,n)
j,w
m,n
is the number of times word w
m,n
is assigned to concept j without counting word position (m,n). f
m,j
is the number of concept j assigned
to context words in instance m and f
m,?
is the total number of words in contexts of instance m. V stands
for the size of the word dictionary, i.e. the number of different words in the data. x is an index which
iterates from 1 to f
m,?
.
~
?, ~?
i
and ~?
j
can be estimated in a similar way, we now only show as example the estimation of ~?
i
,
parameters for sense-concept distributions. According to their definitions as multinomial distributions
with Dirichlet prior, applying Bayes? rule yields:
p(~?
i
|~z;~?) =
p(~?
i
;~?) ? p(~z|~?
i
;~?)
Z
~?
i
= Dir(~?
i
|~c
i
+ ~?)
358
where ~c
i
is the vector of concept counts for sense i. Using the expectation of the Dirichlet distribution,
values of ?
i,j
can be worked out as follows:
?
i,j
=
c
i,j
+ ?
?
J
k=1
c
i,k
+ J ? ?
Different read-outs of ?
i,j
are then averaged to produce the final estimation.
4.2 The Non-parametric Model
Chinese restaurant process (CRP) and Chinese restaurant franchise (CRF) process (Teh et al., 2006)
have been widely used as sampling scheme for DP and HDP respectively. As our non-parametric model
involves both DP and HDP, we use both CRP and CRF based sampling for model inference.
In the CRP metaphor to DP, there is one Chinese restaurant with an infinite number of tables, each of
which can seat an infinite number of customers. The first customer enters the restaurant and sits at the
first table. The second customer enters and decides either to sit with the first customer or by herself at
a new table. In general, the n + 1st customer either joins an already occupied table k with probability
proportional to the number n
k
of customers already sitting there, or sits at a new table with probability
proportional to ?. As in our model, when we sample the sense s
m
for each context, we assume that
tables correspond to senses of target words and customers correspond to whole contexts in which the
target word occurs.
In the CRF metaphor to HDP, there are multiple Chinese restaurants, and each one has infinitely many
tables. On each table the restaurant serves one of infinitely many dishes that other restaurants may serve
as well. At each table of each restaurant one dish is ordered from the menu by the first customer who
sits there, and it is shared among all customers who sit at that table. The menu is shared by all the
restaurants. To be specific to our model, when we sample the concept z
m,n
for each context word, we
assume each sense s
m
of the target word corresponds to a restaurant and each word w
m,n
corresponds
to a customer while concept z
m,n
corresponds to the dishes served to the customer by the restaurant.
Neither the number of restaurant nor the number of dishes is finite in our model.
For model inference, we first sample s
m
using CRP-based sampling and then we sample z
m,n
for each
s
m
using CRF-based sampling. The sampling of s
m
and z
m,n
are done alternately, but not independently.
The sampling of s
m
is conditional on the current value of z
m,n
and vice versa, conforming to the scheme
of Gibbs Sampling.
The equation for sampling s
m
is derived as in equation (4):
p(s
m
= i|~s
?m
, ~z, ~w) ?
{
c
?m
i
? p(~z
m
|~z
?m
, s
m
= i) if i = old
? ? p(~z
m
|~z
?m
, s
m
= i
new
) else
where
p(~z
m
|~z
?m
, s
m
= i) =
?
J
j=1
?
f
m,j
x=1
(c
?m
i,j
+ ? ?
c
?m
t,j
c
?m
t,?
+?
+ x? 1)
?
f
m,?
x=1
(
?
J
j=1
c
?m
i,j
+ ? + x? 1)
(4)
Here p(~z
m
|~z
?m
, s
m
= i) is estimated block-wise for context m according to the CRF metaphor. c
?m
i
and c
?m
i,j
are defined in the same way as that in equation (2). c
?m
t,j
is the number of tables with dish j in
all restaurants but m and c
?m
t,?
means the number of tables in all restaurants but m. x is an index which
iterates from 1 to f
m,?
.
Sampling z
m,n
needs more steps than sampling s
m
as we need to record the table assignment for each
dish (concept). For each dish z
m,n
of a customer w
m,n
, we first sample the table at which the customer
sits according to the following equations:
p(t
m,n
= t|
~
t
?(m,n)
, ~z
?(m,n)
, w
m,n
, s
m
= i) ?
{
c
?(m,n)
i,t
? p
?(m,n)
j
(w
m,n
) if t = old
? ? p(w
m,n
|
~
t
?(m,n)
, t
m,n
= t, ~z
?(m,n)
, w
m,n
) else
where
p
?(m,n)
j
(w
m,n
) = p(w
m,n
|z
m,n
= j, ~w
?(m,n)
) =
c
?(m,n)
j,w
m,n
+ ?
?
V
t=1
c
?(m,n)
j,t
+ V ?
359
Basic Model BNP
? 1.0 0.2
? 0.05 0.01
? 0.05 0.2
? N/A 0.001
K 0.27 N/A
Concept number 20 N/A
Context window ? 5 words ? 9 words
Table 1: Hyperparamters of our models
Here c
?(m,n)
i,t
is the number of customers on table t in restaurant i and c
?(m,n)
j,w
m,n
has the same meaning as
in equation (3). If the sampled table t is previously occupied, then z
m,n
is set to the dish j assigned to
t according to the CRF metaphor. If the sampled table t is new, the probability p(w
m,n
|
~
t
?(m,n)
, t
m,n
=
t, ~z
?(m,n)
, w
m,n
) is calculated using equation (5), which is the sum of the probability of all previously
ordered dishes and the newly ordered dish.
p(w
m,n
|
~
t
?(m,n)
, t
m,n
= t, ~z
?(m,n)
, w
m,n
) =
J
?
j=1
c
?(m,n)
t,j
c
?(m,n)
t,?
+ ?
? p
?(m,n)
j
(w
m,n
) +
?
c
?(m,n)
t,?
+ ?
? p
?(m,n)
j
new
(5)
Because a new table is added, we then sample a new dish for this table according to equation (6).
p(z
m,n
= j|
~
t, ~z
?(m,n)
) ?
{
c
?(m,n)
t,j
? p
?(m,n)
j
(w
m,n
) if j = old
? ? p
?(m,n)
j
new
(w
m,n
) if j = new
(6)
After the dish j is sampled, it is assigned to the new table and the number of table serving dish j is added.
Parameters
~
?, ~?
i
and ~?
j
can be estimated in the same way as described in section 4.1.
5 Experiment
5.1 Experiment Setup
Data Our primary WSI evaluation is based on the standard dataset in Semeval-2010 Word sense induc-
tion & Disambiguation task (Manandhar et al., 2010). The target word dataset consists of 100 words, 50
nouns and 50 verbs. There are a total number of 879,807 sentences in training set and 8,915 sentences in
testing set. The average number of word senses in the data is 3.79.
Model Selection The trail data of Semeval-2010 WSI task is used as development set for parameter
tuning, which consists of training and test portions of 4 verbs. The 4 verbs are different words than the
100 target words in the training data. There are only about 138 instances on average for each target word
in the training part of the trial data. To make a development set of more reasonable size, the trial data
are supplemented with 6K instances of the 4 verbs extracted from the British National Corpus (BNC)
1
corpus. As we use the Zipf?s law of meaning to guide the selection of number of senses, BNC was also
used to count word frequencies.
The final hyper-parameters are set as in Table 1. In all the following experiments, Gibbs sampler is
run for 2000 iterations with burn-in period of 500 iterations. Every 10th sample is read out for parameter
estimating after the burn-in period to avoid autocorrelation. Due to the randomized property of Gibbs
sampler, all results in the next sections are averaged over 5 runs. The average running time for each target
word is about 7 minutes on a computer equipped with an Intel Core i5 processor working at 3.1GHz and
8GB RAM.
Pre-Processing For each instance of the target word in training data and testing data, all words are
lemmatized and stop words like ?of ?, ?the?, ?a? which are irrelevant to word sense distinction are filtered.
Words occurring less than twice are removed.
Evaluation method Semeval-2010 WSI task presents two evaluation schemes which are supervised
evaluation and unsupervised evaluation. In supervised evaluation, the gold standard dataset is split into
1
www.natcorp.ox.ac.uk/
360
Model
Supervised Evaluation Unsupervised Evaluation
Averaged #s
80-20 split 60-40 split V-Measure Paired-Fscore
Basic Model 64.12 63.68 11.52 44.42 5
Basic Model + Zipf 66.4 65.25 15.2 35.12 7.66
BNP 69.3 68.9 21.4 23.1 15.62
Table 2: Test results with different configurations.
Figure 4: Examples of concepts induced with the BNP model specific to the target word address.n (with
c
i
denoting concept)
a mapping and an evaluation parts. The first part is used to map the automatically induced senses to
gold standard senses. The mapping is then used to calculate the system?s F-Score on the second part.
According to the size of mapping data and evaluation data, the evaluation results are measured on two
different splits which are 80-20 splits and 60-40 splits. 80-20 splits means that 80% of the test data are
used for mapping and 20% are used for evaluation. In unsupervised evaluation, the system outputs are
compared by using metrics V-Measure (Rosenberg and Hirschberg, 2007) and Paired F-Score (Artiles et
al., 2009).
5.2 Experiment Results
Table 2 lists all experiment results. The Basic Model stands for the parametric model with fixed number
of senses for all target words. The number of senses is set to 5 which gives the best performance on
development set. Basic Model + Zipf is the model with the number of sense estimated by Zipf?s law of
meaning. BNP stands for our non-parametric model. As we can see, compared with the Basic Model with
fixed sense number, the model using Zipf?s law of meaning achieves improved performance. This means
Zipf?s law of meaning has positive effect in setting the sense number of the WSI task. BNP achieves the
best performance on both supervised evaluation and V-measure evaluation. In terms of Paired F-score,
however, the Basic Model gets the best results while BNP performs worst. This is consistent with what
claimed by Manandhar et al. (2010), that Paired F-score tends to penalize the model with higher number
of clusters.
As stated before, our models not only perform word sense induction but also group the context words
into concepts. Figure 4 shows 4 of the concepts induced by BNP with regard to the target word address.n.
Senses of address.n are defined as the mixture of concepts and concepts are defined as distributions over
context words. We only list the top five words with the highest probabilities under each concept. As
shown in Table 2, the non-parametric model induces much finer granularity of senses than the gold
standard, it makes distinction among email address, web address, and even ip address. A possible
solution is to further measure the closeness of senses based on the sense representations induced and
merge similar senses to produce coarser granularity of senses.
361
Model F-score(%)
BNP+position 69.7
BNP 69.3
Basic Model + Zipf 66.4
Basic Model 64.1
HDP 65.8
HDP+position (Lau et al., 2012) 68
distNB (Choe and Charniak, 2013) 65.4
UoY (Korkontzelos and Manandhar, 2010) 62.4
Model F-score(%)
BNP+position 88.0
BNP 86.1
HDP (Yao and Van Durme, 2011) 85.7
HDP+position (Lau et al., 2012) 86.9
Feature-LDA (Brody and Lapata, 2009) 85.5
1-layer-LDA (Brody and Lapata, 2009) 84.6
HRG (Klapaftis and Manandhar, 2010) 87.6
I2R (Niu et al., 2007) 86.8
Table 3: Comparison with state-of-the-arts on Semeval-2010 data (left) and Semeval-2007 data (right)
5.3 Comparison with previous work
Much previous work (Brody and Lapata, 2009; Klapaftis and Manandhar, 2010; Yao and Van Durme,
2011) tested their models only on Semeval-2007 dataset (Agirre and Soroa, 2007) which consists of
roughly 27K instances of 65 target verbs and 35 target nouns, coming from the Wall Street Journal
corpus (WSJ) (Agirre and Soroa, 2007). For a complete comparison, we also test our model on the
Semeval-2007 dataset. Since training data was not provided as part of the original Semeval-2007 dataset,
we follow the approach of previous work (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau et
al., 2012) to construct training data for each target word by extracting instances from the BNC corpus.
Following paractices as much previous work (Brody and Lapata, 2009; Yao and Van Durme, 2011; Lau
et al., 2012) did, we compare with previous work with supervised F-score on 80-20 data split in Semeval-
2010 and noun data in Semeval-2007.
Table 3 (left) compares our models against the state-of-the-art systems tested on 80-20 data split in
Semeval-2010. HDP+position (Lau et al., 2012) improved the HDP model (Yao and Van Durme, 2011)
by including a position feature. distNB (Choe and Charniak, 2013) extends the naive Bayes model
by reweighting the conditional probability of a context word given the sense by its distance to the tar-
get word. UoY (Korkontzelos and Manandhar, 2010) is the best performing system in Semeval-2010
competition which used a graph-based model. We re-implemented and tested the HDP model on the
Semeval-2010 dataset since Yao and Van Durme (2011) and Lau et al. (2012) did not report their HDP
results on this dataset.
Different with normal practice in WSI work, there is no feature engineering in our model. However,
our BNP model outperformed all the systems on supervised evaluation. Even the Basic Model outper-
formed the best performing Semeval-2010 system. Especially, our BNP model performs much better
than the HDP model. Both Lau et al. (2012) and Choe and Charniak (2013) show benefit of using po-
sitional information. Since our model does not exclude further feature engineering, we also introduce a
position feature
2
into our non-parametric model (BNP+position) as in Lau et al. (2012). This contributes
to a further 0.4% rise in performance.
Table 3 (right) compares our models with previous work on the nouns dataset in Semeval-2007. We
divides systems being compared into two groups. The first group model the WSI task with Bayesian
framework, while the second group uses models other than Bayesian model. Feature-LDA is the LDA-
based model proposed by Brody and Lapata (2009) which incorporates a large number of features into the
model. The 1-layer-LDA is their model with only bag-of-words features. HRG is a hierarchical random
graph model. I2R is the best performing system in Semeval-2007. As shown in Table 3 (right), our
BNP model with position feature (BNP+position) outperforms all systems. If we restrict our attention
to the first group in which all models are Bayesian model, our BNP model without feature engineering
outperforms the HDP model which is also non-parametric model without feature engineering.
6 Related Work
A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies
on the distributional hypothesis, which states that words occurring in similar contexts will have similar
meanings. Different work exploits distributional information in different forms, including context clus-
tering models (Sch?utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010),
graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian
2
Formally, the position feature is the context words with its relative position to the target word.
362
models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task.
They used the LDA-based model in which contexts of target word were viewed as documents as in the
LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word
and included a variety of features such as words, part-of-speech and dependency information. Yao and
Van Durme (2011) extended LDA-based model into non-parametric HDP model but removed the feature
engineering. Lau et al. (2012) showed improved supervised F-score by including position feature to the
HDP model. Choe and Charniak (2013) proposed a reweighted naive Bayes model by incorporating the
idea that words closer to the target word are more relevant in predicting the sense.
Our model differs from the context clustering models and graph-based models, as it is a Bayesian
probabilistic model. Our work also differs from the LDA-based models. LDA topics were actually
re-interpreted as senses of target word as Brody and Lapata (2009) applied the LDA to WSI tasks, so
did Yao and Van Durme (2011) and Lau et al. (2012). They induced word senses by firstly tagging
(sampling) senses (of target words) to context words and selecting the mostly tagged sense as sense of
target words. Our model could be viewed as an extension of LDA, but fit the WSI task more naturally
and much better. We distinguish senses of target words from concepts of context words and assume that
they are separate. Therefore, our model has two hidden layers corresponding to the sense of the target
word and the concepts of the context words respectively. Basically, one decide the sense of the target
word based on the concept configuration of context words, instead of tagging senses of target word to
context words. The separation of senses of target word and concepts of context words is actually not
only required by linguistic intuition but also leads to improvement by our experiment. Our model is also
different from the naive Bayes model since our model induces senses of the target word at concept level
while naive Bayes model works at word level and does not involve conceptualization to context words at
all.
7 Conclusion
In this paper, we first proposed a parametric Bayesian generative model to the task of Word Sense Induc-
tion. It is distinct from previous work in that it introduces a layer of latent concepts that generalize the
context words and thus enable the model to measure the sense similarity at a more general level. We also
show in this paper that Zipf?s law of meaning can be used to guide the setting of sense numbers on an
all-word basis, which is not only simple but also independent of the clustering methods being used. We
further extend our parametric model to non-parametric model which not only simplifies the problem of
model selection but also bring improved performance. The test results on the benchmark datasets show
that our model outperforms the state-of-the-art systems.
Acknowledgments
This work is supported by National Natural Science Foundation of China under Grant No. 61273318
and National Key Basic Research Program of China 2014CB340504.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination
systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7?12. Association
for Computational Linguistics.
Javier Artiles, Enrique Amig?o, and Julio Gonzalo. 2009. The role of named entities in web people search. In
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume
2, pages 534?542. Association for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference
of the European Chapter of the Association for Computational Linguistics, pages 103?111. Association for
Computational Linguistics.
George Casella and Edward I George. 1992. Explaining the gibbs sampler. The American Statistician, 46(3):167?
174.
363
Do Kook Choe and Eugene Charniak. 2013. Naive Bayes word sense induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1433?1437, Seattle, Washington,
USA, October. Association for Computational Linguistics.
Phillip Edmonds. 2006. Disambiguation, lexical. Encyclopedia of Language and Linguistics. Second Edition.
Elsevier.
Wesam Elshamy, Doina Caragea, and William H Hsu. 2010. Ksu kdd: Word sense induction by clustering in topic
space. In Proceedings of the 5th international workshop on semantic evaluation, pages 367?370. Association
for Computational Linguistics.
Roman Kern, Markus Muhr, and Michael Granitzer. 2010. Kcdc: Word sense induction by using grammatical
dependencies and sentence phrase structure. In Proceedings of the 5th international workshop on semantic
evaluation, pages 351?354. Association for Computational Linguistics.
Ioannis P Klapaftis and Suresh Manandhar. 2010. Word sense induction & disambiguation using hierarchical
random graphs. In Proceedings of the 2010 conference on empirical methods in natural language processing,
pages 745?755. Association for Computational Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy: Graphs of unambiguous vertices for word sense in-
duction and disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction
for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association
for Computational Linguistics, pages 591?601. Association for Computational Linguistics.
Jiwei Li and Claire Cardie. 2014. Timeline generation: Tracking individuals on twitter. In Proceedings of the
23rd international conference on World wide web, pages 643?652.
Jiwei Li and Sujian Li. 2013. Evolutionary hierarchical dirichlet process for timeline summarization. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
pages 556?560, Sofia, Bulgaria, August. Association for Computational Linguistics.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan. 2010. Semeval-2010 task
14: Word sense induction & disambiguation. In Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 63?68. Association for Computational Linguistics.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word sense discrimination,
chinese word sense disambiguation, and english word sense disambiguation. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 177?182. Association for Computational Linguistics.
Ted Pedersen. 2010. Duluth-wsi: Senseclusters applied to the sense induction task of semeval-2. In Proceed-
ings of the 5th international workshop on semantic evaluation, pages 363?366. Association for Computational
Linguistics.
Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evalua-
tion measure. In EMNLP-CoNLL, volume 7, pages 410?420.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes.
Journal of the american statistical association, 101(476).
Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric bayesian word sense induction. In Graph-based
Methods for Natural Language Processing, pages 10?14.
George Kingsley Zipf. 1945. The meaning-frequency relationship of words. The Journal of General Psychology,
33(2):251?256.
364
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 854?863,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Joint Model for Unsupervised Chinese Word Segmentation
Miaohong Chen Baobao Chang Wenzhe Pei
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871
miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn
Abstract
In this paper, we propose a joint model for
unsupervised Chinese word segmentation
(CWS). Inspired by the ?products of ex-
perts? idea, our joint model firstly com-
bines two generative models, which are
word-based hierarchical Dirichlet process
model and character-based hidden Markov
model, by simply multiplying their proba-
bilities together. Gibbs sampling is used
for model inference. In order to further
combine the strength of goodness-based
model, we then integrated nVBE into our
joint model by using it to initializing the
Gibbs sampler. We conduct our experi-
ments on PKU and MSRA datasets pro-
vided by the second SIGHAN bakeoff.
Test results on these two datasets show
that the joint model achieves much bet-
ter results than all of its component mod-
els. Statistical significance tests also show
that it is significantly better than state-
of-the-art systems, achieving the highest
F-scores. Finally, analysis indicates that
compared with nVBE and HDP, the joint
model has a stronger ability to solve both
combinational and overlapping ambigui-
ties in Chinese word segmentation.
1 Introduction
Unlike English and many other western languages,
there are no explicit word boundaries in Chinese
sentences. Therefore, word segmentation is a cru-
cial first step for many Chinese language process-
ing tasks such as syntactic parsing, information re-
trieval and machine translation. A great deal of su-
pervised methods have been proposed for Chinese
word segmentation. While successful, they re-
quire manually labeled resources and often suffer
from issues like poor domain adaptability. Thus,
unsupervised word segmentation methods are still
attractive to researchers due to its independence on
domain and manually labeled corpora.
Previous unsupervised approaches to word seg-
mentation can be roughly classified into two types.
The first type uses carefully designed goodness
measure to identify word candidates. Popular
goodness measures include description length gain
(DLG) (Kit and Wilks, 1999), accessor variety
(AV) (Feng et al., 2004), boundary entropy (BE)
(Jin and Tanaka-Ishii, 2006) and normalized vari-
ation of branching entropy (nVBE) (Magistry and
Sagot, 2012) etc. Goodness measure based model
is not segmentation model in a very strict mean-
ing and is actually strong in generating word list
without supervision. It inherently lacks capabil-
ity to deal with ambiguous string, which is one of
main sources of segmentation errors and has been
extensively explored in supervised Chinese word
segmentation.
The second type focuses on designing sophis-
ticated statistical model, usually nonparametric
Bayesian models, to find the segmentation with
highest posterior probability, given the observed
character sequences. Typical statistical mod-
els includes Hierarchical Dirichlet process (HDP)
model (Goldwater et al., 2009), Nested Pitman-
Yor process (NPY) model (Mochihashi et al.,
2009) etc, which are actually nonparametric lan-
guage models and therefor can be categorized as
word-based model. Word-based model makes de-
cision on wordhood of a candidate character se-
quence mainly based on information outside the
sequence, namely, the wordhood of character se-
quences being adjacent to the concerned sequence.
Inspired by the success of character-based
model in supervised word segmentation, we pro-
pose a Bayesian HMM model for unsupervised
Chinese word segmentation. With the Bayesian
HMM model, we formulate the unsupervised seg-
mentation tasks as procedure of tagging positional
854
tags to characters. Different from word-based
model, character-based model like HMM-based
model as we propose make decisions on word-
hood of a candidate character sequence based on
information inside the sequence, namely, ability of
characters to form words. Although the Bayesian
HMM model alone does not produce competi-
tive results, it contributes substantially to the joint
model as proposed in this paper.
Our joint model takes advantage from three dif-
ferent models: namely, a character-based model
(HMM-based), a word-based model (HDP-based)
and a goodness measure based model (nVBE
model). The combination of HDP-based model
and HMM-based model enables to utilize infor-
mation of both word-level and character-level. We
also show that using nVBE model as initialization
model could further improve the performance to
outperform the state-of-the-art systems and leads
to improvement in both wordhood judgment and
disambiguation ability.
Word segmentation systems are usually eval-
uated with metrics like precision, recall and F-
Score, regardless of supervised or unsupervised.
Following normal practice, we evaluate our model
and compare it with state-of-the-art systems us-
ing F-Score. However, we argue that the ability
to solve segmentation ambiguities is also impor-
tant when evaluating different types of unsuper-
vised word segmentation systems.
This paper is organized as follows. In Section
2, we will introduce several related systems for
unsupervised word segmentation. Then our joint
model is presented in Section 3. Section 4 shows
our experiment results on the benchmark datasets
and Section 5 concludes the paper.
2 Related Work
Unsupervised Chinese word segmentation has
been explored in a number of previous works and
by various methods. Most of these methods can
be divided into two categories: goodness measure
based methods and nonparametric Bayesian meth-
ods.
There have been a plenty of work that is based
on a specific goodness measure. Zhao and Kit
(2008) compared several popular unsupervised
models within a unified framework. They tried
various types of goodness measures, such as De-
scription Length Gain (DLG) proposed by Kit and
Wilks (1999), Accessor Variety (AV) proposed by
Feng et al. (2004) and Boundary Entropy (Jin and
Tanaka-Ishii, 2006). A notable goodness-based
method is ESA: ?Evaluation, Selection, Adjust-
ment?, which is proposed by Wang et al. (2011)
for unsupervised Mandarin Chinese word segmen-
tation. ESA is an iterative model based on a new
goodness algorithm that adopts a local maximum
strategy and avoids threshold setting. One disad-
vantage of ESA is that it needs to iterate the pro-
cess several times on the corpus to get good perfor-
mance. Another disadvantage is the requirement
for a manually segmented training corpus to find
best value for parameters (they called it proper ex-
ponent). Another notable work is nVBE: Mag-
istry and Sagot (2012) proposed a model based
on the Variation of Branching Entropy. By adding
normalization and viterbi decoding, they improve
performance over Jin and Tanaka-Ishii (2006)
and remove most of the parameters and thresholds
from the model.
Nonparametric Bayesian models also achieved
state-of-the-art performance in unsupervised word
segmentation. Goldwater et al. (2009) introduced
a unigram and a bigram model for unsupervised
word segmentation, which are based on Dirichlet
process and hierarchical Dirichlet process (Teh et
al., 2006) respectively. The main drawback is that
it needs almost 20,000 iterations before the Gibbs
sampler converges. Mochihashi et al. (2009) ex-
tended this method by introducing a nested charac-
ter model and an efficient blocked Gibbs sampler.
Their method is based on what they called nested
Pitman-Yor language model.
One disadvantage of goodness measure based
methods is that they do not have any disambigua-
tion ability in theory in spite of their competitive
performances. This is because once the goodness
measure is given, the decoding algorithm will seg-
ment any ambiguous strings into the same word
sequences, no matter what their context is. In
contrast, nonparametric Bayesian language mod-
els aim to segment character string into a ?reason-
able? sentence according to the posterior probabil-
ity. Thus, theoretically, this method should have
better ability to solve ambiguities over goodness
measure based methods.
3 Joint Model
In this section, we will discuss our joint model in
detail.
855
3.1 Combining HDP and HMM
In supervised Chinese word segmentation lit-
erature, word-based approaches and character-
based approaches often have complementary ad-
vantages (Wang et al., 2010).Since the two types
of model try to solve the problem from different
perspectives and by utilizing different levels of in-
formation (word level and character level). In un-
supervised Chinese word segmentation literature,
the HDP-base model can be viewed as a typi-
cal word-based method. And we can also build
a character-based unsupervised model by using a
hidden Markov model. We believe that the HDP-
based model and the HMM-based model are also
complementary with each other, and a combina-
tion of them will take advantage of both and thus
capture different levels of information.
Now the problem we are facing is how to com-
bine these two models. To keep the joint model
simple and involve as little extra parameters as
possible, we combine the two baseline models by
just multiplying their probabilities together and
then renormalizing it. Let C = c
1
c
2
? ? ? c
|C|
be a
string of characters andW = w
1
w
2
? ? ?w
|W |
is the
corresponding segmented words sequence. Then
the conditional probability of the segmentation W
given the character string C in our joint model is
defined as:
P
J
(W |C) =
1
Z(C)
P
D
(W |C)P
M
(W |C) (1)
where P
D
(W |C) is the probability from the HDP
model as given in Equation 6 and P
M
(W |C)
is the probability given by the Bayesian HMM
model as given in Equation 2. Z(C) is a nor-
malization term to make sure that P
J
(W |C) is a
probability distribution. The combining method is
inspired by Hinton (1999), which proved that it is
possible to combine many individual expert mod-
els by multiplying the probabilities and then renor-
malizing it. They called it ?product of experts?.
We can see that combining models in this way
does not involve any extra parameters and Gibbs
sampling can be easily used for model inference.
3.2 Bayesian HMM
The dominant method for supervised Chinese
word segmentation is character-based model
which was first proposed by Xue (2003). This
method treats word segmentation as a tagging
problem, each tag indicates the position of a char-
acter within a word. The most commonly used
tag set is {Single, Begin, Middle, End}. Specifi-
cally, S means the character forms a single word,
B/E means the character is the begining/ending
character of the word, and M means the charac-
ter is in the middle of the word. Existing models
are trained on manually annotated data in a super-
vised way based on discriminative models such as
Conditional Random Fields (Peng et al., 2004;
Tseng et al., 2005). Supervised character-based
methods make full use of character level informa-
tion and thus have been very successful in the last
decade. However, no unsupervised model has uti-
lized character level information in the way as su-
pervised method does.
We can also build a character-based model for
Chinese word segmentation using hidden Markov
model(HMM) as formulated in the following
equation:
P
M
(W |C) =
|C|
?
i=1
P
t
(t
i
|t
i?1
)P
e
(c
i
|t
i
) (2)
where C and W have the same meaning as be-
fore. P
t
(t
i
|t
i?1
) is the transition probability of
tag t
i
given its former tag t
i?1
and P
e
(c
i
|t
i
) is the
emission probability of character c
i
given its tag t
i
.
This model can be easily trained with Maximum
Likelihood Estimation (MLE) on annotated data
or with Expectation Maximization (EM) on raw
texts. But using any of this methods will make it
difficult to combine it with the HDP-based model.
Instead, we propose a Bayesian HMM for unsu-
pervised word segmentation. The Bayesian HMM
model is defined as follows:
t
i
|t
i?1
= t, p
t
? Mult(p
t
)
c
i
|t
i
= t, e
t
? Mult(e
t
)
p
t
|? ? Dirichlet(?)
e
t
|? ? Dirichlet(?)
where p
t
and e
t
are transition and emission dis-
tributions, ? and ? are the symmetric parameters
of Dirichlet distributions. Now suppose we have
observed tagged text h, then the conditional prob-
ability P
M
(w
i
|w
i?1
= l, h) can be obtained:
P
M
(w
i
|w
i?1
= l, h)
=
|w
i
|
?
j=1
P
t
(t
j
|t
j?1
, h)P
e
(c
j
|t
j
, h) (3)
where < w
i?1
, w
i
> is a word bigram, l is the in-
dex of word w
i?1
, c
j
is the jth character in word
856
wi
and t
j
is the corresponding tag.P
t
(t
j
|t
j?1
, h)
and P
e
(c
j
|t
j
, h) are the posterior probabilities,
they are given as:
P
t
(t
j
|t
j?1
, h) =
n
<t
j?1
,t
j
>
+ ?
n
<t
j?1
,?>
+ T?
(4)
P
e
(c
j
|t
j
, h) =
n
<t
j
,c
j
>
+ ?
n
<t
j
,?>
+ V ?
(5)
where n
<t
j?1
,t
j
>
is the tag bigram count of <
t
j?1
, t
j
> in h, n
<t
j
,c
j
>
denotes the number of oc-
currences of tag t
j
and character c
j
, and ? means
a sum operation. T and V are the size of character
tag set (we follow the commonly used {SBME}
tag set and thus T = 4 in this case) and character
vocabulary.
3.3 HDP Model
Goldwater et al. (2009) proposed a nonparametric
Bayesian model for unsupervised word segmenta-
tion which is based on HDP (Teh et al., 2006). In
this model, the conditional probability of the seg-
mentation W given the character string C is de-
fined as:
P
D
(W |C) =
|W |
?
i=0
P
D
(w
i
|w
i?1
) (6)
where w
i
is the ith word in W . This is actually
a nonparametric bigram language model. This bi-
gram model assumes that each different word has
a different distribution over words following it, but
all these different distributions are linked through
a HDP model:
w
i
|w
i?1
= l ? G
l
G
l
? DP (?
1
, G
0
)
G
0
? DP (?,H)
where DP denotes a Dirichlet process.
Suppose we have observed segmentation re-
sult h, then we can get the posterior probability
P
D
(w
i
|w
i?1
= l, h) by integrating out G
l
:
P
D
(w
i
|w
i?1
= l, h)
=
n
<w
i?1
,w
i
>
+ ?
1
P
D
(w
i
|h)
n
<w
i?1
,?>
+ ?
1
(7)
where n
<w
i?1
,w
i
>
denotes the total number of oc-
currences of the bigram < w
i?1
, w
i
> in the ob-
servation h. And P
D
(w
i
|h) can be got by integrat-
ing out G
0
:
P
D
(w
i
|h) =
t
w
i
+ ?H(w
i
)
t+ ?
(8)
where t
w
i
denotes the number of tables associ-
ated with w
i
in the Chinese Restaurant Franchise
metaphor (Teh et al., 2006), t is the total number
of tables and H(w
i
) is the base measure of G
0
. In
fact, H(w
i
) is the prior distribution over words, so
prior knowledge can be injected in this distribution
to enhance the performance.
In Goldwater et al. (2009)?s work, the base
measureH(w
i
) are defined as a character unigram
model:
H(w
i
) = (1? p
s
)
|w
i
|?1
p
s
?
j
P (c
ij
)
where, p
s
is the probability of generating a word
boundary. P (c
ij
) is the probability of the jth char-
acter c
ij
in word w
i
, this probability can be esti-
mated from the training data using maximum like-
lihood estimation.
3.4 Initializing with nVBE
Among various goodness measure based models,
we choose nVBE (Magistry and Sagot, 2012) to
initialize our Gibbs sampler with its segmentation
results. nVBE achieved a relatively high perfo-
mance over other goodness measure based meth-
ods. And it?s very simple as well as efficient.
Theoretically, the Gibbs sampler may be initial-
ized at random or using any other methods. Initial-
ization does not make a difference since the Gibbs
sampler will eventually converge to the posterior
distribution if it iterates as much as possible. This
is an essential attribute of Gibbs sampling. How-
ever, we believe that initializing the Gibbs sam-
pler with the result of nVBE will benefit us in
two ways. On one hand, in consideration of its
combination of nonparametric Bayesian method
and goodness-based method, it will improve the
overall performance as well as solve more seg-
mentation ambiguities with the help of HDP-based
model. On the other hand, it makes the conver-
gence of Gibbs sampling faster. In practice, ran-
dom initialization often leads to extremely slow
convergence.
3.5 Inference with Gibbs Sampling
In our proposed joint model, Gibbs sam-
pling (Casella and George, 1992) can be easily
used to identify the highest probability segmen-
tation from among all possibilities. Following
Goldwater et al. (2009), we can repeatedly sample
from potential word boundaries. Each boundary
857
variable can only take on two possible values, cor-
responding to a word boundary or not word bound-
ary.
For instance, suppose we have obtained a seg-
mentation result ?|c
i?2
c
i?1
c
i
c
i+1
c
i+2
|?, where ?
and ? are the words sequences to the left and
right and c
i?2
c
i?1
c
i
c
i+1
c
i+2
are characters be-
tween them. Now we are sampling at location i
to decide whether there is a word boundary be-
tween c
i
and c
i+1
. Denote h
1
as the hypothesis
that it forms a word boundary (the correspond-
ing result is ?w
1
w
2
? where w
1
= c
i?2
c
i?1
c
i
and
w
2
= c
i+1
c
i+2
), and h
2
as the opposite hypoth-
esis (then the corresponding result is ?w? where
w = c
i?2
c
i?1
c
i
c
i+1
c
i+2
). The posterior probabil-
ity for these two hypotheses would be:
P (h
1
|h
?
) ? P
D
(h
1
|h
?
)P
M
(h
1
|h
?
) (9)
P (h
2
|h
?
) ? P
D
(h
2
|h
?
)P
M
(h
2
|h
?
) (10)
where P
D
(h|h
?
) and P
M
(h|h
?
) are the pos-
terior probabilities in HDP-based model and in
HMM-based model, and h
?
denotes the current
segmentation results for all observed data except
c
i?2
c
i?1
c
i
c
i+1
c
i+2
. Note that the normalization
term Z(C) can be ignored during inference. The
posterior probabilities for these two hypotheses in
the HDP-based model is given as:
P
D
(h
1
|h
?
) = P
D
(w
1
|w
l
, h
?
)
? P
D
(w
2
|w
1
, h
?
)P
D
(w
r
|w
2
, h
?
) (11)
P
D
(h
2
|h
?
) = P
D
(w|w
l
, h
?
)
? P
D
(w
r
|w, h
?
) (12)
where w
l
(w
r
) is the first word to the left (right) of
w. And the posterior probabilities for the Bayesian
HMM model is given as:
P
M
(h
1
|h
?
)
?
i+2
?
j=i?2
P
t
(t
j
|t
j?1
, h
?
)P
e
(c
j
|t
j
, h
?
) (13)
P
M
(h
2
|h
?
)
?
i+2
?
j=i?2
P
t
(t
j
|t
j?1
, h
?
)P
e
(c
j
|t
j
, h
?
) (14)
where P
t
(t
j
|t
j?1
, h
?
) and P
e
(c
j
|t
j
, h
?
) are given
in Equation 4 and 5. The difference is that un-
der hypothesis h
1
, c
i?2
c
i?1
c
i
c
i+1
c
i+2
are tagged
as ?BMEBE? and under hypothesis h
2
as ?BM-
MME?.
Once the Gibbs sampler is converged, a natu-
ral way to is to treat the result of last iteration as
the final segmentation result, since each set of as-
signments to the boundary variables uniquely de-
termines a segmentation.
4 Experiments
In this section, we test our joint model on PKU
and MSRA datesets provided by the Second Seg-
mentation Bake-off (SIGHAN 2005) (Emerson,
2005). Most previous works reported their results
on these two datasets, this will make it convenient
to directly compare our joint model with theirs.
4.1 Setting
The second SIGHAN Bakeoff provides several
large-scale labeled data for evaluating the per-
formance of Chinese word segmentation systems.
Two of the four datasets are used in our exper-
iments. Both of the dataset contains only sim-
plified Chinese. Table 1 shows the statistics of
the two selected corpus. For development set, we
randomly select a small subset (about 10%) of
the training data. Specifically, 2000 sentences are
selected for PKU corpus and 8000 sentences for
MSRA corpus. The rest training data plus the test
set is then combined for segmentation but only test
data is used for evaluation. The development set is
used to tune parameters of the HDP-based model
and HMM-based model separately. Since our joint
model does not involve any additional parameters,
we reuse the parameters of the HDP-based model
and HMM-based model in the joint model. Specif-
ically, we set ?
1
= 1000.0, ? = 10.0, p
s
= 0.5 for
the HDP-based model and set ? = 1.0, ? = 0.01
for the HMM-based model.
For evaluation, we use standard F-Score on
words for all following experiments. F-Score is
the harmonic mean of the word precision and re-
call. Precision is given as:
P =
#correct words in result
#total words in result
and recall is given as:
R =
#correct words in result
#total words in gold corpus
then F-Score is calculated as:
F =
2?R ? F
R + F
858
Corpus TrainingSize (words) TestSize (words)
PKU 1.1M 104K
MSRA 2.37M 107K
Table 1: Statistics of training and testing data
Huang and Zhao (2007) provided an empirical
method to estimate the consistency between the
four different segmentation standards involved in
the Bakeoff-3. A lowest consistency rate 84.8%
is found among the four standards. Zhao and Kit
(2008) considered this figure as the upper bound
for any unsupervised Chinese word segmentation
systems. We also use it as the topline in our com-
parison.
4.2 Prior Knowledge Used
When it comes to the evaluation and compari-
son for unsupervised word segmentation systems,
an important issue is what kind of pre-processing
steps and prior knowledge are needed. To be fully
unsupervised, any prior knowledge such as punc-
tuation information, encoding scheme and word
length could not be used in principle. Neverthe-
less, information like punctuation can be easily in-
jected to most existing systems and significantly
enhance the performance. The problem we are
faced with is that we don?t know for sure what
kind of prior information are used in other sys-
tems. One may use a small punctuation set to
segment a long sentence into shorter ones, while
another may write simple regular expressions to
identify dates and numbers. Lot of work we com-
pare to don?t even mention this subject.
Fortunately, we notice that Wang et al. (2011)
provided four kinds of preprocessings (they call
settings). In their settings 1 and 2, punctuation
and other encoding information are not used. In
setting 3, punctuation is used to segment charac-
ter sequences into sentences, and both punctuation
and other encoding information are used in setting
4. Then the results reported in Magistry and Sagot
(2012) relied on setting 3 and setting 4. In order
to make the comparison as fair as possible, we use
setting 3 in our experiment, i.e., only a punctua-
tion set for simplified Chinese is used in all our
experiments. We will compare our experiment re-
sults to previous work on the same setting if they
are provided .
4.3 Experiment Results
Table 2 summarizes the F-Scores obtained by dif-
ferent models on PKU and MSRA corpus, as well
as several state-of-the-art systems. Detailed infor-
mation about the presented models are listed as
follows:
? nVBE: the model based on Variation of
Branching Entropy in Magistry and Sagot
(2012). We re-implement their model on set-
ting 3
1
.
? HDP: the HDP-based model proposed by
Goldwater et al. (2009), initialized randomly.
? HDP+HMM: the model combining HDP-
based model and HMM-based model as pro-
posed in Section 3, initialized randomly.
? HDP+nVBE: the HDP-based model, initial-
ized with the results of nVBE model.
? Joint: the ?HDP+HMM? model initialized
with nVBE model.
? ESA: the model proposed in Wang et al.
(2011), as mentioned above, the conducted
experiments on four different settings, we re-
port their results on setting 3.
? NPY(2): the 2-gram language model pre-
sented by Mochihashi et al. (2009).
? NPY(3): the 3-gram language model pre-
sented by Mochihashi et al. (2009).
For all of our Gibbs samplers, we run 5 times to
get the averaged F-Scores. We also give the vari-
ance of the F-Scores in Table 2. For each run, we
find that random initialization takes around 1,000
iterations to converge, while initialing with nVBE
only takes as few as 10 iterations. This makes
1
The results we got with our implementation is slightly
lower than what was reported in Magistry and Sagot (2012).
According to Pei et al. (2013), they had contacted the authors
and confirmed that the higher results was due to a bug in code.
So we report the results with our bug free implementation as
Pei et al. (2013) did. Our reported results are identical to
those of Pei et al. (2013)
859
System
PKU MSRA
R P F R P F
nVBE 78.3 77.5 77.9 79.1 77.3 78.2
HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020)
HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013)
HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005)
Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005)
ESA N/A N/A 77.4 N/A N/A 78.4
NPY(2) N/A N/A N/A N/A N/A 80.2
NPY(3) N/A N/A N/A N/A N/A 80.7
Topline N/A N/A 84.8 N/A N/A 84.8
Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote
the variance the of F-Scores.
our joint model very efficient and possible to work
in practical applications as well. At last, a single
sample (the last one) is used for evaluation.
From Table 2, we can see that the joint
model (Joint) outperforms all the presented sys-
tems in F-Score on all testing corpora. Specifi-
cally, comparing ?HDP+HMM? with ?HDP?, the
former model increases the overall F-Score from
68.7% to 75.3% (+6.6%) in PKU corpora and
from 69.9% to 76.3% (+6.4%) in MSRA corpora,
which proves that the character information in
the HMM-based model can actually enhance the
performance of the HDP-based model. Compar-
ing ?HDP+nVBE? with ?HDP?, the former model
also increases the overall F-Score by 10.6%/9.6%
in PKU/MSRA corpora, which demonstrates that
initializing the HDP-based model with nVBE will
improve the performance by a large margin. Fi-
nally, the joint model ?Joint? take advantage from
both from the character-based HMM model and
the nVBE model, it achieves a F-Score of 81.1%
on PKU and 81.7% on MSRA. This result outper-
forms all its component baselines such as ?HDP?,
?HDP+HMM? and ?HDP+nVBE?.
Our joint model also shows competitive advan-
tages over several state-of-the-art systems. Com-
pared with nVBE,the F-Score increases by 3.2%
on PKU corpora and by 3.5% on MSRA cor-
pora. Compared with ESA, the F-Score increases
by 3.7%/3.3% in PKU/MSRA corpora. Lastly,
compared to the nonparametric Bayesian models
(NPY(n)), our joint model still increases the F-
Score by 1.5% (NPY(2)) and 1.0% (NPY(3)) on
MSRA corpora. Moreover, compared with the
empirical topline figure 84.8%, our joint model
achieves a pretty close F-Score. The differences
are 3.7% on PKU corpora and 3.1% on MSRA
corpora.
An phenomenon we should pay attention to is
the poor performance of the HMM-based model.
With our implementation of the Bayesian HMM,
we achieves a 34.3% F-Score on PKU corpora and
a 34.9% F-Score on MSRA corpora, just slightly
better than random segmentation. The result show
that the hidden Markov Model alone is not suit-
able for character-based Chinese word segmenta-
tion problem. However, it still substantially con-
tributes to the joint model.
We find that the variance of the results are rather
small, this shows the stability of our Gibbs sam-
plers. From the segmentation results generated
by the joint model, we also found that quite a
large amount of errors it made are related to dates,
numbers (both Chinese and English) and English
words. This problem can be easily addressed dur-
ing preprocessing by considering encoding infor-
mation as previous work, and we believe this will
bring us much better performance.
4.4 Disambiguation Ability
Previous unsupervised work usually evaluated
their models using F-score, regardless of goodness
measure based model or nonparametric Bayesian
model. However, segmentation ambiguity is
a very important factor influencing accuracy of
Chinese word segmentation systems (Huang and
Zhao, 2007). We believe that the disambigua-
tion ability of the models should also be consid-
ered when evaluating different types of unsuper-
vised segmentation systems, since different type
of models shows different disambiguation ability.
We will compare the disambiguation ability of dif-
860
ferent systems in this section.
In general, there are mainly two kinds of ambi-
guity in Chinese word segmentation problem:
? Combinational Ambiguity: Given charac-
ter strings ?A? and ?B?, if ?A?, ?B?, ?AB?
are all in the vocabulary, and ?AB? or ?A-B?
(here ?-? denotes a space) occurred in the real
text,then ?AB? can be called a combinational
ambiguous string.
? Overlapping Ambiguity: Given character
strings ?A?, ?J? and ?B?, if ?A?, ?B?, ?AJ?
and ?JB? are all in the vocabulary, and ?A-
JB? or ?AJ-B? occurred in the real text, then
?AJB? can be called an overlapping ambigu-
ous string.
We count the total number of mistakes differ-
ent systems made at ambiguous strings (the vo-
cabulary is obtained from the gold standard an-
swer of testing set). As we have mentioned in
Section 2, goodness measure based methods such
as nVBE do not have any disambiguation ability
in theory. Our observation is identical to this ar-
gument. We find that nVBE always segments am-
biguous strings into the same result. Take a combi-
national string ??k? as an example, ?? (just)?,
?k (have)? and ??k (only)? are all in the vo-
cabulary. In the PKU test set, this string occurs
14 times as ??-k (just have)? and 18 times as
??k (only)?, 32 times in total. nVBE segments
all the 32 strings into ??k (only)? (i.e. 18 of
them are correct), while the joint model segments
it 22 times as ??k (only)? and 10 times as ??-
k (just have)? according to its context, and 24 of
them are correct.
Table 3 and 4 show the statistics of combi-
national ambiguity and overlapping ambiguity re-
spectively. The numbers in parentheses denote the
total number of ambiguous strings. From these
tables, we can see that HDP+nVBE makes less
mistakes than nVBE in most circumstances, ex-
cept that it solves less combinational ambigui-
ties on MSRA corpora. But our proposed joint
model solves the most combinational and over-
lapping ambiguities, on both PKU and MSRA
corpora. Specifically, compared to nVBE, the
joint model correctly solves 171/871 more com-
binational ambiguities on PKU/MSRA corpora,
which is a 0.6%/13.8% relative error reduction.
It also solves 28/45 more overlapping ambiguities
on PKU/MSRA corpora, which is a 11.5%/23.4%
relative error reduction. This indicates that the
joint model has a stronger ability of disambigua-
tion over the compared systems.
System PKU(35371) MSRA(38506)
nVBE 8087 7236
HDP+nVBE 7970 7500
Joint 7916 6305
Table 3: Statistics of combinational ambiguity.
This table shows the total number of mistakes
made by different systems at combinational am-
biguous strings. The numbers in parentheses de-
note the total number of combinational ambiguous
strings.
System PKU(603) MSRA(467)
nVBE 244 192
HDP+nVBE 239 164
Joint 216 157
Table 4: Statistics of overlapping ambiguity. This
table shows the total number of mistakes made
by different systems at overlapping ambiguous
strings. The numbers in parentheses denote the to-
tal number of overlapping ambiguous strings.
4.5 Statistical Significance Test
The main results presented in Table 2 has shown
that our proposed joint model outperforms the
two baselines as well as state-of-the-art systems.
But it is also important to know if the improve-
ment is statistically significant over these sys-
tems. So we conduct statistical significance tests
of F-scores among these various models. Follow-
ing Wang et al. (2010), we use the bootstrapping
method (Zhang et al., 2004).
Here is how it works: suppose we have a testing
set T
0
to test several word segmentation systems,
there are N testing examples (sentences or line of
characters) in T
0
. We create a new testing set T
1
with N examples by sampling with replacement
from T
0
, then repeat these process M ? 1 times.
And we will have a total M +1 testing sets. In our
test procedures, M is set to 2000.
Since we just implement our joint model and
its component models, we can not generate paired
samples for other models (i.e. ESA and NPY(n)).
Instead, we follow Wang et al. (2010)?s method
and first calculate the 95% confidence interval for
861
our proposed model. Then other systems can be
compared with the joint model in this way: if the
F-score of system B doesn?t fall into the 95% con-
fidence interval of system A, they are considered
as statistically significantly different from each
other.
For all significant tests, we measure the 95%
confidence interval for the difference between
two models. First, the test results show that
?HDP+nVBE? and ?HDP+HMM? are both sig-
nificantly better than ?HDP?. Second, the
?Joint? model significantly outperforms all its
component models, including ?HDP?, ?nVBE?,
?HDP+nVBE? and ?HDP+HMM?. Finally, the
comparison also shows that the joint model signif-
icantly outperforms state-of-the-art systems like
ESA and NPY(n).
5 Conclusion
In this paper, we proposed a joint model for un-
supervised Chinese word segmentation. Our joint
model is a combination of the HDP-based model,
which is a word-based model, and HMM-based
model, which is a character-based model. The
way we combined these two component base-
lines makes it natural and simple to inference with
Gibbs sampling. Then the joint model take ad-
vantage of a goodness-based method (nVBE) by
using it to initialize the sampler. Experiment re-
sults conducted on PKU and MSRA datasets pro-
vided by the second SIGHAN Bakeoff show that
the proposed joint model not only outperforms the
baseline systems but also achieves better perfor-
mance (F-Score) over several state-of-the-art sys-
tems. Significance tests showed that the improve-
ment is statistically significant. Analysis also in-
dicates that the joint model has a stronger abil-
ity to solve ambiguities in Chinese word segmen-
tation. In summary, the joint model we pro-
posed combines the strengths of character-based
model, nonparametric Bayesian language model
and goodness-based model.
Acknowledgments
The contact author of this paper, according to
the meaning given to this role by Key Labora-
tory of Computational Linguistics, Ministry of Ed-
ucation, School of Electronics Engineering and
Computer Science, Peking University, is Baobao
Chang. And this work is supported by National
Natural Science Foundation of China under Grant
No. 61273318 and National Key Basic Research
Program of China 2014CB340504.
References
George Casella, Edward I. George. 1992. Explain-
ing the Gibbs sampler. The American Statistician,
46(3): 167-174.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of
the Fourth SIGHANWorkshop on Chinese Language
Processing, 133. MLA.
Haodi Feng, Kang Chen, Xiaotie Deng, et al. 2004.
Accessor variety criteria for Chinese word extraction
Computational Linguistics, 30(1): 75-93.
Sharon Goldwater, Thomas L. Griffiths, Mark Johnson.
2009. A Bayesian framework for word segmenta-
tion: Exploring the effects of context. Cognition
112(1): 21-54.
Geoffrey E. Hinton. 1999. Products of experts. Arti-
ficial Neural Networks. Ninth International Confer-
ence on Vol. 1.
Changning Huang, Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3): 8-20.
Zhihui Jin, Kumiko Tanaka-Ishii. 2006. Unsupervised
segmentation of Chinese text by use of branching
entropy. Proceedings of the COLING/ACL on Main
conference poster sessions, page 428-435.
Chunyu Kit, Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. Proceedings of the CoNLL99 ACL Workshop.
Bergen, Norway: Association for Computational
Linguis-tics, page 1-6.
Pierre Magistry, Benoit Sagot. 2012. Unsuper-
vized word segmentation: the case for mandarin
chinese. Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2. Association for Computa-
tional Linguistics, page 383-387.
Daichi Mochihashi, Takeshi Yamada, Naonori Ueda.
2009. Bayesian unsupervised word segmentation
with nested Pitman-Yor language modeling. Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1. Association for Com-
putational Linguistics, page 100-108.
Wenzhe Pei, Dongxu Han, Baobao Chang. 2013. A
Refined HDP-Based Model for Unsupervised Chi-
nese Word Segmentation. Chinese Computational
Linguistics and Natural Language Processing Based
on Naturally Annotated Big Data. Springer Berlin
Heidelberg, page 44-51.
862
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, et
al. 2006. Sharing Clusters among Related Groups:
Hierarchical Dirichlet Processes. NIPS.
Fuchun Peng, Fangfang Feng, Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. Proceedings
of COLING, page 562-568.
Huihsin Tseng, Pichuan Chang, Galen Andrew, et al.
2005. A conditional random field word segmenter
for sighan bakeoff 2005. Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, Vol. 171.
Kun Wang, Chengqing Zong, Keh-Yih Su. 2010. A
character-based joint model for Chinese word seg-
mentation. Proceedings of the 23rd International
Conference on Computational Linguistics. Associa-
tion for Computational Linguistics, page 1173-1181.
Hanshi Wang, Jian Zhu, Shiping Tang, et al. 2011. A
new unsupervised approach to word segmentation.
Computational Linguistics, 37(3): 421-454.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1): 29-48.
Hai Zhao, Chunyu Kit. 2008. An Empirical Compar-
ison of Goodness Measures for Unsupervised Chi-
nese Word Segmentation with a Unified Framework.
IJCNLP, page 6-16.
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? LREC.
863
