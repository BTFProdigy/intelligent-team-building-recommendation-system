First Joint Conference on Lexical and Computational Semantics (*SEM), pages 54?58,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Detecting Text Reuse with Modified and Weighted N-grams
Rao Muhammad Adeel Nawab?, Mark Stevenson? and Paul Clough?
?Department of Computer Science and ?iSchool
University of Sheffield, UK.
{r.nawab@dcs, m.stevenson@dcs, p.d.clough@} .shef.ac.uk
Abstract
Text reuse is common in many scenarios and
documents are often based, at least in part, on
existing documents. This paper reports an ap-
proach to detecting text reuse which identifies
not only documents which have been reused
verbatim but is also designed to identify cases
of reuse when the original has been rewrit-
ten. The approach identifies reuse by compar-
ing word n-grams in documents and modifies
these (by substituting words with synonyms
and deleting words) to identify when text has
been altered. The approach is applied to a cor-
pus of newspaper stories and found to outper-
form a previously reported method.
1 Introduction
Text reuse is the process of creating new docu-
ment(s) using text from existing document(s). Text
reuse is standard practice in some situations, such as
journalism. Applications of automatic detection of
text reuse include the removal of (near-)duplicates
from search results (Hoad and Zobel, 2003; Seo and
Croft, 2008), identification of text reuse in journal-
ism (Clough et al, 2002) and identification of pla-
giarism (Potthast et al, 2011).
Text reuse is more difficult to detect when the
original text has been altered. We propose an ap-
proach to the identification of text reuse which is
intended to identify reuse in such cases. The ap-
proach is based on comparison of word n-grams, a
popular approach to detecting text reuse. However,
we also account for synonym replacement and word
deletion, two common text editing operations (Bell,
1991). The relative importance of n-grams is ac-
counted for using probabilities obtained from a lan-
guage model. We show that making use of modified
n-grams and their probabilities improves identifica-
tion of text reuse in an existing journalism corpus
and outperforms a previously reported approach.
2 Related Work
Approaches for identifying text reuse based on
word-level comparison (such as the SCAM copy de-
tection system (Shivakumar and Molina, 1995)) tend
to identify topical similarity between a pair of doc-
uments, whereas methods based on sentence-level
comparison (e.g. the COPS copy detection sys-
tem (Brin et al, 1995)) are unable to identify when
text has been reused if only a single word has been
changed in a sentence.
Comparison of word and character n-grams has
proven to be an effective method for detecting text
reuse (Clough et al, 2002; Ceden?o et al, 2009; Chiu
et al, 2010). For example, Ceden?o et al (2009)
showed that comparison of word bigrams and tri-
grams are an effective method for detecting reuse in
journalistic text. Clough et al (2002) also applied
n-gram overlap to identify reuse of journalistic text,
combining it with other approaches such as sentence
alignment and string matching algorithms. Chiu et
al. (2010) compared n-grams to identify duplicate
and reused documents on the web. Analysis of word
n-grams has also proved to be an effective method
for detecting plagiarism, another form of text reuse
(Lane et al, 2006).
However, a limitation of n-gram overlap approach
is that it fails to identify reuse when the original
54
text has been altered. To overcome this problem we
propose using modified n-grams, which have been
altered by deleting or substituting words in the n-
gram. The modified n-grams are intended to im-
prove matching with the original document.
3 Determining Text Reuse with N-gram
Overlap
3.1 N-grams Overlap (NG)
Following Clough et al (2002), the asymmetric con-
tainment measure (eqn 1) was used to quantify the
degree of text within a document (A) that is likely to
have been reused in another document (B).
scoren(A,B) =
?
ngram?B
count(ngram,A)
?
ngram?B
count(ngram,B)
(1)
where count(ngram,A) is the number of times
ngram appears in document A. A score of 1 means
that document B is contained in document A and a
score of 0 that none of the n-grams in B occur in A.
3.2 Modified N-grams
N-gram overlap has been shown to be useful for
measuring text reuse as derived texts typically share
longer n-grams (? 3 words). However, the approach
breaks down when an original document has been
altered. To counter this problem we applied vari-
ous techniques for modifying n-grams that allow for
word deletions (Deletions) and word substitutions
(WordNet and Paraphrases), two common text edit-
ing operations.
Deletions (Del) Assume that w1, w2, ...wn is an
n-gram. Then a set of modified n-grams can be cre-
ated by removing one of the w2 ... wn?1. The first
and last words in the n-gram are not removed since
they will also be generated as shorter n-grams. An
n-gram will generate n ? 2 deleted n-grams and no
deleted n-grams will be generated for unigrams and
bigrams.
Substitutions Further n-grams can be created by
substituting one of the words in an n-gram with one
of its synonyms from WordNet (WN). For words
with multiple senses we use synonyms from all
senses. Modified n-grams are created by substitut-
ing one of the words in the n-gram with one of its
synonyms from WordNet.
Similarly to the WordNet approach, n-grams can
be created by substituting one of the words with an
equivalent term from a paraphrase lexicon, which
we refer to as Paraphrases (Para). A paraphrase
lexicon was generated automatically (Burch, 2008)
and ten lexical equivalents (the default setting) pro-
duced for each word. Modified n-grams were cre-
ated by substituting one of the words in the n-gram
with one of the lexical equivalents.
3.3 Comparing Modified N-grams
The modified n-grams are applied in the text reuse
score by generating modified n-grams for the docu-
ment that is suspected to contain reused text. These
n-grams are then compared with the original docu-
ment to determine the overlap. However, the tech-
niques in Section 3.2 generate a large number of
modified n-grams which means that the number
of n-grams that overlap with document A can be
greater than the total number of n-grams in B, lead-
ing to similarity scores greater than 1. To avoid this
the n-gram overlap counts are constrained in a simi-
lar way that they are clipped in BLEU and ROUGE
(Papineni et al, 2002; Lin, 2004).
For each n-gram in B, a set of modified n-grams,
mod(ngram), is created.1 The count for an in-
dividual n-gram in B, exp count(ngram,B), can
be computed as the number of times any n-gram in
mod(ngram) occurs in A, see equation 2.
?
ngram? ?mod(ngram)
count(ngram?, A) (2)
However, the contribution of this count to the text
reuse score has to be bounded to ensure that the com-
bined count of the modified n-grams appearing in
A does not exceed the number of times the origi-
nal n-gram occurs in B. Consequently the text reuse
score, scoren(A,B), is computed using equation 3.
?
ngram
?B
min(exp count(ngram,A), count(ngram,B))
?
ngram?B
count(ngram,B)
(3)
3.4 Weighting N-grams
Probabilities of each n-gram, obtained using a lan-
guage model, are used to increase the importance of
1This is the set of n-grams that could have been created by
modifing an n-gram in B and includes the original n-gram itself.
55
rare n-grams and decrease the contribution of com-
mon ones. N-gram probabilities are computed us-
ing the SRILM language modelling toolkit (Stolcke,
2002). The score for each n-gram is computed as
its Information Content (Cover and Thomas, 1991),
ie. ?log(P ). When the language model (LM) is
applied the scores associated with each n-gram are
used instead of counts in equations 2 and 3.
4 Experiments
4.1 METER Corpus
The METER corpus (Gaizauskas et al, 2001) con-
tains 771 Press Association (PA) articles, some of
which were used as source(s) for 945 news stories
published by nine British newspapers.
These 945 documents are classified as Wholly De-
rived (WD), Partially Derived (PD) and Non De-
rived (ND). WD means that the newspaper article
is likely derived entirely from the PA source text;
PD reflects the situation where some of the newspa-
per article is derived from the PA source text; news
stories likely to be written independently of the PA
source fall into the category of ND. In our experi-
ments, the 768 stories from court and law reporting
were used (WD=285, PD=300, ND=183) to allow
comparison with Clough et al (2002). To provide a
collection to investigate binary classification we ag-
gregated the WD and PD cases to form a Derived set.
Each document was pre-processed by converting to
lower case and removing all punctuation marks.
4.2 Determining Reuse
The text reuse task aims to distinguish between lev-
els of text reuse, i.e. WD, PD and ND. Two versions
of a classification task were used: binary classifica-
tion distinguishes between Derived (i.e. WD ? PD)
and ND documents, and ternary classification distin-
guishes all three levels of reuse.
A Naive Bayes classifier (Weka version 3.6.1) and
10-fold cross validation were used for the experi-
ments. Containment similarity scores between all
PA source texts and news articles on the same story
were computed for word uni-grams, bi-grams, tri-
grams, four-grams and five-grams. These five simi-
larity scores were used as features. Performance was
measured using precision, recall and F1 measures
with the macro-average reported across all classes.
The language model (Section 3.4) was trained us-
ing 806,791 news articles from the Reuters Corpus
(Rose et al, 2002). A high proportion of the news
stories selected were related to the topics of enter-
tainment and legal reports to reflect the subjects of
the new articles in the METER corpus.
5 Results and Analysis
Tables 1 and 2 show the results of the binary
and ternary classification experiments respectively.
?NG? refers to the comparison of n-grams in each
document (Section 3.1), while ?Del?, ?WN? and
?Para? refer to the modified n-grams created us-
ing deletions, WordNet and paraphrases respectively
(Section 3.2). The prefix ?LM? (e.g. ?LM-NG?) in-
dicates that the n-grams are weighted using the lan-
guage model probability scores (Section 3.4).
For the binary classification task (Table 1) it can
be observed that including modified n-grams im-
proves performance. This improvement is observed
when each of the three types of modified n-grams
is applied individually, with a greater increase being
observed for the n-grams created using the WordNet
and paraphrase approaches. Further improvement is
observed when different types of modified n-grams
are combined with the best performance obtained
when all three types are used. All improvements
over the baseline approach (NG) are statistically
significant (Wilcoxon signed-rank test, p < 0.05).
These results demonstrate that the various types of
modified n-grams all contribute to identifying when
text is being reused since they capture different types
of rewrite operations.
In addition, performance consistently improves
when n-grams are weighted using language model
scores. The improvement is significant for all types
of n-grams. This demonstrates that the information
provided by the language model is useful in deter-
mining the relative importance of n-grams.
Several of the results are higher than those re-
ported by Clough et al (2002) (F1=0.763), despite
the fact their approach supplements n-gram overlap
with additional techniques such as sentence align-
ment and string search algorithms.
Results of the ternary classification task are
shown in Table 2. Results show a similar pattern
to those observed for the binary classification task
56
Approach P R F1
NG 0.836 0.706 0.732
LM-NG 0.846 0.722 0.746
Del 0.851 0.745 0.767
LM-Del 0.858 0.765 0.785
WN 0.876 0.801 0.817
LM-WN 0.879 0.810 0.825
Para 0.884 0.821 0.834
LM-Para 0.888 0.831 0.843
Del+WN 0.889 0.835 0.847
LM-Del+WN 0.884 0.848 0.855
Del+Para 0.892 0.841 0.853
LM-Del+Para 0.896 0.849 0.860
WN+Para 0.894 0.848 0.858
LM-WN+Para 0.896 0.865 0.871
Del+WN+Para 0.897 0.856 0.865
LM-Del+WN+Para 0.903 0.876 0.882
(Clough et al, 2002) ? ? 0.763
Table 1: Results for binary classification
and the best result is also obtained when all three
types of modified n-grams are included and n-grams
are weighted with probability scores. Once again
weighting n-grams with language model scores im-
proves results for all types of n-gram and this im-
provement is significant. Results for several types of
n-gram are also better than those reported by Clough
et al (2002) (F1=0.664).
Results for all approaches are lower for the
ternary classification. This is because the binary
classification task involves distinguishing between
two classes of documents which are relatively dis-
tinct (derived and non-derived) while the ternary
task divides the derived class into two (WD and PD)
which are more difficult to separate (see Table 3
showing confusion matrix for the approach which
gave best results for ternary classification).
6 Conclusion
This paper describes an approach to the analysis of
text reuse which is based on comparison of n-grams.
This approach is augmented by modifying the n-
grams in various ways and weighting them with
probabilities derived from a language model. Evalu-
ation is carried out on a standard data set containing
examples of reused journalistic texts. Making use of
Approach P R F1
NG 0.596 0.557 0.551
LM-NG 0.615 0.579 0.574
Del 0.612 0.584 0.579
LM-Del 0.633 0.611 0.606
WN 0.644 0.636 0.631
LM-WN 0.649 0.640 0.635
Para 0.662 0.653 0.647
LM-Para 0.669 0.659 0.654
Del+WN 0.655 0.649 0.643
LM-Del+WN 0.668 0.656 0.650
Del+Para 0.665 0.658 0.652
LM-Del+Para 0.661 0.662 0.655
WN+Para 0.668 0.661 0.655
LM-WN+Para 0.680 0.675 0.668
Del+WN+Para 0.669 0.666 0.660
LM-Del+WN+Para 0.688 0.689 0.683
(Clough et al, 2002) ? ? 0.664
Table 2: Results for ternary classification
Classified as WD PD ND
WD 139 94 14
PD 57 206 54
ND 1 13 191
Table 3: Confusion matrix when ?LM-Del+WN+Para?
approach used for ternary classification
modified n-grams with appropriate weights is found
to improve performance when detecting text reuse
and the approach described here outperforms an ex-
isting approach. In future we plan to experiment
with other methods for modifying n-grams and also
to apply this approach to other types of text reuse.
Acknowledgments
This work was funded by the COMSATS Institute
of Information Technology, Islamabad, Pakistan un-
der the Faculty Development Program (FDP) and a
Google Research Award.
References
Alberto B. Ceden?o, Paolo Rosso, and Jose M. Bened
2009. Reducing the Plagiarism Detection Search
Space on the basis of the Kullback-Leibler Distance
Proceedings of CICLing-09, 523?534.
57
Allan Bell 1991. The Language of News Media. Black-
well.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
901?904.
Chin-Yew Lin. 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Proceedings of the ACL-
04 Workshop, 74?81.
Chris Callison-Burch. 2008. Syntactic Constraints on
Paraphrases Extracted from Parallel Corpora. In Pro-
ceedings of EMNLP?08, 196?205.
Jangwon Seo and W. Bruce Croft. 2008. Local Text
Reuse Detection. In Proceedings of SIGIR?08, 571?
578. In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, 571?578.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
J. Zhu. 2002. Bleu: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL?02, 311?318.
Martin Potthast, Andreas Eiselt, Alberto Barro?n-Ceden?o,
Benno Stein and Paolo Rosso. 2011. Overview of the
3rd International Competition on Plagiarism Detec-
tion. Notebook Papers of CLEF 11 Labs and Work-
shops.
Narayanan Shivakumar and Hector G. Molina. 1995.
SCAM: A Copy Detection Mechanism for Digital Doc-
uments. Proceedings of the 2nd Annual Conference
on the Theory and Practice of Digital Libraries, Texas,
USA.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. Measuring Text Reuse. In Pro-
ceedings of ACL?02, Philadelphia, USA, 152?159.
Peter C. R. Lane, Caroline M. Lyon, and James A. Mal-
colm. 2006. Demonstration of the Ferret plagiarism
detector. Proceedings of the 2nd International Plagia-
rism Conference, Newcastle, UK.
Robert Gaizauskas, Jonathan Foster, Yorick Wilks, John
Arundel, Paul Clough, and Scott S.L. Piao. 2001. The
METER Corpus: A Corpus for Analysing Journalistic
Text Reuse. In Proceedings of the Corpus Linguistics
Conference, 214?223.
Sergey Brin, James Davis and Hector G. Molina. 1995.
Copy Detection Mechanisms for Digital Documents.
Proceedings ACM SIGMOD?95, 398?409.
Stanford Chiu, Ibrahim Uysal, Bruce W. Croft. 2010.
Evaluating text reuse discovery on the web. In Pro-
ceedings of the third symposium on Information inter-
action in context, 299?304.
Thomas M. Cover, Joy A. Thomas. 1991. Elements of
Information Theory. Wiley, New York, USA.
Timothy C. Hoad and Justin Zobel. 2003. Methods
for Identifying Versioned and Plagiarized Documents.
Journal of the American Society for Information Sci-
ence and Technology, 54(3):203?215.
Tony Rose, Mark Stevenson, Miles Whitehead. 2002.
The Reuters Corpus Volume 1 - from Yesterday?s news
to tomorr ow?s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC-02), 827?832.
58
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?47,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Natural Language Descriptions of Visual Scenes:
Corpus Generation and Analysis
Muhammad Usman Ghani Khan Rao Muhammad Adeel Nawab Yoshihiko Gotoh
University of Sheffield, United Kingdom
{ughani, r.nawab, y.gotoh}@dcs.shef.ac.uk
Abstract
As video contents continue to expand, it is
increasingly important to properly annotate
videos for effective search, mining and re-
trieval purposes. While the idea of anno-
tating images with keywords is relatively
well explored, work is still needed for anno-
tating videos with natural language to im-
prove the quality of video search. The fo-
cus of this work is to present a video dataset
with natural language descriptions which is
a step ahead of keywords based tagging.
We describe our initial experiences with a
corpus consisting of descriptions for video
segments crafted from TREC video data.
Analysis of the descriptions created by 13
annotators presents insights into humans?
interests and thoughts on videos. Such re-
source can also be used to evaluate auto-
matic natural language generation systems
for video.
1 Introduction
This paper presents our experiences in manu-
ally constructing a corpus, consisting of natural
language descriptions of video segments crafted
from a small subset of TREC video1 data. In
a broad sense the task can be considered one
form of machine translation as it translates video
streams into textual descriptions. To date the
number of studies in this field is relatively small
partially because of lack of appropriate dataset
for such task. Another obstacle may be inher-
ently larger variation for descriptions that can be
produced for videos than a conventional transla-
tion from one language to another. Indeed hu-
mans are very subjective while annotating video
1
www-nlpir.nist.gov/projects/trecvid/
streams, e.g., two humans may produce quite dif-
ferent descriptions for the same video. Based on
these descriptions we are interested to identify the
most important and frequent high level features
(HLFs); they may be ?keywords?, such as a par-
ticular object and its position/moves, used for a
semantic indexing task in video retrieval. Mostly
HLFs are related to humans, objects, their moves
and properties (e.g., gender, emotion and action)
(Smeaton et al, 2009).
In this paper we present these HLFs in the form
of ontologies and provides two hierarchical struc-
tures of important concepts ? one most relevant
for humans and their actions, and another for non
human objects. The similarity of video descrip-
tions is quantified using a bag of word model. The
notion of sequence of events in a video was quan-
tified using the order preserving sequence align-
ment algorithm (longest common subsequence).
This corpus may also be used for evaluation of
automatic natural language description systems.
1.1 Background
The TREC video evaluation consists of on-going
series of annual workshops focusing on a list of
information retrieval (IR) tasks. The TREC video
promotes research activities by providing a large
test collection, uniform scoring procedures, and
a forum for research teams interested in present-
ing their results. The high level feature extrac-
tion task aims to identify presence or absence of
high level semantic features in a given video se-
quence (Smeaton et al, 2009). Approaches to
video summarisation have been explored using
rushes video2 (Over et al, 2007).
2Rushes are the unedited video footage, sometimes re-
ferred to as a pre-production video.
38
TREC video also provides a variety of meta
data annotations for video datasets. For the HLF
task, speech recognition transcripts, a list of mas-
ter shot references, and shot IDs having HLFs in
them are provided. Annotations are created for
shots (i.e., one camera take) for the summarisa-
tion task. Multiple humans performing multiple
actions in different backgrounds can be shown in
one shot. Annotations typically consist of a few
phrases with several words per phrase. Human
related features (e.g., their presence, gender, age,
action) are often described. Additionally, camera
motion and camera angle, ethnicity information
and human?s dressing are often stated. On the
other hand, details relating to events and objects
are usually missing. Human emotion is another
missing information in many of such annotations.
2 Corpus Creation
We are exploring approaches to natural language
descriptions of video data. The step one of the
study is to create a dataset that can be used for
development and evaluation. Textual annotations
are manually generated in three different flavours,
i.e., selection of HLFs (keywords), title assign-
ment (a single phrase) and full description (mul-
tiple phrases). Keywords are useful for identifica-
tion of objects and actions in videos. A title, in a
sense, is a summary in the most compact form; it
captures the most important content, or the theme,
of the video in a short phrase. On the other hand,
a full description is lengthy, comprising of several
sentences with details of objects, activities and
their interactions. Combination of keywords, a ti-
tle, and a full descriptions will create a valuable
resource for text based video retrieval and sum-
marisation tasks. Finally, analysis of this dataset
provides an insight into how humans generate nat-
ural language description for video.
Most of previous datasets are related to spe-
cific tasks; PETS (Young and Ferryman, 2005),
CAVIAR (Fisher et al, 2005) and Terrascope
(Jaynes et al, 2005) are for surveillance videos.
KTH (Schuldt et al, 2004) and the Hollywood ac-
tion dataset (Marszalek et al, 2009) are for hu-
man action recognition. MIT car dataset is for
identification of cars (Papageorgiou and Poggio,
1999). Caltech 101 and Caltech 256 are image
datasets with 101 and 256 object categories re-
spectively (Griffin et al, 2007) but there is no
information about human actions or emotions.
There are some datasets specially generated for
scene settings such as MIT outdoor scene dataset
(Oliva and Torralba, 2009). Quattoni and Tor-
ralba (2009) created indoor dataset with 67 differ-
ent scenes categories. For most of these datasets
annotations are available in the form of keywords
(e.g., actions such as sit, stand, walk). They were
developed for keyword search, object recognition
or event identification tasks. Rashtchian et al
(2010) provided an interesting dataset of 1000 im-
ages which contain natural language descriptions
of those images.
In this study we select video clips from TREC
video benchmark for creating annotations. They
include categories such as news, meeting, crowd,
grouping, indoor/outdoor scene settings, traffic,
costume, documentary, identity, music, sports and
animals videos. The most important and proba-
bly the most frequent content in these videos ap-
pears to be a human (or humans), showing their
activities, emotions and interactions with other
objects. We do not intend to derive a dataset
with a full scope of video categories, which is be-
yond our work. Instead, to keep the task manage-
able, we aim to create a compact dataset that can
be used for developing approaches to translating
video contents to natural language description.
Annotations were manually created for a small
subset of data prepared form the rushes video
summarisation task and the HLF extraction task
for the 2007 and 2008 TREC video evaluations.
It consisted of 140 segments of videos ? 20 seg-
ments for each of the following seven categories:
Action videos: Human posture is visible and hu-
man can be seen performing some action
such as ?sitting?, ?standing?, ?walking? and
?running?.
Close-up: Human face is visible. Facial expres-
sions and emotions usually define mood of
the video (e.g., happy, sad).
News: Presence of an anchor or reporters. Char-
acterised by scene settings such as weather
boards at the background.
Meeting: Multiple humans are sitting and com-
municating. Presence of objects such as
chairs and a table.
Grouping: Multiple humans interaction scenes
that do not belong to a meeting scenario. A
39
table or chairs may not be present.
Traffic: Presence of vehicles such as cars, buses
and trucks. Traffic signals.
Indoor/Outdoor: Scene settings are more obvi-
ous than human activities. Examples may be
park scenes and office scenes (where com-
puters and files are visible).
Each segment contained a single camera shot,
spanning between 10 and 30 seconds in length.
Two categories, ?Close-up? and ?Action?, are
mainly related to humans? activities, expressions
and emotions. ?Grouping? and ?Meeting? de-
pict relation and interaction between multiple hu-
mans. ?News? videos explain human activities
in a constrained environment such as a broad-
cast studio. Last two categories, ?Indoor/Outdoor?
and ?Traffic?, are often observed in surveillance
videos. They often shows for humans? interac-
tion with other objects in indoor and outdoor set-
tings. TREC video annotated most video seg-
ments with a brief description, comprising of mul-
tiple phrases and sentences. Further, 13 human
subjects prepared additional annotation for these
video segments, consisting of keywords, a title
and a full description with multiple sentences.
They are referred to as hand annotations in the
rest of this paper.
2.1 Annotation Tool
There exist several freely available video anno-
tation tools. One of the popular video anno-
tation tool is Simple Video Annotation tool3.
It allows to place a simple tag or annotation
on a specified part of the screen at a particular
time. The approach is similar to the one used by
YouTube4. Another well-known video annotation
tool is Video Annotation Tool5. A video can be
scrolled for a certain time period and place anno-
tations for that part of the video. In addition, an
annotator can view a video clip, mark a time seg-
ment, attach a note to the time segment on a video
timeline, or play back the segment. ?Elan? anno-
tation tool allows to create annotations for both
audio and visual data using temporal information
(Wittenburg et al, 2006). During that annotation
process, a user selects a section of video using the
3
videoannotation.codeplex.com/
4
www.youtube.com/t/annotations about
5dewey.at.northwestern.edu/ppad2/documents/help/video.html
Figure 1: Video Description Tool (VDT). An anno-
tator watches one video at one time, selects all HLFs
present in the video, describes a theme of the video as
a title and creates a full description for important con-
tents in the video.
timeline capability and writes annotation for the
specific time.
We have developed our own annotation tool be-
cause of a few reasons. None of existing annota-
tion tools provided the functionality of generat-
ing a description and/or a title for a video seg-
ment. Some tools allows selection of keywords in
a free format, which is not suitable for our pur-
pose of creating a list of HLFs. Figure 1 shows
a screen shot of the video annotation tool devel-
oped, which is referred to as Video Description
Tool (VDT). VDT is simple to operate and assist
annotators in creating quality annotations. There
are three main items to be annotated. An anno-
tator is shown one video segment at one time.
Firstly a restricted list of HLFs is provided for
each segment and an annotator is required to se-
lect all HLFs occurring in the segment. Second,
a title should be typed in. A title may be a theme
of the video, typically a phrase or a sentence with
several words. Lastly, a full description of video
contents is created, consisting of several phrases
and sentences. During the annotation, it is pos-
sible to stop, forward, reverse or play again the
same video if required. Links are provided for
navigation to the next and the previous videos. An
annotator can delete or update earlier annotations
if required.
40
2.2 Annotation Process
A total of 13 annotators were recruited to create
texts for the video corpus. They were undergradu-
ate or postgraduate students and fluent in English.
It was expected that they could produce descrip-
tions of good quality without detailed instructions
or further training. A simple instruction set was
given, leaving a wide room for individual inter-
pretation about what might be included in the de-
scription. For quality reasons each annotator was
given one week to complete the full set of videos.
Each annotator was presented with a complete
set of 140 video segments on the annotation tool
VDT. For each video annotators were instructed
to provide
? a title of one sentence long, indicating the
main theme of the video;
? description of four to six sentences, related
to what are shown in the video;
? selection of high level features (e.g., male,
female, walk, smile, table).
The annotations are made with open vocabulary
? that is, they can use any English words as long
as they contain only standard (ASCII) characters.
They should avoid using any symbols or computer
codes. Annotators were further guided not to use
proper nouns (e.g., do not state the person name)
and information obtained from audio. They were
also instructed to select all HLFs appeared in the
video.
3 Corpus Analysis
13 annotators created descriptions for 140 videos
(seven categories with 20 videos per category),
resulting in 1820 documents in the corpus. The
total number of words is 30954, hence the av-
erage length of one document is 17 words. We
counted 1823 unique words and 1643 keywords
(nouns and verbs).
Figure 2 shows a video segment for a meet-
ing scene, sampled at 1 fps (frame per second),
and three examples for hand annotations. They
typically contain two to five phrases or sentences.
Most sentences are short, ranging between two to
six words. Descriptions for human, gender, emo-
tion and action are commonly observed. Occa-
sionally minor details for objects and events are
also stated. Descriptions for the background are
Hand annotation 1
(title) interview in the studio;
(description) three people are sitting on a red ta-
ble; a tv presenter is interviewing his guests; he is
talking to the guests; he is reading from papers in
front of him; they are wearing a formal suit;
Hand annotation 2
(title) tv presenter and guests
(description) there are three persons; the one is
host; others are guests; they are all men;
Hand annotation 3
(title) three men are talking
(description) three people are sitting around the
table and talking each other;
Figure 2: A montage showing a meeting scene in a
news video and three sets of hand annotations. In
this video segment, three persons are shown sitting on
chairs around a table ? extracted from TREC video
?20041116 150100 CCTV4 DAILY NEWS CHN33050028?.
often associated with objects rather than humans.
It is interesting to observe the subjectivity with the
task; the variety of words were selected by indi-
vidual annotators to express the same video con-
tents. Figure 3 shows another example of a video
segment for a human activity and hand annota-
tions.
3.1 Human Related Features
After removing function words, the frequency for
each word was counted in hand annotations. Two
classes are manually defined; one class is related
directly to humans, their body structure, identity,
action and interaction with other humans. (An-
other class represents artificial and natural objects
and scene settings, i.e., all the words not directly
related to humans, although they are important
for semantic understanding of the visual scene ?
described further in the next section.) Note that
some related words (e.g., ?woman? and ?lady?)
were replaced with a single concept (?female?);
concepts were then built up into a hierarchical
structure for each class.
Figure 4 presents human related information
observed in hand annotations. Annotators paid
full attention to human gender information as the
number of occurrences for ?female? and ?male? is
41
Figure 4: Human related information found in 13 hand annotations. Information is divided into structures (gen-
der, age, identity, emotion, dressing, grouping and body parts) and activities (facial, hand and body). Each box
contains a high level concept (e.g., ?woman? and ?lady? are both merged into ?female?) and the number of its
occurrences.
Hand annotation 1
(title) outdoor talking scene;
(description) young woman is sitting on chair in
park and talking to man who is standing next to
her;
Hand annotation 2
(title) A couple is talking;
(description) two person are talking; a lady is sit-
ting and a man is standing; a man is wearing a
black formal suit; a red bus is moving in the street;
people are walking in the street; a yellow taxi is
moving in the street;
Hand annotation 3
(title) talk of two persons;
(description) a man is wearing dark clothes; he is
standing there; a woman is sitting in front of him;
they are saying to each other;
Figure 3: A montage of video showing a human activ-
ity in an outdoor scene and three sets of hand annota-
tions. In this video segment, a man is standing while
a woman is sitting in outdoor ? from TREC video
?20041101 160000 CCTV4 DAILY NEWS CHN 41504210?.
the highest among HLFs. This highlights our con-
clusion that most interesting and important HLF
is humans when they appear in a video. On the
other hand age information (e.g., ?old ?, ?young?,
?child ?) was not identified very often. Names for
human body parts have mixed occurrences rang-
ing from high (?hand ?) to low (?moustache?). Six
basic emotions ? anger, disgust, fear, happiness,
sadness, and surprise as discussed by Paul Ek-
man6 ? covered most of facial expressions.
Dressing became an interesting feature when
a human was in a unique dress such as a formal
suit, a coloured jacket, an army or police uni-
form. Videos with multiple humans were com-
mon, and thus human grouping information was
frequently recognised. Human body parts were
involved in identification of human activities; they
included actions such as standing, sitting, walk-
ing, moving, holding and carrying. Actions re-
lated to human body and posture were frequently
identified. It was rare that unique human identi-
ties, such as police, president and prime minister,
were described. This may indicate that a viewer
might want to know a specific type of an object
to describe a particular situation instead of gener-
alised concepts.
3.2 Objects and Scene Settings
Figure 5 shows the hierarchy created for HLFs
that did not appear in Figure 4. Most of the words
are related to artificial objects. Humans inter-
act with these objects to complete an activity ?
6
en.wikipedia.org/wiki/Paul Ekman
42
Figure 5: Artificial and natural objects and scene set-
tings were summarised into six groups.
e.g., ?man is sitting on a chair?, ?she is talking
on the phone?, ?he is wearing a hat ?. Natural ob-
jects were usually in the background, providing
the additional context of a visual scene ? e.g.,
?human is standing in the jungle, ?sky is clear to-
day?. Place and location information (e.g., room,
office, hospital, cafeteria) were important as they
show the position of humans or other objects in
the scene ? e.g., ?there is a car on the road, ?peo-
ple are walking in the park ?.
Colour information often plays an important
part in identifying separate HLFs ? e.g., ?a man
in black shirt is walking with a woman with green
jacket?, ?she is wearing a white uniform ?. The
large number of occurrences for colours indicates
human?s interest in observing not only objects but
also their colour scheme in a visual scene. Some
hand descriptions reflected annotator?s interest in
scene settings shown in the foreground or in the
background. Indoor/outdoor scene settings were
also interested in by some annotators. These ob-
servations demonstrate that a viewer is interested
in high level details of a video and relationships
between different prominent objects in a visual
scene.
3.3 Spatial Relations
Figure 6 presents a list of the most frequent words
and phrases related to spatial relations found in
hand annotations. Spatial relations between HLFs
are important when explaining the semantics of
visual scenes. Their effective use leads to the
smooth description. Spatial relations can be cate-
gorised into
in (404); with (120); on (329); near (68); around
(63); at (55); on the left (35); in front of (24);
down (24); together (24); along (16); beside (16);
on the right (16); into (14); far (11); between (10);
in the middle (10); outside (8); off (8); over (8);
pass-by (8); across (7); inside (7); middle (7); un-
der (7); away (6); after (7)
Figure 6: List of frequent spatial relations with their
counts found in hand annotations.
static: relations between stationary objects;
dynamic: direction and path of moving objects;
inter-static and dynamic: relations between moving
and not moving objects.
Static relations can establish the scene settings
(e.g., ?chairs around a table? may imply an indoor
scene). Dynamic relations are used for finding ac-
tivities present in the video (e.g., ?a man is run-
ning with a dog?). Inter-static and dynamic rela-
tions are a mixture of stationary and non station-
ary objects; they explain semantics of the com-
plete scene (e.g., ?persons are sitting on the chairs
around the table? indicates a meeting scene).
3.4 Temporal Relations
Video is a class of time series data formed with
highly complex multi dimensional contents. Let
video X be a uniformly sampled frame sequence
of length n, denoted by X = {x1, . . . , xn}, and
each frame xi gives a chronological position of
the sequence (Figure 7). To generate full de-
scription of video contents, annotators use tempo-
ral information to join descriptions of individual
frames. For example,
A man is walking. After sometime he en-
ters the room. Later on he is sitting on the
chair.
Based on the analysis of the corpus, we describe
temporal information in two flavors:
1. temporal information extracted from activi-
ties of a single human;
2. interactions between multiple humans.
Most common relations in video sequences are
?before?, ?after?, ?start ? and ?finish ? for single hu-
mans, and ?overlap?, ?during? and ?meeting? for
multiple humans.
Figure 8 presents a list of the most frequent
words in the corpus related to temporal relations.
It can be observed that annotators put much focus
43
Figure 7: Illustration of a video as a uniformly sam-
pled sequence of length n. A video frame is denoted
by xi, whose spatial context can be represented in the
d dimensional feature space.
single human:
then (25); end (24); before (22); after (16); next
(12); later on (12); start (11); previous (11);
throughout (10); finish (8); afterwards (6); prior
to (4); since (4)
multiple humans:
meet (114); while (37); during (27); at the same
time (19); overlap (12); meanwhile (12); through-
out (7); equals (4)
Figure 8: List of frequent temporal relations with their
counts found in hand annotations.
on keywords related to activities of multiple hu-
mans as compared to single human cases. ?Meet?
keyword has the highest frequency, as annota-
tors usually consider most of the scenes involving
multiple humans as the meeting scene. ?While?
keyword is mostly used for showing separate ac-
tivities of multiple humans such as ?a man is walk-
ing while a woman is sitting?.
3.5 Similarity between Descriptions
A well-established approach to calculating human
inter-annotator agreement is kappa statistics (Eu-
genio and Glass, 2004). However in the current
task it is not possible to compute inter-annotator
agreement using this approach because no cat-
egory was defined for video descriptions. Fur-
ther the description length for one video can vary
among annotators. Alternatively the similarity be-
tween natural language descriptions can be calcu-
lated; an effective and commonly used measure
to find the similarity between a pair of documents
is the overlap similarity coefficient (Manning and
Schu?tze, 1999):
Simoverlap(X,Y ) =
|S(X,n) ? S(Y, n)|
min(|S(X,n)|, |S(Y, n)|)
where S(X,n) and S(Y, n) are the set of distinct
n-grams in documents X and Y respectively. It is
a similarity measure related to the Jaccard index
(Tan et al, 2006). Note that when a set X is a sub-
set of Y or the converse, the overlap coefficient is
equal to one. Values for the overlap coefficient
range between 0 and 1, where ?0? presents the sit-
uation where documents are completely different
and ?1? describes the case where two documents
are exactly the same.
Table 1 shows the average overlap similarity
scores for seven scene categories within 13 hand
annotations. The average was calculated from
scores for individual description, that was com-
pared with the rest of descriptions in the same
category. The outcome demonstrate the fact that
humans have different observations and interests
while watching videos. Calculation were repeated
with two conditions; one with stop words re-
moved and Porter stemmer (Porter, 1993) applied,
but synonyms NOT replaced, and the other with
stop words NOT removed, but Porter stemmer ap-
plied and synonyms replaced. It was found the
latter combination of preprocessing techniques re-
sulted in better scores. Not surprisingly synonym
replacement led to increased performance, indi-
cating that humans do express the same concept
using different terms.
The average overlap similarity score was higher
for ?Traffic? videos than for the rest of categories.
Because vehicles were the major entity in ?Traf-
fic? videos, rather than humans and their actions,
contributing for annotators to create more uniform
descriptions. Scores for some other categories
were lower. It probably means that there are more
aspects to pay attention when watching videos in,
e.g., ?Grouping? category, hence resulting in the
wider range of natural language expressions pro-
duced.
3.6 Sequence of Events Matching
Video is a class of time series data which can
be partitioned into time aligned frames (images).
These frames are tied together sequentially and
temporally. Therefore, it will be useful to know
how a person captures the temporal information
present in a video. As the order is preserved in a
sequence of events, a suitable measure to quantify
sequential and temporal information of a descrip-
tion is the longest common subsequence (LCS).
This approach computes the similarity between
a pair of token (i.e., word) sequences by simply
counting the number of edit operations (insertions
and deletions) required to transform one sequence
into the other. The output is a sequence of com-
mon elements such that no other longer string is
44
Action Close-up Indoor Grouping Meeting News Traffic
unigram (A) 0.3827 0.3913 0.4217 0.3809 0.3968 0.4378 0.4687
(B) 0.4135 0.4269 0.4544 0.4067 0.4271 0.4635 0.5174
bigram (A) 0.1483 0.1572 0.1870 0.1605 0.1649 0.1872 0.1765
(B) 0.2490 0.2616 0.2877 0.2619 0.2651 0.2890 0.2825
trigram (A) 0.0136 0.0153 0.0301 0.0227 0.0219 0.0279 0.0261
(B) 0.1138 0.1163 0.1302 0.1229 0.1214 0.1279 0.1298
Table 1: Average overlapping similarity scores within 13 hand annotations. For each of unigram, bigram and
trigram, scores are calculated for seven categories in two conditions: (A) stop words removed and Porter stemmer
applied, but synonyms NOT replaced; (B) stop words NOT removed, but Porter stemmer applied and synonyms
replaced.
raw synonym keyword
Action 0.3782 0.3934 0.3955
Close-up 0.4181 0.4332 0.4257
Indoor 0.4248 0.4386 0.4338
Grouping 0.3941 0.4104 0.3832
Meeting 0.3939 0.4107 0.4124
News 0.4382 0.4587 0.4531
Traffic 0.4036 0.4222 0.4093
Table 2: Similarity scores based on the longest com-
mon subsequence (LCS) in three conditions: scores
without any preprocessing (raw), scores after synonym
replacement (synonym), and scores by keyword com-
parison (keyword). For keyword comparison, verbs
and nouns were presented as keywords after stemming
and removing stop words.
available. In the experiments, the LCS score be-
tween word sequences is normalised by the length
of the shorter sequence.
Table 2 presents results for identifying se-
quences of events in hand descriptions using the
LCS similarity score. Individual descriptions
were compared with the rest of descriptions in the
same category and the average score was calcu-
lated. Relatively low scores in the table indicate
the great variation in annotators? attention on the
sequence of events, or temporal information, in a
video. Events described by one annotator may not
have been listed by another annotator. The News
videos category resulted in the highest similarity
score, confirming the fact that videos in this cate-
gory are highly structured.
3.7 Video Classification
To demonstrate the application of this corpus with
natural language descriptions, a supervised docu-
ment classification task is outlined. Tf-idf score
can express textual document features (Dumais et
al., 1998). Traditional tf-idf represents the rela-
tion between term t and document d. It provides
a measure of the importance of a term within a
particular document, calculated as
tfidf(t, d) = tf(t, d) ? idf(d) (1)
where the term frequency tf(t, d) is given by
tf(t, d) =
Nt,d
?
k
Nk,d
(2)
In the above equation Nt,d is the number of occur-
rences of term t in document d, and the denomina-
tor is the sum of the number of occurrences for all
terms in document d, that is, the size of the docu-
ment |d|. Further the inverse document frequency
idf(d) is
idf(d) = log
N
W (t)
(3)
where N is the total number of documents in the
corpus and W (t) is the total number of document
containing term t.
A term-document matrix X is presented by
T ? D matrix tfidf(t, d). In the experiment
Naive Bayes probabilistic supervised learning al-
gorithm was applied for classification using Weka
machine learning library (Hall et al, 2009). Ten-
fold cross validation was applied. The perfor-
mance was measured using precision, recall and
F1-measure (Table 3). F1-measure was low for
?Grouping? and ?Action? videos, indicating the
difficulty in classifying these types of natural lan-
guage descriptions. Best classification results
were achieved for ?Traffic? and ?Indoor/Outdoor?
scenes. Absence of humans and their actions
might have contributed obtaining the high clas-
sification scores. Human actions and activities
were present in most videos in various categories,
hence the ?Action? category resulted in the low-
est results. ?Grouping? category also showed
45
precision recall F1-measure
Action 0.701 0.417 0.523
Close-up 0.861 0.703 0.774
Grouping 0.453 0.696 0.549
Indoor 0.846 0.915 0.879
Meeting 0.723 0.732 0.727
News 0.679 0.823 0.744
Traffic 0.866 0.869 0.868
average 0.753 0.739 0.736
Table 3: Results for supervised classification using the
tf-idf features.
Figure 9: The average overlap similarity scores for
titles and for descriptions. ?uni?, ?bi?, and ?tri? indi-
cate the unigram, bigram, and trigram based similarity
scores, respectively. They were calculated without any
preprocessing such as stop word removal or synonym
replacement.
weaker result; it was probably because process-
ing for interaction between multiple people, with
their overlapped actions, had not been fully devel-
oped. Overall classification results are encourag-
ing which demonstrates that this dataset is a good
resource for evaluating natural language descrip-
tion systems of short videos.
3.8 Analysis of Title and Description
A title may be considered a very short form of
summary. We carried out further experiments to
calculate the similarity between a title and a de-
scription manually created for a video. The length
of a title varied between two to five words. Figure
9 shows the average overlapping similarity scores
between titles and descriptions. It can be ob-
served that, in general, scores for titles were lower
than those for descriptions, apart from ?News?
and ?Meeting? videos. It was probably caused by
the short length of titles; by inspection we found
phrases such as ?news video? and ?meeting scene?
for these categories.
Another experiment was performed for classi-
fication of videos based on title information only.
Figure 10 shows comparison of classification per-
Figure 10: Video classification by titles, and by de-
scriptions.
formance with titles and with descriptions. We
were able to make correct classification in many
videos with titles alone, although the performance
was slightly less for titles only than for descrip-
tions.
4 Conclusion and Future Work
This paper presented our experiments using a cor-
pus created for natural language description of
videos. For a small subset of TREC video data in
seven categories, annotators produced titles, de-
scriptions and selected high level features. This
paper aimed to characterise the corpus based on
analysis of hand annotations and a series of exper-
iments for description similarity and video clas-
sification. In the future we plan to develop au-
tomatic machine annotations for video sequences
and compare them against human authored anno-
tations. Further, we aim to annotate this corpus
in multiple languages such as Arabic and Urdu
to generate a multilingual resource for video pro-
cessing community.
Acknowledgements
M U G Khan thanks University of Engineering &
Technology, Lahore, Pakistan and R M A Nawab
thanks COMSATS Institute of Information Tech-
nology, Lahore, Pakistan for funding their work
under the Faculty Development Program.
46
References
S. Dumais, J. Platt, D. Heckerman, and M. Sahami.
1998. Inductive learning algorithms and represen-
tations for text categorization. In Proceedings of
the seventh international conference on Information
and knowledge management, pages 148?155. ACM.
B.D. Eugenio and M. Glass. 2004. The kappa statis-
tic: A second look. Computational linguistics,
30(1):95?101.
R. Fisher, J. Santos-Victor, and J. Crowley. 2005.
Caviar: Context aware vision using image-based ac-
tive recognition.
G. Griffin, A. Holub, and P. Perona. 2007. Caltech-
256 object category dataset.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explo-
rations Newsletter, 11(1):10?18.
C. Jaynes, A. Kale, N. Sanders, and E. Gross-
mann. 2005. The terrascope dataset: A scripted
multi-camera indoor video surveillance dataset with
ground-truth. In Proceedings of the IEEE Workshop
on VS PETS, volume 4. Citeseer.
Christopher Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
M. Marszalek, I. Laptev, and C. Schmid. 2009. Ac-
tions in context.
A. Oliva and A. Torralba. 2009. Mit outdoor scene
dataset.
P. Over, A.F. Smeaton, and P. Kelly. 2007. The
trecvid 2007 bbc rushes summarization evaluation
pilot. In Proceedings of the international work-
shop on TRECVID video summarization, pages 1?
15. ACM.
C. Papageorgiou and T. Poggio. 1999. A trainable
object detection system: Car detection in static im-
ages. Technical Report 1673, October. (CBCL
Memo 180).
M.F. Porter. 1993. An algorithm for suffix stripping.
Program: electronic library and information sys-
tems, 14(3):130?137.
A. Quattoni and A. Torralba. 2009. Recognizing in-
door scenes.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
amazon?s mechanical turk. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 139?147. Association for Computa-
tional Linguistics.
C. Schuldt, I. Laptev, and B. Caputo. 2004. Recog-
nizing human actions: A local svm approach. In
Pattern Recognition, 2004. ICPR 2004. Proceed-
ings of the 17th International Conference on, vol-
ume 3, pages 32?36. IEEE.
A.F. Smeaton, P. Over, and W. Kraaij. 2009. High-
level feature detection from video in trecvid: a
5-year retrospective of achievements. Multimedia
Content Analysis, pages 1?24.
P.N. Tan, M. Steinbach, V. Kumar, et al 2006. Intro-
duction to data mining. Pearson Addison Wesley
Boston.
P. Wittenburg, H. Brugman, A. Russel, A. Klassmann,
and H. Sloetjes. 2006. Elan: a professional frame-
work for multimodality research. In Proceedings of
LREC, volume 2006. Citeseer.
D.P. Young and J.M. Ferryman. 2005. Pets metrics:
On-line performance evaluation service. In Joint
IEEE International Workshop on Visual Surveil-
lance and Performance Evaluation of Tracking and
Surveillance (VS-PETS), pages 317?324.
47
