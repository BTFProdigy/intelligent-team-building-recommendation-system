Proceedings of the SIGDIAL 2014 Conference, pages 79?83,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Alex: Bootstrapping a Spoken Dialogue System for a New Domain by Real
Users
?
Ond
?
rej Du?ek, Ond
?
rej Pl?tek, Luk?? ?ilka, and Filip Jur
?
c?
?
cek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m
?
est? 25, CZ-11800 Prague, Czech Republic
{odusek,oplatek,zilka,jurcicek}@ufal.mff.cuni.cz
Abstract
When deploying a spoken dialogue sys-
tem in a new domain, one faces a situation
where little to no data is available to train
domain-specific statistical models. We de-
scribe our experience with bootstrapping
a dialogue system for public transit and
weather information in real-word deploy-
ment under public use. We proceeded in-
crementally, starting from a minimal sys-
tem put on a toll-free telephone number to
collect speech data. We were able to incor-
porate statistical modules trained on col-
lected data ? in-domain speech recogni-
tion language models and spoken language
understanding ? while simultaneously ex-
tending the domain, making use of auto-
matically generated semantic annotation.
Our approach shows that a successful sys-
tem can be built with minimal effort and
no in-domain data at hand.
1 Introduction
The Alex Public Transit Information System is an
experimental Czech spoken dialogue system pro-
viding information about all kinds of public tran-
sit in the Czech Republic, publicly available at a
toll-free 800 telephone number.
1
It was launched
for public use as soon as a first minimal working
version was developed, using no in-domain speech
data. We chose an incremental approach to sys-
tem development in order to collect call data and
use them to bootstrap statistical modules. Nearly
?
This work was funded by the Ministry of Education,
Youth and Sports of the Czech Republic under the grant
agreement LK11221 and core research funding, SVV project
260 104, and grants GAUK 2058214 and 2076214 of Charles
University in Prague. It used language resources stored and
distributed by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech Republic
(project LM2010013).
1
Call 800-899-998 from the Czech Republic.
a year after launch, we have collected over 1,300
calls from the general public, which enabled us
to train and deploy an in-domain language model
for Automatic Speech Recognition (ASR) and a
statistical Spoken Language Understanding (SLU)
module. The domain supported by the system has
extended from transit information in one city to ca.
5,000 towns and cities in the whole country, plus
weather and time information. This shows that a
even a very basic system is useful in collecting in-
domain data and that the incremental approach is
viable.
Spoken dialogue systems have been a topic of
research for the past several decades, and many
experimental systems were developed and tested
with users (Walker et al., 2001; Ga?i
?
c et al., 2013;
Janarthanam et al., 2013). However, few experi-
mental systems became available to general public
use. Let?s Go (Raux et al., 2005; Raux et al., 2006)
is a notable example in the public transportation
domain. Using interaction with users from the
public to bootstrap data-driven methods and im-
prove the system is also not a common practice.
Both Let?s Go and the GOOG-411 business finder
system (Bacchiani et al., 2008) collected speech
data, but applied data-driven methods only to im-
prove statistical ASR. We use the call data for sta-
tistical SLU as well and plan to further introduce
statistical modules for dialogue management and
natural language generation.
Our spoken dialogue system framework is
freely available on GitHub
2
and designed for easy
adaptation to new domains and languages. An En-
glish version of our system is in preparation.
We first present the overall structure of the Alex
SDS framework and then describe the minimal
system that has been put to public use, as well as
our incremental extensions. Finally, we provide
an evaluation of our system based on the recorded
calls.
2
http://github.com/UFAL-DSG/alex
79
2 Overall Alex SDS System Structure
The basic architecture of Alex is modular and con-
sists of the traditional SDS components: automatic
speech recognizer (ASR), spoken language under-
standing (SLU), dialogue manager (DM), natural
language generator (NLG), and a text-to-speech
(TTS) module.
We designed the system to allow for easy re-
placement of the individual components: There is
a defined interface for each of them. As the in-
terfaces are domain-independent, changing the do-
main is facilitated as well by this approach.
3 Baseline Transit Information System
We decided to create a minimal working system
that would not require any in-domain data and
open it to general public to collect call data as soon
as possible. We believe that this is a viable al-
ternative to Wizard-of-Oz experiments (Rieser and
Lemon, 2008), allowing for incremental develop-
ment and producing data that correspond to real
usage scenarios (see Section 4).
3.1 Baseline Implementation of the
Components
Having no in-domain data available, we resorted
to very basic implementations using hand-written
rules or external services:
? ASR used a neural network based voice activity
detector trained on small out-of-domain data.
Recordings classified as speech were fed to the
the web-based Google ASR service.
? SLU was handcrafted for our domain using sim-
ple keyword-spotting rules.
? In DM, the dialogue tracker held only one value
per dialogue slot, and the dialogue policy was
handcrafted for the basic tasks in our domain.
? NLG is a simple template-based module.
? We use a web-based Czech TTS service pro-
vided to us by SpeechTech.
3
3.2 Baseline Domain
At baseline, our domain only consisted of a very
basic public transport information for the city of
Prague. Our ontology contained ca. 2,500 public
transit stops. The system was able to present the
next connection between two stops requested by
the user, repeat the information, or return several
3
http://www.speechtech.cz/
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 120
25
30
35
40
45
50
Google ASR
Kaldi ASR Training set portion
Wo
rde
rror
rate
(%)
Figure 1: ASR word error rate depending on the
size of in-domain language model training data
The full training set amounts to 9,495 utterances (30,126 to-
kens). The test set contains 1,187 utterances (4,392 tokens).
following connections. Connection search was
based on Google Directions API.
4
4 Collecting Data and Extending the
System in Real Usage
We launched our system at a public toll-free 800
number and advertised the service at our univer-
sity, among friends, and via Facebook. We also
cooperate with the Czech Blind United associa-
tion,
5
promoting our system among its members
and receiving comments about its use. We adver-
tised our extensions and improvements using the
same channels.
We record and collect all calls to the system,
including our own testing calls, to obtain training
data and build statistical models into our system.
4.1 Speech Recognition: Building In-Domain
Models
The Google on-line ASR service, while reach-
ing state-of-the-art performance in some tasks
(Morbini et al., 2013), showed very high word er-
ror rate in our specific domain (see Figure 1). We
replaced it with the Kaldi ASR engine (Povey et
al., 2011) trained on general-domain Czech acous-
tic data (Korvas et al., 2014) with an in-domain
class-based language model built using collected
call data and lists of all available cities and stops.
We describe our modifications to Kaldi for on-
line decoding in Pl?tek and Jur
?
c?
?
cek (2014). A
performance comparison of Google ASR with
4
https://developers.google.com/maps/
documentation/directions/
5
http://www.sons.cz
80
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 164
66
68
70
72
74
76
78
80
SLU trained on Google ASR
SLU trained on Kaldi ASR
Training set portion
Dia
logu
eac
tite
ms
F-m
eas
ure
(%)
Figure 2: SLU performance (F-measure on dia-
logue act items) depending on training data size
The same data sets as in Figure 1 are used, with semantic
annotations from handcrafted SLU running on manual tran-
scriptions.
Kaldi trained on our data is shown in Figure 1.
One can see that the in-domain language model
brings a substantial improvement, even with very
small data sizes.
4.2 Spoken Language Understanding
To increase system robustness, we built a statisti-
cal SLU based on a set of logistic regression clas-
sifiers and word n-gram features (Jur
?
c?
?
cek et al.,
2014). We train it on the output of our handcrafted
SLU applied to manual transcriptions. We chose
this approach over obtaining manual semantic an-
notation due to two main reasons:
1. Obtaining semantic annotation for Czech data is
relatively slow and complicated; using crowd-
sourcing is not a possibility due to lack of
speakers of Czech on the platforms.
2. As we intended to gradually extend our domain,
semantic annotation changed over time as well.
This approach still allows the statistical SLU to
improve on a handcrafted one by compensating
for errors made by the ASR. Figure 2 shows that
the performance of the statistical SLU module in-
creases with more training data and with the in-
domain ASR models.
4.3 Dialogue Manager
We have replaced the initial simplistic dialogue
state tracker (see Section 3.1) by the probabilis-
tic discriminative tracker of ?ilka et al. (2013),
which achieves near state-of-the-art performance
while remaining completely parameter-free. This
property allowed us to employ the tracker without
any training data; our gradual domain extensions
also required no further adjustments.
The dialogue policy is handcrafted, though it
takes advantage of uncertainty estimated by the
belief tracker. Its main logic is similar to that of
Jur
?
c?
?
cek et al. (2012). First, it implements a set of
domain-independent actions, such as:
? dialogue opening, closing, and restart,
? implicit confirmation of changed slots with high
probability of the most probable value,
? explicit confirmation for slots with a lower
probability of the most probable value,
? a choice among two similarly probable values.
Second, domain-specific actions are imple-
mented for the domain(s) described in Section 4.4.
4.4 Extending the Domain
We have expanded our public transit information
domain with the following tasks:
? The user may specify departure or arrival time
in absolute or relative terms (?in ten minutes?,
?tomorrow morning?, ?at 6 pm.?, ?at 8:35? etc.).
? The user may request more details about the
connection: number of transfers, journey dura-
tion, departure and arrival time.
? The user may travel not only among public
transport stops within one city, but also among
multiple cities or towns.
The expansion to multiple cities has lead to an
ontology improvement: The system is able to find
the corresponding city in the database based on a
stop name, and can use a default stop for a given
city. We initially supported three Czech major
cities covered by the Google Directions service,
then extended the coverage to the whole country
(ca. 44,000 stops in 5,000 cities and towns) using
Czech national public transport database provided
by CHAPS.
6
We now also include weather information for all
Czech cities in the system. The user may ask for
weather at the given time or on the whole day. We
use OpenWeatherMap as our data source.
7
Furthermore, the user may ask about the current
time at any point in the dialogue.
5 System Evaluation from Recorded
Calls
We have used the recorded call data for an eval-
uation of our system. Figure 3 presents the num-
6
http://www.idos.cz
7
http://openweathermap.org/
81
Jun '13 Jul Aug Sep Oct Nov Dec Jan '14 Feb Mar Apr May0
10
20
30
40
50
60
70
80
90
100
110 Total calls (incl.testing)
Public user callsA
B
C
D
E F
G
H
I
Figure 3: Number of calls per week
The dashed line shows all recorded calls, including those
made by the authors. The full line shows calls from the public
only.
Spikes: A ? initial testing, B ? first advertising, C ? system
partially offline due to a bug, D ? testing statistical SLU mod-
ule, E ? larger advertising with Czech Blind United, F ? test-
ing domain enhancements, G ? no advertising and limited
system performance, H ? deploying Kaldi ASR and nation-
wide coverage, I ? no further advertising.
Jun '13 Jul Aug Sep Oct Nov Dec Jan '14 Feb Mar Apr May0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
% Informed or apologized% Rather positive answer
Figure 4: System success rates by month
Percentage of calls where the system provided information
(or apology for not having one) and percentage of rather pos-
itive responses to the final question, both shown with standard
error bars.
ber of calls to our system per week and reflects
the testing and advertising phases, as well as some
of our extensions and improvements described in
Section 4. A steeper usage increase is visible in
recent weeks after the introduction of Kaldi ASR
engine and nationwide coverage (see Sections 4.1
and 4.4). The number of calls and unique users
(caller phone numbers) grows steadily; so far,
more than 300 users from the public have made
over 1,300 calls to the system (cf. Figure 5 and
Table 1 in the appendix).
8
Figure 4 (and Table 1 in the appendix) give a de-
tailed view of the success of our system. Informa-
8
We only count calls with at least one valid user utterance,
disregarding calls where users hang up immediately.
tion is provided in the vast majority of calls. Upon
manual inspection of call transcripts, we discov-
ered that about half of the cases where no infor-
mation is provided can be attributed to the system
failing to react properly; the rest is off-topic calls
or users hanging up too early.
We have also introduced a ?final question? as
an additional success metric. After the user says
good-bye, the system asks them if they received
the information they were looking for. By looking
at the transcriptions of responses to this question,
we recognize a majority of them as rather positive
(?Yes?, ?Nearly? etc.); the proportion of positive
reactions seems to remain stable. However, the fi-
nal question is not an accurate measure as most
users seem to hang up directly after receiving in-
formation from the system.
6 Conclusions and Further Work
We use an iterative approach to build a complex
dialogue system within the public transit informa-
tion domain. The system is publicly available on a
toll-free phone number. Our extensible dialogue
system framework as well as the system imple-
mentation for our domain can be downloaded from
GitHub under the Apache 2.0 license.
We have shown that even very limited work-
ing version can be used to collect calls from
the public, gathering training data for statistical
system components. Our experiments with the
Kaldi speech recognizer show that already a small
amount of in-domain data for the language model
brings a substantial improvement. Generating au-
tomatic semantic annotation from recording tran-
scripts allows us to maintain a statistical spoken
language understanding unit with changing do-
main and growing data.
The analysis of our call logs shows that our sys-
tem is able to provide information in the vast ma-
jority of cases. Success rating provided by the
users themselves is mostly positive, yet the con-
clusiveness of this metric is limited as users tend
to hang up directly after receiving information.
In future, we plan to add an English version
of the system and further expand the domain, al-
lowing more specific connection options. As we
gather more training data, we plan to introduce sta-
tistical modules into the remaining system compo-
nents.
82
A System Evaluation Data
In the following, we include additional data from
call logs evaluation presented in Section 5.
Jun '13 Jul Aug Sep Oct Nov Dec Jan '14 Feb Mar Apr May0
150
300
450
600
750
900
1050
1200
1350
Total calls
Unique callers
Figure 5: Cumulative number of calls and unique
callers from the public by weeks
The growth rates of the number of unique users and the total
number of calls both correspond to the testing and advertising
periods shown in Figure 3.
Total calls 1,359
Unique users (caller phone numbers) 304
System informed (or apologized) 1,124
System informed about directions 990
System informed about weather 88
System informed about current time 41
Apologized for not having information 223
System asked the final question 229
Final question answered by the user 199
Rather positive user?s answer 146
Rather negative user?s answer 23
Table 1: Detailed call statistics
Total absolute numbers of calls from general public users
over the period of nearly one year are shown.
References
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schus-
ter, and B. Strope. 2008. Deploying GOOG-411:
early lessons in data, measurement, and testing. In
Proceedings of ICASSP, page 5260?5263. IEEE.
M. Ga?i?c, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. 2013. On-line policy optimisation of
bayesian spoken dialogue systems via human inter-
action. In Proceedings of ICASSP, page 8367?8371.
IEEE.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas,
A. Dickinson, X. Liu, W. Mackaness, and B. Web-
ber. 2013. Evaluating a city exploration dialogue
system combining question-answering and pedes-
trian navigation. In Proceedings of ACL.
F. Jur?c??cek, B. Thomson, and S. Young. 2012. Rein-
forcement learning for parameter estimation in sta-
tistical spoken dialogue systems. Computer Speech
& Language, 26(3):168?192.
F. Jur?c??cek, O. Du?ek, and O. Pl?tek. 2014. A factored
discriminative spoken language understanding for
spoken dialogue systems. In Proceedings of TSD.
To appear.
M. Korvas, O. Pl?tek, O. Du?ek, L. ?ilka, and F. Ju-
r?c??cek. 2014. Free English and Czech telephone
speech corpus shared under the CC-BY-SA 3.0 li-
cense. In Proceedings of LREC, Reykjav?k.
F. Morbini, K. Audhkhasi, K. Sagae, R. Artstein,
D. Can, P. Georgiou, S. Narayanan, A. Leuski, and
D. Traum. 2013. Which ASR should i choose for
my dialogue system? In Proceedings of SIGDIAL,
page 394?403.
O. Pl?tek and F. Jur?c??cek. 2014. Free on-line speech
recogniser based on kaldi ASR toolkit producing
word posterior lattices. In Proceedings of SIGDIAL.
D. Povey, A. Ghoshal, G. Boulianne, L. Burget,
O. Glembek, N. Goel, M. Hannemann, P. Motlicek,
Y. Qian, P. Schwarz, et al. 2011. The Kaldi speech
recognition toolkit. In Proceedings of ASRU, page
1?4, Hawaii.
A. Raux, B. Langner, D. Bohus, Alan W. Black, and
M. Eskenazi. 2005. Let?s go public! taking a spo-
ken dialog system to the real world. In Proceedings
of Interspeech.
A. Raux, D. Bohus, B. Langner, Alan W. Black, and
M. Eskenazi. 2006. Doing research on a deployed
spoken dialogue system: one year of Let?s Go! ex-
perience. In Proceedings of Interspeech.
V. Rieser and O. Lemon. 2008. Learning effective
multimodal dialogue strategies from Wizard-of-Oz
data: Bootstrapping and evaluation. In Proceedings
of ACL, page 638?646.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001.
Quantitative and qualitative evaluation of DARPA
communicator spoken dialogue systems. In Pro-
ceedings of ACL, page 515?522.
L. ?ilka, D. Marek, M. Korvas, and F. Jur
?
c?
?
cek. 2013.
Comparison of bayesian discriminative and genera-
tive models for dialogue state tracking. In Proceed-
ings of SIGDIAL, page 452?456, Metz, France.
83
Proceedings of the SIGDIAL 2014 Conference, pages 108?112,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Free on-line speech recogniser based on Kaldi ASR toolkit producing
word posterior lattices
Ond
?
rej Pl
?
atek and Filip Jur
?
c??
?
cek
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{oplatek, jurcicek}@ufal.mff.cuni.cz
Abstract
This paper presents an extension of
the Kaldi automatic speech recognition
toolkit to support on-line recognition.
The resulting recogniser supports acous-
tic models trained using state-of-the-
art acoustic modelling techniques. As
the recogniser produces word posterior lat-
tices, it is particularly useful in statisti-
cal dialogue systems, which try to ex-
ploit uncertainty in the recogniser?s out-
put. Our experiments show that the on-
line recogniser performs significantly bet-
ter in terms of latency when compared to
a cloud-based recogniser.
1 Introduction
There are many choices of speech recognisers, but
we find no alternative with both a permissive li-
cense and on-line recognition suitable for a spo-
ken dialogue system. The Google speech recog-
nition service
1
provides state-of-the-art quality for
many tasks (Morbini et al., 2013) and may be used
for free; however, the licensing conditions are not
clear, adaptation of acoustic and language models
to a task at hand is not possible and the service is
not officially supported.
Another option is Nuance cloud based recogni-
tion
2
; however, again adjustments to the system
are not possible. Moreover, it is a paid service.
When considering local ASR systems, we
found no viable alternatives either. The HTK
toolkit does not provide on-line large vocabulary
decoders suitable for real-time decoding. Open-
Julius can be used with custom-built acoustic and
1
The API is available at https://www.google.
com/speech-api/v1/recognize, and its use described
in a blog post at http://mikepultz.com/2013/07/
google-speech-api-full-duplex-php-version/.
2
http://www.nuancemobiledeveloper.com/
language models and for on-line decoding (Aki-
nobu, 2014). However, OpenJulius suffers from
software instability when producing lattices and
confusion networks; therefore, it is not suitable
for practical use. The RWTH decoder is not a free
software and a license must be purchased for com-
mercial applications (Rybach et al., 2011).
As a result, we implemented a lightweight
modification of the LatticeFasterDecoder from
the Kaldi toolkit and created an on-line recogniser
with an interface that is suitable for statistical dia-
logue systems. The Kaldi toolkit as well as the on-
line recogniser is distributed under the Apache
2.0 license
3
. Our on-line recogniser may use
acoustic models trained using the state-of-the-art
techniques, such as Linear Discriminant Analysis
(LDA), Maximum Likelihood Linear Transform
(MLLT), Boosted Maximum Mutual Information
(BMMI), Minimum Phone Error (MPE). It pro-
duces word posterior lattices which can be easily
converted into high quality n-best lists. The recog-
niser?s speed and latency can be effectively con-
trolled off-line by optimising a language model
and during decoding by beam thresholds.
In the next section, the Kaldi recognition
toolkit is briefly described. Section 3 describes
the implementation of the OnlineLatgenRecog-
niser. Section 4 evaluates the accuracy and speed
of the recogniser. Finally, Section 5 concludes this
work.
2 The Kaldi toolkit
The Kaldi toolkit
4
is a speech recognition toolkit
distributed under a free license (Povey et al.,
2011). The toolkit is based on Finite State Trans-
ducers, implements state-of-the-art acoustic mod-
elling techniques, is computationally efficient, and
is already widely adapted among research groups.
3
http://www.apache.org/licenses/
LICENSE-2.0
4
http://sourceforge.net/projects/kaldi
108
Its only major drawback was the lack of on-line
recognition support. Therefore, it could not be
used directly in applications such as spoken dia-
logue systems. Kaldi includes an on-line recogni-
tion application; however, hard-wired timeout ex-
ceptions, audio source fixed to a sound card, and a
specialised 1-best decoder limit its use to demon-
stration of Kaldi recognition capabilities only.
3 OnlineLatgenRecogniser
The standard Kaldi interface between the compo-
nents of the toolkit is based on a batch process-
ing paradigm, where the components assume that
the whole audio signal is available when recog-
nition starts. However, when performing on-line
recognition, one would like to take advantage of
the fact that the signal appears in small chunks and
can be processed incrementally. When properly
implemented, this significantly reduces recogniser
output latency.
3.1 C++ implementation
To achieve this, we implemented Kaldi?s Decod-
ableInterface supporting incremental speech pre-
processing, which includes speech parameterisa-
tion, feature transformations, and likelihood esti-
mation. In addition, we subclassed LatticeFaster-
Decoder and split the original batch processing in-
terface.
The newly implemented OnlineLatgenRecog-
niser makes use of our incremental speech pre-
processing and modified LatticeFasterDecoder. It
implements the following interface:
? AudioIn ? queueing new audio for pre-
processing,
? Decode ? decoding a fixed number of audio
frames,
? PruneFinal ? preparing internal data struc-
tures for lattice extraction,
? GetLattice ? extracting a word posterior lat-
tice and returning log likelihood of processed
audio,
? Reset ? preparing the recogniser for a new ut-
terance,
The C++ example in Listing 1 shows a typi-
cal use of the OnlineLatgenRecogniser interface.
When audio data becomes available, it is queued
into the recogniser?s buffer (line 11) and imme-
diately decoded (lines 12-14). If the audio data
is supplied in small enough chunks, the decod-
ing of queued data is finished before new data ar-
rives. When the recognition is finished, the recog-
niser prepares for lattice extraction (line 16). Line
20 shows how to obtain word posterior lattice as
an OpenFST object. The getAudio() function rep-
resents a separate process supplying speech data.
Please note that the recogniser?s latency is mainly
determined by the time spent in the GetLattice
function.
Please note that we do not present here the func-
tions controlling the input stream of audio chunks
passed to the decoder and processing the output
because these differ according to use case. An
example of a nontrivial use case is in a dialogue
system through a thin Python wrapper (see Sec-
tion 3.2).
1 OnlineLatgenRecogniser rec;
2 rec.Setup(...);
3
4 size_t decoded_now = 0;
5 size_t max_decoded = 10;
6 char
*
audio_array = NULL;
7
8 while (recognitionOn())
9 {
10 size_t audio_len = getAudio(audio_array);
11 rec.AudioIn(audio_array, audio_len);
12 do {
13 decoded_now = rec.Decode(max_decoded);
14 } while(decoded_now > 0);
15 }
16 rec.PruneFinal();
17
18 double tot_lik;
19 fst::VectorFst<fst::LogArc> word_post_lat;
20 rec.GetLattice(&word_post_lat, &tot_lik);
21
22 rec.Reset();
Listing 1: Example of the decoder API
The source code of the OnlineLatgenRecog-
niser is available in Kaldi repository
5
.
3.2 Python extension
In addition, we developed a Python extension ex-
porting the OnlineLatgenRecogniser C++ inter-
face. This can be used as an example of bringing
Kaldi?s on-line speech recognition functionality to
higher-level programming languages. This Python
extension is used in the Alex Dialogue Systems
Framework (ADSF, 2014), an open-source lan-
guage and domain independent framework for
developing spoken dialogue systems. The On-
lineLatgenRecogniser is deployed in an appli-
cation which provides information about public
5
https://sourceforge.net/p/kaldi/code/
HEAD/tree/sandbox/oplatek2/src/dec-wrap/
109
transport and weather in the Czech republic and is
available on a public toll-free telephone number.
4 Evaluation
4.1 Acoustic and language model training
The OnlineLatgenRecogniser is evaluated on
a corpus of audio data from the Public Transport
Information (PTI) domain. In PTI, users can inter-
act in Czech with a telephone-based dialogue sys-
tem to find public transport connections (UFAL-
DSG, 2014). The PTI corpus consist of approx-
imately 12k user utterances with a length vary-
ing between 0.4 s and 18 s with median around
3 s. The data were divided into training, develop-
ment, and test data where the corresponding data
sizes were 9496, 1188, 1188 respectively. For
evaluation, a domain specific the class-based lan-
guage model with a vocabulary size of approxi-
mately 52k and 559k n-grams was estimated from
the training data. Named entities e.g., cities or bus
stops, in class-based language model are expanded
before building a decoding graph.
Since the PTI acoustic data amounts to less then
5 hours, the acoustic training data was extended
by an additional 15 hours of telephone out-of-
domain data from the VYSTADIAL 2013 - Czech
corpus (Korvas et al., 2014). The acoustic mod-
els were obtained by BMMI discriminative train-
ing with LDA and MLLT feature transformations.
The scripts used to train the acoustic models are
publicly available in ASDF (2014) as well as in
Kaldi
6
and a detailed description of the training
procedure is given in Korvas et al. (2014).
4.2 Experiments
We focus on evaluating the speed of the On-
lineLatgenRecogniser and its relationship with the
accuracy of the decoder, namely:
? Real Time Factor (RTF) of decoding ? the ra-
tio of the recognition time to the duration of
the audio input,
? Latency ? the delay between utterance end
and the availability of the recognition results,
? Word Error Rate (WER).
Accuracy and speed of the OnlineLatgenRecog-
niser are controlled by the max-active-states,
6
http://sourceforge.net/p/kaldi/code/
HEAD/tree/trunk/egs/vystadial_en/
beam, and lattice-beam parameters (Povey et al.,
2011). Max-active-states limits the maximum
number of active tokens during decoding. Beam is
used during graph search to prune ASR hypothe-
ses at the state level. Lattice-beam is used when
producing word level lattices after the decoding is
finished. It is crucial to tune these parameters op-
timally to obtain good results.
In general, one aims for a setting RTF smaller
than 1.0. However, in practice, it is useful if
the RTF is even smaller because other processes
running on the machine can influence the amount
of available computational resources. Therefore,
we target the RTF of 0.6 in our setup.
We used grid search on the development set to
identify optimal parameters. Figure 1 (a) shows
the impact of the beam on the WER and RTF
measures. In this case, we set max-active-states
to 2000 in order to limit the worst case RTF to
0.6. Observing Figure 1 (a), we set beam to 13
as this setting balances the WER, 95th RTF per-
centile, and the average RTF. Figure 1 (b) shows
the impact of the lattice-beam on WER and la-
tency when beam is fixed to 13. We set lattice-
beam to 5 based on Figure 1 (b) to obtain the 95th
latency percentile of 200 ms, which is consid-
ered natural in a dialogue (Skantze and Schlangen,
2009). Lattice-beam does not affect WER, but
larger lattice-beam improves the oracle WER of
generated lattices (Povey et al., 2012).
Figure 2 shows the percentile graph of the RTF
and latency measures over the development set.
For example, the 95th percentile is the value of
a measure such that 95% of the data has the mea-
sure below that value. One can see from Fig-
ure 2 that 95% of development utterances is de-
coded with RTF under 0.6 and latency under 200
ms. The extreme values are typically caused by
decoding long noisy utterances where uncertainty
in decoding slows down the recogniser. Using this
setting, OnlineLatgenRecogniser decodes the ut-
terances with a WER of about 21%.
Please note that OnlineLatgenRecogniser only
extends the batch Kaldi decoder for incremental
speech processing interface. It uses the same code
as the batch Kaldi decoder to compute speech
parametrisation, frame likelihoods, and state-level
lattices. Therefore, the accuracy of OnlineLatgen-
Recogniser is equal to that of the batch Kaldi de-
coder given the same parameters.
110
8 9 10 11 12 13 14 15 16
beam
0.0
0.2
0.4
0.6
0.8
1.0
R
T
F
19
20
21
22
23
24
25
W
E
R
a
95th RTF percentile
Average RTF
Desired 0.6 RTF
WER
1 2 3 4 5 6 7 8 9 10
lattice-beam
0
200
400
600
800
1000
L
a
t
e
n
c
y
 
[
m
s
]
19
20
21
22
23
24
25
W
E
R
b
95th latency percentile
Desired latency 200 ms
WER
Figure 1: The left graph (a) shows that WER decreases with increasing beam and the average RTF
linearly grows with the beam. Setting the maximum number of active states to 2000 stops the growth of
the 95th RTF percentile at 0.6, indicating that even in the worst case, we can guarantee an RTF around
0.6. The right graph (b) shows how latency grows in response to increasing lattice-beam.
0 20 40 60 80 100
percentile
0.0
0.5
1.0
1.5
2.0
R
T
F
a
RTF
Desired 0.6 RTF
Critical 1.0 RTF
95th percentile
0 20 40 60 80 100
percentile
0
100
200
300
400
500
600
700
800
L
a
t
e
n
c
y
 
[
m
s
]
b
Latency
Desired latency 200 ms
95th percentile
Figure 2: The percentile graphs show RTF and Latency scores for development data for max-active-
sates=2000, beam=13, lattice-beam=5. Note that 95 % of utterances were decoded with the latency
lower that 200ms.
In addition, we have also experimented with
Google ASR service on the same domain.
The Google ASR service decodes 95% of test ut-
terances with latency under 1900 ms and WER is
about 48%. The high latency is presumably caused
by the batch processing of audio data and net-
work latency, and the high WER is likely caused
by a mismatch between Google?s acoustic and lan-
guage models and the test data.
5 Conclusion
This work presented the OnlineLatgenRecogniser,
an extension of the Kaldi automatic speech recog-
nition toolkit. The OnlineLatgenRecogniser is dis-
tributed under the Apache 2.0 license, and there-
fore it is freely available for both research and
commercial applications. The recogniser and its
Python extension is stable and intensively used
in a publicly available spoken dialogue system
(UFAL-DSG, 2014). Thanks to the use of a stan-
dard Kaldi lattice decoder, the recogniser produces
high quality word posterior lattices. The training
scripts for the acoustic model and the OnlineLat-
genRecogniser code are currently being integrated
in the Kaldi toolkit. Future planned improvements
include implementing more sophisticated speech
parameterisation interface and feature transforma-
tions.
Acknowledgments
We would also like to thank Daniel Povey and Ond?rej Dus?ek
for their useful comments and discussions. We also thank the
anonymous reviewers for their helpful comments and sugges-
tions.
This research was funded by the Ministry of Education,
Youth and Sports of the Czech Republic under the grant
agreement LK11221, by the core research funding of Charles
University in Prague. The language resources presented in
this work are stored and distributed by the LINDAT/CLARIN
project of the Ministry of Education, Youth and Sports of the
Czech Republic (project LM2010013).
111
References
ADSF. 2014. The Alex Dialogue Systems Framework.
https://github.com/UFAL-DSG/alex.
Lee Akinobu. 2014. Open-Source Large Vocabulary CSR
Engine Julius. http://julius.sourceforge.
jp/en_index.php.
Mat?ej Korvas, Ond?rej Pl?atek, Ond?rej Du?sek, Luk?a?s
?
Zilka, and
Filip Jur?c???cek. 2014. Free English and Czech telephone
speech corpus shared under the CC-BY-SA 3.0 license.
In Proceedings of the Eigth International Conference on
Language Resources and Evaluation (LREC 2014).
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Ar-
stein, Doan Can, Panayiotis G. Georgiou, Shrikanth S.
Narayanan, Anton Leuski, and David Traum. 2013.
Which ASR should I choose for my dialogue system? In
Proc. SIGDIAL, August.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-
get, Ondrej Glembek, Nagendra Goel, Mirko Hannemann,
Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. 2011.
The kaldi speech recognition toolkit. In Proc. ASRU,
pages 1?4.
Daniel Povey, Mirko Hannemann, Gilles Boulianne, Lukas
Burget, Arnab Ghoshal, Milos Janda, Martin Karafi?at,
Stefan Kombrink, Petr Motlicek, Yanmin Qian, et al.
2012. Generating exact lattices in the WFST framework.
In Acoustics, Speech and Signal Processing (ICASSP),
2012 IEEE International Conference on, pages 4213?
4216. IEEE.
David Rybach, Stefan Hahn, Patrick Lehnen, David Nolden,
Martin Sundermeyer, Zoltan T?uske, Siemon Wiesler,
Ralf Schl?uter, and Hermann Ney. 2011. RASR-The
RWTH Aachen University Open Source Speech Recogni-
tion Toolkit. In Proc. IEEE Automatic Speech Recognition
and Understanding Workshop.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings of
the 12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 745?753.
Association for Computational Linguistics.
UFAL-DSG. 2014. The Alex Dialogue Systems Framework
- Public Transport Information. https://github.
com/UFAL-DSG/alex.
112
