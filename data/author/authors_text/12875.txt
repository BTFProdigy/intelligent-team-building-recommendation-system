Coling 2010: Poster Volume, pages 1391?1398,
Beijing, August 2010
Unsupervised Part of Speech Tagging Using Unambiguous Substitutes
from a Statistical Language Model
Mehmet Ali Yatbaz
Dept. of Computer Engineering
Koc? University
?myatbaz@ku.edu.tr dyuret@ku.edu.tr
Deniz Yuret
Dept. of Computer Engineering
Koc? University
?
Abstract
We show that unsupervised part of speech
tagging performance can be significantly
improved using likely substitutes for tar-
get words given by a statistical language
model. We choose unambiguous substi-
tutes for each occurrence of an ambiguous
target word based on its context. The part
of speech tags for the unambiguous sub-
stitutes are then used to filter the entry for
the target word in the word?tag dictionary.
A standard HMM model trained using the
filtered dictionary achieves 92.25% accu-
racy on a standard 24,000 word corpus.
1 Introduction
We define the unsupervised part-of-speech (POS)
tagging problem as predicting the correct part-of-
speech tag of a word in a given context using
an unlabeled corpus and a dictionary with possi-
ble word?tag pairs0 The performance of an un-
supervised POS tagging system depends highly
on the quality of the word?tag dictionary (Banko
and Moore, 2004). We propose a dictionary fil-
tering procedure based on likely substitutes sug-
gested by a statistical language model. The pro-
cedure reduces the word?tag dictionary size and
leads to significant improvement in the accuracy
of the POS models.
Probabilistic models such as the hidden Markov
model (HMM) trained by expectation maximiza-
tion (EM), maximum a posteriori (MAP) esti-
mation, and Bayesian methods have been used
0In the POS literature the term ?unsupervised? is typi-
cally used to describe systems that do not directly use the
tagged data. However, many of the unsupervised systems,
including ours, uses the tag?word dictionary.
to solve the unsupervised POS tagging problem
(Merialdo, 1994; Goldwater and Griffiths, 2007).
All of these approaches first learn the parameters
connecting the hidden structure to the observed
sequence of variables and then identify the most
probable values of the hidden structure for a given
observed sequence. They differ in the way they
estimate the model parameters. HMM-EM esti-
mates model parameters by using the maximum
likelihood estimation (MLE), MAP defines a prior
distribution over parameters and finds the param-
eter values that maximize the posterior distribu-
tion given data, and Bayesian methods integrate
over the posterior of the parameters to incorporate
all possible parameter settings into the estimation
process. Some baseline results and performance
reports from the literature are presented in Table 1.
(Johnson, 2007) criticizes the standard EM
based HMM approaches because of their poor per-
formance on the unsupervised POS tagging and
their tendency to assign equal number of words
to each hidden state. (Mitzenmacher, 2004) fur-
ther claims that words have skewed POS tag dis-
tributions, and a Bayesian method with sparse pri-
ors over the POS tags may perform better than
HMM estimated with EM. (Goldwater and Grif-
fiths, 2007) uses a fully Bayesian HMM model
that averages over all possible parameter values.
Their model achieves 86.8% tagging accuracy
with sparse POS priors and outperforms 74.50%
accuracy of the standard second order HMM-EM
(3-gram tag model) on a 24K word subset of
the Penn Treebank corpus. (Smith and Eisner,
2005) take a different approach and use the con-
ditional random fields estimated using contrastive
estimation which outperforms the HMM-EM and
1391
Accuracy System
64.2 Random baseline
74.4 Second order HMM
82.0 First order HMM
86.8 Fully Bayesian approach with sparse priors (Goldwater and Griffiths, 2007)
88.6 CRF/CE (Smith and Eisner, 2005)
91.4 EM-HMM with language specific information, good initialization and manual adjustments to standard
dictionary (Goldberg et al, 2008)
91.8 Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009).
94.0 Most frequent tag baseline
Table 1: Tagging accuracy on a 24K-word corpus. All the systems ? except (Goldwater and Griffiths,
2007) ? use the same 45 tag dictionary that is constructed from the Penn Treebank.
Bayesian methods by achieving 88.6% accuracy
on the same 24K corpus.
Despite the fact that HMM-EM has a poor repu-
tation in POS literature (Goldberg et al, 2008) has
shown that with good initialization together with
some language specific features and language de-
pendent constraints HMM-EM achieves 91.4%
accuracy. Aside from the language specific infor-
mation and the good initialization, they also man-
ually reduce the noise in the word?tag dictionary.
(Ravi and Knight, 2009) focus on the POS tag
collection to find the smallest POS model that ex-
plain the data. They apply integer programming
to construct a minimal bi-gram POS tag set and
use this set to constrain the training phase of the
EM algorithm. The model trained by EM is used
to reduce the dictionary and these steps are iter-
atively repeated until no further improvement is
observed. Their model achieves 91.6% accuracy
on the 24K word corpus (with 100 random starts
this goes up to 91.8%). The main advantage of
this model is the restriction of the tag set so that
rare POS tags or the noise in the corpus do not get
incorporated into the estimation process.
Language models for disambiguation: Recent
work has shown that statistical language models
trained on large amounts of unlabeled text can
be used to improve the performance on various
disambiguation problems. The language model
is used to generate likely substitutes for the tar-
get word in the given context and these benefit
the disambiguation process to the extent that the
likely substitutes are unambiguous or have dif-
ferent ambiguities compared to the target word.
Using statistical language models based on large
corpora for unsupervised word sense disambigua-
tion and lexical substitution has been explored in
(Yuret, 2007; Hawker, 2007; Yuret and Yatbaz,
2010). Unsupervised morphological disambigua-
tion in agglutinative languages using likely sub-
stitutes has been shown to improve on standard
methods in (Yatbaz and Yuret, 2009).
In this paper we use the statistical language
model to reduce the possible number of tags per
word to help the disambiguation process. Specif-
ically we assume that the same hidden tag se-
quence that has generated a particular test sen-
tence can also generate artificial sentences where
one of the words has been replaced with a likely
substitute. POS tags of the likely substitutes can
then be used to reduce the tag set of the target
word. Thus, the substitutes are implicitly incorpo-
rated into the disambiguation process for reducing
the noise and the rare tags in the dictionary.
Currency gyrations can whipsaw(VB/NN) the funds .
Currency gyrations can withdraw(VB) the funds .
Currency gyrations can restore(VB) the funds .
Currency gyrations can modify(VB) the funds .
Currency gyrations can justify(VB) the funds .
Currency gyrations can regulate(VB) the funds .
Table 2: Sample artificial sentences generated for
a test sentence from the Penn Treebank.
Table 2 presents an example where the likely
unambiguous replacements of the target word
?whipsaw? for a given sentence taken from the
Penn Treebank (Marcus et al, 1994) are listed. In
this example each substitute is an unambiguous
verb (VB), confirming our assumption that each
artificial sentence comes from the same hidden se-
quence. For all occurrences of the word ?whip-
saw?, our reduction algorithm will count the POS
tags of the likely substitutes and remove the tags
1392
that have not been observed from the dictionary.
Assuming that the first sentence in Table 2 is the
only sentence in which we observe ?whipsaw?,
the ?NN? tag of ?whipsaw? will be removed.
The next section describes the details of our
dictionary reduction method. Section 3 explains
the details of statistical language model. We ex-
perimentally demonstrate that the word?tag dic-
tionary reduced by the substitutes improve the
performance by constraining the unsupervised
model in Section 4. Finally, Section 5 comments
on the results and discusses the possible exten-
sions of our method.
2 Dictionary Reduction
Our main assumption is that likely replacements
of a target word should have the same POS tag
as the target word in a given context. Motivated
by this idea we propose a new procedure that au-
tomatically reduces the dictionary size by using
the unambiguous replacements of the target word.
For all occurrences of the target word the pro-
cedure counts the POS tags of the replacement
words and removes the unobserved POS tags of
the target word from the dictionary.
Our approach is based on the idea that similar
words in a given context should have the same tag
sequence. To reduce the dictionary with the help
of the replacement words similar to a target word
w, we follow three rules:
1. Choose the replacement word from unam-
biguous substitutes that are likely to appear
in the target word context.
2. Substitutes must be observed in the training
corpus.
3. Count the tags of the replacement for all oc-
currences of the target word.
4. Remove the tags that are not observed as the
tag of replacements in any occurrences of the
target word.
The first rule is used to increase the likelihood
of getting a replacement word with the same POS
tag. The second rule makes sure that the size of
the vocabulary does not change. The third rule
determines the unused POS tags in all occurrences
of w and finally, last rule removes the unobserved
tags of w from the dictionary.
We use the standard first order HMM to test the
performance of our method. In a standard nth or-
der HMM each hidden state is conditioned by its
n preceding hidden states and each observation is
conditioned by its corresponding hidden state. In
POS tagging, the observed variable sequence is
a sentence s and the hidden variables ti are the
POS tags of the words wi in s. The HMM pa-
rameters ? can be estimated by using Baum-Welch
EM algorithm on an unlabeled training corpus D
(Baum, 1972). The tag sequence that maximizes
Pr(t|s, ??) can be identified by the Viterbi search
algorithm.
3 Statistical Language Modeling
In order to estimate highly probable replacement
words for a given wordw in the context cw, we use
an n-gram language model. The context is defined
as the 2n?1 word windoww?n+1 . . . w0 . . . wn?1
and it is centered at the target word position. The
probability of a word in a given context can be
estimated as:
P (w0 = w|cw) ? P (w?n+1 . . . w0 . . . wn?1) (1)
= P (w?n+1)P (w?n+2|w?n+1)
. . . P (wn?1|wn?2?n+1) (2)
? P (w0|w?1?n+1)P (w1|w0?n+2)
. . . P (wn?1|wn?20 ) (3)
where wji represents the sequence of words
wiwi+1 . . . wj . In Equation 1, Pr(w|cw) is pro-
portional to Pr(w?n+1 . . . w0 . . . wn+1) since the
context of the target word replacements is fixed.
Terms without w0 are common for every replace-
ment in Equation 2 therefore they have been
dropped in Equation 3. Finally, because of the
Markov property of n-gram language model, only
n? 1 words are used as a conditional context.
The probabilities in Equation 3 are estimated
using a 4 gram language model for all the words
in the vocabulary of D that are unambiguous and
have a common tag with the target word w. The
words with the highest Pr(r|cw) where r ? D are
selected as the replacement words of w in cw.
1393
To get accurate domain independent proba-
bility estimates we used the Web 1T data-set
(Brants and Franz, 2006), which contains the
counts of word sequences up to length five in a
1012 word corpus derived from publicly accessi-
ble Web pages. The SRILM toolkit is used to train
5-gram language model (Stolcke, 2002). The lan-
guage model parameters are optimized by using a
randomly selected 24K words corpus from Penn
Treebank. In order to efficiently apply the lan-
guage model to a given test corpus, the vocabulary
size is limited to the words seen in the test corpus.
4 Experiments
In this section we present a number of experi-
ments measuring the performance of several vari-
ants of our algorithm. The models in this sec-
tion are trained1 and tested on the same unlabeled
data therefore there aren?t any out-of-vocabulary
words. The experiments in this section focus on:
(1) the analysis of the dictionary reduction (2) the
number of the substitutes used for each ambigu-
ous word and (3) the size of the word?tag dictio-
nary.
4.1 Dataset
We trained HMM-EM models on a corpus that
consists of the first 24K words of the Penn Tree-
bank corpus. To be consistent with the POS tag-
ging literature, the tag dictionary is constructed by
listing all observed tags for each word in the entire
Penn Treebank. Nearly 55% of the words in Penn
Treebank corpus are ambiguous and the average
number of tags is 2.3.
Groups Member POS tags Count %
Noun NN/NNP/NNS/NNPS 7511 31.30
Verb VBD/VB/VBZ/VBN/VBG/VBP 3285 13.69
Adj JJ/JJR/JJS 1718 7.16
Adv RB/RBR 742 3.09
Pronoun CD/PRP/PRP$ 1397 5.82
Content Noun/Verb/Adj/Adv/Pronoun 14653 61.05
Function Other 9347 38.95
Total All 45 POS tags 24K 100.00
Table 3: Group names, members, number and per-
centage of the words according to their gold POS
tags.
1The GMTK tool is used to train HMM-EM model on an
unlabeled corpus (Bilmes and Zweig, 2002).
Table 3 shows the POS speech groups and their
distributions in the 24K word corpus. We report
the model accuracy on several POS groups. Our
motivation is to determine HMM-EM model ac-
curacies on the subgroups before and after imple-
menting the dictionary reduction procedure.
4.2 Baseline
Table 4 presents some standard baselines for com-
parison. We define a random and a most frequent
tag (MFT) baseline on the 24K corpus. The ran-
dom baseline is calculated by randomly picking
one of the tags of each word and it also represents
the amount of ambiguity in the corpus. The MFT
baseline simply selects the most frequent POS tag
of each word from the 1M word Penn Treebank
corpus (counts of the first 24K words is not in-
cluded in the 1M word corpus). If the target word
does not exist in the training set, the MFT base-
line randomly picks one of the possible tags of the
missing word.
The first and second order HMMs can be
treated as the unsupervised baselines. These unsu-
pervised baselines are calculated by training uni-
formly initialized first and second order HMMs on
the target corpus without any smoothing. All the
initial parameters of HMM-EM are uniformly ini-
tialized to observe only the effects of the artificial
sentences on the performance of HMM-EM.
The success of the MFT baseline on the Noun,
Adj, Pronoun and function word groups shows
that tag distributions of the words in these groups
are more skewed towards to one of the available
tags. The MFT baseline performs poorly, com-
pared to the above groups, on V erb, and Adv
which is due to the less skewed POS tag behav-
ior of these tags.
The POS tagging literature widely uses the sec-
ond order HMM as the baseline model; how-
ever, the performance of this model can be out-
performed by an unsupervised first order HMM
model or a simple MFT baseline as presented in
Table 4. A point worth noting is that although the
first order HMM and the MFT baseline have sim-
ilar content word accuracies, the MFT baseline is
significantly better on the function words. This
is expected since EM tends to assign words uni-
formly to the available POS tags. Thus EM can
1394
Noun Verb Adj Adv Pronoun Content Function Total(%)
Random Baseline 76.98 53.87 68.46 72.98 87.64 71.59 52.64 64.21
3-gram HMM 77.43 68.16 78.06 73.32 94.85 76.88 70.45 74.38
2-gram HMM 92.22 83.84 85.22 83.96 95.56 89.42 70.49 82.05
MFT Baseline 96.11 80.30 88.56 83.15 98.75 91.28 98.25 93.99
Table 4: Percentages of words tagged correctly by different models using standard dictionary.
not capture the highly skewed behavior of func-
tion words. Moreover the amount of skewness af-
fects the accuracy of the EM such that the perfor-
mance gain of the MFT baseline over the first or-
der HMM on function words is around 28%-30%
while the performance gain on Noun, Adj and
Pronoun is around 3%-4%.
4.3 Reduced Dictionary
EM can not capture the sparse structure of the
word distributions therefore it tends to assign
equal number of words to each POS tag. Together
with the noisy word?tag dictionary great portion
of the function words are tagged with very rare
POS tags. The abuse of the rare tags is presented
in Table 5 in a similar fashion with (Ravi and
Knight, 2009). The count of replacement word
POS tags and the removed rare POS tags of 2 er-
roneous function words are also shown in Table 5.
Word Tag Gold EM Replacement
dictionary tagging tagging POS counts
of {RB,RP,IN} IN(632) IN(0) IN(2377)
RP(0) RP(632) RP(0)
RB(0) RB(0) RB(850)
a {LS,SYM,NNP, DT(458) DT(0) DT(513)
FW,JJ,IN,DT} IN(1) IN(0) IN(317)
JJ(2) JJ(0) JJ(1329)
SYM(1) SYM(258) SYM(0)
LS(0) LS(230) LS(0)
Table 5: Removed POS tags of the given words
are shown in bold.
The results obtained with the dictionary that is
reduced by using 5 replacements are presented
in Table 6. Note that with reduced dictionary
the uniformly initialized first order HMM-EM
achieves 91.85% accuracy. Dictionary reduction
also removes some of the useful tags therefore
the upper?bound (oracle score) of the 24K dataset
becomes 98.15% after the dictionary reduction.
We execute 100 random restarts of the EM algo-
rithm and select the model with the highest corpus
likelihood, our model achieves 92.25% accuracy
which is the highest accuracy reported for the 24K
corpus so far.
As Table 6 shows, the effect of the dictionary
reduction on the function words is higher than
the effect on the content words. The main reason
for this situation is, function words are frequently
tagged with one of its tags which is also the reason
for the high accuracy of the majority voting based
baseline on the function words.
The reduced dictionary (RD) removes the rare
problematic POS tags of the words as a result the
accuracy on the content and function words shows
a drastic improvement compared to HMM models
trained with the original dictionary.
Pos 2-gram HMM 2-gram HMM RD
groups accuracy(%) accuracy(%)
Noun 92.22 94.01
Verb 83.84 84.90
Adj 85.22 89.52
Adv 83.96 85.18
Pronoun 95.56 95.92
Content 89.42 91.18
Function 70.49 92.92
All 82.05 91.85
Table 6: Percentages of the correctly tagged
words by different models with modified dictio-
nary. The dictionary size is reduced by using the
top 5 replacements of each target word.
4.4 More Data
In this set of experiments we doubled the size of
the data and trained HMM-EM models on a cor-
pus that consists of the first 48K words of the Penn
Treebank corpus. Our aim is to observe the effect
of more data on our dictionary reduction proce-
1395
dure. Using the 5 replacements of each ambigu-
ous word we reduce the dictionary and train a new
HMM-EM model using this dictionary. The ad-
ditional data together with 100 random starts in-
creases the model accuracy to 92.47% on the 48K
corpus.
Pos 3-gram HMM RD 2-gram HMM RD
groups accuracy(%) accuracy(%)
Noun 89.45 93.47
Verb 85.56 88.99
Adj 86.02 87.53
Adv 94.44 95.92
Pronoun 94.08 94.04
Content 88.91 91.97
Function 92.44 92.26
All 90.31 92.09
Table 7: Percentages of the correctly tagged
words by the first and second order HMM-EM
model trained on the 48K corpus with reduced
dictionary. The dictionary size is reduced by using
the top 5 replacements of each target word.
As we mentioned before, when the model is
trained using the original dictionary, the perfor-
mance gap between the first order HMM the sec-
ond order HMM is around 8% as presented in Ta-
ble 4. On the other hand, when we use the re-
duced dictionary together with more data the ac-
curacy gap between the second order and the first
order HMM-EM becomes less than 2% as shown
in Table 7. This confirms the hypothesis that the
low performance of the second order HMM is due
to data sparsity in the 24K-word dataset, and bet-
ter results may be achieved with the second order
HMM in larger datasets.
4.5 Number of Replacements
In this set of experiments we vary the number of
artificial replacement words per each ambiguous
word in s. We run our method on the 24K corpus
with 1, 5, 10, 25 and 50 replacement words per
ambiguous word and we present the results in Ta-
ble 8. The performance of our method affected by
the the number of replacements and highest score
is achieved when 5 replacements are used. Incor-
porating the probability of the substitutes into the
model rather than using a hard cutoff may be a
better solution.
Number of 2-gram HMM RD
replacements accuracy(%)
none 82.05
1 89.65
5 91.85
10 90.09
25 89.97
50 89.83
Table 8: Percentages of the correctly tagged
words by the models trained on the 24K corpus
with different reduced dictionaries. The dictio-
nary size is reduced by using different number re-
placements.
4.6 17-Tagset
To observe the effect our method on a model with
coarse grained dictionary, we collapsed the 45?
tagset treebank dictionary to a 17?tagset coarse
dictionary (Smith and Eisner, 2005). The POS
literature after the work of Smith and Eisner fol-
lows this tradition and also tests the models on this
17?tagset. Table 9 summarizes the previously re-
ported results on coarse grained POS tagging. Our
system achieves 92.9% accuracy where the ora-
cle accuracy of 24K dataset with the reduced 17?
tagset dictionary is 98.3% and the state-of-the-art
system IP+EM scores 96.8%.
Model Accuracy Data Size
BHMM 87.3 24K
CE+spl 88.7 24K
RD 92.9 24K
LDA+AC 93.4 1M
InitEM-HMM 93.8 1M
IP+EM 96.8 24K
Table 9: Performance of different systems using
the coarse grained dictionary.
The IP+EM system constructs a model that de-
scribes the data by using minimum number of bi-
gram POS tags then uses this model to reduce the
dictionary size (Ravi and Knight, 2009). InitEM-
HMM uses the language specific information to-
gether with good initialization and it achieves
93.8% accuracy on the 1M word treebank corpus.
LDA+AC semi-supervised Bayesian model with
strong ambiguity class component given the mor-
phological features of words and scores 93.4% on
the 1M word treebank corpus. (Toutanova and
Johnson, 2007). CE+spl is HMM model estimated
1396
by contrastive estimation method and achieves
88.7% accuracy (Smith and Eisner, 2005). Fi-
nally, BHMM is a fully Bayesian approach that
uses sparse POS priors and scores 87.3% (Gold-
water and Griffiths, 2007).
5 Contributions
In this paper we proposed a dictionary reduction
method that can be applied to unsupervised tag-
ging problems. With the help of a statistical lan-
guage model, our system creates artificial replace-
ments that are assumed to have the same POS tag
as the target word and use them to reduce the size
of the word?tag dictionary. To test our method
we used HMM-EM as the unsupervised model.
Our method significantly improves the prediction
accuracy of the unsupervised first order HMM-
EM system in all of the POS groups and achieves
92.25% and 92.47% word tagging accuracy on the
24K and 48K word corpora respectively. We also
tested our model on a coarse grained dictionary
with 17 tags and achieved an accuracy of 92.8%.
In this work, we show that unambiguous re-
placements of an ambiguous word can reduce the
amount of the ambiguity thus replacement words
might also be incorporated into the other unsuper-
vised disambiguation problems.
Acknowledgments
This work was supported in part by the Scien-
tific and Technical Research Council of Turkey
(TU?BI?TAK Project 108E228).
References
Banko, Michele and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING ?04: Pro-
ceedings of the 20th international conference on
Computational Linguistics, page 556, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Baum, L.E. 1972. An inequality and associated maxi-
mization technique in statistical estimation for prob-
abilistic functions of Markov processes. Inequali-
ties, 3(1):1?8.
Bilmes, J. and G. Zweig. 2002. The Graphical Models
Toolkit: An open source software system for speech
and time-series processing. In IEEE International
Conference On Acoustics Speech and Signal Pro-
cessing, volume 4, pages 3916?3919.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Ver-
sion 1. Linguistic Data Consortium, Philadelphia.
Goldberg, Y., M. Adler, and M. Elhadad. 2008. Em
can find pretty good hmm pos-taggers (when given
a good start). Proceedings of ACL-08. Columbus,
OH, pages 746?754.
Goldwater, S. and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging.
In Annual Meeting-Assosiation for Computational
Linguistics, volume 45, page 744.
Hawker, Tobias. 2007. Usyd: Wsd and lexical substi-
tution using the web1t corpus. In Proceedings of the
Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 446?453, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Johnson, M. 2007. Why doesnt EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 296?305.
Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguis-
tics, 19(2):313?330.
Merialdo, B. 1994. Tagging english text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Mitzenmacher, M. 2004. A brief history of generative
models for power law and lognormal distributions.
Internet mathematics, 1(2):226?251.
Ravi, Sujith and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
ACL-IJCNLP ?09: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1, pages
504?512, Morristown, NJ, USA. Association for
Computational Linguistics.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In ACL ?05: Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 354?362, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Stolcke, A. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
1397
Toutanova, K. and M. Johnson. 2007. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Proceedings of NIPS, volume 20.
Yatbaz, Mehmet Ali and Deniz Yuret. 2009. Unsuper-
vised morphological disambiguation using statisti-
cal language models. In NIPS 2009 Workshop on
Grammar Induction, Representation of Language
and Language Learning.
Yuret, Deniz and Mehmet Ali Yatbaz. 2010. The
noisy channel model for unsupervised word sense
disambiguation. Computational Linguistics, 36(1),
March.
Yuret, Deniz. 2007. KU: Word sense disambigua-
tion by substitution. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 207?214, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
1398
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2303?2313, Dublin, Ireland, August 23-29 2014.
Unsupervised Instance-Based Part of Speech Induction
Using Probable Substitutes
Mehmet Ali Yatbaz Enis R?fat Sert Deniz Yuret
Koc? University Artificial Intelligence Laboratory,
?
Istanbul
{myatbaz,esert,dyuret}@ku.edu.tr
Abstract
We develop an instance (token) based extension of the state of the art word (type) based part-of-
speech induction system introduced in (Yatbaz et al., 2012). Each word instance is represented
by a feature vector that combines information from the target word and probable substitutes
sampled from an n-gram model representing its context. Modeling ambiguity using an instance
based model does not lead to significant gains in overall accuracy in part-of-speech tagging be-
cause most words in running text are used in their most frequent class (e.g. 93.69% in the
Penn Treebank). However it is important to model ambiguity because most frequent words are
ambiguous and not modeling them correctly may negatively affect upstream tasks. Our main
contribution is to show that an instance based model can achieve significantly higher accuracy on
ambiguous words at the cost of a slight degradation on unambiguous ones, maintaining a compa-
rable overall accuracy. On the Penn Treebank, the overall many-to-one accuracy of the system is
within 1% of the state-of-the-art (80%), while on highly ambiguous words it is up to 70% better.
On multilingual experiments our results are significantly better than or comparable to the best
published word or instance based systems on 15 out of 19 corpora in 15 languages. The vector
representations for words used in our system are available for download for further experiments.
1 Introduction
Unsupervised part-of-speech (POS) induction aims to classify words into syntactic categories using un-
labeled, plain text input. The problem of induction is important for studying under-resourced languages
that lack labeled corpora and high quality dictionaries. It is also essential in modeling child language
acquisition because every child manages to induce syntactic categories without access to labeled sen-
tences, labeled prototypes, or dictionary constraints (Ambridge and Lieven, 2011). Categories induced
from data may point to shortcomings or inconsistencies of hand-labeled categories as discussed in Sec-
tion 4. Finally, the induced categories or the vector representations generated by the induction algorithms
may improve natural language processing systems when used as additional features.
Word-based POS induction systems classify different instances of a word in a single category (which
we will refer to as the one-tag-per-word assumption). Instance-based systems classify each occurence of
a word separately and can handle ambiguous words.
Examples of word-based systems include ones that represent each word with the vector of neighboring
words (context vectors) and cluster them (Sch?utze, 1995; Lamar et al., 2010b; Lamar et al., 2010a), use
a prototypical bi-tag HMM that assigns each word to a latent class (Brown et al., 1992; Clark, 2003),
restrict a HMM based Pitman-Yor process to perform one-tag-per-word inference (Blunsom and Cohn,
2011), define a word-based Bayesian multinomial mixture model (Christodoulopoulos et al., 2011), or
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2303





	


        











Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 940?951, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Syntactic Categories Using Paradigmatic Representations of
Word Context
Mehmet Ali Yatbaz Enis Sert Deniz Yuret
Artificial Intelligence Laboratory
Koc? University, I?stanbul, Turkey
{myatbaz,esert,dyuret}@ku.edu.tr
Abstract
We investigate paradigmatic representations
of word context in the domain of unsupervised
syntactic category acquisition. Paradigmatic
representations of word context are based on
potential substitutes of a word in contrast to
syntagmatic representations based on prop-
erties of neighboring words. We compare
a bigram based baseline model with several
paradigmatic models and demonstrate signif-
icant gains in accuracy. Our best model based
on Euclidean co-occurrence embedding com-
bines the paradigmatic context representation
with morphological and orthographic features
and achieves 80% many-to-one accuracy on a
45-tag 1M word corpus.
1 Introduction
Grammar rules apply not to individual words (e.g.
dog, eat) but to syntactic categories of words (e.g.
noun, verb). Thus constructing syntactic categories
(also known as lexical or part-of-speech categories)
is one of the fundamental problems in language ac-
quisition.
Syntactic categories represent groups of words
that can be substituted for one another without alter-
ing the grammaticality of a sentence. Linguists iden-
tify syntactic categories based on semantic, syntac-
tic, and morphological properties of words. There is
also evidence that children use prosodic and phono-
logical features to bootstrap syntactic category ac-
quisition (Ambridge and Lieven, 2011). However
there is as yet no satisfactory computational model
that can match human performance. Thus identify-
ing the best set of features and best learning algo-
rithms for syntactic category acquisition is still an
open problem.
Relationships between linguistic units can be
classified into two types: syntagmatic (concerning
positioning), and paradigmatic (concerning substitu-
tion). Syntagmatic relations determine which units
can combine to create larger groups and paradig-
matic relations determine which units can be sub-
stituted for one another. Figure 1 illustrates the
paradigmatic vs syntagmatic axes for words in a
simple sentence and their possible substitutes.
In this study, we represent the paradigmatic axis
directly by building substitute vectors for each word
position in the text. The dimensions of a substi-
tute vector represent words in the vocabulary, and
the magnitudes represent the probability of occur-
rence in the given position. Note that the substitute
vector for a word position (e.g. the second word in
Fig. 1) is a function of the context only (i.e. ?the
cried?), and does not depend on the word that
does actually appear there (i.e. ?man?). Thus substi-
Figure 1: Syntagmatic vs. paradigmatic axes for words
in a simple sentence (Chandler, 2007).
940
tute vectors represent individual word contexts, not
word types. We refer to the use of features based on
substitute vectors as paradigmatic representations of
word context.
Our preliminary experiments indicated that using
context information alone without the identity or the
features of the target word (e.g. using dimension-
ality reduction and clustering on substitute vectors)
has limited success and modeling the co-occurrence
of word and context types is essential for inducing
syntactic categories. In the models presented in this
paper, we combine paradigmatic representations of
word context with features of co-occurring words
within the co-occurrence data embedding (CODE)
framework (Globerson et al 2007; Maron et al
2010). The resulting embeddings for word types are
split into 45 clusters using k-means and the clusters
are compared to the 45 gold tags in the 1M word
Penn Treebank Wall Street Journal corpus (Mar-
cus et al 1999). We obtain many-to-one accura-
cies up to .7680 using only distributional informa-
tion (the identity of the word and a representation of
its context) and .8023 using morphological and or-
thographic features of words improving the state-of-
the-art in unsupervised part-of-speech tagging per-
formance.
The high probability substitutes reflect both se-
mantic and syntactic properties of the context as
seen in the example below (the numbers in paren-
theses give substitute probabilities):
?Pierre Vinken, 61 years old, will join the
board as a nonexecutive director Nov. 29.?
the: its (.9011), the (.0981), a (.0006), . . .
board: board (.4288), company (.2584),
firm (.2024), bank (.0731), . . .
Top substitutes for the word ?the? consist of
words that can act as determiners. Top substitutes
for ?board? are not only nouns, but specifically
nouns compatible with the semantic context.
This example illustrates two concerns inherent in
all distributional methods: (i) words that are gener-
ally substitutable like ?the? and ?its? are placed in
separate categories (DT and PRP$) by the gold stan-
dard, (ii) words that are generally not substitutable
like ?do? and ?put? are placed in the same category
(VB). Freudenthal et al(2005) point out that cat-
egories with unsubstitutable words fail the standard
linguistic definition of a syntactic category and chil-
dren do not seem to make errors of substituting such
words in utterances (e.g. ?What do you want?? vs.
*?What put you want??). Whether gold standard
part-of-speech tags or distributional categories are
better suited to applications like parsing or machine
translation can be best decided using extrinsic eval-
uation. However in this study we follow previous
work and evaluate our results by comparing them to
gold standard part-of-speech tags.
Section 2 gives a detailed review of related work.
Section 3 describes the dataset and the construction
of the substitute vectors. Section 4 describes co-
occurrence data embedding, the learning algorithm
used in our experiments. Section 5 describes our
experiments and compares our results with previ-
ous work. Section 6 gives a brief error analysis
and Section 7 summarizes our contributions. All the
data and the code to replicate the results given in
this paper is available from the authors? website at
http://goo.gl/RoqEh.
2 Related Work
There are several good reviews of algorithms
for unsupervised part-of-speech induction
(Christodoulopoulos et al 2010; Gao and Johnson,
2008) and models of syntactic category acquisition
(Ambridge and Lieven, 2011).
This work is to be distinguished from supervised
part-of-speech disambiguation systems, which use
labeled training data (Church, 1988), unsupervised
disambiguation systems, which use a dictionary of
possible tags for each word (Merialdo, 1994), or
prototype driven systems which use a small set
of prototypes for each class (Haghighi and Klein,
2006). The problem of induction is important for
studying under-resourced languages that lack la-
beled corpora and high quality dictionaries. It is also
essential in modeling child language acquisition be-
cause every child manages to induce syntactic cat-
egories without access to labeled sentences, labeled
prototypes, or dictionary constraints.
Models of unsupervised part-of-speech induction
fall into two broad groups based on the information
they utilize. Distributional models only use word
941
types and their context statistics. Word-feature mod-
els incorporate additional morphological and ortho-
graphic features.
2.1 Distributional models
Distributional models can be further categorized into
three subgroups based on the learning algorithm.
The first subgroup represents each word type with its
context vector and clusters these vectors accordingly
(Schu?tze, 1995). Work in modeling child syntac-
tic category acquisition has generally followed this
clustering approach (Redington et al 1998; Mintz,
2003). The second subgroup consists of proba-
bilistic models based on the Hidden Markov Model
(HMM) framework (Brown et al 1992). A third
group of algorithms constructs a low dimensional
representation of the data that represents the empir-
ical co-occurrence statistics of word types (Glober-
son et al 2007), which is covered in more detail in
Section 4.
Clustering: Clustering based methods represent
context using neighboring words, typically a sin-
gle word on the left and a single word on the right
called a ?frame? (e.g., the dog is; the cat is). They
cluster word types rather than word tokens based on
the frames they occupy thus employing one-tag-per-
word assumption from the beginning (with the ex-
ception of some methods in (Schu?tze, 1995)). They
may suffer from data sparsity caused by infrequent
words and infrequent contexts. The solutions sug-
gested either restrict the set of words and set of con-
texts to be clustered to the most frequently observed,
or use dimensionality reduction. Redington et al
(1998) define context similarity based on the num-
ber of common frames bypassing the data sparsity
problem but achieve mediocre results. Mintz (2003)
only uses the most frequent 45 frames and Biemann
(2006) clusters the most frequent 10,000 words us-
ing contexts formed from the most frequent 150-200
words. Schu?tze (1995) and Lamar et al(2010b)
employ SVD to enhance similarity between less fre-
quently observed words and contexts. Lamar et al
(2010a) represent each context by the currently as-
signed left and right tag (which eliminates data spar-
sity) and cluster word types using a soft k-means
style iterative algorithm. They report the best clus-
tering result to date of .708 many-to-one accuracy
on a 45-tag 1M word corpus.
HMMs: The prototypical bitag HMM model max-
imizes the likelihood of the corpus w1 . . . wn
expressed as P (w1|c1)
?n
i=2 P (wi|ci)P (ci|ci?1)
where wi are the word tokens and ci are their (hid-
den) tags. One problem with such a model is its ten-
dency to distribute probabilities equally and the re-
sulting inability to model highly skewed word-tag
distributions observed in hand-labeled data (John-
son, 2007). To favor sparse word-tag distributions
one can enforce a strict one-tag-per-word solution
(Brown et al 1992; Clark, 2003), use sparse pri-
ors in a Bayesian setting (Goldwater and Griffiths,
2007; Johnson, 2007), or use posterior regulariza-
tion (Ganchev et al 2010). Each of these techniques
provide significant improvements over the standard
HMM model: for example Gao and Johnson (2008)
show that sparse priors can gain from 4% (.62 to .66
with a 1M word corpus) in cross-validated many-
to-one accuracy. However Christodoulopoulos et al
(2010) show that the older one-tag-per-word models
such as (Brown et al 1992) outperform the more
sophisticated sparse prior and posterior regulariza-
tion methods both in speed and accuracy (the Brown
model gets .68 many-to-one accuracy with a 1M
word corpus). Given that close to 95% of the word
occurrences in human labeled data are tagged with
their most frequent part of speech (Lee et al 2010),
this is probably not surprising; one-tag-per-word is
a fairly good first approximation for induction.
2.2 Word-feature models
One problem with the algorithms in the previous
section is the poverty of their input features. Of the
syntactic, semantic, and morphological information
linguists claim underlie syntactic categories, con-
text vectors or bitag HMMs only represent limited
syntactic information in their input. Experiments
incorporating morphological and orthographic fea-
tures into HMM based models demonstrate signifi-
cant improvements. (Clark, 2003; Berg-Kirkpatrick
and Klein, 2010; Blunsom and Cohn, 2011) incor-
porate similar orthographic features and report im-
provements of 3, 7, and 10% respectively over the
baseline Brown model. Christodoulopoulos et al
(2010) use prototype based features as described in
(Haghighi and Klein, 2006) with automatically in-
942
duced prototypes and report an 8% improvement
over the baseline Brown model. Christodoulopou-
los et al(2011) define a type-based Bayesian multi-
nomial mixture model in which each word instance
is generated from the corresponding word type mix-
ture component and word contexts are represented
as features. They achieve a .728 MTO score by ex-
tending their model with additional morphological
and alignment features gathered from parallel cor-
pora. To our knowledge, nobody has yet tried to
incorporate phonological or prosodic features in a
computational model for syntactic category acquisi-
tion.
2.3 Paradigmatic representations
Sahlgren (2006) gives a detailed analysis of paradig-
matic and syntagmatic relations in the context of
word-space models used to represent word mean-
ing. Sahlgren?s paradigmatic model represents word
types using co-occurrence counts of their frequent
neighbors, in contrast to his syntagmatic model that
represents word types using counts of contexts (doc-
uments, sentences) they occur in. Our substitute
vectors do not represent word types at all, but con-
texts of word tokens using probabilities of likely sub-
stitutes. Sahlgren finds that in word-spaces built by
frequent neighbor vectors, more nearest neighbors
share the same part-of-speech compared to word-
spaces built by context vectors. We find that rep-
resenting the paradigmatic axis more directly using
substitute vectors rather than frequent neighbors im-
prove part-of-speech induction.
Our paradigmatic representation is also related to
the second order co-occurrences used in (Schu?tze,
1995). Schu?tze concatenates the left and right con-
text vectors for the target word type with the left con-
text vector of the right neighbor and the right con-
text vector of the left neighbor. The vectors from the
neighbors include potential substitutes. Our method
improves on his foundation by using a 4-gram lan-
guage model rather than bigram statistics, using the
whole 78,498 word vocabulary rather than the most
frequent 250 words. More importantly, rather than
simply concatenating vectors that represent the tar-
get word with vectors that represent the context we
use S-CODE to model their co-occurrence statistics.
2.4 Evaluation
We report many-to-one and V-measure scores for
our experiments as suggested in (Christodoulopou-
los et al 2010). The many-to-one (MTO) evaluation
maps each cluster to its most frequent gold tag and
reports the percentage of correctly tagged instances.
The MTO score naturally gets higher with increas-
ing number of clusters but it is an intuitive met-
ric when comparing results with the same number
of clusters. The V-measure (VM) (Rosenberg and
Hirschberg, 2007) is an information theoretic met-
ric that reports the harmonic mean of homogeneity
(each cluster should contain only instances of a sin-
gle class) and completeness (all instances of a class
should be members of the same cluster). In Sec-
tion 6 we argue that homogeneity is perhaps more
important in part-of-speech induction and suggest
MTO with a fixed number of clusters as a more in-
tuitive metric.
3 Substitute Vectors
In this study, we predict the part of speech of a word
in a given context based on its substitute vector. The
dimensions of the substitute vector represent words
in the vocabulary, and the entries in the substitute
vector represent the probability of those words be-
ing used in the given context. Note that the substi-
tute vector is a function of the context only and is
indifferent to the target word. This section details
the choice of the data set, the vocabulary and the es-
timation of substitute vector probabilities.
The Wall Street Journal Section of the Penn Tree-
bank (Marcus et al 1999) was used as the test cor-
pus (1,173,766 tokens, 49,206 types). The tree-
bank uses 45 part-of-speech tags which is the set we
used as the gold standard for comparison in our ex-
periments. To compute substitute probabilities we
trained a language model using approximately 126
million tokens of Wall Street Journal data (1987-
1994) extracted from CSR-III Text (Graff et al
1995) (we excluded the test corpus). We used
SRILM (Stolcke, 2002) to build a 4-gram language
model with Kneser-Ney discounting. Words that
were observed less than 20 times in the language
model training data were replaced by UNK tags,
which gave us a vocabulary size of 78,498. The per-
plexity of the 4-gram language model on the test cor-
943
pus is 96.
It is best to use both left and right context when
estimating the probabilities for potential lexical sub-
stitutes. For example, in ?He lived in San Francisco
suburbs.?, the token San would be difficult to guess
from the left context but it is almost certain look-
ing at the right context. We define cw as the 2n ? 1
word window centered around the target word posi-
tion: w?n+1 . . . w0 . . . wn?1 (n = 4 is the n-gram
order). The probability of a substitute word w in a
given context cw can be estimated as:
P (w0 = w|cw) ? P (w?n+1 . . . w0 . . . wn?1)(1)
= P (w?n+1)P (w?n+2|w?n+1)
. . . P (wn?1|w
n?2
?n+1) (2)
? P (w0|w
?1
?n+1)P (w1|w
0
?n+2)
. . . P (wn?1|w
n?2
0 ) (3)
where wji represents the sequence of words
wiwi+1 . . . wj . In Equation 1, P (w|cw) is pro-
portional to P (w?n+1 . . . w0 . . . wn+1) because the
words of the context are fixed. Terms without w0
are identical for each substitute in Equation 2 there-
fore they have been dropped in Equation 3. Finally,
because of the Markov property of n-gram language
model, only the closest n ? 1 words are used in the
experiments.
Near the sentence boundaries the appropriate
terms were truncated in Equation 3. Specifically, at
the beginning of the sentence shorter n-gram con-
texts were used and at the end of the sentence terms
beyond the end-of-sentence token were dropped.
For computational efficiency only the top 100
substitutes and their unnormalized probabilities
were computed for each of the 1,173,766 positions
in the test set1. The probability vectors for each po-
sition were normalized to add up to 1.0 giving us the
final substitute vectors used in the rest of this study.
1The substitutes with unnormalized log probabilities can be
downloaded from http://goo.gl/jzKH0. For a descrip-
tion of the FASTSUBS algorithm used to generate the substitutes
please see http://arxiv.org/abs/1205.5407v1.
FASTSUBS accomplishes this task in about 5 hours, a naive
algorithm that looks at the whole vocabulary would take more
than 6 days on a typical 2012 workstation.
4 Co-occurrence Data Embedding
The general strategy we follow for unsupervised
syntactic category acquisition is to combine features
of the context with the identity and features of the
target word. Our preliminary experiments indicated
that using the context information alone (e.g. clus-
tering substitute vectors) without the target word
identity and features had limited success.2 It is the
co-occurrence of a target word with a particular type
of context that best predicts the syntactic category.
In this section we review the unsupervised meth-
ods we used to model co-occurrence statistics: the
Co-occurrence Data Embedding (CODE) method
(Globerson et al 2007) and its spherical extension
(S-CODE) introduced by (Maron et al 2010).
Let X and Y be two categorical variables with fi-
nite cardinalities |X| and |Y |. We observe a set of
pairs {xi, yi}ni=1 drawn IID from the joint distribu-
tion of X and Y . The basic idea behind CODE and
related methods is to represent (embed) each value
of X and each value of Y as points in a common
low dimensional Euclidean space Rd such that val-
ues that frequently co-occur lie close to each other.
There are several ways to formalize the relationship
between the distances and co-occurrence statistics,
in this paper we use the following:
p(x, y) =
1
Z
p?(x)p?(y)e?d
2
x,y (4)
where d2x,y is the squared distance between the em-
beddings of x and y, p?(x) and p?(y) are empirical
probabilities, and Z =
?
x,y p?(x)p?(y)e
?d2x,y is a
normalization term. If we use the notation ?x for
the point corresponding to x and ?y for the point
corresponding to y then d2x,y = ??x ? ?y?
2. The
log-likelihood of a given embedding `(?, ?) can be
2A 10-nearest-neighbor supervised baseline using cosine
distance between substitute vectors gives .7213 accuracy. Clus-
tering substitute vectors using various distance metrics and di-
mensionality reduction methods give results inferior to this up-
per bound.
944
expressed as:
`(?, ?) =
?
x,y
p?(x, y) log p(x, y) (5)
=
?
x,y
p?(x, y)(? logZ + log p?(x)p?(y)? d2x,y)
= ? logZ + const ?
?
x,y
p?(x, y)d2x,y
The likelihood is not convex in ? and ?. We use
gradient ascent to find an approximate solution for
a set of ?x, ?y that maximize the likelihood. The
gradient of the d2x,y term pulls neighbors closer in
proportion to the empirical joint probability:
?
??x
?
x,y
?p?(x, y)d2x,y =
?
y
2p?(x, y)(?y ? ?x)
(6)
The gradient of the Z term pushes neighbors apart
in proportion to the estimated joint probability:
?
??x
(? logZ) =
?
y
2p(x, y)(?x ? ?y) (7)
Thus the net effect is to pull pairs together if their
estimated probability is less than the empirical prob-
ability and to push them apart otherwise. The gradi-
ents with respect to ?y are similar.
S-CODE (Maron et al 2010) additionally re-
stricts all ?x and ?y to lie on the unit sphere. With
this restriction, Z stays around a fixed value dur-
ing gradient ascent. This allows S-CODE to sub-
stitute an approximate constant Z? in gradient calcu-
lations for the real Z for computational efficiency.
In our experiments, we used S-CODE with its sam-
pling based stochastic gradient ascent algorithm and
smoothly decreasing learning rate.
5 Experiments
In this section we present experiments that evaluate
substitute vectors as representations of word con-
text within the S-CODE framework. Section 5.1
replicates the bigram based S-CODE results from
(Maron et al 2010) as a baseline. The S-CODE
algorithm works with discrete inputs. The substi-
tute vectors as described in Section 3 are high di-
mensional and continuous. We experimented with
two approaches to use substitute vectors in a dis-
crete setting. Section 5.2 presents an algorithm that
partitions the high dimensional space of substitute
vectors into small neighborhoods and uses the par-
tition id as a discrete context representation. Sec-
tion 5.3 presents an even simpler model which pairs
each word with a random substitute. When the left-
word ? right-word pairs used in the bigram model
are replaced with word ? partition-id or word ? sub-
stitute pairs we see significant gains in accuracy.
These results support our running hypothesis that
paradigmatic features, i.e. potential substitutes of
a word, are better determiners of syntactic category
compared to left and right neighbors. Section 5.4
explores morphologic and orthographic features as
additional sources of information and its results im-
prove the state-of-the-art in the field of unsupervised
syntactic category acquisition.
Each experiment was repeated 10 times with dif-
ferent random seeds and the results are reported
with standard errors in parentheses or error bars in
graphs. Table 1 summarizes all the results reported
in this paper and the ones we cite from the literature.
5.1 Bigram model
In (Maron et al 2010) adjacent word pairs (bi-
grams) in the corpus are fed into the S-CODE algo-
rithm as X,Y samples. The algorithm uses stochas-
tic gradient ascent to find the ?x, ?y embeddings for
left and right words in these bigrams on a single 25-
dimensional sphere. At the end each word w in the
vocabulary ends up with two points on the sphere,
a ?w point representing the behavior of w as the
left word of a bigram and a ?w point representing
it as the right word. The two vectors for w are con-
catenated to create a 50-dimensional representation
at the end. These 50-dimensional vectors are clus-
tered using an instance weighted k-means algorithm
and the resulting groups are compared to the cor-
rect part-of-speech tags. Maron et al(2010) report
many-to-one scores of .6880 (.0016) for 45 clusters
and .7150 (.0060) for 50 clusters (on the full PTB45
tag-set). If only ?w vectors are clustered without
concatenation we found the performance drops sig-
nificantly to about .62.
To make a meaningful comparison we re-ran the
bigram experiments using our default settings and
obtained a many-to-one score of .7314 (.0096) and
the V-measure is .6558 (.0052) for 45 clusters. The
following default settings were used: (i) each word
945
Distributional Models MTO VM
(Lamar et al 2010a) .708 -
(Brown et al 1992)* .678 .630
(Goldwater et al 2007) .632 .562
(Ganchev et al 2010)* .625 .548
(Maron et al 2010) .688 (.0016) -
Bigrams (Sec. 5.1) .7314 (.0096) .6558 (.0052)
Partitions (Sec. 5.2) .7554 (.0055) .6703 (.0037)
Substitutes (Sec. 5.3) .7680 (.0038) .6822 (.0029)
Models with Additional Features MTO VM
(Clark, 2003)* .712 .655
(Christodoulopoulos et al 2011) .728 .661
(Berg-Kirkpatrick and Klein, 2010) .755 -
(Christodoulopoulos et al 2010) .761 .688
(Blunsom and Cohn, 2011) .775 .697
Substitutes and Features (Sec. 5.4) .8023 (.0070) .7207 (.0041)
Table 1: Summary of results in terms of the MTO and VM scores. Standard errors are given in parentheses when
available. Starred entries have been reported in the review paper (Christodoulopoulos et al 2010). Distributional
models use only the identity of the target word and its context. The models on the right incorporate orthographic and
morphological features.
was kept with its original capitalization, (ii) the
learning rate parameters were adjusted to ?0 =
50, ?0 = 0.2 for faster convergence in log likeli-
hood, (iii) the number of s-code iterations were in-
creased from 12 to 50 million, (iv) k-means initial-
ization was improved using (Arthur and Vassilvit-
skii, 2007), and (v) the number of k-means restarts
were increased to 128 to improve clustering and re-
duce variance.
5.2 Random partitions
Instead of using left-word ? right-word pairs as in-
puts to S-CODE we wanted to pair each word with a
paradigmatic representation of its context to get a di-
rect comparison of the two context representations.
To obtain a discrete representation of the context,
the random?partitions algorithm first designates a
random subset of substitute vectors as centroids to
partition the space, and then associates each context
with the partition defined by the closest centroid in
cosine distance. Each partition thus defined gets a
unique id, and word (X) ? partition-id (Y ) pairs are
given to S-CODE as input. The algorithm cycles
through the data until we get approximately 50 mil-
lion updates. The resulting ?x vectors are clustered
using the k-means algorithm (no vector concatena-
tion is necessary). Using default settings (64K ran-
dom partitions, 25 s-code dimensions, Z = 0.166)
the many-to-one accuracy is .7554 (.0055) and the
V-measure is .6703 (.0037).
To analyze the sensitivity of this result to our spe-
cific parameter settings we ran a number of experi-
ments where each parameter was varied over a range
of values.
Figure 2 gives results where the number of initial
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 10000  100000
number of random partitions
m2o
Figure 2: MTO is not sensitive to the number of partitions
used to discretize the substitute vector space within our
experimental range.
random partitions is varied over a large range and
shows the results to be fairly stable across two orders
of magnitude.
Figure 3 shows that at least 10 embedding dimen-
sions are necessary to get within 1% of the best re-
sult, but there is no significant gain from using more
than 25 dimensions.
Figure 4 shows that the constant Z? approximation
can be varied within two orders of magnitude with-
out a significant performance drop in the many-to-
one score. For uniformly distributed points on a 25
dimensional sphere, the expected Z ? 0.146. In the
experiments where we tested we found the real Z al-
ways to be in the 0.140-0.170 range. When the con-
stant Z? estimate is too small the attraction in Eq. 6
dominates the repulsion in Eq. 7 and all points tend
to converge to the same location. When Z? is too
high, it prevents meaningful clusters from coalesc-
946
 0.71
 0.2
 0.21
 0.1
 0.11
 0.3
 0.31
 0.4
 0.41
 0.5
 6  60  600
89numb er ofaedm dpnm8ope8o
nte
Figure 3: MTO falls sharply for less than 10 S-CODE
dimensions, but more than 25 do not help.
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 0.01  0.1  1
number o faadbptifs tb
i2b
Figure 4: MTO is fairly stable as long as the Z? constant
is within an order of magnitude of the real Z value.
ing.
We find the random partition algorithm to be
fairly robust to different parameter settings and the
resulting many-to-one score significantly better than
the bigram baseline.
5.3 Random substitutes
Another way to use substitute vectors in a dis-
crete setting is simply to sample individual substi-
tute words from them. The random-substitutes al-
gorithm cycles through the test data and pairs each
word with a random substitute picked from the pre-
computed substitute vectors (see Section 3). We ran
the random-substitutes algorithm to generate 14 mil-
lion word (X) ? random-substitute (Y ) pairs (12
substitutes for each token) as input to S-CODE.
Clustering the resulting ?x vectors yields a many-
to-one score of .7680 (.0038) and a V-measure of
.6822 (.0029).
This result is close to the previous result by the
random-partition algorithm, .7554 (.0055), demon-
strating that two very different discrete represen-
tations of context based on paradigmatic features
give consistent results. Both results are significantly
above the bigram baseline, .7314 (.0096). Figure 5
illustrates that the random-substitute result is fairly
robust as long as the training algorithm can observe
more than a few random substitutes per word.
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 1  10  100
number of random pubptitutep ser ord
m2o
Figure 5: MTO is not sensitive to the number of random
substitutes sampled per word token.
5.4 Morphological and orthographic features
Clark (2003) demonstrates that using morpholog-
ical and orthographic features significantly im-
proves part-of-speech induction with an HMM
based model. Section 2 describes a number other ap-
proaches that show similar improvements. This sec-
tion describes one way to integrate additional fea-
tures to the random-substitute model.
The orthographic features we used are similar to
the ones in (Berg-Kirkpatrick et al 2010) with small
modifications:
? Initial-Capital: this feature is generated for cap-
italized words with the exception of sentence
initial words.
? Number: this feature is generated when the to-
ken starts with a digit.
? Contains-Hyphen: this feature is generated for
lowercase words with an internal hyphen.
947
? Initial-Apostrophe: this feature is generated for
tokens that start with an apostrophe.
We generated morphological features using the
unsupervised algorithm Morfessor (Creutz and La-
gus, 2005). Morfessor was trained on the WSJ sec-
tion of the Penn Treebank using default settings, and
a perplexity threshold of 300. The program induced
5 suffix types that are present in a total of 10,484
word types. These suffixes were input to S-CODE
as morphological features whenever the associated
word types were sampled.
In order to incorporate morphological and ortho-
graphic features into S-CODE we modified its in-
put. For each word ? random-substitute pair gen-
erated as in the previous section, we added word ?
feature pairs to the input for each morphological and
orthographic feature of the word. Words on average
have 0.25 features associated with them. This in-
creased the number of pairs input to S-CODE from
14.1 million (12 substitutes per word) to 17.7 mil-
lion (additional 0.25 features on average for each of
the 14.1 million words).
Using similar training settings as the previous
section, the addition of morphological and ortho-
graphic features increased the many-to-one score of
the random-substitute model to .8023 (.0070) and
V-measure to .7207 (.0041). Both these results im-
prove the state-of-the-art in part-of-speech induction
significantly as seen in Table 1.
6 Error Analysis
Figure 6 is the Hinton diagram showing the rela-
tionship between the most frequent tags and clusters
from the experiment in Section 5.4. In general the
errors seem to be the lack of completeness (multi-
ple large entries in a row), rather than lack of ho-
mogeneity (multiple large entries in a column). The
algorithm tends to split large word classes into sev-
eral clusters. Some examples are:
? Titles like Mr., Mrs., and Dr. are split from the
rest of the proper nouns in cluster (39).
? Auxiliary verbs (10) and the verb ?say? (22)
have been split from the general verb clusters
(12) and (7).
? Determiners ?the? (40), ?a? (15), and capital-
ized ?The?, ?A? (6) have been split into their
own clusters.
? Prepositions ?of? (19), and ?by?, ?at? (17) have
been split from the general preposition cluster
(8).
Nevertheless there are some homogeneity errors as
well:
? The adjective cluster (5) also has some noun
members probably due to the difficulty of sep-
arating noun-noun compounds from adjective
modification.
? Cluster (6) contains capitalized words that span
a number of categories.
Most closed-class items are cleanly separated into
their own clusters as seen in the lower right hand
corner of the diagram. The completeness errors are
not surprising given that the words that have been
split are not generally substitutable with the other
members of their Penn Treebank category. Thus it
can be argued that metrics that emphasize homo-
geneity such as MTO are more appropriate in this
context than metrics that average homogeneity and
completeness such as VM as long as the number of
clusters is controlled.
7 Contributions
Our main contributions can be summarized as fol-
lows:
? We introduced substitute vectors as paradig-
matic representations of word context and
demonstrated their use in syntactic category ac-
quisition.
? We demonstrated that using paradigmatic rep-
resentations of word context and modeling co-
occurrences of word and context types with
the S-CODE learning framework give superior
results when compared to a baseline bigram
model.
? We extended the S-CODE framework to in-
corporate morphological and orthographic fea-
tures and improved the state-of-the-art in un-
supervised part-of-speech induction to 80%
many-to-one accuracy.
948
Figure 6: Hinton diagram comparing most frequent tags and clusters.
? All our code and data, including the sub-
stitute vectors for the one million word
Penn Treebank Wall Street Journal dataset,
is available at the authors? website at
http://goo.gl/RoqEh.
References
B. Ambridge and E.V.M. Lieven, 2011. Child Language
Acquisition: Contrasting Theoretical Approaches,
chapter 6.1. Cambridge University Press.
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 1027?1035. Society for Industrial
and Applied Mathematics.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1288?1297, Uppsala, Sweden, July.
Association for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
C. Biemann. 2006. Unsupervised part-of-speech tagging
employing efficient graph clustering. In Proceedings
of the 21st International Conference on computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics: Student Research
Workshop, pages 7?12. Association for Computational
Linguistics.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479, December.
D. Chandler. 2007. Semiotics: the basics. The Basics
Series. Routledge.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
pos induction: how far have we come? In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 575?
584, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A bayesian mixture model
for pos induction using multiple features. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 638?647, Edin-
burgh, Scotland, UK., July. Association for Computa-
tional Linguistics.
949
Kenneth Ward Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted text. In
Proceedings of the second conference on Applied nat-
ural language processing, ANLC ?88, pages 136?143,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics - Volume 1, EACL ?03, pages 59?66,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR?05, Inter-
national and Interdisciplinary Conference on Adap-
tive Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June.
D. Freudenthal, J.M. Pine, and F. Gobet. 2005. On the
resolution of ambiguities in the extraction of syntactic
categories through chunking. Cognitive Systems Re-
search, 6(1):17?25.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. J. Mach. Learn. Res.,
99:2001?2049, August.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 344?352, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. J. Mach. Learn. Res., 8:2265?2295,
December.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
David Graff, Roni Rosenfeld, and Doug Paul. 1995. Csr-
iii text. Linguistic Data Consortium, Philadelphia.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
?06, pages 320?327, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Michael Lamar, Yariv Maron, and Elie Bienenstock.
2010a. Latent-descriptor clustering for unsupervised
pos induction. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 799?809, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010b. Svd and clustering for unsuper-
vised pos tagging. In Proceedings of the ACL 2010
Conference Short Papers, pages 215?219, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 853?861, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Yariv Maron, Michael Lamar, and Elie Bienenstock.
2010. Sphere embedding: An application to part-of-
speech induction. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1567?1575.
Bernard Merialdo. 1994. Tagging english text with a
probabilistic model. Comput. Linguist., 20:155?171,
June.
T.H. Mintz. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cognition,
90(1):91?117.
M. Redington, N. Crater, and S. Finch. 1998. Distribu-
tional information: A powerful cue for acquiring syn-
tactic categories. Cognitive Science, 22(4):425?469.
A. Rosenberg and J. Hirschberg. 2007. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 410?420.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
950
on European chapter of the Association for Compu-
tational Linguistics, EACL ?95, pages 141?148, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286, November.
951
The Noisy Channel Model for Unsupervised
Word Sense Disambiguation
Deniz Yuret?
Koc? University
Mehmet Ali Yatbaz
Koc? University
We introduce a generative probabilistic model, the noisy channel model, for unsupervised word
sense disambiguation. In our model, each context C is modeled as a distinct channel through
which the speaker intends to transmit a particular meaning S using a possibly ambiguous word
W. To reconstruct the intended meaning the hearer uses the distribution of possible meanings
in the given context P(S|C) and possible words that can express each meaning P(W|S). We
assume P(W|S) is independent of the context and estimate it using WordNet sense frequencies.
The main problem of unsupervised WSD is estimating context-dependent P(S|C) without access
to any sense-tagged text. We show one way to solve this problem using a statistical language
model based on large amounts of untagged text. Our model uses coarse-grained semantic classes
for S internally and we explore the effect of using different levels of granularity on WSD per-
formance. The system outputs fine-grained senses for evaluation, and its performance on noun
disambiguation is better than most previously reported unsupervised systems and close to the
best supervised systems.
1. Introduction
Word sense disambiguation (WSD) is the task of identifying the correct sense of an
ambiguous word in a given context. An accurate WSD system would benefit appli-
cations such as machine translation and information retrieval. The most successful
WSD systems to date are based on supervised learning and trained on sense-tagged
corpora. In this article we present an unsupervised WSD algorithm that can leverage
untagged text and can perform at the level of the best supervised systems for the all-
nouns disambiguation task.
The main drawback of the supervised approach is the difficulty of acquiring
considerable amounts of training data, also known as the knowledge acquisition
bottleneck. Yarowsky and Florian (2002) report that each successive doubling of the
training data for WSD only leads to a 3?4% error reduction within their experimental
range. Banko and Brill (2001) experiment with the problem of selection among
confusable words and show that the learning curves do not converge even after
? Koc? University, Department of Computer Engineering, 34450 Sar?yer, I?stanbul, Turkey.
E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr.
Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication:
12 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
a billion words of training data. They suggest unsupervised, semi-supervised, or
active learning to take advantage of large data sets when labeling is expensive. Yuret
(2004) observes that in a supervised naive Bayes WSD system trained on SemCor,
approximately half of the test instances do not contain any of the contextual features
(e.g., neighboring content words or local collocation patterns) observed in the training
data. SemCor is the largest publicly available corpus of sense-tagged text, and has only
about a quarter million sense-tagged words. In contrast, our unsupervised system uses
the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains
counts from a 1012 word corpus derived from publicly-available Web pages.
A note on the term ?unsupervised? may be appropriate here. In the WSD literature
?unsupervised? is typically used to describe systems that do not directly use sense-
tagged corpora for training. However, many of these unsupervised systems, including
ours, use sense ordering or sense frequencies from WordNet (Fellbaum 1998) or other
dictionaries. Thus it might be more appropriate to call them weakly supervised or
semi-supervised. More specifically, context?sense pairs or context?word?sense triples
are not observed in the training data, but context-word frequencies (from untagged
text) and word-sense frequencies (from dictionaries or other sources) are used in model
building. One of the main problems we explore in this study is the estimation of context-
dependent sense probabilities when no context?sense pairs have been observed in the
training data.
The first contribution of this article is a probabilistic generative model for word
sense disambiguation that seamlessly integrates unlabeled text data into the model
building process. Our approach is based on the noisy channel model (Shannon 1948),
which has been an essential ingredient in fields such as speech recognition and machine
translation. In this study we demonstrate that the noisy channel model can also be the
key component for unsupervised word sense disambiguation, provided we can solve
the context-dependent sense distribution problem. In Section 2.1 we show one way
to estimate the context-dependent sense distribution without using any sense-tagged
data. Section 2.2 outlines the complete unsupervised WSD algorithm using this model.
We estimate the distribution of coarse-grained semantic classes rather than fine-grained
senses. The solution uses the two distributions for which we do have data: the distribu-
tion of words used to express a given sense, and the distribution of words that appear
in a given context. The first can be estimated using WordNet sense frequencies, and the
second can be estimated using an n-gram language model as described in Section 2.3.
The second contribution of this article is an exploration of semantic classes at differ-
ent levels of granularity for word sense disambiguation. Using fine-grained senses for
model building is inefficient both computationally and from a learning perspective. The
noisy channel model can take advantage of the close distribution of similar senses if they
are grouped into semantic classes. We take semantic classes to be groups of WordNet
synsets defined using the hypernym hierarchy. In each experiment we designate a
number of synsets high in the WordNet hypernym hierarchy as ?head synsets? and
use their descendants to partition the senses into separate semantic classes. In Section 3
we present performance bounds for such class-based WSD and describe our method of
exploring the different levels of granularity.
In Section 4 we report on our actual experiments and compare our results with
the best supervised and unsupervised systems from SensEval-2 (Cotton et al 2001),
SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, Ma`rquez, and
Wicentowski 2007). Section 5 discusses these results and the idiosyncrasies of the
data sets, baselines, and evaluation metrics used. Section 6 presents related work, and
Section 7 summarizes our contributions.
112
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
2. The Noisy Channel Model for WSD
2.1 Model
The noisy channel model has been the foundation of standard models in speech recog-
nition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al 1990).
In this article we explore its application to WSD. The noisy channel model can be
used whenever a signal received does not uniquely identify the message being sent.
Bayes? Law is used to interpret the ambiguous signal and identify the most probable
intended message. In WSD, we model each context as a distinct channel where the
intended message is a word sense (or semantic class) S, and the signal received is an
ambiguous wordW. In this section we will describe how to model a given context C as
a noisy channel, and in particular how to estimate the context-specific sense distribution
without using any sense-tagged data.
Equation (1) expresses the probability of a sense S of wordW in a given context C.
This is the well-known Bayes? formula with an extra P(.|C) in each term indicating the
dependence on the context.
P(S|W,C) =
P(W|S,C)P(S|C)
P(W|C)
(1)
To perform WSD we need to find the sense S that maximizes the probability P(S|W,C).
This is equivalent to the maximization of the product P(W|S,C)P(S|C) because the
denominator P(W|C) does not depend on S. To perform the maximization, the two
distributions P(W|S,C) and P(S|C) need to be estimated for each context C.
The main challenge is to estimate P(S|C), the distribution of word senses that can
be expressed in the given context. In unsupervised WSD we do not have access to any
sense-tagged data, thus we do not know what senses are likely to be expressed in any
given context. Therefore it is not possible to estimate P(S|C) directly.
What we do have is the word frequencies for each sense P(W|S), and the word
frequencies for the given context P(W|C). We use the WordNet sense frequencies to
estimate P(W|S) and a statistical language model to estimate P(W|C) as detailed in
Section 2.3. We make the independence assumption P(W|S,C) = P(W|S), that is, the
distribution of words used to express a particular sense is the same for all contexts.
Finally, the relationship between the three distributions, P(S|C), P(W|S,C), and P(W|C)
is given by the total probability theorem:
P(W|C) =
?
S
P(S|C)P(W|S,C) (2)
We can solve for P(S|C) using linear algebra. Let WS be a matrix, s and w two vectors
such that:
WSij = P(W = i|S = j)
sj = P(S = j|C = k)
wi = P(W = i|C = k) (3)
113
Computational Linguistics Volume 36, Number 1
Using this new form, we can see that Equation (2) is equivalent to the linear equation
w =WS? s and s can be solved using a linear solver. Typically WS is a tall matrix and
the system has no exact solutions. We use the Moore?Penrose pseudoinverse WS+ to
compute an approximate solution:
s =WS+ ? w (4)
Appendix A discusses possible scaling issues of this solution and offers alternative
solutions. We use the pseudoinverse solution in all our experiments because it can be
computed fast and none of the alternatives we tried made a significant difference in
WSD performance.
2.2 Algorithm
Section 2.1 described how to apply the noisy channel model for WSD in a single context.
In this section we present the steps we follow in our experiments to simultaneously
apply the noisy channel model to all the contexts in a given word sense disambiguation
task.
Algorithm 1
1. Let W be the vocabulary. In this study we took the vocabulary to be the
approximately 12,000 nouns in WordNet that have non-zero sense
frequencies.
2. Let S be the set of senses or semantic classes to be used. In this study we
used various partitions of noun synsets as semantic classes.
3. Let C be the set of contexts (nine-word windows for a 5-gram model)
surrounding each target word in the given WSD task.
4. Compute the matrix WC where WCik = P(W = i|C = k). Here i ranges
over the vocabulary W and k ranges over the contexts C. This matrix
concatenates the (w) word distribution vectors from Equation (4) for each
context. The entries of the matrix are computed using the n-gram language
model described in Section 2.3. This is the most expensive step in the
algorithm (see Appendix B for a discussion of implementation efficiency).
5. Compute the matrix WS where WSij = P(W = i|S = j). Here i ranges over
the vocabulary W and j ranges over the semantic classes S. The entries of
the matrix are computed using the WordNet sense frequencies.
6. Compute the matrix SC =WS+ ?WC where SCjk = P(S = j|C = k).
Here j ranges over the semantic classes S and k ranges over the contexts C.
This step computes the pseudoinverse solution described in Section 2.1
simultaneously for all the contexts, and the resulting SC matrix is a
concatenation of the (s) solution vectors from Equation (4) for each context.
WS+ is the pseudoinverse of the matrix WS.
7. Compute the best semantic class for each WSD instance by using
argmaxSP(S|W,C) ? P(W|S)P(S|C). Here P(S|C) comes from the column
of the SC matrix that corresponds to the context of the WSD instance
114
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
and P(W|S) comes from the row of the WS matrix that corresponds to the
word to be disambiguated.
8. Compute the fine-grained answer for each WSD instance by taking the
most frequent (lowest numbered) sense in the chosen semantic class.
9. Apply the one sense per discourse heuristic: If a word is found to have
multiple senses in a document, replace them with the majority answer.
2.3 Estimation Procedure
In Section 2.1, we showed how the unsupervised WSD problem expressed as a noisy
channel model can be decomposed into the estimation of two distributions: P(W|S) and
P(W|C). In this section we detail our estimation procedure for these two distributions.
To estimate P(W|S), the distribution of words that can be used to express a given
meaning, we used the WordNet sense frequencies.1 We did not perform any smoothing
for the zero counts and used the maximum likelihood estimate: count(W,S)/count(S).
As described in later sections, we also experimented with grouping similar WordNet
senses into semantic classes. In this case S stands for the semantic class, and the counts
from various senses of a word in the same semantic class are added together to estimate
P(W|S).
To estimate the distribution of words in a given context, P(W|C), we used a 5-gram
language model. We define the context as the nine-word window centered on the target
word w1w2 . . .w9, whereW = w5. The probability of a word in the given context can be
expressed as:
P(W = w5) ? P(w1 . . .w9) (5)
= P(w1)P(w2|w1) . . . P(w9|w1 . . .w8) (6)
? P(w5|w1 . . .w4)P(w6|w2 . . .w5)P(w7|w3 . . .w6) (7)
P(w8|w4 . . .w7)P(w9|w5 . . .w8)
Equation (5) indicates that P(W|C) is proportional to P(w1 . . .w9) because the other
words in the context are fixed for a given WSD instance. Equation (6) is the standard
decomposition of the probability of a word sequence into conditional probabilities.
The first four terms do not include the target word w5, and have been dropped in Equa-
tion (7). We also truncate the remaining conditionals to four words reflecting the Markov
assumption of the 5-gram model. Finally, using an expression that is proportional to
P(W|C) instead of P(W|C) itself will not change the WSD result because we are taking
the argmax in Equation (1).
Each term on the right hand side of Equation (7) is estimated using a 5-gram
language model. To get accurate domain-independent probability estimates we used the
Web 1T data set (Brants and Franz 2006), which contains the counts of word sequences
up to length five in a 1012 word corpus derived from publicly-accessible Web pages.
Estimation of P(W|C) is the most computationally expensive step of the algorithm, and
some implementation details are given in Appendix B.
1 The sense frequencies were obtained from the index.sense file included in the WordNet distribution.
We had to correct the counts of three words (person, group, and location) whose WordNet counts
unfortunately include the corresponding named entities and are thus inflated.
115
Computational Linguistics Volume 36, Number 1
Figure 1
Upper bound on fine-grained accuracy for a given number of semantic classes.
3. Semantic Classes
Our algorithm internally differentiates semantic classes rather than fine-grained senses.
Using fine-grained senses in the noisy channel model would be computationally ex-
pensive because the word?sense matrix needs to be inverted (see Equation [4]). It is
also unclear whether using fine-grained senses for model building will lead to better
learning performance: The similarity between the distributions of related senses is
ignored and the data becomes unnecessarily fragmented.
Even though we use coarse-grained semantic classes for model building, we use
fine-grained senses for evaluation. During evaluation, the coarse-grained semantic
classes predicted by the model are mapped to fine-grained senses by picking the lowest
numbered WordNet sense in the chosen semantic class.2 This is necessary to perform a
meaningful comparison with published results.
We take semantic classes to be groups of WordNet synsets defined using the hyper-
nym hierarchy (see Section 6 for alternative definitions). Section 4 presents three WSD
experiments using different sets of semantic classes at different levels of granularity.
In each experiment we designate a number of synsets high in the WordNet hypernym
hierarchy as ?head synsets? and use their descendants to form the separate semantic
classes.
An arbitrary set of head synsets will not necessarily have mutually exclusive and
collectively exhaustive descendants. To assign every synset to a unique semantic class,
we impose an ordering on the semantic classes. Each synset is assigned only to the first
semantic class whose head it is a descendant of according to this ordering. If there are
synsets that are not descendants of any of the heads, they are collected into a separate
semantic class created for that purpose.
Using the coarse-grained semantic classes for prediction, Algorithm 1 will be unable
to return the correct fine-grained sense when this is not the lowest numbered sense in
a semantic class. To quantify the restrictive effect of working with a small number of
semantic classes, Figure 1 plots the number of semantic classes versus the best possible
2 The sense numbers are ordered by the frequency of occurrence in WordNet.
116
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
oracle accuracy for the nouns in the SemCor corpus. To compute the oracle accuracy, we
assume that the program can find the correct semantic class for each instance, but has to
pick the first sense in that class as the answer. To construct a given number of semantic
classes, we used the following algorithm:
Algorithm 2
1. Initialize all synsets to be in a single ?default? semantic class.
2. For each synset, compute the following score: the oracle accuracy achieved
if that synset and all its descendants are split into a new semantic class.
3. Take the synset with the highest score and split that synset and its
descendants into a new semantic class.
4. Repeat steps 2 and 3 until the desired number of semantic classes is
achieved.
The upper bound on fine-grained accuracy given a small number of semantic
classes is surprisingly high. In particular, the best reported noun WSD accuracy (78%)
is achievable if we could perfectly distinguish between five semantic classes.
4. Three Experiments
We ran three experiments with the noisy channel model using different sets of semantic
classes. The first experiment uses the 25 WordNet semantic categories for nouns, the
second experiment looks at what happens when we group all the senses to just two
or three semantic classes, and the final experiment optimizes the number of semantic
classes using one data set (which gives 135 classes) and reports the out-of-sample result
using another data set.
The noun instances from the last three SensEval/SemEval English all-words tasks
are used for evaluation. We focus on the disambiguation of nouns for several reasons.
Nouns constitute the largest portion of content words (48% of the content words in the
Brown corpus [Kucera and Francis 1967] are nouns). For many tasks and applications
(e.g., Web queries [Jansen, Spink, and Pfaff 2000]) nouns are the most frequently encoun-
tered and important part of speech. Finally, WordNet has a more complete coverage
of noun semantic relations than other parts of speech, which is important for our
experiments with semantic classes.
As described in Section 2.2 we use the model to assign each ambiguous word to its
most likely semantic class in all the experiments. The lowest numbered sense in that
class is taken as the fine-grained answer. Finally we apply the one sense per discourse
heuristic: If the same word has been assigned more than one sense within the same
document, we take a majority vote and use sense numbers to break the ties.
Table 1 gives some baselines for comparison. The performance of the best super-
vised and unsupervised systems on noun disambiguation for each data set are given.
The first-sense baseline (FSB) is obtained by always picking the lowest numbered sense
for the word in the appropriate WordNet version. We prefer the FSB baseline over the
commonly used most-frequent-sense baseline because the tie breaking is unambiguous.
All the results reported are for fine-grained sense disambiguation. The top three systems
given in the table for each task are all supervised systems; the result for the best
117
Computational Linguistics Volume 36, Number 1
Table 1
Baselines for the three SensEval English all-words tasks; the WordNet version used (WN);
number of noun instances (Nouns); percentage accuracy of the first-sense baseline (FSB); the top
three supervised systems; and the best unsupervised system (Unsup). The last row gives the
total score of the best systems on the three tasks.
Task WN Nouns FSB 1st 2nd 3rd Unsup
senseval2 1.7 1,067 71.9 78.0 74.5 70.0 61.8
senseval3 1.7.1 892 71.0 72.0 71.2 71.0 62.6
semeval07 2.1 159 64.2 68.6 66.7 66.7 63.5
total 2,118 70.9 74.4 72.5 70.2 62.2
unsupervised system is given in the last column. The reported unsupervised systems
do use the sense ordering and frequency information from WordNet.
4.1 First Experiment: The 25 WordNet Categories
In previous work, descendants of 25 special WordNet synsets (known as the unique
beginners) have been used as the coarse-grained semantic classes for nouns (Crestan,
El-Be`ze, and De Loupy 2001; Kohomban and Lee 2005). These unique beginners were
used to organize the nouns into 25 lexicographer files based on their semantic category
during WordNet development. Figure 2 shows the synsets at the top of the noun
hierarchy in WordNet. The 25 unique beginners have been shaded, and the two graphics
show how the hierarchy evolved between the two WordNet versions used in this study.
Figure 2
The top of the WordNet noun hypernym hierarchy for version 1.7 (left) and version 2.1 (right).
The 25 WordNet noun categories are shaded.
118
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Table 2
The performance of the noisy channel model with the 25 semantic classes based on WordNet
lexicographer files. The columns give the data set, the percentage of times the model picks the
correct semantic class, maximum possible fine-grained score if the model had always picked the
correct class, and the actual score.
Data Set CorrClass MaxScore Score
senseval2 85.1 90.3 77.7
senseval3 78.0 88.7 70.1
semeval07 75.5 86.2 64.8
total 81.4 89.3 73.5
We ran our initial experiments using these 25 WordNet categories as semantic
classes. The distribution of words for each semantic class, P(W|S), is estimated based
on WordNet sense frequencies. The distribution of words for each context, P(W|C), is
estimated using a 5-gram model based on the Web 1T corpus. The system first finds the
most likely semantic class based on the noisy channel model, then picks the first sense in
that class. Table 2 gives the results for the three data sets, which are significantly higher
than the previously reported unsupervised results.
To illustrate which semantic classes are the most difficult to disambiguate, Table 3
gives the confusion matrix for the Senseval2 data set. We can see that frequently occur-
ring concrete classes like person and body are disambiguated well. The largest source
of errors are the abstract classes like act, attribute, cognition, and communication. These
25 classes may not be the ideal candidates for word sense disambiguation. Even though
they allow a sufficient degree of fine-grained distinction (Table 2 shows that we can get
Table 3
Confusion matrix for Senseval2 data with the 25 WordNet noun classes. The rows are actual
classes, the columns are predicted classes. Column names have been abbreviated to save space.
The last two columns give the frequency of the class (F) and the accuracy of the class (A).
ac an ar at bo co co ev fe fo gr lo mo ob pe ph po pr qu re sh st su ti F A
act 58 0 4 7 0 7 2 3 2 0 5 0 0 0 0 0 1 4 1 1 0 2 0 0 9.1 59.8
animal 0 17 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.1 77.3
artifact 0 0 66 2 0 0 6 5 0 0 5 1 0 1 0 0 0 0 0 0 3 1 0 0 8.4 73.3
attribute 3 0 0 19 0 3 0 0 0 0 0 1 0 1 2 0 2 1 0 0 1 3 0 0 3.4 52.8
body 0 0 0 0 123 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 11.6 99.2
cognition 6 0 1 2 0 82 5 1 0 0 0 2 1 1 1 0 1 0 5 1 0 5 0 0 10.7 71.9
communicat 2 0 1 0 0 2 29 1 0 0 0 2 5 0 0 1 0 0 0 1 0 0 0 2 4.3 63.0
event 0 0 0 0 0 0 0 19 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2.0 90.5
feeling 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.4 100.
food 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 100.
group 0 0 0 2 0 5 0 0 0 0 69 2 0 3 0 0 0 0 0 1 1 0 0 1 7.9 82.1
location 0 0 0 1 0 0 0 0 0 0 0 22 0 0 0 0 0 0 0 0 0 0 0 0 2.2 95.7
motive 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0.2 50.0
object 2 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0.7 14.3
person 2 4 0 0 0 1 1 0 0 0 1 0 0 0 168 0 0 0 0 0 0 0 0 0 16.6 94.9
phenomenon 1 0 0 1 0 1 0 2 0 0 0 0 0 0 0 3 0 0 0 3 0 0 0 0 1.0 27.3
possession 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0.4 100.
process 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 12 0 0 0 1 0 0 1.4 80.0
quantity 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 10 0 0 0 0 0 1.2 76.9
relation 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0.3 66.7
shape 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0.1 100.
state 1 0 1 5 0 1 1 2 0 0 1 0 0 0 1 0 0 0 0 0 0 98 0 0 10.4 88.3
substance 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0.9 100.
time 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 44 4.8 86.3
119
Computational Linguistics Volume 36, Number 1
Table 4
The performance of the noisy channel model with two to three semantic classes. The columns
give the data set, the head synsets, the percentage of times the model picks the correct semantic
class, maximum possible fine-grained score if the model had always picked the correct class, and
the actual score.
Data Set Heads CorrClass MaxScore Score
senseval2 entity/default 86.6 76.8 74.9
senseval3 entity/default 94.2 75.8 71.2
senseval3 object/entity/default 93.8 77.4 72.9
semeval07 psychological-feature/default 91.2 74.8 68.6
85?90% if we could pick the right class every time), they seem too easy to confuse. In the
next few experiments we will use these observations to design better sets of semantic
classes.
4.2 Second Experiment: Distinguishing Mental and Physical Concepts
Figure 1 shows that the upper bound for fine-grained disambiguation is relatively high
even for a very small number of semantic classes. In our next experiment we look at
how well our approach can perform differentiating only two or three semantic classes.
We use Algorithm 2 applied to the appropriate version of SemCor to pick the head
synsets used to define the semantic classes. Figure 2 shows that the top level of the
hypernym hierarchy has changed significantly between the WordNet versions. Thus,
different head synsets are chosen for different data sets. However, the main distinction
captured by our semantic classes seems to be between mental and physical concepts.
Table 4 gives the results. The performance with a few semantic classes is comparable to
the top supervised algorithms in each of the three data sets.
4.3 Third Experiment: Tuning the Number of Classes
Increasing the number of semantic classes has two opposite effects on WSD perfor-
mance. The higher the number, the finer distinctions we can make, and the maximum
possible fine-grained accuracy goes up. However, the more semantic classes we define,
the more difficult it becomes to distinguish them from one another. For an empirical
analysis of the effect of semantic class granularity on the fine-grained WSD accuracy,
we generated different sets of semantic classes using the following algorithm.
Algorithm 3
1. Sort all the synsets according to their ?subtree frequency?: i.e., the total
frequency of each synset?s descendants in the hypernym tree.
2. Take the desired number of synsets with the highest subtree frequency and
use them as head synsets, that is, split their descendants into separate
semantic classes.
Figure 3 shows the fine-grained accuracy we achieved on the Senseval2 data set
with up to 600 semantic classes defined based on Algorithm 3. Note the differences: (i)
120
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Figure 3
The fine-grained accuracy on Senseval2 data set for a given number of semantic classes.
Figure 1 gives the best possible oracle accuracy, Figure 3 gives the actual WSD accuracy;
(ii) Algorithm 2 chooses the head synsets based on their oracle score, Algorithm 3
chooses them based on their subtree frequency.
As we suspected, the relationship is not simple or monotonic. However, one can
identify distinct peaks at 3, 25, and 100?150 semantic classes. One hypothesis is that
these peaks correspond to ?natural classes? at different levels of granularity. Here are
some example semantic classes from each peak:
3 classes entity, abstraction
25 classes action, state, content, location, attribute, ...
135 classes food, day, container, home, word, business, feeling, material, job, man, ...
To test the out-of-sample effect of tuning the semantic classes based on the peaks
of Figure 3, we used the SemEval-2007 data set as our test sample. When the 135
semantic classes from the highest peak are used for the disambiguation of the nouns
in the SemEval-2007 data set, an accuracy of 69.8% was achieved. This is higher than
the accuracy of the best supervised system on this task (68.6%), although the difference
is not statistically significant.
5. Discussion
In this section we will address several questions raised by the results of the experi-
ments. Why do we get different results from different data sets? Are the best results
significantly different than the first-sense baseline? Can we improve our results using
better semantic classes?
5.1 Why Do We Get Different Results from Different Data Sets?
Table 5 summarizes our results from the three experiments of Section 4. There are some
significant differences between the data sets.
121
Computational Linguistics Volume 36, Number 1
Table 5
Result summary for the three data sets. The columns give the data set, the results of the three
experiments, best reported result, the first-sense baseline, and the number of instances.
Data set Exp1 Exp2 Exp3 Best FSB Instances
senseval2 77.7 74.9 - 78.0 71.9 1,067
senseval3 70.1 72.9 - 72.0 71.0 892
semeval07 64.8 68.6 69.8 68.6 64.2 159
The SemEval-2007 data set appears to be significantly different from the other two
with its generally lower baseline and scores. The difference in accuracy is probably
due to the difference in data preparation. In the two Senseval data sets all content
words were targeted for disambiguation. In the SemEval-2007 data set only verbs and
their noun arguments were selected, targeting only about 465 lemmas from about 3,500
words of text. For the Senseval-3 data set none of our results, or any published result
we know of, is significantly above the baseline for noun disambiguation. This may be
due to extra noise in the data?the inter-annotator agreement for nouns in this data set
was 74.9%.
5.2 Are the Best Results Significantly Different Than the FSB?
Among all the published results for these three data sets, our two results for the
Senseval-2 data set and the top supervised result for the Senseval-2 data set are the
only ones statistically significantly above the FSB for noun disambiguation at the 95%
confidence interval. This is partly because of the lack of sufficient data. For example, the
SemEval-2007 data set has only 159 nouns; and a result of 71.8% would be needed to
demonstrate a difference from the baseline of 64.2% at the 95% confidence interval.
More importantly, however, statistical significance should not be confused with
?significance? in general. A statistically significant difference may not be necessary or
sufficient for a significant impact on an application. Even a WSD system that is statis-
tically indistinguishable from the baseline according to the ?total accuracy? metric is
most probably providing significantly different answers compared to always guessing
the first sense. There are metrics that can reveal these differences, such as ?balanced
error rate? (i.e., arithmetic average of the error rates for different senses) or ?accuracy in
detecting the use of a non-dominant sense.?
Finally, the relatively high first-sense baseline (e.g., 71.0% for Senseval-3 nouns)
combined with the relatively low inter-annotator agreement (e.g., 74.9% for Senseval-
3 nouns) makes progress in the traditional WSD task difficult. Annotators who are
perfectly proficient in comprehending language nevertheless find it difficult to distin-
guish between artificially-created dictionary senses. If our long term goal is to model
human competence in language comprehension, it would make sense to focus on tasks
at which humans are naturally competent. Dictionary-independent tasks such as lexical
substitution or textual entailment may be the right steps in this direction.
5.3 Can We Improve Our Results Using Better Semantic Classes?
In order to get an upper bound for our approach, we searched for the best set of semantic
classes specific to each data set using the following greedy algorithm.
122
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Table 6
The performance of the noisy channel model with the best set of semantic classes picked for each
data set. The columns give the data set, the number of classes, maximum possible score if the
model always picks the correct class, percentage of times it actually picks the correct class, and
its fine-grained accuracy.
Data Set NumClass MaxScore CorrClass Score
senseval2 23 89.2 88.8 80.1
senseval3 29 87.2 87.4 77.4
semeval07 12 84.9 89.9 79.2
Algorithm 4
1. Initialize all synsets to be in a single ?default? semantic class.
2. For each synset, compute the following score: the WSD accuracy achieved
if that synset and all its descendants are split into a new semantic class.
3. Take the synset with the highest score and split that synset and its
descendants into a new semantic class.
4. Repeat steps 2 and 3 until the WSD accuracy can no longer be improved.
Algorithm 4 was run for each of the three data sets, which resulted in three different
sets of semantic classes. The noisy channel model was applied with the best set of
semantic classes for each data set. Table 6 summarizes the results. Note that these results
are not predictive of out-of-sample accuracy because Algorithm 4 picks a specific set of
semantic classes optimal for a given data set. But the results do indicate that a better
set of semantic classes may lead to significantly better WSD accuracy. In particular
each result in Table 6 is significantly higher than previously reported supervised or
unsupervised results.
How to construct a good set of semantic classes that balance specificity and identi-
fiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised
solution using feature-based clustering that tries to maintain feature?class coherence.
Non-parametric Bayesian approaches such as Teh et al (2006) applied to context distri-
butions could reveal latent senses in an unsupervised setting.
6. Related Work
For a general overview of different approaches to WSD, see Navigli (2009) and
Stevenson (2003). The Senseval and SemEval workshops (Cotton et al 2001; Mihalcea
and Edmonds 2004; Agirre, Ma`rquez, and Wicentowski 2007) are good sources of recent
work, and have been used in this article to benchmark our results.
Generative models based on the noisy channel framework have previously been
used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation
(Brown et al 1990), question answering (Echihabi and Marcu 2003), spelling correction
(Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among
others. To our knowledge our work is the first application of the noisy channel model
to unsupervised word sense disambiguation.
123
Computational Linguistics Volume 36, Number 1
Using statistical language models based on large corpora for WSD has been ex-
plored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this
article see Yuret (2008); for a more general review of statistical language modeling see
Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001).
Grouping similar senses into semantic classes for WSD has been explored in previ-
ous work. Senses that are similar have been identified using WordNet relations (Peters,
Peters, and Vossen 1998; Crestan, El-Be`ze, and De Loupy 2001; Kohomban and Lee
2005), discourse domains (Magnini et al 2003), annotator disagreements (Chklovski
and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE
(Dolan 1994), and ODE (Navigli 2006).
Ciaramita and Altun (2006) build a supervised HMM tagger using ?supersenses,?
essentially the 25 WordNet noun categories we have used in our first experiment in
addition to 15 verb categories similarly defined. They report a supersense precision of
67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for
Senseval-3 nouns. However, the results are not directly comparable because they do not
report the noun and verb scores separately or calculate the corresponding fine-grained
score to compare with other Senseval-3 results.
Kohomban and Lee (2007) go beyond the WordNet categories based on lexicogra-
pher files and experiment with clustering techniques to construct their semantic classes.
Their classes are based on local features from sense-labeled data and optimize feature?
class coherence rather than adhering to the WordNet hierarchy. Their supervised system
achieves an accuracy of 74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns.
The systems mentioned so far are supervised WSD systems. Agirre and Martinez
(2004) explore the large-scale acquisition of sense-tagged examples from the Web and
train supervised, minimally supervised (requiring sense bias information from hand-
tagged corpora, similar to our system), and fully unsupervised WSD algorithms using
this corpus. They report good results on the Senseval-2 lexical sample data compared to
other unsupervised systems. Martinez, de Lacalle, and Agirre (2008) test a similar set of
systems trained using automatically acquired corpora on Senseval-3 nouns. Their mini-
mally supervised system obtains 63.9% accuracy on polysemous nouns from Senseval-3
(corresponding to 71.86% on all nouns).
7. Contributions
We have introduced a new generative probabilistic model based on the noisy channel
framework for unsupervised word sense disambiguation. The main contribution of this
model is the reduction of the word sense disambiguation problem to the estimation of
two distributions: the distribution of words used to express a given sense, and the dis-
tribution of words that appear in a given context. In this framework, context similarity
is determined by the distribution of words that can be placed in the given context. This
replaces the ad hoc contextual feature design process by a statistical language model,
allowing the advances in language modeling and the availability of large unlabeled
corpora to have a direct impact on WSD performance.
We have provided a detailed analysis of using coarse-grained semantic classes for
fine-grained WSD. The noisy channel model is a good fit for class-based WSD, where
the model decides on a coarse-grained semantic class instead of a fine-grained sense.
The chosen semantic class is then mapped to a specific sense based on the WordNet
ordering during evaluation. We show that the potential loss from using coarse-grained
classes is limited, and state-of-the-art performance is possible using only a few semantic
classes. We explore semantic classes at various levels of granularity and show that
124
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
the relationship between granularity and fine-grained accuracy is complex, thus more
work is needed to determine an ideal set of semantic classes.
In several experiments we compare the performance of our unsupervised WSD
system with the best systems from previous Senseval and SemEval workshops. We
consistently outperform any previously reported unsupervised results and achieve
comparable performance to the best supervised results.
Appendix A: Solutions for P(S|C)
To solve for P(S|C) using P(W|C) and P(W|S), we represent the first two as vectors: sj =
P(S = j|C = k) and wi = P(W = i|C = k), and the last one as a matrix: WSij = P(W =
i|S = j). Our problem becomes finding a solution to the linear equation w =WS? s.
Using the Moore?Penrose pseudoinverse, WS+, we find a solution s =WS+ ? w. This
solution minimizes the distance |WS? s? w|. There are two potential problems with
this pseudoinverse solution. First, it may violate the non-negativity and normalization
constraints of a probability distribution. Second, a maximum likelihood estimate should
minimize the cross entropy between WS? s and w, not the Euclidean distance. We
addressed the normalization problem using a constrained linear solver and the cross-
entropy problem using numerical optimization. However, our experiments showed the
difference in WSD performance to be less than 1% in each case. The pseudoinverse
solution, s =WS+ ? w, can be computed quickly and works well in practice, so this
is the solution that is used in all our experiments.
Appendix B: Estimating P(W|C)
Estimating P(W|C) for each context is expensive because the number of words that need
to be considered is large. The Web 1T data set contains 13.5 million unique words, and
WordNet defines about 150,000 lemmas. To make the computation feasible we needed
to limit the set of words for which P(W|C) needs to be estimated. We limited our set to
WordNet lemmas with the same part of speech as the target word. We further required
the word to have a non-zero count in WordNet sense frequencies. The inflection and
capitalization of each word W was automatically matched to the target word. As a
result, we estimated P(W|C) for about 10,000 words for each noun context and assumed
the other words had zero probability. The n-grams required for all the contexts were
listed, and their counts were extracted from the Web 1T data set in one pass. The P(W|C)
was estimated for all the words and contexts based on these counts. In the end, we only
used the 100 most likely words in each context for efficiency, as the difference in results
using the whole distribution was not significant. For more details on smoothing with a
large language model see Yuret (2008), although we did not see a significant difference
in WSD performance based on the smoothing method used.
Acknowledgments
This work was supported in part by
the Scientific and Technical Research
Council of Turkey (TU?BI?TAK Project
108E228). We would like to thank Peter
Turney, Rada Mihalcea, Diana McCarthy,
and the four anonymous reviewers
for their helpful comments and suggestions.
References
Agirre, E. and D. Martinez. 2004.
Unsupervised WSD based on
automatically retrieved examples:
The importance of bias. In Proceedings
of the Conference on Empirical Methods
in Natural Language Processing (EMNLP),
pages 25?32, Barcelona.
125
Computational Linguistics Volume 36, Number 1
Agirre, Eneko, Llu??s Ma`rquez, and Richard
Wicentowski, editors. 2007. Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
Prague.
Bahl, Lalit R., Frederick Jelinek, and Robert L.
Mercer. 1983. A maximum likelihood
approach to continuous speech
recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
5(2):179?190.
Banko, Michele and Eric Brill. 2001.
Scaling to very very large corpora
for natural language disambiguation.
In Proceedings of 39th Annual Meeting of
the Association for Computational Linguistics,
pages 26?33, Toulouse, France, July.
Association for Computational Linguistics.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram version 1. Linguistic Data
Consortium, Philadelphia. LDC2006T13.
Brill, Eric and Robert C. Moore. 2000. An
improved error model for noisy channel
spelling correction. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 286?293,
Hong Kong.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Chen, S. F. and J. Goodman. 1999. An
empirical study of smoothing techniques
for language modeling. Computer
Speech and Language, 13(4):359?394.
Chklovski, Timothy and Rada Mihalcea. 2003.
Exploiting agreement and disagreement
of human annotators for word sense
disambiguation. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing, pages 357?366,
Borovetz.
Ciaramita, Massimiliano and
Yasemin Altun. 2006. Broad-coverage
sense disambiguation and information
extraction with a supersense sequence
tagger. In Proceedings of the 2006
Conference on Empirical Methods in
Natural Language Processing,
pages 594?602, Sydney.
Cotton, Scott, Phil Edmonds,
Adam Kilgarriff, and Martha Palmer,
editors. 2001. SENSEVAL-2: Second
International Workshop on Evaluating Word
Sense Disambiguation Systems, Toulouse,
France.
Crestan, E., M. El-Be`ze, and C. De Loupy.
2001. Improving WSD with multi-level
view of context monitored by similarity
measure. In Proceedings of SENSEVAL-2:
Second International Workshop on Evaluating
Word Sense Disambiguation Systems,
Toulouse.
Daume III, Hal and Daniel Marcu. 2002.
A noisy-channel model for document
compression. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 449?456,
Philadelphia, PA.
Dolan, W. B. 1994. Word sense ambiguation:
clustering related senses. In Proceedings
of the 15th conference on Computational
Linguistics, pages 05?09, Kyoto.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 16?23,
Sapporo.
Fellbaum, Christiane, editor. 1998.Wordnet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Goodman, Joshua. 2001. A bit of progress in
language modeling. Computer Speech and
Language, 15:403?434.
Hawker, Tobias. 2007. Usyd: WSD and lexical
substitution using the Web1t corpus. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 446?453,
Prague.
Jansen, B. J., A. Spink, and A. Pfaff. 2000.
Linguistic aspects of Web queries. In
Proceedings of the ASIS Annual Meeting,
pages 169?176, Chicago, IL.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2005. Learning semantic
classes for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL?05), pages 34?41, Ann Arbor, MI.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2007. Optimizing classifier
performance in word sense disambiguation
by redefining word sense classes. In
Proceedings of the International Joint
Conference on Artificial Intelligence,
pages 1635?1640, Hyderabad.
Kucera, Henry and W. Nelson Francis. 1967.
Computational Analysis of Present-Day
American English. Brown University Press,
Providence, RI.
Magnini, B., C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2003. The role of domain
information in word sense disambiguation.
126
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
Natural Language Engineering,
8(04):359?373.
Martinez, D., O. Lopez de Lacalle, and
E. Agirre. 2008. On the use of automatically
acquired examples for all-nouns
word sense disambiguation. Journal
of Artificial Intelligence Research, 33:79?107.
Mihalcea, Rada and Phil Edmonds, editors.
2004. SENSEVAL-3: The Third International
Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona.
Navigli, Roberto. 2006. Meaningful clustering
of senses helps boost word sense
disambiguation performance. In
Proceedings of the 21st International
Conference on Computational Linguistics
and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 105?112, Sydney.
Navigli, Roberto. 2009. Word sense
disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Peters, W., I. Peters, and P. Vossen. 1998.
Automatic sense clustering in
EuroWordNet. In Proceedings of the
International Conference on Language
Resources and Evaluation, pages 409?416,
Granada.
Rosenfeld, Ronald. 2000. Two decades
of statistical language modeling:
Where do we go from here? Proceedings
of the IEEE, 88:1270?1278.
Shannon, Claude Elwood. 1948. A
mathematical theory of communication.
The Bell System Technical Journal,
27:379?423, 623?656.
Stevenson, Mark. 2003.Word Sense
Disambiguation: The Case for Combinations
of Knowledge Sources. CSLI, Stanford, CA.
Teh, Y. W., M. I. Jordan, M. J. Beal, and
D. M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical
Association, 101(476):1566?1581.
Yarowsky, David. 1992. Word sense
disambiguation using statistical
models of Roget?s categories trained
on large corpora. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 454?460, Nantes.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation across
diverse parameter spaces.Natural Language
Engineering, 8(4):293?310.
Yuret, Deniz. 2004. Some experiments
with a Naive Bayes WSD system. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 265?268,
Barcelona.
Yuret, Deniz. 2007. KU: Word sense
disambiguation by substitution. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 207?214, Prague.
Yuret, Deniz. 2008. Smoothing a tera-word
language model. In Proceedings of
ACL-08: HLT, Short Papers,
pages 141?144, Columbus, OH.
127

CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 223?227
Manchester, August 2008
Discriminative vs. Generative Approaches in Semantic Role Labeling
Deniz Yuret
Koc? University
dyuret@ku.edu.tr
Mehmet Ali Yatbaz
Koc? University
myatbaz@ku.edu.tr
Ahmet Engin Ural
Koc? University
aural@ku.edu.tr
Abstract
This paper describes the two algorithms
we developed for the CoNLL 2008 Shared
Task ?Joint learning of syntactic and se-
mantic dependencies?. Both algorithms
start parsing the sentence using the same
syntactic parser. The first algorithm
uses machine learning methods to identify
the semantic dependencies in four stages:
identification and labeling of predicates,
identification and labeling of arguments.
The second algorithm uses a generative
probabilistic model, choosing the seman-
tic dependencies that maximize the proba-
bility with respect to the model. A hybrid
algorithm combining the best stages of
the two algorithms attains 86.62% labeled
syntactic attachment accuracy, 73.24% la-
beled semantic dependency F1 and 79.93%
labeled macro F1 score for the combined
WSJ and Brown test sets1.
1 Introduction
In this paper we describe the system we developed
for the CoNLL 2008 Shared Task (Surdeanu et al,
2008). Section 2 describes our approach for iden-
tifying syntactic dependencies. For semantic role
labeling (SRL), we pursued two independent ap-
proaches. Section 3 describes our first approach,
where we treated predicate identification and la-
beling, and argument identification and labeling as
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1These numbers are slightly higher than the official results
due to a small bug in our submission.
four separate machine learning problems. The fi-
nal program consists of four stages, each stage tak-
ing the answers from the previous stage as given
and performing its own identification or labeling
task based on a model generated from the train-
ing set. Section 4 describes our second approach
where we used a generative model based on the
joint distribution of the predicate, the arguments,
their labels and the syntactic dependencies con-
necting them. Section 5 summarizes our results
and suggests possible improvements.
2 Syntactic dependencies
We used a non-projective dependency parser based
on spanning tree algorithms. The parameters were
determined based on the experimental results of
the English task in (McDonald et al, 2005), i.e. we
used projective parsing and a first order feature set
during training. Due to the new representation of
hyphenated words in both training and testing data
of our shared task and the absence of the gold part
of speech (GPOS) column in the test data, the for-
mat of the CoNLL08 shared task is slightly differ-
ent from the format of the CoNLL05 shared task,
which is supported by the McDonald?s parser. We
reformatted the data accordingly. The resulting la-
beled attachment score on the test set is 87.39% for
WSJ and 80.46% for Brown.
3 The 4-stage discriminative approach
Our first approach to SRL consists of four distinct
stages: (1) predicate identification, (2) predicate
labeling, (3) argument identification, and (4) argu-
ment labeling.
A discriminative machine learning algorithm is
trained for each stage using the gold input and out-
put values from the training set. The following sec-
223
tions describe the machine learning algorithm, the
nature of its input/output, and the feature selection
process for each stage. The performance of each
stage is compared to a most frequent class base-
line and analyzed separately for the two test sets
and for nouns and verbs. In addition we look at the
performance given the input from the gold data vs.
the input from the previous stage.
3.1 Predicate identification
The task of this stage is to determine whether a
given word is a nominal or a verb predicate using
the dependency-parsed input. As potential predi-
cates we only consider words that appear as a pred-
icate in the training data or have a corresponding
PropBank or NomBank XML file. The method
constructs feature vectors for each occurrence of
a target word in the training and test data. It as-
signs class labels to the target words in the training
data depending on whether a target word is a pred-
icate or not, and finally classifies the test data. We
experimented with combinations of the following
features for each word in a 2k + 1 word window
around the target: (1) POS(W): the part of speech
of the word, (2) DEP(W, HEAD(W)): the syntac-
tic dependency of the word, (3) LEMMA(W): the
lemma of the word, (4) POS(HEAD(W)): the part
of speech of the syntactic head.
We empirically selected the combination that
gives the highest accuracy in terms of the precision
and recall scores on the development data. The
method achieved its highest score when we used
features 1-3 for the target word and features 1-2 for
the neighbors in a [-3 +3] word window. TiMBL
(Daelemans et al, 2004) was used as the learning
algorithm.
Table 1 (4-stage, All1) shows the results of our
learning method on the WSJ and Brown test data.
The noun and verb results are given separately
(Verb1, Noun1). To distinguish the mistakes com-
ing from parsing we also give the results of our
method after the gold parse (4-stage-gold). Our re-
sults are significantly above the most frequent class
baseline which gives 72.3% on WSJ and 65.3% on
Brown.
3.2 Predicate labeling
The task of the second stage is deciding the correct
frame for a word given that the word is a predicate.
The input of the stage is 11-column data, where the
columns contain part of speech, lemma and syn-
tactic dependency for each word. The first stage?s
decision for the frame is indicated by a string in
the predicate column. The output of the stage is
simply the replacement of that string with the cho-
sen frame of the word. The chosen frame of the
word may be word.X, where X is a valid number
in PropBank or NomBank.
The statistics of the training data show that by
picking the most frequent frame, the system can
pick the correct frame in a large percent of the
cases. Thus we decided to use the most frequent
frame baseline for this stage. If the word is never
seen in the training, first frame of the word is
picked as default.
In the test phase, the results are as the follow-
ing; in the Brown data, assuming that the stage 1
is gold, the score is 80.8%, noting that 11% of the
predicates are not seen in the training phase. In
WSJ, the score based on gold input is 88.3%, and
only 5% of the predicates are not seen in the train-
ing phase. Table 1 gives the full results for Stage 2
(4-stage, Verb2, Noun2, All2).
3.3 Argument identification
The input data at this stage contains the syntac-
tic dependencies, predicates and their frames. We
look at the whole sentence for each predicate and
decide whether each word should be an argument
of that predicate or not. We mark the words we
choose as arguments indicating which predicate
they belong to and leave the labeling of the ar-
gument type to the next stage. Thus, for each
predicate-word pair we have a yes/no decision to
make.
As input to the learning algorithm we experi-
mented with representations of the syntactic de-
pendency chain between the predicate and the
argument at various levels of granularity. We
identified the syntactic dependency chain between
the predicate and each potential argument using
breadth-first-search on the dependency tree. We
tried to represent the chain using various subsets
of the following elements: the argument lemma
and part-of-speech, the predicate frame and part-
of-speech, the parts-of-speech and syntactic de-
pendencies of the intermediate words linking the
argument to the predicate.
The syntactic dependencies leading from the ar-
gument to the predicate can be in the head-modifier
or the modifier-head direction. We marked the di-
rection associated with each dependency relation
in the chain description. We also experimented
224
with using fine-grained and coarse-grained parts of
speech. The coarse-grained part of speech consists
of the first two characters of the Penn Treebank
part of speech given in the training set.
We used a simple learning algorithm: choose
the answer that is correct for the majority of the
instances with the same chain description from
the training set. Not having enough detail in the
chain description leaves crucial information out
that would help with the decision process, whereas
having too much detail results in bad classifica-
tions due to sparse data. In the end, neither the ar-
gument lemma, nor the predicate frame improved
the performance. The best results were achieved
with a chain description including the coarse parts
of speech and syntactic dependencies of each word
leading from the argument to the predicate. The
results are summarized in Table 1 (4-stage, Verb3,
Noun3, All3).
3.4 Argument labeling
The task of this stage is choosing the correct argu-
ment tag for a modifier given that it is modifying
a particular predicate. Input data format has ad-
ditional columns indicating which words are argu-
ments for which predicates. There are 54 possible
values for a labeled argument. As a baseline we
take the most frequent argument label in the train-
ing data (All1) which gives 37.8% on the WSJ test
set and 33.8% on the Brown test set.
The features to determine the correct label of an
argument are either lexical or syntactic. In a few
cases, they are combined. The following list gives
the set we have used. Link is the type of the syntac-
tic dependency. Direction is left or right, depend-
ing the location of the head and the modifier in the
sentence. LastLink is the type of the dependency
at the end of the dependency chain and firstLink
is type of the dependency at the beginning of the
dependency chain.
Feature1 : modifierStem + headStem
Feature2 : modifierStem + coarsePosModifier +
headStem + coarsePosHead + direction
Feature3 : coarsePosModifier + headPos +
firstLink + lastLink + direction
Feature4: modifierStem + coarsePosModifier
The training phase includes building simple his-
tograms based on four features. Feature1 and Fea-
ture2 are sparser than the other two features and
are better features as they include lexical informa-
tion. Last two features are less sparse, covering
most of the development data, i.e. their histograms
give non-zero values in the development phase. In
order to match all the instances in the development
and use the semantic information, a cascade of the
features is implemented similar to the one done by
Gildea and Jurafsky(2002), although no weighting
and a kind of back-off smoothing is used. First,
a match is searched in the histogram of the first
feature, if not found it is searched in the following
histogram. After a match, the most frequent argu-
ment with that match is returned. Table 1 gives the
performance (4-stage, Verb4, Noun4, All4).
4 The generative approach
One problem with the four-stage approach is that
the later stages provide no feedback to the earlier
ones. Thus, a frame chosen because of its high
prior probability will not get corrected when we
fail to find appropriate arguments for it. A gen-
erative model, on the other hand, does not suffer
from this problem. The probability of the whole
assignment, including predicates, arguments, and
their labels, is evaluated together and the highest
probability combination is chosen.
4.1 The generative model
Figure 1: The graphical model depicting the con-
ditional independence assumptions.
Our generative model specifies the distribution
of the following random variables: P is the lemma
(stem+pos) of a candidate predicate. F is the
frame chosen for the predicate (could be null). A
i
is the argument label of word i with respect to a
given predicate (could be null). W
i
is the lemma
(stem+pos) of word i. L
i
is the syntactic depen-
dency chain leading from word i to the given pred-
icate (similar to Section 3.3).
We consider each word in the sentence as a can-
didate predicate and use the joint distribution of the
above variables to find the maximum probability F
225
WSJ Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All4
4-stage 97.1 85.5 85.7 71.7 84.6 78.4 61.1 49.4 90.6 81.8 76.6 63.5
generative 96.1 88.4 83.4 74.0 82.8 79.5 69.8 63.2 89.0 83.6 77.4 69.2
4-stage-gold 97.4 88.3 95.2 82.7 85.2 92.7 70.5 81.9 91.1 90.5 86.0 82.4
generative-gold 96.3 92.6 91.1 88.0 83.4 95.5 80.7 86.9 89.4 94.0 86.7 87.5
hybrid 97.1 89.3 85.7 74.7 84.6 80.9 70.9 64.0 90.6 84.9 79.5 70.2
Brown Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All4
4-stage 93.0 74.5 78.9 59.0 74.4 58.6 52.3 38.8 86.0 68.6 72.8 54.3
generative 91.4 71.7 76.1 60.0 70.8 59.3 54.0 45.3 83.1 66.6 69.6 55.7
4-stage-gold 93.0 80.8 93.7 73.2 75.7 80.3 70.1 70.5 86.5 80.8 88.2 72.4
generative-gold 91.6 80.6 85.8 78.05 71.2 85.9 70.5 75.1 83.5 82.6 81.8 77.1
hybrid 93.0 73.3 78.9 60.4 74.4 62.9 57.6 47.5 86.0 69.3 73.4 57.0
Table 1: The F1 scores for different datasets, models, stages, and predicate parts of speech. The ?Verb?
in the column heading indicates verbal predicates, ?Noun? indicates nominal predicates, ?All? indicates
all predicates. The numbers 1-4 in column headings indicate the 4 stages: (1) predicate identification, (2)
predicate labeling, (3) argument identification, (4) argument labeling. The gold results assume perfect
output from the previous stages. The highest number in each column is marked with boldface.
and A
i
labels given P , W
i
, and L
i
. The graphical
model in Figure 1 specifies the conditional inde-
pendence assumptions we make. Equivalently, we
take the following to be proportional to the joint
probability of a particular assignment:
Pr(F |P )
?
i
Pr(A
i
|F ) Pr(W
i
|FA
i
) Pr(L
i
|FA
i
)
4.2 Parameter estimation
To estimate the parameters of the generative model
we used the following methodology:
For Pr(F |P ) we use the maximum likelihood
estimate from the training data. As a consequence,
frames that were never observed in the training
data have zero probability. One exception is lem-
mas which have not been observed in the training
data, for which each frame is considered equally
likely.
For Pr(A
i
|F ) we also use the maximum like-
lihood estimate and normalize it using sentence
length. For a given argument label we find the
expected number of words in a sentence with that
label for frame F . We divide this expected num-
ber with the length of the given sentence to find
Pr(A
i
|F ) for a single word. Any leftover prob-
ability is given to the null label. If the sentence
length is shorter than the expected number of ar-
guments, all probabilities are scaled down propor-
tionally.
For the remaining two terms Pr(L
i
|F,A
i
) and
Pr(W
i
|F,A
i
) using the maximum likelihood esti-
mate is not effective because of data sparseness.
The arguments in the million word training data
contain about 16,000 unique words and 25,000
unique dependency chains. To handle the sparse-
ness problem we smoothed these two estimates us-
ing the part-of-speech argument distribution, i.e.
Pr(L
i
|POS, A
i
) and Pr(W
i
|POS, A
i
), where POS
represents the coarse part of speech of the predi-
cate.
5 Results and Analysis
Table 1 gives the F1 scores for the two models
(4-stage and generative), presented separately for
noun and verb predicates and the four stages of
predicate identification/labeling, argument identi-
fication/labeling. In order to isolate the perfor-
mance of each stage we also give their scores with
gold input. The rest of this section analyzes these
results and suggests possible improvements.
A hybrid algorithm: A comparison of the two
algorithms show that the 4-stage approach is su-
perior in predicate and verbal-argument identifica-
tion and the generative algorithm is superior in the
labeling of predicates and arguments and nominal-
argument identification. This suggests a hybrid al-
gorithm where we restrict the generative model to
take the answers for the better stages from the 4-
stage algorithm (Noun1, Verb1, Verb3) as given.
Tables 1 and 2 present the results for the hybrid
algorithm compared to the 4-stage and generative
models.
Parsing performance: In order to see the effect
of syntactic parsing performance, we ran the hy-
brid algorithm starting with the gold parse. The
labeled semantic score went up to 78.84 for WSJ
and 67.20 for Brown, showing that better parsing
226
Data/algorithm Unlabeled Labeled
WSJ 4-stage 81.15 69.44
WSJ generative 81.01 73.66
WSJ hybrid 82.94 74.74
Brown 4-stage 76.91 58.76
Brown generative 73.76 59.05
Brown hybrid 77.22 60.80
Table 2: Semantic scores for the 4-stage, genera-
tive, and hybrid algorithms
can add about 4-6% to the overall performance.
Syntactic vs lexical features: Our algorithms
use two broad classes of features: information
from the dependency parse provides syntactic ev-
idence, and the word pairs themselves provide se-
mantic evidence for a possible relation. To iden-
tify their relative contributions, we experimented
with two modifications of the generative algo-
rithm: gen-l does not use the Pr(W
i
|FA
i
) term
and gen-w does not use the Pr(L
i
|FA
i
) term. gen-
l, using only syntactic information and the pred-
icate, gets a labeled semantic score of 70.97 for
WSJ and 58.83 for Brown, a relatively small de-
crease. In contrast gen-w, using only lexical infor-
mation gets 43.06 for WSJ and 33.17 for Brown
causing almost a 40% decrease in performance.
On the other hand, we find that the lexical fea-
tures are essential for certain tasks. In labeling the
arguments of nominal predicates, finding an exact
match for the lexical pair guarantees a 90% accu-
racy. If there is no exact match, the 4-stage algo-
rithm falls back on a syntactic match, which only
gives a 75% accuracy.
Future work: The hybrid algorithm shows the
strengths and weaknesses of our two approaches.
The generative algorithm allows feedback from the
later stages to the earlier stages and the 4-stage ma-
chine learning approach allows the use of better
features. One way to improve the system could be
by adding feedback to the 4-stage algorithm (later
stages can veto input coming from previous ones),
or adding more features to the generative model
(e.g. information about neighbor words when pre-
dicting F ). More importantly, there is no feedback
between the syntactic parser and the semantic role
labeling in our systems. Treating both problems
under the same framework may lead to better re-
sults.
Another property of both models is the indepen-
dence of the argument label assignments from each
other. Even though we try to control the number of
arguments of a particular type by adjusting the pa-
rameters, there are cases when we end up with no
assignments for a mandatory argument or multiple
assignments where only one is allowed. A more
strict enforcement of valence constraints needs to
be studied.
The use of smoothing in the generative model
was critical, it added about 20% to our final F1
score. This raises the question of finding more
effective smoothing techniques. In particular, the
jump from specific frames to coarse parts of speech
is probably not optimal. There may be interme-
diate groups of noun and verb predicates which
share similar semantic or syntactic argument dis-
tributions. Identifying and using such groups will
be considered in future work.
References
Daelemans, W., J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory-
Based Learner. Tilburg University.
Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245 288.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line Large-Margin Training of Dependency Parsers.
Ann Arbor, 100.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
227
