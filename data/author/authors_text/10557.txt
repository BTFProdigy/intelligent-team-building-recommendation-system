Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 429?437,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Acquisition of Japanese Unknown Morphemes
using Morphological Constraints
Yugo Murawaki Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
murawaki@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
We propose a novel lexicon acquirer that
works in concert with the morphological ana-
lyzer and has the ability to run in online mode.
Every time a sentence is analyzed, it detects
unknown morphemes, enumerates candidates
and selects the best candidates by comparing
multiple examples kept in the storage. When
a morpheme is unambiguously selected, the
lexicon acquirer updates the dictionary of the
analyzer, and it will be used in subsequent
analysis. We use the constraints of Japanese
morphology and effectively reduce the num-
ber of examples required to acquire a mor-
pheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and
improved the quality of morphological analy-
sis.
1 Introduction
Morphological analysis is the first step for most nat-
ural language processing applications. In Japanese
morphological analysis, segmentation is processed
simultaneously with the assignment of a part of
speech (POS) tag to each morpheme. Segmentation
is a nontrivial task in Japanese because it does not
delimit words by white-space.
Japanese morphological analysis has successfully
adopted dictionary-based approaches (Kurohashi et
al., 1994; Asahara and Matsumoto, 2000; Kudo et
al., 2004). In these approaches, a sentence is trans-
formed into a lattice of morphemes by searching a
pre-defined dictionary, and an optimal path in the
lattice is selected.
This area of research may be considered almost
completed, as previous studies reported the F-score
of nearly 99% (Kudo et al, 2004). When applied
to web texts, however, more errors are made due to
unknown morphemes. In previous studies, exper-
iments were performed on newspaper articles, but
web texts include slang words, informal spelling al-
ternates (Nishimura, 2003) and technical terms. For
example, the verb ????? (gugu-ru, to google) is
erroneously segmented into ???? (gugu) and ???
(ru).
One solution to this problem is to augment the
lexicon of the morphological analyzer by extracting
unknown morphemes from texts (Mori and Nagao,
1996). In the previous method, a morpheme extrac-
tion module worked independently of the morpho-
logical analyzer and ran in off-line (batch) mode.
It is inefficient because almost all high-frequency
morphemes have already been registered to the pre-
defined dictionary. Moreover, it is inconvenient
when applied to web texts because the web corpus
is huge and diverse compared to newspaper corpora.
It is not necessarily easy to build subcorpora before
lexicon acquisition. Suppose that we want to ana-
lyze whaling-related documents. It is unnecessary
and probably harmful to acquire morphemes that are
irrelevant to the topic. A whaling-related subcorpus
should be extracted from the whole corpus but it is
not clear how large it must be.
We propose a novel lexicon acquirer that works
in concert with the morphological analyzer and has
the ability to run in online mode. As shown in Fig-
ure 1, every time a sentence is analyzed, the lexicon
acquirer detects unknown morphemes, enumerates
429
text
Analyzer
JUMAN
(morph.
analyzer)
KNP
(parser)
analysis
DetectorEnumeratorSelector
accumulated
examples
hand-crafted
dictionary
automatically constructed
dictionary
update
lookup
Lexicon Acquirer
analysis
Figure 1: System architecture
candidates and selects the best candidates by com-
paring multiple examples kept in the storage. When
a morpheme is unambiguously selected, the lexicon
acquirer updates the automatically constructed dic-
tionary, and it will be used in subsequent analysis.
The proposed method is flexible and gives the sys-
tem more control over the process. We do not have
to limit the target corpus beforehand and the system
can stop whenever appropriate.
We use the constraints of Japanese morphology
that have already been coded in the morphological
analyzer. These constraints effectively reduce the
number of examples required to acquire an unknown
morpheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and im-
proved the quality of morphological analysis.
2 Japanese Morphology
In order to understand the task of lexicon acquisi-
tion, we briefly describe the Japanese morpholog-
ical analyzer JUMAN.1 We explain Japanese mor-
phemes in Section 2.1, morphological constraints in
Section 2.2, and unknown morpheme processing in
Section 2.3.
2.1 Morpheme
In JUMAN, the POS tagset consists of four ele-
ments: class, subclass, conjugation type and con-
jugation form. The classes are noun, verb, adjec-
tive and others. Noun has subclasses such as com-
mon noun, sa-group noun, proper noun, organiza-
1
http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman.html
tion, place, personal name. Verb and adjective have
no subclasses.
Verbs and adjectives among others change their
form according to the morphemes that occur after
them, which is called conjugation. Conjugable mor-
phemes are grouped by conjugation types such as
vowel verb, ra-row verb, i-type adjective and na-
type adjective. Each conjugable morpheme takes
one of conjugation forms in texts. It has an invari-
ant stem and an ending which changes according to
conjugation type and conjugation form.
In this paper, the tuple of class, subclass and con-
jugation type is referred to as a POS tag. For sim-
plicity, POS tags for nouns are called by their sub-
classes and those for verbs and adjectives by their
conjugation types.
There are two types of morphemes: abstract dic-
tionary entries, and examples or actual occurrences
in texts. An entry consists of a stem and a POS tag
while an example consists of a stem, a POS tag and
a conjugation form. For example, the entry of the
ra-row verb ???? (hashi-ru, to run) can be repre-
sented as
(??? (hashi), ra-row verb),
and their examples ???? (hashi-ra) and ????
(hashi-ri) as
(??? (hashi), ra-row verb, imperfective),
and
(??? (hashi), ra-row verb, plain continu-
ative)
respectively. As nouns do not conjugate, the entry
of the sa-group noun ???? (kibou, hope) can be
represented as
(???? (kibou), sa-group noun)
and its sole example form is
(???? (kibou), sa-group noun, NIL).
2.2 Morphological Constraints
Japanese is an agglutinative language. Depending
on its grammatical roles, a morpheme is followed by
a sequence of grammatical suffixes, auxiliary verbs
and particles, and the connectivity of these elements
is bound by morphological constraints. For exam-
ple, the particle ??? (wo, accusative case) can fol-
low a verb with the conjugation form of plain contin-
uative, as in ????? (hashi-ri-wo, running-ACC),
430
but it cannot follow an imperfective verb (?*????
(*hashi-ra-wo)).
These constraints are used by JUMAN to reduce
the ambiguity. They can be also used in lexicon ac-
quisition.
2.3 Unknown Morpheme Processing
Given a sentence, JUMAN builds a lattice of mor-
phemes by searching a pre-defined dictionary, and
then selects an optimal path in the lattice. To han-
dle morphemes that cannot be found in the dictio-
nary, JUMAN enumerates unknown morpheme can-
didates using character type-based heuristics, and
adds them to the morpheme lattice. Unknown mor-
phemes are given the special POS tag ?undefined,?
which is treated as noun.
Character type-based heuristics are based on the
fact that Japanese is written with several different
character types such as kanji, hiragana and katakana,
and that the choice of character types gives some
clues on morpheme boundaries. For example, a se-
quence of katakana characters are considered as an
unknown morpheme candidate, as in ??????
(gu?guru, Google) out of ??????? (gu?guru-ga,
Google-NOM). Kanji characters are segmented per
character, which is sometimes wrong but prevents
error propagation.
These heuristics are simple and effective, but far
from perfect. They cannot identify mixed-character
morphemes, verbs and adjectives correctly. For ex-
ample, the verb ????? (gugu-ru, to google) is
wrongly divided into the katakana unknown mor-
pheme ???? (gugu) and the hiragana suffix ???
(ru).
3 Lexicon Acquisition
3.1 Task
The task of lexicon acquisition is to generate dictio-
nary entries inductively from their examples in texts.
Since the morphological analyzer provides a basic
lexicon, the morphemes to be acquired are limited
to those unknown to the analyzer.
In order to generate an entry, its stem and POS
tag need to be identified. Determining the stem of
an example is to draw the front and rear boundaries
in a character sequence in texts which corresponds
to the stem. The POS tag is selected from the tagset
given by the morphological analyzer.
3.2 System Architecture
Figure 1 shows the system architecture. Each sen-
tence in texts is processed by the morphological an-
alyzer JUMAN and the dependency parser KNP.2
JUMAN consults a hand-crafted dictionary and an
automatically constructed dictionary. KNP is used
to form a phrasal unit called bunsetsu by chunking
morphemes.
Every time a sentence is analyzed, the lexicon
acquirer receives the analysis. It detects examples
of unknown morphemes and keeps them in storage.
When an entry is unambiguously selected, the lex-
icon acquirer updates the automatically constructed
dictionary, and it will be used in subsequent analy-
sis.
3.3 Algorithm Overview
The process of lexicon acquisition has four phases:
detection, candidate enumeration, aggregation and
selection. First the analysis is scanned to detect ex-
amples of unknown morphemes. For each exam-
ple, one or more candidates for dictionary entries are
enumerated. It is added to the storage, and multiple
examples in the storage that share the candidates are
aggregated. They are compared and the best candi-
date is selected from it.
Take the ra-row verb ????? (gugu-ru) for ex-
ample. Its example ????????? (gugu-tte-
mi-ta, to have tried to google) can be interpreted in
many ways as shown in Figure 2. Similarly, multi-
ple candidates are enumerated for another example
??????? (gugu-ru-no-ha, to google-TOPIC). If
these examples are compared, we can see that the
ra-row verb ????? (gugu-ru) can explain them.
3.4 Suffixes
Morphological constraints are used for candidate
enumeration. Since they are coded in JUMAN, we
first transform them into a set of strings called suf-
fixes. A suffix is created by concatenating the end-
ing of a morpheme (if any) and subsequent ancillary
morphemes. Each POS tag is associated with a set
of suffixes, as shown in Table 1. This means that a
stem can be followed by one of the suffixes specified
2
http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html
431
Table 1: Examples of suffixes
POS tag base form stem ending conjugation form1 suffixes
ra-row verb hashi-ru hashi
ra imperfective razu, ranaide
ri plain continuative riwo, riwomo
ru plain ru, rukawo
vowel verb akogare-ru akogare
? imperfective zu, naide
? plain continuative wo, womo
ru plain ru, rukawo
sa-group noun kibou kibou NIL wo wo, womoNIL suru suru, shitara
1 The conjugation form of a noun is substituted with the base form of its immediate
ancillary morpheme because nouns do not conjugate.
suffix
???????
google -CONT try-PAST
stem
stem
stem
suffix
suffix
[POS tags]
? ra-row verb
? wa-row verb
? ta-row verb
? ma-row verb
? vowel verb
? ta-row verb
? (EOB)
stem
Figure 2: Candidate enumeration
by its POS tag and cannot be followed by any other
suffix.
In preparation for lexicon acquisition, suffixes are
acquired from a corpus. We used a web corpus that
was compiled through the procedures proposed by
Kawahara and Kurohashi (2006). Suffixes were ex-
tracted from examples of registered morphemes and
were aggregated per POS tag.
We found that the number of suffixes did not con-
verge even in this large-scale corpus. It was because
ancillary morphemes included the wide variety of
auxiliary verbs and formal nouns. Alternatively, we
used the first five characters as a suffix. In the exper-
iments, we obtained 500 thousand unique suffixes
from 100 million pages. The number of POS tags
that corresponded to a suffix was 1.33 on average.
3.5 Unknown Morpheme Detection
The first step of lexicon acquisition is unknown mor-
pheme detection. Every time the analysis of a sen-
tence was given, the sequence of morphemes are
scanned, and suspicious points that probably repre-
sent unknown morphemes are detected.
Currently, we use the POS tag ?undefined? to de-
tect unknown morphemes. For example, the exam-
ple ????????? is detected because ????
is given ?undefined.? This simple method cannot
detect unknown morphemes if they are falsely seg-
mented into combinations of registered morphemes.
We leave the comprehensive detection of unknown
morphemes to future work.
3.6 Candidate Enumeration
For each example, one or more candidates for the
dictionary entry are enumerated. Each candidate is
represented by a combination of a front boundary
and the pair of a rear boundary and a POS tag.
The search range for enumeration is based on bun-
setsu phrases, which is created by chunking mor-
phemes. The range is at most the corresponding
bunsetsu and the two immediately preceding and
succeeding bunsetsu, which we found wide enough
to contain correct candidates.
The candidates for the rear boundary and the POS
tag are enumerated by string matching of suffixes as
shown in Figure 2. If a suffix matches, the start-
ing position of the suffix becomes a candidate for
the rear boundary and the suffix is mapped to one or
more corresponding POS tags.
In addition, the candidates for the front and
rear boundaries are enumerated by scanning the se-
quence of morphemes. The boundary markers we
use are
? punctuations,
? grammatical prefixes such as ??? (go-, hon-
orific prefix), for front boundaries,
432
? grammatical suffixes such as ??? (-sama, hon-
orific title), for rear boundaries, and
? bunsetsu boundaries given by KNP.
Each rear boundary candidate whose correspond-
ing POS tag is not decided is given the special tag
?EOB? (end-of-bunsetsu). This means that no suf-
fix is attached to the candidate. Since nouns, vowel
verbs and na-type adjectives can appear in isolation,
it will be expanded to these POS tags when selecting
the best POS tag.
3.7 Aggregation of Examples
Selection of the best candidate is done by compar-
ing multiple examples. Each example is added to
the storage, and then examples that possibly repre-
sent the same entry with it are extracted from the
storage. Examples aggregated at this phase share the
front boundary but may be unrelated to the example
in question. They are pruned in the next phase.
In order to manage examples efficiently, we im-
plement a trie. The example is added to the trie for
each front boundary candidate. The key is the char-
acter sequence determined by the front boundary
and the leftmost rear boundary. To retrieve examples
that share the front boundary with it, we check every
node in the path from the root to the node where it is
stored, and collect examples stored in each node.
3.8 Selection
The best candidate is selected by identifying the
front boundary, the rear boundary and the POS tag
in this order. Starting from the rightmost front
boundary candidate, multiple rear boundary candi-
dates that share the front boundary are compared and
some are dropped. Then starting from the leftmost
surviving rear boundary candidate, the best POS tag
is selected from the examples that share the stem.
If the selected candidate satisfies simple termination
conditions, it is added to the dictionary and the ex-
amples are removed from the storage.
For each front boundary candidate, some inappro-
priate rear boundary candidates are dropped by ex-
amining the inclusion relation between the examples
of a pair of candidates. The assumption behind this
is that an appropriate candidate can interpret more
examples than incorrect ones. Let p and q be a pair
of the candidates for the rear boundary, and R
p
and
R
q
be the sets of examples for which p and q are
enumerated. If p is a prefix of q and p is the correct
stem, then R
q
must be contained in R
p
. In practice
we loosen this condition, considering possible errors
in candidate enumeration
For each stem candidate, the appropriate POS tag
is identified. Similarly to rear boundary identifica-
tion, POS identification is done by checking inclu-
sion relation.
If the POS tag is successfully disambiguated, sim-
ple termination conditions is checked to prevent the
accidental acquisition of erroneous candidates. The
first condition is that the number of unique conjuga-
tion forms that appear in the examples should be 3 or
more. If the candidate is a noun, it is substituted with
the number of the unique base forms of their imme-
diate ancillary morphemes. The second condition is
that the front boundaries of some examples are de-
cided by clear boundary markers such as punctua-
tions and the beginning of sentence. This prevents
oversegmentation. For example, the stem candidate
?*??? (*sengumi) is always enumerated for exam-
ples of ????? (Shingengumi, a historical organi-
zation) since ??? (shin-, new) is a prefix. This can-
didate is not acquired because ?*??? (*sengumi)
does not occur alone and is always accompanied by
??? (shin-). Thresholds are chosen empirically.
3.9 Decompositionality
Since a morpheme is extracted from a small num-
ber of examples, it is inherently possible that the ac-
quired morpheme actually consists of two or more
morphemes. For example, the noun phrase ???
???? (karyuu-taipu, granular type) may be ac-
quired as a morpheme before ???? (karyuu, gran-
ule) is extracted. To handle this phenomenon, it
is checked at the time of acquisition whether the
new morpheme (kairyuu) can decompose registered
morphemes (kairyuu-taipu). If found, a composite
?morpheme? is removed from the dictionary.
Currently we leave the decompositionality check
to the morphological analyzer. Possible compounds
are enumerated by string matching and temporar-
ily removed from the dictionary. Each candidate
is analyzed by the morphological analyzer and it is
checked whether the candidate is divided into a com-
bination of registered morphemes. If not, the candi-
date is restored to the dictionary.
433
Table 2: Statistical information per query
query
number of number of number of number of number of
sentences affected acquired correct examples1
sentences morphs morphs
(ratio) (precision)
???? 135,379 2,444 293 290 4
(whaling issue) (1.81%) (99.0%)
??????? 74,572 775 107 105 4
(baby hatch) (1.04%) (98.1%)
?????? 195,928 6,259 913 907 4
(JASRAC) (3.19%) (99.3%)
???? 77,962 12,012 243 238 5
(tsundere) (15.4%) (97.4%)
????? 78,922 3,037 114 107 9
(agaricus) (3.85%) (93.9%)
1 The median number of examples used for acquisition.
4 Experiments
4.1 Experimental Design
We used the default dictionary of the morphological
analyzer JUMAN as the initial lexicon. It contained
30 thousand basic morphemes. If spelling variants
were expanded and proper nouns were counted, the
total number of morphemes was 120 thousands.
We used domain-specific corpora as target texts
because efficient acquisition was expected. If target
texts shared a topic, relevant unknown morphemes
were used frequently. In the experiments, we used
search engine TSUBAKI (Shinzato et al, 2008) and
casted the search results as domain-specific corpora.
For each query, our system sequentially read pages
from the top of the result and acquired morphemes.
We terminated the acquisition at the 1000th page
and analyzed the same 1000 pages with the aug-
mented lexicon. The queries used were ????
?? (whaling issue), ????????? (baby hatch),
???????? (JASRAC, a copyright collective),
?????? (tsundere, a slang word) and ?????
?? (agaricus).
4.2 Evaluation Measures
The proposed method is evaluated by measuring the
accuracy of acquired morphemes and their contri-
bution to the improvement of morphological analy-
sis. A morpheme is considered accurate if both seg-
mentation and the POS tag are correct. Note that
segmentation is a nontrivial problem for evaluation.
In fact, the disagreement over segmentation criteria
was considered one of the main reasons for reported
errors by Nagata (1999) and Uchimoto et al (2001).
It is difficult to judge whether a compound term
should be divided because there is no definite stan-
dard for morpheme boundaries in Japanese. For ex-
ample, ?????? (minku-kujira, minke whale) can
be extracted as a single morpheme or decomposed
into ????? and ??.? While segmentation is an
open question in Japanese morphological analysis,
?correct? segmentation is not necessarily important
for applications using morphological analysis. Even
if a noun is split into two or more morphemes in
morphological analysis, they are chunked to form
a phrasal unit called bunsetsu in dependency pars-
ing, and to extract a keyword (Nakagawa and Mori,
2002).
To avoid the decompositionality problem, we
adopted manual evaluation. We analyzed the tar-
get texts with both the initial lexicon and the aug-
mented lexicon. Then we checked differences be-
tween the two analyses and extracted sentences that
were affected by the augmentation. Among these
sentences, we evaluated randomly selected 50 sen-
tences per query. We checked the accuracy of seg-
mentation and POS tagging of each ?diff? block,
which is illustrated in Figure 3. The segmentation of
a block was judged correct unless morpheme bound-
aries were clearly wrong.
In the evaluation of POS tagging, we did not dis-
tinguish subclasses of noun3 such as common noun
3In the experiments, we regarded demonstrative pronouns as
434
Table 3: Examples of acquired morphemes
query examples
whaling issue ?????? (moratorium),????? (giant beaked whale),?? (bycatch)
baby hatch ??? (husband),??? (midwife),??? (to abandon),?? (to inquire)
JASRAC ??? (an organization),??? Q (a pop-rock band),?? (geek)
tsundere ??? (abbr. of Akihabara),??? (fujoshi, a slang word),??? (to be popular)
agaricus ??? (abbr. of suppliment),??? (aroma),?? (enhanced nutritional function)
Table 4: Evaluation of ?diff? blocks
segmentation POS tagging
query E ? C C ? C E ? E C ? E E ? C C ? C E ? E C ? E total
whaling issue 11 45 0 2 11 45 0 2 58
baby hatch 37 12 0 3 37 12 0 3 52
JASRAC 16 23 1 12 16 23 1 12 52
tsundere 17 39 0 1 17 39 0 1 57
agaricus 22 31 0 0 22 31 0 0 53
(Legend ? C: correct; E: erroneous)
???????????
)QQING KVCPFYGYKNNHKPFCNQV
?? WPFGHKPGF MCVCMCPC
? UWHHKZ? XGTDCNUWHHKZ
 ??? XGTD TCTQYXGTD
Figure 3: A ?diff? block in a sentence
and proper noun. The special POS tag ?undefined?
given by JUMAN was treated as noun.
4.3 Results
Table 2 summarizes statistical information per
query. The number of sentences affected by the
augmentation varied considerably (1.04%?15.4%).
The initial lexicon of the morphological analyzer
lacked morphemes that appeared frequently in some
corpora because morphological analysis had been
tested mainly with newspaper articles.
The precision of acquired morphemes was high
(97.4%?99.3%), and the number of examples used
for acquisition was as little as 4?9. These results are
astonishing considering that Mori and Nagao (1996)
ignored candidates that appeared less than 10 times
(because they were unreliable).
nouns because their morphological behaviors were the same as
those of nouns. Although demonstrative nouns are closed class
morphemes, their katakana forms such as ???? (this) were
acquired as nouns. The morphological analyzer assumed that
demonstrative pronouns were written in hiragana, e.g., ???,?
as they always are in a newspaper.
Table 3 shows some acquired morphemes. As
expected, the overwhelming majority were nouns
(93.0%?100%) and katakana morphemes (80.7%?
91.6%). Some were mixed-character morphemes
(????? and ????Q?), which cannot be recog-
nized by character-type based heuristics, and slang
words (????,? ???,? etc.) which did not ap-
pear in newspaper articles. Some morphemes were
spelling variants of those in the pre-defined dictio-
nary. Uncommon kanji characters were used in ba-
sic words (????? for ????? and ???? for
????) and katakana was used to change nuances
(????? for ????? and ????? for ????).
Table 4 shows the results of manual evaluation of
?diff? blocks. The overwhelming majority of blocks
were correctly analyzed with the augmented lexicon
(E ? C and C ? C). On the other hand, adverse
effects were observed only in a few blocks (C ?
E). In conclusion, acquired morphemes improve the
quality of morphological analysis.
4.4 Error Analysis
Some short katakana morphemes oversegmented
other katakana nouns. For example, ??????
(sa?ba?, server) was wrongly segmented by newly-
acquired ???? (sa?, sir) and preregistered ????
(ba?, bar). Neither the morphological analyzer and
the lexicon acquirer could detect this semantic mis-
match. Curiously, one example of ???? (sa?) was
actuallly part of ?????? (sa?ba?), which was erro-
435
 0
 200
 400
 600
 800
 1000
 0  100000  200000
 0
 8000
 16000
 24000
 32000
 40000
nu
m
. o
f a
cq
ui
re
d 
m
or
ph
em
es
nu
m
. o
f e
xa
m
pl
es
num. of sentences
acquired morphemes
stored examples
acquired morphemes in re-analysis
Figure 4: Process of online acquisition
neously segmented when extracting sentences from
HTML.
The katakana adjective ???? (i-i, good), a
spelling variant of the basic morpheme ???,? was
falsely identified as a noun because its ending ???
was written in katakana. The morphological ana-
lyzer, and hence the lexicon acquirer, assume that
the ending of a verb or adjective is written in hi-
ragana. This assumption is reasonable for stan-
dard Japanese, but does not always hold when we
analyze web texts. In order to recognize uncon-
ventional spellings that are widely used in web
texts (Nishimura, 2003), more flexible analysis is
needed.
4.5 Discussion
It is too costly or impractical to calculate the re-
call of acquisition, or the ratio of the number of ac-
quired morphemes against the total number of un-
known morphemes because it requires human judges
to find undetected unknown morphemes from a large
amount of raw texts.
Alternatively, we examined the ratio against the
number of detected unknown morphemes. Figure 4
shows the process of online acquisition for the query
?JASRAC.? The monotonic increase of the num-
bers of acquired morphemes and stored examples
suggests that the vocabulary size did not converge.
The number of occurrences of acquired morphemes
in re-analysis was approximately the same with the
number of examples kept in the storage during ac-
quisition. This means that, in terms of frequency of
occurrence, about half of unknown morphemes were
acquired. Most unknown morphemes belong to the
?long tail? and the proposed method seems to have
seized a ?head? of the long tail.
Although some previous studies emphasized cor-
rect identification of low frequency terms (Nagata,
1999; Asahara and Matsumoto, 2004), it is no longer
necessary because very large scale web texts are
available today. If a small set of texts needs to
be analyzed with high accuracy, we can incorporate
similar texts retrieved from the web, to increase the
number of examples of unknown morphemes. The
proposed method can be modified to check if un-
known morphemes detected in the initial set are ac-
quired and to terminate whenever sufficient acquisi-
tion coverage is achieved.
5 Related Work
Since most languages delimit words by white-space,
morphological analysis in these languages is to seg-
ment words into morphemes. For example, Mor-
pho Challenge 2007 (Kurimo et al, 2007) was eval-
uations of unsupervised segmentation for English,
Finnish, German and Turkish.
While Japanese is an agglutinative language,
other non-segmented languages such as Chinese and
Thai are analytic languages. Among them, Chinese
has been a subject of intensive research. Peng et
al. (2004) integrated new word detection into word
segmentation. They detected new words by comput-
ing segment confidence and re-analyzed the inputs
with detected words as features.
The Japanese language is unique in that it is writ-
ten with several different character types. Heuris-
tics widely used in unknown morpheme process-
ing are based on character types. They were also
used as important clues in statistical methods. Na-
gata (1999) integrated a probabilistic unknown word
models into the word segmentation model. Uchi-
moto et al (2001) incorporated them as feature func-
tions of a Maximum Entropy-based morphological
analyzer. Asahara and Matsumoto (2004) used them
as a feature of character-based chunking of unknown
words using Support Vector Machines.
Mori (1996) extracted words from texts and esti-
mated their POSs using distributional analysis. The
appropriateness of a word candidate was measured
436
by the distance between probability distributions of
the candidate and a model. In this method, mor-
phological constraints were indirectly represented
by distributions.
Nakagawa and Matsumoto (2006) presented a
method for guessing POS tags of pre-segmented un-
known words that took into consideration all the oc-
currences of each unknown word in a document.
This setting is impractical in Japanese because POS
tagging is inseparable from segmentation.
6 Conclusion
We propose a novel method that augments the lexi-
con of a Japanese morphological analyzer by acquir-
ing unknown morphemes from texts in online mode.
Unknown morphemes are acquired with high accu-
racy and improve the quality of morphological anal-
ysis.
Unknown morphemes are one of the main sources
of error in morphological analysis when we analyze
web texts. The proposed method has the potential
to overcome the unknown morpheme problem, but
it cannot be achieved without recognizing or being
robust over various phenomena such as unconven-
tional spellings and typos. These phenomena are not
observed in newspaper articles but cannot be ignored
in web texts. In the future, we will work on these
phenomena.
Morphological analysis is now very mature. It
is widely applied as preprocessing for NLP appli-
cations such as parsing and information retrieval.
Hence in the future, we aim to use the proposed
method to improve the quality of these applications.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Procs. of COLING 2000, pages 21?27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Procs. of COLING 2004, pages
459?465.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Procs. of LREC-06, pages
1344?1347.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Procs. of EMNLP 2004,
pages 230?237.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop, pages
19?21.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Procs. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22?38.
Shinsuke Mori and Makoto Nagao. 1996. Word extrac-
tion from corpora and its part-of-speech estimation us-
ing distributional analysis. In Procs. of COLING 1996,
pages 1119?1122.
Masaaki Nagata. 1999. A part of speech estimation
method for Japanese unknown words using a statistical
model of morphology and context. In Procs. of ACL
1999, pages 277?284.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Procs. of COLING-ACL 2006, pages 705?
712.
Hiroshi Nakagawa and Tatsunori Mori. 2002. A sim-
ple but powerful automatic term extraction method. In
COLING-02 on COMPUTERM 2002, pages 29?35.
Yukiko Nishimura. 2003. Linguistic innovations and in-
teractional features of casual online communication in
Japanese. Journal of Computer-Mediated Communi-
cation, 9(1).
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Procs. of COLING
?04, pages 562?568.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access methodology. In
Procs. of IJCNLP-08, pages 189?196.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Procs. of EMNLP 2001, pages 91?99.
437
Coling 2010: Poster Volume, pages 876?884,
Beijing, August 2010
Semantic Classification of Automatically Acquired Nouns
using Lexico-Syntactic Clues
Yugo Murawaki
Graduate School of Informatics
Kyoto University
murawaki@nlp.kuee.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we present a two-stage ap-
proach to acquire Japanese unknown mor-
phemes from text with full POS tags as-
signed to them. We first acquire unknown
morphemes only making a morphology-
level distinction, and then apply semantic
classification to acquired nouns. One ad-
vantage of this approach is that, at the sec-
ond stage, we can exploit syntactic clues
in addition to morphological ones because
as a result of the first stage acquisition, we
can rely on automatic parsing. Japanese
semantic classification poses an interest-
ing challenge: proper nouns need to be
distinguished from common nouns. It
is because Japanese has no orthographic
distinction between common and proper
nouns and no apparent morphosyntactic
distinction between them. We explore
lexico-syntactic clues that are extracted
from automatically parsed text and inves-
tigate their effects.
1 Introduction
A dictionary plays an important role in Japanese
morphological analysis, or the joint task of
segmentation and part-of-speech (POS) tag-
ging (Kurohashi et al, 1994; Asahara and Mat-
sumoto, 2000; Kudo et al, 2004). Like Chi-
nese and Thai, Japanese does not delimit words
by white-space. This makes the first step of nat-
ural language processing more ambiguous than
simple POS tagging. Accordingly, morphemes in
a pre-defined dictionary compactly represent our
knowledge about both segmentation and POS.
One obvious problem with the dictionary-based
approach is caused by unknown morphemes,
or morphemes not defined in the dictionary.
Even though, historically, extensive human re-
sources were used to build high-coverage dictio-
naries (Yokoi, 1995), texts other than newspa-
per articles, in particular web pages, contain a
large number of unknown morphemes. These un-
known morphemes often cause segmentation er-
rors. For example, morphological analyzer JU-
MAN 6.01 wrongly segments the phrase ????
??? (saQporo eki, ?Sapporo Station?), where ?
????? (saQporo) is an unknown morpheme,
as follows:
??? (sa, noun-common, ?difference?),
??? (Q, UNK), ??? (po, UNK),
??? (ro, noun-common, ?sumac?) and
??? (eki, noun-common, ?station?),
where UNK refers to unknown morphemes auto-
matically identified by the analyzer. Such an er-
roneous sequence has disastrous effects on appli-
cations of morphological analysis. For example, it
can hardly be identified as a LOCATION in named
entity recognition.
One solution to the unknown morpheme prob-
lem is unknown morpheme acquisition (Mori and
Nagao, 1996; Murawaki and Kurohashi, 2008). It
is the task of automatically augmenting the dictio-
nary by acquiring unknown morphemes from text.
In the above example, the goal is to acquire the
morpheme ?????? (saQporo) with the POS
tag ?noun-location name.? However, unknown
morpheme acquisition usually adopts a coarser
POS tagset that only represents the morphology
level distinction among noun, verb and adjective.
This means that ?????? (saQporo) is acquired
as just a noun and that the semantic label ?loca-
tion name? remains to be assigned. The reason
only the morphology level distinction is made is
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
876
that the semantic level distinction cannot easily
be captured with morphological clues that are ex-
ploited in unknown morpheme acquisition.
In this paper, we investigate the remaining
problem and introduce the new task of seman-
tic classification that is to be applied to automat-
ically acquired nouns. In this task, we can ex-
ploit syntactic clues in addition to morphologi-
cal ones because, as a result of acquisition, we
can now rely on automatic parsing. For exam-
ple, since text containing ?????? (saQporo,
noun-unclassified) is correctly segmented, we can
extract not only the phrase ?saQporo station,? but
the tree fragment ?? go to saQporo,? and we can
determine its semantic label.
Japanese semantic classification poses an inter-
esting challenge: proper nouns need to be distin-
guished from common nouns. Like Chinese and
Thai, Japanese has no orthographic distinction be-
tween common and proper nouns as there is no
such thing as capitalization. In addition, there
seems no morphosyntactic (i.e. grammatical) dis-
tinction between them.
In this paper, we explore lexico-syntactic clues
that can be extracted from automatically parsed
text. We train a classification model on manually
registered nouns and apply it to automatically ac-
quired nouns. We then investigate the effects of
lexico-syntactic clues.
2 Semantic Classification Task
2.1 Two-Stage Approach to Unknown
Morpheme Acquisition
Our goal is to identify unknownmorphemes in un-
segmented text and assign POS tags to them. In
this section, we omit the details of boundary iden-
tification (segmentation) and review the Japanese
POS tagset to see why we propose a two-stage ap-
proach to assign full POS tags.
The Japanese POS tagset derives from tradi-
tional grammar. It is a mixture of several linguis-
tic levels: morphology, syntax and semantics. In
other words, information encoded in a POS tag
is more than how the morpheme behaves in a se-
quence of morphemes. In fact, POS tags given to
pre-defined morphemes are useful for applications
of morphological analysis, such as dependency
parsing (Kudo and Matsumoto, 2002), named en-
tity recognition (Asahara and Matsumoto, 2003;
Sasano and Kurohashi, 2008) and anaphora res-
olution (Iida et al, 2009; Sasano and Kurohashi,
2009). In these applications, POS tags are incor-
porated as features for models.
On the other hand, the mixed nature of the POS
tagset poses a challenge to unknown morpheme
acquisition. Previous approaches (Mori and Na-
gao, 1996; Murawaki and Kurohashi, 2008) di-
rectly or indirectly reply on morphology, or our
knowledge on how a morpheme behaves in a se-
quence of morphemes. This means that semantic
level distinction is difficult to make in these ap-
proaches, and in fact, is left unresolved. To be
specific, nouns are only distinguished from verbs
and adjectives but they have subcategories in the
original tagset. These are what we try to classify
acquired nouns into in this paper.
2.2 Semantic Labels
The Japanese noun subcategories may require an
explanation since they are different from the En-
glish ones (Marcus et al, 1993) in many re-
spects. Singular and mass nouns are not distin-
guished from plural nouns because Japanese has
no grammatical distinction between them. More
importantly for this paper, proper nouns have sub-
categories such as person name, location name
and organization name in addition to the distinc-
tion from common nouns. These subcategories
provide important information to named entity
recognition among other applications. For proper
nouns, we adopt these subcategories as semantic
labels in our task.
In contrast to proper nouns, common nouns
have only one subcategory ?common.? How-
ever, we consider that subcategories of common
nouns similar to those of proper nouns are use-
ful for, for example, anaphora resolution (Sasano
and Kurohashi, 2009). We adopt the ?categories?
of morphological analyzer JUMAN, with which
common nouns in its dictionary are annotated.
There are 22 ?categories? including PERSON,
ORGANIZATION and CONCEPT. We collapse
these ?categories? into coarser semantic labels
that roughly correspond to those for proper nouns.
To sum up, we define 9 semantic labels as shown
877
Table 1: List of semantic labels.
labels P/C sources1 manually registered nouns automatically acquired nouns
PSN-P
proper
subPOS:person name ?? (matsui, a surname) ??? (sayuri, a given name)???? (jo?ji, ?George?) ??? (kyoN, a nickname)
LOC-P subPOS:place name ?? (kyouto, ?Kyoto?) ??? (akiba, ?Akihabara?)??? (doitsu, ?Germany?) ???? (waikiki, ?Waikiki?)
ORG-P subPOS:organization name ?? (nichigin, a bank) ??? (matsuda, ?Mazda?)NHK (a broadcaster) ??? (yahu?, ?Yahoo?)
OTH-P subPOS:proper noun ?? (heisei, an era name) ???? (jipush??, ?Gypsy?)??? (surabu, ?Slav?)
PSN-C
common
category:PERSON ?? (seNsei, ?teacher?) ??? (merutomo, ?keypal?)???? (sutaQfu, ?staff?) ??? (n??to, ?NEET?)
LOC-C category:PLACE-?2 ?? (shokuba, ?office?) ??? (irori, ?hearth?)??? (kafe, ?cafe?) ?? (hojou, ?farm field?)
ORG-C category:ORGANIZATION ?? (seifu, ?government?) ??? (me?ka, ?manufacturer?)??? (ch??mu, ?team?) ?? (heisho, ?our office?)
ANI-C category:ANIMAL and ? (inu, ?dog?) ??? (chiwawa, ?Chihuahua?)category:ANIMAL-PART ? (kao, ?face?) ??? (maNta, ?manta?)
OTH-C other categories ?? (shuchou, ?argument?) ?? (jiNbei, a kind of clothing)? (makura, ?pillow?) ??? (chakumero, ?ringtone?)
1 A subPOS refers to a subcategory of noun. For example, PSN-P corresponds to the POS tag ?noun-person name?.
2 category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others.
in Table 1.
2.3 Related Tasks
A line of research is dedicated to identify un-
known morphemes with varying degrees of identi-
fication. Asahara and Matsumoto (2004) only fo-
cus on boundary identification (segmentation) of
unknown morphemes. Mori and Nagao (1996),
Nagata (1999) and Murawaki and Kurohashi
(2008) assign POS tags at the morphology level.
Uchimoto et al (2001) assign full POS tags but
unsurprisingly the accuracy is low. Nakagawa
and Matsumoto (2006) also assign full POS tags.
They address the fact that local information used
in previous studies is inherently insufficient and
present a method that uses global information,
in other words, takes into consideration all oc-
currences of each unknown word in a document.
They report an improvement in tagging proper
nouns in Japanese.
A related task is named entity recognition
(NER). It can handle a named entity longer than
a single morpheme and is usually formalized as a
chunking problem. Since Japanese does not de-
limit words by white-space, the unit of chunk-
ing can be a character (Asahara and Matsumoto,
2003; Kazama and Torisawa, 2008) or a mor-
pheme (Sasano and Kurohashi, 2008). In either
case, NER models encode the output of morpho-
logical analysis and therefore are affected by its
errors. In fact, Saito et al (2007) report that a ma-
jority of unknown named entities (those never ap-
pear in a training corpus) contain unknown mor-
phemes as their constituents and that NER models
perform poorly on them. A straightforward solu-
tion to this problem would be to acquire unknown
morphemes and to assign semantic labels to them.
Another related task is supersense tagging (Cia-
ramita and Johnson, 2003; Curran, 2005; Cia-
ramita and Altun, 2006). A supersense corre-
sponds to one of the 26 broad categories defined
by WordNet (Fellbaum, 1998). Each noun synset
is associated with a supersense. For example,
?chair? has supersenses PERSON, ARTIFACT
and ACT because it belongs to several synsets.
Since supersense tagging is studied in English,
it differs from our task in several respects. In En-
glish, the distinction between common and proper
nouns is clear. In fact, the tagging models can use
POS features even for unknown nouns. In addi-
tion, the syntactic behavior of English nouns is
different from that of Japanese nouns (Gil, 1987).
Definiteness is not marked in Japanese as it lacks
determiners (e.g. ?the? and ?a?), and Japanese has
no obligatory plural marking. On the other hand,
Japanese obligatorily uses numeral classifiers to
indicate the count of nouns, as in
(1) saN
three
satsu
CL
no
GEN
hoN
book
three volumes of books, or three books,
878
where ?satsu? is a numeral classifier for books. A
number together with its numeral classifier forms
a numeral quantifier. Numeral quantifiers would
be informative about the semantic categories of
nouns. Note that Japanese shares the above fea-
tures with Chinese and Thai. Our findings in this
paper may hold for these languages.
3 Proposed Method
3.1 Lexico-Syntactic Clues
In the task of semantic classification, we can ex-
ploit syntactic clues in addition to morpholog-
ical ones. As a result of unknown morpheme
acquisition, text containing acquired morphemes,
or former unknown morphemes, is correctly seg-
mented. Now we can treat automatic parsing as
(at least partly) reliable with regard to acquired
morphemes.
For noun X , we use the following sets of fea-
tures for classification.
call: noun phrase Y that appears in a pat-
tern like ?Y called X? and ?Y such as X ,? e.g.
?call:kuni? from
X
X
to
QT
iu
call
kuni
country
a country called X .
cf: predicate with a case marker with which it
takes X as an argument, e.g. ?cf:tooru:wo? from
X
X
wo
ACC
tooru
pass
? pass through X .
demo: demonstrative that modifies X , e.g.
?demo:kono? from ?kono X? (this X) and
?demo:doNna? from ?doNna X? (what kind of
X).
ncf1: noun phrase which X modifies with the
genitive case marker ?no,? e.g. ?ncf1:heya? from
X
X
no
GEN
heya
room
X?s room.
ncf2: noun phrase that modifies X with the
genitive case marker ?no,? e.g. ?ncf2:subete?
from
subete
all
no
GEN
X
X
all X .
suf: suffix or suffix-like noun that follows X ,
e.g. ?suf:saN? from ?X saN? (Mr./Ms. X) and
?suf:eki? from ?X eki? (X station).
Using automatically parsed text to extract syn-
tactic features has an advantage. Since no manual
annotation is necessary, we can utilize a huge raw
corpus. On the other hand, parsing errors are in-
evitable. However, we can circumvent this prob-
lem by using the constraints of Japanese depen-
dency structures: head-final and projective. The
simplest example is the second last element of a
sentence, which always depends on the last ele-
ment. With these constraints, we can focus on
syntactically unambiguous dependency pairs and
extract syntactic features accurately. We follow
Kawahara and Kurohashi (2001) to extract a pair
of an argument noun and a predicate (cf), and
Sasano et al (2004) to extract a pair of nouns con-
nected with the genitive case marker ?no? (ncf1
and ncf2).
Noun X can be part of a compound noun. We
leave it for named entity recognition. Except for
suf, we extract features only when X alone forms
a word. Similarly, we extract suf features only
when X and a suffix alone form a noun phrase.
For call, ncf1, and ncf2, we generalize
numerals within noun phrases. For ?hoN?
(book) in example 1, we extract the feature
?ncf2:<NUM>satsu.?
3.2 Instances for Classification
Now that features are extracted for each noun, the
question is how to combine them together to make
an instance for classification. One factor we need
to consider is polysemy: a noun can be a person
name in one context and a location name in an-
other. If we combine features extracted from the
whole corpus, they may represent several seman-
tic labels.
Modeling a mixture of semantic labels might
be a solution, but we do not take this approach on
the grounds that each occurrence of a noun corre-
sponds to a single semantic label.
In our strategy, we perform classification mul-
tiple times for each noun and aggregate the results
at the end. The features for each classification are
extracted from a relatively small subset of a cor-
pus where the noun is supposedly consistent in
879
terms of semantic labels. In the field of named
entity recognition, it is known that label consis-
tency holds strongly at the level of a document
and less strongly across different documents (Kr-
ishnan and Manning, 2006). Thus we start with a
document and gradually cluster related documents
until a sufficient number of features are obtained.
For the specific procedures we took in the experi-
ments, see Section 4.1.
3.3 Training Data
Following unknown morpheme acquisition (Mu-
rawaki and Kurohashi, 2008), we create training
data using manually registered nouns, for which
we can obtain correct semantic labels. We per-
form the same procedure as above to make in-
stances of registered nouns.
Some registered nouns are tagged with more
than one semantic label, which we call ?explicit
polysemy.? We drop them from the training data.
The remaining problem is ?implicit polysemy.?
Nouns are sometimes used with an uncovered
sense. In preliminary experiments, we found that
a typical case of implicit polysemy was that a
proper noun derived from a basic noun. To al-
leviate this problem, we use an NE tagger for fil-
tering. We run an NE tagger over a small portion
of the corpus and extract common nouns that are
frequently tagged as named entities. Then we re-
move these nouns from the training data.
We also drop nouns that appear extremely fre-
quently such as ??? (hito, ?person?), ??? (koto,
?thing?) and ??? (watashi, ?I?2). Since acquired
nouns to be classified are typically low frequency
morphemes, they would not behave similarly to
these basic nouns.
3.4 Classifier
To assign a semantic label to each instance, we use
a multiclass discriminative classifier. The input it
takes is an instance that is represented by a feature
vector x ? Rd. The output is one semantic label
y ? Y , where Y is the set of semantic labels.
We use a linear classifier. It has a weight vector
wy ? Rd for each y and outputs y that maximizes
2Japanese personal pronouns are treated as common
nouns because they show no special morphosyntactic behav-
ior.
the inner product of wy and x.
y = argmax
y
?wy, x?.
Several methods have been proposed to esti-
mate weight vector wy from training data. We use
online algorithms because they are easy to imple-
ment and scale to huge instances. We try the Per-
ceptron family of algorithms.
4 Experiments
4.1 Settings
We used JUMAN for morphological analysis and
KNP3 for dependency parsing. The dictionary
of JUMAN was augmented with automatically
acquired morphemes (Murawaki and Kurohashi,
2008). The number of manually registered mor-
phemes was 120 thousands while there were
13,071 acquired morphemes, of which 12,615
morphemes were nouns.
We used a web corpus that was compiled
through the procedures proposed by Kawahara
and Kurohashi (2006). It consisted of 100 million
pages.
We first extracted features from the web cor-
pus. To keep the model size manageable, we
used 447,082 features that appeared more than
100 times in the corpus.
We constructed training data from manually
registered nouns and test data from automatically
acquired nouns. For each noun, we combined text
together until the number of features grew to more
than 100. We started with a single web page, then
merge pages that share a domain name and fi-
nally clustered texts across different domains. We
split the web corpus into 40 subcorpora and ap-
plied this procedure in parallel. We used Bayon4
for clustering domain texts. We sequentially read
texts and applied the repeated bisections cluster-
ing every time some 5,000 pages were appended.
The vectors for clustering were nouns, both regis-
tered and acquired, with their tf-idf scores. We ob-
tained 4,843,085 instances for 10,613 registered
nouns and 196,098 instances for 2,556 acquired
nouns.
3http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
4http://code.google.com/p/bayon/
880
Table 2: Results of semantic classification.
learning algorithms acquired nouns registered nouns
Averaged Perceptron 86.40% (432 / 500) 88.59% (123,113 / 138,971)
Passive-Aggressive 87.00% (435 / 500) 91.68% (127,407 / 138,971)
Confidence-Weighted 85.20% (426 / 500) 89.66% (124,604 / 138,971)
baseline1 69.60% (348 / 500) 79.14% (109,980 / 138,971)
1 assign OTH-C to all instances.
Table 3: Examples of aggregated instances.
acquired nouns instances labels
??? (hikaru, a person name) 84 PSN-P:58.33%, PSN-C:41.67%
??? (chiwawa, ?Chihuahua?) 128 ANI-C:54.69%, OTH-C:45.31%
???? (kamisaN, colloq. ?wife?) 131 PSN-C:100%
????? (rasubegasu, ?Las Vegas?) 136 LOC-P:97.06%, LOC-C:2.94%
???? (aQpuru, ?Apple/apple?) 187 ORG-P:63.10%, PSN-C:34.76%, OTH-C:2.14%
???? (merumaga, abbr. of ?mail magazine?) 1,622 OTH-C:99.32%, LOC-C:0.55%, PSN-C:0.06%
In order to handle polysemy, we evaluated se-
mantic classification on an instance-by-instance
basis. We randomly selected 500 instances from
the test data and manually assigned the correct la-
bels to them. For comparison purposes, we also
classified registered nouns. We split the training
data: 829 nouns or 138,971 instances for testing
and the rest for training.
We trained the model with three online learn-
ing algorithms, (1) the averaged version (Collins,
2002) of Perceptron (Crammer and Singer, 2003),
(2) the Passive-Aggressive algorithm (Crammer
et al, 2006), and (3) the Confidence-Weighted
algorithm (Crammer et al, 2009). For Passive-
Aggressive algorithm, we used PA-I and set pa-
rameter C to 1. For Confidence-Weighted, we
used the single-constraint updates. All algorithms
iterated five times through the training data.
4.2 Results
Table 2 shows the results of semantic classifica-
tion. All algorithms significantly improved over
the baseline. As suggested by the gap in accu-
racy between acquired and registered nouns in the
baseline method, the label distribution of the train-
ing data differed from that of the test data, but the
decrease in accuracy was smaller than expected.
The Passive-Aggressive algorithm performed
best on both acquired and registered nouns. For
the rest of this paper, we report the results of the
Passive-Aggressive algorithm.
Table 3 shows aggregated instances of some ac-
quired nouns. Although classification sometimes
failed, correct labels took the majority. How-
ever, it is noticeable that PSN-P was frequently
misidentified as PSN-C while PSN-C was cor-
rectly identified. This phenomenon is clearly seen
in the confusion matrix (Table 4). Half of PSN-P
instances were misidentified as PSN-C but the
percentage of errors in the opposite direction was
just above 9%. We will investigate this in the next
section.
4.3 Discussion
Our interest is in determining what kinds of fea-
tures are effective in semantic classification. We
first performed standard ablation experiments. We
trained a series of models on the training data af-
ter removing each feature set. The training and
test data were the same with those in Section 4.1.
Table 5 shows the results of ablation experi-
ments. Significant decreases in accuracy are ob-
served in the cf dataset. This is easily explained by
the fact that more than half of features belonged
to cf. The ratio of ncf1 was much the same with
that of ncf2, but the removal of ncf1 resulted in a
worse performance in classifying registered nouns
than that of ncf2. This means that a modifiee of a
noun explains more about the noun than its modi-
fier.
The ablation experiments cannot capture inter-
esting properties of features because each feature
set has a great diversity within it. Next, we di-
rectly examine features instead. Since we use a
simple linear classifier, a feature has |Y | corre-
sponding weights, each of which represents how
likely a noun belongs to label y. For example,
features whose weights for PSN-C are the largest
881
Table 4: Confusion matrix of acquired nouns.
Actual
PSN-P LOC-P ORG-P OTH-P PSN-C LOC-C ORG-C ANI-C OTH-C
Pre
dic
ted
PSN-P 16 1 4 1
LOC-P 1
ORG-P 4
OTH-P
PSN-C 16 39 1 2
LOC-C 2 2 1 10 4
ORG-C 2
ANI-C 28
OTH-C 3 1 1 1 13 9 338
Table 5: Results of ablation experiments.
feature set ratio1 acquired nouns registered nouns
-call 0.23% 87.60% (438 / 500) 91.58% (127,276 / 138,971)
-cf 54.84% 84.80% (424 / 500) 88.96% (123,630 / 138,971)
-demo 2.40% 88.00% (440 / 500) 91.38% (126,996 / 138,971)
-ncf1 19.03% 87.20% (436 / 500) 89.23% (124,008 / 138,971)
-ncf2 18.40% 85.60% (428 / 500) 91.54% (127,220 / 138,971)
-suf 5.10% 87.40% (437 / 500) 91.30% (126,889 / 138,971)
all 87.00% (435 / 500) 91.68% (127,407 / 138,971)
1 The proportion of each feature set that appears in the instances of the test
data.
include:
? cf:nakusu:wo (?? lose X to the disease?),
? cf:oshieru:ni (??1 teach X ?2?),
? ncf2:ooku (?many/much X?), and
? ncf2:<NUM>niN (X is modified by
<NUM> plus a numeral classifier for
persons).
As briefly mentioned in Section 2.3, Japanese
numeral quantifiers received scholarly attention
in the fields of linguistic philosophy and lin-
guistics in relation to the count/mass distinc-
tion (Quine, 1969; Gil, 1987). In our feature
sets, numeral quantifiers typically appear as ncf2,
e.g. ?ncf2:<NUM>niN.? The weights given to
them demonstrate their effectiveness in semantic
classification. They discriminate common nouns
from proper nouns as the weights given to com-
mon nouns are larger with wide margins. It is not
surprising because, say, the phrase ?two Johns? is
semantically acceptable but extremely rare in re-
ality. They are also informative about the distinc-
tion among PSN, LOC and others. For example,
the classifier ?niN? for persons suggest the noun in
question is a person while ?keN? for houses would
modify a location-like noun. However, we found
quite a few ?noises? about these features in data.
The modifiee of a numeral expression is not al-
ways the noun to be counted, as demonstrated by
the following example:
(2) saN
three
niN
CL
no
GEN
moNdai
problem
matters among the three persons.
From the above, the feature ?ncf2:<NUM>niN?
is extracted although ?moNdai? is OTH-C. Theis
?noise? is attributed to the genitive case marker
?no? because it can denote a wide range of rela-
tions between two nouns. We might be able to
avoid this problem if we focus on ?floating? nu-
meral quantifiers. A floating numeral quantifier
has no direct dependency relation to the noun to
be counted, as in
(3) seito
student
ga
NOM
saN
three
niN
CL
keQseki
absence
shita
do
three students were absent,
where the numeral quantifier modifies the verb
phrase instead of the noun. Further work is
needed to anchor floating numeral quantifiers
since they bring a different kind of ambiguity
themselves (Bond et al, 1998).
Closely related to numeral quantifiers are quan-
tificational nouns that appear as ?ncf2:ooku?
(?many/much?), ?ncf2:subete? (?all?) and oth-
ers. They distinguish common nouns from proper
882
nouns but does not make a further classifica-
tion. The same is true of other numeral expres-
sions such as ?cf:hueru:ga? (?X increase in num-
ber?) and ?cf:nai:ga? (?there is no X? or ?X
do not exist?). We found that, other than nu-
meral expressions, some features distinguished
common nouns from proper nouns because they
indicated the noun denoted an attribute. Such fea-
tures include ?cf:naru:ni? (?? become X?) and
?cf:kaneru:wo? (?? double as X?).
We expected that demonstratives (demo)
served similar functions to quantificational ex-
pressions, but it turned out to be more com-
plex. The distal demonstrative ?ano? (?that?) of-
ten modifies proper nouns to give emphasis. In
fact, the model gave larger weights to proper
nouns. On the other hand, interrogative demon-
stratives such as ?dono? (?which?) and ?doNna?
(?what kind of?) are rarely used with proper nouns
although semantically acceptable.
As seen above, there is an abundant variety
of features that distinguish common nouns from
proper nouns. Also, it is not difficult to make a
distinction among PSN, LOC and others although
the far largest cluster OTH-C sometimes absorbs
other instances. The remaining question is how to
distinguish proper nouns from common nouns, or
specifically PSN-P from PSN-C. We examined
features that gave larger weights to PSN-P than
to PSN-C. They generally had smaller margins
in weights than those which distinguish PSN-C
from PSN-P. Among them, features such as
?cf:utau:ga? (?X sing?) and ?cf:hanasu:ni? (??
talk to X?) have no problem with being used for
common nouns in terms of both semantics and
pragmatics. They seem to have resulted from
over-training. There were seemingly appropriate
features such as ?suf:saNchi? (?X?s house?) and
?suf:seNshu? (honorific suffix for players), but
they were not ubiquitous in the corpus. PSN-P in-
stances suffered from lack of distinctive features.
One solution to this problem is to combine ad-
ditional knowledge about person names. For ex-
ample, a Japanese family name is followed by a
given name, and most Chinese names consist of
three Chinese characters. However, quite a few
person names in the web corpus do not follow
the usual patterns of person names because they
are handles (or nicknames) and names for fic-
tional characters. Thus it would be desirable to be
able to classify person names without additional
knowledge.
5 Conclusion
In this paper, we presented the new task of seman-
tic classification of Japanese nouns and applied it
to nouns automatically acquired from text. Unlike
in unknown morpheme identification in previous
studies, we can exploit automatically parsed text.
We explored lexico-syntactic clues and investi-
gated their effects. We found plenty of features
that distinguished common nouns from proper
nouns, but few features worked in the opposite di-
rection. Further work is needed to overcome this
bias.
References
Asahara, Masayuki and Yuji Matsumoto. 2000. Ex-
tended models and tools for high-performance part-
of-speech tagger. In Proc. of COLING 2000, pages
21?27.
Asahara, Masayuki and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proc. of HLT/NAACL
2003, pages 8?15.
Asahara, Masayuki and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Proc. COLING 2004, pages
459?465.
Bond, Francis, Daniela Kurz, and Satoshi Shirai.
1998. Anchoring floating quantifiers in Japanese-
to-English machine translation. In Proc. of COL-
ING 1998, pages 152?159.
Ciaramita, Massimiliano and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proc. of EMNLP 2006, pages 594?602.
Ciaramita, Massimiliano and Mark Johnson. 2003.
Supersense tagging of unknown nouns in WordNet.
In Proc. of EMNLP 2003, pages 168?175.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP 2002, pages 1?8.
883
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Crammer, Koby, Mark Dredze, and Alex Kulesza.
2009. Multi-class confidence weighted algorithms.
In Proc. of EMNLP 2009, pages 496?504.
Curran, James R. 2005. Supersense tagging of un-
known nouns using semantic similarity. In Proc. of
ACL 2005, pages 26?33.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge, MA.
Gil, David. 1987. Definiteness, NP configurationality
and the count-mass distinction. In Reuland, Eric J.
and Alice G. B. ter Meulen, editors, The Representa-
tion of (In)definiteness, pages 254?269. MIT Press.
Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2009.
Capturing salience with a trainable cache model for
zero-anaphora resolution. In Proc. of ACL/IJCNLP
2009, pages 647?655.
Kawahara, Daisuke and Sadao Kurohashi. 2001.
Japanese case frame construction by coupling the
verb and its closest case component. In Proc. of
HLT 2001, pages 204?210.
Kawahara, Daisuke and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC-06,
pages 1344?1347.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Proc. of
ACL 2008, pages 407?415, June.
Krishnan, Vijay and Christopher D. Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proc. of COLING-ACL 2006, pages 1121?1128.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CONLL 2002, pages 1?7.
Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proc. of
EMNLP 2004, pages 230?237.
Kurohashi, Sadao, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer JUMAN. In
Proc. of The International Workshop on Sharable
Natural Language Resources, pages 22?38.
Marcus, Mitchell P., Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Com-
putational Linguistics, 19(2):313?330.
Mori, Shinsuke and Makoto Nagao. 1996. Word ex-
traction from corpora and its part-of-speech estima-
tion using distributional analysis. In Proc. of COL-
ING 1996, volume 2, pages 1119?1122.
Murawaki, Yugo and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes us-
ing morphological constraints. In Proc. of EMNLP
2008, pages 429?437.
Nagata, Masaaki. 1999. A part of speech estimation
method for Japanese unknown words using a statis-
tical model of morphology and context. In Proc. of
ACL 1999, pages 277?284.
Nakagawa, Tetsuji and Yuji Matsumoto. 2006. Guess-
ing parts-of-speech of unknown words using global
information. In Proc. of COLING-ACL 2006, pages
705?712.
Quine, Willard Van. 1969. Ontological Relativity and
Other Essays. Columbia University Press.
Saito, Kuniko, Jun Suzuki, and Kenji Imamura. 2007.
Extraction of named entities from blogs using CRF.
In Proc. of The 13th Annual Meeting of The Associ-
ation for Natural Language Processing, pages 107?
110. (in Japanese).
Sasano, Ryohei and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proc. of IJCNLP 2008,
pages 607?612.
Sasano, Ryohei and Sadao Kurohashi. 2009. A prob-
abilistic model for associative anaphora resolution.
In Proc. of EMNLP 2009, pages 1455?1464.
Sasano, Ryohei, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proc. of COLING 2004, pages 1201?
1207.
Uchimoto, Kiyotaka, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a mor-
phological analysis of Japanese using maximum en-
tropy aided by a dictionary. In Proc. of EMNLP
2001, pages 91?99.
Yokoi, Toshio. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
884
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605?615,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Non-parametric Bayesian Segmentation of Japanese Noun Phrases
Yugo Murawaki and Sadao Kurohashi
Graduate School of Informatics
Kyoto University
{murawaki, kuro}@i.kyoto-u.ac.jp
Abstract
A key factor of high quality word segmenta-
tion for Japanese is a high-coverage dictio-
nary, but it is costly to manually build such
a lexical resource. Although external lexical
resources for human readers are potentially
good knowledge sources, they have not been
utilized due to differences in segmentation cri-
teria. To supplement a morphological dictio-
nary with these resources, we propose a new
task of Japanese noun phrase segmentation.
We apply non-parametric Bayesian language
models to segment each noun phrase in these
resources according to the statistical behavior
of its supposed constituents in text. For in-
ference, we propose a novel block sampling
procedure named hybrid type-based sampling,
which has the ability to directly escape a lo-
cal optimum that is not too distant from the
global optimum. Experiments show that the
proposed method efficiently corrects the initial
segmentation given by a morphological ana-
lyzer.
1 Introduction
Word segmentation is the first step of natural lan-
guage processing for Japanese, Chinese and Thai
because they do not delimit words by white-space.
Segmentation for Japanese is a successful field of re-
search, achieving the F-score of nearly 99% (Kudo
et al, 2004). This success rests on a high-coverage
dictionary. Unknown words, or words not covered
by the dictionary, are often misidentified.
Historically, researchers have devoted exten-
sive human resources to build and maintain high-
coverage dictionaries (Yokoi, 1995). Since the or-
thography of Japanese does not specify a standard
for segmentation, researchers define their own crite-
ria before constructing lexical resources. For this
reason, it is difficult to exploit existing external
resources, such as dictionaries and encyclopedias
for human readers, where entry words are not seg-
mented according to the criteria. Among them,
encyclopedias are especially important in that they
contain a lot of terms that a morphological dictio-
nary fails to cover. Most of these terms are noun
phrases and consist of more than one word (mor-
pheme). For example, an encyclopedia has an en-
try ????? (tsuneyama-jou, ?Tsuneyama Castle?).
According to our segmentation criteria, it consists
of two words ???? (tsuneyama) and ??? (jou).
However, the morphological analyzer wrongly seg-
ments it into ??? (tsune) and ???? (yamashiro)
because ???? (tsuneyama) is an unknown word.
In this paper, we present the first attempt to uti-
lize encyclopedias for word segmentation. We seg-
ment each entry noun phrase into words. To do this,
we examine the main text of the entry, on the as-
sumption that if the noun phrase in question con-
sists of more than one word, its constituents appear
in the main text either freely or as part of other
noun phrases. For ????? (tsuneyama-jou), its
constituent ???? (tsune) appears by itself and as
constituents of other nouns phrases such as ????
?? (peak of Tsuneyama) and ????? (Tsuneyama
Station) while ???? (yamashiro) does not.
To segment each noun phrase, we use non-
parametric Bayesian language models (Goldwater et
al., 2009; Mochihashi et al, 2009). Our approach
605
is based on two key factors: the bigram model and
type-based block sampling. The bigram model al-
leviates a problem of the unigram model, that is, a
tendency to misidentify a sequence of words in com-
mon collocations as a single word. Type-based sam-
pling (Liang et al, 2010) has the ability to directly
escape a local optimum, making inference very ef-
ficient. However, type-based sampling is not easily
applicable to the bigrammodel owing to sparsity and
its dependence on latent assignments.
We propose a hybrid type-based sampling proce-
dure, which combines the Metropolis-Hastings al-
gorithm with Gibbs sampling. We circumvent the
sparsity problem by joint sampling of unigram-level
type. Also, instead of calculating the probability of
every possible state of the jointly sampled random
variables, we only compare the current state with
a proposed state. This greatly eases the sampling
procedure while retaining the efficiency of type-
based sampling. Experiments show that the pro-
posed method quickly corrects the initial segmen-
tation given by a morphological analyzer.
2 Related Work
Japanese Morphological Analysis and Lexical
Acquisition Word segmentation for Japanese is
usually solved as the joint task of segmentation and
part-of-speech tagging, which is called morpholog-
ical analysis (Kurohashi et al, 1994; Asahara and
Matsumoto, 2000; Kudo et al, 2004). The stan-
dard approach in Japanese morphological analysis
is lattice-based path selection instead of character-
based IOB tagging. Given a sentence, an analyzer
first builds a lattice of words with dictionary look-up
and then selects an optimal path using pre-defined
parameters. This approach enables fast decoding
and achieves accuracy high enough for practical use.
This success, however, depends on a high-
coverage dictionary, and unknown words are often
misidentified. Although a line of research attempts
to identify unknown words on the fly (Uchimoto et
al., 2001; Asahara and Matsumoto, 2004), it by no
means provides a definitive solution because it suf-
fers from locality of contextual information avail-
able for identification (Nakagawa and Matsumoto,
2006). Therefore we like to perform separate lexical
acquisition processes in which wider context can be
examined.
Our approach in this paper has a complementary
relationship with unknown word acquisition from
text, which we previously proposed (Murawaki and
Kurohashi, 2008). Since, unlike Chinese and Thai,
Japanese is rich in morphology, morphological reg-
ularity can be used to determine if an unknown
word candidate in text is indeed the word to be ac-
quired. In general, this method works pretty well,
but one exception is noun phrases. Noun phrases
can hardly be distinguished from single nouns be-
cause in Japanese, no morphological marker is at-
tached to join nouns to form a noun phrase. We
previously resort to a heuristic measure to segment
noun phrases. The new statistical method provides a
straightforward solution to this problem.
Meanwhile, our language models have their own
problem. The assumption that language is a se-
quence of invariant words fails to capture rich mor-
phology, as our segmentation criteria specify that
each verb or adjective consists of an invariant stem
and an ending that changes its form according to
its grammatical roles. For this reason, we limit our
scope to noun phrases in this paper.
Use of Noun Phrases Named entity recogni-
tion (NER) is a field where encyclopedic knowl-
edge plays an important role. Kazama and Tori-
sawa (2008) encode information extracted from a
gazetteer (e.g. Wikipedia) as features of a CRF-
based Japanese NE tagger. They formalize the NER
task as the character-based labeling of IOB tags.
Noun phrases extracted from a gazetteer are also
straightforwardly represented as IOB tags. How-
ever, this does not fully solve the knowledge bot-
tleneck problem. They also used the output of a
morphological analyzer, which does not utilize en-
cyclopedic knowledge. NER performance may be
affected by segmentation errors in morphological
analysis involving unknown words.
Chinese word segmentation is often formalized as
a character tagging problem (Xue, 2003). In this
setting, it is easy to incorporate external resources
into the model. Low et al (2005) introduce an exter-
nal dictionary as features of a discriminative model.
However, they only use words up to 4 characters in
length. We conjecture that words in their dictionary
are not noun phrases. External resources used by
606
Peng et al (2004) are also lists of short words and
characters.
Non-parametric Language Models Non-
parametric Bayesian statistics offers an elegant
solution to the task of unsupervised word segmen-
tation, in which the vocabulary size is not known in
advance (Goldwater et al, 2009; Mochihashi et al,
2009). It does not compete with supervised segmen-
tation, however. Unsupervised word segmentation
is used elsewhere, for example, with theoretical
interest in children?s language acquisition (Johnson,
2008; Johnson and Demuth, 2010) and with the
application to statistical machine translation, in
which segmented text is merely an intermediate rep-
resentation (Xu et al, 2008; Nguyen et al, 2010).
In this paper we demonstrate that non-parametric
models can complement supervised segmentation.
3 Japanese Noun Phrase Segmentation
Our goal is to overcome the unknown word prob-
lem in morphological analysis by utilizing existing
resources such as dictionaries and encyclopedias for
human readers. In our settings, we are given a list of
entries from external resources. Almost all of them
are noun phrases and each entry consists of one or
more words.
A na??ve implementation would be to use noun
phrases as they are. In fact, ipadic1 regards as single
words a large number of long proper nouns like ??
??????????? (literally, Kansai Interna-
tional Airport Company Connecting Bridge). How-
ever, this approach has various drawbacks. For ex-
ample, in information retrieval, the query ?Kansai
International Airport? does not match the ?single?
word for the bridge. So we apply segmentation.
Each entry is associated with text, which is usu-
ally the main text of the entry.2 We assume the text
as the key to segmenting the noun phrase. If the
noun phrase in question consists of more than one
word, its constituents would appear in the text either
freely or as part of other noun phrases.
We obtain the segmentation of an entry noun
phrase by considering the segmentation of the whole
1http://sourceforge.jp/projects/ipadic/
2We may augment the text with related documents if the
main text is not large enough.
text. One may instead consider a pipeline ap-
proach in which we first extract noun phrases in
text and then identify boundaries within these noun
phrases. However, noun phrases in text are not triv-
ially identifiable in the case that they contain un-
known words as their constituents. For example,
the analyzer erroneously segments the word ???
???? (chiNsukou) into ???? (chiN) and ???
?? (sukou), and since the latter is misidentified as
a verb, the incorrect noun phrase ???? (chiN) is
extracted.
We have a morphological analyzer with a dictio-
nary that covers frequent words. Although it often
misidentifies unknown words, the overall accuracy
is reasonably high. For this reason, we like to use
the segmentation given by the analyzer as the ini-
tial state and to make small changes to them to get
a desired output. We also use an annotated corpus,
which was used to build the analyzer. As the an-
notated corpus encodes our segmentation criteria, it
can be used to force the models to stick with our
segmentation criteria.
We concentrate on segmentation in this paper, but
we also need to assign a POS tag to each constituent
word and to incorporate segmented noun phrases
into the dictionary of the morphological analyzer.
We leave them for future work.3
4 Non-parametric Bayesian Language
Models
To correct the initial segmentation given by the an-
alyzer, we use non-parametric Bayesian language
models that have been applied to unsupervised word
segmentation (Goldwater et al, 2009). Specifically,
we adopt unigram and bigram models. We propose
a small modification to these models in order to ex-
ploit an annotated corpus when it is much larger than
raw text.
4.1 Unigram Model
In the unigram model, a word in the corpus wi is
generated as follows:
G|?0, P0 ? DP(?0, P0)
wi|G ? G
3Fortunately, the morphological analyzer JUMAN is capa-
ble of handling phrases, each of which consists of more than
one word. All we need to do is POS tagging.
607
where G is a distribution over a countably infinite
set of words, and DP(?0, P0) is a Dirichlet pro-
cess (Ferguson, 1973) with the concentration param-
eter ?0 and the base distribution P0, for which we
use a zerogram model described in Section 4.3.
Marginalizing out G, we can interpret the model
as a Chinese restaurant process. Suppose that we
have observed i ? 1 words w?i = w1, ? ? ? , wi?1,
the probability of wi is given by
P1(wi = w|w?i) =
nw?iw + ?0P0
i? 1 + ?0
, (1)
where nw?iw is the number of word label w observed
in w?i.
The unigram model is known for its tendency to
misidentify a sequence of words in common collo-
cations as a single word (Goldwater et al, 2009). In
preliminary experiments, we found that the unigram
model often interpreted a noun phrase as a single
word, even in the case that its constituents frequently
appeared in text.
4.2 Bigram Model
The problem of the unigram model can be alleviated
by the bigram model based on a hierarchical Dirich-
let process (Goldwater et al, 2009). In the bigram
model, word wi is generated as follows:
G|?0, P0 ? DP(?0, P0)
Hl|?1, G ? DP(?1, G)
wi|wi?1 = l,Hl ? Hl
Marginalizing out G and Hl, we can again explain
the model with the Chinese restaurant process. Un-
like the unigram model, however, the bigram model
depends on the latent table assignments z?i.
P2(wi|h?i) =
nh?i(wi?1,wi) + ?1P1(wi|h?i)
nh?i(wi?1,?) + ?1
(2)
P1(wi|h?i) =
th?iwi + ?0P0(wi)
th?i? + ?0
(3)
where h?i = (w?i, z?i), th?iwi is the number of ta-
bles labeled with wi and th?i? is the total number of
tables. Thanks to exchangeability, we do not need to
track the exact seating assignments. Still, we need to
maintain a histogram for each w that consists of fre-
quencies of table customers (Blunsom et al, 2009).
4.3 Zerogram Model
Following Nagata (1996) and Mochihashi et al
(2009), we model the zerogram distribution P0 with
the word length k and the character sequence w =
c1, ? ? ? , ck. Specifically, we define P0 as the combi-
nation of a Poisson distribution with mean ? and a
bigram distribution over characters.
P0(w) = P (k;?)
P (c1, ? ? ? , ck, k|?)
P (k|?)
P (k;?) = e???
k
k!
P (c1, ? ? ? , ck, k|?) =
k+1?
i=1
P (ci|ci?1)
? is the zerogrammodel, and c0 and ck+1 are a word
boundary marker. P (k|?) can be estimated by ran-
domly generating words from the model. We use
different ? for different scripts. The Japanese writ-
ing system uses several scripts, and each word can
be classified by script such as hiragana, katakana,
kanji, the mixture of hiragana and kanji, etc. The op-
timal value for ? depends on scripts. For example,
katakana, which predominantly denotes loan words,
is longer on average than hiragana, which is often
used for short function words.
We obtain the parameters and counts from an an-
notated corpus and fix them during noun phrase seg-
mentation. This greatly simplifies inference but may
make the model fragile with unknown words. For
this reason, we set a hierarchical Pitman-Yor process
prior (Teh, 2006; Goldwater et al, 2006) for the bi-
gram probability P (ci|ci?1) with the base distribu-
tion of character unigrams. Note that even character
bigrams are sparse because thousands of characters
are used in Japanese.
4.4 Mixing an Annotated Corpus
An annotated corpus can be used to force the mod-
els to stick with our segmentation criteria. A
straightforward way to do this is to mix it with
raw text while fixing the segmentation during infer-
ence (Mochihashi et al, 2009). A word found in
the annotated corpus is generally preferred because
it has fixed counts obtained from the annotated cor-
pus. We call this method direct mixing.
Direct mixing is problematic when raw text is
much smaller than the annotated corpus. With this
608
situation, the role of raw text associated with the
noun phrase in question is marginalized by the an-
notated corpus.
As a solution to this problem, we propose another
mixing method called back-off mixing. In back-off
mixing, the annotated corpus is used as part of the
base distribution. In the unigram model, P0 in (1) is
replaced by
PBM0 = ?IPP0 + (1? ?IP)PREF1 ,
where ?IP is a parameter for linear interpolation and
PREF1 is the unigram probability obtained from the
annotated text. The loose coupling makes the mod-
els robust to an imbalanced pair of texts. Similarly,
the back-off mixing bigram model replaces P1 in (2)
with
PBM1 = ?IPP1 + (1? ?IP)PREF2 .
5 Inference
Collapsed Gibbs sampling is widely used to find
an optimal segmentation (Goldwater et al, 2009).
In this section, we first show that simple collapsed
sampling can hardly escape the initial segmentation.
To address this problem, we apply a block sam-
pling algorithm named type-based sampling (Liang
et al, 2010) to the unigram model. Since type-based
sampling is not applicable to the bigram model, we
propose a novel sampling procedure for the bigram
model, which we call hybrid type-based sampling.
5.1 Collapsed Sampling
In collapsed Gibbs sampling, the sampler repeatedly
samples every possible boundary position, condi-
tioned on the current state of the rest of the corpus.
It stochastically decides whether the corresponding
local area consists of a single word w1 or two words
w2w3 (w1 = w2.w3). The conditional probabilities
can be derived from (1).
Collapsed sampling is known for slow conver-
gence. This property is especially problematic in
our settings where the initial segmentation is given
by a morphological analyzer. Since the analyzer de-
terministically segments text using pre-defined pa-
rameters, the resultant segmentation is fairly consis-
tent. Segmentation errors involving unknown words
also occur in a regular way. Intuitively, we start with
a local optimum although it is not too distant from
the global optimum. The collapsed Gibbs sampler is
easily entrapped by this local optimum. For this rea-
son, the initial segmentation is usually chosen at ran-
dom (Goldwater et al, 2009). Sentence-based block
sampling is also susceptible to consistent initializa-
tion (Liang et al, 2010).
5.2 Type-based Sampling
To achieve fast convergence, we adopt a block sam-
pling algorithm named type-based sampling (Liang
et al, 2010). For the unigram model, a type-based
sampler jointly samples multiple positions that share
the same type. Two positions have the same type
if the corresponding areas are both of the form w1
or w2w3. Type-based sampling takes advantage of
the exchangeability of multiple positions with the
same type. Given n positions with the same type,
the sampler first samples the number of new bound-
aries m? (0 ? m? ? n), and then uniformly arranges
m? boundaries out of n positions.
Type-based sampling has the ability to jump from
a local optimum (e.g. consistently segmented) to an-
other stable state (consistently unsegmented). While
Liang et al (2010) used random initialization, we
take particular note of the possibility of efficiently
correcting the consistent segmentation by the ana-
lyzer.
Type-based sampling is, however, not applicable
to the bigram model for two reasons. The first prob-
lem is sparsity. For the bigram model, we need to
consider adjacent words, wl on the left and wr on
the right. This means that each type consists of
three or four words, wlw1wr or wlw2w3wr. Con-
sequently, few positions share the same type and
we fail to change closely-related areas wl?w1wr? and
wl?w2w3wr? , making inference inefficient.
The second and more fundamental problem arises
from the hierarchical settings. Since the bigram
model depends on latent table assignments, the joint
distribution of multiple positions is no longer a
closed-form function of counts.
Strictly speaking, we need to update the model
counts even when sampling one position because
the observation of the bigram ?wlw1?, for exam-
ple, may affect the probability P2(w2|h?, ?wlw1?).
Goldwater et al (2009) approximate the probability
by not updating the model counts in collapsed Gibbs
609
sampling (i.e. P2(w2|h?, ?wlw1?) ? P2(w2|h?)).
They rely on the assumption that repeated bigrams
are rare. Obviously this does not hold true for type-
based sampling. Hence for type-based sampling, we
have to update the model counts whenever we ob-
serve a new word.
One way to obtain the joint probability is to ex-
plicitly simulate the updates of histograms and other
model counts. This is very cumbersome as we need
to simulate n+ 1 ways of model updates.
5.3 Hybrid Type-based Sampling
To address these problems, we propose a hybrid
sampler which incorporates the Metropolis-Hastings
algorithm into blocked Gibbs sampling. Metropolis-
Hastings is another technique for sampling from a
Markov chain. It first draws a proposed next state
h? based on the current state h according to some
proposal distribution Q(h?;h). Then it accepts the
proposal with the probability of
min
{P (h?)Q(h;h?)
P (h)Q(h?;h) , 1
}
. (4)
If the proposal is not accepted, the current state is
used as the next state. Metropolis-Hastings is useful
when it is difficult to directly sample from P .
We use the Metropolis-Hastings algorithm within
Gibbs sampling. Instead of calculating the n + 1
probabilities of the number of boundaries, we only
compare the current state with a proposed bound-
ary arrangement. Also, the set of positions sampled
jointly is chosen at unigram-level type instead of
bigram-level type. The positions are no longer ex-
changeable. Therefore we calculate the conditional
probability of one specific boundary arrangement.
When n = 1, the only choice is to flip the cur-
rent state (i.e. (m,m?) ? {(0, 1), (1, 0)}). This re-
duces to simple collapsed sampling. Otherwise we
draw a proposed state in two steps. Given the n
positions and the number of current boundaries m,
we first draw the number of proposed boundaries m?
from a probability distribution fn(m?;m). We then
randomly arrange m? boundaries. The probability
mass is uniformly divided by nCm? arrangements.
One exception is the case when m /? {0, n} and
m? = m. In this case we perform permutation to
obtain h? ?= h. To sum up, the proposal distribution
 0
 0.1
 0.2
 0.3
 0.4
 0  2  4  6  8  10 0
 0.1
 0.2
 0.3
 0.4
p
r
o
b
a
b
i
l
i
t
y
m?
Figure 1: Probability of # of boundaries f10(m?; 3).
is defined as follows:
Q(h?;h) = fn(m
?;m)
nCm? ? In(m,m?)
, (5)
where In(m,m?) is 1 if m /? {0, n} and m? = m;
otherwise 0.
We construct fn(m?;m) by discretizing a beta
distribution (? = ? < 1) and a normal distribution
with mean m, as shown in Figure 1. The former fa-
vors extreme values while the latter prefers smaller
moves.
The sampling of each type is done in the follow-
ing steps.
1. Collect n positions that share a unigram-level
type.
2. Propose a new boundary arrangement. In what
follows, we only focus on flipped boundaries
because the rest does not change the likelihood
ratio of the current and proposed states.
3. Calculate the current conditional probability.
This can be done by repeatedly applying (2)
while removing words one-by-one and updat-
ing the model counts accordingly.
4. Calculate the proposed conditional probability
while adding words one-by-one.
5. Decide whether to accept the proposal accord-
ing to (4). If the proposal is accepted, we final-
ize the arrangement; otherwise we revert to the
current state.
We implement skip approximation (Liang et al,
2010) and sample each type once per iteration. This
is motivated by the observation that although the
610
joint sampling of a large number of positions is com-
putationally expensive, the proposal is accepted very
infrequently.
5.4 Additional Constraints
Partial annotations (Tsuboi et al, 2008; Neubig and
Mori, 2010) can be used for inference. If we know in
advance that a certain position is a boundary or non-
boundary, we simply keep it unaltered. As partially-
annotated text, we can use markup. Suppose that the
original text is written with wiki markup as follows:
*JR[[???]][[???]]
[gloss] JR Ube Line Tsuneyama Station
It is clear that the position between ??? (line) and
??? (tsune) is a boundary.
Similarly, we can impose our trivial rules of seg-
mentation on the model. For example, we can keep
punctuation markers (Li and Sun, 2009) separate
from others.
6 Experiments
6.1 Settings
Data Set We evaluated our approach on Japanese
Wikipedia. For each entry of Wikipedia, we re-
garded the title as a noun phrase and used both the
title and main text for segmentation. We separately
applied our segmentation procedure to each entry.
We constructed the data set as follows. We ex-
tracted each entry from an XML dump of Japanese
Wikipedia.4 We normalized the title by dropping
trailing parentheses that disambiguate entries with
similar names (e.g. ??? (??)? for Akagi (aircraft
carrier)). We extracted the main text from wikitext
and used wiki markup as boundary markers. We ap-
plied both the title and main text to the morphologi-
cal analyzer JUMAN5 to get an initial segmentation.
If the resultant segmentation conflicted with markup
information, we overrode the former. The initial seg-
mentation was also used as the baseline.
We only used entries that satisfied all of the fol-
lowing conditions.
1. The (normalized) title is longer than one char-
acter and contains hiragana, katakana and/or
kanji.
4http://download.wikimedia.org/jawiki/
5http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?JUMAN
2. The main text is longer than 1,000 characters.
3. The title appears at least 5 times in the main
text.
The first condition ensures that there are segmenta-
tion ambiguities. The second and third conditions
exclude entries unsuitable for statistical methods.
14% of the entries satisfied these conditions.
We randomly selected 500 entries and manually
segmented their titles for evaluation. The 2-person
inter-annotator Kappa score was 0.95.
As an annotated corpus, we used Kyoto Text Cor-
pus.6 It contained 1,675,188 characters.
Models We compared the unigram and bigram
models. As for inference procedures, we used col-
lapsed Gibbs sampling (CL) for both models, type-
based sampling (TB) for the unigram model and
hybrid type-based sampling (HTB) for the bigram
model.
We tested two mixing methods of the annotated
corpus, direct mixing (DM) and back-off mixing
(BM).
To investigate the effect of initialization, we also
tried randomly segmented text as the initial state
(RAND). For random initialization, we placed a
boundary with probability 0.5 on each position un-
less it was a fixed boundary.
The unigram model has one Dirichlet process
concentration hyperparameter ?0 and the bigram
model has ?0 and ?1. For each model, we experi-
mented with the following values.
?0: 0.1, 0.5, 1 5 10, 50, 100, 500, 1,000 and 5,000
?1: 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100 and 500
For comparison, we also performed hyperparame-
ter sampling. Following Escobar and West (1995),
we set a gamma prior and introduced auxiliary vari-
ables to infer concentration parameters from data.
For back-off mixing, we used the linear interpola-
tion parameter ?IP = 0.5. The zerogram model was
trained on the annotated corpus.
In each run, we performed 10 burn-in iterations.
We then performed another 10 iterations to collect
samples.
6http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?Kyoto%20University%20Text%
20Corpus
611
Table 1: Results of segmentation of entry titles (F-score (precision/recall)).
model best median inferred
unigram + CL 81.35 (77.78/85.27)** 80.09 (75.80/84.89) 80.86 (76.81/85.36)
unigram + TB 55.87 (66.71/48.06) 51.04 (62.64/43.06) 42.63 (54.91/34.84)
bigram + CL 80.65 (76.73/84.99) 79.96 (75.50/84.99) 80.54 (76.84/84.61)
bigram + HTB 83.23 (85.25/81.30)** 74.52 (71.33/78.00) 34.52 (46.69/27.38)
unigram + CL + DM 85.29 (83.14/87.54)** 81.62 (77.93/85.70)** 80.91 (82.87/79.04)
unigram + TB + DM 35.26 (47.74/29.95) 33.81 (46.20/26.66) 31.90 (44.30/24.93)
bigram + CL + DM 80.37 (76.01/85.27) 79.88 (75.42/84.89) 73.77 (78.49/69.59)
bigram + HTB + DM 69.66 (67.68/71.77) 67.39 (64.35/70.73) 31.54 (43.79/24.64)
unigram + CL + BM 81.28 (77.48/85.46) 80.23 (76.06/84.89) 81.42 (77.75/85.46)
unigram + TB + BM 57.22 (68.01/49.39) 52.98 (64.50/44.95) 42.43 (54.69/34.66)
bigram + CL + BM 81.33 (77.34/85.74) 80.07 (75.69/84.99) 81.46 (77.82/85.46)**
bigram + HTB + BM 86.32 (85.67/86.97)** 76.35 (71.89/81.40) 40.81 (53.35/33.05)
unigram + TB + RAND 56.01 (66.93/48.16) 50.89 (62.21/43.06) 42.68 (54.81/34.94)
bigram + HTB + RAND 79.68 (80.13/79.23) 68.16 (63.64/73.37) 34.99 (47.05/27.86)
unigram + TB + BM + RAND 57.44 (67.91/49.76) 50.86 (61.92/43.15) 42.31 (54.55/34.56)
bigram + HTB + BM + RAND 84.03 (83.10/84.99) 70.46 (65.25/76.58) 40.16 (52.60/32.48)
baseline (JUMAN) 80.09 (75.80/84.89)
** Statistically significant improvement with p < 0.01.
Evaluation Metrics We evaluated the segmenta-
tion accuracy of 500 entry titles. Specifically we
evaluated the performance of a model with preci-
sion, recall and the F-score, all of which were based
on tokens. We report the score of the most frequent
segmentation among 10 samples.
Following Lee et al (2010), we report the best and
median settings of hyperparameters based on the F-
score, in addition to inferred values.
In order to evaluate the degree of difference
between a pair of segmentations, we employed
character-based evaluation. Following Kudo et
al. (2004), we converted a word sequence into
character-based BI labels and examined labeling dis-
agreements. McNemar?s test of significance was
based on this metric.
6.2 Results
Table 1 shows segmentation accuracy of various
models. One would notice that the baseline score
is much lower than the score previously reported re-
garding newspaper articles (Kudo et al, 2004). It
is because unlike newspaper articles, the titles of
Wikipedia entries contain an unusually high pro-
portion of unknown words. As suggested by rel-
atively low precision, unknown words tend to be
over-segmented by the morphological analyzer.
In the best hyperparameter settings, the back-off
mixing bigram model with hybrid type-based sam-
pling (bigram + HTB + BM) significantly outper-
formed the baseline and achieved the best F-score.
It did not performed well in the median setting as
it was sensitive to the value of ?1. Hyperparameter
estimation led to catastrophic decreases in bigram
models as it made the hyperparameters much larger
than those in the best settings.
Collapsed sampling (+CL) returned scores com-
parable to that of the baseline. It is simply because
it did not change the initial segmentation a lot. In
contrast, type-based sampling (+TB) brought large
moves to the unigram model and significantly hurt
accuracy. As suggested by relatively low recall, the
unigram model prefers under-segmentation.
When combined with (hybrid) type-based sam-
pling (+TB/+HTB), back-off mixing (+BM) in-
creased accuracy from the corresponding non-
mixing models. By contrast, direct mixing (+DM)
drastically decreased accuracy from the non-mixing
models. We can confirm that when the main text
is orders of magnitude smaller than the annotated
text, the role of constituent words in the main text
is underestimated. To our surprise, collapsed sam-
pling with mixing models (+CL, +DM/+BM) out-
performed the baseline. However, the scores of type-
based sampling (+TB) suggest that with much more
iterations, the models would converge to undesired
states.
612
The unigram model with random initialization
was indifferent from that with default initialization.
By contrast, the performance of the bigram model
slightly degenerated with random initialization.
6.3 Convergence
Figure 2 shows how segmentations differed from
the initial state in the course of inference.7 A diff
is defined as the number of character-based dis-
agreements between the baseline segmentation and a
model output. Hyperparameters used were those of
the best model with (hybrid) type-based sampling.
We can see that collapsed sampling was almost
unable to escape the initial state. With type-based
sampling (+TB), the unigram model went further
than the bigram model, but to an undesired direc-
tion. The bigram model with hybrid type-based
sampling (bigram + HTB) converged in few itera-
tions. Although the model with random initializa-
tion (+RAND) converged to a nearby point, the ini-
tial segmentation by the morphological analyzer re-
alized a bit faster convergence and better accuracy.
Figure 2 shows how acceptance rates changed
during inference. For comparison, a sample by a
type-based Gibbs sampler was treated as ?accepted?
if the number of new boundaries was different from
that of the current boundaries (i.e. m? ?= m). The
acceptance rates were low and samplers seemingly
stayed around modes.
6.4 Approximation
Up to this point, we consider every possible bound-
ary position. However, this seems wasteful, given
that a large portion of text has only marginal influ-
ence on the segmentation of the noun phrase in ques-
tion. For this reason, we implemented approxima-
tion named matching skip. We sampled a boundary
only if the corresponding local area contained a sub-
string of the noun phrase in question.
Table 2 shows the result of approximation. Hy-
perparameters used were those of the best models
with full sampling. Matching skip steadily worsened
performance although not to a large extent. Mean-
7For a fair comparison, we might need to report changes
over time instead of iterations. However, the difference of con-
vergence speed is obvious in the iteration-based comparison al-
though (hybrid) type-based sampling takes several times longer
than collapsed sampling in the current na??ve implementation.
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 0  5  10  15  20
d
i
f
f
iteration
unigram + CLunigram + TBbigram + CLbigram + HTBunigram + TB + RANDbigram + HTB + RAND
Figure 2: Diffs in the course of iteration. All models were
with back-off mixing (+BM).
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0  5  10  15  20
a
c
c
e
p
t
a
n
c
e
 
r
a
t
e
iteration
unigram + TBbigram + HTBunigram + TB + RANDbigram + HTB + RAND
Figure 3: Acceptance rates for a noun phrase in the
course of iteration. All models were with back-off mix-
ing (+BM).
while it drastically reduced the number of sampled
positions. The median skip rate was 90.87%, with a
standard deviation of 8.5.
6.5 Discussion
Figure 4 shows some segmentations corrected by
the back-off mixing bigram model with hybrid type-
based sampling. ????? (ichihino) is a rare place
name but can be identified by the model because
it is frequently used in the article. ???????
(konamiruku in hiragana) seems a pun on ????
?? (kona miruku, ?powdered milk?) and ?????
(konami in katakana, a company). We consider it
as a single word because we cannot reconstruct the
etymology solely based on the main text. Note the
different scripts. In Japanese, people often change
the script to derive a proper noun from a common
noun, which a na??ve analyzer fails to recognize. It is
613
Table 2: Effect of matching skip (F-score (precision/recall)).
model full matching skip
bigram + HTB 83.23 (85.25/81.30)** 82.86 (84.27/81.49)
bigram + HTB + BM 86.32 (85.67/86.97)** 83.87 (82.60/85.17)**
bigram + HTB + RAND 79.68 (80.13/79.23) 78.81 (78.64/75.07)
bigram + HTB + BM + RAND 84.03 (83.10/84.99) 81.08 (80.22/81.96)
baseline (JUMAN) 80.09 (75.80/84.89)
** Statistically significant improvement with p < 0.01.
? +? +? +? +? +????
hiwaki
+?
chou
+???
ichihino
(Ichihino, Hiwaki Town, an address)
? +?? +???????
risona
+???
kaRdo
(Risona Card, a company)
?? +?? +?????????
chiritotechiN (name of a play)
?? +?? +???????
konamiruku
(a shop affiliated with Konami Corporation)
?? +???????
haiziI (stage name of a comedian)
?? +?????????
chiNsukou (a traditional sweet)
??????????????????
koNtora
+???
aruto
+??????
kurarineQto (Contra-alto clarinet)
Figure 4: Examples of improved segmentations.
very important to identify hiragana words correctly.
As hiragana is mainly used to write function words
and other basic words, segmentation errors concern-
ing hiragana often bring disastrous effects on ap-
plications of morphological analysis. For example,
the analyzer over-segments ???????? (chiri-
totechiN) into three shorter words among which the
second word ???? (tote) is a particle, and this se-
quence of words is transformed into a terrible parse
tree.
Most improvements come from correction of
over-segmentation because the initial segmenta-
tion by the analyzer shows a tendency of over-
segmentation. An example of corrected under-
segmentation is ?contra-alto clarinet.? The pres-
ence of ?clarinet,? ?alto? and ?contrabass? and oth-
ers in the main text allowed the model to iden-
tify the constituents. On the other hand, the seg-
mentation failed when our assumption about con-
stituents does not hold. For example, the person
name ?????? (kikuchi shuNkichi) is two words
but was erroneously combined into a single word by
the model because unfortunately he was always re-
ferred to by the full name.
7 Conclusions
In this paper, we proposed a new task of Japanese
noun phrase segmentation. We adopted non-
parametric Bayesian language models and proposed
hybrid type-based sampling that can efficiently cor-
rect segmentation given by the morphological an-
alyzer. Although supervised segmentation is very
competitive, we showed that it can be supplemented
with our unsupervised approach.
We applied the proposed method to encyclopedic
text to segment noun phrases in it. The proposed
method can be applied to other tasks. For example,
in unknown word acquisition (Murawaki and Kuro-
hashi, 2008), noun phrases are often acquired from
text as single words. We can now segment them into
words in a more sophisticated way.
In the future we will assign a POS tag to each
word in order to use segmented noun phrases in mor-
phological analysis. We assume that the meaning
of constituents in a noun phrase rarely depends on
outer context. So it would be helpful to augment
them with rich semantic information in advance in-
stead of disambiguating their meaning every time we
analyze given text.
Acknowledgments
This work was partly supported by JST CREST.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
614
tagger. In Proc. of COLING 2000, pages 21?27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Proc. COLING 2004, pages 459?
465.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, andMark
Johnson. 2009. A note on the implementation of hier-
archical Dirichlet processes. In Proc. of ACL-IJCNLP
2009: Short Papers, pages 337?340.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures.
Journal of the American Statistical Association,
90(430):577?588.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Statistics,
1(2):209?230.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In NIPS 18, pages
459?466.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Mark Johnson and Katherine Demuth. 2010. Unsu-
pervised phonemic Chinese word segmentation using
adaptor grammars. In Proc. of COLING 2010, pages
528?536.
Mark Johnson. 2008. Using adaptor grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proc. of ACL 2008, pages 398?
406.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL
2008, pages 407?415, June.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proc. of EMNLP 2004,
pages 230?237.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proc. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22?38.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proc. of EMNLP 2010, pages 853?861.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35(4):505?512.
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Proc. of NAACL 2010, pages
573?581.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proc. of the 4th SIGHAN Workshop, pages
161?164.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proc. of
ACL-IJCNLP 2009, pages 100?108.
Yugo Murawaki and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes using
morphological constraints. In Proc. of EMNLP 2008,
pages 429?437.
Masaaki Nagata. 1996. Automatic extraction of new
words from Japanese texts using generalized forward-
backward search. In Proc. of EMNLP 1996, pages 48?
59.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Proc. of COLING-ACL 2006, pages 705?
712.
Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction. In
Proc. of LREC 2010.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. of COLING 2010, pages 815?
823.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proc. of COLING
?04, pages 562?568.
Yee Whye Teh. 2006. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical Report TRA2/06,
School of Computing, National University of Singa-
pore.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training conditional
random fields using incomplete annotations. In Proc.
of COLING 2008, pages 897?904.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Proc. of EMNLP 2001, pages 91?99.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised Chinese word
segmentation for statistical machine translation. In
Proc. of COLING 2008, pages 1017?1024.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Toshio Yokoi. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
615
