Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 9?16,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems
Ioannis Klasinas
1
, Elias Iosif
2,4
, Katerina Louka
3
, Alexandros Potamianos
2,4
1
School of ECE, Technical University of Crete, Chania 73100, Greece
2
School of ECE, National Technical University of Athens, Zografou 15780, Greece
3
Voiceweb S.A., Athens 15124, Greece
4
Athena Research Center, Marousi 15125, Greece
iklasinas@isc.tuc.gr,{iosife,potam}@telecom.tuc.gr,klouka@voiceweb.eu
Abstract
In this paper we present the SemEval-
2014 Task 2 on spoken dialogue gram-
mar induction. The task is to classify
a lexical fragment to the appropriate se-
mantic category (grammar rule) in order
to construct a grammar for spoken dia-
logue systems. We describe four sub-
tasks covering two languages, English and
Greek, and three speech application do-
mains, travel reservation, tourism and fi-
nance. The classification results are com-
pared against the groundtruth. Weighted
and unweighted precision, recall and f-
measure are reported. Three sites partic-
ipated in the task with five systems, em-
ploying a variety of features and in some
cases using external resources for training.
The submissions manage to significantly
beat the baseline, achieving a f-measure of
0.69 in comparison to 0.56 for the base-
line, averaged across all subtasks.
1 Introduction
This task aims to foster the application of com-
putational models of lexical semantics to the field
of spoken dialogue systems (SDS) for the problem
of grammar induction. Grammars constitute a vi-
tal component of SDS representing the semantics
of the domain of interest and allowing the system
to correctly respond to a user?s utterance.
The task has been developed in tight collabo-
ration between the research community and com-
mercial SDS grammar developers, under the aus-
pices of the EU-IST PortDial project
1
. Among the
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
1
http://www.portdial.eu/
project aims is to help automate the grammar de-
velopment and localization process. Unlike previ-
ous approaches (Wang and Acero, 2006; Cramer,
2007) that have focused on full automation, Port-
Dial adopts a human-in-the-loop approach were
a developer bootstraps each grammar rule or re-
quest type with a few examples (use cases) and
then machine learning algorithms are used to pro-
pose grammar rule enhancements to the developer.
The enhancements are post-edited by the devel-
oper and new grammar rule suggestions are pro-
posed by the system, in an iterative fashion un-
til a grammar of sufficient quality is achieved. In
this task, we focus on a snapshot of this process,
where a portion of the grammar is already induced
and post-edited by the developer and new candi-
date fragments are rolling in order to be classified
to an existing rule (or rejected). The goal is to de-
velop machine learning algorithms for classifying
candidate lexical fragments to the correct grammar
rule (semantic category). The task is equally rel-
evant for both finite-state machine and statistical
grammar induction.
In this task the semantic hierarchy of SDS
grammars has two layers, namely, low- and high-
level. Low-level rules are similar to gazetteers
referring to terminal concepts that can be as rep-
resented as sets of lexical entries. For example,
the concept of city name can be represented as
<CITY> = (?London?, ?Paris?, ...). High-level
rules are defined on top of low-level rules, while
they can be lexicalized as textual fragments (or
chunks), e.g., <TOCITY> = (?fly to <CITY>?,
...). Using the above examples the sentence ?I
want to fly to Paris? will be first parsed as ?I
want to fly to <CITY>? and finally as ?I want to
<TOCITY>?.
In this task, we focus exclusively on high-level
rule induction, assuming that the low-level rules
are known. The problem of fragment extraction
and selection is simplified by investigating the
9
binary classification of (already extracted) frag-
ments into valid and non-valid. The task boils
down mainly to a semantic similarity estimation
problem for the assignment of valid fragments into
high-level rules.
2 Prior Work
The manual development of grammars is a time-
consuming and tedious process that requires hu-
man expertise, posing an obstacle to the rapid port-
ing of SDS to new domains and languages. A
semantically coherent workflow for SDS gram-
mar development starts from the definition of low-
level rules and proceeds to high-level ones. This
process is also valid for the case of induction
algorithms. Automatic or machine-aided gram-
mar creation for spoken dialogue systems can
be broadly divided in two categories (Wang and
Acero, 2006): knowledge-based (or top-down)
and data-driven (or bottom-up) approaches.
Knowledge-based approaches rely on the man-
ual or semi-automatic development of domain-
specific grammars. They start from the domain on-
tology (or taxonomy), often in the form of seman-
tic frames. First, terminal concepts in the ontology
(that correspond to low-level grammar rules) get
populated with values, e.g., <CITY>, and then
high-level concepts (that correspond to high-level
grammar rules) get lexicalized creating grammar
fragments. Finally, phrase headers and trailers are
added to create full sentences. The resulting gram-
mars often suffer from limited coverage (poor re-
call). In order to improve coverage, regular ex-
pressions and word/phrase order permutations are
used, however at the cost of over-generalization
(poor precision). Moreover, knowledge-based
grammars are costly to create and maintain, as
they require domain and engineering expertise,
and they are not easily portable to new domains.
This led to the development of grammar authoring
tools that aim at facilitating the creation and adap-
tation of grammars. SGStudio (Semantic Gram-
mar Studio), (Wang and Acero, 2006), for exam-
ple, enables 1) example-based grammar learning,
2) grammar controls, i.e., building blocks and op-
erators for building more complex grammar frag-
ments (regular expressions, lists of concepts), and
3) configurable grammar structures, allowing for
domain-adaptation and word-spotting grammars.
The Grammatical Framework Resource Grammar
Library (GFRGL) (Ranta, 2004) enables the cre-
ation of multilingual grammars adopting an ab-
straction formalism, which aims to hide the lin-
guistic details (e.g., morphology) from the gram-
mar developer.
Data-driven approaches rely solely on corpora
(bottom-up) of transcribed utterances (Meng and
Siu, 2002; Pargellis et al., 2004). The induction
of low-level rules consists of two steps dealing
with the 1) identification of terms, and 2) assign-
ment of terms into rules. Standard tokenization
techniques can be used for the first step, however,
different approaches are required for the case of
multiword terms, e.g., ?New York?. In such cases,
gazetteer lookup and named entity recognition can
be employed (if the respective resources and tools
are available), as well as corpus-based colloca-
tion metrics (Frantzi and Ananiadou, 1997). Typ-
ically, the identified terms are assigned into low-
level rules via clustering algorithms operating over
a feature space that is built according to the term
semantic similarity. The distributional hypothe-
sis of meaning (Harris, 1954) is a widely-used ap-
proach for estimating term similarity. A compar-
ative study of similarity metrics for the induction
of SDS low-level rules is presented in (Pargellis
et al., 2004), while the combination of metrics
was investigated in (Iosif et al., 2006). Different
clustering algorithms have been applied includ-
ing hard- (Meng and Siu, 2002) and soft-decision
(Iosif and Potamianos, 2007) agglomerative clus-
tering.
High-level rule induction is a less researched
area that consists of two main sub-problems: 1)
the extraction and selection of candidate frag-
ments from a corpus, and 2) assignment of terms
into rules. Regarding the first sub-problem,
consider the fragments ?I want to depart from
<CITY> on? and ?depart from <CITY>? for the
air travel domain. Both express the meaning of de-
parture city, however, the (semantics of the) latter
fragment are more concise and generalize better.
The application of syntactic parsers for segment
extraction is not straightforward since the output
is a full parse tree. Moreover, such parsers are
typically trained over annotated corpora of formal
language usage, while the SDS corpora often are
ungrammatical due to spontaneous speech. There
are few statistical parsing algorithms that rely only
on plain lexical features (Ponvert et al., 2011; Bisk
and Hockenmaier, 2012) however, as other algo-
rithms, one needs to decide where to prune the
10
parse tree. In (Georgiladakis et al., 2014), the ex-
plicit extraction and selection of fragments is in-
vestigated following an example-driven approach
where few rule seeds are provided by the gram-
mar developer. The second sub-problem of high-
level rule induction deals with the formulation
of rules using the selected fragments. Each rule
is meant to consist of semantically similar frag-
ments. For this purpose, clustering algorithms can
be employed exploiting the semantic similarity be-
tween fragments as features. This is a challenging
problem since the fragments are multi-word struc-
tures whose overall meaning is composed accord-
ing to semantics of the individual constituents. Re-
cently, several models have been proposed regard-
ing phrase (Mitchell and Lapata, 2010) and sen-
tence similarity (Agirre et al., 2012), while an
approach towards addressing the issue of seman-
tic compositionality is presented in (Milajevs and
Purver, 2014).
The main drawback of data-driven approaches
is the problem of data sparseness, which may af-
fect the coverage of the grammar. A popular so-
lution to the data sparseness bottleneck is to har-
vest in-domain data from the web. Recently, this
has been an active research area both for SDS
systems and language modeling in general. Data
harvesting is performed in two steps: (i) query
formulation, and (ii) selection of relevant docu-
ments or sentences (Klasinas et al., 2013). Posing
the appropriate queries is important both for ob-
taining in-domain and linguistically diverse sen-
tences. In (Sethy et al., 2007), an in-domain lan-
guage model was used to identify the most ap-
propriate n-grams to use as web queries. An in-
domain language model was used in (Klasinas et
al., 2013) for the selection of relevant sentences.
A more sophisticated query formulation was pro-
posed in (Sarikaya, 2008), where from each in-
domain utterance a set of queries of varying length
and complexity was generated. These approaches
assume the availability of in-domain data (even if
limited) for the successful formulation of queries;
this dependency is also not eliminated when us-
ing a mildly lexicalized domain ontology to for-
mulate the queries, as in (Misu and Kawahara,
2006). Selecting the most relevant sentences that
get returned from web queries is typically done
using statistical similarity metrics between in do-
main data and retrieved documents, for example
the BLEU metric (Papineni et al., 2002) of n-
gram similarity in (Sarikaya, 2008) and a metric
of relative entropy (Kullback-Leibler) in (Sethy et
al., 2007). In cases where in-domain data is not
available, cf. (Misu and Kawahara, 2006), heuris-
tics (pronouns, sentence length, wh-questions) and
matches with out-of-domain language models can
be used to identify sentences for training SDS
grammars. In (Sarikaya, 2008), the produced
grammar fragments are also parsed and attached
to the domain ontology. Harvesting web data can
produce high-quality grammars while requiring up
to 10 times less in-domain data (Sarikaya, 2008).
Further, data-driven approaches induce syntac-
tic grammars but do not learn their corresponding
meanings, for this purpose an additional step is re-
quired of parsing the grammar fragments and at-
taching them to the domain ontology (Sarikaya,
2008). Also, in many cases it was observed
that the fully automated bottom-up paradigm re-
sults to grammars of moderate quality (Wang
and Acero, 2006), especially on corpora con-
taining longer sentences and more lexical vari-
ety (Cramer, 2007). Finally, algorithms focusing
on crosslingual grammar induction, like CLIoS
(Kuhn, 2004), are often even more resource-
intensive, as they require training corpora of par-
allel text and sometimes also a grammar for one of
the languages. Grammar quality can be improved
by introducing a human in the loop of grammar in-
duction (Portdial, 2014a); an expert that validates
the automatically created results (Meng and Siu,
2002).
3 Task Description
Next we describe in detail the candidate grammar
fragment classification SemEval task. This task
is part of a grammar rule induction scenario for
high-level rules. The evaluation focuses in spoken
dialogue system grammars for multiple domains
and languages.
3.1 Task Design
The goal of the task is to classify a number frag-
ment to the rules available in the grammar. For
each grammar we provide a training and develop-
ment set, i.e., a set of rules with the associated
fragments and the test set which is composed of
plain fragments. An excerpt of the train set for the
rule ?<TOCITY>? is ?ARRIVE AT <CITY>,
ARRIVES AT <CITY>, GOING TO <CITY>?
and of the test set ?GOING INTO <CITY>, AR-
11
RIVES INTO <CITY>?.
In preliminary experiments during the task de-
sign we noticed that if the test set consists of valid
fragments only, good classification performance is
achieved, even when using the naive baseline sys-
tem described later in this paper. To make the task
more realistic we have included a set of ?junk?
fragments not corresponding to any specific rule.
Junk fragments were added both in the train set
where they are annotated as such and in the test
set. For this task we have artificially created the
junk fragments by removing or adding words from
legitimate fragments. Example junk fragments
used are ?HOLD AT AT <TIME> TRY? and
?ANY CHOICE EXCEPT <AIRLINE> OR?, the
first one having a repetition of the word ?AT?
while the second one should include one more
time the concept ?<AIRLINE>? in the end to be
meaningful.
Junk fragments help better model a real-world
scenario, where the candidate fragments will in-
clude irrelevant examples too. For example, if
web corpora are used to extract the candidate frag-
ments grammatical mistakes and out-of-domain
sentences might appear. Similarly, if the transcrip-
tions from a deployed SDS system are used for
grammar induction, transcription errors might in-
troduce noise (Bechet et al., 2014).
Junk fragments account for roughly 5% of the
train test and 15% of the test set. The discrep-
ancy between train and test set ratios is due to a
conscious effort to model realistic train/test condi-
tions, where train data is manually processed and
does not include errors, while candidate fragments
are typically more noisy.
3.2 Datasets
We have provided four datasets, travel English,
travel Greek, tourism English and finance English.
The travel domain grammar covers flight, car and
hotel reservation utterances. The tourism domain
covers touristic information including accommo-
dation, restaurants and movies. The finance do-
main covers utterances of a bank client asking
questions about his bank account as well as re-
porting problems. In Table 1 are presented typical
examples of fragments for every subtask.
All grammars have been manually constructed
by a grammar developer. For the three English
grammars, a small corpus (between 500 and 2000
sentences) was initially available. The grammar
developer first identified terminal concepts, which
correspond to low-level rules. Typical examples
include city names for the travel domain, restau-
rant names for the tourism domain and credit card
names in the finance domain. After covering all
low-level rules the grammar developer proceeded
to identify high-level rules present in the corpus,
like the departure city in the travel domain, or the
user request type for a credit card. The gram-
mar developer was instructed to identify all rules
present in the corpus, but also spend some effort
to include rules not appearing in the corpus so that
the resulting grammar better covers the domain at
hand. For the case of Greek travel grammar no
corpus was initially available. The Greek gram-
mar was instead produced by manually translat-
ing the English one, accounting for the differences
in syntax between the two languages. The gram-
mars have been developed as part of the PortDial
FP7 project and are explained in detail in (Portdial,
2014b).
For the first three datasets that have been avail-
able from the beginning of the campaign we have
split the release into train, development and test
set. For the finance domain which was announced
when the test sets were released we only provided
the train and test set, to simulate a resource poor
scenario. The statistics of the datasets for all lan-
guage/domain pairs are given in Table 2.
In addition to the high-level rules we made
available the low-level rules for each grammar,
which although not used in the evaluation, can be
useful for expanding the high-level rules to cover
all lexicalizations expressed by the grammar.
3.3 Evaluation
For the evaluation of the task we have used preci-
sion, recall and f-measure, both weighted and un-
weighted.
If R
j
denotes the set of fragments for one rule
and C
j
the set of fragments classified to this rule
by a system then per-rule precision is computed by
the equation:
Pr
j
=
|R
j
? C
j
|
|C
j
|
and per-rule recall by:
Rc
j
=
|R
j
? C
j
|
|R
j
|
F-measure is then computed by:
12
Grammar Rule Fragment
Travel English <FLIGHTFROM> FLIGHT FROM <CITY>
Travel Greek <FLIGHTFROM> ?TH?H A?O <CITY>
Tourism English <TRANSFERQ> TRANSFERS FROM <airportname> TO <cityname>
Finance English <CARDNAME> <BANKNAME> CARD
Table 1: Example grammar fragments for each application domain.
Grammar Rules Fragments
Train set Dev set Test set
Travel English 32 623 331 284
Travel Greek 35 616 340 324
Tourism English 24 694 334 285
Finance English 9 136 - 37
Table 2: Number of rules in the training, development and test sets for each application domain.
F
j
=
2Pr
j
Rc
j
Pr
j
+ Rc
j
.
Precision for all the J rules R
j
, 1 ? j ? J is
computed by the following equation:
Pr =
?
j
Pr
j
w
j
In the unweighted case the weight w
j
has a fixed
value for all rules, so w
j
=
1
J
. Taking into account
the fact that the rules are not balanced in terms of
fragments, a better way to compute for the weight
is w
j
=
|R
j
|
?
j
|R
j
|
. In the latter, weighted, case the
total precision will better describe the results.
Recall is similarly computed using the same
weighting scheme as:
Rc =
?
j
Rc
j
w
j
3.4 Baseline
For comparison purposes we have developed a
naive baseline system. To classify a test fragment,
first its similarity with all the train fragments is
computed, and it is classified to the rule where
the most similar train fragment belongs. Fragment
similarity is computed as the ratio of their Longest
Common Substring (LCS) divided by the sum of
their lengths:
Sim(s, t) =
|LCS(s, t)|
|s|+ |t|
where s and t are two strings, |s| and |t| their
length in characters and |LCS(s, t)| the length of
their LCS. This is a very simple baseline, comput-
ing similarity without taking into account context
or semantics.
4 Participating Systems
Three teams have participated in the task with five
systems. All teams participated in all subtasks
with the exception of travel Greek, where only
two teams participated. An overview of core
system features is presented in Table 3. The
remainder of this section briefly describes each
of the submissions and then compares them. A
brief description for each system is provided in
the following paragraphs.
tucSage. The core of the tucSage system is
a combination of two components. The first
component is used for the selection of candidate
rule fragments from a corpus. Specifically, the
posterior probability of a candidate fragment
belonging to a rule is computed using a variety of
features. The feature set includes various lexical
features (e.g., the number of tokens), the fragment
perplexity computed using n-gram language
modeling, and features based on lexical similarity.
The second component is used for computing
the similarity between a candidate fragment and
a grammar rule. In total, two different types of
similarity metrics are used relying on the overlap
of character bigrams and contextual features.
These similarities are fused with the posterior
probabilities produced by the fragment selection
model. The contribution of the two components is
adjusted using an exponential weight.
SAIL-GRS. The SAIL-GRS system is based
on the well-established term frequency?inverse
document frequency (TF ?IDF ) measurement.
This metric is adapted to the present task by
considering each grammar rule as a ?document?.
For each rule, all its fragments are aggregated
13
System Use of Features Similarity External Language-
acronym machine learn. used metrics corpora specific
Baseline no lexical Longest Common no no
Substring
tucSage yes: lexical, perplexity, character overlap, web no
random forests similarity-based , heuristic cosine similarity documents
SAIL-GRS no lexical cosine similarity no no
Biel no lexical, expansion of cosine Wikipedia yes
low-level rules similarity articles
Table 3: Overview of the characteristics of the participating systems.
and the frequency of the respective n-grams
(constituents) is computed. The inverse document
frequency is casted as inverse rule frequency
and it is computed for the extracted n-grams.
The process is performed for both unigrams and
bigrams.
Biel. The fundamental idea behind the Biel
system is the encoding of domain semantics via
topic modeling. For this purpose a background
document space is constructed using thousands
of Wikipedia articles. Particular focus is given
to the transformation of the initial document
space according to the paradigm of explicit
semantic analysis. For each domain, a topic
space is defined and a language-specific function
is employed for the mapping of documents. In
essence, the mapping function is an association
measurement that is based on TF?IDF scores.
An approximation regarding the construction of
the topic space is investigated in order to reduce
data sparsity, while a number of normalization
schemes are also presented.
Overall, only the tucSage system employs a ma-
chine learning-based approach (random forests),
while an unsupervised approach is followed by the
SAIL-GRS and Biel systems. All systems exploit
lexical information extracted from rule fragments.
This information is realized as the lexical surface
form of the constituents of fragments. For ex-
ample, consider the ?depart for <CITY>? frag-
ment that corresponds to the high-level rule refer-
ring to the notion of departure city. The follow-
ing set of lexical features can be extracted from
the aforementioned fragment: (?depart?, ?from?,
?<CITY>?). Unlike the other systems, the Biel
system utilizes low-level rules to expand high-
level rules with terminal concept instances. For
example, the ?<CITY>? rule is not processed as
is, but it is represented as a list of city names
(?New York?, ?Boston?, . . . ). The most rich fea-
ture set is used by the tucSage system which com-
bines lexical, perplexity and similarity features
with a set of heuristic rules. All three systems
employ the widely-used cosine similarity metric.
Both SAIL-GRS and Biel systems rely solely on
this metric during the assignment of an unknown
fragment to a high-level rule. A more sophis-
ticated approach is presented by tucSage, where
first a classifier is built for every grammar rule,
computing the probability of a fragment belong-
ing to this rule and then the similarity between the
fragment and the rule is computed. Classification
is then performed by combining the two scores.
Also, another difference regarding the employ-
ment of the cosine similarity deals with the com-
putation of the vectorial feature values. A simple
binary scheme is used in the tucSage system, while
variations of the term frequency-inverse document
frequency scheme are used in SAIL-GRS and Biel.
Besides cosine similarity, a similarity metric based
on the overlap of character bigrams is used by the
tucSage system. External corpora (i.e., corpora
that were not provided as part of the official task
data) were used by the tucSage and Biel systems.
Such corpora were meant as an additional source
of information with respect to the domains under
investigation. Regarding tucSage, the training data
were exploited in order to construct web search
queries for harvesting a collection of web docu-
ments from which a number of sentences were se-
lected for corpus creation. In the case of the Biel
system, a set of Wikipedia articles was exploited.
Language specific resources where used for the
Biel system, while the other two teams used lan-
guage agnostic methods.
5 Results
The results for all participating teams and the
baseline system are given in Table 4. The tucSage
team submitted three runs, the first one being the
primary, indicated with an asterisk in the results.
14
Focusing on the weighted F-measure we see
that in all domains but the tourism English, at
least one submission manages to outperform the
baseline provided by the organizers. In travel En-
glish the baseline system achieves 0.51 weighted
f-measure, with two out of the three systems
achieving 0.68 and 0.58. The improvement over
the baseline is greater for the travel Greek sub-
task, where the baseline score of 0.26 is much
lower than the achieved 0.52 from tucSage. In the
tourism English subtask the best submitted sys-
tems managed to match the performance of the
baseline system, but not to exceed it. This can
be attributed to the good performance of the base-
line system, due to the fact that the tourism gram-
mar is composed of longer fragments than the rest,
helping the naive baseline system achieve top per-
formance exploiting lexical similarity only. We
can however assume that more complex systems
would beat the baseline if the test set fragments
were built using different lexicalizations, as would
be the case in unannotated data coming from de-
ployed SDS.
In the finance domain, even though the amount
of training data is quite smaller than in all other
subtasks the submitted systems still manage to
outperform the baseline system. This means that
the submitted systems display robust performance
both in resource-rich and resource-poor condi-
tions.
6 Conclusion
The tucSage and SAIL-GRS systems are shown to
be portable across domains and languages, achiev-
ing performance that exceeds the baseline for three
out of four datasets. The highest performance of
the tucSage system compared to the SAIL-GRS
system may be attributed to the use of a model for
fragment selection. Interestingly, the simple vari-
ation of the TF?IDF scheme used by the SAIL
system achieved very good results being a close
second performer. The UNIBI system proposed
a very interesting new application of the frame-
work of topic modeling to the task of grammar in-
duction, however, the respective performance does
not exceed the state-of-the-art. The combination
of the tucSage and SAIL-GRS systems could give
better results.
team Weighted Unweighted
Pr. Rec. F-m. Pr. Rec. F-m.
Travel English
Baseline 0.40 0.69 0.51 0.38 0.67 0.48
tucSage1
?
0.60 0.73 0.66 0.59 0.74 0.66
tucSage2 0.59 0.72 0.65 0.59 0.74 0.65
tucSage3 0.69 0.67 0.68 0.66 0.69 0.67
SAIL-GRS 0.54 0.62 0.58 0.57 0.66 0.61
Biel 0.13 0.39 0.20 0.09 0.34 0.14
Travel Greek
Baseline 0.17 0.65 0.26 0.16 0.73 0.26
tucSage1
?
0.47 0.58 0.52 0.55 0.72 0.62
tucSage2 0.46 0.53 0.49 0.50 0.59 0.54
tucSage3 0.51 0.48 0.49 0.52 0.56 0.54
SAIL-GRS 0.46 0.51 0.49 0.49 0.62 0.55
Biel - - - - - -
Tourism English
Baseline 0.80 0.94 0.87 0.82 0.94 0.87
tucSage1
?
0.79 0.94 0.86 0.76 0.91 0.83
tucSage2 0.78 0.93 0.85 0.73 0.90 0.80
tucSage3 0.80 0.93 0.86 0.77 0.90 0.83
SAIL-GRS 0.75 0.90 0.82 0.75 0.90 0.82
Biel 0.04 0.14 0.06 0.02 0.08 0.04
Finance English
Baseline 0.48 0.78 0.60 0.40 0.63 0.49
tucSage1
?
0.61 0.81 0.70 0.43 0.54 0.48
tucSage2 0.55 0.74 0.63 0.40 0.51 0.45
tucSage3 0.52 0.67 0.58 0.39 0.43 0.41
SAIL-GRS 0.78 0.78 0.78 0.67 0.62 0.65
Biel 0.22 0.30 0.25 0.06 0.18 0.09
Average over all four tasks
Baseline 0.46 0.73 0.56 0.44 0.74 0.53
tucSage1
?
0.62 0.77 0.69 0.58 0.73 0.65
tucSage2 0.60 0.73 0.66 0.56 0.69 0.61
tucSage3 0.63 0.69 0.65 0.59 0.65 0.61
SAIL-GRS 0.63 0.70 0.67 0.62 0.70 0.66
Biel 0.13 0.28 0.17 0.06 0.20 0.09
Table 4: Weighted and unweighted precision, re-
call and f-measure for all systems. Best perfor-
mance per metric and dataset shown in bold.
Acknowledgements
The task organizers wish to thank Maria Gian-
noudaki and Maria Vomva for the editing of the
hand-crafted grammars used in this evaluation
task. The authors would like to thank the anony-
mous reviewer for the valuable comments and sug-
gestions to improve the quality of the paper. This
work has been partially funded by the SpeDial and
PortDial projects, supported by the EU Seventh
Framework Programme (FP7), with grant number
611396 and 296170 respectively.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
15
of the First Joint Conference on Lexical and Compu-
tational Semantics, pages 385?393.
Frederic Bechet, Benoit Favre, Alexis Nasr, and Math-
ieu Morey. 2014. Retrieving the syntactic structure
of erroneous ASR transcriptions for open-domain
spoken language understanding. In Proceedings of
the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 4125?4129.
Yonatan Bisk and Julia Hockenmaier. 2012. Simple
robust grammar induction with combinatory catego-
rial grammars. In Proceedings of the 26th Confer-
ence on Artificial Intelligence, pages 1643?1649.
Bart Cramer. 2007. Limitations of current grammar
induction algorithms. In Proceedings of the 45th
annual meeting of the ACL: Student Research Work-
shop, pages 43?48.
Katerina T. Frantzi and Sophia Ananiadou. 1997. Au-
tomatic term recognition using contextual cues. In
Proceedings of the International Joint Conference
on Artificial Intelligence, pages 41?46.
Spiros Georgiladakis, Christina Unger, Elias Iosif,
Sebastian Walter, Philipp Cimiano, Euripides Pe-
trakis, and Alexandros Potamianos. 2014. Fusion
of knowledge-based and data-driven approaches to
grammar induction. In Proceedings of Interspeech
(accepted).
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Elias Iosif and Alexandros Potamianos. 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. In Proceedings of Interspeech, pages
1609?1612.
Elias Iosif, Athanasios Tegos, Apostolos Pangos, Eric
Fosler-Lussier, and Alexandros Potamianos. 2006.
Unsupervised combination of metrics for semantic
class induction. In Proceedings of the International
Workshop on Spoken Language Technology (SLT),
pages 86?89.
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spiros Georgiladakis, and Gianluca Mameli. 2013.
Web data harvesting for speech understanding gram-
mar induction. In Proceedings of Interspeech, pages
2733?2737.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd an-
nual meeting of the ACL, pages 470?477.
Helen M. Meng and Kai-chung Siu. 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1):172?181.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vec-
tor Space Models and their Compositionality, pages
40?47.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proceedings of Interspeech, pages 9?12.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the ACL, pages 311?318.
Andrew N. Pargellis, Eric Fosler-Lussier, Chin-Hui
Lee, Alexandros Potamianos, and Augustine Tsai.
2004. Auto-induced semantic classes. Speech Com-
munication, 43(3):183?203.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw
text with cascaded finite state models. In Proceed-
ings of the 49th annual meeting of the ACL, pages
1077?1086.
Portdial. 2014a. PortDial project, final
report on automatic grammar induction
and evaluation D3.3. Technical report,
https://sites.google.com/site/portdial2/deliverables-
publications.
Portdial. 2014b. PortDial project, free
data deliverable D3.2. Technical report,
https://sites.google.com/site/portdial2/deliverables-
publications.
Aarne Ranta. 2004. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145?189.
Ruhi Sarikaya. 2008. Rapid bootstrapping of statisti-
cal spoken dialogue systems. Speech Communica-
tion, 50(7):580?593.
Abhinav Sethy, Shrikanth S. Narayanan, and Bhuvana
Ramabhadran. 2007. Data driven approach for lan-
guage model adaptation using stepwise relative en-
tropy minimization. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 177?180.
Ye-Yi Wang and Alex Acero. 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3-4):390?416.
16
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 668?672,
Dublin, Ireland, August 23-24, 2014.
tucSage: Grammar Rule Induction for Spoken Dialogue Systems via
Probabilistic Candidate Selection
Arodami Chorianopoulou
?
, Georgia Athanasopoulou
?
, Elias Iosif
? ?
,
Ioannis Klasinas
?
, Alexandros Potamianos
?
?
School of ECE, Technical University of Crete, Chania 73100, Greece
?
School of ECE, National Technical University of Athens, Zografou 15780, Greece
?
?Athena? Research Center, Marousi 15125, Greece
{achorianopoulou,gathanasopoulou,iklasinas}@isc.tuc.gr
iosife@telecom.tuc.gr, apotam@gmail.com
Abstract
We describe the grammar induction sys-
tem for Spoken Dialogue Systems (SDS)
submitted to SemEval?14: Task 2. A sta-
tistical model is trained with a rich fea-
ture set and used for the selection of can-
didate rule fragments. Posterior probabil-
ities produced by the fragment selection
model are fused with estimates of phrase-
level similarity based on lexical and con-
textual information. Domain and language
portability are among the advantages of
the proposed system that was experimen-
tally validated for three thematically dif-
ferent domains in two languages.
1 Introduction
A critical task for Spoken Dialogue Systems
(SDS) is the understanding of the transcribed user
input, that utilizes an underlying domain grammar.
An obstacle to the rapid deployment of SDS to
new domains and languages is the time-consuming
development of grammars that require human ex-
pertise. Machine-assisted grammar induction has
been an open research area for decades (K. Lari
and S. Young, 1990; S. F. Chen, 1995) aiming
to lower this barrier. Induction algorithms can
be broadly distinguished into resource-based, e.g.,
(A. Ranta, 2004), and data-driven, e.g., (H. Meng
and K.-C. Siu, 2002). The main drawback of
the resource-based paradigm is the requirement of
pre-existing knowledge bases. This is addressed
by the data-driven paradigm that relies (mostly)
on plain corpora. SDS grammars are built by uti-
lizing low- and high-level rules. Low-level rules
This work is licenced under a Creative Commons Attri-
bution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
are similar to gazetteers consisting of terminal en-
tries, e.g., list of city names. High-level rules can
be lexicalized as textual fragments (or chunks),
which are semantically defined on top of low-
level rules, e.g., ?depart from <City>?.
The data-driven induction of low-level rules is a
well-researched area enabled by various technolo-
gies including web harvesting for corpora creation
(Klasinas et al., 2013), term extraction (K. Frantzi
and S. Ananiadou, 1997), word-level similarity
computation (Pargellis et al., 2004) and cluster-
ing (E. Iosif and A. Potamianos, 2007). High-level
rule induction is a less researched area that poses
two main challenges: 1) the extraction and selec-
tion of salient candidate fragments from a corpus
that convey semantics relevant to the domain of in-
terests and 2) the organization of such fragments
(e.g., via clustering) according to their semantic
similarity. Despite the recent interest on phrase (J.
Mitchell and M. Lapata, 2010) and sentence simi-
larity, each respective problem remains open.
Next, our submission
1
for the Se-
mEval?14: Task2 is briefly described, which
constitutes a data-driven approach for inducing
high-level SDS grammar rules. At the system?s
core lies a statistical model for the selection of
textual fragments based on a rich set of features.
This set includes various lexical features, aug-
mented with statistics from n-gram language
models, as well as with heuristic features. The
candidate selection model posterior is fused
with a phrase-level semantic similarity metric.
Two different approaches are used for similarity
computation relying on the overlap of character
bigrams or context-based similarity according
to the distributional hypothesis of meaning.
The domain and language portability of the
proposed system is demonstrated by its successful
application across three different domains and
1
Please note that the last three authors of this submission
are among the organizers of this task.
668
two languages. All the four subtasks defined by
the organizers were completed with very good
performance that exceeds the baseline.
2 System Description
The basic functionality of the proposed system
is the mapping (assignment) of unknown textual
fragments into known high-level grammar rules.
Let E be the set of unknown fragments, while the
set of known rules is denoted byR. Each unknown
fragment f ?E is allowed to be mapped to a sin-
gle high-level rule r
s
?R, where 1? s? m and
m is the total number of rules in the grammar.
Figure 1: Overview of system architecture.
The system consists of three major components as
shown at the system architecture diagram in Fig.
1, specifically: 1) candidate selection: a set of
classifiers is built, one for each r
s
to select whether
f ? E is a candidate member of the specific rule
2
,
2) similarity computation between f and r
s
, and
3) mapping f to a high-level rule r
s
(denoted as
f 7? r
s
) according to the following model:
argmax
s
{p(r
s
|f)
w
S(f, r
s
)} : f 7? r
s
(1)
where p(r
s
|f) stands for the probability of f
belonging to rule r
s
and it is estimated via the
respective classifier. The similarity between
f and r
s
is denoted by S(f |r
s
), while w is
a fixed weight taking values in the interval
[0 ?). The fusion weight w controls the rela-
tive importance of the candidate selection and
semantic similarity modules, e.g., for w = 0
only the similarity metric S(f, r
s
) is used in the
decision. For example, consider the fragment f
?leaving <City>?. Also, assume two high-
level rules, namely, <ArrCity>={?arrive
2
The requirement for building a classifier for each gram-
mar rule is realistic for the case of SDS, especially for the typ-
ical iterative human-in-the-loop grammar development sce-
nario.
at <City>?,...} and <DepCity>=
{?depart <City>?,...}. According to (1)
f is mapped to the <DepCity> rule.
2.1 Candidate Selection
In this section, the features used for building the
candidate selection module for each r
s
? R are
briefly described. Given a pair (f ,r
s
) a two-class
statistical classification model that corresponds to
r
s
is used for estimating p(r
s
|f) in (1).
Definitions. A high-level rule r
s
can be con-
sidered as a set of fragments, e.g.,?depart
<City>?, ?leaving <City>?. For each
fragment there are two types of constituents,
namely, lexical (e.g., ?depart?,?leaving?)
and low-level rules (e.g., ?<City>?). The fol-
lowing features are extracted for r
s
considering its
respective fragments, as well as for f .
Shallow features. 1) the number of constituents
(i.e., tokens), 2) the count of lexical constituents
to the number of tokens, 3) the count of low-level
rules to the number of tokens, 4) the count of lex-
ical constituents that follow the right-most low-
level rule of the fragment, and 5) the count of low-
level rules that appear twice in a fragment.
Perplexity-based features. A fragment
?
f can
be represented as a sequence of tokens as
w
1
w
2
... w
z
. The perplexity of
?
f is defined as
PP (
?
f)=2
H(
?
f)
, where H(
?
f)=
1
z
log(p(
?
f)). p(
?
f)
stands for the probability of
?
f estimated using an
n-gram language model. Two PP values were
used as features computed for n=2, 3.
Features of lexical similarity. Four scores of lex-
ical similarity computed between f and r
s
were
used as features. Let N
s
denote the set of frag-
ments that are included in the training set of each
rule r
s
. The following metrics were employed
for computing the similarity between the unknown
fragment f and a fragment f
s
? N
s
: 1) the nor-
malized longest common subsequence (Stoilos et
al., 2005) denoted as S
C
, 2) the normalized over-
lap in character bigrams that is denoted as S
B
and
it is defined in (2), 3) a proposed variation of the
Levenshtein distance, S
L
, defined as S
L
(f, f
s
) =
l
1
?L(f,f
s
)
l
1
+d
, where l
1
and l
2
are the lengths (in char-
acters) of the lengthiest and the shortest fragment
between f and f
s
, respectively, while d= l
1
? l
2
.
L(.) stands for the Levenshtein distance (V. I. Lev-
enshtein, 1966; R. A. Wagner and M. J. Fischer,
1974). 4) if f and f
s
differ by one token exactly
S
L
is applied, otherwise their similarity is set to
0. Regarding S
C
and S
B
, the similarity between
669
f and r
s
was estimated as the maximum similarity
yielded when computing the similarities between
f and each f
s
?N
s
. For the rest metrics, the sim-
ilarity between f and r
s
was estimated by averag-
ing the |N
s
| similarities computed between f and
each f
s
?N
s
.
Heuristic features. Considering an unknown
fragment f and the set of training fragments N
s
corresponding to rule r
s
, in total nine features
were used: 1) the difference between the aver-
age length (in tokens) of fragments in N
s
and the
length of f , 2) the difference between the average
number of low-level rules in N
s
and the number
of low-level rules in f , 3) as 2) but considering
the lexical constituents instead of low-level rules,
4) the number of low-level rules shared between
N
s
and f , 5) as 4) but considering the lexical con-
stituents instead of low-level rules, 6) a boolean
function that equals 1 if f is a substring of at least
one f
s
? N
s
, 7) a boolean function that equals 1 if
f shares the same lexical constituents at least one
f
s
? N
s
, 8) a boolean function that equals 1 if f
is shorter by one token compared to any f
s
? N
s
,
9) a boolean function that equals 1 if f is lengthier
by one token compared to any f
s
? N
s
.
Selection. The aforementioned features are used
for building a binary classifier for each r
s
? R,
where 1 ? s ? m, for deciding whether f can
be regarded as a candidate member of r
s
or not.
Given an unknown fragment f these classifiers are
employed for estimating in total m probabilities
p(r
s
|f).
2.2 Similarity Metrics
Here, two types of similarity metrics are defined,
which are used for estimating S(f, r
s
) in (1).
String-based similarity. Consider two fragments
f
i
and f
j
whose sets of character bigrams are de-
noted as M
i
and M
j
, respectively. Also, M
min
=
min(|M
i
|, |M
j
|) and M
max
= max(|M
i
|, |M
j
|
). The similarity between f
i
and f
j
is based on
the overlap of their respective character bigrams
defined as (Jimenez et al., 2012):
S
B
(f
i
, f
j
) =
|M
i
?M
j
|
?M
max
+ (1? ?)M
min
, (2)
where 0??? 1, while, here we use ?=0.5. The
similarity between a fragment f and a rule r
s
is
computed by averaging the similarities computed
between f and each f
s
?N
s
.
Context-based similarity. This is a corpus-based
metric relying on the distributional hypothesis of
meaning suggesting that similarity of context im-
plies similarity of meaning (Z. Harris, 1954). A
contextual window of size 2K+1 words is cen-
tered on the fragment of interest f
i
and lexical
features are extracted. For every instance of f
i
in
the corpus the K words left and right of f
i
for-
mulate a feature vector v
i
. For a given value of K
the context-based semantic similarity between two
fragments, f
i
and f
j
, is computed as the cosine of
their feature vectors: S
K
(f
i
, f
j
) =
v
i
.v
j
||v
i
|| ||v
j
||
. The
elements of feature vectors can be weighted ac-
cording various schemes (E. Iosif and A. Potami-
anos, 2010), while, here we use a binary scheme.
The similarity between a fragment f and a rule
r
s
is computed by averaging the similarities com-
puted between f and each f
s
?N
s
.
2.3 Mapping of Unknown Fragments
The output of the described system is the mapping
of a fragment f to a single (i.e., one-to-one assign-
ment) high-level rule r
s
? R, where 1 ? s ? m.
This is achieved by applying (1). The p(r
s
|f)
probabilities were estimated as described in Sec-
tion 2.1. The S(f, r
s
) similarities were estimated
using either S
K
or S
B
defined in Section 2.2.
3 Datasets and Experiments
Datasets. The data was organized with respect to
three different domains: 1) air travel (flight book-
ing, car rental etc.), 2) tourism (information for
city guide), and 3) finance (currency exchange). In
total, there are four separate datasets: two datasets
for the air travel domain in English (EN) and
Greek (GR), one dataset for the tourism domain
in English, and one dataset for the finance domain
in English.
The number of high-level rules for each dataset
Domain #rules #train frag. #test frag.
Travel:EN 32 982 284
Travel:GR 35 956 324
Tourism:EN 24 1004 285
Finance:EN 9 136 37
Table 1: Number of rules and train/test fragments.
are shown in Table 1, along with the number
of fragments included in training and test data.
Experiments. Regarding the computation of
perplexity-based features (defined in Section 2.1)
the SRILM toolkit (A. Stolcke, 2002) was used.
The n-gram probabilities were estimated over a
corpus that was created by aggregating all the
670
valid fragments included in the training data.
For the computation of the context-based similar-
ity metric S
K
(defined in Section 2.2) a corpus
of web-harvested data was created for each do-
main/language. The context window size K was
Domain # sentences
Travel:EN 5721
Travel:GR 6359
Tourism:EN 829516
Finance:EN 168380
Table 2: Size of corpora used in S
K
metric.
set to 1. The size of the used corpora are presented
Table 2, while the process of corpus creation is
detailed in (Klasinas et al., 2013). The classifiers
used for the candidate selection module, described
in Section 2.1 were random forests with 50 trees
(L. Breiman, 2001).
4 Evaluation Metrics and Results
The proposed model defined by (1) was evaluated
in terms of weighted F-measure, (FM ). Initially,
we run our system using the training and develop-
ment set provided by the task organizers, in order
to tune the w and K parameters. The tuning was
conducted on the Travel English domain, while the
respective evaluation results are shown in Table 3
in terms of FM . We observe that the best re-
Weight w 0 1 50 500
FM 0.68 0.72 0.70 0.72
Table 3: Results for the tuning of w.
sults are achieved for w = 1 and w = 500. In
the case where w = 0 the rule mapping relies only
on the similarity metric. In addition, we exper-
imented with various values the context window
size K of the context-based similarity metric S
K
:
K = 1, 3, 7. For all values of K similar perfor-
mance was obtained (0.70). Given the aforemen-
Domains Baseline Run 1 Run 2 Run 3
Travel:EN 0.51 0.66 0.65 0.68
Travel:GR 0.26 0.52 0.49 0.49
Tourism:EN 0.87 0.86 0.85 0.86
Finance:EN 0.60 0.70 0.63 0.58
UA 0.56 0.69 0.66 0.65
WA 0.52 0.66 0.64 0.65
Table 4: Official results.
tioned tuning the following values were selected
for the official runs: w = 1, w = 500 and K = 1.
In total, three system runs were submitted:
Run 1. The character bigram similarity metric was
used, while w was set to 1.
Run 2. The context-based similarity metrics was
used with K = 1, while w was set to 1.
Run 3. The character bigram similarity metric was
used, while w was set to 500.
The results for the aforementioned runs, along
with the baseline performance are shown in Ta-
ble 4. An overview of the participating systems
suggests that our submission achieved the high-
est performance for almost all domains and lan-
guages. The weighted (WA) and unweighted (UA)
average across the 4 datasets are also presented,
where the weight depends on the number of rules
in the dataset. Using these measures, our main
run (Run 1) obtained the best results. We ob-
serve that the performance is consistently worse
for Runs 2 and 3, with the exception of the Travel
English dataset. Comparing the performance of
Runs 1 and 2, we observe that the character bigram
metric consistently outperforms the context-based
one. For individual datasets, our system underper-
forms for the Finance (in Run 3) and the Tourism
domain (in all Runs). For the case of the Finance
domain this may be attributed to the relatively lim-
ited training data.
5 Conclusions
We proposed a supervised grammar induction sys-
tem using the fusion of a grammar fragment se-
lection and similarity estimation modules. The
best configuration of our system was Run 1 which
achieved the highest performance compared to
other submissions, in almost all domains. To sum-
marize, 1) the selection module boost the sys-
tem?s performance significanlty, 2) the high per-
formance in different domains is a promising indi-
cator for domain and language portability. Future
work should involve the implementation of more
complex features for the candidate selection algo-
rithm and further investigation of phrase level sim-
ilarity metrics.
Acknowledgements
This work has been partially funded by the
projects: 1) SpeDial, and 2) PortDial, supported
by the EU Seventh Framework Programme (FP7),
with grant number 611396 and 296170, respec-
tively.
671
References
Elias Iosif and Alexandros Potamianos. 2010. Un-
supervised semantic similarity computation between
terms using web documents. IEEE Transactions on
Knowledge and Data Engineering, 22(11), pp. 1637-
1647.
Sergio Jimenez, Claudia Becerra and Alexander Gel-
bukh. 2012. Soft Cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pp. 449-453
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spyros Georgiladakis and Gianluka Mameli. 2013.
Web data harvesting for speech understanding
grammar induction. in Proceedings of the Inter-
speech.
Helen M. Meng and Kai-Chung Siu 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1), pp. 172-181.
PortDial Project free data deliverable D3.1.
https://sites.google.com/site/portdial2/deliverables-
publication
Andreas Stolcke 2002 Srilm-an extensible language
modeling toolkit in Proceedings of the Interspeech
2002
Karim Lari and Steve J. Young 2002. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1), pp. 35-56.
Stanley F. Chen 1995. Bayesian grammar induction
for language modeling. in Proceedings of the 33rd
annual meeting of ACL
Zellig Harris 1954. Distributional structure. Word,
10(23), pp. 146-162.
Rebecca Hwa 1999. Supervised grammar induction
using training data with limited constituent informa-
tion. in Proceedings of the 37th annual meeting of
ACL
Matthew Lease, Eugene Charniak, and Mark Johnson
2005. Parsing and its applications for conversa-
tional speech. in Proceedings of Acoustics, Speech,
and Signal Processing (ICASSP)
Vladimir I. Levenshtein 1966. Binary codes capable
of correcting deletions, insertions and reversals. in
Soviet physics doklady, 10(8), pp. 707-710.
Leo Breiman 2001. Random forests. in Machine
Learning, 45(1), pp. 5-32.
Dan Jurafsky and James H. Martin 2009. Speech
and language processing an introduction to natural
language processing, computational linguistics, and
speech. Pearson Education Inc
Giorgos Stoilos, Giorgos Stamou, and Stefanos Kollias
2005. A string metric for ontology alignment. in
The Semantic WebISWC, pp. 624637
Robert A. Wagner and Michael J. Fisher 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1), pp. 168-173
Katerina Frantzi and Sophia Ananiadou 1997. Au-
tomatic term recognition using contextual cues. in
Proceedings of International Joint Conferences on
Artificial Intelligence
Elias Iosif and Alexandros Potamianos 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. in Proceedings of Interspeech
Jeffrey Mitchell and Mirela Lapata 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388-1429.
Ye-Yi Wang and Alex Acero 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3), pp. 360-416.
Eric Brill 1992. A simple rule-based part of speech
tagger. in Proceedings of the workshop on Speech
and Natural Language
Alexander Clark 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. in Proceedings of the 2001 work-
shop on Computational Natural Language Learning
Benjamin Snyder, Tahira Naseem, and Regina Barzilay
2009. Unsupervised multilingual grammar induc-
tion. in Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL
Aarne Ranta 2009. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming: 14(2), pp. 145-189
Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee,
Alexandros Potamianos and Augustine Tsai 2009.
Auto-induced Semantic Classes. Speech Communi-
cation: 43(3), pp. 183-203
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. in Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*Sem), pp. 385-393
672
