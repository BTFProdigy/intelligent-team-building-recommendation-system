Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 142?145,
Paris, October 2009. c?2009 Association for Computational Linguistics
Application of feature propagation to dependency parsing 
Kepa Bengoetxea Koldo Gojenola 
IXA NLP Group, Technical School of Engineering, Bilbao 
University of the Basque Country, Plaza La Casilla 3, 48012, Bilbao 
kepa.bengoetxea@ehu.es, koldo.gojenola@ehu.es 
Abstract 
This paper presents a set of experiments per-
formed on parsing the Basque Dependency 
Treebank. We have applied feature propaga-
tion to dependency parsing, experimenting the 
propagation of several morphosyntactic fea-
ture values. In the experiments we have used 
the output of a parser to enrich the input of a 
second parser. Both parsers have been gener-
ated by Maltparser, a freely data-driven de-
pendency parser generator. The transforma-
tions, combined with the pseudoprojective 
graph transformation, obtain a LAS of 77.12% 
improving the best reported results for Basque. 
1 Introduction 
This work presents several experiments per-
formed on dependency parsing of the Basque 
Dependency Treebank (BDT, Aduriz et al 
2003). We have experimented the idea of feature 
propagation through dependency arcs, in order to 
help the parser. Feature propagation has been 
used in classical unification-based grammars as a 
means of propagating linguistic information 
through syntax trees. We apply this idea in the 
context of inductive dependency parsing, com-
bining a reduced set of linguistic principles that 
express feature propagation among linguistic 
elements with Maltparser (Nivre et al 2007a), an 
automatic dependency parser generator. 
We have concentrated on propagating several 
morphosyntactic feature values from: a) auxiliary 
verbs to the main verb, b) the last constituent to 
the head noun, in noun phrases c)  the last con-
junct to the conjunction, in coordination.  
This work was developed in the context of de-
pendency parsing exemplified by the CoNLL 
shared task on dependency parsing in years 2006 
and 2007 (Nivre et al 2007b), where several sys-
tems had to compete analyzing data from a typo-
logically varied range of 11 languages. The tree-
banks for all languages were standardized using 
a previously agreed CoNLL-X format (see Fig-
ure 1). BDT was one of the evaluated treebanks, 
which will allow a direct comparison of results. 
Many works on treebank parsing have dedi-
cated an effort to the task of pre-processing train-
ing trees (Nilsson et al 2007). When these ap-
proaches have been applied to dependency pars-
ing several works (Nilsson et al 2007; Ben-
goetxea and Gojenola 2009) have concentrated 
on modifying the structure of the dependency 
tree, changing the shape of the graph. In contrast, 
rather than modifying the tree structure, we will 
experiment changing the information contained 
in the nodes of the tree. This approach requires 
having an initial dependency tree in order to ap-
ply the feature propagation process, which will 
be obtained by means of a standard trained 
model. This way, the features will be propagated 
through some incorrect dependency arcs, and the 
process will be dependent on the reliability of the 
initial arcs. After enriching the tree, a second 
parsing model will try to use this new informa-
tion to improve the standard model. This process 
can also be seen as an example of stacked learn-
ing (Martins et al 2008, Nivre and McDonald 
2008) where a second parser is used to improve 
the performance of a first one. 
The rest of the paper is organized as follows. 
Section 2 presents the main resources used in this 
work. Section 3 presents three different propos-
als for the propagation of the most important 
morphological features. Next, section 4 will 
evaluate the results of each transformation, and 
the last section outlines the main conclusions. 
2 Resources 
This section will describe the main elements that 
have been used in the experiments. First, subsec-
tion 2.1 will present the Basque Dependency 
Treebank data, while subsection 2.2 will describe 
the main characteristics of Maltparser, a state of 
the art and data-driven dependency parser. 
2.1 The Basque Dependency Treebank 
The BDT can be considered a pure dependency 
treebank, as its initial design considered that all 
the dependency arcs would connect sentence to-
kens. Although this decision had consequences 
on the annotation process, its simplicity is also 
an advantage when applying several of the most 
142
efficient parsing algorithms. The treebank con-
sists of 55,469 tokens forming 3,700 sentences, 
334 of which were used as test data. 
(1) Etorri (come) dela (that-has) eta 
(and) joan  (go) dela (that-has) esan 
(tell) zien (did) mutil (boy) 
txikiak(the-little) 
He told the little boy that he has come 
and he has gone 
 
Figure 1 contains an example of a sentence 
(1), annotated in the CoNLL-X format. The text 
is organized in eight tab-separated columns: 
word-number, form, lemma, category , subcate-
gory, morphological features, and the depend-
ency relation (headword + dependency). Basque 
is an agglutinative language and it presents a 
high power to generate inflected word-forms. 
The information in Figure 1 has been simplified 
due to space reasons, as typically the Features 
column will contain many morphological fea-
tures, which are relevant for parsing. 
2.2 Maltparser 
Maltparser (Nivre et al 2007a) is a state of the 
art dependency parser that has been successfully 
applied to typologically different languages and 
treebanks. While several variants of the base 
parser have been implemented, we will use one 
of its standard versions (Maltparser version 0.4). 
The parser obtains deterministically a depend-
ency tree in linear-time in a single pass over the 
input. To determine which is the best action at 
each parsing step, the parser uses history-based 
feature models and discriminative machine learn-
ing. In all the following experiments, we made 
use of a SVM classifier. The specification of the 
features used for learning can in principle be any 
kind of data in Figure 1 (such as word-form, 
lemma, category or morphological features). 
3 Experiments  
We applied the following steps: 
a) Application of feature propagation to the 
training data, using the gold standard arcs, ob-
taining a ?enriched training data?.  
b) Training Maltparser on the ?enriched train-
ing data? to generate a ?enriched parser?.  
c) Training Maltparser with the training data, 
without any transformation, to generate a 
?standard parser?.  
d) Parse the test data with the ?standard 
parser?, obtaining the ?standard output?.  
e) Apply feature propagation to the ?standard 
output?, using the dependency arcs given by 
the parser (with some incorrect arcs), obtain-
ing the ?standard parser?s enriched output?. 
f) Finally, parsing the ?standard parser?s en-
riched output? with the ?enriched parser?, 
Index Word Lemma Category Subcategory Features  Head Dependency 
1 etorri etorri V  V  _   3 lot 
2 dela izan AUXV  AUXV  SC:CMP|SUBJ:3S 1 auxmod 
3 eta eta CONJ  CONJ  _   6 ccomp_obj 
4 joan joan V  V  _   3 lot 
5 dela izan AUXV  AUXV  SC:CMP|SUBJ:3S 4 auxmod 
6 esan esan V  V  _   0 ROOT 
7 zien *edun AUXV  AUXV  SUBJ:3S|OBJ:3P 6 auxmod 
8 mutil mutil NOUN  NOUN  _   6 ncsubj 
9 txikiak txiki ADJ  ADJ  CASE:ERG|NUM:S 8 ncmod 
10 . . PUNT  PUNT_PUNT _   9 PUNC 
 
Figure 1: Example of a BDT sentence in the CONLL-X format 
(V = main verb, AUXV = auxiliary verb, SC = subordinated clause, CMP = completive, ccomp_obj = clausal 
complement object,  SUBJ:3S: subject in 3rd person sing., OBJ:3P: object in 3rd person pl.). 
 
auxmod 
coord 
auxmod auxmod 
coord 
ccomp_obj 
 
 
 
 
Etorri da+la  eta joan da+la  esan zien  mutil txiki+ak  
come has+he+that and go has+he+that tell did+he+them  boy little+the 
V AUXV+3S+CMP CONJ V AUXV+3S+CMP V    AUXV+SUBJ3S+OBJ3P  NOUN ADJ+ERG 
Figure 2: Dependency tree for the sentence in Figure 1. 
(V = main verb; AUXV: auxiliary verb; CMP: completive subordinated mark; CONJ: conjunction; ERG: ergative case). 
 
ncmod 
ncsubj 
143
evaluating the output with the gold test data. 
We have applied three types of feature propa-
gation of the most important morphological fea-
ture values: a) from auxiliary verbs to the main 
verb (verb phrases) b) from post-modifiers to the 
head noun (noun phrases) c) from the last con-
junct to the conjunction (coordination). This was 
done because Basque is a head final language, 
where many relevant features are located at the 
end of constituents. Figure 3 shows (dotted lines) 
the arcs that will propagate features from child to 
parent. The three transformations will be de-
scribed in the following subsections.  
3.1 Verb compounds 
In BDT the verbal elements are organized around 
the main verb, but much syntactically relevant 
verbal information, like subordination type, as-
pect, tense and agreement usually appear at-
tached to the auxiliary verb, which is the de-
pendent. Its main consequence for parsing is that 
the elements bearing the relevant information for 
parsing are situated far in the tree with respect to 
their head. In Figure 2, we can see that the mor-
pheme ?la, indicating a subordinated completive 
sentence, appears down in the tree, and this could 
affect the correct attachment of the two coordi-
nated verbs to the conjunction (eta), as conjunc-
tions should link elements showing similar 
grammatical features (-la in this example). Simi-
larly, it could affect the decision about the de-
pendency type of eta (and) with respect to the 
main verb esan (to say), as the dependency rela-
tion ccomp_obj is defined by means of the ?la 
(completive) morpheme, far down in the tree. 
Figure 3 shows the effect of propagating the 
completive feature value (CMP) from the auxil-
iary verb to the main verb through the auxmod 
(auxiliary modifier) relation. 
3.2 Noun Phrases 
In noun phrases and postpositional phrases, the 
most important morphological feature values 
(case and number) are situated in the last post-
modifier after the noun. Figure 3 shows the ef-
fect of propagating the ergative (ERG) case fea-
ture value from the adjective (the last constituent 
of the noun phrase) to the noun through the rela-
tion ncmod (non-clausal modifier).  
3.3 Coordination 
Coordination in BDT was annotated in the so 
called Prague Style, where the conjunction is 
taken as the head, and the conjuncts depend on it. 
Basque is head final, so usually the last conjunct 
contains syntactically relevant features. We ex-
perimented the promotion of the category, case 
and subordination information from the last con-
junct to the conjunction. In the example in Figure 
3, the conjunction (eta) receives a new feature 
(HV for Head:Verb) from its dependent. This can 
be seen as an alternative to (Nilsson et al 2007) 
who transform dependency arcs. 
4 Evaluation 
Evaluation was performed dividing the treebank 
in three sets: training set (45,000 tokens), devel-
opment and test sets (5,000 tokens each). Train-
ing and testing of the system have been per-
formed on the same datasets presented at the 
CoNLL 2007 shared task, which will allow for a 
direct comparison. Table 1 presents the Labeled 
Attachment Score (LAS) of the different tests on 
development and test data. The first row presents 
the best system score (76.94% LAS) in CoNLL 
2007. This system combined six variants of a 
base parser (Maltparser). The second row shows 
the single Maltparser approach which obtained 
the fifth position. Row 3 presents Bengoetxea 
and Gojenola?s results (76.80% LAS) when ap-
plying graph transformations (pseudo-projective, 
coordination and verb groups) to Basque, in the 
spirit of Nilsson et al (2007). Row 4 shows our 
results after applying several feature optimiza-
tions, which we will use as our baseline. 
auxmod 
coord 
auxmod auxmod 
coord 
ccomp_obj  
 
 
 
 
Etorri  da+la  eta  joan  da+la  esan zien    mutil  txiki+ak  
come  has+he+that and   go  has+he+that tell did+he+them   boy    little+the 
V+CMP   AUXV+3S+CMP CONJ+HV    V+CMP AUXV+3S+CMP      V    AUXV+SUBJ3S+OBJ3P NOUN+ERG ADJ+ERG 
Figure 3: Dependency tree after propagating the morphological features. 
 
ncmod 
ncsubj 
144
Feature propagation in verb groups (PVG) im-
proves LAS in almost 0.5% (row 6 in Table 1). 
While coordination and case propagation do not 
improve significantly the accuracy by themselves 
(rows 7 and 8), their combination with PVG (verb 
groups) significantly increases LAS (+0.86%, 
see row 10). Looking at the accuracy of the de-
pendency arcs used for feature propagation, aux-
liary verbs are the most reliable elements, as 
their arcs (linking it to its head, the main verb) 
have 97% precision and 98% recall. This is in 
accord with PVG giving the biggest increase, 
while arcs related to coordination (63% precision 
and 65% recall) give a more modest contribution. 
BDT contains 2.9% of nonprojective arcs, so 
we experimented the effect of combining the 
pseudoprojective transformation (Nilsson et al 
2007) with feature propagation, obtaining a LAS 
of 77.12%, the best reported results for the BDT. 
5 Conclusions 
We have performed a set of experiments using 
the output of a parser to enrich the input of a 
second parser, propagating the relevant morpho-
logical feature values through dependency arcs. 
The best system, after applying three types of 
feature propagation, obtains a 77.12% LAS 
(2.05% improvement over the baseline) on the 
test set, which is the best reported result for 
Basque dependency parsing, improving the better 
published result for a combined parser (76.94%). 
Acknowledgements 
This research was supported by the Basque Gov-
ernment (EPEC-RS, S-PE08UN48) and the Uni-
versity of the Basque Country (EHU-EJIE, 
EJIE07/05).  
References  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque dependency 
treebank. Treebanks and Linguistic Theories. 
Kepa Bengoetxea and Koldo Gojenola. 2009. Explor-
ing Treebank Transformations in Dependency 
Parsing. Proceedings of RANLP?2009. 
Johan Hall, Jens Nilsson, Joakim Nivre J., Eryigit G., 
Megyesi B., Nilsson M. and Saers M. 2007. Single 
Malt or Blended? A Study in Multilingual 
Parser Optimization. Proceedings of the CoNLL 
Shared Task EMNLP-CoNLL. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, 
Eric P. Xing. 2008. Stacking Dependency Pars-
ing. EMNLP-2008. 
Jens Nilsson, Joakim Nivre and Johan Hall. 2007. 
Tree Transformations for Inductive Depend-
ency Parsing. Proceedings of the 45th ACL. 
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A., 
G?lsen Eryi?it, Sandra K?bler, Marinov S., and 
Edwin Marsi. 2007a. MaltParser: A language-
independent system for data-driven depend-
ency parsing. Natural Language Engineering.  
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared 
Task on Dependency Parsing. EMNLP-CoNLL. 
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graphbased and transition-based depend-
ency parsers. ACL-2008. 
  LAS 
 System Development Test 
1 Nivre et al 2007b (CoNLL 2007) -  76.94%  
2 Hall et al 2007 (CoNLL 2007)   74.99%  
3 Bengoetxea and Gojenola 2009   76.80%  
4 Feature optimization (baseline) 77.46%  75.07%  
5 Proj 78.16%  (+0.70) *75.99%  (+0.92) 
6 PVG 78.14%  (+0.68) 75.54%  (+0.47) 
7 PCOOR 77.36%  (-0.10) 75.22%  (+0.15) 
8 PCAS 77.32%  (-0.14) 74.86%  (-0.21) 
9 PVG + PCAS  78.53%  (+1.09) 75.42%  (+0.35) 
10 PCOOR + PVG + PCAS  78.31%  (+0.85) *75.93%  (+0.86) 
11 PCOOR + PVG 78.25%  (+0.79) *75.93%  (+0.86) 
12 Proj + PVG  78.91%  (+1.45) *76.12%  (+1.05) 
13 Proj + PVG + PCOOR  + PCAS 78.31%  (+0.85) *77.12%  (+2.05) 
Table 1. Evaluation results  
(Proj: Pseudo-projective, PVG, PCAS, PCOOR: Propagation on verb compounds, case (NPs)  and coordination; *: statistically 
significant in McNemar's test with respect to labeled attachment score with p < 0.01) 
 
145
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699?703,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Dependency Parsing with Semantic Classes 
Eneko Agirre*, Kepa Bengoetxea*, Koldo Gojenola*, Joakim Nivre+ 
* Department of Computer Languages and Systems, University of the Basque Country 
UPV/EHU 
+ Department of Linguistics and Philosophy, Uppsala University 
{e.agirre, kepa.bengoetxea, koldo.gojenola}@ehu.es joakim.nivre@lingfil.uu.se 
 
 
 
 
Abstract  
This paper presents the introduction of 
WordNet semantic classes in a dependency 
parser, obtaining improvements on the full 
Penn Treebank for the first time. We tried 
different combinations of some basic se-
mantic classes and word sense disambigua-
tion algorithms. Our experiments show that 
selecting the adequate combination of se-
mantic features on development data is key 
for success. Given the basic nature of the 
semantic classes and word sense disam-
biguation algorithms used, we think there is 
ample room for future improvements. 
1 Introduction 
Using semantic information to improve parsing 
performance has been an interesting research ave-
nue since the early days of NLP, and several re-
search works have tried to test the intuition that 
semantics should help parsing, as can be exempli-
fied by the classical PP attachment experiments 
(Ratnaparkhi, 1994). Although there have been 
some significant results (see Section 2), this issue 
continues to be elusive. In principle, dependency 
parsing offers good prospects for experimenting 
with word-to-word-semantic relationships. 
We present a set of experiments using semantic 
classes in dependency parsing of the Penn Tree-
bank (PTB). We extend the tests made in Agirre et 
al. (2008), who used different types of semantic 
information, obtaining significant improvements in 
two constituency parsers, showing how semantic 
information helps in constituency parsing.  
As our baseline parser, we use MaltParser 
(Nivre, 2006). We will evaluate the parser on both 
the full PTB (Marcus et al 1993) and on a sense-
annotated subset of the Brown Corpus portion of 
PTB, in order to investigate the upper bound per-
formance of the models given gold-standard sense 
information, as in Agirre et al (2008). 
2 Related Work 
Agirre et al (2008) trained two state-of-the-art sta-
tistical parsers (Charniak, 2000; Bikel, 2004) on 
semantically-enriched input, where content words 
had been substituted with their semantic classes. 
This was done trying to overcome the limitations 
of lexicalized approaches to parsing (Magerman, 
1995; Collins, 1996; Charniak, 1997; Collins, 
2003), where related words, like scissors and knife 
cannot be generalized. This simple method allowed 
incorporating lexical semantic information into the 
parser. They tested the parsers in both a full pars-
ing and a PP attachment context. The experiments 
showed that semantic classes gave significant im-
provement relative to the baseline, demonstrating 
that a simplistic approach to incorporating lexical 
semantics into a parser significantly improves its 
performance. This work presented the first results 
over both WordNet and the Penn Treebank to show 
that semantic processing helps parsing.  
Collins (2000) tested a combined parsing/word 
sense disambiguation model based in WordNet 
which did not obtain improvements in parsing. 
Koo et al (2008) presented a semisupervised 
method for training dependency parsers, using 
word clusters derived from a large unannotated 
corpus as features. They demonstrate the effective-
ness of the approach in a series of dependency 
parsing experiments on PTB and the Prague De-
pendency Treebank, showing that the cluster-based 
features yield substantial gains in performance 
across a wide range of conditions. Suzuki et al 
(2009) also experiment with the same method 
combined with semi-supervised learning. 
699
Ciaramita and Attardi (2007) show that adding 
semantic features extracted by a named entity tag-
ger (such as PERSON or MONEY) improves the 
accuracy of a dependency parser, yielding a 5.8% 
relative error reduction on the full PTB. 
Candito and Seddah (2010) performed experi-
ments in statistical parsing of French, where termi-
nal forms were replaced by more general symbols, 
particularly clusters of words obtained through 
unsupervised clustering. The results showed that 
word clusters had a positive effect. 
Regarding dependency parsing of the English 
PTB, currently Koo and Collins (2010) and Zhang 
and Nivre (2011) hold the best results, with 93.0 
and 92.9 unlabeled attachment score, respectively. 
Both works used the Penn2Malt constituency-to-
dependency converter, while we will make use of 
PennConverter (Johansson and Nugues, 2007). 
Apart from these, there have been other attempts 
to make use of semantic information in different 
frameworks and languages, as in (Hektoen 1997; 
Xiong et al 2005; Fujita et al 2007). 
3 Experimental Framework 
In this section we will briefly describe the data-
driven parser used for the experiments (subsection 
3.1), followed by the PTB-based datasets (subsec-
tion 3.2). Finally, we will describe the types of se-
mantic representation used in the experiments. 
3.1 MaltParser 
MaltParser (Nivre et al 2006) is a trainable de-
pendency parser that has been successfully applied 
to typologically different languages and treebanks. 
We will use one of its standard versions (version 
1.4). The parser obtains deterministically a de-
pendency tree in linear-time in a single pass over 
the input using two main data structures: a stack of 
partially analyzed items and the remaining input 
sequence. To determine the best action at each 
step, the parser uses history-based feature models 
and SVM classifiers. One of the main reasons for 
using MaltParser for our experiments is that it eas-
ily allows the introduction of semantic informa-
tion, adding new features, and incorporating them 
in the training model. 
3.2 Dataset 
We used two different datasets: the full PTB and 
the Semcor/PTB intersection (Agirre et al 2008). 
The full PTB allows for comparison with the state-
of-the-art, and we followed the usual train-test 
split. The Semcor/PTB intersection contains both 
gold-standard sense and parse tree annotations, and 
allows to set an upper bound of the relative impact 
of a given semantic representation on parsing. We 
use the same train-test split of Agirre et al (2008), 
with a total of 8,669 sentences containing 151,928 
words partitioned into 3 sets: 80% training, 10% 
development and 10% test data. This dataset is 
available on request to the research community. 
We will evaluate the parser via Labeled Attach-
ment Score (LAS). We will use Bikel?s random-
ized parsing evaluation comparator to test the 
statistical significance of the results using word 
sense information, relative to the respective base-
line parser using only standard features.  
We used PennConverter (Johansson and 
Nugues, 2007) to convert constituent trees in the 
Penn Treebank annotation style into dependency 
trees. Although in general the results from parsing 
Pennconverter?s output are lower than with other 
conversions, Johansson and Nugues (2007) claim 
that this conversion is better suited for semantic 
processing, with a richer structure and a more fine-
grained set of dependency labels. For the experi-
ments, we used the best configuration for English 
at the CoNLL 2007 Shared Task on Dependency 
Parsing (Nivre et al, 2007) as our baseline.  
3.3 Semantic representation and disambigua-
tion methods 
We will experiment with the range of semantic 
representations used in Agirre et al (2008), all of 
which are based on WordNet 2.1. Words in Word-
Net (Fellbaum, 1998) are organized into sets of 
synonyms, called synsets (SS). Each synset in turn 
belongs to a unique semantic file (SF). There are a 
total of 45 SFs (1 for adverbs, 3 for adjectives, 15 
for verbs, and 26 for nouns), based on syntactic 
and semantic categories. For example, noun se-
mantic files (SF_N) differentiate nouns denoting 
acts or actions, and nouns denoting animals, 
among others. We experiment with both full syn-
sets and SFs as instances of fine-grained and 
coarse-grained semantic representation, respec-
tively. As an example of the difference in these 
two representations, knife in its tool sense is in the 
EDGE TOOL USED AS A CUTTING 
INSTRUMENT singleton synset, and also in the 
ARTIFACT SF along with thousands of other 
700
words including cutter. Note that these are the two 
extremes of semantic granularity in WordNet. 
As a hybrid representation, we also tested the ef-
fect of merging words with their corresponding SF 
(e.g. knife+ARTIFACT). This is a form of seman-
tic specialization rather than generalization, and 
allows the parser to discriminate between the dif-
ferent senses of each word, but not generalize 
across words. For each of these three semantic rep-
resentations, we experimented with using each of: 
(1) all open-class POSs (nouns, verbs, adjectives 
and adverbs), (2) nouns only, and (3) verbs only. 
There are thus a total of 9 combinations of repre-
sentation type and target POS: SS (synset), SS_N 
(noun synsets), SS_V (verb synsets), SF (semantic 
file), SF_N (noun semantic files), SF_V (verb se-
mantic files), WSF (wordform+SF), WSF_N 
(wordform+SF for nouns) and WSF_V (for verbs).  
For a given semantic representation, we need 
some form of WSD to determine the semantics of 
each token occurrence of a target word. We ex-
perimented with three options: a) gold-standard 
(GOLD) annotations from SemCor, which gives 
the upper bound performance of the semantic rep-
resentation, b) first Sense (1ST), where all token 
instances of a given word are tagged with their 
most frequent sense in WordNet, and c) automatic 
Sense Ranking (ASR) which uses the sense re-
turned by an unsupervised system based on an in-
dependent corpus (McCarthy et al 2004). For the 
full Penn Treebank experiments, we only had ac-
cess to the first sense, taken from Wordnet 1.7. 
4 Results 
In the following two subsections, we will first pre-
sent the results in the SemCor/PTB intersection, 
with the option of using gold, 1st sense and auto-
matic sense information (subsection 4.1) and the 
next subsection (4.2) will show the results on the 
full PTB, using 1st sense information. All results 
are shown as labelled attachment score (LAS). 
4.1 Semcor/PTB (GOLD/1ST/ASR) 
We conducted a series of experiments testing: 
? Each individual semantic feature, which 
gives 9 possibilities, also testing different 
learning configurations for each one. 
? Combinations of semantic features, for in-
stance, SF+SS_N+WSF would combine the 
semantic file with noun synsets and word-
form+semantic file. 
Although there were hundreds of combinations, 
we took the best combination of semantic features 
on the development set for the final test. For that 
reason, the table only presents 10 results for each 
disambiguation method, 9 for the individual fea-
tures and one for the best combination. 
Table 1 presents the results obtained for each of 
the disambiguation methods (gold standard sense 
information, 1st sense, and automatic sense rank-
ing) and individual semantic feature. In all cases 
except two, the use of semantic classes is benefi-
 System            LAS 
Baseline  81.10  
SS 81.18 +0.08 
SS_N 81.40 +0.30 
SS_V *81.58 +0.48 
SF **82.05 +0.95 
SF_N
 81.51 +0.41 
SF_V 81.51 +0.41 
WSF 81.51 +0.41 
WSF_N 81.43 +0.33 
WSF_V *81.51 +0.41 
 
 
Gold 
SF+SF_N+SF_V+SS+WSF_N *81.74 +0.64 
SS 81.30 +0.20 
SS_N *81.56 +0.46 
SS_V *81.49 +0.39 
SF 81.00 -0.10 
SF_N
 80.97 -0.13 
SF_V **81.66 +0.56 
WSF 81.32 +0.22 
WSF_N *81.62 +0.52 
WSF_V **81.72 +0.62 
 
 
ASR 
SF_V+SS_V 81.41 +0.31 
SS 81.40 +0.30 
SS_N 81.39 +0.29 
SS_V *81.48 +0.38 
SF *81.59 +0.49 
SF_N
 81.38 +0.28 
SF_V *81.52 +0.42 
WSF *81.57 +0.46 
WSF_N 81.40 +0.30 
WSF_V 81.42 +0.32 
 
 
1ST 
SF+SS_V+WSF_N **81.92 +0.81 
Table 1. Evaluation results on the test set for the 
Semcor-Penn intersection. Individual semantic 
features and best combination. 
(**: statistically significant, p < 0.005; *: p < 0.05) 
 
701
cial albeit small. Regarding individual features, the 
SF feature using GOLD senses gives the best im-
provement. However, GOLD does not seem to 
clearly improve over 1ST and ASR on the rest of 
the features. Comparing the automatically obtained 
classes, 1ST and ASR, there is no evident clue 
about one of them being superior to the other. 
Regarding the best combination as selected in 
the training data, each WSD method yields a dif-
ferent combination, with best results for 1ST. The 
improvement is statistically significant for both 
1ST and GOLD. In general, the results in Table 1 
do not show any winning feature across all WSD 
algorithms. The best results are obtained when us-
ing the first sense heuristic, but the difference is 
not statistically significant. This shows that perfect 
WSD is not needed to obtain improvements, but it 
also shows that we reached the upperbound of our 
generalization and learning method. 
4.2 Penn Treebank and 1st sense 
We only had 1st sense information available for 
the full PTB. We tested MaltParser on the best 
configuration obtained for the reduced Sem-
cor/PTB on the full treebank, taking sections 2-21 
for training and section 23 for the final test. Table 
2 presents the results, showing that several of the 
individual features and the best combination give 
significant improvements. To our knowledge, this 
is the first time that WordNet semantic classes help 
to obtain improvements on the full Penn Treebank. 
It is interesting to mention that, although not 
shown on the tables, using lemmatization to assign 
semantic classes to wordforms gave a slight in-
crease for all the tests (0.1 absolute point approxi-
mately), as it helped to avoid data sparseness. We 
applied Schmid?s (1994) TreeTagger. This can be 
seen as an argument in favour of performing mor-
phological analysis, an aspect that is many times 
neglected when processing morphologically poor 
languages as English. 
We also did some preliminary experiments us-
ing Koo et al?s (2008) word clusters, both inde-
pendently and also combined with the WordNet-
based features, without noticeable improvements. 
5 Conclusions 
We tested the inclusion of several types of seman-
tic information, in the form of WordNet semantic 
classes in a dependency parser, showing that: 
? Semantic information gives an improvement 
on a transition-based deterministic depend-
ency parsing. 
? Feature combinations give an improvement 
over using a single feature. Agirre et al 
(2008) used a simple method of substituting 
wordforms with semantic information, 
which only allowed using a single semantic 
feature. MaltParser allows the combination 
of several semantic features together with 
other features such as wordform, lemma or 
part of speech. Although tables 1 and 2 only 
show the best combination for each type of 
semantic information, this can be appreci-
ated on GOLD and 1ST in Table 1. Due to 
space reasons, we only have showed the best 
combination, but we can say that in general 
combining features gives significant in-
creases over using a single semantic feature. 
? The present work presents a statistically sig-
nificant improvement for the full treebank 
using WordNet-based semantic information 
for the first time. Our results extend those of 
Agirre et al (2008), which showed im-
provements on a subset of the PTB. 
Given the basic nature of the semantic classes 
and WSD algorithms, we think there is room for 
future improvements, incorporating new kinds of 
semantic information, such as WordNet base con-
cepts, Wikipedia concepts, or similarity measures. 
 
 System            LAS 
Baseline  86.27  
SS *86.53 +0.26 
SS_N 86.33 +0.06 
SS_V *86.48 +0.21 
SF **86.63 +0.36 
SF_N
 *86.56 +0.29 
SF_V 86.34 +0.07 
WSF *86.50 +0.23 
WSF_N 86.25 -0.02 
WSF_V *86.51 +0.24 
 
 
1ST 
SF+SS_V+WSF_N *86.60 +0.33 
 
Table 1. Evaluation results (LAS) on the test 
set for the full PTB. Individual features and 
best combination. 
(**: statistically, p < 0.005; *: p < 0.05) 
 
702
References  
Eneko Agirre, Timothy Baldwin, and David Martinez. 
2008. Improving parsing and PP attachment perform-
ance with sense information. In Proceedings of ACL-
08: HLT, pages 317?325, Columbus, Ohio. 
Daniel M. Bikel. 2004. Intricacies of Collins? parsing 
model. Computational Linguistics, 30(4):479?511. 
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In Proceedings of the NAACL HLT 2010 First 
Workshop on Statistical Parsing of Morphologically-
Rich Language, Los Angeles, USA. 
M. Ciaramita and G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information, In Proceedings of the 10th In-
ternational Conference on Parsing Technology.  
Eugene Charniak. 1997. Statistical parsing with a con-
text-free grammar and word statistics. In Proc. of the 
15th Annual Conference on Artificial Intelligence 
(AAAI-97), pages 598?603, Stanford, USA. 
Eugene Charniak. 2000. A maximum entropy-based 
parser. In Proc. of the 1st Annual Meeting of the 
North American Chapter of Association for Compu-
tational Linguistics (NAACL2000), Seattle, USA. 
Michael J. Collins. 1996. A new statistical parser based 
on lexical dependencies. In Proc. of the 34th Annual 
Meeting of the ACL, pages 184?91, USA. 
Michael Collins. 2000. A Statistical Model for Parsing 
and Word-Sense Disambiguation. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora. 
Michael Collins. 2003. Head-driven statistical models 
for natural language parsing. Computational Linguis-
tics, 29(4):589?637. 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge. 
Sanae Fujita, Francis Bond, Stephan Oepen, and Taka-
aki Tanaka. 2007. Exploiting semantic information 
for HPSG parse selection. In Proc. of the ACL 2007 
Workshop on Deep Linguistic Processing. 
Richard Johansson and Pierre Nugues. 2007. Extended 
Constituent-to-dependency Conversion for English. 
In Proceedings of NODALIDA 2007, Tartu, Estonia. 
Erik Hektoen. 1997. Probabilistic parse selection based 
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08, pages 595?603, USA. 
Terry Koo, and Michael Collins. 2008. Efficient Third-
order Dependency Parsers. In Proceedings of ACL-
2010, pages 1?11, Uppsala, Sweden. 
Shari Landes, Claudia Leacock, and Randee I. Tengi. 
1998. Building semantic concordances. In Christiane 
Fellbaum, editor, WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, USA. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proc. of the 33rd Annual 
Meeting of the ACL, pages 276?83, USA. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn treebank. Computational 
Linguistics, 19(2):313?30. 
Diana McCarthy, Rob Koeling, Julie Weeds, and John 
Carroll. 2004. Finding predominant senses in 
untagged text. In Proc. of the 42nd Annual Meeting 
of the ACL, pages 280?7, Barcelona, Spain. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Text, Speech and Language Technology series, 
Springer. 2006, XI, ISBN: 978-1-4020-4888-3. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task 
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. Prague, Czech Republic. 
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 
1994. A maximum entropy model for prepositional 
phrase attachment. In HLT ?94: Proceedings of the 
Workshop on Human Language Technology, USA. 
Helmut Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. September 1994 
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Mi-
chael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for De-
pendency Parsing. In Proceedings of EMNLP, pages 
551?560. Association for Computational Linguistics. 
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and 
Yueliang Qian. 2005. Parsing the Penn Chinese 
Treebank with semantic knowledge. In Proc. of the 
2nd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05), Korea. 
Yue Zhang, and Joakim Nivre. 2011. Transition-Based 
Parsing with Rich Non-Local Features. In Proceed-
ings of the 49th Annual Meeting of the Association 
for Computational Linguistics. 
703
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649?655,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On WordNet Semantic Classes and Dependency Parsing
Kepa Bengoetxea?, Eneko Agirre?, Joakim Nivre?,
Yue Zhang*, Koldo Gojenola?
?University of the Basque Country UPV/EHU / IXA NLP Group
?Uppsala University / Department of Linguistics and Philology
? Singapore University of Technology and Design
kepa.bengoetxea@ehu.es, e.agirre@ehu.es,
joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,
koldo.gojenola@ehu.es
Abstract
This paper presents experiments with
WordNet semantic classes to improve de-
pendency parsing. We study the effect
of semantic classes in three dependency
parsers, using two types of constituency-
to-dependency conversions of the English
Penn Treebank. Overall, we can say that
the improvements are small and not sig-
nificant using automatic POS tags, con-
trary to previously published results using
gold POS tags (Agirre et al, 2011). In
addition, we explore parser combinations,
showing that the semantically enhanced
parsers yield a small significant gain only
on the more semantically oriented LTH
treebank conversion.
1 Introduction
This work presents a set of experiments to investi-
gate the use of lexical semantic information in de-
pendency parsing of English. Whether semantics
improve parsing is one interesting research topic
both on parsing and lexical semantics. Broadly
speaking, we can classify the methods to incor-
porate semantic information into parsers in two:
systems using static lexical semantic repositories,
such as WordNet or similar ontologies (Agirre et
al., 2008; Agirre et al, 2011; Fujita et al, 2010),
and systems using dynamic semantic clusters au-
tomatically acquired from corpora (Koo et al,
2008; Suzuki et al, 2009).
Our main objective will be to determine
whether static semantic knowledge can help pars-
ing. We will apply different types of semantic in-
formation to three dependency parsers. Specifi-
cally, we will test the following questions:
? Does semantic information in WordNet help
dependency parsing? Agirre et al (2011)
found improvements in dependency parsing
using MaltParser on gold POS tags. In this
work, we will investigate the effect of seman-
tic information using predicted POS tags.
? Is the type of semantic information related
to the type of parser? We will test three
different parsers representative of successful
paradigms in dependency parsing.
? How does the semantic information relate to
the style of dependency annotation? Most ex-
periments for English were evaluated on the
Penn2Malt conversion of the constituency-
based Penn Treebank. We will also examine
the LTH conversion, with richer structure and
an extended set of dependency labels.
? How does WordNet compare to automati-
cally obtained information? For the sake of
comparison, we will also perform the experi-
ments using syntactic/semantic clusters auto-
matically acquired from corpora.
? Does parser combination benefit from seman-
tic information? Different parsers can use se-
mantic information in diverse ways. For ex-
ample, while MaltParser can use the semantic
information in local contexts, MST can in-
corporate them in global contexts. We will
run parser combination experiments with and
without semantic information, to determine
whether it is useful in the combined parsers.
After introducing related work in section 2, sec-
tion 3 describes the treebank conversions, parsers
and semantic features. Section 4 presents the re-
sults and section 5 draws the main conclusions.
2 Related work
Broadly speaking, we can classify the attempts to
add external knowledge to a parser in two sets:
using large semantic repositories such as Word-
Net and approaches that use information automat-
ically acquired from corpora. In the first group,
Agirre et al (2008) trained two state-of-the-art
constituency-based statistical parsers (Charniak,
649
2000; Bikel, 2004) on semantically-enriched in-
put, substituting content words with their seman-
tic classes, trying to overcome the limitations of
lexicalized approaches to parsing (Collins, 2003)
where related words, like scissors and knife, can-
not be generalized. The results showed a signi-
cant improvement, giving the first results over both
WordNet and the Penn Treebank (PTB) to show
that semantics helps parsing. Later, Agirre et al
(2011) successfully introduced WordNet classes in
a dependency parser, obtaining improvements on
the full PTB using gold POS tags, trying different
combinations of semantic classes. MacKinlay et
al. (2012) investigate the addition of semantic an-
notations in the form of word sense hypernyms, in
HPSG parse ranking, reducing error rate in depen-
dency F-score by 1%, while some methods pro-
duce substantial decreases in performance. Fu-
jita et al (2010) showed that fully disambiguated
sense-based features smoothed using ontological
information are effective for parse selection.
On the second group, Koo et al (2008) pre-
sented a semisupervised method for training de-
pendency parsers, introducing features that incor-
porate word clusters automatically acquired from
a large unannotated corpus. The clusters include
strongly semantic associations like {apple, pear}
or {Apple, IBM} and also syntactic clusters like
{of, in}. They demonstrated its effectiveness in
dependency parsing experiments on the PTB and
the Prague Dependency Treebank. Suzuki et al
(2009), Sagae and Gordon (2009) and Candito
and Seddah (2010) also experiment with the same
cluster method. Recently, T?ackstr?om et al (2012)
tested the incorporation of cluster features from
unlabeled corpora in a multilingual setting, giving
an algorithm for inducing cross-lingual clusters.
3 Experimental Framework
In this section we will briefly describe the PTB-
based datasets (subsection 3.1), followed by the
data-driven parsers used for the experiments (sub-
section 3.2). Finally, we will describe the different
types of semantic representation that were used.
3.1 Treebank conversions
Penn2Malt
1
performs a simple and direct conver-
sion from the constituency-based PTB to a depen-
dency treebank. It obtains projective trees and has
been used in several works, which allows us to
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
compare our results with related experiments (Koo
et al, 2008; Suzuki et al, 2009; Koo and Collins,
2010). We extracted dependencies using standard
head rules (Yamada and Matsumoto, 2003), and a
reduced set of 12 general dependency tags.
LTH
2
(Johansson and Nugues, 2007) presents
a conversion better suited for semantic process-
ing, with a richer structure and a more fine-grained
set of dependency labels (42 different dependency
labels), including links to handle long-distance
phenomena, giving a 6.17% of nonprojective sen-
tences. The results from parsing the LTH output
are lower than those for Penn2Malt conversions.
3.2 Parsers
We have made use of three parsers representative
of successful paradigms in dependency parsing.
MaltParser (Nivre et al, 2007) is a determinis-
tic transition-based dependency parser that obtains
a dependency tree in linear-time in a single pass
over the input using a stack of partially analyzed
items and the remaining input sequence, by means
of history-based feature models. We added two
features that inspect the semantic feature at the top
of the stack and the next input token.
MST
3
represents global, exhaustive graph-
based parsing (McDonald et al, 2005; McDon-
ald et al, 2006) that finds the highest scoring di-
rected spanning tree in a graph. The learning pro-
cedure is global since model parameters are set
relative to classifying the entire dependency graph,
in contrast to the local but richer contexts used
by transition-based parsers. The system can be
trained using first or second order models. The
second order projective algorithm performed best
on both conversions, and we used it in the rest of
the evaluations. We modified the system in or-
der to add semantic features, combining them with
wordforms and POS tags, on the parent and child
nodes of each arc.
ZPar
4
(Zhang and Clark, 2008; Zhang and
Nivre, 2011) performs transition-based depen-
dency parsing with a stack of partial analysis
and a queue of remaining inputs. In contrast to
MaltParser (local model and greedy deterministic
search) ZPar applies global discriminative learn-
ing and beam search. We extend the feature set of
ZPar to include semantic features. Each set of se-
mantic information is represented by two atomic
2
http://nlp.cs.lth.se/software/treebank converter
3
http://mstparser.sourceforge.net
4
www.sourceforge.net/projects/zpar
650
Base WordNet WordNet Clusters
line SF SS
Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)
MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)?
ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)? 91.74 (+0.22)
Table 1: LAS results with several parsing algo-
rithms, Penn2Malt conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
feature templates, associated with the top of the
stack and the head of the queue, respectively. ZPar
was directly trained on the Penn2Malt conversion,
while we applied the pseudo-projective transfor-
mation (Nilsson et al, 2008) on LTH, in order to
deal with non-projective arcs.
3.3 Semantic information
Our aim was to experiment with different types of
WordNet-related semantic information. For com-
parison with automatically acquired information,
we will also experiment with bit clusters.
WordNet. We will experiment with the seman-
tic representations used in Agirre et al (2008) and
Agirre et al (2011), based on WordNet 2.1. Word-
Net is organized into sets of synonyms, called
synsets (SS). Each synset in turn belongs to a
unique semantic file (SF). There are a total of 45
SFs (1 for adverbs, 3 for adjectives, 15 for verbs,
and 26 for nouns), based on syntactic and seman-
tic categories. For example, noun SFs differen-
tiate nouns denoting acts or actions, and nouns
denoting animals, among others. We experiment
with both full SSs and SFs as instances of fine-
grained and coarse-grained semantic representa-
tion, respectively. As an example, knife in its
tool sense is in the EDGE TOOL USED AS A
CUTTING INSTRUMENT singleton synset, and
also in the ARTIFACT SF along with thousands
of words including cutter. These are the two ex-
tremes of semantic granularity in WordNet. For
each semantic representation, we need to deter-
mine the semantics of each occurrence of a target
word. Agirre et al (2011) used i) gold-standard
annotations from SemCor, a subset of the PTB, to
give an upper bound performance of the semantic
representation, ii) first sense, where all instances
of a word were tagged with their most frequent
sense, and iii) automatic sense ranking, predicting
the most frequent sense for each word (McCarthy
et al, 2004). As we will make use of the full PTB,
we only have access to the first sense information.
Clusters. Koo et al (2008) describe a semi-
Base WordNet WordNet Clusters
line SF SS
Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)
MST 85.06 85.35 (+0.29)? 84.99 (-0.07) 86.18 (+1.12)?
ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)
Table 2: LAS results with several parsing algo-
rithms in the LTH conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
supervised approach that makes use of cluster fea-
tures induced from unlabeled data, providing sig-
nificant performance improvements for supervised
dependency parsers on the Penn Treebank for En-
glish and the Prague Dependency Treebank for
Czech. The process defines a hierarchical cluster-
ing of the words, which can be represented as a
binary tree where each node is associated to a bit-
string, from the more general (root of the tree) to
the more specific (leaves). Using prefixes of vari-
ous lengths, it can produce clusterings of different
granularities. It can be seen as a representation of
syntactic-semantic information acquired from cor-
pora. They use short strings of 4-6 bits to represent
parts of speech and the full strings for wordforms.
4 Results
In all the experiments we employed a baseline fea-
ture set using word forms and parts of speech, and
an enriched feature set (WordNet or clusters). We
firstly tested the addition of each individual se-
mantic feature to each parser, evaluating its contri-
bution to the parser?s performance. For the combi-
nations, instead of feature-engineering each parser
with the wide array of different possibilities for
features, as in Agirre et al (2011), we adopted
the simpler approach of combining the outputs of
the individual parsers by voting (Sagae and Lavie,
2006). We will use Labeled Attachment Score
(LAS) as our main evaluation criteria. As in pre-
vious work, we exclude punctuation marks. For
all the tests, we used a perceptron POS-tagger
(Collins, 2002), trained on WSJ sections 2?21, to
assign POS tags automatically to both the training
(using 10-way jackknifing) and test data, obtaining
a POS tagging accuracy of 97.32% on the test data.
We will make use of Bikel?s randomized parsing
evaluation comparator to test the statistical signi-
cance of the results. In all of the experiments the
parsers were trained on sections 2-21 of the PTB
and evaluated on the development set (section 22).
Finally, the best performing system was evaluated
on the test set (section 23).
651
Parsers LAS UAS
Best baseline (ZPar) 91.52 92.57
Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63
Best combination (3 baseline parsers) 91.90 (+0.38) 93.01
Best combination of 3 parsers:
3 baselines + 3 SF extensions 91.93 (+0.41) 92.95
Best combination of 3 parsers:
3 baselines + 3 SS extensions 91.87 (+0.35) 92.92
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 91.90 (+0.38) 92.90
Table 3: Parser combinations on Penn2Malt.
Parsers LAS UAS
Best baseline (ZPar) 89.15 91.81
Best single parser (ZPar + SF) 89.33 (+0.15) 92.01
Best combination (3 baseline parsers) 89.15 (+0.00) 91.81
Best combination of 3 parsers:
3 baselines + 3 SF extensions 89.56 (+0.41)? 92.23
Best combination of 3 parsers:
3 baselines + 3 SS extensions 89.43 (+0.28) 93.12
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 89.52 (+0.37)? 92.19
Table 4: Parser combinations on LTH (?: p <0.05,
?: p <0.005).
4.1 Single Parsers
We run a series of experiments testing each indi-
vidual semantic feature, also trying different learn-
ing configurations for each one. Regarding the
WordNet information, there were 2 different fea-
tures to experiment with (SF and SS). For the bit
clusters, there are different possibilities, depend-
ing on the number of bits used. For Malt and MST,
all the different lengths of bit strings were used.
Given the computational requirements and the pre-
vious results on Malt and MST, we only tested all
bits in ZPar. Tables 1 and 2 show the results.
Penn2Malt. Table 1 shows that the only signifi-
cant increase over the baseline is for ZPar with SS
and for MST with clusters.
LTH. Looking at table 2, we can say that the dif-
ferences in baseline parser performance are accen-
tuated when using the LTH treebank conversion,
as ZPar clearly outperforms the other two parsers
by more than 4 absolute points. We can see that
SF helps all parsers, although it is only significant
for MST. Bit clusters improve significantly MST,
with the highest increase across the table.
Overall, we see that the small improvements
do not confirm the previous results on Penn2Malt,
MaltParser and gold POS tags. We can also con-
clude that automatically acquired clusters are spe-
cially effective with the MST parser in both tree-
bank conversions, which suggests that the type of
semantic information has a direct relation to the
parsing algorithm. Section 4.3 will look at the de-
tails by each knowledge type.
4.2 Combinations
Subsection 4.1 presented the results of the base al-
gorithms and their extensions based on semantic
features. Sagae and Lavie (2006) report improve-
ments over the best single parser when combining
three transition-based models and one graph-based
model. The same technique was also used by the
winning team of the CoNLL 2007 Shared Task
(Hall et al, 2007), combining six transition-based
parsers. We used MaltBlender
5
, a tool for merging
the output of several dependency parsers, using the
Chu-Liu/Edmonds directed MST algorithm. After
several tests we noticed that weighted voting by
each parser?s labeled accuracy gave good results,
using it in the rest of the experiments. We trained
different types of combination:
? Base algorithms. This set includes the 3 base-
line algorithms, MaltParser, MST, and ZPar.
? Extended parsers, adding semantic informa-
tion to the baselines. We include the three
base algorithms and their semantic exten-
sions (SF, SS, and clusters). It is known (Sur-
deanu and Manning, 2010) that adding more
parsers to an ensemble usually improves ac-
curacy, as long as they add to the diver-
sity (and almost regardless of their accuracy
level). So, for the comparison to be fair, we
will compare ensembles of 3 parsers, taken
from sets of 6 parsers (3 baselines + 3 SF,
SS, and cluster extensions, respectively).
In each experiment, we took the best combina-
tion of individual parsers on the development set
for the final test. Tables 3 and 4 show the results.
Penn2Malt. Table 3 shows that the combina-
tion of the baselines, without any semantic infor-
mation, considerably improves the best baseline.
Adding semantics does not give a noticeable in-
crease with respect to combining the baselines.
LTH (table 4). Combining the 3 baselines does
not give an improvement over the best baseline, as
ZPar clearly outperforms the other parsers. How-
ever, adding the semantic parsers gives an increase
with respect to the best single parser (ZPar + SF),
which is small but significant for SF and clusters.
4.3 Analysis
In this section we analyze the data trying to under-
stand where and how semantic information helps
most. One of the obstacles of automatic parsers
is the presence of incorrect POS tags due to auto-
5
http://w3.msi.vxu.se/users/jni/blend/
652
LAS on sentences LAS on sentences
POS tags Parser LAS test set without POS errors with POS errors
Gold ZPar 90.45 91.68 89.14
Automatic ZPar 89.15 91.62 86.51
Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)
3 baselines + 3 SF extensions
Automatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)
3 baselines + 3 SS extensions
Automatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)
3 baselines + 3 cluster extensions
Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-
rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).
matic tagging. For example, ZPar?s LAS score on
the LTH conversion drops from 90.45% with gold
POS tags to 89.12% with automatic POS tags. We
will examine the influence of each type of seman-
tic information on sentences that contain or not
POS errors, and this will clarify whether the incre-
ments obtained when using semantic information
are useful for correcting the negative influence of
POS errors or they are orthogonal and constitute
a source of new information independent of POS
tags. With this objective in mind, we analyzed the
performance on the subset of the test corpus con-
taining the sentences which had POS errors (1,025
sentences and 27,300 tokens) and the subset where
the sentences had (automatically assigned) correct
POS tags (1,391 sentences and 29,386 tokens).
Table 5 presents the results of the best single
parser on the LTH conversion (ZPar) with gold
and automatic POS tags in the first two rows. The
LAS scores are particularized for sentences that
contain or not POS errors. The following three
rows present the enhanced (combined) parsers
that make use of semantic information. As the
combination of the three baseline parsers did not
give any improvement over the best single parser
(ZPar), we can hypothesize that the gain coming
from the parser combinations comes mostly from
the addition of semantic information. Table 5 sug-
gests that the improvements coming from Word-
Net?s semantic file (SF) are unevenly distributed
between the sentences that contain POS errors and
those that do not (an increase of 0.28 for sentences
without POS errors and 0.55 for those with er-
rors). This could mean that a big part of the in-
formation contained in SF helps to alleviate the
errors performed by the automatic POS tagger. On
the other hand, the increments are more evenly
distributed for SS and clusters, and this can be
due to the fact that the semantic information is
orthogonal to the POS, giving similar improve-
ments for sentences that contain or not POS errors.
We independently tested this fact for the individ-
ual parsers. For example, with MST and SF the
gains almost doubled for sentences with incorrect
POS tags (+0.37 with respect to +0.21 for sen-
tences with correct POS tags) while the gains of
adding clusters? information for sentences without
and with POS errors were similar (0.91 and 1.33,
repectively). This aspect deserves further inves-
tigation, as the improvements seem to be related
to both the type of semantic information and the
parsing algorithm.We did an initial exploration but
it did not give any clear indication of the types of
improvements that could be expected using each
parser and semantic data.
5 Conclusions
This work has tried to shed light on the contribu-
tion of semantic information to dependency pars-
ing. The experiments were thorough, testing two
treebank conversions and three parsing paradigms
on automatically predicted POS tags. Compared
to (Agirre et al, 2011), which used MaltParser on
the LTH conversion and gold POS tags, our results
can be seen as a negative outcome, as the improve-
ments are very small and non-significant in most
of the cases. For parser combination, WordNet
semantic file information does give a small sig-
nificant increment in the more fine-grained LTH
representation. In addition we show that the im-
provement of automatic clusters is also weak. For
the future, we think tdifferent parsers, eitherhat a
more elaborate scheme is needed for word classes,
requiring to explore different levels of generaliza-
tion in the WordNet (or alternative) hierarchies.
Acknowledgments
This research was supported by the the Basque
Government (IT344- 10, S PE11UN114), the Uni-
versity of the Basque Country (GIU09/19) and
the Spanish Ministry of Science and Innovation
(MICINN, TIN2010-20218).
653
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment per-
formance with sense information. In Proceedings
of ACL-08: HLT, pages 317?325, Columbus, Ohio,
June. Association for Computational Linguistics.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency pars-
ing with semantic classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 699?703, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Marie Candito and Djam?e Seddah. 2010. Pars-
ing word clusters. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?84, Los
Angeles, CA, USA, June. Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on Lan-
guage and Computation, 8(1):122.
Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,
Beta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multi-
lingual parser optimization. In Proceedings of the
CoNLL Shared Task EMNLP-CoNLL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,
and Timothy Baldwin. 2012. The effects of seman-
tic annotations on precision parse ranking. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM), page 228236, Montreal, Canada,
June. Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL 2006.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2008.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th Con-
ference of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,
Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin
Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural
Language Engineering.
Kenji Sagae and Andrew Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Pro-
ceedings of the Eleventh International Conference
on Parsing Technologies.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551?560, Singapore, August. As-
sociation for Computational Linguistics.
654
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477?
487, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
655
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 31?39,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Application of Different Techniques to Dependency Parsing of Basque 
 
Kepa Bengoetxea Koldo Gojenola 
IXA NLP Group IXA NLP Group 
University of the Basque Country University of the Basque Country 
Technical School of Engineering, Bilbao,  
Plaza La Casilla 3, 48012, Bilbao 
Technical School of Engineering, Bilbao,  
Plaza La Casilla 3, 48012, Bilbao 
kepa.bengoetxea@ehu.es koldo.gojenola@ehu.es 
 
  
 
 
Abstract 
We present a set of experiments on depend-
ency parsing of the Basque Dependency Tree-
bank (BDT). The present work has examined 
several directions that try to explore the rich 
set of morphosyntactic features in the BDT: i) 
experimenting the impact of morphological 
features, ii) application of dependency tree 
transformations, iii) application of a two-stage 
parsing scheme (stacking), and iv) combina-
tions of the individual experiments. All the 
tests were conducted using MaltParser (Nivre 
et al, 2007a), a freely available and state of 
the art dependency parser generator. 
1 Introduction 
This paper presents several experiments performed 
on dependency parsing of the Basque Dependency 
Treebank (BDT, Aduriz et al, 2003). Basque can 
be briefly described as a morphologically rich lan-
guage with free constituent order of the main sen-
tence elements with respect to the main verb. 
This work has been developed in the context of 
dependency parsing exemplified by the CoNLL 
Shared Task on Dependency Parsing in years 2006 
and 2007 (Nivre et al, 2007b), where several sys-
tems competed analyzing data from a typologically 
varied range of 19 languages. The treebanks for all 
languages were standardized using a previously 
agreed CoNLL-X format (see Figure 1). An early 
version of the BDT (BDT I) was one of the evalu-
ated treebanks, which will allow a comparison with 
our results. One of the conclusions of the CoNLL 
2007 workshop (Nivre et al, 2007a) was that there 
is a class of languages, those that combine a rela-
tively free word order with a high degree of inflec-
tion, that obtained the worst scores. This asks for 
the development of new methods and algorithms 
that will help to reach the parsing performance of 
the more studied languages, as English. 
In this work, we will take the opportunity of 
having a new fresh version of the BDT, (BDT II 
henceforth), which is the result of an extension 
(three times bigger than the original one), and its 
redesign (see section 3.2). Using MaltParser, a 
freely available and state of the art dependency 
parser for all the experiments (Nivre et al, 2007a), 
this paper will concentrate on the application of 
different techniques to the task of parsing this new 
treebank, with the objective of giving a snapshot 
that can show the expected gains of each tech-
nique, together with some of their combinations. 
Some of the techniques have already been evalu-
ated with other languages/treebanks or BDT I, 
while others have been adapted or extended to deal 
with specific aspects of the Basque language or the 
Basque Treebank. We will test the following: 
? Impact of rich morphology. Although many 
systems performed feature engineering on the 
BDT at CoNLL 2007, providing a strong 
baseline, we will take a step further to im-
prove parsing accuracy taking into account the 
effect of specific morphosyntactic features. 
? Application of dependency-tree transforma-
tions. Nilsson et al (2007) showed that they 
can increase parsing accuracy across lan-
guages/treebanks. We have performed similar 
experiments adapted to the specific properties 
of Basque and the BDT. 
? Several works have tested the effect of using a 
two-stage parser (Nivre and McDonald, 2008; 
Martins et al, 2008), where the second parser 
takes advantage of features obtained by the 
first one. Similarly, we will experiment the 
31
addition of new features to the input of the 
second-stage parser, in the form of morpho-
syntactic features propagated through the first 
parser?s dependency tree and also as the addi-
tion of contextual features (such as category 
or dependency relation of parent, grandparent, 
and descendants). 
? Combinations of the individual experiments. 
The rest of the paper is organized as follows. 
After presenting related work in section 2, section 
3 describes the main resources used in this work. 
Next, section 4 will examine the details of the dif-
ferent experiments to be performed, while section 
5 will evaluate their results. Finally, the last section 
outlines our main conclusions. 
2 Related work 
Until recently, many works on treebank parsing 
have been mostly dedicated to languages with poor 
morphology, as exemplified by the Penn English 
Treebank. As the availability of treebanks for typo-
logically different languages has increased, there 
has been a growing interest towards research on 
extending the by now standard algorithms and 
methods to the new languages and treebanks (Tsar-
faty et al, 2009). For example, Collins et al (1999) 
adapted Collins? parser to Czech, a highly-
inflected language. Cowan and Collins (2005) ap-
ply the same parser to Spanish, concluding that the 
inclusion of morphological information improves 
the analyzer. Eryi?it et al (2008) experiment the 
use of several types of morphosyntactic informa-
tion in Turkish, showing how the richest the in-
formation improves precision. They also show that 
using morphemes as the unit of analysis (instead of 
words) gets better results, as a result of the aggluti-
native nature of Turkish, where each wordform 
contains several morphemes that can be individu-
ally relevant for parsing. Goldberg and Tsarfaty 
(2008) concluded that an integrated model of mor-
phological disambiguation and syntactic parsing in 
Hebrew Treebank parsing improves the results of a 
pipelined approach. This is in accord with our ex-
periment of dividing words into morphemes and 
transforming the tree accordingly (see section 4.2). 
Since the early times of treebank-based parsing 
systems, a lot of effort has been devoted to aspects 
of preprocessing trees in order to improve the re-
sults (Collins, 1999). When applied to dependency 
parsing, several works (Nilsson et al, 2007; Ben-
goetxea and Gojenola, 2009a) have concentrated 
on modifying the structure of the dependency tree, 
changing its original shape. For example, Nilsson 
et al (2007) present the application of pseudopro-
jective, verbal group and coordination transforma-
tions to several languages/treebanks using 
MaltParser, showing that they improve the results.  
Another interesting research direction has exam-
ined the application of a two-stage parser, where 
the second parser tries to improve upon the result 
of a first parser. For example, Nivre and McDonald 
(2008) present the combination of two state of the 
art dependency parsers feeding each another, 
showing that there is a significant improvement 
over the simple parsers. This experiment can be 
seen as an instance of stacked learning, which was 
also tested on dependency parsing of several lan-
guages in (Martins et al, 2008) with significant 
improvements over the base parser. 
3 Resources 
This section will describe the main resources that 
have been used in the experiments. First, subsec-
Index Word   Lemma   Category Subcategory Features  Head Dependency 
1 etorri   etorri   V  V  _   3 coord 
2 dela   izan     AUXV AUXV  REL:CMP|SUBJ:3S 1 auxmod 
3 eta   eta     CONJ CONJ  _   6 ccomp_obj 
4 joan   joan     V  V  _   3 coord 
5 dela   izan     AUXV AUXV  REL:CMP|SUBJ:3S 4 auxmod 
6 esan   esan     V  V  _   0 ROOT 
7 zien   *edun    AUXV AUXV  SUBJ:3S|OBJ:3P 6 auxmod 
8 mutilak  mutil   NOUN NOUN_C  CASE:ERG|NUM:S 6 ncsubj 
9 .   .     PUNT PUNT_PUNT _   8 PUNC 
 
Figure 1: Example of a BDT sentence in the CoNLL-X format 
(V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj = 
clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing., OBJ:3P: object in 3rd person pl, coord = 
coordination, auxmod = auxiliary, ncsubj = non-clausal subject, ncmod = non-clausal modifier). 
 
32
tion 3.1 will describe the Basque Dependency 
Treebank, which has increased its size from 55,469 
tokens in its original version to more than 150,000, 
while subsection 3.2 will present the main charac-
teristics of MaltParser, a state of the art and data-
driven dependency parser. 
3.1 The Basque Dependency Treebank 
Basque can be described as an agglutinative lan-
guage that presents a high power to generate in-
flected word-forms, with free constituent order of 
sentence elements with respect to the main verb. 
The BDT can be considered a pure dependency 
treebank from its original design, due mainly to the 
syntactic characteristics of Basque.  
(1) Etorri  dela  eta joan  dela   esan  zien mutilak 
    come  that-has and go  that-has tell did  boy-the 
  The boy told them that he has come and gone 
 
Figure 1 contains an example of a sentence (1), 
annotated in the CoNLL-X format. The text is or-
ganized in eight tab-separated columns: word-
number, form, lemma, category, subcategory, mor-
phological features, and the dependency relation 
(headword + dependency). The information in Fig-
ure 1 has been simplified due to space reasons, as 
typically the Features column will contain many 
morphosyntactic1 features (case, number, type of 
subordinated sentence, ?), which are relevant for 
parsing. The first version of the Basque Depend-
ency Treebank contained 55,469 tokens forming 
3,700 sentences (Aduriz et al, 2003). This tree-
bank was used as one of the evaluated treebanks in 
the CoNLL 2007 Shared Task on Dependency 
Parsing (Nivre et al, 2007b). Our work will make 
use of the second version of the BDT (BDT II), 
which is the consequence of a process of extension 
and redesign of the original requirements: 
? The new version contains 150,000 tokens 
(11,225 sentences), a three-fold increase. 
? The new design considered that all the de-
pendency arcs would connect sentence tokens. 
In contrast, the original annotation contained 
empty nodes, especially when dealing with el-
lipsis and some kinds of coordination. As a 
result, the number of non-projective arcs di-
                                                          
1 We will use the term morphosyntactic to name the set of 
features attached to each word-form, which by the agglutina-
tive nature of Basque correspond to both morphology and 
syntax. 
minished from 2.9% in the original treebank 
to 1.3% in the new version. 
? The annotation follows a stand-off markup 
approach, inspired on TEI-P4 (Artola et al, 
2005). There was a conversion process from a 
set of interconnected XML files to the 
CoNLL-X format of the present experiments. 
Although the different characteristics and size of 
the two treebank versions do not allow a strict 
comparison, our preliminary experiments showed 
that the results on both treebanks were similar re-
garding our main evaluation criterion (Labeled 
Attachment Score, or LAS). In the rest of the paper 
we will only use the new BDT II. 
3.2 MaltParser 
MaltParser (Nivre et al 2007a) is a state of the art 
dependency parser that has been successfully ap-
plied to typologically different languages and tree-
banks. While several variants of the base parser 
have been implemented, we will use one of its 
standard versions (MaltParser version 1.3). The 
parser obtains deterministically a dependency tree 
in linear-time in a single pass over the input using 
two main data structures: a stack of partially ana-
lyzed items and the remaining input sequence. To 
determine the best action at each parsing step, the 
parser uses history-based feature models and dis-
criminative machine learning. In all the following 
experiments, we will make use of a SVM classi-
fier. The specification of the configuration used for 
learning can in principle include any kind of col-
umn in Figure 1 (such as word-form, lemma, cate-
gory, subcategory or morphological features), 
together with a feature function. This means that a 
learning model can be described as a series of 
(column, function) pairs, where column represents 
the name of a column in Figure 1, and function 
makes reference to the parser?s main data struc-
tures. For example, the two pairs (Word, Stack[0]), 
and (Word, Stack[1]) represent two features that 
correspond to the word-forms on top and next to 
top elements of the stack, respectively, while 
(POSTAG, Input[0]) represents the POS category 
of the first token in the remaining input sequence. 
4 Experiments 
The following subsections will present three types 
of techniques that will be tested with the aim of 
33
improving the results of the syntactic analyzer. 
Subsection 4.1 presents the process of fine-tuning 
the rich set of available morphosyntactic features. 
Then, 4.2 will describe the application of three 
types of tree transformations, while subsection 4.3 
will examine the application of propagating syntac-
tic features through a first-stage dependency tree, a 
process that can also be seen as an application of 
stacked learning, as tested in (Nivre and McDon-
ald, 2008; Martins et al, 2008) 
4.1 Feature engineering 
The original CoNLL-X format uses 10 different 
columns (see Figure 12), grouping the full set of 
morphosyntactic features in a single column. We 
will experiment the effect of individual features, 
following two steps: 
? First, we tested the effect of incorporating 
each individual lexical feature, concluding 
that there were two features that individually 
gave significant performance increases. They 
were syntactic case, which is relevant for 
marking a word?s syntactic function (or, 
equivalently, the type of dependency relation), 
and subordination type (REL henceforth). 
This REL feature appears in verb-ending mor-
phemes that specify a type of subordinated 
sentence, such as in relative, completive, or 
indirect interrogative clauses. The feature is, 
therefore, relevant for establishing the main 
structure of a sentence, helping to delimit 
main and subordinated clauses, and it is also 
crucial for determining the dependency rela-
tion between the subordinated sentence and 
the main verb (head). 
? Then, we separated these features in two in-
dependent columns, grouping the remaining 
features under the Features column. This way, 
Maltparser?s learning specification can be 
more fine-grained, in terms of three morpho-
syntactic feature sets (CASE, REL and the 
rest, see Table 2). 
This will allow us testing learning models with 
different configurations for each column, instead 
of treating the full set of features as a whole. So, 
we will have the possibility of experimenting with 
                                                          
2 As a matter of fact, Figure 1 only shows 8 columns, although 
the CoNLL-X format includes two additional columns for the 
projective head (PHEAD) and projective dependency relation 
(PDEPREL), which have not been used in our work. 
richer contexts (that is, advancing the Stack and/or 
Input3 functions for each feature). 
4.2 Tree transformations  
Tree transformations have long been applied with 
the objective of improving parsing results (Collins, 
1999; Nilsson et al, 2007). The general process 
consists of the following steps: 
? Apply tree transformations to the treebank 
? Train the system on the modified treebank 
? Apply the parser to the test set 
? Apply the inverse transformations 
? Evaluate the result on the original treebank 
We will test three different tree transformations, 
which had already been applied to the Treebank 
(BDT I) (Bengoetxea and Gojenola, 2009a): 
? Projectivization (TP). This is a language inde-
pendent transformation already tested in sev-
eral languages (Nivre and Nilsson, 2005). 
This transformation is totally language inde-
pendent, and can be considered a standard 
transformation. Its performance on the first 
version of BDT had been already tested (Hall 
et al, 2007), giving significant improvements 
This is in accordance with BDT I having a 
2.9% of non-projective arcs. 
? Coordination (TC). The transformation on co-
ordinated sentences can be considered general 
(Nilsson et al, 2007) but it is also language 
dependent, as it depends on the specific con-
figurations present in each language, mainly 
the set of coordination conjunctions and the 
types of elements that can be coordinated, to-
gether with their morphosyntactic properties 
(such as head initial or final). Coordination in 
BDT (both versions) is annotated in the so 
called Prague Style (PS, see Figure 2), where 
the conjunction is taken as the head, and the 
                                                          
3 Maltparser allows a rich set of functions to be specified for 
each column. In our experiments we mainly used the Stack 
and Input functions, which allow the inspection of the contents 
of the top elements of the Stack (Stack[0], Stack[1], ?) or the 
currently unanalyzed input sequence (Input[0], Input [1], ?). 
  C1 C2  S C3   C1 C2 S C3  C1 C2 S C3  
 
(PS) (MS) (MS-sym) 
Figure 2. Dependency structures for coordination. 
 
34
conjuncts depend on it. Nilsson et al (2007) 
advocate the Mel?cuk style (MS) for parsing 
Czech, taking the first conjunct as the head, 
and creating a chain where each element de-
pends on the preceding one. Basque is a head 
final language, where many important syntac-
tic features, like case or subordinating mor-
phemes are located at the end of constituents. 
For that reason, Bengoetxea and Gojenola 
(2009a) proposed MS-sym, a symmetric 
variation of MS in which the coordinated 
elements will be dependents of the last con-
junct (which will be the head, see Figure 2).  
? Transformation of subordinated sentences 
(TS). They are formed in Basque by attaching 
the corresponding morphemes to the auxiliary 
verbs. However, in BDT (I and II) the verbal 
elements are organized around the main verb 
(semantic head) while the syntactic head 
corresponds to the subordination morpheme, 
which appears usually attached to the 
auxiliary. Its main consequence is that the 
elements bearing the relevant information for 
parsing are situated far in the tree with respect 
to their head. In Figure 3, we see that the 
morpheme ?la, indicating a subordinated 
completive sentence, appears down in the tree, 
and this could affect the correct attachment to 
the main verb (esan). Figure 4 shows the 
effect of transforming the original tree in 
Figure 3. The subordination morpheme (-la) is 
separated from the auxiliary verb (da), and is 
?promoted? as the syntactic head of  the 
subordinated sentence. New arcs are created 
from the main verb (etorri) to the morpheme 
(which is now the head), and also a new 
dependency relation (SUB).  
Overall, the projectivization transformation (TP) 
is totally language-independent. TC (coordination) 
can be considered in the middle, as it depends on 
the general characteristics of the language. Finally, 
the transformation of subordinated sentences (TS) 
is specific to the treebank and intrinsecally linked 
to the agglutinative nature of Basque. Bengoetxea 
and Gojenola (2009a) also found that the order of 
transformations can be relevant. Their best system, 
after applying all the transformations, obtained a 
76.80% LAS on BDT I (2.24% improvement over 
a baseline of 74.52%) on the test set. We include 
these already evaluated transformations in the pre-
sent work with two objectives in mind: 
? We want to test its effect on BDT II, 3 times 
larger than BDT I, and also with a lower 
proportion of non-projective arcs (1.3%). 
? We are also interested in testing its 
combination with the rest of the techniques 
(see subsections 4.1 and 4.3).  
4.3 Two-stage parsing (stacking) 
Bengoetxea and Gojenola (2009b) tested the effect 
of propagating several morphosyntactic feature 
values after a first parsing phase, as in classical 
unification-based grammars, as a means of propa-
gating linguistic information through syntax trees. 
They applied three types of feature propagation of 
the morphological feature values: a) from auxiliary 
verbs to the main verb (verb groups) b) propaga-
tion of case and number from post-modifiers to the 
head noun (noun phrases) c) from the last conjunct 
to the conjunction (coordination). This was done 
mainly because Basque is head final, and relevant 
features are located at the end of constituents.  
Nivre and McDonald (2008) present an 
application of stacked learning to dependency 
parsing, in which a second predictor is trained to 
improve the performance of the first. Martins et al 
(2008) specify the following steps: 
? Split training data D into L partitions D1, ... , 
DL. 
? Train L instances of the level 0 parser in the 
following way: the l-th instance, gl, is trained 
auxmod 
ccomp_obj 
 
Figure 4. Effect of applying the transformation on 
subordinated sentences to the tree in Figure 3 
(dotted lines represent the modified arcs). 
 
Etorri   da    +la  esan   du   
 come      has+he  that  told     did+he    
  V       AUXV+3S  COMPL   V      AUXV 
SUB auxmod 
auxmod 
ccomp_obj 
 
auxmod 
Figure 3. Dependency tree for the sentence Etorri 
dela esan du (He told that he would come). 
 
Etorri    da+la     esan   du   
 come      has+he+that   told    did+he    
  V       AUXV+3S+COMPL   V       AUXV 
35
on D?l = D \ Dl. Then use gl to output 
predictions for the (unseen) partition Dl. At 
the end, we have an augmented dataset D* = D 
+ new set of stacked/propagated features. 
? Train the level 0 parser g on the original 
training data D. 
? Train the level 1 parser on the augmented 
training data D*. 
In our tests, it was enough with two partitions (L 
= 2), as experiments with L > 2 did not give any 
significant improvement. Figure 5 shows the types 
of information that can be added to each target 
element. The token X can take several kinds of 
information from its children (A and B) or his par-
ent (H). The information that is propagated can 
vary, including part of speech, morphosyntactic 
features or the dependency relations between X 
and its children/parent. We can roughly classify the 
stacked features in two different sets: 
? Linguistic features (feature passing), such as 
case or number, which are propagated 
applying linguistic principles, such as ?the 
syntactic case is propagated from the 
dependents towards the head of NPs and 
postpositional phrases?. The idea is to 
propagate several morphosyntactic features 
(case, number, ?) from dependents to  heads. 
? Parser features. They will be based solely on 
different dependency tree configurations (see 
Figure 5), similarly to (Nivre and McDonald, 
2008; Martins et al, 2008). Among them, we 
will test the inclusion of several features 
(dependency relation, category and 
morphosyntactic features) from the following: 
parent, grandparent, siblings, and children. 
In the present work, we have devised the follow-
ing experiments: 
? We will test the effect of propagating 
linguistic features on the new BDT II. In 
contrast to (Bengoetxea and Gojenola, 
2009b), who used the enriched gold data as D* 
directly, we will test Martins et al?s proposal, 
in which the level 1 parser will be able to 
learn on the errors of the level 0 parser. 
? We will extend these experiments with the use 
of different parser features (Nivre and 
McDonald, 2008; Martins et al, 2008). 
4.4 Combination 
Finally, we will combine the different techniques. 
An important point is to determine whether the 
techniques are independent (and accumulative) or 
it could also be that they can serve as alternative 
treatments to deal with the same phenomena. 
5 Evaluation 
BDT I was used at the CoNLL 2007 Shared Task, 
where many systems competed on it (Nivre et al, 
2007b). We will use Labeled Attachment Score 
(LAS) as the evaluation measure: the percentage of 
correct arcs (both dependency relation and head) 
over all arcs, with respect to the gold standard. Ta-
ble 1 shows the best CoNLL 2007 results on BDT 
I. The best system obtained a score of 76.94%, 
combining six variants of MaltParser, and compet-
ing with 19 systems. Carreras (2007) and Titov and 
Henderson (2007) obtained the second and third 
positions, respectively. We consider the last two 
lines in Table 1 as our baselines, which consist in 
applying a single MaltParser version (Hall et al, 
2007), that obtained the fifth position at CoNLL 
2007. Although Hall et al (2007) applied the pro-
jectivization transformation (TP), we will not use it 
in our baseline because we want to evaluate the 
effect of multiple techniques over a base parser. 
Although we could not use the subset of BDT II 
corresponding to BDT I, we run4 a test with a set 
of sentences the size of BDT I. As could be ex-
                                                          
4 For space reasons, we do not specify details of the algorithm 
and the parameters. These data can be obtained, together with 
the BDT II data, from any of the authors. 
 System LAS 
Nivre et al 2007b (MaltParser, 
combined) 
76.94%   
Carreras, 2007 75.75%   
Titov and Henderson, 2007 75.49%   
C 
o 
N 
L 
L 
 
07 
Hall et al, 2007 (MaltParser 
(single parser) + pseudoprojec-
tive transformation) 
74.99% 
 
 
 
BDT I 
MaltParser (single parser) 74.52% 
BDT I size 74.83% BDT II  MaltParser (single 
parser)  Baseline 77.08% 
Table 1. Top LAS scores for Basque dependency parsing. 
d2 
d1 
d3 
Figure 5. Stacked features. X can take several 
features from its descendants (dependency arcs 
d2 and d3) or his head (d1). 
 
 A            X           B    H   
36
pected, the three-fold increase in the new treebank 
gives a 2.35% improvement over BDT I. 
For evaluation, we divided the treebank in three 
sets, corresponding to training, development, and 
test (80%, 10%, and 10%, respectively). All the 
experiments were done on the development set, 
leaving the best systems for the final test. 
5.1 Single systems 
Table 3 shows the results for the basic systems 
employing each of the techniques advanced in Sec-
tion 4. As a first result, we see that a new step of 
reengineering MaltParser?s learning configuration 
was rewarding (see row 2 in Table 3), as morpho-
syntactic features were more finely specified with 
respect to the most relevant features. Table 2 pre-
sents the baseline and the best learning model5. We 
see that advancing the input lookahead for CASE 
and REL gives an increase of 0.82 points. 
Looking at the transformations (rows 3 to 7), the 
new Treebank BDT II obtains results similar to 
those described in (Bengoetxea and Gojenola, 
2009a). As could be expected from the reduction 
of non-projective arcs (from 2.9% to 1.3%), the 
gains of TP are proportionally lower than in BDT I. 
Also, we can observe that TS alone worsens the 
baseline, but it gives the best results when com-
bined with the rest (rows 6 and 7). This can be ex-
plained because TS creates new non-projective 
arcs, so it is effective only if TP is applied later. 
The transformation on coordination (TC) alone 
does not get better results, but when combined 
with TP and TS gives the best results. 
Applying feature propagation and stacking (see 
rows 9-17), we can see that most of the individual 
techniques (rows 9-14) give improvements over 
the baseline. When combining what we defined as 
                                                          
5 This experiment was possible due to the fact that Malt-
Parser?s functionality was extended, allowing the specification 
of new columns/features, as the first versions of MaltParser 
only permitted a single column that included all the features. 
linguistic features (those morphosyntactic features 
propagated by the application of three linguistic 
principles), we can see that their combination 
seems accumulative (row 15). The parser features 
also give a significant improvement individually 
(rows 12-14), but, when combined either among 
themselves (row 16) or with the linguistic features 
(row 17), their effect does not seem to be additive. 
5.2 Combined systems 
After getting significant improvements on the indi-
vidual techniques and some of their combinations, 
we took a further step to integrate different tech-
niques. An important aspect that must be taken into 
account is that the combination is not trivial all the 
times. For example, we have seen (section 5.1) that 
combinations of the three kinds of tree transforma-
tions must be defined having in mind the possible 
side-effects of any previous transformation. When 
combining different techniques, care must be taken 
to avoid any incompatibility. For that reason we 
only tested some possibilities. Rows 18-21 show 
some of the combined experiments. Combination 
of feature optimization with the pseudoprojective 
transformation yields an accumulative improve-
ment (row 18). However, the combination of all 
the tree transformations with FO (row 19) does not 
accumulate. This can be due to the fact that feature 
optimization already cancelled the effect of the 
transformation on coordination and subordinated 
sentences, or otherwise it could also need a better 
exploration of their interleaved effect. Finally, row 
21 shows that feature optimization, the pseudopro-
jective transformation and feature propagation are 
also accumulative, giving the best results. The rela-
tions among the rest of the transformations deserve 
future examination, as the results do not allow us 
to extract a precise conclusion.  
6 Conclusions and future work 
We studied several proposals for improving a base-
line system for parsing the Basque Treebank. All 
the results were evaluated on the new version, 
BDT II, three times larger than the previous one. 
We have obtained the following main results: 
? Using rich morphological features. We have 
extended previous works, giving a finer 
grained description of morphosyntactic 
features on the learner?s configuration, 
  Stack[0] Input[0] Input[1] Input[2] 
1 Features + +   
CASE + + +  
REL + + + + 
2 
Features 
(rest) 
+ +   
Table 2. Learning configurations for morphosyntactic fea-
tures (1 = best model for the whole set of features. 
2 = best model when specializing features). 
37
showing that it can significantly improve the 
results. In particular, differentiating case and 
the type of subordinated sentence gives the 
best LAS increase (+0.82%).  
? Tree transformations. We have replicated the 
set of tree transformations that were tested in 
the old treebank (Bengoetxea and Gojenola 
2009a). Two of the transformations 
(projectivization and coordination) can be 
considered language independent, while the 
treatment of subordination morphemes is 
related to the morphological nature of Basque. 
? Feature propagation. We have experimented 
the effect of a stacked learning scheme. Some 
of the stacked features were language-
independent, as in (Nivre and McDonald. 
2008), but we have also applied a 
generalization of the stacking mechanism to a 
morphologically rich language, as some of the 
stacked features are morphosyntactic features 
(such as case and number) which were 
propagated through a first stage dependency 
tree by the application of linguistic principles 
(noun phrases, verb groups and coordination). 
? Combination of techniques. Although several 
of the combined approaches are accumulative 
with respect to the individual systems, some 
others do not give a improvement over the 
basic systems. A careful study must be 
conducted to investigate whether the 
approaches are exclusive or complementary. 
For example, the transformation on 
subordinated sentences and feature 
propagation on verbal groups seem to be 
attacking the same problem, i. e., the relations 
between main and subordinated sentences. In 
this respect, they can be viewed as alternative 
approaches to dealing with these phenomena. 
The results show that the application of these 
techniques can give noticeable results, getting an 
overall improvement of 1.90% (from 77.08% until 
78.98%), which can be roughly comparable to the 
effect of doubling the size of the treebank (see the 
last two lines of Table 1).  
Acknowledgements 
This research was supported by the Department of 
Industry of the Basque Government (IE09-262) 
and the University of the Basque Country 
(GIU09/19). Thanks to Joakim Nivre and his team 
for their support using Maltparser and his fruitful 
suggestion about the use of stacked features. 
  Row System LAS 
Baseline 1  77.08%  
Feature optimization 2 FO *77.90% (+0.82) 
3 TP **77.92% (+0.84) 
4 TS 75.95% (-1.13) 
5 TC 77.05% (-0.03) 
6 TS + TP **78.41% (+1.33) 
 
 
Transformations 
7 TS + TC + TP **78.59% (+1.51) 
9 SVG **77.68%  (+0.60) 
10 SNP 77.17% (+0.09) 
11 SC 77.40% (+0.32) 
12 SP *77.70% (+0.62) 
13 SCH *77.80% (+0.72) 
14 SGP 77.37% (+0.29) 
15 SVG + SNP + SC **78.22% (+1.14) 
16 SP + SCH **77.96% (+0.88) 
 
 
 
 
 
 
 
 
Single  
technique 
 
 
 
 
Stacking 
17 SVG + SNP + SC + SP + SCH **78.44% (+1.36) 
 18 FO + TP **78.78% (+1.70) 
 19 FO + TS + TC + TP **78.47% (+1.39) 
 20 TP + SVG + SNP + SC **78.56% (+1.48) 
Combination 
 21 FO + TP + SVG + SNP + SC **78.98% (+1.90) 
Table 3. Evaluation results. 
(FO: feature optimization; TP TC TS: Pseudo-projective, Coordination and Subordinated sentence transformations; 
SVG, SNP, SC: Stacking (feature passing) on Verb Groups, NPs  and Coordination; 
SP, SCH, SGP: Stacking (category, features and dependency) on Parent, CHildren and GrandParent; 
*: statistically significant in McNemar's test, p < 0.005; **: statistically significant, p < 0.001) 
38
References  
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arrio-
la, Aitziber Atutxa, Arantza Diaz de Ilarraza, Aitzpea 
Garmendia and Maite Oronoz. 2003. Construction of 
a Basque dependency treebank. Treebanks and Lin-
guistic Theories. 
Xabier Artola, Arantza  D?az de Ilarraza, Nerea Ezei-
za, Koldo Gojenola, Gorka Labaka, Aitor Sologais-
toa, Aitor Soroa.  2005. A framework for 
representing and managing linguistic annotations 
based on typed feature structures. Proceedings of the 
International Conference on Recent Advances in 
Natural Language Processing, RANLP 2005. 
Kepa Bengoetxea and Koldo Gojenola. 2009a. Explor-
ing Treebank Transformations in Dependency Pars-
ing. Proceedings of the International Conference on 
Recent Advances in Natural Language Processing, 
RANLP?2009. 
Kepa Bengoetxea and Koldo Gojenola. 2009b. Applica-
tion of feature propagation to dependency parsing. 
Proceedings of the International Workshop on Pars-
ing Technologies (IWPT?2009). 
Xavier Carreras.  2007. Experiments with a high-order 
projective dependency parser. In Proceedings of the 
CoNLL 2007 Shared Task (EMNLP-CoNLL). 
Shay B. Cohen and Noah A. Smith. 2007. Joint Mor-
phological and Syntactic Disambiguation. In Pro-
ceedings of the CoNLL 2007 Shared Task. 
Michael Collins, Jan Hajic, Lance Ramshaw and Chris-
toph Tillmann. 1999. A Statistical Parser for Czech. 
Proceedings of ACL. 
Michael Collins. 1999. Head-Driven Statistical Models 
for Natural Language Parsing. PhD Dissertation, 
University of Pennsylvania.. 
Brooke Cowan and Michael Collins. 2005. Morphology 
and Reranking for the Statistical Parsing of Span-
ish. In Proceedings of EMNLP 2005. 
G?lsen Eryi?it, Joakim Nivre and Kemal Oflazer. 2008. 
Dependency Parsing of Turkish. Computational 
Linguistics, Vol. 34 (3). 
Yoav Goldberg and Reut Tsarfaty. 2008. A Single Gen-
erative Model for Joint Morphological Segmenta-
tion and Syntactic Parsing. Proceedings of ACL-
HLT 2008, Colombus, Ohio, USA.  
Johan Hall, Jens Nilsson, Joakim Nivre, G?lsen Eryigit, 
Be?ta Megyesi, Mattias Nilsson and Markus Saers. 
2007. Single Malt or Blended? A Study in Multilin-
gual Parser Optimization. Proceedings of the CoNLL 
Shared Task EMNLP-CoNLL. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, 
Eric P. Xing. 2008. Stacking Dependency Parsing. 
Proceedings of EMNLP-2008. 
Jens Nilsson, Joakim Nivre and Johan Hall. 2007. Gen-
eralizing Tree Transformations for Inductive De-
pendency Parsing. Proceedings of the 45th 
Conference of the ACL. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Springer. 
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A., 
G?lsen Eryi?it, Sandra K?bler, Marinov S., and 
Edwin Marsi. 2007a. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering.  
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task 
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. 
Joakim Nivre and Ryan McDonald. 2008. Integrating 
graph-based and transition-based dependency pars-
ers. Proceedings of ACL-2008. 
Ivan Titov and James Henderson. 2007. Fast and robust 
multilingual dependency parsing with a generative 
latent variable model. In Proceedings of the CoNLL 
2007 Shared Task (EMNLP-CoNLL). 
Reut Tsarfaty, Khalil Sima?an, and Remko Scha. 2009. 
An Alternative to Head-Driven Approaches for 
Parsing a (Relatively) Free Word-Order Language. 
Proceedings of EMNLP. 
 
 
39
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48?54,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Combining Rule-Based and Statistical Syntactic Analyzers 
 
 
Mar?a Jes?s Aranzabe*, Arantza D?az de Ilarraza, Nerea Ezeiza, Kepa Bengoetxea, 
Iakes Goenaga, Koldo Gojenola,  
Department of Computer Languages and Systems / * Department of Basque Philology 
University of the Basque Country UPV/EHU 
{maxux.aranzabe@ehu.es, kepa.bengoetxea, jipdisaa@si.ehu.es, 
n.ezeiza@ehu.es, koldo.gojenola@ehu.es, iakes@gmail.com} 
 
 
 
Abstract 
This paper presents the results of a set of 
preliminary experiments combining two 
knowledge-based partial dependency 
analyzers with two statistical parsers, 
applied to the Basque Dependency 
Treebank. The general idea will be to apply 
a stacked scheme where the output of the 
rule-based partial parsers will be given as 
input to MaltParser and MST, two state of 
the art statistical parsers. The results show 
a modest improvement over the baseline, 
although they also present interesting lines 
for further research. 
1. Introduction 
In this paper we present a set of preliminary 
experiments on the combination of two 
knowledge-based partial syntactic analyzers with 
two state of the art data-driven statistical parsers. 
The experiments have been performed on the 
Basque Dependency Treebank (Aduriz et al, 
2003). 
In the last years, many attempts have been 
performed trying to combine different parsers 
(Surdeanu and Manning, 2010), with significant 
improvements over the best individual parser?s 
baseline. The two most successful approaches have 
been stacking (Martins et al, 2008) and voting 
(Sagae and Lavie, 2006, Nivre and McDonald, 
2008, McDonald and Nivre, 2011). In this paper 
we will experiment the use of the stacking 
technique, giving the tags obtained by the rule-
based syntactic partial parsers as input to the 
statistical parsers. 
Morphologically rich languages present new 
challenges, as the use of state of the art parsers for 
more configurational and non-inflected languages 
like English does not reach similar performance 
levels in languages like Basque, Greek or Turkish 
(Nivre et al, 2007a). As it was successfully done 
on part of speech (POS) tagging, where the use of 
rule-based POS taggers (Tapanainen and 
Voutilainen, 1994) or a combination of a rule-
based POS tagger with a statistical one (Aduriz et 
al., 1997, Ezeiza et al, 1998) outperformed purely 
statistical taggers, we think that exploring the 
combination of knowledge-based and data-driven 
systems in syntactic processing can be an 
interesting line of research. 
Most of the experiments on combined parsers 
have relied on different types of statistical parsers 
(Sagae and Lavie, 2006, Martins et al, 2008, 
McDonald and Nivre, 2011), trained on an 
automatically annotated treebank. Yeh (2000) used 
the output of several baseline diverse parsers to 
increase the performance of a second 
transformation-based parser. In our work we will 
study the use of two partial rule-based syntactic 
analyzers together with two data-driven parsers: 
? A rule-based chunker (Aduriz et al, 2004) 
that marks the beginning and end of noun 
phrases, postpositional phrases and verb 
chains, in the IOB (Inside/ 
Outside/Beginning of a chunk) style. 
? A shallow dependency relation annotator 
(Aranzabe et al, 2004), which tries to 
detect dependency relations by assigning a 
48
set of predefined tags to each word, where 
each tag gives both the name of a 
dependency relation (e.g. subject) together 
with the direction of its head (left or right). 
? We will use two statistical dependency 
parsers, MaltParser (Nivre et al, 2007b) 
and MST (McDonald et al 2005). 
In the rest of this paper, section 2 will first 
present the corpus and the different parsers we will 
combine, followed by the experimental results in 
section 3, and the main conclusions of the work. 
2. Resources 
This section will describe the main resources that 
have been used in the experiments. First, 
subsection 2.1 will describe the Basque 
Dependency Treebank, and then subsection 2.2 
will explain the main details of the analyzers that 
have been employed. The analyzers are a rule-
based chunker, a rule-based shallow dependency 
parser and two state of the art data-driven 
dependency parsers, MaltParser and MST.  
2.1 Corpora 
Our work will make use the second version of the 
Basque dependency Treebank (BDT II, Aduriz et 
al., 2003), containing 150,000 tokens (11,225 
sentences). Figure 1 presents an example of a 
syntactically annotated sentence. Each word 
contains its form, lemma, category or coarse part 
of speech (CPOS), POS, morphosyntactic features 
such as case, number of subordinate relations, and 
the dependency relation (headword + dependency). 
The information in figure 1 has been simplified 
due to space reasons, as typically each word 
contains many morphosyntactic features (case, 
number, type of subordinated sentence, ...), which 
are relevant for parsing. The last two lines of the 
sentence in figure 1 do not properly correspond to 
the treebank, but are the result of the rule-based 
partial syntactic analyzers (see subsection 2.2). For 
evaluation, we divided the treebank in three sets, 
corresponding to training, development, and test 
(80%, 10%, and 10%, respectively). The 
experiments were performed on the development 
set, leaving the best system for the final test. 
2.2 Analyzers 
This subsection will present the four types of 
analyzers that have been used. The rule-based 
analyzers are based on the Contraint Grammar 
(CG) formalism (Karlsson et al, 1995), based on 
the assignment of morphosyntactic tags to words 
using a formalism that has the capabilities of finite 
state automata or regular expressions, by means of 
a set of rules that examine mainly local contexts of 
words to determine the correct tag assignment. 
The rule-based chunker (RBC henceforth, 
Aranzabe et al, 2009) uses 560 rules, where 479 of 
the rules deal with noun phrases and the rest with 
verb phrases. The chunker delimits the chunks with 
three tags, using a standard IOB marking style (see 
figure 1). The first one is to mark the beginning of 
the phrase (B-VP if it is a verb phrase and B-NP 
whether it's a noun phrase) and the other one to 
mark the continuation of the phrase (I-NP or I-VP, 
meaning that the word is inside an NP or VP). The 
last tag marks words that are outside a chunk. The 
evaluation of the chunker on the BDT gave a result 
of 87% precision and 85% recall over all chunks. 
We must take into account that this evaluation was 
auxmod 
ccomp_obj 
 auxmod 
Gizonak    mutil    handia   etorri     dela        esan      du . 
The-man       boy        tall-the    come         has+he+that     tell      he+did+it   
N-ERG-S       N          ADJ-ABS-S   V            AUXV+S+COMPL    V         AUXV 
B-NP          B-NP       I-NP        B-VP         I-NP            B-VP      I-VP 
&NCSUBJ>      &NCSUBJ>   $<NCMOD     $CCOMP_OBJ>  &<AUXMOD        &MAINV    &<AUXMOD 
ncsubj 
ncmod 
ncsubj 
Figure 1. Dependency tree for the sentence Gizonak mutil handia etorri dela esan du (the man told that the tall 
boy has come). The two last lines show the tags assigned by the rule-based chunker and the rule-based 
dependency analyzer, respectively. 
(V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG = 
ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, 
&MAINV = main verb, &<AUXMOD = verbal auxiliary modifier). 
 
49
performed on the gold POS tags, rather than on 
automatically assigned POS tasks, as in the present 
experiment. For that reason, the results can serve 
as an upper bound on the real results. 
The rule-based dependency analyzer (RBDA, 
Aranzabe et al, 2004) uses a set of 505 CG rules 
that try to assign dependency relations to 
wordforms. As the CG formalism only allows the 
assignment of tags, the rules only aim at marking 
the name of the dependency relation together with 
the direction of the head (left or right). For 
example, this analyzer assigns tags of the form 
&NCSUBJ> (see figure 1), meaning that the 
corresponding wordform is a non-clausal syntactic 
subject and that its head is situated to its right (the 
?>? or ?<? symbols mark the direction of the 
head). This means that the result of this analysis is 
on the one hand a partial analysis and, on the other 
hand, it does not define a dependency tree, and can 
also be seen as a set of constraints on the shape of 
the tree. The system was evaluated on the BDT, 
obtaining f-scores between 90% for the auxmod 
dependency relation between the auxiliary and the 
main verb and 52% for the subject dependency 
relation, giving a (macro) average of 65%. 
Regarding the data-driven parsers, we have 
made use of MaltParser (Nivre et al, 2007b) and 
MST Parser (McDonald et al, 2006), two state of 
the art dependency parsers representing two 
dominant approaches in data-driven dependency 
parsing, and that have been successfully applied to 
typologically different languages and treebanks 
(McDonald and Nivre, 2007).  
MaltParser (Nivre, 2006) is a representative of 
local, greedy, transition-based dependency parsing 
models, where the parser obtains deterministically 
a dependency tree in a single pass over the input 
using two data structures: a stack of partially 
analyzed items and the remaining input sequence. 
To determine the best action at each step, the 
parser uses history-based feature models and 
discriminative machine learning. The learning 
configuration can include any kind of information 
(such as word-form, lemma, category, subcategory 
or morphological features). Several variants of the 
parser have been implemented, and we will use 
one of its standard versions (MaltParser version 
1.4). In our experiments, we will use the Stack-
Lazy algorithm with the liblinear classifier.  
The MST Parser can be considered a 
representative of global, exhaustive graph-based 
parsing (McDonald et al, 2005, 2006). This 
algorithm finds the highest scoring directed 
spanning tree in a dependency graph forming a 
valid dependency tree. To learn arc scores, it uses 
large-margin structured learning algorithms, which 
optimize the parameters of the model to maximize 
the score margin between the correct dependency 
graph and all incorrect dependency graphs for 
every sentence in a training set. The learning 
procedure is global since model parameters are set 
relative to classifying the entire dependency graph, 
and not just over single arc attachments. This is in 
contrast to the local but richer contexts used by 
transition-based parsers. We use the freely 
available version of MSTParser1. In the following 
experiments we will make use of the second order 
non-projective algorithm.  
3. Experiments  
We will experiment the effect of using the output 
of the knowledge-based analyzers as input to the 
data-driven parsers in a stacked learning scheme. 
Figure 1 shows how the two last lines of the 
example sentence contain the tags assigned by the 
rule-based chunker (B-NP, I-NP, B-VP and I-VP) 
and the rule-based partial dependency analyzer 
(&NCSUBJ, &<NCMOD, &<AUXMOD, 
&CCOMP_OBJ and &MAINV) . 
The first step consisted in applying the complete 
set of text processing tools for Basque, including: 
? Morphological analysis. In Basque, each 
word can receive multiple affixes, as each 
lemma can generate thousands of word-
forms by means of morphological 
properties, such as case, number, tense, or 
different types of subordination for verbs. 
Consequently, the  morphological analyzer 
for Basque (Aduriz et al 2000) gives a 
high ambiguity. If only categorial (POS) 
ambiguity is taken into account, there is an 
average of 1.55 interpretations per word-
form, which rises to 2.65 when the full 
morphosyntactic information is taken into 
account, giving an overall 64% of 
ambiguous word-forms. 
? Morphological disambiguation. 
Disambiguating the output of 
morphological analysis, in order to obtain 
a single interpretation for each word-form, 
                                                           
1 http://mstparser.sourceforge.net 
50
can pose an important problem, as 
determining the correct interpretation for 
each word-form requires in many cases the 
inspection of local contexts, and in some 
others, as the agreement of verbs with 
subject, object or indirect object, it could 
also suppose the examination of elements 
which can be far from each other, added to 
the free constituent order of the main 
sentence elements in Basque. The 
erroneous assignment of incorrect part of 
speech or morphological features can 
difficult the work of the parser. 
? Chunker 
? Partial dependency analyzer 
When performing this task, we found the 
problem of matching the treebank tokens with 
those obtained from the analyzers, as there were 
divergences on the treatment of multiword units, 
mostly coming from Named Entities, verb 
compounds and complex postpositions (formed 
with morphemes appearing at two different words). 
For that reason, we performed a matching process 
trying to link the multiword units given by the 
morphological analysis module and the treebank, 
obtaining a correct match for 99% of the sentences.  
Regarding the data-driven parsers, they are 
trained using two kinds of tags as input: 
? POS and morphosyntactic tags coming 
from the automatic morphological 
processing of the dependency treebank. 
Disambiguation errors, such as an 
incorrect POS category or morphological 
analyses (e.g. the assignment of an 
incorrect case) can harm the parser, as 
tested in Bengoetxea et al (2011). 
? The output of the rule-based partial 
syntactic analyzers (two last lines of the 
example in figure 1). These tags contain 
errors of the CG-based syntactic taggers. 
As the analyzers are applied after 
morphological processing, the errors can 
be propagated and augmented. 
Table 1 shows the results of using the output of 
the knowledge-based analyzers as input to the 
statistical parsers. We have performed three 
experiments for each statistical parser, trying with 
the chunks provided by the chunker, the partial 
dependency parser, and both. The table shows 
modest gains, suggesting that the rule-based 
analyzers help the statistical ones, giving slight 
increases over the baseline, which are statistically 
significant when applying MaltParser to the output 
of the rule-based dependency parser and a 
combination of the chunker and rule-based parsers. 
As table 1 shows, the parser type is relevant, as 
MaltParser seems to be sensitive when using the 
stacked features, while the partial parsers do not 
seem to give any significant improvement to MST. 
3.1 Error analysis 
Looking with more detail at the errors made by the 
different versions of the parsers, we observe 
significant differences in the results for different 
dependency relations, seeing that the statistical 
parsers behave in a different manner regarding to 
each relation, as shown in table 2. The table shows 
the differences in f-score2  corresponding to five 
local dependency relations, (determination of 
verbal modifiers, such as subject, object and 
indirect object).  
McDonald and Nivre (2007) examined the types 
of errors made by the two data-driven parsers used 
in this work, showing how the greedy algorithm of 
MaltParser performed better with local dependency 
relations, while the graph-based algorithm of MST 
was more accurate for global relations. As both the 
chunker and the partial dependency analyzer are 
based on a set of local rules in the CG formalism, 
we could expect that the stacked parsers could 
benefit mostly on the local dependency relations. 
                                                           
2 f-score = 2 * precision * recall / (precision + recall) 
 MaltParser MST Parser 
 LAS UAS LAS UAS 
Baseline 76.77% 82.09%  77.96% 84.04% 
+ RBC 77.10% (+0.33) 82.29% (+0.20)  77.99% (+0.03) 83.99% (-0.05) 
+ RBDA *77.15% (+0.38) 82.27% (+0.18)  78.03% (+0.07) 83.76% (-0.28) 
+ RBC + RBDA  *77.25% (+0.48) 82.18% (+0.09)  78.00% (+0.04) 83.34% (-0.70) 
Table 1. Evaluation results  
(RBC = rule-based chunker, RBDA = rule-based dependency analyzer, LAS: Labeled Attachment Score,  
UAS: Unlabeled Attachment Score, *: statistically significant in McNemar's test, p < 0.05) 
 
51
Table 2 shows how the addition of the rule-based 
parsers? tags performs in accord with this behavior, 
as MaltParser gets f-score improvements for the 
local relations. Although not shown in Table 2, we 
also inspected the results on the long distance 
relations, where we did not observe noticeable 
improvements with respect to the baseline on any 
parser. For that reason, MaltParser, seems to 
mostly benefit of the local nature of the stacked 
features, while MST does not get a significant 
improvement, except for some local dependency 
relations, such as ncobj and ncsubj. 
We performed an additional test using the partial 
dependency analyzer?s gold dependency relations 
as input to MaltParser. As could be expected, the 
gold tags gave a noticeable improvement to the 
parser?s results, reaching 95% LAS. However, 
when examining the scores for the output 
dependency relations, we noticed that the gold 
partial dependency tags are beneficial for some 
relations, although negative for some others. For 
example the non-clausal modifier (ncmod) 
relation?s f-score increases 3.25 points, while the 
dependency relation for clausal subordinate 
sentences functioning as indirect object decreases 
0.46 points, which is surprising in principle. 
For all those reasons, the relation between the 
input dependency tags and the obtained results 
seems to be intricate, and we think that it deserves 
new experiments in order to determine their nature. 
As each type of syntactic information can have an 
important influence on the results on specific 
relations, their study can shed light on novel 
schemes of parser combination. 
4. Conclusions  
We have presented a preliminary effort to integrate 
different syntactic analyzers, with the objective of 
getting the best from each system. Although the 
potential gain is in theory high, the experiments 
have shown very modest improvements, which 
seem to happen in the set of local dependency 
relations. We can point out some avenues for 
further research: 
? Development of the rule-based 
dependency parser using the dependencies 
that give better improvements on the gold 
dependency tags, as this can measure the 
impact of each kind of shallow 
dependency tag on the data-driven parsers. 
? Development of rules that deal with the 
phenomena where the statistical parsers 
perform worse. This requires a careful 
error analysis followed by a redesign of 
the manually developed CG tagging rules. 
? Application of other types of combining 
schemes, such as voting, trying to get the 
best from each type of parser. 
Finally, we must also take into account that the 
rule-based analyzers were developed mainly 
having linguistic principles in mind, such as 
coverage of diverse linguistic phenomena or the 
treatment of specific syntactic constructions 
(Aranzabe et al, 2004), instead of performance-
oriented measures, such as precision and recall. 
This means that there is room for improvement in 
the first-stage knowledge-based parsers, which will 
have, at least in theory, a positive effect on the 
second-phase statistical parsers, allowing us to test 
whether knowledge-based and machine learning-
based systems can be successfully combined. 
Acknowledgements 
This research was supported by the Department of 
Industry of the Basque Government (IT344-10, S-
PE11UN114), the University of the Basque 
Country (GIU09/19) and the Spanish Ministry of 
 MaltParser MST Parser 
Dependency 
relation 
Baseline + RBC + RBDA + RBC  
+ RBDA 
Baseline + RBC + RBDA + RBC  
+ RBDA 
ncmod 75,29 75,90 76,08 76,40 77,15 77,44 76,39 76,92 
ncobj 67,34 68,49 69,67 69,54 64,85 64,86 65,56 66,18 
ncpred 61,37 61,92 61,26 63,50 60,37 57,55 58,44 59,27 
ncsubj 61,92 61,90 63,96 63,91 59,19 59,26 62,23 61,61 
nciobj 75,76 76,53 77,16 76,29 74,23 74,47 72,16 69,08 
Table 2. Comparison of the different parsers? f-score with regard to specific dependency relations 
(ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, 
nciobj = non-clausal indirect object) 
52
Science and Innovation (MICINN, TIN2010- 
20218). 
References  
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, 
Arantza D?az de Ilarraza, Koldo Gojenola and 
Montse Maritxalar. 1997. Morphosyntactic 
disambiguation for Basque based on the Constraint 
Grammar Formalism. Conference on Recent 
Advances in Natural Language Processing 
(RANLP), Bulgaria. 
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki 
Alegria, Xabier Arregi, Jose Mari Arriola, Xabier 
Artola, Koldo Gojenola, Montserrat Maritxalar, Kepa 
Sarasola, and Miriam Urkia. 2000. A word-grammar 
based morphological analyzer for agglutinative 
languages. Coling 2000, Saarbrucken. 
Itziar Aduriz, Jos? Mar?a Arriola, Arantza D?az de 
Ilarraza, Koldo Gojenola, Maite Oronoz and Larraitz 
Uria. 2004. A cascaded syntactic analyser for 
Basque. In Computational Linguistics and Intelligent 
Text Processing, pages 124-135. LNCS Series. 
Springer Verlag. Berlin. 2004 
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria 
Arriola, Aitziber Atutxa, Arantza Diaz de Ilarraza, 
Aitzpea Garmendia and Maite Oronoz. 2003. 
Construction of a Basque dependency treebank. 
Treebanks and Linguistic Theories. 
Mar?a Jes?s Aranzabe, Jos? Mar?a Arriola and Arantza 
D?az de Ilarraza. 2004. Towards a Dependency 
Parser for Basque. In Proceedings of the Workshop 
on Recent Advances in Dependency Grammar, 
Geneva, Switzerland. 
Maria Jesus Aranzabe, Jose Maria Arriola and Arantza 
D?az de Ilarraza. 2009. Theoretical and 
Methodological issues of tagging Noun Phrase 
Structures following Dependency Grammar 
Formalism. In Artiagoitia, X. and Lakarra J.A. (eds) 
Gramatika Jaietan. Patxi Goenagaren omenez. 
Donostia: Gipuzkoako Foru Aldundia-UPV/EHU. 
Kepa Bengoetxea and Koldo Gojenola. 2010. 
Application of Different Techniques to Dependency 
Parsing of Basque. Proceedings of the 1st Workshop 
on Statistical Parsing of Morphologically Rich 
Languages (SPMRL), NAACL-HLT Workshop. 
Kepa Bengoetxea, Arantza Casillas and Koldo 
Gojenola. 2011. Testing the Effect of Morphological 
Disambiguation in Dependency Parsing of Basque. 
Proceedings of the International Conference on 
Parsing Technologies (IWPT). 2nd Workshop on 
Statistical Parsing Morphologically Rich Languages 
(SPMRL), Dublin, Ireland. 
G?lsen Eryi?it, Joakim Nivre and Kemal Oflazer. 2008. 
Dependency Parsing of Turkish. Computational 
Linguistics, Vol. 34 (3).  
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n 
Urizar and Itziar Aduriz. 1998. Combining 
Stochastic and Rule-Based Methods for 
Disambiguation in Agglutinative Languages. 
COLING-ACL?98, Montreal. 
Fred Karlsson, Atro Voutilainen, Juka Heikkila and 
Arto Anttila. 1995. Constraint Grammar: A 
Language-independent System for Parsing 
Unrestricted Text. Mouton de Gruyter. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith and 
Eric P. Xing. 2008. Stacking Dependency Parsing. 
Proceedings of EMNLP-2008. 
Ryan McDonald, Kevin Lerman and Fernando Pereira. 
2005. Online large-margin training of dependency 
parsers. In Proceedings of ACL. 
Ryan McDonald, Kevin Lerman and Fernando Pereira. 
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. CoNLL. 
Ryan McDonald and Joakim Nivre. 2007. 
Characterizing the Errors of Data-Driven 
Dependency Parsing Models. Proceedings of the 
2007 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning, EMNLP/CoNLL. 
Ryan McDonald and Joakim Nivre. 2011. Analyzing 
and Integrating Dependency Parsers. Computational 
Linguistics, Vol. 37(1), 197-230. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Springer. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson, Sebastian Riedel and Deniz 
Yuret. 2007a. The CoNLL 2007 Shared Task on 
Dependency Parsing. Proceedings of EMNLP-
CoNLL. 
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, 
G?lsen Eryi?it, Sandra K?bler, S. Marinov and 
Edwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency 
parsing. Natural Language Engineering.  
Joakim Nivre and Ryan McDonald. 2008. Integrating 
graph-based and transition-based dependency 
parsers. Proceedings of ACL-2008. 
Kenji Sagae and Alon Lavie. 2006. Parser Combination 
by Reparsing. Proceedings of the Human Language 
53
Technology Conference of the North American 
Chapter of the ACL, pages 129?132, New York. 
Mihai Surdeanu and Christopher D. Manning. 2010. 
Ensemble Models for Dependency Parsing: Cheap 
and Good? Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the ACL. 
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging 
Accurately-Don?t guess if you know. Proceedings 
of the Conference on Applied Natural Language 
Processing, ANLP?94. 
Alexander Yeh. 2000. Using existing systems to 
supplement small amounts of annotated 
grammatical relations training data. Proceedings of 
ACL 2000. 
54
