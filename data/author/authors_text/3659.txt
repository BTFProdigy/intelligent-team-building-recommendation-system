Text Simplification for Reading Assistance: A Project Note
Kentaro Inui Atsushi Fujita Tetsuro Takahashi Ryu Iida
Nara Advanced Institute of Science and Technology
Takayama, Ikoma, Nara, 630-0192, Japan
finui,atsush-f,tetsu-ta,ryu-ig@is.aist-nara.ac.jp
Tomoya Iwakura
Fujitsu Laboratories Ltd.
Kamikodanaka, Nakahara, Kawasaki, Kanagawa, 211-8588, Japan
iwakura.tomoya@jp.fujitsu.com
Abstract
This paper describes our ongoing research
project on text simplification for congenitally
deaf people. Text simplification we are aiming
at is the task of offering a deaf reader a syn-
tactic and lexical paraphrase of a given text for
assisting her/him to understand what it means.
In this paper, we discuss the issues we should
address to realize text simplification and re-
port on the present results in three different
aspects of this task: readability assessment,
paraphrase representation and post-transfer er-
ror detection.
1 Introduction
This paper reports on our ongoing research into
text simplification for reading assistance. Potential
users targeted in this research are congenitally deaf
people (more specifically, students at (junior-)high
schools for the deaf), who tend to have difficulties
in reading and writing text. We are aiming at the
development of the technology of text simplification
with which a reading assistance system lexically and
structurally paraphrases a given text into a simpler
and plainer one that is thus more comprehensible.
The idea of using paraphrases for reading as-
sistance is not necessarily novel. For example,
Carroll et al (1998) and Canning and Taito (1999)
report on their project in which they address syn-
tactic transforms aiming at making newspaper text
accessible to aphasics. Following this trend of re-
search, in this project, we address four unexplored
issues as below besides the user- and task-oriented
evaluation of the overall system.
Before going to the detail, we first clarify the four
issues we have addressed in the next section. We
then reported on the present results on three of the
four, readability assessment, paraphrase representa-
tion and post-transfer error detection, in the subse-
quent sections.
2 Research issues and our approach
2.1 Readability assessment
The process of text simplification for reading as-
sistance can be decomposed into the following three
subprocesses:
a. Problem identification: identify which portions of
a given text will be difficult for a given user to
read,
b. Paraphrase generation: generate possible candi-
date paraphrases from the identified portions, and
c. Evaluation: re-assess the resultant texts to choose
the one in which the problems have been resolved.
Given this decomposition, it is clear that one of the
key issues in reading assistance is the problem of as-
sessing the readability or comprehensibility1 of text
because it is involved in subprocesses (a) and (c).
Readability assessment is doubtlessly a tough is-
sue (Williams et al, 2003). In this project, however,
we argue that, if one targets only a particular popu-
lation segment and if an adequate collection of data
is available, then corpus-based empirical approaches
may well be feasible. We have already proven that
one can collect such readability assessment data by
conducting survey questionnaires targeting teachers
at schools for the deaf.
1In this paper, we use the terms readability and comprehen-
sibility interchangeably, while strictly distinguishing them from
legibility of each fragment (typically, a sentence or paragraph)
of a given text.
2.2 Paraphrase acquisition
One of the good findings that we obtained through
the aforementioned surveys is that there are a broad
range of paraphrases that can improve the readabil-
ity of text. A reading assistance system is, therefore,
hoped to be able to generate sufficient varieties of
paraphrases of a given input. To create such a sys-
tem, one needs to feed it with a large collection of
paraphrase patterns. Very timely, the acquisition of
paraphrase patterns has been actively studied in re-
cent years:
 Manual collection of paraphrases in the context of
language generation, e.g. (Robin and McKeown,
1996),
 Derivation of paraphrases through existing lexical
resources, e.g. (Kurohashi et al, 1999),
 Corpus-based statistical methods inspired by the
work on information extraction, e.g. (Jacquemin,
1999; Lin and Pantel, 2001), and
 Alignment-based acquisition of paraphrases from
comparable corpora, e.g. (Barzilay and McKe-
own, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003).
One remaining issue is how effectively these meth-
ods contribute to the generation of paraphrases in our
application-oriented context.
2.3 Paraphrase representation
One of the findings obtained in the previous stud-
ies for paraphrase acquisition is that the automatic
acquisition of candidates of paraphrases is quite re-
alizable for various types of source data but acquired
collections tend to be rather noisy and need manual
cleaning as reported in, for example, (Lin and Pan-
tel, 2001). Given that, it turns out to be important to
devise an effective way of facilitating manual correc-
tion and a standardized scheme for representing and
storing paraphrase patterns as shared resources.
Our approach is (a) to define first a fully express-
ible formalism for representing paraphrases at the
level of tree-to-tree transformation and (b) devise an
additional layer of representation on its top that is de-
signed to facilitate handcoding transformation rules.
2.4 Post-transfer text revision
In paraphrasing, the morpho-syntactic informa-
tion of a source sentence should be accessible
throughout the transfer process since a morpho-
syntactic transformation in itself can often be a mo-
tivation or goal of paraphrasing. Therefore, such
an approach as semantic transfer, where morpho-
syntactic information is highly abstracted away as
in (Dorna et al, 1998; Richardson et al, 2001),
does not suit this task. Provided that the morpho-
syntactic stratum be an optimal level of abstraction
for representing paraphrasing/transfer patterns, one
must recall that semantic-transfer approaches such as
those cited above were motivated mainly by the need
for reducing the complexity of transfer knowledge,
which could be unmanageable in morpho-syntactic
transfer.
Our approach to this problem is to (a) leave the de-
scription of each transfer pattern underspecified and
(b) implement the knowledge about linguistic con-
straints that are independent of a particular trans-
fer pattern separately from the transfer knowledge.
There are a wide range of such transfer-independent
linguistic constraints. Constraints on morpheme
connectivity, verb conjugation, word collocation,
and tense and aspect forms in relative clauses are typ-
ical examples of such constraints.
These four issues can be considered as different
aspects of the overall question how one can make
the development and maintenance of a gigantic re-
source for paraphrasing tractable. (1) The introduc-
tion of readability assessment would free us from
cares about the purposiveness of each paraphrasing
rule in paraphrase acquisition. (2) Paraphrase ac-
quisition is obviously indispensable for scaling up
the resource. (3) A good formalism for representing
paraphrasing rules would facilitate the manual re-
finement and maintenance of them. (4) Post-transfer
error detection and revision would make the system
tolerant to flows in paraphrasing rules.
While many researchers have addressed the issue
of paraphrase acquisition reporting promising results
as cited above, the other three issues have been left
relatively unexplored in spite of their significance in
the above sense. Motivated by this context, in the
rest of this paper, we address these remaining three.
3 Readability assessment
To the best of our knowledge, there have never
been no reports on research to build a computational
model of the language proficiency of deaf people, ex-
cept for the remarkable reports by Michaud and Mc-
Coy (2001). As a subpart of their research aimed at
developing the ICICLE system (McCoy and Master-
man, 1997), a language-tutoring application for deaf
learners of written English, Michaud and McCoy de-
veloped an architecture for modeling the writing pro-
ficiency of a user called SLALOM. SLALOM is de-
signed to capture the stereotypic linear order of ac-
quisition within certain categories of morphological
and/or syntactic features of language. Unfortunately,
the modeling method used in SLALOM cannot be
directly applied to our domain for three reasons.
 Unlike writing tutoring, in reading assistance, tar-
get sentences are in principle unlimited. We
therefore need to take a wider range of morpho-
syntactic features into account.
 SLALOM is not designed to capture the difficulty
of any combination of morpho-syntactic features,
which it is essential to take into account in reading
assistance.
 Given the need to consider feature combinations,
a simple linear order model that is assumed in
SLALOM is unsuitable.
3.1 Our approach: We ask teachers
To overcome these deficiencies, we took yet an-
other approach where we designed a survey ques-
tionnaire targeting teachers at schools for the deaf,
and have been collecting readability assessment data.
In this questionnaire, we ask the teachers to compare
the readability of a given sentence with paraphrases
of it. The use of paraphrases is of critical importance
in our questionnaire since it makes manual readabil-
ity assessment significantly easier and more reliable.
3.1.1 Targets
We targeted teachers of Japanese or English liter-
acy at schools for the deaf for the following reasons.
Ideally, this sort of survey would be carried out
by targeting the population segment in question, i.e.,
deaf students in our study. In fact, pedagogists and
psycholinguists have made tremendous efforts to ex-
amine the language proficiency of deaf students by
giving them proficiency tests. Such efforts are very
important, but they have had difficulty in capturing
enough of the picture to develop a comprehensive
and implementable reading proficiency model of the
population due to the expense of extensive language
proficiency testing.
In contrast, our approach is an attempt to model
the knowledge of experts in this field (i.e., teaching
deaf students). The targeted teachers have not only
rich experiential knowledge about the language pro-
ficiency of their students but are also highly skilled in
paraphrasing to help their students? comprehension.
Since such knowledge gleaned from individual ex-
periences already has some generality, extracting it
through a survey should be less costly and thus more
comprehensive than investigation based on language
proficiency testing.
3.1.2 Questionnaire
In the questionnaire, each question consists of sev-
eral paraphrases, as shown in Figure 1 (a), where
(A) is a source sentence, and (B) and (C) are para-
phrases of (A). Each respondent was asked to as-
sess the relative readability of the paraphrases given
for each source sentence, as shown in Figure 1 (b).
The respondent judged sentence (A) to be the most
difficult and judged (B) and (C) to be comparable.
A judgment that sentence s
i
is easier than sentence
s
j
means that s
i
is judged likely to be understood
by a larger subset of students than s
j
. We asked
the respondents to annotate the paraphrases with
format-free comments, giving the reasons for their
judgments, alternative paraphrases, etc., as shown in
Figure 1 (b).
To make our questionnaire efficient for model ac-
quisition, we had to carefully control the variation in
paraphrases. To do that, we first selected around 50
morpho-syntactic features that are considered influ-
ential in sentence readability for deaf people. For
each of those features, we collected several sim-
ple example sentences from various sources (literacy
textbooks, grammar references, etc.). We then man-
ually produced several paraphrases from each of the
collected sentences so as to remove the feature that
characterized the source sentence from each para-
phrase. For example, in Figure 1, the feature char-
acterizing sentence (A) is a non-restrictive relative
clause (i.e., sentence (A) was selected as an example
of this feature). Neither (B) nor (C) has this feature.
We also controlled the lexical variety to minimize
the effect of lexical factors on readability; we also
restricted the vocabulary to a top-2000 basic word
set (NIJL, 1991).
3.1.3 Administration
We administrated a preliminary survey targeting
three teachers. Through the survey, we observed that
(a) the teachers largely agreed in their assessments of
relative readability, (b) their format-free comments
indicated that the observed differences in readabil-
ity were largely explainable in terms of the morpho-
syntactic features we had prepared, and (c) a larger-
scaled survey was needed to obtain a statistically re-
liable model. Based on these observations, we con-
ducted a more comprehensive survey, in which we
prepared 770 questions and sent questionnaires with
a random set of 240 of them to teachers of Japanese
or English literacy at 50 schools for the deaf. We
Figure 1: Sample question and response
asked them to evaluate as many as possible anony-
mously. We obtained 4080 responses in total (8.0
responses per question).
3.2 Readability ranking model
The task of ranking a set of paraphrases can be de-
composed into comparisons between two elements
combinatorially selected from the set. We consider
the problem of judging which of a given pair of para-
phrase sentences is more readable/comprehensible
for deaf students. More specifically, given para-
phrase pair (s
i
; s
j
), our problem is to classify it into
either left (s
i
is easier), right (s
j
is easier), or com-
parable (s
i
and s
j
are comparable).
Once the problem is formulated this way, we can
use various existing techniques for classifier learn-
ing. So far, we have examined a method of using the
support vector machine (SVM) classification tech-
nique.
A training/testing example is paraphrase pair
(s
i
; s
j
) coupled with its quantified class label
D(s
i
; s
j
) 2 [ 1; 1]. Each sentence s
i
is character-
ized by a binary feature vector F
s
i
, and each pair
(s
i
; s
j
) is characterized by a triple of feature vectors
hF
C
s
i
s
j
; F
L
s
i
s
j
; F
R
s
i
s
j
i, where
 F
C
s
i
s
j
= F
s
i
^ F
s
j
(features shared by s
i
and s
j
),
 F
L
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
i
),
 F
R
s
i
s
j
= F
s
i
^F
s
j
(features belonging only to s
j
).
D(s
i
; s
j
) represents the difference in readability be-
tween s
i
and s
j
; it is computed in the following way.
1. Let T
s
i
s
j
be the set of respondents who assessed
(s
i
; s
j
).
2. Given the degree of readability respondent t as-
signed to s
i
(s
j
), map it to real value dor(t; s) 2
[0; 1] so that the lowest degree maps to 0 and the
highest degree maps to 1. For example, the de-
gree of readability assigned to (A) in Figure 1 (b)
maps to around 0.1, whereas that assigned to (B)
maps to around 0.9.
3. D(s
i
; s
j
) =
1
jT
s
i
s
j
j
P
t2T
s
i
s
j
dor(t; s
i
)  dor(t; s
j
):
Output score Sc
M
(s
i
; s
j
) 2 [ 1; 1] for input
(s
i
; s
j
) was given by the normalized distance be-
tween (s
i
; s
j
) and the hyperplane.
3.3 Evaluation and discussion
To evaluate the two modeling methods, we con-
ducted a ten-fold cross validation on the set of 4055
paraphrase pairs derived from the 770 questions used
in the survey. To create a feature vector space, we
used 355 morpho-syntactic features. Feature annota-
tion was done semi-automatically with the help of a
morphological analyzer and dependency parser.
The task was to classify a given paraphrase pair
into either left, right, or comparable. Model M ?s
output class for (s
i
; s
j
) was given by
Cls
M
(s
i
; s
j
) =
(
left (Sc
M
(s
i
; s
j
)   
m
)
right (Sc
M
(s
i
; s
j
)  
m
)
comparable (otherwise)
;
where 
m
2 [ 1; 1] is a variable threshold used to
balance precision with recall.
We used the 473 paraphrase pairs that satisfied the
following conditions:
 jD(s
i
; s
j
)j was not less than threshold 
a
(
a
=
0:5). The answer of (s
i
; s
j
) is given by
Cls
Ans
(s
i
; s
j
) =
n
left (D(s
i
; s
j
)   
a
)
right (D(s
i
; s
j
)  
a
) :
 (s
i
; s
j
) must have been assessed by more then one
respondent, i.e., jT
s
i
s
j
j > 1:
 Agreement ratio Agr(s
i
; s
j
) must be suffi-
ciently high, i.e., Agr(s
i
; s
j
)  0:9, where
Agr(s
i
; s
j
) = (for (s
i
; s
j
)   agst(s
i
; s
j
))=
jT
s
i
s
j
j, and for (s
i
; s
j
) and agst(s
i
; s
j
) are the
number of respondents who agreed and disagreed
with Cls
Ans
(s
i
; s
j
), respectively.
We judged output class Cls
M
(s
i
; s
j
) correct if and
only if Cls
M
(s
i
; s
j
) = Cls
Ans
(s
i
; s
j
). The overall
performance was evaluated based on recall Rc and
precision Pr:
Rc =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
Ans
(s
i
;s
j
)2fleft;rightggj
Pr =
jf(s
i
;s
j
)j Cls
M
(s
i
; s
j
) is correctgj
jf(s
i
;s
j
)j Cls
M
(s
i
;s
j
)2fleft;rightgj
.
The model achieved 95% precision with 89% re-
call. This result confirmed that the data we collected
through the questionnaires were reasonably noiseless
and thus generalizable. Furthermore, both models
exhibited a clear trade-off between recall and preci-
sion, indicating that their output scores can be used
as a confidence measure.
4 Paraphrase representation
We represent paraphrases as transfer patterns be-
tween dependency trees. In this section, we propose
a three-layered formalism for representing transfer
patterns.
4.1 Types of paraphrases of concern
There are various levels of paraphrases as the fol-
lowing examples demonstrate:
(1) a. She burst into tears, and he tried to comfort
her.
b. She cried, and he tried to console her.
(2) a. It was a Honda that John sold to Tom.
b. John sold a Honda to Tom.
c. Tom bought a Honda from John.
(3) a. They got married three years ago.
b. They got married in 2000.
Lexical vs. structural paraphrases Example (1)
includes paraphrases of the single word ?comfort?
and the canned phrase ?burst into tears?. The sen-
tences in (2), on the other hand, exhibit structural
and thus more general patterns of paraphrasing. Both
types of paraphrases, lexical and structural para-
phrases, are considered useful for many applications
including reading assistance and thus should be in
the scope our discussion.
Atomic vs. compositional paraphrases The pro-
cess of paraphrasing (2a) into (2c) is compositional
because it can be decomposed into two subpro-
cesses, (2a) to (2b) and (2b) to (2c). In develop-
ing a resource for paraphrasing, we have only to
cover non-compositional (i.e., atomic) paraphrases.
Compositional paraphrases can be handled if an ad-
ditional computational mechanism for combining
atomic paraphrases is devised.
Meaning-preserving vs. reference-preserving
paraphrases It is also useful to distinguish
reference-preserving paraphrases from meaning-
preserving ones. The above example in (3) is of the
reference-preserving type. This types of paraphras-
ing requires the computation of reference to objects
outside discourse and thus should be excluded from
our scope for the present purpose.
4.2 Dependency trees (MDSs)
Previous work on transfer-based machine transla-
tion (MT) suggests that the dependency-based repre-
sentation has the advantage of facilitating syntactic
transforming operations (Meyers et al, 1996; Lavoie
et al, 2000). Following this, we adopt dependency
trees as the internal representations of target texts.
We suppose that a dependency tree consists of a set
of nodes each of which corresponds to a lexeme or
compound and a set of edges each of which repre-
sents the dependency relation between its ends. We
call such a dependency tree a morpheme-based de-
pendency structure (MDS). Each node in an MDS is
supposed to be annotated with an open set of typed
features that indicate morpho-syntactic and semantic
information. We also assume a type hierarchy in de-
pendency relations that consists of an open set of de-
pendency classes including dependency, compound,
parallel, appositive and insertion.
4.3 Three-layered representation
Previous work on transfer-based MT sys-
tems (Lavoie et al, 2000; Dorna et al, 1998)
and alignment-based transfer knowledge acqui-
sition (Meyers et al, 1996; Richardson et al,
2001) have proven that transfer knowledge can be
best represented by declarative structure mapping
(transforming) rules each of which typically consists
of a pair of source and target partial structures as in
the middle of Figure 2.
Adopting such a tree-to-tree style of representa-
tion, however, one has to address the issue of the
trade-off between expressibility and comprehensi-
bility. One may want a formalism of structural
rule editing
translation
compilation
simplified MDS transfer rule
N shika V- nai  ->  V no wa N dake da.
(someone does not V to nothing but N)   (it is only to N that someone does V)
MDS transfer rule
sp_rule(108, negation, RefNode) :-
  match(RefNode, X4=[pos:postp,lex: shika]),
  depend(X3=[pos:verb], empty, X4),
  depend(X1=[pos:aux_verb,lex: nai],
         X2=[pos:aux_verb*], X3),
  depend(X4, empty, X5=[pos:noun]),
  replace(X1, X6=[pos:aux_verb,lex: da]),
  substitute(X5, X12=[pos:noun]),
  move_dtrs(X5, X12),
  substitute(X3, X10=[pos:verb]),
                            :
pos: postp
lex: shika (except)
pos: aux_verb
lex:  da (copula)
pos: postp
lex: wa (TOP)
X6
X11
X12pos: nounlex:  no (thing)
pos: postp
lex: dake (only)
pos: noun
pos: noun
aux_verb*
pos: aux_verb
lex: nai (not)
pos: verbX3
X4
X1
X5
X2 X7
X8
X10 pos: verb
X9 vws
MDS processing operators
(=X5)
(=X2)
(=X3)
Figure 2: Three-layered rule representation
transformation patterns that is powerful enough to
represent a sufficiently broad range of paraphrase
patterns. However, highly expressible formalisms
would make it difficult to create and maintain rules
manually.
To mediate this trade-off, we devised a new layer
of representation to add on the top of the layer of
tree-to-tree pattern representation as illustrated in
Figure 2. At this new layer, we use an extended natu-
ral language to specify transformation patterns. The
language is designed to facilitate the task of hand-
coding transformation rules. For example, to define
the tree-to-tree transformation pattern given in the
middle of Figure 2, a rule editor needs only to spec-
ify its simplified form:
(4) N shika V- nai ! V no ha N dake da.
(Someone does V to nothing but N ! It is only to
N that someone does V)
A rule of this form is then automatically translated
into a fully-specified tree-to-tree transformation rule.
We call a rule of the latter form an MDS rewriting
rule (SR rule), and a rule of the former form a sim-
plified SR rule (SSR rule).
The idea is that most of the specifications of an SR
rule can usually be abbreviated if a means to auto-
matically complement it is provided. We use a parser
and macros to do so; namely, the rule translator com-
plements an SSR rule by macro expansion and pars-
ing to produce the corresponding SR rule specifica-
tions. The advantages of introducing the SSR rule
layer are the following:
 The SSR rule formalism allows a rule writer to
edit rules with an ordinary text editor, which
makes the task of rule editing much more efficient
than providing her/him with a GUI-based com-
plex tool for editing SR rules directly.
 The use of the extended natural language also
has the advantage in improving the readability of
rules for rule writers, which is particularly impor-
tant in group work.
 To parse SSR rules, one can use the same parser
as that used to parse input texts. This also im-
proves the efficiency of rule development because
it significantly reduces the burden of maintaining
the consistency between the POS-tag set used for
parsing input and that used for rule specifications.
The SSR rule layer shares underlying motiva-
tions with the formalism reported by Hermjakob et
al. (2002). Our formalism is, however, considerably
extended so as to be licensed by the expressibility of
the SR rule representation and to be annotated with
various types of rule applicability conditions includ-
ing constraints on arbitrary features of nodes, struc-
tural constraints, logical specifications such as dis-
junction and negation, closures of dependency rela-
tions, optional constituents, etc.
The two layers for paraphrase representation
are fully implemented on our paraphrasing engine
KURA (Takahashi et al, 2001) coupled with another
layer for processing MDSs (the bottom layer illus-
trated in Figure 2). The whole system of KURA
and part of the transer rules implemented on it
(see Section 5 below) are available at http://cl.aist-
nara.ac.jp/lab/kura/doc/.
5 Post-transfer error detection
What kinds of transfer errors tend to occur in lex-
ical and structural paraphrasing? To find it out, we
conducted a preliminary investigation. This section
reports a summary of the results. See (Fujita and
Inui, 2002) for further details.
We implemented over 28,000 transfer rules for
Japanese paraphrases on the KURA paraphrasing en-
gine based on the rules previously reported in (Sato,
1999; Kondo et al, 1999; Kondo et al, 2001; Iida et
al., 2001) and existing lexical resources such as the-
sauri and case frame dictionaries. The implemented
rules ranged from such lexical paraphrases as those
that replace a word with its synonym to such syn-
tactic/structural paraphrases as those that remove a
cleft construction from a sentence, devide a sentence,
etc. We then fed KURA with a set of 1,220 sentences
randomly sampled from newspaper articles and ob-
tained 630 transferred output sentences.
The following are the tendencies we observed:
 The transfer errors observed in the experiment ex-
hibited a wide range of variety from morphologi-
cal errors to semantic and discourse-related ones.
 Most types of errors tended to occur regardless
of the types of transfer. This suggests that if one
creates an error detection module specialized for
a particular error type, it works across different
types of transfer.
 The most frequent error type involved inappropri-
ate conjugation forms of verbs. It is, however,
a matter of morphological generation and can be
easily resolved.
 Errors in regard to verb valency and selectional
restriction also tended to be frequent and fatal,
and thus should have preference as a research
topic.
 The next frequent error type was related to the
difference of meaning between near synonyms.
However, this type of errors could often be de-
tected by a model that could detect errors of verb
valency and selectional restriction.
Based on these observations, we concluded that
the detection of incorrect verb valences and verb-
complement cooccurrence was one of the most se-
rious problems that should have preference as a re-
search topic. We are now conducting experiments
on empirical methods for detecting this type of er-
rors (Fujita et al, 2003).
6 Conclusion
This paper reported on the present results of our
ongoing research on text simplification for reading
assistance targeting congenitally deaf people. We
raised four interrelated issues that we needed address
to realize this application and presented our previ-
ous activities focuing on three of them: readabil-
ity assessment, paraphrase representation and post-
transfer error detection.
Regarding readability assessment, we proposed a
novel approach in which we conducted questionnaire
surveys to collect readability assessment data and
took a corpus-based empirical method to obtain a
readability ranking model. The results of the sur-
veys show the potential impact of text simplification
on reading assistance. We conducted experiments on
the task of comparing the readability of a given para-
phrase pair and obtained promising results by SVM-
based classifier induction (95% precision with 89%
recall). Our approach should be equally applicable
to other population segments such as aphasic read-
ers and second-language learners. Our next steps
includes the investigation of the drawbacks of the
present bag-of-features modeling approach. We also
need to consider a method to introduce the notion
of user classes (e.g. beginner, intermediate and ad-
vanced). Textual aspects of readability will also need
to be considered, as discussed in (Inui and Nogami,
2001; Siddahrthan, 2003).
Regarding paraphrase representation, we pre-
sented our revision-based lexico-structural para-
phrasing engine. It provides a fully expressible
scheme for representating paraphrases, while pre-
serving the easiness of handcraft paraphrasing rules
by providing an extended natural language as a
means of pattern editting. We have handcrafted over
a thousand transfer rules that implement a broad
range of lexical and structural paraphrasing.
The problem of error detection is also critical.
When we find a effective solution to it, we will be
ready to integrate the technologies into an applica-
tion system of text simplification and conduct user-
and task-oriented evaluations.
Acknowledgments
The research presented in this paper was partly
funded by PREST, Japan Science and Technology
Corporation. We thank all the teachers at the schools
for the deaf who cooperated in our questionnaire sur-
vey and Toshihiro Agatsuma (Joetsu University of
Education) for his generous and valuable coopera-
tion in the survey. We also thank Yuji Matsumoto
and his colleagues (Nara Advanced Institute of Sci-
ence and Technology) for allowing us to use their
NLP tools ChaSen and CaboCha, Taku Kudo (Nara
Advanced Institute of Science and Technology) for
allowing us to use his SVM tool, and Takaki Makino
and his colleagues (Tokyo University) for allow-
ing us to use LiLFeS, with which we implemented
KURA. We also thank the anonymous reviewers for
their suggestive and encouraging comments.
References
Barzilay, R. and McKeown, K. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 50?57.
Barzilay, R. and Lee, L. 2003. Learning to paraphrases: an
unsupervised approach using multiple-sequence align-
ment. In Proc. of HLT-NAACL.
Canning, Y. and Taito, J. 1999. Syntactic simplification of
newspaper text for aphasic readers. In Proc. of the 22nd
Annual International ACM SIGIR Conference (SIGIR).
Carroll, J., Minnen, G., Canning, Y., Devlin, S. and Tait, J.
1998. Practical simplification of English newspaper
text to assist aphasic readers. In Proc. of AAAI-98
Workshop on Integrating Artificial Intelligence and As-
sistive Technology.
Dorna, M., Frank, A., Genabith, J. and Emele, M. 1998.
Syntactic and semantic transfer with F-structures. In
Proc. of COLING-ACL, pages 341?347.
Fujita, A. and Inui, K. 2002. Decomposing linguistic
knowledge for lexical paraphrasing. In Information
Processing Society of Japan SIG Technical Reports,
NL-149, pages 31?38. (in Japanese)
Fujita, A., Inui, K. and Matsumoto, Y. 2003. Automatic
detection of verb valency errors in paraphrasing. In In-
formation Processing Society of Japan SIG Technical
Reports, NL-156. (in Japanese)
Hermjakob, U., Echihabi, A. and Marcu, D. 2002. Nat-
ural language based reformulation resource and Web
exploitation for question answering. In Proc. of the
TREC-2002 Conference.
Iida, R., Tokunaga, Y., Inui, K. and Eto, J. 2001. Explo-
ration of clause-structural and function-expressional
paraphrasing using KURA. In Proc. of the 63th Annual
Meeting of Information Processing Society of Japan,
pages 5?6. (in Japanese).
Inui, K. and Nogami, M. 2001. A paraphrase-based explo-
ration of cohesiveness criteria. In Proc. of the Eighth
European Workshop on Natulan Language Generation,
pages 101?110.
Jacquemin, C. 1999. Syntagmatic and paradigmatic rep-
resentations of term variations. In Proc. of the 37th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 341?349.
Kondo, K., Sato, S. and Okumura, M. 1999. Paraphras-
ing of ?sahen-noun + suru?. Journal of Information
Processing Society of Japan, 40(11):4064?4074. (in
Japanese).
Kondo, K., Sato, S. and Okumura, M. 2001. Para-
phrasing by case alternation. Journal of Informa-
tion Processing Society of Japan, 42(3):465?477. (in
Japanese).
Kurohashi, S. and Sakai, Y. 1999. Semantic analysis of
Japanese noun phrases: a new approach to dictionary-
based understanding. In Proc. of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 481?488.
Lavoie, B. Kittredge, R. Korelsky, T. Rambow, O. 2000.
A framework for MT and multilingual NLG ystems
based on uniform lexico-structural processing. In Proc.
of ANLP-NAACL.
Lin, D. and Pantel, P. 2001. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
McCoy ,K. F. and Masterman (Michaud), L. N. 1997. A
Tutor for Teaching English as a Second Language for
Deaf Users of American Sign Language, In Proc. of
ACL/EACL ?97 Workshop on Natural Language Pro-
cessing for Communication Aids.
Meyers, A., Yangarber, R. and Grishman, R. 1996. Align-
ment of shared forests for bilingual corpora. In Proc.
of the 16th International Conference on Computational
Linguistics (COLING), pages 460?465.
Michaud, L. N. and McCoy, K. F. 2001. Error profiling:
toward a model of English acquisition for deaf learn-
ers. In Proc. of the 39th Annual Meeting and the 10th
Conference of the European Chapter of Association for
Computational Linguistics (EACL), pages 386?393.
NIJL, the National Institute for Japanese Language. 1991.
Nihongo Kyo?iku-no tame-no Kihon-Goi Cho?sa (The
basic lexicon for the education of Japanese). Shuei
Shuppan, Japan. (In Japanese)
Richardson, S., Dolan, W., Menezes, A. and Corston-
Oliver, M. 2001. Overcoming the customization bottle-
neck using example-based MT. In Proc. of the 39th An-
nual Meeting and the 10th Conference of the European
Chapter of Association for Computational Linguistics
(EACL), pages 9?16.
Robin, J. and McKeown, K. 1996. Empirically designing
and evaluating a new revision-based model for sum-
mary generation. Artificial Intelligence, 85(1?2):135?
179.
Sato, S. 1999. Automatic paraphrase of technical pa-
pers? titles. Journal of Information Processing Society
of Japan, 40(7):2937?2945. (in Japanese).
Shinyama, Y., Sekine, S. Kiyoshi, Sudo. and Grishman,
R. 2002. Automatic paraphrase acquisition from news
articles. In Proc. of HLT, pages 40?46.
Siddahrthan, A. 2003. Preserving discourse structure
when simplifying text. In Proc. of European Workshop
on Natural Language Generation, pages 103?110.
Takahashi, T., Iwakura, T., Iida, R., Fujita, A. and Inui, K.
2001. KURA: a transfer-based lexico-structural para-
phrasing engine. In Proc. of the 6th Natural Language
Processing Pacific Rim Symposium (NLPRS) Workshop
on Automatic Paraphrasing: Theories and Applica-
tions, pages 37?46.
Williams, S., Reiter, E. and Osman, L. 2003. Experiments
with discourse-level choices and readability. In Proc. of
European Workshop on Natural Language Generation,
pages 127?134.
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 17?24
Manchester, August 2008
A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking
Tomoya Iwakura Seishi Okamoto
Fujitsu Laboratories Ltd.
1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588, Japan
{iwakura.tomoya,seishi}@jp.fujitsu.com
Abstract
Combination of features contributes to a
significant improvement in accuracy on
tasks such as part-of-speech (POS) tag-
ging and text chunking, compared with us-
ing atomic features. However, selecting
combination of features on learning with
large-scale and feature-rich training data
requires long training time. We propose a
fast boosting-based algorithm for learning
rules represented by combination of fea-
tures. Our algorithm constructs a set of
rules by repeating the process to select sev-
eral rules from a small proportion of can-
didate rules. The candidate rules are gen-
erated from a subset of all the features with
a technique similar to beam search. Then
we propose POS tagging and text chunk-
ing based on our learning algorithm. Our
tagger and chunker use candidate POS tags
or chunk tags of each word collected from
automatically tagged data. We evaluate
our methods with English POS tagging and
text chunking. The experimental results
show that the training time of our algo-
rithm are about 50 times faster than Sup-
port Vector Machines with polynomial ker-
nel on the average while maintaining state-
of-the-art accuracy and faster classification
speed.
1 Introduction
Several boosting-based learning algorithms have
been applied to Natural Language Processing
problems successfully. These include text catego-
rization (Schapire and Singer, 2000), Natural Lan-
guage Parsing (Collins and Koo, 2005), English
syntactic chunking (Kudo et al, 2005) and so on.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Furthermore, classifiers based on boosting-
based learners have shown fast classification speed
(Kudo et al, 2005).
However, boosting-based learning algorithms
require long training time. One of the reasons is
that boosting is a method to create a final hypoth-
esis by repeatedly generating a weak hypothesis in
each training iteration with a given weak learner.
These weak hypotheses are combined as the fi-
nal hypothesis. Furthermore, the training speed
of boosting-based algorithms becomes more of a
problem when considering combination of features
that contributes to improvement in accuracy.
This paper proposes a fast boosting-based algo-
rithm for learning rules represented by combina-
tion of features. Our learning algorithm uses the
following methods to learn rules from large-scale
training samples in a short time while maintaining
accuracy; 1) Using a rule learner that learns sev-
eral rules as our weak learner while ensuring a re-
duction in the theoretical upper bound of the train-
ing error of a boosting algorithm, 2) Repeating to
learn rules from a small proportion of candidate
rules that are generated from a subset of all the fea-
tures with a technique similar to beam search, 3)
Changing subsets of features used by weak learner
dynamically for alleviating overfitting.
We also propose feature-rich POS tagging and
text chunking based on our learning algorithm.
Our POS tagger and text chunker use candidate
tags of each word obtained from automatically
tagged data as features.
The experimental results with English POS tag-
ging and text chunking show drastically improve-
ment of training speeds while maintaining compet-
itive accuracy compared with previous best results
and fast classification speeds.
2 Boosting-based Learner
2.1 Preliminaries
We describe the problem treated by our boosting-
based learner as follows. Let X be the set of ex-
amples and Y be a set of labels {?1,+1}. Let
F = {f
1
, f
2
, ..., f
M
} be M types of features rep-
resented by strings. Let S be a set of training sam-
17
## S = {(x
i
, y
i
)}
m
i=1
: x
i
? X ,y
i
? {?1}
## a smoothing value ? =1
## rule number r: the initial value is 1.
Initialize: For i=1,...,m: w
1,i
= exp(
1
2
log(
W
+1
W
?1
));
While (r ? R)
## Train weak-learner using (S, {w
r,i
}
m
i=1
)
## Get ? types of rules: {f
j
}
?
j=1
{f
j
}
?
j=1
? weak-learner(S,{w
r,i
}
m
i=1
);
## Update weights with confidence value
Foreach f ? {f
j
}
?
j=1
c =
1
2
log(
W
r,+1
(f)+?
W
r,?1
(f)+?
)
For i=1,...,m: w
r+1,i
= w
r,i
exp(?y
i
h
?f ,c?
)
f
r
= f ; c
r
= c; r++;
endForeach
endWhile
Output: F (x) = sign(log(
W
+1
W
?1
) +
P
R
r=1
h
?f
r
,c
r
?
(x))
Figure 1: A generalized version of our learner
ples {(x
1
, y
1
), ..., (x
m
, y
m
)}, where each example
x
i
? X consists of features in F , which we call a
feature-set, and y
i
? Y is a class label. The goal is
to induce a mapping
F : X ? Y
from S.
Let |x
i
| (0 < |x
i
| ? M) be the number of fea-
tures included in a feature-set x
i
, which we call
the size of x
i
, and x
i,j
? F (1 ? j ? |x
i
| ) be a
feature included in x
i
.
1
We call a feature-set of
size k a k-feature-set. Then we define subsets of
feature-sets as follows.
Definition 1 Subsets of feature-sets
If a feature-set x
j
contains all the features in a
feature-set x
i
, then we call x
i
is a subset of x
j
and
denote it as
x
i
? x
j
.
Then we define weak hypothesis based on the
idea of the real-valued predictions and abstaining
(RVPA, for short) (Schapire and Singer, 2000).
2
Definition 2 Weak hypothesis for feature-sets
Let f be a feature-set, called a rule, x be a
feature-set, and c be a real number, called a con-
fidence value, then a weak-hypothesis for feature-
sets is defined as
h
?f ,c?
(x) =
{
c f ? x
0 otherwise
.
1
Our learner can handle binary vectors as in (Morishita,
2002). When our learner treats binary vectors for M attributes
{X
1
,...,X
m
}, the learner converts each vector to the corre-
sponding feature-set as x
i
? {f
i
|X
i,j
? X
i
? X
i,j
= 1}
(1 ? i ? m, 1 ? j ? M ).
2
We use the RVPA because training with RVPA is faster
than training with Real-valued-predictions (RVP) while main-
taining competitive accuracy (Schapire and Singer, 2000).
The idea of RVP is to output a confidence value for samples
which do not satisfy the given condition too.
2.2 Boosting-based Rule Learning
Our boosting-based learner selects R types of rules
for creating a final hypothesis F on several training
iterations. The F is defined as
F (x) = sign(
P
R
r=1
h
?f
r
,c
r
?
(x)).
We use a learning algorithm that generates
several rules from a given training samples
S = {(x
i
, y
i
)}
m
i=1
and weights over samples
{w
r,1
, ..., w
r,m
} as input of our weak learner. w
r,i
is the weight of sample number i after selecting
r?1 types of rules, where 0<w
r,i
, 1 ? i ? m and
1 ? r ? R.
Given such input, the weak learner selects ?
types of rules {f
j
}
?
j=1
(f
j
? F) with gain:
gain(f)
def
= |
p
W
r,+1
(f)?
p
W
r,?1
(f)|,
where f is a feature-set, and W
r,y
(f) is
W
r,y
(f) =
P
m
i=1
w
r,i
[[f ? x
i
? y
i
= y]],
and [[pi]] is 1 if a proposition pi holds and 0 other-
wise.
The weak learner selects a feature-set having the
highest gain as the first rule, and the weak learner
finally selects ? types of feature-sets having gain
in top ? as {f
j
}
?
j=1
at each boosting iteration.
Then the boosting-based learner calculates the
confidence value of each f in {f
j
}
?
j=1
and updates
the weight of each sample. The confidence value
c
j
for f
j
is defined as
c
j
=
1
2
log(
W
r,+1
(f
j
)
W
r,?1
(f
j
)
).
After the calculation of c
j
for f
j
, the learner up-
dates the weight of each sample with
w
r+1,i
= w
r,i
exp(?y
i
h
?f
j
,c
j
?
). (1)
Then the learner adds (f
j
, c
j
) to F as the r-
th rule and its confidence value.
3
When we
calculate the confidence value c
j+1
for f
j+1
, we
use {w
r+1,1
, ..., w
r+1,m
}. The learner adds (f
j+1
,
c
j+1
) to F as the r+1-th rule and confidence value.
After the updates of weights with {f
j
}
?
j=1
, the
learner starts the next boosting iteration. The
learner continues training until obtaining R rules.
Our boosting-based algorithm differs from the
other boosting algorithms in the number of rules
learned at each iteration. The other boosting-based
algorithms usually learn a rule at each iteration
3
Eq. (1) is the update of the AdaBoost used in ADTrees
learning algorithm (Freund and Mason, 1999). We use
this AdaBoost by the following two reasons. 1) The pa-
per (Iwakura and Okamoto, 2007) showed that the accuracy
of text chunking with the AdaBoost of ADTrees is slightly
higher than text chunking with the AdaBoost of BoosTexter
for RVPA (Schapire and Singer, 2000), 2) We expect the Ad-
aBoost of ADTrees can realize faster training because this Ad-
aBoost does not normalize weights at each update compared
with the AdaBoost of BoosTexter normalizes weights at each
iteration.
18
## sortByW (F ,fq): Sort features (f ? F )
## in ascending order based on weights of features
## (a % b): Return the reminder of (a? b)
## |B|-buckets: B = {B[0], ..., B[|B| ? 1]}
procedure distFT(S, |B|)
##Calculate the weight of each feature
Foreach (f?F) W
r
(f) =
P
m
i=1
w
r,i
[[{f} ? x
i
]]
##Sort features based on thier weights and
## store the results in Fs
Fs ? sortByW (F ,W
r
)
## Distribute features to buckets
For i=0...M : B[(i % |B|)] = (B[(i % |B|)] ? Fs[i])
return B
Figure 2: Distribute features to buckets based on weights
(Schapire and Singer, 2000; Freund and Mason,
1999). Despite the difference, our boosting-based
algorithm ensures a reduction in the theoretical up-
per bound of training error of the AdaBoost. We
list the detailed explanation in Appendix.A.
Figure 1 shows an overview of our boosting-
based rule learner. To avoid to happen that
W
r,+1
(f) or W
r,?1
(f) is very small or even zero,
we use the smoothed values ? (Schapire and
Singer, 1999). Furthermore, to reflect imbalance
class distribution, we use the default rule (Freund
and Mason, 1999), defined as
1
2
log(
W
+1
W
?1
), where
W
y
=
?
m
i=1
[[y
i
= y]] for y ? {?1}. The initial
weights are defined with the default rule.
3 Fast Rule Learner
3.1 Generating Candidate Rules
We use a method to generate candidate rules with-
out duplication (Iwakura and Okamoto, 2007).
We denote f
?
= f + f as the generation of k + 1-
feature-set f
?
consisting of a feature f and a k-
feature-set f . Let ID(f) be the integer corre-
sponding to f , called id, and ? be 0-feature-set.
Then we define gen generating a feature-set as
gen(f , f) =
(
f + f if ID(f) > max
f
?
?f
ID(f
?
)
? otherwise
.
We assign smaller integer to more infrequent fea-
tures as id. If there are features having the same
frequency, we assign id to each feature with lexi-
cographic order of features. Training based on this
candidate generation showed faster training speed
than generating candidates by an arbitrary order
(Iwakura and Okamoto, 2007).
3.2 Training with Redistributed Features
We propose a method for learning rules by repeat-
ing to select a rule from a small portion of can-
didate rules. We evaluated the effectiveness of
four types of methods to learn a rule from a sub-
set of features on boosting-based learners with a
text chunking task (Iwakura and Okamoto, 2007).
The results showed that Frequency-based distribu-
tion (F-dist) has shown the best accuracy. F-dist
## F
k
: A set of k-feature-sets
## R
o
: ? optimal rules (feature-sets)
## R
k,?
: ? k-feature-sets for generating candidates
## selectNBest(R, n, S, W
r
): n best rules from R
## with gain on {w
i,r
}
m
i=1
and training samples S
procedure weak-learner(F
k
, S, W
r
)
## ? best feature-sets as rules
R
o
= selectNBest( R
o
? F
k
, ?, S, W
r
);
if (? ? k) return R
o
; ## Size constraint
## ? best feature-sets in F
k
for generating candidates
R
k,?
= selectNBest(F
k
, ?, S, W
r
);
? = min
f?R
o
gain(f); ## The gain of ?-th optimal rule
Foreach ( f
k
? R
k,?
)
if ( u(f
k
) < ?) continue; ## Upper bound of gain
Foreach (f ? F ) ## Generate candidates
f
k+1
= gen(f
k
, f);
if (? ?
P
m
i=1
[[f
k+1
? x
i
]]) F
k+1
= (F
k+1
? f
k+1
);
end Foreach
end Foreach
return weak-learner(F
k+1
, S,W );
Figure 3: Find optimal feature-sets with given weights
distributes features to subsets of features, called
buckets, based on frequencies of features.
However, we guess training using a subset of
features depends on how to distribute features to
buckets like online learning algorithms that gener-
ally depend on the order of the training examples
(Kazama and Torisawa, 2007).
To alleviate the dependency on selected buck-
ets, we propose a method that redistributes fea-
tures, called Weight-based distribution (W-dist).
W-dist redistributes features to buckets based on
the weight of feature defined as
W
r
(f) =
P
m
i=1
w
r,i
[[{f} ? x
i
]]
for each f ? F after examining all buckets. Fig-
ure 2 describes an overview of W-dist.
3.3 Weak Learner for Learning Several Rules
We propose a weak learner that learns several rules
from a small portion of candidate rules.
Figure 3 describes an overview of the weak
learner. At each iteration, one of the |B|-buckets
is given as an initial 1-feature-sets F
1
. The weak
learner finds ? best feature-sets as rules from can-
didates consisting of F
1
and feature-sets generated
from F
1
. The weak learner generates candidates k-
feature-sets (1 < k) from ? best (k-1)-feature-sets
in F
k?1
with gain.
We also use the following pruning techniques
(Morishita, 2002; Kudo et al, 2005).
? Frequency constraint: We examine candidates
seen on at least ? different examples.
? Size constraint: We examine candidates whose
size is no greater than a size threshold ?.
? Upper bound of gain: We use the upper bound
of gain defined as
u(f)
def
= max(
p
W
r,+1
(f),
p
W
r,?1
(f)).
For any feature-set f
?
?F , which contains f (i.e.
f ? f
?
), the gain(f
?
) is bounded under u(f), since
0 ? W
r,y
(f
?
) ? W
r,y
(f) for y ? {?1}. Thus, if u(f)
19
## S = {(x
i
, y
i
)}
m
i=1
: x
i
?X , y
i
? {+1}
## W
r
= {w
r,i
}
m
i=1
: Weights of samples after learning
## r types of rules. w
1,i
= 1 (1 ? i ? m)
## |B| : The size of bucket B = {B[0], ..., B[|B| ? 1]}
## b, r : The current bucket and rule number
procedure AdaBoost.SDF()
B = distFT(S, |B|); ## Distributing features into B
## Initialize values and weights:
r = 1; b = 0; c
0
=
1
2
log(
W
+1
W
?1
);
For i = 1,...,m: w
1,i
= exp(c
0
);
While (r ? R) ## Learning R types of rules
##Select ? rules and increment bucket id b
R = weak-learner(B[b], S,W
r
); b++;
Foreach (f ? R) ##Update weights with each rule
c =
1
2
log(
W
r,+1
(f)+1
W
r,?1
(f)+1
);
For i=1,..,m w
r+1,i
= w
r,i
exp(?y
i
h
?f ,c?
);
f
r
= f ; c
r
= c; r++;
end Foreach
if (b == |B|) ## Redistribution
B = distFT(S, |B|); b=0;
end if
end While
return F (x) = sign(c
0
+
P
R
r=1
h
?f
r
,c
r
?
(x))
Figure 4: An overview of AdaBoost.SDF
? words, words that are turned into all capitalized,
prefixes and suffixes (up to 4) in a 7-word window.
? labels assigned to three words on the right.
? whether the current word has a hyphen,
a number, a capital letter
? whether the current word is all capital, all small
? candidate POS tags of words in a 7-word window
Figure 5: Feature types for POS tagging
is less than the gain of the current optimal rule ? ,
candidates containing f are safely pruned.
Figure 4 describes an overview of our algorithm,
which we call AdaBoost for a weak learner learn-
ing Several rules from Distributed Features (Ad-
aBoost.SDF, for short).
The training of AdaBoost.SDF with (? =
1, ? = ?, 1 < |B| ) is equivalent to the approach
of AdaBoost.DF (Iwakura and Okamoto, 2007). If
we use (|B| = 1,? = 1), AdaBoost.SDF examines
all features on every iteration like (Freund and Ma-
son, 1999; Schapire and Singer, 2000).
4 POS tagging and Text Chunking
4.1 English POS Tagging
We used the Penn Wall Street Journal treebank
(Marcus et al, 1994). We split the treebank into
training (sections 0-18), development (sections 19-
21) and test (sections 22-24) as in (Collins, 2002).
We used the following candidate POS tags, called
candidate feature, in addition to commonly used
features (Gim?enez and M`arquez, 2003; Toutanova
et al, 2003) shown in Figure 5.
We collect candidate POS tags of each word
from the automatically tagged corpus provided for
the shared task of English Named Entity recog-
nition in CoNLL 2003.
4
The corpus includes
17,003,926 words with POS tags and chunk tags
4
http://www.cnts.ua.ac.be/conll2003 /ner/
? words and POS tags in a 5-word window.
? labels assigned to two words on the right.
? candidate chunk tags of words in a 5-word window
Figure 6: Feature types for text chunking
annotated by a POS tagger and a text chunker.
Thus, the corpus includes wrong POS tags and
chunk tags.
We collected candidate POS tags of words that
appear more than 9 times in the corpus. We express
these candidates with one of the following ranges
decided by their frequency fq; 10 ? fq < 100,
100 ? fq < 1000 and 1000 ? fq.
For example, we express ?work? annotated as
NN 2000 times like ?1000?NN?. If ?work? is cur-
rent word, we add 1000?NN as a candidate POS
tag feature of the current word. If ?work? appears
the next of the current word, we add 1000?NN as
a candidate POS tag of the next word.
4.2 Text Chunking
We used the data prepared for CoNLL-2000 shared
tasks.
5
This task aims to identify 10 types of
chunks, such as, NP, VP and PP, and so on.
The data consists of subsets of Penn Wall Street
Journal treebank; training (sections 15-18) and test
(section 20). We prepared the development set
from section 21 of the treebank as in (Tsuruoka
and Tsujii, 2005).
6
Each base phrase consists of one word or more.
To identify word chunks, we use IOE2 representa-
tion. The chunks are represented by the following
tags: E-X is used for end word of a chunk of class
X. I-X is used for non-end word in an X chunk. O
is used for word outside of any chunk.
For instance, ?[He] (NP) [reckons] (VP) [the
current account deficit] (NP)...? is represented by
IOE2 as follows; ?He/E-NP reckons/E-VP the/I-
NP current/I-NP account/I-NP deficit/E-NP?.
We used features shown in Figure 6. We col-
lected the followings as candidate chunk tags from
the same automatically tagged corpus used in POS
tagging.
? Candidate tags expressed with frequency infor-
mation as in POS tagging
? The ranking of each candidate decided by fre-
quencies in the automatically tagged data
? Candidate tags of each word
For example, if we collect ?work? anno-
tated as I-NP 2000 times and as E-VP 100
time, we generate the following candidate fea-
tures for ?work?; 1000?I-NP, 100?E-VP<1000,
rank:I-NP=1 rank:E-NP=2, candidate=I-NP and
candidate=E-VP.
5
http://lcg-www.uia.ac.be/conll2000/chunking/
6
We used http://ilk.uvt.nl/?sabine/chunklink/chunklink 2-2-2000 for conll.pl
for creating development data.
20
Table 1: Training data for experiments: ] of S, M , ] of
cl and av. ] of ft indicate the number samples, the distinct
number of feature types, the number of class in each data set,
and the average number of features, respectively. POS and
ETC indicate POS-tagging and text chunking. The ?-c? in-
dicates using candidate features collected from parsed unla-
beled data.
data ] of S M ] of cl av. ] of ft
POS 912,344 579,052 45 22.09
POS-c 912,344 579,793 45 35.39
ETC 211,727 92,825 22 11.37
ETC-c 211,727 93,333 22 45.49
We converted the chunk representation of the
automatically tagged corpus to IOE2 and we col-
lected chunk tags of each word appearing more
than nine times.
4.3 Applying AdaBoost.SDF
AdaBoost.SDF treats the binary classification
problem. To extend AdaBoost.SDF to multi-class,
we used the one-vs-the-rest method.
To identify proper tag sequences, we use Viterbi
search. We map the confidence value of each clas-
sifier into the range of 0 to 1 with sigmoid function
7
, and select a tag sequence which maximizes the
sum of those log values by Viterbi search.
5 Experiments
5.1 Experimental Settings
We compared AdaBoost.SDF with Support Vec-
tor Machines (SVM). SVM has shown good per-
formance on POS tagging (Gim?enez and M`arquez,
2003) and Text Chunking (Kudo and Matsumoto,
2001). Furthermore, SVM with polynomial kernel
implicitly expands all feature combinations with-
out increasing the computational costs. Thus, we
compared AdaBoost.SDF with SVM.
8
To evaluate the effectiveness of candidate fea-
tures, we examined two types of experiments with
candidate features and without them. We list the
statics of training sets in Table 1.
We tested R=100,000, |B|=1,000, ? =
{1,10,100}, ?={1,10,100,?}, ?={1,2,3}, and
?={1,5} for AdaBoost.SDF. We tested the soft
margin parameter C={0.1,1,10} and the kernel
degree d={1,2,3} for SVM.
9
We used the followings for comparison; Train-
ing time is time to learn 100,000 rules. Best train-
ing time is time for generating rules to show the
best F-measure (F
?=1
) on development data. Ac-
curacy is F
?=1
on a test data with the rules at best
training time.
7
s(X) = 1/(1 + exp(??X)), where X = F (x) is a
output of a classifier. We used ?=5 in this experiment.
8
We used TinySVM (http://chasen.org/?taku/software/TinySVM/).
9
We used machines with 2.66 GHz QuadCore Intel Xeon
and 10 GB of memory for all the experiments.
Table 2: Experimental results of POS tagging and Text
Chunking (TC) with candidate features. F and time indicate
the average F
?=1
of test data and time (hour) to learn 100,000
rules for all classes with F-dist. These results are listed sepa-
rately with respect to each ? = {1, 5}.
? POS(? = 1) POS (? = 5) TC (? = 1) TC (? = 5)
F time F time F time F time
1 97.27 196.3 97.23 195.7 93.98 145.3 93.95 155.8
10 97.23 23.05 97.17 22.35 93.96 2.69 93.88 2.70
100 96.82 2.99 96.83 2.91 93.16 0.74 93.14 0.56
 88
 89
 90
 91
 92
 93
 94
 0  2  4  6  8  10
Ac
cur
acy
 (F-
mea
sure
)
Training Time (hour)
?=3 ?=?  ?=1?=3 ?=?  ?=10?=3 ?=?  ?=100?=3 ?=1 ?=1?=3 ?=1 ?=10?=3 ?=1 ?=100?=3 ?=10 ?=1?=3 ?=10?=10?=3 ?=10 ?=100?=3 ?=100 ?=1?=3 ?=100 ?=10?=3 ?=100 ?=100
Figure 7: Accuracy on development data of Text Chunk-
ing (? = 3) obtained with parsers based on F-dist. We mea-
sured accuracy obtained with rules at each training time. The
widest line is AdaBoost.SDF (
?=1,?=?
). The others are Ad-
aBoost.SDF (
?=10
(?),
?=100
(?),
?=1&?={1,10,100}
).
5.2 Effectiveness of Several Rule Learning
Table 2 shows average accuracy and training time.
We used F-dist as the distribution method. These
average accuracy obtained with rules learned by
AdaBoost.SDF (
?=10
) on both tasks are competi-
tive with the average accuracy obtained with rules
learned by AdaBoost.SDF (
?=1
). These results
have shown that learning several rules at each iter-
ation contributes significant improvement of train-
ing time. These results have also shown that the
learning several rule at each iteration methods are
more efficient than training by just using the fre-
quency constraint ?.
Figure 7 shows a snapshot for accuracy ob-
tained with chunkers using different number of
rules. This graph shows that chunkers based
on AdaBoost.SDF (
?=10,100
) and AdaBoost.SDF
(
?=1,?={1,10,100}
) have shown better accuracy than
chunkers based on AdaBoost.SDF (
?=1,?=?
) at
each training time. These result have shown that
learning several rules at each iteration and learning
combination of features as rules with a technique
similar to beam search are effective in improving
training time while giving a better convergence.
Figure 7 also implies that taggers and chunkers
based on AdaBoost.SDF (
?=100
) will show better
or competitive accuracy than accuracy of the oth-
ers by increasing numbers of rules to be learned
while maintaining faster convergence speed.
21
Table 3: Experimental results on POS tagging and Text
Chunking. Accuracies (F
?=1
) on test data and training time
(hour) of AdaBoost.SDF are averages of ?={1,10,100,?} for
each ? with F-dist and ? = 1. F
?=1
and time (hour) of SVMs
are averages of C={0.1,1,10} for each kernel parameter d.
POS tagging without candidate features
Alg. / 1 2 3
? (d) F
?=1
time F
?=1
time F
?=1
time
?=1 96.96 5.09 97.10 27.90 97.10 30.92
?=10 96.89 0.79 97.12 4.56 97.07 4.74
?=100 96.57 0.10 96.82 0.81 96.73 0.81
SVMs 96.60 101.63 97.15 166.76 96.93 625.32
POS tagging with candidate features
Alg. / 1 2 3
? (d) F
?=1
time F
?=1
time F
?=1
time
?=1 97.06 6.65 97.30 109.20 97.29 330.82
?=10 96.98 1.27 97.29 13.26 97.23 38.27
?=100 96.61 0.14 96.93 1.64 96.76 5.05
SVMs 96.76 170.24 97.31 206.39 97.23 1346.04
Text Chunking without candidate features
Alg. / 1 2 3
? (d) F
?=1
time F
?=1
time F
?=1
time
?=1 92.50 0.12 93.60 0.26 93.47 0.41
?=10 92.34 0.02 93.50 0.05 93.39 0.07
?=100 89.70 0.008 92.31 0.02 92.03 0.02
SVMs 92.14 8.55 93.91 7.38 93.49 9.82
Text Chunking with candidate features
Alg. / 1 2 3
? (d) F
?=1
time F
?=1
time F
?=1
time
?=1 92.89 0.25 94.19 26.10 94.04 300.77
?=10 92.85 0.04 94.11 2.97 94.08 3.06
?=100 91.99 0.01 93.37 0.32 93.24 0.34
SVMs 92.77 12.74 94.31 9.63 94.20 49.27
5.3 Comparison with SVM
Table 3 lists average accuracy and training time
on POS tagging and text chunking with respect
to each (?, ?) for AdaBoost.SDF and d for SVM.
AdaBoost.SDF with
?=10
and
?=100
have shown
much faster training speeds than SVM and Ad-
aBoost.SDF (
?=1,?=?
) that is equivalent to the
AdaBoost.DF (Iwakura and Okamoto, 2007).
Furthermore, the accuracy of taggers and chun-
kers based on AdaBoost.SDF (
?=10
) have shown
competitive accuracy with those of SVM-based
and AdaBoost.DF-based taggers and chunkers.
AdaBoost.SDF (
?=10
) showed about 6 and 54
times faster training speeds than those of Ad-
aBoost.DF on the average in POS tagging and text
chunking. AdaBoost.SDF (
?=10
) showed about
147 and 9 times faster training speeds than the
training speeds of SVM on the average of POS
tagging and text chunking. On the average of the
both tasks, AdaBoost.SDF (
?=10
) showed about
25 and 50 times faster training speed than Ad-
aBoost.DF and SVM. These results have shown
that AdaBoost.SDF with a moderate parameter ?
can improve training time drastically while main-
taining accuracy.
These results in Table 3 have also shown that
rules represented by combination of features and
the candidate features collected from automati-
cally tagged data contribute to improved accuracy.
5.4 Effectiveness of Redistribution
We compared F
?=1
and best training time of F-
dist and W-dist. We used ? = 2 that has shown
Table 4: Results obtained with taggers and chunkers based
on F-dist and W-dist. These results obtained with taggers and
chunkers trained with ? = {1, 10, 100,?} and ? = 2. F
and time indicate average F
?=1
on test data and average best
training time.
POS tagging with F-dist
? ?=1 ?=10 ?=100 ?=?
F time F time F time F time
1 97.31 30.03 97.31 64.25 97.32 142.9 97.26 89.59
10 97.26 3.21 97.32 9.57 97.30 15.54 97.30 19.64
100 96.86 0.62 96.95 1.32 96.95 2.13 96.96 2.43
POS tagging with W-dist
? ?=1 ?=10 ?=100 ?=?
F time F time F time F time
1 97.32 29.96 97.31 57.05 97.31 163.2 97.32 98.71
10 97.24 2.66 97.30 25.70 97.28 16.20 97.29 20.49
100 97.00 0.54 97.02 1.31 97.07 2.22 97.08 2.58
Text Chunking with F-dist
? ?=1 ?=10 ?=100 ?=?
F time F time F time F time
1 93.95 7.42 94.30 23.30 94.22 34.74 94.31 21.26
10 93.99 0.98 94.08 2.44 94.19 3.11 94.18 3.18
100 93.32 0.16 93.33 0.32 93.42 0.40 93.42 0.40
Text Chunking with W-dist
? ?=1 ?=10 ?=100 ?=?
F time F time F time F time
1 93.99 2.93 94.24 24.77 94.32 35.72 94.32 35.61
10 93.98 0.71 94.30 2.82 94.29 3.60 94.30 4.05
100 93.66 0.17 93.65 0.36 93.50 0.42 93.50 0.42
better average accuracy than ? = {1, 3} in both
tasks. Table 4 lists comparison of F-dist and W-
dist on POS tagging and text chunking. Most of
accuracy obtained with W-dist-based taggers and
parsers better than accuracy obtained with F-dist-
based taggers and parsers. These results have
shown that W-dist improves accuracy without dras-
tically increasing training time. The text chunker
and the tagger trained with AdaBoost.SDF (? = 10,
? = 10 and W-dist) has shown competitive accu-
racy with that of the chunker trained with Ad-
aBoost.SDF (? = 1, ? = ? and F-dist) while main-
taining about 7.5 times faster training speed.
5.5 Tagging and Chunking Speeds
We measured testing speeds of taggers and chun-
kers based on rules or models listed in Table 5.
10
We examined two types of fast classification al-
gorithms for polynomial kernel: Polynomial Ker-
nel Inverted (PKI) and Polynomial Kernel Ex-
panded (PKE). The PKI leads to about 2 to 12
times improvements, and the PKE leads to 30 to
300 compared with normal classification approach
of SVM (Kudo and Matsumoto, 2003).
11
The POS-taggers based on AdaBoost.SDF,
SVM with PKI, and SVM with PKE processed
4,052 words, 159 words, and 1,676 words per sec-
ond, respectively. The chunkers based on these
three methods processed 2,732 words, 113 words,
and 1,718 words per second, respectively.
10
We list average speeds of three times tests measured with
a machine with Xeon 3.8 GHz CPU and 4 GB of memory.
11
We use a chunker YamCha for evaluating classification
speeds based on PKI or PKE (http://www.chasen.org/?taku/software/
yamcha/). We list the average speeds of SVM-based tagger and
chunker with PKE of a threshold parameter ? = 0.0005 for
rule selection in both task. The accuracy obtained with mod-
els converted by PKE are slightly lower than the accuracy ob-
tained with their original models in our experiments.
22
Table 5: Comparison with previous best results: (Top :
POS tagging, Bottom: Text Chunking )
POS tagging F
?=1
Perceptron (Collins, 2002) 97.11
Dep. Networks (Toutanova et al, 2003) 97.24
SVM (Gim?enez and M`arquez, 2003) 97.05
ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 97.15
Guided learning for bidirectional sequence classification (Shen et al, 2007) 97.33
AdaBoost.SDF with candidate features (?=2,?=1,?=100, W-dist) 97.32
AdaBoost.SDF with candidate features (?=2,?=10,?=10, F-dist) 97.32
SVM with candidate features (C=0.1, d=2) 97.32
Text Chunking F
?=1
Regularized Winnow + full parser output (Zhang et al, 2001) 94.17
SVM-voting (Kudo and Matsumoto, 2001) 93.91
ASO + unlabeled data (Ando and Zhang, 2005) 94.39
CRF+Reranking(Kudo et al, 2005) 94.12
ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 93.70
LaSo (Approximate Large Margin Update) (Daum?e III and Marcu, 2005) 94.4
HySOL (Suzuki et al, 2007) 94.36
AdaBoost.SDF with candidate featuers (?=2,?=1,?=?, W-dist) 94.32
AdaBoost.SDF with candidate featuers (?=2,?=10,?=10,W-dist) 94.30
SVM with candidate features (C=1, d=2) 94.31
One of the reasons that boosting-based classi-
fiers realize faster classification speed is sparseness
of rules. SVM learns a final hypothesis as a linear
combination of the training examples using some
coefficients. In contrast, this boosting-based rule
learner learns a final hypothesis that is a subset of
candidate rules (Kudo and Matsumoto, 2004).
6 Related Works
6.1 Comparison with Previous Best Results
We list previous best results on English POS tag-
ging and Text chunking in Table 5. These results
obtained with the taggers and chunkers based on
AdaBoost.SDF and SVM showed competitive F-
measure with previous best results. These show
that candidate features contribute to create state-
of-the-art taggers and chunkers.
These results have also shown that
AdaBoost.SDF-based taggers and chunkers
show competitive accuracy by learning combi-
nation of features automatically. Most of these
previous works manually selected combination
of features except for SVM with polynomial
kernel and (Kudo and Matsumoto, 2001) a
boosting-based re-ranking (Kudo et al, 2005).
6.2 Comparison with Boosting-based
Learners
LazyBoosting randomly selects a small proportion
of features and selects a rule represented by a fea-
ture from the selected features at each iteration
(Escudero et al, 2000).
Collins and Koo proposed a method only up-
dates values of features co-occurring with a rule
feature on examples at each iteration (Collins and
Koo, 2005).
Kudo et al proposed to perform several pseudo
iterations for converging fast (Kudo et al, 2005)
with features in the cache that maintains the fea-
tures explored in the previous iterations.
AdaBoost.MH
KR
learns a weak-hypothesis rep-
resented by a set of rules at each boosting iteration
(Sebastiani et al, 2000).
AdaBoost.SDF differs from previous works in
the followings. AdaBoost.SDF learns several rules
at each boosting iteration like AdaBoost.MH
KR
.
However, the confidence value of each hypothe-
sis in AdaBoost.MH
KR
does not always minimize
the upper bound of training error for AdaBoost
because the value of each hypothesis consists of
the sum of the confidence value of each rule.
Compared with AdaBoost.MH
KR
, AdaBoost.SDF
computes the confidence value of each rule to min-
imize the upper bound of training error on given
weights of samples at each update.
Furthermore, AdaBoost.SDF learns several
rules represented by combination of features from
limited search spaces at each boosting itera-
tion. The creation of subsets of features in Ad-
aBoost.SDF enables us to recreate the same classi-
fier with same parameters and training data. Recre-
ation is not ensured in the random selection of sub-
sets in LazyBoosting.
7 Conclusion
We have proposed a fast boosting-based learner,
which we call AdaBoost.SDF. AdaBoost.SDF re-
peats to learn several rules represented by combi-
nation of features from a small proportion of can-
didate rules. We have also proposed methods to
use candidate POS tags and chunk tags of each
word obtained from automatically tagged data as
features in POS tagging and text chunking.
The experimental results have shown drastically
improvement of training speed while maintaining
competitive accuracy compared with previous best
results.
Future work should examine our approach on
several tasks. Future work should also compare
our algorithm with other learning algorithms.
Appendix A: Convergence
The upper bound of the training error for AdaBoost
of (Freund and Mason, 1999), which is used in Ad-
aBoost.SDF, is induced by adopting THEOREM 1
presented in (Schapire and Singer, 1999). Let Z
R
be
?
m
i=1
w
R+1,i
that is a sum of weights updated
with R rules. The bound holds on the training er-
ror after selecting R rules,
P
m
i=1
[[F (x
i
) 6= y
i
]] ? Z
R
is induced as follows.
By unraveling the Eq. (1), we obtain
w
R+1,i
= exp(?y
i
?
R
r=1
h
?f
r
,c
r
?
(x
i
)). Thus, we
obtain [[F (x
i
) 6= y
i
]] ? exp(?y
i
P
R
t=1
h
?f
r
,c
r
?
(x
i
)),
since if F (x
i
) 6= y
i
, then exp(?y
i
P
R
r=1
h
?f
r
?
(x
i
)) ?
1 . Combining these equations gives the stated
bound on training error
23
mX
i=1
[[F (x
i
) 6= y
i
]] ?
m
X
i=1
exp(?y
i
R
X
t=1
h
?f
r
,c
r
?
(x
i
))
=
m
X
i=1
w
R+1,i
= Z
R
. (2)
Then we show that the upper bound of training er-
ror Z
R
for R rules shown in Eq. (2) is less than or
equal to the upper bound of the training error Z
R?1
for R-1 rules. By unraveling the (2) and plug-
ging the confidence values c
R
= {
1
2
log(
W
r,+1
(f
R
)
W
r,?1
(f
R
)
), 0
} given by the weak hypothesis into the unraveled
equation, we obtain Z
R
?Z
R?1
, since
Z
R
=
m
X
i=1
w
R+1,i
=
m
X
i=1
w
R,i
exp(?y
i
h
?f
R
,c
R
?
)
=
m
X
i=1
w
R,i
?W
r,+1
(f
R
)?W
r,+1
(f
R
) +
W
r,+1
(f
R
)exp(?c
R
) +W
r,?1
(f
R
)exp(c
R
)
= Z
R?1
? (
p
W
R,+1
(f
R
)?
p
W
R,?1
(f
R
))
2
References
Ando, Rie and Tong Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Proc. of
43rd Meeting of Association for Computational Linguis-
tics, pages 1?9.
Collins, Michael and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computational
Linguistics, 31(1):25?70.
Collins, Michael. 2002. Discriminative training methods
for Hidden Markov Models: theory and experiments with
perceptron algorithms. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing,
pages 1?8.
Daum?e III, Hal and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin methods
for structured prediction. In Proc. of 22th International
Conference on Machine Learning, pages 169?176.
Escudero, Gerard, Llu??s M`arquez, and German Rigau. 2000.
Boosting applied to word sense disambiguation. In Proc.
of 11th European Conference on Machine Learning, pages
129?141.
Freund, Yoav and Llew Mason. 1999. The alternating de-
cision tree learning algorithm,. In Proc. of 16th Interna-
tional Conference on Machine Learning, pages 124?133.
Gim?enez, Jes?us and Llu??s M`arquez. 2003. Fast and accu-
rate part-of-speech tagging: The SVM approach revisited.
In Proc. of International Conference Recent Advances in
Natural Language Processing 2003, pages 153?163.
Iwakura, Tomoya and Seishi Okamoto. 2007. Fast training
methods of boosting algorithms for text analysis. In Proc.
of International Conference Recent Advances in Natural
Language Processing 2007, pages 274?279.
Kazama, Jun?ichi and Kentaro Torisawa. 2007. A new per-
ceptron algorithm for sequence labeling with non-local
features. In Proc. of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 315?324.
Kudo, Taku and Yuji Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of The Conference of the
North American Chapter of the Association for Computa-
tional Linguistics, pages 192?199.
Kudo, Taku and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proc. of 41st Meeting of As-
sociation for Computational Linguistics, pages 24?31.
Kudo, Taku and Yuji Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In Proc.
of the 2004 Conference on Empirical Methods in Natural
Language Processing 2004, pages 301?308, July.
Kudo, Taku, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features. In
Proc. of 43rd Meeting of Association for Computational
Linguistics, pages 189?196.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The Penn Treebank. pages 313?330.
Morishita, Shinichi. 2002. Computing optimal hypotheses
efficiently for boosting. Proc. of 5th International Confer-
ence Discovery Science, pages 471?481.
Schapire, Robert E. and Yoram Singer. 1999. Improved
boosting using confidence-rated predictions. Machine
Learning, 37(3):297?336.
Schapire, Robert E. and Yoram Singer. 2000. Boostexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135?168.
Sebastiani, Fabrizio, Alessandro Sperduti, and Nicola Val-
dambrini. 2000. An improved boosting algorithm and its
application to text categorization. In Proc. of International
Conference on Information and Knowledge Management,
pages 78?85.
Shen, Libin, Giorgio Satta, and Aravind Joshi. 2007. Guided
learning for bidirectional sequence classification. In Proc.
of 45th Meeting of Association for Computational Linguis-
tics, pages 760?767.
Suzuki, Jun, Akinori Fujino, and Hideki Isozaki. 2007. Semi-
supervised structured output learning based on a hybrid
generative and discriminative approach. In Proc. of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Lan-
guage Learning, pages 791?800.
Toutanova, Kristina, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proc. of the 2003
Human Language Technology Conference of the North
American Chapter of the Association for Computational
Linguistics, pages 173?180.
Tsuruoka, Yoshimasa and Junichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tagging
sequence data. In Proc. of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 467?474.
Zhang, Tong, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Proc. of
39th Meeting of Association for Computational Linguis-
tics, pages 539?546.
24
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 47?55,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Boosted Semi-Markov Perceptron
Tomoya Iwakura
Fujitsu Laboratories Ltd.
1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588, Japan
iwakura.tomoya@jp.fujitsu.com
Abstract
This paper proposes a boosting algorithm
that uses a semi-Markov perceptron. The
training algorithm repeats the training of a
semi-Markov model and the update of the
weights of training samples. In the boost-
ing, training samples that are incorrectly
segmented or labeled have large weights.
Such training samples are aggressively
learned in the training of the semi-Markov
perceptron because the weights are used
as the learning ratios. We evaluate our
training method with Noun Phrase Chunk-
ing, Text Chunking and Extended Named
Entity Recognition. The experimental re-
sults show that our method achieves better
accuracy than a semi-Markov perceptron
and a semi-Markov Conditional Random
Fields.
1 Introduction
Natural Language Processing (NLP) basic tasks,
such as Noun Phrase Chunking, Text Chunking,
and Named Entity Recognition, are realized by
segmenting words and labeling to the segmented
words. To realize these tasks, supervised learn-
ing algorithms have been applied successfully. In
the early stages, algorithms for training classifiers,
including Maximum Entropy Models (Tsuruoka
and Tsujii, 2005), AdaBoost-based learning algo-
rithms (Carreras et al, 2002), and Support Vector
Machines (SVMs) (Kudo and Matsumoto, 2001)
were widely used. Recently, learning algorithms
for structured prediction, such as linear-chain
structured predictions, and semi-Markov model-
based ones, have been widely used. The examples
of linear-chain structured predictions include Con-
ditional Random Fields (CRFs) (Lafferty et al,
2001) and structured perceptron (Collins, 2002).
The examples of semi-Markov model-based ones
include semi-Markov model perceptron (Cohen
and Sarawagi, 2004), and semi-Markov CRFs
(Sarawagi and Cohen, 2005). Among these
methods, semi-Markov-based ones have shown
good performance in terms of accuracy (Cohen
and Sarawagi, 2004; Sarawagi and Cohen, 2005;
Okanohara et al, 2006; Iwakura et al, 2011).
One of the reasons is that a semi-Markov learner
trains models that assign labels to hypothesized
segments (i.e., word chunks) instead of labeling
to individual words. This enables use of features
that cannot be easily used in word level processing
such as the beginning word of a segment, the end
word of a segment, and so on.
To obtain higher accuracy, boosting methods
have been applied to learning methods for training
classifiers. Boosting is a method to create a final
hypothesis by repeatedly generating a weak hy-
pothesis and changing the weights of training sam-
ples in each training iteration with a given weak
learner such as a decision stump learner (Schapire
and Singer, 2000) and a decision tree learner (Car-
reras et al, 2002). However, to the best of our
knowledge, there are no approaches that apply
boosting to learning algorithms for structured pre-
diction. In other words, if we can successful apply
boosting to learning algorithms for structured pre-
diction, we expect to obtain higher accuracy.
This paper proposes a boosting algorithm for
a semi-Markov perceptron. Our learning method
uses a semi-Markov perceptron as a weak learner,
and AdaBoost is used as the boosting algorithm.
To apply boosting to the semi-Markov perceptron,
the following methods are proposed; 1) Use the
weights of training samples decided by AdaBoost
as the learning ratios of the semi-Markov percep-
tron, and 2) Training on AdaBoost with the loss
between the correct output of a training sample
and the incorrect output that has the highest score.
By the first method, the semi-Markov perceptron
can aggressively learn training samples that are in-
47
correctly classified at previous iteration because
such training samples have large weights. The sec-
ond method is a technique to apply AdaBoost to
learning algorithms for structured prediction that
generate negative samples from N-best outputs
(Cohen and Sarawagi, 2004), or consider all pos-
sible candidates (Sarawagi and Cohen, 2005). We
also prove the convergence of our training method.
This paper is organized as follows: In Section
2, we describe AdaBoost and Semi-Markov per-
ceptron is described in Section 3. Our proposed
method is described in Section 4, and the experi-
mental setting, the experimetal results and related
work are described in Section 5, 6, and 7.
2 AdaBoost
Let X be a domain or sample space and Y be
a set of labels {?1,+1}. The goal is to in-
duce a mapping F : X ? Y. Let S be
{(x1, y1), ..., (xm, ym)}, which is a set of training
samples, where xi is a sample in X , and each yi
belongs toY . Each boosting learner learns T types
of weak hypothesis with a given weak learner to
produce a final hypothesis F :
F (x) = sign(
?T
t=1
?tht(x)).
where sign(x) is 1 if x is positive, otherwise, it
returns -1.
The ht (1 ? t ? T ) is the t-th weak hypothe-
sis learned by the weak learner. ht(x) is the pre-
diction to x ? X with ht, and ?t is the confi-
dence value of ht that is calculated by the boosting
learner.
The given weak learner learns a weak hypoth-
esis ht from training samples S = {(xi, yi)}mi=1
and weights over samples {wt,1, ..., wt,m} at
round t. wt,i is the weight of sample number i
at round t for 1 ? i ? m. We set w1,i to 1/m.
After obtaining t-th weak hypothesis ht, the
boosting learner calculates the confidence-value
?t for ht. Then, the boosting learner updates the
weight of each sample. We use the AdaBoost
framework (Freund and Schapire, 1997; Schapire
and Singer, 1999). The update of the sample
weights in AdaBoost is defined as follows:
wt+1,i = wt,i
e?yi ?tht(xi)
Zt(?t),
(1)
where e is Napier?s constant and
Zt(?t) =
m?
i=1
wt,ie?yi ?tht(xi) (2)
# Training data: S = {(Xi,Yi)}mi=1
# The learning rations of S: {?i}mi=1
# The maximum iteration of perceptron: P
SemiMarkovPerceptron(S, P, {?i}mi=1)
w = ?0, ..., 0? # Weight vector
a = ?0, ..., 0? # For averaged perceptron
c = 1 # The total number of iteration
For p = 1...P
For i = 1...m
Y?i = argmax
Y?Y(Xi)
w ? ?(Xi,Y)
IfY?i ?= Yi
w = w + ?i(?(Xi,Yi) ? ?(Xi,Y?i ))
a = w + c?i(?(Xi,Yi) ? ?(Xi,Y?i ))
endIf
c++
endFor
endFor
return (w - a / c)
Figure 1: A pseudo code of a semi-Markov per-
ceptron.
is the normalization factor for?mi=1 wt+1,i = 1.
Let pi be any predicate and [[pi]] be 1 if pi holds
and 0 otherwise. The following upper bound holds
for the training error of F consisting of T weak
hypotheses (Schapire and Singer, 1999):
1
m
m?
i=1
[[F (xi) ?= yi]] ?
T?
t=1
Zt(?t). (3)
Eq. (1) and Eq. (3) suggest AdaBoost-based learn-
ing algorithms will converge by repeatedly select-
ing a confidence-value of ?t for ht at each round,
that satisfies the following Eq. (4) at each round:
Zt(?t) < 1. (4)
3 Semi-Markov Perceptron
In a semi-Markov learner, instead of labeling indi-
vidual words, hypothesized segments are labeled.
For example, if a training with an input ?I win?
and a label set {NP ,V P } is conducted, consid-
ered segments with their labels are the follow-
ing: ?[I](NP ) [win](NP )?, ?[I](NP ) [win](V P )?,
?[I](V P ) [win](NP )?, ?[I](V P ) [win](V P )?, ?[I
win](NP )?, and ?[I win](V P )?.
Figure 1 shows a pseudo code of a semi-Markov
perceptron (Semi-PER) (Cohen and Sarawagi,
2004). We used the averaged perceptron (Collins,
48
2002) based on the efficient implementation de-
scribed in (Daume? III, 2006). Let S =
{(Xi,Yi)}mi=1 be a set of m training data, Xi
be i-th training sample represented by a word se-
quence, and Yi be the correct segments and the
correct labeling of Xi. Yi consists of |Yi| seg-
ments. Yi(j) means the j-th segment of Yi, and
l(Yi(j)) means the label ofYi(j).
?(X,Y) is a mapping to a D-dimensional fea-
ture vector defined as
?(X,Y) =
D?
d=1
|Y|?
j=1
?d(X,Y(j)),
where ?d is a feature represented by an indicator
function that maps an inputX and a segment with
its label Y(j) to a D-dimensional vector. For ex-
ample, ?100(X,Y(j)) might be the 100-th dimen-
sion?s value is 1 if the beginning word of Y(j) is
?Mr.? and the label l(Y(j)) is ?NP?.
w is a weight vector trained with a semi-
Markov perceptron. w??(X,Y) is the score given
to segments with their labels Y of X, and Y(X)
is the all possible segments with their labels for
X. The learning ratios of the training samples are
{?i}mi=1, and the ratios are set to 1 in a usual semi-
Markov perceptron training.
In the training of the Semi-PER, for a givenXi,
the learner finds Y?i with the Viterbi decoding as
described in (Cohen and Sarawagi, 2004):
Y?i = argmax
Y?Y(Xi)
w ? ?(X,Y).
If Y?i is not equivalent to Yi (i.e. Y?i ?= Yi), the
weight w is updated as follows:
w = w + ?i(?(Xi,Yi) ? ?(Xi,Y?i )).
The algorithm takes P passes over the training
samples.
4 A Boosted Semi-Markov Perceptron
This section describes how we apply AdaBoost to
a semi-Markov perceptron training.
4.1 Applying Boosting
Figure 2 shows a pseudo code for our boosting-
based Semi-PER. To train the Semi-PER within
an AdaBoost framework, we used the weights of
samples decided by AdaBoost as learning ratios.
The initial weight value of i-th sample at boosting
# Training data: S = {(Xi,Yi)}mi=1
# A weight vector at boosting round t: Wt
# The weights of S at round t: {wt,i}mi=1
# The iteration of perceptron training: P
# The iteration of boosting training: T
SemiBoost(S, T , P )
W0 = ?0, ..., 0?
Set initial value: w1,i = 1/m (for 1 ? i ? m)
While t ? T
wt=SemiMarkovPerceptron(S,P,{wt,i}mi=1)
Find ?t that satisfies Z?t(?t) < 1.
Update :Wt = Wt?1 + ?twt
For i = 1...m
wt+1,i = wt,i ? e? ?tdt(Xi)/Z?t(?t)
t++
endWhile
returnWT
Figure 2: A pseudo code of a boosting for a semi-
Markov perceptron.
round 1 is w1,i = 1/m. In the first iteration, Semi-
PER is trained with the initial weights of samples.
Then, we update the weights of training sam-
ples. Our boosting algorithm assigns larger
weights to training samples incorrectly segmented
or labeled. To realize this, we first define a loss for
Xi at boosting round t as follows:
dt(Xi) = st(Xi,Yi) ? st(Xi,Yti),
where,
Yti = argmax
Y?Y(Xi)?Y ?=Yi
st(Xi,Y),
and
st(X,Y) = wt ? ?(X,Y).
st(X,Y) is a score of a word sequence X that is
segmented and labeled as Y, and wt is a weight
vector trained by Semi-PER at boosting round t.
When a given input is correctly segmented and
labeled, the second best output is generated with
a forward-DP backward-A* N-best search algo-
rithm (Nagata, 1994). Then we find a confidence-
value ?t that satisfies Z?t(?t) < 1:
Z?t(?t) =
m?
i=1
wt,ie? ?tdt(Xi). (5)
After obtaining ?t, the weight of each sample is
updated as follows:
wt+1,i = wt,i ? e? ?tdt(Xi)/Z?t(?t). (6)
49
If st(Xi,Yi) is greater than st(Xi,Yti) (i.e., 0 <
dt(Xi)), the weight of Xi is decreased because
Xi is correctly segmented and labeled. Otherwise
(dt(Xi) < 0), Xi has a larger weight value. The
updated weights are used as the learning ratios
in the training of Semi-PER at the next boosting
round. Finally, we update the weight vector Wt
trained with boosting as follows:
Wt = Wt?1 + ?twt
This process is repeated T times, and a model
WT , which consists of T types of Semi-PER-
based models, is obtained.
In test phase, the segments and labels of a word
sequenceX is decided as follows:
Y? = argmax
Y?Y(X)
WT ? ?(X,Y).
4.2 Learning a Confidence Value
Since our algorithm handles real valued scores
of samples given by Semi-PER on the exponen-
tial loss of AdaBoost, it?s difficult to analyti-
cally determine a confidence-value ?t that satisfies
Z?t(?t) < 1 at boosting round t.
Therefore, we use a bisection search to find a
confidence-value. To detemin the range for the bi-
section search, we use a range between 0 and the
confidence-value for a weak hypothesis ht that re-
turns its prediction as one of {-1,+1}. We define
ht(Xi) as sign(dt(Xi)). Schapire and Singer pro-
posed an algorithm based on AdaBoost, called real
AdaBoost (Schapire and Singer, 1999). The real
AdaBoost analytically calculates the confidence-
value that minimizes Eq. (2). The derivation of
Zt(?t) with ?t is
Z ?t(?t) =
m?
i=1
?ht(Xi)wt,ie? ?tht(Xi).
By solving Z ?t(?t) = 0, we obtain
??t =
1
2 log(
?m
i=1 wt,i[[ht(Xi) = 1]]?m
i=1 wt,i[[ht(Xi) = ?1]]
).
Finally, we select the value that minimizes Eq.
(5) from the range between 0 and 2 ? ??t with the
bisection search as the confidence-value ?t. This
is because we expect to find a better confidence-
value from a wider range.
4.3 Convergence Analysis
If we repeatedly find a confidence-value (0 < ?t)
that satisfies Z?t(?t) < 1 at each boosting round,
the training of the semi-Markov model will be
converged as in the classification case described
in Section 2.1 The following bound on the train-
ing error can be proved:
1
m
m?
i=1
[[Y?i ?= Yi]] ?
T?
t=1
Z?t(?t)
where
Y?i = argmax
Y?Y(Xi)
WT ? ?(Xi,Y).
By unraveling Eq. (6), we have that
wT+1,i = wT,i ? e? ?tdt(Xi)/Z?t(?t)
= e
??Tt=1 ?tdt(Xi)
m?Tt=1 Z?t(?t)
= e
??Tt=1 ?twt?(?(Xi,Yi)??(Xi,Yti))
m
?T
t=1 Z?t(?t)
.
Therefore, ifY?i ?= Yi,
e?
?T
t=1 ?twt?(?(Xi,Yi)??(Xi,Y?i ))
m?Tt=1 Z?t(?t)
? wT+1,i,
since, for 1 ? t ? T ,
wt ? ?(Xi,Y?i ) ? wt ? ?(Xi,Yti).
Moreover, when Y?i ?= Yi, the following is satis-
fied.
1 ? e?
?T
t=1 ?twt?(?(Xi,Yi)??(Xi,Y?i ))
? e?
?T
t=1 ?twt?(?(Xi,Yi)??(Xi,Yti))
= e?
?T
t=1 ?tdt(Xi).
Therefore,
[[Y?i ?= Yi]] ? e?
?T
t=1 ?tdt(Xi).
These give the stated bound on training error;
1
m
m?
i=1
[[Y?i ?= Yi]] ?
?m
i=1 e?
?T
t=1 ?tdt(Xi)
m
=
m?
i=1
(
T?
t=1
Z?t(?t))wT+1,i
=
T?
t=1
Z?t(?t).
10 < ?t means the weighted error of the current Semi-
PER,?mi=1[[Yti ?= Yi]]wi,t, is less than 0.5 on the trainingdata. Fortunately, this condition was always satisfied with the
training of Semi-PER in our experiments.
50
5 Experimental Settings
5.1 Noun Phrase Chunking
The Noun Phrase (NP) chunking task was cho-
sen because it is a popular benchmark for test-
ing a structured prediction. In this task, noun
phrases called base NPs are identified. ?[He] (NP)
reckons [the current account deficit] (NP)...? is
an example. The training set consists of 8,936
sentences, and the test set consists of 2,012 sen-
tences.2 To tune parameters for each algorithm,
we used the 90% of the train data for the training
of parameter tuning, and the 10% of the training
data was used as a development data for measur-
ing accuracy at parameter tuning. A final model
was trained from all the training data with the pa-
rameters that showed the highest accuracy on the
development data.
5.2 Text Chunking
We used a standard data set prepared for CoNLL-
2000 shared task.3 This task aims to identify
10 types of chunks, such as, NP, VP, PP, ADJP,
ADVP, CONJP, INITJ, LST, PTR, and SBAR.
?[He] (NP) [reckons] (VP) [the current account
deficit] (NP)...? is an example of text chunk-
ing. The data consists of subsets of Penn Wall
Street Journal treebank; training (sections 15-18)
and test (section 20). To tune parameters for each
algorithm, we used the same approach of the NP
chunking one.
5.3 Japanese Extended NE Recognition
To evaluate our algorithm on tasks that include
large number of classes, we used an extended NE
recognition (ENER) task (Sekine et al, 2002).
This Japanese corpus for ENER (Hashimoto et al,
2008) consists of about 8,500 articles from 2005
Mainichi newspaper. The corpus includes 240,337
tags for 191 types of NEs. To segment words from
Japanese sentences, we used ChaSen.4 Words may
include partial NEs because words segmented with
ChaSen do not always correspond with NE bound-
aries. If such problems occur when we segment
the training data, we annotated a word chunk with
the type of the NE included in the word chunk.
The evaluations are performed based on the gold
2We used the data obtained from ftp://ftp.cis.upenn.edu/
pub/chunker/ .
3http://lcg-www.uia.ac.be/conll2000/chunking/
4We used ChaSen-2.4.2 with Ipadic-2.7.0. ChaSen?s web
page is http://chasen-legacy.sourceforge.jp/.
Table 1: Features.
[tj , CLj ], [tj ,WBj ], [tj , PBj ],
[tj , wbp], [tj , pbp],
[tj , wep], [tj , pep], [tj , wip],[tj , pip] ,
[tj , wbp, wep], [tj , pbp, pep],
[tj , wbp, pep], [tj , pbp, wep],
[tj , wbp?1], [tj , pbp?1], [tj , wbp?2], [tj , pbp?2],
[tj , wep+1], [tj , pep+1], [tj , wep+2], [tj , pep+2],
[tj , pbp?2, pbp?1], [tj , pep+1, pep+2],
[tj , pbp?2, pbp?1, pbp], [tj , pep, pep+1, pep+2]
% Features used for only Text Chunking and NP Chunking
[tj , wbp, wip], [tj , wbp, pip],
[tj , wbp, pip], [tj , pbp, pip],
[tj , wep, wip], [tj , wep, pip],
[tj , wep, pip], [tj , pep, pip],
[tj , wbp, wep, wip], [tj , wbp, wep, pip],
[tj , wbp, wep, pip], [tj , wbp, pep, pip]
standard data for the test. We created the follow-
ing sets for this experiment. Training data is news
articles from January to October 2005 in the cor-
pus, which includes 205,876 NEs. Development
data is news articles in November 2005 in the cor-
pus, which includes 15,405 NEs. Test data is news
articles in December 2005 in the corpus, which in-
cludes 19,056 NEs.
5.4 Evaluation Metrics
Our evaluation metrics are recall (RE), precision
(PR), and F-measure (FM ) defined as follows:
RE = Cok/Call, PR = Cok/Crec
and
FM = 2 ?RE ? PR/(RE + PR),
where Cok is the number of correctly recognized
chunks with their correct labels,Call is the number
of all chunks in a gold standard data, and Crec is
the number of all recognized chunks.
5.5 Features
Table 1 lists features used in our experiments. For
NP Chunking and Text Chunking, we added fea-
tures derived from segments in addition to ENER
features.5
wk is the k-th word, and pk is the Part-Of-
Speech (POS) tag of k-th word. bp is the position
of the first word of the current segment in a given
5We did not use the additional features for ENER because
the features did not contribute to accuracy.
51
word sequence. ep indicates the position of the last
word of the current segment. ip is the position of
words inside the current segment (bp < ip < ep).
If the length of the current segment is 2, we use
features that indicate there is no inside word as the
features of ip-th words. tj is the NE class label
of j-th segment. CLj is the length of the current
segment, whether it be 1, 2, 3, 4, or longer than 4.
WBj indicates word bigrams, and PBj indicates
POS bigrams inside the current segment.
5.6 Algorithms to be Compared
The following algorithms are compared with our
method.
? Semi-Markov perceptron (Semi-PER)
(Cohen and Sarawagi, 2004): We used one-
best output for training. This Semi-PER is
also used as the weak learner of our boosting
algorithm.
? Semi-Markov CRF (Semi-CRF) (Sarawagi
and Cohen, 2005): To train Semi-CRF, a
stochastic gradient descent (SGD) training
for L1-regularized with cumulative penalty
(Tsuruoka et al, 2009) was used. The batch
size of SGD was set to 1.
These algorithms are based on sequentially
classifying segments of several adjacent words,
rather than single words. Ideally, all the possi-
ble word segments of each input should be con-
sidered for this algorithm. However, the training
of these algorithms requires a great deal of mem-
ory. Therefore, we limit the maximum size of the
word-segments. We use word segments consisting
of up to ten words due to the memory limitation.
We set the maximum iteration for Semi-PER
to 100, and the iteration number for Semi-CRF
trained with SGD to 100 ? m, where m is the
number of training samples. The regularization
parameter C of Semi-CRF and the number of it-
eration for Semi-PER are tuned on development
data.6 For our boosting algorithm, the number of
boosting iteration is tuned on development data
with the number of iteration for Semi-PER tuned
on development data. We set the maximum itera-
tion number for boosting to 50.
6For C of Semi-CRF,
{1, 10?1, 10?2, 10?3, 10?4, 10?5, 10?6, 10?7, 10?8, 10?9}
were examined.
Table 2: Results of NP Chunking.
Learner F-measure Recall Precision
Semi-PER 94.32 94.53 94.11
Semi-CRF 94.32 94.52 94.13
Semi-Boost 94.60 94.85 94.35
Table 3: Results of Text Chunking.
Learner F-measure Recall Precision
Semi-PER 94.10 94.15 94.05
Semi-CRF 93.79 93.96 93.62
Semi-Boost 94.15 94.27 94.03
6 Experimental Results
We used a machine with Intel(R) Xeon(R) CPU
X5680@ 3.33GHz and 72 GBmemory. In the fol-
lowing, our proposed method is referred as Semi-
Boost.
6.1 NP Chunking
Table 2 shows the experimental results on NP
Chunking. Semi-Boost showed the best accuracy.
Semi-Boost showed 0.28 higher F-measure than
Semi-PER and Semi-CRF. To compare the results,
we employed a McNemar paired test on the label-
ing disagreements as was done in (Sha and Pereira,
2003). All the results indicate that there is a sig-
nificant difference (p < 0.01). This result shows
that Semi-Boost showed high accuracy.
6.2 Text Chunking
Table 3 shows the experimental results on Text
Chunking. Semi-Boost showed 0.36 higher F-
measure than Semi-CRF, and 0.05 higher F-
measure than Semi-PER. The result of McNemar
test indicates that there is a significant difference
(p < 0.01) between Semi-Boost and Semi-CRF.
However, there is no significant difference be-
tween Semi-Boost and Semi-PER.
6.3 Extended Named Entity Recognition
Table 4 shows the experimental results on ENER.
We could not train Semi-CRF because of the lack
of memory for this task. Semi-Boost showed 0.24
higher F-measure than that of Semi-PER. The re-
sults indicate there is a significant difference (p <
52
Table 4: Experimental results for ENER.
Learner F-measure Recall Precision
Semi-PER 81.86 79.06 84.87
Semi-CRF N/A
Semi-Boost 82.10 79.36 85.03
Table 5: Training time of each learner (second)
for NP Chunking (NP), Text Chunking (TC) and
ENER. The number of Semi-Boost iteration is
only one time. The +20 cores means training of
Semi-Boost with 20 cores.
Learner NP TC ENER
Semi-PER 475 559 13,559
Semi-CRF 2,120 8,228 N/A
Semi-Boost 499 619 32,370
+20 cores 487 650 19,598
0.01).7
6.4 Training Speed
We compared training speed under the following
condition; The iteration for Semi-PER is 100, the
iteration number for Semi-CRF trained with SGD
is 100?m, wherem is the number of training sam-
ples, and the one time iteration of boosting with
the perceptron iteration 100. Therefore, all train-
ing methods attempted 100?m times estimation.
Table 5 shows the training time of each learner.
In NP Chunking, the training time of Semi-PER,
Semi-CRF, and Semi-Boost were 475 seconds,
2,120 seconds, and 499 seconds. In Text Chunk-
ing, the training time of Semi-PER, Semi-CRF,
and our method were 559 seconds, 8,228 seconds,
and 619 seconds. Semi-Boost shows competitive
training speed with Semi-PER and 4 to 13 times
faster training speed in terms of the total number
of parameter estimations The difference of time
between Semi-PER and our method is the time for
calculating confidence-value of boosting.
When Semi-Boost trained a model for ENER,
the training speed was degraded. The training time
of Semi-Boost was 32,370 and the training time
of Semi-PER was 13,559. One of the reasons is
the generation of an incorrect output of each train-
7The results on the test data were compared by character
units as in Japanese morphological analysis (Iwakura et al,
2011). This is because the ends or beginnings of Japanese
NEs do not always correspond with word boundaries.
Table 6: The best results for NP Chunking (FM ).
(Kudo and Matsumoto, 2001) 94.22
(Sun et al, 2009) 94.37
This paper 94.60
ing sample. In our observation, when the num-
ber of classes is increased, the generation speed of
incorrect outputs with N-best search is degraded.
To improve training speed, we used 20 cores for
generating incorrect outputs. When the training
with 20 cores was conducted, the training data was
split to 20 portions, and each portion was pro-
cessed with one of each core. The training time
with the 20 cores was 19,598 for ENER. However,
the training time of NP Chunking was marginally
improved and that of Text Chunking was slightly
increased. This result implies that multi-core pro-
cessing is effective for the training of large classes
like ENER in Semi-Boost.
In fact, since Semi-Boost requires additional
boosting iterations, the training time of Semi-
Boost increases. However, the training time in-
creases linearly by the number of boosting itera-
tion. Therefore, Semi-Boost learned models from
the large training data of ENER.
6.5 Memory Usage
Semi-Boost consumed more memory than Semi-
PER. This is because our learning method main-
tains a weight vector for boosting in addition to
the weight vector of Semi-PER. Compared with
Semi-CRF, Semi-Boost showed lower memory
consumption. On the training data for Text Chunk-
ing, the memory size of Semi-Boost, Semi-PER,
and Semi-CRF are 4.4 GB, 4.1 GB, and 18.0 GB.
When we trained models for ENER, Semi-PER
consumed 32 GB and Semi-Boost consumed 33
GB. However, Semi-CRF could not train mod-
els because of the lack of memory. This is be-
cause Semi-CRF maintains a weight vector and a
parameter vector for L1-norm regularization and
Semi-CRF considers all possible patterns gener-
ated from given sequences in training. In contrast,
Semi-PER and Semi-Boost only consider features
that appeared in correct ones and incorrectly rec-
ognized ones. These results indicate that Semi-
Boost can learn models from large training data.
53
Table 7: The best results for Text Chunking (FM ).
Semi-supervised learning
(Ando and Zhang, 2005) 94.39
(Iwakura and Okamoto, 2008) 94.32
(Suzuki and Isozaki, 2008) 95.15
With additional resources
(Zhang et al, 2001) 94.17
(Daume? III and Marcu, 2005) 94.4
Without lexical resources
(Kudo and Matsumoto, 2001) 93.91
(Kudo et al, 2005) 94.12
(Tsuruoka and Tsujii, 2005) 93.70
(Tsuruoka et al, 2009) 93.68
This paper 94.15
7 Related Work
7.1 NP Chunking
Table 6 shows the previous best results for NP
Chunking. The F-measure of Semi-Boost is 94.60
that is 0.23 higher than that of (Sun et al, 2009)
and 0.38 higher than that of (Kudo and Mat-
sumoto, 2001).
7.2 Text Chunking
Table 7 shows the previous best results for Text
Chunking. We see that our method attained
a higher accuracy than the previous best re-
sults obtained without any additional lexical re-
sources such as chunking methods based on SVM
(Kudo and Matsumoto, 2001), CRF with rerank-
ing (Kudo et al, 2005), Maximum Entropy (Tsu-
ruoka and Tsujii, 2005), and CRF (Tsuruoka et al,
2009). This result indicates that our method per-
forms well in terms of accuracy.
The previous results with lexical resources or
semi-supervised ones showed higher accuracy
than that of our method. For example, lexical re-
sources such as lists of names, locations, abbrevi-
ations and stop words were used (Daume? III and
Marcu, 2005), and a full parser output was used
in (Zhang et al, 2001). Semi-supervised ones
used a generative model trained from automati-
cally labeled data (Suzuki and Isozaki, 2008), the
candidate tags of words collected from automati-
cally labeled data (Iwakura and Okamoto, 2008),
or automatically created classifiers by learning
from thousands of automatically generated aux-
iliary classification problems from unlabeled data
(Ando and Zhang, 2005). Our algorithm can also
incorporate the lexical resources and the semi-
supervised approaches. Future work should evalu-
ate the effectiveness of the incorporation of them.
7.3 Extended Named Entity Recognition
For ENER, the best result was the Semi-PER one
(Iwakura et al, 2011). The F-measure of Semi-
PER was 81.95, and the result was higher than NE
chunker based on structured perceptron (Collins,
2002), and NE chunkers based on shift-reduce-
parsers (Iwakura et al, 2011). Our method showed
0.15 higher F-measure than that of the Semi-PER
one. This result is also evidence that our method
performs well in terms of accuracy.
7.4 Training Methods
There have been methods proposed to improve the
training speed for semi-Markov-based learners.
With regard to reducing the space of lattices built
into the semi-Markov-based algorithms, a method
was proposed to filter nodes in the lattices with a
naive Bayes classifier (Okanohara et al, 2006). To
improve training speed of Semi-CRF, a succinct
representation of potentials common across over-
lapping segments of semi-Markov model was pro-
posed (Sarawagi, 2006). These methods can also
be applied to Semi-PER. Therefore, we can expect
improved training speed with these methods.
Recent online learners update both parameters
and the estimate of their confidence (Dredze and
Crammer, 2008; Crammer et al, 2009; Mejer
and Crammer, 2010; Wang et al, 2012). In
these algorithms, less confident parameters are up-
dated more aggressively than more confident ones.
These algorithms maintain the confidences of fea-
tures. In contrast, our boosting approach main-
tains the weights of training samples. In future
work, we?d like to consider the use of these algo-
rithms in boosting of semi-Markov learners.
8 Conclusion
This paper has proposed a boosting algorithm with
a semi-Markov perceptron. The experimental re-
sults on Noun Phrase Chunking, Text Chunking
and Japanese Extended Named Entity Recognition
have shown that our method achieved better accu-
racy than a semi-Markov perceptron and a semi-
Markov CRF. In future work, we?d like to evaluate
the boosting algorithm with structured prediction
tasks such as POS tagging and parsing.
54
References
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunking.
In Proc. of ACL?05, pages 1?9.
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?.
2002. Named entity extraction using adaboost. In
Proc. of CoNLL?02, pages 167?170.
William W. Cohen and Sunita Sarawagi. 2004. Ex-
ploiting dictionaries in named entity extraction:
combining semi-markov extraction processes and
data integration methods. In Proc. of KDD?04,
pages 89?98.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP?02, pages 1?8.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009. Adaptive regularization of weight vectors. In
Proc. of NIPS?09, pages 414?422.
Hal Daume? III and Daniel Marcu. 2005. Learn-
ing as search optimization: Approximate large mar-
gin methods for structured prediction. In Proc. of
ICML?05, pages 169?176.
Hal Daume? III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. In
Proc. of EMNLP?08, pages 689?697.
Yoav Freund and Robert E. Schapire. 1997. A
decision-theoretic generalization of on-line learning
and an application to boosting. J. Comput. Syst. Sci.,
55(1):119?139.
Taiichi Hashimoto, Takashi Inui, and Koji Murakami.
2008. Constructing extended named entity anno-
tated corpora. IPSJ SIG Notes, 2008(113):113?120.
Tomoya Iwakura and Seishi Okamoto. 2008. A fast
boosting-based learner for feature-rich tagging and
chunking. In Proc. of CoNLL?08, pages 17?24.
Tomoya Iwakura, Hiroya Takamura, and Manabu Oku-
mura. 2011. A named entity recognition method
based on decomposition and concatenation of word
chunks. In Proc. of IJCNLP?11, pages 828?836.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
Support Vector Machines. In Proc. of NAACL?01,
pages 192?199.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In Proc. of ACL?05, pages 189?196.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. of ICML?01, pages 282?289.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proc. of EMNLP?10, pages 971?981.
Masaaki Nagata. 1994. A stochastic japanese mor-
phological analyzer using a forward-dp backward-
a* n-best search algorithm. In Proc. of COLING?94,
pages 201?207.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In Proc. of
ACL?06, pages 465?472.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
markov conditional random fields for information
extraction. In Proc. of NIPS?04, pages 1185?1192.
Sunita Sarawagi. 2006. Efficient inference on se-
quence segmentation models. In Proc. of ICML?06,
pages 793?800.
Robert E. Schapire and Yoram Singer. 1999. Improved
boosting algorithms using confidence-rated predic-
tions. Machine Learning, 37(3):297?336.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proc. of
LREC?02.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. of NAACL
HLT?03, pages 134?141.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun?ichi Tsujii. 2009. Latent variable perceptron
algorithm for structured classification. In Proc. of
IJCAI?09, pages 1236?1242.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proc. of ACL-08:
HLT, pages 665?673.
Yoshimasa Tsuruoka and Junichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP,
pages 467?474.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proc. of ACL/IJCNLP, pages 477?485.
Jialei Wang, Peilin Zhao, and Steven C.H. Hoi. 2012.
Exact soft confidence-weighted learning. In Proc. of
ICML?12, pages 121?128.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Proc. of
ACL?01, pages 539?546.
55
