Proceedings of NAACL HLT 2007, pages 420?427,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Cascaded Machine Learning Approach
to Interpreting Temporal Expressions
David Ahn Joris van Rantwijk Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
{ahn, rantwijk, mdr}@science.uva.nl
Abstract
A new architecture for identifying and in-
terpreting temporal expressions is intro-
duced, in which the large set of com-
plex hand-crafted rules standard in sys-
tems for this task is replaced by a series
of machine learned classifiers and a much
smaller set of context-independent seman-
tic composition rules. Experiments with
the TERN 2004 data set demonstrate that
overall system performance is comparable
to the state-of-the-art, and that normaliza-
tion performance is particularly good.
1 Introduction
In order to fully understand a piece of text, we
must understand its temporal structure. The first
step toward such an understanding is identifying ex-
plicit references to time. We focus on the task of
automatically annotating temporal expressions (or
timexes)?both identifying them in text and inter-
preting them to determine what times they refer to.
Timex annotation is more than normalizing date ex-
pressions. First, time consists of more than calen-
dar dates and clock times?it also includes points of
finer and coarser granularity, durations, and sets of
times. Second, the expressions that refer to time are
not just full date and time expressions?they may be
underspecified, ambiguous, and anaphoric.
Building a system for the full timex identifica-
tion and interpretation task can be tedious, requiring
a great deal of manual effort. The 2004 Temporal
Expression Recognition and Normalization (TERN)
evaluation1 evaluated systems on two tasks: timex
1http://timex2.mitre.org/tern.html
recognition (identification) alone and recognition
and normalization (interpretation) together. All the
full-task systems were rule-based systems; the top
performing full-task system uses in excess of one
thousand hand-crafted rules, which probe words and
their contexts in order to both identify timexes and
to assemble information necessary to interpret them
(Negri and Marseglia, 2004). By contrast, machine
learned systems dominated the recognition-only task
and even achieved slightly better recognition scores
than their rule-based counterparts.
We seek to demonstrate that a timex annotation
system that performs both recognition and normal-
ization need not be a tangle of rules that serve dou-
ble duty for identification and interpretation and that
mix up context-dependent and context-independent
processing. We propose a novel architecture that
clearly separates syntactic, semantic, and prag-
matic processing and factors out context-dependent
from context-independent processing. Factoring
out context-dependent disambiguation into separate
classification tasks introduces the opportunity for
using machine learning, which supports our main
goal: building a portable, trainable timex annota-
tion system in which the role of hand-crafted rules
is minimized. The system we present here (avail-
able from http://ilps.science.uva.nl/
Resources/timextag/) achieves the goal of
making use of only a small set of hand-crafted,
context-independent rules to achieve state-of-the-art
normalization performance.
In the following section, we define what a timex
is. We give an overview of our system architecture
in ?3 and describe the components in ?4?7. ?8 pro-
vides an evaluation of our system on the full timex
annotation task, and we conclude in ?9.
420
2 What is a timex?
Temporal semantics receives a great deal of attention
in the semantics literature (cf. (Mani et al, 2005)),
but the focus is generally on verbal semantics (i.e.,
tense and aspect). In determining what a timex is
and how one should be normalized, we simply fol-
low the TIDES TIMEX2 standard for timex annota-
tion (Ferro et al, 2004). According to this standard,
timexes are phrases or words that refer to times,
where times may be points or durations, or sets of
points or durations. Points are more than just in-
stanteous moments in time?a point may also be a
time with some duration, as long as it spans a single
unit of some temporal granularity. Whether a timex
refers to a point or a duration is a question of per-
spective rather than of ontology. A point-referring
timex such as October 18, 2006 refers to an interval
of one day as an atom at the granularity of a day. A
duration-referring timex such as the whole day may
refer to the same temporal interval, but it focuses on
the durative nature of this interval.
In addition to specifying which phrases are
timexes, the TIMEX2 standard also provides a set
of attributes for normalizing these timexes. We fo-
cus on the VAL attribute, which takes values that are
an extension of the ISO-8601 standard for represent-
ing time (ISO, 1997). TIMEX2 VAL attributes can
take one of three basic types of values:
Points are expressed as a string matching the pat-
tern dddd-dd-ddTdd:dd:dd.d+, where d in-
dicates a digit. Such a string is to be interpreted as
year-month-dateThour:minute:seconds, and may be
truncated from the right, indicating points of coarser
granularity. Any place may be filled with a place-
holder X, which indicates an unknown or vague
value, and there are also a handful of token values
(character strings) for seasons and parts of the day
which may substitute for months and times. There is
also an alternate week-based format dddd-Wdd-d,
interpreted as year-Wweek number-day of the week.
Durations are expressed as a string matching the
pattern Pd+u or PTd+u, where d+ indicates one or
more digits and u indicates a unit token (such as Y
for years). A placeholder X may be used instead of
a number to indicate vagueness.
Vague points: past ref, present ref,
future ref.
Parsed
document
Phrase 
classifier
Semantic 
classifier
phrases
timexes
Direction 
classifier
pre-norm
points
Temporal 
anchoring
Final 
normalization
pre-norm 
points
w/dir & 
anchor
pre-norm
points w/dir
Timex-
annotated
document
normalized
points
Semantic composition rules
(class-specific)
d
u
r
a
t
i
o
n
s
g
e
n
p
o
i
n
t
s
r
e
c
u
r
s
normalized
non-points
p
o
i
n
t
s
A B
C
D
E
F
g
e
n
d
u
r
s
Figure 1: Timex annotation architecture (letters for
ease of reference).
The other attribute which we address in this paper
is the boolean-valued SET attribute; a SET timex
is one that refers to a recurring time. The remain-
ing attributes are MOD, ANCHOR VAL, and AN-
CHOR DIR; our system produces values for these
attributes, but we do not address them in this paper.
The TIMEX2 annotation standard has been used
to create several manually annotated corpora. For
the experiments we present in this paper, we use
the corpora annotated for the TERN 2004 evalua-
tion (Ferro, 2004). These consist of a training set
of 511 documents of newswire and broadcast news
transcripts, with 5326 TIMEX2s, and a test set of
192 similar documents, with 1828 TIMEX2s.
3 Architecture
The architecture of our timex annotation system is
depicted in Fig. 1. Our system begins with parsed
documents as input. Our recognition module is a
machine learned classifier (A); it is described in ?4.
Phrases that have been classified as timexes are
then sent to the semantic class classifier (B). Seman-
tic class disambiguation is the first point at which
context dependence enters into timex interpretation.
While some timexes are unambiguous with respect
to whether they refer to a point, a duration, or a
set, many timexes are semantically ambiguous and
can only be disambiguated in context. The machine
learned classifier for this task is described in ?5.
Based on the class assigned by the semantic class
421
classifier, the semantic composition component (C)
generates (underspecified) semantic representations
using class-specific, context-independent rules. The
rules we use are simple pattern-matching rules that
map lexical items or sequences of lexical items
within a timex to semantic representations. We de-
scribe the semantic composition component in ?6.
For most classes of timexes, the semantic compo-
sition component generates a semantic representa-
tion that can be directly translated into a normalized
value. Timexes that refer to specific points are the
only exception. While some point timexes are fully
qualified, and thus also directly normalizable, many
need to be anchored to another time in context in
order to be fully normalized. Thus, context depen-
dence again enters the timex interpretation process,
and now in two ways. One is obvious: these refer-
ential timexes, which need a temporal anchor, have
to find it in context. This task requires a reference
resolution process (E), which is described in ?7.1.
The second ambiguity regards the relation be-
tween a referential timex and its anchor. Referen-
tial timexes, like anaphoric definites, relate to their
anchors through a bridging relation, which is deter-
mined primarily by the content of the timex?e.g.,
two years later refers to a point two years after its
anchor. For some referential timexes, though, the
direction of the relation (before or after the anchor)
is not specified. The machine learned classifier (D)
resolves this ambiguity; see ?7.2.
For referential timexes, final normalization (F) is
a straightforward combination of semantic represen-
tation, temporal anchor, and direction class.
Not pictured in Fig. 1 is a module that recognizes
and normalizes timexes in document metadata using
a set of simple regular expressions (REs; 14 in total).
This module also determines the document time-
stamp for referential timexes by using a few heuris-
tics to choose from among multiple timestamps or a
date from the document text, if necessary.
While our architecture is novel, we are not the first
to modularize timex annotation systems. Even thor-
oughly rule-based systems (Negri and Marseglia,
2004; Saquete et al, 2002), separate temporal an-
chor tracking from the rest of the normalization pro-
cess. The system of Mani and Wilson (2000) goes
further in using separate sets of hand-crafted rules
for recognition and normalization and in separating
out several disambiguation tasks. Ahn et al (2005b)
decouple recognition from normalization?even us-
ing machine learning for recognition?and handle
several disambiguation tasks separately. In none of
these systems, though, are context-independent and
context-dependent processing thoroughly separated,
as here, and in all these systems, it is the rules that
drive the processing?in both Mani et al and Ahn
et al?s systems, sets of rules are used to determine
which timexes need to be disambiguated.
4 Component A: Recognizing timexes
Systems that perform both recognition and nor-
malization tend to take a rule-based approach to
recognition (Mani and Wilson, 2000; Saquete et
al., 2002; Schilder, 2004; Negri and Marseglia,
2004). Recognition-only systems are often based on
machine learned classifiers (Hacioglu et al, 2005;
Bethard and Martin, 2006), although some do use
finite-state methods (Boguraev and Ando, 2005).
Ahn et al (2005a) find a benefit to decoupling recog-
nition from normalization, and since our goal is
to build a modular, trainable system, we take a
machine-learning approach to recognition that is in-
dependent of our normalization components.
Generally, machine learned timex recognition
systems reduce the task of identifying a timex
phrase to one of classifying individual words by us-
ing (some variant of) B-I-O tagging, in which each
word is tagged as (B)eginning, (I)nside, or (O)utside
a timex phrase. Such a tagging scheme is not in-
herently sensitive to syntactic constituency and not
well-suited to identifying nested timexes (but cf.
(Hacioglu et al, 2005)). Considering that syntactic
parsers are readily available, we have explored sev-
eral ways of leveraging parse information in recog-
nition, although we describe here only the method
we use for experiments later in this paper.
We treat timex recognition as a binary phrase
classification task: syntactic constituents are clas-
sified as timexes or non-timexes. We restrict clas-
sification to the following phrase types and lexical
categories (based on (Ferro et al, 2004, ?5)): NP,
ADVP, ADJP, NN, NNP, JJ, CD, RB, and PP.2 In
order to identify candidate phrases and to extract
2We include PPs despite the TIDES guidelines, which ex-
plicitly exclude temporal PPs such as before Thursday because
of prepositional modifiers such as around and about.
422
Identification Exact match
prec rec F prec rec F
TEXT 0.912 0.786 0.844 0.850 0.732 0.787
DOC 0.929 0.813 0.867 0.878 0.769 0.819
BRO 0.973 0.891 0.930 0.905 0.829 0.865
BFT 0.976 0.880 0.926 0.885 0.798 0.839
Table 1: Recognition results: Identification.
parse-based features, we parse the TEXT elements
of our documents with the Charniak parser (Char-
niak, 2000). Because of both parser and annotator
errors, only 90.2% of the timexes in the training data
align exactly with a parse, which gives an estimated
upper-bound on recall using this method.
We use support vector machines for classification,
in particular, the LIBSVM linear kernel implemen-
tation (Chang and Lin, 2001). The features we ex-
tract include character type patterns, lexical features
such as weekday name and numeric year, a context
window of two words to the left, and several parse-
based features: the phrase type, the phrase head and
initial word (and POS tag), and the dependency par-
ent (and corresponding relation) of the head.
As with all our experiments in this paper, we
train on the TERN training corpus and test on the
test corpus. Our scores (precision, recall and F-
measure for both identification (i.e., overlap) and
exact-match) are given in Table 1, along with the
scores of the best recognition-only (BRO) and full-
task (BFT) TERN 2004 systems. Since our phrase
classification method is only applied within docu-
ment TEXT elements, we also present results using
both our RE-based document metadata tagger and
our phrase classifier for full documents (DOC). Only
these scores can be compared with the TERN scores.
Our scores using this method approach those of
the best systems, but there is still a gap, which, as
we see in ?8, affects our overall task performance.
5 Component B: Semantic classification
Timexes may refer to points, durations, or recur-
rences. While some timexes refer unambiguously to
one of these, many timexes are ambiguous between
two or even three of these (see (Hitzeman, 1993) for
a theoretical semantic perspective on this ambigu-
ity). Timexes may also refer generically or vaguely,
which is another source of ambiguity.
While the TIMEX2 standard does not explicitly
specify semantic classes in its annotations, the se-
mantic classes we distinguish for our normalization
system can be easily inferred from the form of the
values of the attributes that are annotated, as follows:
Recurrence (recur): SET attribute set to true
Generic or vague duration (gendur): VAL begins
with PX or PTX
Duration: VAL begins with P[0-9] or PT[0-9]
Generic or vague point (genpoint): Three possi-
bilities: time-of-day w/o associated date expression
(VAL begins with T[0-9]); general reference to past,
present, or future (VAL is one of the vague tokens);
date expression with unspecified high-order position
(i.e., millennium position is X)
Point: Date expression with specified high-order
position (may be precise or not?i.e., may include X
at other positions?also may be of any granularity,
from millennium down to hundredths of a second).
Resolving semantic class ambiguities is a context-
dependent task that can be easily factored out of se-
mantic interpretation, reducing the burden on the se-
mantic interpretation rules. The classification task is
straightforward: each timex must be classified into
one of the five classes described above or into the
null class (for timexes that have no VAL). Since the
TERN data is not explicitly annotated for semantic
class, we use the class definitions above to derive the
semantic class of a timex from its VAL attribute.
We again use the LIBSVM linear kernel for clas-
sification, with the same features as for recogni-
tion. Even though some timexes are unambiguous
with respect to semantic class, we train the classi-
fier over all timexes, in the expectation that the con-
texts of unambiguous timexes will be similar enough
to those of ambiguous timexes of the same class to
help in classification. We compare the performance
of our machine learned classifier to a heuristic base-
line classifier that uses the head of the timex and the
presence of numbers, names, and certain modifiers
within the timex to decide how to classify it.
Table 2 gives the error rates, per class and overall,
for the baseline and learned classifiers over phrase-
aligned gold-standard timexes. The machine learned
classifier halves the error rate of the baseline, mostly
as a result of better performance on the duration and
point classes. In ?8, we see how this improvement
in classification affects end-to-end performance.
Mani and Wilson (2000) and Ahn et al (2005b)
423
classifier overall null duration . . .
BL 0.2085 1.0000 0.2534 . . .
SVM 0.1078 0.4143 0.1507 . . .
class dist 1290 70 146 . . .
. . . gendur genpoint point recur
. . . 0.0204 0.1462 0.1322 0.6087
. . . 0.1020 0.1462 0.0496 0.2174
. . . 49 253 726 46
Table 2: Error rates: semantic class.
also perform limited semantic class disambiguation.
Both use machine learned classifiers to distinguish
specific and generic uses of today, and Ahn et al
also use a machine learned classifier to disambiguate
timexes between a point and a duration reading.
Their error rate for this task is 27%, but since a set
of heuristics is first used to select just ambiguous
timexes, this score cannot be compared to ours.
6 Component C: Semantic composition
The semantic composition module uses context-
independent, class-specific rules to compute for each
timex an underspecified representation?a typed
feature structure that depends on the timex?s seman-
tic class (features include unit and value for dura-
tions, year, month, date, and referential class for
points; cf. (Dale and Mazur, 2006)). As the rules are
not responsible for identification or class or direc-
tion disambiguation, they are fewer in number and
simpler than in other systems (cf. 1000+ in (Negri
and Marseglia, 2004)). Each rule consists of an RE-
pattern, which may refer to a small lexicon of names,
units, and numeric words, and is applied using a cus-
tom transducer. In total, there are 89 rules; Table 3
gives the distribution of rules and an example rule
for each class. Tokens in ALLCAPS indicate lexical
classes; tokens in MixedCase indicate other rules;
and tokens in lowercase indicate lexical items.
7 Temporal anchors
Some point timexes are fully qualified, while others
require a reference time, or temporal anchor, to be
fully normalized.3 There are three ways in which
a temporal anchor is chosen for a timex. Some
timexes, such as today, three years ago, and next
week, are deictic and anchored to the time of speech
3Our use of the term temporal anchor is distinct from the
ANCHOR VAL and ANCHOR DIR attributes.
class rules example
dur 13 Numeric -? (UNIT | UNITS)
gendur 3 (UNIT | UNITS)
genpt 21 (NUM24 | NUMWORD) o ? clock
point 31 ? Approx? DAYNAME? MONTHNAME
.? Num31OrRank ,? YearNum
recur 11 (every | per) Numeric UNITS
misc 10 NUMWORD ((and | -)? NUMWORD)*
Table 3: Distribution of semantic composition rules.
(for us, the document timestamp). Others, such as
two months earlier and the next week, are anaphoric
and anchored to a salient time in discourse, just like
an anaphoric pronoun or definite. The distinction
between deictic and anaphoric timexes is not always
clear-cut, since many anaphoric timexes, in the ab-
sence of an appropriate antecedent, are anchored de-
ictically. A timex may also contain its own anchor:
e.g., two days after May 3, whose anchor is the em-
bedded anaphoric timex May 3.
Once a referential timex?s temporal anchor has
been determined, the value of the anchor must be
combined with the timex, which may be either an
offset or a name-like timex. Offsets, such as two
months earlier, provide a unit u, a magnitude m,
and optionally, a direction (before or after); the value
of an offset is the point (of granularity u) that is m
u units from its anchor in the indicated direction.
Name-like timexes provide a position in a cycle,
such as a day name within a week, and optionally,
a direction. The value of a name-like timex is the
time point bearing the name within the correspond-
ing cycle of its anchor (or the immediately preceding
or succeeding cycle, depending on the direction).
For both offsets and name-like timexes, the direc-
tion indication is optional. When no direction in-
dication is given, the appropriate direction must be
determined from context, as in this initial sentence
from an article from 1998-11-28:
(1) A fundamentalist Muslim lawmaker has vowed
to stop a shopping festival planned in February,
a newspaper reported Saturday.
The first timex, February, clearly refers to the Febru-
ary following its anchor (the timestamp), while the
second timex, Saturday, seems to refer to a point
preceding its anchor (also the timestamp).
The next two sections describe our methods for
temporal anchoring and direction classification.
424
7.1 Component E: Temporal anchor tracking
Since temporal anchors are not annotated in the
TIMEX2 standard, our system uses a simple heuris-
tic method for temporal anchoring (cf. (Wiebe et al,
1997), who use a more complex rule-based system
for timex anchoring in scheduling dialogues). Since
we distinguish deictic and anaphoric timexes during
semantic composition, we use a combination of two
methods: for deictic timexes, the document time-
stamp is used, and for (some) anaphoric timexes, the
most recent point timex, if it is fine-grained enough,
is used as the temporal anchor (otherwise, the docu-
ment timestamp is used). Because the documents in
our corpora are short news texts, we actually treat
anaphoric name-like points as deictic and use the
most recent timex only for anaphoric offsets.
7.2 Component D: Direction classification
The idea of separating direction classification from
the remainder of the normalization task is not new.
(Mani and Wilson, 2000) use a heuristic method
for this task, while (Ahn et al, 2005b) use a ma-
chine learned classifier. In contrast to Ahn et al,
who use a set of heuristics to identify ambiguous
timexes and train and test only on those, we train
our classifier on all point and genpoint timexes and
apply it to all point timexes. Genpoint timexes and
many point timexes are not ambiguous w.r.t. direc-
tion, but we expect that the contexts of unambiguous
timexes will be similar enough to those of ambigu-
ous timexes of the same class to help classification.
Direction class is not annotated as part of the
TIMEX2 standard. Given a temporal anchor track-
ing method, though, it is possible to derive imperfect
direction class information from the VAL attribute.
We use our anchor tracking method to associate each
point and genpoint timex with an anchor and then
compare the VAL of the timex with that of its an-
chor to decide what its direction class should be.
We again use the LIBSVM linear kernel for clas-
sification. We add two sets of features to those used
for recognition and semantic classification. The first
is inspired by Mani et al, who rely on the tense of
neighboring verbs to decide direction class. Since
verb tense alone is inherently deictic, it is not suffi-
cient to decide the direction, but we do add both the
closest verb (w.r.t. dependency paths) and its POS
classifier overall after before same
BL 0.1749 0.4587 0.0802 0.1934
SVM 0.2245 0.4404 0.1578 0.2305
SVM VERB 0.2094 0.3119 0.1631 0.2346
SVM ALL 0.1185 0.2110 0.0989 0.1070
class dist 726 109 374 243
Table 4: Error rates: direction class.
tag (as well as any verbs directly related to this verb)
as features. The second set of features compares day
names, month names, and years to the document
timestamp. The comparison determines whether,
within a single cycle of the appropriate granularity
(week for day-names and year for month-names),
the point named by the timex would be before, after,
or the same as the point referred to by the timestamp.
We compare our learned classifier with a heuristic
baseline classifier which first checks for the presence
of a year or certain modifiers such as ago or next in
the timex; if that fails, it computes the date features
described above for each word in the timex and re-
turns same if any word compares to the timestamp
as same; if that fails, it uses the tense of the nearest
verb; and finally, it defaults to same.
Table 4 shows the results of applying our clas-
sifiers to all phrase-aligned gold-standard point
timexes. BL is the baseline; SVM, SVM VERB, and
SVM ALL are the classifiers learned using our basic
feature set, the basic feature set plus the verb fea-
tures, and all the features, respectively. The learned
classifier using all the features reduces the error rate
of the baseline classifier by about a third. Note,
though, that the learned classifiers without the date
comparison features (SVM and SVM VERB) perform
substantially worse than even the baseline. One rea-
son for this becomes clear from Table 5, which gives
the error rates for the classifiers restricted to timexes
consisting solely of a month or a day name. Unlike
points in general, these timexes are all ambiguous
with respect to direction and are, in fact, the primary
motivation for both Mani et al and Ahn et al to con-
sider direction classification as a separate task.
These results demonstrate that the date compari-
son feature is responsible for a substantial reduction
in error rate (over 85% from SVM to SVM ALL) and
that for the same class, performance is perfect. This
is largely due to the writing style of the documents,
in which the current day is often referred to by name
425
classifier overall after before same
BL 0.1000 0.4348 0.1061 0.0000
SVM 0.3647 0.6087 0.3485 0.3086
SVM VERB 0.3176 0.3478 0.3485 0.2840
SVM ALL 0.0529 0.1304 0.0909 0.0000
class dist 170 23 66 81
Table 5: Error rates: direction month/day.
instead of as today, as in example (1).
Although both Mani et al and Ahn et al build
direction classifiers, neither provide comparable re-
sults. Mani et al do not evaluate their direction
heuristics at all, and Ahn et al train and test their
machine learned classifier only on timexes deter-
mined to be ambiguous by their heuristics. In any
case, their error rate is significantly higher, at 38%.
8 End-to-end performance
We now consider the performance of the entire sys-
tem and the contributions of the components. First,
though, we discuss our evaluation metrics.
8.1 Scoring
The official TERN scoring script computes precision
and recall for VAL only with respect to correctly rec-
ognized TIMEX2s with a non-null VAL. While this
may be useful in determining how far behind nor-
malization is from recognition for a given system, it
does not provide an accurate picture of end-to-end
system performance, since the recall base does not
include all possible timexes and the precision base
does not include incorrectly recognized timexes.
The scoring script provides several raw counts
that can be used to compute measures that are more
indicative of end-to-end performance: actTIMEX2
(# of actually recognized TIMEX2s); corrTIMEX2
(# of correctly recognized TIMEX2s); posVAL
(# of correctly recognized TIMEX2s with a non-
null gold VAL); corrVAL (# of correctly recog-
nized TIMEX2s with a non-null gold VAL for
which the system assigns the correct VAL); and
spurVAL (# of correctly recognized TIMEX2s with
null gold VAL for which the system assigns a
VAL). With these counts, we can define corrNOVAL
(# of correctly recognized TIMEX2s with a null
gold VAL for which the system assigns a null
VAL), as corrTIMEX2 ? posVAL ? spurVAL. We
then define end-to-end precision (absP) and recall
(absR) as (corrVAL + corrNOVAL)/actTIMEX2
and (corrVAL+corrNOVAL)/possTIMEX2, respec-
tively. Official precision and recall for VAL are com-
puted as corrVAL/actVAL and corrVAL/possVAL.
8.2 Results
Our first set of results (Table 6(Top)), which are
restricted to timexes in document TEXT elements,
compares our system (LLL) to a version of our sys-
tem (BL) that uses the baseline classifiers for seman-
tic and direction class. It also presents a series of or-
acle results that demonstrate the effect of swapping
in perfect classification for each of the learned clas-
sifiers. The oracle runs are labeled with a three-letter
code in which the first letter ((P)erfect or (L)earned)
refers to phrase classification; the second, to seman-
tic classification; and the third, to direction classi-
fication. Note: perfect phrase classification is not
the same as perfect recognition, since it excludes
timexes that fail to align with parsed phrases.
Using the learned classifiers (LLL), which reduce
error rates by about one-half for semantic class and
one-third for direction class over the baseline clas-
sifiers, results in a five-point improvement in abso-
lute F-measure over the baseline system (BL). We
also see from runs LLP, LPL, and LPP that further
improvement of these classifiers would substantially
improve end-to-end performance. Finally, we see
from runs PLL and PPP that recognition performance
is a major limiting factor in our end-to-end scores.
In Table 6(Bottom), we present results over full
documents, including metadata and text. LLL and
PLL are the same as before; ITC-IRST is the sys-
tem of (Negri and Marseglia, 2004), which achieved
the highest official F-measure in the TERN 2004
evaluation. The results of our system (LLL) are
comparable to those of ITC-irst: because we recog-
nize fewer timexes, our official F-measure is higher
(0.899 vs. 0.872) while our absolute F-measure is
lower (0.769 vs. 0.806). We see from run PLL that
our recognition module is largely to blame?with
perfect phrase classification for recognition, our nor-
malization modules produce substantially better re-
sults. With a system such as ITC-irst?s, it is not pos-
sible to separate recognition performance from nor-
malization performance, since there is a single rule
base that jointly performs the two tasks?all normal-
izable timexes are presumably already recognized.
426
System corrVAL corrNOVAL actTIMEX2 P R F absP absR absF
BL 859 32 1245 0.813 0.787 0.800 0.716 0.624 0.667
LLL 931 33 1245 0.882 0.853 0.867 0.774 0.676 0.722
LLP 938 33 1245 0.912 0.859 0.885 0.780 0.680 0.727
LPL 951 39 1245 0.916 0.871 0.893 0.795 0.694 0.741
LPP 987 39 1245 0.951 0.904 0.927 0.824 0.719 0.768
PLL 1008 63 1287 0.886 0.828 0.856 0.832 0.751 0.789
PPP 1097 70 1287 0.966 0.901 0.932 0.907 0.818 0.860
LLL 1285 33 1601 0.910 0.887 0.899 0.823 0.721 0.769
PLL 1362 63 1643 0.912 0.866 0.888 0.867 0.780 0.821
ITC-IRST 1365 35 1648 0.875 0.870 0.872 0.850 0.766 0.806
Table 6: Performance on VAL. (Top): TEXT-only. (Bottom): full document.
9 Conclusion
We have described a novel architecture for a timex
annotation system that eschews the complex set
of hand-crafted rules that is a hallmark of other
systems. Instead, we decouple recognition from
normalization and factor out context-dependent se-
mantic and pragmatic processing from context-
independent semantic composition. Our architec-
ture allows us to use machine learned classifiers to
make context-dependent disambiguation decisions,
which in turn allows us to use a small set of sim-
ple, context-independent rules for semantic compo-
sition. The normalization performance of this sys-
tem is competitive with the state of the art and our
overall performance is limited primarily by recog-
nition performance. Improvement in semantic and
direction classification will yield further improve-
ments in overall performance. Our other plans for
the future include experimenting with dependency
relations for semantic composition instead of lexi-
cal patterns, evaluating our temporal anchor tracking
method, and training the full system on other cor-
pora and adapting it for other languages.
Acknowledgement This research was supported
by the Netherlands Organization for Scientific Re-
search (NWO) under project numbers 017.001.190,
220-80-001, 264-70-050, 354-20-005, 600.065.120,
612-13- 001, 612.000.106, 612.066.302,
612.069.006, 640.001.501, 640.002.501, and
by the E.U. IST programme of the 6th FP for RTD
under project MultiMATCH contract IST-033104.
References
D. Ahn, S. Fissaha Adafre, and M. de Rijke. 2005a. Extracting
temporal information from open domain text: A comparative
exploration. In R. van Zwol, editor, Proc. DIR?05.
D. Ahn, S. Fissaha Adafre, and M. de Rijke. 2005b. Recog-
nizing and interpreting temporal expressions in open domain
texts. In S. Artemov et al, editors, We Will Show Them:
Essays in Honour of Dov Gabbay, Vol 1, pages 31?50.
S. Bethard and J.H. Martin. 2006. Identification of event men-
tions and their semantic class. In Proc. EMNLP 2006.
B. Boguraev and R. Kubota Ando. 2005. TimeML-compliant
text analysis for temporal reasoning. In Proc. IJCAI-05.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL 2000, pages 132?139.
R. Dale and P. Mazur. 2006. Local semantics in the interpre-
tation of temporal expressions. In Proc. Workshop on Anno-
tating and Reasoning about Time and Events, pages 9?16.
L. Ferro, L. Gerber, I. Mani, and G. Wilson, 2004. TIDES 2003
Std. for the Annotation of Temporal Expressions. MITRE.
L. Ferro. 2004. Annotating the TERN corpus.
http://timex2.mitre.org/tern 2004/
ferro2 TERN2004 annotation sanitized.pdf.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Automatic time
expression labeling for English and Chinese text. In A.F.
Gelbukh, editor, CICLing, volume 3406 of Lecture Notes in
Computer Science, pages 548?559.
J. Hitzeman. 1993. Temporal Adverbials and the Syntax-
Semantics Interface. Ph.D. thesis, University of Rochester.
ISO. 1997. ISO 8601: Information interchange ? representa-
tion of dates and times.
I. Mani and G. Wilson. 2000. Robust temporal processing of
news. In Proc. ACL?2000, pages 69?76.
I. Mani, J. Pustejovsky, and R. Gaizauskas, editors. 2005. The
Language of Time: A Reader. Oxford University Press.
M. Negri and L. Marseglia. 2004. Recognition and normaliza-
tion of time expressions: ITC-irst at TERN 2004. Technical
report, ITC-irst, Trento.
E. Saquete, P. Mart??nez-Barco, and R. Mun?oz. 2002. Recogniz-
ing and tagging temporal expressions in Spanish. In Work-
shop on Annotation Standards for Temporal Information in
Natural Language, LREC 2002, pages 44?51.
F. Schilder. 2004. Extracting meaning from temporal nouns
and temporal prepositions. ACM TALIP.
J. Wiebe, T. O?Hara, K. McKeever, and T. O?hrstro?m Sandgren.
1997. An empirical approach to temporal reference resolu-
tion. In Proceedings of EMNLP-97, pages 174?186.
427
The University of Amsterdam at Senseval-3:
Semantic Roles and Logic Forms
David Ahn Sisay Fissaha Valentin Jijkoun Maarten de Rijke
Informatics Institute, University of Amsterdam
Kruislaan 403
1098 SJ Amsterdam
The Netherlands
{ahn,sfissaha,jijkoun,mdr}@science.uva.nl
Abstract
We describe our participation in two of the tasks or-
ganized within Senseval-3: Automatic Labeling of
Semantic Roles and Identification of Logic Forms
in English.
1 Introduction
This year (2004), Senseval, a well-established fo-
rum for the evaluation and comparison of word
sense disambiguation (WSD) systems, introduced
two tasks aimed at building semantic representa-
tions of natural language sentences. One task, Auto-
matic Labeling of Semantic Roles (SR), takes as its
theoretical foundation Frame Semantics (Fillmore,
1977) and uses FrameNet (Johnson et al, 2003) as
a data resource for evaluation and system develop-
ment. The definition of the task is simple: given
a natural language sentence and a target word in
the sentence, find other fragments (continuous word
sequences) of the sentence that correspond to ele-
ments of the semantic frame, that is, that serve as
arguments of the predicate introduced by the target
word.
For this task, the systems receive a sentence, a
target word, and a semantic frame (one target word
may belong to multiple frames; hence, for real-
world applications, a preliminary WSD step might
be needed to select an appropriate frame). The out-
put of a system is a list of frame elements, with their
names and character positions in the sentence. The
evaluation of the SR task is based on precision and
recall. For this year?s task, the organizers chose 40
frames from FrameNet 1.1, with 32,560 annnotated
sentences, 8,002 of which formed the test set.
The second task, Identification of Logic Forms
in English (LF), is based on the LF formalism de-
scribed in (Rus, 2002). The LF formalism is a sim-
ple logical form language for natural language se-
mantics with only predicates and variables; there
is no quantification or negation, and atomic predi-
cations are implicitly conjoined. Predicates corre-
spond directly to words and are composed of the
base form of the word, the part of speech tag, and a
sense number (corresponding to the WordNet sense
of the word as used). For the task, the system is
given sentences and must produce LFs. Word sense
disambiguation is not part of the task, so the pred-
icates need not specify WordNet senses. System
evaluation is based on precision and recall of pred-
icates and predicates together with all their argu-
ments as compared to a gold standard.
2 Syntactic Processing
For both tasks, SR and LF, the core of our systems
was the syntactic analysis module described in de-
tail in (Jijkoun and de Rijke, 2004). We only have
space here to give a short overview of the module.
Every sentence was part-of-speech tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
parsed using a state-of-the-art wide coverage phrase
structure parser (Collins, 1999). Both the tagger and
the parser are trained on the Penn Treebank Wall
Street Journal Corpus (WSJ in the rest of this paper)
and thus produce structures similar to those in the
Penn Treebank. Unfortunately, the parser does not
deliver some of the information available in WSJ
that is potentially useful for our two applications:
Penn functional tags (e.g., subject, temporal, closely
related, logical subject in passive) and non-local de-
pendencies (e.g., subject and object control, argu-
ment extraction in relative clauses). Our syntactic
module tries to compensate for this and make this
information explicit in the resulting syntactic analy-
ses.
As a first step, we converted phrase trees pro-
duced by the parser to dependency structures, by
detecting heads of constituents and then propagat-
ing the lexical head information up the syntactic
tree, similarly to (Collins, 1999). The resulting de-
pendency structures were labeled with dependency
labels derived from corresponding Penn phrase la-
bels: e.g., a verb phrase (VP) modified by a prepo-
sitional phrase (PP) resulted in a dependency with
label ?VP|PP?.
Then, the information available in the WSJ (func-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
VP
to seek NP
seats
VP
planned
S
directors
this month
      NP
     NP  S
planned
directors
VP|S
S|NP
S|NP
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
planned
directors
VP|SS|NP?SBJ
S|NP?TMP
S|NP?SBJ
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
(a) (b) (c)
Figure 1: Stages of the syntactic processing: (a) the parser?s output, (b) the result of conversion to a depen-
dency structure, (c) final output of our syntactic module
tional tags, non-local dependencies) was added to
dependency structures using Memory-Based Learn-
ing (Daelemans et al, 2003): we trained the learner
to change dependency labels, or add new nodes or
arcs to dependency structures. Trained and tested
on WSJ, our system achieves state-of-the-art perfor-
mance for recovery of Penn functional tags and non-
local dependencies (Jijkoun and de Rijke, 2004).
Figure 1 shows three stages of the syntactic anal-
ysis of the sentence Directors this month planned to
seek seats (a simplified actual sentence from WSJ):
(a) the phrase structure tree produced by Collins?
parser, (b) the phrase structure tree converted to a
dependency structure and (c) the transformed de-
pendency structure with added functional tags and a
non-local dependency?the final output of our syn-
tactic module. Dependencies are shown as arcs
from heads to dependents.
3 Automatic Labeling of Semantic Roles
For the SR task, we applied a method very similar to
the one used in (Jijkoun and de Rijke, 2004) for re-
covering syntactic structures and somewhat similar
to the first method for automatic semantic role iden-
tification described in (Gildea and Jurafsky, 2002).
Essentially, our method consists of extracting possi-
ble syntactic patterns (paths in syntactic dependency
structures), introducing semantic relations from a
training corpus, and then using a machine learn-
ing classifier to predict which syntactic paths cor-
respond to which frame elements.
Our main assumption was that frame elements,
as annotated in FrameNet, correspond directly to
constituents (constituents being complete subtrees
of dependency structures). Similarly to (Gildea and
Jurafsky, 2002), our own evaluation showed that
about 15% of frame elements in FrameNet 1.1 do
not correspond to constituents, even when applying
some straighforward heuristics (see below) to com-
pensate for this mismatch. This observation puts an
upper bound of around 85% on the accuracy of our
system (with strict evaluation, i.e., if frame element
boundaries must match the gold standard exactly).
Note, though, that these 15% of ?erroneous? con-
stituents also include parsing errors.
Since the core of our SR system operates on
words, constituents, and dependencies, two im-
portant steps are the conversion of FrameNet el-
ements (continuous sequences of characters) into
head words of constituents, and vice versa. The con-
version of FrameNet elements is straightforward:
we take the head of a frame element to be the word
that dominates the most words of this element in
the dependency graph of the sentence. In the other
direction, when converting a subgraph of a depen-
dency graph dominated by a word w into a contin-
uous sequence of words, we take all (i.e., not only
immediate) dependents of w, ignoring non-local de-
pendencies, unless w is the target word of the sen-
tence, in which case we take the word w alone. This
latter heuristic helps us to handle cases when a noun
target word is a semantic argument of itself. Sev-
eral other simple heristics were also found helpful:
e.g., if the result of the conversion of a constituent to
a word sequence contains the target word, we take
only the words to the right of the target word.
With this conversion between frame elements and
constituents, the rest of our system only needs to
operate on words and labeled dependencies.
3.1 Training: the major steps
First, we extract from the training corpus
(dependency-parsed FrameNet sentences, with
words marked as targets and frame elements) all
shortest undirected paths in dependency graphs that
connect target words with their semantic arguments.
In this way, we collect all ?interesting? syntactic
paths from the training corpus.
In the second step, for all extracted syntactic
paths and again for all training sentences, we extract
all occurences of the paths (i.e., paths, starting from
a target word, that actually exist in the dependency
graph), recording for each such occurrence whether
it connects a target word to one of its semantic ar-
guments. For performance reasons, we consider for
each target word only syntactic paths extracted from
sentences annotated with respect to the same frame,
and we ignore all paths of length more than 3.
For every extracted occurrence, we record the
features describing the occurrence of a path in more
detail: the frame name, the path itself, the words
along the path (including the target word and the
possible head of a frame element?first and last
node of the path, respectively), their POS tags and
semantic classes. For nouns, the semantic class
of a word is defined as the hypernym of the first
sense of the noun in WordNet, one of 19 manu-
ally selected terms (animal, person, social group,
clothes, feeling, property, phenomenon, etc.) For
lexical adverbs and prepositions, the semantic class
is one of the 6 clusters obtained automatically using
the K-mean clustering algorithm on data extracted
from FrameNet. Examples of the clusters are:
(abruptly, ironically, slowly, . . . ), (above, beneath,
inside, . . . ), (entirely, enough, better, . . . ). The list
of WordNet hypernyms and the number of clusters
were chosen experimentally. We also added features
describing the subcategorization frame of the tar-
get word; this information is straightforwardly ex-
tracted from the dependency graph. In total, the sys-
tem used 22 features.
The set of path occurrences obtained in the sec-
ond step, with all the extracted features, is a pool of
positive and negative examples of whether certain
syntactic patterns correspond to any semantic argu-
ments. The pool is used as an instance base to train
TiMBL, a memory-based learner (Daelemans et al,
2003), to predict whether the endpoint of a syntac-
tic path starting at a target word corresponds to a
semantic argument, and if so, what its name is.
We chose TiMBL for this task because we had
previously found that it deals successfully with
complex feature spaces and data sparseness (in our
case, in the presence of many lexical features) (Ji-
jkoun and de Rijke, 2004). Moreover, TiMBL is
very flexible and implements many variants of the
basic k-nearest neighbor algorithm. We found that
tuning various parameters (the number of neigh-
bors, weighting and voting schemes) made substan-
tial differences in the performance of our system.
3.2 Applying the system
Once the training is complete, the system can be
applied to new sentences (with the indicated target
word and its frame) as follows. A sentence is parsed
and its dependency structure is built, as described in
Section 2. All occurences of ?interesting? syntac-
tic paths are extracted, along with their features as
described above. The resulting feature vectors are
fed to TiMBL to determine whether the endpoints
of the syntactic paths correspond to semantic argu-
ments of the target word. For the path occurences
classified positively, the constituents of their end-
points are converted to continuous word sequences,
as described earlier; in this case the system has de-
tected a frame element.
3.3 Results
During the development of our system, we used
only the 24,558 sentences from FrameNet set aside
for training by the SR task organizers. To tune the
system, this corpus was randomly split into training
and development sets (70% and 30%, resp.), evenly
for all target words. The official test set (8002 sen-
tences) was used only once to produce the submitted
run, with the whole training set (24,558 sentences)
used for training.
We submitted one run of the system (with iden-
tification of both element boundaries and element
names). Our official scores are: precision 86.9%,
recall 75.2% and overlap 84.7%. Our own evalua-
tion of the submitted run with the strict measures,
i.e., an element is considered correct only if both its
name and boundaries match the gold standard, gave
precision 73.5% and recall 63.6%.
4 Logic Forms
4.1 Method
For the LF task, it was straightforward to turn de-
pendency structures into LFs. Since the LF for-
malism does not attempt to represent the more sub-
tle aspects of semantics, such as quantification, in-
tensionality, modality, or temporality (Rus, 2002),
the primary information encoded in a LF is based
on argument structure, which is already well cap-
tured by the dependency parses. Our LF genera-
tor traverses the dependency structure, turning POS-
tagged lexical items into LF predicates, creating ref-
erential variables for nouns and verbs, and using
dependency labels to order the arguments for each
predicate. We make one change to the dependency
graphs originally produced by the parser. Instead of
taking coordinators, such as and, to modify the con-
stituents they coordinate, we take the coordinated
constituents to be arguments of the coordinator.
Our LF generator builds a labeled directed graph
from a dependency structure and traverses this
graph depth-first. In general, a well-formed depen-
dency graph has exactly one root node, which cor-
responds to the main verb of the sentence. Sen-
tences with multiple independent clauses may have
one root per clause. The generator begins traversing
the graph at one of these root nodes; if there is more
than one, it completes traversal of the subgraph con-
nected to the first node before going on to the next
node.
The first step in processing a node?producing an
LF predicate from the node?s lexical item?is taken
care of in the graph-building stage. We use a base
form dictionary to get the base form of the lexical
item and a simple mapping of Penn Treebank tags
into ?n?, ?v?, ?a?, and ?r? to get the suffix. For words
that are not tagged as nouns, verbs, adjectives, or
adverbs, the LF predicate is simply the word itself.
As the graph is traversed, the processing of a node
depends on its type. The greatest amount of pro-
cessing is required for a node corresponding to a
verb. First, a fresh referential variable is generated
as the event argument of the verbal predication. The
out-edges are then searched for nodes to process.
Since the order of arguments in an LF predication
is important and some sentence constitutents are ig-
nored for the purposes of LF, the out-edges are cho-
sen in order by label: first particles (?VP|PRT?),
then arguments (?S|NP-SBJ?, ?VP|NP?, etc.), and
finally adjuncts. We attempt to follow the argu-
ment order implicit in the description of LF given
in (Rus, 2002), and as the formalism requires, we
ignore auxiliary verbs and negation. The processing
of each of these arguments or adjuncts is handled re-
cursively and returns a set of predications. For mod-
ifiers, the event variable also has to be passed down.
For referential arguments and adjuncts, a referen-
tial variable also is returned to serve as an argument
for the verb?s LF predicate. Once all the arguments
and adjuncts have been processed, a new predica-
tion is generated, in which the verb?s LF predicate
is applied to the event variable and the recursively
generated referential variables. This new predica-
tion, along with the recursively generated ones, is
returned.
The processing of a nominal node proceeds sim-
ilarly. A fresh referential variable is generated?
since determiners are ignored in the LF formalism,
it is simply assumed that all noun phrases corre-
spond to a (possibly composite) individual. Out-
edges are examined for modifiers and recursively
processed. Both the referential variable and the set
of new predications are returned. Noun compounds
introduce some additional complexity; each modi-
fying noun introduces two additional variables, one
for the modifying noun and one for composite indi-
vidual realizing the compound. This latter variable
then replaces the referential variable for the head
noun.
Processing of other types of nodes proceeds in a
similar fashion. For modifiers such as adjectives,
adverbs, and prepositional phrases, a variable (cor-
responding to the individual or event being modi-
fied) is passed in, and the LF predicate of the node
is applied to this variable, rather than to a fresh
variable. In the case of prepositional phrases, the
predicate is applied to this variable and to the vari-
able corresponding to the object of the preposition,
which must be processed, as well. The latter vari-
able is then returned along with the new predica-
tions. For other modifiers, just the predications are
returned.
4.2 Development and results
The rules for handling dependency labels were writ-
ten by hand. Of the roughly 1100 dependency la-
bels that the parser assigns (see Section 2), our sys-
tem handles 45 labels, all of which fall within the
most frequent 135 labels. About 50 of these 135
labels are dependencies that can be ignored in the
generation of LFs (labels involving punctuation, de-
terminers, auxiliary verbs, etc.); of the remaining
85 labels, the 45 labels handled were chosen to pro-
vide reasonable coverage over the sample corpus
provided by the task organizers. Extending the sys-
tem is straightforward; to handle a dependency label
linking two node types, a rule matching the label
and invoking the dependent node handler is added
to the head node handler.
On the sample corpus of 50 sentences to which
our system was tuned, predicate identification, com-
pared to the provided LFs, including POS-tags, was
performed with 89.1% precision and 87.1% recall.
Argument identification was performed with 78.9%
precision and 77.4% recall. On the test corpus of
300 sentences, our official results, which exclude
POS-tags, were 82.0% precision and 78.4% recall
for predicate identification and 73.0% precision and
69.1% recall for argument identification.
We did not get the gold standard for the test cor-
pus in time to perform error analysis for our official
submission, but we did examine the errors in the
LFs we generated for the trial corpus. Most could
be traced to errors in the dependency parses, which
is unsurprising, since the generation of LFs from de-
pendency parses is relatively straightforward. A few
errors resulted from the fact that our system does not
try to identify multi-word compounds.
Some discrepancies between our LFs and the LFs
provided for the trial corpus arose from apparent
inconsistencies in the provided LFs. Verbs with
particles were a particular problem. Sometimes,
as in sentences 12 and 13 of the trial corpus, a
verb-particle combination such as look forward to
is treated as a single predicate (look forward to); in
other cases, such as in sentence 35, the verb and its
particle (go out) are treated as separate predicates.
Other inconsistencies in the provided LFs include
missing arguments (direct object in sentence 24),
and verbs not reduced to base form (felt, saw, and
found in sentences 34, 48, 50).
5 Conclusions
Our main finding during the development of the sys-
tems for the two Senseval tasks was that semantic
relations are indeed very close to syntactic depen-
dencies. Using deep dependency structures helped
to keep the manual rules for the LF task simple
and made the learning for the SR task easier. Also
we found that memory-based learning can be effi-
ciently applied to complex, highly structured prob-
lems such as the identification of semantic roles.
Our future work includes more accurate fine-
tuning of the learner for the SR task, extending the
coverage of the LF generator, and experimenting
with the generated LFs for question answering.
6 Acknowledgments
Ahn and De Rijke were supported by a grant from
the Netherlands Organization for Scientific Re-
search (NWO) under project number 612.066.302.
Fissaha, Jijkoun, and De Rijke were supported by a
grant from NWO under project number 220-80-001.
De Rijke was also supported by grants from NWO,
under project numbers 365-20-005, 612.069.006,
612.000.106, and 612.000.207.
References
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch, 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10. Available from
http://ilk.kub.nl/downloads/pub/papers/ilk0310.pdf.
C. J. Fillmore. 1977. The need for a frame semantics in
linguistics. Statistical Methods in Linguistics, 12:5?
29.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Proceed-
ings of ACL 2004.
C. Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-
penhofer, and C. Fillmore. 2003. Framenet: Theory
and practice. http://www.icsi.berkeley.edu/ framenet.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference.
V. Rus. 2002. Logic Form for WordNet Glosses. Ph.D.
thesis, Southern Methodist University.
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The stages of event extraction
David Ahn
Intelligent Systems Lab Amsterdam
University of Amsterdam
ahn@science.uva.nl
Abstract
Event detection and recognition is a com-
plex task consisting of multiple sub-tasks
of varying difficulty. In this paper, we
present a simple, modular approach to
event extraction that allows us to exper-
iment with a variety of machine learning
methods for these sub-tasks, as well as to
evaluate the impact on performance these
sub-tasks have on the overall task.
1 Introduction
Events are undeniably temporal entities, but they
also possess a rich non-temporal structure that is
important for intelligent information access sys-
tems (information retrieval, question answering,
summarization, etc.). Without information about
what happened, where, and to whom, temporal in-
formation about an event may not be very useful.
In the available annotated corpora geared to-
ward information extraction, we see two mod-
els of events, emphasizing these different aspects.
On the one hand, there is the TimeML model, in
which an event is a word that points to a node
in a network of temporal relations. On the other
hand, there is the ACE model, in which an event
is a complex structure, relating arguments that are
themselves complex structures, but with only an-
cillary temporal information (in the form of tem-
poral arguments, which are only noted when ex-
plicitly given). In the TimeML model, every event
is annotated, because every event takes part in the
temporal network. In the ACE model, only ?in-
teresting? events (events that fall into one of 34
predefined categories) are annotated.
The task of automatically extracting ACE
events is more complex than extracting TimeML
events (in line with the increased complexity of
ACE events), involving detection of event anchors,
assignment of an array of attributes, identification
of arguments and assignment of roles, and deter-
mination of event coreference. In this paper, we
present a modular system for ACE event detection
and recognition. Our focus is on the difficulty and
importance of each sub-task of the extraction task.
To this end, we isolate and perform experiments
on each stage, as well as evaluating the contribu-
tion of each stage to the overall task.
In the next section, we describe events in the
ACE program in more detail. In section 3, we pro-
vide an overview of our approach and some infor-
mation about our corpus. In sections 4 through 7,
we describe our experiments for each of the sub-
tasks of event extraction. In section 8, we compare
the contribution of each stage to the overall task,
and in section 9, we conclude.
2 Events in the ACE program
The ACE program1 provides annotated data, eval-
uation tools, and periodic evaluation exercises for
a variety of information extraction tasks. There are
five basic kinds of extraction targets supported by
ACE: entities, times, values, relations, and events.
The ACE tasks for 2005 are more fully described
in (ACE, 2005). In this paper, we focus on events,
but since ACE events are complex structures in-
volving entities, times, and values, we briefly de-
scribe these, as well.
ACE entities fall into seven types (person, or-
ganization, location, geo-political entity, facility,
vehicle, weapon), each with a number of subtypes.
Within the ACE program, a distinction is made be-
tween entities and entity mentions (similarly be-
1http://www.nist.gov/speech/tests/ace/
1
tween event and event mentions, and so on). An
entity mention is a referring expression in text (a
name, pronoun, or other noun phrase) that refers
to something of an appropriate type. An entity,
then, is either the actual referent, in the world,
of an entity mention or the cluster of entity men-
tions in a text that refer to the same actual entity.
The ACE Entity Detection and Recognition task
requires both the identification of expressions in
text that refer to entities (i.e., entity mentions) and
coreference resolution to determine which entity
mentions refer to the same entities.
There are also ACE tasks to detect and recog-
nize times and a limited set of values (contact in-
formation, numeric values, job titles, crime types,
and sentence types). Times are annotated accord-
ing to the TIMEX2 standard, which requires nor-
malization of temporal expressions (timexes) to an
ISO-8601-like value.
ACE events, like ACE entities, are restricted
to a range of types. Thus, not all events in a
text are annotated?only those of an appropriate
type. The eight event types (with subtypes in
parentheses) are Life (Be-Born, Marry, Divorce,
Injure, Die), Movement (Transport), Transaction
(Transfer-Ownership, Transfer-Money), Business
(Start-Org, Merge-Org, Declare-Bankruptcy, End-
Org), Conflict (Attack, Demonstrate), Contact
(Meet, Phone-Write), Personnel (Start-Position,
End-Position, Nominate, Elect), Justice (Arrest-
Jail, Release-Parole, Trial-Hearing, Charge-Indict,
Sue, Convict, Sentence, Fine, Execute, Extradite,
Acquit, Appeal, Pardon). Since there is nothing
inherent in the task that requires the two levels of
type and subtype, for the remainder of the paper,
we will refer to the combination of event type and
subtype (e.g., Life:Die) as the event type.
In addition to their type, events have four other
attributes (possible values in parentheses): modal-
ity (Asserted, Other), polarity (Positive, Nega-
tive), genericity (Specific, Generic), tense (Past,
Present, Future, Unspecified).
The most distinctive characteristic of events
(unlike entities, times, and values, but like rela-
tions) is that they have arguments. Each event type
has a set of possible argument roles, which may be
filled by entities, values, or times. In all, there are
35 role types, although no single event can have all
35 roles. A complete description of which roles go
with which event types can be found in the anno-
tation guidelines for ACE events (LDC, 2005).
Events, like entities, are distinguished from
their mentions in text. An event mention is a span
of text (an extent, usually a sentence) with a dis-
tinguished anchor (the word that ?most clearly ex-
presses [an event?s] occurrence? (LDC, 2005)) and
zero or more arguments, which are entity men-
tions, timexes, or values in the extent. An event is
either an actual event, in the world, or a cluster of
event mentions that refer to the same actual event.
Note that the arguments of an event are the enti-
ties, times, and values corresponding to the entity
mentions, timexes, and values that are arguments
of the event mentions that make up the event.
The official evaluation metric of the ACE pro-
gram is ACE value, a cost-based metric which
associates a normalized, weighted cost to system
errors and subtracts that cost from a maximum
score of 100%. For events, the associated costs
are largely determined by the costs of the argu-
ments, so that errors in entity, timex, and value
recognition are multiplied in event ACE value.
Since it is useful to evaluate the performance of
event detection and recognition independently of
the recognition of entities, times, and values, the
ACE program includes diagnostic tasks, in which
partial ground truth information is provided. Of
particular interest here is the diagnostic task for
event detection and recognition, in which ground
truth entities, values, and times are provided. For
the remainder of this paper, we use this diagnos-
tic methodology, and we extend it to sub-tasks
within the task, evaluating components of our
event recognition system using ground truth out-
put of upstream components. Furthermore, in our
evaluating our system components, we use the
more transparent metrics of precision, recall, F-
measure, and accuracy.
3 Our approach to event extraction
3.1 A pipeline for detecting and recognizing
events
Extracting ACE events is a complex task. Our goal
with the approach we describe in this paper is to
establish baseline performance in this task using a
relatively simple, modular system. We break down
the task of extracting events into a series of clas-
sification sub-tasks, each of which is handled by a
machine-learned classifier.
1. Anchor identification: finding event anchors
(the basis for event mentions) in text and as-
signing them an event type;
2
2. Argument identification: determining which
entity mentions, timexes, and values are ar-
guments of each event mention;
3. Attribute assignment: determining the values
of the modality, polarity, genericity, and tense
attributes for each event mention;
4. Event coreference: determining which event
mentions refer to the same event.
In principle, these four sub-tasks are highly inter-
dependent, but for the approach described here,
we do not model all these dependencies. Anchor
identification is treated as an independent task. Ar-
gument finding and attribute assignment are each
dependent only on the results of anchor identifica-
tion, while event coreference depends on the re-
sults of all of the other three sub-tasks.
To learn classifiers for the first three tasks, we
experiment with TiMBL2, a memory-based (near-
est neighbor) learner (Daelemans et al, 2004),
and MegaM3, a maximum entropy learner (Daume?
III, 2004). For event coreference, we use only
MegaM, since our approach requires probabilities.
In addition to comparing the performance of these
two learners on the various sub-tasks, we also ex-
periment with the structure of the learning prob-
lems for the first two tasks.
In the remainder of this paper, we present exper-
iments for each of these sub-tasks (sections 4? 7),
focusing on each task in isolation, and then look at
how the sub-tasks affect performance in the over-
all task (section 8). First, we discuss the prepro-
cessing of the corpus required for our experiments.
3.2 Preprocessing the corpus
Because of restrictions imposed by the organiz-
ers on the 2005 ACE program data, we use only
the ACE 2005 training corpus, which contains 599
documents, for our experiments. We split this cor-
pus into training and test sets at the document-
level, with 539 training documents and 60 test
documents. From the training set, another 60 doc-
uments are reserved as a development set, which
is used for parameter tuning by MegaM. For the
remainder of the paper, we will refer to the 539
training documents as the training corpus and the
60 test documents as the test corpus.
For our machine learning experiments, we need
a range of information in order to build feature
2http://ilk.uvt.nl/timbl/
3http://www.isi.edu/?hdaume/megam/
vectors. Since we are interested only in perfor-
mance on event extraction, we follow the method-
ology of the ACE diagnostic tasks and use the
ground truth entity, timex2, and value annotations
both for training and testing. Additionally, each
document is tokenized and split into sentences us-
ing a simple algorithm adapted from (Grefenstette,
1994, p. 149). These sentences are parsed using
the August 2005 release of the Charniak parser
(Charniak, 2000)4. The parses are converted into
dependency relations using a method similar to
(Collins, 1999; Jijkoun and de Rijke, 2004). The
syntactic annotations thus provide access both to
constituency and dependency information. Note
that with respect to these two sources of syntactic
information, we use the word head ambiguously to
refer both to the head of a constituent (i.e., the dis-
tinguished word within the constituent from which
the constituent inherits its category features) and
to the head of a dependency relation (i.e., the word
on which the dependent in the relation depends).
Since parses and entity/timex/value annotations
are produced independently, we need a strategy for
matching (entity/timex/value) mentions to parses.
Given a mention, we first try to find a single con-
stituent whose offsets exactly match the extent of
the mention. In the training and development data,
there is an exact-match constituent for 89.2% of
the entity mentions. If there is no such constituent,
we look for a sequence of constituents that match
the mention extent. If there is no such sequence,
we back off to a single word, looking first for a
word whose start offset matches the start of the
mention, then for a word whose end offset matches
the end of the mention, and finally for a word that
contains the entire mention. If all these strategies
fail, then no parse information is provided for the
mention. Note that when a mention matches a se-
quence of constituents, the head of the constituent
in the sequence that is shallowest in the parse tree
is taken to be the (constituent) head of the entire
sequence. Given a parse constituent, we take the
entity type of that constituent to be the type of the
smallest entity mention overlapping with it.
4 Identifying event anchors
4.1 Task structure
We model anchor identification as a word classifi-
cation task. Although an event anchor may in prin-
ciple be more than one word, more than 95% of
4ftp://ftp.cs.brown.edu/pub/nlparser/
3
the anchors in the training data consist of a single
word. Furthermore, in the training data, anchors
are restricted in part of speech (to nouns: NN,
NNS, NNP; verbs: VB, VBZ, VBP, VBG, VBN,
VBD, AUX, AUXG, MD; adjectives: JJ; adverbs:
RB, WRB; pronouns: PRP, WP; determiners: DT,
WDT, CD; and prepositions: IN). Thus, anchor
identification for a document is reduced to the task
of classifying each word in the document with an
appropriate POS tag into one of 34 classes (the 33
event types plus a None class for words that are
not an event anchor).
The class distribution for these 34 classes is
heavily skewed. In the 202,135 instances in
the training data, the None class has 197,261
instances, while the next largest class (Con-
flict:Attack) has only 1410 instances. Thus, in ad-
dition to modeling anchor identification as a sin-
gle multi-class classification task, we also try to
break down the problem into two stages: first, a
binary classifier that determines whether or not a
word is an anchor, and then, a multi-class classi-
fier that determines the event type for the positive
instances from the first task. For this staged task,
we train the second classifier on the ground truth
positive instances.
4.2 Features for event anchors
We use the following set of features for all config-
urations of our anchor identification experiments.
? Lexical features: full word, lowercase word,
lemmatized word, POS tag, depth of word in
parse tree
? WordNet features: for each WordNet POS
category c (from N, V, ADJ, ADV):
? If the word is in catgory c and there is a
corresponding WordNet entry, the ID of
the synset of first sense is a feature value
? Otherwise, if the word has an entry in
WordNet that is morphologically related
to a synset of category c, the ID of the
related synset is a feature value
? Left context (3 words): lowercase, POS tag
? Right context (3 words): lowercase, POS tag
? Dependency features: if the candidate word
is the dependent in a dependency relation, the
label of the relation is a feature value, as are
the dependency head word, its POS tag, and
its entity type
? Related entity features: for each en-
tity/timex/value type t:
? Number of dependents of candidate
word of type t
? Label(s) of dependency relation(s) to
dependent(s) of type t
? Constituent head word(s) of depen-
dent(s) of type t
? Number of entity mentions of type t
reachable by some dependency path
(i.e., in same sentence)
? Length of path to closest entity mention
of type t
4.3 Results
In table 1, we present the results of our anchor
classification experiments (precision, recall and F-
measure). The all-at-once conditions refer to ex-
periments with a single multi-class classifier (us-
ing either MegaM or TiMBL), while the split con-
ditions refer to experiments with two staged clas-
sifiers, where we experiment with using MegaM
and TiMBL for both classifiers, as well as with
using MegaM for the binary classification and
TiMBL for the multi-class classification. In ta-
ble 2, we present the results of the two first-stage
binary classifiers, and in table 3, we present the
results of the two second-stage multi-class classi-
fiers on ground truth positive instances. Note that
we always use the default parameter settings for
MegaM, while for TiMBL, we set k (number of
neighbors to consider) to 5, we use inverse dis-
tance weighting for the neighbors and weighted
overlap, with information gain weighting, for all
non-numeric features.
Both for the all-at-once condition and for multi-
class classification of positive instances, the near-
est neighbor classifier performs substantially bet-
ter than the maximum entropy classifier. For bi-
nary classification, though, the two methods per-
form similarly, and staging either binary classi-
fier with the nearest neighbor classifier for posi-
tive instances yields the best results. In practical
terms, using the maximum entropy classifier for
binary classification and then the TiMBL classifier
to classify only the positive instances is the best
solution, since classification with TiMBL tends to
be slow.
4
Precision Recall F
All-at-once/megam 0.691 0.239 0.355
All-at-once/timbl 0.666 0.540 0.596
Split/megam 0.589 0.417 0.489
Split/timbl 0.657 0.551 0.599
Split/megam+timbl 0.725 0.513 0.601
Table 1: Results for anchor detection and classifi-
cation
Precision Recall F
Binary/megam 0.756 0.535 0.626
Binary/timbl 0.685 0.574 0.625
Table 2: Results for anchor detection (i.e., binary
classification of anchor instances)
5 Argument identification
5.1 Task structure
Identifying event arguments is a pair classification
task. Each event mention is paired with each of the
entity/timex/value mentions occurring in the same
sentence to form a single classification instance.
There are 36 classes in total: 35 role types and a
None class. Again, the distribution of classes is
skewed, though not as heavily as for the anchor
task, with 20,556 None instances out of 29,450
training instances. One additional consideration
is that no single event type allows arguments of
all 36 possible roles; each event type has its own
set of allowable roles. With this in mind, we ex-
periment with treating argument identification as
a single multi-class classification task and with
training a separate multi-class classifier for each
event type. Note that all classifiers are trained us-
ing ground truth event mentions.
5.2 Features for argument identification
We use the following set of features for all our ar-
gument classifiers.
? Anchor word of event mention: full, lower-
case, POS tag, and depth in parse tree
Accuracy
Multi/megam 0.649
Multi/timbl 0.824
Table 3: Accuracy for anchor classification (i.e.,
multi-class classification of positive anchor in-
stances)
Precision Recall F
All-at-once/megam 0.708 0.430 0.535
All-at-once/timbl 0.509 0.453 0.480
CPET/megam 0.689 0.490 0.573
CPET/timbl 0.504 0.535 0.519
Table 4: Results for arguments
? Event type of event mention
? Constituent head word of entity mention:
full, lowercase, POS tag, and depth in parse
tree
? Determiner of entity mention, if any
? Entity type and mention type (name, pro-
noun, other NP) of entity mention
? Dependency path between anchor word and
constituent head word of entity mention, ex-
pressed as a sequence of labels, of words, and
of POS tags
5.3 Results
In table 4, we present the results for argument
identification. The all-at-once conditions refer
to experiments with a single classifier for all in-
stances. The CPET conditions refer to experi-
ments with a separate classifier for each event
type. Note that we use the same parameter settings
for MegaM and TiMBL as for anchor classifica-
tion, except that for TiMBL, we use the modified
value difference metric for the three dependency
path features.
Note that splitting the task into separate tasks
for each event type yields a substantial improve-
ment over using a single classifier. Unlike in the
anchor classification task, maximum entropy clas-
sification handily outperforms nearest-neighbor
classification. This may be related to the binariza-
tion of the dependency-path features for maximum
entropy training: the word and POS tag sequences
(but not the label sequences) are broken down into
their component steps, so that there is a separate
binary feature corresponding to the presence of a
given word or POS tag in the dependency path.
Table 5 presents results of each of the classi-
fiers restricted to Time-* arguments (Time-Within,
Time-Holds, etc.). These arguments are of partic-
ular interest not only because they provide the link
between events and times in this model of events,
but also because Time-* roles, unlike other role
5
Precision Recall F
All-at-once/megam 0.688 0.477 0.564
All-at-once/timbl 0.500 0.482 0.491
CPET/megam 0.725 0.451 0.556
CPET/timbl 0.357 0.404 0.379
Table 5: Results for Time-* arguments
Accuracy
megam 0.795
timbl 0.793
baseline 0.802
majority (in training) 0.773
Table 6: Genericity
types, are available to all event types. We see that,
in fact, the all-at-once classifiers perform better
for these role types, which suggests that it may be
worthwhile to factor out these role types and build
a classifier specifically for temporal arguments.
6 Assigning attributes
6.1 Task structure
In addition to the event type and subtype attributes,
(the event associated with) each event mention
must also be assigned values for genericity, modal-
ity, polarity, and tense. We train a separate classi-
fier for each attribute. Genericity, modality, and
polarity are each binary classification tasks, while
tense is a multi-class task. We use the same fea-
tures as for the anchor identification task, with the
exception of the lemmatized anchor word and the
WordNet features.
6.2 Results
The results of our classification experiments are
given in tables 6, 7, 8, and 9. Note that modal-
ity, polarity, and genericity are skewed tasks where
it is difficult to improve on the baseline majority
classification (Asserted, Positive, and Specific, re-
spectively) and where maximum entropy and near-
est neighbor classification perform very similarly.
For tense, however, both learned classifiers per-
form substantially better than the majority base-
line (Past), with the maximum entropy classifier
providing the best performance.
Accuracy
megam 0.750
timbl 0.759
baseline 0.738
majority (in training) 0.749
Table 7: Modality
Accuracy
megam 0.955
timbl 0.955
baseline 0.950
majority (in training) 0.967
Table 8: Polarity
7 Event coreference
7.1 Task structure
For event coreference, we follow the approach
to entity coreference detailed in (Florian et al,
2004). This approach uses a mention-pair coref-
erence model with probabilistic decoding. Each
event mention in a document is paired with ev-
ery other event mention, and a classifier assigns
to each pair of mentions the probability that the
paired mentions corefer. These probabilities are
used in a left-to-right entity linking algorithm in
which each mention is compared with all already-
established events (i.e., event mention clusters) to
determine whether it should be added to an exist-
ing event or start a new one. Since the classifier
needs to output probabilities for this approach, we
do not use TiMBL, but only train a maximum en-
tropy classifier with MegaM.
7.2 Features for coreference classification
We use the following set of features for our
mention-pair classifier. The candidate is the ear-
lier event mention in the text, and the anaphor is
the later mention.
? CandidateAnchor+AnaphorAnchor, also
POS tag and lowercase
Accuracy
megam 0.633
timbl 0.613
baseline 0.535
majority (in training) 0.512
Table 9: Tense
6
Precision Recall F
megam 0.761 0.580 0.658
baseline 0.167 1.0 0.286
Table 10: Coreference
? CandidateEventType+AnaphorEventType
? Depth of candidate anchor word in parse tree
? Depth of anaphor anchor word in parse tree
? Distance between candidate and anchor, mea-
sured in sentences
? Number, heads, and roles of shared argu-
ments (same entity/timex/value w/same role)
? Number, heads, and roles of candidate argu-
ments that are not anaphor arguments
? Number, heads, and roles of anaphor argu-
ments that are not candidate arguments
? Heads and roles of arguments shared by can-
didate and anaphor in different roles
? CandidateModalityVal+AnaphorModalityVal,
also for polarity, genericity, and tense
7.3 Results
In table 10, we present the performance of our
event coreference pair classifier. Note that the
distribution for this task is also skewed: only
3092 positive instances of 42,736 total training in-
stances. Simple baseline of taking event mentions
of identical type to be coreferent does quite poorly.
8 Evaluation with ACE value
Table 11 presents results of performing the full
event detection and recognition task, swapping in
ground truth (gold) or learned classifiers (learned)
for the various sub-tasks (we also swap in major-
ity classifiers for the attribute sub-task). For the
anchor sub-task, we use the split/megam+timbl
classifier; for the argument sub-task, we use the
CPET/megam classifier; for the attribute sub-
tasks, we use the megam classifiers; for the coref-
erence sub-task, we use the approach outlined in
section 7. Since in our approach, the argument and
attribute sub-tasks are dependent on the anchor
sub-task and the coreference sub-task is depen-
dent on all of the other sub-tasks, we cannot freely
swap in ground truth?e.g., if we use a learned
classifier for the anchor sub-task, then there is no
ground truth for the corresponding argument and
attribute sub-tasks.
The learned coreference classifier provides a
small boost to performance over doing no coref-
erence at all (7.5% points for the condition in
which all the other sub-tasks use ground truth (1
vs. 8), 0.6% points when all the other sub-tasks
use learned classifiers (7 vs. 12)). From perfect
coreference, using ground truth for the other sub-
tasks, the loss in value is 11.4% points (recall that
maximum ACE value is 100%). Note that the dif-
ference between perfect coreference and no coref-
erence is only 18.9% points.
Looking at the attribute sub-tasks, the effects on
ACE value are even smaller. Using the learned
attribute classifiers (with ground truth anchors and
arguments) results in 4.8% point loss in value from
ground truth attributes (1 vs. 5) and only a 0.5%
point gain in value from majority class attributes
(4 vs. 5). With learned anchors and arguments, the
learned attribute classifiers result in a 0.4% loss in
value from even majority class attributes (3 vs. 7).
Arguments clearly have the greatest impact on
ACE value (which is unsurprising, given that ar-
guments are weighted heavily in event value). Us-
ing ground truth anchors and attributes, learned ar-
guments result in a loss of value of 35.6% points
from ground truth arguments (1 vs. 2). When the
learned coreference classifier is used, the loss in
value from ground truth arguments to learned ar-
guments is even greater (42.5%, 8 vs. 10).
Anchor identification also has a large impact on
ACE value. Without coreference but with learned
arguments and attributes, the difference between
using ground truth anchors and learned anchors is
22.2% points (6 vs. 7). With coreference, the dif-
ference is still 21.0% points (11 vs. 12).
Overall, using the best learned classifiers for
the various subtasks, we achieve an ACE value
score of 22.3%, which falls within the range of
scores for the 2005 diagnostic event extraction
task (19.7%?32.7%).5 Note, though, that these
scores are not really comparable, since they in-
volve training on the full training set and testing
on a separate set of documents (as noted above,
the 2005 ACE testing data is not available for fur-
ther experimentation, so we are using 90% of the
original training data for training/development and
5For the diagnostic task, ground truth entities, values, and
times, are provided, as they are in our experiments.
7
anchors args attrs coref ACE value
1 gold gold gold none 81.1%
2 gold learned gold none 45.5%
3 learned learned maj none 22.1%
4 gold gold maj none 75.8%
5 gold gold learned none 76.3%
6 gold learned learned none 43.9%
7 learned learned learned none 21.7%
8 gold gold gold learned 88.6%
9 gold gold learned learned 79.4%
10 gold learned gold learned 46.1%
11 gold learned learned learned 43.3%
12 learned learned learned learned 22.3%
Table 11: ACE value
10% for the results presented here).
9 Conclusion and future work
In this paper, we have presented a system for ACE
event extraction. Even with the simple breakdown
of the task embodied by the system and the limited
feature engineering for the machine learned classi-
fiers, the performance is not too far from the level
of the best systems at the 2005 ACE evaluation.
Our approach is modular, and it has allowed us to
present several sets of experiments exploring the
effect of different machine learning algorithms on
the sub-tasks and exploring the effect of the differ-
ent sub-tasks on the overall performance (as mea-
sured by ACE value).
There is clearly a great deal of room for im-
provement. As we have seen, improving anchor
and argument identification will have the great-
est impact on overall performance, and the exper-
iments we have done suggest directions for such
improvement. For anchor identification, taking
one more step toward binary classification and
training a binary classifier for each event type (ei-
ther for all candidate anchor instances or only for
positive instances) may be helpful. For argument
identification, we have already discussed the idea
of modeling temporal arguments separately; per-
haps introducing a separate classifier for each role
type might also be helpful.
For all the sub-tasks, there is more feature en-
gineering that could be done (a simple example:
for coreference, boolean features corresponding to
identical anchors and event types). Furthermore,
the dependencies between sub-tasks could be bet-
ter modeled.
References
2005. The ACE 2005 (ACE05) evaluation plan.
http://www.nist.gov/speech/tests/
ace/ace05/doc/ace05-evalplan.v3.
pdf.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of NAACL, pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg Mem-
ory Based Learner, version 5.1, Reference Guide.
University of Tilburg, ILK Technical Report ILK-
0402. http://ilk.uvt.nl/.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://www.isi.edu/?hdaume/docs/
daume04cg-bfgs.ps, August.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proceedings of HLT/NAACL-04.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer.
Valentin Jijkoun and Maarten de Rijke. 2004. Enrich-
ing the output of a parser using memory-based learn-
ing. In Proceedings of the 42nd Meeting of the ACL.
Linguistic Data Consortium, 2005. ACE (Automatic
Content Extraction) English Annotation Guidelines
for Events, version 5.4.3 2005.07.01 edition.
8
Representing and Querying Multi-dimensional Markup
for Question Answering
Wouter Alink, Valentin Jijkoun, David Ahn, Maarten de Rijke
ISLA, University of Amsterdam
alink,jijkoun,ahn,mdr@science.uva.nl
Peter Boncz, Arjen de Vries
CWI, Amsterdam, The Netherlands
boncz,arjen@cwi.nl
Abstract
This paper describes our approach to rep-
resenting and querying multi-dimensional,
possibly overlapping text annotations, as
used in our question answering (QA) sys-
tem. We use a system extending XQuery,
the W3C-standard XML query language,
with new axes that allow one to jump eas-
ily between different annotations of the
same data. The new axes are formulated in
terms of (partial) overlap and containment.
All annotations are made using stand-off
XML in a single document, which can be
efficiently queried using the XQuery ex-
tension. The system is scalable to giga-
bytes of XML annotations. We show ex-
amples of the system in QA scenarios.
1 Introduction
Corpus-based question answering is a complex
task that draws from information retrieval, infor-
mation extraction and computational linguistics to
pinpoint information users are interested in. The
flexibility of natural language means that poten-
tial answers to questions may be phrased in differ-
ent ways?lexical and syntactic variation, ambi-
guity, polysemy, and anaphoricity all contribute to
a gap between questions and answers. Typically,
QA systems rely on a range of linguistic analyses,
provided by a variety of different tools, to bridge
this gap from questions to possible answers.
In our work, we focus on how we can integrate
the analyses provided by completely independent
linguistic processing components into a uniform
QA framework. On the one hand, we would like
to be able, as much as possible, to make use of
off-the-shelf NLP tools from various sources with-
out having to worry about whether the output of
the tools are compatible, either in a strong sense
of forming a single hierarchy or even in a weaker
sense of simply sharing common tokenization. On
the other hand, we would like to be able to issue
simple and clear queries that jointly draw upon an-
notations provided by different tools.
To this end, we store annotated data as stand-
off XML and query it using an extension of
XQuery with our new StandOff axes, inspired by
(Burkowski, 1992). Key to our approach is the use
of stand-off annotation at every stage of the anno-
tation process. The source text, or character data,
is stored in a Binary Large OBject (BLOB), and all
annotations, in a single XML document. To gen-
erate and manage the annotations we have adopted
XIRAF (Alink, 2005), a framework for integrating
annotation tools which has already been success-
fully used in digital forensic investigations.
Before performing any linguistic analysis, the
source documents, which may contain XMLmeta-
data, are split into a BLOB and an XML docu-
ment, and the XML document is used as the ini-
tial annotation. Various linguistic analysis tools
are run over the data, such as a named-entity tag-
ger, a temporal expression (timex) tagger, and syn-
tactic phrase structure and dependency parsers.
The XML document will grow during this analy-
sis phase as new annotations are added by the NLP
tools, while the BLOB remains intact. In the end,
the result is a fully annotated stand-off document,
and this annotated document is the basis for our
QA system, which uses XQuery extended with the
new axes to access the annotations.
The remainder of the paper is organized as fol-
lows. In Section 2 we briefly discuss related work.
Section 3 is devoted to the issue of querying multi-
dimensional markup. Then we describe how we
coordinate the process of text annotation, in Sec-
3
tion 4, before describing the application of our
multi-dimensional approach to linguistic annota-
tion to question answering in Section 5. We con-
clude in Section 6.
2 Related Work
XML is a tree structured language and provides
very limited capabilities for representing several
annotations of the same data simultaneously, even
when each of the annotations is tree-like. In par-
ticular, in the case of inline markup, multiple an-
notation trees can be put together in a single XML
document only if elements from different annota-
tions do not cross each other?s boundaries.
Several proposals have tried to circumvent this
problem in various ways. Some approaches are
based on splitting overlapping elements into frag-
ments. Some use SGML with the CONCUR fea-
ture or even entirely different markup schemes
(such as LMNL, the Layered Markup and An-
notation Language (Piez, 2004), or GODDAGs,
generalized ordered-descendant directed acyclic
graphs (Sperberg-McQueen and Huitfeldt, 2000))
that allow arbitrary intersections of elements from
different hierarchies. Some approaches use empty
XML elements (milestones) to mark beginnings
and ends of problematic elements. We refer to
(DeRose, 2004) for an in-depth overview.
Although many approaches solve the problem
of representing possibly overlapping annotations,
they often do not address the issue of accessing
or querying the resulting representations. This
is a serious disadvantage, since standard query
languages, such as XPath and XQuery, and stan-
dard query evaluation engines cannot be used with
these representations directly.
The approach of (Sperberg-McQueen and Huit-
feldt, 2000) uses GODDAGs as a conceptual
model of multiple tree-like annotations of the
same data. Operationalizing this approach,
(Dekhtyar and Iacob, 2005) describes a system
that uses multiple inline XML annotations of the
same text to build a GODDAG structure, which
can be queried using EXPath, an extension of
XPath with new axis steps.
Our approach differs from that of Dekhtyar and
Iacob in several ways. First of all, we do not use
multiple separate documents; instead, all annota-
tion layers are woven into a single XML docu-
ment. Secondly, we use stand-off rather than in-
line annotation; each character in the original doc-
ument is referred to by a unique offset, which
means that specific regions in a document can be
denoted unambiguously with only a start and an
end offset. On the query side, our extended XPath
axes are similar to the axes of Dekhtyar and Iacob,
but less specific: e.g., we do not distinguish be-
tween left-overlapping and right-overlapping char-
acter regions.
In the setting of question answering there
are a few examples of querying and retrieving
semistructured data. Litowski (Litkowksi, 2003;
Litkowksi, 2004) has been advocating the use of
an XML-based infrastructure for question answer-
ing, with XPath-based querying at the back-end,
for a number of years. Ogilvie (2004) outlines the
possibility of using multi-dimensional markup for
question answering, with no system or experimen-
tal results yet. Jijkoun et al (2005) describe initial
experiments with XQuesta, a question answering
system based on multi-dimensional markup.
3 Querying Multi-dimensional Markup
Our approach to markup is based on stand-off
XML. Stand-off XML is already widely used, al-
though it is often not recognized as such. It can
be found in many present-day applications, es-
pecially where annotations of audio or video are
concerned. Furthermore, many existing multi-
dimensional-markup languages, such as LMNL,
can be translated into stand-off XML.
We split annotated data into two parts: the
BLOB (Binary Large OBject) and the XML anno-
tations that refer to specific regions of the BLOB.
A BLOB may be an arbitrary byte string (e.g., the
contents of a hard drive (Alink, 2005)), and the
annotations may refer to regions using positions
such as byte offsets, word offsets, points in time
or frame numbers (e.g., for audio or video appli-
cations). In text-based applications, such as de-
scribed in this paper, we use character offsets. The
advantage of such character-based references over
word- or token-based ones is that it allows us to
reconcile possibly different tokenizations by dif-
ferent text analysis tools (cf. Section 4).
In short, a multi-dimensional document consists
of a BLOB and a set of stand-off XML annota-
tions of the BLOB. Our approach to querying such
documents extends the common XML query lan-
guages XPath and XQuery by defining 4 new axes
that allow one to move from one XML tree to an-
other. Until recently, there have been very few
4
AB
C
E
D
XML tree 1
XML tree 2
BLOB
(text characters)
Figure 1: Two annotations of the same data.
approaches to querying stand-off documents. We
take the approach of (Alink, 2005), which allows
the user to relate different annotations using con-
tainment and overlap conditions. This is done us-
ing the new StandOff XPath axis steps that we add
to the XQuery language. This approach seems to
be quite general: in (Alink, 2005) it is shown that
many of the query scenarios given in (Iacob et al,
2004) can be easily handled by using these Stand-
Off axis steps.
Let us explain the axis steps by means of an
example. Figure 1 shows two annotations of the
same character string (BLOB), where the first
XML annotation is
<A start="10" end="50">
<B start="30" end="50"/>
</A>
and the second is
<E start="20" end="60">
<C start="20" end="40"/>
<D start="55" end="60">
</E>
While each annotation forms a valid XML tree and
can be queried using standard XML query lan-
guages, together they make up a more complex
structure.
StandOff axis steps, inspired by (Burkowski,
1992), allow for querying overlap and contain-
ment of regions, but otherwise behave like reg-
ular XPath steps, such as child (the step be-
tween A and B in Figure 1) or sibling (the step
between C and D). The new StandOff axes, de-
noted with select-narrow, select-wide,
reject-narrow, and reject-wide select
contained, overlapping, non-contained and non-
overlapping region elements, respectively, from
possibly distinct layers of XML annotation of the
data. Table 1 lists some examples for the annota-
tions of our example document.
In XPath, the new axis steps are used in exactly
the same way as the standard ones. For example,
Context Axis Result nodes
A select-narrow B C
A select-wide B C E
A reject-narrow E D
A reject-wide D
Table 1: Example annotations.
the XPath query:
//B/select-wide::*
returns all nodes that overlap with the span of a
B node: in our case the nodes A, B, C and E. The
query:
//*[./select-narrow::B]
returns nodes that contain the span of B: in our
case, the nodes A and E.
In implementing the new steps, one of our de-
sign decisions was to put all stand-off annotations
in a single document. For this, an XML processor
is needed that is capable of handling large amounts
of XML. We have decided to use MonetDB/X-
Query, an XQuery implementation that consists of
the Pathfinder compiler, which translates XQuery
statements into a relational algebra, and the re-
lational database MonetDB (Grust, 2002; Boncz,
2002).
The implementation of the new axis steps in
MonetDB/XQuery is quite efficient. When the
XMark benchmark documents (XMark, 2006)
are represented using stand-off notation, query-
ing with the StandOff axis steps is interactive for
document size up to 1GB. Even millions of re-
gions are handled efficiently. The reason for the
speed of the StandOff axis steps is twofold. First,
they are accelerated by keeping a database in-
dex on the region attributes, which allows fast
merge-algorithms to be used in their evaluation.
Such merge-algorithms make a single linear scan
through the index to compute each StandOff
step. The second technical innovation is ?loop-
lifting.? This is a general principle inMonetDB/X-
Query(Boncz et al, 2005) for the efficient execu-
tion of XPath steps that occur nested in XQuery
iterations (i.e., inside for-loops). A naive strategy
would invoke the StandOff algorithm for each it-
eration, leading to repeated (potentially many) se-
quential scans. Loop-lifted versions of the Stand-
Off algorithms, in contrast, handle all iterations to-
gether in one sequential scan, keeping the average
complexity of the StandOff steps linear.
5
The StandOff axis steps are part of release
0.10 of the open-source MonetDB/XQuery prod-
uct, which can be downloaded from http://
www.monetdb.nl/XQuery.
In addition to the StandOff axis steps, a key-
word search function has been added to the
XQuery system to allow queries asking for re-
gions containing specific words. This function
is called so-contains($node, $needle)
which will return a boolean specifying whether
$needle occurs in the given region represented
by the element $node.
4 Combining Annotations
In our QA application of multi-dimensional
markup, we work with corpora of newspaper arti-
cles, each of which comes with some basic anno-
tation, such as title, body, keywords, timestamp,
topic, etc. We take this initial annotation structure
and split it into raw data, which comprises all tex-
tual content, and the XML markup. The raw data
is the BLOB, and the XML annotations are con-
verted to stand-off format. To each XML element
originally containing textual data (now stored in
the BLOB), we add a start and end attribute
denoting its position in the BLOB.
We use a separate system, XIRAF, to coordi-
nate the process of automatically annotating the
text. XIRAF (Figure 2) combines multiple text
processing tools, each having an input descriptor
and a tool-specific wrapper that converts the tool
output into stand-off XML annotation. Figure 3
shows the interaction of XIRAF with an automatic
annotation tool using a wrapper.
The input descriptor associated with a tool is
used to select regions in the data that are candi-
dates for processing by that tool. The descrip-
tor may select regions on the basis of the original
metadata or annotations added by other tools. For
example, both our sentence splitter and our tempo-
ral expression tagger use original document meta-
data to select their input: both select document
text, with //TEXT. Other tools, such as syntac-
tic parsers and named-entity taggers, require sep-
arated sentences as input and thus use the output
annotations of the sentence splitter, with the input
descriptor //sentence. In general, there may
be arbitrary dependencies between text-processing
tools, which XIRAF takes into account.
In order to add the new annotations generated
by a tool to the original document, the output of
the tool must be represented using stand-off XML
annotation of the input data. Many text process-
ing tools (e.g., parsers or part-of-speech taggers)
do not produce XML annotation per se, but their
output can be easily converted to stand-off XML
annotation. More problematically, text process-
ing tools may actually modify the input text in the
course of adding annotations, so that the offsets
referenced in the new annotations do not corre-
spond to the original BLOB. Tools make a vari-
ety of modifications to their input text: some per-
form their own tokenization (i.e., inserting whites-
paces or other word separators), silently skip parts
of the input (e.g., syntactic parsers, when the pars-
ing fails), or replace special symbols (e.g., paren-
theses with -LRB- and -RRB-). For many of the
available text processing tools, such possible mod-
ifications are not fully documented.
XIRAF, then, must map the output of a process-
ing tool back to the original BLOB before adding
the new annotations to the original document. This
re-alignment of the output of the processing tools
with the original BLOB is one of the major hur-
dles in the development of our system. We ap-
proach the problems systematically. We compare
the text data in the output of a given tool with the
data that was given to it as input and re-align in-
put and output offsets of markup elements using
an edit-distance algorithm with heuristically cho-
sen weights of character edits. After re-aligning
the output with the original BLOB and adjusting
the offsets accordingly, the actual data returned by
the tool is discarded and only the stand-off markup
is added to the existing document annotations.
5 Question Answering
XQuesta, our corpus-based question-answering
system for English and Dutch, makes use of the
multi-dimensional approach to linguistic annota-
tion embodied in XIRAF. The system analyzes an
incoming question to determine the required an-
swer type and keyword queries for retrieving rel-
evant snippets from the corpus. From these snip-
pets, candidate answers are extracted, ranked, and
returned.
The system consults Dutch and English news-
paper corpora. Using XIRAF, we annotate the
corpora with named entities (including type infor-
mation), temporal expressions (normalized to ISO
values), syntactic chunks, and syntactic parses
(dependency parses for Dutch and phrase structure
6
;4XHVWD ;,5$))HDWXUH([WUDFWLRQ)UDPHZRUN;4XHU\6\VWHP
    	

 
 Supporting temporal question answering:
strategies for offline data collection
David Ahn
ISLA, University of Amsterdam
ahn@ science. uva. nl
Steven Schockaert, Martine De Cock, and Etienne Kerre
Ghent University
Steven. Schockaert,Martine. DeCock,Etienne. Kerre@ UGent. be
Abstract
We pursue two strategies for offline data collection for a temporal question answering
system that uses both quantitative methods and fuzzy methods to reason about time
and events. The first strategy extracts event descriptions from the structured year
entries in the online encyclopedia Wikipedia, yielding clean quantitative temporal
information about a range of events. The second strategy mines the web using
patterns indicating temporal relations between events and times and between events.
Web mining leverages the volume of data available on the web to find qualitative
temporal relations between known events and new, related events and to build fuzzy
time spans for events for which we lack crisp metric temporal information.
1 Introduction
Time structures our world, and the questions we ask reflect that. Not only
do we want to know quantitative information?when did some event happen
or how long did some state of affairs persist?but we also want qualitative
information?what was going on before or during major events, or what hap-
pened afterwards. While the amount of information available to answer such
questions continues to increase, the temporal information needed is not always
fully specified. No information source is obliged to timestamp every referenced
event, so while evidence for qualitative temporal relations abounds, there is
often no quantitative information to verify it. Furthermore, many events, such
as the Cold War or the Great Depression, are inherently vague?with gradual
beginnings or endings?or ill-defined aggregations of smaller events.
In order for a temporal QA system to be able to make use of such limited,
incomplete temporal information, careful consideration must be given both to
the extraction of temporal information it needs and to the temporal reasoning
mechanisms it employs. We are presently at work on a temporal QA system
that provides access to events extracted from Wikipedia and satellite events
mined from the web and that models the time span of vague events as fuzzy sets
and qualitative temporal relations as fuzzy relations. In this paper, we focus
on the creation of the knowledge base of events and temporal information,
which takes place offline, prior to any user interaction. A separate paper [10]
describes the fuzzy reasoning mechanisms the system uses.
In ?2 and ?3, we introduce temporal questions and sketch the architec-
ture of our temporal QA system. In ?4, we describe event extraction from
Wikipedia, an online encyclopedia. In ?5, we describe web mining for fuzzy
and qualitative temporal information. Note that the work described here is
still in progress, so while our extraction methods are in use, we are still ex-
perimenting with them, and the QA system is not yet complete.
2 Temporal questions
There are a variety of question types that fall under the umbrella of temporal
questions, including questions that ask for times as answers, those that ask for
temporal relations, and those that ask for information restricted to a certain
time period [6]. The degree of explicitness of temporal reference in a temporal
question also varies significantly: some temporal questions refer explicitly to a
date or time, while others refer to times only implicitly, by reference to events
or states. Here, we focus on temporally restricted questions, and in particular,
those restricted by events, such as these (from the CLEF 2005 QA track):
(1) Who played the role of Superman before he was paralyzed?
(2) What disease did many American soldiers get after the Gulf War?
The data collection we describe, though, can be used to support the answering
of other types of temporal questions, as well.
Temporally restricted questions consist of two parts: the main clause,
which indicates the information request, and the temporal restriction, which
is a subordinate clause or PP headed by a temporal preposition or connective,
such as before, after, during, etc., which we refer to as temporal signals [7]. The
temporal signal that connects the two parts of a temporally restricted ques-
tion indicates the temporal relation that must hold between the time spans
of the restricting event and the requested events. Since much of the tempo-
ral information we have access to regarding events is vague and incomplete,
we explore the use of fuzzy temporal reasoning for temporal QA, instead of
the standard Allen algebra of temporal interval relations [1]. The model we
use is a generalization of Allen?s algebra that is suitable for vague events and
relations. For crisp events, our reasoning algorithm is equivalent to Allen?s
path-consistency algorithm. For vague events, fuzzy relations can express that
a given qualitative relation is only satisfied to a certain degree [11].
3 Architecture of a temporal QA system
Our temporal QA system follows a strategy of extracting information likely to
be useful in answering questions?in our case, a knowledge base of events and
temporal relations?in a pre-processing stage, before any questions are asked
[3,4]. The system consists of several components: a question analysis module,
the knowledge base, and an answer selection module.
Question analysis: Since we are focusing on temporal questions in which
a temporal relation restricts the information being queried, our question anal-
ysis module must separate the non-temporal part of the question?the actual
information request?from the temporal restriction. Our question analysis
module parses the question and extracts phrases headed by temporal signals
as potential temporal restrictions. It then uses standard pattern-based tech-
niques to extract keyword queries and the expected answer type.
Knowledge base: The knowledge base (KB) consists of two parts: an
XML database containing descriptions of individual events and a temporal
relation network containing inclusion and before/after relations for events in
the KB. Quantitative temporal information about events (i.e., starting and
ending dates for crisp events and fuzzy sets for vague ones) is contained in the
XML database. The rest of this paper describes the construction of the KB.
Answer selection: To answer a temporally restricted question, we must
find events that match the non-temporal part of the question and filter out
those that do not satisfy the restriction. We treat the problem of finding the
events as a retrieval problem, using the keyword queries from question analy-
sis, with event descriptions as target documents. Checking whether an event
satisfies the restriction is a matter of inferring whether an appropriate qualita-
tive temporal relation holds between the event and a time or event matching
the restriction. We use IR techniques to find events matching the restriction,
and we use both quantitative and fuzzy temporal reasoning to make the infer-
ence [10]. From the remaining event descriptions, we use standard techniques
to extract an answer. Typically, the information request is mapped to a named
entity type by the question analysis module, so appropriate named entities are
harvested and scored and the top-scoring entity is returned.
4 Extracting events from Wikipedia
Wikipedia is a free, open-domain, web-based encyclopedia [12]. In addition
to traditional encyclopedia entries, it also has entries for a variety of time
periods, which contain lists of historical and/or current events. We extract
events from the entries for years. The standardized formatting of year entries
in Wikipedia, together with the wiki markup used for this formatting, makes
extracting event descriptions straightforward. A typical year entry contains
sections delimited by the second-level headings ==Events==, ==Births==, and
==Deaths==. Each of these sections is optionally split into subsections de-
limited by third-level headings indicating months (e.g., ===May===) or the
lack of a date (===Unknown date===). Within these subsections are asterisk-
delimited lists, each item of which corresponds to an event (or a date with a
list of events). Event descriptions begin with a date or a date range, if known,
and then continue with one or two sentences describing the event. This text
contains phrases marked up as wiki links (pointers to Wikipedia entries):
(3) [[March 10]] - The [[New Hampshire]] primary is won by [[Henry Cabot
Lodge]], Ambassador to [[South Vietnam]].
Sometimes, an event description begins with a wiki link, set off with a colon,
indicating a larger event (which we call a super-event) of which it is a part:
(4) [[August 8]] - [[Watergate scandal]]: US President [[Richard Nixon]]
announces his resignation (effective [[August 9]])
Given the structured nature of year entries, simple hand-built patterns
can be used to perform what amounts to shallow semantic interpretation,
extracting event descriptions from the entries, including temporal location
information and limited participant and mereological information (via wiki
links and super-events, when present). For each extracted event description,
the date(s) and any embedded wiki links are extracted, using simple pattern-
matching, and the text of the description is parsed. This information is added
to our XML database as an event element with the following sub-elements:
date/start date/end date (normalized dates; which one(s) depends on what
is given in the entry), super event (wiki link to super event, if present),
description (text of the description), and parse (converted to XML).
From the entries for the years from 1600 to 2005, we have extracted about
33,000 events, somewhat over half (about 19,000) birth and death events.
5 Web mining for fuzzy and qualitative information
The basic idea behind web mining is that there is enough information on the
web that if there is a significant connection between two events, we should be
able to find this connection by searching for patterns that typically express it.
We use web mining to build representations of the time span of vague events
and to find both additional events related to events already in the KB and for
qualitative temporal relations between events in the KB.
While most of the smaller-scale events extracted from Wikipedia come
with quantitative temporal information, it is not always fully specified. Fur-
thermore, many of the super-events from Wikipedia, as well as the new events
we mine from the web, lack such information. We cope with this by search-
ing the web for beginning and ending dates using a simple pattern-based
approach?sending patterns to Google and extracting information from the
returned snippets. The patterns we use include, e.g., ?event? began on ?date?
and ?event? lasted until ?date?. If there is sufficient agreement among different
web pages about the beginning and ending date of an event, we represent the
time span of this event as an interval. If not, we use the techniques described
in [9] to construct a suitable fuzzy set [13] to represent the time span, which
is stored in the XML database as part of the event representation. Of course,
for some events, we may fail to find sufficient information about beginning or
ending dates, in which case they remain undated, or ungrounded, events.
We also use a pattern-based approach to mine the web both for new events
and for temporal relations relating ungrounded events to grounded events. Be-
cause new events are only usable if they can be temporally connected to events
already in the KB, we can use a uniform set of hand-crafted patterns that in-
dicate a temporal relation between events. The patterns we use include, e.g.,
?NP1? gave way to ?NP2? and ?NP2? took place after ?NP1?, for before/after
relations, and ?NP1? and other events during ?NP2? and ?NP1? took place
during ?NP2?, for inclusion relations. All our patterns relate NP descriptions
of events, which means that they can only be used with known events that
have NP descriptions. Fortunately, this includes all super-events and newly
mined events, which make up most of the ungrounded events in the KB.
Since we can use the same patterns to mine for new events and for tempo-
ral relations for ungrounded events, we combine the tasks. Our basic proce-
dure for mining with these patterns is as follows. Substitute either ?NP1? or
?NP2? with the NP description of a known event, send the resulting pattern to
Google, and parse and tag the returned snippets with named entities. Extract
NPs in the other NP position, discarding those tagged as person, location,
or organization. For each remaining NP, if it refers to a known event in the
KB, add to the temporal relation network a link between the original known
event and the event referred to by the mined NP. Otherwise, add a new event
for the mined NP and a link between the original event and the new event.
The hardest step is determining whether a mined NP refers to an event
already in the KB. Coreference resolution is clearly necessary to temporal
constraints on coreferring event descriptions, but to maintain consistency in
the KB, we must be careful in asserting coreference. We do not try to solve
the cross-document coreference task completely but instead split mined NPs
into two groups. The first group, which includes all indefinite, demonstrative,
quantificational, and pronominal NPs, is added to the temporal network but is
never considered for coreference. The second group contains NPs that we are
confident refer to a unique event and that are thus candidates for coreference.
To find these NPs, we are experimenting with heuristics to determine
unique reference. Some heuristics are capitalization-based, while others are
based on collocation measures using web hit counts [5,8], such as the ratio
between the number of hits for the entire NP and the product of the hits
for each of the individual words in the NP (similar to pointwise mutual in-
formation [2]). When an NP is determined to be uniquely referring, we use
string matching to determine whether it is coreferent with an existing event
description. If it is, the temporal relation mined is added to the KB for this
existing event. Otherwise, a new event and relation are added to the KB.
6 Conclusion
We have described the construction of a knowledge base of events and temporal
relations for a temporal QA system. We take advantage of a freely available,
structured resource?Wikipedia?to obtain relatively accurate quantitative in-
formation about events. We also mine the web to build fuzzy sets for vague
events and to find events for which we can only get qualitative information.
Acknowledgements The first author was supported by the Netherlands
Organization for Scientific Research (NWO), under project number 612.066.302.
The second author was supported by a PhD grant from the Research Foun-
dation ? Flanders.
References
[1] Allen, J., Maintaining knowledge about temporal intervals, Communications of
the ACM 26 (1983), pp. 832?843.
[2] Church, K. et al, Using statistics in lexical analysis, in: Lexical Acquisition:
Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum, 1991 .
[3] Fleischman, M., E. Hovy and A. Echihabi, Offline strategies for online question
answering: Answering questions before they are asked, in: ACL 2003, 2003.
[4] Jijkoun, V., G. Mishne and M. de Rijke, Preprocessing documents to answer
Dutch questions, in: BNAIC?03, 2003.
[5] Magnini, B., M. Negri, R. Prevete and H. Tanev, Is it the right answer?
Exploiting web redundancy for answer validation, in: ACL-02, 2002.
[6] Pustejovsky, J. et al, TERQAS final report, http://www.cs.brandeis.edu/
?jamesp/arda/time/readings/TERQAS-FINAL-REPORT.pdf (2002).
[7] Saur??, R. et al, TimeML annotation guidelines, http://www.cs.brandeis.
edu/?jamesp/arda/time/timeMLdocs/annguide12wp.pdf (2004).
[8] Schlobach, S., D. Ahn, M. de Rijke and V. Jijkoun, Data-driven type checking
in open domain question answering, Journal of Applied Logic (to appear).
[9] Schockaert, S., Construction of membership functions for fuzzy time periods, in:
J. Gervain, editor, ESSLLI 2005 Student Session, 2005.
[10] Schockaert, S., D. Ahn, M. De Cock and E. E. Kerre, Question answering with
imperfect temporal information, in: FQAS-2006, to appear.
[11] Schockaert, S., M. De Cock and E. E. Kerre, Imprecise temporal interval
relations, in: LNCS 3849, Springer, 2006 .
[12] Wikipedia, Wikipedia, the free encyclopedia, http://en.wikipedia.org/w/
index.php?title=Wikipedia&oldid=35397363, [Accessed 16-January-2006].
[13] Zadeh, L. A., Fuzzy sets, Information and Control 8 (1965), pp. 338?353.
Sentential Structure and Discourse Parsing 
Livia Polanyi, Chris Culy, Martin van den Berg, Gian Lorenzo Thione, David Ahn1 
FX Palo Alto Laboratory 
3400 Hillview Ave, Bldg. 4 
Palo Alto, CA 94304 
{polanyi|culy|vdberg|thione}@fxpal.com, ahn@science.uva.nl 
 
 
Abstract1 
In this paper, we describe how the LIDAS 
System (Linguistic Discourse Analysis Sys-
tem), a discourse parser built as an implemen-
tation of the Unified Linguistic Discourse 
Model (U-LDM) uses information from sen-
tential syntax and semantics along with lexical 
semantic information to build the Open Right 
Discourse Parse Tree (DPT) that serves as a 
representation of the structure of the discourse 
(Polanyi et al, 2004; Thione 2004a,b). More 
specifically, we discuss how discourse seg-
mentation, sentence-level discourse parsing, 
and text-level discourse parsing depend on the 
relationship between sentential syntax and dis-
course. Specific discourse rules that use syn-
tactic information are used to identify possible 
attachment points and attachment relations for 
each Basic Discourse Unit to the DPT.  
1 Introduction 
In this paper, we describe discourse parsing under 
the Unified Linguistic Discourse Model (U-LDM) 
(Polanyi et al 2004). In particular, we describe the 
relationship between the output of sentential pars-
ing and discourse processing.  
The goal of discourse parsing under the U-LDM 
is to assign a proper semantic interpretation to 
every utterance in a text. In order to do so, the 
model constructs a structural representation of rela-
tions among the discourse segments that constitute 
a text. This representation is realized as a Dis-
course Parse Tree (DPT). Incoming discourse ut-
terances are matched with the informational con-
text needed for interpretation, and attached to 
nodes on the right edge of the tree.  
                                                     
1 Current address:  
Language and Inference Technology Group,  
Informatics Institute 
Kruislaan 403 
1098 SJ Amsterdam, The Netherlands  
1.1 Discourse Parse Tree 
The U-LDM builds upon the insights and mecha-
nisms of the Linguistic Discourse Model (LDM) 
(Polanyi 1988). The DPT specifies which segments 
are coordinated to one another (bear a similar rela-
tionship to a common higher order construct), 
which are subordinated to other constituents (give 
more information about entities or situations de-
scribed in that constituent, or, alternatively, inter-
rupt the flow of the discourse to interject unrelated 
information), and which are related as constituents 
of logical, language specific, rhetorical, genre spe-
cific or interactional structures (n-ary relations). 
Importantly, the representation identifies which 
constituents are available for continuation at any 
moment in the development of a text and which are 
no longer structurally accessible.  
1.2 Discourse Parsing 
 While full understanding of the meaning of con-
stituent utterances, world knowledge, inference 
and complex reasoning are needed to create the 
correct Discourse Parse Tree to represent the struc-
ture of discourse under the LDM, in developing the 
U-LDM, it became apparent that most of the in-
formation needed to assign structural relations to a 
text can be recovered from relating lexical, syntac-
tic and semantic information in constituent sen-
tences to information available at nodes on the 
DPT. Largely formal methods involving manipu-
lating information in lexical ontologies and the 
output of sentential syntactic and semantic analysis 
are sufficient to account for most cases of dis-
course continuity, and give rise to only limited 
ambiguity in most other cases.2 The assignment of 
correct temporal, personal and spatial interpreta-
tion to utterances, which relies in large part on the 
relative location of referential expression and their 
                                                     
2 Complex default logic based reasoning as in Struc-
tured Discourse Representation Theory (Asher 1993; 
Asher and Lascarides 2003), speculations about the in-
tentions or beliefs of speakers (as in Grosz and Sidner 
(1986)), or the intricate node labeling exercises familiar 
from Rhetorical Structure Theory (Mann and Thompson 
1988; Marcu 1999, 2000) are not necessary. 
referents in the DPT representation of the structure 
of the discourse, can then often be recovered. From 
a computational point of view, this is good news. 
Parsing consists of the following steps for every 
sentence of the discourse. 
1. The sentence is parsed by a sentence level 
parser, in our case the LFG-based Xerox Lin-
guistic Environment (XLE).  
2. The sentence is broken up in discourse relevant 
segment based on the syntactic information 
from the sentence parser.  
3. The segments are recombined into one or more 
small discourse trees, called Basic Discourse 
Unit (BDU) trees, representing the discourse 
structure of the sentence.  
4. The BDU trees corresponding to the sentence 
are each attached to the DPT tree.3  
 
In the remainder of this paper, we will describe 
how the LIDAS System (Linguistic Discourse 
Analysis System), a discourse parser built as an 
implementation of the U-LDM, uses information 
from sentential syntax and semantics along with 
lexical semantic information to build up the struc-
tural representation of source texts4. Specifically, 
we will discuss discourse segmentation, sentence-
level discourse parsing, and text-level discourse 
parsing?phases of the discourse parsing process 
that depend on the relationship between sentential 
syntax and discourse.  
2 Discourse Segments and Basic Discourse 
Units 
U-LDM discourse segmentation is based on the 
syntactic reflexes of the semantic content of the 
linguistic phenomena making up discourse. Since 
elementary discourse units must be identified to 
build up discourse structure recursively, discourse 
segments under the U-LDM are identified as the 
syntactic units that encode a minimum unit of dis-
course function and/or meaning that must be inter-
preted relative to a set of contexts to be under-
stood. Minimal Functional units include greetings, 
connectives, discourse PUSH/POP markers and 
other ?cue phrases? that connect or modify content 
segments. Minimal meaning units are units that 
express information about not more than one event, 
                                                     
3 If a sentence is sufficiently complex, it may consist 
of two or more completely different independent dis-
course units. Such cases are treated as if the two parts of 
the sentence were really two different sentences. One 
consequence of this is that a syntactic coordination of 
two sentences may correspond to a subordination in the 
DPT, because they are treated independently. 
4 The LiveTree environment for discourse parsing is 
described in detail in Thione 2004b. 
event-type or state of affairs in a possible world. 
Roughly speaking they are ?elementary proposi-
tions? or ?event-type predicates? corresponding in 
(neo-) Davidsonian semantics to an elementary 
statement that contains at most one event-
quantifier. Structurally, a minimum meaning unit 
does not contain a proper subpart that itself com-
municates content and has a syntactic correlate that 
can stand on its own.  
Under the U-LDM, segments can be discontinu-
ous (if there is overt material on both sides of an 
intervening segment) or fragmentary. A single 
word answer to a question is a complete segment, 
whereas the same word uttered in an incomplete 
and unrecoverable phrase is a fragment. 
The U-LDM defines segmentation in purely sen-
tence syntactic terms. However, the choices of 
definitions are motivated by semantic considera-
tions. For example, since auxiliaries and modals do 
not refer to events distinctly from the main verb, 
they do not form segments separate from the corre-
sponding main verbs. By the same reasoning, other 
modal constructions that involve infinitives (e.g. 
have to, ought to, etc.) also constitute a single 
segment with their complements as do Cleft con-
structions, despite the presence of two verbs.7 On 
the other hand, Equi verbs (e.g., try, persuade) and 
Raising verbs (e.g., seem, believe, etc) form sepa-
rate BDUs from their verbal complements, since 
both eventualities can be continued. In contrast, 
even though event nominals, including gerunds, 
refer to eventualities (possibly) distinct from the 
verbs of which they are arguments or adjuncts, 
                                                     
5 For example, ?Chris served the chapter | as Social 
Chairman? 
6 For example: ?Finally, | the audio is summarized.? 
7 For example: ?It is segmentation that we are dis-
cussing.? 
Content segments  
(BDUs) 
Simple clauses, Subordinate 
clauses and participial phrases, 
Secondary predications,5 Inter-
polations (e.g. parentheticals, 
appositives, interruptions, etc.), 
Fragments (e.g. section head-
ings, bullet items, etc.) 
Operator segments Conjunctions that conjoin seg-
ments, Discourse operators 
(e.g. ?scene-setting? preposed 
modifiers)6Discourse connec-
tives 
Table 1: Examples BDU and Operator Seg-
ments. 
those eventualities can not (easily) be continued 
and therefore are not segments.8 
The U-LDM, in contrast to other discourse theo-
ries, has a fine-grained taxonomy of minimal units. 
The most prominent distinction of discourse seg-
ments is between Basic Discourse Units (BDUs) 
and Operator segments (see Table 1). BDUs are 
segments realized in a linguistic utterance that can 
independently provide the context for segments 
later in the discourse. Operator segments can not 
do so, they can only provide context for segments 
in their scope. 
In the following section, we explain more fully 
how sentence syntax is used to segment sentences 
into discourse segments.  
2.1 The role of the XLE in discourse segmen-
tation 
Dividing a text into discourse segments begins 
by applying a sentence breaking algorithm to de-
termine the tokens to be segmented. These tokens 
are normally complete sentences, but may also be 
fragments in some cases such as titles or elliptical 
phrases such as ?Yes?. The tokens are processed 
by a discourse segmenter. After the segmenter has 
completed its work, segments are passed to a U-
LDM BDU-Tree parser, which constructs one or 
more BDU-Trees from the BDUs identified in each 
sentence.  
The LIDAS segmenter first sends the input 
chunk to be parsed by the Xerox Linguistic Envi-
ronment parser (XLE) (Maxwell and Kaplan 
1989). The XLE is a robust Lexical Functional 
Grammar (LFG) parser. The XLE tries to parse the 
input, and returns either a packed representation of 
all possible parses or the most probable parse as 
selected by its stochastic disambiguation compo-
nent (Riezler et al 2002). If the XLE can not find a 
parse of the input as a complete sentence it tries to 
construct a sequence of partially parsed fragments.9 
The LIDAS segmenter segments the most probable 
parse and, as a backup, the first parse from the 
packed representation, if the first and the most 
probable parse differ. 
                                                     
8 Note that the contrast between modal verbs on the 
one hand and Raising and Equi verbs on the other 
clearly shows that the surface form of a phrase (e.g., the 
infinitival complements) is not always sufficient to de-
termine segment status. Similarly, a finite verb is not a 
sufficient indicator of a segment, as seen in the cleft 
constructions. Crucially, and appropriately, we need to 
refer to the discourse property of potential continuation. 
9 The XLE has a failure rate (i.e., no parse whatso-
ever) of approximately 0.5% on our corpus of technical 
reports. 
The parse information consists of a c(onstituent) 
structure (essentially a standard parse tree) and a 
f(unctional) structure containing predicate-
argument information, modification information, 
and other grammatical information such as tense 
and agreement features. F-structures make up the 
primary source of linguistic information used in 
discourse parsing.  
To identify the discourse segments within the 
sentence, seven syntactic configurations in c-
structure and f-structure are examined. The rela-
tively small set of configurations in Table 2 ac-
counts for the full range of discourse segments. 
According to these segmentation rules, it is pos-
sible for a segment to be embedded in another 
segment. Because on the tree-projection of U-
LDM structures terminal nodes represent a con-
tiguous textual span, we recombine the two parts 
of a non-contiguous segment in the BDU-tree (sec-
tion 4) and DPT (section 5) using the concatena-
tion relation (+). Concatenated nodes contain the 
complete f-structure information of the completed 
segment and are full-fledged BDUs, available for 
further ?anaphoric anchoring?. 
All segments are returned to the discourse parser 
in an XML format, which includes the f-structure 
information as well as the textual spans for each 
sentential segment. 
3 Discourse Parsing 
The next step after segmentation is combining the 
segments into a BDU-tree according to the rules of 
discourse parsing, resulting in a discourse tree rep-
resenting the sentence. After that, the BDU-tree is 
                                                     
10 F-structures with SUBJ (subject) attributes are con-
sidered possible discourse segments since they typically 
encode an eventuality. However, because they do not 
introduce independent anchor points for future attach-
ment, the f-structures corresponding to modal and auxil-
iary verbs are excluded. 
Content Segments 
Certain F-structures with subjects10 
Fragments 
Parentheticals 
Headers 
Syntactic coordination (except conjunct itself)  
Operator segments 
Conjuncts in coordination 
Initial comma separated modifiers 
Subordinating conjunctions 
Table 2. Segment classification configura-
tions. 
combined with the Discourse Parse Tree, again 
according to the discourse parsing rules.  
In discourse parsing, units of discourse are at-
tached to an emergent discourse tree. Attachment 
always takes place on the right edge of the tree. 
The parser has to make two decisions: what unit to 
attach to and what relationship obtains between the 
incoming unit and the selected attachment site. The 
types of evidence are used to determine this in-
clude: 
 
? syntactic information 
o subordinate/complement relations, parallel 
syntactic structure, topic/focus and centering 
information, syntactic status of re-used lex-
emes, preposed adverbial constituents, etc.  
? lexical information 
o Information from lexical ontology: re-use of 
same lexeme, synonym/antonym, hypernym, 
participation in the same lexical frame as 
well as specific discourse connectives tem-
poral and adverbial connectives indicating 
set or sub-set membership of any type for 
example, specifically, alternatively11. 
o Modal information: realis status, modality, 
genericity, tense, aspect, point of view12),  
? Structural information of both the local at-
tachment point and of the BDU-tree 
? Presence of constituents of incomplete n-ary 
constructions on the right edge 
o questions, initial greetings, genre-internal 
units such as sections and sub-sections, etc. 
 
The combined weight of the evidence determines 
the attachment point and the attachment relation. 
Interestingly, the weight given to each type of in-
formation is different for attachment site selection 
and relationship determination. Lexical ontological 
information, for example, is generally more impor-
tant in determining site, while semantic, syntactic 
and lexical ?cue? information is more relevant in 
determining relationship. 
In Polanyi et al 2004, a small set of robust rules 
is given for determining the attachment site and 
relationship of an incoming BDU-tree to the exist-
ing parse tree of the discourse. In the present pa-
per, which has as its focus understanding the rela-
tionship of sentential syntax to discourse structure, 
we concentrate on describing some fundamental 
                                                     
11 These expressions are used as input to specific 
lexically driven rules that indicate language or genre 
specific binary relations between BDUs as suggested by 
Webber, Joshi and colleagues in their work on D-
LTAGs. (Webber and Joshi, 1998; Webber, Knott and 
Joshi, 1999; Webber, Knott, Stone and Joshi 1999.) 
12 See discussion Wiebe, 1994. 
aspects of the relationship between the sentential 
syntactic structure of an incoming sentence and its 
corresponding sentential discourse structure. 
3.1 The Sentential Syntax - Discourse Struc-
ture Interface 
In discourse parsing with LIDAS, the output of 
the XLE parser supplies functional constraints and 
roles for syntactic structures. The syntactic struc-
ture of a sentence accounts for the discourse-
relevant relationships between segments within a 
sentence. LIDAS grammars exploit this informa-
tion by mapping syntactic relations onto discourse 
relations. The LIDAS grammar formalism permits 
the parser to leverage the XLE?s output by (1) 
checking positive or negative constraints (equality 
or inequality operators) (2) recursively searching f-
structures for specified attributes (* and ? wild-
cards), (3) enforcing dependent constraints,13 and 
(4) using Boolean connectives to combine con-
straints. (5) applying constraints universally or ex-
istentially to the set of matching f-structures. While 
LIDAS grammar rules can incorporate constraints 
that operate on all supported types of linguistic 
evidence, Table 3 gives three examples of rules 
that specify attachments based on the syntactic re-
lationship between constituents. 
 
1. 
BDU-1,BDU-2:  
BDU-1/phi = BDU-2/ADJUNCT/link;  
 ? Right-Subordination. 
2. 
BDU-1,BDU-2:  
BDU-1/*/ADJUNCT/link = BDU-2/phi;  
 ? Subordination. 
3. 
BDU-1,BDU-2:  
BDU-1/*/{XCOMP|COMP|COMP-EX}/link 
  = BDU-2/phi;  
 ? Context. 
Table 3: Rules based on syntactic relationship. 
 
Rule 1 captures one general case for preposed 
modifiers. Prepositional and adverbial phrases, 
often temporal modifiers, that precede the main 
clause they modify can either elaborate on the 
main clause or modify the context in which the 
main clause is interpreted. Lexical information is 
used to distinguish among different types of modi-
fiers. Rule 2 expresses the general case of subordi-
nate adjunct clauses and shows that the syntax for 
                                                     
13 For example, the following constraints show a de-
pendency between (1) and (3),  
(1) BDU-1/(*)/ADJUNCT/link = BDU-2/phi; 
(2) BDU-2/ADJUNCT-TYPE = ?relative?; 
(3) $1/SPEC/DET/DET-TYPE = "indef"; 
One or more sub-f-structures that match the wildcard 
in the first constraint are tested in the third constraint. 
This constraint is part of a rule of sentential discourse 
syntax used to identify non-restrictive relative clauses. 
LIDAS? discourse rules allows for recursive search 
over f-structures by seeking adjunct phrases match-
ing the incoming BDU anywhere within the f-
structure of the attachment point. Rule 3, which 
shows a compact syntax for disjunctive constraint, 
builds a sentence level discourse relation, the Con-
text Binary, that forms a complex context unit 
from its child constituents. Context Binaries are the 
general case for clausal complementizers. 
In the next section, we discuss one of the basic 
discourse parsing rules. 
3.2 Discourse Subordination  
We will assume the following extended hierarchy 
of grammatical functions:14 PRED > SUBJ > OBJ 
> COMP > ADJUNCT > SPEC.15 Given this hier-
archy we propose as a general principle of dis-
course construction that promotion in the hierar-
chy means demotion in the discourse. For exam-
ple, if the SUBJ of the incoming unit of discourse 
refers to the same entity16 as OBJ at the attachment 
node, then the relationship between them will be a 
subordination. In general, if an expression with 
grammatical function GF in a BDU B refers to the 
same (or a subcase of the same) entity as an ex-
pression with grammatical function GF? in an ac-
cessible antecedent BDU A, where GF > GF?, then 
B will be attached as a subordinate structure to A 
on the DPT. This principle is expressed as a rule in 
the grammar that fires if it is not superseded by 
other rules. For example, the Narrative rule, which 
coordinates event clauses, takes precedence over 
the Discourse Demotion Rule. 
If the grammatical function hierarchy rule does 
not apply, but the BDU refers to a subclass17 of the 
antecedent BDU, there is evidence for a subordina-
tion relation. For example, if the subject of the 
BDU stands in a part-of relationship with the sub-
ject of the antecedent BDU, we can conclude that 
the relationship is a subordination. 
                                                     
14 PRED denotes the tensed verb. It plays a role in the 
following discussion because verbs can be nominalized. 
15 For the purposes of this hierarchy, grammatical 
function COMP includes the features COMP-EX and 
XCOMP. An element inside an expression with 
a grammatical function GF is itself in that position with 
respect to the elements that are not in that expression, 
although a separate ordering might exist between ele-
ments within the same expression. C.f. Obliqueness 
command in HPSG (Pollard and Sag, 1994). 
16 In LIDAS, no reference resolution is done. Identity 
of reference is approximated using lexical semantics. 
17 The notion of subcase as used here covers a num-
ber of different notions. An expression e is a subcase of 
f if (1) e is a set that is a subset of f, (2) e is a subtype of 
f, or (3) e is a part of f, among other relations. 
Table 4 gives the interaction of this rule with the 
hierarchy rule and the resulting relationships: G 
denotes whether a shift in the grammatical function 
hierarchy occurred, and L whether the shifted ele-
ment refers to the same entity, part of that entity or 
to an entity that is larger. If more than one such 
expression can be found, we consider the expres-
sion in the incoming unit with the highest gram-
matical function. 
 
 G+ G0 G- 
L+  S/C18 C S 
L0 S C N/A 
L- S S N/A 
Table 4. Interactions of the hierarchy 
and subcase rules. 
The table is read as follows: take the expressions in 
the incoming unit that have a relationship with an 
expression in the attachment point. Let e be the 
expression among these that has the highest gram-
matical function, and f be the corresponding ex-
pression in the attachment point. If the grammati-
cal function of e is higher than that of f, we write 
G+, if it is the same G0, if it is lower G-. Similarly, 
if e is a supercase of f, we write L+, if e and f refer 
to the same entity L0, and if e is a subcase of f, L-. 
For example 
 
1. U1: The man was wearing a coat. 
U2: The Burberry looked brand new. 
 
In this case we notice that the words coat and 
Burberry are such that L<coat, Burberry> = L- 
and we also notice that Burberry gets promoted to 
the subject of the incoming unit, while coat was 
the direct object of U1. Therefore G<coat, Bur-
berry> = G+. From Table 4 we correctly identify 
that U2 does indeed subordinate on U1. 
Both L- and G+ give evidence for discourse sub-
ordination. Grammatical function demotion G- is a 
less clear case.19 Some mixed combinations sug-
gest discourse coordination (as for the preservation 
case <L0 , G0>) while others contribute too little 
                                                     
18 Semantics would help to disambiguate this case. If 
the antecedent f is more specific than the anaphoric 
element e, two cases are possible. Either e is used as a 
definite description referring to the same entity as f, in 
which case the relation is a coordination, or e is used to 
denote a larger class of entities than f, in which therela-
tion is likely to be a subordination. 
19 As is understanding precisely what is involved 
from a discourse relation point of view with complex, 
mixed promotion/demotion phenomena (from the sub-
ject of an XCOMP to the adjunct of an OBJ, for exam-
ple) 
significant information to independently determine 
the discourse attachment (N/A cases). In those 
cases, evidence from other rules in the grammar 
determines the result.  
4  BDU-Trees 
Identifying the relationship of a BDU to the dis-
course in which it occurs is a complex parsing 
process involving lexical, semantic, structural and 
syntactic information. The first step in understand-
ing the relationship of a given BDU to the extra-
sentential discourse context is to understand the 
role a BDU plays within the sentence in which it 
occurs. As a first step of constructing a discourse 
parse-tree of the sentence, the XLE parse and sen-
tential discourse rules are used to identify relation-
ships between the BDUs within the sentence re-
sulting in one or more BDU-trees. These small 
sentence-level discourse trees consist of one main 
clause and its subordinated clauses or preposed 
modifiers.20 
The root node of a BDU-tree represents the non-
subordinated material (often referred to as active 
material) within that BDU-tree, including at least 
the information of the main clause. The projection 
of the root node on the active leaves is referred to 
as the M-BDU (Main BDU). Only syntactic infor-
mation from the M-BDU is used to attached the 
BDU-tree to the DPT. In general, a sentence yields 
as many BDU-trees as top-level coordinated 
clauses.  
Consider, Example 2, a sentence taken from our 
corpus of Technical Reports: 
 
2. As a consequence, any hypertext link followed 
opened a new browser window, which we think of 
as a "Rabbit Hole" because the new window indi-
cates to users that they are no longer navigating in-
side the slideshow, but are instead navigating the 
Web. 
 
The U-LDM segmentation of this sentence:  
 
As a consequence | any hypertext link | followed | 
opened a new browser window | which we think of as a 
"Rabbit Hole" | because | the new window indicates to 
users | that they | are no longer navigating inside the 
slideshow | but | are instead navigating the Web. 
                                                     
20 BDU-trees are very similar to the D-LTAG dis-
course segments (D-LDSs). However, BDU-trees in-
clude subordinated material, so they are typically larger 
than D-LDSs. Furthermore, because U-LDM sentence 
and discourse grammars are different, two BDU-trees 
may stand in a coordinated relationship in sentence 
grammar and be subordinated on the discourse level (cf. 
example 1). 
 
For compound sentences, members of the top-
level coordinate structures are attached independ-
ently, reflecting the fact that top-level coordinated 
clauses can escape the boundaries of the sentence 
when attaching to the main discourse structure. For 
example, the discourse parse of the following pas-
sage, illustrated in Example 3, consists of two sen-
tences, five segments, four BDUs and three BDU-
Trees that eventually form one DPT. 
 
3. S1: B1: The man soaked himself in the water.  
S2: B2: It was warm and soothing B3 and he 
decided to linger a little longer than usual.  
 
Despite the apparent syntactic coordination be-
tween the two main clauses, the two BDU-Trees in 
S2 show the independence of BDU-Tree/DPT at-
tachments. B1 describes a punctual event on a 
main story line. B3 describes the next event. The 
semantic and aspectual information of the verb to 
soak in BDU-1 and of the copula in BDU-2 com-
municates a switch from an event-line to an em-
bedded elaboration. The syntactic promotion of 
water from object of the preposition in the adjunct 
phrase in the water to the subject (through ana-
phoric reference) in the next segment indicates dis-
course subordination. Given L0<water, it> and 
G+<ADJUNCT/OBJ/PRED, SUBJ/PRED>, we 
find from table X, <L0 ,G+> ? S. As a conse-
quence, B2 is subordinated to B1. In the DPT, B3 
is coordinated with B1 at a point above B2, despite 
being syntactically coordinated to it at the senten-
tial level.  
5 Global Discourse Construction 
BDU-trees are attached to the DPT of the entire 
discourse as single units by computing the rela-
tionship between their M-BDUs and the accessible 
nodes aligned along the right edge of the DPT. 
Rules of discourse attachment that include those 
discussed for BDU construction as well as other 
genre and structural level rules are used in global 
DPT construction. 
The parsing process at the Discourse Parse Tree 
(DPT) level works as follows. Once BDU-Trees 
have been constructed and are ready to be attached 
to the DPT, each node along the right edge is ex-
amined, and, through a set of discourse rules, an 
ordered set of active Discourse Constituent Units 
(DCUs) is produced, representing possible attach-
ment points.21 This set can then be pruned of its n 
                                                     
21 Lexical information is the main source of evidence 
in attachment site determination while other sources 
contribute to a lesser degree. The opposite is true for 
determining attachment relations 
lowest scoring constituents, according to a preset 
threshold or other criteria. 
 
Example 
Our group is developing new techniques for helping 
manage information for enhanced collaboration. We 
explore solutions for seamlessly connecting people to 
their personal and shared resources. Our solutions in-
clude services for contextual and proactive information 
access, personalized and collaborative office appli-
ances, collaborative annotation, and symbolic, statisti-
cal, and hybrid processing of natural language. 
Our team includes researchers with diverse back-
grounds including: ubiquitous computing, computer-
supported collaboration, HCI, IR , and NLP. 
Segmented Text 
1. Our group is developing new techniques for helping 
2. manage information for enhanced collaboration.  
3. We explore solutions for seamlessly connecting 
people to their personal and shared resources.  
4. Our solutions include services for contextual and 
proactive information access, personalized and col-
laborative office appliances, collaborative annota-
tion, and symbolic, statistical, and hybrid process-
ing of natural language. 
5. Our team includes researchers with diverse back-
grounds  
6. including:  
7. ubiquitous computing, computer-supported col-
laboration, HCI, IR , and NLP. 
Rules 
1-2 Intrasentential XCOMPS --> CX 
CX(1,2)-3 Demotion 2  
 [ Pres. Progressive to Simple Pre-
sent] 
 [ Our group --> We ] 
 [ Same Verb Class ] --> S 
3-4 Promotion  
 [Solutions from OBJ to SUBJ] --> S 
5-6 Relative Adjunct with Null Deter-
miner --> S 
6-7 Colon + OBJ linked to NP segment 
 --> CX 
CX(1,2)-5 Synonym Subjects, Same 
Tense --> C22 
Table 5. Analyzed Webpage Example 
In the second stage, attachment rules are checked 
against possible attachment sites. Rules that fire 
successfully attach the BDU-Tree to the DPT at the 
chosen site with the relationship specified by the 
rule. Local semantic, lexical and syntactic informa-
tion is then percolated into the parent DCU. If mul-
tiple attachments at different sites are possible, 
                                                     
22 Discourse parsing is not unambiguous and different 
analyses may apply. So, here the same subject, change 
in progressive feature, and different verb class, which 
also apply would create an S. Future research is needed 
to understand precisely which rules take precedence. 
ambiguous parses are generated; less preferred at-
tachments are discarded and the remaining attach-
ment choices generate valid parse trees. 
Once a BDU-tree has been attached, its leaves 
become terminal nodes of the DPT and nodes on 
its right edge are accessible for continuation and 
may serve as contextual anchors for subsequent 
sentences. Table 5 shows an example text taken 
from the FXPAL Webpage describing our research 
group, a segmentation of the text and the DPT con-
structed following the rules. Figure 1 gives a s-
creenshot of the resulting tree. 
 
 
Figure 1: Tree of Analyzed Webpage Example 
6 Comparison with D-LTAG 
LIDAS is a purely symbolic discourse parser most 
similar to the D-LTAG parser described in (Forbes 
et. al. 2003). The overall structures of the LIDAS 
and D-LTAG parsers are almost identical. There 
are a number of apparently significant differences 
between the underlying theories although some of 
these may turn out to be notational variants after 
more extensive analysis. 
An important difference between D-LTAG and 
U-LDM derives from the fundamental question of 
segmentation; D-LTAG segments are larger than 
our segments, corresponding more or less to BDU-
trees (but cf. footnote 20). In D-LTAG, because 
only one grammar formalism governs both dis-
course and sentence level parsing, continuation can 
also take place on parts of segments as defined by 
sentence-level syntactic relations. Under the U-
LDM, which employs independent sentential and 
discourse grammars, only segments are potential 
anchors for continuation. Not only because we use 
an external syntactic parser as an oracle that in-
forms segment attachment on the BDU-tree, but 
more importantly because sentential syntax can be 
overridden by discourse syntax in some cases. 
Another basic difference between the two ap-
proaches is that D-LTAG builds its initial and aux-
iliary trees around connectives. Every discourse 
relation is expressed by a, possibly empty, connec-
tive. In U-LDM, connectives give evidence about 
the possible discourse relations, as do other parts 
of the sentence, but they do not solely determine 
discourse relation. We can thus account correctly 
for cases in which the wrong connective is selected 
to express a semantic relationship among seg-
ments.  
Lastly, our parser is meant to be incremental at 
the discourse level, whereas the D-LTAG parser 
appears to operate on the discourse as a whole. 23 
7 Conclusion 
In this paper we described a novel approach to dis-
course segmentation and discourse structure analy-
sis that unifies sentential syntax with discourse 
structure and argued that most of the information 
needed to assign a structural description to a text is 
available from sentential syntactic parsing, senten-
tial semantic analysis and relationships among 
words captured in a lexical ontology such as 
WordNet. The U-LDM discourse rules and parsing 
strategies presented here are a first step. We have 
tested out these rules in analyzing a corpus of over 
300 Technical Reports that have been summarized 
under the PALSUMM System that operates on top 
of LIDAS. (Polanyi et al2004; Thione et al2004) 
Much work remains to be done. Understanding the 
complex inter-relationships between rules is a for-
midable task. Critically important, too, is to unify 
the semantically motivated structural analyses pre-
sented here with an explicit S-DRT type formal 
semantic account of discourse semantics. How-
ever, we believe that the results presented here rep-
resent an important advance in understanding the 
nature of natural language texts. 
8 References 
Nicholas Asher. 1993. Reference to Abstract Objects in 
English: A Philosophical Semantics for Natural Lan-
guage Metaphysics. Kluwer Academic Publishers. 
Nicholas Asher and Alex Lascarides. 2003. Logics of 
Conversation. Cambridge University Press. 
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, 
Anoop Sarkar, Aravind Joshi and Bonnie Webber. 
2003. D-LTAG System - Discourse Parsing with a 
Lexicalized Tree-Adjoining Grammar, Journal of 
Language, Logic and Information, 12(3). 
Barbara Grosz and Candace Sidner. 1986. Attention, 
Intention and the Structure of Discourse. Computa-
tional Linguistics 12:175-204. 
William C. Mann and Sandra A. Thompson. 1988. Rhe-
torical Structure Theory: Towards a Functional The-
ory of Text Organization. Text 8(3)243-281. 
                                                     
23 In the current LIDAS implementation, we do not 
represent ambiguity directly, but implement a greedy 
parsing algorithm with backtracking. The non-locality 
of the D-LTAG parser as described in Forbes et. al. 
2003 may likewise be a consequence of their current 
implementation. 
Daniel Marcu. 1999. Discourse trees are good indicators 
of importance in text. In Advances in Automatic Text 
Summarization. I. Mani and Mark Maybury (eds), 
123-136, The MIT Press. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press. 
Cambridge, MA. 
John Maxwell and Ronald M. Kaplan. 1989. An over-
view of disjunctive constraint satisfaction. In Pro-
ceedings of the International Workshop on Parsing 
Technologies, Pittsburgh, PA. 
Livia Polanyi. 1988. A Formal Model of Discourse 
Structure. Journal of Pragmatics 12: 601-639. 
Livia Polanyi. 2004. A Rule-based Approach to Dis-
course Parsing. In Proceedings of SIGDIAL ?04. 
Boston MA. 
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, 
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a 
Lexical-Functional Grammar and discriminative es-
timation techniques. In Proceedings of the 40th An-
nual Meeting of the Association for Computational 
Linguistics (ACL?02), Philadelphia, PA. 
Stefan Riezler, Tracy H. King, Richard Crouch, and 
Annie Zaenen. 2003. Statistical Sentence Condensa-
tion using Ambiguity Packing and Stochastic Disam-
biguation Methods for Lexical-Functional Grammar. 
In Proceedings of HLT-NAACL'03, Edmonton, Can-
ada.  
Radu Soricut and Daniel Marcu. 2003. Sentence Level 
Discourse Parsing using Syntactic and Lexical In-
formation. In Proceedings of HLT/NAACL?03, May 
27-June 1, Edmonton, Canada 
Thione, Gian Lorenzo, Martin van den Berg, Chris Culy 
and Livia Polanyi. 2004a. Hybrid Text Summariza-
tion: Combining external relevance measures with 
Structural Analysis. Proceedings ACL Workshop 
Text Summarization Branches Out. Barcelona. 
Thione, Gian Lorenzo, Martin van den Berg, Chris Culy 
and Livia Polanyi. 2004b. LiveTree: An Integrated 
Workbench for Discourse Processing. Proceedings 
ACL Workshop on Discourse Annotation. Barcelona. 
Bonnie Webber and Aravind Joshi, 1998. Anchoring a 
lexicalized tree-adjoining grammar for discourse. 
COLING/ACL Workshop in Discourse Realtions and 
Discourse Markers. Montreal, Canada. 86-92. 
Bonnie Webber, Alistair Knott and Aravind Joshi. 
1999a. Multiple discourse connectives in a lexical-
ized grammar for discourse. In 3rd Int?l Workshop on 
Computational Semantics. Tilburg. 309-325. 
Bonnie Webber, Alistair Knott, Matthew Stone and 
Aravind Joshi. 1999b. Discourse Relations: A Struc-
tural and Presuppositional Account Using Lexical-
ized TAGS. 37th ACL. College Park, MD. 41-48. 
Wiebe, Janyce M. 1994. Tracking point of view in 
narrative. Computational Linguistics 20 (2): 233-287. 
 A Rule Based Approach to Discourse Parsing 
Livia Polanyi, Chris Culy, Martin van den Berg, Gian Lorenzo Thione, David Ahn1 
FX Palo Alto Laboratory 
3400 Hillview Avenue, Bldg 4 
Palo Alto CA 94304 
{polanyi|culy|vdberg|thione}@fxpal.com ; ahn@science.uva.nl 
 
 
Abstract 
In this paper we present an overview of 
recent developments in discourse the-
ory and parsing under the Linguistic 
Discourse Model (LDM) framework, a 
semantic theory of discourse structure. 
We give a novel approach to the prob-
lem of discourse segmentation based 
on discourse semantics and sketch a 
limited but robust approach to sym-
bolic discourse parsing based on syn-
tactic, semantic and lexical rules. To 
demonstrate the utility of the system in 
a real application, we briefly describe 
the architecture of the PALSUMM sys-
tem, a symbolic summarization system 
being developed at FX Palo Alto Labo-
ratory that uses discourse structures 
constructed using the theory outlined 
to summarize written English prose 
texts. 1 
1 Introduction 
In this paper we present an overview of recent 
theoretical and computational developments in 
discourse theory and parsing under the Linguistic 
Discourse Model (LDM) framework, a semantic 
account of discourse structure. In Section 2, we 
                                                          
1 Current address:  
Language and Inference Technology Group,  
ILLC, University of Amsterdam,  
Nieuwe Achtergracht 166,  
1018 WV Amsterdam, The Netherlands 
present an overview of what we will term the 
Classical LDM (C-LDM) and identify critical 
problems encountered in implementing the 
model: the difficulty in segmenting complex 
sentences within a text and calculating the at-
tachment site and relationship of an incoming 
unit to an appropriate node in a developing Dis-
course Tree. In Sections 3 and 4 we introduce the 
Unified Linguistic Discourse Model (U-LDM) 
that incorporates solutions to these problems. 
Specifically, in Section 3 we describe a novel 
approach to discourse segmentation based on the 
relationship of sentential syntax to discourse 
semantics. In Section 4, a limited but robust ap-
proach to symbolic discourse processing based 
on syntactic, semantic and lexical rules is given. 
In Section 5, we sketch the architecture of the 
PALSUMM system, a summarization system 
being developed at FX Palo Alto Laboratory that 
uses algorithms operating on discourse represen-
tations generated by a U-LDM parser to summa-
rize written English prose texts. In Section 6 we 
present our conclusions and suggest directions 
for future research. 
2 The Classical Linguistic Dis-
course Model (C-LDM) 
Unlike the Discourse Structures Model (DSM) of 
Grosz and Sidner (1986), a pragmatic and psy-
chological theory that aims to clarify the rela-
tionship between speakers? intentions and their 
focus of attention in discourse, or the rhetorical 
model of Rhetorical Structures Theory (Mann 
and Thompson, 1988) that is designed to identify 
the coherence relations between segments of 
text, the Linguistic Discourse Model (LDM) 
(Polanyi and Scha, 1984; Polanyi, 1988; Polanyi 
 and van den Berg, 1996) is a syntactically in-
formed, semantically driven model developed to 
provide proper semantic interpretation for every 
utterance in a discourse despite the apparent dis-
continuities that are present even in well struc-
tured written texts. In its focus on understanding 
discourse meaning, the LDM is close in spirit to 
Structured Discourse Representation Theory (S-
DRT) (Asher, 1993). While S-DRT attempts to 
account for discourse structure purely semanti-
cally, the LDM framework is concerned to main-
tain a separation between discourse ?syntactic? 
structure, on the one hand, and discourse inter-
pretation on the other. Therefore, like DSM and 
RST, the LDM incorporates an explicit tree 
structured model of relationships between dis-
course segments as its model of discourse ?syn-
tax?. In discourse parsing under the LDM, any 
attachment to the developing discourse tree of a 
textual unit is treated as an instruction to update 
an appropriate semantic representation. We con-
struct dynamic semantic representations (DSRs), 
similar to the Discourse Representation Struc-
tures (Kamp, 1981; Kamp and Reyle, 1993) used 
in S-DRT as its model of discourse semantics. 
The DSRs correspond to the contexts relative to 
which subsequent segments can be interpreted.  
The analysis of intra-sentential structure is 
done by sentential syntax which identifies the 
syntactic and semantic structures within the sen-
tence and makes the resulting analysis available 
for discourse processing. 
2.1 Overview of the Classic LDM 
In the Linguistic Discourse Model (LDM) dis-
course is formed through the recursive combina-
tion of discourse constituent units (DCUs). The 
structure of a discourse is represented by an open 
right tree of DCUs. Basic discourse units 
(BDUs), resulting from a segmentation of the 
discourse according to rules of discourse seg-
mentation, form the content of the leaves of the 
tree. Once a text has been segmented into BDUs, 
an open right tree representing the structure of 
the discourse is built up. The completed tree 
shows, for any given point in the discourse, 
which discourse units (DCUs) remain available 
for continuation and which DCUs are no longer 
available. Because discourse anaphora resolution 
is critically constrained by discourse structure, 
the tree representation makes clear the domain in 
which the antecedent for a given anaphoric refer-
ential expression is to be found. Antecedents 
must be available at a node along the right edge 
of the discourse tree. (Polanyi, 1985; Grosz and 
Sidner 1986; Webber, 1991) 
The LDM posits three structural relations be-
tween discourse units: 
 
1. discourse coordination  
a. Units related by bearing a similar rela-
tionship to an existing or newly formed 
common parent in the tree (lists, narra-
tives).  
b. Available at the C-node is information 
common to all child nodes. 
2. discourse subordination  
a. Units related by an elaboration relation-
ship in which the subordinated unit pro-
vides more information about an entity 
or situation described in the subordinat-
ing unit. 
b. Units unrelated to existing units avail-
able on the right edge of the tree, 
viewed as intrusions or interruptions.  
c. Available at the S-node is information 
specific only to the subordinating or 
dominant constituent (usually the left 
child). 
3. n-ary constructions 
a. Units related by logical or rhetorical, 
genre or interactional conventions spe-
cific to a given language. 
b. Preposed modifier, sentence initial ad-
verbial, ?cue word?, (reported speech) 
attribution phrase. 
c. Available at N-nodes is information 
about each constituent and the relation-
ship connecting them. 
 
Although we believe that the general approach to 
discourse structure captured by the Classical 
LDM is essentially sound, there are three critical 
problems with the existing framework:  
 
1. Segmenting the incoming text into BDUs 
2. Determining the existing or new node at 
which to attach an incoming BDU  
3. Determining the relationship between the 
incoming BDU and the attachment node 
 
Although very difficult challenges associated 
with each of these discourse parsing tasks re-
main, in developing the Unified Linguistic Dis-
course Model (U-LDM) we have made 
significant progress recently on solving them. 
These are discussed in Sections 3 and 4 below. 
3 Discourse Segmentation  
The problem of segmenting discourse into the 
elementary units appropriate for building up the 
 structure of the discourse is an extremely diffi-
cult one. Each discourse theory must specify 
how ?segments? should be identified in light of 
the questions the theory is set up to answer. 
Models based on Grosz and Sidner?s 1986 work, 
especially those which form the basis of spoken 
language systems, define segments in terms of 
the intentions of the speaker: when the speaker?s 
intention shifts, the segment associated with that 
intention ends and immediately following talk is 
included in new (or resumed) segments. While 
very useful in dealing with task oriented talk 
where speakers move between asking questions, 
informing others and giving commands, this 
model is less applicable to determining discourse 
segments within a sentence. The problem is an 
acute one for the analysis of written texts be-
cause often a subsequent, not necessarily adja-
cent, segment will continue the development of 
material introduced in a sub-sentential, often 
subordinate, constituent. Construction of the 
appropriate representation of the rhetorical or 
semantic structure of discourse must therefore 
keep sub-sentential units available for attach-
ment at independent nodes on the tree along. The 
entire sentence or sentential main constituent 
must also be available to be continued after any 
continuation on sub-sentential units has been 
completed. As reported by Carlson et al (2003), 
under RST2, lexical and syntactic information 
used to segment discourse into Elementary Dis-
course Units (EDUs) is based on verbal constitu-
ents including clauses and infinitives.3  
As we show below, the approach taken to 
segmentation under the U-LDM, while it in-
cludes as segments (and non-segments) many of 
the constructions currently used in RST, pro-
vides a rationalization for the choice of units. 
Rather than posit which syntactic objects func-
tion as discourse segments, we started by estab-
lishing the semantic basis for functioning as a 
segment and then identified which syntactic con-
structions carry the semantic information needed 
for discourse segment status. We then identified 
as Basic Discourse Units (BDUs) segments that 
have the potential to independently establish an 
anchor point for future continuation. We then 
drew a further distinction between BDUs as a 
class of syntactic structures with the potential to 
                                                          
2  Under S-DRT, no explicit structural tree is 
constructed and no explicit segmentation criteria 
have been proposed in the literature. 
3 Although some clauses are not treated as ele-
mentary units and ?a small number of phrasal EDUs 
are allowed, provided that the phrase begins with a 
strong discourse marker.? 
establish anchor points and the actual BDUs in a 
given sentence which can function as indexical 
anchor points in a specific discourse. We believe 
these distinctions, while cumbersome, are neces-
sary for both theoretical and practical text analy-
sis. 
3.1 Discourse Segments under the U-LDM 
As a semantic theory, the U-LDM must account 
for the interpretation of utterances. Specifically, 
we must account for the availability for update of 
appropriate discourse contexts or sub-contexts 
introduced in earlier text. In order to do so, we 
must be able to match incoming discourse utter-
ances with their target contexts, some of which 
may have been introduced in syntactically sub-
ordinated positions within a sentence. Therefore, 
in designing U-LDM discourse segmentation, we 
have identified the syntactic reflexes of the se-
mantic content of the linguistic or paralinguistic 
phenomena making up discourse. 
 Since elementary discourse units are needed 
to build up discourse structure recursively, we 
have identified as discourse segments the syntac-
tic constructions that encode a minimum unit of 
meaning and/or discourse function interpretable 
relative to a set of contexts. We understand a 
minimum unit of meaning to communicate in-
formation about not more than one ?event?, 
?event-type? or state of affairs in a ?possible 
world? of some type4. Clauses, and many other 
verb based structures, carry indexical informa-
tion that ties the content to the context in which 
it is to be interpreted. Minimal functional units, 
on the other hand encode information about how 
previously occurring (or possibly subsequent) 
linguistic gestures relate structurally, semanti-
cally, interactionally or rhetorically to other units 
in the discourse or to information in the context 
in which the discourse takes place5. 
 Examples of discourse segments are given in 
Table 1. Note that while discourse segments un-
der the U-LDM are the syntactic reflex of a lin-
guistically realized semantic ?gesture? 
interpreted relative to context, they need not be 
contiguous, but may completely surround an-
other segment (e.g. an appositive, or non-
restrictive relative clause.) Discontinuous seg-
                                                          
4  Roughly speaking an ?elementary proposi-
tion?, ?event-type predicate? etc. In a Davidsonian 
style semantics, quantification over an event vari-
able signals a separate unit of meaning. 
5 Greetings, discourse PUSH/POP markers and 
other ?cue phrases?, connectives etc. are all func-
tional segments. 
 ments occur when there is overt material on both 
sides of the intervening segment. With fragmen-
tary segments, the full interpretation remains 
unrecoverable from surrounding context. For 
example: a single word answer to a question is a 
complete segment, whereas the same word ut-
tered but ?left hanging? would be an un-
interpretable fragment. (See Appendix for exten-
sive example of a segmented text.) 
3.2 Basic Discourse Units 
An important contrast between the U-LDM and 
other approaches to segmentation concerns the 
distinction made in the U-LDM between dis-
course segments such as those we have identified 
above and Basic Discourse Units (BDUs). While 
all BDUs under the U-LDM are segments, not 
all segments are BDUs. BDUs, under this model, 
are discourse segments of a type that can be in-
dependently continued: operator segments are 
one example of non-BDU segments. Other verb 
bases constituents that might be expected to be 
segments are not because they do not establish 
an interpretation context independent of other 
segments that can be updated by subsequent 
units. In general, these ?notable non-segments?, 
summarized in Table 2, are heavily integrated 
into other nominal or verbal constructions and 
cannot be accessed for independent continuation. 
 
Non-segments Examples 
Gerunds [Singing is fun.] 
Nominalizations [Rationalization is useless.] 
Auxiliary and 
modal verbs 
[I might have succeeded.] 
Clefts [It was the tiger that we liked best.] 
Table 2. Notable non-segments (underlined). 6 
                                                          
6 In answer to a reviewer who asked if in "Sing-
ing is fun", singing should not be an independent 
In order to account for continuation in specific 
sentences, we further identify one class of in-
stances of BDU: Active BDUs (A-BDUs) are 
BDUs on the right edge of a discourse tree. The 
main clause of any sentence will be an A-BDU 
and, depending on the deployment of BDU seg-
ments within a given sentence, other BDUs may 
also be accessible for continuation. (See Section 
4 below.)  
4 Discourse Parsing with the 
U-LDM 
Ascertaining the relationship of a BDU to the 
discourse is a complex parsing process involving 
lexical, semantic, structural and syntactic infor-
mation7. For the case of written prose we are 
concentrating on here, the unit of analysis is the 
sentence (or sentence fragment). Sentences are 
attached to the DPT of the text as a unit8. Dis-
course attachment of the sentence involves two 
decisions: where along the right edge to attach, 
and what is the relationship to the attachment 
point. The process, which includes constructing a 
BDU tree of the sentence, can be summarized as 
follows: 
                                                                                
segment, we would answer that this sentences con-
cerns one eventuality (something being fun), not two. 
Since any noun can be referred to by a pronoun in 
the next sentence simply referring to the noun is not 
equivalent to referring to the eventuality in which 
the referent of the noun is a participant. 
7 Although the linguistic (and lexical) informa-
tion we discuss could be augmented with processes 
relying upon high level world knowledge and infer-
ence, we believe that it is extremely significant to 
see how far one can get with discourse parsing with-
out invoking non-linguistic information. 
8 See discussion of MBDU below. 
 
Segments Common realizations Examples 
Content segments 
Clauses: main, subordinate [I heard the dog] [that was barking.] 
Predication [California elected Schwarzenegger] [governor] 
Participial modifiers [The donkey [braying next door] was annoying.] 
Eventualities (activities or 
states) and their participants. 
Infinitival modifiers [We persuaded them] [to leave.] 
[They left] [to get the tickets.] 
Parentheticals [The show [(and what a show it was)] lasted 4 hours.] 
Appositives [The building, [an example of the Mozarabic style,] was 
recently restored.] 
Interpolations 
Interruptions [They were [? Stop that! ?] leaving at 8:00.] 
Fragments Section headings [4. Discussion] 
List items [e.g., [hydrogen,] [helium]]  
?Restarted? material [My dog,] [no,] [my cat ran away.] 
Operator Segments 
Conjunction Conjunctions [We arrived] [and] [got seats.] 
Discourse operators ?scene-setting? preposed modifi-
ers  
[On Tuesday,] [we will see the sites.] 
 ?cue? words  [Anyway,] [we did get there on time.] 
Table 1. Examples of Discourse Segments, Unlabelled bracketing is used to indicate segments. 
  
? Identify potential BDUs within sentence 
using sentential syntax  
? Construct a BDU-tree from the segments of 
the sentence, using sentential syntactic in-
formation and discourse rules to map seg-
ments and relationships among them. This 
BDU-tree is itself an Open Right Tree domi-
nated by the node corresponding to the Main 
clause of sentence9. (This is the Main BDU 
or MBDU). 
? Attach the BDU-tree as a unit to the Dis-
course Parse Tree by computing the rela-
tionship of MBDU and preposed modifiers, 
if any, to accessible DCUs aligned along the 
right edge of the tree using rules of dis-
course relations (See Section 4.1 below). 
Lexical information used for attachment de-
cisions can come from anywhere in the 
BDU tree. 
? Once the BDU-tree is attached, its terminal 
leaves are terminal nodes of the Discourse 
Parse Tree (DPT) and any terminal or inter-
mediary nodes on the right edge of the BDU 
tree are DCUs on the DPT accessible for at-
tachment in the next iteration of the process. 
 
In order to determine which accessible DCUs are 
candidates for M-BDU attachment and what re-
lationship obtains between the incoming unit and 
the selected DCU, a number of distinct types of 
evidence are used, including: 
 
1) lexical information 
reuse somewhere in the BDU tree of the 
same lexeme, synonym/antonym, hypernym, 
or participation in the same lexical frame or 
?semantic field? as item in target node. 
2) syntactic information 
parallel syntactic structure; topic/focus and 
centering information, syntactic status of re-
used lexemes, pre-posed adverbial constitu-
ents, etc. 
3) semantic information 
realis status, genericity, tense, aspect, point 
of view etc. in the MBDU 
                                                          
9 This process is too complex to describe in de-
tail here but it involves looking at both the F-
structure of the sentential parsing information re-
turned by the XLE and applying discourse rules to 
the BDUs identified. Soricut and Marcu (2003) also 
build up RST sentential trees to use in discourse 
parsing. Both the information and methods used to 
construct RST trees as well as the trees themselves 
differ from ours.  
4) constituents of incomplete n-ary construc-
tions on the right edge 
Questions, initial greetings, genre-internal 
units like sections and sub-sections, etc. 
5) structure of both the local attachment 
point and the BDU-tree 
 
While we are still experimenting with under-
standing the complexities involved in attach-
ment, we believe that different types of evidence 
have different weights10 and that the combined 
weight of evidence determines the attachment 
point. We have noted, however even at this stage 
of our investigations, that the weight given to 
each type of information differs for attachment 
site selection and relationship determination. 
Lexical information, for example, is often very 
important in determining site, while semantic 
and syntactic information is most relevant in 
determining relationship. In the remainder of this 
section we will give a small set of robust rules 
for determining the attachment site and relation-
ship of an incoming BDU-tree to the existing 
parse tree of the discourse.  
4.1 Rules for Determining Discourse 
Attachment Site Candidates and 
Attachment Relations 
Both the attachment site choice and the actual 
attachment process rely on partially ordered sets 
of hybrid rules, each of which are conditioned on 
a set of constraints. Constraints for rules used in 
attachment site selection are primarily lexical 
constraints, although other information is also 
relevant.  
All types of evidence play a role in choosing 
the attachment relation. A rule is a pair: Rule 
<C, O> where C is the set of constraints that 
enable the rule and O is the associated operation. 
The operation associated with a rule can there-
fore be either the markup of a DCU as a possible 
attachment site, or an actual discourse relation, 
such as Subordination, Coordination or N-ary. A 
rule is enabled when all sub-conditions in C are 
satisfied and no other rules having priority are 
enabled. Rules may combine different sources of 
evidential information (semantic, syntactic, 
structural and lexical). If more than one rule is 
enabled at the same time, ambiguous parses are 
produced11. Some rules are listed in Table 3. 
                                                          
10 We assign weights heuristically at this point. 
11 At this stage in our research, we rely only on a 
partial order among the rules. In future work, we 
will investigate (1) how evidence is weighed and 
combined in order to make better attachment deci-
 The parsing process at the Discourse Parse 
Tree (DPT) level works as follows. When a 
BDU-Tree has been constructed and is ready to 
be attached to the right edge of the DPT, each 
DCU along the right edge is examined and the 
lexical information in the right-edge DPT nodes 
are compared with the lexical evidence retrieved 
                                                                                
sions and (2) the extent to which discourse ambigu-
ity generated in this fashion is legitimate and how to 
reduce grammar overgeneration by more efficient 
handling of interactions among rules and the weigh-
ing of the linguistic evidence. 
from the incoming BDU-Tree. This process, 
guided by the set of discourse rules, produces an 
ordered set of active DCUs, representing the 
possible attachment points in order of likelihood. 
The set can then be pruned of its n lowest scor-
ing constituents, according to an appropriate 
policy such as a threshold. 
In a second stage, each attachment rule is 
checked against possible attachment sites. Rules 
that fire successfully attach the BDU-Tree to the 
DPT at the chosen site with the relationship 
specified by the rule. Local semantic, lexical and 
syntactic information is then percolated up to the 
Attachment Relation Sub Conditions 
Nary-Attachment 
Frame(AP,MBDU) matches genre-specific construction 
Greetings, Argument, Question/Answer, Speech Event,  
Genre Meta Structure(Story, Technical Paper, Lecture, etc..) 
Reported speech/reporting clause 
Subordination M-BDU Realis status differs from Status of AP (MBDU is Irrealis; AP is Realis OR MBDU is Realis; AP is Irrealis) 
Nary-Attachment 
(intrasentential) 
Tense(AP) = past  
Tense(MBDU) = pluperfect  
AP is time-reference for MBDU 
Nary-Attachment 
(intrasentential) 
VerbClass(AP)=?SpeechAct? 
Type(MBDU) = ADJUNCT  
Nary-Attachment 
(intrasentential) 
Tense(AP) = present  
Tense(MBDU) = past  
AP is time-reference for MBDU 
Coordinate 
Parent(AP) is Coordination 
Parent(AP) would coordinate with MBDU  
AP would coordinate with MBDU 
Subordination 
Tense(AP) = past 
Genericty(AP) = specific 
Tense(MBDU) = present 
Genericty(MBDU) = generic 
Subordination M-BDU genericity status differs from Status of AP (MBDU is specific; AP is generic OR MBDU is generic; AP is specific) 
Subordination SUBJ(MBDU) = OBJ(AP) 
Subordination SUBJ(MBDU) = XCOMP(AP) 
Subordination MBDU/Lexeme is a subcase of AP/Lexeme Role(AP/Lexeme) = Role(MBDU/Lexeme) 
Right Headed Subordination 
(intrasentential) 
Type(AP) = ADJUNCT 
Type(MBDU) = S 
Nary-Attach 
(intrasentential) 
PRED(ADJUNCT(AP)) = ?if? 
AP is Irrealis 
MBDU is Realis 
Nary-Attachment 
(intrasentential) AP and MBDU related by logical connective (cf Webber& Joshi, 1998; Forbes (2003) 
Subordination  Tense(AP) = past Tense(MBDU) = pluperfect 
Subordination Tense(AP) = present Tense(MBDU) = past 
Subordinate AP is Bottom of DPT M-BDU is Footnote or Parenthetical 
Coordinate AP is Narrative( = Specific, punctual ,event) MBDU is Narrative 
Coordinate Tense(AP) = Tense(MBDU) Aspect(AP) = Aspect(MBDU) 
Coordinate MBDU/Lexeme is synonym or antonym of AP/Lexeme Role(AP/Lexeme) = Role(MBDU/Lexeme) 
Subordinate AP is Bottom of DPT 
Table 3. Discourse Attachment Rules ordered to express priority of the rules. AP denotes (potential) 
attachment point.  
 
 DCU consisting of the parent of both attachment 
point and incoming MBDU according to con-
straints of the discourse relation selected. If mul-
tiple attachments at different sites are possible, 
ambiguous parses are generated; less preferred 
attachments are discarded and the remaining 
attachment choices generate valid parse trees. 
5 PALSUMM Text Summarization  
So far, we have described the U-LDM only as a 
theoretical approach to discourse parsing. We 
now turn briefly to describe a computational 
implementation of these methods. The 
PALSUMM Text Summarization System is a 
domain independent symbolic sentence extrac-
tion system that produces high level readable 
summaries that preserve the language and style 
of the original text and eliminate problems with 
unresolved or incorrect reference. Our system is 
currently used to summarize a corpus of 300 
technical reports produced by our laboratory. 12 
The PALSUMM System relies on the Xerox 
Linguistic Environment (XLE) to parse the sen-
tences of our source texts. The f-structure output 
of the XLE parser is segmented into units ac-
cording to the criteria identified above. The seg-
ments are then combined into a BDU-tree. Using 
syntactic information about syntactic coordina-
tion and subordination relations, lexical onto-
logical information taken from WordNet and a 
customized lexical domain ontology as well as 
discourse rules, the M-BDU of the sentence 
along with any other BDUs that must be accessi-
ble along the right edge of the discourse tree to 
accommodate possible continuations are identi-
fied, Both the site of attachment and the attach-
ment relation are then computed using discourse 
attachment rules of the type presented above. 
Text summarization algorithms are then applied 
to the resulting tree.  
Running in purely symbolic mode, the tree is 
pruned at a given level of embeddedness to pro-
duce a summary of a desired length or degree of 
summarization.13 Because the resulting summa-
                                                          
12 For illustration purposes, we present in Ap-
pendix A a summary of a document that was hand 
coded using the rules given and then summarized 
automatically using the PALSUMM tree pruning al-
gorithm. The PALSUMM Summaries were judged 
to be significantly more readable than summaries 
produced by MEAD in a small comparative study. 
In Appendix B, we present a diagram of the 
PALSUMM system. 
13 Although closely related to methods reported 
by Marcu (1999, 2000) for summarization using 
ries may be longer than desired, alternatively we 
also use statistical methods to identify salient 
information (see discussion and references in 
Marcu 2003) and then construct a partial dis-
course tree that includes only information identi-
fied as most salient and the text at all nodes 
dominating that salient information.  
 
6 Conclusions and Directions for 
future work 
The U-LDM discussed in this paper represents a 
significant advance in the theoretical understand-
ing of the nature of discourse structure. The ex-
plicit rules for discourse segmentation based on 
the syntactic reflexes of semantic structures al-
low analysts for the first time to relate the se-
mantics underlying the syntactic structure of 
sentences to the discourse segments needed to 
account for continuity. In order to adapt the rules 
to other languages which may have different 
syntactic reflexes of semantic information, un-
derstanding the semantic justification for the 
choice of segments is important. In addition, the 
rules for discourse attachment for the first time 
make clear the principles of discourse continuity 
for ?coherent? discourse. In the future, we plan 
to deepen our understanding of the rules for dis-
course attachment and, in particular, begin to 
apply machine learning techniques to increase 
our understanding of the complex interrelation-
ship that obtain among them.  
While full implementation of the principles 
of discourse organization outlined here are be-
yond the state of the art in some respects (i.e. 
determining that a sentence is generic in English 
is non-trivial in many instances although ma-
chine learning techniques might be useful in this 
regard), we believe that the PALSUMM System 
demonstrates the practicality of symbolic dis-
course parsing using the U-LDM Model. The 
infrastructure for this system has been success-
fully applied to the task of summarizing docu-
ments without a complex semantic component, 
extensive world knowledge and inference or a 
subjectively annotated corpus. We believe that 
the U-LDM parsing methods discussed here can 
be used for all other complex NLP tasks in 
which symbolic parsing is appropriate, especially 
                                                                                
RST trees, our basic algorithm is essentially simpler 
because RST trees are dependency trees over a large 
set of different link types, whereas LDM trees are 
constituent trees over effectively two basic node 
types: subordinations and non-subordinations. 
 those involving high value document collections 
where precision is critical. In addition, the struc-
tures generated through symbolic parsing by the 
system will be invaluable for training statistical 
and probabilistic systems.  
References 
 
Nicholas Asher. 1993. Reference to Abstract 
Objects in English: A Philosophical Seman-
tics for Natural Language Metaphysics. Klu-
wer Academic Publishers. 
Lynn Carlson, Daniel Marcu, and Mary Ellen 
Okurowski. To appear. Building a Discourse-
Tagged Corpus in the Framework of Rhetori-
cal Structure Theory. In Current Directions in 
Discourse and Dialogue, Jan van Kuppevelt 
and Ronnie Smith eds. Kluwer Academic 
Publishers.  
Katherine M. Forbes, Eleni Miltsakaki, Rashmi 
Prasad, Anoop Sarkar, Aravind Joshi and 
Bonnie Webber. 2003. D-LTAG System - 
Discourse Parsing with a Lexicalized Tree-
Adjoining Grammar, Journal of Language, 
Logic and Information, 12(3). 
Barbara Grosz and Candace Sidner. 1986. Atten-
tion, Intention and the Structure of Discourse. 
Computational Linguistics 12:175-204.. 
Hans Kamp. 1981. A theory of truth and seman-
tic representation.? In Formal Methods in the 
Study of Language. Jeroen A. G. Groenendijk, 
Theo Janssen, and Martin Stokhof (eds.). Am-
sterdam: Mathematisch Centrum, 277-322.  
Hans Kamp and Uwe Reyle. 1993. From Dis-
course to Logic: Introduction to Model-
theoretic Semantics of Natural Language, 
Formal Logic and Discourse Representation 
Theory. Kluwer Academic Publishers, 
Dordrecht, The Netherlands. 
William C. Mann and Sandra A. Thompson. 
1988. Rhetorical Structure Theory: Towards a 
Functional Theory of Text Organization. Text 
8(3)243-281. 
Daniel Marcu. 1999. Discourse trees are good 
indicators of importance in text. In Advances 
in Automatic Text Summarization. I. Mani and 
Mark Maybury (Eds.), 123-136, The MIT 
Press. 
Daniel Marcu. 2000. The Theory and Practice of 
Discourse Parsing and Summarization. The 
MIT Press. Cambridge, MA. 
Daniel Marcu. 2003. Automatic Abstracting, 
Encyclopedia of Library and Information Sci-
ence, 245-256, 2003. 
Livia Polanyi. 1988. A Formal Model of Dis-
course Structure. Journal of Pragmatics 12: 
601-639. 
Livia Polanyi and Martin van den Berg. 1996. 
Discourse Structure and Discourse Interpreta-
tion. In Proceedings of the 10th Amsterdam 
Colloquium on Formal Semantics. University 
of Amsterdam. 
Livia Polanyi and Remko Scha. 1984. A syntac-
tic approach to discourse semantics. In Pro-
ceedings of COLING 6. Stanford, CA. 413-
419. 
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, and John Blitzer, Arda ?elebi, 
Elliott Drabek, Wai Lam, Danyu Liu, Hong 
Qi, Horacio Saggion, Simone Teufel, Michael 
Topper, Adam Winkel. 2003. ?The MEAD 
Multidocument Summarizer?. 
http://www.summarization.com/mead/ 
Radu Soricut and Daniel Marcu. 2003. Sentence 
Level Discourse Parsing using Syntactic and 
Lexical Information. In Proceedings of 
HLT/NAACL?03, May 27-June 1, Edmonton, 
Canada 
Bonnie Webber.1991. Structure and Ostension in 
the Interpretation of discourse Deixis. In Lan-
guage and Cognitive Processes, 6(2):107-135. 
Bonnie Webber and Aravind Joshi. 1998. An-
choring a Lexicalized Tree-Adjoining Gram-
mar for Discourse. ACL/COLING Workshop 
on Discourse Relations and Discourse Mark-
ers, Montreal, Canada.  
Alec Wilkinson. 2003. Talk of the Town, Sep-
tember 26, 2003, New Yorker 
 APPENDIX A. PALSUMM Example 
The text below, taken from a recent issue of The 
New Yorker magazine (Alec Wilkinson, 2003)14, 
has been analyzed by hand using the segmenta-
tion and discourse structure construction rules 
given in Sections 3 and 4 above, resulting in the 
Discourse Parse Tree given in Figure 1. The 
summary of the text was automatically generated 
using the automatic summarization algorithm 
mentioned in Section 5 and a Genre Specific rule 
for stories in which stories are treated as consist-
ing of an Orientation, Narrative and Coda. The 
first specific, non-habitual eventive clause closes 
the Orientation and begins the Narrative Section. 
The function of a Coda is to make the point of a 
story explicit. This is often done, as in the pre-
sent case, by using an anaphor that refers to an 
entire section of text (Webber, 1991)15. 
 
(1) In the spring of 1947, (2)William Katavolos 
is the solitary occupant of the Ram?s Head Inn, 
(3) on Ram Island,(4) off eastern Long Island. 
(5) Katavolos is twenty-three. (6) His father has 
leased the inn. (7) Katavolos has returned from 
the war (8) and (9) wants a place (10) where he 
can paint (11) and (12) be left alone. (13) The 
hotel is reached by a causeway from Shelter Is-
land, (14) and the causeway sometimes floods, 
(15) leaving Katavolos as isolated as a light-
house keeper. (16) To amuse himself one evening, 
(17) he puts some water in a glass, (18) covers 
the rim of the glass with waxed paper, (19) then 
presses the paper into the water (20) to create a 
vacuum. (21) He secures the paper to the glass 
with a rubber band, (22) then turns the glass 
upside down. (23) The water fills the vacuum, 
(24) preserving the dome (25) ? it looks like the 
bottom of a wine bottle. (26) Then he begins to 
wonder (27) what would happen (28) if he re-
peated the experiment on a larger scale.(29) A 
few days later, (30) he throws a tarpaulin over a 
section of Gardiners Bay (31) He weights down 
the edges (32) so that no air can get beneath the 
tarpaulin, (33) then he swims underneath it. (34) 
Using two oars, (35) he raises the center of the 
tarpaulin. (36) The water fills the cavity (37) and 
                                                          
14 Alec Wilkinson. 2003. Talk of the Town, Sep-
tember 26, 2003, New Yorker 
15 Bonnie Webber.1991. Structure and Ostension 
in the Interpretation of discourse Deixis. In Lan-
guage and Cognitive Processes, 6(2):107-135. 
 
he swims into it, (38) floating above sea level, 
(39) which, (40) he says later, (41) ?fascinated 
the hell out of me.? (42) This is the beginning of 
(43) what Katavolos will call hydronics, (44) the 
practice of making buildings from soft plastic 
forms (45) filled with water. (46) In 1949, (47) 
Katavolos gives up painting (48) to design furni-
ture (49) ? his chairs are in the collections of 
the Museum of Modern Art, the Metropolitan 
Museum, and the Louvre? (50) and, (51) in 
1960, (52) he begins teaching architecture at the 
Pratt Institute, (54) in Brooklyn, (55) where he 
will become the co-director of the Center for 
Experimental Structures. (56) In 1970, (57) in a 
courtyard at Pratt, (58) he builds the first hy-
dronic structure (59) ? a plastic dome filled 
with water (60) and supported by a plastic cylin-
der, (61) also filled with water. (62) The plastic 
is like Saran Wrap, (63) only thicker. (64) Each 
year after that, (65) he builds a new structure 
(66) He calls the structures (67) liquid villas. 
(68) They consist of columns, arches, and vaults. 
(69) The elements, (70) that is (71) of classical 
architecture. 
Summary 152/363 = 42% 
In the spring of 1947, William Katavolos is the 
solitary occupant of the Ram?s Head Inn, 
Katavolos is twenty-three. To amuse himself one 
evening, he puts some water in a glass, covers 
the rim of the glass with waxed paper, then 
presses the paper into the water. He secures the 
paper to the glass with a rubber band, then turns 
the glass upside down. A few days later, he 
throws a tarpaulin over a section of Gardiners 
Bay. 
He weights down the edges then he swims 
underneath it. Using two oars, he raises the cen-
ter of the tarpaulin. The water fills the cavity, 
This is the beginning of what Katavolos will call 
hydronics. In 1949, Katavolos gives up painting 
and in 1960 he begins teaching architecture at 
the Pratt Institute. In 1970, in a courtyard at 
Pratt, he builds the first hydronic structure. Each 
year after that, he builds a new structure. 
 
   
 
Figure 1. Discourse Parse Tree of the New Yorker text. 
 
 
APPENDIX B. System Diagram 
 
WordNet
Lookup
Flat
Ontology
Files
DAML
Encoded
Ontology
HTML
document
XLE Server
XLE
Parser
Segmentation
Webservice
Lexical Ontology Server
Ontology
Webservice
Lexical
Ontology
Lookup
English
Grammar
LiveTree
Sentence
Breaker
Sentence
Level
Discourse
Parser
Text
Level
Discourse
Parser
Discourse
tree
XML
XML
document
Hybrid
Summarizer
MEAD Statistical Summarizer
WebserviceSentence
Level
Discourse
Segmenter
Sentence
Level
Discourse
Grammar
Text
Level
Discourse
Grammar
LDM
Engine
TEXT XMLXML
SOAP
 
Figure 2. Diagram of the PALSUMM system, a symbolic summarization system currently being devel-
oped at FX Palo Alto Laboratory. 
 
