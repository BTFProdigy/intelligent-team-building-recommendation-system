Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 181?186, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
KLUE-CORE: A regression model of semantic textual similarity
Paul Greiner and Thomas Proisl and Stefan Evert and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6
91054 Erlangen, Germany
{paul.greiner,thomas.proisl,stefan.evert,besim.kabashi}@fau.de
Abstract
This paper describes our system entered for the
*SEM 2013 shared task on Semantic Textual
Similarity (STS). We focus on the core task
of predicting the semantic textual similarity of
sentence pairs.
The current system utilizes machine learn-
ing techniques trained on semantic similarity
ratings from the *SEM 2012 shared task; it
achieved rank 20 out of 90 submissions from
35 different teams. Given the simple nature of
our approach, which uses only WordNet and
unannotated corpus data as external resources,
we consider this a remarkably good result, mak-
ing the system an interesting tool for a wide
range of practical applications.
1 Introduction
The *SEM 2013 shared task on Semantic Textual
Similarity (Agirre et al, 2013) required participants
to implement a software system that is able to pre-
dict the semantic textual similarity (STS) of sentence
pairs. Being able to reliably measure semantic simi-
larity can be beneficial for many applications, e.g. in
the domains of MT evaluation, information extrac-
tion, question answering, and summarization.
For the shared task, STS was measured on a scale
ranging from 0 (indicating no similarity at all) to 5
(semantic equivalence). The system predictions were
evaluated against manually annotated data.
2 Description of our approach
Our system KLUE-CORE uses two approaches to
estimate STS between pairs of sentences: a distri-
butional bag-of-words model inspired by Sch?tze
(1998), and a simple alignment model that links each
word in one sentence to the semantically most similar
word in the other sentence. For the alignment model,
word similarities were obtained from WordNet (using
a range of state-of-the-art path-based similarity mea-
sures) and from two distributional semantic models
(DSM).
All similarity scores obtained in this way were
passed to a ridge regression learner in order to obtain
a final STS score. The predictions for new sentence
pairs were then transformed to the range [0,5], as
required by the task definition.
2.1 The training data
We trained our system on manually annotated sen-
tence pairs from the STS task at SemEval 2012
(Agirre et al, 2012). Pooling the STS 2012 training
and test data, we obtained 5 data sets from differ-
ent domains, comprising a total of 5343 sentence
pairs annotated with a semantic similarity score in
the range [0,5]. The data sets are paraphrase sen-
tence pairs (MSRpar), sentence pairs from video de-
scriptions (MSRvid), MT evaluation sentence pairs
(MTnews and MTeuroparl), and glosses from two
different lexical semantic resources (OnWN).
All sentence pairs were pre-processed with Tree-
Tagger (Schmid, 1995)1 for part-of-speech annota-
tion and lemmatization.
1http://www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/treetagger.html
181
2.2 Similarity on word level
Our alignment model (Sec. 2.3.1) is based on similar-
ity scores for pairs of words. We obtained a total of
11 different word similarity measures from WordNet
(Miller et al, 1990) and in a completely unsupervised
manner from distributional semantic models.
2.2.1 WordNet
We computed three state-of-the-art WordNet simi-
larity measures, namely path similarity, Wu-Palmer
similarity and Leacock-Chodorow similarity (Budan-
itsky and Hirst, 2006). As usual, for each pair of
words the synsets with the highest similarity score
were selected. For all three measures, we made use of
the implementations provided as part of the Natural
Language ToolKit for Python (Bird et al, 2009).
2.2.2 Distributional semantics
Word similarity scores were also obtained from two
DSM: Distributional Memory (Baroni and Lenci,
2010) and a model compiled from a version of the
English Wikipedia.2 For Distributional Memory, we
chose the collapsed W ?W matricization, resulting
in a 30686?30686 matrix that was further reduced
to 300 latent dimensions using randomized SVD
(Halko et al, 2009). For the Wikipedia DSM, we
used a L2/R2 context window and mid-frequency
feature terms, resulting in a 77598?30484 matrix.
Co-occurrence frequency counts were weighted us-
ing sparse log-likelihood association scores with a
square root transformation, and reduced to 300 latent
dimensions with randomized SVD. In both cases, tar-
get terms are POS-disambiguated lemmas of content
words, and the angle between vectors was used as a
distance measure (equivalent to cosine similarity).
For each DSM, we computed the following se-
mantic distances: (i) angle: the angle between the
two word vectors; (ii) fwdrank: the (logarithm of
the) forward neighbour rank, i.e. which rank the sec-
ond word occupies among the nearest neighbours
of the first word; (iii) bwdrank: the (logarithm of
the) backward neighbour rank, i.e. which rank the
first word occupies among the nearest neighbours of
the second word; (iv) rank: the (logarithm of the)
arithmetic mean of forward and backward neighbour
2For this purpose, we used the pre-processed and linguis-
tically annotated Wackypedia corpus available from http://
wacky.sslmit.unibo.it/.
rank; (v) lowrank: the (logarithm of the) harmonic
mean of forward and backward neighbour rank.
A composite similarity score in the range [0,1]
was obtained by linear regression on all five distance
measures, using the WordSim-353 noun similarity
ratings (Finkelstein et al, 2002) for parameter esti-
mation. This score is referred to as similarity below.
Manual inspection showed that word pairs with simi-
larity < 0.7 were completely unrelated in many cases,
so we also included a ?strict? version of similarity
with all lower scores set to 0. We further included
rank and angle, which were linearly transformed to
similarity values in the range [0,1].
2.3 Similarity on sentence level
Similarity scores for sentence pairs were obtained in
two different ways: with a simple alignment model
based on the word similarity scores from Sec. 2.2
(described in Sec. 2.3.1) and with a distributional
bag-of-words model (described in Sec. 2.3.2).
2.3.1 Similarity by word alignment
The sentence pairs were preprocessed in the follow-
ing way: input words were transformed to lower-
case; common stopwords were eliminated; and dupli-
cate words within each sentence were deleted. For
the word similarity scores from Sec. 2.2.2, POS-
disambiguated lemmas according to the TreeTagger
annotation were used.
Every word of the first sentence in a given pair
was then compared with every word of the second
sentence, resulting in a matrix of similarity scores
for each of the word similarity measures described
in Sec. 2.2. Since we were not interested in an asym-
metric notion of similarity, matrices were set up so
that the shorter sentence in a pair always corresponds
to the rows of the matrix, transposing the similarity
matrix if necessary. From each matrix, two similar-
ity scores for the sentence pair were computed: the
arithmetic mean of the row maxima (marked as short
in Tab. 4), and the artihmetic mean of the column
maxima (marked as long in Tab. 4).
This approach corresponds to a simple word align-
ment model where each word in the shorter sentence
is aligned to the semantically most similar word in
the longer sentence (short), and vice versa (long).
Note that multiple source words may be aligned to
the same target word, and target words can remain
182
unaligned without penalty. Semantic similarities are
then averaged across all alignment pairs.
In total, we obtained 22 sentence similarity scores
from this approach.
2.3.2 Distributional similarity
We computed distributional similarity between the
sentences in each pair directly using bag-of-words
centroid vectors as suggested by Sch?tze (1998),
based on the two word-level DSM introduced in
Sec. 2.2.2.
For each sentence pair and DSM, we computed (i)
the angle between the centroid vectors of the two sen-
tences and (ii) a z-score relative to all other sentences
in the same data set of the training or test collection.
Both values are measures of semantic distance, but
are automatically transformed into similarity mea-
sures by the regression learner (Sec. 2.4).
For the z-scores, we computed the semantic dis-
tance (i.e. angle) between the first sentence of a given
pair and the second sentences of all word pairs in the
same data set. The resulting list of angles was stan-
dardized to z-scores, and the z-score corresponding
to the second sentence from the given pair was used
as a measure of forward similarity between the first
and second sentence. In the same way, a backward
z-score between the second and first sentence was
determined. We used the average of the forward and
backward z-score as our second STS measure.
The z-transformation was motivated by our obser-
vation that there are substantial differences between
the individual data sets in the STS 2012 training and
test data. For some data sets (MSRpar and MSRvid),
sentences are often almost identical and even a single-
word difference can result in low similarity ratings;
for other data sets (e.g. OnWN), similarity ratings
seem to be based on the general state of affairs de-
scribed by the two sentences rather than their par-
ticular wording of propositional content. By using
other sentences in the same data set as a frame of
reference, corpus-based similarity scores can roughly
be calibrated to the respective notion of STS.
In total, we obtained 4 sentence (dis)similarity
scores from this approach. Because of technical is-
sues, only the z-score measures were used in the
submitted system. The experiments in Sec. 3 also
focus on these z-scores.
2.4 The regression model
The 24 individual similarity scores described in
Sec. 2.3.1 and 2.3.2 were combined into a single
STS prediction by supervised regression.
We conducted experiments with various machine
learning algorithms implemented in the Python li-
brary scikit-learn (Pedregosa et al, 2011). In partic-
ular, we tested linear regression, regularized linear
regression (ridge regression), Bayesian ridge regres-
sion, support vector regression and regression trees.
Our final system submitted to the shared task uses
ridge regression, a shrinkage method applied to linear
regression that uses a least-squares regularization on
the regression coefficients (Hastie et al, 2001, 59).
Intuitively speaking, the regularization term discour-
ages large value of the regression coefficients, which
makes the learning technique less prone to overfit-
ting quirks of the training data, especially with large
numbers of features.
We tried to optimise our results by training the indi-
vidual regressors for each test data set on appropriate
portions of the training data. For our task submis-
sion, we used the following training data based on
educated guesses inspired by the very small amount
of development data provied: for the headlines test
set we trained on both glosses and statistical MT
data, for the OnWN and FNWN test sets we trained
on glosses only (OnWN), and for the SMT test set
we trained on statistical MT data only (MTnews and
MTeuroparl). We decided to omit the Microsoft Re-
search Paraphrase Corpus (MSRpar and MSRvid)
because we felt that the types of sentence pairs in this
corpus were too different from the development data.
For our submission, we used all 24 features de-
scribed in Sec. 2.3 as input for the ridge regression
algorithm. Out of 90 submissions by 35 teams, our
system ranked on place 20.3
3 Experiments
In this section, we describe some post-hoc experi-
ments on the STS 2013 test data, which we performed
in order to find out whether we made good decisions
regarding the machine learning method, training data,
3This paper describes the run listed as KLUE-approach_2 in
the official results. The run KLUE-approach_1 was produced by
the same system without the bag-of-words features (Sec. 2.3.2);
it was only submitted as a safety backup.
183
similarity features, and other parameters. Results of
our submitted system are typeset in italics, the best
results in each column are typeset in bold font.
3.1 Machine learning algorithms
Tab. 1 gives an overview of the performance of vari-
ous machine learning algorithms. All regressors were
trained on the same combinations of data sets (see
Sec. 2.4 above) using all available features, and eval-
uated on the STS 2013 test data. Overall, our choice
of ridge regression is justified. Especially for the
OnWN test set, however, support vector regression
is considerably better (it would have achieved rank
11 instead of 17 on this test set). If we had happened
to use the best learning algorithm for each test set,
we would have achieved a mean score of 0.54768
(putting our submission at rank 14 instead of 20).
3.2 Regularization strength
We also experimented with different regularization
strengths, as determined by the parameter ? of the
ridge regression algorithm (see Tab. 2). Changing ?
from its default value ? = 1 does not seem to have
a large impact on the performance of the regressor.
Setting ? = 2 for all test sets would have minimally
improved the mean score (rank 19 instead of 20).
Even choosing the optimal ? for each test set would
only have resulted in a slightly improved mean score
of 0.53811 (also putting our submission at rank 19).
3.3 Composition of training data
As described above, we suspected that using different
combinations of the training data for different test
sets might lead to better results. The overview in
Tab. 3 confirms our expectations. We did, however,
fail to correctly guess the optimal combinations for
each test set. We would have obtained the best re-
sults by training on glosses (OnWN) for the headlines
test set (rank 35 instead of 40 in this category), by
training on MSR data (MSRpar and MSRvid) for the
OnWN (rank 11 instead of 17) and FNWN test sets
(rank 9 instead of 10), and by combining glosses and
machine translation data (OnWN, MTnews MTeu-
roparl) for the SMT test set (rank 30 instead of 33).
Had we found the optimal training data for each test
set, our system would have achieved a mean score of
0.55021 (rank 11 instead of 20).
3.4 Features
For our submission, we used all the features de-
scribed in Sec. 2. Tab. 4 shows what results each
group of features would have achieved by itself (all
runs use ridge regression, default ? = 1 and the same
combinations of training data as in our submission).
In Tab. 4, the line labelled wp500 shows the re-
sults obtained using only word-alignment similarity
scores (Sec. 2.3.1) based on the Wikipedia DSM
(Sec. 2.2.2) as features. The following two lines give
separate results for the alignments from shorter to
longer sentence, i.e. row maxima (wp500-short) and
from longer to shorter sentence, i.e. column maxima
(wp500-long), respectively. Below are corresponding
results for word alignments based on Distributional
Memory (dm, dm-short, dm-long) and WordNet simi-
larity as described in Sec. 2.2.1 (WN, WN-short, WN-
long). The line labelled bow represents the two z-
score similarities obtained from distributional bag-of-
words models (Sec. 2.3.2); bow-wp500 (Wikipedia
DSM) and bow-dm (Distributional Memory) each
correspond to a single distributional feature.
Combining all the available features indeed results
in the highest mean score. However, for OnWN and
SMT a subset of the features would have led to better
results. Using only the bag-of-words scores would
have improved the results for the OnWN test set by
a considerable margin (rank 8 instead of 17), using
only the alignment scores based on WordNet would
have improved the results for the SMT test set (rank
17 instead of 33). If we had used the optimal subset
of features for each test set, the mean score would
have increased to 0.55556 (rank 9 instead of 20).
4 Conclusion
Our experiments show that it is essential for high-
quality semantic textual similarity to adapt a corpus-
based system carefully to each particular data set
(choice of training data, feature engineering, tuning
of machine learning algorithm). Many of our edu-
cated guesses for parameter settings turned out to be
fairly close to the optimal values, though there would
have been some room for improvement.
Overall, our simple approach, which makes very
limited use of external resources, performs quite well
? achieving rank 20 out of 90 submissions ? and will
be a useful tool for many real-world applications.
184
headlines OnWN FNWN SMT mean
Ridge Regression 0.65102 0.68693 0.41887 0.33599 0.53546
Linear Regression 0.65184 0.68118 0.39707 0.32756 0.52966
Bayesian Ridge 0.65164 0.68962 0.42344 0.33003 0.53474
SVM SVR 0.52208 0.73330 0.40479 0.30810 0.49357
Decision Tree 0.29320 0.50633 0.05022 0.17072 0.28510
Table 1: Evaluation results for different machine learning algorithms
? headlines OnWN FNWN SMT mean
1 0.65102 0.68693 0.41887 0.33599 0.53546
0.01 0.65184 0.68129 0.39773 0.32773 0.52980
0.1 0.65186 0.68224 0.40246 0.32900 0.53087
0.5 0.65161 0.68492 0.41346 0.33311 0.53374
0.9 0.65114 0.68660 0.41816 0.33560 0.53523
2 0.64941 0.68917 0.42290 0.33830 0.53659
5 0.64394 0.69197 0.42265 0.33669 0.53491
Table 2: Evaluation results for different regularization strengths of the ridge regression learner
headlines OnWN FNWN SMT mean
def 0.65440 0.68693 0.41887 0.32694 0.53357
smt 0.65322 0.62643 0.24895 0.33599 0.50684
def+smt 0.65102 0.59665 0.24953 0.33867 0.49962
msr 0.63633 0.73396 0.43073 0.33168 0.54185
def+smt+msr 0.65008 0.65093 0.39636 0.28645 0.50777
approach2 0.65102 0.68693 0.41887 0.33599 0.53546
Table 3: Evaluation results for different training sets (?approach2? refers to our shared task submission, cf. Sec. 2.4)
headlines OnWN FNWN SMT mean
wp500 0.57099 0.59199 0.31740 0.31320 0.46899
wp500-long 0.57837 0.59012 0.30909 0.30075 0.46614
wp500-short 0.58271 0.58845 0.34205 0.29474 0.46794
dm 0.42129 0.55945 0.21139 0.27426 0.38910
dm-long 0.40709 0.56511 0.28993 0.23826 0.38037
dm-short 0.44780 0.53555 0.28709 0.24484 0.38853
WN 0.63654 0.65149 0.41025 0.35624 0.52783
WN-long 0.62749 0.63828 0.39684 0.33399 0.51297
WN-short 0.64986 0.66175 0.41441 0.33350 0.52759
bow 0.52384 0.74046 0.31917 0.24611 0.46808
bow-wp500 0.52726 0.73624 0.32797 0.24460 0.46841
bow-dm 0.21908 0.66873 0.17096 0.20176 0.32138
all 0.65102 0.68693 0.41887 0.33599 0.53546
Table 4: Evaluation results for different sets of similarity scores as features (cf. Sec. 3.4)
185
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2012. Semeval-2012 task 6:
A pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Semantics,
pages 385?393. Association for Computational Linguis-
tics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional Memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?712.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O?Reilly
Media, Sebastopol, CA. Online version available at
http://www.nltk.org/book.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?47.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited.
ACM Transactions on Information Systems, 20(1):116?
131.
N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Find-
ing structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute
of Technology, September.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning. Data Min-
ing, Inference, and Prediction. Springer, New York,
NY.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduc-
tion to WordNet: An on-line lexical database. Interna-
tional Journal of Lexicography, 3(4):235?244.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceedings
of the EACL SIGDAT-Workshop, pages 47?50, Dublin.
Hinrich Sch?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
186
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 395?401, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
KLUE: Simple and robust methods for polarity classification
Thomas Proisl and Paul Greiner and Stefan Evert and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6
91054 Erlangen, Germany
{thomas.proisl,paul.greiner,stefan.evert,besim.kabashi}@fau.de
Abstract
This paper describes our approach to the
SemEval-2013 task on ?Sentiment Analysis
in Twitter?. We use simple bag-of-words mod-
els, a freely available sentiment dictionary auto-
matically extended with distributionally similar
terms, as well as lists of emoticons and inter-
net slang abbreviations in conjunction with fast
and robust machine learning algorithms. The
resulting system is resource-lean, making it rel-
atively independent of a specific language. De-
spite its simplicity, the system achieves compet-
itive accuracies of 0.70?0.72 in detecting the
sentiment of text messages. We also apply our
approach to the task of detecting the context-
dependent sentiment of individual words and
phrases within a message.
1 Introduction
The SemEval-2013 task on ?Sentiment Analysis in
Twitter? (Wilson et al, 2013) focuses on polarity clas-
sification, i. e. the problem of determining whether
a textual unit, e. g. a document, paragraph, sentence
or phrase, expresses a positive, negative or neutral
sentiment (for a review of research topics and re-
cent developments in the field of sentiment analysis
see Liu (2012)). There are two subtasks: in task B,
?Message Polarity Classification?, whole messages
have to be classified as being of positive, negative
or neutral sentiment; in task A, ?Contextual Polarity
Disambiguation?, a marked instance of a word or
phrase has to be classified in the context of a whole
message.
The training data for task B consist of approxi-
mately 10 200 manually annotated Twitter messages,
the training data for task A of approximately 9 500
marked instances in approximately 6 300 Twitter mes-
sages.1 The test data consist of in-domain Twit-
ter messages (3 813 messages for task B and 4 435
marked instances in 2 826 messages for task A) and
out-of-domain SMS text messages (2 094 messages
for task B, 2 334 marked instances in 1 437 messages
for task A). The distribution of messages and marked
instances over sentiment categories in the training
and test sets is shown in Tab. 1.
pos neg neu total
train-B 3 783 1 600 4 832 10 215
test-B Twitter 1 572 601 1 640 3 813
test-B SMS 492 394 1 208 2 094
train-A 5 862 3 166 463 9 491
test-A Twitter 2 734 1 541 160 4 435
test-A SMS 1 071 1 104 159 2 334
Table 1: The data sets for both tasks
The main focus of the current paper lies on experi-
menting with resource-lean and robust methods for
task B, the classification of whole messages. We do,
however, apply our approach also to task A.
2 Features used for polarity classification
Our general approach is quite simple: we extract
feature vectors from the training data (based on the
1These figures indicate the amount of training data we were
actually able to use. Due to Twitter?s licensing conditions, the
training data could only be made available as a collection of IDs.
Even when using the official Twitter API for collecting the actual
messages rather than the screen-scraping approach suggested by
the task organizers, ca. 10% of the data were not (or no longer)
available.
395
original messages and a small number of additional
resources) and feed them into fast and robust super-
vised machine learning algorithms implemented in
the Python machine learning library scikit-learn (Pe-
dregosa et al, 2011). For task B, the features are
computed on the basis of the whole message; for task
A, we use essentially the same features, but compute
them once for the marked word or phrase and once
for the rest of the message. All the features we use
are described in some more detail in the following
subsections.
2.1 Bag of words
We experimented with three different sets of bag-of-
words features: unigrams, unigrams and bigrams, and
an extended unigram model that includes a simple
treatment of negation. For all three models we simply
use the word frequencies as feature weights.
Our preprocessing pipeline starts with a simple
preliminary tokenization step (lowercasing the whole
message and splitting it on whitespace). In the re-
sulting list of tokens, all user IDs and web URLs are
replaced with placeholders.2 Any remaining punctu-
ation is stripped from the tokens and empty tokens
are deleted. In the extended unigram model, up to
three tokens following a negation marker are then
prefixed with not_ (fewer tokens if another negation
marker or the end of the message is reached). Finally
all words are stemmed using the Snowball stemmer.3
For a token unigram or bigram to be included in
the bag of words models, it has to occur in at least
five messages.
As an additional feature we include the total num-
ber of tokens per message.
2.2 Features based on a sentiment dictionary
Widely-used algorithms such as SentiStrength (Thel-
wall et al, 2010) rely heavily on dictionaries contain-
ing sentiment ratings of words and/or phrases. We
use features based on an extended version of AFINN-
111 (Nielsen, 2011).4
The AFINN sentiment dictionary contains senti-
ment ratings ranging from ?5 (very negative) to 5
2The regular expression for matching web URLs has
been taken from http://daringfireball.net/2010/07/
improved_regex_for_matching_urls.
3http://snowball.tartarus.org/
4http://www2.imm.dtu.dk/pubdb/p.php?6010
(very positive) for 2 476 word forms. In order to ob-
tain a better coverage, we extended the dictionary
with distributionally similar words. For this pur-
pose, large-vocabulary distributional semantic mod-
els (DSM) were constructed from a version of the
English Wikipedia5 and the Google Web 1T 5-Grams
database (Brants and Franz, 2006). The Wikipedia
DSM consists of 122 281 case-folded word forms
as target terms and 30 484 mid-frequency content
words (lemmatised) as feature terms; the Web1T5
DSM of 241 583 case-folded word forms as target
terms and 100 063 case-folded word forms as fea-
ture terms. Both DSMs use a context window of two
words to the left and right, and were reduced to 300
latent dimensions using randomized singular value
decomposition (Halko et al, 2009).
For each AFINN entry, the 30 nearest neighbours
according to each DSM were considered as exten-
sion candidates. Sentiment ratings for the new candi-
dates were computed by averaging over the 30 near-
est neighbours of the respective candidate term (with
scores set to 0 for all neighbours not listed in AFINN),
and rescaling to the range [?5,5].6 After some ini-
tial experiments, only candidates with a computed
rating ??2.5 or ? 2.5 were retained, resulting in an
extended dictionary of 2 820 word forms.
As with the bag of words model, we make use of
a simple heuristic treatment of negation: following a
negation marker, the polarity of the next sentiment-
carrying token up to a distance of at most four tokens
is multiplied by ?1.
The sentiment dictionary is used to extract four
features: I) the number of tokens that express a posi-
tive sentiment, II) the number of tokens that express
a negative sentiment, III) the total number of tokens
that express a sentiment according to our sentiment
dictionary and IV) the arithmetic mean of all the sen-
timent scores from the sentiment dictionary in the
message.
5We used the pre-processed and linguistically annotated
Wackypedia corpus available from http://wacky.sslmit.
unibo.it/.
6Scaling coefficients were determined by regression on ex-
tension candidates that were already listed in AFINN.
396
2.3 Features based on emoticons and internet
slang abbreviations
In addition to the sentiment dictionary we use a list
of 212 emoticons and 95 internet slang abbreviations
from Wikipedia. We manually classified these 307
emotion markers as negative (?1), neutral (0) or pos-
itive (1).
The extracted features based on this list are similar
to the ones based on the sentiment dictionary. We use
I) the number of positive emotion markers, II) the
number of negative emotion markers, III) the total
number of emotion markers and IV) the arithmetic
mean of all the emotion markers in the message.
3 Experiments
In this section we evaluate different classifiers (multi-
nomial Naive Bayes,7 Linear SVM8 and Maximum
Entropy9) and various combinations of features on
the gold test sets. We vary the bag-of-words model
(bow), the use of AFINN (sent), our extensions to
the sentiment dictionary (ext) and the list of emotion
markers (emo). To present as clear a picture of the
classifiers? performances as possible, we report F-
scores for each of the three classes, the weighted av-
erage of all three F-scores (Fw), the (unweighted) av-
erage of the positive and negative F-scores (Fpos+neg;
this is the value shown in the official task results and
used for ranking systems), as well as accuracy.
Results for submitted systems are typeset in italics,
the best results in each column are typeset in bold
font.
3.1 Task B: Message Polarity Classification
Experiments with just a simple unigram bag-of-
words model show that for both the Twitter (Tab. 3)
and the SMS data (Tab. 4) the Maximum Entropy
classifier outperforms multinomial Naive Bayes and
Linear SVM by a considerable margin. For compar-
ison, we also include some weak baselines (Tab. 2).
The random baselines classify messages randomly,10
7We always use the default setting alpha = 1.0.
8In all experiments, we use the following parameters:
penalty = ?l1?, dual = False, C = 1.0.
9We use the following parameter settings in our experiments:
penalty = ?l1?, C = 1.0.
10randomuniform assumes a uniform probability distribution
(all categories have equal probabilities), randomweighted has
learned the probability distribution from the training data,
the majority baselines simply assign all messages
to the most frequent category in the training data.11
As one would expect, all three learning algorithms
are vastly superior to those baselines. Using both
unigrams and bigrams in the bag-of-words model
improves classifier peformance; so does the extended
unigram model with negations.
For the Twitter data, adding the sentiment dictio-
nary, the dictionary extensions and the list of emo-
tion markers further improves classifier performance,
with the best results being achieved by a combina-
tion of all these features with a uni- and bigram bag-
of-words model. The best combination of features
would have been the fourth best system out of 35
constrained systems (sixth best out of all 51 systems),
one rank higher than our task submission.12
For the SMS data, adding the sentiment dictio-
nary and the dictionary extensions seems to improve
the official score Fpos+neg, but slightly decreases
weighted average F-score and accuracy. This might
be due to the greater orthographical variation in SMS
texts. Emotion markers seem to be a much better
sentiment indicator in the SMS data. But while just
combining the list of emotion markers with the ex-
tended unigram bag-of-words model leads to the best
weighted average F-score and accuracy, Fpos+neg is
best when a combination of all features is used. This
is also the system we submitted, being the third best
system (out of 44) for that task.
3.2 Task A: Contextual Polarity
Disambiguation
The results for task A are similar to those for task
B in that Maximum Entropy is the best classifier for
the unigram bag-of-words model for both the Twitter
(Tab. 5) and the SMS data (Tab. 6). Adding negation
treatment to the bag-of-words model increases classi-
fier performance, as do the inclusion of AFINN and
the use of emotion markers. Interestingly, extend-
ing the sentiment dictionary based on distributional
similarity leads to slightly worse results. Therefore,
randomweighted,binary uses the same probability distribution but
classifies messages only as either positive or negative.
11majority classifies all messages as neutral, as this is the most
frequent category in the training data, majoritybinary does binary
classification and thus classifies all messages as positive.
12Evaluation results for all SemEval-2013 tasks are avail-
able online: http://www.cs.york.ac.uk/semeval-2013/
index.php?id=evaluation-results.
397
classifier Fpos Fneg Fneu Fw Fpos+neg Acc
randomuniform 0.3666 0.2128 0.3745 0.3458 0.2897 0.3318
randomweighted 0.3912 0.1681 0.4521 0.3820 0.2796 0.3835
randomweighted,binary 0.5186 0.2042 0.000 0.2460 0.3614 0.3349
majority 0.0000 0.0000 0.6015 0.2587 0.0000 0.4301
majoritybinary 0.5838 0.0000 0.0000 0.2407 0.2919 0.4123
Table 2: Some weak baselines for task B, Twitter test set
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.6355 0.5093 0.6898 0.6390 0.5724 0.6423
LinearSVM uni - - - 0.6412 0.4884 0.6876 0.6371 0.5648 0.6418
MaxEnt uni - - - 0.6705 0.5109 0.7212 0.6671 0.5907 0.6761
MaxEnt uni+bi - - - 0.6845 0.5192 0.7257 0.6762 0.6019 0.6845
MaxEnt unineg - - - 0.6797 0.5284 0.7242 0.6750 0.6041 0.6824
MaxEnt unineg + - - 0.6860 0.5661 0.7284 0.6854 0.6261 0.6911
MaxEnt unineg - - + 0.6807 0.5393 0.7229 0.6766 0.6100 0.6835
MaxEnt unineg + + - 0.6841 0.5529 0.7258 0.6814 0.6185 0.6874
MaxEnt unineg + + + 0.6963 0.5650 0.7325 0.6912 0.6306 0.6968
MaxEnt unineg + - + 0.6952 0.5753 0.7338 0.6929 0.6353 0.6984
MaxEnt uni+bi + - + 0.7034 0.5706 0.7358 0.6964 0.6370 0.7018
MaxEnt uni+bi + + + 0.7052 0.5720 0.7371 0.6979 0.6386 0.7031
MaxEnt - + + + 0.6920 0.3532 0.6533 0.6220 0.5226 0.6370
Table 3: Evaluation results for task B on the Twitter test set
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.4918 0.4773 0.5541 0.5250 0.4845 0.5153
LinearSVM uni - - - 0.5833 0.5046 0.7229 0.6490 0.5440 0.6442
MaxEnt uni - - - 0.6260 0.5015 0.7903 0.6974 0.5638 0.7015
MaxEnt uni+bi - - - 0.6003 0.5380 0.7658 0.6840 0.5692 0.6829
MaxEnt unineg - - - 0.6528 0.5412 0.7884 0.7100 0.5970 0.7125
MaxEnt unineg + - - 0.6399 0.5955 0.7744 0.7092 0.6177 0.7073
MaxEnt unineg - - + 0.6596 0.5507 0.8033 0.7220 0.6052 0.7259
MaxEnt unineg + + - 0.6374 0.5905 0.7731 0.7068 0.6140 0.7049
MaxEnt unineg + + + 0.6506 0.5900 0.7903 0.7198 0.6203 0.7197
MaxEnt unineg + - + 0.6556 0.5833 0.7908 0.7200 0.6195 0.7202
MaxEnt uni+bi + - + 0.6318 0.5896 0.7750 0.7064 0.6107 0.7044
MaxEnt uni+bi + + + 0.6341 0.5783 0.7746 0.7047 0.6062 0.7030
MaxEnt - + + + 0.5961 0.3421 0.7179 0.6186 0.4691 0.6342
Table 4: Evaluation results for task B on the SMS test set
398
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.7799 0.6164 0.0498 0.6967 0.6981 0.7067
LinearSVM uni - - - 0.7759 0.6046 0.0576 0.6905 0.6902 0.6949
MaxEnt uni - - - 0.7974 0.6155 0.0110 0.7059 0.7065 0.7218
MaxEnt uni+bi - - - 0.8071 0.6320 0.0222 0.7179 0.7195 0.7335
MaxEnt unineg - - - 0.8058 0.6380 0.0110 0.7188 0.7219 0.7342
MaxEnt unineg + - - 0.8160 0.6610 0.0317 0.7339 0.7385 0.7479
MaxEnt unineg + + - 0.8153 0.6583 0.0316 0.7325 0.7368 0.7466
MaxEnt unineg + + + 0.8141 0.6608 0.0330 0.7326 0.7374 0.7468
MaxEnt unineg + - + 0.8153 0.6664 0.0331 0.7353 0.7409 0.7493
Table 5: Evaluation results for task A on the Twitter test set
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.6766 0.6657 0.0213 0.6268 0.6712 0.6452
LinearSVM uni - - - 0.6628 0.6533 0.0365 0.6157 0.6581 0.6290
MaxEnt uni - - - 0.6829 0.6630 0.0117 0.6277 0.6729 0.6491
MaxEnt uni+bi - - - 0.6825 0.6504 0.0230 0.6224 0.6665 0.6435
MaxEnt unineg - - - 0.7008 0.6770 0.0120 0.6427 0.6889 0.6654
MaxEnt unineg + - - 0.7127 0.6962 0.0238 0.6579 0.7044 0.6804
MaxEnt unineg + + - 0.7108 0.6954 0.0238 0.6568 0.7031 0.6791
MaxEnt unineg + + + 0.7090 0.7017 0.0237 0.6589 0.7054 0.6808
MaxEnt unineg + - + 0.7114 0.7034 0.0238 0.6608 0.7074 0.6829
Table 6: Evaluation results for task A on the SMS test set
we could have improved upon our task submission
by excluding the sentiment dictionary extensions ?
however, the gains are very small and the system?s
ranks would still be the same (17/28 for the Twitter
data, 16/26 for the SMS data).
4 Discussion
4.1 Error analysis
4.1.1 Task B: Message Polarity Classification
The most prominent problem, according to the con-
fusion matrix in Tab. 7, is that a lot of negative mes-
sages are classified as neutral; the same problem
exists to a lesser extent for positive messages.
A qualitative analysis of mis-classified messages
for which the MaxEnt classifier indicated high con-
fidence suggests that the human annotators did not
clearly distinguish between sentiment expressed by
the authors of messages and their own response to
message content. For example, the messages shown
predicted
pos neg neu
go
ld
pos 979 352 70 40 523 100
neg 70 47 287 213 244 134
neu 191 191 58 75 1391 942
Table 7: Task B, confusion matrix for tweets/SMS
in (1) and (2) report a negative and positive event,
respectively, in a neutral way and should therefore
be annotated with neutral sentiment. However, in the
test data they are labelled as negative and positive by
the human annotators.
(1) MT @LccSy #Syria, Deir Ezzor | Marba?eh:
Aerial shelling dropped explosive barrels on
residential buildings in the town. Tue, 23
October.
399
(2) European Exchanges open with a slight
rise: (AGI) Rome, October 24 - Euro-
pean Exchanges opened with a slight ris...
http://t.co/mAljf6eT
This problem is probably a major factor in the mis-
classification of many negative and positive messages
as neutral. In order to better reproduce the human
annotations, the system would additionally have to
decide whether a reported event is of a negative, pos-
itive or neutral nature per se ? a quite different task
that would require external training data and world
knowledge.
An analysis of mis-classified positive messages
further suggests that certain punctuation marks, espe-
cially multiple exclamation marks, might be useful
as additional features.
4.1.2 Task A: Contextual Polarity
Disambiguation
The confusion matrix in Tab. 8 shows that mes-
sages marked as negative in the test data often mis-
classified as positive and vice versa, while neutral
instances are overwhelmingly classified as positive
or negative. This suggests that for the classifiers we
use, there might be too few neutral instances in the
training data (cf. Tab. 1).
predicted
pos neg neu
go
ld
pos 2329 826 397 239 8 6
neg 550 341 980 761 11 2
neu 109 92 48 65 3 2
Table 8: Task A, confusion matrix for tweets/SMS
4.2 Conclusion and future work
We use a resource-lean approach, relying only on
three external resources: a stemmer, a relatively
small sentiment dictionary and an even smaller list
of emotion markers. Stemmers are already avail-
able for many languages and both kinds of lexical
resources can be gathered relatively easily for other
languages. The list of emotion markers should apply
to most languages. This makes our whole system rel-
atively language-independent, provided that a similar
amount of manually labelled training data is avail-
able.13 In fact, the learning curve for our system
(Fig. 1) suggests that even as few as 3 000?3 500
labelled messages might be sufficient. The similar
0.0
0.2
0.4
0.6
0.8
1.0
Amount of training data
Scor
e
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
Fpos+negAccuracy
Figure 1: Learning curve of our system for the ?Message
Polarity Classification? task, evaluated on the Twitter data
evaluation results for the Twitter and the SMS data
show that not relying on Twitter-specific features like
hashtags pays off: by making our system as generic
as possible, it is robust, not overfitted to the training
data, and generalizes well to other types of data. The
methods discussed in the current paper are particu-
larly well suited to the ?Message Polarity Classifica-
tion? task, our system ranking amongst the best. It
turns out, however, that simply applying the same ap-
proach to the ?Contextual Polarity Disambiguation?
task yields only mediocre results.
In the future, we would like to experiment with a
couple of additional features. Determining the near-
est neighbors of a message based on Latent Semantic
Analysis might be a useful addition, as might be the
use of part-of-speech tags created by an in-domain
POS tagger (Gimpel et al, 2011)14. We would also
like to find out whether a heuristic treatment of inten-
sifiers and detensifiers, the normalization of character
repetitions, or the inclusion of some punctuation-
based features could further improve classifier per-
formance.
13For task B, even the extended unigram bag-of-words model
by itself, without any additional resources, would have per-
formed quite well as the 9th best constrained system on the
Twitter test set (13th best system overall) and the 5th best system
on the SMS test set.
14http://www.ark.cs.cmu.edu/TweetNLP/
400
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadelphia,
PA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 42?47, Portland,
Oregon. Association for Computational Linguistics.
N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Find-
ing structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute
of Technology, September.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technologies.
Morgan & Claypool.
Finn ?rup Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages, number 718 in CEUR Workshop Proceedings,
pages 93?98, Heraklion.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in short
strength detection informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in Twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013). Association for
Computational Linguistics.
401
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 532?540,
Dublin, Ireland, August 23-24, 2014.
SemantiKLUE: Robust Semantic Similarity at Multiple Levels Using
Maximum Weight Matching
Thomas Proisl and Stefan Evert and Paul Greiner and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg (FAU)
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen, Germany
{thomas.proisl,stefan.evert,paul.greiner,besim.kabashi}@fau.de
Abstract
Being able to quantify the semantic similar-
ity between two texts is important for many
practical applications. SemantiKLUE com-
bines unsupervised and supervised tech-
niques into a robust system for measuring
semantic similarity. At the core of the sys-
tem is a word-to-word alignment of two
texts using a maximum weight matching
algorithm. The system participated in three
SemEval-2014 shared tasks and the com-
petitive results are evidence for its usability
in that broad field of application.
1 Introduction
Semantic similarity measures the semantic equiv-
alence between two texts ranging from total dif-
ference to complete semantic equivalence and is
usually encoded as a number in a closed interval,
e. g. [0,5]. Here is an example for interpreting the
numeric similarity scores taken from Agirre et al.
(2013, 33):
0. The two sentences are on different topics.
1. The two sentences are not equivalent, but are
on the same topic.
2. The two sentences are not equivalent, but
share some details.
3. The two sentences are roughly equivalent, but
some important information differs/missing.
4. The two sentences are mostly equivalent, but
some unimportant details differ.
5. The two sentences are completely equivalent,
as they mean the same thing.
Systems capable of reliably predicting the semantic
similarity between two texts can be beneficial for a
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
broad range of NLP applications, e. g. paraphrasing,
MT evaluation, information extraction, question
answering and summarization.
A general system for semantic similarity aiming
at being applicable in such a broad scope has to
be able to adapt to the use case at hand, because
different use cases might, for example, require dif-
ferent similarity scales: For one application, two
texts dealing roughly with the same topic should
get a high similarity score, whereas for another ap-
plication being able to distinguish between subtle
differences in meaning might be important. The
three SemEval-2014 shared tasks focussing on se-
mantic similarity (cf. Sections 3, 4 and 5 for more
detailed task descriptions) provide a rich testbed
for such a general system, as the individual tasks
and subtasks have slightly different objectives.
In the remainder of this paper, we describe
SemantiKLUE, a general system for measuring se-
mantic similarity between texts that we built based
on our experience from participating in the *SEM
2013 shared task on ?Semantic Textual Similarity?
(Greiner et al., 2013).
2 System Description
SemantiKLUE operates in two stages. In the first,
unsupervised stage, a number of similarity mea-
sures are computed. Those measures are the same
for all tasks and range from simple heuristics to dis-
tributional approaches to resource-heavy methods
based on WordNet and dependency structures. The
idea is to have a variety of similarity measures that
can capture small differences in meaning as well
as broad thematical similarities. In the second, su-
pervised stage, all similarity measures obtained in
this way are passed to a support vector regression
learner that is trained on the available gold standard
data in order to obtain a final semantic similarity
score. This way, the proper similarity scale for a
given task can be learned. The few remaining out-
liers in the predictions for new text pairs are cut
532
off to fit the interval required by the task definition
([0,4] or [0,5]).
Our submissions for the individual tasks were
created using incomplete versions from different
developmental stages of the system. In the follow-
ing sections we describe the current version of the
complete system for which we also report compa-
rable results for all tasks (cf. Sections 3? 5).
The whole system is implemented in Python.
2.1 Preprocessing
We use Stanford CoreNLP
1
for part-of-speech tag-
ging, lemmatizing and parsing the input texts. We
utilize the CCprocessed variant of the Stanford De-
pendencies (collapsed dependencies with propaga-
tion of conjunct dependencies; de Marneffe and
Manning (2008, 13?15)) to create a graph represen-
tation of the texts using the NetworkX
2
(Hagberg
et al., 2008) module. All the similarity measures
described below are computed on the basis of that
graph representation. It is important to keep in
mind that by basing all computations on the Stan-
ford Dependencies model we effectively ignore
most of the prepositions when using measures that
work on tokens.
3
For some tasks, we perform some
additional task-specific preprocessing steps prior
to parsing, cf. task descriptions below.
2.2 Simple Measures
We use four simple heuristic similarity measures
that need very little preprocessing. The first two
are word form overlap and lemma overlap between
the two texts. We take the sets of word form to-
kens/lemmatized tokens in text A and text B and
calculate the Jaccard coefficient:
overlap =
|A?B|
|A?B|
.
The third is a heuristic for the difference in text
length that was used by Gale and Church (1993) as
a similarity measure for aligning sentences:
z
i
=
d
i
?
d
, where d
i
= b
i
?
?
N
j=1
b
j
?
N
j=1
a
j
a
i
.
For each of the N text pairs we calculate the differ-
ence d
i
between the observed length of text B and
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://networkx.github.com/
3
That is because in the CCprocessed variant of the Stanford
Dependencies most prepositions are ?collapsed? into depen-
dency relations and are therefore represented as edges and not
as vertices in the graph.
the expected length of text B based on the length
of text A. By dividing that difference d
i
by the stan-
dard deviation of all those differences, we obtain
our heuristic z
i
.
The fourth is a binary feature expressing whether
the two texts differ in their use of negation. We
check if one of the texts contains any of the lem-
mata no, not or none and the other doesn?t. That
feature is motivated by the comparatively large
number of sentences in the SICK dataset (Marelli
et al., 2014b) that mainly differ in their use of nega-
tion, e. g. sentence pair 42 in the training data that
has a gold similarity score of 3.4:
? Two people are kickboxing and spectators are
watching
? Two people are kickboxing and spectators are
not watching
2.3 Measures Based on Distributional
Document Similarity
We obtain document similarity scores from two
large-vocabulary distributional semantic models
(DSMs).
The first model is based on a 10-billion word
Web corpus consisting of Wackypedia and ukWaC
(Baroni et al., 2009), UMBC WebBase (Han et
al., 2013), and UKCOW 2012 (Sch?fer and Bild-
hauer, 2012). Target terms and feature terms are
POS-disambiguated lemmata.
4
We use parame-
ters suggested by recent evaluation experiments:
co-occurrence counts in a symmetric 4-word win-
dow, the most frequent 30,000 lexical words as
features, log-likelihood scores with an additional
log-transformation, and SVD dimensionality re-
duction of L2-normalized vectors to 1000 latent
dimensions. This model provides distributional
representations for 150,000 POS-disambiguated
lemmata as target terms.
The second model was derived from the second
release of the Google Books N-Grams database
(Lin et al., 2012), using the dependency pairs pro-
vided in this version. Target and feature terms are
case-folded word forms; co-occurrence ounts are
based on direct syntactic relations. Here, the most
frequent 50,000 word forms were used as features.
All other parameters are identical to the first DSM.
This model provides distributional representations
for 250,000 word forms.
We compute bag-of-words centroid vectors for
each text as suggested by (Sch?tze, 1998). For each
4
e.g. can_N for the noun can
533
text pair and DSM, we calculate the cosine similar-
ity between the two centroid vectors as a measure
of their semantic similarity. We also determine the
number of unknown words in both texts according
to both DSMs as additional features.
2.4 Alignment-based Measures
We also use features based on word-level similar-
ity. We separately compute similarities between
words using state-of-the-art WordNet similarity
measures and the two distributional semantic mod-
els described above. The words from both texts
are then aligned using those similarity scores to
maximize the similarity total. We use two types
of alignment: One-to-one alignment where some
words in the longer text remain unaligned and one-
to-many alignment where all words are aligned.
The one-to-many alignment is based on the one-to-
one alignment and aligns each previously unaligned
word in the longer text to the most similar word in
the shorter text. The discussion of the alignment
algorithm is based on the former case.
2.4.1 Alignment via Maximum Weight
Matching
We opt for a graphical solution to the alignment
problem. The similarities between the words from
both texts can be modelled as a bipartite graph in
which every word from text A is a vertice on the
left-hand side of the graph and every word from
text B a vertex on the right-hand side. Weighted
edges connect every word from text A to every
word from text B. The weight of an edge corre-
sponds to the similarity between the two words it
connects. In order to obtain an optimal one-to-one
alignment we have to select edges in such a way
that no two edges share a common vertice and that
the sum of the edge weights is maximized. That
corresponds to the problem of finding the maxi-
mum weight matching in the graph. SemantiKLUE
utilizes the NetworkX implementation of Galil?s
(1986) algorithms for finding that maximum weight
matching.
Figure 1 visualizes the one-to-one alignment be-
tween two sentences. For the one-to-many align-
ment, the previously unaligned words are aligned
as indicated by the dashed lines.
2.4.2 Measures Based on Distributional
Word Similarities
For each of the two DSMs described in Section 2.3
we compute the best one-to-one and the best one-
A
woman
is
using
a
machine
made
for
sewing
A
woman
is
sewing
with
a
machine
Figure 1: Alignment between a sentence pair from
the SICK data set.
to-many alignment using the cosine similarity be-
tween two words as edge weight. For each of
those two alignments we compute the following
two similarity measures: I) the arithmetic mean of
the cosines between all the aligned words from text
A and text B and II) the arithmetic mean ignoring
identical word pairs.
In addition to those eight measures, we use the
lemma-based DSM for computing the distribution
of cosines between lemma pairs. For both align-
ments, we categorize the cosines between aligned
lemma pairs into five heuristically determined inter-
vals ([0.2,0.35), [0.35,0.5), [0.5,0.7), [0.7,0.999),
[0.999,1.0])
5
and use the proportions as features.
Intuitively, the top bins correspond to links between
identical words, paradigmatically related words
and topically related words. All in all, we use a
total of 18 features computed from the DSM-based
alignments.
2.4.3 Measures Based on WordNet
We utilize two state-of-the-art (Budanitsky and
Hirst, 2006) WordNet similarity measures for cre-
ating alignments: Leacock and Chodorow?s (1998)
normalized path length and Lin?s (1998) universal
similarity measure. For both of those similarity
measures we compute the best one-to-one and the
best one-to-many alignment. For each alignment
we compute the following two similarity measures:
I) the arithmetic mean of the similarities between
the aligned words from text A and text B and II) the
arithmetic mean ignoring identical word pairs.
5
Values in the interval [0.0,0.2) are discarded as they
would be collinear with the other features.
534
We also include the number of unknown words
in both texts according to WordNet as additional
features.
2.5 Measures Using the Dependency
Structure
We expect that the information encoded in the de-
pendency structure of the texts can be beneficial in
determining the semantic similarity between them.
Therefore, we use three heuristics for measuring
similarity on the level of syntactic dependencies.
The first simply measures the overlap of depen-
dency relation labels between the two texts (cf. Sec-
tion 2.2). The second utilizes the fact that the Stan-
ford Dependencies are organized in a hierarchy (de
Marneffe and Manning, 2008, 11?12) to compute
Leacock and Chodorow?s normalized path lengths
between individual dependency relations. That
measure for the similarity between dependency re-
lations is then used to determine the best one-to-one
alignment between dependency relations from text
A and text B and to compute the arithmetic mean
of the similarities between the aligned dependency
relations. The third heuristic gives an indication
of the quality of the one-to-one alignment and can
be used to distinguish texts that contain the same
words in different syntactic structures. It uses the
one-to-one alignment created with similarity scores
from the lemma-based DSM (cf. Section 2.4.2) to
compute the average overlap of neighbors for all
aligned word pairs. The overlap of neighbors is
determined by computing the Jaccard coefficient
of sets N
A
and N
B
. Set N
A
contains all words from
text B that are aligned to words from text A that
are connected to the target word via a single depen-
dency relation. N
B
contains all words from text B
that are connected to the word aligned to the target
word in text A via a single dependency relation.
2.6 Experimental Features
As an experiment, we included features from a com-
mercial text clustering software that is currently
being developed by our team (Greiner and Evert, in
preparation). We used this tool ? which combines
ideas from Latent Semantic Indexing and distribu-
tional semantics with multiple clustering steps ? as
a black box.
We loaded all training, development and test
items for a given task into the system and applied
the clustering algorithm. However, we did not
make use of the resulting topic clusters. Instead, we
computed cosine similarities for each pair (s
1
,s
2
)
of sentences (or other textual units) based on the in-
ternal representation. In addition, we computed the
average neighbour rank of the two sentences, based
on the rank of s
2
among the nearest neighbours of
s
1
and vice versa.
Since these features are generated from the task
data themselves, they should adapt automatically
to the range of meaning differences present in a
given data set.
2.7 Machine Learning
Using all the features described above, we have a
total of 39 individual features that measure seman-
tic similarity between two texts (cf. Sections 2.2 to
2.5) and two experimental features (cf. Section 2.6).
In order to obtain a single similarity score, we use
the scikit-learn
6
(Pedregosa et al., 2011) implemen-
tation of support vector regression. In our cross-
validation experiments we got the best results with
an RBF kernel of degree 2 and a penaltyC= 0.7, so
those are the parameters we use in our experiments.
The SemEval-2014 Task 1 also includes a classi-
fication subtask for which we use the same 39+2
features for training a support vector classifier.
Cross-validation suggests that the best parameter
setting is a polynomial kernel of degree 2 and a
penalty C = 2.5.
3 SemEval-2014 Task 1
3.1 Task Description
The focus of the shared task on ?Evaluation of com-
positional distributional semantic models on full
sentences through semantic relatedness and textual
entailment? (Marelli et al., 2014a) lies on the com-
positional nature of sentence semantics. By using
a specially created data set (Marelli et al., 2014b)
that tries to avoid multiword expressions and other
idiomatic features of language outside the scope of
compositional semantics, it provides a testbed for
systems implementing compositional variants of
distributional semantics. There is also an additional
subtask for detecting the entailment relation (entail-
ment, neutral, contradiction) between to sentences.
Although SemantiKLUE lacks a truly sophisti-
cated component for dealing with compositional
semantics (besides trying to incorporate the depen-
dency structure of the texts), the system takes the
seventh place in the official ranking by Pearson
correlation with a correlation coefficient of 0.780
6http://scikit-learn.org/
535
(best of 17 systems: 0.828). In the entailment sub-
task, the system even takes the fourth place with an
accuracy of 0.823 (best of 18 systems: 0.846).
3.2 Experiments
The official runs we submitted for this task were
created by a work-in-progress version of Semanti-
KLUE that did not contain all the features de-
scribed above. In this section, we report on some
post-hoc experiments with the complete system us-
ing all the features as well as various subsets of
features. See Table 1 for an overview of the results.
Run r ? MSE Acc.
primary run 0.780 0.736 0.403 0.823
best run 0.782 0.738 0.398 0.823
complete system 0.798 0.754 0.373 0.820
no deps 0.793 0.748 0.383 0.817
no deps, no WN 0.763 0.713 0.432 0.793
complete + experimental 0.801 0.757 0.367 0.823
only DSM alignment 0.729 0.670 0.484 0.746
only WordNet 0.708 0.636 0.515 0.715
only simple 0.676 0.667 0.561 0.754
only DSM document 0.660 0.568 0.585 0.567
only deps 0.576 0.565 0.688 0.614
Table 1: Results for task 1 (Pearson?s r, Spearman?s
? , mean squared error and accuracy).
The whole system as described above, without
the experimental features, performs even a bit bet-
ter in the semantic similarity subtask (taking place
6) and only slightly worse in the entailment subtask
(still taking place 4) than the official submissions.
Adding the experimental features slightly improves
the results but does not lead to a better position in
the ranking.
We are particularly interested in the impact of
the resource-heavy features derived from the de-
pendency structure of the texts and from Word-
Net. If we use the complete system without the
dependency-based features (emulating the case of
a language for which we have access to a WordNet-
like resource but not to a parser), we get results
that are only marginally worse than those for the
complete system and lead to the same places in the
rankings. Additionally leaving out WordNet has a
bigger impact and results in places 9 and 8 in the
rankings.
Regarding the individual feature groups, the
DSM-alignment-based measures are the best fea-
ture group for predicting semantic similarity and
the simple heuristic measures are the best feature
group for predicting entailment.
4 SemEval-2014 Task 3
4.1 Task Description
Unlike the other tasks, which focus on similar-sized
texts, the shared task on ?Cross-Level Semantic
Similarity? (Jurgens et al., 2014) is about measur-
ing semantic similarity between textual units of
different lengths. It comprises four subtasks com-
paring I) paragraphs to sentences, II) sentences to
phrases, III) phrases to words and IV) words to
word senses (taken from WordNet). Due to the
nature of this task, performance in it might be es-
pecially useful as an indicator for the usefulness of
a system in the area of summarization.
SemantiKLUE takes the fourth place out of 38 in
both the official ranking by Pearson correlation and
the alternative ranking by Spearman correlation.
4.2 Additional Preprocessing
For the official run we perform some additional pre-
processing on the data for the two subtasks on com-
paring phrases to words and words to word senses.
On the word level we combine the word with the
glosses of all its WordNet senses and on the word
sense level we replace the WordNet sense indica-
tion with its corresponding lemmata and gloss. As
our post-hoc experiments show that has a nega-
tive effect on performance in the phrase-to-word
subtask. Therefore, we skip the additional prepro-
cessing on that level for our experiments described
below.
4.3 Experiments
For each of the four subtasks, we perform the
same experiments as described in Section 3.2: We
compare the official run submitted from a work-
in-progress version of SemantiKLUE with the re-
sults from the whole system; we see how the sys-
tem performs without dependency-based features
and WordNet-based features; we try out the experi-
mental features; we determine the most important
feature group for the subtask. Table 2 gives an
overview of the results.
4.3.1 Paragraph to Sentence
Our submitted run takes the fifth place (ties with
another system) in the official ranking by Pearson
correlation with a correlation coefficient of 0.817
(best of 34 systems: 0.837) and seventh place in
the alternative ranking by Spearman correlation.
536
Run Paragraph to sent. Sent. to phrase Phrase to word Word to sense
r ? r ? r ? r ?
official 0.817 0.802 0.754 0.739 0.215 0.218 0.314 0.327
complete system 0.817 0.802 0.754 0.739 0.284 0.289 0.316 0.330
no deps 0.815 0.802 0.752 0.739 0.309 0.313 0.312 0.329
no deps, no WN 0.813 0.802 0.736 0.721 0.335 0.335 0.234 0.248
complete + experimental 0.816 0.800 0.752 0.738 0.292 0.298 0.318 0.330
only DSM alignment 0.799 0.789 0.724 0.711 0.302 0.301 0.216 0.216
only WordNet 0.787 0.769 0.664 0.641 0.186 0.171 0.313 0.311
only simple 0.807 0.793 0.686 0.672 0.128 0.121 0.089 0.093
only DSM document 0.629 0.624 0.546 0.558 0.247 0.240 0.144 0.148
only deps 0.655 0.621 0.449 0.440 0.036 0.057 ?0.080 ?0.076
Table 2: Results for task 3 (Pearson?s r and Spearman?s ?).
The complete SemantiKLUE system gives identical
results. Leaving out the resource-heavy features
based on the dependency structure and WordNet
diminishes the results only very slightly, though
it still resolves the tie and puts the system on the
sixth place in the Pearson ranking. Adding the
experimental features to the complete system has a
minor negative effect.
Probably due to the length of the texts, our sim-
ple heuristic measures surpass the DSM-alignment-
based measures as the best feature group for pre-
dicting semantic similarity.
4.3.2 Sentence to Phrase
In this subtask, SemantiKLUE takes the fourth
place in both the official ranking with a Pearson
correlation coefficient of 0.754 (best of 34 systems:
0.777) and in the alternative ranking by Spearman
correlation. The complete system performs iden-
tically to our submitted run and leaving out the
dependency-based features has little impact on the
results. Additionally also leaving out the WordNet-
based features has more impact on the results and
puts the system on the eighth place in the official
ranking. Just as in the paragraph-to-sentence sub-
task, adding the experimental features to the com-
plete system has a slightly negative effect.
For this subtask, the DSM-alignment-based mea-
sures are clearly the feature group that yields the
best results.
4.3.3 Phrase to Word
For our submitted run we performed the additional
preprocessing described in Section 4.2 resulting
in the eleventh place in the official ranking with
a Pearson correlation coefficient of 0.215 (best of
22 systems: 0.415) and the 14th place in the alter-
native ranking by Spearman correlation. For our
experiments with the complete system we skip that
additional preprocessing step, i. e. we do not add
the WordNet glosses to the word, and drastically
improve the results, putting our system on the third
place in the official ranking. Even more interesting
is the observation that leaving out the resource-
heavy features further improves the results, putting
the system on the second place. In consistency
with those observations, the DSM-alignment-based
measures are not only the strongest individual fea-
ture group but also yield better results when taken
alone than the complete system.
In contrast to the first two subtasks, adding the
experimental features to the complete systems has
a slightly positive effect here.
4.3.4 Word to Sense
In the word-to-sense subtask, SemantiKLUE takes
the third place in both the official ranking with a
Pearson correlation coefficient of 0.316 (best of
20 systems: 0.381) and in the alternative rank-
ing by Spearman correlation. The complete sys-
tem performs slightly better than our submitted
run and adding the experimental features gives
another marginal improvement. Leaving out the
dependency-based features has little impact but
also leaving out the WordNet-based features sev-
erly hurts performance. The reason for that be-
haviour becomes clear when we look at the results
for the individual feature groups: the WordNet-
based measures are clearly the strongest feature
group for predicting the semantic similarity be-
tween words and word senses.
5 SemEval-2014 Task 10
5.1 Task Description
The shared task on ?Multilingual Semantic Textual
Similarity? (Agirre et al., 2014) is a continuation
of the SemEval-2012 and *SEM 2013 shared tasks
537
Run d
e
f
t
-
f
o
r
u
m
d
e
f
t
-
n
e
w
s
h
e
a
d
l
i
n
e
s
i
m
a
g
e
s
O
n
W
N
t
w
e
e
t
-
n
e
w
s
w
.
m
e
a
n
best run 0.349 0.643 0.733 0.773 0.855 0.640 0.694
complete (all training data) 0.432 0.638 0.660 0.736 0.810 0.659 0.676
best overall training data 0.464 0.672 0.657 0.771 0.836 0.690 0.700
best overall, no deps 0.457 0.675 0.636 0.764 0.834 0.690 0.694
best overall, no deps, no WN 0.426 0.653 0.617 0.719 0.780 0.636 0.654
best overall + experimental 0.466 0.674 0.673 0.772 0.849 0.687 0.706
best individual training data 0.475 0.706 0.711 0.788 0.852 0.715 0.727
best individ., no deps 0.465 0.700 0.699 0.781 0.848 0.722 0.722
best individ., no deps, no WN 0.448 0.722 0.677 0.752 0.791 0.706 0.697
best individ. + experimental 0.475 0.711 0.715 0.795 0.864 0.721 0.733
Table 3: Results for task 10.
on semantic textual similarity (Agirre et al., 2012;
Agirre et al., 2013). It comprises two subtasks:
English semantic textual similarity and Spanish
semantic textual similarity. For each subtask, there
are sentence pairs from various genres.
We only participate in the English subtask and
take the 13th place out of 38 with a weighted mean
of Pearson correlation coefficients of 0.694 (best
system: 0.761).
5.2 Experiments
From participating in the *SEM 2013 shared task
on semantic textual similarity (Greiner et al., 2013)
we already know that the composition of the train-
ing data is one of the strongest influences on system
performance in this task. As the individual data sets
are not very similar to each other, we tried to come
up with a good subset of the available training data
for each data set. In doing so, we were moderately
successful as the results in Table 3 show. Run-
ning the complete system with all of the available
training data on all test data sets results in a lower
weighted mean than our submitted run. If we stick
to using the same training data for all test data sets
and optimize the subset of the training data we use,
we achieve a slightly better result than our submit-
ted run (the optimal subset consists of the FNWN,
headlines, MSRpar, MSRvid and OnWN data sets).
Using that optimal subset of the training data and
adding the experimental features to the complete
system has a minor positive effect on the weighted
mean, with the biggest impact on the headlines and
OnWN data sets. Using the complete system with-
out the dependency-based features gives roughly
the same results but omitting all resource-heavy
features has clearly a negative impact on the re-
sults.
In another experiment we try to optimize our
strategy of finding the best subset of the training
data for each test data set. Doing that gives us a
considerably higher weighted mean than using the
same training data for every test data set, putting
our system on the eighth place. Using the complete
system, we find that the best training data subsets
for the individual test data sets are those shown in
Table 4.
test set training sets
deft-forum FNWN, headlines, MSRvid
deft-news FNWN, MSRpar, MSRvid
headlines FNWN, headlines, MSRpar
images FNWN, MSRpar, MSRvid
OnWN FNWN, MSRvid, OnWN
tweet-news FNWN, headlines, MSRpar, MSRvid
Table 4: Optimal subsets of training data for use
with the complete SemantiKLUE system.
If we add the experimental features to the com-
plete system and still optimize the training data sub-
sets, we get a small boost to the results. Leaving out
the dependency-based features does not really hurt
performance but also omitting the WordNet-based
features has a negative impact on the results.
6 Conclusion
SemantiKLUE is a robust system for predicting the
semantic similarity between two texts that can also
be used to predict entailment. The system achieves
good or very good results in three SemEval-2014
tasks representing a broad variety of semantic simi-
larity problems (cf. Table 5 for an overview of the
results of all subtasks). Our two-staged strategy of
computing several similarity measures and using
them as input for a machine learning mechanism
538
Subtask submitted run complete system winner score
score rank score rank
Task 1, similarity 0.780 7/17 0.798 6/17 0.828
Task 1, entailment 0.823 4/18 0.820 4/18 0.846
Task 3, par-2-sent 0.817 5/34 0.817 5/34 0.837
Task 3, sent-2-phr 0.754 4/34 0.754 4/34 0.777
Task 3, phr-2-word 0.215 11/22 0.284 3/22 0.415
Task 3, word-2-sense 0.314 3/20 0.316 3/20 0.381
Task 3 overall N/A 4/38 N/A 3/38 N/A
Task 10, deft-forum 0.349 20/38 0.464 12/38 0.531
Task 10, deft-news 0.643 22/37 0.672 19/37 0.785
Task 10, headlines 0.733 15/37 0.657 20/37 0.784
Task 10, images 0.773 16/37 0.771 17/37 0.834
Task 10, OnWN 0.855 3/36 0.836 7/36 0.875
Task 10, tweet-news 0.640 20/37 0.690 12/37 0.792
Task 10 overall 0.694 13/38 0.700 13/38 0.761
Table 5: Overview of results.
proves itself to be adaptable to the needs of the
individual tasks.
Using the maximum-weight-matching algorithm
for aligning words from both texts that have similar
distributional semantics leads to very sound fea-
tures. Even without the resource-heavy features,
the system yields competitive results. In some use
cases, those expensive features are almost negligi-
ble. Without being dependent on the availability of
resources like a dependency parser or a WordNet-
like lexical database, SemantiKLUE can easily be
adapted to other languages.
Our experimental features from the commercial
topic clustering software are useful in some cases;
in others at least they do not hurt performance.
We feel that the heuristics based on the depen-
dency structure of the texts do not exhaust all the
possibilities that dependency parsing has to offer.
In the future we would like to try out more mea-
sures based on those structures. Probably some
kind of graph edit distance incorporating the sim-
ilarities between both dependency relations and
words might turn out to be a powerful feature.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2012. SemEval-2012 task
6: A pilot on semantic textual similarity. In First
Joint Conference on Lexical and Computational Se-
mantics, pages 385?393. ACL.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), volume 1: Proceedings of the Main
Conference and the Shared Task, pages 32?43. ACL.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed Web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Marie-Catherine de Marneffe and Christopher D. Man-
ning, 2008. Stanford typed dependencies manual.
Stanford University.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Zvi Galil. 1986. Efficient algorithms for finding
maximum matching in graphs. Computing Surveys,
18(1):23?38.
Paul Greiner and Stefan Evert. in preparation. The
Klugator Engine: A distributional approach to open
questions in market research.
Paul Greiner, Thomas Proisl, Stefan Evert, and Besim
Kabashi. 2013. KLUE-CORE: A regression model
of semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), volume 1: Proceedings of the Main Con-
ference and the Shared Task, pages 181?186. ACL.
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart.
2008. Exploring network structure, dynamics, and
function using NetworkX. In G?el Varoquaux,
539
Travis Vaught, and Jarrod Millman, editors, Pro-
ceedings of the 7th Python in Science Conference
(SciPy2008), pages 11?15, Pasadena, CA.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013.
UMBC_EBIQUITY-CORE: Semantic textual
similarity systems. In Proceedings of the Second
Joint Conference on Lexical and Computational
Semantics. ACL.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval-2014).
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 265?283. MIT Press, Cambridge, MA.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books Ngram
Corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169?174, Jeju Island, Korea.
ACL.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, San Francisco, CA. Morgan Kaufmann.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014b. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
ceedings of LREC 2014, Reykjavik. ELRA.
Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ?douard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Roland Sch?fer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient
tool chain. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Eval-
uation (LREC ?12), pages 486?493, Istanbul, Turkey.
ELRA.
Hinrich Sch?tze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97?123.
540
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 551?555,
Dublin, Ireland, August 23-24, 2014.
SentiKLUE: Updating a Polarity Classifier in 48 Hours
Stefan Evert and Thomas Proisl and Paul Greiner and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen, Germany
{stefan.evert,thomas.proisl,paul.greiner,besim.kabashi}@fau.de
Abstract
SentiKLUE is an update of the KLUE po-
larity classifier ? which achieved good and
robust results in SemEval-2013 with a sim-
ple feature set ? implemented in 48 hours.
1 Introduction
The SemEval-2014 shared task on ?Sentiment
Analysis in Twitter? (Rosenthal et al., 2014) is a re-
run of the corresponding shared task from SemEval-
2013 (Nakov et al., 2013) with new test data.
It focuses on polarity classification in computer-
mediated communication such as Twitter, other
micro-blogging services, and SMS. There are two
subtasks: the goal of Message Polarity Classifica-
tion (B) is to classify an entire SMS, tweet or other
message as positive (pos), negative (neg) or neutral
(ntr); in the subtask on Contextual Polarity Disam-
biguation (A), a single word or short phrase has to
be classified in the context of the whole message.
The training data are the same as in SemEval-
2013. The test data from 2013 are used as a devel-
opment set in order to select features and tune ma-
chine learning algorithms, but may not be included
in the training data. The 2014 test set comprises
the development data, new Twitter messages, Live-
Journal entries as out-of-domain data, and a small
number of tweets containing sarcasm (see Rosen-
thal et al. (2014) for further details). For subtask B,
there are 10,239 training items, 5,907 items in the
development set, and 3,861 additional unseen items
in the new test set. For subtask A, there are 9,505
training items, 6,769 items in the development set,
and 3,912 additional items in the test set.
Our team participated in the SemEval-2013
shared task with a relatively simple, but robust
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
system (KLUE) based on a maximum entropy clas-
sifier and a small set of features (Proisl et al., 2013).
Despite its simplicity, KLUE performed very well
in subtask B, ranking 5th out of 36 constrained
systems on the Twitter data and 3rd out of 28 on
the SMS data. Results for contextual polarity dis-
ambiguation (subtask A) were less encouraging,
with rank 14 out of 21 constrained systems on the
Twitter data and rank 12 out of 19 on the SMS data.
This paper describes our efforts to bring the
KLUE system up to date within a period of 48
hours. The results obtained by the new SentiKLUE
system are summarised in Table 1, showing that the
update was successful. The ranking of the system
has improved substantially in subtask A, making it
one of the best-performing systems in the shared
task. Rankings in subtask B are similar to those
of the previous year, showing that SentiKLUE has
kept up with recent developments. Moreover, dif-
ferences to the best-performing systems are much
smaller than in SemEval-2013.
2 Updating the KLUE polarity classifier
The KLUE polarity classifier is described in de-
tail by Proisl et al. (2013). It used the following
features as input for a maximum entropy classifier:
? The AFINN sentiment lexicon (Nielsen, 2011),
which provides numeric polarity scores ranging
from ?5 to +5 for 2,476 English word forms,
extended with distributionally similar words.
For each input message, the number of positive
and negative words as well as their average
polarity score were computed.
? Emoticons and Internet slang expressions that
were manually classified as positive, negative
or neutral. Features were generated in the same
way as for the sentiment lexicon.
? A bag-of-words representation that generates a
separate feature for each word form that occurs
in at least 5 different messages ( f ? 5). Only
551
task subset rank score best
B LJ14 3 / 42 73.99 74.84
B SMS13 4 / 42 67.40 70.28
B Twit13 6 / 42 69.06 72.12
B Twit14 10 / 42 67.02 70.96
B Sarcasm 24 / 42 43.36 58.16
A LJ14 1 / 20 85.61 85.61
A SMS13 6 / 20 85.16 89.31
A Twit13 2 / 20 90.11 90.14
A Twit14 2 / 20 84.83 86.63
A Sarcasm 2 / 20 79.32 82.75
Table 1: SentiKLUE results in SemEval 2014
Task 9 (among constrained systems). See Rosen-
thal et al. (2014) for further details and rankings
including the unconstrained systems.
single words (unigrams) were used, since ex-
periments with additional bigram features did
not lead to a clear improvement.
? A negation heuristic, which inverts the polar-
ity score of the first sentiment word within 4
tokens after a negation marker. In the bag-of-
words representation, the next 3 tokens after a
negation marker are prefixed with not_.
? For subtask A, these features were computed
both for the marked word or phrase and for the
rest of the message.
In order to improve the KLUE classifier, we drew
inspiration from two other systems participating
in the SemEval-2013 task: NRC-Canada (Moham-
mad et al., 2013), which won the task by a large
margin over competing systems, and GU-MLT-LT
(G?nther and Furrer, 2013), which used similar fea-
tures to our classifier, but obtained better results
due to careful selection and tuning of the machine
learning algorithm.
Mohammad et al. (2013) used a huge set of fea-
tures, including several sentiment lexica (both man-
ually and automatically created), word n-grams (up
to 4-grams with low frequency threshold), charac-
ter n-grams (3-grams to 5-grams), Twitter-derived
word clusters and a negation heuristic similar to
our approach. Features with the largest impact
in subtask B were sentiment lexica (esp. large au-
tomatically generated word lists), word n-grams,
character n-grams and the negation heuristic, in this
order. NRC-Canada achieved F-scores of 68.46
(SMS) and 69.02 (Twitter) in task B, as well as
88.00 (SMS) and 88.93 (Twitter) in task A.
G?nther and Furrer (2013) claim that state-of-
the-art results can be obtained with a small fea-
ture set if a suitable machine learning algorithm
is chosen. They used stochastic gradient descent
(SGD) and tuned its parameters by grid search. GU-
MLT-LT achieved scores of 62.15 (SMS) and 65.27
(Twitter) in task B, as well as 88.37 (SMS) and
85.19 (Twitter) in task A.
We therefore decided to make use of a wider
range of sentiment lexica, extend the bag-of-words
representation to bigrams, implement character n-
gram features, and experiment with different ma-
chine learning algorithms, resulting in the Senti-
KLUE system described in the following section.
3 The SentiKLUE system
SentiKLUE is an improved version of the KLUE
system and uses the same tokenisation, preprocess-
ing and negation heuristics; see Proisl et al. (2013)
for details. The features described below are used
as input for a machine learning classifier that pre-
dicts the polarity categories positive (pos), nega-
tive (neg) or neutral (ntr). As in KLUE and GU-
MLT-LT, the implementations of the Python library
scikit-learn (Pedregosa et al., 2011)
1
are used. We
tested four different learning algorithms: logistic
regression (MaxEnt), stochastic gradient descent
(SGD), linear SVM (LinSVM) and SVM with a
RBF kernel (SVM). Parameters were tuned by grid
search and the best-performing algorithm was cho-
sen for each subtask. SentiKLUE makes use of the
following features:
? Several sentiment lexica, which are treated as
lists of positive and negative polarity words.
Numerical scores are converted by setting ap-
propriate cutoff thresholds. For each lexicon,
we compute the number of positive and neg-
ative words occurring in a message as fea-
tures, with separate counts for negated and non-
negated contexts.
? AFINN (Nielsen, 2011)
2
? Bing Liu lexicon (Hu and Liu, 2004)
3
? MPQA (Wilson et al., 2005)
4
? SentiWords (Guerini et al., 2013)
5
; we cre-
1
http://scikit-learn.org/
2
http://www2.imm.dtu.dk/pubdb/views/publication_de-
tails.php?id=6010
3
http://www.cs.uic.edu/~liub/FBS/sentiment-
analysis.html
4
http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/
5
http://hlt.fbk.eu/technologies/sentiwords
552
ated two word lists with score thresholds of
0.3 and 0.1
? Sentiment140 (Mohammad et al., 2013)
6
,
which was compiled from a corpus of 1.6
million tweets for NRC-Canada; we created
separate lists for normal words and hashtags
with a score threshold of 1.0
? NRC Hashtag Sentiment Lexicon (Moham-
mad et al., 2013)
7
, which contains words
that exhibit a strong statistical association
(PMI score) to positive or negative hashtags,
also compiled for NRC-Canada; again, we
created separate lists for normal words and
hashtags with a score threshold of 0.8
? a manual extension including synonyms,
antonyms and several word lists from on-
line sources, compiled by the SNAP team
(Schulze Wettendorf et al., 2014)
? an automatic extension with distributionally
similar words (DSM extension), using a strat-
egy similar to Proisl et al. (2013)
? Word form unigrams and bigrams. After
some experimentation, the document frequency
threshold was set to f ? 5 for subtask B and
f ? 2 for subtask A.
? In order to include information from character
n-grams, we used a Perl implementation of n-
gram language models (Evert, 2008) that has
already been applied successfully to text cat-
egorization tasks (boilerplate detection in the
CLEANEVAL 2007 competition). We trained
three separate models on positive, negative and
neutral messages. We selected a 5-gram model
(n = 5) with strong smoothing (q = 0.7), which
minimized cross-entropy on the training data
(measured by cross-validation). For each mes-
sage in the training and test data, three features
were generated, specifying per-character cross-
entropy for each of the three n-gram models.
8
? Counts of positive and negative emoticons us-
ing the same lists as in the KLUE system.
? The same negation heuristic as in KLUE.
9
6
http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/
NRC-SentimentAnalysis.htm
7
ibid.
8
Note that these features had to be generated by cross-
validation on the training data to avoid catastrophic overfitting.
9
The full list of negation markers is not, don?t, doesn?t,
won?t, can?t, mustn?t, isn?t, aren?t, wasn?t, weren?t, couldn?t,
shouldn?t, wouldn?t. To our surprise, including further nega-
tion markers such as none, ain?t or hasn?t led to a decrease in
classification quality.
For subtask A, we chose a simplistic strategy and
computed the same set of features for the marked
word or phrase instead of the entire message. In
order to take context into account, the three class
probabilities assigned to the complete message by
a MaxEnt classifier were included as additional
features. No other features describing the context
of the marked expression were used.
Optionally, features were standardized and prior
class weights (2? for positive, 4? for negative)
were used in order to balance the predicted labels.
The best-performing machine learning algorithms
on the development set were MaxEnt for subtask B
(L1 penalty, C = 0.3) and linear SVM for subtask A
(L1 penalty, L2 loss, C = 0.5), as shown in Table 2.
4 Experiments and conclusion
In order to determine the importance of individ-
ual features, ablation experiments were carried out
for both subtasks by deactivating one group of fea-
tures at a time. Tables 3 and 4 show the resulting
changes in the official criterion F
p/n
separately for
each subset of the development and test sets, as
well as micro-averaged across the full development
set (DEV) and test set (GOLD). Rows are ordered
by feature impact on the full gold standard. Posi-
tive values indicate that a feature group has a neg-
ative impact on classification quality: results are
improved by omitting the features (which is often
the case for the Sarcasm subset).
The most important features are bag-of-words
unigrams and bigrams, closely followed by senti-
ment lexica. Training class weights had a strong
positive impact in subtask B, but decreased per-
formance in subtask A. In our official submission,
they were only used for subtask B. Full-message
polarity is the third most important feature in sub-
task A. Other features contributed relatively small
individual effects, but were necessary to achieve
state-of-the-art performance in combination. They
are often specific to one of the subtasks or to a
particular subset of the gold standard.
The bottom half of each table shows ablation
results for individual sentiment lexica, with all
other features active. Key resources are the stan-
dard lexica (AFINN, Liu, MPQA) as well as
Twitter-specific lexica (Sentiment140, NRC Hash-
tag). Noisy word lists (DSM extension, SNAP,
SentiWords) have a small or even a negative effect.
Surprisingly, the standard lexica seem to give mis-
leading cues on the Twitter 2014 subset (Table 3).
553
CV development set test set (gold standard)
task classifier F
all
F
pos
F
neg
F
ntr
F
all
F
p/n
acc. F
pos
F
neg
F
ntr
F
all
F
p/n
acc.
B MaxEnt .727 .724 .651 .772 .735 .688 .734 .731 .650 .750 .726 .691 .725
B SGD .725 .728 .645 .773 .736 .686 .734 .733 .656 .749 .727 .695 .726
B LinSVM .702 .687 .604 .743 .700 .646 .701 .699 .599 .716 .689 .649 .690
B SVM .702 .721 .631 .742 .716 .676 .712 .729 .636 .720 .709 .683 .706
A MaxEnt .864 .890 .872 .179 .849 .881 .863 .893 .853 .171 .841 .873 .856
A SGD .864 .889 .867 .223 .849 .878 .860 .891 .847 .188 .839 .869 .852
A LinSVM .860 .892 .876 .064 .847 .884 .865 .895 .856 .064 .838 .875 .857
A SVM .855 .890 .873 .024 .842 .881 .862 .892 .853 .014 .832 .872 .854
Table 2: Performance of different machine learning algorithms on the training data (CV), development set
and test set (F
all
= weighted average F-score; F
p/n
= official score; best results highlighted in bold font).
Task B SMS Twitter DEV LJ14 SMS13 Twit13 Twit14 Sarcasm GOLD
? bag of words ?.0837 ?.0322 ?.0502 ?.0344 ?.0807 ?.0316 ?.0335 +.0511 ?.0430
? sentiment lexica ?.0445 ?.0354 ?.0389 ?.0690 ?.0422 ?.0372 ?.0092 +.0750 ?.0363
? training weights ?.0033 ?.0413 ?.0266 ?.0275 ?.0077 ?.0482 ?.0204 ?.0342 ?.0294
? emoticons ?.0071 ?.0107 ?.0087 ?.0006 ?.0067 ?.0105 +.0004 +.0492 ?.0048
? bow bigrams ?.0074 ?.0005 ?.0035 +.0010 ?.0105 ?.0012 ?.0096 +.0956 ?.0028
? feature scaling ?.0027 ?.0010 ?.0014 ?.0021 ?.0030 ?.0026 ?.0004 ?.0034 ?.0020
? character n-grams +.0029 ?.0068 ?.0033 +.0012 +.0040 ?.0044 ?.0056 +.0056 ?.0015
? negation ?.0098 +.0019 ?.0014 ?.0016 ?.0049 +.0002 ?.0012 +.0351 ?.0002
? bow f ? 2 +.0017 +.0026 +.0022 +.0004 +.0021 ?.0003 +.0021 +.0171 +.0013
sentiment lexica:
? standard lexica ?.0206 ?.0135 ?.0152 ?.0245 ?.0234 ?.0124 +.0035 +.0586 ?.0124
? Twitter lexica ?.0026 +.0000 ?.0019 ?.0118 ?.0073 ?.0007 ?.0094 +.0034 ?.0066
? SentiWords ?.0008 ?.0010 ?.0009 ?.0034 ?.0015 ?.0005 ?.0075 +.0165 ?.0017
? hashtag lexica ?.0011 +.0021 +.0005 ?.0045 ?.0039 +.0035 +.0011 ?.0302 ?.0005
? DSM extension +.0047 ?.0032 ?.0002 ?.0070 +.0039 +.0022 ?.0025 +.0392 +.0002
? manual extension ?.0008 ?.0018 ?.0011 ?.0015 ?.0019 +.0000 +.0041 +.0361 +.0009
only standard lexica ?.0124 ?.0119 ?.0120 ?.0088 ?.0101 ?.0108 ?.0095 +.0439 ?.0094
only DSM extension ?.0303 ?.0260 ?.0262 ?.0427 ?.0287 ?.0251 +.0021 +.0183 ?.0230
Table 3: Results of feature ablation experiments for subtask B. Values show change in F
p/n
-score if feature
is excluded. Rows are sorted by impact of features on the full SemEval-2014 test data (GOLD).
Task A SMS Twitter DEV LJ14 SMS13 Twit13 Twit14 Sarcasm GOLD
? bag of words ?.0283 ?.0252 ?.0256 ?.0207 ?.0292 ?.0249 ?.0411 ?.0041 ?.0273
? sentiment lexica ?.0027 ?.0231 ?.0151 ?.0078 ?.0023 ?.0245 ?.0144 ?.0109 ?.0141
? context (class probs) +.0027 ?.0050 ?.0022 ?.0105 +.0017 ?.0057 ?.0171 +.0390 ?.0062
? negation ?.0081 ?.0041 ?.0052 ?.0064 ?.0063 ?.0024 ?.0058 +.0000 ?.0043
? bow bigrams ?.0045 ?.0009 ?.0022 ?.0014 ?.0046 +.0007 ?.0033 +.0208 ?.0014
? character n-grams ?.0015 +.0003 ?.0004 +.0003 ?.0038 +.0001 ?.0012 +.0085 ?.0012
? feature scaling +.0001 +.0001 +.0001 +.0009 +.0005 ?.0002 ?.0029 ?.0041 ?.0004
? emoticons +.0023 +.0026 +.0025 +.0016 +.0038 +.0012 ?.0062 +.0000 +.0004
bow f ? 5 +.0027 +.0000 +.0009 +.0082 +.0027 +.0006 ?.0025 +.0243 +.0015
? training weights +.0046 +.0072 +.0059 +.0104 +.0037 +.0050 +.0000 ?.0145 +.0040
sentiment lexica:
? standard lexica ?.0100 ?.0024 ?.0050 +.0014 ?.0086 ?.0035 ?.0055 +.0000 ?.0044
? Twitter lexica ?.0039 ?.0016 ?.0024 ?.0009 ?.0038 ?.0024 ?.0052 ?.0085 ?.0031
? hashtag lexica ?.0023 ?.0007 ?.0012 +.0000 ?.0014 ?.0019 ?.0030 ?.0126 ?.0017
? manual extensions ?.0016 +.0003 ?.0004 +.0021 ?.0025 ?.0009 +.0002 +.0000 ?.0007
? SentiWords +.0017 +.0005 +.0010 +.0001 +.0013 ?.0013 +.0001 +.0000 ?.0002
? DSM extensions +.0099 +.0011 +.0044 ?.0008 +.0098 ?.0006 ?.0004 ?.0085 +.0019
only standard lexica +.0030 ?.0038 ?.0011 ?.0019 +.0035 ?.0048 ?.0027 ?.0168 ?.0019
only DSM lexica ?.0114 ?.0085 ?.0094 ?.0035 ?.0117 ?.0104 ?.0057 ?.0338 ?.0089
Table 4: Results of feature ablation experiments for subtask A. Values show change in F
p/n
-score if feature
is excluded. Rows are sorted by impact of features on the full SemEval-2014 test data (GOLD).
554
References
Stefan Evert. 2008. A lightweight and efficient tool for
cleaning Web pages. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008), Marrakech, Morocco.
Marco Guerini, Lorenzo Gatti, and Marco Turchi.
2013. Sentiment analysis: How to derive prior po-
larities from SentiWordNet. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2013), pages 1259?
1269, Seattle, WA, October.
Tobias G?nther and Lenz Furrer. 2013. GU-MLT-LT:
Sentiment analysis of short messages using linguis-
tic features and stochastic gradient descent. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the
Seventh International Workshop on Semantic Evalu-
ation (SemEval 2013), pages 328?332, Atlanta, GA.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD ?04), pages
168?177, Seattle, WA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), pages 321?327, Atlanta, GA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval-2013).
Finn ?rup Nielsen. 2011. A new ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
In Proceedings of the ESWC2011 Workshop on Mak-
ing Sense of Microposts: Big things come in small
packages, number 718 in CEUR Workshop Proceed-
ings, pages 93?98, Heraklion, Greece, May.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825?2830.
Thomas Proisl, Paul Greiner, Stefan Evert, and Besim
Kabashi. 2013. KLUE: Simple and robust meth-
ods for polarity classification. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 395?401, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment analysis in Twitter. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Clemens Schulze Wettendorf, Robin Jegan, Allan
K?rner, Julia Zerche, Nataliia Plotnikova, Julian
Moreth, Tamara Schertl, Verena Obermeyer, Su-
sanne Streil, Tamara Willacker, and Stefan Evert.
2014. SNAP: A multi-stage XML pipeline for as-
pect based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 347?354,
Vancouver, BC, Canada.
555
