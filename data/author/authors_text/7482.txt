171
172
173
174
	 
    	 
  Retrieving Meaning-equivalent Sentences
for Example-based Rough Translation
Mitsuo Shimohata Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
mitsuo.shimohata@atr.co.jp
eiichiro.sumita@atr.co.jp
Yuji Matsumoto
Nara Institute of
Science and Technology
matsu@is.aist-nara.ac.jp
Abstract
Example-based machine translation (EBMT)
is a promising translation method for speech-
to-speech translation because of its robust-
ness. It retrieves example sentences similar to
the input and adjusts their translations to ob-
tain the output. However, it has problems in
that the performance degrades when input sen-
tences are long and when the style of inputs
and that of the example corpus are different.
This paper proposes a method for retrieving
?meaning-equivalent sentences? to overcome
these two problems. A meaning-equivalent
sentence shares the main meaning with an in-
put despite lacking some unimportant informa-
tion. The translations of meaning-equivalent
sentences correspond to ?rough translations.?
The retrieval is based on content words, modal-
ity, and tense.
1 Introduction
Speech-to-speech translation (S2ST) technologies con-
sist of speech recognition, machine translation (MT), and
speech synthesis (Waibel, 1996; Wahlster, 2000; Ya-
mamoto, 2000). The MT part receives speech texts rec-
ognized by a speech recognizer. The nature of speech
causes difficulty in translation since the styles of speech
are different from those of written text and are sometimes
ungrammatical (Lazzari, 2002). Therefore, rule-based
MT cannot translate speech accurately compared with its
performance for written-style text .
Example-based MT (EBMT) is one of the corpus-
based machine translation methods. It retrieves examples
similar to inputs and adjusts their translations to obtain
the output (Nagao, 1981). EBMT is a promising method
for S2ST in that it performs robust translation of ungram-
matical sentences and requires far less manual work than
rule-based MT.
However, there are two problems in applying EBMT
to S2ST. One is that the translation accuracy drastically
drops as input sentences become long. As the length of
a sentence becomes long, the number of retrieved similar
sentences greatly decreases. This often results in no out-
put when translating long sentences. The other problem
arises due to the differences in style between input sen-
tences and the example corpus. It is difficult to acquire
a large volume of natural speech data since it requires
much time and cost. Therefore, we cannot avoid using a
corpus with written-style text, which is different from that
of natural speech. This style difference makes retrieval of
similar sentences difficult and degrades the performance
of EBMT.
This paper proposes a method of retrieving sentences
whose meaning is equivalent to input sentences to over-
come the two problems. A meaning-equivalent sentence
means a sentence having the main meaning of an input
sentence despite lacking some unimportant information.
Such a sentence can be more easily retrieved than a simi-
lar sentence, and its translation is useful enough in S2ST.
We call this translation strategy example-based ?rough
translation.?
Retrieval of meaning-equivalent sentences is based on
content words, modality, and tense. This provides robust-
ness against long inputs and in the differences in style be-
tween the input and the example corpus. This advantage
distinguishes our method from other translation methods.
We describe the difficulties in S2ST in Section 2. Then,
we describe our purpose, features for retrieval, and re-
trieval method for meaning-equivalent sentences in Sec-
tion 3. We report an experiment comparing our method
with two other methods in Section 4. The experiment
demonstrates the robustness of our method to length of
input and the style differences between inputs and the ex-
ample corpus.
020
40
60
80
100
2-5 6-10 11-15 16-
N
um
be
r o
f S
en
te
nc
es
Sentence Length (Words)
Untranslated
Translated
Figure 1: Distribution of Untranslated Inputs by Length
2 Difficulty in Example-based S2ST
2.1 Translation Degradation by Input Length
A major problem with machine translation, regardless of
the translation method, is that performance drops rapidly
as input sentences become longer. For EBMT, the longer
input sentences become, the fewer similar example sen-
tences exist in the example corpus. Figure 1 shows
translation difficulty in long sentences in EBMT (Sumita,
2001). The EBMT system is given 591 test sentences
and returns translation result as translated/untranslated.
Untranslated means that there exists no similar example
sentences for the input. Although the EBMT is equipped
with a large example corpus (about 170K sentences), it
often failed to translate long inputs.
2.2 Style Differences between Concise and
Conversational
The performance of example-based S2ST greatly de-
pends on the example corpus. It is advantageous for an
example corpus to have a large volume and the same
style as the input sentences. A corpus of texts dictated
from conversational speech is favorable for S2ST. Un-
fortunately, it is very difficult to prepare such an exam-
ple corpus since this task requires laborious work such as
speech recording and speech transcription.
Therefore, we cannot avoid using a written-style cor-
pus, such as phrasebooks, to prepare a sufficiently large
volume of examples. Contained texts are almost gram-
matical and rarely contain unnecessary words. We call
the style used in such a corpus ?concise? and the style
seen in conversational speech ?conversational.?
Table 1 shows the average numbers of words in con-
cise (Takezawa et al, 2002) and conversational corpora
(Takezawa, 1999). Sentences in conversational style are
about 2.5 words longer than those in concise style in both
Language
English Japanese
Concise 5.4 6.2
Conversational 7.9 8.9
Table 1: Number of Words by Sentences
Language Model
Concise Conversational
Concise 16.4 58.3Test Conversational 72.3 16.3
Table 2: Cross Perplexity
English and Japanese. This is because conversational
style sentences contain unnecessary words or subordinate
clauses, which have the effects of assisting the listener?s
comprehension and avoiding the possibility of giving the
listener a curt impression.
Table 2 shows cross perplexity between concise and
conversational corpora (Takezawa et al, 2002). Perplex-
ity is used as a metric for how well a language model
derived from a training set matches a test set (Jurafsky
and Martin, 2000). Cross perplexities between concise
and conversational corpora are much higher than the self-
perplexity of either of the two styles. This result also
illustrates the great difference between the two styles.
3 Meaning-equivalent Sentence
Example-based S2ST has the difficulties described in
Section 2 when it attempts to translate inputs exactly.
Here, we set our translation goal to translating input sen-
tences not exactly but roughly. We assume that a rough
translation is useful enough for S2ST, since unimportant
information rarely disturbs the progress of dialogs and
can be recovered in the following dialog if needed. We
call this translation strategy ?rough translation.?
We propose ?meaning-equivalent sentence? to carry
out rough translation. Meaning-equivalent sentences are
defined as follows:
meaning-equivalent sentence
(to an input sentence)
A sentence that shares the main meaning with
the input sentence despite lacking some unim-
portant information. It does not contain infor-
mation additional to that in the input sentence.
Important information is subjectively recognized
mainly due to one of two reasons: (1) It can be surmised
from the general situation, or (2) It does not place a strong
restriction on the main information.
Input Sentence Unimportant?
1 Would you take a picture of me? Yes
2 Would you take a picture of this painting? No
3 Could you tell me a Chinese restaurant around here? Yes
4 Could you tell me a Chinese restaurant around here? No
5 My baggage was stolen from my room while I was out. Yes
6 Please change my room because the room next door is noisy. Yes
Figure 2: Examples of Unimportant Information
Figure 2 shows examples of unimportant/important in-
formation. Information to be examined is written in bold.
The information ?of me? in (1) and ?around here? in (3)
can be surmised from the general situation, while the in-
formation ?of this painting? in (2) and ?Chinese? would
not be surmised since it denotes a special object. The sub-
ordinate sentences in (4) and (5) are regarded as unimpor-
tant since they have small significance and are omittable.
3.1 Basic Idea of Retrieval
The retrieval of meaning-equivalent sentence depends on
content words and basically does not depend on func-
tional words. Independence from functional words brings
robustness to the difference in styles.
However, functional words include important informa-
tion for sentence meaning: the case relation of content
words, modality, and tense. Lack of case relation infor-
mation is compensated by the nature of the restricted do-
main. A restricted domain, as a domain of S2ST, has a
relatively small lexicon and meaning variety. Therefore,
if content words included in an input are given, their re-
lation is almost determined in the domain. Information
of modality and tense is extracted from functional words
and utilized in classifying the meaning of a sentence (de-
scribed in Section 3.2.2).
This retrieval method is similar to information re-
trieval in that content words are used as clues for retrieval
(Frakes and Baeza-Yates, 1992). However, our task has
two difficulties: (1) Retrieval is carried out not by docu-
ments but by single sentences. This reduces the effective-
ness of word frequencies. (2) The differences in modality
and tense in sentences have to be considered since they
play an important role in determining a sentence?s com-
municative meaning.
3.2 Features for Retrieval
3.2.1 Content Words
Words categorized as either noun1, adjective, adverb,
or verb are recognized as content words. Interrogatives
1Number and pronoun are included.
Modality Clues
tekudasai (auxiliary verb)Request
teitadakeru (auxiliary verb)
shi-tai (expression)
Desire te-hoshii (expression)
negau (verb)
ka (final particle)Question
ne (final particle)
nai (auxiliary verb or adjective)Negation
masen (auxiliary verb)
Tense Clues
Past ta (auxiliary verb)
Table 3: Clues for Discriminating Modalities in Japanese
are also included. Words such as particles, auxiliary
verbs, conjunctions, and interjections are recognized as
functional words.
We utilize a thesaurus to expand the coverage of the
example corpus. We call the relation of two words that
are the same ?identical? and words that are synonymous
in the given thesaurus ?synonymous.?
3.2.2 Modality and Tense
The meaning of a sentence is discriminated by its
modality and tense, since these factors obviously deter-
mine meaning. We defined two modality groups and
one tense group by examining our corpus. The modal-
ity groups are (?request?, ?desire?, ?question?, ?confir-
mation?, ?others?,) and (?negation?, ?others?.) The tense
group is (?past?, ?others?.) These modalities and tense
are distinguished by surface clues, mainly by particles
and auxiliary verbs. Table 3 shows a part of the clues
used for discriminating modalities in Japanese. Sentences
having no clues are classified as others. Figure 3 2 shows
2Japanese content words are written in sans serif style and
Japanese functional words in italic style.
Modality &
Sentence3 Tense4
hoteru wo yoyaku shi tekudasai request
(Will you reserve this hotel?)
hoteru wo yoyaku shi tai desire
(I want to reserve this hotel.)
hoteru wo yoyaku shi mashi ta ka? question
(Did you reserve this hotel?) past
hoteru wo yoyaku shi tei masen negation
(I do not reserve this hotel.)
Figure 3: Sentences and their Modality and Tense
sample sentences and their modality and tense. Clues are
underlined.
A speech act is a concept similar to modality in which
speakers? intentions are represented. The two studies in-
troduced information of the speech act in their S2ST sys-
tems (Wahlster, 2000; Tanaka and Yokoo, 1999). The two
studies and our method differ in the effect of speech act
information. Their effect of speech act information is so
small that it is limited to generating the translation text.
Translation texts are refined by selecting proper expres-
sions according to the detected speakers? intention.
3.3 Retrieval and Ranking
Sentences that satisfy the conditions below are recog-
nized as meaning-equivalent sentences.
1. It is required to have the same modality and tense as
the input sentence.
2. All content words are included (identical or synony-
mous) in the input sentence. This means that the set
of content words of a meaning-equivalent sentence
is a subset of the input.
3. At least one content word is included (identical) in
the input sentence.
If more than one sentence is retrieved, we must rank
them to select the most similar one. We introduce ?focus
area? in the ranking process to select sentences that are
meaning-equivalent to the main sentence in complex sen-
tences. We set the focus area as the last N words from the
word list of an input sentence. N denotes the number of
content words in meaning-equivalent sentences. This is
because main sentences in complex sentences tend to be
placed at the end in Japanese.
3Space characters are inserted into word boundaries in
Japanese texts.
4The value ?others? in all modality/tense groups is omitted.
Input
gaishutsu shi teiru aida ni,
(While I was out),
kaban wo nusuma re mashi ta
(my baggage was stolen.)
Meaning-equivalent Sentence
baggu wo nusuma re ta
(My bag was stolen).
C1 nusumu5 1
C2 ( kaban = baggu ) 1
C3 - 0
C4 - 0
C5 wo, re, ta 3
C6 suru, teiru, ni, masu 4
Figure 4: Example of Conditions for Ranking
Retrieved sentences are ranked by the conditions de-
scribed below. Conditions are described in order of prior-
ity. If there is more than one sentence having the highest
score under these conditions, the most similar sentence is
selected randomly.
C1: # of identical words in focus area.
C2: # of synonymous words in focus area.
C3: # of identical words in non-focus area.
C4: # of synonymous words in non-focus area.
C5: # of common functional words.
C6: # of different functional words.
(the fewer, the higher priority)
Figure 4 shows an example of conditions for ranking.
Content word in a focus area of input are underlined and
functional words are written in italic.
4 Experiment
4.1 Test Data
We used a bilingual corpus of travel conversation, which
has Japanese sentences and their English translations
(Takezawa et al, 2002). This corpus was sentence-
aligned, and a morphological analysis was done on both
languages by our morphological analysis tools. The bilin-
gual corpus was divided into example data (Example) and
test data (Concise) by extracting test data randomly from
the whole set of data.
In addition to this, we used a conversational speech
corpus for another set of test data (Takezawa, 1999). This
corpus contains dialogs between a traveler and a hotel
5Words are converted to base form.
020
40
60
80
100
1-5 6-10 11-15 16-
0
20
40
60
80
100
1-5 6-10 11-15 16-
0
20
40
60
80
100
1-5 6-10 11-15 16-
Method-1 Method-2 Proposed
Concise
Conversational
Input Length (Words)
Ac
cu
ra
cy
  (%
)
(Strict with Func. ) (Rough with Func.) (Rough w/o Func.)
Input Length (Words) Input Length (Words)
Ac
cu
ra
cy
  (%
)
Ac
cu
ra
cy
  (%
)
Figure 5: Results
# of AverageCorpus Sentences Length
Example 92,397 7.4
Concise 1,588 6.6
Conversational 800 10.1
Table 4: Statistics of the Corpora
receptionist. It tests the robustness in styles. We call this
test corpus ?Conversational.?
We use sentences including more than one content
word among the three corpora. The statistics of the three
corpora are shown in Table 4.
The thesaurus used in the experiment was ?Kadokawa-
Ruigo-Jisho? (Ohno and Hamanishi, 1984). Each word
has semantic code consisting of three digits, that is, this
thesaurus has three hierarchies. We defined ?synony-
mous? words as sharing exact semantic codes.
4.2 Compared Retrieval Methods
We use two example-based retrieval methods to show the
characteristic of the proposed method. The first method
(Method-1) uses ?strict? retrieval, which does not al-
low missing words in input. The method takes func-
tional words into account on retrieval. This method cor-
responds to the conventional EBMT method. The second
method (Method-2) uses ?rough? retrieval, which does
allow missing words in input, but still takes functional
words into account.
4.3 Evaluation Methodology
Evaluation was carried out by judging whether retrieved
sentences are meaning-equivalent to inputs. It must be
noted that inputs and retrieved sentences are both in
Japanese. We did not compare inputs and translations of
retrieved sentences, since translation accuracy is a matter
of the example corpus and does not concern our method.
The sentence with the highest score among retrieved
sentences was taken and evaluated. The sentences are
marked manually as meaning-equivalent or not by a
Japanese native. A meaning-equivalent sentence includes
all important information in the input but may lack some
unimportant information.
4.4 Results
Figure 5 shows the accuracy of the three methods with
the concise and conversational style data. Accuracy is
defined as the ratio of the number of correctly equivalent
sentences to that of total inputs. Inputs are classified into
four types by their word length.
The performance of Method-1 reflects the narrow cov-
erage and style-dependency of conventional EBMT. The
longer input sentences become, the more steeply its per-
formance degrades in both styles. The method can re-
trieve no similar sentence for inputs longer than eleven
words in conversational style.
Method-2 adopts a ?rough? strategy in retrieval. It
attains higher accuracy than Method-1, especially with
longer inputs. This indicates the robustness of the rough
retrieval strategy to longer inputs. However, the method
still has an accuracy difference of about 15% between the
two styles.
The accuracy of the proposed method is better than
that of Method-2, especially in conversational style. The
accuracy difference in longer inputs becomes smaller
(about 4%) than that of Method-2. This indicates the ro-
bustness of the proposed method to the differences be-
tween the two styles.
5 Related Work
5.1 EBMT
The rough translation proposed in this paper is a type of
EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999;
Brown, 2000). The basic idea of EBMT is that sentences
similar to the inputs are retrieved from an example corpus
and their translations become the basis of outputs.
Here, let us consider the difference between our
method and other EBMT methods by dividing similar-
ity into a content-word part and a functional-word part.
In the content-word part, our method and other EBMT
methods are almost the same. Content words are im-
portant information in a similarity measure process, and
thesauri are utilized to extend lexical coverage. In the
functional-word part, our method is characterized by dis-
regarding functional words, while other EBMT meth-
ods still rely on them for the similarity measure. In our
method, the lack of functional word information is com-
pensated by the semantically narrow variety in S2ST do-
mains and the use of information on modality and tense.
Consequently, our method gains robustness to length and
the style differences between inputs and the example cor-
pus.
5.2 Translation Memory
Translation memory (TM) is aimed at retrieving infor-
mative translation example from example corpus. TM
and our method share the retrieval strategy of rough and
wide coverage. However, recall is more highly weighted
than precision in TM, while recall and precision should
be equally considered in our method. To carry out
wide coverage retrieval, TM relaxed various conditions
on inputs: Preserving only mono-gram and bi-gram on
words/characters (Baldwin, 2001; Sato, 1992), remov-
ing functional words (Kumano et al, 2002; Wakita et al,
2000), and removing content words (Sumita and Tsut-
sumi, 1988). In our method, information on functional
words is removed and that on modality and tense is in-
troduced instead. Information on word order is also re-
moved while instead we preserve information on whether
each word is located in the focus area.
6 Conclusions
In this paper, we introduced the idea of meaning-
equivalent sentences for robust example-based S2ST.
Meaning-equivalent sentences have the same main mean-
ing as the input despite lacking some unimportant infor-
mation. Translation of meaning-equivalent sentences cor-
responds to rough translations, which aim not at exact
translation with narrow coverage but at rough translation
with wide coverage. For S2ST, we assume that this trans-
lation strategy is sufficiently useful.
Then, we described a method for retrieving meaning-
equivalent sentences from an example corpus. Retrieval
is based on content words, modality, and tense. This
strategy is feasible owing to the restricted domains, of-
ten adopted in S2ST, which have relatively small variety
in lexicon and meaning. An experiment demonstrated the
robustness of our method to input length and the style dif-
ferences between inputs and the example corpus.
Most MT systems aim to achieve exact translation, but
unfortunately they often output bad or no translation for
long conversational speeches. The rough translation pro-
posed in this paper achieves robustness in translation for
such inputs. This method compensates for the shortcom-
ings of conventional MT and makes S2ST technology
more practical.
Acknowledgements
The research reported here was supported in part by a
contract with the Telecommunications Advancement Or-
ganization of Japan entitled, ?A study of speech dialogue
translation technology based on a large corpus?.
References
T. Baldwin. 2001. Low-cost, high-performance transla-
tion retrieval: Dumber is better. In Proc. of the 39th
ACL, pages 18?25.
R. D. Brown. 2000. Automated generalization of trans-
lation examples. In Proc. of the 18th COLING.
M. Carl. 1999. Inducing translation templates for
example-based machine translation. In Proc. of the MT
Summit VII, pages 250?258.
W. B. Frakes and R. Baeza-Yates, editors. 1992. Infor-
mation Retrieval Data Structures & Algorithms. Pren-
tice Hall.
D. Jurafsky and J. H. Martin, editors. 2000. Speech and
Language Processing. Prentice Hall.
T. Kumano, I. Goto, H. Tanaka, N. Uratani, and T. Ehara.
2002. A translation aid system by retrieving bilingual
news database. In System and Computers in Japan,
pages 19?29.
G. Lazzari. 2002. The V1 framework program in Eu-
rope: Some thoughts about speech to speech trans-
lation research. In Proc. of 40th ACL Workshop on
Speech-to-Speech Translation, pages 129?135.
M. Nagao. 1981. A framework of a mechanical transla-
tion between Japanese and English by analogy princi-
ple. In Artificial and Human Intelligence, pages 173?
180.
S. Ohno and M. Hamanishi, editors. 1984. Ruigo-Shin-
Jiten. Kadokawa. (in Japanese).
S. Sato. 1992. CTM: An example-based translation aid
system. In Proc. of the 14th COLING, pages 1259?
1263.
E. Sumita and Y. Tsutsumi. 1988. A translation aid
system using flexible text retrieval based on syntax-
matching. In TRL Research Report TR87-1019. IBM
Tokyo Research Laboratory.
E. Sumita. 2001. Example-based machine translation
using DP-matching between work sequences. In Proc.
of the ACL 2001 Workshop on Data-Driven Methods
in Machine Translation, pages 1?8.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of the 3rd LREC, pages
147?152.
T. Takezawa. 1999. Building a bilingual travel con-
versation database for speech translation research. In
Proc. of the 2nd international workshop on East-Asian
resources and evaluation conference on language re-
sources and evaluation, pages 17?20.
H. Tanaka and A. Yokoo. 1999. An efficient statistical
speech act type tagging system for a speech translation
systems. In Proc. of the Association for Computational
Linguistics, pages 381?388.
T. Veale and A. Way. 1997. Gaijin: A bootstrapping,
template-driven approach to example-based MT. In
Proc. of the NeMNLP97.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
Alex Waibel. 1996. Interactive translation of conversa-
tional speech. IEEE Computer, 29(7):41?48.
Y. Wakita, K. Matsui, and Y. Sagisaka. 2000. Fine
keyword clustering using a thesaurus and example se-
tences for speech translation. In Proc. of International
Conference of Speech Language Processing, pages
390?393.
S. Yamamoto. 2000. Toward speech communications
beyond language barrier - research of spoken language
translation technologies at ATR -. In Proc. of ICSLP,
volume 4, pages 406?411.
Acquiring Synonyms from Monolingual
Comparable Texts
Mitsuo Shimohata1 and Eiichiro Sumita2
1 Oki Electric Industry Co., Ltd.,
2-5-7, Honmachi, Chuo-ku, Osaka City, Japan
shimohata363@oki.com
2 ATR Spoken Language Translation Research Laboratories,
2-2-2 Hikaridai, Keihanna Science City, Kyoto, Japan
eiichiro.sumita@atr.jp
Abstract. This paper presents a method for acquiring synonyms from
monolingual comparable text (MCT). MCT denotes a set of monolin-
gual texts whose contents are similar and can be obtained automatically.
Our acquisition method takes advantage of a characteristic of MCT that
included words and their relations are confined. Our method uses con-
textual information of surrounding one word on each side of the target
words. To improve acquisition precision, prevention of outside appear-
ance is used. This method has advantages in that it requires only part-of-
speech information and it can acquire infrequent synonyms. We evaluated
our method with two kinds of news article data: sentence-aligned par-
allel texts and document-aligned comparable texts. When applying the
former data, our method acquires synonym pairs with 70.0% precision.
Re-evaluation of incorrect word pairs with source texts indicates that
the method captures the appropriate parts of source texts with 89.5%
precision. When applying the latter data, acquisition precision reaches
76.0% in English and 76.3% in Japanese.
1 Introduction
There is a great number of synonyms, which denote a set of words sharing the
same meaning, in any natural language. This variety among synonyms causes
difficulty in natural language processing applications, such as information re-
trieval and automatic summarization, because it reduces the coverage of lexical
knowledge. Although many manually constructed synonym resources, such as
WordNet [4] and Roget?s Thesaurus [12], are available, it is widely recognized
that these knowledge resources provide only a small coverage of technical terms
and cannot keep up with newly coined words.
We propose a method to acquire synonyms from monolingual comparable
text (MCT). MCT denotes sets of different texts1 that share similar contents.
MCT are appropriate for synonym acquisition because they share not only many
1 In this paper, ?text? can denote various text chunks, such as documents, articles,
and sentences.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 233?244, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
234 M. Shimohata and E. Sumita
synonymous words but also the relations between the words in a each text.
Automatic MCT construction can be performed in practice through state-of-
the-art clustering techniques [2]. News articles are especially favorable for text
clustering since they have both titles and date of publication.
Synonym acquisition is based on a distributional hypothesis that words with
similar meanings tend to appear in similar contexts [5]. In this work, we adopt
loose contextual information that considers only the surrounding one word from
each side of the target words. This narrow condition enables extraction from
source texts2 that have different structures. In addition, we use another con-
straint, prevention of outside appearance, which reduces improper extraction by
looking over outside places of other texts. This constraint eliminates many non-
synonyms having the same surrounding words by chance. Since our method does
not cut off acquired synonyms by frequency, synonyms that appear only once
can be captured.
In this paper, we describe related work in Sect. 2. Then, we present our acqui-
sition method in Sect. 3 and describe its evaluation in Sect. 4. In the experiment,
we provide a detailed analysis of our method using monolingual parallel texts.
Following that, we explain an experiment on automatically constructed MCT
data of news articles, and conclude in Sect. 5
2 Related Work
Word Clustering from Non-comparable Text
There have been many studies on computing similarities between words based
on their distributional similarity [6,11,7]. The basic idea of the technique is that
words sharing a similar characteristic with other entities form a single cluster
[9,7]. A characteristic can be determined from relations with other entities, such
as document frequency, co-occurrence with other words, and adjectives depend-
ing on target nouns.
However, this approach has shortcomings in obtaining synonyms. First, words
clustered by this approach involve not only synonyms but also many near-
synonyms, hypernyms, and antonyms. It is difficult to distinguish synonyms
from other related words [8]. Second, words to be clustered need to have high
frequencies to determine similarity, therefore, words appearing only a few times
are outside the scope of this approach. These shortcomings are greatly reduced
with synonym acquisition from MCT owing to its characteristics.
Lexical Paraphrase Extraction from MCT
Here, we draw comparisons with works sharing the same conditions for acquiring
synonyms (lexical paraphrases) from MCT. Barzilay et al [1] shared the same
conditions in that their extraction relies on local context. The difference is that
2 We call texts that yield synonyms as ?source texts.?
Acquiring Synonyms from Monolingual Comparable Texts 235
their method introduces a refinement of contextual conditions for additional
improvement, while our method introduces two non-contextual conditions.
Pang et al [10] built word lattices from MCT, where different word paths
that share the same start nodes and end nodes represent paraphrases. Lattices
are formed by top-down merging based on structural information. Their method
has a remarkable advantage in that synonyms do not need to be surrounded
with the same words. On the other hand, their method is not applicable to
structurally different MCTs.
Shimohata et al [13] extracted lexical paraphrases based on the substitution
operation of edit operations. Text pairs having more than three edit distances
are excluded from extraction. Therefore, their method considers sentential word
ordering. Our findings, however, suggest that local contextual information is
reliable enough for extracting synonyms.
3 Synonym Acquisition
Synonym extraction relies on word pairs that satisfy the following three con-
straints: (1) agreement of context words; (2) prevention of outside appearance;
and (3) POS agreement. Details of these constraints are described in the follow-
ing sections. Then, we describe refinement of the extracted noun synonyms in
Sect. 3.4.
3.1 Agreement of Context Words
Synonyms in MCTs are considered to have the same context since they generally
share the same role. Therefore, agreement of surrounding context is a key feature
for synonym extraction. We define contextual information as surrounding one
word on each side of the target words. This minimum contextual constraint
permits extraction from MCT having different sentence structures.
Figure 1 shows two texts that have different structures. From this text
pair, we can obtain the following two word pairs WP-1 and WP-2 with con-
text words (synonym parts are written in bold). These two word pairs placed
in different parts would be missed if we used a broader range for contextual
information.
Sentence 1 The    severely    wounded    man    was    later    rescued    by    an    armored    personnel    carrier.
Troops    arived    in    an    armored    troop    carrier    and    saved    the    seriously    wounded    man.Sentence 2
Fig. 1. Extracting Synonyms with Context Words
236 M. Shimohata and E. Sumita
WP-1 ?the severely wounded? ? ?the seriously wounded?
WP-2 ?armored personnel carrier? ? ?armored troop carrier?
Words are dealt with based on their appearance, namely, by preserving their
capitalization and inflection. Special symbols representing ?Start-of-Sentence?
and ?End-of-Sentence? are attached to sentences. Any contextual words are ac-
cepted, but cases in which the surrounding words are both punctuation marks
and parentheses/brackets are disregarded.
3.2 Prevention of Outside Appearance
Prevention of outside appearance is a constraint based on characteristics of MCT.
It filters incorrect word pairs by looking into outside of synonym words and
context words in the other text (we call this outside region the ?outside part.?).
This constraint is based on the assumption that an identical context word ?
either a noun, verb, adjective, or adverb ? appears only once in a text. Actually,
our investigation of English texts in the Multiple-Translation Chinese Corpus
data (MTCC data described in Sect. 4.1) proves that 95.2% of either nouns,
verbs, adjectives, or adverbs follow this assumption.
This constraint eliminates word pairs that have a word satisfying the follow-
ing two constraints.
C1 The word appears in the outside part of the other text.
C2 The word does not appear in the synonym part of the other text.
The constraint C1 means that the word in the outside part of the other text
is considered as a correspondent word, and a captured word is unlikely to be
corresponding. In other words, appearance of the word itself is more reliable
than local context coincidence. The constraint C2 means that if the word is
included in the synonym part of the other text, this word pair is considered to
capture a corresponding word independent of the outside part.
Figure 2 illustrates an example of outside appearance. From S1 and S2, the
word pair ?Monetary Union? and ?Finance Minister Engoran? can be extracted.
However, the word ?Monetary? in S1 does appear in the synonym part of S2 but
does appear in another part of S2. This word pair is eliminated due to outside
appearance. However, if the word appears in the synonym part of S2, it remains
independent of the outside part.
This constraint is a strong filtering tool for reducing incorrect extraction, al-
though it inevitably involves elimination of appropriate word pairs. When apply-
ing this constraint to the MTCC data (described in Sect. 4.1), this filtering reduces
acquired noun pairs from 9,668 to 2,942 (reduced to 30.4% of non-filtered pairs).
3.3 POS Agreement
Word pairs to be extracted should have the same POS. This is a natural con-
straint since synonyms described in ordinary dictionaries share the same POS.
In addition, we focus our target synonym on content words such as nouns, verbs,
adjectives, and adverbs. A definition of each POS is given below.
Acquiring Synonyms from Monolingual Comparable Texts 237
Outside  Appearance
... the member countries of Economic  and   Monetary Union   of  Western Africa ...
Economy  and   Finance Minister Engoran   of  Cote d?Ivoire
said that the member of countries of the West Afcican Economic and Monetary Union
Word Pair
S1
S2
Fig. 2. Text Pair Having Outside Appearance
Nouns Consist of a noun sequence. Length of sequences is not limited.
Verbs Consist of one verb.
Adjectives Consist of one adjective.
Adverbs Consist of one adverb.
The word pair WP-1 satisfies the constraint for adverbs, and WP-2 satisfies
that for nouns. The MCT in Fig. 1 can produce the word pair ?the severely
wounded man? and ?the seriously wounded man.? This word pair is elimi-
nated because the synonym part consists of an adverb and an adjective and does
not satisfy the constraint.
3.4 Refinement of Noun Synonym Pairs
Acquired noun pairs require two refinement processes, incorporating context
words and eliminating synonyms that are subsets of others, since nouns are
allowed to contain more than one word.
After the extraction process, we can obtain noun pairs with their surrounding
context words. If these context words are considered to be a part of compound
nouns, they are incorporated into the synonym part. A context word attached to
the front of the synonym part is incorporated if it is either a noun or an adjective.
One attached to the back of the synonym part is incorporated if it is a noun.
Thus, when the noun pair ?air strike operation? = ?air attack operation? is
extracted, both context words remain since they are nouns.
Next, a noun pair included in another noun pair is deleted since the shorter
noun pair is considered a part of the longer noun pair. If the following noun pairs
Noun-1 and Noun-2 are extracted3, Noun-1 is deleted by this process.
Noun-1 ?British High? ? ?British Supreme?
Noun-2 ?British High Court? ? ?British Supreme Court?
3 All words in these expressions belong to ?proper noun, singular? (represented as
NNP in the Penn Treebank manner).
238 M. Shimohata and E. Sumita
4 Experiment
We used two types of MCT data: sentence-aligned parallel texts (MTCC) and
document-aligned comparable texts (Google News). Both data are based on news
articles, and their volumes are relatively small. The former data are used for
detailed analysis and the latter data are employed to show practical performance.
The Google News data consists of both English and Japanese versions. Table 1
shows the statistics of the experimental data, with the major difference between
MTCC and Google News data being ?Words per Text.? The text length of
Google News data is much longer than MTCC data since texts in Google News
data denote a whole article whereas those in MTCC data denote a sentence.
These two English data and the one Japanese data originally contained plain
text data. We applied the Charniak parser [3] to the English data and Chasen4
to the Japanese data to obtain POS information. It should be noted that we do
not use any information except that of POS from parsed results.
Table 1. Statistics of Three Experimental Data
MTCC Google News (E) Google News (J)
Text Clusters 993 61 88
Texts 10,655 394 417
Words 302,474 176,482 127,482
Texts per Cluster (Mean) 10.7 6.5 4.7
Words per Text (Mean) 28.4 447.9 305.7
(Variance) 364.5 64591.3 55495.7
MTCC: Multiple-reference Data from LDC
4.1 Multiple-Translation Chinese Corpus
The Linguistic Data Consortium (LDC) releases several multiple-translation cor-
pora to support the development of automatic means for evaluating translation
quality. The Multiple-Translation Chinese Corpus5 (MTCC) is one of those, and
it contains 105 news stories and 993 sentences selected from three sources of
journalistic Mandarin Chinese text. Each Chinese sentence was independently
translated into 11 English sentences by translation teams. We applied the Char-
niak parser to these 10,923 translations and obtained 10,655 parsed results. This
data comprises high-quality comparable texts, namely parallel texts.
We applied our method to the data and obtained 2,952 noun pairs, 887 verb
pairs, 311 adjective pairs, and 92 adverb pairs. Samples of acquired synonyms
are shown in Appendix A. Roughly speaking, the number of acquired word pairs
for each POS is proportional to the frequency of occurrence for that POS in the
MTCC data.
4 http://chasen.naist.jp/hiki/ChaSen/
5 Linguistic Data Consortium (LDC) Catalog Number LDC2002T01.
Acquiring Synonyms from Monolingual Comparable Texts 239
Extracted word pairs were manually evaluated by two methods: evaluation
with source texts and without source texts. First, an evaluator judged whether
extracted word pairs were synonyms or not without source texts. If two words
could be considered synonyms in many cases, they were marked ?yes,? otherwise
?no.? The criterion for judgment conformed to that of ordinary dictionaries, i.e.,
the evaluator judges whether given a word pair would be described as a synonym
by an ordinary dictionary. Therefore, word pairs heavily influenced by the source
texts are judged as ?no,? since these word pairs are not synonymous in general
situations. Morphological difference (e.g. singular/plural in nouns) is not taken
into consideration.
Next, word pairs evaluated as non-synonyms were re-evaluated with their
source texts. This evaluation is commonly used in paraphrase evaluation [1,10].
When word pairs could be considered to have the same meaning for the given
sentence pair, the evaluator marked ?yes,? otherwise ?no.? This evaluation clar-
ifies the ratio of the these two causes of incorrect acquisition.
1. The method captures proper places in sentences from source texts, but the
semantic difference between words in this place pair exceeds the range of
synonyms.
2. The method captures improper places in sentences from source texts that
have the same local context by chance.
An example of evaluation with source texts and without source texts is shown
in Fig. 3. Samples of this evaluation are also shown in Appendix A.
The precision, the ratio of ?yes? to the total, on MTCC data by each POS is
shown in Fig. 4, where the All POS precision with source texts reaches 89.5%.
This result suggests that our method could capture proper places of MCT pairs
with this level of precision. However, this precision falls to 70.0% without source
texts that represents synonym acquisition precision. This is because some of the
extracted word pairs have a hypernymous relationship or have great influence
on context in source texts.
Acquired word pairs include those occurring only once since our method does
not cut off according to word frequency. The amount of those occurring only once
accounts for 88.8% of the total. This feature is advantageous for acquiring proper
nouns; acquired word pairs including proper nouns account for 63.9% of the total
noun pairs.
Word pair judged as non-synonym
Synonym-1 Muslim robe
Synonym-2 sarong
Source Text Pair
Sentence-1 A resident named Daxiyate wears a turban and Muslim robe.
Sentence-2 A citizen named Daciat wore a Moslem hat and sarong.
Fig. 3. Example of Evaluation with Source Texts
240 M. Shimohata and E. Sumita
0 20 40 60 80 100  (%)
Nouns
Verbs
Adjectives
Adverbs
All POS
... Precision w/o Src. (%)
... Precision w/   Src. (%)
... Error Ratio (%)
Fig. 4. Precisions for MTCC Data
Here, we discuss our method?s coverage of all the synonyms in the training
data. Since it is very difficult to list all synonyms appearing in the training data,
we substitute identical word pairs for synonym pairs to estimate coverage. We
counted identical word pairs from all MCT pairs (Total) and those that have the
same context words (Same Context). The ratio of ?Same Context? to ?Total?
denotes coverage of our method and it was found to be 27.7%. If the tendency
of local context for identical word pairs is equal to that of synonym word pairs,
our method can capture 27.7% of the embedded synonyms in the training data.
We looked up acquired word pairs in WordNet6, a well-known publicly avail-
able thesaurus, to see how much general synonym knowledge is included in the
acquired synonyms. We could obtain 1,001 different word pairs of verbs, adjec-
tives, and adverbs after unifying conjugation7. WordNet knows, i.e., both words
are registered as entries, 951 word pairs (95.0%) among the 1,001 acquired pairs.
The thesaurus covers, i.e., both words are registered as synonyms, 205 word pairs
(21.6%) among 951 known pairs. This result shows that our method can actually
capture general synonym information. The remaining acquired word pairs are
still valuable since they include either general knowledge not covered by WordNet
or knowledge specific to news articles. For example, extracted synonym pairs,
?express?=?say,? ?present?=?report,? and ?decrease?=?drop? are found from
the data and are not registered as synonyms in WordNet.
4.2 Google News Data
We applied our method to Google News data acquired from ?Google News, 8?
provided by Google, Inc. This site provides clustered news articles that describe
the same events from among approximately 4,500 news sources worldwide.
6 http://www.cogsci.princeton.edu/?wn/
7 Acquired nouns are excluded from the consulting since many proper names are
acquired but are not covered in WordNet.
8 English version: http://news.google.com/
Japanese version: http://news.google.com/nwshp?ned=jp
Acquiring Synonyms from Monolingual Comparable Texts 241
From the Google News site, we gathered articles with manual layout-level
checking. This layout-level checking eliminates unrelated text such as menus
and advertisements. Our brief investigation found that clustered articles often
have a small overlap in described facts since each news site has its own interest
and viewpoint in spite of covering the same topic.
We use entire articles as ?texts? and do not employ an automatic sentence
segmentation and alignment tool. This is because the results derived from au-
tomatic sentence segmentation and alignment on the Google News data would
probably be unreliable, since the articles greatly differ in format, style, and con-
tent. Since our method considers only one-word-length context in each direction,
it can be applied to this rough condition. On the other hand, this condition en-
ables us to acquire synonyms placed at distant places in articles.
The next issue for the experimental conditions is the range for outside-
appearance checking. Following the condition of MTCC data, the outside-ap-
pearance checking range covers entire texts, i.e., outside appearance should be
checked throughout an article. However, this condition is too expensive to follow
since text length is much longer than that of MTCC data. We tested various
ranges of 0 (no outside-appearance checking), 10, 20, 40, 70, 100, 200, and un-
limited words. Figure 5 illustrates the range of outside-appearance checking.
We limit the words to be tested to nouns since the acquired amounts of other
POS types are not sufficient. Acquired noun pairs are evaluated without source
-20 words +20 words
Article
Synonym
+40 words
+ unlimited
-40 words
- unlimited
Fig. 5. Range for Outside-Appearance Checking
0
20
40
60
80
100
0 10 20 40 70 100 200 Unlimited
10
100
1000
Acquired Pairs
Precision
Precision (%) # of Acquired Pairs   
Range
Fig. 6. Precisions of Google (E) by Outside-Appearance Checking Range
242 M. Shimohata and E. Sumita
0
20
40
60
80
100
0 10 20 40 70 100 200 Unlimited
10
100
1000
Precision (%) # of Acquired Pairs   
Range
Acquired Pairs Precision
Fig. 7. Precisions of Google (J) by Outside-Appearance Checking Range
texts. Appendix B shows examples. Figures 6 and 7 display the amount and
precision for acquired nouns in each range of English data and Japanese data,
respectively.
The tendencies of these two data are similar, as the range expands, precision
increases and the amount of acquired pairs decreases at an exponential rate.
When the range is close to unlimited, precision levels off. The average preci-
sion at this stable range is 76.0% in English data and 76.3% in Japanese. The
precision improvement (from 13.8% to 76.0% in English data and from 9.5% to
76.3% in Japanese data) shows the great effectiveness of prevention of outside
appearance.
5 Conclusions
We proposed a method to acquire synonyms from monolingual comparable texts.
MCT data are advantageous for synonym acquisition and can be obtained auto-
matically by a document clustering technique. Our method relies on agreement
of local context, i.e., the surrounding one word on each side of the target words,
and prevention of outside appearance.
The experiment on monolingual parallel texts demonstrated that the method
acquires synonyms with a precision of 70.0%, including infrequent words. Our
simple method captures the proper place of MCT text pairs with a precision of
89.5%. The experiment on comparable news data demonstrated the robustness
of our method by attaining a precision of 76.0% for English data and 76.3%
for Japanese data. In particular, prevention of outside-appearance played an
important role by improving the precision greatly.
The combination of our acquisition method, an automatic document cluster-
ing technique, and daily updated Web texts enables automatic and continuous
synonym acquisition. We believe that the combination will bring great practical
benefits to NLP applications.
Acquiring Synonyms from Monolingual Comparable Texts 243
Acknowledgment
The research reported here was supported in part by a contract with the National
Institute of Information and Communications Technology entitled ?A study of
speech dialogue translation technology based on a large corpus?.
References
1. R. Barzilay and K. McKeown. Extracting paraphrases from a parallel corpus. In
Proc. of ACL-01, pages 50?57, 2001.
2. M.W. Berry, editor. Survey of Text Mining Clustering, Classification, and Re-
trieval. Springer, 2004.
3. E. Charniak. A maximum-entropy-inspired parser. In Proc. of the 1st Conference
of the North American Chapter of the Association for Computational Linguistics,
2000.
4. C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press, 1998.
5. Z. Harris. Mathematical Structures of Language. Interscience Publishers, 1968.
6. D. Hindle. Noun classification from predicate-argument structures. In Proc. of
ACL-90, pages 268?275, 1990.
7. D. Lin. Automatic retrieval and clustering of similar words. In Proc. of COLING-
ACL 98, pages 768?774, 1998.
8. D. Lin, S. Zhao, L. Qin, and M. Zhou. Identifying synonyms among distributionally
similar words. In Proc. of the 18th International Joint Conference on Artificial
Intelligence (IJCAI), pages 1492?1493, 2003.
9. C.D. Manning and H. Schu?tze, editors. Foundations of Statistical Natural Language
Processing, pages 265?314. MIT Press, 1999.
10. B. Pang, K. Knight, and D. Marcu. Syntax-based alignment of multiple trans-
lations: Extracting paraphrases and generating new sentences. In Proc. of HLT-
NAACL 2003, pages 181-188, 2003.
11. F. Pereira, N. Tishby, and L. Lee. Distributional clustering of English words. In
Proc. of ACL-93, pages 183?190, 1993.
12. P.M. Roget. Roget?s International Thesaurus. Thomas Y. Crowell, 1946.
13. M. Shimohata and E. Sumita. Identifying synonymous expressions from a bilin-
gual corpus for example-based machine translation. In Proc. of the 19th COLING
Workshop on Machine Translation in Asia, pages 20?25, 2002.
Appendix
A Samples of Acquired Words from MTCC and Their
Evaluation
Synonym-1 Synonym-2 Evaluation
press conference news conference Yes
foreign funds foreign capital Yes
Nouns complete finish Yes
disclose reveal Yes
military officials military officers No
Sunday radio program Sunday TV program No
244 M. Shimohata and E. Sumita
indicate show Yes
believe think Yes
Verbs cease stop Yes
consider study No
believe trust No
basic essential Yes
notable significant Yes
Adjectives massive substantial Yes
active good No
direct strong No
currently now Yes
certainly definitely Yes
Adverbs extremely very Yes
now officially No
absolutely entirely No
B Samples of Acquired Nouns from Google News (E)
and Their Evaluation
Synonym-1 Synonym-2 Evaluation
Karzai President Karzai Yes
Abu Omar Abu Umar Yes
Nouns relief effort relief mission Yes
Muslim community Muslim minority No
World Food Program World Health Organization No
