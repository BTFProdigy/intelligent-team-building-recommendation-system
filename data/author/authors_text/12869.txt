Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 671?679,
Beijing, August 2010
Learning the Scope of Negation via Shallow Semantic Parsing 
Junhui Li  Guodong Zhou?  Hongling Wang  Qiaoming Zhu 
School of Computer Science and Technology 
        Soochow University at Suzhou 
{lijunhui, gdzhou, redleaf, qmzhu}@suda.edu.cn 
 
                                                          
? Corresponding author 
Abstract 
In this paper we present a simplified shallow 
semantic parsing approach to learning the 
scope of negation (SoN). This is done by 
formulating it as a shallow semantic parsing 
problem with the negation signal as the 
predicate and the negation scope as its ar-
guments. Our parsing approach to SoN 
learning differs from the state-of-the-art 
chunking ones in two aspects. First, we ex-
tend SoN learning from the chunking level 
to the parse tree level, where structured syn-
tactic information is available. Second, we 
focus on determining whether a constituent, 
rather than a word, is negated or not, via a 
simplified shallow semantic parsing frame-
work. Evaluation on the BioScope corpus 
shows that structured syntactic information 
is effective in capturing the domination rela-
tionship between a negation signal and its 
dominated arguments. It also shows that our 
parsing approach much outperforms the 
state-of-the-art chunking ones. 
1 Introduction 
Whereas negation in predicate logic is 
well-defined and syntactically simple, negation 
in natural language is much complex. Gener-
ally, learning the scope of negation involves 
two subtasks: negation signal finding and nega-
tion scope finding. The former decides whether 
the words in a sentence are negation signals 
(i.e., words indicating negation, e.g., no, not, 
fail, rather than), where the semantic informa-
tion of the words, rather than the syntactic in-
formation, plays a critical role. The latter de-
termines the sequences of words in the sen-
tence which are negated by the given negation 
signal. Compared with negation scope finding, 
negation signal finding is much simpler and has 
been well resolved in the literature, e.g. with 
the accuracy of 95.8%-98.7% on the three 
subcorpora of the Bioscope corpus (Morante 
and Daelemans, 2009). In this paper, we focus 
on negation scope finding instead. That is, we 
assume golden negation signal finding. 
Finding negative assertions is essential in 
information extraction (IE), where in general, 
the aim is to derive factual knowledge from 
free text. For example, Vincze et al (2008) 
pointed out that the extracted information 
within the scopes of negation signals should 
either be discarded or presented separately 
from factual information. This is especially 
important in the biomedical domain, where 
various linguistic forms are used extensively to 
express impressions, hypothesized explanations 
of experimental results or negative findings. 
Szarvas et al (2008) reported that 13.45% of 
the sentences in the abstracts subcorpus of the 
BioScope corpus and 12.70% of the sentences 
in the full papers subcorpus of the Bioscope 
corpus contain negative assertions. In addition 
to the IE tasks in the biomedical domain, SoN 
learning has attracted more and more attention 
in some natural language processing (NLP) 
tasks, such as sentiment classification (Turney, 
2002). For example, in the sentence ?The chair 
is not comfortable but cheap?, although both 
the polarities of the words ?comfortable? and 
?cheap? are positive, the polarity of ?the chair? 
regarding the attribute ?cheap? keeps positive 
while the polarity of ?the chair? regarding the 
attribute ?comfortable? is reversed due to the 
negation signal ?not?.  
Most of the initial research on SoN learning 
focused on negated terms finding, using either 
some heuristic rules (e.g., regular expression), 
or machine learning methods (Chapman et al, 
2001; Huang and Lowe, 2007; Goldin and 
Chapman, 2003). Negation scope finding has 
been largely ignored until the recent release of 
671
the BioScope corpus (Szarvas et al, 2008; 
Vincze et al, 2008). Morante et al (2008) and 
Morante and Daelemans (2009) pioneered the 
research on negation scope finding by formu-
lating it as a chunking problem, which classi-
fies the words of a sentence as being inside or 
outside the scope of a negation signal. How-
ever, this chunking approach suffers from low 
performance, in particular on long sentences, 
due to ignoring structured syntactic information. 
For example, given golden negation signals on 
the Bioscope corpus, Morante and Daelemans 
(2009) only got the performance of 50.26% in 
PCS (percentage of correct scope) measure on 
the full papers subcorpus (22.8 words per sen-
tence on average), compared to 87.27% in PCS 
measure on the clinical reports subcorpus (6.6 
words per sentence on average). 
This paper explores negation scope finding 
from a parse tree perspective and formulates it 
as a shallow semantic parsing problem, which 
has been extensively studied in the past few 
years (Carreras and M?rquez, 2005). In par-
ticular, the negation signal is recast as the pre-
dicate and the negation scope is recast as its 
arguments. The motivation behind is that 
structured syntactic information plays a critical 
role in negation scope finding and should be 
paid much more attention, as indicated by pre-
vious studies in shallow semantic parsing 
(Gildea and Palmer, 2002; Punyakanok et al, 
2005). Our parsing approach to negation scope 
finding differs from the state-of-the-art chunk-
ing ones in two aspects. First, we extend nega-
tion scope finding from the chunking level into 
the parse tree level, where structured syntactic 
information is available. Second, we focus on 
determining whether a constituent, rather than a 
word, is negated or not. Evaluation on the 
BioScope corpus shows that our parsing ap-
proach much outperforms the state-of-the-art 
chunking ones. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 in-
troduces the Bioscope corpus on which our 
approach is evaluated. Section 4 describes our 
parsing approach by formulating negation 
scope finding as a simplified shallow semantic 
parsing problem. Section 5 presents the ex-
perimental results. Finally, Section 6 concludes 
the work. 
2 Related Work 
While there is a certain amount of literature 
within the NLP community on negated terms 
finding (Chapman et al, 2001; Huang and 
Lowe, 2007; Goldin and Chapman, 2003), 
there are only a few studies on negation scope 
finding (Morante et al, 2008; Morante and 
Daelemans, 2009).  
Negated terms finding  
Rule-based methods dominated the initial re-
search on negated terms finding. As a repre-
sentative, Chapman et al (2001) developed a 
simple regular expression-based algorithm to 
detect negation signals and identify medical 
terms which fall within the negation scope. 
They found that their simple regular expres-
sion-based algorithm can effectively identify a 
large portion of the pertinent negative state-
ments from discharge summaries on determin-
ing whether a finding or disease is absent. Be-
sides, Huang and Lowe (2007) first proposed 
some heuristic rules from a parse tree perspec-
tive to identify negation signals, taking advan-
tage of syntactic parsing, and then located ne-
gated terms in the parse tree using a corre-
sponding negation grammar. 
As an alternative to the rule-based methods, 
various machine learning methods have been 
proposed for finding negated terms. As a rep-
resentative, Goldin and Chapman (2003) a-
dopted both Na?ve Bayes and decision trees to 
distinguish whether an observation is negated 
by the negation signal ?not? in hospital reports.  
Negation scope finding  
Morante et al (2008) pioneered the research on 
negation scope finding, largely due to the 
availability of a large-scale annotated corpus, 
the Bioscope corpus. They approached the ne-
gation scope finding task as a chunking prob-
lem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecu-
tiveness of the negation scope. Morante and 
Daelemans (2009) further improved the per-
formance by combing several classifiers.  
Similar to SoN learning, there are some ef-
forts in the NLP community on learning the 
scope of speculation. As a representative, 
?zg?r and Radev (2009) divided speculation 
672
learning into two subtasks: speculation signal 
finding and speculation scope finding. In par-
ticular, they formulated speculation signal 
finding as a classification problem while em-
ploying some heuristic rules from the parse tree 
perspective on speculation scope finding. 
3 Negation in the BioScope Corpus 
This paper employs the BioScope corpus 
(Szarvas et al, 2008; Vincze et al, 2008)1, a 
freely downloadable negation resource from 
the biomedical domain, as the benchmark cor-
pus. In this corpus, every sentence is annotated 
with negation signals and speculation signals 
(if it has), as well as their linguistic scopes. 
Figure 1 shows a self-explainable example. In 
this paper, we only consider negation signals, 
rather than speculation ones. Our statistics 
shows that 96.57%, 3.23% and 0.20% of nega-
tion signals are represented by one word, two 
words and three or more words, respectively. 
Additional, adverbs (e.g., not, never) and de-
terminers (e.g., no, neither) occupy 45.66% and 
30.99% of negation signals, respectively. 
 
The Bioscope corpus consists of three sub-
corpora: the full papers and the abstracts from 
the GENIA corpus (Collier et al, 1999), and 
clinical (radiology) reports. Among them, the 
full papers subcorpus and the abstracts subcor-
pus come from the same genre, and thus share 
some common characteristics in statistics, such 
as the number of words in the negation scope to 
the right (or left) of the negation signal and the 
average scope length. In comparison, the clini-
cal reports subcorpus consists of clinical radi-
ology reports with short sentences. For detailed 
statistics about the three subcorpora, please see 
Morante and Daelemans (2009). 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
For preprocessing, all the sentences in the 
Bioscope corpus are tokenized and then parsed 
using the Berkeley parser2 (Petrov and Klein, 
2007) trained on the GENIA TreeBank (GTB) 
1.0 (Tateisi et al, 2005)3, which is a bracketed 
corpus in (almost) PTB style. 10-fold 
cross-validation on GTB1.0 shows that the 
parser achieves the performance of 86.57 in 
F1-measure. It is worth noting that the GTB1.0 
corpus includes all the sentences in the ab-
stracts subcorpus of the Bioscope corpus. 
4 Negation Scope Finding via Shallow 
Semantic Parsing 
In this section, we first formulate the negation 
scope finding task as a shallow semantic pars-
ing problem. Then, we deal with it using a sim-
plified shallow semantic parsing framework.  
4.1 Formulating Negation Scope Finding  
as a Shallow Semantic Parsing Prob-
lem 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the 
constituents in the sentence into their corre-
sponding semantic arguments (roles) of the 
predicate. As far as negation scope finding 
considered, the negation signal can be regarded 
as the predicate4, while the scope of the nega-
tion signal can be mapped into several con-
stituents which are negated and thus can be 
regarded as the arguments of the negation sig-
nal. In particular, given a negation signal and 
its negation scope which covers wordm, ?, 
wordn, we adopt the following two heuristic 
rules to map the negation scope of the negation 
signal into several constituents which can be 
deemed as its arguments in the given parse tree. 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus. 
1) The negation signal itself and all of its an-
cestral constituents are non-arguments. 
2) If constituent X is an argument of the given 
negation signal, then X should be the high-
est constituent dominated by the scope of 
wordm, ?, wordn. That is to say, X?s parent 
constituent must cross-bracket or include 
the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA  
4 If a negation signal consists of multiply words 
(e.g., rather than), the last word (e.g., than) is cho-
sen to represent the negation signal. 
673
 Figure 2: An illustration of a negation signal and its arguments in a parse tree. 
These findings 
indicates 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities 
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
predicate
arguments
 
The first rule ensures that no argument cov-
ers the negation signal while the second rule 
ensures no overlap between any two arguments. 
For example, in the sentence ?These findings 
indicate that corticosteroid resistance can not 
be explained by abnormalities?, the negation 
signal ?can not? has the negation scope ?corti-
costeroid resistance can not be explained by 
abnormalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation signal 
?can not? while its arguments include three 
constituents {NP4,5, MD6,6, and VP8,11}. It is 
worth noting that according to the above rules, 
negation scope finding via shallow semantic 
parsing, i.e. determining the arguments of a 
given negation signal, is robust to some varia-
tions in parse trees. This is also empirically 
justified by our later experiments. For example, 
if the VP6,11 in Figure 2 is incorrectly expanded 
by the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, 
the negation scope of the negation signal ?can 
not? can still be correctly detected as long as 
{NP4,5, MD6,6, VB8,8, and VP9,11} are predicted 
as the arguments of the negation signal ?can 
not?. 
Compared with common shallow semantic 
parsing which needs to assign an argument 
with a semantic label, negation scope finding 
does not involve semantic label classification 
and thus could be divided into three consequent 
phases: argument pruning, argument identifica-
tion and post-processing. 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the nega-
tion signal-scope structures in negation scope 
finding can be also classified into several cer-
tain types and argument pruning can be done 
by employing several heuristic rules to filter 
out constituents, which are most likely 
non-arguments of a negation signal. Similar to 
the heuristic algorithm as proposed in Xue and 
Palmer (2004) for argument pruning in com-
mon shallow semantic parsing, the argument 
pruning algorithm adopted here starts from 
designating the negation signal as the current 
node and collects its siblings. It then iteratively 
moves one level up to the parent of the current 
node and collects its siblings. The algorithm 
ends when it reaches the root of the parse tree. 
To sum up, except the negation signal and its 
ancestral constituents, any constituent in the 
parse tree whose parent covers the given nega-
tion signal will be collected as argument can-
didates. Taking the negation signal node 
?RB7,7? in Figure 2 as an example, constituents 
{MD6,6, VP8,11, NP4,5, IN3,3, VBP2,2, and NP0,1} 
are collected as its argument candidates conse-
quently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine 
the argument candidates as either valid argu-
ments or non-arguments. Similar to argument 
674
identification in common shallow semantic 
parsing, the structured syntactic information 
plays a critical role in negation scope finding.  
Basic Features 
Table 1 lists the basic features for argument 
identification. These features are also widely 
used in common shallow semantic parsing for 
both verbal and nominal predicates (Xue, 2008; 
Li et al, 2009). 
Feature Remarks 
b1 Negation: the stem of the negation signal, 
e.g., not, rather_than. (can_not) 
b2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
b3 Path: the syntactic path from the argument 
candidate to the negation signal. 
(NP<S>VP>RB) 
b4 Position: the positional relationship of the
argument candidate with the negation sig-
nal. ?left? or ?right?. (left) 
Table 1: Basic features and their instantiations for 
argument identification in negation scope finding, 
with NP4,5 as the focus constituent (i.e., the argu-
ment candidate) and ?can not? as the given negation 
signal, regarding Figure 2. 
Additional Features 
To capture more useful information in the ne-
gation signal-scope structures, we also explore 
various kinds of additional features. Table 2 
shows the features in better capturing the de-
tails regarding the argument candidate and the 
negation signal. In particular, we categorize the 
additional features into three groups according 
to their relationship with the argument candi-
date (AC, in short) and the given negation sig-
nal (NS, in short). 
Some features proposed above may not be 
effective in argument identification. Therefore, 
we adopt the greedy feature selection algorithm 
as described in Jiang and Ng (2006) to pick up 
positive features incrementally according to 
their contributions on the development data. 
The algorithm repeatedly selects one feature 
each time which contributes most, and stops 
when adding any of the remaining features fails 
to improve the performance. As far as the ne-
gation scope finding task concerned, the whole 
feature selection process could be done by first 
running the selection algorithm with the basic 
features (b1-b4) and then incrementally picking 
up effective features from (ac1-ac6, AC1-AC2, 
ns1-ns4, NS1-NS2, nsac1-nsac2, and NSAC1 
-NSAC7). 
Feature Remarks 
argument candidate (AC) related 
ac1 the headword (ac1H) and its POS (ac1P). 
(resistance, NN) 
ac2 the left word (ac2W) and its POS (ac2P). 
(that, IN) 
ac3 the right word (ac3W) and its POS (ac3P). 
(can, MD) 
ac4 the phrase type of its left sibling (ac4L) 
and its right sibling (ac4R). (NULL, VP) 
ac5 the phrase type of its parent node. (S) 
ac6 the subcategory. (S:NP+VP) 
combined features (AC1-AC2) 
b2&fc1H, b2&fc1P 
negation signal (NS) related 
ns1 its POS. (RB) 
ns2 its left word (ns2L) and right word (ns2R). 
(can, be) 
ns3 the subcategory. (VP:MD+RB+VP) 
ns4 the phrase type of its parent node. (VP) 
combined features (NS1-NS2) 
b1&ns2L, b1&ns2R 
NS-AC-related 
nsac1 the compressed path of b3: compressing 
sequences of identical labels into one.  
(NP<S>VP>RB) 
nsac2 whether AC and NS are adjacent in posi-
tion. ?yes? or ?no?. (no) 
combined features (NSAC1-NSAC7) 
b1&b2, b1&b3, b1&nsac1, b3&NS1, b3&NS2, 
b4&NS1, b4&NS2 
Table 2: Additional features and their instantiations 
for argument identification in negation scope find-
ing, with NP4,5 as the focus constituent (i.e., the 
argument candidate) and ?can not? as the given 
negation signal, regarding Figure 2. 
4.4 Post-Processing 
Although a negation signal in the BioScope 
corpus always has only one continuous block 
as its negation scope (including the negation 
signal itself), the negation scope finder may 
result in discontinuous negation scope due to 
independent prediction in the argument identi-
fication phase. Given the golden negation sig-
nals, we observed that 6.2% of the negation 
scopes predicted by our negation scope finder 
are discontinuous.  
Figure 3 demonstrates the projection of all 
the argument candidates into the word level. 
According to our argument pruning algorithm 
in Section 4.2, except the words presented by 
675
the negation signal, the projection covers the 
whole sentence and each constituent (LACi or 
RACj in Figure 3) receives a probability distri-
bution of being an argument of the given nega-
tion signal in the argument identification phase. 
 Since a negation signal is deemed inside of its 
negation scope in the BioScope corpus, our 
post-processing algorithm first includes the 
negation signal in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the negation signal 
itself, the leftmost word of constituent LACi 
(1<=i<=m). Supposing LACi receives prob-
ability of Pi being an argument, we use the fol-
lowing formula to determine LACk* whose 
leftmost word represents the boundary of the 
left scope. If k*=0, then the negation signal 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ? P?
                                                          
 
Similarly, the right boundary of the given 
negation signal can be decided. 
5 Experimentation 
We have evaluated our shallow semantic pars-
ing approach to negation scope finding on the 
BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante 
and Daelemans (2009), the abstracts subcorpus 
is randomly divided into 10 folds so as to per-
form 10-fold cross validation, while the per-
formance on both the papers and clinical re-
ports subcorpora is evaluated using the system 
trained on the whole abstracts subcorpus. In 
addition, SVMLight5 is selected as our classi-
fier. In particular, we adopt the linear kernel 
and the training parameter C is fine-tuned to 
0.2. 
1
5 http://svmlight.joachims.org/ 
The evaluation is made using the accuracy. 
We report the accuracy using three measures: 
PCLB and PCRB, which indicate the percent-
ages of correct left boundary and right bound-
ary respectively, PCS, which indicates the per-
centage of correct scope as a whole.  
LACm   ?.   LAC1 RAC1   ?.   RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
5.2 Experimental Results on Golden Parse 
Trees 
In order to select beneficial features from the 
additional features proposed in Section 4.3, we 
randomly split the abstracts subcorpus into 
training and development datasets with propor-
tion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 
features {NSAC5, ns2R, NS1, ac1P, ns3, 
NSAC7, ac4R} are selected consecutively for 
argument identification. Table 3 presents the 
effect of selected features in an incremental 
way on the development data. It shows that the 
additional features significantly improve the 
performance by 11.66% in PCS measure from 
74.93% to 86.59% ( ). 2; 0.0p? <
 
Feature PCLB PCRB PCS 
Baseline 84.26 88.92 74.93 
+NSAC5 90.96 88.92 81.34 
+ns2R 91.55 88.92 81.92 
+NS1 92.42 89.50 83.09 
+ac1P 93.59 89.50 84.26 
+ns3 93.88 90.09 84.84 
+NSAC7 94.75 89.80 85.42 
+ac4R 95.04 90.67 86.59 
Table 3: Performance improvement (%) of includ-
ing the additional features in an incremental way on 
the development data (of the abstracts subcorpus). 
However, Table 3 shows that the additional 
features behave quite differently in terms of 
PCLB and PCRB measures. For example, 
PCLB measure benefits more from features 
NSAC5, ns2R, NS1, ac1P, and NSAC7 while 
PCRB measure benefits more from features 
NS1 and ac4R. It also shows that the features 
(e.g., NSAC5, ns2R, NS1, NSAC7) related to 
neighboring words of the negation signal play a 
critical role in recognizing both left and right 
boundaries. This may be due to the fact that 
neighboring words usually imply sentential 
information. For example, ?can not be? indi-
cates a passive clause while ?did not? indicates 
an active clause. Table 3 also shows that the 
recognition of left boundaries is much easier 
than that of right boundaries. This may be due 
676
to the fact that 83.6% of negation signals have 
themselves as the left boundaries in the ab-
stracts subcorpus.  
gument candidate is outside or cross-brackets 
with the golden negation scope, then it is a 
non-argument. The oracle performance is pre-
sented in the rows of oracle in Table 5 and Ta-
ble 6. 
Table 4 presents the performance on the ab-
stracts subcorpus by performing 10-fold 
cross-validation. It shows that the additional 
features significantly improve the performance 
over the three measures ( ). 2; 0.0p? <
Table 5 and Table 6 show that: 
1) Automatic syntactic parsing lowers the per-
formance of negation scope finding on the 
abstracts subcorpus in all three measures (e.g. 
from 83.10 to 81.84 in PCS). As expected, 
the parser trained on the whole GTB1.0 
corpus works better than that trained on 
6,691 sentences (e.g. 64.02 Vs. 62.70, and 
89.79 Vs. 85.21 in PCS measure on the full 
papers and the clinical reports subcorpora, 
respectively). However, the performance de-
crease shows that negation scope finding is 
not as sensitive to automatic syntactic pars-
ing as common shallow semantic parsing, 
whose performance might decrease by about 
~10 in F1-measure (Toutanova et al, 2005). 
This indicates that negation scope finding 
via shallow semantic parsing is robust to 
some variations in the parse trees. 
1
Feature PCLB PCRB PCS 
Baseline 84.29 87.82 74.05 
+selected features 93.06 88.96 83.10 
Table 4: Performance (%) of negation scope finding 
on the abstracts subcorpus using 10-fold 
cross-validation.  
5.3 Experimental Results on Automatic 
Parse Trees 
The GTB1.0 corpus contains 18,541 sentences 
in which 11,850 of them (63.91%) overlap with 
the sentences in the abstracts subcorpus6. In 
order to get automatic parse trees for the sen-
tences in the abstracts subcorpus, we train the 
Berkeley parser with the remaining 6,691 sen-
tences in GTB1.0. The Berkeley parser trained 
on 6,691 sentences achieves the performance of 
85.22 in F1-measure on the other sentences in 
GTB1.0. For both the full papers and clinical 
reports subcorpora, we get their automatic 
parse trees by using two Berkeley parsers: one 
trained on 6,691 sentences in GBT1.0, and the 
other trained on all the sentences in GTB1.0.  
2) autoparse(test) consistently outperforms 
autoparse(t&t) on both the abstracts and the 
full papers subcorpora. However, it is sur-
prising to find that autoparse(t&t) achieves 
better performance on the clinical reports 
subcorpus than autoparse(test). This may be 
due to the special characteristics of the 
clinical reports subcorpus, which mainly 
consists of much shorter sentences with 6.6 
words per sentence on average, and better 
adaptation of the argument identification 
classifier to the variations in the automatic 
parse trees. 
To test the performance on automatic parse 
trees, we employ two different configurations. 
First, we train the argument identification clas-
sifier on the abstracts subcorpus using auto-
matic parse trees produced by Berkeley parser 
trained on 6,691 sentences. The experimental 
results are presented in the rows of auto-
parse(t&t) in Table 5 and Table 6. Then, we 
train the argument identification classifier on 
the abstracts subcorpus using golden parse 
trees. The experimental results are presented in 
the rows of autoparse(test) in Table 5 and Ta-
ble 6.  
3) The performance on all three subcorpora 
indicates that the recognition of right 
boundary is much harder than that of left 
boundary. This may be due to the longer 
right boundary on an average. Our statistics 
shows that the average left/right boundaries 
are 1.1/6.9, 0.1/3.7, and 1.2/6.5 words on the 
abstracts, the full papers and the clinical re-
ports subcorpora, respectively. 
We also report an oracle performance to ex-
plore the best possible performance of our sys-
tem by assuming that our negation scope finder 
can always correctly determine whether a can-
didate is an argument or not. That is, if an ar-
4) The oracle performance is less sensitive to 
automatic syntactic parsing. In addition, 
given the performance gap between the per-
formance of our negation scope finder and 
the oracle performance, there is still much 
room for further performance improvement. 
                                                          
6 There are a few cases where two sentences in the 
abstracts subcorpus map into one sentence in GTB. 
677
 Abstracts Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 91.97 87.82 80.88 85.45 67.20 59.26 97.48 88.30 85.89
autoparse(test) 92.71 88.33 81.84 87.57 68.78 62.70 97.48 87.73 85.21
oracle 99.72 94.59 94.37 98.94 84.13 83.33 99.89 98.39 98.39
Table 5: Performance (%) of negation scope finding on the three subcorpora by using automatic parser trained 
with 6,691 sentences in GTB1.0.  
 Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 85.98 67.99 60.32 97.48 92.66 90.48 
autoparse(test) 87.83 70.11 64.02 97.36 92.20 89.79 
oracle 98.94 83.86 83.07 99.77 97.94 97.82 
Table 6: Performance (%) of negation scope finding on the two subcorpora by using automatic parser trained 
with all the sentences in GTB1.0.  
 
Method Abstracts Papers Clinical 
M et al (2008) 57.33 n/a n/a 
M & D (2009) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Our final system 81.84 64.02 89.79 
Table 7: Performance comparison over the PCS 
measure (%) of our system with other 
state-of-the-art ones.  
Table 7 compares our performance in PCS 
measure with related work. It shows that even 
our baseline system with four basic features as 
presented in Table 1 performs better than 
Morante et al (2008) and Morante and Daele-
mans(2009). This indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syn-
tactic information on negation scope finding. It 
also shows that our final system significantly 
outperforms the state-of-the-art ones using a 
chunking approach, especially on the abstracts 
and full papers subcorpora. However, the im-
provement on the clinical reports subcorpus is 
less apparent, partly due to the fact that the 
sentences in this subcorpus are much simpler 
(with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Following are two typical sen-
tences from the clinical reports subcorpus, 
where the negation scope covers the whole sen-
tence (except the period punctuation). Such 
sentences account for 57% of negation sen-
tences in the clinical reports subcorpus. 
 
6 Conclusion 
In this paper we have presented a simplified 
shallow semantic parsing approach to negation 
scope finding by formulating it as a shallow 
semantic parsing problem, which has been ex-
tensively studied in the past few years. In par-
ticular, we regard the negation signal as the 
predicate while mapping the negation scope 
into several constituents which are deemed as 
arguments of the negation signal. Evaluation on 
the Bioscope corpus shows the appropriateness 
of our shallow semantic parsing approach and 
that structured syntactic information plays a 
critical role in capturing the domination rela-
tionship between a negation signal and its ne-
gation scope. It also shows that our parsing 
approach much outperforms the state-of-the-art 
chunking ones. To our best knowledge, this is 
the first research on exploring negation scope 
finding via shallow semantic parsing. 
Future research will focus on joint learning 
of negation signal and its negation scope find-
ings. Although Morante and Daelemans (2009) 
reported the performance of 95.8%-98.7% on 
negation signal finding, it lowers the perform-
ance of negation scope finding by about 
7.29%-16.52% in PCS measure.  
Acknowledgments 
This research was supported by Projects 
60683150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 20093201110006 under the Specialized 
Research Fund for the Doctoral Program of 
Higher Education of China. 
(1) No evidence of focal pneumonia . 
 
(2) No findings to account for symptoms . 
678
References 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 
2001. A Simple Algorithm for Identifying Ne-
gated Findings and Diseases in Discharge Sum-
maries. Journal of Biomedical Informatics, 34: 
301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et 
al. 1999. The GENIA project: corpus-based 
knowledge acquisition and information extrac-
tion from genome research papers. In Proceed-
ings of EACL 1999.  
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Parsing for Predicate Argument Rec-
ognition. In Proceedings of ACL 2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. 
Learning to Detect Negation with ?Not? in Medi-
cal Texts. In Proceedings of SIGIR 2003. 
Yang Huang and Henry Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection 
in Clinical Radiology Reports. Journal of the 
American Medical Informatics Association, 14(3): 
304-311. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic Role Labeling of NomBank: A Maximum En-
tropy Approach. In Proceedings of EMNLP 
2006. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information 
and Automatic Predicate Recognition. In Pro-
ceedings of EMNLP 2009. 
Roser Morante, Anthony Liekens, and Walter 
Daelemans. 2008. Learning the Scope of Nega-
tion in Biomedical Texts. In Proceedings of 
EMNLP 2008. 
Roser Morante and Walter Daelemans. 2009. A 
Metalearning Approach to Processing the Scope 
of Negation. In Proceedings of CoNLL 2009. 
Arzucan ?zg?r; Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific 
Text. In Proceedings of EMNLP 2009. 
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of 
NAACL 2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 
2005. The Necessity of Syntactic Parsing for 
Semantic Role Labeling. In Proceedings of IJCAI 
2005. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
annotation for negation, uncertainty and their 
scope in biomedical texts. In Proceedings of 
BioNLP 2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP 2005, 
Companion volume. 
Kristina Toutanova, Aria Haghighi, and Christopher 
D. Manning. 2005. Joint Learning Improves Se-
mantic Role Labeling. In Proceedings of ACL 
2005. 
Peter D. Turney. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to Unsu-
pervised Classification of Reviews. In Proceed-
ings of ACL 2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Pro-
ceedings of EMNLP 2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
 
 
679
Coling 2010: Poster Volume, pages 757?765,
Beijing, August 2010
Dependency-Driven Feature-based Learning for Extracting 
Protein-Protein Interactions from Biomedical Text 
Bing Liu   Longhua Qian   Hongling Wang   Guodong Zhou 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University 
Email: liubingnlp@gmail.com 
{qianlonghua,redleaf,gdzhou}@suda.edu.cn
                                                          
 Corresponding author 
Abstract
Recent kernel-based PPI extraction 
systems achieve promising perform-
ance because of their capability to 
capture structural syntactic informa-
tion, but at the expense of computa-
tional complexity. This paper incorpo-
rates dependency information as well 
as other lexical and syntactic knowl-
edge in a feature-based framework. 
Our motivation is that, considering the 
large amount of biomedical literature 
being archived daily, feature-based 
methods with comparable performance 
are more suitable for practical applica-
tions. Additionally, we explore the 
difference of lexical characteristics be-
tween biomedical and newswire do-
mains. Experimental evaluation on the 
AIMed corpus shows that our system 
achieves comparable performance of 
54.7 in F1-Score with other 
state-of-the-art PPI extraction systems, 
yet the best performance among all the 
feature-based ones.  
1 Introduction 
In recent years, automatically extracting 
biomedical information has been the subject of 
significant research efforts due to the rapid 
growth in biomedical development and 
discovery. A wide concern is how to 
characterize protein interaction partners since 
it is crucial to understand not only the 
functional role of individual proteins but also 
the organization of the entire biological 
process. However, manual collection of 
relevant Protein-Protein Interaction (PPI) 
information from thousands of research papers 
published every day is so time-consuming that 
automatic extraction approaches with the help 
of Natural Language Processing (NLP) 
techniques become necessary.  
Various machine learning approaches for 
relation extraction have been applied to the 
biomedical domain, which can be classified 
into two categories: feature-based methods 
(Mitsumori et al, 2006; Giuliano et al, 2006; 
S?tre et al, 2007) and kernel-based methods 
(Bunescu et al, 2005; Erkan et al, 2007; 
Airola et al, 2008; Kim et al, 2010). 
Provided a large-scale manually annotated 
corpus, the task of PPI extraction can be 
formulated as a classification problem. 
Typically, for featured-based learning each 
protein pair is represented as a vector whose 
features are extracted from the sentence 
involving two protein names. Early studies 
identify the existence of protein interactions 
by using ?bag-of-words? features (usually 
uni-gram or bi-gram) around the protein 
names as well as various kinds of shallow 
linguistic information, such as POS tag, 
lemma and orthographical features. However, 
these systems do not achieve promising results 
since they disregard any syntactic or semantic 
information altogether, which are very useful 
for the task of relation extraction in the 
newswire domain (Zhao and Grishman, 2005; 
Zhou et al, 2005). Furthermore, feature-based 
methods fail to effectively capture the 
structural information, which is essential to 
757
identify the relationship between two proteins 
in a syntactic representation. 
With the wide application of kernel-based 
methods to many NLP tasks, various kernels 
such as subsequence kernels (Bunescu and 
Mooney, 2005) and tree kernels (Li et al, 
2008), are also applied to PPI detection.. 
Particularly, dependency-based kernels such 
as edit distance kernels (Erkan et al, 2007) 
and graph kernels (Airola et al, 2008; Kim et 
al., 2010) show some promising results for PPI 
extraction. This suggests that dependency 
information play a critical role in PPI 
extraction as well as in relation extraction 
from newswire stories (Culotta and Sorensen, 
2004). In order to appreciate the advantages of 
both feature-based methods and kernel-based 
methods, composite kernels (Miyao et al, 
2008; Miwa et al, 2009a; Miwa et al, 2009b) 
are further employed to combine structural 
syntactic information with flat word features 
and significantly improve the performance of 
PPI extraction. However, one critical 
challenge for kernel-based methods is their 
computation complexity, which prevents them 
from being widely deployed in real-world 
applications regarding the large amount of 
biomedical literature being archived everyday.  
Considering the potential of dependency in-
formation for PPI extraction and the challenge 
of computation complexity of kernel-based 
methods, one may naturally ask the question: 
?Can the essential dependency information be 
maximally exploited in featured-based PPI 
extraction so as to enhance the performance 
without loss of efficiency?? ?If the answer is 
Yes, then How?? 
This paper addresses these problems, focus-
ing on the application of dependency informa-
tion to feature-based PPI extraction. Starting 
from a baseline system in which common 
lexical and syntactic features are incorporated 
using Support Vector Machines (SVM), we 
further augment the baseline with various fea-
tures related to dependency information, 
including predicates in the dependency tree. 
Moreover, in order to reveal the linguistic 
difference between distinct domains we also 
compare the effects of various features on PPI 
extraction from biomedical texts with those on 
relation extraction from newswire narratives. 
Evaluation on the AIMed and other PPI cor-
pora shows that our method achieves the best 
performance among all feature-based systems. 
The rest of the paper is organized as follows. 
A feature-based PPI extraction baseline system 
is given in Section 2 while Section 3 describes 
our dependency-driven method. We report our 
experiments in Section 4, and compare our 
work with the related ones in Section 5.  
Section 6 concludes this paper and gives some 
future directions. 
2 Feature-based PPI extraction: 
Baseline
For feature-based methods, PPI extraction task 
is re-cast as a classification problem by first 
transforming PPI instances into 
multi-dimensional vectors with various fea-
tures, and then applying machine learning ap-
proaches to detect whether the potential 
relationship exists for a particular protein pair. 
In training, a feature-based classifier learning 
algorithm, such as SVM or MaxEnt, uses the 
annotated PPI instances to learn a classifier 
while, in testing, the learnt classifier is in turn 
applied to new instances to determine their PPI 
binary classes and thus candidate PPI instances 
are extracted. 
As a baseline, various linguistic features, 
such as words, overlap, chunks, parse tree fea-
tures as well as their combined ones are ex-
tracted from a sentence and formed as a vector 
into the feature-based learner. 
1) Words 
Four sets of word features are used in our sys-
tem: 1) the words of both the proteins; 2) the 
words between the two proteins; 3) the words 
before M1 (the 1st protein); and 4) the words 
after M2 (the 2nd protein). Both the words be-
fore M1 and after M2 are classified into two 
bins: the first word next to the proteins and the 
second word next to the proteins. This means 
that we only consider the two words before M1 
and after M2. Words features include: 
x MW1: bag-of-words in M1 
x MW2: bag-of-words in M2 
x BWNULL: when no word in between 
x BWO: other words in between except 
first and last words when at least three 
words in between 
x BWM1FL: the only word before M1 
758
x BWM1F: first word before M1 
x BWM1L: second word before M1 
x BWM1: first and second word before 
M1
x BWM2FL: the only word after M2 
x BWM2F: first word after M2 
x BWM2L: second word after M2 
x BWM2: first and second word after M2 
2) Overlap 
The numbers of other protein names as well as 
the words that appear between two protein 
names are included in the overlap features. 
This category of features includes: 
x #MB: number of other proteins in be-
tween
x #WB: number of words in between 
x E-Flag: flag indicating whether the two 
proteins are embedded or not 
3) Chunks
It is well known that chunking plays an 
important role in the task of relation extraction 
in the ACE program (Zhou et al, 2005). How-
ever, its significance in PPI extraction has not 
fully investigated. Here, the Stanford Parser1
is first employed for full parsing, and then 
base phrase chunks are derived from full parse 
trees using the Perl script2. The chunking fea-
tures usually concern about the head words of 
the phrases between the two proteins, which 
are further classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between. In 
addition, the path of phrasal labels connecting 
two proteins is also a common syntactic 
indicator of the polarity of the PPI instance, 
just as the path NP_VP_PP_NP in the sen-
tence ?The ability of PROT1 to interact with 
the PROT2 was investigated.? is likely to sug-
gest the positive interaction between two pro-
teins. These base phrase chunking features 
contain:
x CPHBNULL: when no phrase in be-
tween.
x CPHBFL: the only phrase head when 
only one phrase in between 
x CPHBF: the first phrase head in between 
when at least two phrases in between. 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
2 http://ilk.kub.nl/~sabine/chunklink/ 
x CPHBL: the last phrase head in between 
when at least two phrase heads in be-
tween.
x CPHBO: other phrase heads in between 
except first and last phrase heads when 
at least three phrases in between. 
x CPP: path of phrase labels connecting 
the two entities in the chunking 
Furthermore, we also generate a set of 
bi-gram features which combine the above 
chunk features except CPP with their corre-
sponding chunk types.  
4) Parse Tree 
It is obvious that full pares trees encompass 
rich structural information of a sentence. 
Nevertheless, it is much harder to explore 
such information in featured-based methods 
than in kernel-based ones. Thus so far only 
the path connecting two protein names in the 
full-parse tree is considered as a parse tree 
feature.
x PTP: the path connecting two protein 
names in the full-parse tree. 
 Again, take the sentence ?The ability of 
PROT1 to interact with the PROT2 was 
investigated.? as an example, the parse path 
between PROT1 and PROT2 is 
NP_S_VP_PP_NP, which is slightly different 
from the CPP feature in the chunking feature 
set.
3 Dependency-Driven PPI Extraction 
The potential of dependency information for 
PPI extraction lies in the fact that the depend-
ency tree may well reveal non-local or 
long-range dependencies between the words 
within a sentence. In order to capture the 
necessary information inherent in the 
depedency tree for identifying their 
relationship, various kernels, such as edit 
distance kernel based on dependency path 
(Erkan et al, 2007), all-dependency-path 
graph kernel (Airola et al, 2008), and 
walk-weighted subsequence kernels (Kim et 
al., 2010) as well as other composite kernels 
(Miyao et al, 2008; Miwa et al, 2009a; Miwa 
et al, 2009b), have been proposed to address 
this problem. It?s true that these methods 
achieve encouraging results, neverthless, they 
suffer from prohibitive computation burden. 
759
Thus, our solution is to fold the structural 
dependency information back into flat 
features in a feature-based framework so as to 
speed up the learning process while retaining 
comparable performance. This is what we 
refer to as dependency-driven PPI extraction. 
 First, we construct dependency trees from 
grammatical relations generated by the Stan-
ford Parser. Every grammatical relation has the 
form of dependent-type (word1, word2),
Where word1 is the head word, word2 is de-
pendent on word1, and dependent-type denotes 
the pre-defined type of dependency. Then, 
from these grammatical relations the following 
features called DependenecySet1 are taken 
into consideration as illustrated in Figure 1: 
x DP1TR: a list of words connecting 
PROT1 and the dependency tree root. 
x DP2TR: a list of words connecting 
PROT2 and the dependency tree root. 
x DP12DT: a list of dependency types 
connecting the two proteins in the 
dependency tree. 
x DP12: a list of dependent words com-
bined with their dependency types con-
necting the two proteins in the depend-
ency tree. 
x DP12S: the tuple of every word com-
bined with its dependent type in DP12. 
x DPFLAG: a boolean value indicating 
whether the two proteins are directly 
dependent on each other. 
The typed dependencies produced by the 
Stanford Parser for the sentence ?PROT1 
contains a sequence motif binds to PROT2.? 
are listed as follows: 
nsubj(contains-2,PROT1-1)
det(motif-5, a-3) 
nn(motif-5, sequence-4) 
nsubj(binds-6, motif-5) 
ccomp(contains-2, binds-6) 
prep_to(binds-6, PROT2-8) 
Each word in a dependency tuple is fol-
lowed by its index in the original sentence, 
ensuring accurate positioning of the head 
word and dependent word. Figure 1 shows the 
dependency tree we construct from the above 
grammatical relations.  
contains 
PROT1 
motif 
binds 
PROT2
a sequence 
nsubj ccomp 
prep_to
nsubj 
det nn 
Figure 1: Dependency tree for the sentence 
?PROT1 contains a sequence motif binds to 
PROT2.? 
Erkan et al (2007) extract the path 
information between PROT1 and PROT2 in 
the dependency tree for kernel-based PPI 
extraction and report promising results, 
neverthless, such path is so specific for 
feature-based methods that it may incure 
higher precision but lower recall. Thus we 
alleviate this problem by collapsing the feature 
into multiple ones with finer granularity, 
leading to the features such as DP12S. 
It is widely acknowledged that predicates 
play an important role in PPI extraction. For 
example, the change of a pivot predicate 
between two proteins may easily lead to the 
polarity reversal of a PPI instance. Therefore, 
we extract the predicates and their positions in 
the dependency tree as predicate features 
called DependencySet2:  
x FVW: the predicates in the DP12 feature 
occurring prior to the first protein. 
x LVW: the predicates in the DP12 feature 
occurring next to the second entity. 
x MVW: other predicates in the DP12 
features. 
x #FVW: the number of FVW 
x #LVW: the number of LVW 
x #MVW: the number of MVW 
4 Experimentation
This section systematically evaluates our fea-
ture-based method on the AIMed corpus as 
well as other commonly used corpus and re-
ports our experimental results. 
760
4.1 Data Sets 
We use five corpora3 with the AIMed corpus 
as the main experimental data, which contains 
177 Medline abstracts with interactions be-
tween two interactions, and 48 abstracts with-
out any PPI within single sentences. There are 
4,084 protein references and around 1,000 
annotated interactions in this data set.  
For corpus pre-procession, we first rename 
two proteins of a pair as PROT1 and PROT2 
respectively in order to blind the learner for 
fair comparison with other work.  Then, all 
the instances are generated from the sentences 
which contain at least two proteins,  that is, if 
a sentence contains n different proteins, there 
are n2 different pairs of proteins and these 
pairs are considered untyped and undirected. 
For the purpose of comparison with previous 
work, all the self-interactions (59 instances) 
are removed, while all the PPI instances with 
nested protein names are retained (154 in-
stances). Finally, 1002 positive instances and 
4794 negative instances are generated and 
their corresponding features are extracted.  
We select Support Vector Machines (SVM) 
as the classifier since SVM represents the 
state-of-the-art in the machine learning re-
search community. In particular, we use the 
binary-class SVMLigh 4 developed by 
Joachims (1998) since it satisfies our require-
ment of detecting potential PPI instances. 
Evaluation is done using 10-fold docu-
ment-level cross-validation. Particularly, we 
apply the extract same 10-fold split that was 
used by Bunescu et al (2005) and Giuliano et 
al. (2006). Furthermore, OAOD (One Answer 
per Occurrence in the Document) strategy is 
adopted, which means that the correct interac-
tion must be extracted for each occurrence. 
This guarantees the maximal use of the avail-
able data, and more important, allows fair 
comparison with earlier relevant work.  
The evaluation metrics are commonly used 
Precision (P), Recall (R) and harmonic 
F1-score (F1). As an alternative to F1-score, 
the AUC (area under the receiver operating 
characteristics curve) measure is proved to be 
invariant to the class distribution of the train-
ing dataset. Thus we also provide AUC scores 
                                                          
3 http://mars.cs.utu.fi/PPICorpora/GraphKernel.html 
4 http://svmlight.joachims.org/
for our system as Airola et al (2008) and 
Miwa et al (2009a). 
4.2 Results and Discussion 
Features P(%) R(%) F1 
Baseline features 
Words 59.4 40.6 47.6
+Overlap 60.4 39.9 47.4
+Chunk 59.2 44.5 50.6
+Parse 60.9 44.8 51.4
Dependency-driven features 
+Dependency Set1 62.9 48.0 53.9
+Dependency Set2 63.4 48.8 54.7
Table 1: Performance of PPI extraction with vari-
ous features in the AIMed corpus 
We present in Table 1 the performance of our 
system using document-wise evaluation 
strategies and 10-fold cross-validation with 
different features in the AIMed corpus, where 
the plus sign before a feature means it is 
incrementally added to the feature set. Table 1 
reports that our system achieves the best per-
formance of 63.4/48.8/54.7 in P/R/F scores. It 
also shows that: 
x Words features alone achieve a relatively 
low performance of 59.4/40.9/47.6 in 
P/R/F, particularly with fairly low recall 
score. This suggests the difficulty of PPI 
extraction and words features alone can?t 
effectively capture the nature of protein 
interactions.
x Overlap features slightly decrease the per-
formance. Statistics show that both the 
distributions of #MB and #WB between 
positives and negatives are so similar that 
they are by no means the discriminators for 
PPI extraction. Hence, we exclude the 
overlap features in the succeeding experi-
ments.
x Chunk features significantly improves the 
F-measure by 3 units largely due to the in-
crease of recall by 3.9%, though at the 
slight expense of precision. This suggests 
the effectiveness of shallow parsing infor-
mation in the form of headwords captured 
by chunking on PPI extraction.  
x The usefulness of the parse tree features is 
quite limited. It only improves the 
F-measure by 0.8 units. The main reason 
may be that these paths are usually long 
761
and specific, thus they suffer from the 
problem of data sparsity. Furthermore, 
some of the parse tree features are already 
involved in the chunk features.  
x The DependencySet1 features are very 
effective in that it can increase the preci-
sion and recall by 2.0 and 3.2 units 
respectively, leading to the increase of F1 
score by 2.5 units. This means that the de-
pendency-related features can effectively 
retrieve more PPI instances without intro-
ducing noise that will severely harm the 
precision. According to our statistics, there 
are over 60% sentences with more than 5 
words between their protein entities in the 
AIMed corpus. Therefore, dependency in-
formation exhibit great potential to PPI 
extraction since they can capture 
long-range dependencies within sentences. 
Take the aforementioned sentence 
?PROT1 contains a sequence motif binds 
to PROT2.? as an example, although the 
two proteins step over a relatively long 
distance, the dependency path between 
them is concise and accurate, reflecting the 
essence of the interaction. 
x The predicate features also contribute to 
the F1-score gain of 0.8 units. It is not 
surprising since some predicates, such as 
?interact?, ?activate? and ?inhibit? etc, are 
strongly suggestive of the interaction 
polarity between two proteins. 
We compare in Table 2 the performance of 
our system with other systems in the AIMed 
corpus using the same 10-fold cross validation 
strategy. These systems are grouped into three 
distinct classes: feature-based, kernel-based 
and composite kernels. Except for Airola et al 
(2008) Miwa et al (2009a) and Kim et al 
(2010), which adopt graph kernels, our system 
performs comparably with other systems. In 
particular, our dependency-driven system 
achieves the best F1-score of 54.7 among all 
feature-based systems. 
In order to measure the generalization abil-
ity of our dependency-driven PPI extraction 
system across different corpora, we further 
apply our method to other four publicly avail-
able PPI corpora: BioInfer, HPRD50, IEPA 
and LLL.  
Table 2: Comparison with other PPI extraction 
systems in the AIMed corpus 
The corresponding performance of 
F1-score and AUC metrics as well as their 
standard deviations is present in Table 3.  
Comparative available results from Airola et 
al. (2008) and Miwa et al (2009a) are also 
included in Table 3 for comparison. This table 
shows that our system performs almost 
consistently with the other two systems, that is, 
the LLL corpus gets the best performance yet 
with the greatest variation, while the AIMed 
corpus achieves the lowest performance with 
reasonable variation. 
It is well known that biomedical texts ex-
hibit distinct linguistic characteristics from 
newswire narratives, leading to dramatic per-
formance gap between PPI extraction and 
relation detection in the ACE corpora. How-
ever, no previous work has ever addressed this 
problem and empirically characterized this 
difference. In this paper, we devise a series of 
experiments over the ACE RDC corpora using 
our dependency-driven feature-based method 
as a touchstone task. In order to do that, a sub-
                                                          
5 Airola et al (2008) repeat the method published by 
Giuliano et al (2006) with a correctly preprocessed 
AIMed and reported an F1-score of 52.4%. 
6 The results from Table 1 (Miyao et al, 2009) with the 
most similar settings to ours (Stanford Parser with SD 
representation) are reported. 
Systems P(%) R(%) F1
Feature-based methods 
Our system 63.4 48.8 54.7
Giuliano et al, 20065 60.9 57.2 59.0
S?tre et al, 2007 64.3 44.1 52.0
Mitsumori et al, 2006 54.2 42.6 47.7
Yakushiji et al, 2005 33.7 33.1 33.4
Kernel-based methods 
Kim et al, 2010 61.4 53.3 56.7
Airola et al, 2008 52.9 61.8 56.4
Bunescu et al, 2006  65.0 46.4 54.2
Composite kernels 
Miwa et al, 2009a - - 62.0
Miyao et al, 20086 51.8 58.1 54.5
762
set of 5796 relation instances is randomly 
sampled from the ACE 2003 and 2004 cor-
pora respectively.  The same cross-validation 
and evaluation metrics are applied to these 
two sets as PPI extraction in the AIMed cor-
pus.
Our system Airola et al (2008) 7 Miwa et al (2009a) 
Corpus F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC
AIMed 54.7 4.5 82.4 3.5 56.4 5.0 84.8 2.3 60.8 6.6 86.8 3.3
BioInfer 59.8 3.5 80.9 3.3 61.3 5.3 81.9 6.5 68.1 3.2 85.9 4.4
HPRD50 64.9 13.4 79.8 8.5 63.4 11.4 79.7 6.3 70.9 10.3 82.2 6.3
IEPA 62.1 6.2 74.8 6.6 75.1 7.0 85.1 5.1 71.7 7.8 84.4 4.2
LLL 78.1 15.8 85.1 8.3 76.8 17.8 83.4 12.2 80.1 14.1 86.3 10.8
Table 3: Comparison of performance across the five PPI corpora 
AIMed ACE2003 ACE2004 
Features
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Words 59.4 40.6 47.6 66.5 51.6 57.9 68.1 59.6 63.4
+Overlap +1.0 -0.7 -0.2 +5.4 +1.8 +3.2 +4.6 +1.2 +2.7
+Chunk -1.7 +4.6 +3.2 +2.3 +5.1 +4.0 +1.5 +1.9 +1.7
+Parse +1.7 +0.3 +0.8 +0.3 +0.6 +0.5 +0.6 +0.4 +0.5
+Dependency Set1 +2.0 +3.2 +2.5 +0.8 +0.7 +0.7 +0.5 +0.9 +0.7
+Dependency Set2 +0.5 +0.8 +0.8 +0.3 +0.2 +0.3 +0.2 +0.4 +0.3
Table 4: Comparison of contributions of different features to relation detection across multiple domains 
Table 4 compares the performance of our 
method over different domains. The table re-
ports that the words features alone achieve the 
best F1-score of 63.4 in ACE2004 but the low-
est F1-score of 47.6 in AIMed. This suggests 
the wide difference of lexical distribution be-
tween these domains. We extract the words 
appearing before the 1st mention, between the 
two mentions and after the 2nd mention from 
the training sets of these corpora respectively, 
and summarize the statistics (the number of 
tokens, the number of occurrences) in Table 5, 
where the KL divergence between positives 
and negatives is summed over the distribution 
of the 500 most frequently occurring words. 
                                                          
7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa et 
al. (2009b) 
Table 5: Lexical statistics on three corpora 
The table shows that AIMed uses the most 
kinds of words and the most words around the 
two mentions than the other two. More impor-
tant, AIMed has the least distribution differ-
ence between the words appearing in positives 
and negatives, as indicated by its least KL 
divergence. Therefore, the lexical words in 
AIMed are less discriminative for relation 
detection than they do in the other two. This 
naturally explains the reason why the perform-
ance by words feature alone is 
AIMed<ACE2003<ACE2004. In addition, 
Table 4 also shows that: 
x The overlap features significantly improve 
the performance in ACE while slightly 
deteriorating that in AIMed. The reason is 
that, as indicated in Zhou et al (2005), most 
of the positive relation instances in ACE 
exist in local contexts, while the positive 
interactions in AIMed occur in relative 
long-range just as the negatives, therefore 
these features are not discriminative for 
AIMed.
Statistics AIMed ACE2003 ACE2004
# of tokens 2,340 2,064 2,099
# of occurrences 69,976 53,744 49,570
KL divergence  0.22 0.28 0.33 
x The chunk features consistently greatly 
boost the performance across multiple cor-
pora. This implies that the headwords in 
chunk phrases can well capture the partial 
nature of relation instances regardless of 
their genre. 
x It?s not surprising that the parse feature 
attain moderate performance gain in all do-
mains since these parse paths are usually 
763
long and specificity, leading to data 
sparseness problem. 
x It is interesting to note that the depend-
ency-related features exhibit more signifi-
cant improvement in AIMed than that in 
ACE. The reason may be that, these 
dependency features can effectively cap-
ture long-range relationships prevailing in 
AIMed, while in ACE a large number of 
local relationships dominate the corpora. 
5 Related Work 
Among feature-based methods, the PreBIND 
system (Donaldson et al, 2003) uses words and 
word bi-grams features to identify the existence 
of protein interactions in abstracts and such 
information is used to enhance manual expert 
reviewing for the BIND database. Mitsumori et 
al. (2006) use SVM to extract protein-protein 
interactions, where bag-of-words features, spe-
cifically the words around the protein names, 
are employed. Sugiyama et al (2003) extract 
various features from the sentences based on 
the verbs and nouns in the sentences such as the 
verbal forms, and the part-of-speech tags of the 
20 words surrounding the verb. In addition to 
word features, Giuliano et al (2006) extract 
shallow linguistic information such as POS tag, 
lemma, and orthographic features of tokens for 
PPI extraction. Unlike our dependency-driven 
method, these systems do not consider any 
syntactic information.  
For kernel-based methods, there are several 
systems which utilize dependency information. 
Erkan et al (2007) defines similarity functions 
based on cosine similarity and edit distance 
between dependency paths, and then incorpo-
rate them in SVM and KNN learning for PPI 
extraction. Airola et al (2008) introduce 
all-dependency-paths graph kernel to capture 
the complex dependency relationships between 
lexical words and attain significant perform-
ance boost at the expense of computational 
complexity. Kim et al (2010) adopt 
walk-weighted subsequence kernel based on 
dependency paths to explore various substruc-
tures such as e-walks, partial match, and 
non-contiguous paths. Essentially, their kernel 
is also a graph-based one. 
For composite kernel methods, S?tre et al 
(2007) combine a ?bag-of-words? kernel with 
dependency and PAS (Predicate Argument 
Structure) tree kernels to exploit both the words 
features and the structural syntactic information. 
Hereafter, Miyao et al (2008) investigate the 
contribution of various syntactic features using 
different representations from dependency 
parsing, phrase structure parsing and deep 
parsing by different parsers. Miwa et al 
(2009a) integrate ?bag-of-words? kernel, PAS 
tree kernel and all-dependency-paths graph 
kernel to achieve the higher performance. They 
(Miwa et al, 2009b) also use similar compos-
ite kernels for corpus weighting learning 
across multiple PPI corpora.  
6 Conclusion and Future Work 
In this paper, we have combined various lexical 
and syntactic features, particularly dependency 
information, into a feature-based PPI extraction 
system. We find that the dependency informa-
tion as well as the chunk features contributes 
most to the performance improvement.  The 
predicate features involved in the dependency 
tree can also moderately enhance the perform-
ance. Furthermore, comparative study between 
biomedical domain and the ACE newswire 
domain shows that these domains exhibit 
different lexical characteristics, rendering the 
task of PPI extraction much more difficult than 
that of relation detection from the ACE cor-
pora.
In future work, we will explore more syntac-
tic features such as PAS information for fea-
ture-based PPI extraction to further boost the 
performance. 
Acknowledgment 
This research is supported by Projects 
60873150 and 60970056 under the National 
Natural Science Foundation of China and Pro-
ject BK2008160 under the Natural Science 
Foundation of Jiangsu, China. We are also very 
grateful to Dr. Antti Airola from Truku 
University for providing partial experimental 
materials. 
References 
A. Airola, S. Pyysalo, J. Bj?rne, T. Pahikkala, F. 
Ginter, and T. Salakoski. 2008. All-paths graph 
kernel for protein-protein interaction extraction 
764
with evaluation of cross corpus learning. BMC
Bioinformatics.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney, 
A. Ramani, and Y. Wong. 2005. Comparative 
Experiments on learning information extractors 
for Proteins and their interactions. Journal of 
Artificial Intelligence In Medicine, 33(2).  
R. Bunescu and R. Mooney. 2005. Subsequence 
kernels for relation extraction. In Proceedings of  
NIPS?05, pages 171?178. 
A. Culotta and J. Sorensen. 2004.  Dependency 
Tree Kernels for Relation Extraction. In 
Proceedings of ACL?04.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting, V. 
Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. 
Bader, K. Michalockova, T. Pawson, and C. W. V. 
Hogue. 2003. Prebind and textomy - mining the 
biomedical literature for protein-protein 
interactions using a support vector machine. 
Journal of BMC Bioinformatics, 4(11). 
G. Erkan, A. ?zg?r, and D.R. Radev. 2007. 
Semi-Supervised Classification for Extracting 
Protein Interaction Sentences using Dependency 
Parsing, In Proceedings of EMNLP-CoNLL?07,
pages 228?237. 
C. Giuliano, A. Lavelli, and L. Romano. 2006. 
Exploiting Shallow Linguistic Information for 
Relation Extraction from Biomedical Literature. 
In Proceedings of EACL?06, pages 401?408. 
 S. Kim, J. Yoon, J. Yang, and S. Park. 2010. 
Walk-weighted subsequence kernels for 
protein-protein interaction extraction. Journal of 
BMC Bioinformatics, 11(107). 
J. Li, Z. Zhang, X. Li, and H. Chen. 2008. 
Kernel-Based Learning for Biomedical Relation 
extraction. Journal of the American Society for 
Information Science and Technology, 59(5). 
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. 
Doi. 2006. Extracting protein-protein interaction 
information from biomedical text with SVM. 
IEICE Transactions on Information and System, 
E89-D (8).
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009a. 
Protein-Protein Interaction Extraction by 
Leveraging Multiple Kernels and Parsers. 
Journal of Medical Informatics, 78(2009). 
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009b.  
A Rich Feature Vector for Protein-Protein 
Interaction Extraction from Multiple Corpora. In 
Proceedings of EMNLP?09, pages 121?130.  
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and 
J.Tsujii. 2008. Task-oriented evaluation of 
syntactic parsers and their representations. In 
Proceedings of ACL?08, pages 46?54. 
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi. 
2001. Automated extraction of information on 
protein-protein interactions from the biological 
literature. Journal of Bioinformatics, 17(2). 
 K. Sugiyama, K. Hatano, M. Yoshikawa, and S. 
Uemura. 2003. Extracting information on 
protein-protein interactions from biological 
literature based on machine learning approaches. 
Journal of Genome Informatics, (14): 699?700. 
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic 
features for protein-protein interaction extraction. 
In Proceedings of LBM?07, pages 6.1?6.14. 
A. Yakushiji, M. Yusuke, T. Ohta, Y. Tateishi, J. 
Tsujii. 2006. Automatic construction of 
predicate-argument structure patterns for 
biomedical information extraction. In 
Proceedings of EMNLP?06, pages 284?292. 
S.B. Zhao and R. Grishman. 2005. Extracting 
Relations with Integrated Information Using 
Kernel Methods. In Proceedings of ACL?05,
pages 419-426.  
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation 
extraction.  In Proceedings of ACL?05, pages 
427-434.  
765
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714?724,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Unified Framework for Scope Learning via Simplified Shallow Seman-
tic Parsing 
 
Qiaoming Zhu    Junhui Li    Hongling Wang    Guodong Zhou?  
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
{qmzhu, lijunhui, hlwang, gdzhou}@suda.edu.cn 
 
 
                                                          
? Corresponding author 
Abstract 
This paper approaches the scope learning 
problem via simplified shallow semantic pars-
ing. This is done by regarding the cue as the 
predicate and mapping its scope into several 
constituents as the arguments of the cue. 
Evaluation on the BioScope corpus shows that 
the structural information plays a critical role 
in capturing the relationship between a cue 
and its dominated arguments. It also shows 
that our parsing approach significantly outper-
forms the state-of-the-art chunking ones. Al-
though our parsing approach is only evaluated 
on negation and speculation scope learning 
here, it is portable to other kinds of scope 
learning.  
1 Introduction 
Recent years have witnessed an increasing interest 
in the analysis of linguistic scope in natural lan-
guage. The task of scope learning deals with the 
syntactic analysis of what part of a given sentence 
is under user?s special interest. For example, of 
negation assertion concerned, a negation cue (e.g., 
not, no) usually dominates a fragment of the given 
sentence, rather than the whole sentence, especially 
when the sentence is long. Generally, scope learn-
ing involves two subtasks: cue recognition and its 
scope identification. The former decides whether a 
word or phrase in a sentence is a cue of a special 
interest, where the semantic information of the 
word or phrase, rather than the syntactic informa-
tion, plays a critical role. The latter determines the 
sequences of words in the sentence which are 
dominated by the given cue.  
Recognizing the scope of a special interest (e.g., 
negative assertion and speculative assertion) is es-
sential in information extraction (IE), whose aim is 
to derive factual knowledge from free text. For 
example, Vincze et al (2008) pointed out that the 
extracted information within the scope of a nega-
tion or speculation cue should either be discarded 
or presented separately from factual information. 
This is especially important in the biomedical and 
scientific domains, where various linguistic forms 
are used extensively to express impressions, hy-
pothesized explanations of experimental results or 
negative findings. Besides, Vincez et al (2008) 
reported that 13.45% and 17.70% of the sentences 
in the abstracts subcorpus of the BioScope corpus 
contain negative and speculative assertions, respec-
tively, while 12.70% and 19.44% of the sentences 
in the full papers subcorpus contain negative and 
speculative assertions, respectively. In addition to 
the IE tasks in the biomedical domain, negation 
scope learning has attracted increasing attention in 
some natural language processing (NLP) tasks, 
such as sentiment classification (Turney, 2002). 
For example, in the sentence ?The chair is not 
comfortable but cheap?, although both the polari-
ties of the words ?comfortable? and ?cheap? are 
positive, the polarity of ?the chair? regarding the 
attribute ?cheap? keeps positive while the polarity 
of ?the chair? regarding the attribute ?comfortable? 
is reversed due to the negation cue ?not?. Similarly, 
seeing the increasing interest in speculation scope 
learning, the CoNLL?2010 shared task (Farkas et 
al., 2010) aims to detect uncertain information in 
resolving the scopes of speculation cues. 
Most of the initial research in this literature fo-
cused on either recognizing negated terms or iden-
tifying speculative sentences, using some heuristic 
714
rules (Chapman et al, 2001; Light et al, 2004), 
and machine learning methods (Goldin and Chap-
man, 2003; Medlock and Briscoe, 2007). However, 
scope learning has been largely ignored until the 
recent release of the BioScope corpus (Szarvas et 
al., 2008), where negation/speculation cues and 
their scopes are annotated explicitly. Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b) pioneered the research on scope learning 
by formulating it as a chunking problem, which 
classifies the words of a sentence as being inside or 
outside the scope of a cue. Alternatively, ?zg?r 
and Radev (2009) and ?vrelid et al (2010) defined 
heuristic rules for speculation scope learning from 
constituency and dependency parse tree perspec-
tives, respectively. 
Although the chunking approach has been 
evaluated on negation and speculation scope learn-
ing and can be easily ported to other scope learning 
tasks, it ignores syntactic information and suffers 
from low performance. Alternatively, even if the 
rule-based methods may be effective for a special 
scope learning task (e.g., speculation scope learn-
ing), it is not readily adoptable to other scope 
learning tasks (e.g., negation scope learning). In-
stead, this paper explores scope learning from 
parse tree perspective and formulates it as a simpli-
fied shallow semantic parsing problem, which has 
been extensively studied in the past few years 
(Carreras and M?rquez, 2005). In particular, the 
cue is recast as the predicate and the scope is recast 
as the arguments of the cue. The motivation behind 
is that the structured syntactic information plays a 
critical role in scope learning and should be paid 
much more attention, as indicated by previous 
studies in shallow semantic parsing (Gildea and 
Palmer, 2002; Punyakanok et al, 2005). Although 
our approach is evaluated only on negation and 
speculation scope learning here, it is portable to 
other kinds of scope learning. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 intro-
duces the Bioscope corpus on which our approach 
is evaluated. Section 4 describes our parsing ap-
proach by formulating scope learning as a simpli-
fied shallow semantic parsing problem. Section 5 
presents the experimental results. Finally, Section 
6 concludes the work. 
 
 
2 Related Work  
Most of the previous research on scope learning 
falls into negation scope learning and speculation 
scope learning.  
Negation Scope Learning 
Morante et al (2008) pioneered the research on 
negation scope learning, largely due to the avail-
ability of a large-scale annotated corpus, the Bio-
scope corpus. They approached negation cue 
recognition as a classification problem and formu-
lated negation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecutive-
ness of the negation scope. Morante and Daele-
mans (2009a) further improved the performance by 
combing several classifiers and achieved the accu-
racy of ~98% for negation cue recognition and the 
PCS (Percentage of Correct Scope) of ~74% for 
negation scope identification on the abstracts sub-
corpus. However, this chunking approach suffers 
from low performance, in particular on long sen-
tences. For example, given golden negation cues 
on the Bioscope corpus, Morante and Daelemans 
(2009a) only got the performance of 50.26% in 
PCS on the full papers subcorpus (22.8 words per 
sentence on average), compared to 87.27% in PCS 
on the clinical reports subcorpus (6.6 words per 
sentence on average). 
Speculation Scope Learning 
Similar to Morante and Daelemans (2009a), 
Morante and Daelemans (2009b) formulated 
speculation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the speculation scope, 
with proper post-processing to ensure consecutive-
ness of the speculation scope. They concluded that 
their method for negation scope identification is 
portable to speculation scope identification. How-
ever, of speculation scope identification concerned, 
it also suffers from low performance, with only 
60.59% in PCS for the clinical reports subcorpus 
of short sentences. 
Alternatively, ?zg?r and Radev (2009) em-
ployed some heuristic rules from constituency 
parse tree perspective on speculation scope identi-
fication. Given golden speculation cues, their rule-
based method achieves the accuracies of 79.89% 
715
and 61.13% on the abstracts and the full papers 
subcorpora, respectively. The more recent 
CoNLL?2010 shared task was dedicated to the de-
tection of speculation cues and their linguistic 
scope in natural language processing (Farkas et al, 
2010). As a representative, ?vrelid et al (2010) 
adopted some heuristic rules from dependency 
parse tree perspective to identify their speculation 
scopes. 
3 Cues and Scopes in the BioScope Cor-
pus 
This paper employs the BioScope corpus (Szarvas 
et al, 2008; Vincze et al, 2008) 1 , a freely 
downloadable resource from the biomedical do-
main, as the benchmark corpus. In this corpus, 
every sentence is annotated with negation cues and 
speculation cues (if it has), as well as their linguis-
tic scopes. Figure 1 shows a self-explainable ex-
ample. It is possible that a negation/speculation cue 
consists of multiple words, i.e., ?can not?/?indicate 
that? in Figure 1. 
 
The Bioscope corpus consists of three sub-
corpora: biological full papers from FlyBase and 
from BMC Bioinformatics, biological paper ab-
stracts from the GENIA corpus (Collier et al, 
1999), and clinical (radiology) reports. Among 
them, the full papers subcorpus and the abstracts 
subcorpus come from the same genre, and thus 
share some common characteristics in statistics, 
such as the number of words in the nega-
tion/speculation scope to the right (or left) of the 
negation/speculation cue and the average scope 
length. In comparison, the clinical reports subcor-
pus consists of clinical radiology reports with short 
sentences. For detailed statistics and annotation 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
guidelines about the three subcorpora, please see 
Morante and Daelemans (2009a & 2009b). 
For preprocessing, all the sentences in the Bio-
scope corpus are tokenized and then parsed using 
the Berkeley parser (Petrov and Klein, 2007) 2  
trained on the GENIA TreeBank (GTB) 1.0 
(Tateisi et al, 2005)3, which is a bracketed corpus 
in (almost) PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves the per-
formance of 86.57 in F1-measure. It is worth not-
ing that the GTB1.0 corpus includes all the 
sentences in the abstracts subcorpus of the Bio-
scope corpus. 
4 Scope Learning via Simplified Shallow 
Semantic Parsing 
In this section, we first formulate the scope learn-
ing task as a simplified shallow semantic parsing 
problem. Then, we deal with it using a simplified 
shallow semantic parsing framework. 
4.1 Formulating Scope Learning as a Simpli-
fied Shallow Semantic Parsing Problem 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the con-
stituents in the sentence into their corresponding 
semantic arguments (roles) of the predicate or not. 
As far as scope learning considered, the cue can be 
regarded as the predicate4, while its scope can be 
mapped into several constituents dominated by the 
cue and thus can be regarded as the arguments of 
the cue. In particular, given a cue and its scope 
which covers wordm, ?, wordn, we adopt the fol-
lowing two heuristic rules to map the scope of the 
cue into several constituents which can be deemed 
as its arguments in the given parse tree. 
1) The cue itself and all of its ancestral constituents 
are non-arguments. 
2) If constituent X is an argument of the given cue, 
then X should be the highest constituent domi-
nated by the scope of wordm, ?, wordn. That is 
to say, X?s parent constituent must cross-bracket 
or include the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 If a speculation cue consists of multiply words (e.g., whether 
or not), the first word (e.g., whether) is chosen to represent the 
speculation signal. However, the last word (e.g., not) is chosen 
to represent the negation cue if it consists of multiple words 
(e.g., can not). 
716
 Figure 2: Examples of a negation/speculation cue and its arguments in a parse tree 
These findings 
indicate 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
neg-predicate
neg-arguments
spec-predicate
spec-argument
 
The first rule ensures that no argument covers 
the cue while the second rule ensures no overlap 
between any two arguments. These two constraints 
between a cue and its arguments are consistent 
with shallow semantic parsing (Carreras and 
M?rquez, 2005). For example, in the sentence 
?These findings indicate that corticosteroid resis-
tance can not be explained by abnormalities?, the 
negation cue ?can not? has the negation scope 
?corticosteroid resistance can not be explained by 
abnormalities? while the speculation cue ?indicate 
that? has the speculation scope ?indicate that cor-
ticosteroid resistance can not be explained by ab-
normalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation cue ?can 
not? while its arguments include three constituents 
{NP4,5, MD6,6, and VP8,11}. Similarly, the node 
?VBP2,2? (i.e., indicate) represents the  speculation 
cue ?indicate that? while its arguments include one 
constituent SBAR3,11. It is worth noting that ac-
cording to the above rules, scope learning via shal-
low semantic parsing, i.e. determining the 
arguments of a given cue, is robust to some varia-
tions in the parse trees. This is also empirically 
justified by our later experiments. For example, if 
the VP6,11 in Figure 2 is incorrectly expanded by 
the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, the 
negation scope of the negation cue ?can not? can 
still be correctly detected as long as {NP4,5, MD6,6, 
VB8,8, and VP9,11} are predicated as the arguments 
of the negation cue ?can not?. 
Compared with common shallow semantic pars-
ing which needs to assign an argument with a se-
mantic label, scope identification does not involve 
semantic label classification and thus could be di-
vided into three consequent phases: argument 
pruning, argument identification and post-
processing. 
 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the cue-scope 
structures in scope learning can be also classified 
into several certain types and argument pruning 
can be done by employing several heuristic rules 
accordingly to filter out constituents, which are 
most likely non-arguments of a given cue. Similar 
to the heuristic algorithm proposed in Xue and 
Palmer (2004) for argument pruning in common 
shallow semantic parsing, the argument pruning 
algorithm adopted here starts from designating the 
cue node as the current node and collects its sib-
lings. It then iteratively moves one level up to the 
parent of the current node and collects its siblings. 
The algorithm ends when it reaches the root of the 
parse tree. To sum up, except the cue node itself 
and its ancestral constituents, any constituent in the 
parse tree whose parent covers the given cue will 
be collected as argument candidates. Taking the 
negation cue node ?RB7,7? in Figure 2 as an exam-
ple, constituents {MD6,6, VP8,11, NP4,5, IN3,3,  
717
 
 
Feature Remarks 
B1 Cue itself: the word of the cue, e.g., not,
rather_than. (can_not) 
B2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
B3 Path: the syntactic path from the argument 
candidate to the cue. (NP<S>VP>RB) 
B4 Position: the positional relationship of the
argument candidate with the cue. ?left? or 
?right?. (left) 
Table 1: Basic features and their instantiations for ar-
gument identification in scope learning, with NP4,5 as 
the focus constituent (i.e., the argument candidate) and 
?can not? as the given cue, regarding Figure 2. 
 
 
Feature Remarks 
Argument Candidate (AC) related 
AC1 The headword (AC1H) and its POS
(AC1P). (resistance, NN) 
AC2 The left word (AC2W) and its POS
(AC2P). (that, IN) 
AC3 The right word (AC3W) and its POS
(AC3P). (can, MD) 
AC4 The phrase type of its left sibling (AC4L)
and its right sibling (AC4R). (NULL, VP)
AC5 The phrase type of its parent node. (S) 
AC6 The subcategory. (S:NP+VP) 
Cue/Predicate (CP) related 
CP1 Its POS. (RB) 
CP2 Its left word (CP2L) and right word
(CP2R). (can, be) 
CP3 The subcategory. (VP:MD+RB+VP) 
CP4 The phrase type of its parent node. (VP) 
Combined Features related with the Argument Candi-
date  (CFAC1-CFAC2) 
b2&AC1H, b2&AC1P 
Combined Features related with the given
Cue/Predicate  (CFCP1-CFCP2) 
B1&CP2L, B1&CP2R 
Combined Features related with both the Argument 
Candidate and the given Cue/Predicate (CFACCP1-
CFACCP7) 
B1&B2, B1&B3, B1&CP1, B3&CFCP1, B3&CFCP2, 
B4&CFCP1, B4&CFCP2 
Table 2: Additional features and their instantiations for 
argument identification in scope identification, with 
NP4,5 as the focus constituent (i.e., the argument candi-
date) and ?can not? as the given cue, regarding Figure 2. 
 
VBP2,2, and NP0,1} are collected as its argument 
candidates consequently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine the 
argument candidates as either valid arguments or 
non-arguments. Similar to argument identification 
in common shallow semantic parsing, the struc-
tured syntactic information plays a critical role in 
scope learning. 
Basic Features 
Table 1 lists the basic features for argument identi-
fication. These features are also widely used in 
common shallow semantic parsing for both verbal 
and nominal predicates (Xue, 2008; Li et al, 2009). 
Additional Features 
To capture more useful information in the cue-
scope structures, we also explore various kinds of 
additional features. Table 2 shows the features in 
better capturing the details regarding the argument 
candidate and the cue. In particular, we categorize 
the additional features into three groups according 
to their relationship with the argument candidate 
(AC, in short) and the given cue/predicate (CP, in 
short). 
Some features proposed above may not be effec-
tive in argument identification. Therefore, we 
adopt the greedy feature selection algorithm as de-
scribed in Jiang and Ng (2006) to pick up positive 
features incrementally according to their contribu-
tions on the development data. The algorithm re-
peatedly selects one feature each time, which con-
tributes most, and stops when adding any of the 
remaining features fails to improve the perform-
ance. 
4.4 Post-Processing 
Although a cue in the BioScope corpus always has 
only one continuous block as its scope (including 
the cue itself), the scope identifier may result in 
discontinuous scope due to independent predica-
tion in the argument identification phase. Given the 
golden negation/speculation cues, we observe that 
6.2%/9.1% of the negation/speculation scopes pre-
dicted by our scope identifier are discontinuous. 
718
 
Figure 3 demonstrates the projection of all the 
argument candidates into the word level. Accord-
ing to our argument pruning algorithm in Section 
4.2, except the words presented by the cue, the pro-
jection covers the whole sentence and each con-
stituent (LACi or RACj in Figure 3) receives a 
probability distribution of being an argument of the 
given cue in the argument identification phase. 
Since a cue is deemed inside its scope in the 
BioScope corpus, our post-processing algorithm 
first includes the cue in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the cue itself, the left-
most word of constituent LACi (1<=i<=m). Sup-
posing LACi receives probability of Pi being an 
argument, we use the following formula to deter-
mine LACk* whose leftmost word represents the 
boundary of the left scope. If k*=0, then the cue 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ?  P?
Similarly, the right boundary of the given cue 
can be decided. 
4.5 Cue Recognition 
Automatic recognition of cues of a special interest 
is the prerequisite for a scope learning system. The 
approaches to recognizing cues of a special interest 
usually fall into two categories: 1) substring 
matching approaches, which require a set of cue 
words or phrases in advance (e.g., Light et al, 
2004); 2) machine learning approaches, which 
train a classifier with either supervised or semi-
supervised learning methods (e.g., ?zg?r and 
Radev, 2009; Szarvas, 2008). Without loss of gen-
erality, we adopt a machine learning approach and 
train a classifier with supervised learning. In par-
ticular, we make an independent classification for 
each word with a BIO label to indicate whether it 
is the first word of a cue, inside a cue, or outside of 
it, respectively. 
LACm    ?.      LAC1 RAC1      ?.    RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
Inspired by previous studies on similar tasks 
such as WSD and nominal predicate recognition in 
shallow semantic parsing (Lee and Ng, 2002; Li et 
al., 2009), where various features on the word it-
self, surrounding words and syntactic information 
have been successfully used, we believe that such 
information is also valuable to automatic recogni-
tion of cues. Table 3 shows the features employed 
for cue recognition. In particular, we categorize 
these features into three groups: 1) features about 
the cue candidate itself (CC in short); 2) features 
about surrounding words (SW in short); and 3) 
structural features derived from the syntactic parse 
tree (SF in short).
 
Feature Remarks 
Cue Candidate (CC) related 
CC1 The cue candidate itself. (indicate) 
CC2 The stem of the cue candidate. (indicate)
CC3 The POS tag of the cue candidate. (VBP)
Surrounding Words (SW) related 
SW1 The left surrounding words with the win-
dow size of 3. (these, findings) 
SW2 The right surrounding words with the 
window size of 3. (that, corticosteroid,
resistance) 
Structural Features (SF) 
SF1 The subcategory of the candidate node.  
(VP-->VBP+SBAR) 
SF2 The subcategory of the candidate node?s 
parent. (S-->NP+VP) 
SF3 POS tag of the candidate node + Phrase 
type of its parent node + Phrase type of its 
grandpa node. (VBP + VP + S) 
Table 3: Features and their instantiations for cue recog-
nition, with VBP2,2 as the cue candidate, regarding Fig-
ure 2. 
5 Experimentation 
We have evaluated our simplified shallow seman-
tic parsing approach to negation and speculation 
scope learning on the BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b), the abstracts subcorpus is randomly di-
vided into 10 folds so as to perform 10-fold cross-
validation, while the performance on both the pa-
719
pers and clinical reports subcorpora is evaluated 
using the system trained on the whole abstracts 
subcorpus. In addition, SVMLight  is selected as 
our classifier. 
5
For cue recognition, we report its performance 
using precision/recall/F1-measure. For scope iden-
tification, we report the accuracy in PCS (Percent-
age of Correct Scopes) when the golden cues are 
given, and report precision/recall/F1-measure 
when the cues are automatically recognized. 
5.2 Experimental Results on Golden Parse 
Trees and Golden Cues 
In order to select beneficial features from the addi-
tional features proposed in Section 4.3, we ran-
domly split the abstracts subcorpus into the 
training data and the development data with pro-
portion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 7 
features {CFACCP5, CP2R, CFCP1, AC1P, CP3, 
CFACCP7, AC4R} are selected consecutively for 
negation scope identification while 11 features 
{CFACCP5, AC2W, CFACCP2, CFACCP4, AC5, 
CFCP1, CFACCP7, CFACCP1, CP4, AC3P, 
CFAC2} are selected for speculation scope identi-
fication. Table 4 gives the contribution of addi-
tional features on the development data. It shows 
that the additional features significantly improve 
the performance by 11.66% in accuracy from 
74.93% to 86.59% ( ) for negation scope 
identification and improve the performance by 
11.07% in accuracy from 77.29% to 88.36% 
( ) for speculation scope identification. 
The feature selection experiments suggest that the 
features (e.g., CFACCP5, AC2W, CFCP1) related 
to neighboring words of the cue play a critical role 
for both negation and speculation scope identifica-
tion. This may be due to the fact that neighboring 
words usually imply important sentential informa-
tion. For example, ?can not be? indicates a passive 
clause while ?did not? indicates an active clause. 
2; 0.0p? < 1
1
                                                          
2; 0.0p? <
Since the additional selected features signifi-
cantly improve the performance for both negation 
and speculation scope identification, we will in-
clude those additional selected features in all the 
remaining experiments. 
 
 
5 http://svmlight.joachims.org/ 
Task Features Acc (%) 
Baseline 74.93 Negation scope 
identification +selected features 86.59 
Baseline 77.29 Speculation scope 
identification +selected features 88.36 
Table 4: Contribution of additional selected features on 
the development dataset of the abstracts subcorpus 
 
Since all the sentences in the abstracts subcorpus 
are included in the GTB1.0 corpus while we do not 
have golden parse trees for the sentences in the full 
papers and the clinical reports subcorpora, we only 
evaluate the performance of scope identification on 
the abstracts subcorpus with golden parse trees. 
Table 5 presents the performance on the abstracts 
subcorpus by performing 10-fold cross-validation. 
It shows that given golden parse trees and golden 
cues, speculation scope identification achieves 
higher performance (e.g., ~3.3% higher in accu-
racy) than negation scope identification. This is 
mainly due to the observation on the BioScope 
corpus that the scope of a speculation cue can be 
usually characterized by its POS and the syntactic 
structures of the sentence where it occurs. For ex-
ample, the scope of a verb in active voice usually 
starts at the cue itself and ends at its object (e.g., 
the speculation cue ?indicate that? in Figure 2 
scopes the fragment of ?indicate that corticoster-
oid resistance can not be explained by abnormali-
ties?). Moreover, the statistics on the abstracts 
subcorpus shows that the number of arguments per 
speculation cue is smaller than that of arguments 
per negation cue (e.g., 1.5 vs. 1.8). 
 
Task Acc (%) 
Negation scope identification 83.10 
Speculation scope identification 86.41 
Table 5: Accuracy (%) of scope identification with 
golden parse trees and golden cues on the abstracts sub-
corpus using 10-fold cross-validation 
 
It is worth nothing that we adopted the post-
processing algorithm proposed in Section 4.4 to 
ensure the continuousness of identified scope. As 
to examine the effectiveness of the algorithm, we 
abandon the proposed algorithm by simply taking 
the left and right-most boundaries of any nodes in 
the tree which are classified as in scope. Experi-
ments on the abstracts subcorpus using 10-fold 
cross-validation shows that the simple post-
processing rule gets the performance of 80.59 and 
86.08 in accuracy for negation and speculation 
720
scope identification, respectively, which is lower 
than the performance in Table 5 achieved by our 
post-processing algorithm.  
5.3 Experimental Results on Automatic 
Parse Trees and Golden Cues 
The GTB1.0 corpus contains 18,541 sentences in 
which 11,850 of them (63.91%) overlap with the 
sentences in the abstracts subcorpus6. In order to 
get automatic parse trees, we train the Berkeley 
parser with the remaining 6,691 sentences in 
GTB1.0, which achieves the performance of 85.22 
in F1-measure on the remaining 11,850 sentences 
in GTB1.0. Table 6 shows the performance of 
scope identification on automatic parse trees and 
golden cues. In addition, we also report an oracle 
performance to explore the best possible perform-
ance of our system by assuming that our scope 
finder can always correctly determine whether a 
candidate is an argument or not. That is, if an ar-
gument candidate falls within the golden scope, 
then it is a argument. This is to measure the impact 
of automatic syntactic parsing itself. Table 6 shows 
that: 
1) For both negation and speculaiton scope 
identification, automatic syntactic parsing 
lowers the performance on the abstracts 
subcorpus (e.g., from 83.10% to 81.84% in 
accuracy for negation scope identification and 
from 86.41% to 83.74% in accuracy for 
speculaiton scope identification). However, the 
performance drop shows that both negation and 
speculation scope identification are not as 
senstive to automatic syntactic parsing as 
common shallow semantic parsing, whose 
performance might decrease by about ~10 in F1-
measure (Toutanova et al, 2005). This indicates 
that scope identification via simplified shallow 
semantic parsing is robust to some variations in 
the parse trees.  
2) Although speculation scope identification 
consistently achieves higher performance than 
negaiton scope identification when golden parse 
trees are availabe, speculation scope 
identification achieves comparable performance 
with negation scope identification on the 
abstracts subcorpus and the full papers 
                                                          
6 There are a few cases where two sentences in the abstracts 
subcorpus map into one sentence in GTB1.0. 
subcorpus while speculation scope identification 
even performs ~20% lower in accuracy than 
negation scope identification on the clinical 
report subcorpus. This is largely due to that 
specuaiton scope identification is more sensitive 
to syntactic parsing errors than negation scope 
identification due to the wider scope of a 
speculation cue while the sentences of the 
clinical reports come from a different genre, 
which indicates low performance in syntactic 
parsing.  
3) Given the performance gap between the 
performance of our scope finder and the oracle 
performance, there is still much room for further 
performance improvement. 
 
Task Method Abstracts Papers Clinical
auto 81.84 62.70 85.21 Negation scope 
identification oracle 94.37 83.33 98.39 
auto 83.74 61.29 67.90 Speculation scope
identification oracle 95.69 83.72 83.29 
Table 6: Accuracy (%) of scope identification on the 
three subcorpora using automatic parser trained on 
6,691 sentences in GTB1.0 
 
Task Method Abstracts Papers Clinical
M et al (2008) 57.33 n/a n/a 
M & D (2009a) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Negation 
scope 
identification 
Our final  81.84 64.02 89.79 
M & D (2009b) 77.13 47.94 60.59 
? & R (2009) 79.89 61.13 n/a 
Our baseline 77.39 54.55 61.92 
Speculation 
scope 
identification 
Our final  83.74 63.49 68.78 
Table 7: Performance comparison of our system with 
the state-of-the-art ones in accuracy (%). Note that all 
the performances achieved on the full papers subcorpus 
and the clinical subcorpus are achieved using the whole 
GTB1.0 corpus of 18,541 sentences while all the per-
formances achieved on the abstract subcorpus are 
achieved using 6,691 sentences from GTB1.0 due to 
overlap of the abstract subcorpus with GTB1.0. 
 
Table 7 compares our performance with related 
ones. It shows that even our baseline system with 
the four basic features presented in Table 1 
achieves comparable performance with Morante et 
al. (2008) and Morante and Daelemans (2009a & 
2009b). This further indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syntactic 
information on scope identification. It also shows 
that our final system significantly outperforms the 
721
state-of-the-art ones using a chunking approach, 
especially on the abstracts and full papers subcor-
pora. However, the improvement on the clinical 
reports subcorpora for negation scope identifica-
tion is much less apparent, partly due to the fact 
that the sentences in this subcorpus are much sim-
pler (with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Table 7 also shows that our parsing 
approach to speculation scope identification out-
performs the rule-based method in ?zg?r and 
Radev (2009), where 10-fold cross-validation is 
performed on both the abstracts and the full papers 
subcorpora. 
5.4 Experimental Results with Automatic 
Parse Trees and Automatic Cues 
So far negation/speculation cues are assumed to be 
manually annotated and available. Here we turn to 
a more realistic scenario in which cues are auto-
matically recognized. In the following, we first 
report the results of cue recognition and then the 
results of scope identification with automatic cues. 
Cue Recognition 
Task Features R (%) P (%) F1 
CC + SW 93.80 94.39 94.09Negation cue  
recognition CC+SW+SF 95.50 95.72 95.61
CC + SW 83.77 92.04 87.71Speculation cue  
recognition CC+SW+SF 84.33 93.07 88.49
Table 8: Performance of automatic cue recognition with 
gold parse trees on the abstracts subcorpus using 10-fold 
cross-validation 
 
Table 8 lists the performance of cue recognition on 
the abstracts subcorpus, assuming all words in the 
sentences as candidates. It shows that as a com-
plement to features derived from word/pos infor-
mation (CC+SW features), structural features (SF 
features) derived from the syntactic parse tree sig-
nificantly improve the performance of cue recogni-
tion by about 1.52 and 0.78 in F1-measure for 
negation and speculation cue recognition, respec-
tively, and thus included thereafter. In addition, we 
have also experimented on only these words, 
which happen to be a cue or inside a cue in the 
training data as cue candidates. However, this ex-
perimental setting achieves a lower performance 
than that when all words are considered. 
 
Task Corpus R (%) P (%) F1 
Abstracts 94.99 94.35 94.67 
Papers 90.48 87.47 88.95 
Negation cue 
recognition 
Clinical 86.81 88.54 87.67 
Abstracts 83.74 93.14 88.19 
Papers 73.02 82.31 77.39 
Speculation cue 
recognition 
Clinical 33.33 91.77 48.90 
Table 9: Performance of automatic cue recognition with 
automatic parse trees on the three subcorpora 
 
Table 9 presents the performance of cue recog-
nition achieved with automatic parse trees on the 
three subcorpora. It shows that: 
1) The performance gap of cue recognition 
between golden parse trees and automatic parse 
trees on the abstracts subcorpus is not salient 
(e.g., 95.61 vs. 94.67 in F1-measure for negation 
cues and 88.49 vs. 88.19 for speculation cues), 
largely due to the features defined for cue 
recognition are local and insenstive to syntactic 
variations. 
2) The performance of negation cue recognition is 
higher than that of speculation cue recognition 
on all the three subcorpora. This is prabably due 
to the fact that the collection of negation cue 
words or phrases is limitted while speculation 
cue words or phrases are more open. This is 
illustrated by our statistics that about only 1% 
and 1% of negation cues in the full papers and 
the clinical reports subcorpora are absent from 
the abstracts subcorpus, compared to about 6% 
and 20% for speculation cues. 
3) Unexpected, the recall of speculation cue 
recognition on the clinical reports subcorpus is 
very low (i.e., 33.33% in recall measure). This is 
probably due to the absence of about 20% 
speculation cues from the training data of the 
abstracts subcorpus. Moreover, the speculation 
cue ?or?, which accounts for about 24% of 
specuaiton cues in the clinical reports subcorpus, 
only acheives about 2% in recall largely due to 
the errors caused by the classifier trained on the 
abstracts subcorpus, where only about 11% of 
words ?or? are annotated as speculation cues. 
Scope Identification with Automatic Cue Rec-
ognition 
Table 10 lists the performance of both negation 
and speculation scope identification with automatic 
cues and automatic parse trees. It shows that auto-
matic cue recognition lowers the performance by 
722
3.34, 6.80, and 8.38 in F1-measure for negation 
scope identification on the abstracts, the full papers 
and the clinical reports subcorpora, respectively, 
while it lowers the performance by 6.50, 13.14 and 
31.23 in F1-measures for speculation scope identi-
fication on the three subcorpora, respectively, sug-
gesting the big challenge of cue recognition in the 
two scope learning tasks. 
 
Task Corpus R (%) P (%) F1 
Abstracts 78.77 78.24 78.50
Papers 58.20 56.27 57.22
Negation scope 
identification 
Clinical 80.62 82.22 81.41
Abstracts 73.34 81.58 77.24
Papers 47.51 53.55 50.35
Speculation scope 
identification 
Clinical 25.59 70.46 37.55
Table 10: Performance of both negation and speculation 
scope identification with automatic cues and automatic 
parse trees 
6 Conclusion  
In this paper we have presented a new approach to 
scope learning by formulating it as a simplified 
shallow semantic parsing problem, which has been 
extensively studied in the past few years. In par-
ticular, we regard the cue as the predicate and map 
its scope into several constituents which are 
deemed as arguments of the cue. Evaluation on the 
Bioscope corpus shows the appropriateness of our 
parsing approach and that structured syntactic in-
formation plays a critical role in capturing the 
domination relationship between a cue and its 
dominated arguments. It also shows that our pars-
ing approach outperforms the state-of-the-art 
chunking ones. Although our approach is only 
evaluated on negation and speculation scope learn-
ing here, it is portable to other kinds of scope 
learning. 
For the future work, we will explore tree kernel-
based methods to further improve the performance 
of scope learning in better capturing the structural 
information, and apply our parsing approach to 
other kinds of scope learning. 
Acknowledgments 
This research was supported by Projects 60873150, 
60970056, and 90920004 under the National Natu-
ral Science Foundation of China, Project 
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion of China. 
References  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. CoNLL? 2005. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34: 301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et al 
1999. The GENIA Project: Corpus-Based Knowl-
edge Acquisition and Information Extraction from 
Genome Research Papers. EACL?1999. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. CoNLL?2010: 
Shared Task. 
Daniel Gildea and Martha Palmer. 2002. The Necessity 
of Parsing for Predicate Argument Recognition. 
ACL?2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
SIGIR?2003. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
Role Labeling of NomBank: A Maximum Entropy 
Approach. EMNLP? 2006. 
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empiri-
cal Evaluation of Knowledge Sources and Learning 
Algorithms for Word Sense Disambiguation. 
EMNLP?2002. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. 2009. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information and 
Automatic Predicate Recognition. EMNLP? 2009. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Specula-
tions, and Statements in Between. BioLink?2004. 
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific 
Literature. ACL?2007. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. EMNLP?2008. 
723
Roser Morante and Walter Daelemans. 2009a. A 
Metalearning Approach to Processing the Scope of 
Negation. CoNLL?2009. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. 
BioNLP?2009. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
COLING?2010. 
Arzucan ?zg?r, Dragomir R. Radev. 2009. Detecting 
Speculations and their Scopes in Scientific Text. 
EMNLP?2009. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. NAACL?2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic Role 
Labeling. IJCAI? 2005. 
Gy?rgy Szarvas. 2008. Hedge Classification in Bio-
medical Texts with a Weakly Supervised Selection of 
Keywords. ACL?2008. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in 
Biomedical Texts. BioNLP?2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. IJCNLP?2005 (Companion volume). 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL?2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. EMNLP?2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic Roles. Computational Linguistics, 
34(2):225-255.  
724
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253?257
Manchester, August 2008
Dependency Tree-based SRL with Proper Pruning and Extensive 
Feature Engineering 
Hongling Wang    Honglin Wang   Guodong Zhou   Qiaoming Zhu 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, 
Soochow University, Suzhou, China 215006 
{redleaf, 064227065055,gdzhou, qmzhu}@suda.edu.cn 
 
 
 
Abstract 
This paper proposes a dependency tree-
based SRL system with proper pruning and 
extensive feature engineering. Official 
evaluation on the CoNLL 2008 shared task 
shows that our system achieves 76.19 in la-
beled macro F1 for the overall task, 84.56 
in labeled attachment score for syntactic 
dependencies, and 67.12 in labeled F1 for 
semantic dependencies on combined test 
set, using the standalone MaltParser. Be-
sides, this paper also presents our unofficial 
system by 1) applying a new effective 
pruning algorithm; 2) including additional 
features; and 3) adopting a better depend-
ency parser, MSTParser. Unofficial evalua-
tion on the shared task shows that our sys-
tem achieves 82.53 in labeled macro F1, 
86.39 in labeled attachment score, and 
78.64 in labeled F1, using MSTParser on 
combined test set. This suggests that proper 
pruning and extensive feature engineering 
contributes much in dependency tree-based 
SRL.  
1 Introduction 
Although CoNLL 2008 shared task mainly 
evaluates joint learning of syntactic and semantic 
parsing, we focus on dependency tree-based se-
mantic role labeling (SRL). SRL refers to label 
the semantic roles of predicates (either verbs or 
nouns) in a sentence. Most of previous SRL sys-
tems (Gildea and Jurafsky, 2002; Gildea and 
Palmer, 2002; Punyakanok et al, 2005; Pradhan 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
 
et al, 2004, 2005) work on constituent structure 
trees and has shown to achieve remarkable re-
sults. For example, Punyakanok et al (2005) 
achieved the best performance in the CoNLL 
2005 shared task with 79.44 in F-measure on the 
WSJ test set and 77.92 on the combined test set 
(WSJ +Brown). 
With rapid development of dependency pars-
ing in the last few years, more and more re-
searchers turn to dependency tree-based SRL 
with hope to advance SRL from viewpoint of 
dependency parsing. Hacioglu (2004) pioneered 
this work by formulating SRL as a classification 
problem of mapping various dependency rela-
tions into semantic roles. Compared with previ-
ous researches on constituent structure tree-based 
SRL which adopts constituents as labeling units, 
dependency tree-based SRL adopts dependency 
relations as labeling units. Due to the difference 
between constituent structure trees and depend-
ency trees, their feature spaces are expected to be 
somewhat different. 
In the CoNLL 2008 shared task, we extend the 
framework by Hacioglu (2004) with maximum 
entropy as our classifier. For evaluation, we will 
mainly report our official SRL performance us-
ing MaltParser (Nivre and Nilsson, 2005). Be-
sides, we will also present our unofficial system 
by 1) applying a new effective pruning algorithm; 
2) including additional features; and 3) adopting 
a better dependency parser, MSTParser (McDon-
ald, 2005). 
In the remainder of this paper, we will briefly 
describe our system architecture, present various 
features used by our models and report the per-
formance on CoNLL 2008 shared task (both offi-
cial and unofficial). 
253
2 System Description 
In CoNLL 2008 shared task, we adopt a standard 
three-stage process for SRL: pruning, argument 
identification and argument classification. To 
model the difference between verb and noun 
predicates, we carry out separate training and 
testing for verb and noun predicates respectively. 
In addition, we adopt OpenNLP maximum en-
tropy package 1  in argument identification and 
classification. 
2.1 Predicate identification 
Most of Previous SRL systems only consider 
given predicates. However, predicates are not 
given in CoNLL 2008 shared task and required 
to be determined automatically by the system. 
Therefore, the first step of the shared task is to 
identify the verb and noun predicates in a sen-
tence. Due to time limitation, a simple algorithm 
is developed to identify noun and verb predicates: 
1) For the WSJ corpus, we simply adopt the 
annotations provided by PropBank and 
NomBank. That is, we only consider the verb 
and noun predicates annotated in PropBank 
and NomBank respectively.  
2) For the Brown corpus, verb predicates are 
identified simply according to its POS tag 
and noun predicates are determined using a 
simple method that only those nouns which 
can also be used as verbs are identified. To 
achieve this goal, an English lexicon of about 
56K word is applied to identify noun predi-
cates.  
Evaluation on the test set of CoNLL 2008 
shared task shows that our simple predicate iden-
tification algorithm achieves the accuracies of 
98.6% and 92.7 in the WSJ corpus for verb and 
noun predicates respectively, with overall accu-
racy of 95.5%, while it achieves the accuracies of 
73.5% and 43.1% in the Brown corpus for verb 
and noun predicates respectively with overall 
accuracy of 61.8%. This means that the perform-
ance of predicate identification in the Brown 
corpus is much lower than the one in the WSJ 
corpus. This further suggests that much work is 
required to achieve reasonable predicate identifi-
cation performance in future work. 
2.2 Preprocessing 
Using the dependency relations returned by a 
dependency parser (either MaltParser or 
                                                 
1https://sourceforge.net/project/showfiles.php?group_id=59
61 
MSTParser in this paper), we can construct cor-
responding dependency tree for a given sentence. 
For example, Figure 1 shows the dependency 
tree of the sentence ?Meanwhile, overall evi-
dence on the economy remains fairly clouded.?. 
Here, W is composed of two parts: word and its 
POS tag with ?/? as a separator while R means a 
dependency relation and ARG represents a se-
mantic role. 
In Hacioglu (2004), a simple pruning algo-
rithm is applied to filter out unlikely dependency 
relation nodes in a dependency tree by only 
keeping the parent/children/grand-children of the 
predicate, the siblings of the predicates, and the 
children/grandchildren of the siblings. This paper 
extends the algorithm a little bit by including the 
nodes two more layers upward and downward 
with regard to the predicate?s parent, such as the 
predicate?s grandparent, the grandparent?s chil-
dren and the grandchildren?s children. For the 
example as shown in Figure 1, all the nodes in 
the entire tree are kept. Evaluation on the training 
set shows that our pruning algorithm signifi-
cantly reduces the training instances by 76.9%. 
This is at expanse of wrongly pruning 1.0% se-
mantic arguments for verb predicates. However, 
this figure increases to 43.5% for noun predicates 
due to our later observation that about half of 
semantic arguments of noun predicates distrib-
utes over ancestor nodes out of our consideration. 
This suggests that a specific pruning algorithm is 
necessary for noun predicates to include more 
ancestor nodes. 
2.3 Features 
Some of the features are borrowed from Ha-
cioglu (2004) with some additional features mo-
tivated by constituent structure tree-based SRL 
(Pradhan et al2005; Xue and Palmer, 2004). In 
the following, we explain these features and give 
examples with regard to the dependency tree as 
shown in Figure 1. We take the word evidence in 
Figure 1 as the predicate and the node ?on? as 
the node on focus.  
The following eight basic features are moti-
vated from constituent structure tree-based SRL:  
1)  Predicate: predicate lemma. (evidence) 
2) Predicate POS: POS of current predicate. 
(NN) 
3)  Predicate Voice: Whether the predicate (verb) 
is realized as an active or passive construc-
tion. If the predicate is a noun, the value is 
null and presented as ?_?. ( _ ) 
 
254
 
Figure 1. Example of a dependency tree augmented with semantic roles  
for the given predicate evidence. 
 
4)  Relation type: the dependency relation type 
of the current node. (NMOD) 
5) Path: the chain of relations from current rela-
tion node to the predicate. (NMOD->SBJ) 
6) Sub-categorization: The relation type of 
predicate and the left-to-right chain of the re-
lation label sequence of the predicate?s chil-
dren. (SBJ->NMOD-NMOD) 
7)  Head word: the head word in the relation, 
that is, the headword of the parent of the cur-
rent node. (evidence) 
8)  Position: the position of the headword of the 
current node with respect to the predicate po-
sition in the sentence, which can be before, 
after or equal. (equal) 
Besides, we also include following additional 
features borrowed from Hacioglu (2004): 
1) Family membership: the relationship be-
tween current node and the predicate node in 
the family tree, such as parent, child, sibling. 
(child) 
2)  Dependent word: the modifying word in the 
relation, that is, the word of current node. (on) 
3) POS of headword: the POS tag of the head-
word of current word. (NN) 
4)  POS of dependent word: the POS tag of cur-
rent word. (IN) 
5)  POS pattern of predicate's children: the 
left-to-right chain of the POS tag sequence of 
the predicate?s children. (JJ-IN) 
6)  Relation pattern of predicate?s children: 
the left-to-right chain of the relation label se-
quence of the predicate?s children. (NMOD-
NMOD) 
7)  POS pattern of predicate?s siblings: the 
left-to-right chain of the POS tag sequence of 
the predicate?s siblings. (RB-.-VBN-.) 
8)  Relation pattern of predicate?s siblings: the 
left-to-right chain of the relation label se-
quence of the predicate?s siblings. (TMP-P-
PRD-P) 
3 System Performance 
All  the training data are included in our system, 
which costs 70 minutes in training and 5 seconds 
on testing on a PC platform with a Pentium D 
3.0G CPU and 2G Memory. In particular, the 
argument identification stage filters out those 
nodes whose probabilities of not being semantic 
arguments are more than 0.98 for verb and noun 
predicates. 
   Labeled 
Macro F1 
Labeled 
F1 
LAS 
Test WSJ 78.39 70.41 85.50
Test Brown 59.89 42.67 77.06
Test WSJ+Brown 76.19 67.12 84.56
Table 1: Official performance using MaltParser 
(with the SRL model trained and tested on the 
automatic output of MaltParser) 
 
All the performance is returned on the test set 
using the CoNLL 2008 evaluation script 
eval08.pl provided by the organizers. Table 1 
shows the official performance using MaltParser 
(with the SRL model trained and tested on the 
automatic output of MaltParser provided by the 
task organizers) as the dependency parser. It 
shows that our system performs well on the WSJ 
corpus and badly on the Brown corpus largely 
due to bad performance on predicate identifica-
tion.  
4 Post-evaluation System 
To gain more insights into dependency tree-
based SRL, we improve the system with a new 
255
pruning algorithm and additional features, after 
submitting our official results. 
4.1 Effective pruning 
Our new pruning algorithm is motivated by the 
one proposed by Xue and Palmer (2004), which  
only keeps those siblings to a node on the path 
from current predicate to the root are included, 
for constituent structure tree-based SRL. Our 
pruning algorithm further cuts off the nodes 
which are not related with the predicate. Besides, 
it filters out those nodes which are punctuations 
or with ?symbol? dependency relations. Evalua-
tion on the Brown corpus shows that our pruning 
algorithm significantly reduces the training data 
by 75.5% at the expense of wrongly filtering out 
0.7% and 0.5% semantic arguments for verb and 
noun predicates respectively. This suggests that 
our new pruning algorithm significantly performs 
better than the old one in our official system, es-
pecially for the identification of noun predicates. 
Furthermore, the argument identification stage 
filters out those nodes whose probabilities of not 
being semantic arguments are more than 0.90 
and 0.85 for verb and noun predicates respec-
tively, since we that our original threshold of 
0.98 in the official system is too reserved. 
Finally, those rarely-occurred semantic roles 
which occur less than 200 in the training set are 
filtered out and thus not considered in our system, 
such as A5, AA, C-A0, C-AM-ADV, R-A2 and SU. 
4.2 Extensive Feature Engineering 
Motivated by constituent structure tree-based 
SRL, two more combined features are considered 
in our post-evaluation system:  
1) Predicate + Headword: (evidence + remain) 
2) Headword + Relation: (remain + Root) 
In order to better evaluate the contribution of 
various additional feature, we build a baseline 
system using hand-corrected dependency rela-
tions and the eight basic features, motivated by 
constituent structure tree-based SRL, as de-
scribed in Section 2.3. Table 2 shows the effect 
of various additional features by adding one in-
dividually to the baseline system. It shows that 
the feature of dependent word is most useful, 
which improves the labeled F1 score from 
81.38% to 84.84%. It also shows that the two 
features about predicate?s sibling deteriorate the 
performance. Therefore, we delete these two fea-
tures from remaining experiments. Although the 
combined feature of ?predicate+head word? is 
useful in constituent structure tree-based SRL, it 
slightly decrease the performance in dependency 
tree-based SRL. For convenience, we include it 
in our system. 
 P R F1 
Baseline 84.31 78.64 81.38
+ Family membership 84.70 78.87 81.68
+ Dependent word  86.74 83.01 84.84
+ POS of headword 84.44 78.55 81.38
+ POS of dependent 
word 
84.42 78.33 81.47
+ POS pattern of 
predicate's children 
84.35 78.73 81.47
+ Relation pattern of 
predicate?s children 
84.75 78.97 81.76
+ Relation pattern of 
predicate?s siblings 
84.29 78.52 81.30
+ POS pattern of 
predicate?s siblings 
83.75 78.32 80.95
+ Predicate  +  Head-
word 
83.30 78.94 81.30
+Headword + Relation 84.66 79.37 81.93
Table 2: Effects of various additional features 
4.3 Best performance 
Table 3 shows our system performance after ap-
plying above effective pruning strategy and addi-
tional features using the default MaltParser. Ta-
ble 3 also reports our performance using the 
state-of-the-art MSTParser. To show the impact 
of predicate identification in dependency tree-
based SRL, Table 4 report the performance on 
gold predicate identification, i.e. only using an-
notated predicates in the corpora. 
Comparison of Table 1 and Table 3 using the 
MaltParser shows that our new extension with 
effective pruning and extensive engineering sig-
nificantly improves the performance. It also 
shows that MSTParser-based SRL performs 
slightly better than MaltParser-based one, much 
less than the performance difference on depend-
ency parsing between them. This suggests that 
such difference between these two state-of-the-
art dependency parsers does not much affect cor-
responding SRL systems. This is also confirmed 
by the results in Table 4. 
Comparison of Table 3 and Table 4 in labeled 
F1 on the Brown test data shows that the system 
with gold predicate identification significantly 
outperforms the one with automatic predicate 
identification using our simple algorithm by 
about 22 in labeled F1. This suggests that the 
performance of predicate identification is critical 
to SRL.  
256
 
MSTParser MaltParser  
Labeled Macro 
F1 
Labeled F1 LAS Labeled Macro 
F1 
Labeled F1 LAS 
Test WSJ 84.50 81.95 87.01 83.69 81.82 85.50
Test Brown 67.61 53.69 81.46 65.09 53.03 77.06
Test 
WSJ+Brown 82.53 78.64 86.39 81.52 78.45 84.56
Table 3: Unofficial performance using MSTParser and MaltParser 
 with predicates automatically identified 
 
MSTParser MaltParser  
Labeled Macro 
F1 
Labeled F1 LAS Labeled Macro 
F1 
Labeled F1 LAS 
Test WSJ 84.75 82.45 87.01 84.04 82.52 85.50
Test Brown 78.31 75.07 81.46 75.72 74.28 77.06
Test 
WSJ+Brown 84.05 81.66 86.39 83.13 81.64 84.56
Table 4: Unofficial performance using MSTParser and MaltParser with gold predicate identification 
 
5 Conclusions 
This paper presents a dependency tree-based 
SRL system by proper pruning and extensive 
feature engineering. Evaluation on the CoNLL 
2008 shared task shows that proper pruning and 
extensive feature engineering contributes much. 
It also shows that SRL heavily depends on the 
performance of predicate identification. 
In future work, we will explore better ways in 
predicate identification. In addition, we will ex-
plore more on dependency parsing and further 
joint learning on syntactic and semantic parsing. 
Acknowledgment 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China and Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China.  
References 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational Lin-
guistics, 28:3, pages 245-288. 
Gildea, Daniel and Martha Palmer. 2002. The Neces-
sity of Syntactic Parsing for Predicate Argument 
Recognition. In Proceedings of the 40th  Associa-
tion for Computational Linguistics,  2002.  
Hacioglu, Kadri. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics 
(COLING). 2004. 
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, 
Jan Haji?. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In the pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in 
Natural Language Processing, 2005 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008). 
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 99-106, 2005 
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, 
James H. Martin, Dan Jurafsky. 2004. Shallow 
Semantic Parsing Using Support Vector Machines. 
In Proceedings of (HLT-NAACL-2004), 2004. 
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, 
James H. Martin, Dan Jurafsky. 2005. Semantic 
role labeling using different syntactic views. In 
Proceedings of the 43rd  Association for Computa-
tional Linguistics (ACL-2005), 2005. 
Punyakanok, Vasin, Peter Koomen, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. In Pro-
ceedings of 9th Conference on Computational 
Natural Language Learning (CoNLL-2005).2005 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Proceedings 
of Conference on Empirical Methods in Natural 
Language Processing (EMNLP), 2004. 
257
