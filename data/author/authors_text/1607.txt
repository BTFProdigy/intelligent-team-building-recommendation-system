219
220
221
222
235
236
237
238
239
240
241
242
Perceptron Learning for Chinese Word Segmentation
Yaoyong Li?, Chuanjiang Miao?, Kalina Bontcheva?, Hamish Cunningham?
?Department of Computer Science, The University of Sheffield, Sheffield, S1 4DP, UK
{yaoyong,kalina,hamish}@dcs.shef.ac.uk
?Institute of Chinese Information Processing, Beijing Normal University, Beijing, 100875, China
miaochj@bnu.edu.cn
Abstract
We explored a simple, fast and effective
learning algorithm, the uneven margins
Perceptron, for Chinese word segmen-
tation. We adopted the character-based
classification framework and trans-
formed the task into several binary clas-
sification problems. We participated
the close and open tests for all the four
corpora. For the open test we only used
the utf-8 code knowledge for discrimi-
nation among Latin characters, Arabic
numbers and all other characters. Our
system performed well on the as, cityu
and msr corpora but was clearly worse
than the best result on the pku corpus.
1 Introduction
We participated in the closed and open tests for
all the four corpora, referred to as, cityu, msr and
pku, respectively. We adopted the character-based
methodology for Chinese word segmentation, that
processed text character by character. We ex-
plored a simple and effective learning algorithm,
the Perceptron with Uneven Margins (PAUM) for
Chinese word segmentation task.
For the open task, we only used the minimal ex-
ternal information ? the utf-8 code knowledge to
distinguish Latin characters and Arabic numbers
from other characters, justified by the fact that
the English text requires no segmentation since
they has been segmented already, and another fact
that any Arabic number in one particular context
should have the same segmentation.
2 Character Based Chinese Word
Segmentation
We adopted the character based methodology for
Chinese word segmentation, in which every char-
acter in a sentence was checked one by one to
see if it was a word on its own or it was begin-
ning, middle, or end character of a multi-character
word. In contrast, another commonly used strat-
egy, the word based methodology segments a Chi-
nese sentence into the words in a pre-defined
word list possibly with probability information
about each word, according to some maximum
probability criteria ( see e.g. Chen (2003)). The
performance of word based segmentation is de-
pendent upon the quality of word list used, while
the character based method does not need any
word list ? it segments a sentence only based on
the characters in the sentence.
Using character based methodology, we trans-
form the word segmentation problem into four
binary classification problems, corresponding to
single-character word, the beginning, middle and
end character of multi-character word, respec-
tively. For each of the four classes a classifier was
learnt from training set using the one vs. all others
paradigm, in which every character in the train-
ing data belonging to the class considered was re-
garded as positive example and all other charac-
ters were negative examples.
After learning, we applied the four classifiers to
each character in test text and assigned the char-
acter the class which classifier had the maximal
output among the four. This kind of strategy has
been widely used in the applications of machine
learning to named entity recognition and has also
154
been used in Chinese word segmentation (Xue
and Shen, 2003). Finally a word delimiter (often a
blank space, depending on particular corpus) was
added to the right of one character if it was not the
last character of a sentence and it was predicted
as end character of word or as a single character
word.
3 Learning Algorithm
Perceptron is a simple and effective learning al-
gorithm. For a binary classification problem, it
checks the training examples one by one by pre-
dicting their labels. If the prediction is correct,
the example is passed; otherwise, the example is
used to correct the model. The algorithm stops
when the model classifies all training examples
correctly. The margin Perceptron not only classi-
fies every training example correctly but also out-
puts for every training example a value (before
thresholding) larger than a predefined parameter
(margin). The margin Perceptron has better gen-
eralisation capability than the standard Percep-
tron. Li et al (2002) proposed the Perceptron al-
gorithm with uneven margins (PAUM) by intro-
ducing two margin parameters ?+ and ?? into the
update rules for the positive and negative exam-
ples, respectively. Two margin parameters allow
the PAUM to handle imbalanced datasets better
than both the standard Perceptron and the margin
Perceptron. PAUMhas been successfully used for
document classification and information extrac-
tion (Li et al, 2005).
We used the PAUM algorithm to train a clas-
sifier for each of four classes for Chinese word
segmentation. For one test example, the output of
the Perceptron classifier before thresholding was
used for comparison among the four classifiers.
The important parameters of the learning algo-
rithm are the uneven margins parameters ?+ and
??. In all our experiments ?+ = 20 and ?? = 1
were used.
Table 1 presents the results for each of the
four classification problems, obtained from 4-fold
cross-validation on training set. Not surprisingly,
the classification for middle character of multi-
character word was much harder than other three
classification problems, since middle character of
Chinese word is less characteristic than beginning
or end character or single-character word. On the
other hand, improvement on the classification for
middle character, while keeping the performances
of other classification, would improve the overall
performance of segmentation.
Table 1: Results for each of the four classifiers:
F1 (%) averaged over 4-fold cross-validation on
training sets of the four corpora. C1, C2 and
C3 refer to the classifier for beginning, middle
and end character of multi-character word, re-
spectively, and C4 refers to the classifier for single
character word.
C1 C2 C3 C4
as 95.64 90.07 95.47 95.27
cityu 96.64 90.06 96.43 95.14
msr 96.36 89.79 96.00 94.99
pku 96.09 89.99 96.18 94.12
Support vector machines (SVM) is a popular
learning algorithm, which has been successfully
applied to many classification problems in natural
language processing. Similar to the PAUM, SVM
is a maximal margin algorithm. Table 2 presents
a comparison of performances and computation
times between the PAUM and the SVM with lin-
ear kernel1 on three subsets of cityu corpora with
different sizes. The performance of SVM was
better than the PAUM. However, the larger the
training data was, the closer the performance of
PAUM to that of SVM. On the other hand, SVM
took much longer computation time than PAUM.
As a matter of fact, we have run the SVM with
linear kernel on the whole cityu training corpus
using 4-fold cross-validation for one month and it
has not finished yet. In contrast, PAUM just took
about one hour to run the same experiment.
4 Features for Each Character
In our system every character was regarded as
one instance for classification. The features for
one character were the character form itself and
the character forms of the two preceding and
the two following characters of the current one.
In other word, the features for one character c0
were the character forms from a context win-
1The SVMlight package version 5.0, available from
http://svmlight.joachims.org/, was used to learn the SVM
classifiers in our experiments.
155
Table 2: Comparison of the Perceptron with SVM
for Chinese word segmentation: averaged F1 (%)
over the 4-fold cross-validation on three subsets
of cityu corpus and the computation time (in sec-
ond) for each experiment. The three subsets have
100, 1000 and 5000 sentences, respectively.
100 1000 5000
PAUM 73.55 78.00 88.08
4s 14s 92s
SVM 75.50 79.15 88.78
227s 3977s 49353s
dow centering at c0 and containing five char-
acters {c?2, c?1, c0, c1, c2} in a sentence. Our
experiments on training data showed that co-
occurrences of characters in the context win-
dow were helpful. Taking account of all co-
occurrences of characters in context window is
equivalent to using a quadratic kernel in Percep-
tron, while not using any co-occurrence amounts
to a linear kernel. Actually we can only use part
of co-occurrences as features, which can be re-
garded as some kind of semi-quadratic kernel.
Table 3 compares the three types of ker-
nel for Perceptron, where for the semi-
quadratic kernel we used the co-occurrences
of characters in context window as those
used in (Xue and Shen, 2003), namely
{c?2c?1, c?1c0, c0c1, c1c2, c?1c1}. It was
shown that the quadratic kernel gave much better
results than linear kernel and the semi-quadratic
kernel was slightly better than fully quadratic ker-
nel. Semi-quadratic kernel also led to less feature
and less computation time than fully quadratic
kernel. Therefore, this kind of semi-quadratic
kernel was used in our submissions.
Table 3: Comparisons between different kernels
for Perceptron: F1 (%) averaged over 4-fold
cross-validation on three training sets.
linear quadratic semi-quadratic
cityu 81.30 94.78 95.13
msr 79.80 94.78 94.93
pku 82.33 94.80 95.05
Actually it has been noted that quadratic ker-
nel for Perceptron, as well as for SVM, per-
formed better than linear kernel for informa-
tion extraction and other NLP tasks (see e.g.
Carreras et al (2003)). However, quadratic ker-
nel was usually implemented in dual form for
Perceptron and it took very long time for train-
ing. We implemented the quadratic kernel for
Perceptron in primal form by encoding the linear
and quadratic features into feature vector explic-
itly. Actually our implementation performed even
slightly better than the Perceptron with quadratic
kernel as we used only part of quadratic features,
and it was still as efficient as the Perceptron with
linear kernel.
5 Open Test
While closed test required the participants only to
use the information presented in training material,
open test allowed to use any external information
or resources besides the training data. In our sub-
missions for the open test we just used the min-
imal external information, namely the utf-8 code
knowledge for identifying a piece of English text
or an Arabic number. and What we did by us-
ing this kind of knowledge was to pre-process the
text by replacing each piece of English text with
a symbol ?E? and replacing every Arabic num-
ber with another symbol ?N?. This kind of pre-
processing resulted in a smaller training data and
less computation time and yet slightly better per-
formance on training data, as shown in Table 4
which compares the results of collapsing the En-
glish text only and collapsing both the English
text and Arabic number with those for closed test.
Table 4 also presents the 95% confidence intervals
for the F-measures.
6 Results on Test Data
Table 5 presents our official results on test corpora
for both close and open tests. First, comparing
with the results in Table 4, the results on test set
are significantly different from the result using 4-
fold cross validation on training set for all the four
corpora. The test result was better than the results
on training set for the msr corpus but was worse
for other three corpora, especially for the pku cor-
pora. We suspected that this may be caused by
difference between training and test data, which
needs further investigation.
156
Table 4: Comparisons between the results for
close and open tests: averaged F1 (%) and the
95% confidence interval on the 4-fold cross-
validation on the training sets of four corpora and
the computation time (in hour) for each experi-
ment. ?English? means only collapsing English
texts and ?E & N? means collapsing both English
texts and Arabic numbers.
close test English E & N
as 95.53?0.46 95.65?0.47 95.78?0.46
8.88h 7.66h 7.07h
cityu 95.13 ?1.49 95.25 ?1.48 95.25 ?1.48
1.03h 0.86h 0.82h
msr 94.92 ?0.36 94.98 ?0.40 95.00 ?0.39
2.62h 1.69h 1.62h
pku 95.05 ?0.43 95.08 ?0.36 95.15 ?0.46
0.70h 0.63h 0.60h
Secondly, the test results for close and open
tests are close to each other on other three corpora
except the pku corpora, for which the result for
open test is clearly better than that for close test.
This was mainly because of different encoding
of Arabic number in training and test sets of the
pku corpus. Since Arabic number was encoded in
three bytes in training set but was encoded in one
byte in test set for the pku corpora, for close test
the trained model for Arabic number was not ap-
plicable to the Arabic numbers in test set. How-
ever, for open test, as we replaced Arabic num-
ber with one symbol in both training and test sets,
the different encoding of Arabic number in train-
ing and test sets could not cause any problem at
all, which led to better result. On the other hand,
our pre-processing with respect to the English text
and Arabic numbers seemed have slightly effect
on the F-measure for other three corpora.
Finally, comparing with the results of closed
test from other participants, our F1 figures were
no more than 0.008 lower than the best ones on
the as, cityu and msr corpora, but was 0.023 lower
than the best one on the pku corpus.
7 Conclusion
We applied the uneven margins Perceptron to Chi-
nese word segmentation. The learning algorithm
is simple, fast and effective. The results obtained
Table 5: The official results on test set: F-measure
(%) for close and open tests, respectively.
as cityu msr pku
close 94.4 93.6 95.6 92.7
open 94.8 93.6 95.4 93.8
are encouraging.
The performance of Perceptron was close to
that of the SVM on Chinese word segmentation
for large training data. On the other hand, the
Perceptron took much less computation time than
SVM.We implemented the Perceptron with semi-
quadratic kernel in primal form. Our implemen-
tation was both effective and efficient.
Our system performed well for the three of four
corpora, as, cityu and msr corpora. But it was
significantly worse than the best result on the pku
corpora, which needs further investigation.
Acknowledgements
This work is supported by the EU-funded SEKT
project (http://www.sekt-project.org).
References
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. Learn-
ing a perceptron-based named entity chunker via
online recognition feedback. In Proceedings of
CoNLL-2003, pages 156?159. Edmonton, Canada.
A. Chen. 2003. Chinese Word Segmentation Using
Minimal Linguistic Knowledge. In Proceedings of
the 2nd SIGHAN Workshop on Chinese Language
Processing.
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and
J. Kandola. 2002. The Perceptron Algorithm with
Uneven Margins. In Proceedings of the 9th Inter-
national Conference on Machine Learning (ICML-
2002), pages 379?386.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing UnevenMargins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on ComputationalNatural Language Learning
(CoNLL-2005).
N. Xue and L. Shen. 2003. Chinese Word Segmen-
tation as LMR Tagging. In Proceedings of the 2nd
SIGHAN Workshop on Chinese Language Process-
ing.
157
 
	
Abstract
AKT is a major research project
applying a variety of technologies to
knowledge management. Knowledge
is a dynamic, ubiquitous resource,
which is to be found equally in an
expert's head, under terabytes of data,
or explicitly stated in manuals. AKT
will extend knowledge management
technologies to exploit the potential
of the semantic web, covering the use
of knowledge over its entire lifecycle,
from acquisition to maintenance and
deletion. In this paper we discuss how
HLT will be used in AKT and how
the use of HLT will affect different
areas of KM, such as knowledge
acquisition, retrieval and publishing.
1 Introduction
As globalisation reduces the competitive
advantage existing between companies, the role
of proprietary information and its appropriate
management becomes all-important. A
company?s value depends more and more on
?intangible assets?1 which exist in the minds of
employees, in databases, in files and in a
multitude of documents. It is the goal of
knowledge management (KM) technologies to
make computer systems which provide access to
this intangible knowledge present in a company
or organisation.  The system must make it
possible to share, store and retrieve the
collective expertise of all the people in an
organization. At present, many companies spend
                                                     
1 A term coined by Karl-Erik Sveiby
considerable resources on knowledge
management; estimates range between 7 and
10% of revenues (Davenport 1998).
In developing a knowledge management
system, the knowledge must first be captured or
acquired in some form which is usable by a
computer. The knowledge acquisition
bottleneck, so well-known in AI, is just as
important in knowledge management. The
acquisition of knowledge does not become less
difficult in a business environment and often
requires a sea-change in company culture in
order to persuade users to accommodate to the
technology adopted, precisely because
knowledge acquisition is so difficult.
Once knowledge has been acquired, it must be
managed, i.e. modelled, updated and published.
Modelling means representing information in a
way that is both manageable and easy to
integrate with the rest of the company?s
knowledge. Updating is necessary because
knowledge is dynamic. Part of its importance
for a company or individual lies in the fact that
knowledge is ever changing and keeping up
with the change is a crucial dimension in
knowledge management. Publishing is the
process that allows sharing the knowledge
across the company. These needs have
crystallised in efforts to develop the so-called
Semantic Web. It is envisaged that in the future,
the content currently available on the Web (both
Internets and Intranets) as raw data will be
automatically annotated with machine-readable
semantic information.  In such a case, we will
no longer speak of information retrieval but
rather of Knowledge Retrieval because instead
of obtaining thousands of potentially relevant or
irrelevant documents, only the dozen or so
documents that are truly needed by the user will
be presented to them.
Using HLT for Acquiring, Retrieving and Publishing Knowledge in AKT:
Position Paper
K. Bontcheva, C. Brewster, F. Ciravegna, H. Cunningham,
L. Guthrie, R. Gaizauskas, Y. Wilks
Department of Computer Science, the University of Sheffield,
Regent Court, 211 Portobello Street, S1 4DP Sheffield, UK
Email: N.Surname@dcs.shef.ac.uk
 In this paper we present the way Human
Language Technology (HLT) is used to address
several facets of the KM problem:  acquiring,
retrieving, and publishing knowledge. The work
presented in this paper is supported by the AKT
project (Advanced Knowledge Technologies), a
multimillion pound six year research project
funded by the EPSRC in the UK. AKT, started
in 2000, involves the University of
Southampton, the Open University, the
University of Edinburgh, the University of
Aberdeen, and the University of Sheffield
together with a large number of major UK
companies. Its objectives are to develop
technologies to cope with the six main
challenges of knowledge management:
? acquisition ? reuse
? modelling ? publication
? retrieval/extraction ? maintenance
These challenges will be addressed by the
University of Sheffield in the context of AKT
by the application of a variety of human
language technologies. Here, we consider only
the contribution of HLT to the acquisition of
knowledge, its retrieval and extraction, its
publication, and finally the role of appropriate
HLT infrastructure to the completion of these
goals.
2 Knowledge Acquisition
Knowledge acquisition (KA) is concerned with
the process of turning data into coherent
knowledge for a computer program.  The need
for effective KA methods increases as the
quantity of data available electronically
increases year by year, and the importance it
plays in our society is more and more
recognised. The challenge, we believe, lies in
designing effective techniques for acquiring the
vast amounts of (largely) tacit knowledge. KA is
a complex process, which traditionally is
extremely time consuming.
Existing KA methodologies are varied but
almost always require a great deal of manual
input. One methodology, often used in Expert
Systems, involves the time-consuming process
of structured interviews (?protocols?), which are
then analysed by knowledge engineers in order
to codify and model the knowledge of an expert
in a particular domain. Even if a complex expert
system is not required, all forms of KA are very
labour intensive. Yahoo currently employs over
100 people to keep its category hierarchy up to
date (Dom 1999). Some methodologies have
started to appear to automate this process,
although still limited to some steps in the KA
process. They depend on replacing the
introspection of knowledge engineers or the
extended elicitations of the protocol methods
(Ericsson and Simon 1984) by using Human
Language Technologies, more specifically
Information Extraction, Natural Language
Processing and Information Retrieval.
 Although knowledge acquisition produces
data (knowledge) for use by a computer
program, the form and content of that
knowledge is often debated in the research
community.  Ontologies have emerged as one of
the most popular means of modelling the
knowledge of a domain.  The meaning of this
word varies somewhat in the literature, but
minimally it is a hierarchical taxonomy of
categories, concepts or words. Ontologies can
act as an index to the memory of an organisation
and facilitate semantic searches and the retrieval
of knowledge from the corporate memory as it
is embodied in documents and other archives.
Repeated research has shown their usefulness,
especially for specific domains (J?rvelin and
Kek?l?inen 2000). The process of ontology
construction is illustrated in the rest of this
section.
2.1 Taxonomy construction
We propose to introduce automation in the stage
of taxonomy construction mainly in order to
eliminate or reduce the need for extensive
elicitation of data.  In the literature approaches
to construction of taxonomies of concepts have
been proposed (Brown et al 1992, McMahon
and Smith 1996, Sanderson and Croft 1999).
Such approaches either use a large collection of
documents as their sole data source, or they can
attempt to use existing concepts to extend the
taxonomy (Agirre et al2000, Scott 1998).  We
intend to develop a semi-automatic method that,
starting from a seed ontology sketched by the
user, produces the final ontology via a cycle of
refinements by eliciting knowledge from a
collection of texts. In this approach the role of
the user should only be that of proposing an
initial ontology and validating/changing the
different versions proposed by the system.
We intend to integrate a methodology for
automatic hierarchy definition (such as
(Sanderson and Croft 1999)) with a method for
the identification of terms related to a concept
in a hierarchy (such as (Scott 1998)). The
advantage of this integration is that, as
knowledge is continually changing, we can
reconstruct an appropriate domain specific
ontology very rapidly. This does not preclude
incorporating an existing ontology and using the
tools to extend and update it on the basis of
appropriate texts. Finally an ontology defined in
this way has the particular advantage that it
overcomes the well-known ?Tennis problem?
associated with many predefined ontologies
such as WordNet, i.e where terms closely
related in a given domain are structurally very
distant such as ball and court, for example.
In addition we intend to employ classic
Information Extraction techniques (described
below) such as named entity recognition
(Humphreys et al 1998) in order to pre-process
the text, as the identification of complex terms
such as proper names, dates, numbers, etc,
allows to reduce data sparseness in learning
(Ciravegna 2000).
We plan to introduce many cycles of ontology
learning and validation. At each stage the
defined ontology can be: i) validated/corrected
by a user/expert; ii) used to retrieve a larger set
of appropriate documents to be used for further
refinement (J?rvelin and Kek?l?inen 2000); iii)
passed on to the next development stage.
2.2 Learning Other Relations
This stage proceeds to build on the skeletal
ontology in order to specify, as much as
possible without human intervention, relations
among concepts in the ontology, other than
ISAs. In order to flesh the concept relations, we
need to identify relations such as synonymy,
meronymy, antonymy and other relations. We
plan to integrate a variety of methods existing in
the literature, e.g. by using recurrences in verb
subcategorisation as a symptom of general
relations (Basili et al 1998), by using Morin?s
user-guided approach to identify the correct
lexico/syntactic environment (Morin 1999), and
by using methods such as (Hays 1997) to locate
specific cases of synonymy.
3 Knowledge Extraction
Assuming that the shape of knowledge has been
acquired and adequately modelled, it will have
to be stored in a repository from which it is
retrieved as and when needed. On the one hand
there is the problem of retrieving instances in
order to populate the resulting knowledge base.
On the other hand, considering that repositories
could become very substantial in size, there is
the necessity to navigate the repository in order
to extract the knowledge when needed. In this
section we focus on the problem of knowledge
base population, as it is in our opinion the most
challenging from the HLT point of view.
3.1 Knowledge Base  Population
Instance identification for Knowledge Base
population can be performed by HLT-based
document analysis. With the term documents,
we mean a wide variety of types of texts such as
plain texts, web pages, knowledge elicitation
interview transcriptions (protocols), etc.  For the
sake of this paper we limit our analysis to
language related tasks only, ignoring the
problem of multi-media information. As a first
step instance identification requires the
identification of relevant documents containing
citation of the interesting information
(document classification). Then it requires the
ability to identify and extract information from
documents (Information Extraction from text).
3.2 Document Classification
Text classification for IE purposes has been
explored both in the MUC conferences as well
as in some commercially oriented projects
(Ciravegna et al 2000). In concrete terms
classification is used in order to identify the
scenario to apply to a specific set of texts, while
IE will identify (i.e. index) the instances in the
texts.  In most cases of application document
classification is quite straightforward, being
limited to the Boolean classification of a
document between relevant/irrelevant (single
scenario application as in the MUC
conferences). In cases in which knowledge may
be distributed along a number of different
detailed scenarios, full document classification
is then needed. In such cases, two main
characteristics are relevant for the classification
approach: flexibility and refinability (Ciravegna
et al 1999). Flexibility is needed with respect
to both the number of the categories and the
granularity of the classification to be coped
with. Three main types of classification can be
identified: coarse-grained, fine-grained, and
content-based. Coarse-grained classification is
performed among a relatively small number of
classes (e.g., some dozens) that are sharply
different (e.g., sport vs finance). This can be
obtained reliably and efficiently by the
application of statistical classifiers. Fine-
grained classification is performed over a
usually larger number of classes that can be
very similar (e.g., discriminating between news
about private bond issues and news about public
bond issues). This type of classification
generally requires some more knowledge-
oriented approaches such as pattern-based
classification. Sometimes categories are so
similar that classification needs to be content-
based, i.e. it can be performed only by
extracting the news content (e.g., finding news
articles issued by English financial institutions
referring to amounts in excess of 100,000 Euro).
In this case some forms of shallow adaptive
Information Extraction can be used (see next
section). Refinability concerns the possibility
of performing classification in a sequence of
steps, each one providing a more precise
classification (from coarse-grained to content-
based). In the current technological situation
coarse-grained classification can be performed
quickly, while the systems available for more
fine-grained classification are much slower and
less general purpose. When the amount of
textual material is large an incremental
approach, based on some level of coarse-grained
classification further refined by successive
analysis, proves to be very effective. A refinable
classification is generally performed over a
hierarchy of classes. A refinement may revise
the categories assigned to specific texts with
more specialised classes from the hierarchy.
More complex techniques are invoked only
when needed and, in any case, within an already
detected context (Ciravegna et al 1999).
We plan to produce a number of solutions for
text classification, adaptable to different
scenarios and situations, following the criteria
mentioned above.
3.3 Information Extraction
Information extraction from text (IE) is the
process of mapping of texts into fixed format
output (templates) representing the key
information (Gaizauskas 1997). In using IE for
KM, templates represent an intermediate format
for mapping the information in the texts into
ontology instances. Templates can be semi-
automatically derived from the ontology. We
plan to use IE for a number of passes: on the
one hand, we plan to populate a knowledge base
with instances as mentioned above. On the other
hand, IE can be used to monitor relevant
changes in the information, providing a
fundamental contribution to the problem of
knowledge updating. We have a long experience
in IE from texts, Sheffield having actively
participated in the MUC conferences and in the
TIPSTER project, activities that historically
have made a fundamental contribution to
making IE as we now know it.  The new
challenge we are currently addressing is
adaptivity. Adaptivity is a major goal for
Information Extraction, especially in the case of
its application to knowledge management, as
KM is a process that has to be distributed
throughout companies. The real value of IE will
become apparent when it can be adapted to new
applications and scenarios directly by the final
user without the intervention of IE experts. The
goal for research in adaptive IE is to create
systems adaptable to new applications/domains
by using only an analyst?s knowledge, i.e.
knowledge about the domain/scenario.
There are two directions of research in
adaptive IE, both involving the use of Machine
Learning. On the one hand machine learning is
used to automate as much as possible the tasks
an IE expert would perform in application
development (Cardie 1997) (Yangarber et al
2000). The goal here is to reduce the porting
time to a new application (and hence the cost).
This area of research comes mainly from the
MUC community. Currently, the technology
makes use mainly of NLP-intensive
technologies and the type of texts addressed are
mainly journal articles.
On the other hand, there is an attempt to make
IE systems adaptable to new
domains/applications by using only an analyst?s
knowledge, i.e. knowledge about the
domain/scenario only (Kushmerick et al 1997),
(Califf 1998), (Muslea et al 1998), (Freitag and
McCallum 1999), (Soderland 1999), (Freitag
and Kushmerick 2000), (Ciravegna 2001a).
Most research has so far focused on Web-
related texts (e.g. web pages, email, etc.)
Successful commercial products have been
created and there is an increasing interest on IE
in the Web-related market.  Current adaptive
technologies make no use of natural language
processing in the web context, as extra linguistic
structures (e.g. HTML tags, document
formatting, and ungrammatical stereotypical
language) are the elements used to identify
information. Linguistically intensive approaches
are difficult or unnecessary in such cases. When
these non-linguistic approaches are used on
texts with a reduced (or no) structure, they tend
to be ineffective.
There is a technological gap between adaptive
IE on free texts and adaptive IE on web-related
texts. For the purposes of KM, such a gap has to
be bridged so to create a set of technologies able
to cover the whole range of potential
applications for different kinds of texts, as the
type of texts to be analysed for KM may vary
dramatically from case to case. We plan to
bridge this gap via the use of lazy natural
language processing. We intend to use an
approach where the system starts with a range
of potential methodologies (from shallow to
linguistically intensive) and learns from a
training corpus which is the most effective
approach for the particular case under
consideration. A number of factors can
influence the choice: from the type of texts to be
analysed to the type of information the user is
able to provide in adapting the system. In the
first case the system will have to identify what
type of task is under consideration and select the
correct level of analysis  (e.g. language based
for free texts). Formally in this case the level of
language analysis is one of the parameters the
learner will have to learn. Concerning the type
of tagging the user is able to provide: different
users are able to provide different levels of
information in training the system: IE-trained
users are able to provide sophisticated tagging,
maybe inclusive of syntactic, semantic or
pragmatic information. Na?ve users on the other
hand are only able to provide some basic
information (e.g. to spot the relevant
information in the texts and highlight it in
different colours). We plan to develop a system
able to cope with a wide of variety of situations
by starting from the (LP)2 algorithm and
enhancing its learning capabilities on free texts
(Ciravegna 2001) and developing a powerful
human-computer interface for system adaptation
(Ciravegna and Petrelli 2001).
4 Knowledge Publishing
Knowledge is only effective if it is delivered in
the right form, at the right place, to the right
person at the right time. Knowledge publishing
is the process that allows getting knowledge to
the people who need it in a form that they can
use. As a matter of fact, different users need to
see knowledge presented and visualised in quite
different ways. The dynamic construction of
appropriate perspectives is a challenge which, in
AKT, we will address from the perspective of
generating automatically such presentations
from the ontologies acquired by the KA and KE
methods, discussed in the previous sections.
Natural Language Generation (NLG) systems
automatically produce language output (ranging
from a single sentence to an entire document)
from computer-accessible data, usually encoded
in a knowledge or data base (Reiter 2000). NLG
techniques have already been used successfully
in a number of application domains, the most
relevant of which is automatic production of
technical documentation (Reiter et al 1995),
(Paris et al 1996). In the context of KM and
knowledge publishing in particular, NLG is
needed for knowledge diffusion and
documenting ontologies. The first task is
concerned with personalised presentation of
knowledge, in the form needed by each specific
user and tailored to the correct language type
and the correct level of details. The latter is a
very important issue, because as discussed
earlier, knowledge is dynamic and needs to be
updated frequently. Consequently, the
accompanying documentation which is vital for
the understanding and successful use of the
acquired knowledge, needs to be updated in
sync. The use of NLG simplifies the ontology
maintenance and update tasks, so that the
knowledge engineer can concentrate on   the
knowledge itself, because the documentation is
automatically updated as the ontology changes.
The NLG-based knowledge publishing tools
will also utilise the ontology instances extracted
from documents using the IE approaches
discussed in Section 3.3. The dynamically
generated documentation will not only include
these instances, as soon as they get extracted,
but it will also provide examples of their
occurrence in the documents, thus facilitating
users? understanding and use of the ontology.
Our approach to knowledge publishing is based
on an existing framework for generation of user-
adapted hypertext explanations (Bontcheva
2001), (Bontcheva and Wilks 2001). The
framework incorporates a powerful agent
modelling module, which is used to tailor the
explanations to the user?s knowledge, task, and
preferences. We are now also extending the
personalisation techniques to account for user
interests. The main challenge for NLG will be
to develop robust and efficient techniques for
knowledge publishing which can operate on
large-scale knowledge resources and support the
personalised presentation of diverse
information, such as speech, video, text,
graphics (see (Maybury 2001)).
The other challenge in using NLG for
knowledge publishing is to develop tools and
techniques that will enable knowledge
engineers, instead of linguists, to create and
customise the linguistic resources (e.g., domain
lexicon) at the same time as they create and edit
the ontology.  In order to allow such inter-
operability with the KA tools, we will integrate
the NLG tools in the GATE infrastructure,
discussed next.
5 HLT Infrastructure
The range and complexity of the task of
knowledge management make imperative the
need for standardisation. While there has been
much talk about the re-use of knowledge
components such ontologies, much less has
been undertaken to standardise the
infrastructure for tools and their development.
The types of data structures typically involved
are large and complex, and without good tools
to manage and allow succinct viewing of the
data we will continue to work below our
potential. The University of Sheffield has
pioneered in the Gate and Gate 2 projects the
development of an architecture for text
engineering (Cunningham et al 1997),
(Cunningham et al 2000). Given the modular
architecture and component structure of Gate, it
is natural to build on this basis to extend the
capabilities of Gate so as to provide the most
suitable possible environment for tool
development, implementation and evaluation in
AKT. The system will provide a single
interaction and deployment point for the roll-out
of HLT in Knowledge Management. We expect
Gate2 to act as the skeleton for a large range of
knowledge management activities within AKT
and plan to extend its capabilities within the life
of the AKT project by integrating with suitable
ontological and lexical databases in order to
permit the use of  the Gate system with large
bodies of heterogeneous data
6 Conclusion and Future Work
We have presented how we plan to use HLT for
helping KM in AKT. We believe that HLT can
make a substantial contribution to the following
issues in  KM:
? Cost reduction: KM is an expensive task,
especially in the acquisition phase. HLT can
aid in automating both the acquisition of the
structure of the ontology to be learnt and in
populating such ontology with instances. It
will also provide support for automatic
knowledge documentation.
? Time reduction: KM is a slow task: HLT
can help in making it more efficient by
reducing the need for the human effort;
? Subjectivity reduction: this is a main
problem in knowledge identification and
selection. Subjective knowledge is difficult
to integrate with the rest of the company?s
knowledge and its use is somehow difficult.
KM constitutes a challenge for HLT as it
provides a number of fields of application and
in particular it challenges the integration of a set
of techniques for a common goal.
Acknowledgement
This work is supported under the Advanced
Knowledge Technologies (AKT)
Interdisciplinary Research Collaboration (IRC),
which is sponsored by the UK Engineering and
Physical Sciences Research Council under grant
number GR/N15764/01. The AKT IRC
comprises the Universities of Aberdeen,
Edinburgh, Sheffield, Southampton and the
Open University.
References
Agirre, E. O. Ansa, E. Hovy, and  D.
Mart?nez 2000. Enriching very large ontologies
using the WWW, Proceedings of the ECAI 2000
workshop ?Ontology Learning?.
Basili, R., R. Catizone, M. Stevenson, P.
Velardi, M. Vindigni, and Y. Wilks. 1998. ?An
Empirical Approach to Lexical Tuning?.
Proceedings of the Adapting Lexical and
Corpus Resources to Sublanguages and
Applications Workshop, held jointly with 1st
LREC Granada, Spain.
Bontcheva, K. 2001. Generating adaptive
hypertext explainations with a nested agent
model.  Ph. D. Thesis, University of Sheffield.
Bontcheva, K. and Wilks, Y. 2001. Dealing
with Dependencies between Content Planning
and Surface Realisation in a Pipeline
Generation Architecture. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Brown, P.F., Peter F., V. J. Della Pietra, P.
V. DeSouza, J. C. Lai, and R. L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18, 467-479.
 Califf, M. E. 1998. Relational Learning
Techniques for Natural Language Information
Extraction. Ph.D. thesis, Univ. Texas, Austin,
www/cs/utexas.edu/users/mecaliff
C. Cardie, `Empirical methods in
information extraction', AI Journal,18(4), 65-
79, (1997).
F. Ciravegna, A. Lavelli, N. Mana, J.
Matiasek, L. Gilardoni, S. Mazza, M. Ferraro,
W. J. Black F. Rinaldi, and D. Mowatt.
FACILE: Classifying Texts Integrating Pattern
Matching and Information Extraction. In
Proceedings of the 16th International Joint
Conference On Artificial Intelligence
(IJCAI99), Stockholm, Sweden, 1999.
F. Ciravegna, A. Lavelli,, L. Gilardoni, S.
Mazza, W. J. Black, M. Ferraro, N. Mana, J.
Matiasek, F. Rinaldi. Flexible Text
Classification for Financial
Applications: The FACILE System. In
Proceedings of Prestigious Applications sub-
conference (PAIS2000) sub-conference of the
14th European Conference On Artificial
Intelligence (ECAI2000), Berlin, Germany,
August, 2000.
 Ciravegna, F. 2001. Adaptive Information
Extraction from Text by Rule Induction and
Generalisation. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Ciravegna, F. and D. Petrelli. 2001. User
Involvement in customizing Adaptive
Information Extraction from Texts: Position
Paper. Proceedings of the IJCAI01 Workshop on
Adaptive Text Extraction and Mining, Seattle.
Cunningham, H., K. Humphreys, R.
Gaizauskas and Y. Wilks. 1997. Software
Infrastructure for Natural Language Processing.
Proceedings of the Fifth Conference on Applied
Natural Language Processing (ANLP-97).
Cunningham H., K. Bontcheva, V. Tablan
and Y. Wilks. 2000. Software Infrastructure for
Language Resources: a Taxonomy of Previous
Work and a Requirements Analysis.
Proceedings of the Second Conference on
Language Resources Evaluation, Athens.
Dom, B. 1999. Automatically finding the
best pages on the World Wide Web (CLEVER).
Search Engines and Beyond: Developing
efficient knowledge management systems.
Boston, MA.
Ericsson, K. A. and H. A. Simon. 1984.
Protocol Analysis: verbal reports as data. MIT
Press, Cambridge, Mass.
Freitag, D. and A. McCallum. 1999
Information Extraction with HMMs and
Shrinkage. AAAI-99 Workshop on Machine
Learning for Information Extraction, Orlando,
FL. (www.isi.edu/~muslea/RISE/ML4IE/)
Freitag, D. and N. Kushmerick. 2000.
Boosted wrapper induction. F. Ciravegna, R.
Basili, R. Gaizauskas, ECAI2000 Workshop on
Machine Learning for Information Extraction,
Berlin, 2000, (www.dcs.shef.ac.uk/~fabio/ecai-
workshop.html)
Hays, P. R. 1997. Collocational Similarity:
Emergent Patterns in Lexical Environments,
PhD. Thesis. School of English, University of
Birmingham
Humphreys, K., R. Gaizauskas, S. Azzam, C.
Huyck, B. Mitchell, H. Cunningham and  Y.
Wilks. 1998. Description of the University of
Sheffield LaSIE-II System as used for MUC-7.
Proceedings of the 7th Message Understanding
Conference.
J?rvelin, K.  and J. Kek?l?inen. 2000. IR
evaluation methods for retrieving highly
relevant documents. Proceedings of the 23rd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, , Athens.
Kushmerick, N., D. Weld, and R.
Doorenbos. 1997. Wrapper induction for
information extraction. Proceedings of 15th
International Conference on Artificial
Intelligence, IJCAI-97.
Manchester, P. 1999. Survey ? Knowledge
Management. Financial Times, 28.04.99.
Maybury, M.. 2001. Human Language
Technologies for Knowledge Management:
Challenges and Opportunities. Workshop on
Human Language Technology and Knowledge
Management. Toulouse, France.
McMahon, J. G. and  F. J. Smith. 1996
Improving Statistical Language Models
Performance with Automatically  Generated
Word Hierarchies. Computational Linguistics,
22(2), 217-247, ACL/MIT.
Morin, E. 1999. Using Lexico-Syntactic
patterns to Extract Semantic Relations between
Terms from Technical Corpus, TKE 99,
Innsbruck, Austria.
Muslea, I., S. Minton, and C. Knoblock.
1998. Wrapper induction for semi-structured,
web-based information sources. Proceedings of
the Conference on Autonomous Learning and
Discovery CONALD-98.
Paris, C. , K. Vander Linden. 1996.
DRAFTER: An interactive support tool for
writing multilingual instructions, IEEE
Computer, Special Issue on Interactive NLP.
Reiter, E. 1995. NLG vs. Templates.
Proceedings of the 5th European workshop on
natural language generation, (ENLG-95),
Leiden.
Reiter, E. , C. Mellish and J. Levine. 1995
Automatic generation of technical
documentation. Journal of Applied Artificial
Intelligence,  9(3) 259-287, 1995
Sanderson, M.  and B. Croft. 1999. Deriving
concept hierarchies from text. Proceedings of
the 22nd ACM SIGIR Conference, 206-213.
Scott, M. 1998. Focusing on the Text and Its
Key Words. TALC 98 Proceedings, Oxford,
Humanities Computing Unit, Oxford University.
Soderland, S. 1999. Learning information
extraction rules for semi-structured and free
text. Machine Learning, (1), 1-44.
Yangarber, R., R. Grishman, P. Tapanainen
and S. Huttunen. 2000. Automatic Acquisition
of Domain Knowledge for Information
Extraction. Proceedings of COLING 2000: The
18th International Conference on
Computational Linguistics, Saarbr?cken.
The Automatic Generation of Formal Annotations in a Multimedia
Indexing and Searching Environment
Thierry Declerck
DFKI GmbH
Stuhlsatzenhausweg 3
D-66123 Saarbruecken
Germany
declerck@dfki.de
Peter Wittenburg
MPI for Psycholinguistics
Wundtlaan 1, PB 310
NL-6500 AH Nijmegen
The Netherlands
Peter.Wittenburg@mpi.nl
Hamish Cunningham
Dept. of Computer Science
University of Sheffield
Regent Court, 211 Portobello
GB-Sheffield S1 4DP
Great Britain
hamish@dcs.shef.ac.uk
Abstract
We describe in this paper the MU-
MIS Project (Multimedia Indexing and
Searching Environment)1 , which is
concerned with the development and in-
tegration of base technologies, demon-
strated within a laboratory prototype, to
support automated multimedia index-
ing and to facilitate search and retrieval
from multimedia databases. We stress
the role linguistically motivated annota-
tions, coupled with domain-specific in-
formation, can play within this environ-
ment. The project will demonstrate that
innovative technology components can
operate on multilingual, multisource,
and multimedia information and create
a meaningful and queryable database.
1 Introduction
MUMIS develops and integrates basic technolo-
gies, which will be demonstrated within a labora-
tory prototype, for the automatic indexing of mul-
timedia programme material. Various technology
components operating offline will generate for-
mal annotations of events in the data material pro-
cessed. These formal annotations will form the
basis for the integral online part of the MUMIS
project, consisting of a user interface allowing the
querying of videos. The indexing of the video ma-
terial with relevant events will be done along the
1MUMIS is an on-going EU-funded project within the
Information Society Program (IST) of the European Union,
section Human Language Technology (HLT). See for more
information http://parlevink.cs.utwente.nl/projects/mumis/.
line of time codes extracted from the various doc-
uments.
For this purpose the project makes use of data
from different media sources (textual documents,
radio and television broadcasts) in different lan-
guages (Dutch, English and German) to build a
specialized set of lexicons and an ontology for
the selected domain (soccer). It also digitizes
non-text data and applies speech recognition tech-
niques to extract text for the purpose of annota-
tion.
The core linguistic processing for the anno-
tation of the multimedia material consists of
advanced information extraction techniques for
identifying, collecting and normalizing signifi-
cant text elements (such as the names of players
in a team, goals scored, time points or sequences
etc.) which are critical for the appropriate anno-
tation of the multimedia material in the case of
soccer.
Due to the fact that the project is accessing and
processing distinct media in distinct languages,
there is a need for a novel type of merging tool in
order to combine the semantically related annota-
tions generated from those different data sources,
and to detect inconsistencies and/or redundancies
within the combined annotations. The merged an-
notations will be stored in a database, where they
will be combined with relevant metadata.2
Finally the project will develop a user interface
to enable professional users to query the database,
by selecting from menus based on structured an-
2We see in this process of merging extracted informa-
tions and their combination with metadata a fruitful base for
the identification and classification of content or knowledge
from distinct types of documents.
notations and metadata, and to view video frag-
ments retrieved to satisfy the query, offering thus
a tool to formulate queries about multimedia pro-
grammes and directly get interactive access to the
multimedia contents. This tool constitutes the on-
line component of the MUMIS environment.
2 State of the Art
MUMIS differs in many significant ways from ex-
isting technologies and already achieved or ad-
vanced projects3 . Most closely related to the the-
matic focus of MUMIS are the HLT projects Pop-
Eye [POP] and OLIVE [OLI]. Pop-Eye used sub-
titles to index video streams and offered time-
stamped texts to satisfy a user query, on request
displaying a storyboard or video fragment corre-
sponding to the text hit. OLIVE used automatic
speech recognition to generate transcriptions of
the sound tracks of news reports, which were then
indexed and used in ways similar to the Pop-Eye
project; both projects used fuzzy matching IR al-
gorithms to search and retrieve text, offering lim-
ited multilingual access to texts. Instead of using
IR methods to index and search the transcriptions,
MUMIS will create formal annotations to the in-
formation, and will fuse information annotations
from different media sources. The fusion result
is then used to direct retrieval, through interface
techniques such as pop-up menus, keyword lists,
and so on. Search takes the user direct to the sto-
ryboard and video clippings.
The Informedia project at Carnegie-Mellon-
University [INF] has a similar conceptual base-
line to MUMIS. The innovative contribution of
MUMIS is that it uses a variety of multilingual
information sources and fuses them on the ba-
sis of formal domain-specific annotations. Where
Informedia primarily focuses on special applica-
tions, MUMIS aims at the advancement and in-
tegratibility of HLT-enhanced modules to enable
information filtering beyond the textual domain.
Therefore, MUMIS can be seen as complemen-
tary to Informedia with extensions typical for Eu-
rope.
The THISL project [THI] is about spoken doc-
ument retrieval, i.e., automatic speech recognition
3We are aware of more related on-going projects, at least
within the IST program, but we can not compare those to
MUMIS now, since we still lack first reports.
is used to auto-transcribe news reports and then
information retrieval is carried out on this infor-
mation. One main focus of THISL is to improve
speech recognition. Compared to MUMIS it lacks
the strong language processing aspects, the fusion
of multilingual sources, and the multimedia deliv-
ery.
Columbia university is running a project [COL]
to use textual annotations of video streams to in-
dicate moments of interest, in order to limit the
scope of the video processing task which requires
extreme CPU capacities. So the focus is on find-
ing strategies to limit video processing. The Uni-
versity of Massachusetts (Amherst) is also run-
ning projects about video indexing [UMA], but
these focus on the combination of text and im-
ages. Associated text is used to facilitate indexing
of video content. Both projects are funded under
the NSF Stimulate programme [NSF].
Much work has been done on video and im-
age processing (Virage [VIR], the EUROMEDIA
project [EUR], Surfimage [SUR], the ISIS project
[ISI], IBM's Media Miner, projects funded under
the NSF Stimulate program [NSF], and many oth-
ers). Although this technology in general is in its
infancy, there is reliable technology to indicate,
for example, scene changes using very low-level
cues and to extract key frames at those instances
to form a storyboard for easy video access. Some
institutions are running projects to detect subtitles
in the video scene and create a textual annotation.
This task is very difficult, given a sequence of real
scenes with moving backgrounds and so on. Even
more ambitious tasks such as finding real patterns
in real movies (tracing the course of the ball in a
soccer match, for example) are still far from being
achieved.4
3 Formal Annotations for the Soccer
Domain
Soccer has been chosen as the domain to test and
apply the algorithms to be developed. There are a
number of reasons for this choice: availability of
people willing to help in analyzing user require-
ments, existence of many information sources in
4The URLs of the projects mentionned above are given
in the bibliography at the end of this paper.
several languages5 , and great economic and pub-
lic interest. The prototype will also be tested by
TV professionals and sport journalists, who will
report on its practicability for the creation and
management of their programme and information
material.
The principles and methods derived from this
domain can be applied to other as well. This has
been shown already in the context of text-based
Information Extraction (IE), for which method-
ologies for a fast adaptation to new domains
have been developed (see the MUC conferences
and (Neumann et al, 2000)). And generally
speaking the use of IE for automatic annotation
of multimedia document has the advantage of
providing, besides the results of the (shallow)
syntactic processing, accurate semantic (or con-
tent/conceptual) information (and thus potential
annotation) for specific predefined domains, since
a mapping from the linguistically analyzed rele-
vant text parts can be mapped onto an unambigu-
ous conceptual description6 . Thus in a sense it
can be assumed that IE is supporting the word
sense disambiguation task.
It is also commonly assumed (see among oth-
ers (Cunningham, 1999)) that IE occupies an in-
termediate place between Information Retrieval
(with few linguistic knowledge involved) and
Text Understanding (involving the full deep lin-
guistic analysis and being still not realized for the
time being.). IE being robust but offering only a
partial (but mostly accurate) syntactic and content
analysis, it can be said that this language technol-
ogy is actually filling the gap between available
low-level annotated/indexed documents and cor-
pora and the desirable full content annotation of
those documents and corpora. This is the reason
why MUMIS has chosen this technology for pro-
viding automatic annotation (at distinct linguistic
and domain-specific levels) of multimedia mate-
rial, allowing thus to add queryable ?content in-
formation? to this material.7
5We would like to thank at this place the various institu-
tions making available various textual, audio and video data.
6This topic has already been object of a workshop dis-
cussing the relations between IE and Corpus Linguistics
(McNaught, 2000).
7MUMIS was not explicitly designed for supporting
knowledge management tasks, but we assume that the mean-
ingful organization of domain-specific multimedia material
proposed by the project can be adapted to the organization of
4 The Multimedia Material in MUMIS
The MUMIS project is about automatic index-
ing of videos of soccer matches with formal an-
notations and querying that information to get
immediate access to interesting video fragments.
For this purpose the project chose the European
Football Championships 2000 in Belgium and the
Netherlands as its main database. A major project
goal is to merge the formal annotations extracted
from textual and audio material (including the au-
dio part of videos) on the EURO 2000 in three
languages: English, German, Dutch. The mate-
rial MUMIS has to process can be classified in
the following way:
1. Reports from Newspapers (reports about
specific games, general reports) which is
classified as free texts (FrT)
2. Tickers, close captions, Action-Databases
which are classified as semi-formal texts
(SFT)
3. Formal descriptions about specific games
which are classified as formal texts (FoT)
4. Audio material recorded from radio and TV
broadcasts
5. Video material recorded from TV broadcasts
1-4 will be used for automatically generating
formal annotations in order to index 5. MUMIS
is investigating the precise contribution of each
source of information for the overall goal of the
project.
Since the information contained in formal texts
can be considered as a database of true facts, they
play an important role within MUMIS. But never-
theless they contain only few information about a
game: the goals, the substitutions and some other
few events (penalties, yellow and red cards). So
there are only few time points available for in-
dexing videos. Semi-formal texts (SFT), like live
tickers on the web, are offering much more time
points sequences, related with a higher diversity
the distributed information of an enterprise and thus support
the sharing and access to companies expertise and know-
how.
of events (goals scenes, fouls etc,) and seem to of-
fer the best textual source for our purposes. Nev-
ertheless the quality of the texts of online tick-
ers is often quite poor. Free texts, like newspa-
pers articles, have a high quality but the extrac-
tion of time points and their associated events in
text is more difficult. Those texts also offer more
background information which might be interest-
ing for the users (age of the players, the clubs they
are normally playing for, etc.). Figures 1 and 2 in
section 8 show examples of (German) formal and
semi-formal texts on one and the same game.
5 Processing Steps in MUMIS
5.1 Media Pre-Processing
Media material has been delivered in various
formats (AudioDAT, AudioCassettes, Hi-8 video
cassettes, DV video cassettes etc) and qualities.
All audio signals (also those which are part of
the video recordings) are digitized and stored in
an audio archive. Audio digitization is done with
20 kHz sample frequency, the format generated is
according to the de-facto wav standard. For dig-
itization any available tool can be used such as
SoundForge.
Video information (including the audio compo-
nent) of selected games have been digitized into
MPEG1 streams first. Later it will be encoded in
MPEG2 streams. While the quality of MPEG1 is
certainly not satisfying to the end-user, its band-
width and CPU requirements are moderate for
current computer and network technology. The
mean bit rate for MPEG1 streams is about 1.5
Mbps. Current state-of-the-art computers can ren-
der MPEG1 streams in real time and many net-
work connections (Intranet and even Internet) can
support MPEG1. MPEG2 is specified for about 3
to 5 Mbps. Currently the top-end personal com-
puters can render MPEG2, but MPEG2 is not yet
supported for the most relevant player APIs such
as JavaMediaFramework or Quicktime. When
this support is given the MUMIS project will also
offer MPEG2 quality.
For all separate audio recordings as for ex-
ample from radio stations it has to be checked
whether the time base is synchronous to that one
of the corresponding video recordings. In case of
larger deviations a time base correction factor has
to be estimated and stored for later use. Given that
the annotations cannot be created with too high
accuracy a certain time base deviation will be ac-
cepted. For part of the audio signals manual tran-
scriptions have to be generated to train the speech
recognizers. These transcripts will be delivered in
XML-structured files.
Since keyframes will be needed in the user in-
terface, the MUMIS project will develop software
that easily can generate such keyframes around a
set of pre-defined time marks. Time marks will
be the result of information extraction processes,
since the corresponding formal annotations is re-
ferring to to specific moments in time. The soft-
ware to be written has to extract the set of time
marks from the XML-structured formal annota-
tion file and extract a set of keyframes from the
MPEG streams around those time marks. A set of
keyframes will be extracted around the indicated
moments in time, since the estimated times will
not be exact and since the video scenes at such
decisive moments are changing rapidly. There
is a chance to miss the interesting scene by us-
ing keyframes and just see for example specta-
tors. Taking a number of keyframes increases the
chance to grab meaningful frames.
5.2 Multilingual Automatic Speech
Recognition
Domain specific language models will be trained.
The training can be bootstrapped from written re-
ports of soccer matches, but substantial amounts
of transcribed recordings of commentaries on
matches are also required. Novel techniques
will be developed to interpolate the base-line lan-
guage models of the Automatic Speech Recogni-
tion (ASR) systems and the domain specific mod-
els. Moreover, techniques must be developed to
adapt the vocabularies and the language models
to reflect the specific conditions of a match (e.g.,
the names players have to be added to the vocabu-
lary, with the proper bias in the language model).
In addition, the acoustic models must be adapted
to cope with the background noise present in most
recordings.
Automatic speech recognition of the sound
tracks of television and (especially) radio pro-
grammes will make use of closed caption subtitle
texts and information extracted from formal texts
to help in finding interesting sequences and auto-
matically transcribing them. Further, the domain
lexicons will help with keyword and topic spot-
ting. Around such text islands ASR will be used
to transcribe the spoken soundtrack. The ASR
system will then be enriched with lexica contain-
ing more keywords, to increase the number of se-
quence types that can be identified and automati-
cally transcribed.
5.3 Multilingual Domain Lexicon Building
All the collected textual data for the soccer do-
main are used for building the multilingual do-
main lexicons. This data can be in XML, HTML,
plain text format, etc. A number of automatic
processes are used for the lexicon building, first
on a monolingual and secondly on a multilin-
gual level. Manual browsing and editing is tak-
ing place, mainly in order to provide the semantic
links to the terms, but also for the fine-tuning of
the lexicon according to the domain knowledge.
Domain lexicons are built for four lan-
guages, namely English, German, Dutch and
Swedish. The lexicons will be delivered in a
fully structured, XML-compliant, TMX-format
(Translation Memory eXchange format). For
more information about the TMX format see
http://www.lisa.org/tmx/tmx.htm.
We will also investigate how far
EUROWORDNET resources (see
http://www.hum.uva.nl/ ewn/) can be of use
for the organization of the domain-specific
terminology.
5.4 Building of Domain Ontology and Event
Table
The project is currently building an ontology for
the soccer domain, taking into consideration the
requirements of the information extraction and
merging components, as well as users require-
ments. The ontology will be delivered in an XML
format8.
8There are still on-going discussions within the
project consortium wrt the best possible encoding for-
mat for the domain ontology, the alternative being
reduced probably to RDFS, OIL and IFF, see respec-
tively, and among others, http://www.w3.org/TR/rdf-
schema/, http://www.oasis-open.org/cover/oil.html and
http://www.ontologos.org/IFF/The%20IFF%20Language.
html
In parallel to building the ontology an event ta-
ble is being described. It contains the major event
types that can occur in soccer games and their
attributes. This content of the table is matching
with the content of the ontology. The event ta-
ble is a flat structure and guides the information
extraction processes to generate the formal event
annotations. The formal event annotations build
the basis for answering user queries. The event
table is specified as an XML schema to constrain
the possibilities of annotation to what has been
agreed within the project consortium.
5.5 Generation of Formal Annotations
The formal annotations are generated by the IE
technology and are reflecting the typical output of
IE systems, i.e.instantiated domain-specific tem-
plates or event tables. The slots to be filled by
the systems are basically entities (player, teams
etc.), relations (player of, opponents etc.) and
events (goal, substitution etc.), which are all de-
rived from the current version of the domain on-
tology and can be queried for in the online com-
ponent of the MUMIS prototype. All the tem-
plates associated with an event are including a
time slot to be filled if the corresponding informa-
tion is available in a least one of the sources con-
sulted during the IE procedure. This time infor-
mation is necessary for the indexing of the video
material.
The IE systems are applying to distinct sources
(FoT, FrT etc.) but they are not concerned with
achieving consistency in the IE result on distinct
sources about the same event (game): this is the
task of the merging tools, described below.
Since the distinct textual sources are differ-
ently structured, from ?formal? to ?free? texts, the
IE systems involved have adopted a modular ap-
proach: regular expressions for the detection of
Named Entities in the case of formal texts, full
shallow parsing for the free texts. On the base of
the factual information extracted from the formal
texts, the IE systems are also building dynamic
databases on certain entities (like name and age
of the players, the clubs they are normally playing
for, etc.) or certain metadata (final score), which
can be used at the next level of processing.
5.6 The Merging Tool
The distinct formal annotations generated are
passed to a merging component, which is respon-
sible for avoiding both inconsistencies and redun-
dancies in the annotations generated on one event
(in our case a soccer game).
In a sense one can consider this merging
component as an extension of the so-called co-
reference task of IE systems to a cross-document
(and cross-lingual) reference resolution task. The
database generated during the IE process will help
here for operating reference resolution for more
?verbose? types of texts, which in the context
of soccer are quite ?poetic? with respect to the
naming of agents (the ?Kaiser? for Beckenbauer,
the ?Bomber? for Mueller etc...), which would
be quite difficult to achieve within the sole refer-
ential information available within the boundary
of one document. The project will also investi-
gate here the use of inferential mechanisms for
supporting reference resolution. So for example,
?knowing? from the formal texts the final score
of a game and the names of the scorers, follow-
ing formulation can be resolved form this kind
of formulation in a free text (in any language):
?With his decisive goal, the ?Bomber? gave the
victory to his team.?, whereas the special nam-
ing ?Bomber? can be further added to the entry
?Mueller?
The merging tools used in MUMIS will also
take into consideration some general representa-
tion of the domain-knowledge in order to filter out
some annotations generated in the former phases.
The use of general representations9 (like domain
frames), combined with inference mechanisms,
might also support a better sequential organiza-
tion of some event templates in larger scenarios.
It will also allow to induce some events which
are not explicitly mentioned in the sources under
consideration (or which the IE systems might not
have detected).
5.7 User Interface Building
The user first will interact with a web-portal to
start a MUMIS query session. An applet will be
9Like for example the Type Description Language
(TDL), a formalism supporting all kind of operations on
(typed) features as well as multiple inheritance, see (Krieger
and Schaefer, 1994).
down-line loaded in case of showing the MUMIS
demonstration. This applet mainly offers a query
interface. The user then will enter a query that
either refers to metadata, formal annotations, or
both. The MUMIS on-line system will search
for all formal annotations that meet the criteria
of the query. In doing so it will find the appro-
priate meta-information and/or moments in some
media recording. In case of meta-information it
will simply offer the information in scrollable text
widgets. This will be done in a structured way
such that different type of information can eas-
ily be detected by the user. In case that scenes of
games are the result of queries about formal anno-
tations the user interface will first present selected
video keyframes as thumbnails with a direct indi-
cation of the corresponding metadata.
The user can then ask for more metadata
about the corresponding game or for more media
data. It has still to be decided within the project
whether several layers of media data zooming in
and out are useful to satisfy the user or whether
the step directly to the corresponding video frag-
ment is offered. All can be invoked by simple
user interactions such as clicking on the presented
screen object. Playing the media means playing
the video and corresponding audio fragment in
streaming mode requested from a media server.
6 Standards for Multimedia Content
MUMIS is looking for a compliance with exist-
ing standards in the context of the processing of
multimedia content on the computer and so will
adhere to emerging standards such as MPEG4,
which defines how different media objects will be
decoded and integrated at the receiving station,
and MPEG7, which is about defining standards
for annotations which can be seen as multime-
dia objects. Further, MUMIS will also maintain
awareness of international discussions and devel-
opments in the aerea of multimedia streaming
(RTP, RTSP, JMF...), and will follow the discus-
sions within the W3C consortium and the EBU
which are also about standardizing descriptions of
media content.
7 Role of MUMIS for the Annotation of
Multimedia Content
To conclude, we would like to list the points
where we think MUMIS will, directly or indi-
rectly, contribute to extract and access multimedia
content:
  uses multimedia (MM) and multilingual in-
formation sources;
  carries out multimedia indexing by applying
information extraction to a well-delineated
domain and using already existing informa-
tion as constraints;
  uses and extends advanced language tech-
nology to automatically create formal anno-
tations for MM content;
  merges information from many sources
to improve the quality of the annotation
database;
  application of IE to the output of ASR and
the combination of this with already existing
knowledge;
  definition of a complex information annota-
tion structure, which is stored in a standard
document type definition (DTD);
  integration of new methods into a query in-
terface which is guided by domain knowl-
edge (ontology and multilingual lexica).
So in a sense MUMIS is contributing in defin-
ing semantic structures of multimedia contents,
at the level proposed by domain-specific IE anal-
ysis. The full machinery of IE, combined with
ASR (and in the future with Image Analysis)
can be used for multimedia contents development
and so efficiently support cross-media (and cross-
lingual) information retrieval and effective navi-
gation within multimedia information interfaces.
There seems thus that this technolgy can play a
highly relevant role for the purposes of knowl-
edge detection and management. This is prob-
ably specially valid for the merging component,
which is eliminating redundancies in the annota-
tions generated from sets of documents and estab-
lishing complex reference resolutions, thus sim-
plyfying the access to content (and knowledge)
distributed over multiple documents and media.
References
Doug E. Appelt. 1999. An introduction to information
extraction. AI Communications, 12.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech Com-
munication.
K. Bontcheva, H. Brugman, A. Russel, P. Wittenburg,
and H. Cunningham. 2000. An Experiment in
Unifying Audio-Visual and Textual Infrastructures
for Language Processing R&D. In Proceedings of
the Workshop on Using Toolsets and Architectures
To Build NLP Systems at COLING-2000, Luxem-
bourg. http://gate.ac.uk/.
Daan Broeder, Hamish Cunningham, Nancy Ide,
David Roy, Henry Thompson, and Peter Witten-
burg, editors. 2000. Meta-Descriptions and An-
notation Schemes for Multimodal/Multimedia Lan-
gauge Resources LREC-2000.
H. Brugman, K. Bontcheva, P. Wittenburg, and
H. Cunningham. 1999. Integrating Multimedia and
Textual Software Architectures for Language Tech-
nology. Technical report mpi-tg-99-1, Max-Planck
Institute for Psycholinguistics, Nijmegen, Nethed-
lands.
Hamish Cunningham. 1999. An introduction to infor-
mation extraction. Research memo CS - 99 - 07.
Thierry Declerck and G. Neumann. 2000. Using a pa-
rameterisable and domain-adaptive information ex-
traction system for annotating large-scale corpora?
In Proceedings of the Workshop Information Ex-
traction meets Corpus Linguistics, LREC-2000.
Kevin Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, H. Cunningham, and
Y. Wilks. 1998. University of sheffield:
Description of the lasie-ii system as used for
muc-7. In SAIC, editor, Proceedings of the
7th Message Understanding Conference, MUC-7,
http://www.muc.saic.com/. SAIC Information Ex-
traction.
Christopher Kennedy and B. Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora res-
olution without a parser. In Proceedings of the
16th International Conference on Computational
Linguistics, COLING-96, pages 113?118, Copen-
hagen.
Hans-Ulrich Krieger and U. Schaefer. 1994.

?
a type description language for constraint-based
grammars. In Proceedings of the 15th Interna-
tional Conference on Computational Linguistics,
COLING-94, pages 893?899.
Shalom Lappin and H-H. Shih. 1996. A generalized
algorithm for ellipsis resolution. In Proceedings
of the 16th International Conference on Compu-
tational Linguistics, COLING-96, pages 687?692,
Copenhagen.
John McNaught, editor. 2000. Information Extraction
meets Corpus Linguistics, LREC-2000.
Ruslan Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proceedings of the 17th In-
ternational Conference on Computational Linguis-
tics, COLING-98, pages 869?875, Montreal.
MUC, editor. 1995. Sixth Message Understanding
Conference (MUC-6). Morgan Kaufmann.
MUC, editor. 1998. Seventh Message Understanding
Conference (MUC-7), http://www.muc.saic.com/.
SAIC Information Extraction.
Guenter Neumann, R. Backofen, J. Baur, M. Becker,
and C. Braun. 1997. An information extrac-
tion core system for real world german text pro-
cessing. In Proceedings of the 5th Conference on
Applied Natural Language Processing, ANLP-97,
pages 209?216.
Guenter Neumann, C. Braun, and J. Piskorski. 2000.
A divide-and-conquer strategy for shallow parsing
of german free texts. In Proceedings of the 6th Con-
ference on Applied Natural Language Processing,
ANLP-00.
Jakub Piskorski and G. Neumann. 2000. An intel-
ligent text extraction and navigation system. In
Proceedings of the 6th Conference on Recherche
d'Information Assiste?e par Ordinateur, RIAO-2000.
Project URLs:
COL:  
			ffUsing GATE as an Environment for Teaching NLP
Kalina Bontcheva, Hamish Cunningham, Valentin Tablan, Diana Maynard, Oana Hamza
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{kalina,hamish,valyt,diana,oana}@dcs.shef.ac.uk
Abstract
In this paper we argue that the GATE
architecture and visual development
environment can be used as an effec-
tive tool for teaching language engi-
neering and computational linguistics.
Since GATE comes with a customis-
able and extendable set of components,
it allows students to get hands-on ex-
perience with building NLP applica-
tions. GATE also has tools for cor-
pus annotation and performance eval-
uation, so students can go through the
entire application development process
within its graphical development en-
vironment. Finally, it offers com-
prehensive Unicode-compliant multi-
lingual support, thus allowing stu-
dents to create components for lan-
guages other than English. Unlike
other NLP teaching tools which were
designed specifically and only for this
purpose, GATE is a system developed
for and used actively in language en-
gineering research. This unique dual-
ity allows students to contribute to re-
search projects and gain skills in em-
bedding HLT in practical applications.
1 Introduction
When students learn programming, they have
the benefit of integrated development environ-
ments, which support them throughout the en-
tire application development process: from writ-
ing the code, through testing, to documenta-
tion. In addition, these environments offer sup-
port and automation of common tasks, e.g., user
interfaces can be designed easily by assembling
them visually from components like menus and
windows. Similarly, NLP and CL students can
benefit from the existence of a graphical devel-
opment environment, which allows them to get
hands-on experience in every aspect of develop-
ing and evaluating language processing modules.
In addition, such a tool would enable students to
see clearly the practical relevance and need for
language processing, by allowing them to exper-
iment easily with building NLP-powered (Web)
applications.
This paper shows how an existing infrastruc-
ture for language engineering research ? GATE
(Cunningham et al, 2002a; Cunningham, 2002)
? has been used successfully as an NLP teach-
ing environment, in addition to being a suc-
cessful vehicle for building NLP applications
and reusable components (Maynard et al, 2002;
Maynard et al, 2001). The key features of
GATE which make it particularly suitable for
teaching are:
? The system is designed to separate cleanly
low-level tasks such as data storage, data
visualisation, location and loading of com-
ponents and execution of processes from the
data structures and algorithms that actu-
ally process human language. In this way,
the students can concentrate on studying
and/or modifying the NLP data and algo-
rithms, while leaving the mundane tasks to
GATE.
                     July 2002, pp. 54-62.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
? Automating measurement of performance
of language processing components and fa-
cilities for the creation of the annotated cor-
pora needed for that.
? Providing a baseline set of language pro-
cessing components that can be extended
and/or replaced by students as required.
These modules typically separate clearly
the linguistic data from the algorithms that
use it, thus allowing teachers to present
them separately and the students to adapt
the modules to new domains/languages by
just modifying the linguistic data.
? It comes with exhaustive documenta-
tion, tutorials, and online movie demon-
strations, available on its Web site
(http://gate.ac.uk).
GATE and its language processing modules
were developed to promote robustness and scala-
bility of NLP approaches and applications, with
an emphasis on language engineering research.
Therefore, NLP/LE courses based on GATE
offer students the opportunity to learn from
non-toy applications, running on big, realistic
datasets (e.g., British National corpus or news
collected by a Web crawler). This unique re-
search/teaching duality also allows students to
contribute to research projects and gain skills in
embedding HLT in practical applications.
2 GATE from a Teaching
Perspective
GATE (Cunningham et al, 2002a) is an archi-
tecture, a framework and a development envi-
ronment for human language technology mod-
ules and applications. It comes with a set of
reusable modules, which are able to perform ba-
sic language processing tasks such as POS tag-
ging and semantic tagging. These eliminate the
need for students to re-implement useful algo-
rithms and modules, which are pre-requisites
for completing their assignments. For exam-
ple, Marin Dimitrov from Sofia University suc-
cessfully completed his masters? degree by im-
plementing a lightweight approach to pronom-
inal coreference resolution for named entities1,
which uses GATE?s reusable modules for the
earlier processing and builds upon their results
(see Section 4).
For courses where the emphasis is more on
linguistic annotation and corpus work, GATE
can be used as a corpus annotation environment
(see http://gate.ac.uk/talks/tutorial3/). The
annotation can be done completely manually
or it can be bootstrapped by running some
of GATE?s processing resources over the cor-
pus and then correcting/adding new annota-
tions manually. These facilities can also be used
in courses and assignments where the students
need to learn how to create data for quantitative
evaluation of NLP systems.
If evaluated against the requirements for
teaching environments discussed in (Loper and
Bird, 2002), GATE covers them all quite well.
The graphical development environment and
the JAPE language facilitate otherwise difficult
tasks. Inter-module consistency is achieved by
using the annotations model to hold language
data, while extensibility and modularity are the
very reason why GATE has been successfully
used in many research projects (Maynard et al,
2000). In addition, GATE also offers robustness
and scalability, which allow students to experi-
ment with big corpora, such as the British Na-
tional Corpus (approx. 4GB). In the following
subsections we will provide further detail about
these aspects of GATE.
2.1 GATE?s Graphical Development
Environment
GATE comes with a graphical development en-
vironment (or GATE GUI) that facilitates stu-
dents in inspecting the language processing re-
sults and debugging the modules. The envi-
ronment has facilities to view documents, cor-
pora, ontologies (including the popular Prote?ge?
editor (Noy et al, 2001)), and linguistic data
(expressed as annotations, see below), e.g., Fig-
ure 1 shows the document viewer with some
annotations highlighted. It also shows the re-
source panel on the left with all loaded appli-
1The thesis is available at
http://www.ontotext.com/ie/thesis-m.pdf
Figure 1: GATE?s visual development environment
cations, language resources, and processing re-
sources (i.e., modules). There are also view-
ers/editors for complex linguistic data like coref-
erence chains (Figure 2) and syntax trees (Fig-
ure 3). New graphical components can be in-
tegrated easily, thus allowing lecturers to cus-
tomise the environment as necessary. The
GATE team is also developing new visualisation
modules, especially a visual JAPE rule develop-
ment tool.
2.2 GATE API and Data Model
The central concept that needs to be learned by
the students before they start using GATE is
the annotation data model, which encodes all
linguistic data and is used as input and out-
put for all modules. GATE uses a single uni-
fied model of annotation - a modified form of
the TIPSTER format (Grishman, 1997) which
has been made largely compatible with the Atlas
format (Bird and Liberman, 1999). Annotations
are characterised by a type and a set of features
represented as attribute-value pairs. The anno-
tations are stored in structures called annotation
sets which constitute independent layers of an-
notation over the text content. The annotations
format is independent of any particular linguis-
tic formalism, in order to enable the use of mod-
ules based on different linguistic theories. This
generality enables the representation of a wide-
variety of linguistic information, ranging from
very simple (e.g., tokeniser results) to very com-
Figure 2: The coreference chains viewer
plex (e.g., parse trees and discourse representa-
tion: examples in (Saggion et al, 2002)). In
addition, the annotation format allows the rep-
resentation of incomplete linguistic structures,
e.g., partial-parsing results. GATE?s tree view-
ing component has been written especially to
be able to display such disconnected and incom-
plete trees.
GATE is implemented in Java, which makes
it easier for students to use it, because typi-
cally they are already familiar with this lan-
guage from their programming courses. The
GATE API (Application Programming Inter-
face) is fully documented in Javadoc and also
examples are given in the comprehensive User
Guide (Cunningham et al, 2002b). However,
students generally do not need to familiarise
themselves with Java and the API at all, be-
cause the majority of the modules are based on
GATE?s JAPE language, so customisation of ex-
isting and development of new modules only re-
quires knowledge of JAPE and the annotation
model described above.
JAPE is a version of CPSL (Common Pattern
Specification Language) (Appelt, 1996) and is
used to describe patterns to match and annota-
tions to be created as a result (for further de-
tails see (Cunningham et al, 2002b)). Once fa-
miliar with GATE?s data model, students would
not find it difficult to write the JAPE pattern-
based rules, because they are effectively regular
expressions, which is a concept familiar to most
Figure 3: The syntax tree viewer, showing a par-
tial syntax tree for a sentence from a telecom
news text
CS students.
An example rule from an existing named en-
tity recognition grammar is:
Rule: Company1
Priority: 25
(
({Token.orthography == upperInitial})+
{Lookup.kind == companyDesignator}
):companyMatch
-->
:companyMatch.NamedEntity =
{kind = "company", rule = "Company1"}
The rule matches a pattern consisting of any
kind of word, which starts with an upper-cased
letter (recognised by the tokeniser), followed by
one of the entries in the gazetteer list for com-
pany designators (words which typically indi-
cate companies, such as ?Ltd.? and ?GmBH?). It
then annotates this pattern with the entity type
?NamedEntity?, and gives it a feature ?kind?
with value company and another feature ?rule?
with value ?Company1?. The rule feature is
simply used for debugging purposes, so it is clear
which particular rule has fired to create the an-
notation.
The grammars (which are sets of rules) do not
need to be compiled by the students, because
they are automatically analysed and executed by
the JAPE Transducer module, which is a finite-
Figure 4: The visual evaluation tool
state transducer over the annotations in the doc-
ument. Since the grammars are stored in files in
a plain text format, they can be edited in any
text editor such as Notepad or Vi. The rule de-
velopment process is performed by the students
using GATE?s visual environment (see Figure 1)
to execute the grammars and visualise the re-
sults. The process is actually a cycle, where the
students write one or more rules, re-initialise the
transducer in the GATE GUI by right-clicking
on it, then run it on the test data, check the re-
sults, and go back to improving the rules. The
evaluation part of this cycle is performed using
GATE?s visual evaluation tools which also pro-
duce precision, recall, and f-measure automati-
cally (see Figure 4).
The advantage of using JAPE for the student
assignments is that once learned by the students,
it enables them to experiment with a variety
of NLP tasks from tokenisation and sentence
splitter, to chunking, to template-based infor-
mation extraction. Because it does not need to
be compiled and supports incremental develop-
ment, JAPE is ideal for rapid prototyping, so
students can experiment with alternative ideas.
Students who are doing bigger projects, e.g., a
final year project, might want to develop GATE
modules which are not based on the finite-state
machinery and JAPE. Or the assignment might
require the development of more complex gram-
mars in JAPE, in which case they might have
to use Java code on the right-hand side of the
rule. Since such GATE modules typically only
access and manipulate annotations, even then
the students would need to learn only that part
of GATE?s API (i.e., no more than 5 classes).
Our experience with two MSc students ? Partha
Lal and Marin Dimitrov ? has shown that they
do not have significant problems with using that
either.
2.3 Some useful modules
The tokeniser splits text into simple tokens,
such as numbers, punctuation, symbols, and
words of different types (e.g. with an initial capi-
tal, all upper case, etc.). The tokeniser does not
generally need to be modified for different ap-
plications or text types. It currently recognises
many types of words, whitespace patterns, num-
bers, symbols and punctuation and should han-
dle any language from the Indo-European group
without modifications. Since it is available as
open source, one student assignment could be
to modify its rules to cope with other languages
or specific problems in a given language. The to-
keniser is based on finite-state technology, so the
rules are independent from the algorithm that
executes them.
The sentence splitter is a cascade of finite-
state transducers which segments the text into
sentences. This module is required for the tag-
ger. Both the splitter and tagger are domain-
and application-independent. Again, the split-
ter grammars can be modified as part of a stu-
dent project, e.g., to deal with specifically for-
matted texts.
The tagger is a modified version of the Brill
tagger, which assigns a part-of-speech tag to
each word or symbol. To modify the tagger?s
behaviour, students will have to re-train it on
relevant annotated texts.
The gazetteer consists of lists such as cities,
organisations, days of the week, etc. It not only
consists of entities, but also of names of useful
indicators, such as typical company designators
(e.g. ?Ltd.?), titles, etc. The gazetteer lists are
compiled into finite state machines, which anno-
tate the occurrence of the list items in the given
document. Students can easily extend the exist-
ing lists and add new ones by double-clicking on
the Gazetteer processing resource, which brings
up the gazetteer editor if it has been installed,
or using GATE?s Unicode editor.
The JAPE transducer is the module that
runs JAPE grammars, which could be doing
tasks like chunking, named entity recognition,
etc. By default, GATE is supplied with an NE
transducer which performs named entity recog-
nition for English and a VP Chunker which
shows how chunking can be done using JAPE.
An even simpler (in terms of grammar rules
complexity) and somewhat incomplete NP chun-
ker can be obtained by request from the first
author.
The orthomatcher is a module, whose pri-
mary objective is to perform co-reference, or en-
tity tracking, by recognising relations between
entities, based on orthographically matching
their names. It also has a secondary role in im-
proving named entity recognition by assigning
annotations to previously unclassified names,
based on relations with existing entities.
2.4 Support for languages other than
English
GATE uses Unicode (Unicode Consortium,
1996) throughout, and has been tested on a va-
riety of Slavic, Germanic, Romance, and Indic
languages. The ability to handle Unicode data,
along with the separation between data and al-
gorithms, allows students to perform easily even
small-scale experiments with porting NLP com-
ponents to new languages. The graphical devel-
opment environment supports fully the creation,
editing, and visualisation of linguistic data, doc-
uments, and corpora in Unicode-supported lan-
guages (see (Tablan et al, 2002)). In order to
make it easier for foreign students to use the
GUI, we are planning to localise its menus, er-
ror messages, and buttons which currently are
only in English.
2.5 Installation and Programming
Languages Support
Since GATE is 100% Java, it can run on any
platform that has a Java support. To make it
easier to install and maintain, GATE comes with
installation wizards for all major platforms. It
also allows the creation and use of a site-wide
GATE configuration file, so settings need only
be specified once and all copies run by the stu-
dents will have the same configuration and mod-
ules available. In addition, GATE allows stu-
dents to have their own configuration settings,
e.g., specify modules which are available only
to them. The personal settings override those
from GATE?s default and site-wide configura-
tions. Students can also easily install GATE
on their home computers using the installation
program. GATE also allows applications to be
saved and moved between computers and plat-
forms, so students can easily work both at home
and in the lab and transfer their data and ap-
plications between the two.
GATE?s graphical environment comes config-
ured by default to save its own state on exit,
so students will automatically get their applica-
tions, modules, and data restored automatically
the next time they load GATE.
Although GATE is Java-based, modules writ-
ten in other languages can also be integrated
and used. For example, Prolog modules are eas-
ily executable using the Jasper Java-Prolog link-
ing library. Other programming languages can
be used if they support Java Native Interface
(JNI).
3 Existing Uses of GATE for
Teaching
Postgraduates in locations as diverse as Bul-
garia, Copenhagen and Surrey are using the
system in order to avoid having to write sim-
ple things like sentence splitters from scratch,
and to enable visualisation and management
of data. For example, Partha Lal at Impe-
rial College is developing a summarisation sys-
tem based on GATE and ANNIE as a final-
year project for an MEng Degree in Comput-
ing (http://www.doc.ic.ac.uk/? pl98/). His site
includes the URL of his components and once
given this URL, GATE loads his software over
the network. Another student project will be
discussed in more detail in Section 4.
Our colleagues in the Universities of Ed-
inburgh, UMIST in Manchester, and Sussex
(amongst others) have reported using previous
versions of the system for teaching, and the Uni-
versity of Stuttgart produced a tutorial in Ger-
man for the same purposes. Educational users of
early versions of GATE 2 include Exeter Univer-
sity, Imperial College, Stuttgart University, the
University of Edinburgh and others. In order to
facilitate the use of GATE as a teaching tool,
we have provided a number of tutorials, online
demonstrations, and exhaustive documentation
on GATE?s Web site (http://gate.ac.uk).
4 An Example MSc Project
The goal of this work was to develop a corefer-
ence resolution module to be integrated within
the named entity recognition system provided
with GATE. This required a number of tasks to
be performed by the student: (i) corpus anal-
ysis; (ii) implementation and integration; (iii)
testing and quantitative evaluation.
The student developed a lightweight approach
to resolving pronominal coreference for named
entities, which was implemented as a GATE
module and run after the existing NE modules
provided with the framework. This enabled him
also to use an existing annotated corpus from
an Information Extraction evaluation competi-
tion and the GATE evaluation tools to establish
how his module compared with results reported
in the literature. Finally, the testing process was
made simple, thanks to GATE?s visualisation fa-
cilities, which are already capable of displaying
coreference chains in documents.
GATE not only allowed the student to achieve
verifiable results quickly, but it also did not in-
cur substantial integration overheads, because
it comes with a bootstrap tool which automates
the creation of GATE-compliant NLP modules.
The steps that need to be followed are:2
? use the bootstrap tool to create an empty
Java module, then add the implementation
to it. A JAVA development environment
like JBuilder and VisualCafe can be used
for this and the next stages, if the students
are familiar with them;
? compile the class, and any others that it
uses, into a Java Archive (JAR) file (GATE
2For further details and an example see (Cunningham
et al, 2002b).
Figure 5: BootStrap Wizard Dialogue
generates automatically a Makefile too, to
facilitate this process);
? write some XML configuration data for the
new resource;
? tell GATE the URL of the new JAR and
XML files.
5 Example Topics
Since GATE has been used for a wide range of
tasks, it can be used for the teaching of a number
of topics. Topics that can be covered in (part of)
a course, based on GATE are:
? Language Processing, Language Engineer-
ing, and Computational Linguistics: differ-
ences, methodologies, problems.
? Architectures, portability, robustness, cor-
pora, and the Web.
? Corpora, annotation, and evaluation: tools
and methodologies.
? Basic modules: tokenisation, sentence split-
ting, gazetteer lookup.
? Part-of-speech tagging.
? Information Extraction: issues, tasks, rep-
resenting linguistic data in the TIPSTER
annotation format, MUC, results achieved.
? Named Entity Recognition.
? Coreference Resolution
? Template Elements and Relations
? Scenario Templates
? Parsing and chunking
? Document summarisation
? Ontologies and discourse interpretation
? Language generation
While language generation, parsing, summari-
sation, and discourse interpretation modules are
not currently distributed with GATE, they can
be obtained by contacting the authors. Modules
for text classification and learning algorithms in
general are to be developed in the near future.
A lecturer willing to contribute any such mod-
ules to GATE will be very welcome to do so and
will be offered integration support.
6 Example Assignments
The availability of example modules for a vari-
ety of NLP tasks allows students to use them
as a basis for the development of an entire NLP
application, consisting of separate modules built
during their course. For example, let us consider
two problems: recognising chemical formulae in
texts and making an IE system that extracts
information from dialogues. Both tasks require
students to make changes in a number of existing
components and also write some new grammars.
Some example assignments for the chemical
formulae recognition follow:
? tokeniser : while it will probably work well
for the dialogues, the first assignment would
be to make modifications to its regular ex-
pression grammar to tokenise formulae like
H4ClO2 and Al-Li-Ti in a more suitable
way.
? gazetteer : create new lists containing new
useful clues and types of data, e.g., all
chemical elements and their abbreviations.
? named entity recognition: write a new
grammar to be executed by a new JAPE
transducer module for the recognition of the
chemical formulae.
Some assignments for the dialogue application
are:
? sentence splitter : modify it so that it splits
correctly dialogue texts, by taking into ac-
count the speaker information (because dia-
logues often do not have punctuation). For
example:
A: Thank you, can I have your full name?
C: Err John Smith
A: Can you also confirm your postcode and
telephone number for security?
C: Erm it?s 111 111 11 11
A: Postcode?
C: AB11 1CD
? corpus annotation and evaluation: use the
default named entity recogniser to boot-
strap the manual annotation of the test
data for the dialogue application; evaluate
the performance of the default NE gram-
mars on the dialogue texts; suggest possi-
ble improvements on the basis of the infor-
mation about missed and incorrect anno-
tations provided by the corpus benchmark
tool.
? named entity recognition: implement the
improvements proposed at the previous
step, by changing the default NE grammar
rules and/or by introducing rules specific to
your dialogue domain.
Finally, some assignments which are not con-
nected to any particular domain or application:
? chunking : implement an NP chunker using
JAPE. Look at the VP chunker grammars
for examples.
? template-based IE : experiment with ex-
tracting information from the dialogues us-
ing templates and JAPE (an example im-
plementation will be provided soon).
? (for a group of students) building NLP-
enabled Web applications: embed one of the
IE applications developed so far into a Web
application, which takes a Web page and
returns it annotated with the entities. Use
http://gate.ac.uk/annie/index.jsp as an ex-
ample.
In the near future it will be also possible to
have assignments on summarisation and genera-
tion, but these modules are still under develop-
ment. It will be possible to demonstrate parsing
and discourse interpretation, but because these
modules are implemented in Prolog and some-
what difficult to modify, assignments based on
them are not recommended. However, other
such modules, e.g., those from NLTK (Loper
and Bird, 2002), can be used for such assign-
ments.
7 Conclusions
In this paper we have outlined the GATE sys-
tem and its key features that make it an effective
tool for teaching NLP and CL. The main advan-
tage is that GATE is a framework and a graph-
ical development environment which is suitable
both for research and teaching, thus making it
easier to connect the two, e.g., allow a student to
carry out a final-year project which contributes
to novel research, carried out by their lectur-
ers. The development environment comes with
a comprehensive set of tools, which cover the
entire application development cycle. It can be
used to provide students with hands-on experi-
ence in a wide variety of tasks. Universities will-
ing to use GATE as a teaching tool will benefit
from the comprehensive documentation, several
tutorials, and online demonstrations.
References
D.E. Appelt. 1996. The Common Pattern Specifi-
cation Language. Technical report, SRI Interna-
tional, Artificial Intelligence Center.
S. Bird and M. Liberman. 1999. A Formal Frame-
work for Linguistic Annotation. Technical Re-
port MS-CIS-99-01, Department of Computer and
Information Science, University of Pennsylvania.
http://xxx.lanl.gov/abs/cs.CL/9903003.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002a. GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Anniversary Meeting of the Association for
Computational Linguistics.
H. Cunningham, D. Maynard, K. Bontcheva,
V. Tablan, and C. Ursu. 2002b. The GATE User
Guide. http://gate.ac.uk/.
H. Cunningham. 2002. GATE, a General Archi-
tecture for Text Engineering. Computers and the
Humanities, 36:223?254.
R. Grishman. 1997. TIPSTER Architec-
ture Design Document Version 2.3. Techni-
cal report, DARPA. http://www.itl.nist.gov/-
div894/894.02/related projects/tipster/.
E. Loper and S. Bird. 2002. NLTK: The Natural
Language Toolkit. In ACL Workshop on Effective
Tools and Methodologies in Teaching NLP.
D. Maynard, H. Cunningham, K. Bontcheva,
R. Catizone, George Demetriou, Robert
Gaizauskas, Oana Hamza, Mark Hepple, Patrick
Herring, Brian Mitchell, Michael Oakes, Wim
Peters, Andrea Setzer, Mark Stevenson, Valentin
Tablan, Christian Ursu, and Yorick Wilks. 2000.
A Survey of Uses of GATE. Technical Report
CS?00?06, Department of Computer Science,
University of Sheffield.
D. Maynard, V. Tablan, C. Ursu, H. Cunningham,
and Y. Wilks. 2001. Named Entity Recognition
from Diverse Text Types. In Recent Advances
in Natural Language Processing 2001 Conference,
Tzigov Chark, Bulgaria.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu,
H. Saggion, K. Bontcheva, and Y. Wilks. 2002.
Architectural elements of language engineering ro-
bustness. Journal of Natural Language Engineer-
ing ? Special Issue on Robust Methods in Analysis
of Natural Language Data. forthcoming.
N.F. Noy, M. Sintek, S. Decker, M. Crubzy, R.W.
Fergerson, and M.A. Musen. 2001. Creating Se-
mantic Web Contents with Prote?ge?-2000. IEEE
Intelligent Systems, 16(2):60?71.
H. Saggion, H. Cunningham, K. Bontcheva, D. May-
nard, C. Ursu, O. Hamza, and Y. Wilks. 2002.
Access to Multimedia Information through Mul-
tisource and Multilanguage Information Extrac-
tion. In 7th Workshop on Applications of Natural
Language to Information Systems (NLDB 2002),
Stockholm, Sweden.
V. Tablan, C. Ursu, K. Bontcheva, H. Cunningham,
D. Maynard, O. Hamza, Tony McEnery, Paul
Baker, and Mark Leisher. 2002. A unicode-based
environment for creation and use of language re-
sources. In Proceedings of 3rd Language Resources
and Evaluation Conference. forthcoming.
Unicode Consortium. 1996. The Unicode Standard,
Version 2.0. Addison-Wesley, Reading, MA.
 
	Experiments with geographic knowledge for information extraction
Dimitar Manov,
Atanas Kiryakov,
Borislav Popov
Ontotext Lab, Sirma AI Ltd
38A Christo Botev Blvd, Sofia 1000,
Bulgaria
{mitac,naso,borislav}@sirma.bg
Kalina Bontcheva,
Diana Maynard,
Hamish Cunningham
University of Sheffield
Regent Court, 211 Portobello St.,
Sheffield S1 4DP, UK
{kalina,diana,hamish}@dcs.shef.ac.uk
Abstract
Here we present work on using spatial knowl-
edge in conjunction with information extrac-
tion (IE). Considerable volume of location data
was imported in a knowledge base (KB) with
entities of general importance used for seman-
tic annotation, indexing, and retrieval of text.
The Semantic Web knowledge representation
standards are used, namely RDF(S). An exten-
sive upper-level ontology with more than two
hundred classes is designed. With respect to the
locations, the goal was to include the most im-
portant categories considering public and tasks
not specially related to geography or related ar-
eas. The locations data is derived from num-
ber of publicly available resources and com-
bined to assure best performance for domain-
independent named-entity recognition in text.
An evaluation and comparison to high perfor-
mance IE application is given.
1 Introduction
Information Extraction (IE) research has focused mainly
on the recognition of course-grained entities like Loca-
tion, Organization, Person, etc. (Sundheim, 1998). The
application of Information Extraction to new areas like
the Semantic Web and knowledge management has posed
new challenges, from which the most relevant here is the
need for finer-grained recognition of entities, such as lo-
cations.
In this paper we present some experiments with build-
ing a reusable knowledge base of locations which is used
as a component into an IE system, instead of a location
gazetteer. This work is part of the Knowledge and Infor-
mation Management (KIM) platform and still undergoing
development and refinement.
With respect to coverage, the goal was to include the
most important location categories for a wide range of ap-
plications and tasks, not specially related to geography or
related areas. The locations data is derived from a num-
ber of publicly available resources and combined to as-
sure best performance for named-entity recognition. An
evaluation and comparison to high performance IE sys-
tem using very small location gazetteers is given.
One important aspect of our work is that we choose to
create a knowledge base of locations, structured accord-
ing to an ontology and having relations between them, in-
stead of having somewhat flat structures of gazetteer lists
found in other IE systems. While a knowledge base can
be plugged into an IE system instead of a flat gazetteer, it
also has several unique advantages:
? the extra information, especially the transitive sub-
RegionOf relation can be used for disambiguation
and reasoning
? the location entities in the text can be recognised at
the right level of granularity for the target applica-
tion (i.e., as Location or as Country, City, etc).
? the ontology and knowledge base can be modified
by the user and any changes are reflected immedi-
ately in the output of the IE system.
The paper is structured as follows. Section 2 puts
our work in the context of previous research. Section 3
presents briefly the KIM platform, which contains the IE
system and the location knowledge base. Then Section
4 describes the location knowledge base in more detail.
The IE experiments are discussed in Section 5, followed
by a discussion on problems and future work. The paper
concludes by showing how such a knowledge base can be
used to bootstrap a new IE system (Section 7).
2 Related work
In the context of this paper, the two most relevant areas
of work are on large-scale gazetteers and location disam-
biguation. Here we present the Alexandria Digital Li-
brary Gazetteer because we used the ADL Feature Type
Thesaurus as a basis of our location ontology. Related
work on location disambiguation, like the one done in
the Perseus Digital Library project, is relevant because in
future work we will improve the location disambiguation
mechanism in our system.
2.1 Alexandria Digital Library Gazetteer
The Alexandria Digital Library (ADL), an NSF-funded
project at the University of California, Santa Barbara,
has included gazetteer development from its beginning
in 1994. Currently it contains approximately 4.4 mil-
lion entries. The data is taken from various sources, in-
cluding NIMA (National Imagery and Mapping Agency?s
of United States) Gazetteer, a set of countries and U.S.
counties, set of U.S. topographic map quadrangle foot-
prints, set of volcanoes, and set of earthquake epicenters.
The Geographic Names Information System (GNIS) data
from the U.S. Geological Survey has been partly added
to the collection. The results as of today include the-
saurus for feature types, Time Period data for the histori-
cal entries and spatial data with boundaries. The bound-
aries are defined as ?satisficing? rectangles. The term
?satisficing? is described in (Hill, 2000), and additional
information about the project could also be found there
as well as on the ADL gazetteer development page at
http://alexandria.sdc.ucsb.edu/?lhill/adlgaz/.
2.2 Toponym-disambiguation in Perseus Digital
Library project
A disambiguation system for historical place names for
Perseus digital library is described in (Smith and Crane,
2001). The library is concentrated on representing his-
torical data in the humanities from ancient Greece to
nineteenth-century America. The authors present a pro-
cedure for disambiguation of such place names, based on
internal and external evidence from the text. Internal ev-
idence includes the use of honorifics, generic geographic
labels, or linguistic environment. External evidence in-
cludes gazetteers, biographical information, and general
linguistic knowledge. Evaluation of the performance of
the system is given, using standard precision/recall meth-
ods for each of the five corpora: Greek, Roman, London,
California, Upper Midwest. The system is best on Greek
and worst on Upper Midwest corpus, and its overall per-
formance for place names is higher than the most of other
applications.
3 The KIM platform
The KIM Platform provides a novel Knowledge and In-
formation Management (KIM1) infrastructure and ser-
vices for automatic semantic annotation, indexing and re-
trieval of unstructured and semi-structured content. The
ontologies and knowledge bases are kept in Semantic
1KIM, see http://www.ontotext.com/kim
Figure 1: KIM Platform
repositories based on cutting edge Semantic Web technol-
ogy and standards, including RDF(S) repositories2, on-
tology middleware3 (Kiryakov et al 2002) and reason-
ing4. It provides a mature infrastructure for scalable and
customizable information extraction as well as annota-
tion and document management, based on GATE (Cun-
ningham et al, 2002). GATE, a General Architecture
for Text Engineering, is developed by the Sheffield NLP
group and has been used in many language processing
projects; in particular for Information Extraction in a va-
riety of languages (Maynard and Cunningham, 2003).
An essential idea for KIM is the semantic (or entity)
annotation, depicted on figure 1. It can be seen as a clas-
sical named-entity recognition and annotation process.
However, in contrast to most of the existing IE system,
KIM provides for each entity reference in the text (i) a
pointer (URI) to the most specific class in the ontology
and (ii) pointer to the specific instance in the knowledge
base. The latest is (to the best of our knowledge) an
unique KIM feature which allows further indexing and
retrieval of documents with respect to entities.
For the end-user, the usage of a KIM-based application
is straightforward and simple - one can highlight text in
the browser and further explore the available knowledge
for the entity, as shown in figure 3. A semantic query web
user interface allows for queries such as ?Organization-
2Sesame (http://sesame.aidministrator.nl/) is an open source
RDF(S)-based repository and querying facility.
RDF, http://www.w3.org/RDF/. Resource Description Frame-
work is an open standard for knowledge exchange over the Web,
developed by W3C (www.w3.org).
3OMM, http://www.ontotext.com/omm. Ontology Middle-
ware Module is an enterprise back-end for formal knowledge
management.
4BOR, http://www.ontotext.com/bor/, is a DAML+OIL rea-
soner, compliant with the latest OWL specifications.
Figure 2: KIM architecture.
locatedIn-Country? to be executed.
Information retrieval functionality is available, based
on Lucene5, which is adapted to measure relevance to en-
tities instead of tokens and stems. The full architecture is
shown in figure 2. It is important to note that KIM as a
software platform is domain and task independent.
3.1 The ontology
KIM Ontology (KIMO) covers the most general 250
classes of entities and 40 relations. The main classes are
Entity, EntitySource and LexicalResource. The most im-
portant class in the ontology is Entity, further specialized
into Object, Abstract and Happening. LexicalResource
class and its subclasses are used for different IE-related
information. The instances of the Alias class represent
different names of instances of Entity. hasAlias relation
is used to link Entity to its aliases (one-to-many rela-
tion). The hasMainAlias links to the main alias (the of-
ficial name). Each instance of Entity is linked to an in-
stance of EntitySource via generatedBy relation. There
are two types of EntitySource - Trusted and Recognized.
The ?trusted? entities are those pre-defined. The recog-
nized are the ones which were recognized from text as
part of the IE tasks.
The upper part of the ontology can be seen on the same
figure 3 in the left frame.
For ontology representation we choose RDF(S), mainly
because it allows easy extension to OWL6 (Lite).
Location sub-ontology
Because the Geographic features (Locations) form a
large part of the entities of general importance, we de-
5 Lucene, http://jakarta.apache.org/lucene/, high perfor-
mance full text search engine
6Ontology Web Language (OWL),
http://www.w3.org/TR/owl-semantics/
veloped a Location sub-ontology as part of the KIM on-
tology. The goal was to include the most important and
frequently used types of Locations (which are specializa-
tions of Entity), including relations between them (such
as hasCapital, subRegionOf (more specific than part-of)),
relations between Locations and other Entities (Organiza-
tion locatedIn Location) and various attributes.
The Location entity denotes an area in 3D space7,
which includes geographic entities with physical bound-
aries, such as geographical areas and landmasses, bodies
of water, geological formations and also politically
defined areas (e.g. ?U.S. Administered areas?).
The classification hierarchy (consisting of 97 classes)
is based on the ADL Feature Type Thesaurus version
070203. The differences target simplicity; a number of
distinctions and unnecessary levels of abstraction were
removed where irrelevant to general (non-geographic)
context, as we wanted the ontology to be easy to un-
derstand for an average user. Examples of sub-classes
omitted: Territorial waters, Tribal areas, Administrative
Areas (its sub-types are put directly under Location).
The Location ontology provides the following addi-
tional information:
? the exact type of a feature, for example to be able
to recognize a geographic feature as CountryCapital
instead of just Location.
? relations between geographic feature and other en-
tities (e.g. ?Diego Garcia? is a MilitaryBase, lo-
cated somewhere in the Indian Ocean and it is sub-
RegionOf USA).
? the different names of a location (?Peking? and
?Beijing? are two aliases for one location).
? the transitive subRegionOf relation allows one to
search for Entities located in a continent (e.g. ?Mor-
gan Stanley? - locatedIn - ?New York? - subRe-
gionOf - ?NY? - subRegionOf - ?USA? - subRe-
gionOf - ?North America?)
? ?trusted? vs ?recognized? sources in generatedBy
property of a Location is an extra hint in disam-
biguation tasks. The class hierarchy is shown in fig-
ure 5.
7Actually, the instances of Location are Entities with spa-
tial identity criteria (Guarino and Welty, 2000). For instance
a building can be considered as Property, Location or Cultural
Artifact, but the focus in the ontology is placed on the Location
aspect.
Figure 3: KIM usage - highlight and explore. The upper part of KIM ontology (KIMO) is shown in the left frame.
3.2 The knowledge base
Geographic information usually introduces a high level
of ambiguity between named entities, for the following
three reasons:
? there could be several Locations with the same name
(this includes sharing common alias);
? a name of a Location could match a common En-
glish word (e.g. ?Has?, ?The?);
? other named entities (Company, Person, even Date
or Numeric data) could share a common alias
with a Location (examples: ?Paris Corporation?,
?O?Brian? county, ?10? district, ?Departamento de
Nueve de Julio? with alias ?9 de Julio?).
In order to allow easy bootstrapping of applications
based on KIM and to eliminate the need for them to
write a Geo-gazetteer, the KIM knowledge base pro-
vides exhaustive coverage of entities of general impor-
tance. By limiting the Locations to only ?important?
ones, we also keep the system as generic, domain- and
task-independent as possible. The term ?importance? of
a location is hard to define, and part of the problem is that
it is dependent on the domain where the IE tasks are fo-
cused. Yet it is common sense that such locations include
continents, countries, big cities, some rivers, mountains,
etc. In addition to the above predefined locations, KIM:
? learns from the texts it analyses;
? has a comprehensive set of rules and patterns help-
ing it to recognize unknown entities;
? has a Hidden Markov Model learner, capable of cor-
recting symbolic patterns.
As a test domain, KIM uses political and economic news
articles from leading newswires8.
4 Populating the location knowledge base
As a main source of geographic knowledge we used
NIMA?s GEOnet Names Server (GNS) data. GNS
database is the official repository of foreign place-name
decisions approved by the U.S. Board on Geographic
Names (US BGN) and contains approximately 3.9 mil-
lion features with 5.37 million names. Approximately
20,000 of the database?s features are updated monthly.
The data is available for download in standard formatted
text files, which contain: unique feature index (UFI), sev-
eral names per Location (the official name, short name,
sometimes different transcriptions of the name), geo-
graphic coordinates (one point; no bounding rectangle).
Geographic coverage of the data is worldwide, exclud-
ing United States and Antarctica. For U.S. geographic
8See News Collector, http://news.ontotext.com
Figure 4: RDF representation of a Location.
data we used partially USGS/GNIS data9, which fol-
lows similar format as GNS data. For country names we
followed FIPS10, which was natural choice since GNS
data is structured that way. A list of big cities was ob-
tained from UN Statistics site, which covers city data
(http://unstats.un.org/unsd/citydata/).
We then created a mapping between our location classes
and GNS feature designators. Some of the features were
completely ignored (e.g. ?abandoned populated places?,
?drainage ditch?), other were combined into one (e.g.
?ADM2?, ?ADMD? into County).
There is some inconsistency in the way the data is entered
for different countries, mostly because of improper usage
of designators (using different designators for similar ge-
ographic features and vice versa). This made creation of
the mapping a bit harder, as we needed to include more
designators mapped to one class. The per-country files
were almost consistently entered (with some exceptions,
for example in UK, ?England?, ?Scotland?, ?Northern
Ireland? and ?Wales? are entered as AREA, which hints
the same importance as the other 40 areas in UK). We
expect that a per-country mapping instead of a global one
will lead to better performance results, yet we haven?t ex-
perimented with this as it will require manual tuning for
about 250 countries.
The different names of the geographic features are
mapped to aliases of the Location entities, with a main
alias pointing to the official name. The RDF represen-
tation of a Location is shown in figure 4. Because these
names sometimes match common English words and Per-
son names a list of stop words is created and the aliases
are filtered.
The import procedure uses the mapping described
9US Geological Survey (UGCS); Geographic Names Infor-
mation System (GNIS)
10Federal Information Processing Standards,
http://www.itl.nist.gov/fipspubs/
above but can also be restricted by list of countries and
classes to be imported. Currently imported classes are:
Continent, GlobalRegion, Country, Province, County,
CountryCapital, LocalCapital, City, Ocean, Sea, Gulf,
OilField, Monument, Bridge, Plateau, Mountain, Moun-
tainRange, Plain. These classes were selected as ?impor-
tant?, based on common sense and statistical information
derived from GNS data.
The GNS data has three main problems when it comes
to extracting only geographical entities of global impor-
tance and the relations between them:
? There is no way to tell the importance of a location
(e.g. is Chirpan a big city or a small town);
? The only part-of relations available are between a
location and its country, but not province or county;
? Some locations are not country-specific (e. g.
oceans, seas, mountains) but are listed as separate
locations with different identifiers in different per-
country lists.
We addressed the first problem by limiting the types of lo-
cations to a small subset of important ones (as explained
above). The importance of cities was determined by us-
ing a list of all big cities (with population over 100,000).
We attempted to solve the second problem by using an al-
gorithm to calculate the distance between a location and
all provinces/counties in this country, and then to create
a part-of relation with the nearest one. However, our ex-
periments showed that the accuracy of the results was not
satisfactory. This is mostly due to the fact that in GNS
data only the location footprint is given, but not the ex-
tent. Comparing the geographic coordinates of the loca-
tions with a common alias and type and then combining
the matching ones into a single entity in the knowledge
base solved the third problem.
Currently the KB contains about 50,000 Locations
grouped into 6 Continents, 27 GlobalRegions (such as
?Caribbean? or ?Eastern Europe?), 282 Countries, all
country capitals and 4,700 Cities (including all the cities
with population over 100,000). Each location has sev-
eral aliases (usually including English, French and some-
times the local transcription of the location), geographic
coordinates, the designator (DSG) and Unique Feature
Index (UFI), according to GNS. The figures for entities
of global importance in KIM KB are shown in table 1.
5 Experiments with direct use for IE
The locations KB is used for Information Extraction (IE)
as part of the KIM system, combining symbolic and
stochastic approaches, based on the ANNIE IE compo-
nents from GATE. As a baseline, using a gazetteer mod-
ule, the aliases of the entities (including all locations) are
Entities 77,561
Aliases 110,308
Locations 49,348
Cities 4,720
Companies 7,906
Public companies 5,150
Key people 5,500
Organizations 8,365
Table 1: Instances per subclass of Entity.
being looked up in the text. Further, unknown or not
precisely matching entities are recognized with pattern-
based grammars:
? using location pre/post keys to identify locations,
e.g. ?The River Thames?
? using location pre/post keys + Location, e.g. ?north
Egypt?, ?south Wales?
? context-based recognition, such as: ?in? + Token-
with-first-uppercase Number of disambiguation
problems (mostly in the case of Location names oc-
curring in the composite name of other Entities) are
also detected and resolved:
? ambiguity between Person and Organization, e.g.
?U.S. Navy? (this would normally be recognized as
a Person name from the pattern ?two initials + Fam-
ily name?, but in this case the initials match a loca-
tion alias)
? occurrence of locations in person names, e.g. ?Jack
London? (disambiguated because in the KB there is
LexicalResource ?Jack? is a first name of Person)
? occurrence of locations in Organization names, e.g.
?Scotland Yard? (disambiguated because in the KB
there is such Organization)
Finally, some of the recognized Entities (including
Locations), which are not marked as noun by the part of
speech tagger are discarded.
Some of the newly recognized Locations appear fre-
quently in the analyzed texts. Those, which could be
found in the GNS data are potential candidates to be en-
tered in the knowledge base, because there is an extra
evidence for their importance. This is a way to extend the
knowledge base and make it contain all the ?important?
Locations in the sense of frequently used in the one or
more application domain(s).
The performance of the KIM system was measured on
a news corpus using GATE?s evaluation tools. The sys-
tem was also compared to an high-precision named entity
recognition system, which uses small flat gazetteer lists.
Entity Number
Location 792
Organisation 773
Person 764
Date 603
Percent 54
Money 94
Table 2: Distribution of entities in the corpus
5.1 Evaluation Corpus
The corpus was collected from 3 online English news-
papers: the Independent, the Guardian and the Financial
Times. In total it contains 101 documents with 56,221
words. The corpus was manually annotated with entities.
Table 2 shows the number of entities of each type in the
corpus.
5.2 Corpus Benchmark Tool
The Corpus Benchmark Tool(CBT) is one of the compo-
nents in GATE which enables automatic evaluation of an
application in terms of Precision, Recall and F-measure,
against a set of ground truths. Furthermore, it also en-
ables two versions of a system to be compared against
each other (e.g. for regression testing) or two different
systems to be compared. Each system is evaluated by
comparing the annotations produced with a set of key an-
notations (produced manually) and producing a score ?
two systems can therefore be compared with each other
and indications are given as to where they differ from
each other.
5.3 MUSE
MUSE is an information extraction system developed
within GATE which aims to perform named entity recog-
nition on different types of text (Maynard et al 2002).
MUSE recognises the standard MUC entity types of Per-
son, Location, Organisation, Date, Time, Percent, and
some additional types such as Addresses and Identifiers.
The system is based on ANNIE, the default IE system
within GATE, but has been extended to deal with a vari-
ety of text sources and genres, and incorporates a mecha-
nism for automatically selecting the most appropriate set
of resources depending on the text type.
MUSE uses flat-list gazetteers which primarily contain
contextual clues that help with the identification of named
entities, e.g., company designators (such as Ltd, GmbH),
job titles, person titles (such as Mr, Mrs), common first
names, typical organisation types (e.g., Ministry, Univer-
sity). In addition, MUSE has lists enumerating concrete
types of locations which have about 27 500 entries, in-
cluding 25,000 UK ones. Further breakdown is given in
Table 3:
global regions (including continents) 71
aliases of countries 450
provinces 1215
mountains 5
water regions (oceans, lakes, etc) 15
cities world wide 1900
UK regions (such as East Sussex, Essex) 140
cities in UK 23792
UK rivers 3
Table 3: MUSE Location gazetteer entries
As can be seen from the location entries in the MUSE
gazetteers, the system is specifically tailored to recognise
UK locations with high recall and precision, whereas the
KIM locations KB is not skewed towards any particular
country.
We ran the MUSE system over our test corpus to see
how KIM matched up to it.
5.4 Results
MUSE vs KIM performance comparison is given in ta-
ble 4. When interpreting these results one also must bear
in mind that the high-performance IE system is only tag-
ging geographical entities as locations, whereas the GNS-
based system is actually disambiguating them with re-
spect to their specific type (e.g., City, Province, Country).
Investigation of the reasons behind the lower recall shows
that:
? the KB is too coarse-grained, i.e., there are no
?smaller? locations, such as small towns/counties in
UK, we do not import military bases in KB from
GNS data (?Diego Garcia?), etc.
? The application was not specifically tuned for the
corpus/news texts, e.g. we do not use the fact, that
the texts often clarify the locations when they are
first mentioned (e.g., Aberdeen, UK).
? there are not any historical Locations, such as
?Soviet Union?.
It is expected that the first two problems will be fixed
with enhancement of the KB with regard to domain
targeting of a KIM-based application. To check this
assumption we did another experiment. Because the
corpus contains a lot of UK-related information (the
articles are from three English newspapers) and MUSE
is specifically tailored to UK locations, we needed extra
UK-specific information in the KB. As we mentioned
earlier the import procedure is flexible to the extend that
allowed to add all the locations from UK GNS data. The
performance of this enhanced KB is shown in table 5.
The recall is higher than in MUSE (increased to 95% vs
93%).
The precision is 10% behind MUSE (85% vs 95%).
An obvious reason is that we have more entities in KB,
and we do not control the aliases (except for stop words
list), while all the locations in MUSE gazetteer lists
are manually entered and therefore produce very little
ambiguity.
6 Discussion
We produced a KB of locations with world wide cover-
age using GNS data. The size of about 50,000 Location is
more than most other IE systems have. It is not big (com-
pared to 4M locations in ADL Gazetteer), but provides
good coverage of Locations (91%). Because the KB was
not tuned for the test corpus specifics we could expect
similar coverage for other corpora.
Our flexible import procedure allows for domain-
targeted versions of the KB (by means of importing more
Location types) to be produced, which is expected to have
good-enough coverage on locations.
The impact of the location KB on the IE performance
is still under evaluation and improvement. We are work-
ing on improvements in two directions: i) decreasing the
amount of GNS-data entered in KB - for both locations
and their aliases; ii) changing the way in which the IE
system uses the KB to improve precision. On the latter,
we are currently experimenting with applying the regular
named entity recognition grammars first and then using
the location KB to lookup only the unclassified entities,
instead of using it as a gazetteer prior to named entity
recognition as we do now.
7 Bootstrapping IE for new languages
from the KB
We were able to make use of the KB as part of the TIDES
Surprise Language Exercise, a collaborative effort be-
tween a number of sites to develop resources and tools
for various language engineering tasks on an unknown
language. A dry run of this program took place in March
2003, whereby participants were given a week from the
time the language was announced, to collect tools and re-
sources for processing that language. The language cho-
sen was Cebuano, spoken by 24% of the population in
the Phillipines. The University of Sheffield developed a
Named Entity recognition system for Cebuano, to which
we contributed a list of locations from the Philippines.
This was particularly useful as this kind of information
was not readibly available from the Internet, and time was
of the essence. The NE system (developed within a week)
achieved scores for the recognition of locations at 73%
System Correct Partially Correct Missing Spurious Precision Recall F-Measure
MUSE 744 9 54 37 0.947 0.928 0.937
KIM 726 24 61 113 0.855 0.910 0.881
Table 4: MUSE vs KIM performance comparison
System Correct Partially Correct Missing Spurious Precision Recall F-Measure
MUSE 744 9 54 37 0.947 0.928 0.937
KIM-UK 759 28 27 167 0.810 0.950 0.874
Table 5: MUSE vs KB with all UK locations
Precision, 78% Recall and 76% F-measure. We predict
that this kind of information will be very useful for the
full Surprise Language Program in June, where partici-
pants will have more time (a month) to create resources
on another surprise language ? not only for Information
Extraction but also for tasks such as Cross-Language In-
formation Retrieval and Machine Translation.
8 Conclusion and future work
This paper presented work on the creation of a locations
knowledge base and its use for information extraction.
In order to allow easy bootstrapping of IE to different
languages and applications, we are building a knowledge
base (KB) with entities of general importance, including
geographic locations. The aim is to include the most im-
portant and frequently used types of Locations. An evalu-
ation and comparison to high performance IE application
was given.
The system is still under development and future im-
provements are envisaged, mainly related to implement-
ing better disambiguation techniques (e.g., like those de-
scribed in (Smith and Crane, 2001)) and experimenting
with new ways of using the KB from the IE application.
Acknowledgements
Work on GATE has been supported by the Engineering
and Physical Sciences Research Council (EPSRC) un-
der grants GR/K25267 and GR/M31699, and by several
smaller grants. The last author is currently supported by
the EPSRC-funded AKT project (http://www.aktors.org)
grant GR/N15764/01.
References
Atanas Kiryakov, Kiril Simov, Damyan Ognyanov. 2002.
Ontology Middleware and Reasoning In the ?Towards
the Semantic Web: Ontology-Driven Knowledge Man-
agement?, editors John Davies, Dieter Fensel, Frank
van Harmelen. John Wiley & Sons, Europe, 2002.
Beth Sundheim, editor. Proceedings of the Seventh
Message Understanding Conference (MUC-7). ARPA,
Morgan Kaufmann, 1998.
David A. Smith and Gregory Crane 2001. Disambiguat-
ing Geographic Names in a Historical Digital Library.
In Proceedings of ECDL, pages 127-136, Darmstadt,
4-9 September 2001.
Diana Maynard, Valentin Tablan, Hamish Cunningham,
Cristian Ursu, Horacio Saggion, Kalina Bontcheva,
Yorick Wilks 2002. Architectural Elements of Lan-
guage Engineering Robustness. In Journal of Natu-
ral Language Engineering ? Special Issue on Robust
Methods in Analysis of Natural Language Data, 8 (1)
pp 257-274
Diana Maynard and Hamish Cunningham. 2003. Multi-
lingual Adaptations of a Reusable Information Extrac-
tion Tool. In Proceedings of EACL 2003, Budapest,
Hungary, 2003.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva
and Valentin Tablan. 2002. GATE: A Framework and
Graphical Development Environment for Robust NLP
Tools and Applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Compu-
tational Linguistics, 2002.
Linda L. Hill. 2000. Core elements of digital gazetteers:
placenames, categories, and footprints. In J. Borbinha
& T. Baker (Eds.), Research and Advanced Tech-
nology for Digital Libraries : Proceedings of the
4th European Conference, ECDL 2000 Lisbon, Por-
tugal, September 18-20, 2000 (pp. 280-290). Berlin:
Springer.
Nicola Guarino and Christopher Welty. 2000. Towards
a methodology for ontology-based model engineering.
In Proceedings of ECOOP-2000 Workshop on Model
Engineering. Cannes, France.
Apendix A. Ontology screenshots
Figure 5: Location sub-ontology.
Figure 6: Upper level of KIM ontology.
OLLIE: On-Line Learning for Information Extraction
Valentin Tablan, Kalina Bontcheva, Diana Maynard, Hamish Cunningham
University of Sheffield
Regent Court, 211 Portobello St.
Sheffield S1 4DP, UK
{V.Tablan,diana,kalina,hamish}@dcs.shef.ac.uk
Abstract
This paper reports work aimed at develop-
ing an open, distributed learning environ-
ment, OLLIE, where researchers can ex-
periment with different Machine Learning
(ML) methods for Information Extraction.
Once the required level of performance is
reached, the ML algorithms can be used to
speed up the manual annotation process.
OLLIE uses a browser client while data
storage and ML training is performed on
servers. The different ML algorithms use
a unified programming interface; the inte-
gration of new ones is straightforward.
1 Introduction
OLLIE is an on-line application for corpus annota-
tion that harnesses the power of Machine Learning
(ML) and Information Extraction (IE) in order to
make the annotator?s task easier and more efficient.
A normal OLLIE working session starts with the
user uploading a set of documents, selecting which
ML method to use from the several supplied by the
system, choosing the parameters for the learning
module and starting to annotate the texts. During
the initial phase of the manual annotation process,
the system learns in the background (i.e. on the
server) from the user?s actions and, when a certain
degree of confidence is reached, it starts making sug-
gestions by pre-annotating the documents. Initially,
some of these suggestions may be erroneous but, as
the user makes the necessary corrections, the system
will learn from its mistakes and the performance will
increase leading to a reduction in the amount of hu-
man input required.
The implementation is based on a client-server ar-
chitecture where the client is any Java-enabled web
browser and the server is responsible for storing
data, training ML models and providing access ser-
vices for the users.
The client side of OLLIE is implemented as a set
of Java Server Pages (JSPs) and a small number of
Java applets are used for tasks where the user inter-
face capabilities provided by HTML are not enough.
The server side comprises a JSP/servlet server,
a relational database server and an instance of the
GATE architecture for language engineering which
is used for driving all the language-related process-
ing. The general architecture is presented in Figure
1.
The next section describes the client side of the
OLLIE system while Section 3 details the imple-
mentation of the server with a subsection on the inte-
gration of Machine Learning. Section 4 talks about
security; Section 6 about future improvements and
Section 7 concludes the paper.
2 The OLLIE client
The OLLIE client consists of several web pages,
each of them giving the user access to a particular
service provided by the server.
One such page provides facilities for uploading
documents in the system from a URL, a local file,
or created from text pasted in a form. A variety
of formats including XML, HTML, email and plain
text are supported. When a document is created, the
Figure 1: The general architecture of OLLIE
original markup ?if present? is separated from tex-
tual content to prevent it from interfering with the
subsequent language processing.
Another page lets the user choose which machine
learning algorithm is to be used, the attributes that
characterise the instances (e.g., orthography, part-
of-speech), and parameters for the chosen learning
method (e.g., thresholds, smoothing values). The
classes to be learnt (e.g., Person, Organisation) are
provided as part of the user profile, which can be
edited on a dedicated page. All ML methods com-
patible with OLLIE have a uniform way of describ-
ing attributes and classes (see Section 3.1 for more
details on the ML integration); this makes possible
the use of a single user interface for all the ML al-
gorithms available. The fine-tuning parameters are
specific to each ML method and, although the ML
methods can be run with their default parameters, as
established by (Daelemans and Hoste, 2002), sub-
stantial variation in performance can be obtained by
changing algorithm options.
Since OLLIE needs to support the users with the
annotation process by learning in the background
and suggesting annotations, it offers control over
the accuracy threshold for these suggestions. This
avoids annoying the users with wrong suggestions
while assuring that suggestions the system is confi-
dent about are used to pre-annotate the data, reduc-
ing the workload of the user.
The document editor can then be used to annotate
the text (see Figure 2). The right-hand side shows
the classes of annotations (as specified in the user
profile) and the user selects the text to be annotated
(e.g., ?McCarthy?) and clicks on the desired class
(e.g., Person). The new annotation is added to the
document and the server is updated immediately (so
the new data becomes available to the ML algorithm
too). The document editor also provides facilities
for deleting wrong annotations, which are then prop-
agated to the server, in a similar way.
The graphical interface facilities provided by a
web browser could be used to design an interface for
annotating documents but that would mean stretch-
ing them beyond their intended use and it is hard to
believe that such an interface would rate very high
on a usability scale. In order to provide a more er-
gonomic interface, OLLIE uses a Java applet that
integrates seamlessly with the page displayed by the
browser. Apart from better usability, this allows for
greater range of options for the user.
The communication between the editor applet and
the server is established using Java Remote Method
Invocation (a protocol similar to the C++ Remote
Procedure Call ? RPC) which allows the instant no-
tification when updates are needed for the document
stored on the server. The continuous communication
between the client and the server adds the benefit of
data security in case of network failure. The data
on the server always reflects the latest version of the
document so no data loss can occur. The session
data stored by the server expires automatically after
an idle time of one hour. This releases the resources
used on the server in case of persistent network fail-
ures.
The data structures used to store documents on the
server are relatively large because of the numerous
indices stored to allow efficient access to the annota-
tions. The copy downloaded by the client when the
annotation process is initiated is greatly reduced by
filtering out all the unnecessary information. Most
of the data transferred during the client-server com-
munication is also compressed, which reduces the
level of network traffic ? always a problem in client
server architectures that run over the Internet.
Figure 2: Annotating text in the OLLIE client
Another utility provided by the client is a page
that lets the user specify the access rights to the doc-
ument/corpus, which determine whether it can be
shared for viewing or collaborative annotation (see
Section 4 for details on security).
3 Implementation of the OLLIE server
While the client side of the OLLIE application is
presented as set of web pages, the server part is
based on the open source GATE architecture.
GATE is an infrastructure for developing and de-
ploying software components that process human
language (Cunningham et al, 2002). It is written
in Java and exploits component-based software de-
velopment, object orientation and mobile code. One
quality of GATE is that it uses Unicode through-
out (Tablan et al, 2002). Its Unicode capabilities
have been tested on a variety of Slavic, Germanic,
Romance, and Indic languages (Gamba?ck and Ols-
son, 2000; Baker et al, 2002). This allows OL-
LIE to handle documents in languages other than
English. The back-end of OLLIE uses the GATE
framework to provide language processing compo-
nents, services for persistent storage of user data,
security, and application management.
When a document is loaded in the OLLIE client
and subsequently uploaded to the server, its format
is analysed and converted into a GATE document
which consists of textual content and one or more
layers of annotation. The annotation format is a
modified form of the TIPSTER format (Grishman,
1997), is largely isomorphic with the Atlas format
(Bird and Liberman, 1999) and successfully sup-
ports I/O to/from XCES and TEI (Ide et al, 2000).1
An annotation has a type, a pair of nodes pointing
to positions inside the document content, and a set
of attribute-values, encoding further linguistic infor-
mation. Attributes are strings; values can be any
Java object. An annotation layer is organised as a
Directed Acyclic Graph on which the nodes are par-
ticular locations in the document content and the
arcs are made out of annotations. All the markup
contained in the original document is automatically
extracted into a special annotation layer and can be
used for processing or for exporting the document
back to its original format.
1The American National Corpus is using GATE for a large
TEI-based project.
Linguistic data (i.e., annotated documents and
corpora) is stored in a database on the server (see
Figure 1), in order to achieve optimal performance,
concurrent data access, and persistence between
working sessions.
One of the most important tasks for the OLLIE
server is the execution and control of ML algo-
rithms. In order to be able to use ML in OLLIE,
a new processing resource was designed that adds
ML support to GATE.
3.1 Machine Learning Support
Our implementation for ML uses classification al-
gorithms for which annotations of a given type are
instances while the attributes for them are collected
from the context in which the instances occur in the
documents.
Three types of attributes are defined: nominal,
boolean and numeric. The nominal attributes can
take a value from a specified set of possible values
while the boolean and numeric ones have the usual
definitions.
When collecting training data, all the annotations
of the type specified as instances are listed, and for
each of them, the set of attribute values is deter-
mined. All attribute values for an instance refer to
characteristics of a particular instance annotation,
which may be either the current instance or one sit-
uated at a specified relative position.
Boolean attributes refer to the presence (or ab-
sence) of a particular type of annotation overlapping
at least partially with the required instance. Nominal
and numeric attributes refer to features on a partic-
ular type of annotation that (partially) overlaps the
instance in scope.
One of the boolean or nominal attributes is
marked as the class attribute, and the values which
that attribute can take are the labels for the classes
to be learnt by the algorithm. Figure 3 depicts some
types of attributes and the values they would take
in a particular example. The boxes represent an-
notations, Token annotations are used as instances,
the one in the centre being the current instance for
which attribute values are being collected.
Since linguistic information, such as part-of-
speech and gazetteer class, is often used as at-
tributes for ML, OLLIE provides support for iden-
tifying a wide range of linguistic information - part-
of-speech, sentence boundaries, gazetteer lists, and
named entity class. This information, together with
tokenisation information (kind, orthography, and to-
ken length) is obtained by using the language pro-
cessing components available with GATE, as part of
the ANNIE system (Cunningham et al, 2002). See
Section 5 for more details on the types of linguistic
features that can be used. The user chooses which of
this information is to be used as attributes.
An ML implementation has two modes of func-
tioning: training ? when the model is being built,
and application ? when the built model is used to
classify new instances. Our implementation consists
of a GATE processing resource that handles both the
training and application phases. It is responsible for
detecting all the instances in a document and col-
lecting the attribute values for them. The data thus
obtained can then be forwarded to various external
implementations of ML algorithms.
Depending on the type of the attribute that is
marked as class, different actions will be performed
when a classification occurs. For boolean attributes,
a new annotation of the type specified in the attribute
definition will be created. Nominal attributes trigger
the addition of the feature specified in the attribute
definition on an annotation of the required type sit-
uated at the position of the classified instance. If no
such annotation is present, it will be created.
Once an ML model is built it can be stored as part
of the user profile and reloaded for use at a later time.
The execution of the ML processing resource is
controlled through configuration data that selects the
type of annotation to be used as instances, defines all
the attributes and selects which ML algorithm will
be used and with what parameters.
One good source of implementations for many
well-known ML algorithms is the WEKA library
(Witten and Frank, 1999).2 It also provides a wealth
of tools for performance evaluation, testing, and at-
tribute selection, which were used during the devel-
opment process.
OLLIE uses the ML implementations provided by
WEKA which is accessed through a simple wrap-
per that translates the requests from GATE into API
calls ?understood? by WEKA. The main types of re-
quests dealt with by the wrapper are the setting of
2WEKA homepage: http://www.cs.waikato.ac.nz/ml/weka/
Figure 3: Example of attributes and their values.
configuration data, the addition of a new training in-
stance and the classification of an application-time
instance.
4 Security
Because OLLIE is deployed as a web application
? accessible by anyone with Internet access, secu-
rity is an important issue. Users store documents on
the server and the system also keeps some personal
data about the users for practical reasons.3 All users
need to be provided with a mechanism to authen-
ticate themselves to the system and they need to be
able to select who else, apart from them, will be able
to see or modify the data they store on the server.
Every user has a username and a password, used
to retrieve their profiles and determine which doc-
uments they can access. The profiles also contain
information specifying the types of annotations that
they will be creating during the annotation process.
For example, in the case of a basic named entity
3Storing email addresses for instance is useful for sending
password reminders.
recognition task, the profile will specify Person, Or-
ganisation, and Location. These tags will then be
provided in the document annotation pages.
The access rights for the documents are handled
by GATE which implements a security model simi-
lar to that used in Unix file systems. Table 1 shows
the combination of rights that are possible. They
give a good granularity for access rights, ranging
from private to world readable.
The set of known users is shared between GATE
and OLLIE and, once a user is authenticated with
the system, the login details are kept in session data
which is stored on the OLLIE server. This allows for
automatic logins to the underlying GATE platform
and transparent management of GATE sessions.
5 ML Experiments and Evaluation
To the end of evaluating the suitability of the ML
algorithms provided by WEKA for use in OLLIE
we performed several experiments for named entity
recognition on the MUC-7 corpus (SAIC, 1998). We
concentrated on the recognition of ENAMEX enti-
User User?s Group Other Users
Mode Read Write Read Write Read Write
?World Read/Group Write? + + + + + -
?Group Read/Group Write? + + + + - -
?Group Read/Owner Write? + + + - - -
?Owner Read/Owner Write? + + - - - -
Table 1: Security model ? the access rights
ties, i.e., Person, Organisation, and Location. The
MUC-7 corpus contains 1880 Organisation (46%),
1324 Location (32%), and 887 Person (22%) an-
notations in 100 documents. The task has two ele-
ments: recognition of the entity boundaries and clas-
sification of the entities in the three classes. The re-
sults are summarised below.
We first tested the ability of the learners to iden-
tify correctly the boundaries of named entities. Us-
ing 10-fold cross-validation on the MUC 7 corpus
described above, we experimented with different
machine learning algorithms and parameters (using
WEKA), and using different attributes for training.
5 different algorithms have been evaluated: Zero
R and OneR ? as baselines, Naive Bayes, IBK (an
implementation of K Nearest Neighbour) and J48
(an implementation of a C4.5 decision tree).
As expected, the baseline algorithms performed
very poorly (at around 1%). For IBK small windows
gave low results, while large windows were very in-
efficient. The best results (f-measure of around 60%)
were achieved using the J48 algorithm.
The types of linguistic data used for the attribute
collection included part of speech information, or-
thography (upper case, lower case, initial upper case
letter, mixture of upper and lower case), token kind
(word, symbol, punctuation or number), sentence
boundary, the presence of certain known names and
keywords from the gazetteer lists provided by the
ANNIE system. Tokens were used as instance an-
notations and, for each token, the window used for
collecting the attributes was of size 5 (itself plus two
other tokens in each direction).
Additional information, such as features on a
wider window of tokens, tended to improve the re-
call marginally, but decreased the precision substan-
tially, resulting in a lower F-measure, and therefore
the trade off was not worthwhile.
We also tested the algorithms on a smaller news
corpus (which contained around 68,000 instances as
opposed to 300,000 for the MUC7 corpus). Again,
the J48 algorithm scored highest, with the decision
table and the K nearest neighbour algorithms both
scoring approximately 1 percentage point lower than
the J48.
The second set of experiments was to classify the
named entities identified into the three ENAMEX
categories: Organisations, Persons and Locations.
Using 10-fold cross-validation on the MUC 7 corpus
described above, we experimented with the WEKA
machine learning algorithms and parameters, and
using attributes for training similar to those used for
boundary detection. The best results were achieved
again with the J48 algorithm, and, for this easier
task, they were situated at around 90%. The at-
tributes were chosen on the basis of their informa-
tion gain, calculated using WEKA?s attribute selec-
tion facilities.
The named entity recognition experiments were
performed mainly to evaluate the WEKA ML algo-
rithms on datasets of different sizes, ranging from
small to fairly large ones (300,000 instances). The
different ML algorithms had different memory re-
quirements and execution speed, tested on a PIII
1.5GHz PC running Windows 2000 with 1GB RAM.
From all algorithms tested, the decision table and
decision tree were the slowest (325 and 122 seconds
respectively on 68,000 instances) and required most
memory - up to 800MB on the big datasets. Naive
Bayes was very fast (only 0.25 seconds) with 1R fol-
lowing closely (0.28 seconds).
6 Further Work
OLLIE is very much work-in-progress and there are
several possible improvements we are considering.
When dealing with a particular corpus, it is rea-
sonable to assume that the documents may be quite
similar in terms of subject, genre or style. Because
of that, it is possible that the quality of the user ex-
perience can be improved by simply using a list of
positive and negative examples. This would allow
the system not to make the same mistakes by always
missing a particular example or always annotating a
false positive ? which can be very annoying for the
user.
The big difference in execution time for differ-
ent ML algorithms shows that there are practical
advantages that can be gained from having more
than one ML algorithm integrated in OLLIE, when
it comes to supporting the user with the annotation
task. Since the two experiments showed that Naive
Bayes performs only slightly worse than the best,
but slower algorithms, it may be feasible to train
both a fast Naive Bayes classifier and a slower, but
more precise one. In this way, while the slower ML
algorithm is being re-trained on the latest data, OL-
LIE can choose between using the older model of
this algorithm or the newly re-trained faster base-
line, depending on which ones gives better results,
and suggest annotations for the current document.
As with other such environments, this performance
is measured with respect to the latest annotated doc-
ument.
We hope to be able to integrate more learning
methods, e.g., TiMBL (Daelemans et al, 1998) and
we will also provide support for other people willing
to integrate theirs and make them available from our
OLLIE server or run their own server.
We plan to experiment with other NLP tasks, e.g,
relation extraction, coreference resolution and text
planning for language generation.
Finally, we are working on a Web service imple-
mentation of OLLIE for other distributed, Grid and
e-science applications.
7 Conclusion
OLLIE is an advanced collaborative annotation en-
vironment, which allows users to share and annotate
distributed corpora, supported by adaptive informa-
tion extraction that trains in the background and pro-
vides suggestions.
The option of sharing access to documents with
other users gives several users the possibility to en-
gage in collaborative annotation of documents. For
example, one user can annotate a text with organi-
sations, then another annotate it with locations. Be-
cause the documents reside on the shared server one
user can see errors or questionable markup intro-
duced by another user and initiate a discussion. Such
collaborative annotation is useful in the wider con-
text of creating and sharing language resources (Ma
et al, 2002).
A number of Machine Learning approaches for
Information Extraction have been developed re-
cently, e.g., (Collins, 2002; Bikel et al, 1999), in-
cluding some that use active learning, e.g., (Thomp-
son et al, 1999) or offer automated support, e.g,
(Ciravegna et al, 2002), in order to lower the over-
head of annotating training data. While there ex-
ist corpora used for comparative evaluation, (e.g.,
MUC or the CMU seminar corpus), there is no easy
way to test those ML algorithms on other data, eval-
uate their portability to new domains, or experiment
with different parameters of the models. While some
of the algorithms are available for experimentation,
they are implemented in different languages, require
different data formats, and run on different plat-
forms. All of this makes it hard to ensure experimen-
tal repeatability and eliminate site-specific skew ef-
fects. Also, since not all systems are freely available,
we propose an open, distributed environment where
researchers can experiment with different learning
methods on their own data.
Another advantage of OLLIE is that it defines
a simple API (Application Programming Interface)
which is used by the different ML algorithms to ac-
cess the training data (see Section 3.1). Therefore,
the integration of a new machine learning algorithm
in OLLIE amounts to providing a wrapper that im-
plements this API (a straightforward process). We
have already provided a wrapper for the ML algo-
rithms provided by the WEKA toolkit which can be
used as an example.
Although OLLIE shares features with other adap-
tive IE environments (e.g., (Ciravegna et al, 2002))
and collaborative annotation tools (e.g., (Ma et al,
2002)), it combines them in a unique fashion. In ad-
dition, OLLIE is the only adaptive IE system that al-
lows users to choose which ML approach they want
to use and to comparatively evaluate different ap-
proaches.
References
[Baker et al2002] P. Baker, A. Hardie, T. McEnery,
H. Cunningham, and R. Gaizauskas. 2002. EMILLE,
A 67-Million Word Corpus of Indic Languages: Data
Collection, Mark-up and Harmonisation. In Proceed-
ings of 3rd Language Resources and Evaluation Con-
ference (LREC?2002), pages 819?825.
[Bikel et al1999] D. Bikel, R. Schwartz, and R.M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, Special Issue on Natu-
ral Language Learning, 34(1-3), Feb.
[Bird and Liberman1999] S. Bird and M. Liberman.
1999. A Formal Framework for Linguistic Anno-
tation. Technical Report MS-CIS-99-01, Depart-
ment of Computer and Information Science, Uni-
versity of Pennsylvania. http://xxx.lanl.gov/-
abs/cs.CL/9903003.
[Ciravegna et al2002] F. Ciravegna, A. Dingli, D. Pe-
trelli, and Y. Wilks. 2002. User-System Coop-
eration in Document Annotation Based on Informa-
tion Extraction. In 13th International Conference on
Knowledge Engineering and Knowledge Management
(EKAW02), pages 122?137, Siguenza, Spain.
[Collins2002] M. Collins. 2002. Ranking algorithms for
named entity extraction: Boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia,PA.
[Cunningham et al2002] H. Cunningham, D. Maynard,
K. Bontcheva, and V. Tablan. 2002. GATE: A Frame-
work and Graphical Development Environment for
Robust NLP Tools and Applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
[Daelemans and Hoste2002] Walter Daelemans and
Ve?ronique Hoste. 2002. Evaluation of Machine
Learning Methods for Natural Language Processing
Tasks. In LREC 2002 Third International Conference
on Language Resources and Evaluation, pages 755?
760, CNTS Language Technology Group, University
of Antwerp, UIA, Universiteitsplein 1 (bldng A),
B-2610 Antwerpen, Belgium.
[Daelemans et al1998] W. Daelemans, J. Zavrel,
K. van der Sloot, and A. van den Bosch. 1998. Timbl:
Tilburg memory based learner version 1.0. Technical
Report Technical Report 98-03, ILK.
[Gamba?ck and Olsson2000] B. Gamba?ck and F. Olsson.
2000. Experiences of Language Engineering Algo-
rithm Reuse. In Second International Conference on
Language Resources and Evaluation (LREC), pages
155?160, Athens, Greece.
[Grishman1997] R. Grishman. 1997. TIPSTER Ar-
chitecture Design Document Version 2.3. Techni-
cal report, DARPA. http://www.itl.nist.gov/-
div894/894.02/related projects/tipster/.
[Ide et al2000] N. Ide, P. Bonhomme, and L. Romary.
2000. XCES: An XML-based Standard for Linguis-
tic Corpora. In Proceedings of the Second Interna-
tional Language Resources and Evaluation Confer-
ence (LREC), pages 825?830, Athens, Greece.
[Ma et al2002] X. Ma, H. Lee, S. Bird, and K. Maeda.
2002. Models and tools for collaborative annotation.
In Proceedings of 3rd Language Resources and Evalu-
ation Conference (LREC?2002), Gran Canaria, Spain.
[SAIC1998] SAIC. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-
7). http://www.itl.nist.gov/iaui/894.02/-
related projects/muc/index.html.
[Tablan et al2002] V. Tablan, C. Ursu, K. Bontcheva,
H. Cunningham, D. Maynard, O. Hamza, Tony
McEnery, Paul Baker, and Mark Leisher. 2002. A
unicode-based environment for creation and use of
language resources. In Proceedings of 3rd Language
Resources and Evaluation Conference.
[Thompson et al1999] C. A. Thompson, M. E. Califf, and
R. J. Mooney. 1999. Active learning for natural lan-
guage parsing and information extraction. In Pro-
ceedings of the International Conference on Machine
Learning, pages 406?414.
[Witten and Frank1999] I. H. Witten and E. Frank. 1999.
Data Mining: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kauf-
mann.
  	
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 72?79, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Uneven Margins SVM and Perceptron for Information Extraction
Yaoyong Li, Kalina Bontcheva and Hamish Cunningham
Department of Computer Science, The University of Shefeld, Shefeld, S1 4DP, UK
{yaoyong,kalina,hamish}@dcs.shef.ac.uk
Abstract
The classification problem derived from
information extraction (IE) has an imbal-
anced training set. This is particularly
true when learning from smaller datasets
which often have a few positive training
examples and many negative ones. This
paper takes two popular IE algorithms ?
SVM and Perceptron ? and demonstrates
how the introduction of an uneven margins
parameter can improve the results on im-
balanced training data in IE. Our experi-
ments demonstrate that the uneven margin
was indeed helpful, especially when learn-
ing from few examples. Essentially, the
smaller the training set is, the more bene-
ficial the uneven margin can be. We also
compare our systems to other state-of-the-
art algorithms on several benchmarking
corpora for IE.
1 Introduction
Information Extraction (IE) is the process of auto-
matic extraction of information about pre-specified
types of events, entities or relations from text such
as newswire articles or Web pages. IE is useful in
many applications, such as information gathering in
a variety of domains, automatic annotations of web
pages for Semantic Web, and knowledge manage-
ment.
A wide range of machine learning techniques
have been used for IE and achieved state-of-the-art
results, comparable to manually engineered IE sys-
tems. A learning algorithm usually learns a model
from a set of documents which have been manually
annotated by the user. Then the model can be used
to extract information from new documents. Manual
annotation is a time-consuming process. Hence, in
many cases learning from small data sets is highly
desirable. Therefore in this paper we also evaluate
the performance of our algorithms on small amounts
of training data and show their learning curve.
The learning algorithms for IE can be classified
broadly into two main categories: rule learning and
statistical learning. The former induces a set of
rules from training examples. There are many rule
based learning systems, e.g. SRV (Freitag, 1998),
RAPIER (Califf, 1998), WHISK (Soderland, 1999),
BWI (Freitag and Kushmerick, 2000), and (LP )2
(Ciravegna, 2001). Statistical systems learn a statis-
tical model or classifiers, such as HMMs (Freigtag
and McCallum, 1999), Maximal Entropy (Chieu and
Ng., 2002), the SVM (Isozaki and Kazawa, 2002;
Mayfield et al, 2003), and Perceptron (Carreras et
al., 2003). IE systems also differ from each other
in the NLP features that they use. These include
simple features such as token form and capitalisa-
tion information, linguistic features such as part-of-
speech, semantic information from gazetteer lists,
and genre-specific information such as document
structure. In general, the more features the system
uses, the better performance it can achieve.
This paper concentrates on classifier-based learn-
ing for IE, which typically converts the recognition
of each information entity into a set of classification
problems. In the framework discussed here, two bi-
nary classifiers are trained for each type of informa-
tion entity. One classifier is used for recognising the
entity?s start token and the other ? the entity?s end
token.
72
The classification problem derived from IE usu-
ally has imbalanced training data, in which positive
training examples are vastly outnumbered by neg-
ative ones. This is particularly true for smaller data
sets where often there are hundreds of negative train-
ing examples and only few positive ones. Two ap-
proaches have been studied so far to deal with imbal-
anced data in IE. One approach is to under-sample
majority class or over-sample minority class in order
to obtain a relatively balanced training data (Zhang
and Mani, 2003). However, under-sampling can
potentially remove certain important examples, and
over-sampling can lead to over-fitting and a larger
training set. Another approach is to divide the prob-
lem into several sub-problems in two layers, each of
which has less imbalanced training set than the orig-
inal one (Carreras et al, 2003; Sitter and Daelemans,
2003). The output of the classifier in the first layer is
used as the input to the classifiers in the second layer.
As a result, this approach needs more classifiers than
the original problem. Moreover, the classification
errors in the first layer will affect the performance of
the second one.
In this paper we explore another approach to han-
dle the imbalanced data in IE, namely, adapting
the learning algorithms for balanced classification to
imbalanced data. We particularly study two popular
classification algorithms in IE, Support Vector Ma-
chines (SVM) and Perceptron.
SVM is a general supervised machine learning
algorithm, that has achieved state of the art per-
formance on many classification tasks, including
NE recognition. Isozaki and Kazawa (2002) com-
pared three commonly used methods for named en-
tity recognition ? the SVM with quadratic kernel,
maximal entropy method, and a rule based learning
system, and showed that the SVM-based system per-
formed better than the other two. Mayfield et al
(2003) used a lattice-based approach to named en-
tity recognition and employed the SVM with cubic
kernel to compute transition probabilities in a lattice.
Their results on CoNLL2003 shared task were com-
parable to other systems but were not the best ones.
Previous research on using SVMs for IE adopts
the standard form of the SVM, which treats posi-
tive and negative examples equally. As a result, they
did not consider the difference between the balanced
classification problems, where the SVM performs
quite well, and the imbalanced ones. Li and Shawe-
Taylor (2003) proposes an uneven margins version
of the SVM and shows that the SVM with uneven
margins performs significantly better than the stan-
dard SVM on document classification problems with
imbalanced training data. Since the classification
problem for IE is also imbalanced, this paper inves-
tigates the SVM with uneven margins for IE tasks
and demonstrates empirically that the uneven mar-
gins SVM does have better performance than the
standard SVM.
Perceptron is a simple, fast and effective learn-
ing algorithm, which has successfully been applied
to named entity recognition (Carreras et al, 2003).
The system uses a two-layer structure of classifiers
to handle the imbalanced data. The first layer clas-
sifies each word as entity or non-entity. The second
layer classifies the named entities identified by the
first layer in the respective entity classes. Li et al
(2002) proposed another variant of Perceptron, the
Perceptron algorithm with uneven margins (PAUM),
designed especially for imbalanced data. In this pa-
per we explore the application of PAUM to IE.
The rest of the paper is structured as follows. Sec-
tion 2 describes the uneven margins SVM and Per-
ceptron algorithms. Sections 3.1 and 3.2 discuss
the classifier-based framework for IE and the exper-
imental datasets we used, respectively. We compare
our systems to other state-of-the-art systems on three
benchmark datasets in Section 3.3. Section 3.4 dis-
cusses the effects of the uneven margins parameter
on the SVM and Perceptron?s performances. Finally,
Section 4 provides some conclusions.
2 Uneven Margins SVM and Perceptron
Li and Shawe-Taylor (2003) introduced an uneven
margins parameter into the SVM to deal with imbal-
anced classification problems. They showed that the
SVM with uneven margins outperformed the stan-
dard SVM on document classification problem with
imbalanced training data. Formally, given a training
set Z = ((x1, y1), . . . , (xm, ym)),where xi is the n-
dimensional input vector and yi (= +1 or ?1) its
label, the SVM with uneven margins is obtained by
solving the quadratic optimisation problem:
minw, b, ? ?w,w? + C
m
?
i=1
?i
73
s.t. ?w,xi? + ?i + b ? 1 if yi = +1
?w,xi? ? ?i + b ? ?? if yi = ?1
?i ? 0 for i = 1, ...,m
We can see that the uneven margins parameter
? was added to the constraints of the optimisation
problem. ? is the ratio of negative margin to the
positive margin of the classifier and is equal to 1 in
the standard SVM. For an imbalanced dataset with
a few positive examples and many negative ones, it
would be beneficial to use larger margin for positive
examples than for the negative ones. Li and Shawe-
Taylor (2003) also showed that the solution of the
above problem could be obtained by solving a re-
lated standard SVM problem by, for example, using
a publicly available SVM package1 .
Perceptron is an on-line learning algorithm for
linear classification. It checks the training exam-
ples one by one by predicting their labels. If the
prediction is correct, the example is passed; other-
wise, the example is used to correct the model. The
algorithm stops when the model classifies all train-
ing examples correctly. The margin Perceptron not
only classifies every training example correctly but
also outputs for every training example a value (be-
fore thresholding) larger than a predefined parameter
(margin). The margin Perceptron has better general-
isation capability than the standard Perceptron. Li
et al (2002) proposed the Perceptron algorithm with
uneven margins (PAUM) by introducing two margin
parameters ?+ and ?? into the updating rules for the
positive and negative examples, respectively. Sim-
ilar to the uneven margins parameter in SVM, two
margin parameters allow the PAUM to handle im-
balanced datasets better than both the standard Per-
ceptron and the margin Perceptron. Additionally, it
is known that the Perceptron learning will stop after
limited loops only on a linearly separable training
set. Hence, a regularisation parameter ? is used in
PAUM to guarantee that the algorithm would stop
for any training dataset after some updates. PAUM
is simple and fast and performed very well on doc-
ument classification, in particularly on imbalanced
training data.
1The SVMlight package version 3.5, available from
http://svmlight.joachims.org/, was used to learn the SVM clas-
sifiers in our experiments.
3 Experiments
3.1 Classifier-Based Framework for IE
In the experiments we adopted a classifier-based
framework for applying the SVM and PAUM algo-
rithms to IE. The framework consists of three stages:
pre-processing of the documents to obtain feature
vectors, learning classifiers or applying classifiers to
test documents, and finally post-processing the re-
sults to tag the documents.
The aim of the preprocessing is to form input vec-
tors from documents. Each document is first pro-
cessed using the open-source ANNIE system, which
is part of GATE2 (Cunningham et al, 2002). This
produces a number of linguistic (NLP) features, in-
cluding token form, capitalisation information, to-
ken kind, lemma, part-of-speech (POS) tag, seman-
tic classes from gazetteers, and named entity types
according to ANNIE?s rule-based recogniser.
Based on the linguistic information, an input
vector is constructed for each token, as we iter-
ate through the tokens in each document (includ-
ing word, number, punctuation and other symbols)
to see if the current token belongs to an information
entity or not. Since in IE the context of the token is
usually as important as the token itself, the features
in the input vector come not only from the current
token, but also from preceding and following ones.
As the input vector incorporates information from
the context surrounding the current token, features
from different tokens can be weighted differently,
based on their position in the context. The weight-
ing scheme we use is the reciprocal scheme, which
weights the surrounding tokens reciprocally to the
distance to the token in the centre of the context
window. This reflects the intuition that the nearer
a neighbouring token is, the more important it is
for classifying the given token. Our experiments
showed that such a weighting scheme obtained bet-
ter results than the commonly used equal weighting
of features (Li et al, 2005).
The key part of the framework is to convert the
recognition of information entities into binary clas-
sification tasks ? one to decide whether a token is the
start of an entity and another one for the end token.
After classification, the start and end tags of the
2Available from http://www.gate.ac.uk/
74
entities are obtained and need to be combined into
one entity tag. Therefore some post-processing
is needed to guarantee tag consistency and to try
to improve the results by exploring other informa-
tion. The currently implemented procedure has three
stages. First, in order to guarantee the consistency
of the recognition results, the document is scanned
from left to right to remove start tags without match-
ing end tags and end tags without preceding start
tags. The second stage filters out candidate enti-
ties from the output of the first stage, based on their
length. Namely, a candidate entity tag is removed
if the entity?s length (i.e., the number of tokens) is
not equal to the length of any entity of the same type
in the training set. The third stage puts together all
possible tags for a sequence of tokens and chooses
the best one according to the probability which was
computed from the output of the classifiers (before
thresholding) via a Sigmoid function.
3.2 The Experimental Datasets
The paper reports evaluation results on three corpora
covering different IE tasks ? named entity recogni-
tion (CoNLL-2003) and template filling or scenario
templates in different domains (Jobs and CFP). The
CoNLL-20033 provides the most recent evaluation
results of many learning algorithms on named entity
recognition. The Jobs corpus4 has also been used re-
cently by several learning systems. The CFP corpus
was created as part of the recent Pascal Challenge
for evaluation of machine learning methods for IE5.
In detail, we used the English part of the CoNLL-
2003 shared task dataset, which consists of 946 doc-
uments for training, 216 document for development
(e.g., tuning the parameters in learning algorithm),
and 231 documents for evaluation (i.e., testing), all
of which are news articles taken from the Reuters
English corpus (RCV1). The corpus contains four
types of named entities ? person, location, organ-
isation and miscellaneous names. In the other two
corpora domain-specific information was extracted
into a number of slots. The Job corpus includes 300
computer related job advertisements and 17 slots en-
coding job details, such as title, salary, recruiter,
computer language, application, and platform. The
3See http://cnts.uia.ac.be/conll2003/ner/
4See http://www.isi.edu/info-agents/RISE/repository.html.
5See http://nlp.shef.ac.uk/pascal/.
CFP corpus consists of 1100 conference or work-
shop call for papers (CFP), of which 600 were anno-
tated. The corpus includes 11 slots such as work-
shop and conference names and acronyms, work-
shop date, location and homepage.
3.3 Comparison to Other Systems
Named Entity Recognition The algorithms are
evaluated on the CoNLL-2003 dataset. Since this set
comes with development data for tuning the learning
algorithm, different settings were tried in order to
obtain the best performance on the development set.
Different SVM kernel types, window sizes (namely
the number of tokens in left or right side of the token
at the centre of window), and the uneven margins
parameter ? were tested. We found that quadratic
kernel, window size 4 and ? = 0.5 produced best
results on the development set. These settings were
used in all experiments on the CoNLL-2003 dataset
in this paper, unless otherwise stated. The parameter
settings for PAUM described in Li et al (2002), e.g.
?+ = 50, ?? = 1, were adopted in all experiments
with PAUM, unless otherwise stated.
Table 1 presents the results of our system using
three learning algorithms, the uneven margins SVM,
the standard SVM and the PAUM on the CONLL-
2003 test set, together with the results of three
participating systems in the CoNLL-2003 shared
task: the best system (Florian et al, 2003), the
SVM-based system (Mayfield et al, 2003) and the
Perceptron-based system (Carreras et al, 2003).
Firstly, our uneven margins SVM system per-
formed significantly better than the other SVM-
based system. As the two systems are different from
each other in not only the SVM models used but
also other aspects such as the NLP features and the
framework, in order to make a fair comparison be-
tween the uneven margins SVM and the standard
SVM, we also present the results of the two learning
algorithms implemented in our framework. We can
see from Table 1 that, under the same experimental
settings, the uneven margins SVM again performed
better than the standard SVM.
Secondly, our PAUM-based system performed
slightly better than the system based on voted Per-
ceptron, but there is no significant difference be-
tween them. Note that they adopted different mech-
anisms to deal with the imbalanced data in IE (refer
75
Table 1: Comparison to other systems on CoNLL-2003 corpus: F -measure(%) on each entity type and the
overall micro-averaged F-measure. The 90% confidence intervals for results of other three systems are also
presented. The best performance figures for each entity type and overall appear in bold.
System LOC MISC ORG PER Overall
Our SVM with uneven margins 89.25 77.79 82.29 90.92 86.30
Systems Standard SVM 88.86 77.32 80.16 88.93 85.05
PAUM 88.18 76.64 78.26 89.73 84.36
Participating Best one 91.15 80.44 84.67 93.85 88.76(?0.7)
Systems Another SVM 88.77 74.19 79.00 90.67 84.67(?1.0)
Voted Perceptron 87.88 77.97 80.09 87.31 84.30(?0.9)
to Section 1). The structure of PAUM system is sim-
pler than that of the voted Perceptron system.
Finally, the PAUM system performed worse than
the SVM system. On the other hand, training time
of PAUM is only 1% of that for the SVM and the
PAUM implementation is much simpler than that of
SVM. Therefore, when simplicity and speed are re-
quired, PAUM presents a good alternative.
Template Filling On Jobs corpus our systems
are compared to several state-of-the-art learning sys-
tems, which include the rule based systems Rapier
(Califf, 1998), (LP )2 (Ciravegna, 2001) and BWI
(Freitag and Kushmerick, 2000), the statistical sys-
tem HMM (Freitag and Kushmerick, 2000), and the
double classification system (Sitter and Daelemans,
2003). In order to make the comparison as informa-
tive as possible, the same settings are adopted in our
experiments as those used by (LP )2, which previ-
ously reported the highest results on this dataset. In
particular, the results are obtained by averaging the
performance in ten runs, using a random half of the
corpus for training and the rest for testing. Only ba-
sic NLP features are used: token form, capitalisation
information, token types, and lemmas.
Preliminary experiments established that the
SVM with linear kernel obtained better results than
SVM with quadratic kernel on the Jobs corpus (Li
et al, 2005). Hence we used the SVM with linear
kernel in the experiments on the Jobs data. Note that
PAUM always uses linear kernel in our experiments.
Table 2 presents the results of our systems as well
as the other six systems which have been evaluated
on the Jobs corpus. Note that the results for all the
17 slots are available for only three systems, Rapier,
(LP )2 and double classification, while the results
for some slots were available for the other three sys-
tems. We computed the macro-averaged F1 (the
mean of the F1 of all slots) for our systems as well
as for the three fully evaluated systems in order to
make a comparison of the overall performance.
Firstly, the overall performance of our two sys-
tems is significantly better than the other three fully
evaluated systems. The PAUM system achieves the
best performance on 5 out of the 17 slots. The SVM
system performs best on the other 3 slots. Secondly,
the double classification system had much worse
overall performance than our systems and other two
fully evaluated systems. HMM was evaluated only
on two slots. It achieved best result on one slot but
was much worse on the other slot than our two sys-
tems and some of the others. Finally, somewhat sur-
prisingly, our PAUM system achieves better perfor-
mance than the SVM system on this dataset. More-
over, the computation time of PAUM is about 1/3 of
that of the SVM. Hence, the PAUM system performs
quite satisfactory on the Jobs corpus.
Our systems were also evaluated by participating
in a Pascal challenge ? Evaluating Machine Learn-
ing for Information Extraction. The evaluation pro-
vided not only the CFP corpus but also the linguistic
features for all tokens by pre-processing the docu-
ments. The main purpose of the challenge was to
evaluate machine learning algorithms based on the
same linguistic features. The only compulsory task
is task1, which used 400 annotated documents for
training and other 200 annotated documents for test-
ing. See Ireson and Ciravegna (2005) for a short
overview of the challenge. The learning methods ex-
plored by the participating systems included LP 2,
HMM, CRF, SVM, and a variety of combinations
76
Table 2: Comparison to other systems on the jobs corpus: F1 (%) on each entity type and overall perfor-
mance as macro-averaged F1. Standard deviations for the MA F1 of our systems are presented in parenthe-
sis. The highest score on each slot and overall performance appears in bold.
Slot SVM PAUM (LP )2 Rapier DCs BWI HMM semi-CRF
Id 97.7 97.4 100 97.5 97 100 ? ?
Title 49.6 53.1 43.9 40.5 35 50.1 57.7 40.2
Company 77.2 78.4 71.9 70.0 38 78.2 50.4 60.9
Salary 86.5 86.4 62.8 67.4 67 ? ? ?
Recruiter 78.4 81.4 80.6 68.4 55 ? ? ?
State 92.8 93.6 84.7 90.2 94 ? ? ?
City 95.5 95.2 93.0 90.4 91 ? ? ?
Country 96.2 96.5 81.0 93.2 92 ? ? ?
Language 86.9 87.3 91.0 81.8 33 ? ? ?
Platform 80.1 78.4 80.5 72.5 36 ? ? ?
Application 70.2 69.7 78.4 69.3 30 ? ? ?
Area 46.8 54.0 53.7 42.4 17 ? ? ?
Req-years-e 80.8 80.0 68.8 67.2 76 ? ? ?
Des-years-e 81.9 85.6 60.4 87.5 47 ? ? ?
Req-degree 87.5 87.9 84.7 81.5 45 ? ? ?
Des-degree 59.2 62.9 65.1 72.2 33 ? ? ?
Post date 99.2 99.4 99.5 99.5 98 ? ? ?
MA F1 80.8(?1.0) 81.6(?1.1) 77.2 76.0 57.9 ? ? ?
of different learning algorithms. Firstly, the sys-
tem of the challenge organisers, which is based on
LP 2 obtained the best result for Task1, followed by
one of our participating systems which combined the
uneven margins SVM and PAUM (see Ireson and
Ciravegna (2005)). Our SVM and PAUM systems
on their own were respectively in the fourth and fifth
position among the 20 participating systems. Sec-
ondly, at least six other participating system were
also based on SVM but used different IE framework
and possibly different SVM models from our SVM
system. Our SVM system achieved better results
than all those SVM-based systems, showing that the
SVM models and the IE framework of our system
were quite suitable to IE task. Thirdly, our PAUM
based system was not as good as our SVM system
but was still better than the other SVM based sys-
tems. The computation time of the PAUM system
was about 1/5 of that of our SVM system.
Table 3 presents the per slot results and over-
all performance of our SVM and PAUM systems
as well as the system with the best overall result.
Compared to the best system, our SVM system per-
formed better on two slots and had similar results
on many of other slots. The best system had ex-
tremely good results on the two slots, C-acronym
and C-homepage. Actually, the F1 values of the best
system on the two slots were more than double of
those of every other participating system.
3.4 Effects of Uneven Margins Parameter
A number of experiments were conducted to inves-
tigate the influence of the uneven margins parameter
on the SVM and Perceptron?s performances. Table 4
show the results with several different values of un-
even margins parameter respectively for the SVM
and the Perceptron on two datasets ? CoNLL-2003
and Jobs. The SVM with uneven margins (? < 1.0)
had better results than the standard SVM (? = 1).
We can also see that the results were similar for the ?
between 0.6 and 0.4, showing that the results are not
particularly sensitive to the value of the uneven mar-
gins parameter. The uneven margins parameter has
similar effect on Perceptron as on the SVM. Table 4
shows that the PAUM had better results than both the
standard Perceptron and the margin Perceptron
77
Table 3: Results of our SVM and PAUM systems
on CFP corpus: F-measures(%) on individual entity
type and the overall figures, together with the system
with the highest overall score. The highest score on
each slot appears in bold.
SLOT PAUM SVM Best one
W-name 51.9 54.2 35.2
W-acronym 50.4 60.0 86.5
W-date 67.0 69.0 69.4
W-homepage 69.6 70.5 72.1
W-location 60.0 66.0 48.8
W-submission 70.2 69.6 86.4
W-notification 76.1 85.6 88.9
W-camera-ready 71.5 74.7 87.0
C-name 43.2 47.7 55.1
C-acronym 38.8 38.7 90.5
C-homepage 7.1 11.6 39.3
Micro-average 61.1 64.3 73.4
Our conjecture was that the uneven margins pa-
rameter was more helpful on small training sets, be-
cause the smaller a training set is, the more imbal-
anced it could be. Therefore we carried out exper-
iments on a small numbers of training documents.
Table 5 shows the results of the SVM and the uneven
margins SVM on different numbers of training doc-
uments from CoNLL-2003 and Jobs datasets. The
performance of both the standard SVM and the un-
even margins SVM improves consistently as more
training documents are used. Moreover, compared
to the results one large training sets shown in Table
4, the uneven margins SVM obtains more improve-
ments on small training sets than the standard SVM
model. We can see that the smaller the training set
is, the better the results of the uneven margins SVM
are in comparison to the standard SVM.
4 Conclusions
This paper studied the uneven margins versions of
two learning algorithms ? SVM and Perceptron ? to
deal with the imbalanced training data in IE. Our ex-
periments showed that the uneven margin is helpful,
in particular on small training sets. The smaller the
training set is, the more beneficial the uneven margin
could be. We also showed that the systems based on
the uneven margins SVM and Perceptron were com-
Table 4: The effects of uneven margins parameter
of the SVM and Perceptron, respectively: macro av-
eraged F1(%) on the two datasets CoNLL-2003 (de-
velopment set) and Jobs. The standard deviations for
the Jobs dataset show the statistical significances of
the results. In bold are the best performance figures
for each dataset and each system.
? 1.0 0.8 0.6 0.4 0.2
Conll 89.0 89.6 89.7 89.2 85.3
Jobs 79.0 79.9 81.0 80.8 79.0
?1.4 ?1.2 ?0.9 ?1.0 ?1.3
(?+, ??) (0,0) (1,1) (50,1)
Conll 83.5 83.9 84.4
Jobs 74.1 78.8 81.6
?1.5 ?1.0 ?1.1
parable to other state-of-the-art systems.
Our SVM system obtained better results than
other SVM-based systems on the CoNLL-2003 cor-
pus and CFP corpus respectively, while being sim-
pler than most of them. This demonstrates that our
SVM system is both effective and efficient.
We also explored PAUM, a simple and fast
learning algorithm for IE. The results of PAUM
were somehow worse (about 0.02 overall F-measure
lower) than those of the SVM on two out of three
datasets. On the other hand, PAUM is much faster
to train and easier to implement than SVM. It is also
worth noting that PAUM outperformed some other
learning algorithms. Therefore, even PAUM on its
own would be a good learning algorithm for IE.
Moreover, PAUM could be used in combination with
other classifiers or in the more complicated frame-
work such as the one in Carreras et al (2003).
Since many other tasks in Natural Language Pro-
cessing, like IE, often lead to imbalanced classifica-
tion problems and the SVM has been used widely
in Natural Language Learning (NLL), we can ex-
pect that the uneven margins SVM and PAUM are
likely to obtain good results on other NLL problems
as well.
Acknowledgements
This work is supported by the EU-funded SEKT
project (http://www.sekt-project.org).
78
Table 5: The performances of the SVM system with
small training sets: macro-averaged F1(%) on the
two datasets CoNLL-2003 (development set) and
Jobs. The uneven margins SVM (? = 0.4) is com-
pared to the standard SVM model with even margins
(? = 1). The standard deviations are presented for
results on the Jobs dataset.
size 10 20 30 40 50
? = 0.4
Conll 60.6 66.4 70.4 72.2 72.8
Jobs 51.6 60.9 65.7 68.6 71.1
?2.7 ?2.5 ?2.1 ?1.9 ?2.5
? = 1
Conll 46.2 58.6 65.2 68.3 68.6
Jobs 47.1 56.5 61.4 65.4 68.1
?3.4 ?3.1 ?2.7 ?1.9 ?2.1
References
M. E. Califf. 1998. Relational Learning Techniques for
Natural Language Information Extraction. Ph.D. the-
sis, University of Texas at Austin.
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. Learn-
ing a perceptron-based named entity chunker via on-
line recognition feedback. In Proceedings of CoNLL-
2003, pages 156?159. Edmonton, Canada.
H. L. Chieu and H. T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the Eigh-
teenth National Conference on Artificial Intelligence,
pages 786?791.
F. Ciravegna. 2001. (LP)2, an Adaptive Algorithm for
Information Extraction from Web-related Texts. In
Proceedings of the IJCAI-2001 Workshop on Adaptive
Text Extraction and Mining, Seattle.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics (ACL?02).
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Recognition through Classifier Combi-
nation. In Proceedings of CoNLL-2003, pages 168?
171. Edmonton, Canada.
D. Freigtag and A. K. McCallum. 1999. Information Ex-
traction with HMMs and Shrinkage. In Proceesings
of Workshop on Machine Learnig for Information Ex-
traction, pages 31?36.
D. Freitag and N. Kushmerick. 2000. Boosted Wrapper
Induction. In Proceedings of AAAI 2000.
D. Freitag. 1998. Machine Learning for Information Ex-
traction in Informal Domains. Ph.D. thesis, Carnegie
Mellon University.
N. Ireson and F. Ciravegna. 2005. Pascal Chal-
lenge The Evaluation of Machine Learning
for Information Extraction. In Proceedings of
Dagstuhl Seminar Machine Learning for the
Semantic Web (http://www.smi.ucd.ie/Dagstuhl-
MLSW/proceedings/).
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 390?
396, Taipei, Taiwan.
Y. Li and J. Shawe-Taylor. 2003. The SVM with
Uneven Margins and Chinese Document Categoriza-
tion. In Proceedings of The 17th Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC17), Singapore, Oct.
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and
J. Kandola. 2002. The Perceptron Algorithm with Un-
even Margins. In Proceedings of the 9th International
Conference on Machine Learning (ICML-2002), pages
379?386.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. SVM
Based Learning System For Information Extraction.
In Proceedings of Sheffield Machine Learning Work-
shop, Lecture Notes in Computer Science. Springer
Verlag.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named
Entity Recognition Using Hundreds of Thousands of
Features. In Proceedings of CoNLL-2003, pages 184?
187. Edmonton, Canada.
A. De Sitter and W. Daelemans. 2003. Information ex-
traction via double classification. In Proceedings of
ECML/PRDD 2003 Workshop on Adaptive Text Ex-
traction and Mining (ATEM 2003), Cavtat-Dubrovnik,
Croatia.
S. Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1):233?272.
J. Zhang and I. Mani. 2003. KNN Approach to Un-
balanced Data Distributions: A Case Study Involv-
ing Information Extraction. In Proceedings of the
ICML?2003 Workshop on Learning from Imbalanced
Datasets.
79
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 19?24,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
AnnoMarket: An Open Cloud Platform for NLP
Valentin Tablan, Kalina Bontcheva
Ian Roberts, Hamish Cunningham
University of Sheffield,
Department of Computer Science
211 Portobello, Sheffield, UK
Initial.Surname@dcs.shef.ac.uk
Marin Dimitrov
Ontotext AD
47A Tsarigradsko Shosse, Sofia, Bulgaria
marin.dimitrov@ontotext.com
Abstract
This paper presents AnnoMarket, an open
cloud-based platform which enables re-
searchers to deploy, share, and use lan-
guage processing components and re-
sources, following the data-as-a-service
and software-as-a-service paradigms. The
focus is on multilingual text analysis re-
sources and services, based on an open-
source infrastructure and compliant with
relevant NLP standards. We demonstrate
how the AnnoMarket platform can be used
to develop NLP applications with little
or no programming, to index the results
for enhanced browsing and search, and
to evaluate performance. Utilising Anno-
Market is straightforward, since cloud in-
frastructural issues are dealt with by the
platform, completely transparently to the
user: load balancing, efficient data upload
and storage, deployment on the virtual ma-
chines, security, and fault tolerance.
1 Introduction
Following the Software-as-a-Service (SaaS)
paradigm from cloud computing (Dikaiakos et al,
2009), a number of text processing services have
been developed, e.g. OpenCalais1 and Alchemy
API2. These provide information extraction ser-
vices, accessible programmatically and charged
per number of documents processed.
However, they suffer from two key technical
drawbacks. Firstly, document-by-document pro-
cessing over HTTP is inefficient on large datasets
and is also limited to within-document text pro-
cessing algorithms. Secondly, the text process-
ing algorithms are pre-packaged: it is not pos-
sible for researchers to extend the functional-
1http://www.opencalais.com
2http://www.alchemyapi.com
ity (e.g. adapt such a service to recognise new
kinds of entities). Additionally, these text pro-
cessing SaaS sites come with daily rate limits,
in terms of number of API calls or documents
that can be processed. Consequently, using these
services for research is not just limited in terms
of text processing functionality offered, but also
quickly becomes very expensive on large-scale
datasets. A moderately-sized collection of tweets,
for example, comprises small but numerous docu-
ments, which can lead to unfeasibly high process-
ing costs.
Platform-as-a-Service (PaaS) (Dikaiakos et al,
2009) are a type of cloud computing service which
insulates developers from the low-level issues of
utilising cloud infrastructures effectively, while
providing facilities for efficient development, test-
ing, and deployment of software over the Inter-
net, following the SaaS model. In the context
of traditional NLP research and development, and
pre-dating cloud computing, similar needs were
addressed through NLP infrastructures, such as
GATE (Cunningham et al, 2013) and UIMA (Fer-
rucci and Lally, 2004). These infrastructures ac-
celerated significantly the pace of NLP research,
through reusable algorithms (e.g. rule-based pat-
tern matching engines, machine learning algo-
rithms), free tools for low-level NLP tasks, and
support for multiple input and output document
formats (e.g. XML, PDF, DOC, RDF, JSON).
This demonstration introduces the AnnoMar-
ket3 open, cloud-based platform, which has
been developed following the PaaS paradigm.
It enables researchers to deploy, share, and
use language processing components and re-
sources, following the Data-as-a-Service (DaaS)
and Software-as-a-Service (SaaS) paradigms. It
gives researchers access to an open, standard-
compliant NLP infrastructure and enables them
3At the time of writing, a beta version of AnnoMarket is
available at http://annomarket.com
19
to carry out large-scale NLP experiments by har-
nessing the vast, on-demand compute power of
the Amazon cloud. It supports not only NLP al-
gorithm development and execution, but also on-
demand collaborative corpus annotation and per-
formance evaluation. Important infrastructural is-
sues are dealt with by the platform, completely
transparently for the researcher: load balancing,
efficient data upload and storage, deployment on
the virtual machines, security, and fault tolerance.
AnnoMarket differs from previous work (e.g.
(Zhou et al, 2010; Ramakrishnan et al, 2010))
in that it requires no programming in order to
run a GATE-compliant NLP application on a large
dataset. In that sense, it combines the ease of
use of an NLP SaaS with the openness and com-
prehensive facilities of the GATE NLP infras-
tructure. AnnoMarket offers a growing number
of pre-packaged services, in multiple languages.
Additionally, as a specialised NLP PaaS, it also
supports a bring-your-own-pipeline option, which
can be built easily by reusing pre-existing GATE-
compatible NLP components and adding some
new ones. Moreover, in addition to offering entity
extraction services like OpenCalais, our NLP PaaS
also supports manual corpus annotation, semantic
indexing and search, and performance evaluation.
The contributions of this paper are as follows:
1. A demonstration of running AnnoMarket
multilingual NLP services on large datasets,
without programming. The new service
deployment facilities will also be shown,
including how services can optionally be
shared with others.
2. A demonstration on shared research corpora
via the AnnoMarket platform, following the
data-as-a-service model (the sharer is respon-
sible for ensuring no copyright violations).
3. A demonstration of the large-scale search and
browsing interface, which uses the results of
the NLP SaaS to offer enhanced, semantic-
based functionality.
2 The AnnoMarket NLP PaaS
This section first discusses the methodology
underpinning the AnnoMarket platform, then
presents its architecture and key components.
2.1 Development and Deployment
Methodology
The development of text analysis algorithms and
pipelines typically follows a certain methodolog-
ical pattern, or lifecycle. A central problem is
to define the NLP task, such that human anno-
tators can perform it with a high level of agree-
ment and to create high quality training and evalu-
ation datasets. It is common to use double or triple
annotation, where several people perform the an-
notation task independently and we then measure
their level of agreement (Inter-Annotator Agree-
ment, or IAA) to quantify and control the quality
of this data (Hovy, 2010).
The AnnoMarket platform was therefore de-
signed to offer full methodological support for all
stages of the text analysis development lifecycle:
1. Create an initial prototype of the NLP
pipeline, testing on a small document collec-
tion, using the desktop-based GATE user in-
terface (Cunningham et al, 2002);
2. If required, collect a gold-standard corpus for
evaluation and/or training, using the GATE
Teamware collaborative corpus annotation
service (Bontcheva et al, 2013), running in
AnnoMarket;
3. Evaluate the performance of the automatic
pipeline on the gold standard (either locally
in the GATE development environment or on
the cloud). Return to step 1 for further devel-
opment and evaluation cycles, as needed.
4. Upload the large datasets and deploy the NLP
pipeline on the AnnoMarket PaaS;
5. Run the large-scale NLP experiment and
download the results as XML or a standard
linguistic annotation format (Ide and Ro-
mary, 2004). AnnoMarket alo offers scal-
able semantic indexing and search over the
linguistic annotations and document content.
6. Analyse any errors, and if required, iterate
again over the earlier steps.
AnnoMarket is fully compatible with the GATE
open-source architecture (Cunningham et al,
2002), in order to benefit from GATE?s numerous
reusable and multilingual text processing compo-
nents, and also from its infrastructural support for
linguistic standards and diverse input formats.
2.2 Architecture
The architecture of the AnnoMarket PaaS com-
prises of four layers (see Figure 1), combining
20
Figure 1: The AnnoMarket Architecture
components with related capabilities. Addition-
ally, we have identified three aspects, which span
across multiple layers.
The Data Layer is described in Section 2.3, the
Platform Layer ? in Section 2.4, and the Annota-
tion Services ? in Section 2.5.
The fourth, web user interface layer, contains a
number of UI components that allow researchers
to use the AnnoMarket platform in various ways,
e.g. to run an already deployed text annotation ser-
vice on a large dataset, to deploy and share a new
service on the platform, or to upload (and option-
ally share) a document collection (i.e. a corpus).
There is also support for finding relevant services,
deployed on the AnnoMarket platform. Lastly,
due to the platform running on the Amazon cloud
infrastructure, there are account management in-
terfaces, including billing information, payments,
and usage reports.
The first vertical aspect is cloud deployment on
Amazon. This covers support for automatic up and
down-scaling of the allocated Amazon resources,
detection of and recovery from Amazon infras-
tructure failures and network failures, and data
backup.
Usage monitoring and billing is the second
key vertical aspect, since fine-grained pay-as-
you-go ability is essential. Even in the case of
freely-available annotations services, Amazon us-
age charges are incurred and thus such function-
ality is needed. Various usage metrics are mon-
itored and metered so that proper billing can be
guaranteed, including: storage space required by
language resources and data sets; CPU utilisation
of the annotation services; number and size of doc-
uments processed.
Security aspects also have impact on all the lay-
ers of the AnnoMarket platform:
? Data Layer ? data encryption and access con-
trol;
? Platform Layer ? data encryption, authentica-
tion and access control;
? Service layer ? authentication and transport
level encryption;
? User Interface layer ? authentication and
transport level encryption.
In addition, we have implemented a REST pro-
gramming API for AnnoMarket, so that data up-
load and download and running of annotation ser-
vices can all be done automatically, outside of
the web interface. This allows tighter integration
within other applications, as well as support for
synchronous (i.e. document-by-document) calling
of the annotation services.
2.3 The Data Layer
The Data Layer stores various kinds of content,
e.g. crawled web content, users? own corpora (pri-
vate or shared with others), results from running
the annotation services, etc.
Input documents can be in all major formats
(e.g., XML, HTML, JSON, PDF, DOC), based
on GATE?s comprehensive format support. In all
cases, when a document is being processed by An-
noMarket, the format is analysed and converted
into a single unified, graph-based model of an-
notation: the one of the GATE NLP framework
(Cunningham et al, 2002). Then this internal an-
notation format is also used by the collaborative
corpus annotation web tool, and for annotation in-
dexing and search. Annotations produced can be
exported as in-line or stand-off XML, including
XCES (Ide and Romary, 2004).
In implementation terms, Amazon S3 is used to
store content on the platform. S3 provides a REST
service for content access, as well as direct HTTP
access, which provides an easy way for AnnoMar-
ket users to upload and download content.
While stored on the cloud, data is protected by
Amazon?s security procedures. All transfers be-
tween the cloud storage, the annotation services,
and the user?s computer are done via an encrypted
channel, using SSL.
2.4 The Platform Layer
The AnnoMarket platform provides an environ-
ment where text processing applications can be de-
ployed as annotation services on the cloud. It al-
lows processing pipelines that were produced on a
21
Figure 2: Web-based Job Editor
developer?s stand-alone computer to be deployed
seamlessly on distributed hardware resources (the
compute cloud) with the aim of processing large
amounts of data in a timely fashion. This process
needs to be resilient in the face of failures at the
level of the cloud infrastructure, the network com-
munication, errors in the processing pipeline and
in the input data.
The platform layer determines the optimal num-
ber of virtual machines for running a given NLP
application, given the size of the document collec-
tion to be processed and taking into account the
overhead in starting up new virtual machines on
demand. The implementation is designed to be ro-
bust in the face of hardware failures and process-
ing errors. For technical details on the way this
was implemented on Amazon EC2 see (Tablan et
al., 2013).
The GATE plugin-based architecture (Cunning-
ham et al, 2002) is the basis for the platform en-
vironment. Users can upload any pipelines com-
pliant with the GATE Processing Resource (PR)
model and these are automatically deployed as an-
notation services on the AnnoMarket platform.
2.5 Annotation Services
As discussed above, the platform layer in An-
noMarket addresses most of the technical and
methodological requirements towards the NLP
PaaS, making the deployment, execution, and
sharing of annotation services (i.e. pipelines and
algorithms) a straightforward task. From a re-
searcher?s perspective, executing an annotation
service on a dataset involves a few simple steps:
? Upload the document collection to be pro-
cessed or point the system to a shared dataset
on the platform;
? Upload a GATE-based processing pipeline to
be used (or choose an already deployed anno-
tation service);
? Set any required parameter values;
? Press the ?Start? button.
While the job is running, a regularly updated
execution log is made available in the user?s dash-
board. Upon job completion, an email notification
is also sent. Most of the implementation details are
hidden away from the user, who interacts with the
system through a web-based job editor, depicted
in Figure 2, or through a REST API.
The number of already deployed annotation ser-
vices on the platform is growing continuously.
Figure 3 shows a subset of them, as well as the
metadata tags associated with these services, so
that users can quickly restrict which types of ser-
vices they are after and then be shown only the
relevant subset. At the time of writing, there are
services of the following kinds:
? Part-of-Speech-Taggers for English, German,
Dutch, and Hungarian.
? Chunking: the GATE NP and VP chunkers
and the OpenNLP ones;
? Parsing: currently the Stanford Parser 4, but
more are under integration;
? Stemming in 15 languages, via the Snowball
stemmer;
? Named Entity Recognition: in English, Ger-
man, French, Arabic, Dutch, Romanian, and
Bulgarian;
? Biomedical taggers: the PennBio5 and the
AbGene (Tanabe and Wilbur, 2002) taggers;
? Twitter-specific NLP: language detection, to-
kenisation, normalisation, POS tagging, and
4http://nlp.stanford.edu/software/lex-parser.shtml
5http://www.seas.upenn.edu/?strctlrn/BioTagger/BioTagger.html
22
Figure 3: Pre-deployed Text Annotation Services
Figure 4: Creating a New Annotation Service
NER.
The deployment of new annotation services is
done via a web interface (see Figure 4), where an
administrator needs to configure some basic de-
tails related to the utilisation of the platform layer
and provide a self-contained GATE-compatible
application. Platform users can only publish their
own annotation services by contacting an adminis-
trator, who can validate the provided pipeline be-
fore making it publicly available to the other users.
This step is intended to protect the users commu-
nity from malicious or poor quality pipelines.
3 Search and Browsing of Annotated
Corpora
The AnnoMarket platform also includes a service
for indexing and searching over a collection of se-
mantically annotated documents. The output of an
annotation service (see Figure 2) can be fed di-
rectly into a search index, which is created as the
service is run on the documents. This provides fa-
cilities for searching over different views of doc-
ument text, for example one can search the docu-
ment?s words, the part-of-speech of those words,
or their morphological roots. As well as searching
the document text, we also support searches over
the documents? semantic annotations, e.g. named
entity types or semantic roles.
Figure 5 shows a semantic search over 80,000
news web pages from the BBC. They have
first been pre-processed with the POS tagging,
morphological analysis, and NER services on
the platform and the output indexed automat-
ically. The search query is for documents,
where entities of type Person are followed by
any morphological form of the verb say, i.e.
{Person} root:say.
4 Conclusion
This paper described a cloud-based open platform
for text mining, which aims to assist the develop-
ment and deployment of robust, large-scale text
processing applications. By supporting the shar-
ing of annotation pipelines, AnnoMarket alo pro-
23
Figure 5: Example Semantic Search Results
motes reuse and repeatability of experiments.
As the number of annotation services offered by
the platform has grown, we identified a need for
service search, so that users can locate useful NLP
services more effectively. We are currently devel-
oping a new UI, which offers search and brows-
ing functionality, alongside various criteria, such
as functionality (e.g. POS tagger, named entity
recogniser), user ratings, natural language sup-
ported). In the medium- to long-term we have
also planned to support UIMA-based pipelines,
via GATE?s UIMA compatibility layer.
A beta version is currently open to researchers
for experimentation. Within the next six months
we plan to to solicit more shared annotation
pipelines to be deployed on the platform by other
researchers.
Acknowledgments
This work was supported by the European Union
under grant agreement No. 296322 AnnoMarket,6
and a UK EPSRC grant No. EP/I004327/1.
References
Kalina Bontcheva, Hamish Cunningham, Ian Roberts,
Angus. Roberts, Valentin. Tablan, Niraj Aswani, and
Genevieve Gorrell. 2013. GATE Teamware: A
Web-based, Collaborative Text Annotation Frame-
work. Language Resources and Evaluation.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. Gate: an
architecture for development of robust hlt applica-
tions. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, 7?12
July 2002, ACL ?02, pages 168?175, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Hamish Cunningham, Valentin Tablan, Angus Roberts,
and Kalina Bontcheva. 2013. Getting more out of
biomedical documents with gate?s full lifecycle open
6See http://www.annomarket.eu/.
source text analytics. PLoS Computational Biology,
9(2):e1002854, 02.
Marios D Dikaiakos, Dimitrios Katsaros, Pankaj
Mehra, George Pallis, and Athena Vakali. 2009.
Cloud computing: Distributed internet computing
for IT and scientific research. IEEE Internet Com-
puting, 13(5):10?13.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Eduard Hovy. 2010. Annotation. In Tutorial Abstracts
of ACL.
Nancy Ide and Laurent Romary. 2004. Standards for
language resources. Natural Language Engineer-
ing, 10:211?225.
C. Ramakrishnan, W. A. Baumgartner, J. A. Blake,
G. A. P. C. Burns, K. Bretonnel Cohen, H. Drabkin,
J. Eppig, E. Hovy, C. N. Hsu, L. E. Hunter, T. Ingulf-
sen, H. R. Onda, S. Pokkunuri, E. Riloff, C. Roeder,
and K. Verspoor. 2010. Building the scientific
knowledge mine (SciKnowMine): a community-
driven framework for text mining tools in direct ser-
vice to biocuration. In New Challenges for NLP
Frameworks (NLPFrameworks 2010), LREC 2010,
pages 9?14, Valletta, Malta, May. ELRA.
Valentin Tablan, Ian Roberts, Hamish Cunningham,
and Kalina Bontcheva. 2013. GATECloud.net: a
Platform for Large-Scale, Open-Source Text Pro-
cessing on the Cloud. Philosophical Transactions
of the Royal Society A: Mathematical, Physical &
Engineering Sciences, 371(1983):20120071.
Lorraine Tanabe and W. John Wilbur. 2002. Tag-
ging Gene and Protein Names in Full Text Articles.
In Proceedings of the ACL-02 workshop on Natural
Language Processing in the biomedical domain, 7?
12 July 2002, volume 3, pages 9?13, Philadelphia,
PA. Association for Computational Linguistics.
Bin Zhou, Yan Jia, Chunyang Liu, and Xu Zhang.
2010. A distributed text mining system for online
web textual data analysis. In Cyber-Enabled Dis-
tributed Computing and Knowledge Discovery (Cy-
berC), 2010 International Conference on, pages 1?
4, Los Alamitos, CA, USA, October. IEEE Com-
puter Society.
24
