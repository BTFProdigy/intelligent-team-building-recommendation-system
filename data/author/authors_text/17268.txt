Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64?72, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
JU_CSE: A CRF Based Approach to Annotation of Temporal Expres-
sion, Event and Temporal Relations 
 
 
Anup Kumar Kolya1, Amitava Kundu1, 
 Rajdeep Gupta1  
Asif Ekbal2, Sivaji Bandyopadhyay1 
1Dept. of Computer Science & Engineering 2Dept. of Computer Science & Engineering 
Jadavpur Univeristy IIT Patna 
Kolkata-700 032, India Patna-800 013, India 
{anup.kolya,amitava.jucse, 
rajdeepgupta20}@gmail.com 
asif@iitp.ac.in, 
sivaji_ju_cse@yahoo.com 
 
 
 
 
 
Abstract 
In this paper, we present the JUCSE system, 
designed for the TempEval-3 shared task. The 
system extracts events and temporal infor-
mation from natural text in English. We have 
participated in all the tasks of TempEval-3, 
namely Task A, Task B & Task C. We have 
primarily utilized the Conditional Random 
Field (CRF) based machine learning tech-
nique, for all the above tasks. Our system 
seems to perform quite competitively in Task 
A and Task B. In Task C, the system?s per-
formance is comparatively modest at the ini-
tial stages of system development. We have 
incorporated various features based on differ-
ent lexical, syntactic and semantic infor-
mation, using Stanford CoreNLP and Wordnet 
based tools. 
1 Introduction 
Temporal information extraction has been a popu-
lar and interesting research area of Natural Lan-
guage Processing (NLP) for quite some time. 
Generally, a lot of events are described in a variety 
of newspaper texts, stories and other important 
documents where the different events described 
happen at different time instants. The temporal 
location and ordering of these events are either 
specified or implied. Automatic identification of 
time expressions and events and annotation of 
temporal relations constitute an important task in 
text analysis. These are also important in a wide 
range of NLP applications that include temporal 
question answering, machine translation and doc-
ument summarization.  
A lot of research in the area of temporal infor-
mation extraction has been conducted on multiple 
languages, including English and several European 
languages. The TimeML was first developed in 
2002 in an extended workshop called TERQAS 
(Time and Event Recognition for Question An-
swering Systems) and, in 2003, it was further de-
veloped in the context of the TANGO workshop 
(TimeML Annotation Graphical Organizer). Since 
then most of the works in this research arena have 
been conducted in English. The variety of works 
include TimeML (Pustejovsky et al, 2003), the 
development of a temporally annotated corpus 
Time-Bank (Pustejovsky et al, 2003), the temporal 
evaluation challenges TempEval-1 (Verhagen et 
al., 2007), TempEval-2 (Pustejovsky and Verha-
gen, 2010). In the series of Message Understanding 
Conferences (MUCs) that started from 1987 and 
the Sheffield Temporal Annotation scheme 
(STAG) (Setzer &Gaizauskas, 2000) the  aim  was 
to identify events in news text and determine their 
relationship with points on a temporal line. 
In the series of TempEval evaluation exercises, 
TempEval-1 was the first one where the focus was 
on identification of three types of temporal rela-
tion: relation between an event and a time expres-
sion in the same sentence, relation between an 
64
event and the document creation time, and relation 
between two main events in consecutive sentences. 
 TempEval-2 was a follow up to TempEval-1 
and consisted of six subtasks rather than three. It 
added (i) identification of time expressions and 
determination of values of the attributes TYPE and 
VAL (ii) identification of event expressions and 
determination of its attribute values. It included the 
previous three relation tasks from TempEval-1 and 
an additional task of annotating temporal relation 
between a pair of events where one subordinates 
the other.  
We have participated in all three tasks of 
TempEval-3- Task A, Task B and Task C. A com-
bination of CRF based machine learning and rule 
based techniques has been adopted for temporal 
expression extraction and determination of attrib-
ute values of the same   (Task A). We have used a 
CRF based technique for event extraction (Task 
B), with the aid of lexical, semantic and syntactic 
features. For determination of event attribute val-
ues we have used simple rule based techniques. 
Automatic annotation of temporal relation between 
event-time in the same sentence, event-DCT rela-
tions, mainevent-mainevent relations in consecu-
tive sentences and subevent-subevent relations in 
the same sentences has been introduced as a new 
task (Task-C) in the TempEval-3 exercise. We 
have adopted a CRF based technique for the same 
as well. 
2 The JU_CSE System Approach  
The JU_CSE system for the TempEval-3 shared 
task uses mainly a Conditional Random Field 
(CRF) machine learning approach to achieve Task 
A, Task B & Task C. The workflow of our system 
is illustrated in Figure 1. 
2.1 Task A: Temporal Expression Identifica-
tion and Normalization 
Temporal Expression Identification: 
 We have used CRF++ 0.571, an open source im-
plementation of the Conditional Random Field 
(CRF) machine learning classifier for our experi-
ments. CRF++ templates have been used to capture 
the relation between the different features in a se-
quence to identify temporal expressions. Temporal 
                                                        
1 http://crfpp.googlecode.com/svn/trunk/doc/index.html 
expressions mostly appear as multi-word entities 
such as ?the next three days?. Therefore the use of 
CRF classifier that uses context information of a 
token seemed most appropriate.  
 Initially, all the sentences have been changed to 
a vertical token-by-token level sequential structure 
for temporal expressions representation by a B-I-O 
encoding, using a set of mostly lexical features. In 
this encoding of temporal expression, ?B? indi-
cates the ?beginning of sequence?, ?I? indicates a 
token inside a sequence and ?O? indicates an out-
side word. We have carefully chosen the features 
list based on the several entities that denote month 
names, year, weekdays, various digit expressions 
(day, time, AM, PM etc.) In certain temporal ex-
pression patterns (several months, last evening) 
some words (several, last) act as modifiers to the 
following words that represent the time expression. 
Temporal expressions include time expression 
modifiers, relative days, periodic temporal set, 
year-eve day, month name with their short pattern 
forms, season of year, time of day, decade list and 
so on. We have used the POS information of each 
token as a feature. We have carefully accounted for 
a simple intuition revelation that most temporal 
expressions contain some tokens conveying the 
?time? information while others possibly convey-
ing the ?quantity? of time. For example, in the ex-
pression ?next three days?, ?three? quantifies 
?days?. Following are the different temporal ex-
pressions lists that have been utilized: 
 
? A list of time expression modifiers: this, 
mid, recent, earlier, beginning, late etc. 
? A list of relative days: yesterday, tomor-
row etc. 
? A list of periodic temporal set: hourly, 
nightly etc. 
? A list of year eve day: Christmas Day, 
Valentine Day etc. 
? A list of month names with their short pat-
tern forms: April, Apr. etc. 
? A list of season of year: spring, winter etc. 
? A list of time of day: morning, afternoon, 
evening etc. 
? A list of decades list: twenties, thirties etc. 
 
 
65
  
  
Raw Text: 
For his part, Fidel Castro is the ultimate political 
survivor. People have predicted his demise so 
many times, and the US has tried to hasten it on 
several occasions. Time and again, he endures.  
? Tokenize with Stanford CoreNLP 
? Obtain POS tags of tokens 
? Extract features from tokens 
? Identify the features for event annotation and 
temporal annotation separately 
 
CRF  
 
Event & 
Time 
 Features 
T
ag E
V
E
N
T
 
tokens 
Tag 
TIMEX3 
tokens 
. 
       For???  OTHERS 
  nearly ???.. TIMEX3 
       forty?. ?  TIMEX3 
years??.. TIMEX3 
. 
. 
 
. 
People???  OTHERS 
have ???..   OTHERS 
      predicted ?. ?  EVENT 
his ????.. OTHERS 
. 
. 
Annotated Text 
 
For his part, Fidel Castro is the ultimate political survivor. 
People have <EVENT class="I_ACTION" 
eid="e1">predicted</EVENT> his <EVENT 
class="OCCURRENCE" eid="e2">demise</EVENT> so 
many times, and the US has <EVENT class="I_ACTION" 
eid="e3">tried</EVENT> to <EVENT 
class="OCCURRENCE" eid="e4">hasten</EVENT> it on 
several occasions. 
D
eterm
ine 
E
vent 
C
lass 
CoreNLP 
for ?type? 
& ?velue? 
<MAKEINSTANCE eiid="ei1? eventID="e1" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei2? eventID="e2" pos="NOUN" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei3? eventID="e3" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
R
ule based approach to obtain tense, as-
pect, polarity, m
odality etc. for events 
 
Enlist entity pairs with features 
<mainevent-mainevent> 
<event-event> 
<event-dct>  
<event-time> 
 
 
CRF  
 
Temporal Relations: 
 
<TLINK lid="l1" relType="BEFORE" 
eventInstanceID="ei1" relatedTo-
Time="t0" /> 
 
<TLINK lid="l2" relType="BEFORE" 
eventInstanceID="ei2" relatedToEven-
tInstance="ei1" /> 
Figure 1.The JU_CSE System Architecture 
66
Determination of Normalized value and type 
of Temporal Expressions: 
 Temporal expressions in documents are generally 
defined with the type and value attributes. All the 
temporal expressions can be differentiated into 
three types (i) explicit (ii) relative and (iii) implicit 
temporal expressions. For example, the expression 
?October 1998? refers to a specific month of the 
year which can be normalized without any addi-
tional information. On the other hand, the relative 
expression ?yesterday? can?t be normalized with-
out the knowledge of a corresponding reference 
time. The reference time can either be a temporal 
expression or the Document Creation Time marked 
in the document. Consider the following piece of 
text: ?Yesterday was the 50th independence of In-
dia?. The First Independence Day of India is 15th 
august 1947.? Here ?Yesterday? can be normal-
ized as ?15-08-1997?. It may be noted that infor-
mation such as ?First Independence Day of India? 
can be directly accessed from the timestamp calen-
dar, through the metadata of a document. The third 
type of temporal expressions includes implicit ex-
pressions such as names of festival days, birthdays 
and holidays or events. These expressions are 
mapped to available calendar timeline to find out 
their normalized values. 
 
Temporal 
Expression 
Type Value 
A couple of 
years 
 
DURATION P2Y 
October DATE ?1997-10? 
Every day SET P1D 
2 P.M. TIME 2013-02-01T14:00 
Now DATE PRESENT_REF" 
Table 1: TimeML normalized type and value attributes 
for temporal expressions 
 
We have implemented a combined technique us-
ing our handcrafted rules and annotations given by 
the Stanford CoreNLP tool to determine the ?type?-
s and ?value?-s. Four types TIME, DATE, 
DURATION and SET of temporal expressions are 
defined in the TimeML framework. Next, we have 
evaluated the normalized value of temporal expres-
sions using Document Creation Time (DCT) from 
the documents.  In this way, values of different 
dates have been inferred e.g. last year, Monday, 
and today. 
2.2 Task B: Extraction of Event Words and 
Determination of Event Attribute Values  
Event Extraction 
In our evaluation framework, we have used the 
Stanford CoreNLP tool extensively to tokenize, 
lemmatize, named-entity annotate and part-of-
speech tag the text portions of the input files. For 
event extraction, the features have been considered 
at word level, where each word has its own set of 
features. The general features used to train our 
CRF model are: 
Morphological Features: Event words are rep-
resented mostly as verbs and nouns. The major 
problem is detecting the events having non-verbal 
PoS labels. Linguistically, non-verbal wordforms 
are derived from verbal wordforms. Various inflec-
tional and derivational morphological rules are 
involved in the process of evolving from verbal to 
non-verbal wordforms. We have used a set of 
handcrafted rules to identify the suffixes such as (?-
ci?n?, ?-tion? or ?-ion?), i.e., the morphological 
markers of word token, where Person, Location 
and Organization words are not considered. The 
POS and lemma, in a 5-window (-2, +2), has been 
used for event extraction. 
Syntactic Feature: Different event words no-
tions are contained in the sentences such as: verb-
noun combinations structure, the complements of 
aspectual prepositional phrases (PPs) headed by 
prepositions and a particular type of complex 
prepositions. These notions are captured to be used 
as syntactic features for event extraction. 
WordNet Feature: The RiTa Wordnet2 package 
has been effectively used to extract different prop-
erties of words, such as Synonyms, Antonyms, 
Hypernyms, & Hyponyms, Holonyms, Meronyms, 
Coordinates, & Similars, Nominalizations, Verb-
Groups, & Derived-terms. We have used these 
Wordnet properties in the training file for the CRF 
in the form of binary features for verbs and nouns 
indicating if  the words like ?act?, ?activity?, ?phe-
nomenon? etc. occur  in different relations of the 
Wordnet ontology. 
                                                        
2 http://www.rednoise.org/rita/wordnet/documentation/ 
67
Features using Semantic Roles: We use Se-
mantic Role Label (SRL) (Gildea et el, 2002; Pra-
dhan et al 2004; Gurevich et al 2006) to identify 
different useful features for event extraction. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents; determine 
their arguments (agent, patient, etc.) and adjuncts 
(locative, temporal, etc.). Some of the other fea-
tures like predicate, voice and verb sub-
categorization are shared by all the nodes in the 
tree. In the present work, we use predicate as an 
event.  Semantic roles can be used to detect the 
events that are nominalizations of verbs such as 
agreement for agree or construction for construct.  
Event nominalizations often share the same seman-
tic roles as verbs, and often replace them in written 
language. Noun words, morphologically derived 
from verbs, are commonly defined as deverbal 
nouns. Event and result nominalizations constitute 
the bulk of deverbal nouns. The first class refers to 
an event/activity/process, with the nominal ex-
pressing this action (e.g., killing, destruction etc.). 
Nouns in the second class describe the result or 
goal of an action (e.g., agreement, consensus etc.). 
Many nominals denote both the event and result 
(e.g., selection). A smaller class is agent/patient 
nominalizations that are usually identified by suf-
fixes such as -er, -or etc., while patient nominaliza-
tions end with -ee, -ed (e.g. employee).   
Object information of Dependency Relations 
(DR): We have developed handcrafted rules to 
identify features for CRF training, based on the 
object information present in the dependency rela-
tions of parsed sentences. Stanford Parser (de 
Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different Part-of-Speech 
(PoS) tags of Penn Treebank is used to get the 
parsed sentences with dependency relations. The 
dependency relations are found out for the predi-
cates ?dobj? so that the direct object related com-
ponents in the ?dobj? predicate is considered as the 
feature for the event expression. Initially the input 
sentences are passed to the dependency parser3.  
From the parsed output verb noun combination 
direct object (dobj) dependency relations are ex-
tracted. These dobj relations basically inform us 
that direct object of a VP is the noun phrase which 
is the (accusative) object of the verb; the direct 
object of a clause is the direct object of the VP 
                                                        
3 http://nlp.stanford.edu:8080/parser/ 
which is the predicate of that clause. Within the 
dobj relation governing verb word and dependent 
noun words are acting as important features for 
event identification when dependent word is not 
playing any role in other dependency relation 
(nsubj, prep_of, nn ,etc.) of the sentence. 
 
In this way, we have set list of word tokens and 
its features to train the recognition model. Then the 
model will give to each word one of the valid la-
bels.  
Determination of various Event Attribute 
Values: 
Values of different event attributes have been 
computed as follows: 
Class: Identification of the class of an event has 
been done using a simple, intuitive, rule based ap-
proach. Here too, the hypernym list of an event 
token from RitaWordnet has been deployed to de-
termine the class of the respective event. In this 
case, OCCURRENCE has been considered the de-
fault class. 
Tense, Aspect, POS: These three attributes are 
the obligatory attributes of MAKEINSTANCE 
tags. To determine the tense, aspect and polarity of 
an event, we have used the ?parse? annotator in 
CoreNLP. We annotated each sentence with the 
Stanford dependency relations using the above an-
notator. Thereafter various specific relations were 
used to determine the tense, aspect and POS of an 
event token, with another rule based approach. For 
example, in the phrase ?has been abducted?, the 
token ?been? appears as the dependent in an ?aux? 
relation with the event token ?abducted?; and 
hence the aspect ?PERFECTIVE? is inferred. The 
value ?NONE? has been used as the default value 
for both tense and aspect. 
Polarity and Modality: Polarity of event tokens 
are determined using Stanford dependency rela-
tions too; here the ?neg? relation. To determine the 
modality we search for modal words in ?aux? rela-
tions with the event token. 
2.3 Task C: Temporal Relation Annotation 
We have used the gold-standard TimeBank fea-
tures for events and times for training the CRF. In 
the present work, we mainly use the various com-
binations of the following features:  
68
 
(i)  Part of Speech (POS) 
(ii)  Event Tense 
(iii)  Event Aspect 
(iv)  Event Polarity 
(v)  Event Modality 
(vi)  Event Class 
(vii)       Type of temporal expression 
(vii)  Event Stem 
(viii)  Document Creation Time (DCT). 
 
The following subsections describe how various 
temporal relations are computed. 
Event-DCT 
We take the combined features of every event pre-
sent in the text and the DCT for this purpose. 
 
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, following fea-
tures help us to identify the event-DCT relations, 
specially ?AFTER? temporal relations: 
(i)Modal Context: Whether or not the event word 
has one of the modal context words like- will, 
shall, can, may, or any of their variants (might, 
could, would, etc.).In the sentence: ?The entire 
world will [EVENT see] images of the Pope in Cu-
ba?. Here ?will? context word helps us to deter-
mine event-DCT relation ?AFTER?. 
(ii)Preposition Context: Any prepositions preced-
ing an event or time expression. We consider an 
example:?Children and invalids would be permit-
ted to [EVENT leave] Iraq?. Here the preposition 
to helps us to determine event-DCT relation 
?AFTER?. The same principle goes for time too: in 
the expressions on Friday and for nearly forty 
years, the prepositions on and for governs the time.  
(iii)Context word before or after temporal expres-
sion: context words like before, after, less than, 
greater than etc. help us to determine event-time 
temporal relation identification. Consider an ex-
ample: ?After ten years of [EVENT boom] ?.? 
Event-Time 
Derived Features: We extract all events from eve-
ry sentence. For every temporal expression in a 
sentence, we pair an event in the sentence with the 
former so that the temporal relation can be deter-
mined. 
Similar to annotation of event-DCT relations, 
here too, we have identified different types of con-
text based temporal expression features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, the following 
features help us to distinguish between event and 
time relations, specially ?AFTER? and ?BEFORE? 
temporal relations. The following features are de-
rived from text. 
(i)Type of temporal expression: Represents the 
temporal relationship holding between events, 
times, or between an event and a time of the event.   
(ii)Temporal signal: Represents temporal preposi-
tions ?on? (on this coming Sunday) and slightly 
contribute to the overall score of classifiers 
(iii)Temporal Expression in the target sentence: 
Takes the values greater than, less than, equal or 
none. These values contribute to the overall score 
of classifiers. 
Mainevent-Mainevent and Subevent-
Subevent 
The task demands that the main event of every sen-
tence be determined. As a heuristic decision, we 
have assumed that the first event that appears in a 
sentence is its main event. We pair up main events 
(if present) from consecutive sentences and use 
their combined features to determine their temporal 
relation. For the events belonging to a single sen-
tence, we take into account the combined features 
of all possible pairs of sentential events. 
   
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. 
(i)Relational context: If a relation holding be-
tween the previous event and the current event is 
?AFTER?, the current one is in the past. This in-
formation helps us to identify the temporal relation 
between the current event and successive event. 
(ii)Modal Context: Whether or not the event word 
has one of the context words like, will, shall, can, 
may, or any of their variants (might, could, would, 
etc.).  The verb and auxiliaries governing the next 
event play as an important feature in event-event 
temporal relation identification.   
69
(iii)Ordered based context: In event-event rela-
tion identification, when EVENT-1, EVENT-2, 
and EVENT-3 are linearly ordered, then we have 
assigned true/false as feature value from tense and 
aspect shifts in this ordered pair.  
(iv) Co-reference  based feature: We have used 
co-referential features as derived feature using our 
in-house system based on Standford CoreNLP tool, 
where two event words within or outside one sen-
tence are referring to the same event, i.e. two event 
words co-refer in a discourse.  
(v)Event-DCT relation based feature: We have 
included event-document creation times (DCT) 
temporal relation types as feature of event-event 
relation identification. 
(ii) Preposition Context: Any prepositions before 
the event or time, we consider an exam-
ple:?Children and invalids would be permitted to 
[EVENT leave] Iraq?. Here the preposition to 
helps us determine the event-DCT relation 
?AFTER?.  
(vi) Context word before or after temporal ex-
pression: Context words like before, after, less 
than, greater than help us determine event- event 
temporal relations .We consider an example:?After 
ten years of [EVENT boom] ?.? 
(vii)Stanford parser based clause boundaries 
features: The two consecutive sentences are first 
parsed using Stanford dependency parser and then 
clause boundaries are identified. Then, considering 
the prepositional context and tense verb of the 
clause, temporal relations are identified where all 
temporal expressions are situated in the same 
clause.  
 
 
3 Results and Evaluation 
For the extraction of time expressions and events 
(tasks A and B), precision, recall and F1-score 
have been used as evaluation metrics, using the 
following formulae: 
 
precision (P) = tp/(tp + fp) 
recall (R) = tp/(tp + fn) 
F-measure = 2 *(P * R) / (P + R). 
 
Where, tp is the number of tokens that are part of 
an extent in keys and response, fp is the number of 
tokens that are part of an extent in the response but 
not in the key, and fn is the number of tokens that 
are part of an extent in the key but not in the re-
sponse. Additionally attribute accuracies computed 
according to the following formulae have also been 
reported. 
 
Attr. Accuracy = Attr. F1 / Entity Extraction F1  
Attr. R = Attr. Accuracy * Entity R 
Attr. P = Attr. Accuracy * Entity P 
 
Performance in task C is judged with the aid of the 
Temporal Awareness score proposed by UzZaman 
and Allen (2011) 
The JU_CSE system was evaluated on the TE-3 
platinum data. Table 2 reports JU_CSE?s perfor-
mance in timex extraction Task A. Under the re-
laxed match scheme, the F1-score stands at 
86.38% while the strict match scheme yields a F1-
score of 75.41%. As far as TIMEX attributes are 
concerned, the F1-scores are 63.81% and 73.15% 
for value and type respectively.  
 
Timex Extraction Timex Attribute 
F1 P R Strict F1 Strict P Strict R 
Value 
F1 
Type 
F1 
Value 
Accuracy 
Type 
Accuracy 
86.38 93.28 80.43 75.49 81.51 70.29 63.81 73.15 73.87 84.68 
Table 2:JU_CSE system?s TE-3 Results on Timex Task A 
 
 
 
 
Event Extraction Event Attribute 
F1 P R 
Class 
F1 
Tense 
F1 
Aspect 
F1 
Class 
Accuracy 
Tense 
Accuracy 
Aspect 
Accuracy 
78.57 80.85 76.41 52.65 58.58 72.09 67.01 74.56 91.75 
Table 3:JU_CSE system?s TE-3 Results on Event Task B 
  
70
  
Table 3 reports the system?s performance in 
event extraction (Task B) on TE-3 platinum da-
ta. F1-score for event extraction is 78.57%. At-
tribute F1-scores are 52.65%, 58.58% and 
72.09% for class, tense and aspect respectively.  
In both entities extraction tasks recall is nota-
bly lower than precision. The F1-scores for 
event attributes are modest given that the attrib-
utes were computed using handcrafted rules. 
However, the handcrafted approach can be treat-
ed as a good baseline to start with. Normaliza-
tion is proved to be a challenging task. 
 
Task F1 P R 
Task-ABC 24.61 19.17 34.36 
Task-C 26.41 21.04 35.47 
Task-C-relation-only 34.77 35.07 34.48 
 
Table 4: JU_CSE system?s TE-3 Temporal Aware-
ness results on Task ABC, TaskC-only & TaskC-
relation-only 
 
 
Table 4 presents the Temporal Awareness F1-
score for TaskABC, TaskC and TaskC-relation-
only. For TaskC-only evaluation, the event and 
timex annotated data was provided and one had 
to identify the TLINKs and classify the temporal 
relations. In the TaskC-relation-only version the 
timex and event annotations including their at-
tributes as well as TLINKs were provided save 
the relation classes. Only the relation classes had 
to be determined. The system yielded a temporal 
awareness F1-score of 24.6% for TaskABC, 
26.41% for TaskC-only and 34.77% for TaskC-
relation-only version. 
 
4 Conclusions and Future Directions 
  
In this paper, we have presented the JU_CSE 
system for the TempEval-3 shared task. Our sys-
tem in TempEval-3 may be seen upon as an im-
provement over our earlier endeavor in 
TempEval-2. We have participated in all tasks of 
the TempEval-3 exercise. We have incorporated 
a CRF based approach in our system for all 
tasks. The JU_CSE system for temporal infor-
mation extraction is currently undergoing a lot 
of extensive experimentation. The one reported 
in this article seemingly has a significant scope 
of improvement. Preliminarily, the results yield-
ed are quite competitive and encouraging. Event 
extraction and Timex extraction F1-scores at 
78.58% and 86.38% encourage us to further de-
velop our CRF based scheme. We expect better 
results with additional features and like to con-
tinue our experimentations with other semantic 
features for the CRF classifier. Our rule-based 
approach for event attribute determination how-
ever yields modest F1-scores- 52.65% & 
58.58% for class and tense. We intend to explore 
other machine learning techniques for event at-
tribute classification. We also intend to use parse 
tree based approaches for temporal relation an-
notation. 
Acknowledgments 
This work has been partially supported by a 
grant from the English to Indian language Ma-
chine Translation (EILMT) project funded by 
the Department of Information and Technology 
(DIT), Government of India. We would also like 
to thank to Mr. Jiabul Sk. for his technical con-
tribution.  
 
References  
A. Setzer, and R. Gaizauskas. 2000. Annotating 
Events and Temporal Information in Newswire 
Texts. In LREC 2000, pages 1287?1294, Athens. 
D. Gildea, and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
James Pustejovsky, Jos? Castano, Robert Ingria, 
Roser Sauri, Robert Gaizauskas, Andrea Setzer, 
Graham Katz, and Dragomir Radev. 2003. 
TimeML: Robust specification of event and tem-
poral expressions in text. New directions in ques-
tion answering, 3: 28-34. 
Marc Verhagen, Robert Gaizauskas, Frank Schilder, 
Mark Hepple, Graham Katz, and James 
Pustejovsky. 2007. Semeval-2007 task 15: 
Tempeval temporal relation identification. In Pro-
ceedings of the 4th International Workshop on 
Semantic Evaluations, pages 75-80, ACL. 
71
Marc Verhagen, Roser Sauri, Tommaso Caselli, and 
James Pustejovsky. 2010. Semeval-2010 task 13: 
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 
57- 62. ACL. 
Olga Gurevich, Richard Crouch, Tracy H. King, and 
V. de Paiva. 2006. Deverbal Nouns in Knowledge 
Representation. Proceedings of FLAIRS, pages 
670?675, Melbourne Beach, FL. 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chine. Proceedings of HLT/NAACL-2004, 
Boston, MA. 
UzZaman, N. and J.F. Allen (2011), ?Temporal 
Evaluation.? In Proceedings of The 49th Annual 
Meeting of the Association for Computational 
Linguistics: Human Language Technologies 
(Short Paper), Portland, Oregon, USA.
   
 
72
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 69?76,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Improving MT System Using Extracted Parallel Fragments of Text 
from Comparable Corpora 
 
 
Rajdeep Gupta, Santanu Pal, Sivaji Bandyopadhyay 
Department of Computer Science & Engineering 
Jadavpur University 
Kolkata ? 700032, India 
{rajdeepgupta20, santanu.pal.ju}@gmail.com,  
sivaji_cse_ju@yahoo.com 
 
Abstract 
In this article, we present an automated ap-
proach of extracting English-Bengali parallel 
fragments of text from comparable corpora 
created using Wikipedia documents. Our ap-
proach exploits the multilingualism of Wiki-
pedia. The most important fact is that this ap-
proach does not need any domain specific cor-
pus. We have been able to improve the BLEU 
score of an existing domain specific English-
Bengali machine translation system by 
11.14%. 
1 Introduction 
Recently comparable corpora have got great at-
tention in the field of NLP. Extracting parallel 
fragments of texts, paraphrases or sentences from 
comparable corpora are particularly useful for 
any statistical machine translation system (SMT) 
(Smith et al 2010) as the size of the parallel cor-
pus plays major role in any SMT performance. 
Extracted parallel phrases from comparable cor-
pora are added with the training corpus as addi-
tional data that is expected to facilitate better per-
formance of machine translation systems specifi-
cally for those language pairs which have limited 
parallel resources available. In this work, we try 
to extract English-Bengali parallel fragments of 
text from comparable corpora. We have devel-
oped an aligned corpus of English-Bengali doc-
ument pairs using Wikipedia. Wikipedia is a 
huge collection of documents in many different 
languages. We first collect an English document 
from Wikipedia and then follow the inter-
language link to find the same document in Ben-
gali (obviously, if such a link exists). In this way, 
we create a small corpus. We assume that such 
English-Bengali document pairs from Wikipedia 
are already comparable since they talk about the 
same entity. Although each English-Bengali 
document pair talks about the same entity, most 
of the times they are not exact translation of each 
other. And as a result, parallel fragments of text 
are rarely found in these document pairs. The 
bigger the size of the fragment the less probable 
it is to find its parallel version in the target side. 
Nevertheless, there is always chance of getting 
parallel phrase, tokens or even sentences in com-
parable documents. The challenge is to find those 
parallel texts which can be useful in increasing 
machine translation performance. 
In our present work, we have concentrated on 
finding small fragments of parallel text instead of 
rigidly looking for parallelism at entire sentential 
level. Munteanu and Marcu (2006) believed that 
comparable corpora tend to have parallel data at 
sub-sentential level. This approach is particularly 
useful for this type of corpus under 
consideration, because there is a very little 
chance of getting exact translation of bigger 
fragments of text in the target side. Instead, 
searching for parallel chunks would be more 
logical. If a sentence in the source side has a 
parallel sentence in the target side, then all of its 
chunks need to have their parallel translations in 
the target side as well. 
It is to be noted that, although we have 
document level alignment in our corpus, it is 
somehow ad-hoc i.e. the documents in the corpus 
do not belong to any particular domain. Even 
with such a corpus we have been able to improve 
the performance of an existing machine 
translation system built on tourism domain. This 
also signifies our contribution towards domain 
adaptation of machine translation systems. 
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
describes the preparation of the comparable 
corpus. The system architecture is described in 
section 4. Section 5 describes the experiments we 
69
conducted and presents the results. Finally the 
conclusion is drawn in section 6. 
2 Related Work 
There has been a growing interest in approaches 
focused on extracting word translations from 
comparable corpora (Fung and McKeown, 1997; 
Fung and Yee, 1998; Rapp, 1999; Chiao and 
Zweigenbaum, 2002; Dejean et al, 2002; Kaji, 
2005; Gamallo, 2007; Saralegui et al, 2008). 
Most of the strategies follow a standard method 
based on context similarity. The idea behind this 
method is as follows: A target word t is the 
translation of a source word s if the words with 
which t co-occurs are translations of words with 
which s co-occurs. The basis of the method is to 
find the target words that have the most similar 
distributions with a given source word. The 
starting point of this method is a list of bilingual 
expressions that are used to build the context 
vectors of all words in both languages. This list 
is usually provided by an external bilingual 
dictionary. In Gamallo (2007), however, the 
starting list is provided by bilingual correlations 
which are previously extracted from a parallel 
corpus. In Dejean (2002), the method relies on a 
multilingual thesaurus instead of an external 
bilingual dictionary. In all cases, the starting list 
contains the ?seed expressions? required to build 
context vectors of the words in both languages. 
The works based on this standard approach 
mainly differ in the coefficients used to measure 
the context vector similarity. 
Otero et al (2010) showed how Wikipedia 
could be used as a source of comparable corpora 
in different language pairs. They downloaded the 
entire Wikipedia for any two language pair and 
transformed it into a new collection: 
CorpusPedia. However, in our work we have 
showed that only a small ad-hoc corpus 
containing Wikipedia articles could be proved to 
be beneficial for existing MT systems. 
3 Tools and Resources Used 
A sentence-aligned English-Bengali parallel 
corpus containing 22,242 parallel sentences from 
a travel and tourism domain was used in the 
preparation of the baseline system. The corpus 
was obtained from the consortium-mode project 
?Development of English to Indian Languages 
Machine Translation (EILMT) System?. The 
Stanford Parser and the CRF chunker were used 
for identifying individual chunks in the source 
side of the parallel corpus. The sentences on the 
target side (Bengali) were POS-tagged/chunked 
by using the tools obtained from the consortium 
mode project ?Development of Indian Languages 
to Indian Languages Machine Translation 
(ILILMT) System?.  
For building the comparable corpora we have 
focused our attention on Wikipedia documents. 
To collect comparable English-Bengali 
document pairs we designed a crawler. The 
crawler first visits an English page, saves the raw 
text (in HTML format), and then finds the cross-
lingual link (if exists) to find the corresponding 
Bengali document. Thus, we get one English-
Bengali document pair. Moreover, the crawler 
visits the links found in each document and 
repeats the process. In this way, we develop a 
small aligned corpus of English-Bengali 
comparable document pairs. We retain only the 
textual information and all the other details are 
discarded. It is evident that the corpus is not 
confined to any particular domain. The challenge 
is to exploit this kind of corpus to help machine 
translation systems improve. The advantage of 
using such corpus is that it can be prepared easily 
unlike the one that is domain specific. 
The effectiveness of the parallel fragments of 
text developed from the comparable corpora in 
the present work is demonstrated by using the 
standard log-linear PB-SMT model as our 
baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase extraction 
heuristics described in (Koehn et al, 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
with Kneser-Ney smoothing (Kneser and Ney, 
1995) trained with SRILM (Stolcke, 2002), and 
Moses decoder (Koehn et al, 2007). 
4 System Architecture 
4.1 PB-SMT(Baseline System) 
Translation is modeled in SMT as a decision 
process, in which the translation e1
I = e1..ei..eI of 
a source sentence f1
J = f1..fj..fJ  is chosen to 
maximize (1) 
)().|(maxarg)|(maxarg 111,11, 11
IIJ
eI
JI
eI
ePefPfeP
II
?
     (1)  
where )|( 11 IJ efP  and )( 1IeP  denote 
respectively the translation model and the target 
language model (Brown et al, 1993). In log-
linear phrase-based SMT, the posterior 
probability )|( 11 JI feP  is directly modeled as a 
log-linear combination of features (Och and Ney, 
70
2002), that usually comprise of M translational 
features, and the language model, as in (2): 
?
?
?
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?
)(log 1ILM eP??        (2)     
where kk sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          kk iik eee ...? 11??? , 
         kk jbk fff ...? ? .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(
                 (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
? ??
?K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??
     (5) 
where 
m
M
m
mhh ??
1
?
?
? ?
. 
4.2 Chunking of English Sentences 
We have used CRF-based chunking algorithm to 
chunk the English sentences in each document. 
The chunking breaks the sentences into linguistic 
phrases. These phrases may be of different sizes. 
For example, some phrases may be two words 
long and some phrases may be four words long. 
According to the linguistic theory, the interme-
diate constituents of the chunks do not usually 
take part in long distance reordering when it is 
translated, and only intra chunk reordering oc-
curs. Some chunks combine together to make a 
longer phrase. And then some phrases again 
combine to make a sentence. The entire process 
maintains the linguistic definition of a sentence. 
Breaking the sentences into N-grams would have 
always generated phrases of length N but these 
phrases may not be linguistic phrases. For this 
reason, we avoided breaking the sentences into 
N-grams. 
The chunking tool breaks each English sentence 
into chunks. The following is an example of how 
the chunking is done. 
Sentence: India , officially the Republic of India , 
is a country in South Asia. 
After Chunking: (India ,) (officially) (the 
Republic ) (of) (India , ) (is) (a country ) (in 
South Asia ) (.) 
We have further merged the chunks to form 
bigger chunks. The idea is that, we may 
sometimes find the translation of the merged 
chunk in the target side as well, in which case, 
we would get a bigger fragment of parallel text. 
The merging is done in two ways: 
Strict Merging: We set a value ?V?. Starting 
from the beginning, chunks are merged such that 
the number of tokens in each merged chunk does 
not exceed V. 
 
 
Figure 1. Strict-Merging Algorithm. 
 
Figure 1 describes the pseudo-code for strict 
merging. 
For example, in our example sentence the 
merged chunks will be as following, where V=4: 
(India , officially) (the Republic of ) (India , is) 
(a country) (in South Asia .) 
 
 
Figure 2. Window-Based Merging Algorithm. 
Procedure Window_Merging() 
begin 
Set_Chunk?Set of all English Chunks 
L?Number of chunks in Set_Chunk 
for i = 0 to L-1 
 Words?Set of tokens in i-th Chunk in Set_Chunk 
 Cur_wc?number of tokens in Words 
Ol?i-th chunk in Set_Chunk 
for j = (i+1) to (L-1) 
  C?j-th chunk in Set_Chunk 
  w?set of tokens in C 
  l?number of tokens in w 
  if(Cur_wc + l ? V) 
   Append C at the end of Ol 
   Add l to Cur_wc 
  end if 
 end for 
 Output Ol as the next merged chunk 
end for 
end   
 
Procedure Strict_Merge() 
begin 
Oline ? null 
Cur_wc ? 0 
repeat 
Iline?Next Chunk 
Length?Number of Tokens in Iline 
if(Cur_wc + Length > V) 
Output Oline as the next merged chunk 
  Cur_wc?Length 
 else 
  Append Iline at the end of Oline 
  Add Length to Cur_wc 
 end if 
while (there are more chunks) 
end 
71
 
Figure 3. System Architecture for Finding Parallel Fragments
Window-Based Merging: In this type of 
chunking also, we set a value ?V?, and for each 
chunk we try to merge as many chunks as 
possible so that the number of tokens in the 
merged chunk never exceeds V. 
So, we slide an imaginary window over the 
chunks. For example, for our example sentence 
the merged chunks will be as following, where V 
= 4 : 
(India , officially) (officially the Republic of) 
(the Republic of) (of India , is) (India , is) (is a 
country) (a country) (in South Asia .) 
The pseudo-code of window-based merging is   
described in Figure 2. 
4.3 Chunking of Bengali Sentences 
Since to the best of our knowledge, there is no 
good quality chunking tool for Bengali we did 
not use chunking explicitly. Instead, strict 
merging is done with consecutive V number of 
tokens whereas window-based merging is done 
sliding a virtual window over each token and 
merging tokens so that the number of tokens 
does not exceed V. 
4.4 Finding Parallel Chunks 
After finding the merged English chunks they are 
translated into Bengali using a machine 
translation system that we have already 
developed. This is also the same machine 
translation system whose performance we want 
to improve. Chunks of each of the document 
pairs are then compared to find parallel chunks. 
Each translated source chunk (translated from 
English to Bengali) is compared with all the 
target chunks in the corresponding Bengali-
chunk document. When a translated source 
chunk is considered, we try to align each of its 
token to some token in the target chunk. Overlap 
between token two Bengali chunks B1 and B2, 
where B1 is the translated chunk and B2 is the 
chunk in the Bengali document, is defined as 
follows: 
Overlap(B1,B2) = Number of tokens in B1 for 
which an alignment can be found in B2.  
It is to be noted that Overlap(B1,B2) ? 
Overlap(B2 ,B1). Overlap between chunks is 
found in both ways (from translated source 
chunk to target and from target to translated 
source chunk). If 70% alignment is found in both 
the overlap measures then we declare them as 
parallel. Two issues are important here: the com-
parison of two Bengali tokens and in case an 
alignment is found, which token to retrieve 
(source or target) and how to reorder them. We 
address these two issues in the next two sections. 
4.5 Comparing Bengali Tokens 
For our purpose, we first divide the two tokens 
into their matra (vowel modifiers) part and 
consonant part keeping the relative orders of 
characters in each part same. For example, 
Figure 4 shows the division of the word . 
 
English 
Documents 
English 
Chunks 
Merging 
Translation 
Bengali 
Documents 
Bengali 
Chunks 
Find Parallel Chunks and Reorder  
Merging 
72
 
 
 
 
 
Figure 4. Division of a Bengali Word. 
 
Respective parts of the two words are then 
compared. Orthographic similarities like 
minimum edit distance ratio, longest common 
subsequence ratio, and length of the strings are 
used for the comparison of both parts. 
Minimum Edit Distance Ratio: It is defined 
as  follows: 
 
 
where |B| is the length of the string B and ED is 
the minimum edit distance or levenshtein 
distance calculated as the minimum number of 
edit operations ? insert, replace, delete ? needed 
to transform B1 into B2. 
Longest Common Subsequence Ratio: It is 
defined as follows: 
 
 
 
where LCS is the longest common subsequence 
of two strings. 
Threshold for matching is set empirically. We 
differentiate between shorter strings and larger 
strings. The idea is that, if the strings are short 
we cannot afford much difference between them 
to consider them as a match. In those cases, we 
check for exact match. Also, the threshold for 
consonant part is set stricter because our 
assumption is that consonants contribute more 
toward the word?s pronunciation. 
4.6 Reordering of Source Chunks 
When a translated source chunk is compared 
with a target chunk it is often found that the 
ordering of the tokens in the source chunk and 
the target chunk is different. The tokens in the 
target chunk have a different permutation of 
positions with respect to the positions of tokens 
in the source chunk. In those cases, we reordered 
the positions of the tokens in the source chunk so 
as to reflect the positions of tokens in the target 
chunk because it is more likely that the tokens 
will usually follow the ordering as in the target 
chunk. For example, the machine translation 
output of the English chunk ?from the Atlantic 
Ocean? is ? theke  atlantic  
 (mahasagar)?. We found a target 
chunk ?  (atlantic)  (maha-
sagar)  (theke)  (ebong)? with which 
we could align the tokens of the source chunk 
but in different relative order. Figure 5 shows the 
alignment of tokens.  
 
Figure 5. Alignment of Bengali Tokens. 
 
We reordered the tokens of the source chunk 
and the resulting chunk was ?  
 ?.Also, the token ? ? in the 
target chunk could not find any alignment and 
was discarded. The system architecture of the 
present system is described in figure 3. 
5 Experiments And Results 
5.1 Baseline System 
We randomly extracted 500 sentences each for 
the development set and test set from the initial 
parallel corpus, and treated the rest as the 
training corpus. After filtering on the maximum 
allowable sentence length of 100 and sentence 
length ratio of 1:2 (either way), the training 
corpus contained 22,492 sentences.  
 
V=4 V=7 
Number of English 
Chunks(Strict-Merging) 
579037 376421 
Number of English 
Chunks(Window-
Merging) 
890080 949562 
Number of Bengali 
Chunks(Strict-Merging) 
69978 44113 
Number of Bengali 
Chunks(Window-
Merging) 
230025 249330 
Table 1. Statistics of the Comparable Corpus 
 
V=4 V=7 
Number of Parallel 
Chunks(Strict-Merging) 
1032 1225 
Number of Parallel 
Chunks(Window-Merging) 
1934 2361 
Table 2. Number of Parallel Chunks found 
 
 
Kolkata  
matra
73
 BLEU NIST 
Baseline System(PB-SMT) 10.68 4.12 
Baseline + Parallel 
Chunks(Strict-
Merging) 
V=4 10.91 4.16 
V=7 11.01 4.16 
Baseline + Parallel 
Chunks(Window-
Merging) 
V=4 11.55 4.21 
V=7 11.87 4.29 
 
Table 3.Evaluation of the System 
 
In addition to the target side of the parallel cor-
pus, a monolingual Bengali corpus containing 
406,422 words from the tourism domain was 
used for the target language model. We 
experimented with different n-gram settings for 
the language model and the maximum phrase 
length, and found that a 5-gram language model 
and a maximum phrase length of 7 produced the 
optimum baseline result. We therefore carried 
out the rest of the experiments using these 
settings. 
5.2 Improving Baseline System 
The comparable corpus consisted of 582 English-
Bengali document pairs.  
We experimented with the values V=4 and 
V=7 while doing the merging of chunks both in 
English and Bengali. All the single token chunks 
were discarded. Table 1 shows some statistics 
about the merged chunks for V=4 and V=7.It is 
evident that number of chunks in English 
documents is far more than the number of chunks 
in Bengali documents. This immediately 
suggests that Bengali documents are less 
informative than English documents. When the 
English merged chunks were passed to the 
translation module some of the chunks could not 
be translated into Bengali. Also, some chunks 
could be translated only partially, i.e. some 
tokens could be translated while some could not 
be. Those chunks were discarded. Finally, the 
number of (Strict-based) English merged-chunks 
and number of (Window-based) English merged-
chunks were 285756 and 594631 respectively. 
Two experiments were carried out separately. 
Strict-based  merged English chunks were 
compared with Strict-Based merged Bengali 
chunks. Similarly, window-based merged Eng-
lish chunks were compared with window-based 
merged Bengali chunks. While searching for 
parallel chunks each translated source chunk was 
compared with all the target chunks in the 
corresponding document. Table 2 displays the 
number of parallel chunks found. Compared to 
the number of chunks in the original documents 
the number of parallel chunks found was much 
less. Nevertheless, a quick review of the parallel 
list revealed that most of the chunks were of 
good quality. 
5.3 Evaluation 
We carried out evaluation of the MT quality 
using two automatic MT evaluation metrics: 
BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). Table 3 presents the ex-
perimental results. For the PB-SMT experiments, 
inclusion of the extracted strict merged parallel 
fragments from comparable corpora as additional 
training data presented some improvements over 
the PB-SMT baseline. Window based extracted 
fragments are added separately with parallel cor-
pus and that also provides some improvements 
over the PB baseline; however inclusion of win-
dow based extracted phrases in baseline system 
with phrase length 7 improves over both strict 
and baseline in term of BLEU score and NIST 
score. 
Table 3 shows the performance of the PB-
SMT system that shows an improvement over 
baseline with both strict and window based 
merging even if,  we change their phrase length 
from 4 to 7. Table 3 shows that the best 
improvement is achieved when we add parallel 
chunks as window merging with phrase length 7. 
It gives 1.19 BLEU point, i.e., 11.14% relative 
improvement over baseline system. The NIST 
score could be improved  up to 4.12%. Bengali is 
a morphologically rich language and has 
74
relatively free phrase order. The strict based 
extraction does not reflect much improvement 
compared to the window based extraction 
because strict-merging (Procedure Strict_Merge) 
cannot cover up all the segments on either side, 
so very few parallel extractions have been found 
compared to window based extraction.  
6 Conclusion 
In this work, we tried to find English-Bengali 
parallel fragments of text from a comparable 
corpus built from Wikipedia documents. We 
have successfully improved the performance of 
an existing machine translation system. We have 
also shown that out-of-domain corpus happened 
to be useful for training of a domain specific MT 
system. The future work consists of working on 
larger amount of data. Another focus could be on 
building ad-hoc comparable corpus from WEB 
and using it to improve the performance of an 
existing out-of-domain MT system. This aspect 
of work is particularly important because the 
main challenge would be of domain adaptation. 
Acknowledgements 
This work has been partially supported by a grant 
from the English to Indian language Machine 
Translation (EILMT) project funded by the 
Department of Information and Technology 
(DIT), Government of India.  
Reference 
Chiao, Y. C., & Zweigenbaum, P. (2002, August). 
Looking for candidate translational equivalents in 
specialized, comparable corpora. In Proceedings of 
the 19th international conference on Computation-
al linguistics-Volume 2 (pp. 1-5). Association for 
Computational Linguistics. 
D?jean, H., Gaussier, ?., & Sadat, F. (2002). Bilin-
gual terminology extraction: an approach based on 
a multilingual thesaurus applicable to comparable 
corpora. In Proceedings of the 19th International 
Conference on Computational Linguistics COLING 
(pp. 218-224). 
Doddington, G. (2002, March). Automatic evaluation 
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second 
international conference on Human Language 
Technology Research (pp. 138-145). Morgan 
Kaufmann Publishers Inc.. 
Fung, P., & McKeown, K. (1997, August). Finding 
terminology translations from non-parallel corpora. 
In Proceedings of the 5th Annual Workshop on 
Very Large Corpora (pp. 192-202). 
Fung, P., & Yee, L. Y. (1998, August). An IR ap-
proach for translating new words from nonparallel, 
comparable texts. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 1 (pp. 414-420). Association for Computa-
tional Linguistics. 
Hiroyuki, K. A. J. I. (2005). Extracting translation 
equivalents from bilingual comparable corpora. 
IEICE Transactions on information and systems, 
88(2), 313-323. 
 Kneser, R., & Ney, H. (1995, May). Improved back-
ing-off for m-gram language modeling. In Acous-
tics, Speech, and Signal Processing, 1995. 
ICASSP-95., 1995 International Conference on 
(Vol. 1, pp. 181-184). IEEE. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., ... & Herbst, E. (2007, 
June). Moses: Open source toolkit for statistical 
machine translation. In Proceedings of the 45th 
Annual Meeting of the ACL on Interactive Poster 
and Demonstration Sessions (pp. 177-180). 
Association for Computational Linguistics. 
Koehn, P., Och, F. J., & Marcu, D. (2003, May). Sta-
tistical phrase-based translation. In Proceedings of 
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1 
(pp. 48-54). Association for Computational Lin-
guistics. 
 Munteanu, D. S., & Marcu, D. (2006, July). Extract-
ing parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics (pp. 81-88). Association 
for Computational Linguistics.. 
Och, F. J. (2003, July). Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1 (pp. 160-167). As-
sociation for Computational Linguistics. 
Och, F. J., & Ney, H. (2000). Giza++: Training of 
statistical translation models. 
Otero, P. G. (2007). Learning bilingual lexicons from 
comparable english and spanish corpora. Proceed-
ings of MT Summit xI, 191-198. 
Otero, P. G., & L?pez, I. G. (2010). Wikipedia as 
multilingual source of comparable corpora. In Pro-
ceedings of the 3rd Workshop on Building and Us-
ing Comparable Corpora, LREC (pp. 21-25). 
 Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. 
(2002, July). BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the 
40th annual meeting on association for computa-
75
tional linguistics (pp. 311-318). Association for 
Computational Linguistics. 
Rapp, R. (1999, June). Automatic identification of 
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th annual 
meeting of the Association for Computational Lin-
guistics on Computational Linguistics (pp. 519-
526). Association for Computational Linguistics. 
Saralegui, X., San Vicente, I., & Gurrutxaga, A. 
(2008). Automatic generation of bilingual lexicons 
from comparable corpora in a popular science do-
main. In LREC 2008 workshop on building and us-
ing comparable corpora. 
 Smith, J. R., Quirk, C., & Toutanova, K. (2010, 
June).Extracting parallel sentences from 
comparable corpora using document level 
alignment. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics (pp. 403-411). Association for 
Computational Linguistics. 
Stolcke, A. (2002, September). SRILM-an extensible 
language modeling toolkit. In Proceedings of the 
international conference on spoken language 
processing (Vol. 2, pp. 901-904). 
 
 
 
76
