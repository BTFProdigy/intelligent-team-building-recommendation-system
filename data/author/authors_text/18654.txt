Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891?896,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Discriminative Improvements to Distributional Sentence Similarity
Yangfeng Ji
School of Interactive Computing
Georgia Institute of Technology
jiyfeng@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Abstract
Matrix and tensor factorization have been ap-
plied to a number of semantic relatedness
tasks, including paraphrase identification. The
key idea is that similarity in the latent space
implies semantic relatedness. We describe
three ways in which labeled data can im-
prove the accuracy of these approaches on
paraphrase classification. First, we design
a new discriminative term-weighting metric
called TF-KLD, which outperforms TF-IDF.
Next, we show that using the latent repre-
sentation from matrix factorization as features
in a classification algorithm substantially im-
proves accuracy. Finally, we combine latent
features with fine-grained n-gram overlap fea-
tures, yielding performance that is 3% more
accurate than the prior state-of-the-art.
1 Introduction
Measuring the semantic similarity of short units
of text is fundamental to many natural language
processing tasks, from evaluating machine transla-
tion (Kauchak and Barzilay, 2006) to grouping re-
dundant event mentions in social media (Petrovic?
et al, 2010). The task is challenging because of
the infinitely diverse set of possible linguistic real-
izations for any idea (Bhagat and Hovy, 2013), and
because of the short length of individual sentences,
which means that standard bag-of-words representa-
tions will be hopelessly sparse.
Distributional methods address this problem by
transforming the high-dimensional bag-of-words
representation into a lower-dimensional latent space.
This can be accomplished by factoring a matrix
or tensor of term-context counts (Turney and Pan-
tel, 2010); proximity in the induced latent space
has been shown to correlate with semantic similar-
ity (Mihalcea et al, 2006). However, factoring the
term-context matrix means throwing away a consid-
erable amount of information, as the original ma-
trix of size M ?N (number of instances by number
of features) is factored into two smaller matrices of
size M ?K and N ?K, with K  M,N . If the
factorization does not take into account labeled data
about semantic similarity, important information can
be lost.
In this paper, we show how labeled data can con-
siderably improve distributional methods for mea-
suring semantic similarity. First, we develop a
new discriminative term-weighting metric called
TF-KLD, which is applied to the term-context ma-
trix before factorization. On a standard paraphrase
identification task (Dolan et al, 2004), this method
improves on both traditional TF-IDF and Weighted
Textual Matrix Factorization (WTMF; Guo and
Diab, 2012). Next, we convert the latent repre-
sentations of each sentence pair into a feature vec-
tor, which is used as input to a linear SVM clas-
sifier. This yields further improvements and sub-
stantially outperforms the current state-of-the-art
on paraphrase classification. We then add ?fine-
grained? features about the lexical similarity of the
sentence pair. The combination of latent and fine-
grained features yields further improvements in ac-
curacy, demonstrating that these feature sets provide
complementary information on semantic similarity.
891
2 Related Work
Without attempting to do justice to the entire lit-
erature on paraphrase identification, we note three
high-level approaches: (1) string similarity metrics
such as n-gram overlap and BLEU score (Wan et
al., 2006; Madnani et al, 2012), as well as string
kernels (Bu et al, 2012); (2) syntactic operations
on the parse structure (Wu, 2005; Das and Smith,
2009); and (3) distributional methods, such as la-
tent semantic analysis (LSA; Landauer et al, 1998),
which are most relevant to our work. One appli-
cation of distributional techniques is to replace in-
dividual words with distributionally similar alterna-
tives (Kauchak and Barzilay, 2006). Alternatively,
Blacoe and Lapata (2012) show that latent word rep-
resentations can be combined with simple element-
wise operations to identify the semantic similarity
of larger units of text. Socher et al (2011) pro-
pose a syntactically-informed approach to combine
word representations, using a recursive auto-encoder
to propagate meaning through the parse tree.
We take a different approach: rather than repre-
senting the meanings of individual words, we di-
rectly obtain a distributional representation for the
entire sentence. This is inspired by Mihalcea et al
(2006) and Guo and Diab (2012), who treat sen-
tences as pseudo-documents in an LSA framework,
and identify paraphrases using similarity in the la-
tent space. We show that the performance of such
techniques can be improved dramatically by using
supervised information to (1) reweight the individ-
ual distributional features and (2) learn the impor-
tance of each latent dimension.
3 Discriminative feature weighting
Distributional representations (Turney and Pantel,
2010) can be induced from a co-occurrence ma-
trix W ? RM?N , where M is the number of in-
stances and N is the number of distributional fea-
tures. For paraphrase identification, each instance
is a sentence; features may be unigrams, or may
include higher-order n-grams or dependency pairs.
By decomposing the matrix W, we hope to obtain
a latent representation in which semantically-related
sentences are similar. Singular value decomposition
(SVD) is traditionally used to perform this factoriza-
tion. However, recent work has demonstrated the ro-
bustness of nonnegative matrix factorization (NMF;
Lee and Seung, 2001) for text mining tasks (Xu et
al., 2003; Arora et al, 2012); the difference from
SVD is the addition of a non-negativity constraint
in the latent representation based on non-orthogonal
basis.
WhileW may simply contain counts of distribu-
tional features, prior work has demonstrated the util-
ity of reweighting these counts (Turney and Pantel,
2010). TF-IDF is a standard approach, as the inverse
document frequency (IDF) term increases the impor-
tance of rare words, which may be more discrimi-
native. Guo and Diab (2012) show that applying a
special weight to unseen words can further improve-
ment performance on paraphrase identification.
We present a new weighting scheme, TF-KLD,
based on supervised information. The key idea is
to increase the weights of distributional features that
are discriminative, and to decrease the weights of
features that are not. Conceptually, this is similar
to Linear Discriminant Analysis, a supervised fea-
ture weighting scheme for continuous data (Murphy,
2012).
More formally, we assume labeled sentence pairs
of the form ?~w(1)i , ~w
(2)
i , ri?, where ~w
(1)
i is the bi-
narized vector of distributional features for the first
sentence, ~w(2)i is the binarized vector of distribu-
tional features for the second sentence, and ri ?
{0, 1} indicates whether they are labeled as a para-
phrase pair. Assuming the order of the sentences
within the pair is irrelevant, then for k-th distribu-
tional feature, we define two Bernoulli distributions:
? pk = P (w
(1)
ik |w
(2)
ik = 1, ri = 1). This is the
probability that sentence w(1)i contains feature
k, given that k appears in w(2)i and the two sen-
tences are labeled as paraphrases, ri = 1.
? qk = P (w
(1)
ik |w
(2)
ik = 1, ri = 0). This is the
probability that sentence w(1)i contains feature
k, given that k appears in w(2)i and the two sen-
tences are labeled as not paraphrases, ri = 0.
The Kullback-Leibler divergence KL(pk||qk) =
?
x pk(x) log
pk(x)
qk(x)
is then a measure of the discrim-
inability of feature k, and is guaranteed to be non-
892
0.0 0.2 0.4 0.6 0.8 1.0pk
0.0
0.2
0.4
0.6
0.8
1.0
1?q
k
neither
nor
not
fear
same
but
off
shares
studythen
0.200
0.200
0.400
0.400
0.600
0.600
0.800 0.800
1.000
1.000
Figure 1: Conditional probabilities for a few hand-
selected unigram features, with lines showing contours
with identical KL-divergence. The probabilities are es-
timated based on the MSRPC training set (Dolan et al,
2004).
negative.1 We use this divergence to reweight the
features in W before performing the matrix factor-
ization. This has the effect of increasing the weights
of features whose likelihood of appearing in a pair
of sentences is strongly influenced by the paraphrase
relationship between the two sentences. On the other
hand, if pk = qk, then the KL-divergence will be
zero, and the feature will be ignored in the ma-
trix factorization. We name this weighting scheme
TF-KLD, since it includes the term frequency and
the KL-divergence.
Taking the unigram feature not as an example, we
have pk = [0.66, 0.34] and qk = [0.31, 0.69], for a
KL-divergence of 0.25: the likelihood of this word
being shared between two sentence is strongly de-
pendent on whether the sentences are paraphrases.
In contrast, the feature then has pk = [0.33, 0.67]
and qk = [0.32, 0.68], for a KL-divergence of 3.9?
10?4. Figure 1 shows the distributions of these and
other unigram features with respect to pk and 1?qk.
The diagonal line running through the middle of the
plot indicates zero KL-divergence, so features on
this line will be ignored.
1We obtain very similar results with the opposite divergence
KL(qk||pk). However, the symmetric Jensen-Shannon diver-
gence performs poorly.
1 unigram recall
2 unigram precision
3 bigram recall
4 bigram precision
5 dependency relation recall
6 dependency relation precision
7 BLEU recall
8 BLEU precision
9 Difference of sentence length
10 Tree-editing distance
Table 1: Fine-grained features for paraphrase classifica-
tion, selected from prior work (Wan et al, 2006).
4 Supervised classification
While previous work has performed paraphrase clas-
sification using distance or similarity in the latent
space (Guo and Diab, 2012; Socher et al, 2011),
more direct supervision can be applied. Specifically,
we convert the latent representations of a pair of sen-
tences ~v1 and ~v2 into a sample vector,
~s(~v1, ~v2) =
[
~v1 + ~v2, |~v1 ? ~v2|
]
, (1)
concatenating the element-wise sum ~v1 +~v2 and ab-
solute difference |~v1 ? ~v2|. Note that ~s(?, ?) is sym-
metric, since ~s(~v1, ~v2) = ~s(~v2, ~v1). Given this rep-
resentation, we can use any supervised classification
algorithm.
A further advantage of treating paraphrase as a
supervised classification problem is that we can ap-
ply additional features besides the latent represen-
tation. We consider a subset of features identified
by Wan et al (2006), listed in Table 1. These fea-
tures mainly capture fine-grained similarity between
sentences, for example by counting specific unigram
and bigram overlap.
5 Experiments
Our experiments test the utility of the TF-
KLD weighting towards paraphrase classification,
using the Microsoft Research Paraphrase Corpus
(Dolan et al, 2004). The training set contains 2753
true paraphrase pairs and 1323 false paraphrase
pairs; the test set contains 1147 and 578 pairs, re-
spectively.
The TF-KLD weights are constructed from only
the training set, while matrix factorizations are per-
893
formed on the entire corpus. Matrix factorization on
both training and (unlabeled) test data can be viewed
as a form of transductive learning (Gammerman et
al., 1998), where we assume access to unlabeled test
set instances.2 We also consider an inductive setting,
where we construct the basis of the latent space from
only the training set, and then project the test set
onto this basis to find the corresponding latent rep-
resentation. The performance differences between
the transductive and inductive settings were gener-
ally between 0.5% and 1%, as noted in detail be-
low. We reiterate that the TF-KLD weights are never
computed from test set data.
Prior work on this dataset is described in sec-
tion 2. To our knowledge, the current state-of-the-
art is a supervised system that combines several ma-
chine translation metrics (Madnani et al, 2012), but
we also compare with state-of-the-art unsupervised
matrix factorization work (Guo and Diab, 2012).
5.1 Similarity-based classification
In the first experiment, we predict whether a pair
of sentences is a paraphrase by measuring their co-
sine similarity in latent space, using a threshold for
the classification boundary. As in prior work (Guo
and Diab, 2012), the threshold is tuned on held-out
training data. We consider two distributional feature
sets: FEAT1, which includes unigrams; and FEAT2,
which also includes bigrams and unlabeled depen-
dency pairs obtained from MaltParser (Nivre et al,
2007). To compare with Guo and Diab (2012), we
set the latent dimensionality to K = 100, which was
the same in their paper. Both SVD and NMF factor-
ization are evaluated; in both cases, we minimize the
Frobenius norm of the reconstruction error.
Table 2 compares the accuracy of a num-
ber of different configurations. The transductive
TF-KLD weighting yields the best overall accu-
racy, achieving 72.75% when combined with non-
negative matrix factorization. While NMF performs
slightly better than SVD in both comparisons, the
major difference is the performance of discrimina-
tive TF-KLD weighting, which outperforms TF-IDF
regardless of the factorization technique. When we
2Another example of transductive learning in NLP is
when Turian et al (2010) induced word representations from a
corpus that included both training and test data for their down-
stream named entity recognition task.
50 100 150 200 250 300 350 400K60
65
70
75
80
Accu
racy
 (%)
Feat1_TF-IDF_SVMFeat2_TF-IDF_SVMFeat1_TF-KLD_SVMFeat2_TF-KLD_SVM
Figure 2: Accuracy of feature and weighting combina-
tions in the classification framework.
perform the matrix factorization on only the training
data, the accuracy on the test set is 73.58%, with F1
score 80.55%.
5.2 Supervised classification
Next, we apply supervised classification, construct-
ing sample vectors from the latent representation as
shown in Equation 1. For classification, we choose
a Support Vector Machine with a linear kernel (Fan
et al, 2008), leaving a thorough comparison of clas-
sifiers for future work. The classifier parameter C is
tuned on a development set comprising 20% of the
original training set.
Figure 2 presents results for a range of latent di-
mensionalities. Supervised learning identifies the
important dimensions in the latent space, yielding
significantly better performance that the similarity-
based classification from the previous experiment.
In Table 3, we compare against prior published
work, using the held-out development set to select
the best value of K (again, K = 400). The best
result is from TF-KLD, with distributional features
FEAT2, achieving 79.76% accuracy and 85.87% F1.
This is well beyond all known prior results on this
task. When we induce the latent basis from only
the training data, we get 78.55% on accuracy and
84.59% F1, also better than the previous state-of-art.
Finally, we augment the distributional represen-
tation, concatenating the ten ?fine-grained? fea-
tures in Table 1 to the sample vectors described
in Equation 1. As shown in Table 3, the accu-
894
Factorization Feature set Weighting K Measure Accuracy (%) F1
SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33
NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14
WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported
SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19
NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48
Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo
and Diab (2012).
Acc. F1
Most common class 66.5 79.9
(Wan et al, 2006) 75.6 83.0
(Das and Smith, 2009) 73.9 82.3
(Das and Smith, 2009) with 18 features 76.1 82.7
(Bu et al, 2012) 76.3 not reported
(Socher et al, 2011) 76.8 83.6
(Madnani et al, 2012) 77.4 84.1
FEAT2, TF-KLD, SVM 79.76 85.87
FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96
Table 3: Supervised classification. Results from prior work are reprinted.
racy now improves to 80.41%, with an F1 score of
85.96%. When the latent representation is induced
from only the training data, the corresponding re-
sults are 79.94% on accuracy and 85.36% F1, again
better than the previous state-of-the-art. These re-
sults show that the information captured by the dis-
tributional representation can still be augmented by
more fine-grained traditional features.
6 Conclusion
We have presented three ways in which labeled
data can improve distributional measures of seman-
tic similarity at the sentence level. The main innova-
tion is TF-KLD, which discriminatively reweights
the distributional features before factorization, so
that discriminability impacts the induction of the la-
tent representation. We then transform the latent
representation into a sample vector for supervised
learning, obtaining results that strongly outperform
the prior state-of-the-art; adding fine-grained lexi-
cal features further increases performance. These
ideas may have applicability in other semantic sim-
ilarity tasks, and we are also eager to apply them to
new, large-scale automatically-induced paraphrase
corpora (Ganitkevitch et al, 2013).
Acknowledgments
We thank the reviewers for their helpful feedback,
and Weiwei Guo for quickly answering questions
about his implementation. This research was sup-
ported by a Google Faculty Research Award to the
second author.
References
Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012.
Learning Topic Models - Going beyond SVD. In
FOCS, pages 1?10.
Rahul Bhagat and Eduard Hovy. 2013. What Is a Para-
phrase? Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Fan Bu, Hang Li, and Xiaoyan Zhu. 2012. String Re-
writing kernel. In Proceedings of ACL, pages 449?
458. Association for Computational Linguistics.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
895
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
468?476, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised Construction of Large Paraphrase Corpora:
Exploiting Massively Parallel News Sources. In COL-
ING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Alexander Gammerman, Volodya Vovk, and Vladimir
Vapnik. 1998. Learning by transduction. In Proceed-
ings of the Fourteenth conference on Uncertainty in
artificial intelligence, pages 148?155. Morgan Kauf-
mann Publishers Inc.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL, pages 758?764.
Association for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling Sentences
in the Latent Space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, pages 864?872, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of NAACL, pages 455?462. Association for Computa-
tional Linguistics.
Thomas Landauer, Peter W. Foltz, and Darrel Laham.
1998. Introduction to Latent Semantic Analysis. Dis-
cource Processes, 25:259?284.
Daniel D. Lee and H. Sebastian Seung. 2001. Al-
gorithms for Non-Negative Matrix Factorization. In
Advances in Neural Information Processing Systems
(NIPS).
Nitin Madnani, Joel R. Tetreault, and Martin Chodorow.
2012. Re-examining Machine Translation Metrics for
Paraphrase Identification. In HLT-NAACL, pages 182?
190. The Association for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI.
Kevin P. Murphy. 2012. Machine Learning: A Proba-
bilistic Perspective. The MIT Press.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of HLT-NAACL, pages 181?
189. Association for Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling And Unfolding Recursive Autoen-
coders For Paraphrase Detection. In Advances in Neu-
ral Information Processing Systems (NIPS).
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word Representation: A Simple and General Method
for Semi-Supervised Learning. In ACL, pages 384?
394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Seman-
tics. JAIR, 37:141?188.
Ssephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using Dependency-based Features to Take the
?Para-farce? out of Paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, pages
25?30. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
Clustering based on Non-Negative Matrix Factoriza-
tion. In SIGIR, pages 267?273.
896
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 13?24,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Representation Learning for Text-level Discourse Parsing
Yangfeng Ji
School of Interactive Computing
Georgia Institute of Technology
jiyfeng@gatech.edu
Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
jacobe@gatech.edu
Abstract
Text-level discourse parsing is notoriously
difficult, as distinctions between discourse
relations require subtle semantic judg-
ments that are not easily captured using
standard features. In this paper, we present
a representation learning approach, in
which we transform surface features into
a latent space that facilitates RST dis-
course parsing. By combining the machin-
ery of large-margin transition-based struc-
tured prediction with representation learn-
ing, our method jointly learns to parse dis-
course while at the same time learning a
discourse-driven projection of surface fea-
tures. The resulting shift-reduce discourse
parser obtains substantial improvements
over the previous state-of-the-art in pre-
dicting relations and nuclearity on the RST
Treebank.
1 Introduction
Discourse structure describes the high-level or-
ganization of text or speech. It is central to
a number of high-impact applications, such as
text summarization (Louis et al, 2010), senti-
ment analysis (Voll and Taboada, 2007; Somasun-
daran et al, 2009), question answering (Ferrucci
et al, 2010), and automatic evaluation of student
writing (Miltsakaki and Kukich, 2004; Burstein
et al, 2013). Hierarchical discourse representa-
tions such as Rhetorical Structure Theory (RST)
are particularly useful because of the computa-
tional applicability of tree-shaped discourse struc-
tures (Taboada and Mann, 2006), as shown in Fig-
ure 1.
Unfortunately, the performance of discourse
parsing is still relatively weak: the state-of-the-art
F-measure for text-level relation detection in the
RST Treebank is only slightly above 55% (Joty
when profit was $107.8 million on sales of $435.5 million.
The projections are in the neighborhood of 50 cents a share to 75 cents, compared with a restated $1.65 a share a year earlier,
CIRCUMSTANCE
COMPARISON
Figure 1: An example of RST discourse structure.
et al, 2013). While recent work has introduced
increasingly powerful features (Feng and Hirst,
2012) and inference techniques (Joty et al, 2013),
discourse relations remain hard to detect, due in
part to a long tail of ?alternative lexicalizations?
that can be used to realize each relation (Prasad et
al., 2010). Surface and syntactic features are not
capable of capturing what are fundamentally se-
mantic distinctions, particularly in the face of rel-
atively small annotated training sets.
In this paper, we present a representation learn-
ing approach to discourse parsing. The core idea
of our work is to learn a transformation from a
bag-of-words surface representation into a latent
space in which discourse relations are easily iden-
tifiable. The latent representation for each dis-
course unit can be viewed as a discriminatively-
trained vector-space representation of its meaning.
Alternatively, our approach can be seen as a non-
linear learning algorithm for incremental struc-
ture prediction, which overcomes feature sparsity
through effective parameter tying. We consider
several alternative methods for transforming the
original features, corresponding to different ideas
of the meaning and role of the latent representa-
tion.
Our method is implemented as a shift-reduce
discourse parser (Marcu, 1999; Sagae, 2009).
Learning is performed as large-margin transition-
based structure prediction (Taskar et al, 2003),
while at the same time jointly learning to project
the surface representation into latent space. The
13
resulting system strongly outperforms the prior
state-of-the-art at labeled F-measure, obtaining
raw improvements of roughly 6% on relation la-
bels and 2.5% on nuclearity. In addition, we show
that the latent representation coheres well with the
characterization of discourse connectives in the
Penn Discourse Treebank (Prasad et al, 2008).
2 Model
The core idea of this paper is to project lexical fea-
tures into a latent space that facilitates discourse
parsing. In this way, we can capture the meaning
of each discourse unit, without suffering from the
very high dimensionality of a lexical representa-
tion. While such feature learning approaches have
proven to increase robustness for parsing, POS
tagging, and NER (Miller et al, 2004; Koo et al,
2008; Turian et al, 2010), they would seem to
have an especially promising role for discourse,
where training data is relatively sparse and ambi-
guity is considerable. Prasad et al (2010) show
that there is a long tail of alternative lexicalizations
for discourse relations in the Penn Discourse Tree-
bank, posing obvious challenges for approaches
based on directly matching lexical features ob-
served in the training data.
Based on this observation, our goal is to learn
a function that transforms lexical features into
a much lower-dimensional latent representation,
while simultaneously learning to predict discourse
structure based on this latent representation. In
this paper, we consider a simple transformation
function, linear projection. Thus, we name the ap-
proach DPLP: Discourse Parsing from Linear Pro-
jection. We apply transition-based (incremental)
structured prediction to obtain a discourse parse,
training a predictor to make the correct incremen-
tal moves to match the annotations of training data
in the RST Treebank. This supervision signal is
then used to learn both the weights and the projec-
tion matrix in a large-margin framework.
2.1 Shift-reduce discourse parsing
We construct RST Trees using shift-reduce pars-
ing, as first proposed by Marcu (1999). At each
point in the parsing process, we maintain a stack
and a queue; initially the stack is empty and the
first elementary discourse unit (EDU) in the docu-
ment is at the front of the queue.
1
The parser can
1
We do not address segmentation of text into elemen-
tary discourse units in this paper. Standard classification-
Notation Explanation
V Vocabulary for surface features
V Size of V
K Dimension of latent space
w
m
Classification weights for class m
C Total number of classes, which correspond to
possible shift-reduce operations
A Parameter of the representation function (also
the projection matrix in the linear representa-
tion function)
v
i
Word count vector of discourse unit i
v Vertical concatenation of word count vectors
for the three discourse units currently being
considered by the parser
? Regularization for classification weights
? Regularization for projection matrix
?
i
Slack variable for sample i
?
i,m
Dual variable for sample i and class m
?
t
Learning rate at iteration t
Table 1: Summary of mathematical notation
then choose either to shift the front of the queue
onto the top of the stack, or to reduce the top two
elements on the stack in a discourse relation. The
reduction operation must choose both the type of
relation and which element will be the nucleus.
So, overall there are multiple reduce operations
with specific relation types and nucleus positions.
Shift-reduce parsing can be learned as a classifi-
cation task, where the classifier uses features of
the elements in the stack and queue to decide what
move to take. Previous work has employed deci-
sion trees (Marcu, 1999) and the averaged percep-
tron (Collins and Roark, 2004; Sagae, 2009) for
this purpose. Instead, we employ a large-margin
classifier, because we can compute derivatives of
the margin-based objective function with respect
to both the classifier weights as well as the projec-
tion matrix.
2.2 Discourse parsing with projected features
More formally, we denote the surface feature vo-
cabulary V , and represent each EDU as the nu-
meric vector v ? N
V
, where V = #|V| and the n-
th element of v is the count of the n-th surface fea-
ture in this EDU (see Table 1 for a summary of no-
tation). During shift-reduce parsing, we consider
features of three EDUs:
2
the top two elements on
based approaches can achieve a segmentation F-measure
of 94% (Hernault et al, 2010); a more complex rerank-
ing model does slightly better, at 95% F-Measure with
automatically-generated parse trees, and 96.6% with gold an-
notated trees (Xuan Bach et al, 2012). Human agreement
reaches 98% F-Measure.
2
After applying a reduce operation, the stack will include
a span that contains multiple EDUs. We follow the strong
14
the stack (v
1
and v
2
), and the front of the queue
(v
3
). The vertical concatenation of these vectors
is denoted v = [v
1
;v
2
;v
3
]. In general, we can
formulate the decision function for the multi-class
shift-reduce classifier as
m? = argmax
m?{1,...,C}
w
>
m
f(v;A) (1)
where w
m
is the weight for the m-th class
and f(v;A) is the representation function
parametrized by A. The score for class m (in
our case, the value of taking the m-th shift-
reduce operation) is computed by the inner prod-
uct w
>
m
f(v;A). The specific shift-reduce opera-
tion is chosen by maximizing the decision value in
Equation 1.
The representation function f(v;A) can be de-
fined in any form; for example, it could be a non-
linear function defined by a neural network model
parametrized by A. We focus on the linear projec-
tion,
f(v;A) = Av, (2)
where A ? R
K?3V
is projects the surface repre-
sentation v of three EDUs into a latent space of
size K  V .
Note that by setting
?
w
>
m
= w
>
m
A, the decision
scoring function can be rewritten as
?
w
>
m
v, which
is linear in the original surface features. Therefore,
the expressiveness of DPLP is identical to a linear
separator in the original feature space. However,
the learning problem is considerably different. If
there are C total classes (possible shift-reduce op-
erations), then a linear classifier must learn 3V C
parameters, while DPLP must learn (3V + C)K
parameters, which will be smaller under the as-
sumption that K < C  V . This can be seen
as a form of parameter tying on the linear weights
?
w
m
, which allows statistical strength to be shared
across training instances. We will consider special
cases of A that reduce the parameter space still
further.
2.3 Special forms of the projection matrix
We consider three different constructions for the
projection matrix A.
? General form: In the general case, we place
compositionality criterion of Marcu (1996) and consider only
the nuclear EDU of the span. Later work may explore the
composition of features between the nucleus and satellite.
no special constraint on the form of A.
f(v;A) = A
?
?
v
1
v
2
v
3
?
?
(3)
This form is shown in Figure 2(a).
? Concatenation form: In the concatenation
form, we choose a block structure for A, in
which a single projection matrix B is applied
to each EDU:
f(v;A) =
[
B 0 0
0 B 0
0 0 B
][
v
1
v
2
v
3
]
(4)
In this form, we transform the representa-
tion of each EDU separately, but do not at-
tempt to represent interrelationships between
the EDUs in the latent space. The number
of parameters in A is
1
3
KV . Then, the total
number of parameters, including the decision
weights {w
m
}, in this form is (
V
3
+ C)K.
? Difference form. In the difference form, we
explicitly represent the differences between
adjacent EDUs, by constructing A as a block
difference matrix,
f(v;A) =
[
C ?C 0
C 0 ?C
0 0 0
][
v
1
v
2
v
3
]
, (5)
The result of this projection is that the la-
tent representation has the form [C(v
1
?
v
2
);C(v
1
?v
3
)], representing the difference
between the top two EDUs on the stack, and
between the top EDU on the stack and the
first EDU in the queue. This is intended
to capture semantic similarity, so that reduc-
tions between related EDUs will be preferred.
Similarly, the total number of parameters to
estimate in this form is (V + 2C)
K
3
.
3 Large-Margin Learning Framework
We apply a large margin structure prediction ap-
proach to train the model. There are two pa-
rameters that need to be learned: the classifica-
tion weights {w
m
}, and the projection matrix A.
As we will see, it is possible to learn {w
m
} us-
ing standard support vector machine (SVM) train-
ing (holding A fixed), and then make a simple
gradient-based update to A (holding {w
m
} fixed).
By interleaving these two operations, we arrive at
a saddle point of the objective function.
15
A W
y
v1 from stack v 2 from stack v 3 from queue
(a) General form
A W
y
v1 from stack v 2 from stack v 3 from queue
(b) Concatenation form
A W
y
v1 from stack v 2 from stack v 3 from queue
(c) Difference form
Figure 2: Decision problem with different representation functions
Specifically, we formulate the following con-
strained optimization problem,
min
{w
1:C
,?
1:l
,A}
?
2
C
?
m=1
?w
m
?
2
2
+
l
?
i=1
?
i
+
?
2
?A?
2
F
s.t. (w
y
i
?w
m
)
>
f(v
i
;A) ? 1? ?
y
i
=m
? ?
i
,
? i,m
(6)
where m ? {1, . . . , C} is the index of the
shift-reduce decision taken by the classifier (e.g.,
SHIFT, REDUCE-CONTRAST-RIGHT, etc), i ?
{1, ? ? ? , l} is the index of the training sample, and
w
m
is the vector of classification weights for class
m. The slack variables ?
i
permit the margin con-
straint to be violated in exchange for a penalty, and
the delta function ?
y
i
=m
is unity if y
i
= m, and
zero otherwise.
As is standard in the multi-class linear
SVM (Crammer and Singer, 2001), we can solve
the problem defined in Equation 6 via Lagrangian
optimization:
L({w
1:C
, ?
1:l
,A, ?
1:l,1:C
}) =
?
2
C
?
m=1
?w
m
?
2
2
+
l
?
i=1
?
i
+
?
2
?A?
2
F
+
?
i,m
?
i,m
{
(w
>
m
?w
>
y
i
)f(v
i
;A) + 1? ?
y
i
=m
? ?
i
}
s.t. ?
i,m
? 0 ?i,m
(7)
Then, to optimize L, we need to find a saddle
point, which would be the minimum for the vari-
ables {w
1:C
, ?
1:l
} and the projection matrix A,
and the maximum for the dual variables {?
1:l,1:C
}.
If A is fixed, then the optimization problem is
equivalent to a standard multi-class SVM, in the
transformed feature space f(v
i
;A). We can obtain
the weights {w
1:C
} and dual variables {?
1:l,1:C
}
from a standard dual-form SVM solver. We then
update A, recompute {w
1:C
} and {?
1:l,1:C
}, and
iterate until convergence. This iterative procedure
is similar to the latent variable structural SVM (Yu
and Joachims, 2009), although the specific details
of our learning algorithm are different.
3.1 Learning Projection Matrix A
We update A while holding fixed the weights and
dual variables. The derivative of L with respect to
A is
?L
?A
= ?A+
?
i,m
?
i,m
(w
>
m
?w
>
y
i
)
?f(v
i
;A)
?A
= ?A+
?
i,m
?
i,m
(w
m
?w
y
i
)v
i
>
(8)
Setting
?L
?A
= 0, we have the closed-form solution,
A =
1
?
?
i,m
?
i,m
(w
m
?w
y
i
)v
i
>
=
1
?
?
i,j
(w
y
i
?
?
m
?
i,m
w
m
)v
i
>
,
(9)
because the dual variables for each instance must
sum to one,
?
m
?
i,m
= 1.
Note that for a given i, the matrix (w
y
i
?
?
m
?
i,m
w
m
)v
i
>
is of (at most) rank-1. There-
fore, the solution of A can be viewed as the lin-
ear combination of a sequence of rank-1 matrices,
where each rank-1 matrix is defined by distribu-
tional representation v
i
and the weight difference
between the weight of true label w
y
i
and the ?ex-
pected? weight
?
m
?
i,m
w
m
.
One property of the dual variables is that
f(v
i
;A) is a support vector only if the dual vari-
able ?
i,y
i
< 1. Since the dual variables for each
instance are guaranteed to sum to one, we have
w
y
i
?
?
m
?
i,m
w
m
= 0 if ?
i,y
i
= 1. In other
words, the contribution from non support vectors
to the projection matrix A is 0. Then, we can fur-
ther simplify the updating equation as
A =
1
?
?
v
i
?SV
(w
y
i
?
?
m
?
i,m
w
m
)v
i
>
(10)
This is computationally advantageous since many
instances are not support vectors, and it shows that
the discriminatively-trained projection matrix only
incorporates information from each instance to the
extent that the correct classification receives low
confidence.
16
Algorithm 1 Mini-batch learning algorithm
Input: Training set D, Regularization parame-
ters ? and ? , Number of iteration T , Initializa-
tion matrix A
0
, and Threshold ?
while t = 1, . . . , T do
Randomly choose a subset of training sam-
ples D
t
from D
Train SVM with A
t?1
to obtain {w
(t)
m
} and
{?
(t)
i,m
}
Update A
t
using Equation 11 with ?
t
=
1
t
if
?A
t
?A
t?1
?
F
?A
2
?A
1
?
F
< ? then
Return
end if
end while
Re-train SVM with D and the final A
Output: Projection matrix A, SVM classifier
with weights w
3.2 Gradient-based Learning for A
Solving the quadratic programming defined by the
dual form of the SVM is time-consuming, espe-
cially on a large-scale dataset. But if we focus on
learning the projection matrix A, we can speed up
learning by sampling only a small proportion of
the training data to compute an approximate op-
timum for {w
1:C
, ?
1:l,1:C
}, before each update of
A. This idea is similar to the mini-batch learning,
which has been used in large-scale SVM problem
(Nelakanti et al, 2013) and deep learning models
(Le et al, 2011).
Specifically, in iteration t, the algorithm ran-
domly chooses a subset of training samples D
t
to
train the model. We cannot make a closed-form
update to A based on this small sample, but we
can take an approximate gradient step,
A
t
= (1? ?
t
?)A
t?1
+
?
t
{
?
v
i
?SV(D
t
)
(
w
(t)
y
i
?
?
m
?
(t)
i,m
w
(t)
m
)
v
i
>
}
,
(11)
where ?
t
is a learning rate. In iteration t, we
choose ?
t
=
1
t
. After convergence, we obtain the
weights w by applying the SVM over the entire
dataset, using the final A. The algorithm is sum-
marized in Algorithm 1 and more details about im-
plementation will be clarified in Section 4. While
minibatch learning requires more iterations, the
SVM training is much faster in each batch, and the
overall algorithm is several times faster than using
the entire training set for each update.
4 Implementation
The learning algorithm is applied in a shift-reduce
parser, where the training data consists of the
(unique) list of shift and reduce operations re-
quired to produce the gold RST parses. On test
data, we choose parsing operations in an online
fashion ? at each step, the parsing algorithm
changes the status of the stack and the queue ac-
cording the selected transition, then creates the
next sample with the updated status.
4.1 Parameters and Initialization
There are three free parameters in our approach:
the latent dimension K, and regularization pa-
rameters ? and ? . We consider the values K ?
{30, 60, 90, 150}, ? ? {1, 10, 50, 100} and ? ?
{1.0, 0.1, 0.01, 0.001}, and search over this space
using a development set of thirty document ran-
domly selected from within the RST Treebank
training data. We initialize each element of A
0
to a uniform random value in the range [0, 1]. For
mini-batch learning, we fixed the batch size to be
500 training samples (shift-reduce operations) in
each iteration.
4.2 Additional features
As described thus far, our model considers only
the projected representation of each EDU in its
parsing decisions. But prior work has shown that
other, structural features can provide useful in-
formation (Joty et al, 2013). We therefore aug-
ment our classifier with a set of simple feature
templates. These templates are applied to individ-
ual EDUs, as well as pairs of EDUs: (1) the two
EDUs on top of the stack, and (2) the EDU on top
of the stack and the EDU in front of the queue.
The features are shown in Table 2. In computing
these features, all tokens are downcased, and nu-
merical features are not binned. The dependency
structure and POS tags are obtained from MALT-
Parser (Nivre et al, 2007).
5 Experiments
We evaluate DPLP on the RST Discourse Tree-
bank (Carlson et al, 2001), comparing against
state-of-the-art results. We also investigate the in-
formation encoded by the projection matrix.
5.1 Experimental Setup
Dataset The RST Discourse Treebank (RST-
DT) consists of 385 documents, with 347 for train-
17
Feature Examples
Words at beginning and end of the EDU
?BEGIN-WORD-STACK1 = but?
?BEGIN-WORD-STACK1-QUEUE1 = but, the?
POS tag at beginning and end of the EDU
?BEGIN-TAG-STACK1 = CC?
?BEGIN-TAG-STACK1-QUEUE1 = CC, DT?
Head word set from each EDU. The set includes words
whose parent in the depenency graph is ROOT or is not
within the EDU (Sagae, 2009).
?HEAD-WORDS-STACK2 = working?
Length of EDU in tokens ?LEN-STACK1-STACK2 = ?7, 8??
Distance between EDUs ?DIST-STACK1-QUEUE1 = 2?
Distance from the EDU to the beginning of the document ?DIST-FROM-START-QUEUE1 = 3?
Distance from the EDU to the end of the document ?DIST-FROM-END-STACK1 = 1?
Whether two EDUs are in the same sentence ?SAME-SENT-STACK1-QUEUE1 = True?
Table 2: Additional features for RST parsing
ing and 38 for testing in the standard split. As
we focus on relational discourse parsing, we fol-
low prior work (Feng and Hirst, 2012; Joty et al,
2013), and use gold EDU segmentations. The
strongest automated RST segmentation methods
currently attain 95% accuracy (Xuan Bach et al,
2012).
Preprocessing In the RST-DT, most nodes have
exactly two children, one nucleus and one satellite.
For non-binary relations, we use right-branching
to binarize the tree structure. For multi-nuclear
relations, we choose the left EDU as ?head?
EDU. The vocabulary V includes all unigrams af-
ter down-casing. No other preprocessing is per-
formed. In total, there are 16250 unique unigrams
in V .
Fixed projection matrix baselines Instead of
learning from data, a simple way to obtain a pro-
jection matrix is to use matrix factorization. Re-
cent work has demonstrated the effectiveness of
non-negative matrix factorization (NMF) for mea-
suring distributional similarity (Dinu and Lapata,
2010; Van de Cruys and Apidianaki, 2011). We
can construct B
nmf
in the concatenation form
of the projection matrix by applying NMF to the
EDU-feature matrix, M ?WH. As a result, W
describes each EDU with aK-dimensional vector,
and H describes each word with a K-dimensional
vector. We can then construct B
nmf
by taking
the pseudo-inverse of H, which then projects from
word-count vectors into the latent space.
Another way to construct B is to use neural
word embeddings (Collobert and Weston, 2008).
In this case, we can view the product Bv as a com-
position of the word embeddings, using the simple
additive composition model proposed by Mitchell
and Lapata (2010). We used the word embeddings
from Collobert and Weston (2008) with dimension
{25, 50, 100}. Grid search over heldout training
data was used to select the optimum latent dimen-
sion for both the NMF and word embedding base-
lines. Note that the size K of the resulting projec-
tion matrix is three times the size of the embed-
ding (or NMF representation) due to the concate-
nate construction.
We also consider the special case where A = I.
Competitive systems We compare our approach
with HILDA (Hernault et al, 2010) and TSP (Joty
et al, 2013). Joty et al (2013) proposed two dif-
ferent approaches to combine sentence-level pars-
ing models: sliding windows (TSP SW) and 1
sentence-1 subtree (TSP 1-1). In the comparison,
we report the results of both approaches. All re-
sults are based on the same gold standard EDU
segmentation. We cannot compare with the re-
sults of Feng and Hirst (2012), because they do
not evaluate on the overall discourse structure, but
rather treat each relation as an individual classifi-
cation problem.
Metrics To evaluate the parsing performance,
we use the three standard ways to measure the per-
formance: unlabeled (i.e., hierarchical spans) and
labeled (i.e., nuclearity and relation) F-score, as
defined by Black et al (1991). The application
of this approach to RST parsing is described by
Marcu (2000b).
3
To compare with previous works
on RST-DT, we use the 18 coarse-grained relations
defined in (Carlson et al, 2001).
3
We implemented the evaluation metrics by ourselves.
Together with the DPLP system, all codes are published on
https://github.com/jiyfeng/DPLP
18
Method Matrix Form +Features K Span Nuclearity Relation
Prior work
1. HILDA (Hernault et al, 2010) 83.0 68.4 54.8
2. TSP 1-1 (Joty et al, 2013) 82.47 68.43 55.73
3. TSP SW (Joty et al, 2013) 82.74 68.40 55.71
Our work
4. Basic features A = 0 Yes 79.43 67.98 52.96
5. Word embeddings Concatenation No 75 75.28 67.14 53.79
6. NMF Concatenation No 150 78.57 67.66 54.80
7. Bag-of-words A = I Yes 79.85 69.01 60.21
8. DPLP Concatenation No 60 80.91 69.39 58.96
9. DPLP Difference No 60 80.47 68.61 58.27
10. DPLP Concatenation Yes 60 82.08 71.13 61.63
11. DPLP General Yes 30 81.60 70.95 61.75
Human annotation 88.70 77.72 65.75
Table 3: Parsing results of different models on the RST-DT test set. The results of TSP and HILDA are
reprinted from prior work (Joty et al, 2013; Hernault et al, 2010).
5.2 Experimental Results
Table 3 presents RST parsing results for DPLP and
some alternative systems. All versions
of DPLP outperform the prior state-of-the-art
on nuclearity and relation detection. This includes
relatively simple systems whose features are
simply a projection of the word count vectors
for each EDU (lines 7 and 8). The addition of
the features from Table 2 improves performance
further, leading to absolute F-score improvement
of around 2.5% in nuclearity and 6% in relation
prediction (lines 9 and 10).
On span detection, DPLP performs slightly
worse than the prior state-of-the-art. These sys-
tems employ richer syntactic and contextual fea-
tures, which might be especially helpful for span
identification. As shown by line 4 of the re-
sults table, the basic features from Table 2 pro-
vide most of the predictive power for spans; how-
ever, these features are inadequate at the more
semantically-oriented tasks of nuclearity and re-
lation prediction, which benefit substantially from
the projected features. Since correctly identifying
spans is a precondition for nuclearity and relation
prediction, we might obtain still better results by
combining features from HILDA and TSP with the
representation learning approach described here.
Lines 5 and 6 show that discriminative learning
of the projection matrix is crucial, as fixed projec-
tions obtained from NMF or neural word embed-
dings perform substantially worse. Line 7 shows
that the original bag-of-words representation to-
gether with basic features could give us some ben-
efit on discourse parsing, but still not as good as
results from DPLP. From lines 8 and 9, we see
that the concatenation construction is superior to
the difference construction, but the comparison
between lines 10 and 11 is inconclusive on the
merits of the general form of A. This suggests
that using the projection matrix to model interre-
lationships between EDUs does not substantially
improve performance, and the simpler concatena-
tion construction may be preferred.
Figure 3 shows how performance changes for
different latent dimensions K. At each value of
K, we employ grid search over a development set
to identify the optimal regularizers ? and ? . For
the concatenation construction, performance is not
overly sensitive to K. For the general form of A,
performance decreases with large K. Recall from
Section 2.3 that this construction has nine times as
many parameters as the concatenation form; with
large values of K, it is likely to overfit.
5.3 Analysis of Projection Matrix
Why does projection of the surface features im-
prove discourse parsing? To answer this question,
we examine what information the projection ma-
trix is learning to encoded. We take the projec-
tion matrix from the concatenation construction
and K = 60 as an example for case study. Re-
calling the definition in equation 4, the projection
matrix A will be composed of three identical sub-
matrices B ? R
20?V
. The columns of the B ma-
trix can be viewed as 20-dimensional descriptors
of the words in the vocabulary.
For the purpose of visualization, we further re-
duce the dimension of latent representation from
K = 20 to 2 dimensions using t-SNE (van der
Maaten and Hinton, 2008). One further simpli-
19
30 60 90 150K76
77
78
79
80
81
82
83
84
F-sco
re
Concatenation DPLPGeneral DPLPTSP 1-1 (Joty, et al, 2013)HILDA (Hernault, et al, 2010)
(a) Span
30 60 90 150K65
66
67
68
69
70
71
72
F-sco
re
Concatenation DPLPGeneral DPLPTSP 1-1 (Joty, et al, 2013)HILDA (Hernault, et al, 2010)
(b) Nuclearity
30 60 90 150K
50
52
54
56
58
60
62
F-sco
re
Concatenation DPLPGeneral DPLPTSP 1-1 (Joty, et al, 2013)HILDA (Hernault, et al, 2010)
(c) Relation
Figure 3: The performance of our parser over different latent dimension K. Results for DPLP include
the additional features from Table 3
fication for visualization is we consider only the
top 1000 frequent unigrams in the RST-DT train-
ing set. For comparison, we also apply t-SNE to
the projection matrix B
nmf
recovered from non-
negative matrix factorization.
Figure 4 highlights words that are related to dis-
course analysis. Among the top 1000 words, we
highlight the words from 5 major discourse con-
nective categories provided in Appendix B of the
PDTB annotation manual (Prasad et al, 2008):
CONJUNCTION, CONTRAST, PRECEDENCE, RE-
SULT, and SUCCESSION. In addition, we also
highlighted two verb categories from the top 1000
words: modal verbs and reporting verbs, with their
inflections (Krestel et al, 2008).
From the figure, it is clear DPLP has learned a
projection matrix that successfully groups several
major discourse-related word classes: particularly
modal and reporting verbs; it has also grouped
succession and precedence connectives with some
success. In contrast, while NMF does obtain com-
pact clusters of words, these clusters appear to be
completely unrelated to discourse function of the
words that they include. This demonstrates the
value of using discriminative training to obtain the
transformed representation of the discourse units.
6 Related Work
Early work on document-level discourse parsing
applied hand-crafted rules and heuristics to build
trees in the framework of Rhetorical Structure
Theory (Sumita et al, 1992; Corston-Oliver, 1998;
Marcu, 2000a). An early data-driven approach
was offered by Schilder (2002), who used distribu-
tional techniques to rate the topicality of each dis-
course unit, and then chose among underspecified
discourse structures by placing more topical sen-
tences near the root. Learning-based approaches
were first applied to identify within-sentence dis-
course relations (Soricut and Marcu, 2003), and
only later to cross-sentence relations at the docu-
ment level (Baldridge and Lascarides, 2005). Of
particular relevance to our inference technique are
incremental discourse parsing approaches, such
as shift-reduce (Sagae, 2009) and A* (Muller et
al., 2012). Prior learning-based work has largely
focused on lexical, syntactic, and structural fea-
tures, but the close relationship between discourse
structure and semantics (Forbes-Riley et al, 2006)
suggests that shallow feature sets may struggle
to capture the long tail of alternative lexicaliza-
tions that can be used to realize discourse rela-
tions (Prasad et al, 2010; Marcu and Echihabi,
2002). Only Subba and Di Eugenio (2009) incor-
porate rich compositional semantics into discourse
parsing, but due to the ambiguity of their seman-
tic parser, they must manually select the correct
semantic parse from a forest of possiblities.
Recent work has succeeded in pushing the state-
of-the-art in RST parsing by innovating on sev-
eral fronts. Feng and Hirst (2012) explore rich
linguistic linguistic features, including lexical se-
mantics and discourse production rules suggested
by Lin et al (2009) in the context of the Penn Dis-
course Treebank (Prasad et al, 2008). Muller et
al. (2012) show that A* decoding can outperform
both greedy and graph-based decoding algorithms.
Joty et al (2013) achieve the best prior results
on RST relation detection by (i) jointly perform-
ing relation detection and classification, (ii) per-
forming bottom-up rather than greedy decoding,
and (iii) distinguishing between intra-sentence and
inter-sentence relations. Our approach is largely
orthogonal to this prior work: we focus on trans-
20
although until howeveralsothough
but
thuslater
cancouldwouldshouldandwhenafter so
once willmight may
beforethen
sayssay reportedsaid sayingbelievethink
mustasked
report
(a) Latent representation of words from projection learning
with K = 20.
butwould whenalso maycan then must
mightoncehoweversothoughthus
although
shouldlateruntilwillbefore aftercould
andsayssaid say
asked saying thinkbelieve
report
ConjunctionContrastPrecedenceResultSuccessionModal verbReporting verb
(b) Latent representation of words from non-negative matrix
factorization with K = 20.
Figure 4: t-SNE Visualization on latent representations of words.
forming the lexical representation of discourse
units into a latent space to facilitate learning. As
shown in Figure 4(a), this projection succeeds
at grouping words with similar discourse func-
tions. We might expect to obtain further improve-
ments by augmenting this representation learning
approach with rich syntactic features (particularly
for span identification), more accurate decoding,
and special treatment of intra-sentence relations;
this is a direction for future research.
Discriminative learning of latent features for
discourse processing can be viewed as a form
of representation learning (Bengio et al, 2013).
Also called Deep Learning, such approaches
have recently been applied in a number of NLP
tasks (Collobert et al, 2011; Socher et al, 2012).
Of particular relevance are applications to the de-
tection of semantic or discourse relations, such
as paraphrase, by comparing sentences in an in-
duced latent space (Socher et al, 2011; Guo and
Diab, 2012; Ji and Eisenstein, 2013). In this work,
we show how discourse structure annotations can
function as a supervision signal to discriminatively
learn a transformation from lexical features to a la-
tent space that is well-suited for discourse parsing.
Unlike much of the prior work on representation
learning, we induce a simple linear transforma-
tion. Extension of our approach by incorporating
a non-linear activation function is a natural topic
for future research.
7 Conclusion
We have presented a framework to perform dis-
course parsing while jointly learning to project to
a low-dimensional representation of the discourse
units. Using the vector-space representation of
EDUs, our shift-reduce parsing system substan-
tially outperforms existing systems on nuclearity
detection and discourse relation identification. By
adding some additional surface features, we ob-
tain further improvements. The low dimensional
representation also captures basic intuitions about
discourse connectives and verbs, as shown in Fig-
ure 4(a).
Deep learning approaches typically apply a
non-linear transformation such as the sigmoid
function (Bengio et al, 2013). We have con-
ducted a few unsuccessful experiments with the
?hard tanh? function proposed by Collobert and
Weston (2008), but a more complete exploration
of non-linear transformations must wait for future
work. Another direction would be more sophis-
ticated composition of the surface features within
each elementary discourse unit, such as the hierar-
chical convolutional neural network (Kalchbren-
ner and Blunsom, 2013) or the recursive tensor
network (Socher et al, 2013). It seems likely that
a better accounting for syntax could improve the
latent representations that our method induces.
Acknowledgments
We thank the reviewers for their helpful feedback,
particularly for the connection to multitask learn-
ing. We also want to thank Kenji Sagae and
Vanessa Wei Feng for the helpful discussion via
email communication. This research was sup-
ported by Google Faculty Research Awards to the
second author.
21
References
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 96?103.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation Learning: A Review and New
Perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798?1828.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Phil Harrison, Don Hin-
dle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
A Procedure for Quantitatively Comparing the Syn-
tactic Coverage of English Grammars. In Speech
and Natural Language: Proceedings of a Workshop
Held at Pacific Grove, California, February 19-22,
1991, pages 306?311.
Jill Burstein, Joel Tetreault, and Martin Chodorow.
2013. Holistic discourse coherence annotation
for noisy essay writing. Dialogue & Discourse,
4(2):34?52.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a Discourse-tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Proceedings of Second SIGdial Work-
shop on Discourse and Dialogue.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, page 111. Association for Computa-
tional Linguistics.
R. Collobert and J. Weston. 2008. A Unified Architec-
ture for Natural Language Processing: Deep Neural
Networks with Multitask Learning. In ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural Lan-
guage Processing (Almost) from Scratch. Journal of
Machine Learning Research, 12:2493?2537.
Simon Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization, pages 9?15.
Koby Crammer and Yoram Singer. 2001. On the Algo-
rithmic Implementation of Multiclass Kernel-based
Vector Machines. Journal of Machine Learning Re-
search, 2:265?292.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing Distributional Similarity in Context. In EMNLP,
pages 1162?1172.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistic Features. In
Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al 2010. Building Watson: An overview
of the DeepQA project. AI magazine, 31(3):59?79.
Katherine Forbes-Riley, Bonnie Webber, and Aravind
Joshi. 2006. Computing discourse semantics: The
predicate-argument semantics of discourse connec-
tives in D-LTAG. Journal of Semantics, 23(1):55?
106.
Weiwei Guo and Mona Diab. 2012. Modeling Sen-
tences in the Latent Space. In Proceedings of ACL,
pages 864?872, Jeju Island, Korea, July. Association
for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1?33.
Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive Improvements to Distributional Sentence Simi-
larity. In EMNLP, pages 891?896, Seattle, Washing-
ton, USA, October. Association for Computational
Linguistics.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of ACL.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, pages 119?126, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of ACL-HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Ralf Krestel, Sabine Bergler, and Ren?e Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In LREC,
Marrakech, Morocco, May. European Language Re-
sources Association (ELRA).
Quoc V. Le, Jiquan Ngiam, Adam Coates, Abhik
Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011.
On Optimization Methods for Deep Learning. In
ICML.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing Implicit Discourse Relations in the
Penn Discourse Treebank. In EMNLP.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147?156. Association for Computa-
tional Linguistics.
22
Daniel Marcu and Abdessamad Echihabi. 2002. An
Unsupervised Approach to Recognizing Discourse
Relations. In Proceedings of ACL, pages 368?375,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Daniel Marcu. 1996. Building Up Rhetorical Structure
Trees. In Proceedings of AAAI.
Daniel Marcu. 1999. A Decision-Based Approach to
Rhetorical Parsing. In Proceedings of ACL, pages
365?372, College Park, Maryland, USA, June. As-
sociation for Computational Linguistics.
Daniel Marcu. 2000a. The Rhetorical Parsing of Un-
restricted Texts: A Surface-based Approach. Com-
putational Linguistics, 26:395?448.
Daniel Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name Tagging with Word Clusters and Dis-
criminative Training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL, pages
337?342, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Philippe Muller, Stergos Afantenos, Pascal Denis, and
Nicholas Asher. 2012. Constrained Decoding for
Text-Level Discourse Parsing. In Coling, pages
1883?1900, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.
Anil Kumar Nelakanti, Cedric Archambeau, Julien
Mairal, Francis Bach, and Guillaume Bouchard.
2013. Structured Penalties for Log-Linear Lan-
guage Models. In EMNLP, pages 233?243, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
LREC.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Realization of discourse relations by other
means: alternative lexicalizations. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1023?1031. Asso-
ciation for Computational Linguistics.
Kenji Sagae. 2009. Analysis of Discourse Structure
with Syntactic Dependencies and Data-Driven Shift-
Reduce Parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies (IWPT),
pages 81?84, Paris, France, October. Association for
Computational Linguistics.
Frank Schilder. 2002. Robust discourse parsing via
discourse markers, topicality and position. Natural
Language Engineering, 8(3):235?255.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In NIPS.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Composi-
tionality Through Recursive Matrix-Vector Spaces.
In EMNLP.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing using Syntactic and Lexical Infor-
mation. In NAACL.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive Discourse Parser that uses Rich Linguistic In-
formation. In NAACL-HLT, pages 566?574, Boul-
der, Colorado, June. Association for Computational
Linguistics.
K. Sumita, K. Ono, T. Chino, T. Ukita, and S. Amano.
1992. A discourse structure analyzer for Japanese
text. In Proceedings International Conference on
Fifth Generation Computer Systems, pages 1133?
1140.
Maite Taboada and William C Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse stud-
ies, 8(4):567?588.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In NIPS.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representation: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of ACL, pages 384?394.
Tim Van de Cruys and Marianna Apidianaki. 2011.
Latent Semantic Word Sense Induction and Disam-
biguation. In Proceedings of ACL, pages 1476?
1485, Portland, Oregon, USA, June. Association for
Computational Linguistics.
23
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2759?2605, November.
Kimberly Voll and Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. In Pro-
ceedings of Australian Conference on Artificial In-
telligence.
Ngo Xuan Bach, Nguyen Le Minh, and Akira Shimazu.
2012. A Reranking Model for Discourse Segmenta-
tion using Subtree Features. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 160?168.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169?1176.
ACM.
24
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 97?106,
Baltimore, Maryland USA, June 27, 2014.
c
?2014 Association for Computational Linguistics
Mining Themes and Interests in the Asperger?s and Autism Community
Yangfeng Ji, Hwajung Hong, Rosa Arriaga, Agata Rozga, Gregory Abowd, Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology
{jiyfeng,hwajung,arriaga,agata,abowd,jacobe}@gatech.edu
Abstract
Discussion forums offer a new source of
insight for the experiences and challenges
faced by individuals affected by mental
disorders. Language technology can help
domain experts gather insight from these
forums, by aggregating themes and user
behaviors across thousands of conversa-
tions. We present a novel model for web
forums, which captures both thematic con-
tent as well as user-specific interests. Ap-
plying this model to the Aspies Central fo-
rum (which covers issues related to As-
perger?s syndrome and autism spectrum
disorder), we identify several topics of
concern to individuals who report being on
the autism spectrum. We perform the eval-
uation on the data collected from Aspies
Central forum, including 1,939 threads,
29,947 posts and 972 users. Quantita-
tive evaluations demonstrate that the top-
ics extracted by this model are substan-
tially more than those obtained by Latent
Dirichlet Allocation and the Author-Topic
Model. Qualitative analysis by subject-
matter experts suggests intriguing direc-
tions for future investigation.
1 Introduction
Online forums can offer new insights on men-
tal disorders, by leveraging the experiences of af-
fected individuals ? in their own words. Such
insights can potentially help mental health profes-
sionals and caregivers. Below is an example dia-
logue from the Aspies Central forum,
1
where indi-
viduals who report being on the autism spectrum
(and their families and friends) exchange advice
and discuss their experiences:
1
http://www.aspiescentral.com
? User A: Do you feel paranoid at work?
. . . What are some situations in which you
think you have been unfairly treated?
? User B: Actually I am going through some-
thing like that now, and it is very difficult to
keep it under control. . .
? User A: Yes, yes that is it. Exactly . . . I think
it might be an Aspie trait to do that, I mean
over think everything and take it too literally?
? User B: It probably is an Aspie trait. I?ve
been told too that I am too hard on myself.
Aspies Central, like other related forums, has
thousands of such exchanges. However, aggregat-
ing insight from this wealth of information poses
obvious challenges. Manual analysis is extremely
time-consuming and labor-intensive, thus limiting
the scope of data that can be considered. In addi-
tion, manual coding systems raise validity ques-
tions, because they can tacitly impose the pre-
existing views of the experimenter on all sub-
sequent analysis. There is therefore a need for
computational tools that support large-scale ex-
ploratory textual analysis of such forums.
In this paper, we present a tool for automati-
cally mining web forums to explore textual themes
and user interests. Our system is based on Latent
Dirichlet Allocation (LDA; Blei et al, 2003), but is
customized for this setting in two key ways:
? By modeling sparsely-varying topics, we can
easily recover key terms of interest, while
retaining robustness to large vocabulary and
small counts (Eisenstein et al., 2011).
? By modeling author preference by topic, we
can quickly identify topics of interest for each
user, and simultaneously recover topics that
better distinguish the perspectives of each au-
thor.
The key technical challenge in this work lies in
bringing together several disparate modalities into
97
a single modeling framework: text, authorship,
and thread structure. We present a joint Bayesian
graphical model that unifies these facets, discov-
ering both an underlying set of topical themes,
and the relationship of these themes to authors.
We derive a variational inference algorithm for
this model, and apply the resulting software on a
dataset gathered from Aspies Central.
The topics and insights produced by our system
are evaluated both quantitatively and qualitatively.
In a blind comparison with LDA and the author-
topic model (Steyvers et al., 2004), both subject-
matter experts and lay users find the topics gener-
ated by our system to be substantially more coher-
ent and relevant. A subsequent qualitative analysis
aligns these topics with existing theory about the
autism spectrum, and suggests new potential in-
sights and avenues for future investigation.
2 Aspies Central Forum
Aspies Central (AC) is an online forum for indi-
viduals on the autism spectrum, and has publicly
accessible discussion boards. Members of the site
do not necessarily have to have an official diag-
nosis of autism or a related condition. Neurotyp-
ical individuals (people not on the autism spec-
trum) are also allowed to participate in the fo-
rum. The forum includes more than 19 discussion
boards with subjects ranging from general discus-
sions about the autism spectrum to private discus-
sions about personal concerns. As of March 2014,
AC hosts 5,393 threads, 89,211 individual posts,
and 3,278 members.
AC consists of fifteen public discussion boards
and four private discussion boards that require
membership. We collected data only from
publicly-accessible discussion boards. In addition,
we excluded discussion boards that were website-
specific (announcement-and-introduce-yourself),
those mainly used by family and friends of in-
dividuals on the spectrum (friends-and-family) or
researchers (autism-news-and-research), and one
for amusement (forum-games). Thus, we focused
on ten discussion boards (aspergers-syndrome-
Autism-and-HFA, PDD-NOS-social-anxiety-and-
others, obsessions-and-interests, friendships-and-
social-skills, education-and-employment, love-
relationships-and-dating, autism-spectrum-help-
and-support, off-topic-discussion, entertainment-
discussion, computers-technology-discussion), in
which AC users discuss their everyday expe-
? ?d
zdpn
wdpn
m
?k
ad
?
bi
yik ?N PD KAK
Figure 1: Plate diagram. Shaded notes represent observed
variables, clear nodes represent latent variables, arrows in-
dicate probabilistic dependencies, and plates indicate repeti-
tion.
riences, concerns, and challenges. Using the
python library Beautiful Soup, we collected 1,939
threads (29,947 individual posts) from the discus-
sion board archives over a time period from June
1, 2010 to July 27, 2013. For a given post, we
extracted associated metadata such as the author
identifier and posting timestamps.
3 Model Specification
Our goal is to develop a model that captures the
preeminent themes and user behaviors from traces
of user behaviors in online forums. The model
should unite textual content with authorship and
thread structure, by connecting these observed
variables through a set of latent variables rep-
resenting conceptual topics and user preferences.
In this section, we present the statistical specifi-
cation of just such a model, using the machinery
of Bayesian graphical models. Specifically, the
model descibes a stochastic process by which the
observed variables are emitted from prior proba-
bility distributions shaped by the latent variables.
By performing Bayesian statistical inference in
this model, we can recover a probability distribu-
tion around the latent variables of interest.
We now describe the components of the model
that generate each set of observed variables. The
model is shown as a plate diagram in Figure 1, and
the notation is summarized in Table 1.
3.1 Generating the text
The part of the model which produces the text it-
self is similar to standard latent Dirichlet alloca-
tion (LDA) (Blei et al., 2003). We assume a set
of K latent topics, which are distributions over
each word in a finite vocabulary. These topics are
98
Symbol Description
D number of threads
P
d
number of posts in thread d
N
p
number of word tokens in post p
? parameter of topic distribution of threads
?
d
the multinomial distribution of topics specific to the thread d
z
dpn
the topic associated with the nth token in post p of thread d
w
dpn
the nth token in post p of thread d
a
d
authorship distribution for question post and answer posts in
thread d respectively
y
ik
the topic-preference indicator of author i on topic k
b
i
the Gaussian distribution of author i?s selection bias
?
k
topic k in log linear space
m background topic
? topic weights matrix
?
2
?
variance of feature weights
?
2
b
variance of selection bias
? prior probability of authors? preference on any topic
Table 1: Mathematical notations
shared among all D threads in the collection, but
each thread has its own distribution over the top-
ics.
We make use of the SAGE parametrization for
generative models of text (Eisenstein et al., 2011).
SAGE uses adaptive sparsity to induce topics that
deviate from a background word distribution in
only a few key words, without requiring a regular-
ization parameter. The background distribution is
written m, and the deviation for topic k is written
?
k
, so that Pr(w = v|?
k
,m) ? exp (m
v
+ ?
kv
).
Each word tokenw
dpn
(the n
th
word in post p of
thread d) is generated from the probability distri-
bution associated with a single topic, indexed by
the latent variable z
dpn
? {1 . . .K}. This latent
variable is drawn from a prior ?
d
, which is the
probability distribution over topics associated with
all posts in thread d.
3.2 Generating the author
We have metadata indicating the author of each
post, and we assume that users are more likely
to participate in threads that relate to their topic-
specific preference. In addition, some people may
be more or less likely to participate overall. We
extend the LDA generative model to incorporate
each of these intuitions.
For each author i, we define a latent preference
vector y
i
, where y
ik
? {0, 1} indicates whether
the author i prefers to answer questions about
topic k. We place a Bernoulli prior on each y
ik
, so
that y
ik
? Bern(?), where Bern(y; ?) = ?
y
(1 ?
?)
(1?y)
. Induction of y is one of the key infer-
ence tasks for the model, since this captures topic-
specific preference.
It is also a fact that some individuals will partic-
ipate in a conversation regardless of whether they
have anything useful to add. To model this gen-
eral tendency, we add an ?bias? variable b
i
? R.
When b
i
is negative, this means that author i will
be reluctant to participate even when she does have
relevant interests.
Finally, various topics may require different lev-
els of preference; some may capture only general
knowledge that many individuals are able to pro-
vide, while others may be more obscure. We in-
troduce a diagonal topic-weight matrix ?, where
?
kk
= ?
k
? 0 is the importance of preference for
topic k. We can easily generalize the model by in-
cluding non-zero off-diagonal elements, but leave
this for future work.
The generative distribution for the observed au-
thor variable is a log-linear function of y and b:
Pr(a
di
= 1|?
d
,y,?, b) =
exp(?T
d
?y
i
+ b
i
)
?
A
j=1
exp(?T
d
?y
j
+ b
j
)
(1)
This distribution is multinomial over authors; each
author?s probability of responding to a thread de-
pends on the topics in the thread (?
d
), the author?s
preference on those topics (y
i
), the importance of
preference for each topic (?), and the bias parame-
ter b
i
. We exponentiate and then normalize, yield-
ing a multinomial distribution.
The authorship distribution in Equation (1)
refers to a probability of user i authoring a single
response post in thread d (we will handle question
posts next). Let us construct a binary vector a
(r)
d
,
where it is 1 if author i has authored any response
posts in thread d, and zero otherwise. The proba-
bility distribution for this vector can be written
P (a(r)
d
|?
d
,y,?, b) ?
A?
i=1
(
exp(?T
d
?y
i
+ b
i
)
?
A
j=1
exp(?T
d
?y
j
+ b
j
)
)
a
(r)
di
(2)
One of the goals of this model is to distinguish
frequent responders (i.e., potential experts) from
individuals who post questions in a given topic.
Therefore, we make the probability of author i ini-
tiating thread d depend on the value 1 ? y
ki
for
each topic k. We write the binary vector a
(q)
d
,
where a
(q)
di
= 1 if author i has written the ques-
tion post, and zero otherwise. Note that there can
only be one question post, so a
(q)
d
is an indicator
vector. Its probability is written as
p(a(q)
d
|?
d
,y,?, b) ?
A?
i=1
(
exp(?T
d
?(1? y
i
) + b
i
)
?
A
j=1
exp(?T
d
?(1? y
j
) + b
j
)
)
a
(q)
di
(3)
99
We can put these pieces together for a complete
distribution over authorship for thread d:
P (a
d
, |?
d
,y,?, b) ?
A?
i=1
(
exp(?T
d
?y
i
+ b
i
)
?
A
j=1
exp(?T
d
?y
j
+ b
j
)
)
a
(r)
di
?
A?
i=1
(
exp(?T
d
?(1? y
i
) + b
i
)
?
A
j=1
exp(?T
d
?(1? y
j
) + b
j
)
)
a
(q)
di
(4)
where a
d
= {a
(q)
d
,a
(r)
d
}. The probability
p(a
d
|?
d
,y,?, b) combines the authorship distri-
bution of authors from question post and answer
posts in thread d. The identity of the original ques-
tion poster does not appear in the answer vector,
since further posts are taken to be refinements of
the original question.
This model is similar in spirit to super-
vised latent Dirichlet allocation (sLDA) (Blei and
McAuliffe, 2007). However, there are two key dif-
ferences. First, sLDA uses point estimation to ob-
tain a weight for each topic. In contrast, we per-
form Bayesian inference on the author-topic pref-
erence y. Second, sLDA generates the metadata
from the dot-product of the weights and
?
z, while
we use ? directly. The sLDA paper argues that
there is a risk of overfitting, where some of the top-
ics serve only to explain the metadata and never
generate any of the text. This problem does not
arise in our experiments.
3.3 Formal generative story
We are now ready to formally define the generative
process of our model:
1. For each topic k
(a) Set the word probabilities ?
k
=
exp(m+?
k
)?
i
exp(m
i
+?
ki
)
2. For each author i
(a) Draw the selection bias b
i
? N (0, ?
2
b
)
(b) For each topic k
i. Draw the author-topic preference
level y
ik
? Bern(?)
3. For each thread d
(a) Draw topic proportions ?
d
? Dir(?)
(b) Draw the author vector a
d
from Equa-
tion (4)
(c) For each post p
i. For each word in this post
A. Draw topic assignment z
dpn
?
Mult(?
d
)
B. Draw word
w
dpn
? Mult(?
z
dpn
)
4 Inference and estimation
The purpose of inference and estimation is to re-
cover probability distributions and point estimates
for the quantities of interest: the content of the
topics, the assignment of topics to threads, au-
thor preferences for each topic, etc. While recent
progress in probabilistic programming has im-
proved capabilities for automating inference and
estimation directly from the model specification,
2
here we develop a custom algorithm, based on
variational mean field (Wainwright and Jordan,
2008). Specifically, we approximate the distribu-
tion over topic proportions, topic indicators, and
author-topic preference P (?, z,y|w,a,x) with a
mean field approximation
q(?,z,y|?, ?, ?) =
A?
i=1
K?
k=1
q(y
ik
|?
ik
)
D?
d=1
P
d?
p=1
N
p,d?
n=1
q(z
dpn
|?
dpn
)
D?
d=1
q(?
d
|?
d
)
(5)
where P
d
is the number of posts in thread d, K
is the number of topics, and N
p
is the number of
word tokens in post P
d
. The variational parame-
ters of q(?) are ?, ?, ?. We will write ??? to indicate
an expectation under the distribution q(?, z,y).
We employ point estimates for the variables
b (author selection bias), ? (topic-time feature
weights), ? (topic-word log-probability devia-
tions), and diagonal elements of ? (topic weights).
The estimation of ? follows the procedure defined
in SAGE (Eisenstein et al., 2011); we explain the
estimation of the remaining parameters below.
Given the variational distribution in Equation
(5), the inference on our topic model can be for-
mulated as constrained optimization of this bound.
min L(?, ?, ?; b,?,?)
s.t.?
dk
? 0 ?d, k
?
dpn
? 0,
?
k
?
dpnk
= 1 ?d, p, n
0 ? ?
ik
? 1 ?i, k
?
k
? 0 ?k
(6)
The constraints are due to the parametric form
of the variational approximation: q(?
d
|?
d
) is
Dirichlet, and requires non-negative parameters;
2
see http://probabilistic-programming.
org/
100
q(z
dpn
|?
dpn
) is multinomial, and requires that
?
dpn
lie on the K ? 1 simplex; q(y
ik
|?
ik
) is
Bernoulli and requires that ?
ik
be between 0 and
1. In addition, as a topic weight, ?
k
should also be
non-negative.
Algorithm 1 One pass of the variational inference
algorithm for our model.
for d = 1, . . . , D do
while not converged do
for p = 1, . . . , P
d
do
for n = 1, . . . , N
p,d
do
Update ?
dpnk
using Equation (7) for each k =
1, . . . ,K
end for
end for
Update ?
dk
by optimizing Equation (6) with Equa-
tion (10) for each k = 1, . . . ,K
end while
end for
for i = 1, . . . , A do
Update ?
ik
by optimizing Equation (6) with Equa-
tion (13) for each k = 1, . . . ,K
Update
?
b
i
by optimizing Equation (6) with Equa-
tion (14)
end for
for k = 1, . . . ,K do
Update ?
k
with Equation (15)
end for
4.1 Word-topic indicators
With the variational distribution in Equation (5),
the inference on ?
dpn
for a given token n in post p
of thread d is same as in LDA. For the nth token
in post p of thread d,
?
dpnk
? ?
kw
dpn
exp(?log ?
dk
?) (7)
where ? is defined in the generative story and
?log ?
dk
? is the expectation of log ?
dk
under the
distribution q(?
dk
|?
d
),
?log ?
dk
? = ?(?
dk
)??(
K
?
k=1
?
dk
) (8)
where ?(?) is the Digamma function, the first
derivative of the log-gamma function.
For the other variational parameters ? and ?, we
can not obtain a closed form solution. As the con-
straints on these parameters are all convex with re-
spect to each component, we employed a projected
quasi-Newton algorithm proposed in (Schmidt et
al., 2009) to optimize L in Equation (6). One pass
of the variational inference procedure is summa-
rized in Algorithm 1.Since every step in this algo-
rithm will not decrease the variational bound, the
overall algorithm is guaranteed to converge.
4.2 Document-topic distribution
The inference for document-topic proportions is
different from LDA, due to the generation of the
author vector a
d
, which depends on ?
d
. For a
given thread d, the part of the bound associated
with the variational parameter ?
d
is
L
?
d
= ?log p(?
d
|?
d
)?+ ?log p(a
d
|?
d
,y,?, b)?
+
P
d?
p=1
N
p,d?
n=1
?log p(z
dpn
|?
d
)? ? ?q(?
d
|?
d
)?
(9)
and the derivative of L
?
d
with respect to ?
dk
is
dL
?
d
d?
dk
= ?
?
(?
dk
)(?
dk
+
P
d?
p=1
N
p,d?
n=1
?
dpnk
? ?
dk
)
??
?
(
K?
k=1
?
dk
)
K?
k=1
(?
dk
+
P
d?
p=1
N
p,d?
n=1
?
dpnk
? ?
dk
)
+
d
d?
dk
?log p(a
d
|?
d
,y,?, b)? ,
(10)
where ?
?
(?) is the trigramma function. The first
two lines of Equation (10) are identical to LDA?s
variational inference, which obtains a closed-form
solution by setting ?
dk
= ?
dk
+
?
p,n
?
dpnk
. The
additional term for generating the authorship vec-
tor a
d
eliminates this closed-form solution and
forces us to turn to gradient-based optimization.
The expectation on the log probability of the
authorship involves the expectation on the log
partition function, which we approximate using
Jensen?s inequality. We then derive the gradient,
?
??
dk
?log p(a
d
|?
d
,y,?, b)?
? ?
k
(
A?
i=1
a
(r)
di
?
ik
?A
(r)
d
A?
i=1
?
ik
?
a
(r)
di
|?
d
,y
?
)
? ?
k
(
A?
i=1
a
(q)
di
?
ik
?
A?
i=1
?
ik
?
a
(q)
di
|?
d
,y
?
)
(11)
The convenience variable A
(r)
d
counts the number
of distinct response authors in thread d; recall that
there can be only one question author. The nota-
tion
?
a
(r)
di
|?
d
,y
?
=
exp(
?
?
T
?
? ?y
i
?+ b
i
)
?
j
exp(
?
?
T
?
? ?y
j
?+ b
j
)
,
represents the generative probability of a
(r)
di
= 1
under the current variational distributions q(?
d
)
and q(y
i
). The notation
?
a
(q)
di
|?
d
,y
?
is analo-
gous, but represents the question post indicator
a
(q)
di
.
101
4.3 Author-topic preference
The variational distribution over author-topic
preference is q(y
ik
|?
ik
); as this distribution is
Bernoulli, ?y
ik
? = ?
ik
, the parameter itself prox-
ies for the topic-specific author preference ? how
much author i prefers to answer posts on topic k.
The part of the variational bound the relates to
the author preferences is
L
?
=
D?
d=1
?log p(a
d
|?
d
,y,?, b)?
+
A?
i=1
K?
k=1
?p(y
ik
|?)? ?
A?
i=1
K?
k=1
?q(y
ik
|?
ik
)?
(12)
For author i on topic k, the derivative of
?log p(a
d
|?
d
,y,?, b)? for document d with re-
spect to ?
ik
is
d
d?
ik
?logP (a
d
|?
d
,y,?, b)?
? ??
dk
??
k
(
a
(r)
di
?
?
a
(r)
di
|?
d
,y
?
? a
(q)
di
+
?
a
(q)
di
|?
d
,y
?)
,
(13)
where ??
dk
? =
?
dk?
k
?
?
dk
?
. Thus, participating as a
respondent increases ?
ik
to the extent that topic k
is involved in the thread; participating as the ques-
tioner decreases ?
ik
by a corresponding amount.
4.4 Point estimates
We make point estimates of the following param-
eters: author selection bias b
i
and topic-specific
preference weights ?
k
. All updates are based
on maximum a posteriori estimation or maximum
likelihood estimation.
Selection bias For the selection bias b
i
of au-
thor i given a thread d, the objective function in
Equation (6) with the prior of b
i
? N (0, ?
2
b
) is
minimized by a quasi-Newton algorithm with the
following derivative
?
?b
i
?logP (a
d
|?
d
,y,?, b)? ? a(r)
d,i
?
?
a
(r)
di
|?
d
,y
?
+ a
(q)
d,i
?
?
a
(q)
di
|?
d
,y
? (14)
The zero-mean Gaussian prior shrinks b
i
towards
zero by subtracting b
i
/?
2
b
from this gradient. Note
that the gradient in Equation (14) is non-negative
whenever author i participates in thread d. This
means any post from this author, whether question
posts or answer posts, will have a positive contri-
bution of the author?s selection bias. This means
that any activity in the forum will elevate the se-
lection bias b
i
, but will not necessarily increase the
imputed preference level.
Topic weights The topic-specific preference
weight ?
k
is updated by considering the derivative
of variational bound with respect to ?
k
?L
??
k
=
D
?
d=1
?
??
k
?p(a
d
|?
d
,y,?, b)? (15)
where for a given document d,
?
??
k
?log p(a
d
|?
d
,y,?, b)? ? ??
dk
??
k
?
A?
i=1
?
ik
(
a
(r)
i
? a
(q)
i
+
?
a
(q)
di
|?
d
,y
?
?A
(r)
d
?
a
(r)
di
|?
d
,y
?)
Thus, ?
k
will converge at a value where the ob-
served posting counts matches the expectations
under ?log p(a
d
|?
d
,y,?, b)?.
5 Quantitative Evaluation
To validate the topics identified by the model,
we performed a manual evaluation, combining the
opinions of both novices as well as subject matter
experts in Autism and Asberger?s Syndrome. The
purpose of the evaluation is to determine whether
the topics induced by the proposed model are more
coherent than topics from generic alternatives such
as LDA and the author-topic model, which are not
specifically designed for forums.
5.1 Experiment Setup
Preprocessing Preprocessing was minimal. We
tokenized texts using white space and removed
punctuations at the beginning/end of each token.
We removed words that appear less than five
times, resulting in a vocabulary of the 4903 most
frequently-used words.
Baseline Models We considered two baseline
models in the evaulation. The first baseline model
is latent Dirichlet allocation (LDA), which consid-
ers only the text and ignores the metadata (Blei
et al., 2003). The second baseline is the Author-
Topic (AT) model, which extends LDA by associ-
ating authors with topics (Rosen-Zvi et al., 2004;
Steyvers et al., 2004). Both baselines are im-
plemented in the Matlab Topic Modeling Tool-
box (Steyvers and Griffiths, 2005).
Parameter Settings For all three models, we set
K = 50. Our model includes the three tunable
parameters ?, the Bernoulli prior on topic-specific
expertise; ?
2
b
, the variance prior on use selection
102
bias; and ?, the prior on document-topic distri-
bution. In the following experiments, we chose
? = 0.2, ?
2
b
= 1.0, ? = 1.0. LDA and AT share
two parameters, ?, the symmetric Dirichlet prior
for document-topic distribution; ?, the symmetric
Dirichlet prior for the topic-word distribution. In
both models, we set ? = 3.0 and ? = 0.01. All
parameters were selected in advance of the experi-
ments; further tuning of these paramters is left for
future work.
5.2 Topic Coherence Evaluation
To be useful, a topic model should produce topics
that human readers judge to be coherent. While
some automated metrics have been shown to co-
here with human coherence judgments (Newman
et al., 2010), it is possible that naive raters might
have different judgments from subject matter ex-
perts. For this reason, we focused on human eval-
uation, including both expert and novice opinions.
One rater, R1, is an author of the paper (HH) and
a Ph.D. student focusing on designing technology
to understand and support individuals with autism
spectrum disorder. The remaining three raters are
not authors of the paper and are not domain ex-
perts.
In the evaluation protocol, raters were presented
with batteries of fifteen topics, from which they
were asked to select the three most coherent. In
each of the ten batteries, there were five topics
from each model, permuted at random. Thus, af-
ter completing the task, all 150 topics ? 50 topics
from each model ? were rated. The user interface
of topic coherence evaluation is given in Figure 2,
including the specific prompt.
We note that this evaluation differs from the
?intrusion task? proposed by Chang et al. (2009),
in which raters are asked to guess which word
was randomly inserted into a topic. While the in-
trusion task protocol avoids relying on subjective
judgments of the meaning of ?coherence,? it pre-
vents expert raters from expressing a preference
for topics that might be especially useful for anal-
ysis of autism spectrum disorder. Prior work has
also shown that the variance of these tasks is high,
making it difficult to distinguish between models.
Table 2 shows, for each rater, the percentage of
topics were chosen from each model as the most
coherent within each battery. On average, 80% of
the topics were chosen from our proposed model.
If all three models are equally good at discover-
Figure 2: The user interface of topic coherence
evaluation.
Rater
Model R1 R2 R3 R4 Average
Our model 70% 93% 80% 77% 80%
AT 17% 7% 13% 10% 12%
LDA 13% 0% 7% 13% 8%
Table 2: Percentage of the most coherent topics that are
selected from three different topic models: our model, the
Author-Topic Model (AT), and latent Dirichlet allocation
(LDA).
ing coherent topics, the average percentage across
three models should be roughly equal. Note that
the opinion of the expert rater R1 is generally sim-
ilar to the other three raters.
6 Analysis of Aspies Central Topics
In this section, we further use our model to ex-
plore more information about the Aspies Central
forum. We want to examine whether the autism-
related topics identified the model can support re-
searchers to gain qualitative understanding of the
needs and concerns of autism forum users. We are
also interested in understanding the users? behav-
ioral patterns on autism-related topics. The anal-
ysis task has three components: first we will de-
scribe the interesting topics from the autism do-
main perpective. Then we will find out the pro-
portion of each topic, including autism related top-
ics. Finally, in order to understand the user activ-
ity patterns on these autism related topics we will
derive the topic-specific preference ranking of the
users from our model.
103
Index Proportion Top keywords Index Proportion Top keywords
1 1.7% dont im organization couldnt construction 2 2.6% yah supervisor behavior taboo phone
3 2.2% game watched games fallout played 4 3.5% volunteering esteem community art self
5 1.1% nobody smell boss fool smelling 6 3.2% firefox razor blades pc console
7 3.4% doesn?t it?s mandarin i?ve that?s 8 2.1% diagnosed facessenses visualize visual
9 1.7% obsessions bookscollecting library authors 10 2.6% ptsd central cure neurotypical we
11 1.2% stims mom nails lip shoes 12 1.8% classroom campus tag numbers exams
13 1.6% battery hawke charlie ive swing 14 1.9% divorce william women marryrates
15 0.1% chocolate pdd milk romance nose 16 5.8% kinda holland neccesarily employment bucks
17 0.6% eat burgers jokes memory foods 18 2.4% dryer martial dream wake schedule
19 3.7% depression beleive christianity buddhism becouse 20 1.4% grudges pairs glasses museum frames
21 0.4% alma star gods alien sun 22 2.6% facebook profiles befriend friendships friends
23 0.4% trilogy sci-fi cartoon iphone grandma 24 2.7% flapping stuffed toes curse animal
25 1.5% empathy smells compassion emotions emotional 26 1.7% males evolution females originally constructive
27 0.5% list dedicate lists humor song 28 4.6% nts aspies autie qc intuitive
29 2.7% captain i?m film anime that?s 30 3.6% homeless pic wild math laugh
31 3.3% shave exhausting during terrified products 32 5.6% you?re you your yourself hiring
33 4.6% dictionary asks there?re offend fog 34 1.5% grade ed school 7th diploma
35 1.0% cave blonde hair bald disney 36 1.9% diagnosis autism syndrome symptoms aspergers
37 1.3% song joanna newsom rap favorites 38 1.8% poetry asleep children ghosts lots
39 2.1% heat iron adhd chaos pills 40 3.6% bike zone rides zoning worrying
41 1.2% uk maths team teams op 42 0.8% book books read reading kindle
43 1.0% husband narcissist husband?s he hyper 44 1.1% songs guitar drums music synth
45 1.3% autism disorder spectrum disorders pervasive 46 0.7% dog noise dogs barking noisy
47 0.6% relationship women relationships sexual sexually 48 0.9% weed marijuana pot smoking fishing
49 0.9% him he his bernard je 50 2.0% her she she?s kyoko she?ll
Table 3: 50 topics identified by our model. The ?proportion? columns show the topic proportions in the
dataset. Furthermore, 14 topics are highlighted as interesting topics for autism research.
Table 3 shows all 50 topics from our model. For
each topic, we show the top five words related to
this topic. We further identified fourteen topics
(highlighted with blue color), which are particu-
larly relevant to understand autism.
Among the identified topics, there are three
popular topics discussed in the Aspies Central fo-
rum: topic 4, topic 19 and topic 31. From the top
word list, we identified that topic 4 is composed
of keywords related to psychological (e.g., self-
esteem, art) and social (e.g., volunteering, com-
munity) well-being of the Aspies Central users.
Topic 19 includes discussion on mental health
issues (e.g., depression) and religious activities
(e.g., believe, christianity, buddhism) as coping
strategies. Topic 31 addresses a specific personal
hygiene issue ? helping people with autism learn
to shave. This might be difficult for individuals
with sensory issues: for example, they may be
terrified by the sound and vibration generated by
the shaver. For example, topic 22 is about mak-
ing friends and maintaining friendship; topic 12 is
about educational issues ranging from seeking ed-
ucational resources to improving academic skills
and adjusting to college life.
In addition to identifying meaningful topics, an-
other capability of our model is to discover users?
topic preferences and expertise. Recall that, for
user i and topic k, our model estimates a author-
topic preference variable ?
ik
. Each ?
ik
ranges
from 0 to 1, indicating the probability of user i to
Topic User index
5 USER 1, USER 2, USER 3, USER 4, USER 5
8 USER 1, USER 2, USER 6, USER 5, USER 7
12 USER 1, USER 2, USER 4, USER 8, USER 3
19 USER 1, USER 2, USER 3, USER 4, USER 7
22 USER 1, USER 2, USER 3, USER 9, USER 7
31 USER 1, USER 3, USER 2, USER 6, USER 10
36 USER 1, USER 2, USER 4, USER 3, USER 11
45 USER 1, USER 3, USER 4, USER 12, USER 13
47 USER 2, USER 14, USER 15, USER 16 , USER 6
48 USER 5, USER 4, USER 6, USER 9, USER 2
Table 4: The ranking of user preference on some interest-
ing topics (we replace user IDs with user indices to avoid
any privacy-related issue). USER 1 is the moderator of this
forum. In total, our model identifies 16 user with high topic-
specific preference from 10 interesting topics. For the other
4 interesting topics, there is no user with significantly high
preference.
answer a question on topic k. As we set the prior
probability of author-topic preference to be 0.2,
we show topic-author pairs for which ?
ik
> 0.2
in Table 4.
The dominance of USER 1 in these topics is ex-
plained by the fact that this user is the moderator
of the forum. Besides, we also find some other
users participating in most of the interesting top-
ics, such as USER 2 and USER 3. On the other
hand, users like USER 14 and USER 15 only show
up in few topics. This observation is supported by
their activities on discussion boards. Searching on
the Aspies Certral forum, we found most answer
posts of user USER 15 are from the board ?love-
104
relationships-and-dating?.
7 Related Work
Social media has become an important source of
health information (Choudhury et al., 2014). For
example, Twitter has been used both for mining
both public health information (Paul and Dredze,
2011) and for estimating individual health sta-
tus (Sokolova et al., 2013; Teodoro and Naaman,
2013). Domain-specific online communities, such
Aspies Central, have their own advantages, tar-
geting specific issues and featuring more close-
knit and long-term relationships among mem-
bers (Newton et al., 2009).
Previous studies on mining health information
show that technical models and tools from com-
putational linguistics are helpful for both under-
standing contents and providing informative fea-
tures. Sokolova and Bobicev (2011) use sentiment
analysis to analyze opinions expressed in health-
related Web messages; Hong et al. (2012) focus
on lexical differences to automatically distinguish
schizophrenic patients from healthy individuals.
Topic models have previously been used to
mine health information: Resnik et al. (2013) use
LDA to improve the prediction for neuroticism
and depression on college students, while Paul and
Dredze (2013) customize their factorial LDA to
model the joint effect of drug, aspect, and route
of administration. Most relevantly for the current
paper, Nguyen et al. (2013) use LDA to discover
autism-related topics, using a dataset of 10,000
posts from ten different autism commnities. How-
ever, their focus was on automated classification of
communities as autism-related or not, rather than
on analysis and on providing support for qualita-
tive autism researchers. The applicability of the
model developed in our paper towards classifica-
tion tasks is a potential direction for future re-
search.
In general, topic models capture latent themes
in document collections, characterizing each doc-
ument in the collection as a mixture of topics (Blei
et al., 2003). A natural extension of topic mod-
els is to infer the relationships between topics and
metadata such as authorship or time. A relatively
simple approach is to represent authors as an ag-
gregation of the topics in all documents they have
written (Wagner et al., 2012). More sophisticated
topic models, such as Author-Topic (AT) model
(Rosen-Zvi et al., 2004; Steyvers et al., 2004) as-
sume that each document is generated by a mix-
ture of its authors? topic distributions. Our model
can be viewed as one further extension of topic
models by incorporating more metadata informa-
tion (authorship, thread structure) in online fo-
rums.
8 Conclusion
This paper describes how topic models can offer
insights on the issues and challenges faced by in-
dividuals on the autism spectrum. In particular,
we demonstrate that by unifying textual content
with authorship and thread structure metadata, we
can obtain more coherent topics and better under-
stand user activity patterns. This coherence is val-
idated by manual annotations from both experts
and non-experts. Thus, we believe that our model
provides a promising mechanism to capture be-
havioral and psychological attributes relating to
the special populations affected by their cognitive
disabilities, some of which may signal needs and
concerns about their mental health and social well-
being.
We hope that this paper encourages future ap-
plications of topic modeling to help psychologists
understand the autism spectrum and other psycho-
logical disorders ? and we hope to obtain further
validation of our model through its utility in such
qualitative research. Other directions for future
work include replication of our results across mul-
tiple forums, and applications to other conditions
such as depression and attention deficit hyperac-
tivity disorder (ADHD).
Acknowledgments
This research was supported by a Google Faculty
Award to the last author. We thank the three re-
viewers for their detailed and helpful suggestions
to improve the paper.
References
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised Topic Models. In NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
Tea Leaves: How Humans Interpret Topic Models.
In Yoshua Bengio, Dale Schuurmans, John D. Laf-
ferty, Christopher K. I. Williams, and Aron Culotta,
105
editors, NIPS, pages 288?296. Curran Associates,
Inc.
Munmun De Choudhury, Meredith Ringel Morris, and
Ryen W. White. 2014. Seeking and Sharing Health
Information Online: Comparing Search Engines and
Social Media. In Procedings of CHI.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse Additive Generative Models of Text. In
ICML.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal Differences in Autobiographical Narratives from
Schizophrenic Patients and Healthy Controls. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
37?47. Association for Computational Linguistics,
July.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100?108. Association for Computa-
tional Linguistics.
A. Taylor Newton, Adam D.I. Kramer, and Daniel N.
McIntosh. 2009. Autism online: a comparison
of word usage in bloggers with and without autism
spectrum disorders. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 463?466. ACM.
Thin Nguyen, Dinh Phung, and Svetha Venkatesh.
2013. Analysis of psycholinguistic processes and
topics in online autism communities. In Multimedia
and Expo (ICME), 2013 IEEE International Confer-
ence on, pages 1?6. IEEE.
Michael J. Paul and Mark Dredze. 2011. You Are
What You Tweet: Analyzing Twitter for Public
Health. In ICWSM.
Michael J. Paul and Mark Dredze. 2013. Drug Ex-
traction from the Web: Summarizing Drug Expe-
riences with Multi-Dimensional Topic Models. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 168?178, Atlanta, Georgia, June. Association
for Computational Linguistics.
Philip Resnik, Anderson Garron, and Rebecca Resnik.
2013. Using Topic Modeling to Improve Prediction
of Neuroticism and Depression in College Students.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The Author-Topic
Model for Authors and Documents. In UAI.
Mark Schmidt, Ewout van den Berg, Michael P. Fried-
lander, and Kevin Muphy. 2009. Optimizing Costly
Functions with Simple Constraints: A Limited-
Memory Projected Quasi-Netton Algorithm. In AIS-
TATS.
Marina Sokolova and Victoria Bobicev. 2011. Sen-
timents and Opinions in Health-related Web mes-
sages. In Proceedings of the International Confer-
ence Recent Advances in Natural Language Pro-
cessing 2011, pages 132?139, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.
Marina Sokolova, Stan Matwin, Yasser Jafer, and
David Schramm. 2013. How Joe and Jane Tweet
about Their Health: Mining for Personal Health In-
formation on Twitter. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing RANLP 2013, pages 626?
632, Hissar, Bulgaria, September. INCOMA Ltd.
Shoumen, BULGARIA.
Mark Steyvers and Thomas Griffiths. 2005. Matlab
Topic Modeling Toolbox 1.4.
Mark Steyvers, Padhraic Smyth, and Thomas Griffiths.
2004. Probabilistic Author-Topic Models for Infor-
mation Discovery. In KDD.
Rannie Teodoro and Mor Naaman. 2013. Fitter with
Twitter: Understanding Personal Health and Fitness
Activity in Social Media. In Proceedings of the
7th International Conference on Weblogs and Social
Media.
Claudia Wagner, Vera Liao, Peter Pirolli, Les Nel-
son, and Markus Strohmaier. 2012. It?s not in
their tweets: Modeling topical expertise of Twitter
users. In ASE/IEEE International Conference on So-
cial Computing.
Martin J. Wainwright and Michael I. Jordan. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends in Ma-
chine Learning, 1(1-2):1?305.
106
