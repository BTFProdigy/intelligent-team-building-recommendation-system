Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?674,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Modeling Inflection and Word-Formation in SMT
Alexander Fraser? Marion Weller? Aoife Cahill? Fabienne Cap?
?Institut fu?r Maschinelle Sprachverarbeitung ?Educational Testing Service
Universita?t Stuttgart Princeton, NJ 08541
D?70174 Stuttgart, Germany USA
{fraser,wellermn,cap}@ims.uni-stuttgart.de acahill@ets.org
Abstract
The current state-of-the-art in statistical
machine translation (SMT) suffers from is-
sues of sparsity and inadequate modeling
power when translating into morphologi-
cally rich languages. We model both in-
flection and word-formation for the task
of translating into German. We translate
from English words to an underspecified
German representation and then use linear-
chain CRFs to predict the fully specified
German representation. We show that im-
proved modeling of inflection and word-
formation leads to improved SMT.
1 Introduction
Phrase-based statistical machine translation
(SMT) suffers from problems of data sparsity
with respect to inflection and word-formation
which are particularly strong when translating to
a morphologically rich target language, such as
German. We address the problem of inflection
by first translating to a stem-based representation,
and then using a second process to inflect these
stems. We study several models for doing
this, including: strongly lexicalized models,
unlexicalized models using linguistic features,
and models combining the strengths of both of
these approaches. We address the problem of
word-formation for compounds in German, by
translating from English into German word parts,
and then determining whether to merge these
parts to form compounds.
We make the following new contributions: (i)
we introduce the first SMT system combining
inflection prediction with synthesis of portman-
teaus and compounds. (ii) For inflection, we com-
pare the mostly unlexicalized prediction of lin-
guistic features (with a subsequent surface form
generation step) versus the direct prediction of
surface forms, and show that both approaches
have complementary strengths. (iii) We com-
bine the advantages of the prediction of linguis-
tic features with the prediction of surface forms.
We implement this in a CRF framework which
improves on a standard phrase-based SMT base-
line. (iv) We develop separate (but related) pro-
cedures for inflection prediction and dealing with
word-formation (compounds and portmanteaus),
in contrast with most previous work which usu-
ally either approaches both problems as inflec-
tional problems, or approaches both problems as
word-formation problems.
We evaluate on the end-to-end SMT task of
translating from English to German of the 2009
ACL workshop on SMT. We achieve BLEU score
increases on both the test set and the blind test set.
2 Overview of the translation process for
inflection prediction
The work we describe is focused on generaliz-
ing phrase-based statistical machine translation to
better model German NPs and PPs. We particu-
larly want to ensure that we can generate novel
German NPs, where what we mean by novel is
that the (inflected) realization is not present in the
parallel German training data used to build the
SMT system, and hence cannot be produced by
our baseline (a standard phrase-based SMT sys-
tem). We first present our system for dealing with
the difficult problem of inflection in German, in-
cluding the inflection-dependent phenomenon of
portmanteaus. Later, after performing an exten-
sive analysis of this system, we will extend it
664
to model compounds, a highly productive phe-
nomenon in German (see Section 8).
The key linguistic knowledge sources that we
use are morphological analysis and generation of
German based on SMOR, a morphological ana-
lyzer/generator of German (Schmid et al 2004)
and the BitPar parser, which is a state-of-the-art
parser of German (Schmid, 2004).
2.1 Issues of inflection prediction
In order to ensure coherent German NPs, we
model linguistic features of each word in an NP.
We model case, gender, and number agreement
and whether or not the word is in the scope of
a determiner (such as a definite article), which
we label in-weak-context (this linguistic feature
is necessary to determine the type of inflection of
adjectives and other words: strong, weak, mixed).
This is a diverse group of features. The number
of a German noun can often be determined given
only the English source word. The gender of a
German noun is innate and often difficult to deter-
mine given only the English source word. Case
is a function of the slot in the subcategorization
frame of the verb (or preposition). There is agree-
ment in all of these features in an NP. For instance
the number of an article or adjective is determined
by the head noun, while the type of inflection of an
adjective is determined by the choice of article.
We can have a large number of surface forms.
For instance, English blue can be translated as
German blau, blaue, blauer, blaues, blauen. We
predict which form is correct given the context.
Our system can generate forms not seen in the
training data. We follow a two-step process: in
step-1 we translate to blau (the stem), in step-2 we
predict features and generate the inflected form.1
2.2 Procedure
We begin building an SMT system by parsing the
German training data with BitPar. We then extract
morphological features from the parse. Next, we
lookup the surface forms in the SMOR morpholog-
ical analyzer. We use the morphological features
in the parse to disambiguate the set of possible
SMOR analyses. Finally, we output the ?stems?
of the German text, with the addition of markup
taken from the parse (discussed in Section 2.3).
1E.g., case=nominative, gender=masculine, num-
ber=singular, in-weak-context=true; inflected: blaue.
We then build a standard Moses system trans-
lating from English to German stems. We obtain
a sequence of stems and POS2 from this system,
and then predict the correct inflection using a se-
quence model. Finally we generate surface forms.
2.3 German Stem Markup
The translation process consists of two major
steps. The first step is translation of English
words to German stems, which are enriched with
some inflectional markup. The second step is
the full inflection of these stems (plus markup)
to obtain the final sequence of inflected words.
The purpose of the additional German inflectional
markup is to strongly improve prediction of in-
flection in the second step through the addition of
markup to the stems in the first step.
In general, all features to be predicted are
stripped from the stemmed representation because
they are subject to agreement restrictions of a
noun or prepositional phrase (such as case of
nouns or all features of adjectives). However, we
need to keep all morphological features that are
not dependent on, and thus not predictable from,
the (German) context. They will serve as known
input for the inflection prediction model. We now
describe this markup in detail.
Nouns are marked with gender and number: we
consider the gender of a noun as part of its stem,
whereas number is a feature which we can obtain
from English nouns.
Personal pronouns have number and gender an-
notation, and are additionally marked with nom-
inative and not-nominative, because English pro-
nouns are marked for this (except for you).
Prepositions are marked with the case their ob-
ject takes: this moves some of the difficulty in pre-
dicting case from the inflection prediction step to
the stem translation step. Since the choice of case
in a PP is often determined by the PP?s meaning
(and there are often different meanings possible
given different case choices), it seems reasonable
to make this decision during stem translation.
Verbs are represented using their inflected surface
form. Having access to inflected verb forms has a
positive influence on case prediction in the second
2We use an additional target factor to obtain the coarse
POS for each stem, applying a 7-gram POS model. Koehn
and Hoang (2007) showed that the use of a POS factor only
results in negligible BLEU improvements, but we need ac-
cess to the POS in our inflection prediction models.
665
input decoder output inflected merged
in
in<APPR><Dat> in
im
die<+ART><Def> dem
contrast Gegensatz<+NN><Masc><Sg>Gegensatz Gegensatz
to zu<APPR><Dat> zu
zur
the die<+ART><Def> der
animated lebhaft<+ADJ><Pos> lebhaften lebhaften
debate Debatte<+NN><Fem><Sg> Debatte Debatte
Table 1: Re-merging of prepositions and articles after
inflection to form portmanteaus, in dem means in the.
step through subject-verb agreement.
Articles are reduced to their stems (the stem itself
makes clear the definite or indefinite distinction,
but lemmatizing involves removing markings of
case, gender and number features).
Other words are also represented by their stems
(except for words not covered by SMOR, where
surface forms are used instead).
3 Portmanteaus
Portmanteaus are a word-formation phenomenon
dependent on inflection. As we have discussed,
standard phrase-based systems have problems
with picking a definite article with the correct
case, gender and number (typically due to spar-
sity in the language model, e.g., a noun which
was never before seen in dative case will often
not receive the correct article). In German, port-
manteaus increase this sparsity further, as they
are compounds of prepositions and articles which
must agree with a noun.
We adopt the linguistically strict definition of
the term portmanteau: the merging of two func-
tion words.3 We treat this phenomena by split-
ting the component parts during training and re-
merging during generation. Specifically for
German, this requires splitting the words which
have German POS tag APPRART into an APPR
(preposition) and an ART (article). Merging is re-
stricted, the article must be definite, singular4 and
the preposition can only take accusative or dative
case. Some prepositions allow for merging with
an article only for certain noun genders, for exam-
ple the preposition inDative is only merged with
the following article if the following noun is of
masculine or neuter gender. The definite article
3Some examples are: zum (to the) = zu (to) + dem (the)
[German], du (from the) = de (from) + le (the) [French] or al
(to the) = a (to) + el (the) [Spanish].
4This is the reason for which the preposition + article in
Table 2 remain unmerged.
must be inflected before making a decision about
whether to merge a preposition and the article into
a portmanteau. See Table 1 for examples.
4 Models for Inflection Prediction
We present 5 procedures for inflectional predic-
tion using supervised sequence models. The first
two procedures use simple N-gram models over
fully inflected surface forms.
1. Surface with no features is presented with an
underspecified input (a sequence of stems), and
returns the most likely inflected sequence.
2. Surface with case, number, gender is a hybrid
system giving the surface model access to linguis-
tic features. In this system prepositions have addi-
tionally been labeled with the case they mark (in
both the underspecified input and the fully spec-
ified output the sequence model is built on) and
gender and number markup is also available.
The rest of the procedures predict morpholog-
ical features (which are input to a morphological
generator) rather than surface words. We have de-
veloped a two-stage process for predicting fully
inflected surface forms. The first stage takes a
stem and predicts morphological features for that
stem, based on the surrounding context. The aim
of the first stage is to take a stem and predict
four morphological features: case, gender, num-
ber and type of inflection. We experiment with
a number of models for doing this. The sec-
ond stage takes the stems marked with morpho-
logical features (predicted in the first stage) and
uses a morphological generator to generate the
full surface form. For the second stage, a modified
version of SMOR (Schmid et al 2004) is used,
which, given a stem annotated with morphologi-
cal features, generates exactly one surface form.
We now introduce our first linguistic feature
prediction systems, which we call joint sequence
models (JSMs). These are standard language
models, where the ?word? tokens are not repre-
sented as surface forms, but instead using POS
and features. In testing, we supply the input as a
sequence in underspecified form, where some of
the features are specified in the stem markup (for
instance, POS=Noun, gender=masculine, num-
ber=plural), and then use Viterbi search to find the
most probable fully specified form (for instance,
POS=Noun, gender=masculine, number=plural,
666
output decoder input prediction output prediction inflected forms gloss
haben<VAFIN> haben-V haben-V haben have
Zugang<+NN><Masc><Sg> NN-Sg-Masc NN-Masc.Acc.Sg.in-weak-context=false Zugang access
zu<APPR><Dat> APPR-zu-Dat APPR-zu-Dat zu to
die<+ART><Def> ART-in-weak-context=true ART-Neut.Dat.Pl.in-weak-context=true den the
betreffend<+ADJ><Pos> ADJA ADJA-Neut.Dat.Pl.in-weak-context=true betreffenden respective
Land<+NN><Neut><Pl> NN-Pl-Neut NN-Neut.Dat.Pl.in-weak-context=true La?ndern countries
Table 2: Overview: inflection prediction steps using a single joint sequence model. All words except verbs and
prepositions are replaced by their POS tags in the input. Verbs are inflected in the input (?haben?, meaning
?have? as in ?they have?, in the example). Prepositions are lexicalized (?zu? in the example) and indicate which
case value they mark (?Dat?, i.e., Dative in the example).
case=nominative, in-weak-context=true).5
3. Single joint sequence model on features. We
illustrate the different stages of the inflection pre-
diction when using a joint sequence model. The
stemmed input sequence (cf. Section 2.3) contains
several features that will be part of the input to
the inflection prediction. With the exception of
verbs and prepositions, the representation for fea-
ture prediction is based on POS-tags.
As gender and number are given by the heads
of noun phrases and prepositional phrases, and
the expected type of inflection is set by articles,
the model has sufficient information to compute
values for these features and there is no need to
know the actual words. In contrast, the prediction
of case is more difficult as it largely depends on
the content of the sentence (e.g. which phrase is
object, which phrase is subject). Assuming that
verbs and prepositions indicate subcategorization
frames, the model is provided crucial information
for the prediction of case by keeping verbs (recall
that verbs are produced by the stem translation
system in their inflected form) and prepositions
(the prepositions also have case markup) instead
of replacing them with their tags.
After having predicted a single label with val-
ues for all features, an inflected word form for the
stem and the features is generated. The prediction
steps are illustrated in Table 2.
4. Using four joint sequence models (one for
each linguistic feature). Here the four linguistic
feature values are predicted separately. The as-
sumption that the different linguistic features can
be predicted independently of one another is a rea-
5Joint sequence models are a particularly simple HMM.
Unlike the HMMs used for POS-tagging, an HMM as used
here only has a single emission possibility for each state,
with probability 1. The states in the HMM are the fully
specified representation. The emissions of the HMM are the
stems+markup (the underspecified representation).
sonable linguistic assumption to make given the
additional German markup that we use. By split-
ting the inflection prediction problem into 4 com-
ponent parts, we end up with 4 simpler models
which are less sensitive to data sparseness.
Each linguistic feature is modeled indepen-
dently (by a JSM) and has a different input rep-
resentation based on the previously described
markup. The input consists of a sequence of
coarse POS tags, and for those stems that are
marked up with the relevant feature, this feature
value. Finally, we combine the predicted fea-
tures together to produce the same final output as
the single joint sequence model, and then generate
each surface form using SMOR.
5. Using four CRFs (one for each linguistic fea-
ture). The sequence models already presented are
limited to the n-gram feature space, and those that
predict linguistic features are not strongly lexi-
calized. Toutanova et al(2008) uses an MEMM
which allows the integration of a wide variety of
feature functions. We also wanted to experiment
with additional feature functions, and so we train
4 separate linear chain CRF6 models on our data
(one for each linguistic feature we want to pre-
dict). We chose CRFs over MEMMs to avoid the
label bias problem (Lafferty et al 2001).
The CRF feature functions, for each German
word wi, are in Table 3. The common feature
functions are used in all models, while each of the
4 separate models (one for each linguistic feature)
includes the context of only that linguistic feature.
We use L1 regularization to eliminate irrelevant
feature functions, the regularization parameter is
optimized on held out data.
6We use the Wapiti Toolkit (Lavergne et al 2010) on 4
x 12-Core Opteron 6176 2.3 GHz with 256GB RAM to train
our CRF models. Training a single CRF model on our data
was not tractable, so we use one for each linguistic feature.
667
Common lemmawi?5...wi+5 , tagwi?7...wi+7
Case casewi?5...wi+5
Gender genderwi?5...wi+5
Number numberwi?5...wi+5
in-weak-context in-weak-contextwi?5...wi+5
Table 3: Feature functions used in CRF models (fea-
ture functions are binary indicators of the pattern).
5 Experimental Setup
To evaluate our end-to-end system, we perform
the well-studied task of news translation, us-
ing the Moses SMT package. We use the En-
glish/German data released for the 2009 ACL
Workshop on Machine Translation shared task on
translation.7 There are 82,740 parallel sentences
from news-commentary09.de-en and 1,418,115
parallel sentences from europarl-v4.de-en. The
monolingual data contains 9.8 M sentences.8
To build the baseline, the data was tokenized
using the Moses tokenizer and lowercased. We
use GIZA++ to generate alignments, by running
5 iterations of Model 1, 5 iterations of the HMM
Model, and 4 iterations of Model 4. We sym-
metrize using the ?grow-diag-final-and? heuris-
tic. Our Moses systems use default settings. The
LM uses the monolingual data and is trained as
a five-gram9 using the SRILM-Toolkit (Stolcke,
2002). We run MERT separately for each sys-
tem. The recaser used is the same for all systems.
It is the standard recaser supplied with Moses,
trained on all German training data. The dev set
is wmt-2009-a and the test set is wmt-2009-b, and
we report end-to-end case sensitive BLEU scores
against the unmodified reference SGML file. The
blind test set used is wmt-2009-blind (all lines).
In developing our inflection prediction sys-
tems (and making such decisions as n-gram order
used), we worked on the so-called ?clean data?
task, predicting the inflection on stemmed refer-
ence sentences (rather than MT output). We used
the 2000 sentence dev-2006 corpus for this task.
Our contrastive systems consist of two steps,
the first is a translation step using a similar
Moses system (except that the German side is
stemmed, with the markup indicated in Sec-
7http://www.statmt.org/wmt09/translation-task.html
8However, we reduced the monolingual data (only) by
retaining only one copy of each unique line, which resulted
in 7.55 M sentences.
9Add-1 smoothing for unigrams and Kneser-Ney
smoothing for higher order n-grams, pruning defaults.
tion 2.3), and the second is inflection prediction
as described previously in the paper. To derive
the stem+markup representation we first parse
the German training data and then produce the
stemmed representation. We then build a sys-
tem for translating from English words to Ger-
man stems (the stem+markup representation), on
the same data (so the German side of the parallel
data, and the German language modeling uses the
stem+markup representation). Likewise, MERT
is performed using references which are in the
stem+markup representation.
To train the inflection prediction systems, we
use the monolingual data. The basic surface form
model is trained on lowercased surface forms,
the hybrid surface form model with features is
trained on lowercased surface forms annotated
with markup. The linguistic feature prediction
systems are trained on the monolingual data pro-
cessed as described previously (see Table 2).
Our JSMs are trained using the SRILM Toolkit.
We use the SRILM disambig tool for predicting
inflection, which takes a ?map? that specifies the
set of fully specified representations that each un-
derspecified stem can map to. For surface form
models, it specifies the mapping from stems to
lowercased surface forms (or surface forms with
markup for the hybrid surface model).
6 Results for Inflection Prediction
We build two different kinds of translation sys-
tem, the baseline and the stem translation system
(where MERT is used to train the system to pro-
duce a stem+markup sequence which agrees with
the stemmed reference of the dev set). In this sec-
tion we present the end-to-end translation results
for the different inflection prediction models de-
fined in Section 4, see Table 4.
If we translate from English into a stemmed
German representation and then apply a unigram
stem-to-surface-form model to predict the surface
form, we achieve a BLEU score of 9.97 (line 2).
This is only presented for comparison.
The baseline10 is 14.16, line 1. We compare
this with a 5-gram sequence model11 that predicts
10This is a better case-sensitive score than the baselines
on wmt-2009-b in experiments by top-performers Edinburgh
and Karlsruhe at the shared task. We use Moses with default
settings.
11Note that we use a different set, the ?clean data? set, to
determine the choice of n-gram order, see Section 7. We use
668
surface forms without access to morphological
features, resulting in a BLEU score of 14.26. In-
troducing morphological features (case on prepo-
sitions, number and gender on nouns) increases
the BLEU score to 14.58, which is in the same
range as the single JSM system predicting all lin-
guistic features at once.
This result shows that the mostly unlexicalized
single JSM can produce competitive results with
direct surface form prediction, despite not having
access to a model of inflected forms, which is the
desired final output. This strongly suggests that
the prediction of morphological features can be
used to achieve additional generalization over di-
rect surface form prediction. When comparing the
simple direct surface form prediction (line 3) with
the hybrid system enriched with number, gender
and case (line 4), it becomes evident that feature
markup can also aid surface form prediction.
Since the single JSM has no access to lexical
information, we used a language model to score
different feature predictions: for each sentence of
the development set, the 100 best feature predic-
tions were inflected and scored with a language
model. We then optimized weights for the two
scores LM (language model on surface forms)
and FP (feature prediction, the score assigned by
the JSM). This method disprefers feature predic-
tions with a top FP-score if the inflected sen-
tence obtains a bad LM score and likewise dis-
favors low-ranked feature prediction with a high
LM score. The prediction of case is the most
difficult given no lexical information, thus scor-
ing different prediction possibilities on inflected
words is helpful. An example is when the case of
a noun phrase leads to an inflected phrase which
never occurs in the (inflected) language model
(e.g., case=genitive vs. case=other). Applying
this method to the single JSM leads to a negligible
improvement (14.53 vs. 14.56). Using the n-best
output of the stem translation system did not lead
to any improvement.
The comparison between different feature pre-
diction models is also illustrative. Performance
decreases somewhat when using individual joint
sequence models (one for each linguistic feature)
compared to one single model (14.29, line 6).
The framework using the individual CRFs for
a 5-gram for surface forms and a 4-gram for JSMs, and the
same smoothing (Kneser-Ney, add-1 for unigrams, default
pruning).
1 baseline 14.16
2 unigram surface (no features) 9.97
3 surface (no features) 14.26
4 surface (with case, number, gender features) 14.58
5 1 JSM morphological features 14.53
6 4 JSMs morphological features 14.29
7 4 CRFs morphological features, lexical information 14.72
Table 4: BLEU scores (detokenized, case sensitive) on
the development test set wmt-2009-b
each linguistic feature performs best (14.72, line
7). The CRF framework combines the advantages
of surface form prediction and linguistic feature
prediction by using feature functions that effec-
tively cover the feature function spaces used by
both forms of prediction. The performance of the
CRF models results in a statistically significant
improvement12 (p < 0.05) over the baseline. We
also tried CRFs with bilingual features (projected
from English parses via the alignment output by
Moses), but obtained only a small improvement of
0.03, probably because the required information
is transferred in our stem markup (also a poor im-
provement beyond monolingual features is con-
sistent with previous work, see Section 8.3). De-
tails are omitted due to space.
We further validated our results by translating
the blind test set from wmt-2009, which we have
never looked at in any way. Here we also had
a statistically significant difference between the
baseline and the CRF-based prediction, the scores
were 13.68 and 14.18.
7 Analysis of Inflection-based System
Stem Markup. The first step of translating
from English to German stems (with the markup
we previously discussed) is substantially easier
than translating directly to inflected German (we
see BLEU scores on stems+markup that are over
2.0 BLEU higher than the BLEU scores on in-
flected forms when running MERT). The addition
of case to prepositions only lowered the BLEU
score reached by MERT by about 0.2, but is very
helpful for prediction of the case feature.
Inflection Prediction Task. Clean data task re-
sults13 are given in Table 5. The 4 CRFs outper-
form the 4 JSMs by more than 2%.
12We used Kevin Gimpel?s implementation of pairwise
bootstrap resampling with 1000 samples.
1326,061 of 55,057 tokens in our test set are ambiguous.
We report % surface form matches for ambiguous tokens.
669
Model Accuracy
unigram surface (no features) 55.98
surface (no features) 86.65
surface (with case, number, gender features) 91.24
1 JSM morphological features 92.45
4 JSMs morphological features 92.01
4 CRFs morphological features, lexical information 94.29
Table 5: Comparing predicting surface forms directly
with predicting morphological features.
training data 1 model 4 models
7.3 M sentences 92.41 91.88
1.5 M sentences 92.45 92.01
100000 sentences 90.20 90.64
1000 sentences 83.72 86.94
Table 6: Accuracy for different training data sizes of
the single and the four separate joint sequence models.
As we mentioned in Section 4, there is a spar-
sity issue at small training data sizes for the sin-
gle joint sequence model. This is shown in Ta-
ble 6. At the largest training data sizes, model-
ing all 4 features together results in the best pre-
dictions of inflection. However using 4 separate
models is worth this minimal decrease in perfor-
mance, since it facilitates experimentation with
the CRF framework for which the training of a
single model is not currently tractable.
Overall, the inflection prediction works well for
gender, number and type of inflection, which are
local features to the NP that normally agree with
the explicit markup output by the stem transla-
tion system (for example, the gender of a com-
mon noun, which is marked in the stem markup,
is usually successfully propagated to the rest of
the NP). Prediction of case does not always work
well, and could maybe be improved through hier-
archical labeled-syntax stem translation.
Portmanteaus. An example of where the sys-
tem is improved because of the new handling of
portmanteaus can be seen in the dative phrase
im internationalen Rampenlicht (in the interna-
tional spotlight), which does not occur in the par-
allel data. The accusative phrase in das interna-
tionale Rampenlicht does occur, however in this
case there is no portmanteau, but a one-to-one
mapping between in the and in das. For a given
context, only one of accusative or dative case is
valid, and a strongly disfluent sentence results
from the incorrect choice. In our system, these
two cases are handled in the same way (def-article
international Rampenlicht). This allows us to
generalize from the accusative example with no
portmanteau and take advantage of longer phrase
pairs, even when translating to something that will
be inflected as dative and should be realized as a
portmanteau. The baseline does not have this ca-
pability. It should be noted that the portmanteau
merging method described in Section 3 remerges
all occurrences of APPR and ART that can techni-
cally form a portmanteau. There are a few cases
where merging, despite being grammatical, does
not lead to a good result. Such exceptions require
semantic interpretation and are difficult to capture
with a fixed set of rules.
8 Adding Compounds to the System
Compounds are highly productive in German and
lead to data sparsity. We split the German com-
pounds in the training data, so that our stem trans-
lation system can now work with the individual
words in the compounds. After we have trans-
lated to a split/stemmed representation, we deter-
mine whether to merge words together to form a
compound. Then we merge them to create stems
in the same representation as before and we per-
form inflection and portmanteau merging exactly
as previously discussed.
8.1 Details of Splitting Process
We prepare the training data by splitting com-
pounds in two steps, following the technique of
Fritzinger and Fraser (2010). First, possible split
points are extracted using SMOR, and second, the
best split points are selected using the geometric
mean of word part frequencies.
compound word parts gloss
Inflationsrate Inflation Rate inflation rate
auszubrechen aus zu brechen out to break (to break out)
Training data is then stemmed as described in
Section 2.3. The formerly modifying words of the
compound (in our example the words to the left
of the rightmost word) do not have a stem markup
assigned, except for two cases: i) they are nouns
themselves or ii) they are particles separated from
a verb. In these cases, former modifiers are rep-
resented identically to their individual occurring
counterparts, which helps generalization.
8.2 Model for Compound Merging
After translation, compound parts have to be
resynthesized into compounds before inflection.
Two decisions have to be taken: i) where to
670
merge and ii) how to merge. Following the work
of Stymne and Cancedda (2011), we implement
a linear-chain CRF merging system using the
following features: stemmed (separated) surface
form, part-of-speech14 and frequencies from the
training corpus for bigrams/merging of word and
word+1, word as true prefix, word+1 as true suf-
fix, plus frequency comparisons of these. The
CRF is trained on the split monolingual data. It
only proposes merging decisions, merging itself
uses a list extracted from the monolingual data
(Popovic et al 2006).
8.3 Experiments
We evaluated the end-to-end inflection system
with the addition of compounds.15 As in the in-
flection experiments described in Section 5, we
use a 5-gram surface LM and a 7-gram POS
LM, but for this experiment, they are trained on
stemmed, split data. The POS LM helps com-
pound parts and heads appear in correct order.
The results are in Table 7. The BLEU score of the
CRF on test is 14.04, which is low. However the
system produces 19 compound types which are
in the reference but not in the parallel data, and
therefore not accessible to other systems. We also
observe many more compounds in general. The
100-best inflection rescoring technique previously
discussed reached 14.07 on the test set. Blind
test results with CRF prediction are much better,
14.08, which is a statistically significant improve-
ment over the baseline (13.68) and approaches the
result we obtained without compounds (14.18).
Correctly generated compounds are single words
which usually carry the same information as mul-
tiple words in English, and are hence likely un-
derweighted by BLEU. We again see many in-
teresting generalizations. For instance, take the
case of translating English miniature cameras to
the German compound Miniaturkameras. minia-
ture camera or miniature cameras does not occur
in the training data, and so there is no appropri-
ate phrase pair in any system (baseline, inflec-
tion, or inflection&compound-splitting). How-
ever, our system with compound splitting has
learned from split composita that English minia-
14Compound modifiers get assigned a special tag based on
the POS of their former heads, e.g., Inflation in the example
is marked as a non-head of a noun.
15We found it most effective to merge word parts during
MERT (so MERT uses the same stem references as before).
1 1 JSM morphological features 13.94
2 4 CRFs morphological features, lexical information 14.04
Table 7: Results with Compounds on the test set
ture can be translated as German Miniatur- and
gets the correct output.
9 Related Work
There has been a large amount of work on trans-
lating from a morphologically rich language to
English, we omit a literature review here due to
space considerations. Our work is in the opposite
direction, which primarily involves problems of
generation, rather than problems of analysis.
The idea of translating to stems and then in-
flecting is not novel. We adapted the work of
Toutanova et al(2008), which is effective but lim-
ited by the conflation of two separate issues: word
formation and inflection.
Given a stem such as brother, Toutanova et. al?s
system might generate the ?stem and inflection?
corresponding to and his brother. Viewing and
and his as inflection is problematic since a map-
ping from the English phrase and his brother to
the Arabic stem for brother is required. The situ-
ation is worse if there are English words (e.g., ad-
jectives) separating his and brother. This required
mapping is a significant problem for generaliza-
tion. We view this issue as a different sort of prob-
lem entirely, one of word-formation (rather than
inflection). We apply a ?split in preprocessing and
resynthesize in postprocessing? approach to these
phenomena, combined with inflection prediction
that is similar to that of Toutanova et. al. The
only work that we are aware of which deals with
both issues is the work of de Gispert and Marin?o
(2008), which deals with verbal morphology and
attached pronouns. There has been other work
on solving inflection. Koehn and Hoang (2007)
introduced factored SMT. We use more complex
context features. Fraser (2009) tried to solve the
inflection prediction problem by simply building
an SMT system for translating from stems to in-
flected forms. Bojar and Kos (2010) improved on
this by marking prepositions with the case they
mark (one of the most important markups in our
system). Both efforts were ineffective on large
data sets. Williams and Koehn (2011) used uni-
fication in an SMT system to model some of the
671
agreement phenomena that we model. Our CRF
framework allows us to use more complex con-
text features.
We have directly addressed the question as to
whether inflection should be predicted using sur-
face forms as the target of the prediction, or
whether linguistic features should be predicted,
along with the use of a subsequent generation
step. The direct prediction of surface forms is
limited to those forms observed in the training
data, which is a significant limitation. How-
ever, it is reasonable to expect that the use of
features (and morphological generation) could
also be problematic as this requires the use of
morphologically-aware syntactic parsers to anno-
tate the training data with such features, and addi-
tionally depends on the coverage of morpholog-
ical analysis and generation. Despite this, our
research clearly shows that the feature-based ap-
proach is superior for English-to-German SMT.
This is a striking result considering state-of-the-
art performance of German parsing is poor com-
pared with the best performance on English pars-
ing. As parsing performance improves, the per-
formance of linguistic-feature-based approaches
will increase.
Virpioja et al(2007), Badr et al(2008), Luong
et al(2010), Clifton and Sarkar (2011), and oth-
ers are primarily concerned with using morpheme
segmentation in SMT, which is a useful approach
for dealing with issues of word-formation. How-
ever, this does not deal directly with linguistic fea-
tures marked by inflection. In German these lin-
guistic features are marked very irregularly and
there is widespread syncretism, making it difficult
to split off morphemes specifying these features.
So it is questionable as to whether morpheme seg-
mentation techniques are sufficient to solve the in-
flectional problem we are addressing.
Much previous work looks at the impact of us-
ing source side information (i.e., feature func-
tions on the aligned English), such as those
of Avramidis and Koehn (2008), Yeniterzi and
Oflazer (2010) and others. Toutanova et. al.?s
work showed that it is most important to model
target side coherence and our stem markup also
allows us to access source side information. Us-
ing additional source side information beyond the
markup did not produce a gain in performance.
For compound splitting, we follow Fritzinger
and Fraser (2010), using linguistic knowledge en-
coded in a rule-based morphological analyser and
then selecting the best analysis based on the ge-
ometric mean of word part frequencies. Other
approaches use less deep linguistic resources
(e.g., POS-tags Stymne (2008)) or are (almost)
knowledge-free (e.g., Koehn and Knight (2003)).
Compound merging is less well studied. Popovic
et al(2006) used a simple, list-based merging ap-
proach, merging all consecutive words included
in a merging list. This approach resulted in too
many compounds. We follow Stymne and Can-
cedda (2011), for compound merging. We trained
a CRF using (nearly all) of the features they used
and found their approach to be effective (when
combined with inflection and portmanteau merg-
ing) on one of our two test sets.
10 Conclusion
We have shown that both the prediction of sur-
face forms and the prediction of linguistic features
are of interest for improving SMT. We have ob-
tained the advantages of both in our CRF frame-
work, and also integrated handling of compounds,
and an inflection-dependent word formation phe-
nomenon, portmanteaus. We validated our work
on a well-studied large corpora translation task.
Acknowledgments
The authors wish to thank the anonymous review-
ers for their comments. Aoife Cahill was partly
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Alexander Fraser, Marion Weller
and Fabienne Cap were funded by Deutsche
Forschungsgemeinschaft grant Models of Mor-
phosyntax for Statistical Machine Translation.
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement Nr. 248005. This work was sup-
ported in part by the IST Programme of the Euro-
pean Community, under the PASCAL2 Network
of Excellence, IST-2007-216886. This publica-
tion only reflects the authors? views. We thank
Thomas Lavergne and Helmut Schmid.
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. In Proceedings of ACL-
672
08: HLT, pages 763?770, Columbus, Ohio, June.
Association for Computational Linguistics.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for English-to-Arabic statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
Short Papers, pages 153?156, Columbus, Ohio,
June. Association for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 60?66, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 32?42, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Adria` de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statisti-
cal MT. Speech Communication, 50(11-12):1034?
1046.
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 115?119,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth
Workshop on Statistical Machine Translation, pages
224?234. Association for Computational Linguis-
tics.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL ?03:
Proceedings of the 10th conference of the European
chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Minh-Thang Luong, Preslav Nakov, and Min-Yen
Kan. 2010. A Hybrid Morpheme-Word Represen-
tation for Machine Translation of Morphologically
Rich Languages. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 148?157, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Maja Popovic, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Proceedings of FINTAL-06, pages
616?624, Turku, Finland. Springer Verlag, LNCS.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In 4th International Conference on Language
Resources and Evaluation.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of Coling 2004, pages 162?
168, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing.
Sara Stymne and Nicola Cancedda. 2011. Produc-
tive Generation of Compound Words in Statistical
Machine Translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
250?260, Edinburgh, Scotland UK, July. Associa-
tion for Computational Linguistics.
Sara Stymne. 2008. German Compounds in Factored
Statistical Machine Translation. In Proceedings of
GOTAL-08, pages 464?475, Gothenburg, Sweden.
Springer Verlag, LNCS/LNAI.
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying Morphology Generation
Models to Machine Translation. In Proceedings of
ACL-08: HLT, pages 514?522, Columbus, Ohio,
June. Association for Computational Linguistics.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In PROC. OF
MT SUMMIT XI, pages 491?498.
Philip Williams and Philipp Koehn. 2011. Agree-
ment constraints for statistical machine translation
into German. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 217?226,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-Morphology Mapping in Factored Phrase-Based
673
Statistical Machine Translation from English to
Turkish. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 454?464, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
674
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 579?587,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
How to Produce Unseen Teddy Bears:
Improved Morphological Processing of Compounds in SMT
Fabienne Cap, Alexander Fraser
CIS, University of Munich
{cap|fraser}@cis.uni-muenchen.de
Marion Weller
IMS, University of Stuttgart
wellermn@ims.uni-stuttgart.de
Aoife Cahill
Educational Testing Service
acahill@ets.org
Abstract
Compounding in morphologically rich
languages is a highly productive process
which often causes SMT approaches to
fail because of unseen words. We present
an approach for translation into a com-
pounding language that splits compounds
into simple words for training and, due
to an underspecified representation, allows
for free merging of simple words into
compounds after translation. In contrast to
previous approaches, we use features pro-
jected from the source language to predict
compound mergings. We integrate our ap-
proach into end-to-end SMT and show that
many compounds matching the reference
translation are produced which did not ap-
pear in the training data. Additional man-
ual evaluations support the usefulness of
generalizing compound formation in SMT.
1 Introduction
Productive processes like compounding or inflec-
tion are problematic for traditional phrase-based
statistical machine translation (SMT) approaches,
because words can only be translated as they have
occurred in the parallel training data. As paral-
lel training data is limited, it is desirable to ex-
tract as much information from it as possible. We
present an approach for compound processing in
SMT, translating from English to German, that
splits compounds prior to training (in order to ac-
cess the individual words which together form the
compound) and recombines them after translation.
While compound splitting is a well-studied task,
compound merging has not received as much at-
tention in the past. We start from Stymne and Can-
cedda (2011), who used sequence models to pre-
dict compound merging and Fraser et al. (2012)
who, in addition, generalise over German inflec-
tion. Our new contributions are: (i) We project
features from the source language to support com-
pound merging predictions. As the source lan-
guage input is fluent, these features are more re-
liable than features derived from target language
SMT output. (ii) We reduce compound parts to
an underspecified representation which allows for
maximal generalisation. (iii) We present a detailed
manual evaluation methodology which shows that
we obtain improved compound translations.
We evaluated compound processing both on
held-out split data and in end-to-end SMT. We
show that using source language features increases
the accuracy of compound generation. Moreover,
we find more correct compounds than the base-
lines, and a considerable number of these com-
pounds are unseen in the training data. This is
largely due to the underspecified representation we
are using. Finally, we show that our approach im-
proves upon the previous work.
We discuss compound processing in SMT in
Section 2, and summarise related work in Sec-
tion 3. In Section 4 we present our method for
splitting compounds and reducing the component
words to an underspecified representation. The
merging to obtain German compounds is the sub-
ject of Section 5. We evaluate the accuracy of
compound prediction on held-out data in Section 6
and in end-to-end SMT experiments in Section 7.
We conclude in Section 8.
2 Dealing with Compounds in SMT
In German, two (or more) single words (usually
nouns or adjectives) are combined to form a
compound which is considered a semantic unit.
The rightmost part is referred to as the head while
all other parts are called modifiers. EXAMPLE (1)
lists different ways of joining simple words into
compounds: mostly, no modification is required
(A) or a filler letter is introduced (B). More rarely,
a letter is deleted (C), or transformed (D).
579
Werkzeug
Handel
Obst
Kiste
fruit
box
trading
tool Handelswerkzeug
ObstkisteWerkzeugkiste
Obsthandel fruittrading
tool
boxKiste
Werkzeug
Obst
Handel
splitting training
splitting training
testing re?combination
testing re?combination
Figure 1: Compound processing in SMT allows the synthesis of compounds unseen in the training data.
EXAMPLE (1)
(A) Haus+Boot = Hausboot (?house boat?)
(B) Ort+s+Zeit = Ortszeit (?local time?)
(C) Kirche-e+Turm = Kirchturm (?church tower?)
(D) Kriterium+Liste = Kriterienliste (?criteria list?)
German compounds are highly productive,
1
and
traditional SMT approaches often fail in the face
of such productivity. Therefore, special process-
ing of compounds is required for translation into
German, as many compounds will not (e.g. Haus-
boot, ?house boat?) or only rarely have been seen
in the training data.
2
In contrast, most compounds
consist of two (or more) simple words that occur
more frequently in the data than the compound as
a whole (e.g. Haus (7,975) and Boot (162)) and of-
ten, these compound parts can be translated 1-to-
1 into simple English words. Figure 1 illustrates
the basic idea of compound processing in SMT:
imagine, ?Werkzeug? (?tool?) occurred only as a
modifier of e.g. ?Kiste? (?box?) in the training
data, but the test set contains ?tool? as a simple
word or as the head of a compound. Splitting com-
pounds prior to translation model training enables
better access to the component translations and al-
lows for a high degree of generalisation. At test-
ing time, the English text is translated into the split
German representation, and only afterwards, some
sequences of simple words are (re-)combined into
(possibly unseen) compounds where appropriate.
This merging of compounds is much more chal-
lenging than the splitting, as it has to be applied
to disfluent MT output: i.e., compound parts may
not occur in the correct word order and even if they
do, not all sequences of German words that could
form a compound should be merged.
3 Related Work
Compound processing for translation into a com-
pounding language includes both compound split-
1
Most newly appearing words in German are compounds.
2
~30% of the word types and ~77% of the compound
types we identified in our training data occurred ? 3 times.
ting and merging, we thus report on previous ap-
proaches for both of these tasks.
In the past, there have been numerous attempts
to split compounds, all improving translation qual-
ity when translating from a compounding to a non-
compounding language. Several compound split-
ting approaches make use of substring corpus fre-
quencies in order to find the optimal split points of
a compound (e.g. Koehn and Knight (2003), who
allowed only ?(e)s? as filler letters). Stymne et al.
(2008) use Koehn and Knight?s technique, include
a larger list of possible modifier transformations
and apply POS restrictions on the substrings, while
Fritzinger and Fraser (2010) use a morphological
analyser to find only linguistically motivated sub-
strings. In contrast, Dyer (2010) presents a lattice-
based approach to encode different segmentations
of words (instead of finding the one-best split).
More recently, Macherey et al. (2011) presented
a language-independent unsupervised approach in
which filler letters and a list of words not to be split
(e.g., named entities) are learned using phrase ta-
bles and Levenshtein distance.
In contrast to splitting, the merging of com-
pounds has received much less attention in the
past. An early approach by Popovi?c et al. (2006)
recombines compounds using a list of compounds
and their parts. It thus never creates invalid Ger-
man compounds, but on the other hand it is limited
to the coverage of the list. Moreover, in some con-
texts a merging in the list may still be wrong, cf.
EXAMPLE (3) in Section 5 below. The approach
of Stymne (2009) makes use of a factored model,
with a special POS-markup for compound mod-
ifiers, derived from the POS of the whole com-
pound. This markup enables sound mergings of
compound parts after translation if the POS of the
candidate modifier (X-Part) matches the POS of
the candidate compound head (X): Inflations|N-
Part + Rate|N = Inflationsrate|N (?inflation rate?).
In Stymne and Cancedda (2011) the factored ap-
580
Gas|Traum      8.34Gastraum        3.74Gast|Raum    8.59
4) Disambiguation
...
...
...
Amerikanische Medien ...
Tim Baumeister besiegt ...
Der Gastraum des ...
0) Original Text
...
(S(NP(ADJA Amerikanische) (NN Medien)...))
...(S(NP(PN(NE Tim)(NE Baumeister))(VV besiegt)...))
...
(S(NP(ART Der) (NN Gastraum) (ART des)...))
1) Bitpar Parsed Text
> amerikanische
> Gastraum
> BaumeisterBau<NN>Meister<+NN>Baumeister<+NPROP>
Gast<NN>Raum<+NN>Gas<NN>Traum<+NN>
amerikanisch<+ADJ>
3) SMOR Analysis
amerikanische Medien ...ADJA NN
NETim Baumeister besiegt ...NE VV
ARTNNARTder Gastraum des ...
...
...
...
2) True Casing
Figure 2: Compound splitting pipeline 1) The original text is parsed with BITPAR to get unambiguous POS tags,
2) The original text is then true-cased using the most frequent casing for each word and BITPAR tags are added,
3) All words are analysed with SMOR, analyses are filtered using BITPAR tags (only bold-faced analyses are kept),
4) If several splitting options remain, the geometric mean of the word (part) frequencies is used to disambiguate them.
proach was extended to make use of a CRF se-
quence labeller (Lafferty et al., 2001) in order
to find reasonable merging points. Besides the
words and their POS, many different target lan-
guage frequency features were defined to train the
CRF. This approach can even produce new com-
pounds unseen in the training data, provided that
the modifiers occurred in modifier position of a
compound and heads occurred as heads or even as
simple words with the same inflectional endings.
However, as former compound modifiers were left
with their filler letters (cf. ?Inflations?), they can
not be generalised to compound heads or simple
words, nor can inflectional variants of compound
heads or simple words be created (e.g. if ?Rate?
had only been observed in nominative form in the
training data, the genitive ?Raten? could not be
produced). The underspecified representation we
are using allows for maximal generalisation over
word parts independent of their position of oc-
currence or inflectional realisations. Moreover,
their experiments were limited to predicting com-
pounds on held-out data; no results were reported
for using their approach in translation. In Fraser
et al. (2012) we re-implemented the approach of
Stymne and Cancedda (2011), combined it with
inflection prediction and applied it to a transla-
tion task. However, compound merging was re-
stricted to a list of compounds and parts. Our
present work facilitates more independent com-
bination. Toutanova et al. (2008) and Weller et
al. (2013) used source language features for target
language inflection, but to our knowledge, none of
these works applied source language features for
compound merging.
4 Step 1: Underspecified Representation
In order to enhance translation model accuracy,
it is reasonable to have similar degrees of mor-
phological richness between source and target lan-
guage. We thus reduce the German target lan-
guage training data to an underspecified represen-
tation: we split compounds, and lemmatise all
words (except verbs). All occurrences of simple
words, former compound modifiers or heads have
the same representation and can thus be freely
merged into ?old? and ?new? compounds after
translation, cf. Figure 1 above. So that we can later
predict the merging of simple words into com-
pounds and the inflection of the words, we store
all of the morphological information stripped from
the underspecified representation.
Note that erroneous over-splitting might make
the correct merging of compounds difficult
3
(or even impossible), due to the number of
correct decisions required. For example, it
requires only 1 correct prediction to recom-
bine ?Niederschlag|Menge? into ?Niederschlags-
menge? (?amount of precipitation?) but 3 for
the wrong split into ?nie|der|Schlag|Menge?
(?never|the|hit|amount?). We use the compound
splitter of Fritzinger and Fraser (2010), who have
shown that using a rule-based morphological anal-
yser (SMOR, Schmid et al. (2004)) drastically re-
duced the number of erroneous splits when com-
pared to the frequency-based approach of Koehn
and Knight (2003). However, we adapted it to
work on tokens: some words can, depending on
their context, either be interpreted as named enti-
ties or common nouns, e.g., ?Dinkelacker? (a Ger-
man beer brand or ?spelt|field?).
4
We parsed the
training data and use the parser?s decisions to iden-
tify proper names, see ?Baumeister? in Figure 2.
After splitting, we use SMOR to reduce words to
lemmas, keeping morphological features like gen-
der or number, and stripping features like case, as
illustrated for ?
?
Olexporteure? (?oil exporters?):
3
In contrast, they may not hurt translation quality in the
other direction, where phrase-based SMT is likely to learn
the split words as a phrase and thus recover from that error.
4
Note that Macherey et al. (2011) blocked splitting of
words which can be used as named entities, independent of
context, which is less general than our solution.
581
No. Feature Description Example
Experiment
SC T TR
1SC surface form of the word string: Arbeit<+NN><Fem><Sg> X X
2SC main part of speech of the word (from the parser) string: +NN X X
3SC word occurs in a bigram with the next word frequency: 0 X X
4SC word combined to a compound with the next word frequency: 10,000 X X X
5SC word occurs in modifier position of a compound frequency: 100,000 X X
6SC word occurs in a head position of a compound frequency: 10,000 X X
7SC word occurs in modifier position vs. simplex string: P>W (P= 5SC, W= 100,000) X
8SC word occurs in head position vs. simplex string: S<W (S= 6SC, W= 100,000) X
7SC+ word occurs in modifier position vs. simplex ratio: 10 (10**ceil(log10(5SC/W))) X X
8SC+ word occurs in head position vs. simplex ratio: 1 (10**ceil(log10(6SC/W))) X X
9N different head types the word can combine with number: 10,000 X X
Table 1: Target language CRF features for compound merging. SC = features taken from Stymne and Cancedda
(2011), SC+ = improved versions, N = new feature. Experiments: SC = re-implementation of Stymne and Cancedda (2011),
T= use full Target feature set, TR = use Target features, but only a Reduced set.
EXAMPLE (2)
?l<+NN><Neut><Sg> Exporteur<+NN> <Masc><Pl>
?l<NN>Exporteur<+NN><Masc><Nom><Pl>compound
headmodifier
While the former compound head (?Exporteure?)
automatically inherits all morphological features
of the compound as a whole, the features of the
modifier need to be derived from SMOR in an ad-
ditional step. We need to ensure that the repre-
sentation of the modifier is identical to the same
word when it occurs independently in order to ob-
tain full generalisation over compound parts.
5 Step 2: Compound Merging
After translation from English into the underspec-
ified German representation, post-processing is re-
quired to transform the output back into fluent,
morphologically fully specified German. First,
compounds need to be merged where appropriate,
e.g., ?Hausboote? (?house boats?):
Haus<+NN><Neut><Sg> + Boot<+NN><Neut><Pl>
? Haus<NN>Boot<+NN><Neut><Pl> (merged)
and second, all words need to be inflected:
Haus<NN>Boot<+NN><Neut><Acc><Pl>
? Hausbooten (inflected)
5.1 Target Language Features
To decide which words should be combined, we
follow Stymne and Cancedda (2011) who used
CRFs for this task. The features we derived from
the target language to train CRF models are listed
in Table 1. We adapted features No. 1-8 from
Stymne and Cancedda (2011). Then, we modi-
fied two features (7+8) and created a new feature
indicating the productivity of a modifier (9N).
5.2 Projecting Source Language Features
We also use new features derived from the English
source language input, which is coherent and flu-
ent. This makes features derived from it more reli-
able than the target language features derived from
disfluent SMT output. Moreover, source language
features might support or block merging decisions
in unclear cases, i.e., where target language fre-
quencies are not helpful, either because they are
very low or they have roughly equal frequency dis-
tributions when occurring in a compound (as mod-
ifier or head) vs. as a simple word.
In Table 2, we list three types of features:
1. Syntactic features: different English noun
phrase patterns that are aligned to German
compound candidate words (cf. 10E-13E)
2. The POS tag of the English word (cf. 14E)
3. Alignment features, derived from word
alignments (cf. 15E-18E)
The examples given in Table 2 (10E-13E) show
that English compounds often have 1-to-1 corre-
spondences to the parts of a German compound.
Knowing that two consecutive German simple
words are aligned to two English words of the
same noun phrase is a strong indicator that the
German words should be merged:
EXAMPLE (3)
should be merged:
ein erh?ohtes verkehrs aufkommen sorgt f?ur chaos
?an increased traffic volume causes chaos?
(S...(NP(DT An)(VN increased)(NN traffic)(NN volume))..)))
should not be merged:
f?ur die finanzierung des verkehrs aufkommen
?pay for the financing of transport?
(VP(V pay)(PP(IN for)(NP(NP(DT the)(NN financing))
(PP(IN of)(NP(NN transport)..))
In the compound reading of ?verkehr + aufkom-
men?, the English parse structure indicates that
the words aligned to ?verkehr? (?traffic?) and
582
No. Feature Description Type
10E
word and next word are aligned from a noun phrase in the English source sentence:
(NP(NN traffic)(NN accident))? Verkehr (?traffic?) + Unfall (?accident?)
true/false
11E
word and next word are aligned from a gerund construction in the English source sentence:
(NP(VBG developing)(NNS nations))? Entwicklung (?development?) + L?ander (?countries?)
true/false
12E
word and next word are aligned from a genitive construction in the English source sentence:
(NP(NP(DT the)(NN end))(PP(IN of)(NP(DT the)(NN year))? Jahr (?year?) + Ende(?end?)
true/false
13E
word and next word are aligned from an adjective noun construction in the English source sentence:
(NP (ADJ protective)(NNS measures))? Schutz (?protection?) + Ma?nahmen (?measures?)
true/false
14E print the POS of the corresponding aligned English word string
15E
word and next word are aligned 1-to-1 from the same word in the English source sentence, e.g.,
beef
?
?
Rind(?cow?)
Fleisch(?meat?)
true/false
16E like 15E, but the English word contains a dash, e.g., Nobel ? Prize
?
?
Nobel(?Nobel?)
Preis(?prize?)
true/false
17E like 15E, but also considering 1-to-n and n-to-1 links true/false
18E like 16E, but also considering 1-to-n and n-to-1 links true/false
Table 2: List of new source language CRF features for compound merging.
?aufkommen? (?volume?), are both nouns and
part of one common noun phrase, which is a strong
indicator that the two words should be merged
in German. In contrast, the syntactic relation-
ship between ?pay? (aligned to ?aufkommen?)
and ?transport? (aligned to ?verkehr?) is more dis-
tant
5
: merging is not indicated.
We also use the POS of the English words to
learn (un)usual combinations of POS, indepen-
dent of their exact syntactic structure (14E). Re-
consider EXAMPLE (3): NN+NN is a more com-
mon POS pair for compounds than V+NN.
Finally, the alignment features (15E-18E) pro-
mote the merging into compounds whose align-
ments indicate that they should not have been split
in the first place (e.g., Rindfleisch, 15E).
5.3 Compound Generation and Inflection
So far, we reported on how to decide which sim-
ple words are to be merged into compounds, but
not how to recombine them. Recall from EXAM-
PLE (1) that the modifier of a compound some-
times needs to be transformed, before it can be
combined with the head word (or next modifier),
e.g., ?Ort?+?Zeit? = ?Ortszeit? (?local time?).
We use SMOR to generate compounds from a
combination of simple words. This allows us to
create compounds with modifiers that never oc-
curred as such in the training data. Imagine that
?Ort? occurred only as compound head or as a
single word in the training data. Using SMOR, we
are still able to create the correct form of the mod-
ifier, including the required filler letter: ?Orts?.
This ability distinguishes our approach from pre-
5
Note that ?f?ur etwas aufkommen? (lit. ?for sth. arise?,
idiom.: ?to pay for sth.?) is an idiomatic expression.
vious approaches: Stymne and Cancedda (2011)
do not reduce modifiers to their base forms
6
(they
can only create new compounds when the modifier
occurred as such in the training data) and Fraser et
al. (2012) use a list for merging.
Finally, we use the system described in Fraser
et al. (2012) to inflect the entire text.
6 Accuracy of Compound Prediction
We trained CRF models on the parallel training
data (~40 million words)
7
of the EACL 2009
workshop on statistical machine translation
8
us-
ing different feature (sub)sets, cf. the ?Exper-
iment? column in Table 1 above. We exam-
ined the reliability of the CRF compound predic-
tion models by applying them to held-out data:
1. split the German wmt2009 tuning data set
2. remember compound split points
3. predict merging with CRF models
4. combine predicted words into compounds
5. calculate f-scores on how properly the
compounds were merged
Table 3 lists the CRF models we trained, together
with their compound merging accuracies on held-
out data. It can be seen that using more features
(SC?T?ST) is favourable in terms of precision
and overall accuracy and the positive impact of us-
ing source language features is clearer when only
reduced feature sets are used (TR vs. STR).
However, these accuracies only somewhat cor-
relate with SMT performance: while being trained
and tested on clean, fluent German language, the
6
They account for modifier transformations by using char-
acter n-gram features (cf.EXAMPLE (1)).
7
However, target language feature frequencies are derived
from the monolingual training data, ~146 million words.
8
http://www.statmt.org/wmt09
583
exp to be all correct wrong wrong not merging
precision recall f-score
merged merged merged merged merged wrong
SC 1,047 997 921 73 121 3 92.38% 88.13% 90.21%
T 1,047 979 916 59 128 4 93.56% 87.40% 90.38%
ST 1,047 976 917 55 126 4 93.95% 87.58% 90.66%
TR 1,047 893 836 52 204 5 93.62% 80.00% 86.27%
STR 1,047 930 866 58 172 6 93.12% 82.95% 87.74%
Table 3: Compound production accuracies of CRF models on held-out data: SC: re-implementation of Stymne
and Cancedda (2011); T: all target language features, including a new one (cf. Table 1); ST = all Source and Target language
features; TR: only a reduced set of target language features; STR: TR, plus all source language features given in Table 2.
exp BLEU SCORES #compounds found
mert.log BLEU RTS all ref new new*
RAW 14.88 14.25 1.0054 646 175 n.a. n.a.
UNSPLIT 15.86 14.74 0.9964 661 185 n.a. n.a.
SC 15.44 14.45 0.9870 882 241 47 8
T 15.56 14.32 0.9634 845 251 47 8
ST 15.33 14.51 0.9760 820 248 46 9
TR 15.24 14.26 0.9710 753 234 44 5
STR 15.37 14.61 0.9884 758 239 43 7
#compounds in reference text: 1,105 1,105 396 193
Table 4: SMT results. Tuning scores (mert.log) are on merged but uninflected data (except RAW).
RTS: length ratio; all: #compounds produced; ref: reference matches; new: unknown to parallel data; new*: unknown to
target language data. bold face indicates statistical significance wrt. the RAW baseline, SC, T and TR.
models will later be applied to disfluent SMT out-
put and might thus lead to different results there.
Stymne and Cancedda (2011) dealt with this by
noisifying the CRF training data: they translated
the whole data set using an SMT system that was
trained on the same data set. This way, the train-
ing data was less fluent than in its original format,
but still of higher quality than SMT output of un-
seen data. In contrast, we left the training data as
it was, but strongly reduced the feature set for CRF
model training (e.g., no more use of surface words
and POS tags, cf. TR and STR in Table 3) instead.
7 Translation Performance
We integrated our compound processing pipeline
into an end-to-end SMT system. Models were
trained with the default settings of the Moses SMT
toolkit, v1.0 (Koehn et al., 2007) using the data
from the EACL 2009 workshop on statistical ma-
chine translation. All compound processing sys-
tems are trained and tuned identically, except us-
ing different CRF models for compound predic-
tion. All training data was split and reduced
to the underspecified representation described in
Section 4. We used KenLM (Heafield, 2011) with
SRILM (Stolcke, 2002) to train a 5-gram language
model based on all available target language train-
ing data. For tuning, we used batch-mira with ?-
safe-hope? (Cherry and Foster, 2012) and ran it
separately for every experiment. We integrated the
CRF-based merging of compounds into each itera-
tion of tuning and scored each output with respect
to an unsplit and lemmatised version of the tuning
reference. Testing consists of:
1. translation into the split, underspecified
German representation
2. compound merging using CRF models
to predict recombination points
3. inflection of all words
7.1 SMT Results
We use 1,025 sentences for tuning and 1,026 sen-
tences for testing. The results are given in Table 4.
We calculate BLEU scores (Papineni et al., 2002)
and compare our systems to a RAW baseline (built
following the instructions of the shared task) and a
baseline very similar to Fraser et al. (2012), using
a lemmatised representation of words for decod-
ing, re-inflecting them after translation, but with-
out compound processing (UNSPLIT). Table 4
shows that only UNSPLIT and STR (source lan-
guage and a reduced set of target language fea-
tures) are significantly
9
improving over the RAW
baseline. They also significantly outperform all
other systems, except ST (full source and target
language feature set). The difference between STR
(14.61) and the UNSPLIT baseline (14.74) is not
statistically significant.
9
We used pair-wise bootstrap resampling with sample size
1000 and p-value 0.05, from: http://www.ark.cs.cmu.edu/MT
584
group ID example reference english UNSPLIT STR
lexically 1a: perfect match Inflationsrate Inflationsrate inflation rate 185 239
matches 1b: inflection wrong Rohstoffpreisen Rohstoffpreise raw material prices 40 44
the 2a: merging wrong Anwaltsbewegung Anw?altebewegung lawyers movement 5 9
reference 2b: no merging Polizei Chef Polizeichef police chief 101 54
correct 3a: compound Zentralbanken Notenbank central banks 92 171
translation 3b: no compound pflanzliche
?
Ole Speise?ol vegetable oils 345 291
wrong 4a: compound Haushaltsdefizite Staatshaushalts state budget 12 42
translation 4b: no compound Ansporn Linien Nebenlinien spur lines 325 255
Total number of compounds in reference text: 1,105 1,105
Table 5: Groups for detailed manual compound evaluation and results for UNSPLIT and STR.
reference English source UNSPLIT baseline STR
Teddyb?aren teddy bear 4b
Teddy tragen
1a
Teddyb?aren
(Teddy, to bear) (teddy bear)
Emissionsreduktion emissions reduction 3b
Emissionen Reduzierung
3a
Emissionsverringerung
(emissions, reducing) (emission decrease)
Geldstrafe fine 4b
sch?onen
3a
Bu?geld
(fine/nice) (monetary fine)
Tischtennis table tennis 2b
Tisch Tennis
4a
Spieltischtennis
(table, tennis) (play table tennis)
Kreditkartenmarkt credit-card market 2b
Kreditkarte Markt
4a
Kreditmarkt
(credit-card, market) (credit market)
Rotationstempo rotation rate 2b
Tempo Rotation
4a
Temporotation
(rate, rotation) (rate rotation)
Table 6: Examples of the detailed manual compound analysis for UNSPLIT and STR.
Compound processing leads to improvements at
the level of unigrams and as BLEU is dominated
by four-gram precision and length penalty, it does
not adequately reflect compound related improve-
ments. We thus calculated the number of com-
pounds matching the reference for each experi-
ment and verified whether these were known to
the training data. The numbers in Table 4 show
that all compound processing systems outperform
both baselines in terms of finding more exact refer-
ence matches and also more compounds unknown
to the training data. Note that STR finds less ref-
erence matches than e.g. T or ST, but it also pro-
duces less compounds overall, i.e. it is more pre-
cise when producing compounds.
However, as compounds that are correctly com-
bined but poorly inflected are not counted, this is
only a lower bound on true compounding perfor-
mance. We thus performed two additional manual
evaluations and show that the quality of the com-
pounds (Section 7.2), and the human perception of
translation quality is improving (Section 7.3).
7.2 Detailed Evaluation of Compounds
This evaluation focuses on how compounds in the
the reference text have been translated.
10
We:
10
In another evaluation, we investigated the 519 com-
pounds that our system produced but which did not match
the reference: 367 were correct translations of the English,
1. manually identify compounds in German
reference text (1,105 found)
2. manually perform word alignment of these
compounds to the English source text
3. project these English counterparts of com-
pounds in the reference text to the decoded
text using the ??print-alignment-info? flag
4. manually annotate the resulting tuples, us-
ing the categories given in Table 5
The results are given in the two rightmost columns
of Table 5: besides a higher number of reference
matches (cf. row 1a), STR overall produces more
compounds than the UNSPLIT baseline, cf. rows
2a, 3a and 4a. Indirectly, this can also be seen from
the low numbers of STR in category 2b), where
the UNSPLIT baseline produces much more (101
vs. 54) translations that lexically match the refer-
ence without being a compound. While the 171
compounds of STR of category 3a) show that our
system produces many compounds that are correct
translations of the English, even though not match-
ing the reference (and thus not credited by BLEU),
the compounds of categories 2a) and 4a) contain
examples where we either fail to reproduce the
correct compound or over-generate compounds.
We give some examples in Table 6: for ?teddy
bear?, the correct German word ?Teddyb?aren? is
87 contained erroneous lexemes and 65 were over-mergings.
585
missing in the parallel training data and instead
of ?B?ar? (?bear?), the baseline selected ?tragen?
(?to bear?). Extracting all words containing the
substring ?b?ar? (?bear?) from the original parallel
training data and from its underspecified split
version demonstrates that our approach is able
to access all occurrences of the word. This leads
to higher frequency counts and thus enhances
the probabilities for correct translations. We can
generalise over 18 different word types containing
?bear? (e.g. ?polar bears?, ?brown bears?, ?bear
skin?, ?bear fur?) to obtain only 2:
occurrences in raw training data: B?ar (19), B?aren
(26), B?arendienst (42), B?arenfarmen (1), B?arenfell (2),
B?arengalle(1), B?arenhaut (1), B?arenmarkt (1), Braunb?ar
(1), Braunb?aren (3), Braunb?arengebiete (1), Braunb?ar-
Population (1), Eisb?aren(18), Eisb?arenpopulation (2),
Eisb?arenpopulationen (1), Schwarzb?ar (1), Schwarzb?aren (1)
?b?ar? occurring in underspecified split data:
B?ar<+NN><Masc><Sg> (94)
B?ar<+NN><Masc><Pl> (29)
?Emissionsverringerung? (cf. Table 6) is a typ-
ical example of group 3a): a correctly translated
compound that does not lexically match the ref-
erence, but which is semantically very similar to
the reference. The same applies for ?Bu?geld?,
a synonym of ?Geldstrafe?, for which the UN-
SPLIT baseline selected ?sch?onen? (?fine, nice?)
instead. Consider also the wrong compound pro-
ductions, e.g. ?Tischtennis? is combined with
the verb ?spielen? (?to play?) into ?Spieltischten-
nis?. In contrast, ?Kreditmarkt? dropped the mid-
dle part ?Karte? (?card?), and in the case of ?Tem-
porotation?, the head and modifier of the com-
pound are switched.
7.3 Human perception of translation quality
We presented sentences of the UNSPLIT baseline
and of STR in random order to two native speak-
ers of German and asked them to rank the sen-
tences according to preference. In order to pre-
vent them from being biased towards compound-
bearing sentences, we asked them to select sen-
tences based on their native intuition, without re-
vealing our focus on compound processing.
Sentences were selected based on source lan-
guage sentence length: 10-15 words (178 sen-
tences), of which either the reference or our
system had to contain a compound (95 sen-
tences). After removing duplicates, we ended up
with 84 sentences to be annotated in two subse-
(a) Fluency: without reference sentence
? = 0.3631
person 1
STR UNSPLIT equal
p
e
r
s
o
n
2
STR 24 6 7 37
UNSPLIT 5 16 9 30
equal 6 2 9 17
35 24 25 84
(b) Adequacy: with reference sentence
? = 0.4948
person 1
STR UNSPLIT equal
p
e
r
s
o
n
2
STR 23 4 5 32
UNSPLIT 4 21 7 32
equal 5 3 12 20
32 28 24 84
Table 7: Human perception of translation quality.
quent passes: first, without being given the refer-
ence sentence (approximating fluency), then, with
the reference sentence (approximating adequacy).
The results are given in Table 7. Both annotators
preferred more sentences of our system overall,
but the difference is clearer for the fluency task.
8 Conclusion
Compounds require special attention in SMT, es-
pecially when translating into a compounding lan-
guage. Compared with the baselines, all of our ex-
periments that included compound processing pro-
duced not only many more compounds matching
the reference exactly, but also many compounds
that did not occur in the training data. Taking
a closer look, we found that some of these new
compounds could only be produced due to the un-
derspecified representation we are using, which al-
lows us to generalise over occurrences of simple
words, compound modifiers and heads. Moreover,
we demonstrated that features derived from the
source language are a valuable source of informa-
tion for compound prediction: experiments were
significantly better compared with contrastive ex-
periments without these features. Additional man-
ual evaluations showed that compound processing
leads to improved translations where the improve-
ment is not captured by BLEU.
Acknowledgements
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We thank the anonymous re-
viewers for their comments and the annotators.
586
References
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
HLT-NAACL?12: Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, volume 12, pages 34?35. Association
for Computational Linguistics.
Chris Dyer. 2010. A Formal Model of Ambiguity and
its Applications in Machine Translation. Phd disser-
tation, University of Maryland, USA.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In EACL?12: Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 664?
674. Association for Computational Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth Work-
shop on Statistical Machine Translation, pages 224?
234. Association for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, UK, July. Association for Computational
Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL ?03:
Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL?07: Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, Demonstration Session, pages 177?180. Asso-
ciation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML?01: Proceedings of the 18th
International Conference on Machine Learning.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpholog-
ical Operations. In ACL ?11: Proceedings of the
49th annual meeting of the Association for Compu-
tational Linguistics, pages 1395?1404. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In ACL?02:
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318. Association for Computational Linguistics.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In FinTAL?06: Proceedings of the
5th International Conference on Natural Language
Processing, pages 616?624. Springer Verlag.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In LREC ?04: Proceedings of the 4th Confer-
ence on Language Resources and Evaluation, pages
1263?1266.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modelling Toolkit. In ICSLN?02: Proceed-
ings of the international conference on spoken lan-
guage processing, pages 901?904.
Sara Stymne and Nicola Cancedda. 2011. Productive
Generation of Compound Words in Statistical Ma-
chine Translation. In EMNLP?11: Proceedings of
the 6th Workshop on Statistical Machine Transla-
tion and Metrics MATR of the conference on Em-
pirical Methods in Natural Language Processing,
pages 250?260. Association for Computational Lin-
guistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In ACL?08: Pro-
ceedings of the 3rd workshop on statistical machine
translation of the 46th annual meeting of the Associ-
ation for Compuational Linguistics, pages 135?138.
Association for Computational Linguistics,.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
EACL ?09: Proceedings of the Student Research
Workshop of the 12th conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 61?69. Association for Computa-
tional Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In ACL?08: Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 514?522. Association for Computational
Linguistics.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In ACL?13: Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 593?603.
Association for Computational Linguistics.
587
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 593?603,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using subcategorization knowledge to improve case prediction
for translation to German
Marion Weller1 Alexander Fraser2 Sabine Schulte im Walde1
1Institut fu?r Maschinelle 2Centrum fu?r Informations-
Sprachverarbeitung und Sprachverarbeitung
Universita?t Stuttgart Ludwig-Maximilians-Universita?t Mu?nchen
{wellermn|schulte}@ims.uni-stuttgart.de fraser@cis.uni-muenchen.de
Abstract
This paper demonstrates the need and im-
pact of subcategorization information for
SMT. We combine (i) features on source-
side syntactic subcategorization and (ii)
an external knowledge base with quantita-
tive, dependency-based information about
target-side subcategorization frames. A
manual evaluation of an English-to-
German translation task shows that the
subcategorization information has a posi-
tive impact on translation quality through
better prediction of case.
1 Introduction
When translating from a morphologically poor
language to a morphologically rich language we
are faced with two major problems: (i) the rich-
ness of the target-language morphology causes
data sparsity problems, and (ii) information about
morphological features on the target side is not
sufficiently contained in the source language mor-
phology.
We address these two problems using a two-
step procedure. We first replace inflected forms
by their stems or lemmas: building a translation
system on a stemmed representation of the target
side leads to a simpler translation task, and the
morphological information contained in the source
and target language parts of the translation model
is more balanced. In the second step, the stemmed
output of the translation is then inflected: the mor-
phological features are predicted, and the inflected
forms are generated using the stem and predicted
morphological features.
In this paper, we focus on improving case pre-
diction for noun phrases (NPs) in German trans-
lations. The NP feature case is extremely dif-
ficult to predict in German: while the NP fea-
tures gender and number are part of the stem or
can be derived from the source-side input, respec-
tively, the prediction of case requires information
about the subcategorization of the entire clause.
This is due to German being a less configurational
language than English, which encodes grammati-
cal relations (e.g. subject-hood, object-hood, etc.)
through the position of constituents. German sen-
tences exhibit a freer constituent order, and thus
case is an important indicator of the grammatical
functions of noun phrases. Correct case predic-
tion is a crucial factor for the adequacy of SMT
output, cf. the example in table 1 providing an
erroneously inflected output (this is taken from a
baseline ?simple inflection prediction? system, cf.
section 5.2). The translation of the English input
sentence in terms of stems is perfectly acceptable;
after the inflection step, however, the translation
of NP4 ongoing military actions represents a geni-
tive modifier of the subject NP2, instead of a direct
object NP of the verb anordnen (to order). The
meaning is thus why the government of the ongo-
ing military actions ordered, which has only one
NP and is completely wrong.
The translation in table 1 needs verb subcatego-
rization information. This is demonstrated by the
invented examples (1) and (2):
(1) [Der Mitarbeiter]NPnom hat [den Bericht]NPacc [dem
Kollegen]NPdat gegeben.
[The employee]NPnom gave [his colleague]NPdat [the
report]NPacc
(2) [Der Mitarbeiter]NPnom hat [dem Bericht]NPdat [des
Kollegen]NPgen zugestimmt.
[The employee]NPnom agreed [on the report]PP [of
his colleague]PP
Both inflected sentences rely on the stem sequence
[d Mitarbeiter] [d Bericht] [d Kollege] ?verb?,
so the case assignment can only be determined by
the verb: While geben ( to give) has a strong pref-
erence for selecting a ditransitive subcategoriza-
tion frame1, including an agentive subject (nomi-
1A ditransitive verb takes a subject and two objects.
593
input [why]1 [the government]2 [ordered]3 [the ongoing military actions]4
output stemmed [warum]1 [d Regierung]2 [d anhaltend milita?risch Aktion]4 [angeordnet]3inflected [warum]1 [die Regierung]2 [der anhaltenden milita?rischen Aktionen]4 [angeordnet]3
Table 1: Example for case confusion in SMT output when using a simple prediction system.
native case), a benefactive (dative case) and a pa-
tient (accusative case), zustimmen (to agree) has
a strong preference for only selecting an agentive
subject (nominative case) and an indirect object
theme (dative case). So in the latter case the NP
[d Kollege] cannot receive case from the verb and
is instead the genitive modifier of the dative NP.
While for examples (1) and (2) knowledge
about the syntactic verb subcategorization func-
tions is sufficient to correctly predict the NP cases,
examples (3) to (6) require subcategorization in-
formation at the syntax-semantic interface.
(3) [Der Mitarbeiter]NPnom hat [dem Kollegen]NPdat
[den Bericht]NPacc gegeben.
(4) [Der Mitarbeiter]NPnom hat [den Bericht]NPacc [dem
Kollegen]NPdat gegeben.
(5) [Dem Kollegen]NPdat hat [der Mitarbeiter]NPnom
[den Bericht]NPacc gegeben.
(6) [Den Bericht]NPacc hat [der Mitarbeiter]NPnom [dem
Kollegen]NPdat gegeben.
In all four examples, the verb and the participat-
ing noun phrases Mitarbeiter (employee), Kollege
(colleague) and Bericht (report) are identical, and
the noun phrases are assigned the same case. How-
ever, given that the stemmed output of the trans-
lation does not tell us anything about case fea-
tures, in order to predict the appropriate cases of
the three noun phrases, we either rely on ordering
heuristics (such that the nominative NP is more
likely to be in the beginning of the sentence (the
German Vorfeld) than the accusative or dative NP,
even though all three of these would be grammati-
cal), or we need fine-grained subcategorization in-
formation beyond pure syntax. For example, both
Mitarbeiter and Kollege would satisfy the agentive
subject role of the verb geben better than Bericht,
and Bericht is more likely to be the patient of
geben.
The contribution of this paper is to improve the
prediction of case in our SMT system by imple-
menting and combining two alternative routes to
integrate subcategorization information from the
syntax-semantic interface: (i) We regard the trans-
lation as a function of the source language in-
put, and project the syntactic functions of the En-
glish nouns to their German translations in the
SMT output. This subcategorization model is nec-
essary when there are several plausible solutions
for the syntactic functions of a noun in combina-
tion with a verb. For example, both Mitarbeiter
and Kollege are plausible subjects and direct ob-
jects of the verb geben, so the information about
these nouns? roles in the input sentence allows
for disambiguation. (ii) The case of an NP is de-
rived from an external knowledge base comprising
quantitative, dependency-based information about
German verb subcategorization frames and noun
modification. The verb subcategorization infor-
mation is not restricted to syntactic noun func-
tions but models association strength for verb?
noun pairs with regard to the entire subcatego-
rization frame plus the syntactic functions of the
nouns. For example, the database can tell us that
while the verb geben is very likely to subcatego-
rize a ditransitive frame, the verb zustimmen is
very likely to subcategorize only a direct object,
next to the obligatory subject (subcat frame pre-
diction). Furthermore, we can retrieve the infor-
mation that the noun Bericht is less likely to ap-
pear as subject of geben than the nouns Mitar-
beiter and Kollege (verb?noun subcat case pre-
diction). And we can look up that the noun Aktion
is very unlikely to be a genitive modification of
Regierung (cf. table 1), while Kollege is a plausi-
ble genitive modification of Bericht (noun?noun
modification case prediction, cf. example (2)).
In summary, model (i) applies when there are no
obvious preferences concerning verb?noun sub-
categorization or noun?noun modification. Model
(ii) predicts case relying on the subcategoriza-
tion and modification preferences. The combina-
tion of our two models approaches a simplified
level of semantic role definition but only relies on
dependency information that is considerably eas-
ier and cheaper to define and obtain than a very
high quality semantic parser and/or a corpus an-
notated with semantic role information. Integrat-
ing semantic role information into SMT has been
demonstrated by various researchers to improve
translation quality (cf. Wu and Fung (2009a), Wu
and Fung (2009b), Liu and Gildea (2008), Liu
and Gildea (2010)). Our approach is in line with
594
Wu and Fung (2009b) who demonstrated that on
the one hand 84% of verb syntactic functions in
a 50-sentence test corpus projected from Chinese
to English, and that on the other hand about 15%
of the subjects were not translated into subjects,
but their semantic roles were preserved across lan-
guage. These two findings correspond to the ex-
pected uses of our models (i) and (ii), respectively.
2 Previous work
Previous work has already introduced the idea of
generating inflected forms as a post-processing
step for a translation system that has been
stripped of (most) target-language-specific fea-
tures. Toutanova et al (2008) and Jeong et al
(2010) built translation systems that predict in-
flected word forms based on a large array of mor-
phological and syntactic features, obtained from
both source and target side. Kholy and Habash
(2012) and Green and DeNero (2012) work on En-
glish to Arabic translation and model gender, num-
ber and definiteness, focusing primarily on im-
proving fluency.
Fraser et al (2012) used a phrase-based system
to transfer stems and generated inflected forms
based on the stems and their morphological fea-
tures. For case prediction, they trained a CRF with
access to lemmas and POS-tags within a given
window. We re-implemented the system by Fraser
et al as a hierarchical machine translation system
using a string-to-tree setup. In contrast to the flat
phrase-based setting of Fraser et al (2012), syn-
tactic trees on the SMT output allow us to work
with verb?noun structures, which are relevant for
case prediction. While the CRF used for case pre-
diction in Fraser et al (2012) has access to lexi-
cal information, it is limited to a certain window
size and has no direct information about the rela-
tion of verb?noun pairs occurring in the sentence.
Using a window of a limited size is particularly
problematic for German, as there can be large gaps
between the verb and its subcategorized nouns; in-
troducing information about the relation of verbs
and nouns helps to bridge such gaps. Furthermore,
that model was not able to make effective use of
source-side features.
One of the objectives of using an inflection
prediction model is morphologically well-formed
output. Kirchhoff et al (2012) evaluated user re-
actions to different error types in machine trans-
lation and came to the result that morphological
well-formedness has only a marginal impact on
the comprehensibility of SMT output in the case
of English-Spanish translation. As already dis-
cussed, German case is essential to the meaning
of the sentence, so this result will not hold for Ger-
man output.
3 Translation pipeline
This section presents an overview of our two-step
translation process. In the first step, English in-
put is translated to German stems. In the sec-
ond step, morphological features are predicted and
inflected forms are generated based on the word
stems and the morphological features. In subsec-
tions 3.1 to 3.4, we present the simple version of
the inflection prediction system; our new features
are described in sections 4.2 and 4.3.
3.1 Stemmed representation/feature markup
We first parse the German side of the parallel
training data with BitPar (Schmid, 2004). This
maps each surface form appearing in normal text
to a stem and morphological features (case, gen-
der, number). We use this representation to create
the stemmed representation for training the trans-
lation model. With the exception of stem-markup
(discussed below), all morphological features are
removed from the stemmed representation. The
stem markup is used as part of the input to the fea-
ture prediction; the basic idea is that the given fea-
ture values are picked up by the prediction model
and then propagated over the phrase.
Nouns, as the head of NPs and PPs, are anno-
tated with gender and number. We consider gen-
der as part of the stem, whereas the value for num-
ber is derived from the source-side: if marked for
number, singular/plural nouns are distinguished
during word alignment and then translated accord-
ingly. Prepositions are also annotated with case;
many prepositions are restricted to only one case,
some are ambiguous and allow for either dative
or accusative. Other words which are subject to
feature prediction (e.g. adjectives, articles) are re-
duced to their stems with no feature markup, as
are all remaining words. As sole exception, we
keep the inflected forms of verbs (verbal inflec-
tion is not modelled). In addition to the transla-
tion model, the target-side language model, as well
as the reference data for parameter tuning use this
representation.
595
3.2 Building a stemmed translation model
We use a hierarchical translation system. Instead
of translating phrases, a hierarchical system ex-
tracts translation rules (Galley et al, 2004) which
allow the decoder to provide a tree spanning over
the translated sentence. In order to avoid sparsity
during rule extraction, we use a string-to-tree
setup, where only the target-side part of the data
is parsed. Translation rules are of the following
form:
[X]1 allows [X]2 ? [NP]1 [NP]2 erlaubt
[X]1 allows [X]2 ? [NP]1 erlaubt [NP]2
This example illustrates how rules can cover the
different word ordering possibilities in German.
PP nodes are annotated with their respective
case, as well as with the lemma of the preposition
they contain. In our experiments, this enriched an-
notation has small improvements over the simpler
setting with only head categories (details omit-
ted). This outcome, in particular that adding the
lemma of the preposition to the PP node helps to
improve translation quality, has been observed be-
fore in tree restructuring work for improving trans-
lation (Huang and Knight, 2006).
3.3 Feature prediction and generation of
inflected forms
In this section we discuss our focus, which is pre-
diction of case, but also the prediction of num-
ber, gender and strong/weak adjectival inflection.
The latter feature is German-specific; its values2
(strong/weak) depend on the combination of the
other features, as well as on the type of determiner
(e.g. definite/indefinite/none).
Morphological features are predicted on four
separate CRF models, one for each feature. The
models for case, number and gender are indepen-
dent of another, whereas the model for adjecti-
val inflection requires information about these fea-
tures, and is thus the last one to be computed, tak-
ing the output of the 3 other models as part of its
input. In contrast, the adjectival inflection model
in Fraser et al (2012) is independent from the
other features. Each model has access to stems,
POS-tags and the feature to be modelled within a
window of four positions to the right and the left
of the current position3.
2Note that the values for strong/weak inflection are not
always the same over the phrase, but follow a certain pattern
depending on the settings of case, number and gender.
3Preliminary experiments showed that larger windows do
not improve translation quality.
Table 2 illustrates the different steps of the in-
flection process: the markup (number and gender
on nouns) in the stemmed output of the SMT sys-
tem is part of the input to the respective feature
prediction. For gender and number, the values
given on the stems of the nouns are then propa-
gated over the phrase. While the case of prepo-
sitional phrases is determined by the case annota-
tion on prepositions, the case of nominal phrases
is computed only based on the respective contexts.
After predicting all morphological features, the in-
formation required to generate inflected forms is
complete: based on the stems and the features, we
use the morphological tool SMOR (Schmid et al,
2004) for the generation of inflected forms.
One general problem with feature-prediction is
that the ill-formed SMT output is not well repre-
sented by the training data which consists of well-
formed sentences. This problem was also men-
tioned by Stymne and Cancedda (2011) and Kholy
and Habash (2012). They deal with this problem
by translating the training data and annotating it
with the respective features, and then adding this
new data set to the original training data. As
this method comes with its own problems, such as
transferring the morphological annotation to not
necessarily isomorphically translated text, we do
not use translated data as part of the training data.
Instead, we limit the power of the CRF model
through experimenting with the removal of fea-
tures, until we had a system that was robust to this
problem.
3.4 Dealing with word formation issues
To reduce data sparsity, we split portmanteau
prepositions. Portmanteaus are compounds of
prepositions and articles, e.g. zur = zu der (to the).
Being components of nominal phrases, they have
to agree in all morphological features with the rest
of the phrase. As only some combinations of arti-
cles and prepositions can form a portmanteau, the
decision of whether to merge prepositions and ar-
ticles is made after feature prediction. Since our
focus is case prediction, we do not do special mod-
elling of German compounds.
4 Using subcategorization information
Within the area of (automatic) lexical acquisition,
the definition of lexical verb information has been
a major focus, because verbs play a central role
for the structure and the meaning of sentences and
596
SMT output predicted features inflected forms gloss
beeinflussen<VVFIN> ? beeinflussen influence
d<ART> Fem.Acc.Sg.St die the
politisch<ADJ> Fem.Acc.Sg.Wk politische political
Stabilita?t<NN><Fem><Sg> Fem.Acc.Sg.Wk Stabilita?t stability
Table 2: Overview of the inflection process: the stem markup is highlighted in the SMT output.
discourse. On the one hand, this has led to a range
of manually or semi-automatically developed lex-
ical resources focusing on verb information, such
as the Levin classes (Levin, 1993), VerbNet (Kip-
per Schuler, 2006), FrameNet4 (Fillmore et al,
2003), and PropBank (Palmer et al, 2005). On the
other hand, we find automatic approaches to the
induction of verb subcategorization information at
the syntax-semantics interface for a large num-
ber of languages, e.g. Briscoe and Carroll (1997)
for English; Sarkar and Zeman (2000) for Czech;
Schulte im Walde (2002a) for German; Messiant
(2008) for French. This basic kind of verb knowl-
edge has been shown to be useful in many NLP
tasks such as information extraction (Surdeanu et
al., 2003; Venturi1 et al, 2009), parsing (Carroll et
al., 1998; Carroll and Fang, 2004) and word sense
disambiguation (Kohomban and Lee, 2005; Mc-
Carthy et al, 2007).
4.1 Extracting subcategorization information
As described in the introductory section, we make
use of two5 major kinds of subcategorization in-
formation. Verb?noun tuples referring to spe-
cific syntactic functions within verb subcatego-
rization (verb?noun subcat case prediction) are
integrated with an associated probability for ac-
cusative (direct object), dative (indirect object)
and nominative (subject).6 Further to the sub-
ject and object noun phrases, the subcategoriza-
tion information provides quantitative triples for
verb?preposition?noun pairs, thus predicting the
case of NPs within prepositional phrases (we do
this only when the prepositions are ambiguious,
i.e., they could subcategorize either a dative or
an accusative NP). In addition to modelling sub-
categorization information, it is also important to
differentiate between subcategorized noun phrases
(such as object or subject), and noun phrases
4Even though the FrameNets approach does not only in-
clude knowledge about verbal predicates, the actual lexicons
are skewed towards verb behaviour.
5The third kind of information, subcat frame prediction
is implicit, since verb?noun tuples rely on specific frames.
6Genitive objects can also occur in German verb subcate-
gorization frames, but this is extremely rare and verb-specific
and thus not considered in our model.
V-SUBJ V-OBJAcc V-OBJDat
EP 454,350 332,847 53,711
HGC 712,717 329,830 160,377
Both 1,089,492 607,541 206,764
Table 3: Number of verb-noun types extracted
from Europarl (EP) and newspaper data (HGC).
that modify nouns (noun?noun modification case
prediction). Typically, these NP modifiers are
genitive NPs. To this end, we integrate noun-
nounGen tuples with their respective frequencies.
These preferences for a certain function (i.e. sub-
ject, object or modifier) are passed on to the sys-
tem at the level of nouns and integrated into the
CRF through the derived probabilities.
The tuples and triples are obtained from
dependency-parsed data by extracting all occur-
rences of the respective relations; table 3 gives an
overview of the number of extracted tuple types.
For the subcategorization information, the verb-
noun tuples (verb-subject, verb-objectAcc, verb-
objectDat) are then grouped as follows:
tuple gloss Acc Dat Nom
SchemaN folgenV pattern follow 0 322 19
We compute the probabilities for the verb-noun tu-
ple to occur in the respective functions based on
the relative frequencies. In the case of SchemaN
folgenV , we find that the function of Schema as da-
tive object is predominant (to follow a pattern), but
it can also occur in the subject position (the pat-
tern follows). The fact that two functions are pos-
sible for this noun are reflected in their probabili-
ties. The probabilities are discretized into 5 buck-
ets (Bp=0, B0<p?0.25, B0.25<p?0.5, B0.5<p?0.75,
B0.75<p?1). In contrast, noun modification in
noun-nounGen construction is represented by co-
occurrence frequencies.7
7The frequencies are bucketed to the powers of ten, i.e.
f = 1, 2 ? f ? 10, 11 ? f ? 100 , etc. and also f = 0:
this representation allows for a more fine-grained distinction
in the low-to-mid frequency range, providing a good basis
for the decision of whether a given noun-noun pair is a true
noun-nounGen structure or just a random co-occurrence of
two nouns.
597
Gloss Stem Tag Acc Dat Nom Verb Gen N1 Gold
1 companies Unternehmen<NN> NN 0.00 0.00 1.00 erhalten ? ? Nom
2 should sollten<VVFIN> VVFIN ? ? ? ? ? ? ?
3 financial finanziell<ADJ> ADJ ? ? ? ? ? ? Acc
4 funding Mittel<NN> NN 1.00 0.00 0.00 erhalten ? ? Acc
5 for fu?r APPR<Acc> PRP ? ? ? ? ? ? ?
6 the d<ART> ART ? ? ? ? ? ? Acc
7 introduction Einfu?hrung<NN> NN ? ? ? ? ? ? Acc
8 new neu<ADJ> ADJ ? ? ? ? ? ? Gen
9 technologies Technologie<NN> NN ? ? ? ? 100 Einfu?hrung<NN> Gen
10 obtain erhalten<VVINF> VVINF ? ? ? ? ? ? ?
Table 4: Adding subcategorization information into SMT output. (EN input: companies should obtain
financial funding for the introduction of new technologies). On the right, the correct labels are given.
4.2 Integrating subcategorization knowledge
There are two possibilities to integrate subcat-
egorization information into the case prediction
model: (i) It can be integrated into the data set
using the tree-structure provided by the decoder.
Here, verb-noun tuples are extracted from VP and
S structures, and then the probabilities for the dif-
ferent functions are looked up. Similarly, for two
adjacent NPs, the occurrence frequencies of the
respective two nouns are looked up in the list of
noun-nounGen constructions. (ii) The subcatego-
rization information can be integrated based on
the verb-noun tuples obtained by using tuples ob-
tained from source-side dependencies.
The classification task of the CRF consists in
predicting a sequence of labels: case values for
NPs/PPs or no value otherwise, cf. table 4. The
model has access to the basic features stem and
tag, as well as the new features based on subcat-
egorizaion information (explained below), using
unigrams within a window of up to four positions
to the right and the left of the current position, as
well as bigrams and trigrams for stems and tags
(current item + left and/or right item).
An example for integrating subcategorization
features is given in table 4. The first word Un-
ternehmen (companies) is annotated as subject of
erhalten (obtain) with probability 1, and Mittel
(funding) is annotated as direct object of erhal-
ten with probability 1. The word Technologie
(technology) has been marked as a candidate for
a genitive in a noun-nounGen construction8; the
co-occurrence frequency of the tuple Einfu?hrung-
Technologie (introduction - technology) lies in the
bucket 11. . . 100.
In addition to the probability/frequency of the
respective functions, we also provide the CRF
with bigrams containing the two parts of the tuple,
8There is no annotation on Einfu?hrung as the preposition
fu?r is always in accusative case.
DE stemmed output
warum<PWAV>die<ART>Regierung<NN><Sg><Fem>die<ART>anhaltend<ADJ>milit?risch<ADJ>Aktion<NN><Pl><Fem>angeordnet<VVFIN>
derived features
SUBJ  V:anordnen
OBJ  V:anordnen
SUBJ
OBJ
EN input
whythe governmentorderedtheongoingmilitaryactions
Figure 1: Deriving features from dependency-
parsed English data via the word alignment.
i.e. verb+noun or the two nouns of possible noun-
nounGen constructions. As can be seen in the ex-
ample in table 4, the subject (line 1) and the verb
(line 10) are far apart from each other. By pro-
viding the parts of the tuple as unigrams, bigrams
or trigrams to the CRF, all relevant information
is available: verb, noun and the probabilities for
the potential functions of the noun in the sentence.
In addition to bridging the long distance between
verbs and subcategorized nouns, a very common
problem for German, this type of precise informa-
tion also helps to close the gap between the well-
formed training data and the broken SMT-output
as it replaces to a certain extent the target-language
context information (n-grams of stems or lemmas
within a small window).
4.3 Integrating source-side features
For predicting case in SMT output, information
about an NP?s function in the input sentence is
essential. Syntax-semantic functions can be iso-
morphic (e.g., English subjects and objects may
have the same function in a German translation),
but this is not necessarily the case. Despite this,
an important advantage of integrating source-side
features is that the well-formed source-side text
can be reliably parsed, whereas SMT output is of-
ten disfluent and cannot be reliably parsed.
The English features are obtained from
dependency-parsed data (Choi and Palmer, 2012).
The relevant annotation of the parser is transferred
598
to the SMT output via word alignment. We focus
on English subjects, direct objects and noun-of-
noun structures (often equivalent to noun-nounGen
phrases on the German side): these structures
are generally likely to correspond to each other
within source and target language. In contrast
to the subcategorization-based information, the
difference between well-formed training data and
disfluent SMT output tends to work to our benefit
here: while the parallel sentences of the training
data were manually translated with the objective
to produce good target-language sentences, the
syntactic structures of the source and target
sentences are often diverging. In contrast, the
SMT system often produces more isomorphic
translations, which is helpful for annotating
source-side features on the target language.
Figure 1 shows the process of integrating
source-side features: for each German noun that
is aligned with an English noun labelled as subject
or direct object, this annotation is transferred to the
target-side. Using the English dependency struc-
tures, the verb subcategorizing the respective noun
is identified, and via the alignment, the equivalent
German verb is obtained. Similarly, candidates for
noun-nounGen structures are identified by extract-
ing and aligning English noun-of-noun phrases.
5 Experiments and evaluation
In this section, we present experiments using dif-
ferent feature combinations. We also present a
manual evaluation of our best system which shows
that the new features improve translation quality.
5.1 Data and experimental setup
We use the hierarchical translation system that
comes with the Moses SMT-package and GIZA++
to compute the word alignment, using the ?grow-
diag-final-and? heuristics. The rule table was
computed with the default parameter setting for
GHKM extraction (Galley et al, 2004) in the im-
plementation by Williams and Koehn (2012).
Our training data contains 1,485,059 parallel
sentences9; the German part of the parallel data
is used as the target-side language model. The dev
and test sets (1025/1026 lines) are wmt-2009-a/b.
For predicting the grammatical features, we
used the Wapiti Toolkit (Lavergne et al, 2010).10
9English/German data released for the 2009 ACL Work-
shop on Machine Translation shared task.
10To eliminate irrelevant features, we use L1 regulariza-
We train four CRFs on data prepared as shown
in section 3. The corpora used for the extrac-
tion of subcategorization tuples were Europarl and
German newspaper data (200 million words). We
choose this particular data combination in order to
provide data that matches the training data, as well
as to add new data of the test set?s domain (news).
The German part of Europarl was dependency-
parsed with Bohnet (2010), and subcategorization
information was extracted as described in Scheible
et al (2013); the newspaper data (HGC - Huge
German Corpus) was parsed with Schmid (2000),
and subcategorization information was extracted
as described in Schulte im Walde (2002b).
5.2 Results
We report results of two types of systems (ta-
ble 5): first, a regular translation system built on
surface forms (i.e., normal text) and second, four
inflection prediction systems. The first inflection
prediction system (1) uses a simple case predic-
tion model, whereas the remaining systems are
enriched with (2) subcategorization information
(cf. section 4.2), (3) source-side features (cf. sec-
tion 4.3), and (4) both source-side features and
subcategorization information. In (2) and (4), the
subcategorization information was included using
tuples obtained from source-side dependencies11.
The simple prediction system corresponds to that
presented in section 3; for all inflection predic-
tion systems, the same SMT output and models for
number, gender and strong/weak inflection were
used; thus the only difference with the simple pre-
diction system is the model for case prediction.
We present three types of evaluation: BLEU
scores (Papineni et al, 2001), prediction accuracy
on clean data and a manual evaluation of the best
system in section 5.3.
Table 5 gives results in case-insensitive BLEU.
While the inflection prediction systems (1-4) are
significantly12 better than the surface-form sys-
tem (0), the different versions of the inflection sys-
tems are not distinguishable in terms of BLEU;
however, our manual evaluation shows that the
new features have a positive impact on translation
quality.
tion; the regularization parameter is optimized on held out
data.
11Using tuples extracted from the target-side parse tree
(produced by the decoder) results in a BLEU score of 14.00.
12We used Kevin Gimpel?s implementation of pairwise
bootstrap resampling with 1000 samples.
599
0 1 2 3 4
surface simple subcat. features source-side source-side
system prediction (tuples from EN side) features + subcat. featues
BLEU 13.43 14.02 14.05 14.10 14.17
Clean ? 85.05 % 85.65 % 85.61 % 85.81 %
Table 5: Results of the simple prediction vs. three systems enriched with extra features.
One problem with using BLEU as an evalua-
tion metric is that it is a precision-oriented met-
ric and tends to reward fluency rather than ade-
quacy (see (Wu and Fung, 2009a; Liu and Gildea,
2010)). As we are working on improving ade-
quacy, this will not be fully reflected by BLEU.
Furthermore, not all components of an NP do nec-
essarily change their inflection with a new case
value; it might happen that the only indicator for
the case of an NP is the determiner: er sieht [den
alten Mann]NPacc (he sees the old man) vs. er
folgt [dem alten Mann]NPdat (he follows the old
man). While the case marking of NPs is essential
for comprehensibility, one changed word per noun
phrase is hardly enough to be reflected by BLEU.
An alternative to study the effectiveness of the
case prediction model is to evaluate the prediction
accuracy on parsed clean data, i.e. not on SMT
output. In this case, we measure (using the dev
set) how often the case of an NP is predicted cor-
rectly13. In all cases, the prediction accuracy is
better for the enriched systems. This shows that
the additional features improve the model, but also
that a gain in prediction accuracy on clean data is
not necessarily related to a gain in BLEU. We ob-
served that the more complex the model, the less
robust it is to differences between the test data
and the training data. Related to this problem,
we observed that high-order n-gram POS/lemma-
based features in the simple prediction (sequences
of lemmas and tags) are given too much weight in
training and thus make it difficult for the new fea-
tures to have a larger impact, so we restricted the
n-gram order of this type of feature to trigrams.
5.3 Manual evaluation of the best system
In order to provide a better understanding of the
impact of the presented features, in particular to
see whether there is an improvement in adequacy,
we carried out a manual evaluation comparing sys-
13The numbers in table 5 are artificially high and downplay
the difference as they also include cases which are very easy
to predict, such as nouns in PPs where only one value for case
is possible. We measure how many case labels were correctly
predicted, not correct inflected forms.
enriched simple equal
preferred preferred
person 1 23 11 12
(a) person 2 21 8 17
person 3 26 11 9
person 1 23 5 18
(b) person 2 21 11 14
person 3 29 8 9
(c) agreement 17 2 6
Table 6: Manual evaluation of 46 sentences: with-
out (a) and with (b) access to EN input, and the
annotators? agreement in the second part (c).
tem (4) with the simple prediction system (1).
From the set of different sentences between the
simple prediction system and the enriched system
(144 of 1026), we evaluated those where the En-
glish input sentence was between 8 and 25 words
long (46 sentences in total). We specifically re-
stricted the test set in order to provide sentences
which are less difficult to annotate, as longer sen-
tences are often very disfluent and too hard to rate.
Most of the sentences in the evaluation set differ
only in the realization of one NP. For comparing
the two systems, the sentences were presented in
random order to 3 native speakers of German.
The evaluation consists of two parts: first, the
participants were asked to decide which sentence
is better without being given the English input
(this measures fluency). In the second part, they
should to mark that sentence which better repro-
duces the content of the English input sentence
(this measures adequacy). The test set is the same
for both tasks, the only difference being that the
English input is given in the second part. The re-
sults are given in table 6. Summarizing we can
say that the participants prefer the enriched sys-
tem over the simple system in both parts; there is a
high agreement (17 cases) in decisions over those
sentences which were rated as enriched better.
When looking at the pairwise inter-annotator
agreement for the task of annotating the test-set
with the 3 possible labels enriched preferred, sim-
ple preferred and no preference, we find that the
annotators P1 and P2 have a substantial agreement
600
input hundreds of policemen were on alert , and [a helicopter]Subj circled the area with searchlights .
1 simple Hunderte von Polizisten auf Trab , und [einen Helikopter]Acc eingekreist das Gebiet mit searchlights .
enriched Hunderte von Polizisten auf Trab , und [ein Helikopter]Nom eingekreist das Gebiet mit searchlights .
input while 38 %percent put [their trust]Obj in viktor orba?n .
2 simple wa?hrend 38 % [ihres Vertrauens]Gen schenken in Viktor Orba?n .
enriched wa?hrend 38 % [ihr Vertrauen]Acc schenken in Viktor Orba?n .
input more than $ 100 billion will enter [the monetary markets]Obj by means of public sales .
3 simple mehr als 100 Milliarden Dollar werden durch o?ffentlichen Verkauf [der Geldma?rkte]Gen treten .
enriched mehr als 100 Milliarden Dollar werden durch o?ffentlichen Verkauf [die Geldma?rkte]Acc treten .
Table 7: Output from the simple system (1) and the enriched system (4).
in terms of Kappa (? = 0.6184), whereas the agree-
ment of P3 with P1/P2 respectively leads to lower
scores (? = 0.4467 and ? = 0.3596). However, the
annotators tend to agree well on sentences with
the label enriched preferred, but largely disagree
on sentences labelled as either simple preferred or
no preference. The number of decisions where all
three annotators agree on a label when given the
English input is listed in table 6(c): for example,
only two sentences were given the label baseline is
better by all three annotators. This outcome shows
how difficult it is to rate disfluent SMT output. For
evaluating the case prediction system, the distinc-
tion between enriched preferred and enriched dis-
preferred is the most important question to answer.
Redefining the annotation task to annotating only
two values by grouping the labels simple preferred
and no preference into one annotation possibility
leads to ? = 0.7391, ? = 0.4048 and ? = 0.5652.
5.4 Examples
Table 7 shows some examples for output from the
simple system and the system using source-side
and subcategorization features. In the first sen-
tence, the subject NP a helicopter was inflected
as a direct object in the simple system, but as a
subject in the enriched system, which was pre-
ferred by all three annotators. In the second sen-
tence, the NP their trust, i.e. a direct object of put,
was incorrectly predicted as genitive-modifier of
38 % (i.e. 38 % of their trust) in the simple sys-
tem. The enriched system made use of the prefer-
ence for accusative for the pair Vertrauen schenken
(place trust), correctly inflecting this NP as di-
rect object. Interestingly, only two annotators pre-
ferred the enriched system, whereas one was unde-
cided. The third sentence illustrates how difficult
it is to rate case marking on disfluent SMT output:
there are two possibilities to translate enter the
money market; the direct equivalent of the English
phrase (den GeldmarktAcc betreten), or via the use
of a prepositional phrase (auf den GeldmarktAcc
treten: ?to step into the money market?). The
SMT-output contains a mix of both, i.e. the verb
treten (instead of betreten), but without the prepo-
sition, which cannot lead to a fully correct inflec-
tion. While the inflection of the simple system (a
genitive construction meaning the public sales of
the money market) is definitely wrong, the inflec-
tion obtained in the enriched system is not use-
ful either, due to the structure of the translation14.
This difficulty is also reflected by the annotators,
who gave twice the label no preference and once
the label enriched better.
6 Conclusion
We illustrated the necessity of using external
knowledge sources like subcategorization infor-
mation for modelling case for English to Ger-
man translation. We presented a translation sys-
tem making use of a subcategorization database
together with source-side features. Our method
is language-independent with regard to the source
language; furthermore, no language-specific high-
quality semantic annotation is needed for the tar-
get language, but the data required to model the
subcategorization preferences can be obtained us-
ing standard NLP techniques. We showed in a
manual evaluation that the proposed features have
a positive impact on translation quality.
Acknowledgements
This work was funded by the DFG Research
Project Distributional Approaches to Semantic Re-
latedness (Marion Weller), the DFG Heisenberg
Fellowship SCHU-2580/1-1 (Sabine Schulte im
Walde), as well as by the Deutsche Forschungsge-
meinschaft grant Models of Morphosyntax for Sta-
tistical Machine Translation (Alexander Fraser).
14Furthermore, with treten being polysemous, die
Geldma?rkte treten can also mean to kick the money markets.
601
References
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING) 2010, pages 89?
97, Beijing, August.
Ted Briscoe and John Carroll. 1997. Automatic Ex-
traction of Subcategorization from Corpora. In Pro-
ceedings of the 5th ACL Conference on Applied Nat-
ural Language Processing, pages 356?363, Wash-
ington, DC.
John Carroll and Alex C. Fang. 2004. The Auto-
matic Acquisition of Verb Subcategorisations and
their Impact on the Performance of an HPSG Parser.
In Proceedings of the 1st International Joint Confer-
ence on Natural Language Processing, pages 107?
114, Sanya City, China.
John Carroll, Guido Minnen, and Ted Briscoe. 1998.
Can Subcategorisation Probabilities Help a Sta-
tistical Parser? In Proceedings of the 6th
ACL/SIGDAT Workshop on Very Large Corpora,
Montreal, Canada.
Jinho D. Choi and Martha Palmer. 2012. Getting the
Most out of Transition-Based Dependency Parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of the the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), Avignon, France.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT-NAACL).
Spence Green and John DeNero. 2012. A Class-
Based Agreement Model for Generating Accurately
Inflected Translations. pages 146?155.
Bryant Huang and Kevin Knight. 2006. Relabel-
ing Syntax Trees to Improve Syntax-Based Machine
Translation Quality. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A Discriminative Lexicon
Model for Complex Morphology. In Proceedings of
the Ninth Conference of the Association for Machine
Translation in the Americas (AMTA 2010).
Ahmed El Kholy and Nizar Habash. 2012. Translate,
Predict or Generate: Modeling Rich Morphology in
Statistical Machine Translation. In European Asso-
ciation for Machine Translation.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania, Computer and In-
formation Science.
Katrin Kirchhoff, Daniel Capurro, and Anne Turner.
2012. Evaluating User Preferences in Machine
Translation Using Conjoint Analysis. In European
Association for Machine Translation.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning
Semantic Classes for Word Sense Disambiguation.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 34?41,
Ann Arbor, MI.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Ding Liu and Daniel Gildea. 2008. Improved Tree-
to-String Transducers for Machine Translation. In
ACL Workshop on Statistical Machine Translation.
Ding Liu and Daniel Gildea. 2010. Semantic Role
Features for Machine Translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING) 2010.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised Acquisition of Pre-
dominant Word Senses. Computational Linguistics,
33(4):553?590.
Ce?dric Messiant. 2008. A Subcategorization Acqui-
sition System for French Verbs. In Proceedings of
the Student Research Workshop at the 46th Annual
Meeting of the Association for Computational Lin-
guistics, pages 55?60, Columbus, OH.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated Re-
source of Semantic Roles. Computational Linguis-
tics, 31(1):71?106.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center.
Anoop Sarkar and Daniel Zeman. 2000. Automatic
Extraction of Subcategorization Frames for Czech.
In Proceedings of the 18th International Conference
on Computational Linguistics, Saarbru?cken, Ger-
many.
602
Silke Scheible, Sabine Schulte im Walde, Marion
Weller, and Max Kisselew. 2013. A Compact but
Linguistically Detailed Database for German Verb
Subcategorisation relying on Dependency Parses
from a Web Corpus. In Proceedings of the 8th Web
as Corpus Workshop, Lancaster, UK. To appear.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: a German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC).
Helmut Schmid. 2000. LoPar: Design and Imple-
mentation. Arbeitspapiere des Sonderforschungs-
bereichs 340 ?Linguistic Theory and the Foun-
dations of Computational Linguistics? 149, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors.
Sabine Schulte im Walde. 2002a. A Subcategorisa-
tion Lexicon for German Verbs induced from a Lex-
icalised PCFG. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation, vol-
ume IV, pages 1351?1357, Las Palmas de Gran Ca-
naria, Spain.
Sabine Schulte im Walde. 2002b. A Subcategorisa-
tion Lexicon for German Verbs induced from a Lex-
icalised PCFG. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation, vol-
ume IV, pages 1351?1357, Las Palmas de Gran Ca-
naria, Spain.
Sara Stymne and Nicola Cancedda. 2011. Productive
Generation of Compound Words in Statistical Ma-
chine Translation. In Proceedings of the Sixth Work-
shop on Machine Translation.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using Predicate-Argument
Structures for Information Extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15, Sap-
poro, Japan.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL): Human Language Technologies.
Giulia Venturi1, Simonetta Montemagni, Simone
Marchi, Yutaka Sasaki, Paul Thompson, John Mc-
Naught, and Sophia Ananiadou. 2009. Bootstrap-
ping a Verb Lexicon for Biomedical Information
Extraction. In Alexander Gelbukh, editor, Linguis-
tics and Intelligent Text Processing, pages 137?148.
Springer, Heidelberg.
Philip Williams and Phillipp Koehn. 2012. GHKM-
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the 7th Workshop on Statistical Ma-
chine Translation, ACL.
Dekai Wu and Pascale Fung. 2009a. Can Semantic
Role Labeling Improve SMT? In Proceedings of the
13th Annual Conference of the European Associa-
tion for Machine Translation (EAMT).
Dekai Wu and Pascale Fung. 2009b. Semantic Roles
for SMT: A Hybrid two-pass Model. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics and Human Lan-
guage Technologies Conference (NAACL-HLT).
603
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232?239,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions at WMT13:
Morphological and Syntactic Processing for SMT
Marion Weller1, Max Kisselew1, Svetlana Smekalova1, Alexander Fraser2,
Helmut Schmid2, Nadir Durrani3, Hassan Sajjad4, Richa?rd Farkas5
1University of Stuttgart ? (wellermn|kisselmx|smekalsa)@ims.uni-stuttgart.de
2Ludwig-Maximilian University of Munich ? (schmid|fraser)@cis.uni-muenchen.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Qatar Computing Research Institute ? hsajjad@qf.org.qa
5University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
We present 5 systems of the Munich-
Edinburgh-Stuttgart1 joint submissions to
the 2013 SMT Shared Task: FR-EN, EN-
FR, RU-EN, DE-EN and EN-DE. The
first three systems employ inflectional gen-
eralization, while the latter two employ
parser-based reordering, and DE-EN per-
forms compound splitting. For our ex-
periments, we use standard phrase-based
Moses systems and operation sequence
models (OSM).
1 Introduction
Morphologically complex languages often lead to
data sparsity problems in statistical machine trans-
lation. For translation pairs with morphologically
rich source languages and English as target lan-
guage, we focus on simplifying the input language
in order to reduce the complexity of the translation
model. The pre-processing of the source-language
is language-specific, requiring morphological anal-
ysis (FR, RU) as well as sentence reordering (DE)
and dealing with compounds (DE). Due to time
constraints we did not deal with inflection for DE-
EN and EN-DE.
The morphological simplification process con-
sists in lemmatizing inflected word forms and deal-
ing with word formation (splitting portmanteau
prepositions or compounds). This needs to take
into account translation-relevant features (e.g. num-
ber) which vary across the different language pairs:
while French only has the features number and
gender, a wider array of features needs to be con-
sidered when modelling Russian (cf. table 6). In
addition to morphological reduction, we also apply
transliteration models learned from automatically
1The language pairs DE-EN and RU-EN were developed
in collaboration with the Qatar Computing Research Institute
and the University of Szeged.
mined transliterations to handle out-of-vocabulary
words (OOVs) when translating from Russian.
Replacing inflected word forms with simpler
variants (lemmas or the components of split com-
pounds) aims not only at reducing the general com-
plexity of the translation model, but also at decreas-
ing the amount of out-of-vocabulary words in the
input data. This is particularly the case with Ger-
man compounds, which are very productive and
thus often lack coverage in the parallel training
data, whereas the individual components can be
translated. Similarly, inflected word forms (e.g. ad-
jectives) benefit from the reduction to lemmas if
the full inflection paradigm does not occur in the
parallel training data.
For EN-FR, a translation pair with a morpho-
logically complex target language, we describe a
two-step translation system built on non-inflected
word stems with a post-processing component for
predicting morphological features and the genera-
tion of inflected forms. In addition to the advantage
of a more general translation model, this method
also allows the generation of inflected word forms
which do not occur in the training data.
2 Experimental setup
The translation experiments in this paper are car-
ried out with either a standard phrase-based Moses
system (DE-EN, EN-DE, EN-FR and FR-EN) or
with an operation sequence model (RU-EN, DE-
EN), cf. Durrani et al (2013b) for more details.
An operation sequence model (OSM) is a state-
of-the-art SMT-system that learns translation and
reordering patterns by representing a sentence pair
and its word alignment as a unique sequence of
operations (see e.g. Durrani et al (2011), Durrani
et al (2013a) for more details). For the Moses sys-
tems we used the old train-model perl scripts rather
than the EMS, so we did not perform Good-Turing
smoothing; parameter tuning was carried out with
batch-mira (Cherry and Foster, 2012).
232
1 Removal of empty lines
2 Conversion of HTML special characters like
&quot; to the corresponding characters
3 Unification of words that were written both
with an ? or with an oe to only one spelling
4 Punctuation normalization and tokenization
5 Putting together clitics and apostrophes like
l ? or d ? to l? and d?
Table 1: Text normalization for FR-EN.
Definite determiners la / l? / les ? le
Indefinite determiners un / une ? un
Adjectives Infl. form ? lemma
Portmanteaus e. g. au ? a` le
Verb participles Reduced to
inflected for gender non-inflected
and number verb participle form
ending in e?e/e?s/e?es ending in e?
Clitics and apostroph- d? ? de,
ized words are converted qu? ? que,
to their lemmas n? ? ne, ...
Table 2: Rules for morphological simplification.
The development data consists of the concate-
nated news-data sets from the years 2008-2011.
Unless otherwise stated, we use all constrained data
(parallel and monolingual). For the target-side lan-
guage models, we follow the approach of Schwenk
and Koehn (2008) and train a separate language
model for each corpus and then interpolate them
using weights optimized on development data.
3 French to English
French has a much richer morphology than English;
for example, adjectives in French are inflected with
respect to gender and number whereas adjectives
in English are not inflected at all. This causes data
sparsity in coverage of French inflected forms. We
try to overcome this problem by simplifying French
inflected forms in a pre-processing step in order to
adapt the French input better to the English output.
Processing of the training and test data The
pre-processing of the French input consists of two
steps: (1) normalizing not well-formed data (cf.
table 1) and (2) morphological simplification.
In the second step, the normalized training data
is annotated with Part-of-Speech tags (PoS-tags)
and word lemmas using RFTagger (Schmid and
Laws, 2008) which was trained on the French tree-
bank (Abeille? et al, 2003). French forms are then
simplified according to the rules given in table 2.
Data and experiments We trained a French to
English Moses system on the preprocessed and
System BLEU (cs) BLEU (ci)
Baseline 29.90 31.02
Simplified French* 29.70 30.83
Table 3: Results of the French to English system
(WMT-2012). The marked system (*) corresponds
to the system submitted for manual evaluation. (cs:
case-sensitive, ci: case-insensitive)
simplified constrained parallel data.
Due to tractability problems with word align-
ment, the 109 French-English corpus and the UN
corpus were filtered to a more manageable size.
The filtering criteria are sentence length (between
15 and 25 words), as well as strings indicating that
a sentence is neither French nor English, or other-
wise not well-formed, aiming to obtain a subset of
good-quality sentences. In total, we use 9M par-
allel sentences. For the English language model
we use large training data with 287.3M true-cased
sentences (including the LDC Giga-word data).
We compare two systems: a baseline with reg-
ular French text, and a system with the described
morphological simplifications. Results for the
WMT-2012 test set are shown in table 3. Even
though the baseline is better than the simplified
system in terms of BLEU, we assume that the trans-
lation model of the simplified system benefits from
the overall generalization ? thus, human annotators
might prefer the output of the simplified system.
For the WMT-2013 set, we obtain BLEU scores
of 29,97 (cs) and 31,05 (ci) with the system built
on simplified French (mes-simplifiedfrench).
4 English to French
Translating into a morphologically rich language
faces two problems: that of asymmetry of mor-
phological information contained in the source and
target language and that of data sparsity.
In this section we describe a two-step system de-
signed to overcome these types of problems: first,
the French data is reduced to non-inflected forms
(stems) with translation-relevant morphological fea-
tures, which is used to built the translation model.
The second step consists of predicting all neces-
sary morphological features for the translation out-
put, which are then used to generate fully inflected
forms. This two-step setup decreases the complex-
ity of the translation task by removing language-
specific features from the translation model. Fur-
thermore, generating inflected forms based on word
stems and morphological features allows to gener-
233
ate forms which do not occur in the parallel training
data ? this is not possible in a standard SMT setup.
The idea of separating the translation into two
steps to deal with complex morphology was in-
troduced by Toutanova et al (2008). Fraser et
al. (2012) applied this method to the language
pair English-German with an additional special
focus on word formation issues such as the split-
ting and merging of portmanteau prepositions and
compounds. The presented inflection prediction
systems focuses on nominal inflection; verbal in-
flection is not addressed.
Morphological analysis and resources The
morphological analysis of the French training data
is obtained using RFTagger, which is designed
for annotating fine-grained morphological tags
(Schmid and Laws, 2008). For generating inflected
forms based on stems and morphological features,
we use an extended version of the finite-state mor-
phology FRMOR (Zhou, 2007). Additionally, we
use a manually compiled list of abbreviations and
named entities (names of countries) and their re-
spective grammatical gender.
Stemming For building the SMT system, the
French data (parallel and monolingual) is trans-
formed into a stemmed representation. Nouns,
i.e. the heads of NPs or PPs, are marked with
inflection-relevant features: gender is considered
as part of the stem, whereas number is determined
by the source-side input: for example, we expect
source-language words in plural to be translated by
translated by stems with plural markup. This stem-
markup is necessary in order to guarantee that the
number information is not lost during translation.
For a better generalization, portmanteaus are split
into separate parts: au? a`+le (meaning, ?to the?).
Predicting morphological features For predict-
ing the morphological features of the SMT output
(number and gender), we use a linear chain CRF
(Lavergne et al, 2010) trained on data annotated
with these features using n-grams of stems and part-
of-speech tags within a window of 4 positions to
each side of the current word. Through the CRF,
the values specified in the stem-markup (number
and gender on nouns) are propagated over the rest
of the linguistic phrase, as shown in column 2 of
table 4. Based on the stems and the morphological
features, inflected forms can be generated using
FRMOR (column 3).
Post-processing As the French data has been
normalized, a post-processing step is needed in or-
der to generate correct French surface forms: split
portmanteaus are merged into their regular forms
based on a simple rule set. Furthermore, apostro-
phes are reintroduced for words like le, la, ne, ... if
they are followed by a vowel. Column 4 in table 4
shows post-processing including portmanteau for-
mation. Since we work on lowercased data, an
additional recasing step is required.
Experiments and evaluation We use the same
set of reduced parallel data as the FR-EN system;
the language model is built on 32M French sen-
tences. Results for the WMT-2012 test set are given
in table 5. Variant 1 shows the results for a small
system trained only on a part of the training data
(Europarl+News Commentary), whereas variant 2
corresponds to the submitted system. A small-scale
analysis indicated that the inflection prediction sys-
tem tends to have problems with subject-verb agree-
ment. We trained a factored system using addi-
tional PoS-tags with number information which
lead to a small improvement on both variants.
While the small model is significantly better than
the baseline2 as it benefits more from the general-
ization, the result for the full system is worse than
the baseline3. Here, given the large amount of
data, the generalization effect has less influence.
However, we assume that the more general model
from the inflection prediction system produces bet-
ter translations than a regular model containing a
large amount of irrelevant inflectional information,
particularly when considering that it can produce
well-formed inflected sequences that are inaccessi-
ble to the baseline. Even though this is not reflected
in terms of BLEU, humans might prefer the inflec-
tion prediction system.
For the WMT-2013 set, we obtain BLEU scores
of 29.6 (ci) and 28.30 (cs) with the inflection pre-
diction system mes-inflection (marked in table 5).
5 Russian-English
The preparation of the Russian data includes the
following stages: (1) tokenization and tagging and
(2) morphological reduction.
Tagging and tagging errors For tagging, we use
a version of RFTagger (Schmid and Laws, 2008)
2Pairwise bootstrap resampling with 1000 samples.
3However, the large inflection-prediction system has a
slightly better NIST score than the baseline (7.63 vs. 7.61).
234
SMT-output predicted generated after post- gloss
with stem-markup in bold print features forms processing
avertissement<Masc><Pl>[N] Masc.Pl avertissements avertissements warnings
sinistre[ADJ] Masc.Pl sinistres sinistres dire
de[P] ? de du from
le[ART] Masc.Sg le the
pentagone<Masc><Sg>[N] Masc.Sg pentagone pentagone pentagon
sur[P] ? sur sur over
de[P] ? de d? of
e?ventuel[ADJ] Fem.Pl e?ventuelles e?ventuelles potential
re?duction<Fem><Pl>[N] Fem.Pl re?ductions re?ductions reductions
de[P] ? de du of
le[ART] Masc.Sg le the
budget<Masc><Sg>[N] Masc.Sg budget budget budget
de[P] ? de de of
le[ART] Fem.Sg la la the
de?fense<Fem><Sg>[N] Fem.Sg de?fense de?fense de?fense
Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.
that has been developed based on data tagged with
TreeTagger (Schmid, 1994) using a model from
Sharoff et al (2008). The data processed by Tree-
Tagger contained errors such as wrong definition
of PoS for adverbs, wrong selection of gender for
adjectives in plural and missing features for pro-
nouns and adverbs. In order to train RFTagger, the
output of TreeTagger was corrected with a set of
empirical rules. In particular, the morphological
features of nominal phrases were made consistent
to train RFTagger: in contrast to TreeTagger, where
morphological features are regarded as part of the
PoS-tag, RFTagger allows for a separate handling
of morphological features and POS tags.
Despite a generally good tagging quality, some
errors seem to be unavoidable due to the ambiguity
of certain grammatical forms in Russian. A good
example of this are neuter nouns that have the same
form in all cases, or feminine nouns, which have
identical forms in singular genitive and plural nom-
inative (Sharoff et al, 2008). Since Russian has no
binding word order, and the case of nouns cannot
be determined on that basis, such errors cannot be
corrected with empirical rules implemented as post-
System BLEU (ci) BLEU (cs)
1 Baseline 24.91 23.40
InflPred 25.31 23.81
InflPred-factored 25.53 24.04
2 Baseline 29.32 27.65
InflPred* 29.07 27.40
InflPred-factored 29.17 27.46
Table 5: Results for French inflection prediction
on the WMT-2012 test set. The marked system (*)
corresponds to the system submitted for manual
evaluation.
processing. Similar errors occur when specifying
the case of adjectives, since the suffixes of adjec-
tives are even less varied as compared to the nouns.
In our application, we hope that this type of error
does not affect the result due to the following sup-
pression of a number of morphological attributes
including the case of adjectives.
Morphological reduction In comparison to
Slavic languages, English is morphologically poor.
For example, English has no morphological at-
tributes for nouns and adjectives to express gender
or case; verbs have no gender either. In contrast,
Russian is morphologically very rich ? there are
e.g. 6 cases and 3 grammatical genders, which
manifest themselves in different suffixes for nouns,
pronouns, adjectives and some verb forms. When
translating from Russian into English, many of
these attributes are (hopefully) redundant and are
therefore deleted from the training data. The mor-
phological reduction in our system was applied to
nouns, pronouns, verbs, adjectives, prepositions
and conjunctions. The rest of the POS (adverbs,
particles, interjections and abbreviations) have no
morphological attributes. The list of the original
and the reduced attributes is given in Table 6.
Transliteration mining to handle OOVs The
machine translation system fails to translate out-of-
vocabulary words (OOVs) as they are unknown to
the training data. Most of the OOVs are named en-
tities and transliterating them to the target language
script could solve this problem. The transliteration
system requires a list of transliteration pairs for
training. As we do not have such a list, we use
the unsupervised transliteration mining system of
Sajjad et al (2012) that takes a list of word pairs for
235
Part of Attributes Reduced
Speech RFTagger attributes
Noun Type Type
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep gen,notgen
Animate
Case 2
Pronoun Person Person
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep nom,notnom
Syntactic type
Animated
Verb Type Type
VForm VForm
Tense Tense
Person Person
Number Number
Gender
Voice Voice
Definiteness
Aspect Aspect
Case
Adjec- Type Type
tive Degree Degree
Gender
Number
Case
Definiteness
Prep- Type
osition Formation
Case
Conjunc- Type Type
tion Formation Formation
Table 6: Rules for simplifying the morphological
complexity for RU.
training and extracts transliteration pairs that can
be used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows: We
word-align the parallel corpus using GIZA++ and
symmetrize the alignments using the grow-diag-
final-and heuristic. We extract all word pairs which
occur as 1-to-1 alignments (Sajjad et al, 2011) and
later refer to them as a list of word pairs. We train
the unsupervised transliteration mining system on
the list of word pairs and extract transliteration
pairs. We use these mined pairs to build a transliter-
ation system using the Moses toolkit. The translit-
eration system is applied as a post-processing step
to transliterate OOVs.
The morphological reduction of Russian (cf. sec-
tion 5) does not process most of the OOVs as they
are also unknown to the POS tagger. So OOVs that
we get are in their original form. When translit-
Original corpus
SYS WMT-2012 WMT-2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS WMT-2012 WMT-2013
GIZA++ 31.22 24.3
TA-GIZA++ 31.40 24.45
Table 7: Russian to English machine translation
system evaluated on WMT-2012 and WMT-2013.
Human evaluation in WMT13 is performed on the
system trained using the original corpus with TA-
GIZA++ for alignment (marked with *).
erating them, the inflected forms generate wrong
English transliterations as inflectional suffixes get
transliterated too, specially OOV named entities.
We solved this problem by stemming the OOVs
based on a list of suffixes ( , , , , , ) and
transliterating the stemmed forms.
Experiments and results We trained the sys-
tems separately on GIZA++ and transliteration
augmented-GIZA++ (TA-GIZA++) to compare
their results; for more details see Sajjad et al
(2013). All systems are tuned using PROv1 (Nakov
et al, 2012). The translation output is post-
processed to transliterate OOVs.
Table 7 summarizes the results of RU-EN trans-
lation systems trained on the original corpus and
on the morph-reduced corpus. Using TA-GIZA++
alignment gives the best results for both WMT-
2012 and WMT-2013, leading to an improvement
of 0.4 BLEU points.
The system built on the morph-reduced data
leads to decreased BLEU results. However, the per-
centage of OOVs is reduced for both test sets when
using the morph-reduced data set compared to the
original data. An analysis of the output showed
that the morph-reduced system makes mistakes in
choosing the right tense of the verb, which might
be one reason for this outcome. In the future, we
would like to investigate this issue in detail.
6 German to English and English to
German
We submitted systems for DE-EN and EN-DE
which used constituent parses for pre-reordering.
For DE-EN we also deal with word formation is-
sues such as compound splitting. We did not per-
form inflectional normalization or generation for
German due to time constraints, instead focusing
236
our efforts on these issues for French and Russian
as previously described.
German to English German has a wider diver-
sity of clausal orderings than English, all of which
need to be mapped to the English SVO order. This
is a difficult problem to solve during inference, as
shown for hierarchical SMT by Fabienne Braune
and Fraser (2012) and for phrase-based SMT by
Bisazza and Federico (2012).
We syntactically parsed all of the source side
sentences of the parallel German to English data
available, and the tuning, test and blindtest sets.
We then applied reordering rules to these parses.
We use the rules for reordering German constituent
parses of Collins et al (2005) together with the
additional rules described by Fraser (2009). These
are applied as a preprocess to all German data.
For parsing the German sentences, we used the
generative phrase-structure parser BitPar with opti-
mizations of the grammar, as described by Fraser
et al (2013). The parser was trained on the Tiger
Treebank (Brants et al, 2002) along with utilizing
the Europarl corpus as unlabeled data. At the train-
ing of Bitpar, we followed the targeted self-training
approach (Katz-Brown et al, 2011) as follows. We
parsed the whole Europarl corpus using a grammar
trained on the Tiger corpus and extracted the 100-
best parse trees for each sentence. We selected the
parse tree among the 100 candidates which got the
highest usefulness scores for the reordering task.
Then we trained a new grammar on the concatena-
tion of the Tiger corpus and the automatic parses
from Europarl.
The usefulness score estimates the value of a
parse tree for the reordering task. We calculated
this score as the similarity between the word order
achieved by applying the parse tree-based reorder-
ing rules of Fraser (2009) and the word order indi-
cated by the automatic word alignment between
the German and English sentences in Europarl.
We used the Kendall?s Tau Distance as the simi-
larity metric of two word orderings (as suggested
by Birch and Osborne (2010)).
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics. We also split German
portmanteaus like zum? zu dem (meaning to the).
system BLEU BLEU system name
(ci) (cs)
DE-EN (OSM) 27.60 26.12 MES
DE-EN (OSM) 27.48 25.99 not submitted
BitPar not self-trained
DE-EN (Moses) 27.14 25.65 MES-Szeged-
reorder-split
DE-EN (Moses) 26.82 25.36 not submitted
BitPar not self-trained
EN-DE (Moses) 19.68 18.97 MES-reorder
Table 8: Results on WMT-2013 (blindtest)
English to German The task of mapping En-
glish SVO order to the different clausal orders in
German is difficult. For our English to German
systems, we solved this by parsing the English and
applying the system of Gojun and Fraser (2012) to
reorder English into the correct German clausal or-
der (depending on the clause type which is detected
using the English parse, see (Gojun and Fraser,
2012) for further details).
We primarily used the Charniak-Johnson gener-
ative parser (Charniak and Johnson, 2005) to parse
the English Europarl data and the test data. How-
ever, due to time constraints we additionally used
Berkeley parses of about 400K Europarl sentences
and the other English parallel training data. We
also left a small amount of the English parallel
training data unparsed, which means that it was
not reordered. For tune, test and blindtest (WMT-
2013), we used the Charniak-Johnson generative
parser.
Experiments and results We used all available
training data for constrained systems; results for
the WMT-2013 set are given in table 8. For the
contrastive BitPar results, we reparsed WMT-2013.
7 Conclusion
We presented 5 systems dealing with complex mor-
phology. For two language pairs with a morpho-
logically rich source language (FR and RU), the
input was reduced to a simplified representation
containing only translation-relevant morphologi-
cal information (e.g. number on nouns). We also
used reordering techniques for DE-EN and EN-DE.
For translating into a language with rich morphol-
ogy (EN-FR), we applied a two-step method that
first translates into a stemmed representation of
the target language and then generates inflected
forms based on morphological features predicted
on monolingual data.
237
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions, Daniel
Quernheim for providing Berkeley parses of some
of the English data, Stefan Ru?d for help with the
manual evalution, and Philipp Koehn and Barry
Haddow for providing data and alignments.
Nadir Durrani was funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n. 287658. Alexan-
der Fraser was funded by Deutsche Forschungs-
gemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation and from the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Marion Weller was funded from the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Svetlana Smekalova was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Trans-
lation. Helmut Schmid and Max Kisselew were
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Richa?rd Farkas was supported by
the European Union and the European Social Fund
through project FuturICT.hu (grant n. TA?MOP-
4.2.2.C-11/1/KONV-2012-0013). This publication
only reflects the authors? views.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Build-
ing a treebank for french. In A. Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of ACL WMT and MetricsMATR, Upp-
sala, Sweden.
Arianna Bisazza and Marcello Federico. 2012. Mod-
ified distortion matrices for phrase-based statistical
machine translation. In ACL, pages 478?487.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL, pages 173?180, Ann Arbor, MI,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Porceedings of ACL 2005.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL-HLT
2011, Portland, Oregon, USA.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In Proceedings of NAACL
2013, Atlanta, Georgia, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013b. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Anita Gojun Fabienne Braune and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
EAMT 2012.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of EACL 2012,
Avignon, France.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In EACL WMT.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to avoid burning ducks: Combining linguistic analy-
sis and corpus statistics for German compound pro-
cessing. In ACL WMT and Metrics MATR.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL 2010, pages 504?513.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. Mumbai, India.
238
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of ACL 2011, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
ACL 2012, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008, Stroudsburg, PA, USA.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: a German Computational Morphology Cov-
ering Derivation, Composition, and Inflection. In
Proceedings of LREC 2004.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In Proceedings of IJCNLP 2008.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating russian tagsets. In Proceedings of LREC
2008.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-HLT
2008.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diploma Thesis, Insti-
tute for Natural Language Processing, University of
Stuttgart.
239
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71?78,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
CimS ? The CIS and IMS joint submission to WMT 2014
translating from English into German
Fabienne Cap_, Marion Weller_f, Anita Rammf, Alexander Fraser_
_ CIS, Ludwig-Maximilian University of Munich ? (cap|fraser)@cis.uni-muenchen.de
f IMS, University of Stuttgart ? (wellermn|ramm)@ims.uni-stuttgart.de
Abstract
We present the CimS submissions to the
2014 Shared Task for the language pair
EN?DE. We address the major problems
that arise when translating into German:
complex nominal and verbal morphol-
ogy, productive compounding and flex-
ible word ordering. Our morphology-
aware translation systems handle word
formation issues on different levels of
morpho-syntactic modeling.
1 Introduction
In our shared task submissions, we focus on the
English to German translation direction: we ad-
dress different levels of productivity of the Ger-
man language, i.e., nominal and verbal inflec-
tion and productive word formation, which lead
to data sparsity and thus confuse classical SMT
systems.
Our basic goal is to make the two languages
as morphosyntactically similar as possible. We
use a parser and a morphological analyser to re-
move linguistic features from German that are
not present in English and reorder the English
input to make it more similar to the German sen-
tence structure. Prior to training, all words are
lemmatised and compounds are split into single
words. This is not only beneficial for word align-
ment, but it also allows us to generalise over in-
flectional variants of the same lexemes and over
single words which could occur in one place as a
standalone word and in another place as part of
a compound. Translation happens in two steps:
first, we translate from English into split, lemma-
tised German and then, we perform compound
merging and generation of inflection as a post-
processing step. This way, we are able to cre-
ate German compounds and inflectional vari-
ants that have not been seen in the parallel train-
ing data.
In this paper, we investigate the performance of
well-established source-side reordering, nomi-
nal re-inflection and compound processing sys-
tems on an up-to-date shared task. In addition,
we present experimental results on a verbal in-
flection component and a syntax-based variant
including source-side reordering.
2 Related Work
Re-Inflection The two-step translation ap-
proach we use was described by e.g. Toutanova
et al. (2008) and Jeong et al. (2010), who use
a number of morphological and syntactic
features derived from both source and target
language. More recently, Fraser et al. (2012)
describe a similar approach for German using
different CRF-based feature prediction models,
one for each of the four grammatical features
to be predicted for German words in noun
phrases, namely number, gender, case and
definiteness. This approach also handles word-
formation issues such as portmanteau splitting
and compounding. Weller et al. (2013) added
subcategorization information in combination
with source-side syntactic features in order to
improve the prediction of case.
De Gispert and Mari?o (2008) generate verbal
inflection for translation from English into Span-
ish. They use classifiers trained not only on tar-
get language but also on source language fea-
tures, which is even more crucial for the predic-
tion of verbs than it is for nominal inflection.
More recently, Williams and Koehn (2011)
translate directly into target language surface
forms. Agreement within NPs and PPs, and also
between subject and verb is considered during
the decoding process: they use string-to-tree
translation, where the target language (German)
morphology is expressed as a set of unification
constraints automatically learned from a mor-
phologically annotated German corpus.
71
Compound Processing Compound splitting
for SMT has been addressed by numerous dif-
ferent groups, for translation from German
to English, e.g. using corpus-based frequen-
cies (Koehn and Knight, 2003), using POS-
constraints (Stymne et al., 2008), a lattice-based
approach propagating the splitting decision to
the decoder (Dyer, 2009), a rule-based morpho-
logical analyser (Fritzinger and Fraser, 2010) or
unsupervised, language-independent segmen-
tation (Macherey et al., 2011).
Compound processing in the other translation
direction, however, has been much less investi-
gated. Popovic? et al. (2006) describe a list-based
approach, in which words are only re-combined
if they have been seen as compounds in a huge
corpus. However this approach is limited to
the list?s coverage. The approach of Stymne
(2009) overcomes this coverage issue by mak-
ing use of a POS-markup which distinguishes
former compound modifiers from former heads
and thus allows for their adequate recombina-
tion after translation. An extension of this ap-
proach is reported in Stymne and Cancedda
(2011) where a CRF-model is used for compound
prediction. In Cap et al. (2014) their approach
is extended through using source-language fea-
tures and lemmatisation, allowing for maximal
generalisation over compound parts.
Source-side Reordering One major problem in
English to German translation is the divergent
clausal ordering: in particular, German verbs
tend to occur at the very end of clauses, whereas
English sticks to a rigid SVO order in most cases.
Collins et al. (2005), Fraser (2009) and Gojun
and Fraser (2012) showed that restructuring the
source language so that it corresponds to the ex-
pected structure of the target language is helpful
for SMT.
3 Inflection Prediction
German has a rich morphology, both for nom-
inal and verbal inflection. It requires differ-
ent forms of agreement, e.g., for adjectives and
nouns or verbs and their subjects. Traditional
phrase-based SMT systems often get such agree-
ments wrong. In our systems, we explicitly
model agreement using a two-step approach:
first we translate from English into lemmatised
German and then generate fully inflected forms
in a second step. In this section, we describe our
nominal inflection component and first experi-
mental steps towards verbal re-inflection.
3.1 Noun Phrase Inflection
Prior to training, the German data is re-
duced to a lemmatised representation contain-
ing translation-relevant morphological features.
For nominal inflection, the lemmas are marked
with number and gender: gender is considered
as part of the lemma, whereas number is indi-
rectly determined by the source-side, as we ex-
pect nouns to be translated with their appro-
priate number value. We use a linear chain
CRF (Lafferty et al., 2001) to predict the mor-
phological features (number, gender, case and
strong/weak). The features that are part of the
lemma of nouns (number, gender) are propa-
gated over the rest of the linguistic phrase. In
contrast, case depends on the role of the NP in
the sentence (e.g. subject or direct/indirect ob-
ject) and is thus to be determined entirely from
the respective context in the sentence. The value
for strong/weak depends on the combination of
the other features. Based on the lemma and the
predicted features, inflected forms are then gen-
erated using the rule-based morphological anal-
yser SMOR (Schmid et al., 2004). This system is
described in more detail in Fraser et al. (2012).
3.2 Verbal Inflection
German verbs agree in number and person with
their subjects. We thus have to derive this in-
formation from a noun phrase in nominative
case (= the subject) near the verb. This informa-
tion comes from the nominal inflection predic-
tion described in section 3.1. We predict tense
and mode of the verb using a maximum-entropy
classifier which is trained on English and Ger-
man contextual information. After deriving all
information needed for the generation of the
verbs, the inflected forms are generated with
SMOR.
4 Compound Processing
In English to German translation, compound
processing is more difficult than in the oppo-
site direction. Not only do compounds have to
be split accurately, but they also have to be put
together correctly after decoding. The disflu-
ency of MT output and the difficulty of deciding
which single words should be merged into com-
pounds make this task even more challenging.
72
(split+lem.)
Training
Parallel Training Data
LanguageModel
TranslationModel
English text
....
....
....
Target Training Data
....
....
....
....
....
....
German text
German text
....
....
....
tooltradefruit box Werkzeug KisteHandelObst
Werkzeug KisteObst Handel
Parallel Training Data
....
....
....
....
....
....
ObsthandelWerkzeugkiste
Target Training Data
German text
Pre?Processing
Obsthandel....
....
....
Werkzeugkiste
....
....
....
tool boxfruit trade
English text German text
Post?processing 
....
....ObstkisteObsthandel
German(fluent)
Testing
inputEnglish
....fruit tradefruit box
(split+lem.)
(split+lem.)
lemmatisesplit
splitlemmatise ....
....
RecombineRe?inflect
German
(split+lem.)
....
output
Obst KisteObst Handel
Decoder
Figure 1: Pipeline overview of our primary CimS-CoRI system.
We combine compound processing with in-
flection prediction (see Section 3) and thus ex-
tend the two-step approach respectively: com-
pounds are split and lemmatised simultane-
ously, again using SMOR. This allows for maxi-
mal generalisation over former compound parts
and independently occurring simple words. We
use this split representation for training. Af-
ter decoding, we re-combine words into com-
pounds again, using our extended CRF-based
approach, which is based on Stymne and Can-
cedda (2011), but includes source-language fea-
tures and allows for maximal generalisation
through lemmatisation. More details can be
found in Cap et al. (2014). We then use SMOR
to generate sound German compounds (includ-
ing morphological transformations such as in-
troduction or deletion of filler letters). Finally,
the whole text including the newly-created com-
pounds, is re-inflected using the nominal in-
flection prediction models as described in Sec-
tion 3.1 above. This procedure allows us to create
compounds that have not been seen in the par-
allel training data, and also inflectional variants
of seen compounds. See Figure 1 for an overview
of our compound processing pipeline.
4.1 Portmanteaus
Portmanteaus are a special kind of compound.
They are a fusion of a preposition and a defi-
nite article (thus not productive) and their case
must agree with the case of the noun. For ex-
ample, ?zum? can be split into ?zu? + ?dem? =
to+theDati ve . They introduce additional spar-
sity to the training data: imagine a noun oc-
curred with its definite article in the training
data, but not with the portemanteau required at
testing time. Splitting portemanteaus allows a
phrase-based SMT system to access phrases cov-
ering nouns and their corresponding definite ar-
ticles. In a post-processing step, definite articles
are then re-merged with their preceding prepo-
sitions to restore the original portmanteau, see
(Fraser et al., 2012) for details. This generalisa-
tion effect is even larger as we not only split port-
manteaus, but also lemmatise the articles.
5 System descriptions
Our shared task submissions include different
combinations of the inflection and compound
processing procedures as described in the pre-
vious two sections. We give an overview of all
our systems in Table 1. Note that we did not
re-train the compound processing CRFs on the
new dataset, but used our models trained on the
2009 training data instead. However, this does
not hurt performance, as the CRF we use is not
trained on surface forms, but only frequencies
and source-side features instead. See (Fraser et
al., 2012) and (Cap et al., 2014) for more details
on how we trained the respective CRFs. In con-
trast, the verbal classifier has been trained on
WMT 2014 data.
6 Experimental Settings
In all our systems, we only used data distributed
for the shared task. All available German data
was morphologically analysed with SMOR. For
lemmatisation of the German training data, we
disambiguated SMOR using POS tags we ob-
tained through parsing the German section of
the parallel training data with BitPar (Schmid,
73
No. apprart nominal compound verbal source-sidesplitting inflection processing inflection reordering
CimS-RI X X
CimS-CoRIP X X X
CimS-RIVe X X X
CimS-CoRIVe X X X X
CimS-Syntax-RORI X X X
Table 1: Overview of our submission systems.RI = nominal Re-Inflection, Co = Compound process-
ing, Ve = Verbal inflection, RO = source-side Re-Ordering. Syntax = syntax-based SMT P = primary
submission.
2004) and tagging the big monolingual training
data using RFTagger (Schmid and Laws, 2008)1.
Note that we did not normalise German lan-
guage e.g. with respect to old vs. new writing
convention etc. as we did in previous submis-
sions (e.g. (Fraser, 2009)).
For the compound prediction CRFs using syn-
tactic features derived from the source language,
we parsed the English section of the parallel
data using EGRET, a re-implementation of the
Berkeley-Parser by Hui Zhang2. Before training
our models on the English data, we normalised
all occurrences of British vs. American English
variants to British English. We did so for train-
ing, tuning and testing input.
Language Model We trained 5-gram language
models based on all available German monolin-
gual training data from the shared task (roughly
1.5 billion words) using the SRILM toolkit (Stol-
cke, 2002) with Kneser-Ney smoothing. We then
used KenLM (Heafield, 2011) for faster process-
ing. For each of our experiments, we trained
a separate language model on the whole data
set, corresponding to the different underspeci-
fied representations of German used in our ex-
periments, e.g. lemmatised for CimS-RI, lemma-
tised with split compounds for CimS-CoRI, etc.
Phrase-based Translation model We per-
formed word alignment using the multithreaded
GIZA++ toolkit (Och and Ney, 2003; Gao and
Vogel, 2008). For translation model training and
decoding, we used the Moses toolkit (Koehn
et al., 2007) to build phrase-based statistical
machine translation systems, following the
instructions for the baseline system for the
shared task, using only default settings.
1We could not parse the whole monolingual dataset due
to time-constraints and thus used RFTagger as a substitute.
2available from https://sites.google.com/
site/zhangh1982/egret.
Syntax-based Translation model As a variant
to the phrase-based systems, we applied the in-
flection prediction system to a string-to-tree sys-
tem with GHKM extraction (Galley et al. (2004),
Williams and Koehn (2012)). We used the same
data-sets as for the phrase-based systems, and
applied BitPar (Schmid, 2004) to obtain target-
side trees. For this system, we used source-side
reordering according to Gojun and Fraser (2012)
relying on parses obtained with EGRET3.
Tuning For tuning of feature weights, we used
batch-mira with ??safe-hope? (Cherry and Foster,
2012) until convergence (or maximal 25 runs).
We used the 3,000 sentences of newstest2012 for
tuning. Each experiment was tuned separately,
optimising Bleu scores (Papineni et al., 2002)
against a lemmatised version of the tuning ref-
erence. In the compound processing systems we
integrated the CRF-based prediction and merg-
ing procedure into each tuning iteration and
scored each output against the same unsplit and
lemmatised reference as the other systems.
Testing After decoding, the underspecified
representation has to be retransformed into
fluent German text, i.e., compounds need to
be re-combined and all words have to be re-
inflected. The whole procedure can be divided
into the following steps:
1a) translation into lemmatised German
representation (RI, RIVe)
1b) translation into split and lemmatised
German (CoRi, CoRIVe)
2) compound merging (CoRI, CoRIVe):
3) nominal inflection prediction and gen-
eration of full forms using SMOR (all)
4) verbal re-inflection (RIVe, CoRIVe)
5) merging of portmanteaus (all)
3Note that we observed some data-related issues on the
Syntax-RORI experiments that we hope to resolve in the
near future.
74
Experiment mert.log Bleu ci Bleu cs Bleu ci Bleu csnews2012 news2013 news2013 news2014 news2014
raw 16.52 18.62 17.61 17.80 17.25
CimS-RI 18.51 19.23 18.38 18.33 17.75
CimS-CoRIP 18.36 19.13 18.25 18.51 17.87
CimS-RIVe 19.08 18.89 18.06 17.86 17.31
CimS-CoRIVe 18.69 18.60 17.77 17.38 16.78
CimS-Syntax-RORI 18.26 19.04 18.17 18.15 17.59
Table 2: Bleu scores for all CimS-submissions of the 2014 shared task. ci = case-insensitive, cs = case-
sensitive; P = primary submission.
After these post-processing steps, the text was
automatically recapitalised and detokenised, us-
ing the tools provided by the shared task, which
we trained on the whole German dataset. We cal-
culated Bleu (Papineni et al., 2002) scores using
the NIST script version 13a.
7 Results
We evaluated our systems with the 3,000 sen-
tences of last year?s newstest2013 and also the
2,737 sentences of the 2014 blind test set for the
German-English language pair. The Bleu scores
of our systems are given in Table 2, where raw
denotes our baseline system which we ran with-
out any pre- or postprocessing whatsoever. Note
that the big gap in mert.log scores between raw
and the CimS-systems comes from the fact that
raw is scored against the original (i.e. fully in-
flected) version of the tuning reference, while the
CimS-systems are scored against the stemmed
tuning reference.
As for the Bleu scores of the test sets, we ob-
serve similar improvements for the CimS-RI and
CimS-CoRI systems of +0.5/0.6 with respect to
the raw baseline as we did in previous experi-
ments (Cap et al., 2014)4. In contrast, our sys-
tems incorporating verbal prediction inflection
(CimS-RIVe/CoRIVe) cannot yet catch up with
the performance of the well-investigated nom-
inal inflection and compound processing sys-
tems (CimS-RI/CoRI). We attribute this partly to
the positive influence we assume fully inflected
verbs to have in nominal inflection prediction
models, but as the verb processing systems are
still under development, there might be other is-
sues we have not discovered yet. We plan to re-
4We will have a closer look at the data from a compound
processing view in Section 7.1 below.
visit these systems and improve them.
Finally, the syntax-based reordering system
yields scores that are competitive to those of
CimS-RI/CoRI. While Syntax-RORI so far only in-
corporates source-side reordering and nominal
re-inflection, we plan to investigate further ex-
tensions of this approach in the future.
7.1 Additional Evaluation
We manually screened the filtered 2014 test set
and identified 3,456 German compound tokens,
whereof 862 did not occur in the parallel training
data and thereof, 244 did not even occur in the
monolingual training data. For each of our sys-
tems, we calculated the number of compound
reference matches they produced. The results
are given in Table 3.
system ref new
raw 827 0
CimS-RI . 864 5
CimS-CoRIP 1,064 109
CimS-RIVe 853 5
CimS-CoRIVe 1,070 122
CimS-Syntax-RORI 900 20
Table 3: Numbers of compounds produced by
the systems that matched the reference (ref ) and
did not occur in the parallel training data (new).
The compound processing systems (with Co
in the name) generate many more correct com-
pounds than comparable systems without com-
pound handling. Compared to the raw base-
line, CoRI/CoRIVe did not only produce 237/243
more reference matches, but also 109/122 com-
pounds that matched the reference but did not
occur in the parallel training data. A lookup of
those 109/122 compounds in the monolingual
training data (consisting of roughly 1.5 billion
words) revealed, that 8/6 of them did not oc-
75
cur there either5. These were thus not accessi-
ble to a list-based compound merging approach
either. This result also shows that despite the
fact that CoRIVe does not yield a competitive
translation quality performance yet, the com-
pound processing component seems to bene-
fit from the verbal inflection and it is definitely
worth more investigation in the future.
Moreover, it can be seen from Table 3 that
the re-inflection systems (*RI*) produce more
reference matches than the raw baseline. In-
terestingly, they even produce some reference
matches that have not been seen in the par-
allel training data due to inflectional variation,
and in the case of the syntax-based system due
to a naive list-based compound merging: even
though it has not been trained on a split repre-
sentation of German text, it might occasionally
occur that two German nouns occur next to each
other in the MT output. If so, these two words are
merged into a compound, using a list-based ap-
proach, similar to Popovic? et al. (2006).
8 Reordering
For the system CimS-Syntax-RORI, English data
parsed with EGRET was reordered using scripts
written for parse trees produced by the con-
stituent parser (Charniak and Johnson, 2005),
using a model we trained on the standard Penn
Treebank sections. Unfortunately, the reorder-
ing scripts could not be straightforwardly ap-
plied to EGRET parses and require more signifi-
cant modifications than we first expected.
We thus decided to parse the Europarl data
(v7) with (Charniak and Johnson, 2005) instead
and run our reordering scripts on it (CimS-RO).
For evaluation purposes, we build a baseline sys-
tem raw? which has been trained only on Eu-
roparl. Tuning and testing setup is the same as
for the systems described in Section 6 with the
difference that the weights have been tuned on
newstest2013. The evaluation results are shown
in Table 4. Similarly to previous results reported
in (Gojun and Fraser, 2012), the CimS-RO system
shows an improvement of 0.5 Bleu points when
compared to the raw? baseline .
5Namely: Testflugzeugen (test airplanes), Medientri-
bunal (media tribunal), RBS-Mitarbeiter (RBS worker),
Schulmauersanierung (school wall renovation), Anti-
Terror-Organisationen (anti-terror organisations), and
Tabakimpfstoffe (tobacco-plant-created vaccines) in both
and in CoRI also Hand-gep?ckgeb?hr (hand luggage fee)
and Haftungsstreitigkeiten (liability litigation).
Experiment mert.log Bleu ci Bleu csnews2013 news2014 news2014
raw? 16.87 16.25 15.31
CimS-RO 17.76 16.81 15.81
Table 4: Evaluation of the reordering system
trained on Europarl v7.
9 Summary
We presented the CimS systems, a set of
morphology-aware translation systems cus-
tomised for translation from English to German.
Each system operates on a different level of
morphological description, be it nominal inflec-
tion, verbal inflection, compound processing
or source-side reordering. Some of the systems
are well-established (RI, CoRI and RO), others
are still under developement (RIVe, CoRIVe and
Syntax-RORI). However, all of them, with the ex-
ception of CoRIVe, lead to improved translation
quality when evaluated against a contrastive
baseline without linguistic processing. In an
additional evaluation, we could show that the
compound processing systems are able to create
a considerable number of compounds unseen
in the parallel training data.
In the future, we will investigate further com-
binations and extensions of our morphological
components, including reordering, compound
processing and verbal inflection. There are still
many many interesting challenges to be solved
in all of these areas, and this is especially true for
verbal inflection.
Acknowledgments
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We would like to thank
Daniel Quernheim for sharing the workload of
preprocessing the data with us.
Moreover, we thank Edgar Hoch from the IMS
system administration for generously providing
us with disk space and all our colleagues at IMS,
especially Fabienne Braune, Junfei Guo, Nina
Seemann and Jason Utt for postponing their ex-
periments to let us use most of IMS? computing
facilities for a whole week. Thank you each beau-
coup!
76
References
Fabienne Cap, Alexander Fraser, Marion Weller, and
Aoife Cahill. 2014. How to Produce Unseen
Teddy Bears: Improved Morphological Processing
of Compounds in SMT. In Proceedings of EACL
2014.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), Ann Arbor, Michigan.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation.
In Proceedings of HLT-NAACL 2012.
Michael Collins, Philipp Koehn, and Ivona Kuc?erov?.
2005. Clause restructuring for statistical machine
translation. In Proceedings ACL 2005.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of HLT-NAACL 2009.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In Proceedings of EACL 2012.
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translation to and from Ger-
man. In Proceedings of WMT 2009.
Fabienne Fritzinger and Alexander Fraser. 2010.
How to Avoid Burning Ducks: Combining Lin-
guistic Analysis and Corpus Statistics for Ger-
man Compound Processing. In Proceedings of
WMT@ACL2010.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In Proceedings of HLT-NAACL 2004.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In ACL 2008:
Proceedings of the Workshop on Software Engineer-
ing, Testing, and Quality Assurance for Natural
Language Processing.
Adri? De Gispert and Jos? B. Mari?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of WMT
2011.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In Proceedings of
AMTA 2010.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of ACL 2007
(Demo Session).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML?01.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpho-
logical Operations. In Proceedings of ACL 2011.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL 2002.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Proceedings of FinTAL 2006.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In Proceedings of LREC 2004.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of Coling 2004.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modelling Toolkit. In Proceedings of ICSLN
2002.
Sara Stymne and Nicola Cancedda. 2011. Pro-
ductive Generation of Compound Words in Sta-
tistical Machine Translation. In Proceedings of
WMT@EMNLP?11.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In Proceedings
of WMT 2008.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
Proceedings of EACL 2009 (Student Workshop).
77
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying Morphology Generation
Models to Machine Translation. In Proceedings of
HLT-ACL 2008.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In Proceedings of ACL?13.
Philip Williams and Philipp Koehn. 2011. Agreement
constraints for statistical machine translation into
German. In Proceedings of WMT 2011.
Philip Williams and Phillipp Koehn. 2012. GHKM-
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of WMT 2007.
78
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 81?90,
Dublin, Ireland, August 24 2014.
Distinguishing Degrees of Compositionality in Compound Splitting
for Statistical Machine Translation
Marion Weller
1,2
, Fabienne Cap
2
, Stefan M?uller
1
Sabine Schulte im Walde
1
, Alexander Fraser
2
1
IMS, University of Stuttgart
{wellermn;muellesn;schulte}@ims.uni-stuttgart.de
2
CIS, Ludwig-Maximilian University of Munich
{cap;fraser}@cis.uni-muenchen.de
Abstract
The paper presents an approach to morphological compound splitting that takes the degree of
compositionality into account. We apply our approach to German noun compounds and particle
verbs within a German?English SMT system, and study the effect of only splitting compositional
compounds as opposed to an aggressive splitting. A qualitative study explores the translational
behaviour of non-compositional compounds.
1 Introduction
In German, as in many other languages, two (or more) simplex words can be combined to form a com-
pound. This is a productive process, leading to a potentially infinite number of sound German com-
pounds. As a consequence, many NLP applications suffer from coverage issues for compounds which
do not appear or appear only infrequently in language resources. However, while many compounds are
not covered, their component words are often found in lexical resources or training data. Compound
processing allows access to these component words and thus can overcome these sparsity issues.
We use Statistical Machine Translation (SMT) as an example application for compound processing.
Our SMT system translates from German to English, where compounds are usually split in the German
source language prior to training and decoding. The benefits are obvious: vocabulary size is reduced
and the languages are adjusted in terms of granularity, as exemplified by the compound Holzzaun. This
fencewoodenHolzzaun HolzZaun woodenfence
1:1 alignment1:n alignment
results in better alignment quality and model estimation.
Compound splitting also enables the translation of com-
pounds not occurring in the parallel data, if the parts have
been seen and can thus be translated individually. However, these assumptions only hold for compo-
sitional compounds like Holzzaun (?wooden fence?), whose meanings can be derived from the mean-
ings of their constituents, namely Holz (?wood?) and Zaun (?fence?). In contrast, the splitting of non-
compositional compounds may lead to translation errors: e.g. the meaning of J?agerzaun (?lattice fence?)
cannot be represented by the meanings of its constituents J?ager (?hunter?) and Zaun (?fence?). Here, an
erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J?ager
? lattice, in the absence of other evidence about how to translate J?ager. When splitting compounds
for SMT, two important factors should thus be considered: (1) whether a compound is compositional
and should be split, and if so (2) how the compound should be split. Most previous approaches mainly
focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight,
2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT
quality for compound splitting. The decision about whether the compound is compositional and should
be split at all has not received much attention in the past.
In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting
all compounds. To this end, we combine (A) an approach relying on the distributional similarity be-
tween compounds and their constituents, to predict the degree of compositionality and thus to determine
whether to split the compound with (B) a combination of morphological and frequency-based features
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
81
to determine how to split a compound. We experiment with this novel semantically-informed compound
splitting on the source-side data of a German-English SMT system. As far as we know, we are the first
to study the impact of compositionality-aware compound splitting in SMT. We evaluate our systems on
a standard and on a specifically created test set, both for noun compounds and particle verbs. Our re-
sults show that phrase-based SMT is generally robust with regard to over-splitting non-compositional
compounds, with the exception of low-frequency words. This is in line with corresponding assumptions
from previous work. Furthermore, we present a small-scale study about the translational behaviour of
non-compositional compounds, which can surprisingly often be translated component-wise.
2 Related Work
We combine morphology-based compound splitting with distributional semantics to improve phrase-
based SMT. Here, we discuss relevant work of compound splitting in SMT and distributional semantics.
2.1 Compound Splitting in SMT
Compound splitting in SMT is a well-studied task. There is a wide range of previous work, including
purely string- and frequency-based approaches, but also linguistically-informed approaches. All lines
of research improved translation performance due to compound splitting. In Koehn and Knight (2003),
compounds are split through the identification of substrings from a corpus. The splitting is performed
without linguistic knowledge (except for the insertion of the filler letters ?(e)s?), which necessarily leads
to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies
of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological
transformations and imposes POS constraints on the subwords. Nie?en and Ney (2000) and Fritzinger
and Fraser (2010) perform compound splitting by relying on morphological analysers to identify suitable
split points. This has the advantage of returning only linguistically motivated splitting options, but the
analyses are often ambiguous and require disambiguation: Nie?en and Ney (2000) use a parser for
context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best
split for each compound. Other approaches use a two-step word alignment process: first, word alignment
is performed on a split representation of the compounding language. Then, all former compound parts
for which there is no aligned counterpart in the non-compounding language are merged back to the
compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003)
for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This
blocks non-compositional compounds from being split if they are translated as one simplex English word
in the training data (e.g. Heckensch?utze, lit. ?hedge|shooter?; ?sniper?) and aligned correctly. However,
cases like J?agerzaun, ?lattice fence? are not covered.
In the present work, we identify compounds with a morphological analyser, disambiguated with corpus
frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics.
We are not aware of any previous work that takes semantics into account for compound splitting in SMT.
2.2 Distributional Semantics and Compounding
Distributional information has been a steadily increasing, integral part of lexical semantic research over
the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ?you shall know
a word by the company it keeps?, distributional semantics exploits the co-occurrence of words in corpora
to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest.
Among many other tasks, distributional semantic information has been utilised to determine the degree
of compositionality (or: semantic transparency) of various types of compounds, most notably regarding
noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013),
Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson
(2006), K?uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)).
Typically, these approaches rely on co-occurrence information from a corpus (either referring to bags-
of-words, or focusing on target-specific types of features), and compare the distributional features of
the compounds with those of the constituents, in order to predict the degree of compositionality of the
82
OutputInput
Preprocessing 
Step 1:   Identify Component Words Similarity ScoresStep 2:  Predict sitional CompoundsStep 3: Split Compo?
0.311
0.825
0.015
0.725
Holzzaun woodenfence
Holz
Zaun Zaun
Holz
J?gerzaun lattice fenceJ?gerZaun J?gerzaun
SMT
Figure 1: Semantically-informed compound processing in SMT.
compound. The underlying assumption is that a compound which is similar in meaning to a constituent
(as in Holzzaun?Zaun (?wooden fence???fence?) but not in L?owenzahn?Zahn (?lion|tooth (dandelion)??
?tooth?)) is also similar to the constituent with regard to co-occurrence information.
Most related to this work on noun compounds, Reddy et al. (2011) relied on window-based distribu-
tional models to predict the compositionality of English noun compounds, and Schulte im Walde et al.
(2013) compared window-based against syntax-based distributional models to predict the composition-
ality of German noun compounds. Zinsmeister and Heid (2004) used subcategorising verbs to predict
compound?head similarities of German noun compounds. Most recently, Salehi et al. (2014) extended
the previous approaches to take multi-lingual co-occurrence information into account, regarding English
and German noun compounds, and English particle verbs.
3 Methodology
We integrate our semantically-informed compound splitting as a pre-processing step to the German
source language of an SMT system. See Figure 1 for an illustration of our compound processing pipeline.
3.1 Target Compounds
German compounds are combinations of two (or more) simplex words. In some cases, a morphological
transformation is required: for example, when combining the two nouns Ausflug (?excursion?) and Ziel
(?destination?) ? Ausflugsziel (?excursion destination?), a filler letter (here: ?s?) needs to be inserted.
Other such transformations include more filler letters or the deletion/substitution of letters.
Noun compounds are formed of a head noun and a modifier, which can consist of nouns, verbs, adjec-
tives or proper nouns.
Particle verbs are productive compositions of a base verb and a prefix particle, whose part-of-speech
varies between open-class nouns, adjectives, and verbs, and closed-class prepositions and adverbs. In
comparison to noun compounds, the constituents of German particle verbs exhibit a much higher degree
of ambiguity: Verbs in general are more ambiguous than nouns, and the largest sub-class of particles
(those with a preposition particle) is highly ambiguous by itself (e.g. Lechler and Ro?deutscher (2009)
and Springorum (2011)). For example, in anknabbern (?to nibble partially?), the particle an expresses a
partitive meaning , whereas in ankleben (?to glue onto sth.?) an has a topological meaning (to glue sth.
onto an implicit background). In addition, particle verb senses may be transparent or opaque with respect
to their base verbs. For example, abholen ?fetch? is rather transparent with respect to its base verb holen
?fetch?, whereas anfangen ?begin? is more opaque with respect to fangen ?catch?. In contrast, einsetzen
has both transparent (e.g. ?insert?) and opaque (e.g. ?begin?) verb senses with respect to setzen ?put/sit
(down)?. The high degree of ambiguity makes particle verbs a challenge for NLP. Moreover, particle
and base verb can occur separately (er f?angt an: ?he begins?) or in one word (dass er anf?angt: ?that he
begins?), depending on the clausal type. This makes consistent treatment of particle verbs difficult.
3.2 Identification of Component Parts
We use the rule-based morphological analyser SMOR (Schmid et al., 2004) to identify compounds and
their constituents in our parallel training data (cf. Section 4). It relies on a large lexicon of word lemmas
and feature rules for productive morphological processes in German, i.e., compounding, derivation and
83
inflection. In this paper, we will not consider splitting into derivational affixes (as needed for, e.g., Arabic
and Turkish), but instead identify simplex words that may also occur independently. Moreover, we only
keep noun compounds and particle verbs consisting of two constituents. The resulting set consists of
93,299 noun compound types and 3,689 particle verb types.
3.3 Predicting Compositionality based on Distributional Similarity
Starting from this set of compounds as derived from our parallel training data, we collected distributional
co-occurrence information from two large German web corpora and the machine translation training data:
(i) the German COW corpus (Sch?afer and Bildhauer (2012), ?9 billion words), (ii) the SdeWaC (Faa?
and Eckart (2013), ?880 million words), (iii) our MT parallel corpus (?40 million words) and (iv) MT
language model training data (?146 million words). We relied on earlier work and used the 20,000 most
frequent nouns from the SdeWaC as co-occurrence features, looking into a window of 20 words to the left
and to the right of our target compounds and their constituents. We thus obtained a co-occurrence matrix
of all compounds and their constituents with the 20,000 selected nouns. As co-occurrence strength (i.e.,
how strong is a co-occurrence between a target word and a co-occurring noun), we collected frequencies
and transformed them into local mutual information (LMI) values, cf. Evert (2005). Finally, we calcu-
lated the distributional similarity between the compounds and their constituents, relying on the standard
measure cosine. The cosine value is then used to predict the degree of compositionality between the
respective compound?constituent pairs. For example, the cosine value of the pair Baumschule?Baum
1
is
0.38, while the cosine value of the pair Baumschule?Schule is only 0.01.
3.4 Semantically-Informed Compound Splitting
In the two preceding sections, we described how we identified component words and calculated distribu-
tional compositionality scores for all of the compounds found in our training data. Here, we give details
on how we include the semantic information into the compound splitting process. Recall that we only
want to split compositional compounds and keep non-compositional compounds together.
The splitting decision (to split/not split a compound) is based on the compositionality score of the
compound that takes into account either one or both of the compound?constituent cosine values: if the
predicted degree of compositionality is high, the compound is split. We consider and combine four dif-
ferent criteria: i) only the compound?modifier similarity (mod); (ii) only the compound?head similarity
(head); a combination of the compound?modifier and the compound?head similarities, relying on (iii) the
geometric mean (geom) or (iv) on the arithmetic mean (arith). We used different thresholds for each of
these criteria throughout our experiments, with a specific focus on distinguishing the contributions of the
modifiers vs. the heads in the splitting decision, following insights from recent work in psycholinguistic
studies (Gagn?e and Spalding, 2009; Gagn?e and Spalding, 2011) as well as in computational approaches
on noun compounding (Reddy et al., 2011; Schulte im Walde et al., 2013). Furthermore, we compare
the effects of splitting with regard to two types of compounds, noun compounds and particle verbs: Both
types are very productive and can generate a potentially infinite number of new forms.
4 Experimental Setting
This section gives an overview on the technical details of the SMT system and our data sets. Compound
splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as
the input for parameter tuning and testing.
2
Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al.,
2007). We use it with default settings to train a translation model and we do so separately for each of the
different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Fea-
ture weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ?-safe-hope? until convergence.
Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005))
and also newspaper texts, overall ca. 1.5 million sentences
3
(roughly 44 million words). In addition, we
1
Baum|schule: ?tree|school? (tree nursery)
2
Compounds not contained in the parallel data are always split, as they cannot be translated otherwise.
3
Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09
84
use an English corpus of roughly 227 million words (including the English part of the parallel data) to
build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM
(Heafield, 2011). For parameter tuning, we use 1,025 sentences of news data.
Standard Test set 1,026 sentences of news data (test set from the 2009 WMT Shared Task): this set is
to measure the translation quality on a standard SMT test and make it comparable to other work.
Noun/Verb Test set As our main focus lies on sentences containing compounds, we created a second
test set which is rich in compounds. From the combined 2008-2013 Shared Task test sets, we extracted
all sentences containing at least one noun compound for which we have compound-constituent similarity
scores. Moreover, we excluded sentences containing nouns that are not in the parallel training data: such
compounds can only be translated when split which allows to translate their components. The final test
set consists of 2,574 sentences. Similarly, we also created a set rich in particle verbs (855 sentences).
Opaque Test set As the two first test sets mainly contain compositional compounds, we use a third test
set consisting of sentences with only non-compositional compounds. The underlying compounds were
chosen based on a list containing noun compounds and human ratings for compositionality (von der
Heide and Borgwaldt (2009)). As before, the compounds must have occurred in the parallel data. The
result is a list of 14 compounds, of which 11 have a low modifier-compound similarity and 3 have a low
head-compound similarity. We then extracted sentences containing these compounds (5 per compound =
70 in total) from German newspaper data
4
. In contrast to the other sets, we use this test set in a qualitative
study, to approximate the translation quality by counting the number of correctly translated compounds.
5 SMT Results
In this section, we present and discuss the results of our machine translation experiments. We first report
results for two test sets in terms of a standard evaluation metric (BLEU) and then continue with a small-
scale qualitative study on the translational behaviour of non-compositional compounds.
5.1 Compound Splitting within a Standard SMT Task
BLEU (Papineni et al., 2002) is a common metric to automatically measure the quality of SMT output
by comparing n-gram matches of the SMT output with a human reference translation. Table 1 lists the
results for our SMT-systems: we report on different compound-constituent scores and thresholds, for
noun compounds and particle verbs respectively. Note that BLEU scores are not comparable across dif-
nouns particle verbs
stand. noun stand. verb
baseline 21.00 21.08 21.00 20.29
a
g
g
r
DIST 22.00 22.02 21.02 20.11
FREQ 22.04 21.88 21.11 20.21
0
.
0
5
head 21.77 21.58 ? ?
mod. 22.01 21.74 ? ?
geom. 21.99 21.71 ? ?
arith. 21.95 21.95 ? ?
0
.
1
head 21.91 21.69 21.11 20.24
mod. 22.01 21.63 20.98 20.43
geom. 22.06 21.90 21.12 20.55
arith. 22.05 21.73 21.08 20.34
0
.
1
5
head 21.80 21.67 21.10 20.09
mod. 21.71 21.77 21.00 20.25
geom. 21.78 21.64 20.84 20.30
arith. 22.00 21.77 21.24 20.40
0
.
2
head 21.78 21.51 ? ?
mod. 21.78 21.45 ? ?
geom. 21.76 21.54 ? ?
arith. 22.02 21.79 ? ?
Table 1: BLEU scores for all compound-
constituent variations.
ferent test sets, but only illustrate system differences
within one test set. We compare our systems to the
scores of a baseline system (without compound process-
ing) and an aggressive split system in which all noun
compounds and particle verbs are split. The labels DIST
and FREQ indicate how several possible splittings were
disambiguated: DIST means we chose the splitting option
having the higher geometric mean of the two compound-
constituent scores, assuming that the variant expressing a
higher compositionality score leads to the more probable
splitting analysis. For FREQ, the decision is based on the
geometric mean of corpus frequencies of the respective
components of the compound, as is common practise for
the disambiguation of multiple splitting options in SMT
(Koehn and Knight, 2003; Fritzinger and Fraser, 2010).
In terms of BLEU, there is little difference for these two
variants. For further experiments, we thus decided to al-
ways use FREQ for disambiguation, assuming that com-
ponents chosen by frequency are potentially better repre-
4
www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.html
85
rating compound gloss mod. head translation
HIGHLY Staats|bankrott nation|bankruptcy 0.4779 0.6527 national bankruptcy
COMP. Staats|gebilde nation|structure 0.6955 0.3431 national structure
MEDIUM Industrie|staat industry|nation 0.0258 0.1488 industrial nation
COMP. Staats|kasse nation|cash box 0.0718 0.2757 public purse, treasury
LOW Staats|spitze nation|top 0.0024 0.0040 top/head of state
COMP. Staats|monotheismus nation|monotheism 0.0071 0.0071 national monotheism
Table 2: Examples for different compound-constitutent score ranges: HIGH: highly compositional,
MEDIUM: cases of doubt, LOW: highly non-compositional, according to their scores.
sented in the training data. Thus, we first use frequencies to determine the best split option in the case
of several possibilities, and then we apply distributional semantics to determine whether to split at all.
The remainder of Table 1 reports on different variants of the semantically-informed splitting criteria we
used. The notation head/mod/geom/arith indicates which (combination of) compound-constituent scores
were applied as criterion, with the threshold indicated by the vertical number. We performed the first
set of experiments with different thresholds for noun compounds, and then applied the medium-range
thresholds to the particle verbs. Generally, there are no considerable differences between the systems
with semantically restricted splitting and the aggressive split systems, even though there seems to be a
slightly positive effect for particle verbs. Having a closer look, we find that for noun compounds on the
standard test set, the best results (threshold: 0.1) are at the same level as the aggressive split systems;
with some small losses in BLEU on some of the other settings.
5.2 Discussion
All settings clearly outperform the baseline system (without compound processing). This indicates that
phrase-based SMT is rather robust with regard to non-semantic splitting as it is can often recover from
over-splitting by translating the word sequence as a phrase. This is in line with previous observations
of Koehn and Knight (2003). The results for the noun test set, which is biased towards containing more
nominal compounds, even suggests that less splitting might harm the system, as the BLEU scores tend
to drop when increasing the threshold. For particle verbs,
5
the picture is slightly different: first, splitting
only particle verbs does not lead to a considerable improvement over the baseline, as in the case of noun
compounds. For the verb test set, it even leads to a drop in BLEU. However, a more restricted splitting
leads to improved BLEU scores, even though not significantly better than the un-split baseline system.
Even though the handling of particle verbs needs to be refined in terms of dealing with their structural
behaviour (split vs. unsplit depending on the sentence structure) or ambiguities of the particle verb,
we consider this an encouraging result indicating that particle verbs can benefit from a semantically-
informed splitting process.
There are several possible reasons why a more restricted splitting might not lead to an improvement,
even though the idea of splitting only compositional compounds is intuitive and straightforward.
Inconsistent Splitting Compositionality is a continuum rather than a binary decision, with the scores of
many (compositional) compounds being in the medium range. Thus, it happens that some compounds
containing a certain constituent are split, whereas others are not: such inconsistent splittings do not
contribute to the generalization compound splitting aims for. Table 2 gives examples for compounds
with different degrees of compositionality, which illustrate this issue: for Industriestaat (?industrial
nation?) and Staatskasse (?public purse?) in the middle part of the table, a splitting decision based on
the head scores for thresholds of 0.15 or 0.2 leads to inconsistent splitting. Only compounds with high
scores, as the examples at the top of Table 2 are always split. The bottom part gives examples with
comparatively low compound-constituent scores that would benefit from splitting, but which will not be
split in any of our systems.
5
Note that there are considerably less particle verbs than noun compounds in the standard test set and the parallel data.
86
compound gloss translation unsplit f split f
Seehunde sea|dogs seals seals 5 seals 5
Flohmarkt flea|market flea market flea market 5 flea market 5
Kopfsalat head|salad lettuce lettuce 5 lettuce 5
Handtuch hand|cloth towel towel 5 towel 5
Kronleuchter crown|candelabra chandelier chandelier 5 crown leuchter 5
G?urteltiere belt|animal armadillo armadillos 5 belt animals 5
Wasserhahn water|rooster tap
water tap 2
tap 5
water supply 3
Meerschweinchen
sea|piglet guinea pig
guinea pig 4
guinea pig 5
sea pig 1
Taschenbuch pocket|book paperback
paper back 3
paperback 5
pocket book 2
Kronkorken crown|cork crown cap *kronkorken 5 crown corks 5
Taschenlampe pocket|lamp flashlight
pocket lamp 4
*taschenlampe 5
bag lamp 1
Fleischwolf meat|wolf meat grinder *fleischwolf 5 meat wolf 5
Marienk?afer Mary|bug ladybug *marienk?afer 5 *marie k?afer 5
Blockfl?oten block|flute recorder
block might 4
*blockfl?oten 5
bloc might 1
Table 3: correct vs. wrong ? Translation of non-compositional compounds (opaque test set) without
being split (unsplit) vs. being split prior to translation. ?*? highlights untranslated compounds.
Coverage of Opaque Compounds Another relevant factor concerns the frequency ranges of compounds
that are most interesting for this approach. High/mid frequency compounds are usually well-covered by
the training data of an SMT system, and in most cases they are translated correctly even if they have
been split erroneously. This is due to the fact that split compounds can be learned and translated as a
phrase if there were enough instances for the system to learn a valid translation. In the case of low-
frequency compounds, the system is less likely to learn a correct translation from the parallel data.
However, low-frequency compounds are not well covered by the system and splitting should thus be
highly beneficial. Newly created, i.e. highly compositional compounds, tend to be of low frequency, as
is illustrated by the example of Staatsmonotheismus (freq=1 in the parallel data) in Table 2. However,
a wrong splitting decision for a non-compositional compound of low frequency is likely to lead to an
incorrect translation as the SMT system has better statistics for the individual parts than for the sequence
of the compounds constituents. We assume that for low-frequency compounds the distributional similar-
ity scores are generally less reliable, even though using LMI helps to minimize this. To a certain extent,
we expect non-compositional compounds ?which are typically considered as lexicalized? to occur with
higher frequencies than novel compositional compounds.
6
Furthermore, there are considerably more
compositional than non-compositional compounds in standard text. Thus, being in favor of splitting in
the case of low-frequency words should be reasonable in most contexts.
6 A Closer Look at Translating Opaque Compounds
In this section, we compare the translations of non-compositional compounds when they are unsplit
and when they are split. We use a small test set containing 70 sentences, 5 for each of the 14 non-
compositional compounds (see Section 4). Then we conduct a small-scale qualitative analysis focusing
on the correct translation of opaque compounds.
Table 3 reports on correct translations for the non-compositional compounds for an experiment where
they have been split or not split (unsplit) prior to translation. Even though all compounds occurred in
the parallel data, five (which are marked with ?*?) cannot be translated by the unsplit system due to not
being aligned correctly. The other compounds are translated correctly (marked with ?+? in Table 3).
In the course of our study, we found that many of the correct translations remain the same (seals, flea
market, lettuce, towel). In the case of guinea pig, paperback and tap there are mixed results of correct and
incorrect translations. Only in the cases of chandelier (?crown leuchter?) and armadillo (?belt animal?),
6
It has to be noted, though, that the model is influenced by the somewhat different domain of the parallel data (European
Parliament proceedings, a standard data set for SMT).
87
compound gloss translation compound gloss translation
B?arlauch bear|leek bear leek Handtasche hand|bag handbag
Baumschule tree|school tree nursery Hirschk?afer stag|beetle stag beetle
L?owenanteil lion|share lion?s share H?uttenk?ase cottage|cheese cottage cheese
Fliegenpilz fly|mushroom fly agaric Kronkorken crown|cork crown cap
Flohmarkt flea|market flea market Teelicht tea|light tea candle
Table 4: Examples for (near) literal translation of non-compositional compounds.
which were translated correctly with the unsplit system, all translations obtained with the split system are
wrong. Somewhat surprisingly, in some cases there even is a benefit from splitting the non-compositional
compounds: Kronkorken, previously not translated at all, is correctly generated as crown cork. For other
previously untranslated words, Fleischwolf and Taschenlampe, literal translations of the constituents
are given: while meat wolf (instead of meat grinder) is probably not understandable, the translation of
Taschenlampe as pocket lamp is certainly preferable to the untranslated compound.
Due to the observed unexpected translational behaviour of 2 of the 14 non-compositional compounds
(Flohmarkt and Kronkorken), which can be translated literally and thus ?in theory? benefit from splitting,
we present a small study illustrating that this phenomenon is not as rare as one would intuitively expect.
This study is not meant to be comprehensive, but rather to point out that the translational behaviour of
non-compositional compounds can correspond to that of compositional compounds; Table 4 lists a few
such examples. We assume that this behaviour is due to the fact that English and German are similar
languages with a similar background. Thus, the ?images? used in non-compositional words often tend to
be similar. For some of the compounds (e.g. Flohmarkt) this is even true for some Romance languages,
too (IT: mercato delle pulci, FR: march?e aux puces) .
Generally, the SMT system should even be able to handle cases where the translation of one part is
not strictly literal (e.g. cap?cork or agaric?mushroom). In comparison to a dictionary, which only lists
few translations, the translation model offers a large choice of translation options that are not always
strictly synonymous, but can cover a large range of related meanings. In combination with the target-
side language model, this could allow to ?guess? good translations of such compounds. However, the
component-wise translation of non-compositional compounds only works if the source- and target lan-
guage compounds contain the same number of constituents. For example, consider translating the word
Faultier (lazy|animal: ?sloth?): even if the SMT system offers the translation faul?sloth, it would also
need to produce a translation for the constituent tier, probably resulting in something like sloth animal.
In conclusion, while phrase-based SMT is often able to recover from over-splitting by translating
a word sequence as a phrase, this is not always necessary for opaque compounds as they can have a
literal or near-literal translation. Thus, for explicitly handling non-compositional compounds in SMT, a
monolingual estimation of compositionality is not the only relevant factor. The translational behaviour
of compounds should also be taken into account.
7 Conclusion and Future Work
We studied the impact of compositionality in German-English SMT by restricting compound splitting to
compositional compounds. The decision about compositionality is based on the distributional similarity
between a compound and its constituents. We experimented with different threshold/score combinations
on a standard and a specifically created test set. Our results indicate that phrase-based SMT is very robust
with regard to over-splitting non-compositional noun compounds, with the exception of low-frequency
compounds. Furthermore, we studied the translational behaviour of non-compositional compounds with
a special focus on the fact that non-compositional compounds can in some cases be translated component-
wise, leading to the conclusion that a monolingual estimation of compositionality is not sufficient for an
optimal explicit handling of compounds in SMT applications.
The relatively low impact of distinguishing the degree of compositionality might also be due to the fact
that the task of translating noun compounds can be considered ?easy?, as the split components always
occur adjacently. In contrast, handling other types of non-compositional structures (e.g. noun-verb or
preposition-noun-verb combinations which are non-compositional) is a challenging task for future work.
88
Acknowledgements
This work was funded by the DFG Research Projects ?Distributional Approaches to Semantic Related-
ness? (Marion Weller, Stefan M?uller) and ?Models of Morphosyntax for Statistical Machine Transla-
tion ? Phase 2? (Fabienne Cap, Alexander Fraser, Marion Weller) and the DFG Heisenberg Fellowship
SCHU-2580/1-1 (Sabine Schulte im Walde).
References
Ming-Hong Bai, Keh-Jiann Chen, and Jason S Chang. 2008. Improving word alignment by adjusting chinese word
segmentation. In IJCNLP?08: Proceedings of the 3rd International Joint Conference on Natural Language
Processing, pages 249?256.
Collin Bannard. 2005. Learning about the Meaning of Verb?Particle Constructions from Corpora. Computer
Speech and Language, 19:467?478.
Stefan Bott and Sabine Schulte im Walde. 2014. Optimizing a Distributional Semantic Model for the Prediction
of German Particle Verb Compositionality. In Proceedings of the 9th Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-
NAACL?12: Proceedings of the Human Language Technology Conference of the North American Chapter of the
Association for Computational Linguistics, volume 12, pages 34?35.
Paul Cook and Suzanne Stevenson. 2006. Classifying Particle Semantics in English Verb-Particle Constructions.
In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underly-
ing Properties, pages 45?53, Sydney, Australia.
Steve DeNeefe, Ulf Hermjakob, and Kevin Knight. 2008. Overcoming vocabulary sparsity in mt using lattices.
In AMTA?08: Proceedings of the 8th Biennial Conference of the Association for Machine Translation in the
Americas.
Stefan Evert. 2005. The Statistics of Word Co-Occurrences: Word Pairs and Collocations. Ph.D. thesis, Institut
f?ur Maschinelle Sprachverarbeitung, Universit?at Stuttgart.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC ? a Corpus of Parsable Sentences from the Web. In Proceedings
of the International Conference of the German Society for Computational Linguistics and Language Technology,
pages 61?68, Darmstadt, Germany.
John R. Firth. 1957. Papers in Linguistics 1934-51. Longmans, London, UK.
Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis
and Corpus Statistics for German Compound Processing. In Proceedings of the Fifth Workshop on Statistical
Machine Translation, pages 224?234. Association for Computational Linguistics.
Christina L. Gagn?e and Thomas L. Spalding. 2009. Constituent Integration during the Processing of Compound
Words: Does it involve the Use of Relational Structures? Journal of Memory and Language, 60:20?35.
Christina L. Gagn?e and Thomas L. Spalding. 2011. Inferential Processing and Meta-Knowledge as the Bases for
Property Inclusion in Combined Concepts. Journal of Memory and Language, 65:176?192.
Zellig Harris. 1968. Distributional Structure. In Jerold J. Katz, editor, The Philosophy of Linguistics, Oxford
Readings in Philosophy, pages 26?47. Oxford University Press.
Kenneth Heafield. 2011. Kenlm: faster and smaller language model queries. In EMNLP?11: Proceedings of the
6th workshop on statistical machine translation within the 8th Conference on Empirical Methods in Natural
Language Processing, pages 187?197.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In EACL ?03: Proceedings of
the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 187?193,
Morristown, NJ, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL?07: Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session, pages 177?180.
89
Philipp Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit?05: Proceed-
ings of the 10th machine translation summit, pages 79?86.
Natalie K?uhner and Sabine Schulte im Walde. 2010. Determining the Degree of Compositionality of German Par-
ticle Verbs by Clustering Approaches. In Proceedings of the 10th Conference on Natural Language Processing,
pages 47?56, Saarbr?ucken, Germany.
Andrea Lechler and Antje Ro?deutscher. 2009. German Particle Verbs with auf. Reconstructing their Composition
in a DRT-based Framework. Linguistische Berichte, 220.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and
Treatment, pages 73?80, Sapporo, Japan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In COLING?00:
Proceedings of the 18th International Conference on Computational Linguistics, pages 1081?1085. Morgan
Kaufmann.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51,.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation
of machine translation. In ACL?02: Proceedings of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An Empirical Study on Compositionality in Com-
pound Nouns. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages
210?218, Chiang Mai, Thailand.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014. Using Distributional Similarity of Multi-Way Translations
to Predict Multiword Expression Compositionality. In Proceedings of EACL 2014.
Roland Sch?afer and Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool
Chain. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
486?493, Istanbul, Turkey.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. Smor: A German computational morphology covering
derivation, composition and inflection. In LREC ?04: Proceedings of the 4th Conference on Language Resources
and Evaluation, pages 1263?1266.
Sabine Schulte im Walde, Stefan M?uller, and Stephen Roller. 2013. Exploring Vector Space Models to Predict the
Compositionality of German Noun-Noun Compounds. In Proceedings of the 2nd Joint Conference on Lexical
and Computational Semantics, pages 255?265, Atlanta, GA.
Sylvia Springorum. 2011. DRT-based Analysis of the German Verb Particle ?an?. Leuvense Bijdragen, 97:80?
105.
Andreas Stolcke. 2002. SRILM ? an extensible language modelling toolkit. In ICSLN?02: Proceedings of the
international conference on spoken language processing, pages 901?904.
Sara Stymne. 2008. German compounds in factored statistical machine translation. In GoTAL ?08: Proceedings
of the 6th International Conference on Natural Language Processing, pages 464?475. Springer Verlag.
Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter, Basis und Oberbegriffen. In
Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51?74.
Heike Zinsmeister and Ulrich Heid. 2004. Collocations of Complex Nouns: Evidence for Lexicalisation. In
Proceedings of Konvens, Vienna, Austria.
90
