Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 37?40,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Safe In-vehicle Dialogue Using Learned Predictions of User Utterances
Staffan Larsson
Talkamatic AB
F?orsta L?anggatan 18
413 28 G?oteborg
Sweden
staffan@talkamatic.se
Fredrik Kronlid
Talkamatic AB
F?orsta L?anggatan 18
413 28 G?oteborg
Sweden
fredrik@talkamatic.se
Pontus W
?
arnest
?
al
Halmstad University
Box 823
301 18 Halmstad
Sweden
pontus.warnestal@hh.se
Abstract
We present a multimodal in-vehicle dia-
logue system which uses learned predic-
tions of user answers to enable shorter,
more efficient, and thus safer natural lan-
guage dialogues.
1 Background
1.1 Driver Distraction
Driver distraction is a common cause of accidents,
and is often caused by the driver interacting with
technologies such as mobile phones, media play-
ers or navigation systems. A study, commonly
referred to as the ?100 car study? (Neale et al.,
2005) revealed that secondary task distraction is
the largest cause of driver inattention, and that the
handling of wireless devices is the most common
secondary task.
As interaction complexity in the car increases
due to more advanced infotainment systems and
smartphones, drivers are often executing several
tasks in parallel to the primary task of driving.
The increased functionality of these systems has
resulted in large hierarchical information architec-
tures that prolong interaction time, thereby nega-
tively affecting safety as well as user experience
(Kern and Schmidt, 2009).
1.2 Relation to state of the art
State-of-the-art infotainment systems typically do
not include user models at all. Siri, available on
the Apple iPhone 4S and later models, has a static
user model containing personal information ex-
plicitly provided by the user (home address, etc.).
This information is used in voice interactions; for
example, given that the user has entered their fam-
ily relations, phrases like ?Call my wife? can be
used. A different approach is taken in Google
Now, which dynamically learns user patterns from
observations and presents unrequested informa-
tion as ?cards? on the screen. However, Google
Now does not attempt to integrate predictions into
dialogue interaction.
The work reported here explores the use of
adaptive user modeling in multimodal dialogue
systems. User preferences and behaviour patterns
are learnt from observations of user interactions
with the infotainment system and the context in
which these interactions take place, and are used
proactively to predict user answers and thereby en-
able shorter and more efficient interaction. The
underlying motivating assumption is that using
apps and services in an in-vehicle context inher-
ently leads to distraction, and that reducing inter-
action time will reduce driver distraction.
1.3 TDM
Based on Larsson (2002) and later work, Talka-
matic AB has developed the Talkamatic Dialogue
Manager (TDM).
TDM provides a general interaction model
based on interaction which are basic to human-
human linguistic interaction, resulting in a high
degree of naturalness and flexibility which in-
creases usability. The model is domain-
independent which means that dialogue behaviour
can be altered without touching application prop-
erties and vice versa. TDM also offers integrated
multi-modality which allows user to freely switch
between modalities (Larsson et al., 2011).
1.4 Grounding in TDM
Grounding (Clark and Brennan, 1990) is, roughly,
the process of making sure that dialogue partici-
pants agree on what has been said so far and what
it meant. TDM has an extensive model of ground-
ing (Larsson, 2002). It operates on different levels:
? Perception
? Semantic Understanding
37
? Pragmatic Understanding
? Acceptance
System feedback (positive, negative and in
some cases interrogative) can be generated on each
level:
? Examples: ?I didn?t hear? ? negative percep-
tion
? ?To work, is that right?? ? interrogative se-
mantic understanding
? ?OK? ? positive acceptance.
2 Learning and Classification
Many dialogue applications require the user to an-
swer a number of questions. To make dialogue
shorter, we have extended TDM so that it tries to
predict user answers on the basis of a user model
learned from observations of user behaviour. As
an illustration, we use a road information appli-
cation which tries to predict the user?s destina-
tion and thereby eliminate the need to ask the user
about this.
2.1 Learning Method
Initially, a range of learning methods requir-
ing (N-gram, MDP, POMDP) were explored and
evaluated, but the KNN (K-Nearest Neighbours)
(Mitchell, 1997) was considered the best method.
An important advantage is that KNN can learn
from a relatively small set of observations. This
is in contrast to the MDP and POMDP (and to
a lesser extent, N-gram) methods, which require
large amounts of data to generate useful behaviour.
A potential drawback of KNN is that this model
cannot model sequences of user behaviours.
2.2 Parameter Selection
On the basis of user studies provided from the
user partner of the project, it was decided that the
most important user model parameters was posi-
tion, day of the week and hour of the day. The
training data were simulated and correspond to the
behaviour of an archetypal persona provided by
the user partner in the project.
2.3 Learning and Classification
The learning part of the system listens for a num-
ber of events, such as ?start-car?, ?stop-car? etc..
From these events and information about cur-
rent position, the time of the day and the day of
the week, the system creates new data instances.
The system thus learns how the user?s destination
varies depending on these parameters. A sample
dataset is shown in Figure 1, where data points
show destinations of trips initiated at various times
of the week.
When the dialogue manager requests a predic-
tion of the destination, the KNN algorithm tries to
find the K data points closest to the present data
point, and the top alternatives are returned to the
dialogue manager together with confidence scores
indicating the reliability of the predictions.
3 Integration of Classifications into TDM
3.1 Grounding uncertain information
We treat the information emanating from the user
model as uncertain information about a (predicted)
user utterance. Hence, the same mechanisms used
for grounding utterances have been adapted for in-
tegrating user model data.
3.2 Integrating Classifier Output
TDM is based on the Information State Update
(ISU) approach to dialogue management. The in-
formation state in TDM is based on that of the
system described in Larsson (2002) and includes
Questions Under Discussion, a dialogue plan, and
shared commitments.
The rule for integrating the user model data is
a standard ISU rule, consisting of preconditions
and effects on the information state. We describe
these informally below:
PRECONDITIONS
? If there is a propositional answer from the
user model resolving a question in the current
plan...
? and if the confidence score reported from the
user model is sufficient, then...
EFFECTS
? accept the propositional answer (include it
into the shared commitments), and...
? give appropriate feedback to the user depend-
ing on the confidence score:
? High confidence? embedded feedback
? ?Which route do you want to take to
work??.
38
Figure 1: A sample dataset. The horizontal axis shows days of the week (0=Monday, ..., 6=Sunday)
and the vertical axis shows hour of the day. Data points show destinations of trips initiated at the time
indicated by their position. (?Now? is the current time, in this case Thursday at lunchtime.)
? The user can always reject the prediction
by requesting another destination.
? Medium confidence? positive feedback
? ?I assume you?re going to work?.
? If the user says ?no?, the answer is re-
jected
? Silence is interpreted as acceptance.
? Low confidence? interrogative feedback
? ?To work, is that correct??
? In this case, the user needs to explicitly
accept the proposed answer.
? Otherwise, the user is prompted for an
answer.
3.3 GUI output
If the ISU rule above does not apply because of
too low confidence scores, user model informa-
tion is still used in the GUI. When a Wh-question
is raised by the system, the GUI always presents a
list of possible alternatives. High-confidence alter-
natives are highlighted and sorted before the other
alternatives in the list.
4 Resulting behaviour
The demonstrator enables interaction with a learn-
ing dialogue system which uses predictions to sim-
plify interactions. Here is an sample interaction:
User: Traffic information
Car: Ok. What road?
User: E6.
Car: Showing traffic on the E6
If this is repeated on a number of occasions,
eventually the system will use a prediction:
User: Traffic information
Car: Showing traffic on the E6
The system thus reduces the need for repetitive
and information-scarce utterances from the user.
As soon as the system has started identifying a pat-
tern, it will start to suggest the most probable al-
ternatives. Initially, the most probable answers are
presented to the user as the top items in a list. The
alternatives are also marked in a different color to
make them more visible to the user (not shown
here).
User: Traffic information
Car: Ok. What road?
Car GUI: [E6] [E45] [E20] [155]
User: E6.
Car: Showing traffic on the E6
39
After some further use, the system has identi-
fied a pattern which is prominent enough for the
system to make a suggestion:
User: Traffic information
Car: E6, is that right?
User: Yes.
Car: Showing traffic on the E6
After getting further support for its hypothesis,
the system will merely inform the user that an as-
sumption has been made. If the user is satisfied
with the assumption, she does not need to do any-
thing, but can correct or confirm it if desired.
User: Traffic information
Car: I assume E6.
User: [silence]
Car: Showing traffic on the E6
User: Traffic information
Car: I assume E6.
User: No, E45.
Car: Showing traffic on the E45
If the user rejects the system suggestion with-
out giving another answer, the system will show
a menu where the most probable choices are the
topmost ones, and marked in a distinct colour (not
shown here).
User: Traffic information
Car: I assume E6.
User: No.
Car: What road?
Car GUI: [E6] [E45] [E20] [155]
When the system is certain about its hypothe-
sis, the system will simply provide the user with
the desired information without asking the user for
parameters.
User: Traffic information
Car: Showing traffic on the E6
5 Conclusions and further work
We have designed and implemented a mechanism
which learns user patterns and uses them proac-
tively to simplify and shorten dialogue interac-
tions. The idea of learning user patterns from ob-
servations is similar to Google Now. However,
while Google Now uses ?cards? to provide un-
requested information to the user, we show how
predictions can be integrated into spoken or multi-
modal dialogue.
It remains for future work to evaluate the sys-
tem to establish that this actually reduces the dis-
traction rate of drivers. We also want to test the
performance of the learning mechanism by train-
ing it on real observations of user behaviours (as
opposed to simulated data).
The current mechanism only predicts answers
to individual system questions, which may result
in suboptimal behaviour in cases where there are
dependencies between the questions pertaining to
some task. An interesting area for future work is
to instead predict sequences of answers; however,
this would require a more powerful learning and
classification mechanisms.
Acknowledgements
This work was carried out within the FFI project
?Safe Speech by Knowledge? (2012-00941),
funded by VINNOVA, Volvo Car Corporation and
Talkamatic.
References
H. H. Clark and S. E. Brennan. 1990. Grounding
in communication. In L. B. Resnick, J. Levine,
and S. D. Behrend, editors, Perspectives on Socially
Shared Cognition, pages 127 ? 149. APA.
Dagmar Kern and Albrecht Schmidt. 2009. Design
space for driver-based automotive user interfaces. In
Proceedings of the 1st International Conference on
Automotive User Interfaces and Interactive Vehic-
ular Applications, AutomotiveUI ?09, pages 3?10,
New York, NY, USA. ACM.
Staffan Larsson, Alexander Berman, and Jessica
Villing. 2011. Adding a speech cursor to a mul-
timodal dialogue system. In INTERSPEECH 2011,
12th Annual Conference of the International Speech
Communication Association, Florence, Italy, 2011,
pages 3319?3320.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, G?oteborg University.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer,
Jeremy Sudweeks, and Michael Goodman. 2005.
An overview of the 100-car naturalistic study and
findings. Technical report.
40
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 110?119,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Lekbot: A talking and playing robot for children with disabilities
Peter Ljungl?f
Computer Science and Engineering
University of Gothenburg, Sweden
peter.ljunglof@gu.se
Britt Claesson
Ingrid Mattsson M?ller
DART: Centre for AAC and AT
Queen Silvia Children?s Hospital
Gothenburg, Sweden
{britt.claesson,ingrid.mattsson-muller}@vgregion.se
Stina Ericsson
Cajsa Ottesj?
Philosophy, Linguistics and
Theory of Science
University of Gothenburg, Sweden
{stina.ericsson,cajsa.ottesjo}@gu.se
Alexander Berman
Fredrik Kronlid
Talkamatic AB
Gothenburg, Sweden
{alex,fredrik}@talkamatic.se
Abstract
This paper describes an ongoing project
where we develop and evaluate a setup in-
volving a communication board and a toy
robot, which can communicate with each
other via synthesised speech. The purpose
is to provide children with communicative
disabilities with a toy that is fun and easy
to use together with peers, with and with-
out disabilities. When the child selects
a symbol on the communication board,
the board speaks and the robot responds.
This encourages the child to use language
and learn to cooperate to reach a common
goal. Throughout the project, three chil-
dren with cerebral palsy and their peers
use the robot and provide feedback for fur-
ther development. The multimodal inter-
action with the robot is video recorded and
analysed together with observational data
in activity diaries.
1 Background
The vision of our project is to utilise current
technology in human computer interaction and
dialogue systems to provide young people with
communication disabilities with a fun and ex-
citing toy. Currently there are not many op-
portunities for children with severe disabilities
to play independently and to interact on equal
terms with typically developing children. Our
hope is that the toy will give children, with and
without disabilities, the opportunity to interact
Figure 1: The robot and the communication board
and play with each other. As a side effect this
can also help them develop their communicative
skills.
We are developing a remote-controlled robot
that can be used by children with severe phys-
ical and/or communicative disabilities, such as
cerebral palsy or autism. The child communi-
cates by selecting a symbol on a communication
board, which is translated into an utterance us-
ing a speech synthesiser. The robot responds us-
ing synthesised utterances and physical actions,
that the child in turn can respond to. The com-
munication board acts as an extension of the
child, by giving the child speech as a means of
communication. The robot and its communica-
tion board is shown in Figure 1.
Technically the robot is controlled wirelessly,
110
with no speech recognition. The spoken dialogue
is there for the benefit of the child, and enables
the child to engage in a spoken dialogue, without
having the physical and/or cognitive ability to
do so. Our hope is that this will facilitate the
child?s own language development while having
fun with the radio-controlled robot.
1.1 The Lekbot project
The Lekbot project is a collaboration between
DART,1 Talkamatic AB and the University of
Gothenburg. It is funded by VINNOVA2 and
runs from March 2010 to August 2011.
The project is similar to the TRIK project
(Ljungl?f et al, 2009), which developed a draw-
ing robot that was controlled in the same man-
ner as above. The very limited user study that
was conducted suggested that the product had
great potential. The current project can be seen
as a continuation of TRIK, where we perform a
more full-scale user study, with video recording,
transcription, interaction analyses, etc.
1.2 Dialogue systems and robots
Most existing dialogue systems are meant to be
used by competent language users without phys-
ical, cognitive or communicative disabilities; ei-
ther they are supposed to be spoken to (e.g.,
phone based systems), or one has to be able to
type the utterances (e.g., the interactive agents
that can be found on the web). Dialogue sys-
tems for users with disabilities have so far been
targeted at people with physical disabilities, who
need help in performing daily activities.
Dialogue systems have also been used for sec-
ond language learning; i.e., learning a new lan-
guage for already language competent people.
Two examples are the artificial agent ?Ville: The
Virtual Language Tutor? (Beskow et al, 2004),
and ?SCILL: Spoken Conversational Interface
for Language Learning?, a system for practicing
Mandarin Chinese (Seneff et al, 2004).
However, we are not aware of any examples
where a dialogue system is used for communicat-
1Centre for AAC and AT at the Queen Silvia Chil-
dren?s Hospital
2The Swedish Governmental Agency for Innovation
Systems
ing with people with communication disorders.
With the advent of tablet computers, there
now exist several spoken-language and touch-
screen apps for children?s games and interactive
and linguistic training. In these apps, the in-
teraction is between the child and the tablet,
whereas in Lekbot the child and the tablet act
together as one dialogue participant, interact-
ing with the robot. The Lekbot robot is also a
physical agent, acting in the world, thus adding
another dimension to the interaction.
When it comes to robots, there are a number
of past and present research projects on robots
and children. An early inspiration is the LOGO
robot developed at Massachusetts Institute of
Technology for teaching children to use com-
puters and program simple applications (Papert,
1993). There are several robots focusing on chil-
dren with disabilities (Robins et al, 2008; Sal-
dien et al, 2006; Kozima et al, 2007; Lee et al,
2008; Arent and Wnuk, 2007), and most com-
monly autism. Some of these communicate with
children in different ways. For instance, KAS-
PAR is a child-sized humanoid robot for chil-
dren with autism, and it trains interactional ca-
pabilities through gesture imitation.3 Probo, de-
veloped for hospitalised children, produces non-
sense speech intended to convey different feel-
ings.4 KOALA is a small round ball that in-
teracts with children with autism using lights
and sounds (Arent and Wnuk, 2007). However,
none of these robots and research projects in-
volves natural language communication in any
form between the child and the robot.
2 Project description
Our basic idea is to use a dialogue system
to stimulate play and interaction for children
with severe communicative disabilities. There
are already communication boards connected to
speech synthesis in the form of communication
software on computers. The main values that
this project adds to existing systems are that:
? the child is offered an exciting, creative and
fun activity
3http://kaspar.feis.herts.ac.uk/
4http://probo.vub.ac.be/
111
? the child can play and interact with other
peers on equal terms
? the child can explore language in stimulat-
ing cooperation with the robot and with
other children
By being able to use a symbol-based communi-
cation board the children are given an opportu-
nity to play, interact, explore language, and at
the same time learn to use tools for alternative
and augmentative communication.
2.1 Description of the system
The child has a communication board that can
talk; when the child points at one of the symbols
it is translated to an utterance which the board
expresses via speech synthesis in Swedish. This
is recognised by a robot that moves around in the
room, and performs the commands that the child
expresses through the board. The robot has an
incarnation as a toy animal, currently a bumble-
bee. It has a very basic personality which means
that it can take the initiative, without the child
telling it, refuse actions, or even negotiate with
the child.
The inspiration for the robot comes from robot
toys such as babies, dogs and dinosaurs, but
also from electronic pets such as Tamagotchi and
Talking Tom. The main difference is that our
robot is able to have a dialogue with the child,
to find out what to do, or just to be teasingly
playful.
The Lekbot robot can move forward and back-
ward, and turn right and left. Furthermore it
can perform actions such as laughing, dancing,
yawning, farting and eating. The functionality
is constantly improving during the evaluation, to
keep the children interested in playing with the
robot.
2.2 Needs and potential
The target audience is children with severe
physical, cognitive or communicative disabilities.
These children depend on assistive devices and
persons to be able to interact with other people
and artifacts. The idea is that the robot will be
a fun toy that gives the child an opportunity to
control the artifacts itself, without the help of
other people. Hopefully this will increase the
child?s confidence, and also promote language
development.
2.2.1 The importance of play
Play may be defined by the following terms
(Knutsdotter Olofsson, 1992):
? spontaneous; the child takes the initiative,
not the adults
? not goal-oriented; the game does not have
an explicit purpose
? fun and pleasurable
? repeating; that it can be played many times
as one wants
? voluntary
For children with severe disabilities, playing re-
quires adult help, and it is difficult for the adult
not to control the game, especially if the child
has problems communicating what it wants. Of-
ten play is used as a tool for development train-
ing, and many times play is so scheduled that
it is no longer spontaneous (Brodin and Lind-
strand, 2007). A toy that is always available for
the child to play with whenever it wants, and
on its own terms can help the child to play ?for
real?.
Children learn from each other, and a toy that
is used on equal terms by children, with and
without disabilities, encourages interaction that
otherwise would not have been possible between
children with such diverse backgrounds.
2.2.2 Educational advantages
As discussed in section 3.3 later, the setup
works without the robot and the communication
board actually listening to each others? speech ?
instead, they communicate wirelessly. However,
there is an important educational point in hav-
ing them (apparently) communicate using spo-
ken language. It provides the child with an ex-
perience of participating in a spoken dialogue,
even though the child is not physically able to
speak. For children who are more advanced in
their language development, the robot can offer
112
the opportunity to understand the basic proper-
ties of the dialogue, such as taking turns, asking
and answering questions, the importance of pro-
viding sufficient information, and cooperating to
achieve a shared goal. Another educational ad-
vantage is that the child learns to use tools for
alternative and augmentative communication.
3 Implementation
This section describes some technical aspects of
the implementation of the Lekbot system.
3.1 Components
The final Lekbot setup consists of the following
components:
? a simple LEGO Mindstorms robot which
can turn and move in all directions, can
perform different specialised actions, and
has a ?costume? which makes it look like
a bumble-bee
? a touch-screen computer which functions as
a communication board, and a custom sup-
port frame for the computer
? the dialogue system GoDiS (Larsson, 2002),
using Acapela Multimedia text-to-speech
with Swedish voices
? Bluetooth communication and wireless au-
dio transmission, from the touch-screen
computer to the robot, and two sets of loud-
speakers, for the computer and the robot
If the target user already has his or her ownWin-
dows based communication device, with adapted
accessibility for him or her, this special software
for the robot play can be installed on this device.
Note that it is the communication board com-
puter that controls the robot via the dialogue
system, but the intention is that it should seem
like the robot is autonomous. Every utterance
by the robot is executed by the speech synthe-
siser, and then sent to the robot via radio.
3.2 LEGO Mindstorms
The robot is built using LEGO Mindstorms
NXT,5 a kind of technical lego which can be con-
5http://mindstorms.lego.com/
trolled and programmed via a computer. Apart
from being cheap, this technology makes it easy
to build a prototype and to modify it during the
course of the project.
3.3 Perfect speech recognition
Typically, the most error-prone component of a
spoken dialogue system is speech recognition;
the component responsible for correctly inter-
preting speech. This of course becomes even
more problematic when working with language
learning or communication disorders, since in
these situations it is both more difficult and more
important that the computer correctly hears and
understands the user?s utterances. An advan-
tage of the Lekbot setup is that we will, in a
sense, have ?perfect speech recognition?, since
we are cheating a bit. The robot does not ac-
tually have to listen for the speech generated by
the communication board; since the information
is already electronically encoded, it can instead
be transferred wirelessly. This means that the
robot will never hear ?go forward and then stop?
when the communication board actually says ?go
forward seven steps?.
3.4 The GoDiS dialogue manager
A dialogue system typically consists of several
components: speech recogniser, natural lan-
guage interpreter, dialogue manager, language
generator, speech synthesiser and a short-term
memory for keeping track of the dialogue state.
One can make a distinction between dialogue
systems, which (ideally) are general and reusable
over several domains, and dialogue system appli-
cations, which are specific to a certain domain.
The dialogue manager is the ?intelligence? of the
system, keeping track of what has been said so
far and deciding what should be said next.
The GoDiS dialogue manager (Larsson, 2002)
has been developed at the Department of Philos-
ophy, Linguistics and Theory of Science at the
University of Gothenburg over several years. It
is designed to be easily adaptable to new do-
mains, but nevertheless be able to handle a va-
riety of simpler or more complex dialogues. For
example, GoDiS can either take initiative and
prompt a user for information, or take a back
113
seat and let the experienced user provide infor-
mation in any desired order, without having to
wait for the right question from the system.
From the viewpoint of dialogue systems re-
search, there are some interesting aspects in the
Lekbot setting:
? Constantly changing environment : the sur-
roundings of the robot can change all the
time, and the dialogue system needs to
adapt
? Alternative input modalities: instead of
speech input, we are using a touch screen in-
terface, on which the symbols on the screen
also changes depending on the current dia-
logue state
? Utterance generation: it is important for ev-
eryone, but in particular children with com-
municative disabilities, that information is
presented in a correct way ? with correct
and consequent grammar, lexicon and pro-
nunciation
3.5 Utterance generation
Clear pronunciation is important, and perhaps
even more important when we are dealing with
communicative disabilities. We are experiment-
ing with using different utterance generation
strategies and stressing important words to make
the children understand the robot better. Inter-
estingly, user feedback from children and pre-
schools during the project has also indicated
when default intonation does not work and needs
to be modified.
The Lekbot system uses two different voices,
one for the touch screen, acting as the child?s
voice, and one for the robot. Whereas the touch-
screen voice is a vocalisation of something the
child has already seen on the screen, the utter-
ances of the robot have no visualisations. Hence,
it is particularly important that the robot?s ut-
terances are as clear as possible, and the TTS
voice chosen for the robot is therefore the voice
that was determined to have the best and most
flexible intonation in informal perception tests
at the start of the project.
3.5.1 Contextual intonation
We have incorporated models of information
structure in GoDiS to enable the appropriate
assignment of phonological emphasis (Ericsson,
2005).
Lekbot uses a fairly basic dialogue-move-to-
string mapping for the creation of output utter-
ances, which are then fed to the speech synthe-
siser. Determining the information structure of
an utterance to be generated, involves the deter-
mination of what is informative in the utterance
? the focus ? and what is a reflection of some-
thing already in the context ? the ground (Vall-
duv?, 1992). The system assigns emphasis to all
alternatives, that is, all contrasting elements, in
alternative questions, that are produced by the
robot. Consider the following example:
User : Go forward.
Robot : Do you want me to go forward
a lot or go forward a little?
For the generation of the robot utterance, the
system determines ?go forward a lot? and ?go
forward a little? as alternatives, and assigns em-
phasis to these. Future development of the sys-
tem may involve the inclusion of information
structure also for utterances other than non-
alternative questions, to determine appropriate
intonation assignment more generally.
Unfortunately, we have not yet been able to
use this feature in the actual demonstration sys-
tem, since the Swedish TTS voices do not em-
phasise properly with regard to the markup. In-
stead we have tuned the utterances lexically and
syntactically to make the best possible use of the
default TTS intonation.
4 Evaluation
We are evaluating the Lekbot system during
spring and summer 2011, in parallel with con-
tinued development, in the spirit of eXtreme
Programming (XP). Some major themes in XP
that were deemed particularly interesting in this
project are i) the need to involve the users in
the development process, ii) to work in short it-
erations with frequent releases to get a nearly
constant feedback from users, and iii) to always
114
prioritise the tasks that provide the greatest ben-
efit to users.
4.1 Users
A test group was recruited consisting of three
target children with peers and staff, at three
different pre-schools, was recruited. The target
children, two boys and one girl are in the ages 4?
6 years, two boys and one girl. They have cere-
bral palsy with complex communication needs.
They also have a poor gross motor control, but
are able to use their hands for activating a touch
screen on a computer. They serve as the test
group and as a basis for the specifications of the
further development of the system. During the
course of development the children in the test
group use the system to verify that it works as
intended and help to identify the most important
qualities to develop. The project group works
with one month iterations with a new public re-
lease every second month. Therefore, the users
have in the end used about six releases of the
robot.
Along with the target children, three typically
developed peers, of the same age, or slightly
younger, were recruited at each pre-school. The
three peers were all girls. Hence, there are three
groups of children playing with the robot. At
various occasions other children in the pre-school
group are involved in the robot play.
The children were assessed regarding their re-
ceptive language levels by using Test for Re-
ception of Grammar (TROG) (Bishop et al,
1998). Their communication levels were es-
timated by the project group in cooperation
with the pre-school staff using Communication
Function Classification System (CFCS) for In-
dividuals with Cerebral Palsy (Hidecker et al,
2009). The pre-school staff also completed
Swedish Early Communicative Development In-
ventories (SECDI) forms for each child (Eriks-
son and Berglund, 1999; Berglund and Eriksson,
2000). A pre-school form (F?rskoleformul?r) was
also completed (Granlund and Olsson, 1998). It
consists of questions concerning the child?s en-
gagement in various situations, the pre-school
teacher?s perception of the interaction between
her and the child as well as the interaction be-
tween the child and other children.
With the two youngest target children TROG
testing was not feasible, while the oldest one ap-
peared to have some difficulties in understand-
ing verbs, prepositions and sentences containing
these components, thus a bit lower than his age.
The three peers showed results matching their
age. From here on the target children will be
named Per, Hans and Greta.
The purpose of CFCS is to classify the every
day communication performance of an individ-
ual with cerebral palsy. The levels are ranged
between 1 and 5, where 1 is the highest and 5
the lowest.
? The 6 year old Per shows a level of 3: Ef-
fective sender and effective receiver with fa-
miliar partners.
? The 5 year old Hans is estimated to level
5: Seldom effective sender and effective re-
ceiver with familiar partners, and
? The 4 year old Greta is at level 4: Incon-
sistent sender and/or receiver with familiar
partners.
? All the peers, of course, reach the level of 1.
The CFCS levels will be estimated over again
when the Lekbot testing is finished.
The results of SECDI and the pre-school form
will be presented at a later stage of the Lekbot
project, as they will be redistributed.
4.2 Evaluation tools and methods
The tools used to evaluate the robot play are
three:
? Talking Mats,6 which is an established com-
munication tool that uses a mat with at-
tached symbols as the basis for communi-
cation. It is designed to help people with
communicative and cognitive difficulties to
think about issues discussed with them, and
provide them with a way to effectively ex-
press their opinions. Both the target chil-
dren and their peers were interviewed about
the robot and the interaction, in order to get
6http://www.talkingmats.com
115
feedback for evaluation and for developing
the system.
They were asked questions about the be-
haviour of the robot and answered by
putting symbol cards either at the ?fun? side
of the mat or at the ?boring/not nice? side.
It is also possible to put symbols between
?fun? and ?boring/not nice?. The answers
were then checked and evaluated together
with the children. An example is shown in
Figure 2.
? Video recordings during the robot play were
made by the project group from January
to May 2011, six recordings from each peer
group, in all 18 recordings. The duration
is between 20 and 30 minutes each and
shot with one camera by one of the project
members. Short sequences from the videos
have been transcribed and analysed with
focus on cooperation between the children
and joyfulness. Transcriptions were made
in CLAN7 with detailed descriptions of the
non-verbal actions, signs and gaze. We got
permissions to do the recordings from the
parents of the children.
? Weekly Activity diaries were kept by the
pre-school staff, where they could provide
their reflections about the play sessions.
The diaries included headings regarding
numbers of play occasions, duration of the
play, persons participating, what happened
in the play, functionality of the robot, sug-
gestions for improvement and the children?s
satisfaction with the play perceived by the
staff.
Furthermore, the interaction between the com-
munication board and the robot is logged by the
system, providing valuable information.
Beside these evaluation tools there have also
been discussions with the designated staff at the
current pre-schools.
7http://childes.psy.cmu.edu/clan/
Figure 2: Talking Mats
4.3 Preliminary evaluation results from
the activity diaries
According to the activity diaries, Lekbot was
used 56 times during releases 2?5; just below 10
times each for the early releases, and 20 times
each for releases 4 and 5. There is a great varia-
tion in numbers of performed play sessions and
in completed activity diaries, mainly due to ill-
ness in children or staff, orthopedic surgery in
one child and holidays. In the beginning there
was always the same peer, and only that one,
attending the play sessions. Further on in the
project the staff chose to engage more peers
from the pre-school. That means that sometimes
there was a different peer than originally and
sometimes there was a group of peers interact-
ing in the play. The support person attending
the play sessions was always the same. She also
was the one completing the activity diaries.
4.3.1 Functionality
15 comments were given about the system
working well, where release 5 got the best scores.
Problems with the system were reported 16
times. Comments were given about rebooting
the system, loosing the commands, or problems
with activating them. Dissatisfaction with the
actions of the Lekbot was reported 5 times,
mainly about the delay between activating a
command and the activation of the robot. There
were also reports of improved accessibility of the
system, by finding a mobile piece of furniture
116
for the stand and by changing the angle of the
display.
4.3.2 Interaction
The project group chose not to give strict in-
structions on what to do in the play, just to let
everyone use the Lekbot at suitable level. Thus,
there was a variation in complexity of the com-
ments, as the headings in the activity diaries
gave a structure of open questions. The col-
lected, written comments were categorised in five
groups; Preparations for the Lekbot play, Ex-
plicit support from adult, Target child?s activity
and perception of the play, Peer?s activity and
perception of the play and Shared activity and
perception of the play between target child and
peer(s). The three latter are reported together
release by release.
Preparation for the Lekbot play occurred
mainly for Per?s group, where he and his peers
built different tracks for the robot to follow. Ex-
plicit support by adult is mentioned only for
Per?s group, where the adult chose target point
for the robot and she used the play for educa-
tional matters regarding letter teaching. She
also mediated between the children which im-
proved their cooperation. In the final sessions
Per initiated turn taking after being urged by
the adult.
4.3.3 Activity and perception
Target child?s activity and perception of the
play is mentioned a lot, especially for Per and
Greta. Most frequent among the comments are
those concerning Shared activity and perception
of the play between target child and peer(s).
Release 2: Per initiates turn taking, reacts
to the event followed by the activation of the
command on the display, protests when his peer
choses ?the wrong command?. Together they re-
peatedly perform turn taking and use Per?s dig-
ital communication device in the Lekbot activ-
ity. Hans and his peers make a tunnel and the
children give commands that make the robot go
through it. Greta has high expectations on the
play before the session. Repeatedly she is unwill-
ing to stop the play and she gives oral comments
to the activities of the robot.
Release 3: Per explores the commands and
what happens when using them to answer the
newly implemented supplementary questions.
Around Hans there is turn taking. Several chil-
dren are playing together and the children most
frequently choose the dance command. Greta
is excited and unwilling to stop the play. She
protests when the adult makes the choice for the
robot.
Release 4: Per shows the new commands for
his peer, and the children imitate the robot.
Per and his original peer chose one new peer
each. Interaction between the children takes
place through dancing and hand clapping. Hans
plays with the robot together with adults from
outside the preschool. Greta likes going back-
wards, turning and hitting things with the robot.
She starts telling her peer how to act by us-
ing the commands on the display and her paper
communication chart. Her peer enjoys follow-
ing Greta?s ?instructions? and she likes dancing.
There are repeated turn taking between them
and they enjoy to cooperate getting the robot to
move from one spot to another.
Release 5: Per plays with the new commands,
by himself. He finds strategies for the robot in
finding food. When there are more than two
children in the play, Per chooses to be the one
controlling the display. He cooperates more ?
waits for his turn and shows better understand-
ing for the other?s turn. All children repeatedly
use communication charts and Blissymbolics to
express themselves. They imitate the robot and
they act instead of it when it is out of order.
In Hans?s group there is dancing and looking
for food play. Turn taking takes place and all
children want to participate in the Lekbot play.
Greta decides whose turn it is to control the
robot. Her peer likes the play of finding food.
4.3.4 Satisfaction
Starting in release 3, the level of satisfaction
with the play session was noted in the activity
diary. The staff was asked to estimate how sat-
isfied the target child and the peer were on a
scale from 1 to 5, where 1 is the lowest and 5
the highest. This was done every time at some
117
pre-schools and some times at others. The ten-
dency is that the target children seem to be more
satisfied with the play than their peers from the
start of the play session. This is most protrud-
ing regarding the oldest pair. At release 4 where
Per and his peer interact as a group for the first
time, the scores suddenly are reversed so the Per
is perceived to 3 on the satisfactory scale and the
peer(s) at 5. In release 5 the scores get a more
even variation.
4.4 Video recordings
Most of the interviews with Talking Mats were
video recorded. The full analysis will be done
later in the project. The analysis of the video
recordings of the robot interaction is an ongoing
work were three of the project members partic-
ipate. This part of the work is time consuming
and only minor sequences are transcribed and
analysed so far. Through micro analysis the fine
grained interactional movements and the coop-
eration between the children and the teacher ap-
pears, as well as the joy of playing.
Figure 3 contains a segment from the tran-
scription. The participants are Per, his peer
Selma and his teacher Isa; and the Computer
and the Robot. In the excerpt we can see how
Per asks for Selma?s attention and with the help
of Isa and the communication map tells Selma
to take her turn, which is to make a new com-
mand for the robot to perform. Finally they
both dance to the music.
4.5 Conclusion
All target children have enjoyed the Lekbot play
from the beginning. The more commands and
abilities the robot has received the more appre-
ciated has the play become also by the peers.
Improved play and interaction skills can be ob-
served in varying degrees depending on the level
of each child. The Lekbot has been a nice and
fun artefact for the children to gather round and
it has given both the target children and their
peers experiences of playing with each other.
From Talking Mats interviews performed with
Per and Greta it was revealed that they had no
problems handling the computer display or see-
ing and hearing the display and the robot. Mak-
126 %gaze: Per looks at Selma
127 %move: Selma is standing on her knees, sits down
on her heels, keeps booths hands on her skirt
128 %gaze: Selma looks toward the mirror on the wall
129 %move: Per touches the left hand of Selma, keeps
his arm stretched when Selma moves a bit
131 %gaze: Isa looks at Per?s hand
132 *Selma: ????????
133 %comment : Selma is singing while Per stretches to-
ward her left hand
134 %gesture: Isa draws the pointing map closer
135 %gaze: Per looks down at the map
136 %gaze: Selma looks down at the map
137 *Per : ????????
138 %move: Selma stands up on her knee, departs on a
movement forward
139 *Isa: eh::: (0.3) your turn (0.3) Selma?s turn
140 %gesture: Isa moves her finger back and forth over
the 6th picture on the map
141 %gesture: Isa rests her finger at the picture, then
withdraws it
142 %gesture: Per points at the map
143 %move: Selma moves toward the screen
144 (2.1)
145 %action: Selma makes a fast press at the screen
146 %gaze: Per looks at the screen
147 *Selma: dance: my king ????????
148 %move: Selma moves left with arms swinging, bends
forward, landing on hands and knees
149 %action: Per looks at Selma, smiles
150 *Computer : dance
151 *Selma: mi:ine ?: ?: ?(1.8)?
152 *Robot : okay I gladly dance
153 (1.0)
154 *Robot : plays music 11 sec
155 %comment : both children are dancing, Selma on her
knees and Per sitting down
Figure 3: An example transcription segment, trans-
lated to English
ing the same interview with Hans was not feasi-
ble, though the project group experienced that
he seemed to deal pretty well with the system,
although he needed a little more support than
the two other children, who were able to control
the toy autonomously. More results will be pre-
sented when the video sequences are analysed,
later on in the project.
5 Acknowledgements
We are grateful to 5 anonymous referees for
their valuable comments. The Lekbot project
is financed by Vinnova, and Acapela has kindly
provided us with speech synthesis.
118
References
K. Arent and M. Wnuk. 2007. Remarks on be-
haviours programming of the interactive therapeu-
tic robot Koala based on fuzzy logic techniques.
In First KES International Symposium on Agent
and Multi-Agent Systems: Technologies and Ap-
plications, Wroclaw, Poland.
E. Berglund and M. Eriksson. 2000. Communicative
development in Swedish children 16?28 months
old: The Swedish early communicative develop-
ment inventory ? words and sentences. Scandina-
vian Journal of Psychology, 41(2):133?144.
Jonas Beskow, Olov Engwall, Bj?rn Granstr?m, and
Preben Wik. 2004. Design strategies for a virtual
language tutor. In INTERSPEECH 2004.
Dorothy Bishop, Eva Holmberg, and Eva Lund?lv.
1998. TROG: Test for Reception of Grammar
(Swedish version). SIH L?romedel.
J. Brodin and P. Lindstrand. 2007. Perspektiv p?
IKT och l?rande f?r barn, ungdomar och vuxna
med funktionshinder. Studentlitteratur.
Stina Ericsson. 2005. Information Enriched Con-
stituents in Dialogue. Ph.D. thesis, University of
Gothenburg, Gothenburg, Sweden.
M. Eriksson and E. Berglund. 1999. Swedish early
communicative development inventory ? words
and gestures. First Language, 19(55):55?90.
M. Granlund and C. Olsson. 1998. Familjen och
habiliteringen. Stiftelsen ALA.
M. J. C. Hidecker, N. Paneth, P. Rosenbaum, R. D.
Kent, J. Lillie, and B. Johnson. 2009. Develop-
ment of the Communication Function Classifica-
tion System (CFCS) for individuals with cerebral
palsy. Developmental Medicine and Child Neurol-
ogy, 51(Supplement s2):48.
B. Knutsdotter Olofsson. 1992. I lekens v?rld.
Almqvist och Wiksell.
H. Kozima, C. Nakagawa, and Y. Yasuda. 2007.
Children-robot interaction: a pilot study in autism
therapy. Progress in Brain Research, 164:385?400.
Staffan Larsson. 2002. Issue-based Dialogue Man-
agement. Ph.D. thesis, Department of Linguistics,
University of Gothenburg, Sweden.
C.H. Lee, K. Kim, C. Breazeal, and R.W. Picard.
2008. Shybot: Friend-stranger interaction for chil-
dren living with autism. In CHI2008, Florence,
Italy.
Peter Ljungl?f, Staffan Larsson, Katarina M?hlen-
bock, and Gunilla Thunberg. 2009. TRIK: A talk-
ing and drawing robot for children with commu-
nication disabilities. In Nodalida?09: 17th Nordic
Conference of Computational Linguistics. Short
paper and demonstration.
Seymour Papert. 1993. Mindstorms: Children,
Computers, and Powerful Ideas. Basic Books.
B. Robins, K. Dautenhahn, R. te Boekhorst, and
C.L. Nehaniv. 2008. Behaviour delay and ex-
pressiveness in child-robot interactions: a user
study on interaction kinesics. In HRI?08, 3rd
ACM/IEEE International Conference on Human
Robot Interaction, Amsterdam, Netherlands.
J. Saldien, K. Goris, B. Verrelst, R. Van Ham, and
D. Lefeber. 2006. ANTY: The development of
an intelligent huggable robot for hospitalized chil-
dren. In CLAWAR, 9th International Conference
on Climbing and Walking Robots, Brussels, Bel-
gium.
Stephanie Seneff, Chao Wang, and Julia Zhang.
2004. Spoken conversational interaction for lan-
guage learning. In InSTIL/ICALL 2004 Sympo-
sium on Computer Assisted Learning: NLP and
Speech Technologies in Advanced Language Learn-
ing Systems.
E. Vallduv?. 1992. The Informational Component.
Garland.
119
Proceedings of the SIGDIAL 2013 Conference, pages 354?356,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Integration and test environment for an in-vehicle dialogue system in the
SIMSI project
Staffan Larsson, Sebastian Berlin
University of Gothenburg
Box 200
SE-405 30 Gothenburg
Sweden
sl@ling.gu.se
sebastian.berlin@gu.se
Anders Eliasson
Mecel AB
Box 140 44
SE-400 20 Gothenburg
Sweden
anders.eliasson@mecel.se
Fredrik Kronlid
Talkamatic AB
Fo?rsta la?nggatan 18
SE-413 28 Gothenburg
Sweden
fredrik@talkamatic.se
Abstract
The goal of the SIMSI (Safe In-vehicle
Multimodal Speech Interaction) project is
threefold. Firstly, to integrate a dialogue
system for menu-based dialogue with a
GUI-driven in-vehicle infotainment sys-
tem. Secondly, to further improve the in-
tegrated system with respect to driver dis-
traction, thus making the system safer to
use while driving. Thirdly, to verify that
the resulting system decreases visual dis-
traction and cognitive load during interac-
tion. This demo paper describes the inte-
gration of the two existing systems, and
the test environment designed to enable
evaluation of the system.
1 Background
1.1 Driver distraction and safety
Driver distraction is one common cause of acci-
dents, and is often caused by the driver interact-
ing with technologies such as mobile phones, me-
dia players or navigation systems. The so-called
100-car study (Neale et al, 2005) revealed that
secondary task distraction is the largest cause of
driver inattention, and that the handling of wire-
less devices is the most common secondary task.
The goal of SIMSI is to design systems which en-
able safe interaction with technologies in vehicles,
by reducing the cognitive load imposed by the in-
teraction and minimizing head-down time.
1.2 The Talkamatic Dialogue Manager
Based on Larsson (2002) and later work, Talka-
matic AB has developed the Talkamatic Dialogue
Manager (TDM) with the goal of being the most
competent and usable dialogue manager on the
market, both from the perspective of the user and
from the perspective of the HMI developer. TDM
provides a general interaction model founded in
human interaction patterns, resulting in a high de-
gree of naturalness and flexibility which increases
usability. Also, TDM reduces complexity for de-
velopers and users, helping them to reach their
goals faster and at a lower cost.
A major problem with the current state-of-the-
art in-vehicle spoken dialogue systems is that they
are either too simplistic to be useful to the end
user, or alternatively that they are fairly sophisti-
cated but unmanageable for the manufacturer due
to the size and complexity of the implementation.
TDM offers sophisticated multi-modal interaction
management solutions which allow for easy modi-
fication and development, allowing interaction de-
signers to easily explore new solutions and re-
ducing overhead for new dialogue applications in
terms of code and development man-hours.
TDM deals with several interaction patterns
which are basic to human-human linguistic in-
teraction, and offers truly integrated multimodal-
ity which allows user to freely switch between
(or combine) modalities. All these solutions are
domain-independent which means that they need
not be implemented in each application. Using
Talkamatic technology, dialogue behaviour can be
altered without touching application properties,
and application properties can be updated without
touching the dialogue logic. This makes testing of
different dialogue strategies, prompts etc. consid-
erably quicker and easier than when using regular
state-machine-based dialogue systems.
In addition, as the dialogue strategy is separated
from the application logic, development time for
new dialogue applications can be significantly re-
duced. Furthermore, the developer designing the
application does not need to be a dialogue expert
as the dialogue design is built into the dialogue
manager.
354
1.3 Integrated multimodality in TDM
There are reasons to believe that multi-modal in-
teraction is more efficient and less distracting than
uni-modal interaction (Oviatt et al, 2004). TDM
supports multi-modal interaction where voice out-
put and input (VUI) is combined with a traditional
menu-based GUI with graphical output and hap-
tic input. In cases where a GUI already exists,
TDM can replace the GUI-internal interaction en-
gine, thus adding speech while keeping the origi-
nal GUI design. All system output is realized both
verbally and graphically, and the user can switch
freely between uni-modal (voice or screen/keys)
and multi-modal interaction.
To facilitate the browsing of lists (a well known
interaction problem for dialogue systems), Talka-
matic has developed its Voice-Cursor technology1
(Larsson et al, 2011). It allows a user to browse
a list in a multi-modal dialogue system without
looking at a screen and without being exposed to
large chunks of readout information.
A crucial property of TDM?s integrated multi-
modality is the fact that it enables the driver of a
vehicle to carry out all interactions without ever
looking at the screen, either by speaking to the sys-
tem, by providing haptic input, or by combining
the two. We are not aware of any current mul-
timodal in-vehicle dialogue system offering this
capability. Additional information is available at
www.talkamatic.se.
1.4 Mecel Populus
While TDM offers full menu-based multimodal
interaction, the GUI itself is fairly basic and does
not match the state of the art when it comes to
graphical design. By contrast, Mecel Populus is
an commercial-grade HMI (Human Machine In-
terface) with professionally designed visual out-
put. The Mecel Populus suite is a complete tool
chain for designing, developing and deploying
user interfaces for distributed embedded systems.
It minimizes the time and cost of producing eye-
catching, full-featured HMIs.
The Mecel Populus concept has several unique
features compared to traditional HMI develop-
ment. These features, when combined, remove the
barriers that traditionally exist between the peo-
ple working with requirements, system engineer-
ing, HMI design and implementation. An HMI
is created and verified in Mecel Populus Editor
1Patent Pending
Figure 1: SIMSI system overview
without having to write any software. The HMI is
then downloaded to the target environment where
Mecel Populus Engine executes it. Mecel Popu-
lus has been designed for the automotive industry
to deliver high performance user interfaces with a
short time-to-market and to enable efficient soft-
ware life cycle management. Additional informa-
tion is available at www.mecel.se/products.
2 System integration
The goal of this part of SIMSI is to provide a
project-specific integration of TDM and the Me-
cel Populus platform. In this way, we estab-
lish a commercial-grade HMI for experiments and
demonstrations. At the same time, the integration
of TDM and Populus increases the commercial po-
tential of both platforms, since it integrates a state-
of-the-art HMI tool without voice capabilities and
a dialogue manager with limited graphical capa-
bilities.
The major problem in integrating Populus and
TDM is that both systems keep track of the cur-
rent state of the interaction and manage transitions
between states resulting from user or system ac-
tions. Hence, there is a need to keep the systems in
sync at all times. This is managed by a Transition
Queue (TQ) module which keeps a lock which can
be grabbed by either system at any time, unless
it has already been grabbed by the other system.
The systems then enter into a master-slave rela-
tion where the master is the system which owns
the lock. The master tells the slave how the in-
teraction state is to be updated, and the slave only
waits for messages from the master until the lock
has been returned to the TQ.
355
Figure 2: SIMSI test environment overview
3 Test environment
The purpose of this part of the project is to conduct
ecologically valid test of the applications, and to
begin and continue an iterative development cycle
of testing - evaluation - development. We want to
find the best interaction solutions in cases where it
is not intuitively clear what is best. This involves
implementing variants of a behaviour, testing them
on naive users, collecting data from these interac-
tions, and establishing statistically significant re-
sults based on the collected data.
The test environment consists of two parts, apart
from the dialogue system: a driving simulator
(SCANeR from Octal) and an eye tracker (Smart
Eye Pro from Smarteye). In later tests we will also
include instruments for measuring cognitive load.
In our setup we have three monitors, giving the
user a wide field of view. We also have a gaming
steering wheel, including pedals, gear lever and a
driver?s seat. These are used mainly to control the
driving simulator, but there are also a number of
buttons on the steering wheel which are used to
browse the menus in the HMI and as Push-to-talk
(PTT). An Android tablet (Asus Eee Pad Trans-
former TF101) showing the HMI GUI is placed in
front of the user, trying to match the position of a
display in a car. Both TDM and Populus run on
the same desktop computer as the driving simula-
tor, and a Populus Android app runs on the tablet.
The app allows the user to select items by tapping
them, as well as scrolling in lists in normal smart
phone fashion. The eye tracker runs on a sepa-
rate desktop computer, as it requires a substantial
amount of processing power.
Figure 3: SIMSI test environment in action
Studio software that comes with the driving
simulator is used to design and run scenarios. The
scenarios govern how autonomous traffic should
behave and events, such as weather change and
the state of traffic signals. The simulator logs data
for the environment and each vehicle. Data like
lane deviation (where in the lane the vehicle is)
and how the user handles instruments, e.g. steer-
ing wheel and pedals, can be used to measure cog-
nitive load. At a later stage this kind of data can
also be used to trigger behaviour in the dialogue
system.
The eye tracker uses three cameras to track the
user?s eyes and head at 60 Hz. The cameras are
spaced to give good tracking in the middle of the
scene, where you typically look when you?re driv-
ing, and at the same time capture head movement
to the side. As we are interested in when the user is
looking at the tablet, we placed one of the cameras
specifically to improve eye tracking in this area.
References
Staffan Larsson, Alexander Berman, and Jessica
Villing. 2011. Adding a speech cursor to a mul-
timodal dialogue system. In INTERSPEECH 2011,
12th Annual Conference of the International Speech
Communication Association, Florence, Italy, 2011,
pages 3319?3320.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, Go?teborg University.
Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer,
Jeremy Sudweeks, and Michael Goodman. 2005.
An overview of the 100-car naturalistic study and
findings.
Sharon L. Oviatt, Rachel Coulston, and Rebecca
Lunsford. 2004. When do we interact multi-
modally?: cognitive load and multimodal commu-
nication patterns. In ICMI, pages 129?136.
356
