Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 571?580, Prague, June 2007. c?2007 Association for Computational Linguistics
Cross-lingual Distributional Profiles of Concepts
for Measuring Semantic Distance
Saif Mohammad? Iryna Gurevych? Graeme Hirst? Torsten Zesch?
?Dept. of Computer Science
University of Toronto
Toronto, Canada
{smm,gh}@cs.toronto.edu
?Ubiquitous Knowledge Processing Group
Darmstadt University of Technology
Darmstadt, Germany
{gurevych,zesch}@tk.informatik.tu-darmstadt.de
Abstract
We present the idea of estimating seman-
tic distance in one, possibly resource-poor,
language using a knowledge source in an-
other, possibly resource-rich, language. We
do so by creating cross-lingual distributional
profiles of concepts, using a bilingual lexi-
con and a bootstrapping algorithm, but with-
out the use of any sense-annotated data or
word-aligned corpora. The cross-lingual
measures of semantic distance are evaluated
on two tasks: (1) estimating semantic dis-
tance between words and ranking the word
pairs according to semantic distance, and
(2) solving Reader?s Digest ?Word Power?
problems. In task (1), cross-lingual mea-
sures are superior to conventional monolin-
gual measures based on a wordnet. In task
(2), cross-lingual measures are able to solve
more problems correctly, and despite scores
being affected by many tied answers, their
overall performance is again better than the
best monolingual measures.
1 Introduction
Accurately estimating the semantic distance be-
tween concepts or between words in context has per-
vasive applications in computational linguistics, in-
cluding machine translation, information retrieval,
speech recognition, spelling correction, and text cat-
egorization (see Budanitsky and Hirst (2006) for dis-
cussion), and it is becoming clear that basing such
measures on a combination of corpus statistics with
a knowledge source, such as a dictionary, published
thesaurus, or WordNet, can result in higher accu-
racies (Mohammad and Hirst, 2006b). This is be-
cause such knowledge sources capture semantic in-
formation about concepts and, to some extent, world
knowledge. They also act as sense inventories for
the words in a language.
However, applying algorithms for semantic dis-
tance to most languages is hindered by the lack of
linguistic resources. In this paper, we propose a
new method that allows us to compute semantic dis-
tance in a possibly resource-poor language by seam-
lessly combining its text with a knowledge source
in a different, preferably resource-rich, language.
We demonstrate the approach by combining German
text with an English thesaurus to create English?
German distributional profiles of concepts, which in
turn will be used to measure the semantic distance
between German words.
Two classes of methods have been used in deter-
mining semantic distance. Semantic measures of
concept-distance, such as those of Jiang and Con-
rath (1997) and Resnik (1995), rely on the structure
of a knowledge source, such as WordNet, to deter-
mine the distance between two concepts defined in
it (see Budanitsky and Hirst (2006) for a survey).
Distributional measures of word-distance1, such
as cosine and ?-skew divergence (Lee, 2001), deem
1Many distributional approaches represent the sets of con-
texts of the target words as points in multidimensional co-
occurrence space or as co-occurrence distributions. A measure,
such as cosine, that captures vector distance or a measure, such
as ?-skew divergence, that captures distance between distribu-
tions is then used to measure distributional distance. We will
therefore refer to these measures as distributional measures.
571
two words to be closer or less distant if they occur in
similar contexts (see Mohammad and Hirst (2005)
for a comprehensive survey).
Distributional measures rely simply on raw text
and possibly some shallow syntactic processing.
They do not require any other manually-created re-
source, and tend to have a higher coverage. How-
ever, by themselves they perform poorly when com-
pared to semantic measures (Mohammad and Hirst,
2006b) because when given a target word pair
we usually need the distance between their closest
senses, but distributional measures of word-distance
tend to conflate the distances between all possible
sense pairs. Latent semantic analysis (LSA) (Lan-
dauer et al, 1998) has also been used to measure dis-
tributional distance with encouraging results (Rapp,
2003). However, it too measures the distance be-
tween words and not senses. Further, the dimen-
sionality reduction inherent to LSA has the effect of
making the predominant sense more dominant while
de-emphasizing the other senses. Therefore, an
LSA-based approach will also conflate information
from the different senses, and even more emphasis
will be placed on the predominant senses. Given the
semantically close target nouns play and actor, for
example, a distributional measure will give a score
that is some sort of a dominance-based average of
the distances between their senses. The noun play
has the predominant sense of ?children?s recreation?
(and not ?drama?), so a distributional measure will
tend to give the target pair a large (and thus erro-
neous) distance score. Also, distributional word-
distance approaches need to create large V ?V co-
occurrence and distance matrices, where V is the
size of the vocabulary (usually at least 100,000).2
Mohammad and Hirst (2006b) proposed a way
of combining written text with a published the-
saurus to measure distance between concepts (or
word senses) using distributional measures, thereby
eliminating sense-conflation and achieving results
better than the simple word-distance measures and
indeed also most of the WordNet-based semantic
measures. We called these measures distributional
measures of concept-distance. Concept-distance
2LSA is especially expensive as singular value decomposi-
tion, a key component for dimensionality reduction, requires
computationally intensive matrix operations; making it less
scalable to large amounts of text (Gorman and Curran, 2006).
measures can be used to measure distance between
a word pair by choosing the distance between their
closest senses. Thus, even though ?children?s recre-
ation? is the predominant sense of play, the ?drama?
sense is much closer to actor and so their dis-
tance will be chosen. These distributional concept-
distance approaches need to create only V ?C co-
occurrence and C?C distance matrices, where C is
the number of categories or senses (usually about
1000). It should also be noted that unlike the best
WordNet-based measures, distributional measures
(both word- and concept-distance ones) can be used
to estimate not just semantic similarity but also se-
mantic relatedness?useful in many tasks includ-
ing information retrieval. However, the high-quality
thesauri and (to a much greater extent) WordNet-like
resources that these methods require do not exist for
most of the 3000?6000 languages in existence today
and they are costly to create.
In this paper, we introduce cross-lingual distri-
butional measures of concept-distance, or simply
cross-lingual measures, that determine the distance
between a word pair belonging to a resource-poor
language using a knowledge source in a resource-
rich language and a bilingual lexicon3. We will use
the cross-lingual measures to calculate distances be-
tween German words using an English thesaurus and
a German corpus. Although German is not resource-
poor per se, Gurevych (2005) has observed that the
German wordnet GermaNet (Kunze, 2004) (about
60,000 synsets) is less developed than the English
WordNet (Fellbaum, 1998) (about 117,000 synsets)
with respect to the coverage of lexical items and lex-
ical semantic relations represented therein. On the
other hand, substantial raw corpora are available for
the German language. Crucially for our evaluation,
the existence of GermaNet alows comparison of our
cross-lingual approach with monolingual ones.
2 Monolingual Distributional Measures
In order to set the context for cross-lingual concept-
distance measures (Section 3), we first summarize
monolingual distributional approaches, with a focus
on distributional concept-distance measures.
3For most languages that have been the subject of academic
study, there exists at least a bilingual lexicon mapping the core
vocabulary of that language to a major world language and a
corpus of at least a modest size.
572
2.1 Word-distance
Words that occur in similar contexts tend to be se-
mantically close. In our experiments, we defined the
context of a target word, its co-occurring words, to
be ?5 words on either side (but not crossing sen-
tence boundaries). The set of contexts of a target
word is usually represented by the strengths of as-
sociation of the target with its co-occurring words,
which we refer to as the distributional profile (DP)
of the word. Here is a constructed example DP of
the word star:
DP of a word
star: space 0.28, movie 0.2, famous 0.13,
light 0.09, rich 0.04, . . .
Simple counts are made of how often the target word
co-occurs with other words in text and how often
the words occur individually. A suitable statistic,
such as pointwise mutual information (PMI), is then
applied to these counts to determine the strengths
of association between the target and co-occurring
words. The distributional profiles of two target
words represent their contexts as points in multi-
dimensional word-space. A suitable distributional
measure (for example, cosine) gives the distance be-
tween the two points, and thereby an estimate of the
semantic distance between the target words.
2.2 Concept-distance
In Mohammad and Hirst (2006b), we show how dis-
tributional profiles of concepts (DPCs) can be used
to measure semantic distance. Below are the DPCs
or DPs of two senses of the word star (the senses
or concepts themselves are glossed by a set of near-
synonymous words, placed in parentheses):
DPs of concepts
?celestial body? (celestial body,
sun, . . . ): space 0.36, light 0.27,
constellation 0.11, . . .
?celebrity? (celebrity, hero, . . . ):
famous 0.24, movie 0.14, rich 0.14, . . .
Thus the profiles of two target concepts represent
their contexts as points in multi-dimensional word-
space. A suitable distributional measure (for exam-
ple, cosine) can then be used to give the distribu-
tional distance between the two concepts in the same
way that distributional word-distance is measured.
But to calculate the strength of association of
a concept with co-occurring words, in order to
create DPCs, we must determine the number of
times a word used in that sense co-occurs with
surrounding words. In Mohammad and Hirst
(2006a), we proposed a way to determine these
counts without the use of sense-annotated data.
Briefly, a word?category co-occurrence matrix
(WCCM) is created having English word types
wen as one dimension and English thesaurus cat-
egories cen as another. We used the Macquarie
Thesaurus (Bernard, 1986) both as a very coarse-
grained sense inventory and a source of possibly
ambiguous English words that together unam-
biguously represent each category (concept). The
WCCM is populated with co-occurrence counts
from a large English corpus (we used the British
National Corpus (BNC)). A particular cell mi j,
corresponding to word weni and concept cenj , is
populated with the number of times weni co-occurs
(in a window of ?5 words) with any word that has
cenj as one of its senses (i.e., weni co-occurs with any
word listed under concept cenj in the thesaurus).
cen1 c
en
2 . . . c
en
j . . .
wen1 m11 m12 . . . m1 j . . .
wen2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
weni mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
This matrix, created after a first pass of the cor-
pus, is the base word?category co-occurrence ma-
trix (base WCCM) and it captures strong associa-
tions between a sense and co-occurring words.4 This
is similar to how Yarowsky (1992) identifies words
that are indicative of a particular sense of the target.
We know that words that occur close to a target
word tend to be good indicators of its intended sense.
Therefore, we make a second pass of the corpus, us-
ing the base WCCM to roughly disambiguate the
words in it. For each word, the strength of associ-
ation of each of the words in its context (?5 words)
4From the base WCCM we can determine the number of
times a word w and concept c co-occur, the number of times
w co-occurs with any concept, and the number of times c co-
occurs with any word. A statistic such as PMI can then give the
strength of association between w and c.
573
with each of its senses is summed. The sense that
has the highest cumulative association is chosen as
the intended sense. A new bootstrapped WCCM
is created such that each cell mi j, corresponding to
word weni and concept cenj , is populated with the
number of times weni co-occurs with any word used
in sense cenj .
Mohammad and Hirst (2006a) used the DPCs cre-
ated from the bootstrapped WCCM to attain near-
upper-bound results in the task of determining word
sense dominance. Unlike the McCarthy et al (2004)
dominance system, our approach can be applied
to much smaller target texts (a few hundred sen-
tences) without the need for a large similarly-sense-
distributed text5. In Mohammad and Hirst (2006a),
the DPC-based monolingual distributional measures
of concept-distance were used to rank word pairs
by their semantic similarity and to correct real-
word spelling errors, attaining markedly better re-
sults than monolingual distributional measures of
word-distance. In the spelling correction task, the
distributional concept-distance measures performed
better than all WordNet-based measures as well, ex-
cept for the Jiang and Conrath (1997) measure.
3 Cross-lingual Distributional Measures
We now describe how distributional measures of
concept-distance can be used in a cross-lingual
framework to determine the distance between words
in (resource-poor) language L1 by combining its text
with a thesaurus in (resource-rich) language L2, us-
ing an L1?L2 bilingual lexicon. We will compare
this approach with the best monolingual approaches;
the smaller the loss in performance, the more ca-
pable the algorithm is of overcoming ambiguities
in word translation. An evaluation, therefore, re-
quires an L1 that in actuality has adequate knowl-
edge sources. Therefore we chose German to stand
in as the resource-poor language L1 and English as
the resource-rich L2; the monolingual evaluation in
German will use GermaNet. The remainder of the
paper describes our approach in terms of German
and English, but the algorithm itself is language in-
dependent.
5The McCarthy et al (2004) system needs to first gener-
ate a distributional thesaurus from the target text (if it is large
enough?a few million words) or from another large text with a
distribution of senses similar to the target text.
3.1 Concept-distance
Given a German word wde in context, we use a
German?English bilingual lexicon to determine its
different possible English translations. Each En-
glish translation wen may have one or more possi-
ble coarse senses, as listed in an English thesaurus.
These English thesaurus concepts (cen) will be re-
ferred to as cross-lingual candidate senses of the
German word wde.6 Figure 1 depicts examples.7
As in the monolingual distributional measures,
the distance between two concepts is calculated by
first determining their DPs. However, in the cross-
lingual approach, a concept is now glossed by near-
synonymous words in an English thesaurus, whereas
its profile is made up of the strengths of associ-
ation with co-occurring German words. Here are
constructed example cross-lingual DPs of the two
senses of star:
Cross-lingual DPs of concepts
?celestial body? (celestial body, sun,
. . . ): Raum 0.36, Licht 0.27,
Konstellation 0.11, . . .
?celebrity? (celebrity, hero, . . . ):
beru?hmt 0.24, Film 0.14, reich 0.14, . . .
In order to calculate the strength of association, we
must first determine individual word and concept
counts, as well as their co-occurrence counts.
3.2 Cross-lingual word?category
co-occurrence matrix
We create a cross-lingual word?category co-
occurrence matrix with German word types wde as
one dimension and English thesaurus concepts cen
6Some of the cross-lingual candidate senses of wde might
not really be senses of wde (e.g., ?celebrity?, ?river bank?, and
?judiciary? in Figure 1). However, as substantiated by experi-
ments in Section 4, our algorithm is able to handle the added
ambiguity.
7Vocabulary of German words needed to understand this dis-
cussion: Bank: 1. financial institution, 2. bench (furniture);
beru?hmt: famous; Film: movie (motion picture); Himmels-
ko?rper: heavenly body; Konstellation: constellation; Licht:
light; Morgensonne: morning sun; Raum: space; reich: rich;
Sonne: sun; Star: star (celebrity); Stern: star (celestial body)
574
}
star bank
river
bank
bench
furniture
judiciary
celestial body
celebrity
}
institution
financial }
Stern Bank wde
cen
wen
Figure 1: The cross-lingual candidate senses of Ger-
man words Stern and Bank.
as another.
cen1 c
en
2 . . . c
en
j . . .
wde1 m11 m12 . . . m1 j . . .
wde2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wdei mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
The matrix is populated with co-occurrence counts
from a large German corpus; we used the newspaper
corpus, taz8 (Sep 1986 to May 1999; 240 million
words). A particular cell mi j, corresponding to word
wdei and concept cenj , is populated with the number
of times the German word wdei co-occurs (in a win-
dow of ?5 words) with any German word having cenj
as one of its cross-lingual candidate senses. For ex-
ample, the Raum??celestial body? cell will have the
sum of the number of times Raum co-occurs with
Himmelsko?rper, Sonne, Morgensonne, Star, Stern,
and so on (see Figure 2). We used the Macquarie
Thesaurus (Bernard, 1986) (about 98,000 words)
for our experiments. The possible German trans-
lations of an English word were taken from the
German?English bilingual lexicon BEOLINGUS9
(about 265,000 entries).
This base word?category co-occurrence matrix
(base WCCM), created after a first pass of the cor-
pus captures strong associations between a category
(concept) and co-occurring words. For example,
even though we increment counts for both Raum?
?celestial body? and Raum??celebrity? for a particu-
lar instance where Raum co-occurs with Star, Raum
will co-occur with a number of words such as Him-
melsko?rper, Sonne, and Morgensonne that each have
the sense of celestial body in common (see Figure
2), whereas all their other senses are likely different
8http://www.taz.de
9http://dict.tu-chemnitz.de
... }
... }
}
sun
Sonne Morgensonne Star
celestial body
celestial body
Stern
star wen
wde
cen
Himmelsko?rper
Figure 2: Words having ?celestial body? as one of
their cross-lingual candidate senses.
and distributed across the set of concepts. There-
fore, the co-occurrence count of Raum and ?celestial
body? will be relatively higher than that of Raum and
?celebrity?.
As in the monolingual case, a second pass of
the corpus is made to disambiguate the (German)
words in it. For each word, the strength of associ-
ation of each of the words in its context (?5 words)
with each of its cross-lingual candidate senses is
summed. The sense that has the highest cumula-
tive association with co-occurring words is chosen
as the intended sense. A new bootstrapped WCCM
is created by populating each cell mi j, correspond-
ing to word wdei and concept cenj , with the number of
times the German word wdei co-occurs with any Ger-
man word used in cross-lingual sense cenj . A statistic
such as PMI is then applied to these counts to deter-
mine the strengths of association between a target
concept and co-occurring words, giving the distri-
butional profile of the concept.
Following the ideas described above, Mohammad
et al (2007) created Chinese?English DPCs from
Chinese text, a Chinese?English bilingual lexicon,
and an English thesaurus. They used these DPCs to
implement an unsupervised na??ve Bayes word sense
classifier that placed first among all unsupervised
systems taking part in the Multilingual Chinese?
English Lexical Sample Task (task #5) of SemEval-
07 (Jin et al, 2007).
4 Evaluation
We evaluated the newly proposed cross-lingual dis-
tributional measures of concept-distance on the tasks
of (1) measuring semantic distance between German
words and ranking German word pairs according to
semantic distance, and (2) solving German ?Word
Power? questions from Reader?s Digest. In order
to compare results with state-of-the-art monolingual
approaches we conducted experiments using Ger-
575
(Cross-lingual) Distributional Measures (Monolingual) GermaNet Measures
Information Content?based Lesk-like
?-skew divergence (Lee, 2001) (ASD) Jiang and Conrath (1997) (JC) hypernym pseudo-gloss (HPG)
cosine (Schu?tze and Pedersen, 1997) (Cos) Lin (1998b) (LinGN) radial pseudo-gloss (RPG)
Jensen-Shannon divergence (JSD) Resnik (1995) (Res)
Lin?s measure (1998a) (Lindist)
Table 1: Distance measures used in our experiments.
Dataset Year Language # pairs PoS Scores # subjects Correlation
Gur65 2005 German 65 N discrete {0,1,2,3,4} 24 .810
Gur350 2006 German 350 N, V, A discrete {0,1,2,3,4} 8 .690
Table 2: Comparison of datasets used for evaluating semantic distance in German.
maNet measures as well. The specific distributional
measures10 and GermaNet-based measures we used
are listed in Table 1. The GermaNet measures are
of two kinds: (1) information content measures,11
and (2) Lesk-like measures that rely on n-gram over-
laps in the glosses of the target senses, proposed by
Gurevych (2005)12.
The cross-lingual measures combined the German
newspaper corpus taz with the English Macquarie
Thesaurus using the German?English bilingual lex-
icon BEOLINGUS. Multi-word expressions in the
thesaurus and the bilingual lexicon were ignored.
We used a context of ?5 words on either side of the
target word for creating the base and bootstrapped
WCCMs. No syntactic pre-processing was done,
nor were the words stemmed, lemmatized, or part-
of-speech tagged.
4.1 Measuring distance in word pairs
4.1.1 Data
A direct approach to evaluate distance measures is
to compare them with human judgments. Gurevych
10JSD and ASD calculate the difference in distributions of
words that co-occur with the targets. Lindist (distributional
measure) and LinGN (GermaNet measure) follow from Lin?s
(1998b) information-theoretic definition of similarity.
11Information content measures rely on finding the lowest
common subsumer (lcs) of the target synsets in a hypernym hi-
erarchy and using corpus counts to determine how specific or
general this concept is. In general, the more specific the lcs is
and the smaller the difference of its specificity with that of the
target concepts, the closer the target concepts are.
12As GermaNet does not have glosses for synsets, Gurevych
(2005) proposed a way of creating a bag-of-words-type pseudo-
gloss for a synset by including the words in the synset and in
synsets close to it in the network.
(2005) and Zesch et al (2007) asked native German
speakers to mark two different sets of German word
pairs with distance values. Set 1 (Gur65) consists
of a German translation of the English Rubenstein
and Goodenough (1965) dataset. It has 65 noun?
noun word pairs. Set 2 (Gur350) is a larger dataset
containing 350 word pairs made up of nouns, verbs,
and adjectives. The semantically close word pairs
in Gur65 are mostly synonyms or hypernyms (hy-
ponyms) of each other, whereas those in Gur350
have both classical and non-classical relations (Mor-
ris and Hirst, 2004) with each other. Details of these
semantic distance benchmarks13 are summarized
in Table 2. Inter-subject correlations are indicative
of the degree of ease in annotating the datasets.
4.1.2 Results and Discussion
Word-pair distances determined using different
distance measures are compared in two ways with
the two human-created benchmarks. The rank order-
ing of the pairs from closest to most distant is evalu-
ated with Spearman?s rank order correlation ?; the
distance judgments themselves are evaluated with
Pearson?s correlation coefficient r. The higher the
correlation, the more accurate the measure is. Spear-
man?s correlation ignores actual distance values af-
ter a list is ranked?only the ranks of the two sets
of word pairs are compared to determine correla-
tion. On the other hand, Pearson?s coefficient takes
into account actual distance values. So even if two
lists are ranked the same, but one has distances be-
13The datasets are publicly available at:
http://www.ukp.tu-darmstadt.de/data/semRelDatasets
576
tween consecutively-ranked word-pairs more in line
with human-annotations of distance than the other,
then Pearson?s coefficient will capture this differ-
ence. However, this makes Pearson?s coefficient
sensitive to outlier data points, and so one must in-
terpret the Pearson correlations with caution.
Table 3 shows the results.14 Observe that on both
datasets and by both measures of correlation, cross-
lingual measures of concept-distance perform not
just as well as the best monolingual measures, but in
fact better. In general, the correlations are lower for
Gur350 as it contains cross-PoS word pairs and non-
classical relations, making it harder to judge even
by humans (as shown by the inter-annotator corre-
lations for the datasets in Table 2). Considering
Spearman?s rank correlation, ?-skew divergence and
Jensen-Shannon divergence perform best on both
datasets. The correlations of cosine and Lindist are
not far behind. Amongst the monolingual GermaNet
measures, radial pseudo-gloss performs best. Con-
sidering Pearson?s correlation, Lindist performs best
overall and radial pseudo-gloss does best amongst
the monolingual measures. Thus, we see that on
both datasets and as per both measures of correla-
tion, the cross-lingual measures perform not just as
well as the best monolingual measures, but indeed
slightly better.
4.2 Solving word choice problems from
Reader?s Digest
4.2.1 Data
Issues of the German edition of Reader?s Digest
include a word choice quiz called ?Word Power?.
Each question has one target word and four alter-
native words or phrases; the objective is to pick the
alternative that is most closely related to the target.
The correct answer may be a near-synonym of the
target or it may be related to the target by some other
classical or non-classical relation (usually the for-
mer). For example:15
Duplikat (duplicate)
a. Einzelstu?ck (single copy) b. Doppelkinn (double chin)
c. Nachbildung (replica) d. Zweitschrift (copy)
Our approach to evaluating distance measures fol-
14In Table 3, all values are statistically significant at the 0.01
level (2-tailed), except for the one in italic (0.212), which is
significant at the 0.05 level (2-tailed).
15English translations are in parentheses.
lows that of Jarmasz and Szpakowicz (2003), who
evaluated semantic similarity measures through their
ability to solve synonym problems (80 TOEFL (Lan-
dauer and Dumais, 1997), 50 ESL (Turney, 2001),
and 300 (English) Reader?s Digest Word Power
questions). Turney (2006) used a similar approach
to evaluate the identification of semantic relations,
with 374 college-level multiple-choice word anal-
ogy questions.
The Reader?s Digest Word Power (RDWP)
benchmark for German consists of 1072 of these
word-choice problems collected from the January
2001 to December 2005 issues of the German-
language edition (Wallace and Wallace, 2005). We
discarded 44 problems that had more than one cor-
rect answer, and 20 problems that used a phrase in-
stead of a single term as the target. The remaining
1008 problems form our evaluation dataset, which is
significantly larger than any of the previous datasets
employed in a similar evaluation.
We evaluate the various cross-lingual and mono-
lingual distance measures by their ability to choose
the correct answer. The distance between the target
and each of the alternatives is computed by a mea-
sure, and the alternative that is closest is chosen. If
two or more alternatives are equally close to the tar-
get, then the alternatives are said to be tied. If one
of the tied alternatives is the correct answer, then
the problem is counted as correctly solved, but the
corresponding score is reduced. We assign a score
of 0.5, 0.33, and 0.25 for 2, 3, and 4 tied alterna-
tives, respectively (in effect approximating the score
obtained by randomly guessing one of the tied al-
ternatives). If more than one alternative has a sense
in common with the target, then the thesaurus-based
cross-lingual measures will mark them each as the
closest sense. However, if one or more of these tied
alternatives is in the same semicolon group of the
thesaurus16 as the target, then only these are chosen
as the closest senses.
The German RDWP dataset contains many
phrases that cannot be found in the knowledge
sources (GermaNet or Macquarie Thesaurus via
translation list). In these cases, we remove stop-
16Words in a thesaurus category are further partitioned into
different paragraphs and each paragraph into semicolon groups.
Words within a semicolon group are more closely related than
those in semicolon groups of the same paragraph or category.
577
Gur65 Gur350
Measure ? r ? r
Monolingual
HPG 0.672 0.702 0.346 0.331
RPG 0.764 0.565 0.492 0.420
JC 0.665 0.748 0.417 0.410
LinGN 0.607 0.739 0.475 0.495
Res 0.623 0.722 0.454 0.466
Cross-lingual
ASD 0.794 0.597 0.520 0.413
Cos 0.778 0.569 0.500 0.212
JSD 0.793 0.633 0.522 0.422
Lindist 0.775 0.816 0.498 0.514
Table 3: Correlations of distance measures with hu-
man judgments.
words (prepositions, articles, etc.) and split the
phrase into component words. As German words
in a phrase can be highly inflected, we lemmatize
all components. For example, the target ?imagina?r?
(imaginary) has ?nur in der Vorstellung vorhanden?
(?exists only in the imagination?) as one of its alter-
natives. The phrase is split into its component words
nur, Vorstellung, and vorhanden. We compute se-
mantic distance between the target and each phrasal
component and select the minimum value as the dis-
tance between target and potential answer.
4.2.2 Results and Discussion
Table 4 presents the results obtained on the Ger-
man RDWP benchmark for both monolingual and
cross-lingual measures. Only those questions for
which the measures have some distance information
are attempted; the column ?Att.? shows the number
of questions attempted by each measure, which is
the maximum score that the measure can hope to
get. Observe that the thesaurus-based cross-lingual
measures have a much larger coverage than the
GermaNet-based monolingual measures. The cross-
lingual measures have a much larger number of cor-
rect answers too (column ?Cor.?), but this number is
bloated due to the large number of ties.17 ?Score?
is the score each measure gets after it is penalized
for the ties. The cross-lingual measures Cos, JSD,
and Lindist obtain the highest scores. But ?Score?
by itself does not present the complete picture ei-
17We see more ties when using the cross-lingual measures
because they rely on the Macquarie Thesaurus, a very coarse-
grained sense inventory (around 800 categories), whereas the
cross-lingual measures operate on the fine-grained GermaNet.
Reader?s Digest Word Power benchmark
Measure Att. Cor. Ties Score P R F
Monolingual
HPG 222 174 11 171.5 .77 .17 .28
RPG 266 188 15 184.7 .69 .18 .29
JC 357 157 1 156.0 .44 .16 .23
LinGN 298 153 1 152.5 .51 .15 .23
Res 299 154 33 148.3 .50 .15 .23
Cross-lingual
ASD 438 185 81 151.6 .35 .15 .21
Cos 438 276 90 223.1 .51 .22 .31
JSD 438 276 90 229.6 .52 .23 .32
Lindist 438 274 90 228.7 .52 .23 .32
Table 4: Performance of distance measures on word
choice problems. (Att.: Attempted, Cor.: Correct)
ther as, given the scoring scheme, a measure that at-
tempts more questions may get a higher score just
from random guessing. We therefore present pre-
cision, recall, and F-scores (P = Score/Att; R =
Score/1008; F = 2?P?R/(P + R)). Observe that
the cross-lingual measures have a higher coverage
(recall) than the monolingual measures but lower
precision. The F scores show that the best cross-
lingual measures do slightly better than the best
monolingual ones, despite the large number of ties.
The measures of Cos, JSD, and Lindist remain the
best cross-lingual measures, whereas HPG and RPG
are the best monolingual ones.
5 Conclusion
We have proposed a new method to determine se-
mantic distance in a possibly resource-poor lan-
guage by combining its text with a knowledge
source in a different, preferably resource-rich, lan-
guage. Specifically, we combined German text with
an English thesaurus to create cross-lingual distri-
butional profiles of concepts?the strengths of as-
sociation between English thesaurus senses (con-
cepts) of German words and co-occurring German
words?using a German?English bilingual lexicon
and a bootstrapping algorithm designed to overcome
ambiguities of word-senses and translations. No-
tably, we do so without the use of sense-annotated
text or word-aligned parallel corpora. We did not
parse or chunk the text, nor did we stem, lemmatize,
or part-of-speech-tag the words.
We used the cross-lingual DPCs to estimate se-
mantic distance by developing new cross-lingual
578
distributional measures of concept-distance. These
measures are like the distributional measures of
concept-distance (Mohammad and Hirst, 2006a,
2006b), except they can determine distance between
words in one language using a thesaurus in a differ-
ent language. We evaluated the cross-lingual mea-
sures against the best monolingual ones operating
on a WordNet-like resource, GermaNet, through an
extensive set of experiments on two different Ger-
man semantic distance benchmarks. In the process,
we compiled a large German benchmark of Reader?s
Digest word choice problems suitable for evaluating
semantic-relatedness measures. Most previous se-
mantic distance benchmarks are either much smaller
or cater primarily to semantic similarity measures.
Even with the added ambiguity of translating
words from one language to another, the cross-
lingual measures performed better than the best
monolingual measures on both the word-pair task
and the Reader?s Digest word-choice task. Fur-
ther, in the word-choice task, the cross-lingual mea-
sures achieved a significantly higher coverage than
the monolingual measure. The richness of En-
glish resources seems to have a major impact, even
though German, with GermaNet, a well-established
resource, is in a better position than most other lan-
guages. This is indeed promising, because achieving
broad coverage for resource-poor languages remains
an important goal as we integrate state-of-the-art ap-
proaches in natural language processing into real-
life applications. These results show that our algo-
rithm can successfully combine German text with an
English thesaurus using a bilingual German?English
lexicon to obtain state-of-the-art results in measur-
ing semantic distance.
These results also support the broader and far-
reaching claim that natural language problems in
a resource-poor language can be solved using a
knowledge source in a resource-rich language (e.g.,
Cucerzan and Yarowsky?s (2002) cross-lingual PoS
tagger). Our future work will explore other tasks
such as information retrieval and text categoriza-
tion. Cross-lingual DPCs also have tremendous po-
tential in tasks inherently involving more than one
language, such as machine translation and multi-
language multi-document summarization. We be-
lieve that the future of natural language process-
ing lies not in standalone monolingual systems but
in those that are powered by automatically created
multilingual networks of information.
Acknowledgments
We thank Philip Resnik, Michael Demko, Suzanne
Stevenson, Frank Rudicz, Afsaneh Fazly, and Afra
Alishahi for helpful discussions. This research is fi-
nancially supported by the Natural Sciences and En-
gineering Research Council of Canada, the Univer-
sity of Toronto, the German Research Foundation
under the grant ?Semantic Information Retrieval?
(SIR), GU 798/1-2.
References
J.R.L. Bernard, editor. 1986. The Macquarie Thesaurus.
Macquarie Library, Sydney, Australia.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th Conference
on Computational Natural Language Learning, pages
132?138, Taipei, Taiwan.
Christiane Fellbaum. 1998. WordNet An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 361?368,
Sydney, Australia.
Iryna Gurevych. 2005. Using the Structure of a Concep-
tual Network in Computing Semantic Relatedness. In
Proceedings of the 2nd International Joint Conference
on Natural Language Processing, pages 767?778, Jeju
Island, Republic of Korea.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
Thesaurus and semantic similarity. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP-2003), pages
212?219.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of International Conference
on Research on Computational Linguistics (ROCLING
X), Taiwan.
579
Peng Jin, Yunfang Wu, and Shiwen Yu. 2007. SemEval-
2007 task 05: Multilingual Chinese-English lexical
sample task. In Proceedings of the Fourth Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text (SemEval-07), Prague,
Czech Republic.
Claudia Kunze, 2004. Lexikalisch-semantische Wort-
netze, chapter Computerlinguistik und Sprachtech-
nologie, pages 423?431. Spektrum Akademischer
Verlag.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104:211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. Introduction to latent semantic analysis. Dis-
course Processes, 25(2?3):259?284.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
Dekang Lin. 1998a. Automatic retreival and cluster-
ing of similar words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
(COLING-98), pages 768?773, Montreal, Canada.
Dekang Lin. 1998b. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304, San
Francisco, CA. Morgan Kaufmann.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 280?267, Barcelona, Spain.
Saif Mohammad and Graeme Hirst. 2005.
Distributional measures as proxies for
semantic relatedness. In submission,
http://www.cs.toronto.edu/compling/Publications.
Saif Mohammad and Graeme Hirst. 2006a. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy.
Saif Mohammad and Graeme Hirst. 2006b. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Saif Mohammad, Graeme Hirst, and Philip Resnik. 2007.
Distributional profiles of concepts for unsupervised
word sense disambigution. In Proceedings of the
Fourth International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text (SemEval-
07), Prague, Czech Republic.
Jane Morris and Graeme Hirst. 2004. Non-classical lex-
ical semantic relations. In Proceedings of the Work-
shop on Computational Lexical Semantics, Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Boston, Massachusetts.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Machine Translation Summit IX, pages 315?322, New
Orleans, Louisiana.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-95), pages 448?453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications to
information retreival. Information Processing and
Management, 33(3):307?318.
Peter Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), pages 491?502, Freiburg, Germany.
Peter Turney. 2006. Expressing implicit semantic rela-
tions without supervision. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL, pages
313?320, Sydney, Australia.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader?s Digest, das Beste fu?r Deutschland. Jan
2001?Dec 2005. Verlag Das Beste, Stuttgart.
David Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Roget?s categories trained on
large corpora. In Proceedings of the 14th International
Conference on Computational Linguistics (COLING-
92), pages 454?460, Nantes, France.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Comparing Wikipedia and German WordNet by
evaluating semantic relatedness on multiple datasets.
In Proceedings of Human Language Technologies:
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2007), pages 205?208, Rochester, New
York.
580
Proceedings of NAACL HLT 2007, Companion Volume, pages 205?208,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Comparing Wikipedia and German Wordnet
by Evaluating Semantic Relatedness on Multiple Datasets
Torsten Zesch and Iryna Gurevych and Max M?hlh?user
Ubiquitous Knowledge Processing Group, Telecooperation Division
Darmstadt University of Technology, D-64289 Darmstadt, Germany
{zesch,gurevych,max} (at) tk.informatik.tu-darmstadt.de
Abstract
We evaluate semantic relatedness mea-
sures on different German datasets show-
ing that their performance depends on: (i)
the definition of relatedness that was un-
derlying the construction of the evalua-
tion dataset, and (ii) the knowledge source
used for computing semantic relatedness.
We analyze how the underlying knowl-
edge source influences the performance
of a measure. Finally, we investigate the
combination of wordnets andWikipedia to
improve the performance of semantic re-
latedness measures.
1 Introduction
Semantic similarity (SS) is typically defined via the
lexical relations of synonymy (automobile ? car)
and hypernymy (vehicle ? car), while semantic re-
latedness (SR) is defined to cover any kind of lexi-
cal or functional association that may exist between
two words. Many NLP applications, like sense tag-
ging or spelling correction, require knowledge about
semantic relatedness rather than just similarity (Bu-
danitsky and Hirst, 2006). For these tasks, it is not
necessary to know the exact type of semantic rela-
tion between two words, but rather if they are closely
semantically related or not. This is also true for the
work presented herein, which is part of a project
on electronic career guidance. In this domain, it
is important to conclude that the words ?baker? and
?bagel? are closely related, while the exact type of a
semantic relation does not need to be determined.
As we work on German documents, we evalu-
ate a number of SR measures on different German
datasets. We show that the performance of mea-
sures strongly depends on the underlying knowledge
source. While WordNet (Fellbaum, 1998) mod-
els SR, wordnets for other languages, such as the
German wordnet GermaNet (Kunze, 2004), contain
only few links expressing SR. Thus, they are not
well suited for estimating SR.
Therefore, we apply theWikipedia category graph
as a knowledge source for SR measures. We show
that Wikipedia based SR measures yield better cor-
relation with human judgments on SR datasets than
GermaNet measures. However, using Wikipedia
also leads to a performance drop on SS datasets,
as knowledge about classical taxonomic relations
is not explicitly modeled. Therefore, we combine
GermaNet with Wikipedia, and yield substantial im-
provements over measures operating on a single
knowledge source.
2 Datasets
Several German datasets for evaluation of SS or SR
have been created so far (see Table 1). Gurevych
(2005) conducted experiments with a German trans-
lation of an English dataset (Rubenstein and Goode-
nough, 1965), but argued that the dataset (Gur65)
is too small (it contains only 65 noun pairs), and
does not model SR. Thus, she created a German
dataset containing 350 word pairs (Gur350) con-
taining nouns, verbs and adjectives that are con-
nected by classical and non-classical relations (Mor-
ris and Hirst, 2004). However, the dataset is bi-
ased towards strong classical relations, as word
pairs were manually selected. Thus, Zesch and
Gurevych (2006) semi-automatically created word
pairs from domain-specific corpora. The resulting
ZG222 dataset contains 222 word pairs that are con-
nected by all kinds of lexical semantic relations.
Hence, it is particularly suited for analyzing the ca-
pability of a measure to estimate SR.
205
CORRELATION r
DATASET YEAR LANGUAGE # PAIRS POS TYPE SCORES # SUBJECTS INTER INTRA
Gur65 2005 German 65 N SS discrete {0,1,2,3,4} 24 .810 -
Gur350 2006 German 350 N, V, A SR discrete {0,1,2,3,4} 8 .690 -
ZG222 2006 German 222 N, V, A SR discrete {0,1,2,3,4} 21 .490 .647
Table 1: Comparison of datasets used for evaluating semantic relatedness.
3 Semantic Relatedness Measures
Semantic wordnet based measures Lesk (1986)
introduced a measure (Les) based on the number of
word overlaps in the textual definitions (or glosses)
of two terms, where higher overlap means higher
similarity. As GermaNet does not contain glosses,
this measure cannot be employed. Gurevych (2005)
proposed an alternative algorithm (PG) generating
surrogate glosses by using a concept?s relations
within the hierarchy. Following the description in
Budanitsky and Hirst (2006), we further define sev-
eral measures using the taxonomy structure.
simPL = l(c1, c2)
simLC = ? log
l(c1, c2)
2? depth
simRes = IC(ci) = ? log(p(lcs(c1, c2)))
distJC = IC(c1) + IC(c2)? 2IC(lcs(c1, c2))
simLin = 2?
IC(lcs(c1, c2))
IC(c1) + IC(c2)
PL is the taxonomic path length l(c1, c2) between
two concepts c1 and c2. LC normalizes the path
length with the depth of the taxonomy. Res com-
putes SS as the information content (IC) of the low-
est common subsumer (lcs) of two concepts, while
JC combines path based and IC features.1 Lin is
derived from information theory.
Wikipedia based measures For computing the
SR of two words w1 and w2 using Wikipedia, we
first retrieve the articles or disambiguation pages
with titles that equal w1 and w2 (see Figure 1). If
we hit a redirect page, we retrieve the correspond-
ing article or disambiguation page instead. In case
of an article, we insert it into the candidate article
set (A1 for w1, A2 for w2). In case of a disam-
biguation page, the page contains links to all en-
coded word senses, but it may also contain other
1Note that JC returns a distance value instead of a similarity
value resulting in negative correlation with human judgments.
Wo
rd w
2
Wo
rd w
1
1
Can
dida
te 
arti
cle sets
Can
dida
te 
arti
cle pair
s
3
a
Sem
ant
ic R
elat
edn
ess
Sem
ant
ic re
late
dne
ss o
f (w
1, w
2)
Ma
x
a
A 2
A 1
2
a
1
aArt
icle
Dis
am
big.
 pa
ge
Red
irec
ts
Figure 1: Steps for computing SR using Wikipedia.
links. Therefore, we only consider links conforming
to the pattern ?Title_(DisambiguationText)?2 (e.g.
?Train_(roller coaster)?). Following all such links
gives the candidate article set. If no disambiguation
links are found, we take the first link on the page, as
most important links tend to come first. We add the
corresponding articles to the candidate set. We form
pairs from each candidate article ai ? A1 and each
article aj ? A2. We then compute SR(ai, aj) for
each pair. The output of the algorithm is the maxi-
mum SR value maxai?A1,aj?A2(SR(ai, aj)).
3
As most SR measures have been developed for
taxonomic wordnets, porting them to Wikipedia re-
quires some modifications (see Figure 2). Text over-
lap measures can be computed based on the article
text, while path based measures operate on the cate-
gory graph. We compute the overlap between article
2?_(DisambiguationText)? is optional.
3Different from our approach, Strube and Ponzetto (2006)
use a disambiguation strategy that returns only a single candi-
date article pair. This unnecessarily limits a measure?s potential
to consider SR between all candidate article pairs. They also
limit the search for a lcs to a manually specified threshold of 4.
206
Wikip
edia
 
cate
gory 
grap
h
A B
A D
B B
B D
C B
C D
Cat
ego
ry pair
sA
C
B
B
D
Cat
ego
ries
 
of a
rticl
e 1
A
H
G
C
E
F
B
D
Artic
le 1 Text
1
AB
C
Text
1
Text
2
Text
2
First 
para
grap
h
Full te
xt
Artic
le 2 Text
2
BD
Text
1
LCS
(B,D
)
Cat
ego
ry gra
ph ba
sed
Text 
bas
ed
Com
bina
tion
 
stra
tegy
Avg
Best
Candidate article pair
Cat
ego
ries
 
of a
rticl
e 2
C 2
C 1
a) b)
Figure 2: SR measures adapted on Wikipedia.
texts based on (i) the first paragraph, as it usually
contains a short gloss, and (ii) the full article text.
As Wikipedia articles do not form a taxonomy, path
based measures have to be adapted to the Wikipedia
category graph (see the right part of Figure 2). We
define C1 and C2 as the set of categories assigned to
article ai and aj , respectively. We compute the SR
value for each category pair (ck, cl) with ck ? C1
and cl ? C2. We use two different strategies to com-
bine the resulting SR values: First, we choose the
best value among all pairs (ck, cl), i.e., the minimum
for path based, and the maximum for information
content based measures. As a second strategy, we
average over all category pairs.
4 Experiments & Results
Table 2 gives an overview of our experimental re-
sults on three German datasets. Best values for each
dataset and knowledge source are in bold. We use
thePGmeasure in optimal configuration as reported
by Gurevych (2005). For the Les measure, we give
the results for considering: (i) only the first para-
graph (+First) and (ii) the full text (+Full). For the
path length based measures, we give the values for
averaging over all category pairs (+Avg), or tak-
ing the best SR value computed among the pairs
(+Best). For each dataset, we report Pearson?s cor-
relation r with human judgments on pairs that are
found in both resources (BOTH). Otherwise, the re-
sults would not be comparable. We additionally use
a subset containing only noun-noun pairs (BOTH
NN). This comparison is fairer, because article titles
in Wikipedia are usually nouns. Table 2 also gives
the inter annotator agreement for each subset. It con-
stitutes an upper bound of a measure?s performance.
Our results on Gur65 using GermaNet are very
close to those published by Gurevych (2005), rang-
ing from 0.69?0.75. For Gur350, the performance
drops to 0.38?0.50, due to the lower upper bound,
and because GermaNet does not model SR well.
These findings are endorsed by an even more sig-
nificant performance drop on ZG222. The measures
based on Wikipedia behave less uniformly. Les
yields acceptable results on Gur350, but is generally
not among the best performing measures. LC +Avg
yields the best performance on Gur65, but is outper-
formed on the other datasets by PL +Best, which
performs equally good for all datasets.
If we compare GermaNet based and Wikipedia
based measures, we find that the knowledge source
has a major influence on performance. When evalu-
ated on Gur65, that contains pairs connected by SS,
GermaNet based measures perform near the upper
bound and outperformWikipedia based measures by
a wide margin. On Gur350 containing a mix of SS
and SR pairs, most measures perform comparably.
Finally, on ZG222, that contains pairs connected by
SR, the best Wikipedia based measure outperforms
all GermaNet based measures.
The impressive performance of PL on the
SR datasets cannot be explained with the struc-
tural properties of the category graph (Zesch and
Gurevych, 2007). Semantically related terms, that
would not be closely related in a taxonomic word-
net structure, are very likely to be categorized under
the same Wikipedia category, resulting in short path
lengths leading to high SR. These findings are con-
trary to that of (Strube and Ponzetto, 2006), where
LC outperformed path length. They limited the
search depth using a manually defined threshold,
and did not compute SR between all candidate ar-
ticle pairs.
Our results show that judgments on the perfor-
mance of a measure must always be made with re-
spect to the task at hand: computing SS or SR. De-
pending on this decision, we can choose the best un-
derlying knowledge source. GermaNet is better for
207
GUR65 GUR350 ZG222
BOTH NN BOTH BOTH NN BOTH BOTH NN
# Word Pairs 53 116 91 55 45
Inter Annotator Agreement 0.80 0.64 0.63 0.44 0.43
GermaNet
PG 0.69 0.38 0.42 0.23 0.21
JC -0.75 -0.52 -0.48 -0.19 -0.25
Lin 0.73 0.50 0.50 0.08 -0.12
Res 0.71 0.42 0.42 0.10 0.13
Wikipedia
Les +First 0.16 0.36 0.32 0.01 0.11
Les +Full 0.19 0.34 0.37 0.13 0.17
PL +Avg -0.32 -0.34 -0.46 -0.36 -0.43
PL +Best -0.35 -0.42 -0.53 -0.43 -0.49
LC +Avg 0.37 0.25 0.34 0.30 0.30
LC +Best 0.21 0.12 0.21 0.15 0.12
Combination
Linear 0.77 0.59 0.60 0.38 0.43
POS - 0.55 - 0.48 -
Table 2: Correlation r of human judgments with SR measures on different datasets.
estimating SS, while Wikipedia should be used to
estimate SR. Therefore, a measure based on a single
knowledge source is unlikely to perform well in all
settings. We computed a linear combination of the
best measure from GermaNet and from Wikipedia.
Results for this experiment are labeled Linear in Ta-
ble 2. POS is an alternative combination strategy,
where Wikipedia is only used for noun-noun pairs.
GermaNet is used for all other part-of-speech (POS)
combinations. For most datasets, we find a combi-
nation strategy that outperforms all single measures.
5 Conclusion
We have shown that in deciding for a specific mea-
sure and knowledge source it is important to con-
sider (i) whether the task at hand requires SS or
SR, and (ii) which POS are involved. We pointed
out that the underlying knowledge source has a ma-
jor influence on these points. GermaNet is better
used for SS, and contains nouns, verbs, and adjec-
tives, while Wikipedia is better used for SR between
nouns. Thus, GermaNet and Wikipedia can be re-
garded as complementary. We have shown that com-
bining them significantly improves the performance
of SR measures up to the level of human perfor-
mance.
Future research should focus on improving the
strategies for combining complementary knowledge
sources. We also need to evaluate a wider range of
measures to validate our findings. As the simple PL
measure performs remarkably well, we should also
consider computing SR based on the Wikipedia arti-
cle graph instead of the category graph.
Acknowledgments
This work was supported by the German Research Foundation
under grant "Semantic Information Retrieval from Texts in the
Example Domain Electronic Career Guidance", GU 798/1-2.
References
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating
WordNet-based Measures of Semantic Distance. Computa-
tional Linguistics, 32(1).
Christiane Fellbaum. 1998. WordNet An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Iryna Gurevych. 2005. Using the Structure of a Conceptual
Network in Computing Semantic Relatedness. In Proc. of
IJCNLP, pages 767?778.
Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze,
chapter Computerlinguistik und Sprachtechnologie, pages
423?431. Spektrum Akademischer Verlag.
Michael Lesk. 1986. Automatic Sense Disambiguation Us-
ing Machine Readable Dictionaries: How to tell a pine cone
from an ice cream cone. In Proc. of the 5th Annual Interna-
tional Conference on Systems Documentation, pages 24?26.
Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical
Semantic Relations. In Proc. of the Workshop on Computa-
tional Lexical Semantics, NAACL-HTL.
Herbert Rubenstein and John B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of the ACM,
8(10):627?633.
Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate!
Computing Semantic Relatedness UsingWikipedia. In Proc.
of AAAI, pages 1219?1224.
Torsten Zesch and Iryna Gurevych. 2006. Automatically Creat-
ing Datasets for Measures of Semantic Relatedness. In Proc.
of the Workshop on Linguistic Distances, ACL, pages 16?24.
T. Zesch and I. Gurevych. 2007. Analysis of the Wikipedia
Category Graph for NLP Applications. In Proc. of the
TextGraphs-2 Workshop, NAACL-HLT, (to appear).
208
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1032?1039,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
What to be? - Electronic Career Guidance Based on Semantic Relatedness
Iryna Gurevych, Christof Mu?ller and Torsten Zesch
Ubiquitous Knowledge Processing Group
Telecooperation, Darmstadt University of Technology
Hochschulstr. 10, 64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de
{gurevych,mueller,zesch}@tk.informatik.tu-darmstadt.de
Abstract
We present a study aimed at investigating
the use of semantic information in a novel
NLP application, Electronic Career Guid-
ance (ECG), in German. ECG is formu-
lated as an information retrieval (IR) task,
whereby textual descriptions of professions
(documents) are ranked for their relevance
to natural language descriptions of a per-
son?s professional interests (the topic). We
compare the performance of two semantic
IR models: (IR-1) utilizing semantic relat-
edness (SR) measures based on either word-
net or Wikipedia and a set of heuristics,
and (IR-2) measuring the similarity between
the topic and documents based on Explicit
Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007). We evaluate the perfor-
mance of SR measures intrinsically on the
tasks of (T-1) computing SR, and (T-2) solv-
ing Reader?s Digest Word Power (RDWP)
questions.
1 Electronic Career Guidance
Career guidance is important both for the person in-
volved and for the state. Not well informed deci-
sions may cause people to drop the training program
they are enrolled in, yielding loss of time and finan-
cial investments. However, there is a mismatch bet-
ween what people know about existing professions
and the variety of professions, which exist in real-
ity. Some studies report that school leavers typi-
cally choose the professions known to them, such
as policeman, nurse, etc. Many other professions,
which can possibly match the interests of the person
very well, are not chosen, as their titles are unknown
and people seeking career advice do not know about
their existence, e.g. electronics installer, or chem-
ical laboratory worker. However, people are very
good at describing their professional interests in nat-
ural language. That is why they are even asked to
write a short essay prior to an appointment with a
career guidance expert.
Electronic career guidance is, thus, a supplement
to career guidance by human experts, helping young
people to decide which profession to choose. The
goal is to automatically compute a ranked list of pro-
fessions according to the user?s interests. A current
system employed by the German Federal Labour
Office (GFLO) in their automatic career guidance
front-end1 is based on vocational trainings, manu-
ally annotated using a tagset of 41 keywords. The
user must select appropriate keywords according to
her interests. In reply, the system consults a knowl-
edge base with professions manually annotated with
the keywords by career guidance experts. There-
after, it outputs a list of the best matching profes-
sions to the user. This approach has two significant
disadvantages. Firstly, the knowledge base has to
be maintained and steadily updated, as the number
of professions and keywords associated with them
is continuously changing. Secondly, the user has to
describe her interests in a very restricted way.
At the same time, GFLO maintains an extensive
database with textual descriptions of professions,
1http://www.interesse-beruf.de/
1032
called BERUFEnet.2 Therefore, we cast the prob-
lem of ECG as an IR task, trying to remove the
disadvantages of conventional ECG outlined above
by letting the user describe her interests in a short
natural language essay, called a professional profile.
Example essay translated to English
I would like to work with animals, to treat and look
after them, but I cannot stand the sight of blood and
take too much pity on them. On the other hand, I like
to work on the computer, can program in C, Python and
VB and so I could consider software development as an
appropriate profession. I cannot imagine working in a
kindergarden, as a social worker or as a teacher, as I
am not very good at asserting myself.
Textual descriptions of professions are ranked
given such an essay by using NLP and IR tech-
niques. As essays and descriptions of professions
display a mismatch between the vocabularies of top-
ics and documents and there is lack of contextual in-
formation, due to the documents being fairly short
as compared to standard IR scenarios, lexical se-
mantic information should be especially beneficial
to an IR system. For example, the profile can con-
tain words about some objects or activities related to
the profession, but not directly mentioned in the de-
scription, e.g. oven, cakes in the profile and pastries,
baker, or confectioner in the document. Therefore,
we propose to utilize semantic relatedness as a rank-
ing function instead of conventional IR techniques,
as will be substantiated below.
2 System Architecture
Integrating lexical semantic knowledge in ECG re-
quires the existence of knowledge bases encoding
domain and lexical knowledge. In this paper, we in-
vestigate the utility of two knowledge bases: (i) a
German wordnet, GermaNet (Kunze, 2004), and (ii)
the German portion of Wikipedia.3 A large body of
research exists on using wordnets in NLP applica-
tions and in particular in IR (Moldovan and Mihal-
cea, 2000). The knowledge in wordnets has been
typically utilized by expanding queries with related
terms (Vorhees, 1994; Smeaton et al, 1994), con-
cept indexing (Gonzalo et al, 1998), or similarity
measures as ranking functions (Smeaton et al, 1994;
Mu?ller and Gurevych, 2006). Recently, Wikipedia
2http://infobub.arbeitsagentur.de/
berufe/
3http://de.wikipedia.org/
has been discovered as a promising lexical seman-
tic resource and successfully used in such different
NLP tasks as question answering (Ahn et al, 2004),
named entity disambiguation (Bunescu and Pasca,
2006), and information retrieval (Katz et al, 2005).
Further research (Zesch et al, 2007b) indicates that
German wordnet and Wikipedia show different per-
formance depending on the task at hand.
Departing from this, we first compare two seman-
tic relatedness (SR) measures based on the informa-
tion either in the German wordnet (Lin, 1998) called
LIN, or in Wikipedia (Gabrilovich and Markovitch,
2007) called Explicit Semantic Analysis, or ESA.
We evaluate their performance intrinsically on the
tasks of (T-1) computing semantic relatedness, and
(T-2) solving Reader?s Digest Word Power (RDWP)
questions and make conclusions about the ability of
the measures to model certain aspects of semantic
relatedness and their coverage. Furthermore, we fol-
low the approach by Mu?ller and Gurevych (2006),
who proposed to utilize the LIN measure and a set
of heuristics as an IR model (IR-1).
Additionally, we utilize the ESA measure in a
semantic information retrieval model, as this mea-
sure is significantly better at vocabulary cover-
age and at modelling cross part-of-speech relations
(Gabrilovich and Markovitch, 2007). We compare
the performance of ESA and LINmeasures in a task-
based IR evaluation and analyze their strengths and
limitations. Finally, we apply ESA to directly com-
pute text similarities between topics and documents
(IR-2) and compare the performance of two seman-
tic IR models and a baseline Extended Boolean (EB)
model (Salton et al, 1983) with query expansion.4
To summarize, the contributions of this paper are
three-fold: (i) we present a novel system, utilizing
NLP and IR techniques to perform Electronic Career
Guidance, (ii) we study the properties and intrinsi-
cally evaluate two SR measures based on GermaNet
and Wikipedia for the tasks of computing seman-
tic relatedness and solving Reader?s Digest Word
Power Game questions, and (iii) we investigate the
performance of two semantic IR models in a task
based evaluation.
4We also ran experiments with Okapi BM25 model as im-
plemented in the Terrier framework, but the results were worse
than those with the EB model. Therefore, we limit our discus-
sion to the latter.
1033
3 Computing Semantic Relatedness
3.1 SR Measures
GermaNet based measures GermaNet is a Ger-
man wordnet, which adopted the major properties
and database technology from Princeton?s Word-
Net (Fellbaum, 1998). However, GermaNet dis-
plays some structural differences and content ori-
ented modifications. Its designers relied mainly on
linguistic evidence, such as corpus frequency, rather
than psycholinguistic motivations. Also, GermaNet
employs artificial, i.e. non-lexicalized concepts, and
adjectives are structured hierarchically as opposed
to WordNet. Currently, GermaNet includes about
40000 synsets with more than 60000 word senses
modelling nouns, verbs and adjectives.
We use the semantic relatedness measure by Lin
(1998) (referred to as LIN), as it consistently is
among the best performing wordnet based measures
(Gurevych and Niederlich, 2005; Budanitsky and
Hirst, 2006). Lin defined semantic similarity using a
formula derived from information theory. This mea-
sure is sometimes called a universal semantic sim-
ilarity measure as it is supposed to be application,
domain, and resource independent. Lin is computed
as:
simc1,c2 =
2 ? log p(LCS(c1, c2))
log p(c1) + log p(c2)
where c1 and c2 are concepts (word senses) corre-
sponding to w1 and w2, log p(c) is the information
content, andLCS(c1, c2) is the lowest common sub-
sumer of the two concepts. The probability p is com-
puted as the relative frequency of words (represent-
ing that concept) in the taz5 corpus.
Wikipedia based measures Wikipedia is a free
online encyclopedia that is constructed in a col-
laborative effort of voluntary contributors and still
grows exponentially. During this process, Wikipedia
has probably become the largest collection of freely
available knowledge. Wikipedia shares many of
its properties with other well known lexical seman-
tic resources (like dictionaries, thesauri, semantic
wordnets or conventional encyclopedias) (Zesch et
al., 2007a). As Wikipedia also models relatedness
between concepts, it is better suited for computing
5http://www.taz.de
semantic relatedness than GermaNet (Zesch et al,
2007b).
In very recent work, Gabrilovich and Markovitch
(2007) introduce a SR measure called Explicit Se-
mantic Analysis (ESA). The ESA measure repre-
sents the meaning of a term as a high-dimensional
concept vector. The concept vector is derived from
Wikipedia articles, as each article focuses on a cer-
tain topic, and can thus be viewed as expressing a
concept. The dimension of the concept vector is the
number of Wikipedia articles. Each element of the
vector is associated with a certain Wikipedia article
(or concept). If the term can be found in this article,
the term?s tfidf score (Salton and McGill, 1983) in
this article is assigned to the vector element. Oth-
erwise, 0 is assigned. As a result, a term?s con-
cept vector represents the importance of the term for
each concept. Semantic relatedness of two terms can
then be easily computed as the cosine of their corre-
sponding concept vectors. If we want to measure
the semantic relatedness of texts instead of terms,
we can also use ESA concept vectors. A text is rep-
resented as the average concept vector of its terms?
concept vectors. Then, the relatedness of two texts
is computed as the cosine of their average concept
vectors.
As ESA uses all textual information in Wikipedia,
the measure shows excellent coverage. Therefore,
we select it as the second measure for integration
into our IR system.
3.2 Datasets
Semantic relatedness datasets for German em-
ployed in our study are presented in Table 1.
Gurevych (2005) conducted experiments with two
datasets: i) a German translation of the English
dataset by Rubenstein and Goodenough (1965)
(Gur65), and ii) a larger dataset containing 350
word pairs (Gur350). Zesch and Gurevych (2006)
created a third dataset from domain-specific corpora
using a semi-automatic process (ZG222). Gur65 is
rather small and contains only noun-noun pairs con-
nected by either synonymy or hypernymy. Gur350
contains nouns, verbs and adjectives that are con-
nected by classical and non-classical relations (Mor-
ris and Hirst, 2004). However, word pairs for
this dataset are biased towards strong classical rela-
tions, as they were manually selected from a corpus.
1034
CORRELATION r
DATASET YEAR LANGUAGE # PAIRS POS SCORES # SUBJECTS INTER INTRA
Gur65 2005 German 65 N discrete {0,1,2,3,4} 24 .810 -
Gur350 2006 German 350 N, V, A discrete {0,1,2,3,4} 8 .690 -
ZG222 2006 German 222 N, V, A discrete {0,1,2,3,4} 21 .490 .647
Table 1: Comparison of datasets used for evaluating semantic relatedness in German.
ZG222 does not have this bias.
Following the work by Jarmasz and Szpakow-
icz (2003) and Turney (2006), we created a sec-
ond dataset containing multiple choice questions.
We collected 1072 multiple-choice word analogy
questions from the German Reader?s Digest Word
Power Game (RDWP) from January 2001 to De-
cember 2005 (Wallace and Wallace, 2005). We dis-
carded 44 questions that had more than one correct
answer, and 20 questions that used a phrase instead
of a single term as query. The resulting 1008 ques-
tions form our evaluation dataset. An example ques-
tion is given below:
Muffin (muffin)
a) Kleingeba?ck (small cake)
b) Spenglerwerkzeug (plumbing tool)
c) Miesepeter (killjoy)
d) Wildschaf (moufflon)
The task is to find the correct choice - ?a)? in this
case.
This dataset is significantly larger than any of the
previous datasets employed in this type of evalua-
tion. Also, it is not restricted to synonym questions,
as in the work by Jarmasz and Szpakowicz (2003),
but also includes hypernymy/hyponymy, and few
non-classical relations.
3.3 Analysis of Results
Table 2 gives the results of evaluation on the task
of correlating the results of an SR measure with hu-
man judgments using Pearson correlation. The Ger-
maNet based LIN measure outperforms ESA on the
Gur65 dataset. On the other datasets, ESA is better
than LIN. This is clearly due to the fact, that Gur65
contains only noun-noun word pairs connected by
classical semantic relations, while the other datasets
also contain cross part-of-speech pairs connected by
non-classical relations. The Wikipedia based ESA
measure can better capture such relations. Addition-
ally, Table 3 shows that ESA also covers almost all
GUR65 GUR350 ZG222
# covered word pairs 53 116 55
Upper bound 0.80 0.64 0.44
GermaNet Lin 0.73 0.50 0.08
Wikipedia ESA 0.56 0.52 0.32
Table 2: Pearson correlation r of human judgments
with SR measures on word pairs covered by Ger-
maNet and Wikipedia.
COVERED PAIRS
DATASET # PAIRS LIN ESA
Gur65 65 60 65
Gur350 350 208 333
ZG222 222 88 205
Table 3: Number of covered word pairs based on Lin
or ESA measure on different datasets.
word pairs in each dataset, while GermaNet is much
lower for Gur350 and ZG222. ESA performs even
better on the Reader?s Digest task (see Table 4). It
shows high coverage and near human performance
regarding the relative number of correctly solved
questions.6 Given the high performance and cover-
age of the Wikipedia based ESA measure, we expect
it to yield better IR results than LIN.
4 Information Retrieval
4.1 IR Models
Preprocessing For creating the search index for
IR models, we apply first tokenization and then re-
move stop words. We use a general German stop
6Values for human performance are for one subject. Thus,
they only indicate the approximate difficulty of the task. We
plan to use this dataset with a much larger group of subjects.
#ANSWERED #CORRECT RATIO
Human 1008 874 0.87
GermaNet Lin 298 153 0.51
Wikipedia ESA 789 572 0.72
Table 4: Evaluation results on multiple-choice word
analogy questions.
1035
word list extended with highly frequent domain spe-
cific terms. Before adding the remaining words to
the index, they are lemmatized. We finally split
compounds into their constituents, and add both,
constituents and compounds, to the index.
EB model Lucene7 is an open source text search
library based on an EB model. After matching the
preprocessed queries against the index, the docu-
ment collection is divided into a set of relevant and
irrelevant documents. The set of relevant documents
is, then, ranked according to the formula given in the
following equation:
rEB(d, q) =
nq?
i=1
tf(tq, d)?idf(tq)?lengthNorm(d)
where nq is the number of terms in the query,
tf(tq, d) is the term frequency factor for term tq
in document d, idf(tq) is the inverse document fre-
quency of the term, and lengthNorm(d) is a nor-
malization value of document d, given the number
of terms within the document. We added a simple
query expansion algorithm using (i) synonyms, and
(ii) hyponyms, extracted from GermaNet.
IR based on SR For the (IR-1) model, we uti-
lize two SR measures and a set of heuristics: (i)
the Lin measure based on GermaNet (LIN), and (ii)
the ESA measure based on Wikipedia (ESA-Word).
This algorithm was applied to the German IR bench-
mark with positive results by Mu?ller and Gurevych
(2006). The algorithm computes a SR score for each
query and document term pair. Scores above a pre-
defined threshold are summed up and weighted by
different factors, which boost or lower the scores for
documents, depending on howmany query terms are
contained exactly or contribute a high enough SR
score. In order to integrate the strengths of tradi-
tional IR models, the inverse document frequency
idf is considered, which measures the general im-
portance of a term for predicting the content of a
document. The final formula of the model is as fol-
lows:
rSR(d, q) =
?nd
i=1
?nq
j=1 idf(tq,j) ? s(td,i, tq,j)
(1 + nnsm) ? (1 + nnr)
7http://lucene.apache.org
where nd is the number of tokens in the document,
nq the number of tokens in the query, td,i the i-th
document token, tq,j the j-th query token, s(td,i, tq,j)
the SR score for the respective document and query
term, nnsm the number of query terms not exactly
contained in the document, nnr the number of query
tokens, which do not contribute a SR score above the
threshold.
For the (IR-2) model, we apply the ESA method
for directly comparing the query with documents, as
described in Section 3.1.
4.2 Data
The corpus employed in our experiments was built
based on a real-life IR scenario in the domain of
ECG, as described in Section 1. The document col-
lection is extracted from BERUFEnet,8 a database
created by the GFLO. It contains textual descrip-
tions of about 1,800 vocational trainings, and 4,000
descriptions of professions. We restrict the collec-
tion to a subset of BERUFEnet documents, consist-
ing of 529 descriptions of vocational trainings, due
to the process necessary to obtain relevance judg-
ments, as described below. The documents contain
not only details of professions, but also a lot of infor-
mation concerning the training and administrative
issues. We only use those portions of the descrip-
tions, which characterize the profession itself.
We collected real natural language topics by ask-
ing 30 human subjects to write an essay about their
professional interests. The topics contain 130 words,
on average. Making relevance judgments for ECG
requires domain expertise. Therefore, we applied an
automatic method, which uses the knowledge base
employed by the GFLO, described in Section 1. To
obtain relevance judgments, we first annotate each
essay with relevant keywords from the tagset of 41
and retrieve a ranked list of professions, which were
assigned one or more keywords by domain experts.
To map the ranked list to a set of relevant and ir-
relevant professions, we use a threshold of 3, as
suggested by career guidance experts. This setting
yields on average 93 relevant documents per topic.
The quality of the automatically created gold stan-
dard depends on the quality of the applied knowl-
edge base. As the knowledge base was created by
8http://berufenet.arbeitsamt.de/
1036
domain experts and is at the core of the electronic ca-
reer guidance system of the GFLO, we assume that
the quality is adequate to ensure a reliable evalua-
tion.
4.3 Analysis of Results
In Table 5, we summarize the results of the ex-
periments applying different IR models on the
BERUFEnet data. We build queries from natural
language essays by (QT-1) extracting nouns, verbs,
and adjectives, (QT-2) using only nouns, and (QT-
3) manually assigning suitable keywords from the
tagset with 41 keywords to each topic. We report the
results with two different thresholds (.85 and .98) for
the Lin model, and with three different thresholds
(.11, .13 and .24) for the ESA-Word models. The
evaluation metrics used are mean average precision
(MAP), precision after ten documents (P10), the
number of relevant returned documents (#RRD). We
compute the absolute value of Spearman?s rank cor-
relation coefficient (SRCC) by comparing the rele-
vance ranking of our system with the relevance rank-
ing of the knowledge base employed by the GFLO.
Using query expansion for the EB model de-
creases the retrieval performance for most configu-
rations. The SR based models outperform the EB
model in all configurations and evaluation metrics,
except for P10 on the keyword based queries. The
Lin model is always outperformed by at least one of
the ESA models, except for (QT-3). (IR-2) performs
best on longer queries using nouns, verbs, adjectives
or just nouns.
Comparing the number of relevant retrieved doc-
uments, we observe that the IR models based on SR
are able to return more relevant documents than the
EB model. This supports the claim that semantic
knowledge is especially helpful for the vocabulary
mismatch problem, which cannot be addressed by
conventional IR models. E.g., only SR-based mod-
els can find the job information technician for a pro-
file which contains the sentence My interests and
skills are in the field of languages and IT. The job
could only be judged as relevant, as the semantic
relation between IT in the profile and information
technology in the professional description could be
found.
In our analysis of the BERUFEnet results ob-
tained on (QT-1), we noticed that many errors were
due to the topics expressed in free natural language
essays. Some subjects deviated from the given task
to describe their professional interests and described
facts that are rather irrelevant to the task of ECG,
e.g. It is important to speak different languages in
the growing European Union. If all content words
are extracted to build a query, a lot of noise is intro-
duced.
Therefore, we conducted further experiments
with (QT-2) and (QT-3): building the query using
only nouns, and using manually assigned keywords
based on the tagset of 41 keywords. For example,
the following query is built for the professional pro-
file given in Section 1.
Keywords assigned:
care for/nurse/educate/teach; use/program computer;
office; outside: outside facilities/natural
environment; animals/plants
IR results obtained on (QT-2) and (QT-3) show
that the performance is better for nouns, and sig-
nificantly better for the queries built of keywords.
This suggests that in order to achieve high IR perfor-
mance for the task of Electronic Career Guidance,
it is necessary to preprocess the topics by perform-
ing information extraction to remove the noise from
free text essays. As a result of the preprocessing,
natural language essays should be mapped to a set
of keywords relevant for describing a person?s in-
terests. Our results suggest that the word-based se-
mantic relatedness IR model (IR-1) performs signif-
icantly better in this setting.
5 Conclusions
We presented a system for Electronic Career Guid-
ance utilizing NLP and IR techniques. Given a nat-
ural language professional profile, relevant profes-
sions are computed based on the information about
semantic relatedness. We intrinsically evaluated and
analyzed the properties of two semantic relatedness
measures utilizing the lexical semantic information
in a German wordnet and Wikipedia on the tasks of
estimating semantic relatedness scores and answer-
ing multiple-choice questions. Furthermore, we ap-
plied these measures to an IR task, whereby they
were used either in combination with a set of heuris-
tics or the Wikipedia based measure was used to di-
rectly compute semantic relatedness of topics and
1037
MODEL
(QT-1) NOUNS, VERBS, ADJ. (QT-2) NOUNS (QT-3) KEYWORDS
MAP P10 #RRD SRCC MAP P10 #RRD SRCC MAP P10 #RRD SRCC
EB .39 .58 2581 .306 .38 .58 2297 .335 .54 .76 2755 .497
EB+SYN .37 .56 2589 .288 .38 .57 2310 .331 .54 .73 2768 .530
EB+HYPO .34 .47 2702 .275 .38 .56 2328 .327 .47 .65 2782 .399
Lin .85 .41 .56 2787 .338 .40 .59 2770 .320 .59 .73 2787 .578
Lin .98 .41 .61 2753 .326 .42 .59 2677 .341 .58 .74 2783 .563
ESA-Word .11 .39 .56 2787 .309 .44 .63 2787 .355 .60 .77 2787 .535
ESA-Word .13 .38 .59 2787 .282 .43 .62 2787 .338 .62 .76 2787 .550
ESA-Word .24 .40 .60 2787 .259 .43 .60 2699 .306 .54 .73 2772 .482
ESA-Text .47 .62 2787 .368 .55 .71 2787 .462 .56 .74 2787 .489
Table 5: Information Retrieval performance on the BERUFEnet dataset.
documents. We experimented with three different
query types, which were built from the topics by:
(QT-1) extracting nouns, verbs, adjectives, (QT-2)
extracting only nouns, or (QT-3) manually assign-
ing several keywords to each topic from a tagset of
41 keywords.
In an intrinsic evaluation of LIN and ESA mea-
sures on the task of computing semantic relatedness,
we found that ESA captures the information about
semantic relatedness and non-classical semantic re-
lations considerably better than LIN, which operates
on an is-a hierarchy and, thus, better captures the in-
formation about semantic similarity. On the task of
solving RDWP questions, the ESA measure signif-
icantly outperformed the LIN measure in terms of
correctness. On both tasks, the coverage of ESA is
much better. Despite this, the performance of LIN
and ESA as part of an IR model is only slightly
different. ESA performs better for all lengths of
queries, but the differences are not as significant as
in the intrinsic evaluation. This indicates that the
information provided by both measures, based on
different knowledge bases, might be complementary
for the IR task.
When ESA is applied to directly compute seman-
tic relatedness between topics and documents, it out-
performs IR-1 and the baseline EB model by a large
margin for QT-1 and QT-2 queries. For QT-3, i.e.,
the shortest type of query, it performs worse than
IR-1 utilizing ESA and a set of heuristics. Also,
the performance of the baseline EB model is very
strong in this experimental setting. This result in-
dicates that IR-2 utilizing conventional information
retrieval techniques and semantic information from
Wikipedia is better suited for longer queries provid-
ing enough context. For shorter queries, soft match-
ing techniques utilizing semantic relatedness tend to
be beneficial.
It should be born in mind, that the construction
of QT-3 queries involved a manual step of assigning
the keywords to a given essay. In this experimen-
tal setting, all models show the best performance.
This indicates that professional profiles contain a lot
of noise, so that more sophisticated NLP analysis
of topics is required. This will be improved in our
future work, whereby the system will incorporate
an information extraction component for automat-
ically mapping the professional profile to a set of
keywords. We will also integrate a component for
analyzing the sentiment structure of the profiles. We
believe that the findings from our work on apply-
ing IR techniques to the task of Electronic Career
Guidance generalize to similar application domains,
where topics and documents display similar proper-
ties (with respect to their length, free-text structure
and mismatch of vocabularies) and domain and lex-
ical knowledge is required to achieve high levels of
performance.
Acknowledgments
This work was supported by the German Research
Foundation under grant ?Semantic Information Re-
trieval from Texts in the Example Domain Elec-
tronic Career Guidance?, GU 798/1-2. We are grate-
ful to the Bundesagentur fu?r Arbeit for providing
the BERUFEnet corpus. We would like to thank the
anonymous reviewers for valuable feedback on this
paper. We would also like to thank Piklu Gupta for
helpful comments.
1038
References
David Ahn, Valentin Jijkoun, Gilad Mishne, Karin
Mu?ller, Maarten de Rijke, and Stefan Schlobach.
2004. Using Wikipedia at the TREC QA Track. In
Proceedings of TREC 2004.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based Measures of Semantic Distance.
Computational Linguistics, 32(1).
Razvan Bunescu and Marius Pasca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. In Proceedings of ACL, pages 9?16, Trento, Italy.
Christiane Fellbaum. 1998. WordNet An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of The
20th International Joint Conference on Artificial In-
telligence (IJCAI), Hyderabad, India, January.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of the Coling-
ACL ?98 Workshop Usage of WordNet in Natural Lan-
guage Processing Systems, Montreal, Canada, August.
Iryna Gurevych and Hendrik Niederlich. 2005. Comput-
ing semantic relatedness in german with revised infor-
mation content metrics. In Proceedings of ?OntoLex
2005 - Ontologies and Lexical Resources? IJCNLP?05
Workshop, pages 28?33, October 11 ? 13.
Iryna Gurevych. 2005. Using the Structure of a Concep-
tual Network in Computing Semantic Relatedness. In
Proceedings of the 2nd International Joint Conference
on Natural Language Processing, pages 767?778, Jeju
Island, Republic of Korea.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s the-
saurus and semantic similarity. In RANLP, pages 111?
120.
Boris Katz, Gregory Marton, Gary Borchardt, Alexis
Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-
Rosenberg, Ben Lu, Federico Mora, Stephan Stiller,
Ozlem Uzuner, and Angela Wilcox. 2005. External
knowledge sources for question answering. In Pro-
ceedings of the 14th Annual Text REtrieval Conference
(TREC?2005), November.
Claudia Kunze, 2004. Lexikalisch-semantische Wort-
netze, chapter Computerlinguistik und Sprachtech-
nologie, pages 423?431. Spektrum Akademischer
Verlag.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Interna-
tional Conference on Machine Learning, pages 296?
304. Morgan Kaufmann, San Francisco, CA.
Dan Moldovan and Rada Mihalcea. 2000. Using Word-
Net and lexical operators to improve Internet searches.
IEEE Internet Computing, 4(1):34?43.
Jane Morris and Graeme Hirst. 2004. Non-Classical
Lexical Semantic Relations. In Workshop on Com-
putational Lexical Semantics, Human Language Tech-
nology Conference of the North American Chapter of
the ACL, Boston.
Christof Mu?ller and Iryna Gurevych. 2006. Exploring
the Potential of Semantic Relatedness in Information
Retrieval. In Proceedings of LWA 2006 Lernen - Wis-
sensentdeckung - Adaptivita?t: Information Retrieval,
pages 126?131, Hildesheim, Germany. GI-Fachgruppe
Information Retrieval.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Gerard Salton andMichael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill, New
York.
Gerard Salton, Edward Fox, and Harry Wu. 1983. Ex-
tended boolean information retrieval. Communication
of the ACM, 26(11):1022?1036.
Alan F. Smeaton, Fergus Kelledy, and Ruari O?Donell.
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding posting lists, query expansion with
WordNet and POS tagging of Spanish. In Proceedings
of TREC-4, pages 373?390.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Ellen Vorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th An-
nual ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 61?69.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader?s Digest, das Beste fu?r Deutschland. Jan
2001?Dec 2005. Verlag Das Beste, Stuttgart.
Torsten Zesch and Iryna Gurevych. 2006. Automatically
Creating Datasets for Measures of Semantic Related-
ness. In Proceedings of the Workshop on Linguistic
Distances, pages 16?24, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007a. Analyzing and Accessing Wikipedia as a Lexi-
cal Semantic Resource. In Biannual Conference of the
Society for Computational Linguistics and Language
Technology, pages 213?221, Tuebingen, Germany.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007b. Comparing Wikipedia and German Word-
net by Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings of NAACL-HLT.
1039
Proceedings of the Workshop on Linguistic Distances, pages 16?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically creating datasets for measures of semantic relatedness
Torsten Zesch and Iryna Gurevych
Department of Telecooperation
Darmstadt University of Technology
D-64289 Darmstadt, Germany
{zesch,gurevych} (at) tk.informatik.tu-darmstadt.de
Abstract
Semantic relatedness is a special form of
linguistic distance between words. Eval-
uating semantic relatedness measures is
usually performed by comparison with hu-
man judgments. Previous test datasets had
been created analytically and were limited
in size. We propose a corpus-based system
for automatically creating test datasets.1
Experiments with human subjects show
that the resulting datasets cover all de-
grees of relatedness. As a result of the
corpus-based approach, test datasets cover
all types of lexical-semantic relations and
contain domain-specific words naturally
occurring in texts.
1 Introduction
Linguistic distance plays an important role in
many applications like information retrieval, word
sense disambiguation, text summarization or
spelling correction. It is defined on different kinds
of textual units, e.g. documents, parts of a docu-
ment (e.g. words and their surrounding context),
words or concepts (Lebart and Rajman, 2000).2
Linguistic distance between words is inverse to
their semantic similarity or relatedness.
Semantic similarity is typically defined via the
lexical relations of synonymy (automobile ? car)
and hypernymy (vehicle ? car), while semantic
relatedness (SR) is defined to cover any kind of
lexical or functional association that may exist be-
1In the near future, we are planning to make the software
available to interested researchers.
2In this paper, word denotes the graphemic form of a to-
ken and concept refers to a particular sense of a word.
tween two words (Gurevych, 2005).3 Dissimilar
words can be semantically related, e.g. via func-
tional relationships (night ? dark) or when they
are antonyms (high ? low). Many NLP applica-
tions require knowledge about semantic related-
ness rather than just similarity (Budanitsky and
Hirst, 2006).
A number of competing approaches for comput-
ing semantic relatedness of words have been de-
veloped (see Section 2). A commonly accepted
method for evaluating these approaches is to com-
pare their results with a gold standard based on
human judgments on word pairs. For that pur-
pose, relatedness scores for each word pair have
to be determined experimentally. Creating test
datasets for such experiments has so far been a
labor-intensive manual process.
We propose a corpus-based system to automat-
ically create test datasets for semantic relatedness
experiments. Previous datasets were created ana-
lytically, preventing their use to gain insights into
the nature of SR and also not necessarily reflecting
the reality found in a corpus. They were also lim-
ited in size. We provide a larger annotated test set
that is used to better analyze the connections and
differences between the approaches for computing
semantic relatedness.
The remainder of this paper is organized as fol-
lows: we first focus on the notion of semantic re-
latedness and how it can be evaluated. Section 3
reviews related work. Section 4 describes our sys-
tem for automatically extracting word pairs from a
corpus. Furthermore, the experimental setup lead-
ing to human judgments of semantic relatedness
3Nevertheless the two terms are often (mis)used inter-
changeably. We will use semantic relatedness in the remain-
der of this paper, as it is the more general term that subsumes
semantic similarity.
16
is presented. Section 5 discusses the results, and
finally we draw some conclusions in Section 6.
2 Evaluating SR measures
Various approaches for computing semantic re-
latedness of words or concepts have been pro-
posed, e.g. dictionary-based (Lesk, 1986),
ontology-based (Wu and Palmer, 1994; Leacock
and Chodorow, 1998), information-based (Resnik,
1995; Jiang and Conrath, 1997) or distributional
(Weeds and Weir, 2005). The knowledge sources
used for computing relatedness can be as different
as dictionaries, ontologies or large corpora.
According to Budanitsky and Hirst (2006),
there are three prevalent approaches for evaluating
SR measures: mathematical analysis, application-
specific evaluation and comparison with human
judgments.
Mathematical analysis can assess a measure
with respect to some formal properties, e.g.
whether a measure is a metric (Lin, 1998).4 How-
ever, mathematical analysis cannot tell us whether
a measure closely resembles human judgments or
whether it performs best when used in a certain
application.
The latter question is tackled by application-
specific evaluation, where a measure is tested
within the framework of a certain application,
e.g. word sense disambiguation (Patwardhan et
al., 2003) or malapropism detection (Budanitsky
and Hirst, 2006). Lebart and Rajman (2000) ar-
gue for application-specific evaluation of similar-
ity measures, because measures are always used
for some task. But they also note that evaluating
a measure as part of a usually complex applica-
tion only indirectly assesses its quality. A certain
measure may work well in one application, but not
in another. Application-based evaluation can only
state the fact, but give little explanation about the
reasons.
The remaining approach - comparison with hu-
man judgments - is best suited for application
independent evaluation of relatedness measures.
Human annotators are asked to judge the related-
ness of presented word pairs. Results from these
experiments are used as a gold standard for eval-
uation. A further advantage of comparison with
human judgments is the possibility to gain deeper
4That means, whether it fulfills some mathematical crite-
ria: d(x, y) ? 0; d(x, y) = 0 ? x = y; d(x, y) = d(y, x);
d(x, z) ? d(x, y) + d(y, z).
insights into the nature of semantic relatedness.
However, creating datasets for evaluation has so
far been limited in a number of respects. Only
a small number of word pairs was manually se-
lected, with semantic similarity instead of related-
ness in mind. Word pairs consisted only of noun-
noun combinations and only general terms were
included. Polysemous and homonymous words
were not disambiguated to concepts, i.e. humans
annotated semantic relatedness of words rather
than concepts.
3 Related work
In the seminal work by Rubenstein and Goode-
nough (1965), similarity judgments were obtained
from 51 test subjects on 65 noun pairs written on
paper cards. Test subjects were instructed to order
the cards according to the ?similarity of meaning?
and then assign a continuous similarity value (0.0 -
4.0) to each card. Miller and Charles (1991) repli-
cated the experiment with 38 test subjects judg-
ing on a subset of 30 pairs taken from the original
65 pairs. This experiment was again replicated by
Resnik (1995) with 10 subjects. Table 1 summa-
rizes previous experiments.
A comprehensive evaluation of SR measures re-
quires a higher number of word pairs. However,
the original experimental setup is not scalable as
ordering several hundred paper cards is a cum-
bersome task. Furthermore, semantic relatedness
is an intuitive concept and being forced to assign
fine-grained continuous values is felt to overstrain
the test subjects. Gurevych (2005) replicated the
experiment of Rubenstein and Goodenough with
the original 65 word pairs translated into German.
She used an adapted experimental setup where test
subjects had to assign discrete values {0,1,2,3,4}
and word pairs were presented in isolation. This
setup is also scalable to a higher number of word
pairs (350) as was shown in Gurevych (2006).
Finkelstein et al (2002) annotated a larger set of
word pairs (353), too. They used a 0-10 range of
relatedness scores, but did not give further details
about their experimental setup. In psycholinguis-
tics, relatedness of words can also be determined
through association tests (Schulte im Walde and
Melinger, 2005). Results of such experiments are
hard to quantify and cannot easily serve as the ba-
sis for evaluating SR measures.
Rubenstein and Goodenough selected word
pairs analytically to cover the whole spectrum of
17
CORRELATION
PAPER LANGUAGE PAIRS POS REL-TYPE SCORES # SUBJECTS INTER INTRA
R/G (1965) English 65 N sim continuous 0?4 51 - .850
M/C (1991) English 30 N sim continuous 0?4 38 - -
Res (1995) English 30 N sim continuous 0?4 10 .903 -
Fin (2002) English 353 N, V, A relat continuous 0?10 16 - -
Gur (2005) German 65 N sim discrete {0,1,2,3,4} 24 .810 -
Gur (2006) German 350 N, V, A relat discrete {0,1,2,3,4} 8 .690 -
Z/G (2006) German 328 N, V, A relat discrete {0,1,2,3,4} 21 .478 .647
Table 1: Comparison of previous experiments. R/G=Rubenstein and Goodenough, M/C=Miller and
Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych
similarity from ?not similar? to ?synonymous?.
This elaborate process is not feasible for a larger
dataset or if domain-specific test sets should be
compiled quickly. Therefore, we automatically
create word pairs using a corpus-based approach.
We assume that due to lexical-semantic cohesion,
texts contain a sufficient number of words re-
lated by means of different lexical and semantic
relations. Resulting from our corpus-based ap-
proach, test sets will also contain domain-specific
terms. Previous studies only included general
terms as opposed to domain-specific vocabularies
and therefore failed to produce datasets that can
be used to evaluate the ability of a measure to cope
with domain-specific or technical terms. This is an
important property if semantic relatedness is used
in information retrieval where users tend to use
specific search terms (Porsche) rather than general
ones (car).
Furthermore, manually selected word pairs
are often biased towards highly related pairs
(Gurevych, 2006), because human annotators tend
to select only highly related pairs connected by re-
lations they are aware of. Automatic corpus-based
selection of word pairs is more objective, leading
to a balanced dataset with pairs connected by all
kinds of lexical-semantic relations. Morris and
Hirst (2004) pointed out that many relations be-
tween words in a text are non-classical (i.e. other
than typical taxonomic relations like synonymy or
hypernymy) and therefore not covered by seman-
tic similarity.
Previous studies only considered semantic re-
latedness (or similarity) of words rather than con-
cepts. However, polysemous or homonymous
words should be annotated on the level of con-
cepts. If we assume that bank has two meanings
(?financial institution? vs. ?river bank?)5 and it is
paired with money, the result is two sense quali-
5WordNet lists 10 meanings.
fied pairs (bankfinancial ? money) and (bankriver
? money). It is obvious that the judgments on the
two concept pairs should differ considerably. Con-
cept annotated datasets can be used to test the abil-
ity of a measure to differentiate between senses
when determining the relatedness of polysemous
words. To our knowledge, this study is the first to
include concept pairs and to automatically gener-
ate the test dataset.
In our experiment, we annotated a high number
of pairs similar in size to the test sets by Finkel-
stein (2002) and Gurevych (2006). We used the re-
vised experimental setup (Gurevych, 2005), based
on discrete relatedness scores and presentation of
word pairs in isolation, that is scalable to the
higher number of pairs. We annotated semantic
relatedness instead of similarity and included also
non noun-noun pairs. Additionally, our corpus-
based approach includes domain-specific techni-
cal terms and enables evaluation of the robustness
of a measure.
4 Experiment
4.1 System architecture
Figure 1 gives an overview of our automatic
corpus-based system for creating test datasets for
evaluating SR measures.
In the first step, a source corpus is preprocessed
using tokenization, POS-tagging and lemmatiza-
tion resulting in a list of POS-tagged lemmas.
Randomly generating word pairs from this list
would result in too many unrelated pairs, yielding
an unbalanced dataset. Thus, we assign weights to
each word (e.g. using tf.idf-weighting). The most
important document-specific words get the high-
est weights and due to lexical cohesion of the doc-
uments many related words can be found among
the top rated. Therefore, we randomly generate
a user-defined number of word pairs from the r
words with the highest weights for each document.
18
Co
rpu
s
Tok
eni
zat
ion
PO
S-t
agg
ing
Lem
ma
tiza
tion
Ter
m
we
igh
tin
g
Wo
rd-
con
cep
t
ma
ppi
ng
con
cep
t p
airs
wit
h g
los
ses
Wo
rd 
pai
r
gen
era
tor
Pre
pro
ces
sin
g
Wo
rd 
pai
r
filte
r
tf.id
f
Wo
rd 
sen
se
dic
tion
ary
Ab
bre
via
tion
s
Sto
plis
t
oth
er u
ser
def
ine
d fi
lter
s
PO
S
com
bin
atio
ns
Figure 1: System architecture for extraction of
concept pairs.
In the next step, user defined filters are applied
to the initial list of word pairs. For example, a fil-
ter can remove all pairs containing only uppercase
letters (mostly acronyms). Another filter can en-
force a certain fraction of POS combinations to be
present in the result set.
As we want to obtain judgment scores for se-
mantic relatedness of concepts instead of words,
we have to include all word sense combinations of
a pair in the list. An external dictionary of word
senses is necessary for this step. It is also used to
add a gloss for each word sense that enables test
subjects to distinguish between senses.
If differences in meaning between senses are
very fine-grained, distinguishing between them is
hard even for humans (Mihalcea and Moldovan,
2001).6 Pairs containing such words are not suit-
able for evaluation. To limit their impact on the
experiment, a threshold for the maximal number
of senses can be defined. Words with a number of
senses above the threshold are removed from the
list.
The result of the extraction process is a list of
sense disambiguated, POS-tagged pairs of con-
cepts.
6E.g. the German verb ?halten? that can be translated as
hold, maintain, present, sustain, etc. has 26 senses in Ger-
maNet.
4.2 Experimental setup
4.2.1 Extraction of concept pairs
We extracted word pairs from three different
domain-specific corpora (see Table 2). This is
motivated by the aim to enable research in infor-
mation retrieval incorporating SR measures. In
particular, the ?Semantic Information Retrieval?
project (SIR Project, 2006) systematically investi-
gates the use of lexical-semantic relations between
words or concepts for improving the performance
of information retrieval systems.
The BERUFEnet (BN) corpus7 consists of de-
scriptions of 5,800 professions in Germany and
therefore contains many terms specific to profes-
sional training. Evaluating semantic relatedness
on a test set based on this corpus may reveal the
ability of a measure to adapt to a very special do-
main. The GIRT (German Indexing and Retrieval
Testdatabase) corpus (Kluck, 2004) is a collec-
tion of abstracts of social science papers. It is a
standard corpus for evaluating German informa-
tion retrieval systems. The third corpus is com-
piled from 106 arbitrarily selected scientific Pow-
erPoint presentations (SPP). They cover a wide
range of topics from bio genetics to computer sci-
ence and contain many technical terms. Due to
the special structure of presentations, this corpus
will be particularly demanding with respect to the
required preprocessing components of an informa-
tion retrieval system.
The three preprocessing steps (tokenization,
POS-tagging, lemmatization) are performed us-
ing TreeTagger (Schmid, 1995). The resulting
list of POS-tagged lemmas is weighted using the
SMART ?ltc?8 tf.idf-weighting scheme (Salton,
1989).
We implemented a set of filters for word pairs.
One group of filters removed unwanted word
pairs. Word pairs are filtered if they contain at
least one word that a) has less than three letters b)
contains only uppercase letters (mostly acronyms)
or c) can be found in a stoplist. Another fil-
ter enforced a specified fraction of combinations
of nouns (N), verbs (V) and adjectives (A) to be
present in the result set. We used the following pa-
rameters: NN = 0.5, NV = 0.15, NA = 0.15,
V V = 0.1, V A = 0.05, AA = 0.05. That means
50% of the resulting word pairs for each corpus
7http://berufenet.arbeitsagentur.de
8l=logarithmic term frequency, t=logarithmic inverse doc-
ument frequency, c=cosine normalization.
19
CORPUS # DOCS # TOKENS DOMAIN
BN 9,022 7,728,501
descriptions
of professions
GIRT 151,319 19,645,417
abstracts of social
science papers
SPP 106 144,074
scientific .ppt
presentations
Table 2: Corpus statistics.
were noun-noun pairs, 15% noun-verb pairs and
so on.
Word pairs containing polysemous words
are expanded to concept pairs using Ger-
maNet (Kunze, 2004), the German equivalent to
WordNet, as a sense inventory for each word. It
is the most complete resource of this type for Ger-
man.
GermaNet contains only a few conceptual
glosses. As they are required to enable test sub-
jects to distinguish between senses, we use artifi-
cial glosses composed from synonyms and hyper-
nyms as a surrogate, e.g. for brother: ?brother,
male sibling? vs. ?brother, comrade, friend?
(Gurevych, 2005). We removed words which had
more than three senses.
Marginal manual post-processing was neces-
sary, since the lemmatization process introduced
some errors. Foreign words were translated into
German, unless they are common technical termi-
nology. We initially selected 100 word pairs from
each corpus. 11 word pairs were removed be-
cause they comprised non-words. Expanding the
word list to a concept list increased the size of the
list. Thus, the final dataset contained 328 automat-
ically created concept pairs.
4.2.2 Graphical User Interface
We developed a web-based interface to obtain
human judgments of semantic relatedness for each
automatically generated concept pair. Test sub-
jects were invited via email to participate in the
experiment. Thus, they were not supervised dur-
ing the experiment.
Gurevych (2006) observed that some annotators
were not familiar with the exact definition of se-
mantic relatedness. Their results differed particu-
larly in cases of antonymy or distributionally re-
lated pairs. We created a manual with a detailed
introduction to SR stressing the crucial points.
The manual was presented to the subjects before
the experiment and could be re-accessed at any
time.
Figure 2: Screenshot of the GUI. Polysemous
words are defined by means of synonyms and re-
lated words.
During the experiment, one concept pair at a
time was presented to the test subjects in random
ordering. Subjects had to assign a discrete related-
ness value {0,1,2,3,4} to each pair. Figure 2 shows
the system?s GUI.
In case of a polysemous word, synonyms or
related words were presented to enable test sub-
jects to understand the sense of a presented con-
cept. Because this additional information can lead
to undesirable priming effects, test subjects were
instructed to deliberately decide only about the re-
latedness of a concept pair and use the gloss solely
to understand the sense of the presented concept.
Since our corpus-based approach includes
domain-specific vocabulary, we could not assume
that the subjects were familiar with all words.
Thus, they were instructed to look up unknown
words in the German Wikipedia.9
Several test subjects were asked to repeat the
experiment with a minimum break of one day. Re-
sults from the repetition can be used to measure
intra-subject correlation. They can also be used
to obtain some hints on varying difficulty of judg-
ment for special concept pairs or parts-of-speech.
5 Results and discussion
21 test subjects (13 males, 8 females) participated
in the experiment, two of them repeated it. The
average age of the subjects was 26 years. Most
subjects had an IT background. The experiment
took 39 minutes on average, leaving about 7 sec-
onds for rating each concept pair.
The summarized inter-subject correlation be-
tween 21 subjects was r=.478 (cf. Table 3), which
9http://www.wikipedia.de
20
CONCEPTS WORDS
INTER INTRA INTER INTRA
all .478 .647 .490 .675
BN .469 .695 .501 .718
GIRT .451 .598 .463 .625
SPP .535 .649 .523 .679
AA .556 .890 .597 .887
NA .547 .773 .511 .758
NV .510 .658 .540 .647
NN .463 .620 .476 .661
VA .317 .318 .391 .212
VV .278 .494 .301 .476
Table 3: Summarized correlation coefficients for
all pairs, grouped by corpus and grouped by POS
combinations.
is statistically significant at p < .05. This correla-
tion coefficient is an upper bound of performance
for automatic SR measures applied on the same
dataset.
Resnik (1995) reported a correlation of
r=.9026.10 The results are not directly compara-
ble, because he only used noun-noun pairs, words
instead of concepts, a much smaller dataset, and
measured semantic similarity instead of semantic
relatedness. Finkelstein et al (2002) did not
report inter-subject correlation for their larger
dataset. Gurevych (2006) reported a correlation
of r=.69. Test subjects were trained students of
computational linguistics, and word pairs were
selected analytically.
Evaluating the influence of using concept pairs
instead of word pairs is complicated because word
level judgments are not directly available. There-
fore, we computed a lower and an upper bound
for correlation coefficients. For the lower bound,
we always selected the concept pair with highest
standard deviation from each set of corresponding
concept pairs. The upper bound is computed by
selecting the concept pair with the lowest standard
deviation. The differences between correlation co-
efficient for concepts and words are not signifi-
cant. Table 3 shows only the lower bounds.
Correlation coefficients for experiments mea-
suring semantic relatedness are expected to be
lower than results for semantic similarity, since the
former also includes additional relations (like co-
occurrence of words) and is thus a more compli-
cated task. Judgments for such relations strongly
depend on experience and cultural background of
the test subjects. While most people may agree
10Note that Resnik used the averaged correlation coeffi-
cient. We computed the summarized correlation coefficient
using a Fisher Z-value transformation.
01234 0
50
100
150
200
250
300
350
Conc
ept p
air
Semantic relatedness score
Figure 3: Distribution of averaged human judg-
ments.
01234 0
10
20
30
40
50
Conc
ept p
air
Semantic relatedness scores
Figure 4: Distribution of averaged human judg-
ments with standard deviation < 0.8.
that (car ? vehicle) are highly related, a strong
connection between (parts ? speech) may only be
established by a certain group. Due to the corpus-
based approach, many domain-specific concept
pairs are introduced into the test set. Therefore,
inter-subject correlation is lower than the results
obtained by Gurevych (2006).
In our experiment, intra-subject correlation was
r=.670 for the first and r=.623 for the second in-
dividual who repeated the experiment, yielding
a summarized intra-subject correlation of r=.647.
Rubenstein and Goodenough (1965) reported an
intra-subject correlation of r=.85 for 15 subjects
judging the similarity of a subset (36) of the orig-
inal 65 word pairs. The values may again not be
compared directly. Furthermore, we cannot gen-
eralize from these results, because the number of
participants which repeated our experiment was
too low.
The distribution of averaged human judgments
on the whole test set (see Figure 3) is almost bal-
anced with a slight underrepresentation of highly
related concepts. To create more highly re-
lated concept pairs, more sophisticated weighting
schemes or selection on the basis of lexical chain-
21
00.30.60.91.21.51.8 0
1
2
3
4
Aver
aged
 judg
men
t
Standard deviation
Figure 5: Averaged judgments and standard devia-
tion for all concept pairs. Low deviations are only
observed for low or high judgments.
ing could be used. However, even with the present
setup, automatic extraction of concept pairs per-
forms remarkably well and can be used to quickly
create balanced test datasets.
Budanitsky and Hirst (2006) pointed out that
distribution plots of judgments for the word pairs
used by Rubenstein and Goodenough display an
empty horizontal band that could be used to sepa-
rate related and unrelated pairs. This empty band
is not observed here. However, Figure 4 shows the
distribution of averaged judgments with the high-
est agreement between annotators (standard devi-
ation < 0.8). The plot clearly shows an empty hor-
izontal band with no judgments. The connection
between averaged judgments and standard devia-
tion is plotted in Figure 5.
When analyzing the concept pairs with lowest
deviation there is a clear tendency for particularly
highly related pairs, e.g. hypernymy: Universit?t
? Bildungseinrichtung (university ? educational
institution); functional relation: T?tigkeit ? aus-
f?hren (task ? perform); or pairs that are obviously
not connected, e.g. logisch ? Juni (logical ? June).
Table 4 lists some example concept pairs along
with averaged judgments and standard deviation.
Concept pairs with high deviations between
judgments often contain polysemous words. For
example, Quelle (source) was disambiguated to
Wasserquelle (spring) and paired with Text
(text). The data shows a clear distinction be-
tween one group that rated the pair low (0) and
another group that rated the pair high (3 or 4). The
latter group obviously missed the point that tex-
tual source was not an option here. High devia-
tions were also common among special technical
terms like (Mips ?Core), proper names (Georg ?
August ? two common first names in German) or
functionally related pairs (agieren ? mobil). Hu-
man experience and cultural background clearly
influence the judgment of such pairs.
The effect observed here and the effect noted
by Budanitsky and Hirst is probably caused by the
same underlying principle. Human agreement on
semantic relatedness is only reliable if two words
or concepts are highly related or almost unrelated.
Intuitively, this means that classifying word pairs
as related or unrelated is much easier than numeri-
cally rating semantic relatedness. For an informa-
tion retrieval task, such a classification might be
sufficient.
Differences in correlation coefficients for the
three corpora are not significant indicating that the
phenomenon is not domain-specific. Differences
in correlation coefficients for different parts-of-
speech are significant (see Table 3). Verb-verb and
verb-adjective pairs have the lowest correlation.
A high fraction of these pairs is in the problem-
atic medium relatedness area. Adjective-adjective
pairs have the highest correlation. Most of these
pairs are either highly related or not related at all.
6 Conclusion
We proposed a system for automatically creating
datasets for evaluating semantic relatedness mea-
sures. We have shown that our corpus-based ap-
proach enables fast development of large domain-
specific datasets that cover all types of lexical and
semantic relations. We conducted an experiment
to obtain human judgments of semantic related-
ness on concept pairs. Results show that averaged
human judgments cover all degrees of relatedness
with a slight underrepresentation of highly related
concept pairs. More highly related concept pairs
could be generated by using more sophisticated
weighting schemes or selecting concept pairs on
the basis of lexical chaining.
Inter-subject correlation in this experiment is
lower than the results from previous studies due
to several reasons. We measured semantic relat-
edness instead of semantic similarity. The for-
mer is a more complicated task for annotators be-
cause its definition includes all kinds of lexical-
semantic relations not just synonymy. In addition,
concept pairs were automatically selected elimi-
nating the bias towards strong classical relations
with high agreement that is introduced into the
dataset by a manual selection process. Further-
more, our dataset contains many domain-specific
22
PAIR
GERMAN ENGLISH CORPUS AVG ST-DEV
Universit?t ? Bildungseinrichtung university ? educational institution GIRT 3.90 0.30
T?tigkeit ? ausf?hren task ? to perform BN 3.67 0.58
strafen ? Paragraph to punish ? paragraph GIRT 3.00 1.18
Quelle ? Text spring ? text GIRT 2.43 1.57
Mips ? Core mips ? core SPP 2.10 1.55
elektronisch ? neu electronic ? new GIRT 1.71 1.15
verarbeiten ? dichten to manipulate ? to caulk BN 1.29 1.42
Leopold ? Institut Leopold ? institute SPP 0.81 1.25
Outfit ? Strom outfit ? electricity GIRT 0.24 0.44
logisch ? Juni logical ? June SPP 0.14 0.48
Table 4: Example concept pairs with averaged judgments and standard deviation. Only one sense is
listed for polysemous words. Conceptual glosses are omitted due to space limitations.
concept pairs which have been rated very differ-
ently by test subjects depending on their expe-
rience. Future experiments should ensure that
domain-specific pairs are judged by domain ex-
perts to reduce disagreement between annotators
caused by varying degrees of familiarity with the
domain.
An analysis of the data shows that test sub-
jects more often agreed on highly related or unre-
lated concept pairs, while they often disagreed on
pairs with a medium relatedness value. This result
raises the question whether human judgments of
semantic relatedness with medium scores are re-
liable and should be used for evaluating seman-
tic relatedness measures. We plan to investigate
the impact of this outcome on the evaluation of
semantic relatedness measures. Additionally, for
some applications like information retrieval it may
be sufficient to detect highly related pairs rather
than accurately rating word pairs with medium
values.
There is also a significant difference between
the correlation coefficient for different POS com-
binations. Further investigations are needed to elu-
cidate whether these differences are caused by the
new procedure for corpus-based selection of word
pairs proposed in this paper or are due to inherent
properties of semantic relations existing between
word classes.
Acknowledgments
We would like to thank Sabine Schulte im Walde
for her remarks on experimental setups. We are
grateful to the Bundesagentur f?r Arbeit for pro-
viding the BERUFEnet corpus. This work was
carried out as part of the ?Semantic Information
Retrieval? (SIR) project funded by the German
Research Foundation.
References
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating
WordNet-based Measures of Semantic Distance. Compu-
tational Linguistics, 32(1).
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, and Gadi Wolfman. 2002. Placing
Search in Context: The Concept Revisited. ACM Trans-
actions on Information Systems, 20(1):116?131.
Iryna Gurevych. 2005. Using the Structure of a Conceptual
Network in Computing Semantic Relatedness. In Pro-
ceedings of the 2nd International Joint Conference on Nat-
ural Language Processing, pages 767?778, Jeju Island,
Republic of Korea.
Iryna Gurevych. 2006. Computing Semantic Relatedness
Across Parts of Speech. Technical report, Darmstadt Uni-
versity of Technology, Germany, Department of Computer
Science, Telecooperation.
Jay J. Jiang and David W. Conrath. 1997. Semantic Similar-
ity Based on Corpus Statistics and Lexical Taxonomy. In
Proceedings of the 10th International Conference on Re-
search in Computational Linguistics.
Michael Kluck. 2004. The GIRT Data in the Evaluation of
CLIR Systems - from 1997 Until 2003. Lecture Notes in
Computer Science, 3237:376?390, January.
Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze,
chapter Computerlinguistik und Sprachtechnologie, pages
423?431. Spektrum Akademischer Verlag.
Claudia Leacock and Martin Chodorow, 1998. WordNet: An
Electronic Lexical Database, chapter Combining Local
Context and WordNet Similarity for Word Sense Identi-
fication, pages 265?283. Cambridge: MIT Press.
Ludovic Lebart and Martin Rajman. 2000. Computing Sim-
ilarity. In Robert Dale, editor, Handbook of NLP. Dekker:
Basel.
Michael Lesk. 1986. Automatic Sense Disambiguation Us-
ing Machine Readable Dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the 5th
Annual International Conference on Systems Documenta-
tion, pages 24?26, Toronto, Ontario, Canada.
Dekang Lin. 1998. An Information-Theoretic Definition of
Similarity. In Proceedings of International Conference on
Machine Learning, Madison, Wisconsin.
23
Rada Mihalcea and Dan Moldovan. 2001. Automatic Gen-
eration of a Coarse Grained WordNet. In Proceedings
of NAACL Workshop on WordNet and Other Lexical Re-
sources, Pittsburgh, PA, June.
George A. Miller and Walter G. Charles. 1991. Contextual
Correlates of Semantic Similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical
Semantic Relations. In Workshop on Computational Lex-
ical Semantics, Human Language Technology Conference
of the North American Chapter of the ACL, Boston.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. 2003. Using Measures of Semantic Relatedness
for Word Sense Disambiguation. In Proceedings of the
Fourth International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico City.
Philip Resnik. 1995. Using Information Content to Evalu-
ate Semantic Similarity. In Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence, pages
448?453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual Correlates of Synonymy. Communications of the
ACM, 8(10):627?633.
Gerard Salton. 1989. Automatic Text Processing: the Trans-
formation, Analysis, and Retrieval of Information by Com-
puter. Addison-Wesley Longman Publishing, Boston,
MA, USA.
Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference on
New Methods in Language Processing, Manchester, UK.
Sabine Schulte im Walde and Alissa Melinger. 2005. Iden-
tifying Semantic Relations and Functional Properties of
Human Verb Associations. In Proceedings of the Joint
Conference on Human Language Technology and Empiri-
cal Methods in NLP, pages 612?619, Vancouver, Canada.
SIR Project. 2006. Project ?Semantic Information
Retrieval?. URL http://www.cre-elearning.
tu-darmstadt.de/elearning/sir/.
Julie Weeds and David Weir. 2005. Co-occurrence Retrieval:
A Flexible Framework For Lexical Distributional Similar-
ity. Computational Linguistics, 31(4):439?475, Decem-
ber.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and
Lexical Selection. In 32nd Annual Meeting of the ACL,
pages 133?138, NewMexico State University, Las Cruces,
New Mexico.
24
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 1?8,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Analysis of the Wikipedia Category Graph for NLP Applications
Torsten Zesch and Iryna Gurevych
Ubiquitous Knowledge Processing Group
Telecooperation Division
Darmstadt University of Technology, Hochschulstra?e 10
D-64289 Darmstadt, Germany
{zesch,gurevych} (at) tk.informatik.tu-darmstadt.de
Abstract
In this paper, we discuss two graphs in
Wikipedia (i) the article graph, and (ii)
the category graph. We perform a graph-
theoretic analysis of the category graph,
and show that it is a scale-free, small
world graph like other well-known lexi-
cal semantic networks. We substantiate
our findings by transferring semantic re-
latedness algorithms defined on WordNet
to the Wikipedia category graph. To as-
sess the usefulness of the category graph
as an NLP resource, we analyze its cover-
age and the performance of the transferred
semantic relatedness algorithms.
1 Introduction
Wikipedia1 is a free multi-lingual online encyclo-
pedia that is constructed in a collaborative effort
of voluntary contributors and still grows exponen-
tially. During this process, Wikipedia has proba-
bly become the largest collection of freely available
knowledge. A part of this knowledge is encoded in
the network structure of Wikipedia pages. In par-
ticular, Wikipedia articles form a network of seman-
tically related terms, while the categories are orga-
nized in a taxonomy-like structure calledWikipedia
Category Graph (WCG).
In this paper, we perform a detailed analysis of
the WCG by computing a set of graph-theoretic pa-
rameters, and comparing them with the parameters
reported for well-known graphs and classical lexical
semantic networks. We show that the WCG, which
is constructed collaboratively, shares many proper-
ties with other lexical semantic networks, such as
1http://www.wikipedia.org
C 1
C 2
C 3
C 4
C 5
A 1
A 2
A 3
A 4
WC
G
Artic
le G
raph
Figure 1: Relations between article graph andWCG.
WordNet (Fellbaum, 1998) or Roget?s Thesaurus2
that are constructed by expert authors. This implies
that the WCG can be used as a resource in NLP ap-
plications, where other semantic networks have been
traditionally employed.
To further evaluate this issue, we adapt algorithms
for computing semantic relatedness on classical se-
mantic networks like WordNet to the WCG. We
evaluate their performance on the task of computing
semantic relatedness using three German datasets,
and show that WCG based algorithms perform very
well.
Article graph Wikipedia articles are heavily
linked, as links can be easily inserted while editing
an article. If we treat each article as a node, and
each link between articles as an edge running from
one node to another, then Wikipedia articles form
a directed graph (see right side of Figure 1). The
article graph has been targeted by numerous stud-
ies, and is not addressed in this paper. Buriol et al
(2006) analyze the development of the article graph
over time, and find that some regions are fairly sta-
ble, while others are advancing quickly. Zlatic et al
2http://thesaurus.reference.com
1
Figure 2: Structures of semantic networks after Steyvers and Tenenbaum (2005). a) a taxonomy, b) an
arbitrary graph, c) scale-free, small-world graph.
(2006) give a comprehensive overview of the graph
parameters for the largest languages in Wikipedia.
Capocci et al (2006) study the growth of the article
graph and show that it is based on preferential at-
tachment (Barabasi and Albert, 1999). Voss (2005)
shows that the article graph is scale-free and grows
exponentially.
Category graph Categories in Wikipedia are or-
ganized in a taxonomy-like structure (see left side of
Figure 1 and Figure 2-a). Each category can have an
arbitrary number of subcategories, where a subcate-
gory is typically established because of a hyponymy
or meronymy relation. For example, a category ve-
hicle has subcategories like aircraft or watercraft.
Thus, the WCG is very similar to semantic word-
nets like WordNet or GermaNet (Kunze, 2004). As
Wikipedia does not strictly enforce a taxonomic cat-
egory structure, cycles and disconnected categories
are possible, but rare. In the snapshot of the Ger-
manWikipedia3 fromMay 15, 2006, the largest con-
nected component in the WCG contains 99,8% of all
category nodes, as well as 7 cycles.
In Wikipedia, each article can link to an arbitrary
number of categories, where each category is a kind
of semantic tag for that article. A category back-
links to all articles in this category. Thus, article
graph and WCG are heavily interlinked (see Fig-
ure 1), and most studies (Capocci et al, 2006; Zlatic
et al, 2006) have not treated them separately. How-
ever, the WCG should be treated separately, as it
differs from the article graph. Article links are es-
tablished because of any kind of relation between
3Wikipedia can be downloaded from http:
//download.wikimedia.org/
articles, while links between categories are typically
established because of hyponymy or meronymy re-
lations.
Holloway et al (2005) create and visualize a cat-
egory map based on co-occurrence of categories.
Voss (2006) pointed out that the WCG is a kind of
thesaurus that combines collaborative tagging and
hierarchical indexing. Zesch et al (2007a) identified
the WCG as a valueable source of lexical semantic
knowledge, but did not analytically analyze its prop-
erties. However, even if the WCG seems to be very
similar to other semantic wordnets, a graph-theoretic
analysis of the WCG is necessary to substantiate this
claim. It is carried out in the next section.
2 Graph-theoretic Analysis of the WCG
A graph-theoretic analysis of the WCG is required
to estimate, whether graph based semantic related-
ness measures developed for semantic wordnets can
be transferred to the WCG. This is substantiated in
a case study on computing semantic relatedness in
section 4.
For our analysis, we treat the directed WCG as
an undirected graph G := (V,E),4 as the relations
connecting categories are reversible. V is a set of
vertices or nodes. E is a set of unordered pairs of
distinct vertices, called edges. Each page is treated
as a node n, and each link between pages is modeled
as an edge e running between two nodes.
Following Steyvers and Tenenbaum (2005), we
characterize the graph structure of a lexical semantic
resource in terms of a set of graph parameters: The
4Newman (2003) gives a comprehensive overview about the
theoretical aspects of graphs.
2
PARAMETER Actor Power C.elegans AN Roget WordNet WikiArt WCG
|V | 225,226 4,941 282 5,018 9,381 122,005 190,099 27,865
D - - - 5 10 27 - 17
k 61.0 2.67 14.0 22.0 49.6 4.0 - 3.54
? - - - 3.01 3.19 3.11 2.45 2.12
L 3.65 18.7 2.65 3.04 5.60 10.56 3.34 7.18
Lrandom 2.99 12.4 2.25 3.03 5.43 10.61 ? 3.30 ? 8.10
C 0.79 0.08 0.28 0.186 0.87 0.027 ? 0.04 0.012
Crandom 0.0003 0.005 0.05 0.004 0.613 0.0001 ? 0.006 0.0008
Table 1: Parameter values for different graphs.
Values for Actor (collaboration graph of actors in feature films), Power (the electrical power grid of the western United
States) andC.elegans (the neural network of the nematode worm C. elegans) are fromWatts and Strogatz (1998). Values
for AN (a network of word associations by Nelson et al (1998)), Roget?s thesaurus and WordNet are from Steyvers
and Tenenbaum (2005). Values for Wikiart (German Wikipedia article graph) are from Zlatic et al (2006). We took
the values for the page set labelled M on their website containing 190,099 pages for German, as it comes closest to a
graph of only articles. Values marked with ?-? in the table were not reported in the studies. The values for the WCG are
computed in this study.
degree k of a node is the number of edges that are
connected with this node. Averaging over all nodes
gives the average degree k. The degree distribution
P (k) is the probability that a random node will have
degree k. In some graphs (like the WWW), the de-
gree distribution follows a power law P (k) ? k??
(Barabasi and Albert, 1999). We use the power law
exponent ? as a graph parameter.
A path pi,j is a sequence of edges that connects
a node ni with a node nj . The path length l(pi,j)
is the number of edges along that path. There can
be more than one path between two nodes. The
shortest path length L is the minimum of all these
paths, i.e. Li,j = min l(pi,j). Averaging over all
nodes gives the average shortest path length L.
The diameterD is the maximum of the shortest path
lengths between all pairs of nodes in the graph.
The cluster coefficient of a certain node ni can
be computed as
Ci =
Ti
ki(ki?1)
2
=
2Ti
ki(ki ? 1)
where Ti refers to the number of edges between the
neighbors of node ni and ki(ki ? 1)/2 is the maxi-
mum number of edges that can exist between the ki
neighbors of node ni.5 The cluster coefficient C for
the whole graph is the average of all Ci. In a fully
connected graph, the cluster coefficient is 1.
5In a social network, the cluster coefficient measures how
many of my friends (neighboring nodes) are friends themselves.
For our analysis, we use a snapshot of the German
Wikipedia fromMay 15, 2006. We consider only the
largest connected component of the WCG that con-
tains 99,8% of the nodes. Table 1 shows our results
on the WCG as well as the corresponding values for
other well-known graphs and lexical semantic net-
works. We compare our empirically obtained values
with the values expected for a random graph. Fol-
lowing Zlatic et al (2006), the cluster coefficient C
for a random graph is
Crandom =
(k
2
? k)2
|V |k
The average path length for a random network can
be approximated as Lrandom ? log |V | / log k
(Watts and Strogatz, 1998).
From the analysis, we conclude that all graphs
in Table 1 are small world graphs (see Figure 2-c).
Small world graphs (Watts and Strogatz, 1998) con-
tain local clusters that are connected by some long
range links leading to low values of L and D. Thus,
small world graphs are characterized by (i) small
values of L (typically L & Lrandom), together with
(ii) large values of C (C  Crandom).
Additionally, all semantic networks are scale-free
graphs, as their degree distribution follows a power
law. Structural commonalities between the graphs
in Table 1 are assumed to result from the growing
process based on preferential attachment (Capocci
et al, 2006).
3
Our analysis shows that WordNet and the WCG
are (i) scale-free, small world graphs, and (ii) have a
very similar parameter set. Thus, we conclude that
algorithms designed to work on the graph structure
of WordNet can be transferred to the WCG.
In the next section, we introduce the task of com-
puting semantic relatedness on graphs and adapt ex-
isting algorithms to the WCG. In section 4, we eval-
uate the transferred algorithms with respect to corre-
lation with human judgments on SR, and coverage.
3 Graph Based Semantic Relatedness
Measures
Semantic similarity (SS) is typically defined via the
lexical relations of synonymy (automobile ? car)
and hypernymy (vehicle ? car), while semantic re-
latedness (SR) is defined to cover any kind of lexi-
cal or functional association that may exist between
two words (Budanitsky and Hirst, 2006). Dissimi-
lar words can be semantically related, e.g. via func-
tional relationships (night ? dark) or when they are
antonyms (high ? low). Many NLP applications re-
quire knowledge about semantic relatedness rather
than just similarity (Budanitsky and Hirst, 2006).
We introduce a number of competing approaches
for computing semantic relatedness between words
using a graph structure, and then discuss the changes
that are necessary to adapt semantic relatedness al-
gorithms to work on the WCG.
3.1 Wordnet Based Measures
A multitude of semantic relatedness measures work-
ing on semantic networks has been proposed.
Rada et al (1989) use the path length (PL) be-
tween two nodes (measured in edges) to compute
semantic relatedness.
distPL = l(n1, n2)
Leacock and Chodorow (1998, LC) normalize the
path-length with the depth of the graph,
simLC(n1, n2) = ? log
l(n1, n2)
2? depth
where depth is the length of the longest path in the
graph.
Wu and Palmer (1994, WP) introduce a measure
that uses the notion of a lowest common subsumer of
two nodes lcs(n1, n2). In a directed graph, a lcs is
the parent of both child nodes with the largest depth
in the graph.
simWP =
2 depth(lcs)
l(n1, lcs) + l(n2, lcs) + 2 depth(lcs)
Resnik (1995, Res), defines semantic similarity be-
tween two nodes as the information content (IC)
value of their lcs. He used the relative corpus fre-
quency to estimate the information content value.
Jiang and Conrath (1997, JC) additionally use the
IC of the nodes.
distJC(n1, n2) = IC(n1) + IC(n2)? 2IC(lcs)
Note that JC returns a distance value instead of a
similarity value.
Lin (1998, Lin) defined semantic similarity using
a formula derived from information theory.
simLin(n1, n2) = 2?
IC(lcs)
IC(n1) + IC(n2)
Because polysemous words may have more than
one corresponding node in a semantic wordnet, the
resulting semantic relatedness between two words
w1 and w2 can be calculated as
SR =
?
??
??
min
n1?s(w1),n2?s(w2)
dist(n1, n2) path
max
n1?s(w1),n2?s(w2)
sim(n1, n2) IC
where s(wi) is the set of nodes that represent senses
of word wi. That means, the relatedness of two
words is equal to that of the most related pair of
nodes.
3.2 Adapting SR Measures to Wikipedia
Unlike other wordnets, nodes in the WCG do not
represent synsets or single terms, but a general-
ized concept or category. Therefore, we cannot use
the WCG directly to compute SR. Additionally, the
WCG would not provide sufficient coverage, as it is
relatively small. Thus, transferring SR measures to
the WCG requires some modifications. The task of
estimating SR between terms is casted to the task
of SR between Wikipedia articles devoted to these
terms. SR between articles is measured via the cate-
gories assigned to these articles.
4
OR
X X+1 X X+1
X
X+1
X+1
X
X+1
X+1
X
X+1
X+1
Figure 3: Breaking cycles in the WCG.
We define C1 and C2 as the set of categories as-
signed to article ai and aj , respectively. We then de-
termine the SR value for each category pair (ck, cl)
with ck ? C1 and cl ? C2. We choose the best
value among all pairs (ck, cl), i.e. the minimum for
path based and the maximum for information con-
tent based measures.
SRbest =
?
?
?
min
ck?C1,cl?C2
(sr(ck, cl)) path based
max
ck?C1,cl?C2
(sr(ck, cl)) IIC based
See (Zesch et al, 2007b) for a more detailed descrip-
tion of the adaptation process.
We substitute Resnik?s information content with
the intrinsic information content (IIC) by Seco et
al. (2004) that is computed only from structural in-
formation of the underlying graph. It yields better
results and is corpus independent. The IIC of a node
ni is computed as a function of its hyponyms,
IIC(n) = 1?
log(hypo(ni) + 1
log(|C|)
where hypo(ni) is the number of hyponyms of node
ni and |C| is the number of nodes in the taxonomy.
Efficiently counting the hyponyms of a node re-
quires to break cycles that may occur in a WCG.
We perform a colored depth-first-search to detect cy-
cles, and break them as visualized in Figure 3. A
link pointing back to a node closer to the top of the
graph is deleted, as it violates the rule that links in
the WCG typically express hyponymy or meronymy
relations. If the cycle occurs between nodes on the
same level, we cannot decide based on that rule and
simply delete one of the links running on the same
level. This strategy never disconnects any nodes
from a connected component.
4 Semantic Relatedness Experiments
A commonly accepted method for evaluating SR
measures is to compare their results with a gold stan-
dard dataset based on human judgments on word
pairs.6
4.1 Datasets
To create gold standard datasets for evaluation, hu-
man annotators are asked to judge the relatedness of
presented word pairs. The average annotation scores
are correlated with the SR values generated by a par-
ticular measure.
Several datasets for evaluation of semantic re-
latedness or semantic similarity have been created
so far (see Table 2). Rubenstein and Goodenough
(1965) created a dataset with 65 English noun pairs
(RG65 for short). A subset of RG65 has been
used for experiments by Miller and Charles (1991,
MC30) and Resnik (1995, Res30).
Finkelstein et al (2002) created a larger dataset
for English containing 353 pairs (Fin353), that has
been criticized by Jarmasz and Szpakowicz (2003)
for being culturally biased. More problematic is that
Fin353 consists of two subsets, which have been an-
notated by a different number of annotators. We per-
formed further analysis of their dataset and found
that the inter-annotator agreement7 differs consider-
ably. These results suggest that further evaluation
based on this data should actually regard it as two
independent datasets.
As Wikipedia is a multi-lingual resource, we are
not bound to English datasets. Several German
datasets are available that are larger than the exist-
ing English datasets and do not share the problems
of the Finkelstein datasets (see Table 2). Gurevych
(2005) conducted experiments with a German trans-
lation of an English dataset (Rubenstein and Good-
enough, 1965), but argued that the dataset is too
small and only contains noun-noun pairs connected
6Note that we do not use multiple-choice synonym question
datasets (Jarmasz and Szpakowicz, 2003), as this is a different
task, which is not addressed in this paper.
7We computed the correlation for all annotators pairwise and
summarized the values using a Fisher Z-value transformation.
5
CORRELATION r
DATASET YEAR LANGUAGE # PAIRS POS TYPE SCORES # SUBJECTS INTER INTRA
RG65 1965 English 65 N SS continuous 0?4 51 - .850
MC30 1991 English 30 N SS continuous 0?4 38 - -
Res30 1995 English 30 N SS continuous 0?4 10 .903 -
Fin353 2002 English 353 N, V, A SR continuous 0?10 13/16 - -
153 13 .731 -
200 16 .549 -
Gur65 2005 German 65 N SS discrete {0,1,2,3,4} 24 .810 -
Gur350 2006 German 350 N, V, A SR discrete {0,1,2,3,4} 8 .690 -
ZG222 2006 German 222 N, V, A SR discrete {0,1,2,3,4} 21 .490 .647
Table 2: Comparison of German datasets used for evaluating semantic relatedness.
by either synonymy or hyponymy. Thus, she cre-
ated a larger German dataset containing 350 word
pairs (Gur350). It contains nouns, verbs and ad-
jectives that are connected by classical and non-
classical relations (Morris and Hirst, 2004). How-
ever, word pairs for this dataset are biased to-
wards strong classical relations, as they were man-
ually selected. Thus, Zesch and Gurevych (2006)
used a semi-automatic process to create word pairs
from domain-specific corpora. The resulting ZG222
dataset contains 222 word pairs that are connected
by all kinds of lexical semantic relations. Hence, it
is particularly suited for analyzing the capability of
a measure to estimate SR.
4.2 Results and Discussion
Figure 4 gives an overview of our experimental re-
sults of evaluating SR measures based on the WCG
on three German datasets. We use Pearson?s prod-
uct moment correlation r to compare the results with
human judgments. From each dataset, we only use
word pairs where Wikipedia articles corresponding
to these words are available (see section 4.3 for a de-
tailed discussion of word pair coverage). For com-
parison, we give the best results obtained by Ger-
maNet based measures (abbreviated as GN).8
Our results show that the graph-based SR mea-
sures have been successfully transferred to the
WCG. Results on the Gur65 dataset (containing only
word pairs connected by strong classical relations)
are lower than values computed using GermaNet.
This is to be expected, as the WCG is created col-
laboratively without strictly enforcing a certain type
8Additionally, Table 2 gives the inter annotator agreement
for each subset. It constitutes an upper bound of a measure?s
performance on a certain dataset.
0.000.200.400.600.80
Correl
ation 
r0.75
0.500.42
0.510.34
0.450.35
GN
Res
JC
Lin
PLW
PLC
Gur65
0.000.200.400.600.80
Correl
ation 
r0.50
0.440.41
0.450.55
0.520.39
GN
Res
JC
Lin
PLW
PLC
Gur350
0.000.200.400.600.80
Correl
ation 
r0.30
0.320.43
0.350.50
0.450.36
GN
Res
JC
Lin
PLW
PLC
ZG222
Figure 4: Correlations on different datasets.
6
of semantic relation between categories, while Ger-
maNet is carefully modelled to represent the strong
classical relations captured by Gur65. Results on the
two other datasets, which contain a majority of word
pairs connected by non-classical semantic relations,
show that the WCG is better suited than GermaNet
to estimate SR.
Performance of WCG based measures depends on
the dataset and the kind of knowledge used. IIC
based measures (Res, JC and Lin) outperform path
based measures (PL, LC and WP ) on the Gur65
dataset, while path based measures are clearly bet-
ter on SR datasets (Gur350 and ZG222). The im-
pressive performance of the simple PL measure on
the SR datasets cannot be explained with the struc-
tural properties of the WCG, as they are very similar
to those of other semantic networks. Semantically
related terms are very likely to be categorized un-
der the same category, resulting in short path lengths
leading to high SR. The generalization process that
comes along with classification seems to capture the
phenomenon of SR quite well. As each article can
have many categories, different kinds of semantic
relations between terms can be established, but the
type of relation remains unknown.
4.3 Coverage of Word Pairs
If the WCG is to be used as a lexical semantic re-
source in large scale NLP applications, it should
provide broad coverage. As was described in sec-
tion 3.2, computing SR using the WCG relies on
categories assigned to articles. Thus, we consider
a word to be covered by the WCG, if there is a cate-
gorized article with matching title.
Table 3 gives an overview of the number of word
pairs covered in GermaNet or the WCG. Only few
words from Gur65 were not found in one of the re-
sources. This proportion is much higher for Gur350
and ZG222, as these datasets contain many domain
specific terms that are badly covered in GermaNet,
and many word pairs containing verbs and adjectives
that are badly covered in the WCG.9 A number of
word pairs (mostly containing combinations of verbs
or adjectives) were found neither in GermaNet nor
9Resulting from an editorial decision, Wikipedia only con-
tains articles devoted to terms of encyclopedic interest - mainly
nouns. Adjectives and verbs redirect to their corresponding
nouns, if they are covered at all.
Wikipedia (see GN ? WCG). If we consider only
noun-noun pairs (NN), the coverage of Wikipedia
exceeds that of GermaNet. The high proportion of
word pairs that are either only found in GermaNet
or in the WCG indicates that they are partially com-
plementary with respect to covered vocabulary.
5 Conclusion
In this paper, we performed a graph-theoretic anal-
ysis of the Wikipedia Category Graph and showed
that it is a scale-free, small-world graph, like other
semantic networks such as WordNet or Roget?s the-
saurus. From this result, we concluded that the
WCG can be used for NLP tasks, where other se-
mantic networks have been traditionally employed.
As Wikipedia is a multi-lingual resource, this en-
ables the transfer of NLP algorithms to languages
that do not have well-developed semantic wordnets.
To substantiate this claim, we described howmea-
sures of semantic relatedness operating on seman-
tic wordnets, like WordNet or GermaNet, can be
adapted to work on the WCG. We showed that the
WCG is well suited to estimate SR between words.
This is due to the categorization process that con-
nects terms which would not be closely related in
a taxonomic wordnet structure. Consequently, Ger-
maNet outperforms the WCG on the task of estimat-
ing semantic similarity. Furthermore, the WCG can-
not be used for tasks that require knowledge about
the exact type of semantic relation.
We performed an analysis of the coverage of
Wikipedia. It covers nouns very well, but is less
suited to compute semantic relatedness across parts-
of-speech. In this case, conventional semantic word-
nets are likely to provide a better knowledge source.
In Zesch et al (2007b), we show that knowledge
from wordnets and from Wikipedia is complemen-
tary, and can be combined to improve the perfor-
mance on the SR task. As the simple PL measure
performs remarkably well on the SR datasets, in our
future work, we will also consider computing SR us-
ing the path length on the Wikipedia article graph
rather than on the WCG.
Acknowledgments
This work was supported by the German Research
Foundation under the grant "Semantic Information
7
DATASET # PAIRS GN WCG GN ? WCG GN \ WCG WCG \ GN GN ? WCG
Gur65 65 57 61 65 4 8 53
Gur350 350 208 161 248 87 40 121
Gur350 NN 173 109 115 129 14 20 95
ZG222 222 86 86 118 32 30 56
ZG222 NN 119 57 61 73 12 16 45
Table 3: Number of covered word pairs based on GermaNet (GN) and the WCG on different datasets.
Retrieval from Texts in the Example Domain Elec-
tronic Career Guidance" (SIR), GU 798/1-2.
References
A. Barabasi and R. Albert. 1999. Emergence of scaling in
random networks. Science, 286:509?512.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-based
Measures of Semantic Distance. Computational Linguistics,
32(1).
L. Buriol, C. Castillo, D. Donato, S. Leonardi, and S. Millozzi.
2006. Temporal Analysis of the Wikigraph. In Proc. of Web
Intelligence, Hong Kong.
A. Capocci, V. D. P. Servedio, F. Colaiori, L. S. Buriol, D. Do-
nato, S. Leonardi, and G. Caldarelli. 2006. Preferential at-
tachment in the growth of social networks: The internet en-
cyclopedia Wikipedia. Physical Review E, 74:036116.
C. Fellbaum. 1998. WordNet An Electronic Lexical Database.
MIT Press, Cambridge, MA.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,
and G. Wolfman. 2002. Placing Search in Context: The
Concept Revisited. ACM TOIS, 20(1):116?131.
I. Gurevych. 2005. Using the Structure of a Conceptual Net-
work in Computing Semantic Relatedness. In Proc. of IJC-
NLP, pages 767?778.
T. Holloway, M. Bozicevic, and K. B?rner. 2005. Analyzing
and Visualizing the Semantic Coverage of Wikipedia and Its
Authors. ArXiv Computer Science e-prints, cs/0512085.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s thesaurus and
semantic similarity. In Proc. of RANLP, pages 111?120.
J. J. Jiang and D. W. Conrath. 1997. Semantic Similarity Based
on Corpus Statistics and Lexical Taxonomy. In Proc. of
the 10th International Conference on Research in Compu-
tational Linguistics.
C. Kunze, 2004. Computerlinguistik und Sprachtechnologie,
chapter Lexikalisch-semantische Wortnetze, pages 423?431.
Spektrum Akademischer Verlag.
C. Leacock and M. Chodorow, 1998. WordNet: An Elec-
tronic Lexical Database, chapter Combining Local Context
andWordNet Similarity for Word Sense Identification, pages
265?283. Cambridge: MIT Press.
D. Lin. 1998. An Information-Theoretic Definition of Similar-
ity. In Proc. of ICML.
G. A. Miller and W. G. Charles. 1991. Contextual Correlates
of Semantic Similarity. Language and Cognitive Processes,
6(1):1?28.
J. Morris and G. Hirst. 2004. Non-Classical Lexical Seman-
tic Relations. In Proc. of the Workshop on Computational
Lexical Semantics, NAACL-HLT.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998. The
University of South Florida word association, rhyme, and
word fragment norms. Technical report, U. of South Florida.
M. E. J. Newman. 2003. The structure and function of complex
networks. SIAM Review, 45:167?256.
R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989. Develop-
ment and Application of a Metric on Semantic Nets. IEEE
Trans. on Systems, Man, and Cybernetics,, 19(1):17?30.
P. Resnik. 1995. Using Information Content to Evaluate Se-
mantic Similarity. In Proc. of IJCAI, pages 448?453.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
Correlates of Synonymy. Communications of the ACM,
8(10):627?633.
N. Seco, T. Veale, and J. Hayes. 2004. An Intrinsic Information
Content Metric for Semantic Similarity inWordNet. In Proc.
of ECAI.
M. Steyvers and J. B. Tenenbaum. 2005. The Large-Scale
Structure of Semantic Networks: Statistical Analyses and a
Model of Semantic Growth. Cognitive Science, 29:41?78.
J. Voss. 2005. Measuring Wikipedia. In Proc. of the 10th In-
ternational Conference of the International Society for Sci-
entometrics and Informetrics, Stockholm, Sweden.
J. Voss. 2006. Collaborative thesaurus tagging the Wikipedia
way. ArXiv Computer Science e-prints, cs/0604036.
D. J. Watts and S. H. Strogatz. 1998. Collective Dynamics of
Small-World Networks. Nature, 393:440?442.
Z. Wu and M. Palmer. 1994. Verb Semantics and Lexical Se-
lection. In Proc. of ACL, pages 133?138.
T. Zesch and I. Gurevych. 2006. Automatically Creating
Datasets for Measures of Semantic Relatedness. In Proc.
of the Workshop on Linguistic Distances, ACL, pages 16?24.
T. Zesch, I. Gurevych, and M. M?hlh?user. 2007a. Analyzing
and Accessing Wikipedia as a Lexical Semantic Resource.
In Proc. of Biannual Conference of the Society for Compu-
tational Linguistics and Language Technology, pages 213?
221.
T. Zesch, I. Gurevych, and M. M?hlh?user. 2007b. Compar-
ing Wikipedia and German Wordnet by Evaluating Semantic
Relatedness on Multiple Datasets. In Proc. of NAACL-HLT,
page (to appear).
V. Zlatic, M. Bozicevic, H. Stefancic, and M. Domazet. 2006.
Wikipedias: Collaborative web-based encyclopedias as com-
plex networks. Physical Review E, 74:016115.
8
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529?538,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Measuring Contextual Fitness Using Error Contexts Extracted from the
Wikipedia Revision History
Torsten Zesch
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information, Frankfurt
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
We evaluate measures of contextual fitness
on the task of detecting real-word spelling
errors. For that purpose, we extract nat-
urally occurring errors and their contexts
from the Wikipedia revision history. We
show that such natural errors are better
suited for evaluation than the previously
used artificially created errors. In partic-
ular, the precision of statistical methods
has been largely over-estimated, while the
precision of knowledge-based approaches
has been under-estimated. Additionally, we
show that knowledge-based approaches can
be improved by using semantic relatedness
measures that make use of knowledge be-
yond classical taxonomic relations. Finally,
we show that statistical and knowledge-
based methods can be combined for in-
creased performance.
1 Introduction
Measuring the contextual fitness of a term in its
context is a key component in different NLP ap-
plications like speech recognition (Inkpen and
De?silets, 2005), optical character recognition
(Wick et al 2007), co-reference resolution (Bean
and Riloff, 2004), or malapropism detection (Bol-
shakov and Gelbukh, 2003). The main idea is al-
ways to test what fits better into the current con-
text: the actual term or a possible replacement that
is phonetically, structurally, or semantically simi-
lar. We are going to focus on malapropism detec-
tion as it allows evaluating measures of contex-
tual fitness in a more direct way than evaluating
in a complex application which always entails in-
fluence from other components, e.g. the quality of
the optical character recognition module (Walker
et al 2010).
A malapropism or real-word spelling error oc-
curs when a word is replaced with another cor-
rectly spelled word which does not suit the con-
text, e.g. ?People with lots of honey usually
live in big houses.?, where ?money? was replaced
with ?honey?. Besides typing mistakes, a major
source of such errors is the failed attempt of au-
tomatic spelling correctors to correct a misspelled
word (Hirst and Budanitsky, 2005). A real-word
spelling error is hard to detect, as the erroneous
word is not misspelled and fits syntactically into
the sentence. Thus, measures of contextual fitness
are required to detect words that do not fit their
contexts.
Existing measures of contextual fitness can be
categorized into knowledge-based (Hirst and Bu-
danitsky, 2005) and statistical methods (Mays et
al., 1991; Wilcox-OHearn et al 2008). Both
test the lexical cohesion of a word with its con-
text. For that purpose, knowledge-based ap-
proaches employ the structural knowledge en-
coded in lexical-semantic networks like WordNet
(Fellbaum, 1998), while statistical approaches
rely on co-occurrence counts collected from large
corpora, e.g. the Google Web1T corpus (Brants
and Franz, 2006).
So far, evaluation of contextual fitness mea-
sures relied on artificial datasets (Mays et al
1991; Hirst and Budanitsky, 2005) which are cre-
ated by taking a sentence that is known to be cor-
rect, and replacing a word with a similar word
from the vocabulary. This has a couple of dis-
advantages: (i) the replacement might be a syn-
onym of the original word and perfectly valid in
the given context, (ii) the generated error might
529
be very unlikely to be made by a human, and
(iii) inserting artificial errors often leads to un-
natural sentences that are quite easy to correct,
e.g. if the word class has changed. However,
even if the word class is unchanged, the origi-
nal word and its replacement might still be vari-
ants of the same lemma, e.g. a noun in singu-
lar and plural, or a verb in present and past form.
This usually leads to a sentence where the error
can be easily detected using syntactical or statis-
tical methods, but is almost impossible to detect
for knowledge-based measures of contextual fit-
ness, as the meaning of the word stays more or
less unchanged. To estimate the impact of this is-
sue, we randomly sampled 1,000 artificially cre-
ated real-word spelling errors1 and found 387 sin-
gular/plural pairs and 57 pairs which were in an-
other direct relation (e.g. adjective/adverb). This
means that almost half of the artificially created
errors are not suited for an evaluation targeted at
finding optimal measures of contextual fitness, as
they over-estimate the performance of statistical
measures while underestimating the potential of
semantic measures. In order to investigate this
issue, we present a framework for mining natu-
rally occurring errors and their contexts from the
Wikipedia revision history. We use the resulting
English and German datasets to evaluate statisti-
cal and knowledge-based measures.
We make the full experimental framework pub-
licly available2 which will allow reproducing our
experiments as well as conducting follow-up ex-
periments. The framework contains (i) methods
to extract natural errors from Wikipedia, (ii) ref-
erence implementations of the knowledge-based
and the statistical methods, and (iii) the evalua-
tion datasets described in this paper.
2 Mining Errors from Wikipedia
Measures of contextual fitness have previously
been evaluated using artificially created datasets,
as there are very few sources of sentences with
naturally occurring errors and their corrections.
Recently, the revision history of Wikipedia has
been introduced as a valuable knowledge source
for NLP (Nelken and Yamangil, 2008; Yatskar et
al., 2010). It is also a possible source of natural
errors, as it is likely that Wikipedia editors make
1The same artificial data as described in Section 3.2.
2http://code.google.com/p/dkpro-spelling-asl/
real-word spelling errors at some point, which
are then corrected in subsequent revisions of the
same article. The challenge lies in discriminating
real-word spelling errors from all sorts of other
changes, including non-word spelling errors, re-
formulations, or the correction of wrong facts.
For that purpose, we apply a set of precision-
oriented heuristics narrowing down the number
of possible error candidates. Such an approach
is feasible, as the high number of revisions in
Wikipedia allows to be extremely selective.
2.1 Accessing the Revision Data
We access the Wikipedia revision data using
the freely available Wikipedia Revision Toolkit
(Ferschke et al 2011) together with the JWPL
Wikipedia API (Zesch et al 2008a).3 The API
outputs plain text converted from Wiki-Markup,
but the text still contains a small portion of left-
over markup and other artifacts. Thus, we per-
form additional cleaning steps removing (i) to-
kens with more than 30 characters (often URLs),
(ii) sentences with less than 5 or more than 200
tokens, and (iii) sentences containing a high frac-
tion of special characters like ?:? usually indicat-
ing Wikipedia-specific artifacts like lists of lan-
guage links. The remaining sentences are part-of-
speech tagged and lemmatized using TreeTagger
(Schmid, 2004). Using these cleaned and anno-
tated articles, we form pairs of adjacent article re-
visions (ri and ri+1).
2.2 Sentence Alignment
Fully aligning all sentences of the adjacent revi-
sions is a quite costly operation, as sentences can
be split, joined, replaced, or moved in the arti-
cle. However, we are only looking for sentence
pairs which are almost identical except for the
real-word spelling error and its correction. Thus,
we form all sentence pairs and then apply an ag-
gressive but cheap filter that rules out all sentences
which (i) are equal, or (ii) whose lengths differ
more than a small number of characters. For the
resulting much smaller subset of sentence pairs,
we compute the Jaro distance (Jaro, 1995) be-
tween each pair. If the distance exceeds a cer-
tain threshold tsim (0.05 in this case), we do not
further consider the pair. The small amount of re-
maining sentence pairs is passed to the sentence
pair filter for in-depth inspection.
3http://code.google.com/p/jwpl/
530
2.3 Sentence Pair Filtering
The sentence pair filter further reduces the num-
ber of remaining sentence pairs by applying a set
of heuristics including surface level and semantic
level filters. Surface level filters include:
Replaced Token Sentences need to consist of
identical tokens, except for one replaced token.
No Numbers The replaced token may not be a
number.
UPPER CASE The replaced token may not be
in upper case.
Case Change The change should not only in-
volve case changes, e.g. changing ?english? into
?English?.
Edit Distance The edit distance between the
replaced token and its correction need to be be-
low a certain threshold.
After applying the surface level filters, the re-
maining sentence pairs are well-formed and con-
tain exactly one changed token at the same posi-
tion in the sentence. However, the change does
not need to characterize a real-word spelling er-
ror, but could also be a normal spelling error or a
semantically motivated change. Thus, we apply a
set of semantic filters:
Vocabulary The replaced token needs to occur
in the vocabulary. We found that even quite com-
prehensive word lists discarded too many valid
errors as Wikipedia contains articles from a very
wide range of domains. Thus, we use a frequency
filter based on the Google Web1T n-gram counts
(Brants and Franz, 2006). We filter all sentences
where the replaced token has a very low unigram
count. We experimented with different values and
found 25,000 for English and 10,000 for German
to yield good results.
Same Lemma The original token and the re-
placed token may not have the same lemma, e.g.
?car? and ?cars? would not pass this filter.
Stopwords The replaced token should not be in
a short list of stopwords (mostly function words).
Named Entity The replaced token should not
be part of a named entity. For this purpose, we
applied the Stanford NER (Finkel et al 2005).
Normal Spelling Error We apply the Jazzy
spelling detector4 and rule out all cases in which
it is able to detect the error.
Semantic Relation If the original token and the
replaced token are in a close lexical-semantic rela-
4http://jazzy.sourceforge.net/
tions, the change is likely to be semantically mo-
tivated, e.g. if ?house? was replaced with ?hut?.
Thus, we do not consider cases, where we detect
a direct semantic relation between the original and
the replaced term. For this purpose, we use Word-
Net (Fellbaum, 1998) for English and GermaNet
(Lemnitzer and Kunze, 2002) for German.
3 Resulting Datasets
3.1 Natural Error Datasets
Using our framework for mining real-word
spelling errors in context, we extracted an En-
glish dataset5, and a German dataset6. Although
the output generally was of high quality, man-
ual post-processing was necessary7, as (i) for
some pairs the available context did not provide
enough information to decide which form was
correct, and (ii) a problem that might be spe-
cific to Wikipedia ? vandalism. The revisions are
full of cases where words are replaced with simi-
lar sounding but greasy alternatives. A relatively
mild example is ?In romantic comedies, there is
a love story about a man and a woman who fall
in love, along with silly or funny comedy farts.?,
where ?parts? was replaced with ?farts? only to be
changed back shortly afterwards by a Wikipedia
vandalism hunter. We removed all cases that re-
sulted from obvious vandalism. For further ex-
periments, a small list of offensive terms could be
added to the stopword list to facilitate this pro-
cess.
A connected problem is correct words that get
falsely corrected by Wikipedia editors (without
the malicious intend from the previous examples,
but with similar consequences). For example, the
initially correct sentence ?Dung beetles roll it into
a ball, sometimes being up to 50 times their own
weight.? was ?corrected? by exchanging weight
with wait. We manually removed such obvious
mistakes, but are still left with some borderline
cases. In the sentence ?By the 1780s the goals
of England were so full that convicts were often
chained up in rotting old ships.? the obvious error
5Using a revision dump from April 5, 2011.
6Using a revision dump from August 13, 2010.
7The most efficient and precise way of finding real-word
spelling errors would of course be to apply measures of con-
textual fitness. However, the resulting dataset would then
only contain errors that are detectable by the measures we
want to evaluate ? a clearly unacceptable bias. Thus, a cer-
tain amount of manual validation is inevitable.
531
?goal? was changed by some Wikipedia editor to
?jail?. However, actually it should have been the
old English form for jail ?gaol? which can be de-
duced when looking at the full context and later
versions of the article. We decided to not remove
these rare cases, because ?jail? is a valid correction
in this context.
After manual inspection, we are left with 466
English and 200 German errors. Given that we
restricted our experiment to 5 million English and
German revisions, much larger datasets can be ex-
tracted if the whole revision history is taken into
account. Our snapshot of the English Wikipedia
contains 305?106 revisions. Even if not all of them
correspond to article revisions, it is safe to assume
that more than 10,000 real-word spelling errors
can be extracted from this version of Wikipedia.
Using the same amount of source revisions, we
found significantly more English than German er-
rors. This might be due to (i) English having more
short nouns or verbs than German that are more
likely to be confused with each other, and (ii) the
English Wikipedia being known to attract a larger
amount of non-native editors which might lead to
higher rates of real-word spelling errors. How-
ever, this issue needs to be further investigated
e.g. based on comparable corpora build on the ba-
sis of different language editions of Wikipedia.
Further refining the identification of real-word er-
rors in Wikipedia would allow evaluating how fre-
quent such errors actually occur, and how long
it takes the Wikipedia editors to detect them. If
errors persist over a long time, using measures
of contextual fitness for detection would be even
more important.
Another interesting observation is that the av-
erage edit distance is around 1.4 for both datasets.
This means that a substantial proportion of errors
involve more than one edit operation. Given that
many measures of contextual fitness allow at most
one edit, many naturally occurring errors will not
be detected. However, allowing a larger edit dis-
tance enormously increases the search space re-
sulting in increased run-time and possibly de-
creased detection precision due to more false pos-
itives.
3.2 Artificial Error Datasets
In contrast to the quite challenging process of
mining naturally occurring errors, creating artifi-
cial errors is relatively straightforward. From a
corpus that is known to be free of spelling errors,
sentences are randomly sampled. For each sen-
tence, a random word is selected and all strings
with edit distance smaller than a given threshold
(2 in our case) are generated. If one of those gen-
erated strings is a known word from the vocabu-
lary, it is picked as the artificial error.
Previous work on evaluating real-word spelling
correction (Hirst and Budanitsky, 2005; Wilcox-
OHearn et al 2008; Islam and Inkpen, 2009)
used a dataset sampled from the Wall Street Jour-
nal corpus which is not freely available. Thus, we
created a comparable English dataset of 1,000 ar-
tificial errors based on the easily available Brown
corpus (Francis W. Nelson and Kuc?era, 1964).8
Additionally, we created a German dataset with
1,000 artificial errors based on the TIGER cor-
pus.9
4 Measuring Contextual Fitness
There are two main approaches for measuring the
contextual fitness of a word in its context: the
statistical (Mays et al 1991) and the knowledge-
based approach (Hirst and Budanitsky, 2005).
4.1 Statistical Approach
Mays et al(1991) introduced an approach based
on the noisy-channel model. The model assumes
that the correct sentence s is transmitted through
a noisy channel adding ?noise? which results in a
word w being replaced by an error e leading the
wrong sentence s? which we observe. The prob-
ability of the correct word w given that we ob-
serve the error e can be computed as P (w|e) =
P (w) ? P (e|w). The channel model P (e|w) de-
scribes how likely the typist is to make an error.
This is modeled by the parameter ?.10 The re-
maining probability mass (1 ? ?) is distributed
equally among all words in the vocabulary within
an edit distance of 1 (edits(w)):
P (e|w) =
{
? if e = w
(1? ?)/|edits(w)| if e 6= w
The source model P (w) is estimated using a
trigram language model, i.e. the probability of the
8http://www.archive.org/details/BrownCorpus (CC-by-na).
9http://www.ims.uni-stuttgart.de/projekte/TIGER/
The corpus contains 50,000 sentences of German newspaper
text, and is freely available under a non-commercial license.
10We optimize ? on a held-out development set of errors.
532
intended word wi is computed as the conditional
probability P (wi|wi?1wi?2). Hence, the proba-
bility of the correct sentence s = w1 . . . wn can
be estimated as
P (s) =
n+2?
i=1
P (wi|wi?1wi?2)
The set of candidate sentences Sc contains all ver-
sions of the observed sentence s? derived by re-
placing one word with a word from edits(w),
while all other words in the sentence remain
unchanged. The correct sentence s is those
sentence from Sc that maximizes P (s|s?) =
argmaxs?Sc P (s) ? P (s
?|s).
4.2 Knowledge Based Approach
Hirst and Budanitsky (2005) introduced a
knowledge-based approach that detects real-word
spelling errors by checking the semantic relations
of a target word with its context. For this pur-
pose, they apply WordNet as the source of lexical-
semantic knowledge.
The algorithm flags all words as error can-
didates and then applies filters to remove those
words from further consideration that are unlikely
to be errors. First, the algorithm removes all
closed-class word candidates as well as candi-
dates which cannot be found in the vocabulary.
Candidates are then tested for having lexical co-
hesion with their context, by (i) checking whether
the same surface form or lemma appears again in
the context, or (ii) a semantically related concept
is found in the context. In both cases, the candi-
date is removed from the list of candidates. For
each remaining possible real-word spelling error,
edits are generated by inserting, deleting, or re-
placing characters up to a certain edit distance
(usually 1). Each edit is then tested for lexical
cohesion with the context. If at least one of it fits
into the context, the candidate is selected as a real-
word error.
Hirst and Budanitsky (2005) use two additional
filters: First, they remove candidates that are
?common non-topical words?. It is unclear how
the list of such words was compiled. Their list
of examples contains words like ?find? or ?world?
which we consider to be perfectly valid candi-
dates. Second, they also applied a filter using a
list of known multi-words, as the probability for
words to accidentally form multi-words is low.
Dataset P R F
Artificial-English .77 .50 .60
Natural-English .54 .26 .35
Artificial-German .90 .49 .63
Natural-German .77 .20 .32
Table 1: Performance of the statistical approach using
a trigram model based on Google Web1T.
It is unclear which list was used. We could use
multi-words from WordNet, but coverage would
be rather limited. We decided not to use both fil-
ters in order to better assess the influence of the
underlying semantic relatedness measure on the
overall performance.
The knowledge based approach uses semantic
relatedness measures to determine the cohesion
between a candidate and its context. In the exper-
iments by Budanitsky and Hirst (2006), the mea-
sure by (Jiang and Conrath, 1997) yields the best
results. However, a wide range of other measures
have been proposed, cf. (Zesch and Gurevych,
2010). Some measures using a wider defini-
tion of semantic relatedness (Gabrilovich and
Markovitch, 2007; Zesch et al 2008b) instead
of only using taxonomic relations in a knowledge
source.
As semantic relatedness measures usually re-
turn a numeric value, we need to determine a
threshold ? in order to come up with a binary
related/unrelated decision. Budanitsky and Hirst
(2006) used a characteristic gap in the stan-
dard evaluation dataset by Rubenstein and Good-
enough (1965) that separates unrelated from re-
lated word pairs. We do not follow this approach,
but optimize the threshold on a held-out develop-
ment set of real-word spelling errors.
5 Results & Discussion
In this section, we report on the results obtained
in our evaluation of contextual fitness measures
using artificial and natural errors in English and
German.
5.1 Statistical Approach
Table 1 summarizes the results obtained by the
statistical approach using a trigram model based
on the Google Web1T data (Brants and Franz,
2006). On the English artificial errors, we ob-
serve a quite high F-measure of .60 that drops to
533
Dataset N-gram model Size P R F
Art-En
Google Web
7 ? 1011 .77 .50 .60
7 ? 1010 .78 .48 .59
7 ? 109 .76 .42 .54
Wikipedia 2 ? 109 .72 .37 .49
Nat-En
Google Web
7 ? 1011 .54 .26 .35
7 ? 1010 .51 .23 .31
7 ? 109 .46 .19 .27
Wikipedia 2 ? 109 .49 .19 .27
Art-De
Google Web
8 ? 1010 .90 .49 .63
8 ? 109 .90 .47 .61
8 ? 108 .88 .36 .51
Wikipedia 7 ? 108 .90 .37 .52
Nat-De
Google Web
8 ? 1010 .77 .20 .32
8 ? 109 .68 .14 .23
8 ? 108 .65 .10 .17
Wikipedia 7 ? 108 .70 .13 .22
Table 2: Influence of the n-gram model on the perfor-
mance of the statistical approach.
.35 when switching to the naturally occurring er-
rors which we extracted from Wikipedia. On the
German dataset, we observe almost the same per-
formance drop (from .63 to .32).
These observations correspond to our earlier
analysis where we showed that the artificial data
contains many cases that are quite easy to correct
using a statistical model, e.g. where a plural form
of a noun is replaced with its singular form (or
vice versa) as in ?I bought a car.? vs. ?I bought
a cars.?. The naturally occurring errors often con-
tain much harder contexts, as shown in the fol-
lowing example: ?Through the open window they
heard sounds below in the street: cartwheels, a
tired horse?s plodding step, vices.? where ?vices?
should be corrected to ?voices?. While the lemma
?voice? is clearly semantically related to other
words in the context like ?hear? or ?sound?, the
position at the end of the sentence is especially
difficult for the trigram-based statistical approach.
The only trigram that connects the error to the
context is (?step?, ?,?, vices/voices) which will
probably yield a low frequency count even for
very large trigram models. Higher order n-gram
models would help, but suffer from the usual data-
sparseness problems.
Influence of the N-gram Model For building
the trigram model, we used the Google Web1T
data, which has some known quality issues and is
Dataset P R F
Artificial-English .26 .15 .19
Natural-English .29 .18 .23
Artificial-German .47 .16 .24
Natural-German .40 .13 .19
Table 3: Performance of the knowledge-based ap-
proach using the JiangConrath semantic relatedness
measure.
not targeted towards the Wikipedia articles from
which we sampled the natural errors. Thus, we
also tested a trigram model based on Wikipedia.
However, it is much smaller than the Web model,
which leads us to additionally testing smaller Web
models. Table 2 summarizes the results.
We observe that ?more data is better data? still
holds, as the largest Web model always outper-
forms the Wikipedia model in terms of recall. If
we reduce the size of the Web model to the same
order of magnitude as the Wikipedia model, the
performance of the two models is comparable.
We would have expected to see better results for
the Wikipedia model in this setting, but its higher
quality does not lead to a significant difference.
Even if statistical approaches quite reliably de-
tect real-word spelling errors, the size of the re-
quired n-gram models remains a serious obstacle
for use in real-world applications. The English
Web1T trigram model is about 25GB, which cur-
rently is not suited for being applied in settings
with limited storage capacities e.g. for intelligent
input assistance in mobile devices. As we have
seen above, using smaller models will decrease
recall to a point where hardly any error will be de-
tected anymore. Thus, we will now have a look on
knowledge-based approaches which are less de-
manding in terms of the required resources.
5.2 Knowledge-based Approach
Table 3 shows the results for the knowledge-based
measure. In contrast to the statistical approach,
the results on the artificial errors are not higher
than on the natural errors, but almost equal for
German and even lower for English; another piece
of evidence supporting our view that the proper-
ties of artificial datasets over-estimate the perfor-
mance of statistical measures.
Influence of the Relatedness Measure As was
pointed out before, Budanitsky and Hirst (2006)
534
Dataset Measure ? P R F
Art-En
JiangConrath 0.5 .26 .15 .19
Lin 0.5 .22 .17 .19
Lesk 0.5 .19 .16 .17
ESA-Wikipedia 0.05 .43 .13 .20
ESA-Wiktionary 0.05 .35 .20 .25
ESA-Wordnet 0.05 .33 .15 .21
Nat-En
JiangConrath 0.5 .29 .18 .23
Lin 0.5 .26 .21 .23
Lesk 0.5 .19 .19 .19
ESA-Wikipedia 0.05 .48 .14 .22
ESA-Wiktionary 0.05 .39 .21 .27
ESA-Wordnet 0.05 .36 .15 .21
Table 4: Performance of knowledge-based approach
using different relatedness measures.
show that the measure by Jiang and Conrath
(1997) yields the best results in their experi-
ments on malapropism detection. In addition, we
test another path-based measure by Lin (1998),
the gloss-based measure by Lesk (1986), and
the ESA measure (Gabrilovich and Markovitch,
2007) based on concept vectors from Wikipedia,
Wiktionary, and WordNet. Table 4 summarizes
the results. In contrast to the findings of Budanit-
sky and Hirst (2006), JiangConrath is not the best
path-based measure, as Lin provides equal or bet-
ter performance. Even more importantly, other
(non path-based) measures yield better perfor-
mance than both path-based measures. Especially
ESA based on Wiktionary provides a good over-
all performance, while ESA based on Wikipedia
provides excellent precision. The advantage of
ESA over the other measure types can be ex-
plained with its ability to incorporate semantic re-
lationships beyond classical taxonomic relations
(as used by path-based measures).
5.3 Combining the Approaches
The statistical and the knowledge-based approach
use quite different methods to assess the con-
textual fitness of a word in its context. This
makes it worthwhile trying to combine both ap-
proaches. We ran the statistical method (using the
full Wikipedia trigram model) and the knowledge-
based method (using the ESA-Wiktionary related-
ness measure) in parallel and then combined the
resulting detections using two strategies: (i) we
merge the detections of both approaches in order
to obtain higher recall (?Union?), and (ii) we only
Dataset Comb.-Strategy P R F
Artificial-English
Best-Single .77 .50 .60
Union .52 .55 .54
Intersection .91 .15 .25
Natural-English
Best-Single .54 .26 .35
Union .40 .36 .38
Intersection .82 .11 .19
Table 5: Results obtained by a combination of the best
statistical and knowledge-based configuration. ?Best-
Single? is the best precision or recall obtained by a sin-
gle measure. ?Union? merges the detections of both
approaches. ?Intersection? only detects an error if both
methods agree on a detection.
count an error as detected if both methods agree
on a detection (?Intersection?). When compar-
ing the combined results in Table 5 with the best
precision or recall obtained by a single measure
(?Best-Single?), we observe that precision can be
significantly improved using the ?Union? strategy,
while recall is only moderately improved using
the ?Intersect? strategy. This means that (i) a large
subset of errors is detected by both approaches
that due to their different sources of knowledge
mutually reinforce the detection leading to in-
creased precision, and (ii) a small but otherwise
undetectable subset of errors requires considering
detections made by one approach only.
6 Related Work
To our knowledge, we are the first to create a
dataset of naturally occurring errors based on the
revision history of Wikipedia. Max and Wis-
niewski (2010) used similar techniques to create
a dataset of errors from the French Wikipedia.
However, they target a wider class of errors in-
cluding non-word spelling errors, and their class
of real-word errors conflates malapropisms as
well as other types of changes like reformulations.
Thus, their dataset cannot be easily used for our
purposes and is only available in French, while
our framework allows creating datasets for all ma-
jor languages with minimal manual effort.
Another possible source of real-word spelling
errors are learner corpora (Granger, 2002), e.g.
the Cambridge Learner Corpus (Nicholls, 1999).
However, annotation of errors is difficult and
costly (Rozovskaya and Roth, 2010), only a small
fraction of observed errors will be real-word
spelling errors, and learners are likely to make dif-
535
ferent mistakes than proficient language users.
Islam and Inkpen (2009) presented another sta-
tistical approach using the Google Web1T data
(Brants and Franz, 2006) to create the n-gram
model. It slightly outperformed the approach by
Mays et al(1991) when evaluated on a corpus of
artificial errors based on the WSJ corpus. How-
ever, the results are not directly comparable, as
Mays et al(1991) used a much smaller n-gram
model and our results in Section 5.1 show that
the size of the n-gram model has a large influence
on the results. Eventually, we decided to use the
Mays et al(1991) approach in our study, as it is
easier to adapt and augment.
In a re-evaluation of the statistical model by
Mays et al(1991), Wilcox-OHearn et al(2008)
found that it outperformed the knowledge-based
method by Hirst and Budanitsky (2005) when
evaluated on a corpus of artificial errors based on
the WSJ corpus. This is consistent with our find-
ings on the artificial errors based on the Brown
corpus, but - as we have seen in the previous sec-
tion - evaluation on the naturally occurring errors
shows a different picture. They also tried to im-
prove the model by permitting multiple correc-
tions and using fixed-length context windows in-
stead of sentences, but obtained discouraging re-
sults.
All previously discussed methods are unsuper-
vised in a way that they do not rely on any training
data with annotated errors. However, real-word
spelling correction has also been tackled by su-
pervised approaches (Golding and Schabes, 1996;
Jones and Martin, 1997; Carlson et al 2001).
Those methods rely on predefined confusion-sets,
i.e. sets of words that are often confounded e.g.
{peace, piece} or {weather, whether}. For each
set, the methods learn a model of the context in
which one or the other alternative is more proba-
ble. This yields very high precision, but only for
the limited number of previously defined confu-
sion sets. Our framework for extracting natural
errors could be used to increase the number of
known confusion sets.
7 Conclusions and Future Work
In this paper, we evaluated two main approaches
for measuring the contextual fitness of terms: the
statistical approach by Mays et al(1991) and
the knowledge-based approach by Hirst and Bu-
danitsky (2005) on the task of detecting real-
word spelling errors. For that purpose, we ex-
tracted a dataset with naturally occurring errors
and their contexts from the Wikipedia revision
history. We show that evaluating measures of con-
textual fitness on this dataset provides a more re-
alistic picture of task performance. In particular,
using artificial datasets over-estimates the perfor-
mance of the statistical approach, while it under-
estimates the performance of the knowledge-
based approach.
We show that n-gram models targeted towards
the domain from which the errors are sampled
do not improve the performance of the statisti-
cal approach if larger n-gram models are avail-
able. We further show that the performance of
the knowledge-based approach can be improved
by using semantic relatedness measures that in-
corporate knowledge beyond the taxonomic rela-
tions in a classical lexical-semantic resource like
WordNet. Finally, by combining both approaches,
significant increases in precision or recall can be
achieved.
In future work, we want to evaluate a wider
range of contextual fitness measures, and learn
how to combine them using more sophisticated
combination strategies. Both - the statistical as
well as the knowledge-based approach - will ben-
efit from a better model of the typist, as not all
edit operations are equally likely (Kernighan et
al., 1990). On the side of the error extraction, we
are going to further improve the extraction pro-
cess by incorporating more knowledge about the
revisions. For example, vandalism is often re-
verted very quickly, which can be detected when
looking at the full set of revisions of an article.
We hope that making the experimental frame-
work publicly available will foster future research
in this field, as our results on the natural errors
show that the problem is still quite challenging.
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We Andreas Kellner and Tristan Miller for check-
ing the datasets, and the anonymous reviewers for
their helpful feedback.
536
References
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proc. of HLT/NAACL, pages
297?304.
Igor A. Bolshakov and Alexander Gelbukh. 2003. On
Detection of Malapropisms by Multistage Colloca-
tion Testing. In Proceedings of NLDB-2003, 8th
International Workshop on Applications of Natural
Language to Information Systems, number Cic.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Andrew J Carlson, Jeffrey Rosen, and Dan Roth.
2001. Scaling Up Context-Sensitive Text Correc-
tion. In Proceedings of IAAI.
C Fellbaum. 1998. WordNet An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently
Accessing Wikipedia?s Edit History. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. System Demonstrations, pages
97?102, Portland, OR, USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics -
ACL ?05, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Francis W. Nelson and Henry Kuc?era. 1964. Manual
of information to accompany a standard corpus of
present-day edited American English, for use with
digital computers.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Arti-
ficial Intelligence, pages 1606?1611.
Andrew R. Golding and Yves Schabes. 1996. Com-
bining Trigram-based and feature-based methods
for context-sensitive spelling correction. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics -, pages 71?78, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sylviane Granger, 2002. A birds-eye view of learner
corpus research, pages 3?33. John Benjamins Pub-
lishing Company.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lex-
ical cohesion. Natural Language Engineering,
11(1):87?111, March.
Diana Inkpen and Alain De?silets. 2005. Semantic
similarity for detecting recognition errors in auto-
matic speech transcripts. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing -
HLT ?05, number October, pages 49?56, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using Google Web IT 3-grams.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing Vol-
ume 3 - EMNLP ?09, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
M A Jaro. 1995. Probabilistic linkage of large public
health data file. Statistics in Medicine, 14:491?498.
Jay J Jiang and David W Conrath. 1997. Seman-
tic Similarity Based on Corpus Statistics and Lex-
ical Taxonomy. In Proceedings of the 10th Inter-
national Conference on Research in Computational
Linguistics, Taipei, Taiwan.
Michael P Jones and James H Martin. 1997. Contex-
tual spelling correction using latent semantic analy-
sis. In Proceedings of the fifth conference on Ap-
plied natural language processing -, pages 166?
173, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Mark D Kernighan, Kenneth W Church, and
William A Gale. 1990. A Spelling Correc-
tion Program Based on a Noisy Channel Model.
In Proceedings of the 13th International Confer-
ence on Computational Linguistics, pages 205?210,
Helsinki, Finland.
Lothar Lemnitzer and Claudia Kunze. 2002. Ger-
maNet - Representation, Visualization, Application.
In Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC),
pages 1485?1491.
M Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine
cone from an ice cream cone. Proceedings of the
5th annual international conference, pages 24?26.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In Proceedings of International
Conference on Machine Learning, pages 296?304,
Madison, Wisconsin.
Aurelien Max and Guillaume Wisniewski. 2010.
Mining Naturally-occurring Corrections and Para-
phrases from Wikipedias Revision History. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10),
pages 3143?3148.
Eric Mays, Fred. J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517?522.
537
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedia?s Article Revision History for Train-
ing Computational Linguistics Algorithms. In
Proceedings of the AAAI Workshop on Wikipedia
and Artificial Intelligence: An Evolving Synergy
(WikiAI), WikiAI08.
Diane Nicholls. 1999. The Cambridge Learner Cor-
pus - Error Coding and Analysis for Lexicography
and ELT. In Summer Workshop on Learner Cor-
pora, Tokyo, Japan.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL Errors: Challenges and Rewards. In The 5th
Workshop on Innovative Use of NLP for Building
Educational Applications (NAACL-HLT).
H Rubenstein and J B Goodenough. 1965. Contextual
Correlates of Synonymy. Communications of the
ACM, 8(10):627?633.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland.
Daniel D. Walker, William B. Lund, and Eric K. Ring-
ger. 2010. Evaluating Models of Latent Document
Semantics in the Presence of OCR Errors. Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, (October):240?
250.
M. Wick, M. Ross, and E. Learned-Miller. 2007.
Context-sensitive error correction: Using topic
models to improve OCR. In Ninth International
Conference on Document Analysis and Recogni-
tion (ICDAR 2007) Vol 2, pages 1168?1172. Ieee,
September.
Amber Wilcox-OHearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction
with trigrams: A reconsideration of the Mays, Dam-
erau, and Mercer model. In Proceedings of the 9th
international conference on Computational linguis-
tics and intelligent text processing (CICLing).
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 365?368.
Torsten Zesch and Iryna Gurevych. 2010. Wisdom
of Crowds versus Wisdom of Linguists - Measur-
ing the Semantic Relatedness of Words. Journal of
Natural Language Engineering, 16(1):25?59.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008a. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC).
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008b. Using wiktionary for computing semantic
relatedness. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 861?867,
Chicago, IL, USA, Jul.
538
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 74?79,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Wikulu: An Extensible Architecture for Integrating Natural Language
Processing Techniques with Wikis
Daniel Ba?r, Nicolai Erbs, Torsten Zesch, and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
www.ukp.tu-darmstadt.de
Abstract
We present Wikulu1, a system focusing on
supporting wiki users with their everyday
tasks by means of an intelligent interface.
Wikulu is implemented as an extensible archi-
tecture which transparently integrates natural
language processing (NLP) techniques with
wikis. It is designed to be deployed with any
wiki platform, and the current prototype inte-
grates a wide range of NLP algorithms such
as keyphrase extraction, link discovery, text
segmentation, summarization, or text similar-
ity. Additionally, we show how Wikulu can
be applied for visually analyzing the results
of NLP algorithms, educational purposes, and
enabling semantic wikis.
1 Introduction
Wikis are web-based, collaborative content author-
ing systems (Leuf and Cunningham, 2001). As they
offer fast and simple means for adding and editing
content, they are used for various purposes such as
creating encyclopedias (e.g. Wikipedia2), construct-
ing dictionaries (e.g. Wiktionary3), or hosting online
communities (e.g. ACLWiki4). However, as wikis do
not enforce their users to structure pages or add com-
plementary metadata, wikis often end up as a mass
of unmanageable pages with meaningless page titles
and no usable link structure (Buffa, 2006).
To solve this issue, we present the Wikulu sys-
tem which uses natural language processing to sup-
port wiki users with their typical tasks of adding,
1Portmanteau of the Hawaiian terms wiki (?fast?) and kukulu
(?to organize?)
2http://www.wikipedia.org
3http://www.wiktionary.org
4http://aclweb.org/aclwiki
organizing, and finding content. For example,
Wikulu supports users with reading longer texts by
highlighting keyphrases using keyphrase extraction
methods such as TextRank (Mihalcea and Tarau,
2004). Support integrated in Wikulu also includes
text segmentation to segment long pages, text simi-
larity for detecting potential duplicates, or text sum-
marization to facilitate reading of lengthy pages.
Generally, Wikulu allows to integrate any NLP com-
ponent which conforms to the standards of Apache
UIMA (Ferrucci and Lally, 2004).
Wikulu is designed to integrate seamlessly with
any wiki. Our system is implemented as an HTTP
proxy server which intercepts the communication
between the web browser and the underlying wiki
engine. No further modifications to the original wiki
installation are necessary. Currently, our system pro-
totype contains adaptors for two widely used wiki
engines: MediaWiki5 and TWiki6. Adaptors for other
wiki engines can be added with minimal effort. Gen-
erally, Wikulu could also be applied to any web-
based system other than wikis with only slight mod-
ifications to its architecture.
In Figure 1, we show the integration of Wikulu
with Wikipedia.7 The additional user interface com-
ponents are integrated into the default toolbar (high-
lighted by a red box in the screenshot). In this ex-
ample, the user has requested keyphrase highlight-
ing in order to quickly get an idea about the main
content of the wiki article. Wikulu then invokes the
5http://mediawiki.org (e.g. used by Wikipedia)
6http://twiki.org (often used for corporate wikis)
7As screenshots only provide a limited overview of
Wikulu?s capabilities, we refer the reader to a screencast:
http://www.ukp.tu-darmstadt.de/research/
projects/wikulu
74
Figure 1: Integration of Wikulu with Wikipedia. The aug-
mented toolbar (red box) and the results of a keyphrase
extraction algorithm (yellow text spans) are highlighted.
corresponding NLP component, and highlights the
returned keyphrases in the article. In the next sec-
tion, we give a more detailed overview of the differ-
ent types of support provided by Wikulu.
2 Supporting Wiki Users by Means of NLP
In this section, we present the different types of
NLP-enabled support provided by Wikulu.
Detecting Duplicates Whenever users add new
content to a wiki there is the danger of duplicating
already contained information. In order to avoid du-
plication, users would need comprehensive knowl-
edge of what content is already present in the wiki,
which is almost impossible for large wikis like
Wikipedia. Wikulu helps to detect potential du-
plicates by computing the text similarity between
newly added content and each existing wiki page.
If a potential duplicate is detected, the user is noti-
fied and may decide to augment the duplicate page
instead of adding a new one. Wikulu integrates text
similarity measures such as Explicit Semantic Anal-
ysis (Gabrilovich and Markovitch, 2007) and Latent
Semantic Analysis (Landauer et al, 1998).
Suggesting Links While many wiki users read-
ily add textual contents to wikis, they often re-
strain from also adding links to related pages. How-
ever, links in wikis are crucial as they allow users
to quickly navigate from one page to another, or
browse through the wiki. Therefore, it may be rea-
sonable to augment a page about the topic sentiment
Figure 2: Automatic discovery of links to other wiki ar-
ticles. Suitable text phrases to place a link on are high-
lighted in green.
analysis by a link to a page providing related in-
formation such as evaluation datasets. Wikulu sup-
ports users in this tedious task by automatically sug-
gesting links. Link suggestion thereby is a two-step
process: (a) first, suitable text phrases are extracted
which might be worth to place a link on (see Fig-
ure 2), and (b) for each phrase, related pages are
ranked by comparing their relevance to the current
page, and then presented to the user. The user may
thus decide whether she wants to use a detected
phrase as a link or not, and if so, which other wiki
page to link this phrase to. Wikulu currently inte-
grates link suggestion algorithms by Geva (2007)
and Itakura and Clarke (2007).
Semantic Searching The capabilities of a wiki?s
built-in search engine are typically rather limited
as it traditionally performs e.g. keyword-based re-
trieval. If that keyword is not found in the wiki, the
query returns an empty result set. However, a page
might exist which is semantically related to the key-
word, and should thus yield a match.
As the search engine is typically a core part of the
wiki system, it is rather difficult to modify its be-
havior. However, by leveraging Wikulu?s architec-
ture, we can replace the default search mechanisms
by algorithms which allow for semantic search to al-
leviate the vocabulary mismatch problem (Gurevych
et al, 2007).
Segmenting Long Pages Due to the open edit-
ing policy of wikis, pages tend to grow rather fast.
75
Figure 3: Analysis of a wiki article with respect to topical
coherence. Suggested segment breaks are highlighted by
yellow bars.
For users, it is thus a major challenge to keep an
overview of what content is present on a certain
page. Wikulu therefore supports users by analyzing
long pages through employing text segmentation al-
gorithms which detect topically coherent segments
of text. It then suggests segment boundaries which
the user may or may not accept for inserting a sub-
heading which makes pages easier to read and better
to navigate. As shown in Figure 3, users are also en-
couraged to set a title for each segment.8 When ac-
cepting one or more of these suggested boundaries,
Wikulu stores them persistently in the wiki. Wikulu
currently integrates text segmentation methods such
as TextTiling (Hearst, 1997) or C99 (Choi, 2000).
Summarizing Pages Similarly to segmenting
pages, Wikulu makes long wiki pages more acces-
sible by generating an extractive summary. While
generative summaries generate a summary in own
words, extractive summaries analyze the original
wiki text sentence-by-sentence, rank each sentence,
and return a list of the most important ones (see Fig-
ure 4). Wikulu integrates extractive text summariza-
tion methods such as LexRank (Erkan and Radev,
2004).
Highlighting Keyphrases Another approach to
assist users in better grasping the idea of a wiki page
at a glance is to highlight important keyphrases (see
Figure 1). As Tucker and Whittaker (2009) have
8In future work, we plan to suggest suitable titles for each
segment automatically.
Figure 4: Extractive summary of the original wiki page
shown in Figure 3
shown, highlighting important phrases assists users
with reading longer texts and yields faster under-
standing. Wikulu thus improves readability by em-
ploying automatic keyphrase extraction algorithms.
Additionally, Wikulu allows to dynamically adjust
the number of keyphrases shown by presenting a
slider to the user. We integrated keyphrase extrac-
tion methods such as TextRank (Mihalcea and Tarau,
2004) and KEA (Witten et al, 1999).
3 Further Use Cases
Further use cases for supporting wiki users include
(i) visually analyzing the results of NLP algorithms,
(ii) educational purposes, and (iii) enabling semantic
wikis.
Visually Analyzing the Results of NLP Algo-
rithms Wikulu facilitates analyzing the results of
NLP algorithms by using wiki pages as input doc-
uments and visualizing the results directly on that
page. Consider an NLP algorithm which performs
sentiment analysis. Typically, we were to put our
analysis sentences in a text file, launch the NLP ap-
plication, process the file, and would read the output
from either a built-in console or a separate output
file. This procedure suffers from two major draw-
backs: (a) it is inconvenient to copy existing data
into a custom input format which can be fed into the
NLP system, and (b) the textual output does not al-
low presenting the results in a visually rich manner.
Wikulu tackles both challenges by using wiki
pages as input/output documents. For instance,
76
by running the sentiment analysis component right
from within the wiki, its output can be written back
to the originating wiki page, resulting in visually
rich, possibly interactive presentations.
Educational Purposes Wikulu is a handy tool for
educational purposes as it allows to (a) rapidly create
test data in a collaborative manner (see Section 2),
and (b) visualize the results of NLP algorithms, as
described above. Students can gather hands-on ex-
perience by experimenting with NLP components in
an easy-to-use wiki system. They can both collab-
oratively edit input documents, and explore possi-
ble results of e.g. different configurations of NLP
components. In our system prototype, we integrated
highlighting parts-of-speech which have been deter-
mined by a POS tagger.
Enabling Semantic Wikis Semantic wikis such
as the Semantic MediaWiki (Kro?tzsch et al, 2006)
augment standard wikis with machine-readable se-
mantic annotations of pages and links. As those
annotations have to be entered manually, this step
is often skipped by users which severely limits the
usefulness of semantic wikis. Wikulu could support
users e.g. by automatically suggesting the type of a
link by means of relation detection or the type of a
page by means of text categorization. Thus, Wikulu
could constitute an important step towards the se-
mantification of the content contained in wikis.
4 System Architecture
In this section, we detail our system architecture and
describe what is necessary to make NLP algorithms
available through our system. We also give a walk-
through of Wikulu?s information flow.
4.1 Core Components
Wikulu builds upon a modular architecture, as de-
picted in Figure 5. It acts as an HTTP proxy server
which intercepts the communication between the
web browser and the target wiki engine, while it al-
lows to run any Apache UIMA-compliant NLP com-
ponent using an extensible plugin mechanism.
In the remainder of this section, we introduce each
module: (a) the proxy server which allows to add
Wikulu to any target wiki engine, (b) the JavaScript
injection that bridges the gap between the client- and
server-side code, (c) the plugin manager which gives
access to any Apache UIMA-based NLP component,
and (d) the wiki abstraction layer which offers a
high-level interface to typical wiki operations such
as reading and writing the wiki content.
Proxy Server Wikulu is designed to work with
any underlying wiki engine such as MediaWiki or
TWiki. Consequently, we implemented it as an
HTTP proxy server which allows it to be enabled at
any time by changing the proxy settings of a user?s
web browser.9 The proxy server intercepts all re-
quests between the user who interacts with her web
browser, and the underlying wiki engine. For ex-
ample, Wikulu passes certain requests to its lan-
guage processing components, or augments the de-
fault wiki toolbar by additional commands. We elab-
orate on the latter in the following paragraph.
JavaScript Injection Wikulu modifies the re-
quests between web browser and target wiki by in-
jecting custom client-side JavaScript code. Wikulu
is thus capable of altering the default behavior of
the wiki engine, e.g. replacing a keyword-based re-
trieval by enhanced search methods (cf. Section 2),
adding novel behavior such as additional toolbar
buttons or advanced input fields, or augmenting the
originating web page after a certain request has been
processed, e.g. an NLP algorithm has been run.
Plugin Manager Wikulu does not perform lan-
guage processing itself. It relies on Apache UIMA-
compliant NLP components which use wiki pages
(or parts thereof) as input texts. Wikulu offers a so-
phisticated plugin manager which takes care of dy-
namically loading those NLP components. The plu-
gin loader is designed to run plugins either every
time a wiki page loads, or manually by picking them
from the augmented wiki toolbar.
The NLP components are available as server-side
Java classes. Via direct web remoting10, those com-
ponents are made accessible through a JavaScript
proxy object. Wikulu offers a generic language pro-
cessing plugin which takes the current page contents
9The process of enabling a custom proxy server can be
simplified by using web browser extensions such as Mul-
tiproxy Switch (https://addons.mozilla.org/de/
firefox/addon/multiproxy-switch).
10http://directwebremoting.org
77
Browser
Duplicate Detection
JavaScript
Injection
P
l
u
g
i
n
M
a
n
a
g
e
r
Wiki Abstraction
Layer
Wiki
Semantic Search
Link Suggestion
Text Segmentation
Text Summarization
Keyphrase Highlighting
...
W
i
k
u
l
u
 
P
r
o
x
y
Apache UIMA-compliant
NLP components
User
Figure 5: Wikulu acts as a proxy server which intercepts
the communication between the web browser and the un-
derlying wiki engine. Its plugin manager allows to inte-
grate any Apache UIMA-compliant NLP component.
as input text, runs an NLP component, and writes its
output back to the wiki. To run a custom Apache
UIMA-compliant NLP component with Wikulu, one
just needs to plug that particular NLP component
into the generic plugin. No further adaptations to
the generic plugin are necessary. However, more ad-
vanced users may create fully customized plugins.
Wiki Abstraction Layer Wikulu communicates
with the underlying wiki engine via an abstraction
layer. That layer provides a generic interface for
accessing and manipulating the underlying wiki en-
gine. Thereby, Wikulu can both be tightly coupled to
a certain wiki instance such as MediaWiki or TWiki,
while being flexible at the same time to adapt to a
changing environment. New adaptors for other tar-
get wiki engines such as Confluence11 can be added
with minimal effort.
4.2 Walk-Through Example
Let?s assume that a user encounters a wiki page
which is rather lengthy. She realizes that Wikulu?s
keyphrase extraction component might help her to
better grasp the idea of this page at a glance, so
she activates Wikulu by setting her web browser to
pass all requests through the proxy server. After
11http://www.atlassian.com/software/
confluence
JS
Injection
Proxy
Server
Keyphr.
Plugin
Wiki 
Abstr. Lay.
Wiki
get content from wiki page
get
page
extract
keyphrases
Browser
highlight
keyphrases
inject
keyphrases
Figure 6: Illustration of Wikulu?s information flow when
a user has requested to highlight keyphrases on the cur-
rent page as described in Section 4.2
applying the settings, the JavaScript injection mod-
ule adds additional links to the wiki?s toolbar on
the originating wiki page. Having decided to ap-
ply keyphrase extraction, she then invokes that NLP
component by clicking the corresponding link (see
Figure 6). Before the request is passed to that com-
ponent, Wikulu extracts the wiki page contents us-
ing the high-level wiki abstraction layer. Thereafter,
the request is passed via direct web remoting to the
NLP component which has been loaded by Wikulu?s
plugin mechanism. After processing the request, the
extracted keyphrases are returned to Wikulu?s cus-
tom JavaScript handlers and finally highlighted in
the originating wiki page.
5 Related Work
Supporting wiki users with NLP techniques has not
attracted a lot of research attention yet. A no-
table exception is the work by Witte and Gitzinger
(2007). They propose an architecture to connect
wikis to services providing NLP functionality which
are based on the General Architecture for Text En-
gineering (Cunningham et al, 2002). Contrary to
Wikulu, though, their system does not integrate
transparently with an underlying wiki engine, but
rather uses a separate application to apply NLP tech-
niques. Thereby, wiki users can leverage the power
of NLP algorithms, but need to interrupt their cur-
rent workflow to switch to a different application.
78
Moreover, their system is only loosely coupled with
the underlying wiki engine. While it allows to read
and write existing pages, it does not allow further
modifications such as adding user interface controls.
A lot of work in the wiki community is done in the
context of Wikipedia. For example, the FastestFox12
plug-in for Wikipedia is able to suggest links to re-
lated articles. However, unlike Wikulu, FastestFox
is tailored towards Wikipedia and cannot be used
with any other wiki platform.
6 Summary
We presented Wikulu, an extensible system which
integrates natural language processing techniques
with wikis. Wikulu addresses the major challenge of
supporting wiki users with their everyday tasks. Be-
sides that, we demonstrated how Wikulu serves as
a flexible environment for (a) visually analyzing the
results of NLP algorithms, (b) educational purposes,
and (c) enabling semantic wikis. By its modular and
flexible architecture, we envision that Wikulu can
support wiki users both in small focused environ-
ments as well as in large-scale communities such as
Wikipedia.
Acknowledgments
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Klaus Tschira Foun-
dation under project No. 00.133.2008. We would like to
thank Johannes Hoffart for designing and implementing
the foundations of this work, as well as Artem Vovk and
Carolin Deeg for their contributions.
References
Michel Buffa. 2006. Intranet Wikis. In Proceedings
of the IntraWebs Workshop at the 15th International
Conference on World Wide Web.
Freddy Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceedings of the
1st Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In
Proc. of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 168?175.
12http://smarterfox.com
Gu?nes? Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Re-
search, 22:457?479.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information Pro-
cessing in the Corporate Research Environment. Nat-
ural Language Engineering, pages 1?26.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Shlomo Geva. 2007. GPX: Ad-Hoc Queries and Auto-
mated Link Discovery in the Wikipedia. In Prepro-
ceedings of the INEX Workshop, pages 404?416.
Iryna Gurevych, Christof Mu?ller, and Torsten Zesch.
2007. What to be??Electronic Career Guidance Based
on Semantic Relatedness. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 1032?1039.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Kelly Y. Itakura and Charles L. A. Clarke. 2007. Univer-
sity of Waterloo at INEX2007: Adhoc and Link-the-
Wiki Tracks. In INEX 2007 Workshop Preproceed-
ings, pages 417?425.
Markus Kro?tzsch, Denny Vrandec?ic?, and Max Vo?lkel.
2006. Semantic MediaWiki. In Proc. of the 5th Inter-
national Semantic Web Conference, pages 935?942.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to Latent Semantic Analysis.
Discourse Processes, 25(2):259?284.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way:
Collaboration and Sharing on the Internet. Addison-
Wesley Professional.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 404?411.
Simon Tucker and Steve Whittaker. 2009. Have A Say
Over What You See: Evaluating Interactive Compres-
sion Techniques. In Proceedings of the Intl. Confer-
ence on Intelligent User Interfaces, pages 37?46.
Rene? Witte and Thomas Gitzinger. 2007. Connecting
wikis and natural language processing systems. In
Proc. of the Intl. Symposium on Wikis, pages 165?176.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Proceed-
ings of the 4th ACM Conference on Digital Libraries,
pages 254?255.
79
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97?102,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia?s Edit History
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de
Abstract
We present an open-source toolkit which
allows (i) to reconstruct past states of
Wikipedia, and (ii) to efficiently access the
edit history of Wikipedia articles. Recon-
structing past states of Wikipedia is a pre-
requisite for reproducing previous experimen-
tal work based on Wikipedia. Beyond that,
the edit history of Wikipedia articles has been
shown to be a valuable knowledge source for
NLP, but access is severely impeded by the
lack of efficient tools for managing the huge
amount of provided data. By using a dedi-
cated storage format, our toolkit massively de-
creases the data volume to less than 2% of
the original size, and at the same time pro-
vides an easy-to-use interface to access the re-
vision data. The language-independent design
allows to process any language represented in
Wikipedia. We expect this work to consolidate
NLP research using Wikipedia in general, and
to foster research making use of the knowl-
edge encoded in Wikipedia?s edit history.
1 Introduction
In the last decade, the free encyclopedia Wikipedia
has become one of the most valuable and com-
prehensive knowledge sources in Natural Language
Processing. It has been used for numerous NLP
tasks, e.g. word sense disambiguation, semantic re-
latedness measures, or text categorization. A de-
tailed survey on usages of Wikipedia in NLP can be
found in (Medelyan et al, 2009).
The majority of Wikipedia-based NLP algorithms
works on single snapshots of Wikipedia, which are
published by the Wikimedia Foundation as XML
dumps at irregular intervals.1 Such a snapshot only
represents the state of Wikipedia at a certain fixed
point in time, while Wikipedia actually is a dynamic
resource that is constantly changed by its millions of
editors. This rapid change is bound to have an influ-
ence on the performance of NLP algorithms using
Wikipedia data. However, the exact consequences
are largely unknown, as only very few papers have
systematically analyzed this influence (Zesch and
Gurevych, 2010). This is mainly due to older snap-
shots becoming unavailable, as there is no official
backup server. As a consequence, older experimen-
tal results cannot be reproduced anymore.
In this paper, we present a toolkit that solves
both issues by reconstructing a certain past state of
Wikipedia from its edit history, which is offered by
the Wikimedia Foundation in form of a database
dump. Section 3 gives a more detailed overview of
the reconstruction process.
Besides reconstructing past states of Wikipedia,
the revision history data also constitutes a novel
knowledge source for NLP algorithms. The se-
quence of article edits can be used as training data
for data-driven NLP algorithms, such as vandalism
detection (Chin et al, 2010), text summarization
(Nelken and Yamangil, 2008), sentence compres-
sion (Yamangil and Nelken, 2008), unsupervised
extraction of lexical simplifications (Yatskar et al,
2010), the expansion of textual entailment corpora
(Zanzotto and Pennacchiotti, 2010), or assesing the
trustworthiness of Wikipedia articles (Zeng et al,
2006).
1http://download.wikimedia.org/
97
However, efficient access to this new resource
has been limited by the immense size of the data.
The revisions for all articles in the current English
Wikipedia sum up to over 5 terabytes of text. Con-
sequently, most of the above mentioned previous
work only regarded small samples of the available
data. However, using more data usually leads to bet-
ter results, or how Church and Mercer (1993) put
it ?more data are better data?. Thus, in Section 4,
we present a tool to efficiently access Wikipedia?s
edit history. It provides an easy-to-use API for pro-
grammatically accessing the revision data and re-
duces the required storage space to less than 2% of
its original size. Both tools are publicly available
on Google Code (http://jwpl.googlecode.
com) as open source software under the LGPL v3.
2 Related Work
To our knowledge, there are currently only two alter-
natives to programmatically access Wikipedia?s re-
vision history.
One possibility is to manually parse the original
XML revision dump. However, due to the huge size
of these dumps, efficient, random access is infeasi-
ble with this approach.
Another possibility is using the MediaWiki API2,
a web service which directly accesses live data from
the Wikipedia website. However, using a web ser-
vice entails that the desired revision for every single
article has to be requested from the service, trans-
ferred over the Internet and then stored locally in
an appropriate format. Access to all revisions of
all Wikipedia articles for a large-scale analysis is
infeasible with this method because it is strongly
constricted by the data transfer speed over the In-
ternet. Even though it is possible to bypass this bot-
tleneck by setting up a local Wikipedia mirror, the
MediaWiki API can only provide full text revisions,
which results in very large amounts of data to be
transferred.
Better suited for tasks of this kind are APIs
that utilize databases for storing and accessing the
Wikipedia data. However, current database-driven
Wikipedia APIs do not support access to article re-
visions. That is why we decided to extend an es-
tablished API with the ability to efficiently access
2http://www.mediawiki.org/wiki/API
Wikipedia?s edit history. Two established Wikipedia
APIs have been considered for this purpose.
Wikipedia Miner3 (Milne and Witten, 2009) is
an open source toolkit which provides access to
Wikipedia with the help of a preprocessed database.
It represents articles, categories and redirects as Java
classes and provides access to the article content ei-
ther as MediaWiki markup or as plain text. The
toolkit mainly focuses on Wikipedia?s structure, the
contained concepts, and semantic relations, but it
makes little use of the textual content within the ar-
ticles. Even though it was developed to work lan-
guage independently, it focuses mainly on the En-
glish Wikipedia.
Another open source API for accessing Wikipedia
data from a preprocessed database is JWPL4 (Zesch
et al, 2008). Like Wikipedia Miner, it also rep-
resents the content and structure of Wikipedia as
Java objects. In addition to that, JWPL contains a
MediaWiki markup parser to further analyze the ar-
ticle contents to make available fine-grained infor-
mation like e.g. article sections, info-boxes, or first
paragraphs. Furthermore, it was explicitly designed
to work with all language versions of Wikipedia.
We have chosen to extend JWPL with our revi-
sion toolkit, as it has better support for accessing ar-
ticle contents, natively supports multiple languages,
and seems to have a larger and more active developer
community. In the following section, we present the
parts of the toolkit which reconstruct past states of
Wikipedia, while in section 4, we describe tools al-
lowing to efficiently access Wikipedia?s edit history.
3 Reconstructing Past States of Wikipedia
Access to arbitrary past states of Wikipedia is re-
quired to (i) evaluate the performance of Wikipedia-
based NLP algorithms over time, and (ii) to repro-
duce Wikipedia-based research results. For this rea-
son, we have developed a tool called TimeMachine,
which addresses both of these issues by making use
of the revision dump provided by the Wikimedia
Foundation. By iterating over all articles in the re-
vision dump and extracting the desired revision of
each article, it is possible to recover the state of
Wikipedia at an earlier point in time.
3http://wikipedia-miner.sourceforge.net
4http://jwpl.googlecode.com
98
Property Description Example Value
language The Wikipedia language version english
mainCategory Title of the main category of the
Wikipedia language version used
Categories
disambiguationCategory Title of the disambiguation category of
the Wikipedia language version used
Disambiguation
fromTimestamp Timestamp of the first snapshot to be
extracted
20090101130000
toTimestamp Timestamp of the last snapshot to be ex-
tracted
20091231130000
each Interval between snapshots in days 30
removeInputFilesAfterProcessing Remove source files [true/false] false
metaHistoryFile Path to the revision dump PATH/pages-meta-history.xml.bz2
pageLinksFile Path to the page-to-page link records PATH/pagelinks.sql.gz
categoryLinksFile Path to the category membership
records
PATH/categorylinks.sql.gz
outputDirectory Output directory PATH/outdir/
Table 1: Configuration of the TimeMachine
The TimeMachine is controlled by a single con-
figuration file, which allows (i) to restore individual
Wikipedia snapshots or (ii) to generate whole snap-
shot series. Table 1 gives an overview of the con-
figuration parameters. The first three properties set
the environment for the specific language version of
Wikipedia. The two timestamps define the start and
end time of the snapshot series, while the interval
between the snapshots in the series is set by the pa-
rameter each. In the example, the TimeMachine re-
covers 13 snapshots between Jan 01, 2009 at 01.00
p.m and and Dec 31, 2009 at 01.00 p.m at an inter-
val of 30 days. In order to recover a single snap-
shot, the two timestamps have simply to be set to
the same value, while the parameter ?each? has no
effect. The option removeInputFilesAfterProcessing
specifies whether to delete the source files after pro-
cessing has finished. The final four properties define
the paths to the source files and the output directory.
The output of the TimeMachine is a set of eleven
text files for each snapshot, which can directly be
imported into an empty JWPL database. It can be
accessed with the JWPL API in the same way as
snapshots created using JWPL itself.
Issue of Deleted Articles The past snapshot of
Wikipedia created by our toolkit is identical to the
state of Wikipedia at that time with the exception of
articles that have been deleted meanwhile. Articles
might be deleted only by Wikipedia administrators
if they are subject to copyright violations, vandal-
ism, spam or other conditions that violate Wikipedia
policies. As a consequence, they are removed from
the public view along with all their revision infor-
mation, which makes it impossible to recover them
from any future publicly available dump.5 Even
though about five thousand pages are deleted every
day, only a small percentage of those pages actually
corresponds to meaningful articles. Most of the af-
fected pages are newly created duplicates of already
existing articles or spam articles.
4 Efficient Access to Revisions
Even though article revisions are available from the
official Wikipedia revision dumps, accessing this in-
formation on a large scale is still a difficult task.
This is due to two main problems. First, the revi-
sion dump contains all revisions as full text. This
results in a massive amount of data and makes struc-
tured access very hard. Second, there is no efficient
API available so far for accessing article revisions
on a large scale.
Thus, we have developed a tool called
RevisionMachine, which solves these issues.
First, we describe our solution to the storage prob-
lem. Second, we present several use cases of the
RevisionMachine, and show how the API simplifies
experimental setups.
5http://en.wikipedia.org/wiki/Wikipedia:
DEL
99
4.1 Revision Storage
As each revision of a Wikipedia article stores the
full article text, the revision history obviously con-
tains a lot of redundant data. The RevisionMachine
makes use of this fact and utilizes a dedicated stor-
age format which stores a revision only by means
of the changes that have been made to the previous
revision. For this purpose, we have tested existing
diff libraries, like Javaxdelta6 or java-diff7, which
calculate the differences between two texts. How-
ever, both their runtime and the size of the result-
ing output was not feasible for the given size of the
data. Therefore, we have developed our own diff
algorithm, which is based on a longest common sub-
string search and constitutes the foundation for our
revision storage format.
The processing of two subsequent revisions can
be divided into four steps:
? First, the RevisionMachine searches for all
common substrings with a user-defined mini-
mal length.
? Then, the revisions are divided into blocks of
equal length. Corresponding blocks of both
revisions are then compared. If a block is
contained in one of the common substrings,
it can be marked as unchanged. Otherwise,
we have to categorize the kind of change
that occurred in this block. We differenti-
ate between five possible actions: Insert,
Delete, Replace, Cut and Paste8. This
information is stored in each block and is later
on used to encode the revision.
? In the next step, the current revision is repre-
sented by means of a sequence of actions per-
formed on the previous revision.
For example, in the adjacent revision pair
r1 : This is the very first sentence!
r2 : This is the second sentence
r2 can be encoded as
REPLACE 12 10 ?second?
DELETE 31 1
6http://javaxdelta.sourceforge.net/
7http://www.incava.org/projects/java/
java-diff
8Cut and Paste operations always occur pairwise. In ad-
dition to the other operations, they can make use of an additional
temporary storage register to save the text that is being moved.
? Finally, the string representation of this ac-
tion sequence is compressed and stored in the
database.
With this approach, we achieve to reduce the de-
mand for disk space for a recent English Wikipedia
dump containing all article revisions from 5470 GB
to only 96 GB, i.e. by 98%. The compressed data is
stored in a MySQL database, which provides sophis-
ticated indexing mechanisms for high-performance
access to the data.
Obviously, storing only the changes instead of
the full text of each revision trades in speed for
space. Accessing a certain revision now requires re-
constructing the text of the revision from a list of
changes. As articles often have several thousand re-
visions, this might take too long. Thus, in order to
speed up the recovery of the revision text, every n-th
revision is stored as a full revision. A low value of
n decreases the time needed to access a certain re-
vision, but increases the demand for storage space.
We have found n = 1000 to yield a good trade-off9.
This parameter, among a few other possibilities to
fine-tune the process, can be set in a graphical user
interface provided with the RevisionMachine.
4.2 Revision Access
After the converted revisions have been stored in
the revision database, it can either be used stand-
alone or combined with the JWPL data and ac-
cessed via the standard JWPL API. The latter op-
tion makes it possible to combine the possibilities
of the RevisionMachine with other components like
the JWPL parser for the MediaWiki syntax.
In order to set up the RevisionMachine, it is only
necessary to provide the configuration details for the
database connection (see Listing 1). Upon first ac-
cess, the database user has to have write permission
on the database, as indexes have to be created. For
later use, read permission is sufficient. Access to the
RevisionMachine is achieved via two API objects.
The RevisionIterator allows to iterate over all revi-
sions in Wikipedia. The RevisionAPI grants access
to the revisions of individual articles. In addition to
9If hard disk space is no limiting factor, the parameter can be
set to 1 to avoid the compression of the revisions and maximize
the performance.
100
/ / S e t up d a t a b a s e c o n n e c t i o n
DatabaseConfiguration db = new DatabaseConfiguration ( ) ;
db .setDatabase ( ? dbname ? ) ;
db .setHost ( ? hos tname ? ) ;
db .setUser ( ? username ? ) ;
db .setPassword ( ?pwd? ) ;
db .setLanguage (Language .english ) ;
/ / C r e a t e API o b j e c t s
Wikipedia wiki = WikiConnectionUtils .getWikipediaConnection (db ) ;
RevisionIterator revIt = new RevisionIterator (db ) ;
RevisionApi revApi = new RevisionApi (db ) ;
Listing 1: Setting up the RevisionMachine
that, the Wikipedia object provides access to JWPL
functionalities.
In the following, we describe three use cases of
the RevisionMachine API, which demonstrate how
it is easily integrated into experimental setups.
Processing all article revisions in Wikipedia
The first use case focuses on the utilization of the
complete set of article revisions in a Wikipedia snap-
shot. Listing 2 shows how to iterate over all revi-
sions. Thereby, the iterator ensures that successive
revisions always correspond to adjacent revisions of
a single article in chronological order. The start of
a new article can easily be detected by checking the
timestamp and the article id. This approach is es-
pecially useful for applications in statistical natural
language processing, where large amounts of train-
ing data are a vital asset.
Processing revisions of individual articles The
second use case shows how the RevisionMachine
can be used to access the edit history of a specific
article. The example in Listing 3 illustrates how all
revisions for the article Automobile can be retrieved
by first performing a page query with the JWPL API
and then retrieving all revision timestamps for this
page, which can finally be used to access the revi-
sion objects.
Accessing the meta data of a revision The third
use case illustrates the access to the meta data of in-
dividual revisions. The meta data includes the name
or IP of the contributor, the additional user comment
for the revision and a flag that identifies a revision as
minor or major. Listing 4 shows how the number of
edits and unique contributors can be used to indicate
the level of edit activity for an article.
5 Conclusions
In this paper, we presented an open-source toolkit
which extends JWPL, an API for accessing
Wikipedia, with the ability to reconstruct past states
of Wikipedia, and to efficiently access the edit his-
tory of Wikipedia articles.
Reconstructing past states of Wikipedia is a
prerequisite for reproducing previous experimen-
tal work based on Wikipedia, and is also a re-
quirement for the creation of time-based series of
Wikipedia snapshots and for assessing the influence
of Wikipedia growth on NLP algorithms. Further-
more, Wikipedia?s edit history has been shown to be
a valuable knowledge source for NLP, which is hard
to access because of the lack of efficient tools for
managing the huge amount of revision data. By uti-
lizing a dedicated storage format for the revisions,
our toolkit massively decreases the amount of data
to be stored. At the same time, it provides an easy-
to-use interface to access the revision data.
We expect this work to consolidate NLP re-
search using Wikipedia in general, and to foster
research making use of the knowledge encoded in
Wikipedia?s edit history. The toolkit will be made
available as part of JWPL, and can be obtained from
the project?s website at Google Code. (http://
jwpl.googlecode.com)
Acknowledgments
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Hessian research
excellence program ?Landes-Offensive zur Entwicklung
Wissenschaftlich-o?konomischer Exzellenz? (LOEWE) as
part of the research center ?Digital Humanities?. We
would also like to thank Simon Kulessa for designing and
implementing the foundations of the RevisionMachine.
101
/ / I t e r a t e ove r a l l r e v i s i o n s o f a l l a r t i c l e s
w h i l e (revIt .hasNext ( ) ) {
Revision rev = revIt .next ( )
rev .getTimestamp ( ) ;
rev .getArticleID ( ) ;
/ / p r o c e s s r e v i s i o n . . .
}
Listing 2: Iteration over all revisions of all articles
/ / Get a r t i c l e wi th t i t l e ? Automobi le ?
Page article = wiki .getPage ( ? Automobi le ? ) ;
i n t id = article .getPageId ( ) ;
/ / Get a l l r e v i s i o n s f o r t h e a r t i c l e
Collection<Timestamp> revisionTimeStamps = revApi .getRevisionTimestamps (id ) ;
f o r (Timestamp t :revisionTimeStamps ) {
Revision rev = revApi .getRevision (id , t ) ;
/ / p r o c e s s r e v i s i o n . . .
}
Listing 3: Accessing the revisions of a specific article
/ / Meta d a t a p r o v i d e d by t h e Rev i s ionAPI
StringBuffer s = new StringBuffer ( ) ;
s .append ( ? The a r t i c l e has ?+revApi .getNumberOfRevisions (pageId ) +? r e v i s i o n s .\ n ? ) ;
s .append ( ? I t has ?+revApi .getNumberOfUniqueContributors (pageId ) +? un iq ue c o n t r i b u t o r s .\ n ? ) ;
s .append (revApi .getNumberOfUniqueContributors (pageId , t r u e ) + ? a r e r e g i s t e r e d u s e r s .\ n ? ) ;
/ / Meta d a t a p r o v i d e d by t h e R e v i s i o n o b j e c t
s .append ( (rev .isMinor ( ) ? ? Minor ? : ? Major ? ) +? r e v i s i o n by : ?+rev .getContributorID ( ) ) ;
s .append ( ?\nComment : ?+rev .getComment ( ) ) ;
Listing 4: Accessing the meta data of a revision
References
Si-Chi Chin, W. Nick Street, Padmini Srinivasan, and
David Eichmann. 2010. Detecting wikipedia vandal-
ism with active learning and statistical language mod-
els. In Proceedings of the 4th workshop on Informa-
tion credibility, WICOW ?10, pages 3?10.
Kenneth W. Church and Robert L. Mercer. 1993. Intro-
duction to the special issue on computational linguis-
tics using large corpora. Computational Linguistics,
19(1):1?24.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from wikipedia.
Int. J. Hum.-Comput. Stud., 67:716?754, September.
D. Milne and I. H. Witten. 2009. An open-source toolkit
for mining Wikipedia. In Proc. New Zealand Com-
puter Science Research Student Conf., volume 9.
Rani Nelken and Elif Yamangil. 2008. Mining
wikipedia?s article revision history for training com-
putational linguistics algorithms. In Proceedings of
the AAAI Workshop on Wikipedia and Artificial Intel-
ligence: An Evolving Synergy (WikiAI), WikiAI08.
Elif Yamangil and Rani Nelken. 2008. Mining wikipedia
revision histories for improving sentence compres-
sion. In Proceedings of ACL-08: HLT, Short Papers,
pages 137?140, Columbus, Ohio, June. Association
for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 365?368.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
wikipedia using co-training. In Proceedings of the
COLING-Workshop on The People?s Web Meets NLP:
Collaboratively Constructed Semantic Resources.
Honglei Zeng, Maher Alhossaini, Li Ding, Richard Fikes,
and Deborah L. McGuinness. 2006. Computing trust
from revision history. In Proceedings of the 2006 In-
ternational Conference on Privacy, Security and Trust.
Torsten Zesch and Iryna Gurevych. 2010. The more the
better? Assessing the influence of wikipedia?s growth
on semantic relatedness measures. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC), Valletta, Malta.
Torsten Zesch, Christof Mueller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC).
102
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 451?455,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Partial Textual Entailment
Omer Levy? Torsten Zesch? Ido Dagan? Iryna Gurevych?
? Natural Language Processing Lab ? Ubiquitous Knowledge Processing Lab
Computer Science Department Computer Science Department
Bar-Ilan University Technische Universita?t Darmstadt
Abstract
Textual entailment is an asymmetric rela-
tion between two text fragments that de-
scribes whether one fragment can be in-
ferred from the other. It thus cannot cap-
ture the notion that the target fragment
is ?almost entailed? by the given text.
The recently suggested idea of partial tex-
tual entailment may remedy this problem.
We investigate partial entailment under the
faceted entailment model and the possibil-
ity of adapting existing textual entailment
methods to this setting. Indeed, our results
show that these methods are useful for rec-
ognizing partial entailment. We also pro-
vide a preliminary assessment of how par-
tial entailment may be used for recogniz-
ing (complete) textual entailment.
1 Introduction
Approaches for applied semantic inference over
texts gained growing attention in recent years,
largely triggered by the textual entailment frame-
work (Dagan et al, 2009). Textual entailment is
a generic paradigm for semantic inference, where
the objective is to recognize whether a textual hy-
pothesis (labeled H) can be inferred from another
given text (labeled T ). The definition of textual
entailment is in some sense strict, in that it requires
that H?s meaning be implied by T in its entirety.
This means that from an entailment perspective, a
text that contains the main ideas of a hypothesis,
but lacks a minor detail, is indiscernible from an
entirely unrelated text. For example, if T is ?mus-
cles move bones?, and H ?the main job of muscles
is to move bones?, then T does not entail H , and
we are left with no sense of how close (T,H) were
to entailment.
In the related problem of semantic text similar-
ity, gradual measures are already in use. The se-
mantic text similarity challenge in SemEval 2012
(Agirre et al, 2012) explicitly defined different
levels of similarity from 5 (semantic equivalence)
to 0 (no relation). For instance, 4 was defined
as ?the two sentences are mostly equivalent, but
some unimportant details differ?, and 3 meant that
?the two sentences are roughly equivalent, but
some important information differs?. Though this
modeling does indeed provide finer-grained no-
tions of similarity, it is not appropriate for seman-
tic inference for two reasons. First, the term ?im-
portant information? is vague; what makes one de-
tail more important than another? Secondly, simi-
larity is not sufficiently well-defined for sound se-
mantic inference; for example, ?snowdrops bloom
in summer? and ?snowdrops bloom in winter?
may be similar, but have contradictory meanings.
All in all, these measures of similarity do not quite
capture the gradual relation needed for semantic
inference.
An appealing approach to dealing with the
rigidity of textual entailment, while preserving the
more precise nature of the entailment definition, is
by breaking down the hypothesis into components,
and attempting to recognize whether each one is
individually entailed by T . It is called partial tex-
tual entailment, because we are only interested in
recognizing whether a single element of the hy-
pothesis is entailed. To differentiate the two tasks,
we will refer to the original textual entailment task
as complete textual entailment.
Partial textual entailment was first introduced
by Nielsen et al (2009), who presented a ma-
chine learning approach and showed significant
improvement over baseline methods. Recently, a
public benchmark has become available through
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment (RTE) Challenge in
SemEval 2013 (Dzikovska et al, 2013), on which
we focus in this paper.
Our goal in this paper is to investigate the idea
of partial textual entailment, and assess whether
451
existing complete textual entailment methods can
be used to recognize it. We assume the facet
model presented in SemEval 2013, and adapt ex-
isting technologies to the task of recognizing par-
tial entailment (Section 3). Our work further ex-
pands upon (Nielsen et al, 2009) by evaluating
these adapted methods on the new RTE-8 bench-
mark (Section 4). Partial entailment may also fa-
cilitate an alternative divide and conquer approach
to complete textual entailment. We provide an ini-
tial investigation of this approach (Section 5).
2 Task Definition
In order to tackle partial entailment, we need to
find a way to decompose a hypothesis. Nielsen et
al. (2009) defined a model of facets, where each
such facet is a pair of words in the hypothesis
and the direct semantic relation connecting those
two words. We assume the simplified model that
was used in RTE-8, where the relation between the
words is not explicitly stated. Instead, it remains
unstated, but its interpreted meaning should corre-
spond to the manner in which the words are related
in the hypothesis. For example, in the sentence
?the main job of muscles is to move bones?, the
pair (muscles, move) represents a facet. While it is
not explicitly stated, reading the original sentence
indicates that muscles is the agent of move.
Formally, the task of recognizing faceted entail-
ment is a binary classification task. Given a text T ,
a hypothesis H , and a facet within the hypothesis
(w1, w2), determine whether the facet is either ex-
pressed or unaddressed by the text. Nielsen et al
included additional classes such as contradicting,
but in the scope of this paper we will only tend to
the binary case, as was done in RTE-8.
Consider the following example:
T: Muscles generate movement in the body.
H: The main job of muscles is to move bones.
The facet (muscles, move) refers to the agent role
in H , and is expressed by T . However, the facet
(move, bones), which refers to a theme or direct
object relation in H , is unaddressed by T .
3 Recognizing Faceted Entailment
Our goal is to investigate whether existing entail-
ment recognition approaches can be adapted to
recognize faceted entailment. Hence, we speci-
fied relatively simple decision mechanisms over a
set of entailment detection modules. Given a text
and a facet, each module reports whether it rec-
ognizes entailment, and the decision mechanism
then determines the binary class (expressed or un-
addressed) accordingly.
3.1 Entailment Modules
Current textual entailment systems operate across
different linguistic levels, mainly on lexical infer-
ence and syntax. We examined three representa-
tive modules that reflect these levels: Exact Match,
Lexical Inference, and Syntactic Inference.
Exact Match We represent T as a bag-of-words
containing all tokens and lemmas appearing in the
text. We then check whether both facet lemmas
w1, w2 appear in the text?s bag-of-words. Exact
matching was used as a baseline in previous rec-
ognizing textual entailment challenges (Bentivogli
et al, 2011), and similar methods of lemma-
matching were used as a component in recogniz-
ing textual entailment systems (Clark and Harri-
son, 2010; Shnarch et al, 2011).
Lexical Inference This feature checks whether
both facet words, or semantically related words,
appear in T . We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term wi as matched if the sim-
ilarity score exceeds a certain threshold (0.9, em-
pirically determined on the training set). Both w1
and w2 must match for this module?s entailment
decision to be positive.
Syntactic Inference This module builds upon
the open source1 Bar-Ilan University Textual En-
tailment Engine (BIUTEE) (Stern and Dagan,
2011). BIUTEE operates on dependency trees by
applying a sequence of knowledge-based transfor-
mations that converts T into H . It determines en-
tailment according to the ?cost? of generating the
hypothesis from the text. The cost model can be
automatically tuned with a relatively small train-
ing set. BIUTEE has shown state-of-the-art per-
formance on previous recognizing textual entail-
ment challenges (Stern and Dagan, 2012).
Since BIUTEE processes dependency trees,
both T and the facet must be parsed. We therefore
extract a path in H?s dependency tree that repre-
sents the facet. This is done by first parsing H ,
and then locating the two nodes whose words com-
pose the facet. We then find their lowest common
ancestor (LCA), and extract the path P from w1 to
1cs.biu.ac.il/?nlp/downloads/biutee
452
w2 through the LCA. This path is in fact a depen-
dency tree. BIUTEE can now be given T and P
(as the hypothesis), and try to recognize whether
the former entails the latter.
3.2 Decision Mechanisms
We started our experimentation process by defin-
ing Exact Match as a baseline. Though very sim-
ple, this unsupervised baseline performed surpris-
ingly well, with 0.96 precision and 0.32 recall on
expressed facets of the training data. Given its
very high precision, we decided to use this mod-
ule as an initial filter, and employ the others for
classifying the ?harder? cases.
We present all the mechanisms that we tested:
Baseline Exact
BaseLex Exact ? Lexical
BaseSyn Exact ? Syntactic
Disjunction Exact ? Lexical ? Syntactic
Majority Exact ? (Lexical ? Syntactic)
Note that since every facet that Exact Match
classifies as expressed is also expressed by Lexi-
cal Inference, BaseLex is essentially Lexical Infer-
ence on its own, and Majority is equivalent to the
majority rule on all three modules.
4 Empirical Evaluation
4.1 Dataset: Student Response Analysis
We evaluated our methods as part of RTE-8. The
challenge focuses on the domain of scholastic
quizzes, and attempts to emulate the meticulous
marking process that teachers do on a daily basis.
Given a question, a student?s response, and a refer-
ence answer, the task of student response analysis
is to determine whether the student answered cor-
rectly. This task can be approximated as a special
case of textual entailment; by assigning the stu-
dent?s answer as T and the reference answer as H ,
we are basically asking whether one can infer the
correct (reference) answer from the student?s re-
sponse.
Recall the example from Section 2. In this case,
H is a reference answer to the question:
Q: What is the main job of muscles?
T is essentially the student answer, though it is
also possible to define T as the union of both the
question and the student answer. In this work, we
chose to exclude the question.
There were two tracks in the challenge: com-
plete textual entailment (the main task) and partial
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
BaseLex .756 .710 .760
BaseSyn .744 .733 .770
Disjunction .695 .655 .703
Majority .782 .765 .816
Table 1: Micro-averaged F1 on the faceted Sci-
EntsBank test set.
entailment (the pilot task). Both tasks made use of
the SciEntsBank corpus (Dzikovska et al, 2012),
which is annotated at facet-level, and provides a
convenient test-bed for evaluation of both partial
and complete entailment. This dataset was split
into train and test subsets. The test set has 16,263
facet-response pairs based on 5,106 student re-
sponses over 15 domains (learning modules). Per-
formance was measured using micro-averaged F1,
over three different scenarios:
Unseen Answers Classify new answers to ques-
tions seen in training. Contains 464 student re-
sponses.
Unseen Questions Classify new answers to
questions that were not seen in training, but other
questions from the same domain were. Contains
631 student responses.
Unseen Domains Classify new answers to un-
seen questions from unseen domains. Contains
4,011 student responses.
4.2 Results
Table 1 shows the F1-measure of each configu-
ration in each scenario. There is some variance
between the different scenarios; this may be at-
tributed to the fact that there are much fewer Un-
seen Answers and Unseen Questions instances. In
all cases, Majority significantly outperformed the
other configurations. While BaseLex and BaseSyn
improve upon the baseline, they seem to make dif-
ferent mistakes, in particular false positives. Their
conjunction is thus a more conservative indicator
of entailment, and proves helpful in terms of F1.
All improvements over the baseline were found
to be statistically significant using McNemar?s test
with p < 0.01 (excluding Disjunction). It is also
interesting to note that the systems? performance
does not degrade in ?harder? scenarios; this is a re-
sult of the mostly unsupervised nature of our mod-
ules.
453
Unfortunately, our system was the only submis-
sion in the partial entailment pilot track of RTE-
8, so we have no comparisons with other sys-
tems. However, the absolute improvement from
the exact-match baseline to the more sophisticated
Majority is in the same ballpark as that of the best
systems in previous recognizing textual entailment
challenges. For instance, in the previous recogniz-
ing textual entailment challenge (Bentivogli et al,
2011), the best system yielded an F1 score of 0.48,
while the baseline scored 0.374. We can therefore
conclude that existing approaches for recognizing
textual entailment can indeed be adapted for rec-
ognizing partial entailment.
5 Utilizing Partial Entailment for
Recognizing Complete Entailment
Encouraged by our results, we ask whether the
same algorithms that performed well on the
faceted entailment task can be used for recogniz-
ing complete textual entailment. We performed an
initial experiment that examines this concept and
sheds some light on the potential role of partial en-
tailment as a possible facilitator for complete en-
tailment.
We suggest the following 3-stage architecture:
1. Decompose the hypothesis into facets.
2. Determine whether each facet is entailed.
3. Aggregate the individual facet results and de-
cide on complete entailment accordingly.
Facet Decomposition For this initial investiga-
tion, we use the facets provided in SciEntsBank;
i.e. we assume that the step of facet decomposition
has already been carried out. When the dataset
was created for RTE-8, many facets were extracted
automatically, but only a subset was selected. The
facet selection process was done manually, as part
of the dataset?s annotation. For example, in ?the
main job of muscles is to move bones?, the facet
(job, muscles) was not selected, because it was not
critical for answering the question. We refer to the
issue of relying on manual input further below.
Recognizing Faceted Entailment This step was
carried out as explained in the previous sections.
We used the Baseline configuration and Majority,
which performed best in our experiments above.
In addition, we introduce GoldBased that uses the
gold annotation of faceted entailment, and thus
Unseen Unseen Unseen
Answers Questions Domains
Baseline .575 .582 .683
Majority .707 .673 .764
GoldBased .842 .897 .852
BestComplete .773 .745 .712
Table 2: Micro-averaged F1 on the 2-way com-
plete entailment SciEntsBank test set.
provides a certain upper bound on the perfor-
mance of determining complete entailment based
on facets.
Aggregation We chose the simplest sensible ag-
gregation rule to decide on overall entailment: a
student answer is classified as correct (i.e. it en-
tails the reference answer) if it expresses each
of the reference answer?s facets. Although this
heuristic is logical from a strict entailment per-
spective, it might yield false negatives on this par-
ticular dataset. This happens because tutors may
sometimes grade answers as valid even if they
omit some less important, or indirectly implied,
facets.
Table 2 shows the experiment?s results. The
first thing to notice is that GoldBased is not per-
fect. There are two reasons for this behavior.
First, the task of student response analysis is only
an approximation of textual entailment, albeit a
good one. This discrepancy was also observed
by the RTE-8 challenge organizers (Dzikovska et
al., 2013). The second reason is because some of
the original facets were filtered when creating the
dataset. This caused both false positives (when
important facets were filtered out) and false neg-
atives (when unimportant facets were retained).
Our Majority mechanism, which requires that
the two underlying methods for partial entailment
detection (Lexical Inference and Syntactic Infer-
ence) agree on a positive classification, bridges
about half the gap from the baseline to the gold
based method. As a rough point of comparison,
we also show the performance of BestComplete,
the winning entry in each setting of the RTE-8
main task. This measure is not directly compara-
ble to our facet-based systems, because it did not
rely on manually selected facets, and due to some
variations in the dataset size (about 20% of the stu-
dent responses were not included in the pilot task
dataset). However, these results may indicate the
454
prospects of using faceted entailment for complete
entailment recognition, suggesting it as an attrac-
tive research direction.
6 Conclusion and Future Work
In this paper, we presented an empirical attempt
to tackle the problem of partial textual entail-
ment. We demonstrated that existing methods for
recognizing (complete) textual entailment can be
successfully adapted to this setting. Our experi-
ments showed that boolean combinations of these
methods yield good results. Future research may
add additional features and more complex fea-
ture combination methods, such as weighted sums
tuned by machine learning. Furthermore, our
work focused on a specific decomposition model
? faceted entailment. Other flavors of partial en-
tailment should be investigated as well. Finally,
we examined the possibility of utilizing partial en-
tailment for recognizing complete entailment in a
semi-automatic setting, which relied on the man-
ual facet annotation in the RTE-8 dataset. Our
preliminary results suggest that this approach is
indeed feasible, and warrant further research on
facet-based entailment methods that rely on fully-
automatic facet extraction.
Acknowledgements
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 287923 (EXCITEMENT).
We would like to thank the Minerva Foundation
for facilitating this cooperation with a short term
research grant.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A pilot on semantic textual similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation, in conjunction with the 1st Joint Con-
ference on Lexical and Computational Semantics,
pages 385?393, Montreal, Canada.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. Proceed-
ings of TAC.
Peter Clark and Phil Harrison. 2010. Blue-lite: a
knowledge-based lexical entailment system for rte6.
Proc. of TAC.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii.
Myroslava O Dzikovska, Rodney D Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210. Association for Computational Lin-
guistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang
Dang. 2013. Semeval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual
entailment challenge. In *SEM 2013: The First
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Rodney D Nielsen, Wayne Ward, and James H Mar-
tin. 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI 1995), pages 448?
453.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical en-
tailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 558?
563, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462.
Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proceedings of the ACL 2012 System
Demonstrations, pages 73?78, Jeju Island, Korea,
July. Association for Computational Linguistics.
455
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 37?42,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DKPro WSD ? A Generalized UIMA-based Framework
for Word Sense Disambiguation
Tristan Miller1 Nicolai Erbs1 Hans-Peter Zorn1 Torsten Zesch1,2 Iryna Gurevych1,2
(1) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de/
Abstract
Implementations of word sense disam-
biguation (WSD) algorithms tend to be
tied to a particular test corpus format and
sense inventory. This makes it difficult to
test their performance on new data sets, or
to compare them against past algorithms
implemented for different data sets. In this
paper we present DKPro WSD, a freely
licensed, general-purpose framework for
WSD which is both modular and exten-
sible. DKPro WSD abstracts the WSD
process in such a way that test corpora,
sense inventories, and algorithms can be
freely swapped. Its UIMA-based architec-
ture makes it easy to add support for new
resources and algorithms. Related tasks
such as word sense induction and entity
linking are also supported.
1 Introduction
Word sense disambiguation, or WSD (Agirre and
Edmonds, 2006)?the task of determining which
of a word?s senses is the one intended in a par-
ticular context?has been a core research problem
in computational linguistics since the very incep-
tion of the field. Despite the task?s importance
and popularity as a subject of study, tools and re-
sources supporting WSD have seen relatively little
generalization and standardization. That is, most
prior implementations of WSD systems have been
hard-coded for particular algorithms, sense inven-
tories, and data sets. This makes it difficult to com-
pare systems or to adapt them to new scenarios
without extensive reimplementation.
In this paper we present DKPro WSD, a
general-purpose framework for word sense disam-
biguation which is both modular and extensible.
Its modularity means that it makes a logical sep-
aration between the data sets (e.g., the corpora
to be annotated, the answer keys, manually anno-
tated training examples, etc.), the sense invento-
ries (i.e., the lexical-semantic resources enumerat-
ing the senses to which words in the corpora are
assigned), and the algorithms (i.e., code which ac-
tually performs the sense assignments and prereq-
uisite linguistic annotations), and provides a stan-
dard interface for each of these component types.
Components which provide the same functional-
ity can be freely swapped, so that one can easily
run the same algorithm on different data sets (irre-
spective of which sense inventory they use), or test
several different algorithms on the same data set.
While DKPro WSD ships with support for a
number of common WSD algorithms, sense inven-
tories, and data set formats, its extensibility means
that it is easy to adapt to work with new meth-
ods and resources. The system is written in Java
and is based on UIMA (Lally et al, 2009), an
industry-standard architecture for analysis of un-
structured information. Support for new corpus
formats, sense inventories, and WSD algorithms
can be added by implementing new UIMA com-
ponents for them, or more conveniently by writing
UIMA wrappers around existing code. The frame-
work and all existing components are released un-
der the Apache License 2.0, a permissive free soft-
ware licence.
DKPro WSD was designed primarily to support
the needs of WSD researchers, who will appre-
ciate the convenience and flexibility it affords in
tuning and comparing algorithms and data sets.
However, as a general-purpose toolkit it could also
be used to implement a WSD module for a real-
world natural language processing application. Its
support for interactive visualization of the disam-
biguation process also makes it a powerful tool for
learning or teaching the principles of WSD.
The remainder of this paper is organized as fol-
lows: In ?2 we review previous work in WSD file
formats and implementations. In ?3 we describe
37
our system and further explain its capabilities and
advantages. Finally, in ?4 we discuss our plans for
further development of the framework.
2 Background
In the early days of WSD research, electronic
dictionaries and sense-annotated corpora tended
to be small and hand-crafted on an ad-hoc ba-
sis. It was not until the growing availability of
large-scale lexical resources and corpora in the
1990s that the need to establish a common plat-
form for the evaluation of WSD systems was rec-
ognized. This led to the founding of the Sens-
eval (and later SemEval) series of competitions,
the first of which was held in 1998. Each com-
petition defined a number of tasks with prescribed
evaluation metrics, sense inventories, corpus file
formats, and human-annotated test sets. For each
task it was therefore possible to compare algo-
rithms against each other. However, sense inven-
tories and file formats still vary across tasks and
competitions. There are also a number of increas-
ingly popular resources used outside Senseval and
SemEval, each with their own formats and struc-
tures: examples of sense-annotated corpora in-
clude SemCor (Miller et al, 1994), MASC (Ide et
al., 2010), and WebCAGe (Henrich et al, 2012),
and sense inventories include VerbNet (Kipper et
al., 2008), FrameNet (Ruppenhofer et al, 2010),
DANTE (Kilgarriff, 2010), BabelNet (Navigli and
Ponzetto, 2012), and online community-produced
resources such as Wiktionary and Wikipedia. So
despite attempts at standardization, the canon of
WSD resources remains quite fragmented.
The few publically available implementa-
tions of individual disambiguation algorithms,
such as SenseLearner (Mihalcea and Csomai,
2005), SenseRelate::TargetWord (Patwardhan et
al., 2005), UKB (Agirre and Soroa, 2009), and
IMS (Zhong and Ng, 2010), are all tied to a partic-
ular corpus and/or sense inventory, or define their
own custom formats into which existing resources
must be converted. Furthermore, where the al-
gorithm depends on linguistic annotations such as
part-of-speech tags, the users are expected to sup-
ply these themselves, or else must use the anno-
tators built into the system (which may not always
be appropriate for the corpus language or domain).
One alternative to coding WSD algorithms from
scratch is to use general-purpose NLP toolkits
such as NLTK (Bird, 2006) or DKPro (Gurevych
et al, 2007). Such toolkits provide individual
components potentially useful for WSD, such as
WordNet-based measures of sense similarity and
readers for the odd corpus format. However, these
toolkits are not specifically geared towards devel-
opment and evaluation of WSD systems; there is
no unified type system or architecture which al-
lows WSD-specific components to be combined or
substituted orthogonally.
The only general-purpose dedicated WSD sys-
tem we are aware of is I Can Sense It (Joshi et al,
2012), a Web-based interface for running and eval-
uating various WSD algorithms. It includes I/O
support for several corpus formats and implemen-
tations of a number of baseline and state-of-the-
art disambiguation algorithms. However, as with
previous single-algorithm systems, it is not possi-
ble to select the sense inventory, and the user is
responsible for pre-annotating the input text with
POS tags. The usability and extensibility of the
system are greatly restricted by the fact that it is a
proprietary, closed-source application fully hosted
by the developers.
3 DKPro WSD
Our system, DKPro WSD, is implemented as a
framework of UIMA components (type systems,
collection readers, annotators, CAS consumers,
resources) which the user combines into a data
processing pipeline. We can best illustrate this
with an example: Figure 1 shows a pipeline for
running two disambiguation algorithms on the Es-
tonian all-words task from Senseval-2. UIMA
components are the solid, rounded boxes in the
lower half of the diagram, and the data and algo-
rithms they encapsulate are the light grey shapes
in the upper half. The first component of the
pipeline is a collection reader which reads the
text of the XML-formatted corpus into a CAS (a
UIMA data structure for storing layers of data
and stand-off annotations) and marks the words
to be disambiguated (the ?instances?) with their
IDs. The next component is an annotator which
reads the answer key?a separate file which as-
sociates each instance ID with a sense ID from
the Estonian EuroWordNet?and adds the gold-
standard sense annotations to their respective in-
stances in the CAS. Processing then passes to
another annotator?in this case a UIMA wrap-
per for TreeTagger (Schmid, 1994)?which adds
POS and lemma annotations to the instances.
38
corpus 
reader 
answer key 
annotator 
linguistic 
annotator 
WSD 
annotator 
WSD 
annotator 
simplified 
Lesk 
evaluator 
sense 
inventory 
Senseval-2 
Estonian 
all-words 
test corpus 
Senseval-2 
Estonian 
all-words 
answer key 
results and 
statistics     JMWNL 
Estonian 
Euro- 
WordNet 
degree 
centrality 
Tree- 
Tagger 
Estonian 
language 
model 
Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2.
Then come the two disambiguation algorithms,
also modelled as UIMA annotators wrapping non-
UIMA-aware algorithms. Each WSD annotator it-
erates over the instances in the CAS and annotates
them with sense IDs from EuroWordNet. (Euro-
WordNet itself is accessed via a UIMA resource
which wraps JMWNL (Pazienza et al, 2008) and
which is bound to the two WSD annotators.) Fi-
nally, control passes to a CAS consumer which
compares the WSD algorithms? sense annotations
against the gold-standard annotations produced by
the answer key annotator, and outputs these sense
annotations along with various evaluation metrics
(precision, recall, etc.).
A pipeline of this sort can be written with just
a few lines of code: one or two to declare each
component and if necessary bind it to the appro-
priate resources, and a final one to string the com-
ponents together into a pipeline. Moreover, once
such a pipeline is written it is simple to substitute
functionally equivalent components. For example,
with only a few small changes the same pipeline
could be used for Senseval-3?s English lexical
sample task, which uses a corpus and sense inven-
tory in a different format and language. Specif-
ically, we would substitute the collection reader
with one capable of reading the Senseval lexical
sample format, we would pass an English instead
of Estonian language model to TreeTagger, and
we would substitute the sense inventory resource
exposing the Estonian EuroWordNet with one for
WordNet 1.7.1. Crucially, none of the WSD algo-
rithms need to be changed.
The most important features of our system are
as follows:
Corpora and data sets. DKPro WSD currently
has collection readers for all Senseval and Sem-
Eval all-words and lexical sample tasks, the AIDA
CoNLL-YAGO data set (Hoffart et al, 2011), the
TAC KBP entity linking tasks (McNamee and
Dang, 2009), and the aforementioned MASC,
SemCor, and WebCAGe corpora. Our prepack-
aged corpus analysis modules can compute statis-
tics on monosemous terms, average polysemy,
terms absent from the sense inventory, etc.
Sense inventories. Sense inventories are ab-
stracted into a system of types and interfaces ac-
cording to the sort of lexical-semantic information
they provide. There is currently support for Word-
Net (Fellbaum, 1998), WordNet++ (Ponzetto and
Navigli, 2010), EuroWordNet (Vossen, 1998), the
Turk Bootstrap Word Sense Inventory (Biemann,
2013), and UBY (Gurevych et al, 2012), which
provides access to WordNet, Wikipedia, Wik-
tionary, GermaNet, VerbNet, FrameNet, Omega-
Wiki, and various alignments between them. The
system can automatically convert between vari-
ous versions of WordNet using the UPC mappings
(Daude? et al, 2003).
Algorithms. As with sense inventories, WSD
algorithms have a type and interface hierarchy ac-
cording to what knowledge sources they require.
Algorithms and baselines already implemented in-
clude the analytically calculated random sense
baseline; the most frequent sense baseline; the
original, simplified, extended, and lexically ex-
panded Lesk variants (Miller et al, 2012); various
39
graph connectivity approaches from Navigli and
Lapata (2010); Personalized PageRank (Agirre
and Soroa, 2009); the supervised TWSI system
(Biemann, 2013); and IMS (Zhong and Ng, 2010).
Our open API permits users to program support
for further knowledge-based and supervised algo-
rithms.
Linguistic annotators. Many WSD algorithms
require linguistic annotations from segmenters,
lemmatizers, POS taggers, parsers, etc. Off-the-
shelf UIMA components for producing such an-
notations, such as those provided by DKPro Core
(Gurevych et al, 2007), can be used in a DKPro
WSD pipeline with little or no adaptation.
Visualization tools. We have enhanced some
families of algorithms with animated, interactive
visualizations of the disambiguation process. For
example, Figure 2 shows part of a screenshot from
the interactive running of the degree centrality al-
gorithm (Navigli and Lapata, 2010). The system is
disambiguating the three content words in the sen-
tence ?I drink milk with a straw.? Red, green, and
blue nodes represent senses (or more specifically,
WordNet sense keys) of the words drink, milk,
and straw, respectively; grey nodes are senses of
other words discovered by traversing semantic re-
lations (represented by arcs) in the sense inven-
tory. The current traversal (toast%2:34:00:: to
fuddle%2:34:00::) is drawn in a lighter colour.
Mouseover tooltips provide more detailed infor-
mation on senses. We have found such visualiza-
tions to be invaluable for understanding and de-
bugging algorithms.
Parameter sweeping. The behaviour of many
components (or entire pipelines) can be altered ac-
cording to various parameters. For example, for
the degree centrality algorithm one must specify
the maximum search depth, the minimum vertex
degree, and the context size. DKPro WSD can
perform a parameter sweep, automatically running
the pipeline once for every possible combination
of parameters in user-specified ranges and con-
catenating the results into a table from which the
optimal system configurations can be identified.
Reporting tools. There are several reporting
tools to support evaluation and error analysis. Raw
sense assignments can be output in a variety of for-
mats (XML, HTML, CSV, Senseval answer key,
etc.), some of which support colour-coding to
Figure 2: DKPro WSD?s interactive visualization
of a graph connectivity WSD algorithm.
highlight correct and incorrect assignments. The
system can also compute common evaluation met-
rics (Agirre and Edmonds, 2006, pp. 76?80) and
plot precision?recall curves for each algorithm in
the pipeline, as well as produce confusion matri-
ces for algorithm pairs. Users can specify backoff
algorithms, and have the system compute results
with and without the backoff. Results can also be
broken down by part of speech. Figure 3 shows
an example of an HTML report produced by the
system?on the left is the sense assignment table,
in the upper right is a table of evaluation metrics,
and in the lower right is a precision?recall graph.
DKPro WSD also has support for tasks closely
related to word sense disambiguation:
Entity linking. Entity linking (EL) is the task of
linking a named entity in a text (e.g., Washington)
to its correct representation in some knowledge
base (e.g., either George Washington or Washing-
ton, D.C. depending on the context). EL is very
similar to WSD in that both tasks involve connect-
ing ambiguous words in a text to entries in some
inventory. DKPro WSD supports EL-specific
sense inventories such as the list of Wikipedia
articles used in the Knowledge Base Population
workshop of the Text Analysis Conference (TAC
KBP). This workshop, held annually since 2009,
provides a means for comparing different EL sys-
tems in a controlled setting. DKPro WSD contains
a reader for the TAC KBP data set, components
for mapping other sense inventories to the TAC
KBP inventory, and evaluation components for the
40
Figure 3: An HTML report produced by DKPro WSD.
official metrics. Researchers can therefore miti-
gate the entry barrier for their first participation at
TAC KBP and experienced participants can extend
their systems by making use of further WSD algo-
rithms.
Word sense induction. WSD is usually per-
formed with respect to manually created sense in-
ventories such as WordNet. In word sense induc-
tion (WSI) a sense inventory for target words is
automatically constructed from an unlabelled cor-
pus. This can be useful for search result cluster-
ing, or for general applications of WSD for lan-
guages and domains for which a sense inventory
is not yet available. It is usually necessary to per-
form WSD at some point in the evaluation of WSI.
DKPro WSD supports WSI by providing state-of-
the art WSD algorithms capable of using arbitrary
sense inventories, including induced ones. It also
includes readers and writers for the SemEval-2007
and -2013 WSI data sets.
4 Conclusions and future work
In this paper we introduced DKPro WSD, a Java-
and UIMA-based framework for word sense dis-
ambiguation. Its primary advantages over exist-
ing tools are its modularity, its extensibility, and
its free licensing. By segregating and providing
layers of abstraction for code, data sets, and sense
inventories, DKPro WSD greatly simplifies the
comparison of WSD algorithms in heterogeneous
scenarios. Support for a wide variety of commonly
used algorithms, data sets, and sense inventories
has already been implemented.
The framework is under active development,
with work on several new features planned or in
progress. These include implementations or wrap-
pers for further algorithms and for the DANTE
and BabelNet sense inventories. A Web inter-
face is in the works and should be operational
by the time of publication. Source code, bi-
naries, documentation, tutorials, FAQs, an issue
tracker, and community mailing lists are avail-
able on the project?s website at https://code.
google.com/p/dkpro-wsd/.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg Professor-
ship Program under grant No? I/82806.
41
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proc.
EACL, pages 33?41.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Lang. Resour. and Eval., 47(1):97?122.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proc. ACL-COLING (Interactive Presen-
tation Sessions), pages 69?72.
Jordi Daude?, Llu??s Padro?, and German Rigau. 2003.
Validation and tuning of WordNet mapping tech-
niques. In Proc. RANLP, pages 117?123.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Reposi-
tory Based on UIMA. In Proc. UIMA Workshop at
GLDV.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY ? A large-scale unified
lexical-semantic resource. In Proc. EACL, pages
580?590.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2012. WebCAGe ? A Web-harvested corpus
annotated with GermaNet senses. In Proc. EACL,
pages 387?396.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fu?rstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proc. EMNLP, pages 782?792.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The Manually Annotated
Sub-Corpus: A community resource for and by the
people. In Proc. ACL (Short Papers), pages 68?73.
Salil Joshi, Mitesh M. Khapra, and Pushpak Bhat-
tacharyya. 2012. I Can Sense It: A comprehensive
online system for WSD. In Proc. COLING (Demo
Papers), pages 247?254.
Adam Kilgarriff. 2010. A detailed, accurate, exten-
sive, available English lexical database. In Proc.
NAACL-HLT, pages 21?24.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. Lang. Resour. and Eval., 42(1):21?
40.
Adam Lally, Karin Verspoor, and Eric Nyberg, editors.
2009. Unstructured Information Management Ar-
chitecture (UIMA) Version 1.0. OASIS.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Proc. TAC.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words
in unrestricted text. In Proc. ACL (System Demos),
pages 53?56.
George A. Miller, Martin Chodorow, Shari Landes,
Claudio Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Proc. HLT, pages 240?243.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proc. COLING, pages
1781?1796.
Roberto Navigli and Mirella Lapata. 2010. An experi-
mental study of graph connectivity for unsupervised
word sense disambiguation. IEEE Trans. on Pattern
Anal. and Machine Intel., 32(4):678?692.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
An overview of BabelNet and its API for multilin-
gual language processing. In Iryna Gurevych and
Jungi Kim, editors, The People?s Web Meets NLP:
Collaboratively Constructed Language Resources.
Springer.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2005. SenseRelate::TargetWord ? A gen-
eralized framework for word sense disambiguation.
In Proc. ACL (System Demos), pages 73?76.
Maria Teresa Pazienza, Armando Stellato, and Alexan-
dra Tudorache. 2008. JMWNL: An extensible mul-
tilingual library for accessing wordnets in different
languages. In Proc. LREC, pages 28?30.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proc. ACL, pages 1522?
1531.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Helmud Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. NeMLaP.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Springer.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proc. ACL (System Demos), pages
78?83.
42
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 121?126,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
DKPro Similarity: An Open Source Framework for Text Similarity
Daniel Ba?r?, Torsten Zesch??, and Iryna Gurevych??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
We present DKPro Similarity, an open
source framework for text similarity. Our
goal is to provide a comprehensive repos-
itory of text similarity measures which
are implemented using standardized inter-
faces. DKPro Similarity comprises a wide
variety of measures ranging from ones
based on simple n-grams and common
subsequences to high-dimensional vector
comparisons and structural, stylistic, and
phonetic measures. In order to promote
the reproducibility of experimental results
and to provide reliable, permanent ex-
perimental conditions for future studies,
DKPro Similarity additionally comes with
a set of full-featured experimental setups
which can be run out-of-the-box and be
used for future systems to built upon.
1 Introduction
Computing text similarity is key to several natu-
ral language processing applications such as au-
tomatic essay grading, paraphrase recognition, or
plagiarism detection. However, only a few text
similarity measures proposed in the literature are
released publicly, and those then typically do not
comply with any standardization. We are currently
not aware of any designated text similarity frame-
work which goes beyond simple lexical similarity
or contains more than a small number of measures,
even though related frameworks exist, which we
discuss in Section 6. This fact was also realized
by the organizers of the pilot Semantic Textual
Similarity Task at SemEval-2012 (see Section 5),
as they argue for the creation of an open source
framework for text similarity (Agirre et al, 2012).
In order to fill this gap, we present DKPro Sim-
ilarity, an open source framework for text simi-
larity. DKPro Similarity is designed to comple-
ment DKPro Core1, a collection of software com-
ponents for natural language processing based on
the Apache UIMA framework (Ferrucci and Lally,
2004). Our goal is to provide a comprehensive
repository of text similarity measures which are
implemented in a common framework using stan-
dardized interfaces. Besides the already available
measures, DKPro Similarity is easily extensible
and intended to allow for custom implementations,
for which it offers various templates and exam-
ples. The Java implementation is publicly avail-
able at Google Code2 under the Apache Software
License v2 and partly under GNU GPL v3.
2 Architecture
DKPro Similarity is designed to operate in ei-
ther of two modes: The stand-alone mode al-
lows to use text similarity measures as indepen-
dent components in any experimental setup, but
does not offer means for further language process-
ing, e.g. lemmatization. The UIMA-coupled mode
tightly integrates similarity computation with full-
fledged Apache UIMA-based language processing
pipelines. That way, it allows to perform any num-
ber of languge processing steps, e.g. coreference
or named-entitiy resolution, along with the text
similarity computation.
Stand-alone Mode In this mode, text similarity
measures can be used independently of any lan-
guage processing pipeline just by passing them a
pair of texts as (i) two strings, or (ii) two lists of
strings (e.g. already lemmatized texts). We there-
fore provide an API module, which contains Java
interfaces and abstract base classes for the mea-
sures. That way, DKPro Similarity allows for a
maximum flexibility in experimental design, as the
text similarity measures can easily be integrated
with any existing experimental setup:
1code.google.com/p/dkpro-core-asl
2code.google.com/p/dkpro-similarity-asl
121
1 TextSimilarityMeasure m =
new GreedyStringTiling();
2 double similarity =
m.getSimilarity(text1, text2);
The above code snippet instantiates the Greedy
String Tiling measure (Wise, 1996) and then com-
putes the text similarity between the given pair of
texts. The resulting similarity score is normal-
ized into [0, 1] where 0 means not similar at all,
and 1 corresponds to perfectly similar.3 By us-
ing the common TextSimilarityMeasure
interface, it is easy to replace Greedy String Tiling
with any measure of choice, such as Latent Se-
mantic Analysis (Landauer et al, 1998) or Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2007). We give an overview of measures available
in DKPro Similarity in Section 3.
UIMA-coupled Mode In this mode, DKPro
Similarity allows text similarity computation to
be directly integrated with any UIMA-based lan-
guage processing pipeline. That way, it is easy to
use text similarity components in addition to other
UIMA-based components in the same pipeline.
For example, an experimental setup may require to
first compute text similarity scores and then to run
a classification algorithm on the resulting scores.
In Figure 1, we show a graphical overview of
the integration of text similarity measures (right)
with a UIMA-based pipeline (left). The pipeline
starts by reading a given dataset, then performs
any number of pre-processing steps such as to-
kenization, sentence splitting, lemmatization, or
stopword filtering, then runs the text similar-
ity computation, before executing any subsequent
post-processing steps and finally returning the pro-
cessed texts in a suitable format for evaluation or
manual inspection. As all text similarity measures
in DKPro Similarity conform to standardized in-
terfaces, they can be easily exchanged in the text
similarity computation step.
With DKPro Similarity, we offer various sub-
classes of the generic UIMA components which
are specifically tailored towards text similarity ex-
periments, e.g. corpus readers for standard eval-
uation datasets as well as evaluation components
for running typical evaluation metrics. By lever-
aging UIMA?s architecture, we also define an
3Some string distance measures such as the Levenshtein
distance (Levenshtein, 1966) return a raw distance score
where less distance corresponds to higher similarity. How-
ever, the score can easily be normalized, e.g. by text length.
UIMA-based Pipeline
Corpus Reader
Pre-processing
Text Similarity
Computation
Post-processing
Evaluation
Sim
ilar
ity
Sco
rer
Text Similarity Measures
Greedy String Tiling
Double Metaphone
...
Explicit Sem. Analysis
Figure 1: DKPro Similarity allows to integrate any
text similarity measure (right) which conforms to
standardized interfaces into a UIMA-based lan-
guage processing pipeline (left) by means of a
dedicated Similarity Scorer component (middle).
additional interface to text similarity measures:
The JCasTextSimilarityMeasure inherits
from TextSimilarityMeasure, and adds a
method for two JCas text representations:4
double getSimilarity
(JCas text1, JCas text2);
The additional interface allows to implement mea-
sures which have full access to UIMA?s document
structure. That way, it is possible to create text
similarity measures which can use any piece of in-
formation that has been annotated in the processed
documents, such as dependency trees or morpho-
logical information. We detail the new set of com-
ponents offered by DKPro Similarity in Section 4.
3 Text Similarity Measures
In this section, we give an overview of the text
similarity measures which are already available in
DKPro Similarity. While we provide new imple-
mentations for a multitude of measures, we rely on
specialized libraries such as the S-Space Package
(see Section 6) if available. Due to space limi-
tations and due to the fact that the framework is
actively under development, we do not provide an
exhaustive list here, but rather mention the most
interesting and most popular measures.
3.1 Simple String-based Measures
DKPro Similarity includes text similarity mea-
sures which operate on string sequences and
determine, for example, the longest common
4The JCas is an object-oriented Java interface to the
Common Analysis Structure (Ferrucci and Lally, 2004),
Apache UIMA?s internal document representation format.
122
(non-)contiguous sequence of characters. It also
contains Greedy String Tiling (Wise, 1996), a mea-
sure which allows to compare strings if parts have
been reordered. The framework also offers mea-
sures which compute sets of character and word
n-grams and compare them using different overlap
coefficients, e.g. the Jaccard index. It further in-
cludes popular string distance metrics such as the
Jaro-Winkler (Winkler, 1990), Monge and Elkan
(1997) and Levenshtein (1966) distance measures.
3.2 Semantic Similarity Measures
DKPro Similarity also contains several measures
which go beyond simple character sequences and
compute text similarity on a semantic level.
Pairwise Word Similarity These measures are
based on pairwise word similarity computations
which are then aggregated for the complete texts.
The measures typically operate on a graph-based
representation of words and the semantic relations
among them within a lexical-semantic resource.
DKPro Similarity therefore contains adapters for
WordNet, Wiktionary5, and Wikipedia, while the
framework can easily be extended to other data
sources that conform to a common interface
(Garoufi et al, 2008). Pairwise similarity mea-
sures in DKPro Similarity include Jiang and Con-
rath (1997) or Resnik (1995). The aggregation for
the complete texts can for example be done using
the strategy by Mihalcea et al (2006).
Vector Space Models These text similarity
measures project texts onto high-dimensional vec-
tors which are then compared. Cosine similar-
ity, a basic measure often used in information re-
trieval, weights words according to their term fre-
quencies or tf-idf scores, and computes the co-
sine between two text vectors. Latent Seman-
tic Analysis (Landauer et al, 1998) alleviates the
inherent sparseness of a high-dimensional term-
document matrix by reducing it to one of reduced
rank. Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) constructs the vector space on
corpora where the documents are assumed to de-
scribe natural concepts such as cat or dog. Orig-
inally, Wikipedia was proposed as the document
collection of choice.
DKPro Similarity goes beyond a single im-
plementation of these measures and comes with
highly customizable code which allows to set var-
5http://www.wiktionary.org
ious parameters for the construction of the vector
space and the comparison of the document vectors,
and further allows to construct the vector space for
arbitrary collections, e.g. domain-specific corpora.
3.3 Further Measures
Previous research (Ba?r et al, 2012b) has shown
promising results for the inclusion of measures
which go beyond textual content and compute
similarity along other text characteristics. Thus,
DKPro Similarity also includes measures for
structural, stylistic, and phonetic similarity.
Structural Similarity Structural similarity be-
tween texts can be computed, for example, by
comparing sets of stopword n-grams (Stamatatos,
2011). The idea here is that similar texts may pre-
serve syntactic similarity while exchanging only
content words. Other measures in DKPro Simi-
larity allow to compare texts by part-of-speech n-
grams, and order and distance features for pairs of
words (Hatzivassiloglou et al, 1999).
Stylistic Similarity DKPro Similarity includes,
for example, a measure which compares function
word frequencies (Dinu and Popescu, 2009) be-
tween two texts. The framework also includes a
set of measures which capture statistical properties
of texts such as the type-token ratio (TTR) and the
sequential TTR (McCarthy and Jarvis, 2010).
Phonetic Similarity DKPro Similarity also al-
lows to compute text similarity based on pair-
wise phonetic comparisons of words. It therefore
contains implementations of well-known phonetic
algorithms such as Double Metaphone (Philips,
2000) and Soundex (Knuth, 1973), which also con-
form to the common text similarity interface.
4 UIMA Components
In addition to a rich set of text similarity mea-
sures as partly described above, DKPro Similar-
ity includes components which allow to integrate
text similarity measures with any UIMA-based
pipeline, as outlined in Figure 1. In the following,
we introduce these components along with their
resources.
Readers & Datasets DKPro Similarity includes
corpus readers specifically tailored towards com-
bining the input texts in a number of ways, e.g.
all possible combinations, or each text paired with
n others by random. Standard datasets for which
123
readers come pre-packaged include, among oth-
ers, the SemEval-2012 STS data (Agirre et al,
2012), the METER corpus (Clough et al, 2002),
or the RTE 1?5 data (Dagan et al, 2006). As far
as license terms allow redistribution, the datasets
themselves are integrated into the framework.
Similarity Scorer The Similarity Scorer allows
to integrate any text similarity measure (which is
decoupled from UIMA by default) into a UIMA-
based pipeline. It builds upon the standardized text
similarity interfaces and thus allows to easily ex-
change the text similarity measure as well as to
specify the data types the measure should operate
on, e.g. tokens or lemmas.
Machine Learning Previous research (Agirre et
al., 2012) has shown that different text similarity
measures can be combined using machine learning
classifiers. Such a combination shows improve-
ments over single measures due to the fact that dif-
ferent measures capture different text characteris-
tics. DKPro Similarity thus provides adapters for
the Weka framework (Hall et al, 2009) and allows
to first pre-compute sets of text similarity scores
which can then be used as features for various ma-
chine learning classifiers.
Evaluation Metrics In the final step of a UIMA
pipeline, the processed data is read by a dedicated
evaluation component. DKPro Similarity ships
with a set of components which for example com-
pute Pearson or Spearman correlation with human
judgments, or apply task-specific metrics such as
average precision as used in the RTE challenges.
5 Experimental Setups
DKPro Similarity further encourages the creation
and publication of complete experimental setups.
That way, we promote the reproducibility of ex-
perimental results, and provide reliable, perma-
nent experimental conditions which can benefit fu-
ture studies and help to stimulate the reuse of par-
ticular experimental steps and software modules.
The experimental setups are instantiations of
the generic UIMA-based language processing
pipeline depicted in Figure 1 and are designed to
precisely match the particular task at hand. They
thus come pre-configured with corpus readers for
the relevant input data, with a set of pre- and post-
processing as well as evaluation components, and
with a set of text similarity measures which are
well-suited for the particular task. The experimen-
tal setups are self-contained systems and can be
run out-of-the-box without further configuration.6
DKPro Similarity contains two major types of
experimental setups: (i) those for an intrinsic eval-
uation allow to evaluate the system performance in
an isolated setting by comparing the system results
with a human gold standard, and (ii) those for an
extrinsic evaluation allow to evaluate the system
with respect to a particular task at hand, where text
similarity is a means for solving a concrete prob-
lem, e.g. recognizing textual entailment.
Intrinsic Evaluation DKPro Similarity con-
tains the setup (Ba?r et al, 2012a) which partic-
ipated in the Semantic Textual Similarity (STS)
Task at SemEval-2012 (Agirre et al, 2012) and
which has become one of the recommended base-
line systems for the second task of this series.7
The system combines a multitude of text similar-
ity measures of varying complexity using a simple
log-linear regression model. The provided setup
allows to evaluate how well the system output re-
sembles human similarity judgments on short texts
which are taken from five different sources, e.g.
paraphrases of news texts or video descriptions.
Extrinsic Evaluation Our framework includes
two setups for an extrinsic evaluation: detecting
text reuse, and recognizing textual entailment.
For detecting text reuse (Clough et al, 2002),
the setup we provide (Ba?r et al, 2012b) combines
a multitude of text similarity measures along dif-
ferent text characteristics. Thereby, it not only
combines simple string-based and semantic sim-
ilarity measures (see Sections 3.1 and 3.2), but
makes extensive use of measures along structural
and stylistic text characteristics (see Section 3.3).
Across three standard evaluation datasets, the sys-
tem consistently outperforms all previous work.
For recognizing textual entailment, we provide
a setup which is similar in configuration to the one
described above, but contains corpus readers and
evaluation components precisely tailored towards
the RTE challenge series (Dagan et al, 2006). We
believe that our setup can be used for filtering
those text pairs which need further analysis by a
dedicated textual entailment system.
6A one-time setup of local lexical-semantic resources
such as WordNet may be necessary, though.
7In 2013, the STS Task is a shared task of the Second
Joint Conference on Lexical and Computational Semantics,
http://ixa2.si.ehu.es/sts
124
6 Related Frameworks
To the best of our knowledge, only a few general-
ized similarity frameworks exist at all. In the fol-
lowing, we discuss them and give insights where
DKPro Similarity uses implementations of these
existing libraries. That way, DKPro Similarity
brings together the scattered efforts by offering ac-
cess to all measures through common interfaces. It
goes far beyond the functionality of the original li-
braries as it generalizes the resources used, allows
a tight integration with any UIMA-based pipeline,
and comes with full-featured experimental setups
which are pre-configured stand-alone text similar-
ity systems that can be run out-of-the-box.
S-Space Package Even though no designated
text similarity library, the S-Space Package (Jur-
gens and Stevens, 2010)8 contains some text sim-
ilarity measures such as Latent Semantic Analysis
(LSA) and Explicit Semantic Analysis (see Sec-
tion 3.2). However, it is primarily focused on
word space models which operate on word distri-
butions in text. Besides such algorithms, it offers
a variety of interfaces, data structures, evaluation
datasets and metrics, and global operation utili-
ties e.g. for dimension reduction using Singular
Value Decomposition or randomized projections,
which are particularly useful with such distribu-
tional word space models. DKPro Similarity inte-
grates LSA based on the S-Space Package.
Semantic Vectors The Semantic Vectors pack-
age is a package for distributional semantics (Wid-
dows and Cohen, 2010)9 that contains measures
such as LSA and allows for comparing documents
within a given vector space. The main focus lies
on word space models with a number of dimension
reduction techniques, and applications on word
spaces such as automatic thesaurus generation.
WordNet::Similarity The open source package
by Pedersen et al (2004)10 is a popular Perl li-
brary for the similarity computation on WordNet.
It comprises six word similarity measures that op-
erate on WordNet, e.g. Jiang and Conrath (1997)
or Resnik (1995). Unfortunately, no strategies
have been added to the package yet which aggre-
gate the word similarity scores for complete texts
in a similar manner as described in Section 3.2.
8code.google.com/p/airhead-research
9code.google.com/p/semanticvectors
10sourceforge.net/projects/wn-similarity
In DKPro Similarity, we offer native Java imple-
mentations of all measures contained in Word-
Net::Similarity, and allow to go beyond WordNet
and use the measures with any lexical-semantic re-
source of choice, e.g. Wiktionary or Wikipedia.
SimMetrics Library The Java library by Chap-
man et al (2005)11 exclusively comprises text sim-
ilarity measures which compute lexical similar-
ity on string sequences and compare texts with-
out any semantic processing. It contains mea-
sures such as the Levenshtein (1966) or Monge and
Elkan (1997) distance metrics. In DKPro Similar-
ity, some string-based measures (see Section 3.1)
are based on implementations from this library.
SecondString Toolkit The freely available li-
brary by Cohen et al (2003)12 is similar to Sim-
Metrics, and also implemented in Java. It also con-
tains several well-known text similarity measures
on string sequences, and includes many of the
measures which are also part of the SimMetrics
Library. Some string-based measures in DKPro
Similarity are based on the SecondString Toolkit.
7 Conclusions
We presented DKPro Similarity, an open source
framework designed to streamline the develop-
ment of text similarity measures. All measures
conform to standardized interfaces and can either
be used as stand-alone components in any ex-
perimental setup (e.g. an already existing system
which is not based on Apache UIMA), or can be
tightly coupled with a full-featured UIMA-based
language processing pipeline in order to allow for
advanced processing capabilities.
We would like to encourage other researchers
to participate in our efforts and invite them to ex-
plore our existing experimental setups as outlined
in Section 5, run modified versions of our setups,
and contribute own text similarity measures to
the framework. For that, DKPro Similarity also
comes with an example module for getting started,
which guides first-time users through both the
stand-alone and the UIMA-coupled modes.
Acknowledgements This work has been supported by the
Volkswagen Foundation as part of the Lichtenberg Profes-
sorship Program under grant No. I/82806, and by the Klaus
Tschira Foundation under project No. 00.133.2008. We thank
Richard Eckart de Castilho and all other contributors.
11sourceforge.net/projects/simmetrics
12sourceforge.net/projects/secondstring
125
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proc. of the
6th Int?l Works. on Semantic Eval., pages 385?393.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012a. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proc. of the 6th Int?l Work-
shop on Semantic Evaluation, pages 435?440.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2012b.
Text Reuse Detection Using a Composition of Text
Similarity Measures. In Proc. of the 24th Int?l Conf.
on Computational Linguistics, pages 167?184.
Sam Chapman, Barry Norton, and Fabio Ciravegna.
2005. Armadillo: Integrating Knowledge for the Se-
mantic Web. In Proceedings of the Dagstuhl Semi-
nar in Machine Learning for the Semantic Web.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of ACL, pages 152?159.
William W. Cohen, Pradeep Ravikumar, and Stephen
Fienberg. 2003. A Comparison of String Metrics
for Matching Names and Records. In Proc. of KDD
Works. on Data Cleaning and Object Consolidation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Machine Learning Challenges,
Lecture Notes in Computer Science, pages 177?190.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62?66.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of IJCAI, pages 1606?1611, Hyderabad, India.
Konstantina Garoufi, Torsten Zesch, and Iryna
Gurevych. 2008. Representational Interoperability
of Linguistic and Collaborative Knowledge Bases.
In Proceedings of the KONVENS Workshop on
Lexical-Semantic and Ontological Resources.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature com-
binations via machine learning. In Proceedings of
EMNLP/VLC, pages 203?212.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of ROCLING, pages 19?33.
David Jurgens and Keith Stevens. 2010. The S-Space
Package: An Open Source Package for Word Space
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden.
Donald E. Knuth. 1973. The Art of Computer
Programming: Volume 3, Sorting and Searching.
Addison-Wesley.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An Introduction to Latent Semantic
Analysis. Discourse Processes, 25(2):259?284.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophis-
ticated approaches to lexical diversity assessment.
Behavior research methods, 42(2):381?392.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In Proceed-
ings of AAAI-06, pages 775?780, Boston, MA, USA.
Alvaro Monge and Charles Elkan. 1997. An ef-
ficient domain-independent algorithm for detecting
approximately duplicate database records. In Pro-
ceedings of the SIGMOD Workshop on Data Mining
and Knowledge Discovery, pages 23?29.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of the
HLT-NAACL: Demonstration Papers, pages 38?41.
Lawrence Philips. 2000. The double metaphone
search algorithm. C/C++ Users Jour., 18(6):38?43.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In
Proceedings of the IJCAI, pages 448?453.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512?2527.
Dominic Widdows and Trevor Cohen. 2010. The Se-
mantic Vectors Package: New Algorithms and Pub-
lic Tools for Distributional Semantics. In Proceed-
ings of IEEE-ICSC, pages 9?15.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the
Survey Research Methods Section, pages 354?359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proc. of the 27th SIGCSE Technical Symposium on
Computer Science Education, pages 130?134.
126
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 31?36,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
DKPro Keyphrases: Flexible and Reusable Keyphrase Extraction
Experiments
Nicolai Erbs
?
, Pedro Bispo Santos
?
, Iryna Gurevych
??
and Torsten Zesch
??
? UKP Lab, Technische Universit?at Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
DKPro Keyphrases is a keyphrase extrac-
tion framework based on UIMA. It offers
a wide range of state-of-the-art keyphrase
experiments approaches. At the same
time, it is a workbench for developing new
extraction approaches and evaluating their
impact. DKPro Keyphrases is publicly
available under an open-source license.
1
1 Introduction
Keyphrases are single words or phrases that pro-
vide a summary of a text (Tucker and Whittaker,
2009) and thus might improve searching (Song et
al., 2006) in a large collection of texts. As man-
ual extraction of keyphrases is a tedious task, a
wide variety of keyphrase extraction approaches
has been proposed. Only few of them are freely
available which makes it hard for researchers to
replicate previous results or use keyphrase extrac-
tion in some other application, such as informa-
tion retrieval (Manning et al., 2008), or question
answering (Kwok et al., 2001).
In this paper, we describe our keyphrase extrac-
tion framework called DKPro Keyphrases. It inte-
grates a wide range of state-of-the-art approaches
for keyphrase extraction that can be directly used
with limited knowledge of programming. How-
ever, for developers of new keyphrase extrac-
tion approaches, DKPro Keyphrases also offers a
programming framework for developing new ex-
traction algorithms and for evaluation of result-
ing effects. DKPro Keyphrases is based on the
Unstructured Information Management Architec-
ture (Ferrucci and Lally, 2004), which provides a
rich source of libraries with preprocessing compo-
nents.
1
http://code.google.com/p/dkpro-keyphrases/
Text
Preprocessing
Select keyphrases
Filter keyphrases
Rank keyphrases
Evaluate
Results
Figure 1: Architecture overview of DKPro
Keyphrases
2 Architecture
The architecture of DKPro Keyphrases models the
five fundamental steps of keyphrase extraction:
(i) Reading of input data and enriching it with
standard linguistic preprocessing, (ii) selecting
phrases as keyphrase candidates based on the pre-
processed text, (iii) filtering selected keyphrases,
(iv) ranking remaining keyphrases, and (v) evalu-
ating ranked keyphrases against a gold standard.
This process is visualized in Figure 1. In this
section, we will describe details of each step, in-
cluding components already included in DKPro
Keyphrases.
2.1 Preprocessing
DKPro Keyphrases relies on UIMA-based pre-
processing components developed in the natu-
ral language processing framework DKPro Core
(Gurevych et al., 2007; Eckart de Castilho and
Gurevych, 2009). Thus, a wide range of linguis-
tic preprocessing components are readily available
such as word segmentation, lemmatization, part-
of-speech tagging, named entity recognition, syn-
31
tactic parsing, or co-reference resolution.
2.2 Selecting Keyphrases
In this step, DKPro Keyphrases selects all phrases
as keyphrases that match user-specified criteria. A
criterium is typically a linguistic type, e.g. tokens,
or more sophisticated types such as noun phrases.
The resulting list of keyphrases should cover all
gold keyphrases and at the same time be as selec-
tive as possible. We use the following sentence
with the two gold keyphrases ?dog? and ?old cat?
as a step through example:
A [dog] chases an [old cat] in my gar-
den.
Taking all uni- and bi-grams as keyphrases will
easily match both gold keyphrases, but it will also
result in many other less useful keyphrases like ?in
my?.
In the given example, the keyphrase list consists
of nine tokens (lemmas, resp.) but covers only one
gold keyphrase (i.e. ?dog?). Noun chunks and
named entities are alternative keyphrases, limiting
the set of keyphrases further. Experiments where
noun chunks are selected as keyphrases perform
best for this example. Named entities are too re-
strictive, but applicable for identifying relevant en-
tities in a text. This is useful for tasks that are
targeted towards entities, e.g. for finding experts
(D?orner et al., 2007) in a collection of domain-
dependent texts. The selection of a linguistic type
is not limited as preprocessing components might
introduce further types.
2.3 Filtering
Filtering can be used together with over-
generating selection approaches like taking all n-
grams to decrease the number of keyphrases be-
fore ranking. One possible approach is based
on POS patterns. For example, using the POS
patterns, Adjective-Noun, Adjective, and
Noun limits the set of possible keyphrases to
?dog?, ?old cat?, ?cat?, and ?garden? in the pre-
vious example. This step can also been per-
formed as part of the selection step, however,
keeping it separated enables researchers to ap-
ply filters to keyphrases of any linguistic type.
DKPro Keyphrases provides the possibility to use
controlled-vocabulary keyphrase extraction by fil-
tering out all keyphrases which are not included in
a keyphrase list.
Developers of keyphrase extraction approaches
can create their own filter simply by extending
from a base class and adding filter-specific code.
Additionally, DKPro Keyphrases does not impose
workflow-specific requirements, such as a fixed
number of filters. This leaves room for keyphrase
extraction experiments testing new or extended fil-
ters.
2.4 Ranking
In this step, a ranker assigns a score to each re-
maining keyphrase candidate. DKPro Keyphrases
contains rankers based on the candidate position,
frequency, tf-idf, TextRank (Mihalcea and Tarau,
2004), and LexRank (Erkan and Radev, 2004).
DKPro Keyphrases also contains a special ex-
tension of tf-idf, called tf-idf
web
, for which Google
web1t (Brants and Franz, 2006) is used for obtain-
ing approximate df counts. In case of keyphrase
extraction for a single document or for domain-
independent keyphrase extraction, web1t provides
reliable n-gram statistics without any domain-
dependence.
2.5 Evaluation
DKPro Keyphrases ships with all the metrics
that have been traditionally used for evaluating
keyphrase extraction. Kim et al. (2010) use
precision and recall for a different number of
keyphrases (5, 10 and 15 keyphrases). These met-
rics are widely used for evaluation in information
retrieval. Precision @5 is the ratio of true pos-
itives in the set of extracted keyphrases when 5
keyphrases are extracted. Recall @5 is the ratio of
true positives in the set of gold keyphrases when
5 keyphrases are extracted. Moreover, DKPro
Keyphrases evaluates with MAP and R-precision.
MAP is the mean average precision of extracted
keyphrases from the highest scored keyphrase to
the total number of extracted keyphrases. For each
position in the rank, the precision at that position
will be computed. Summing up the precision at
each recall point and then taking its average will
return the average precision for the text being eval-
uated. The mean average precision will be the
mean from the sum of each text?s average preci-
sion from the dataset. R-precision is the ratio of
true positives in the set of extracted keyphrases,
when the set is limited to the same size as the set
of gold keyphrases (Zesch and Gurevych, 2009).
32
3 Experimental framework
In this section, we show how researchers can per-
form experiments covering many different config-
urations for preprocessing, selection, and ranking.
To facilitate the construction of experiments, the
framework contains a module to make its archi-
tecture compatible to the DKPro Lab framework
(Eckart de Castilho and Gurevych, 2011), thus al-
lowing to sweep through the parameter space of
configurations. The parameter space is the combi-
nation of all possible parameters, e.g. one parame-
ter with two possible values for preprocessing and
a second parameter with two values for rankers
lead to four possible combinations. We refer to pa-
rameter sweeping experiments when running the
experiment with all possible combinations.
DKPro Keyphrases divides the experimental
setup in three tasks. Tasks are processing steps
defined in the Lab framework, which ? in case of
keyphrase extraction ? are based on the steps de-
scribed in Section 2. In the first task, the input
text is fed into a pipeline and preprocessed. In the
second task, the keyphrases are selected and fil-
tered. In the third and final task they are ranked
and evaluated. The output of the first two tasks are
serialized objects which can be processed further
by the following task. The output of the third task
is a report containing all configurations and results
in terms of all evaluation metrics.
The division into three tasks speeds up process-
ing of the entire experiment. Each task has mul-
tiple configuration parameters which influence the
forthcoming tasks. Instead of running the prepro-
cessing tasks for every single possible combina-
tion, the intermediate objects are stored once and
then used for every possible configuration in the
keyphrase selection step.
To illustrate the advantages of experimental set-
tings in DKPro Keyphrases, we run the previously
used example sentence through the entire parame-
ter space. Hence, tokens, lemmas, n-grams, noun
chunks, and named entities will be combined with
all filters and all rankers (not yet considering all
possible parameters). This results in more than
10,000 configurations. Although the number of
configurations is high, the computation time is
low
2
as not the entire pipeline needs to run that
often. This scales well for longer texts.
The experimental framework runs all possible
2
Less than five minutes on a desktop computer with a 3.4
GHz 8-core processor.
combinations automatically and collects individ-
ual results in a report, such as a spreadsheet or
text file. This allows for comparing results of dif-
ferent rankers, mitigating the influence of differ-
ent preprocessing and filtering components. This
way, the optimal experimental configuration can
be found empirically. It is a great improvement
for researchers because a variety of system con-
figurations can be compared without the effort of
reimplementing the entire pipeline.
Code example 1 shows the main method of an
example experiment, selecting all tokens as pos-
sible keyphrases and ranking them with their tf-
idf values. Lines 1 to 34 show values for dimen-
sions which span the parameter space. A dimen-
sion consists of an identifier, followed by one or
more values. Lines 36 to 40 show the creation of
tasks, and in lines 42 to 48 the tasks and a re-
port are added to one batch task, which is then
executed. Researchers can run multiple configu-
rations by setting multiple values to a dimension.
Line 25 shows an example of a dimension with
two values (using the logarithm or unchanged text
frequency), in this case two configurations
3
for the
ranker based on tf-idf scores.
Code example 1: Example experiment
1 ParameterSpace params = new
ParameterSpace(
2 Dimension.create("language", "en"),
3 Dimension.create("frequencies",
"web1t"),
4 Dimension.create("tfidfFeaturePath",
Token.class"),
5
6 Dimension.create("dataset",
datasetPath),
7 Dimension.create("goldSuffix", ".key"),
8
9 //Selection
10 Dimension.create("segmenter",
OpenNlpSegmenter.class),
11 Dimension.create("keyphraseFeaturePath",
Token.class),
12
13 //PosSequence filter
14 Dimension.create("runPosSequenceFilter",
true),
15 Dimension.create("posSequence",
standard),
16
17 //Stopword filter
18 Dimension.create("runStopwordFilter",
true),
19 Dimension.create("stopwordlists",
"stopwords.txt"),
20
21 // Ranking
3
DKPro Keyphrases provides ways to configure experi-
ments using Groovy and JSON.
33
22 Dimension.create("rankerClass",
TfidfRanking.class),
23
24 //TfIdf
25 Dimension.create("weightingModeTf",
NORMAL, LOG),
26 Dimension.create("weightingModeIdf",
LOG),
27 Dimension.create("tfidfAggregate",
MAX),
28
29 //Evaluator
30 Dimension.create("evalMatchingType",
MatchingType.Exact),
31 Dimension.create("evalN", 50),
32 Dimension.create("evalLowercase",
true),
33 Dimension.create("evalType",
EvaluatorType.Lemma),
34 );
35
36 Task preprocessingTask = new
PreprocessingTask();
37 Task filteringTask = new
KeyphraseFilteringTask();
38 candidateSelectionTask.addImport(
preprocessingTask,
PreprocessingTask.OUTPUT,
KeyphraseFilteringTask.INPUT);
39 Task keyphraseRankingTask = new
KeyphraseRankingTask();
40 keyphraseRankingTask.addImport(
filteringTask,
KeyphraseFilteringTask.OUTPUT,
KeyphraseRankingTask.INPUT);
41
42 BatchTask batch = new BatchTask();
43 batch.setParameterSpace(params);
44 batch.addTask(preprocessingTask);
45 batch.addTask(candidateSelectionTask);
46 batch.addTask(keyphraseRankingTask);
47 batch.addReport(
KeyphraseExtractionReport.class);
48 Lab.getInstance().run(batch);
A use case for the experimental framework is
the evaluation of new preprocessing components.
For example, keyphrase extraction should be eval-
uated with Twitter data: One collects a dataset
with tweets and their corresponding keyphrases
(possibly, the hash tags). The standard preprocess-
ing will most likely fail as non-canonical language
will be hard to process (e.g. hash tags or emoti-
cons).
The preprocessing components can be set as a
parameter and compared directly without chang-
ing the remaining parameters for filters and
rankers. This allows researchers to perform reli-
able extrinsic evaluation of their components in a
keyphrase extraction setting.
Figure 2: Screenshot of web demo in DKPro
Keyphrases
4 Visualization and wrappers
To foster analysis of keyphrase extraction ex-
periments, we created a web-based visualization
framework with Spring
4
. It allows for running off-
the-shelf experiments and manually inspecting re-
sults without the need to install any additional soft-
ware. Figure 2 shows a visualization of one pre-
configured experiment. The web demo is avail-
able online.
5
Currently, a table overview of ex-
tracted keyphrases is implemented, but develop-
ers can change it to highlighting all keyphrases.
The latter is recommend for a binary classification
of keyphrases. This is the case, if a system only
returns keyphrases with a score above a certain
threshold. The table in Figure 2 shows keyphrases
with the assigned scores, which can be sorted to
get a ranking of keyphrases. However, the visual-
ization framework does not provide any evaluation
capabilities.
To help new users of DKPro Keyphrases, it in-
cludes a module with two demo experiments us-
ing preconfigured parameter sets. This is espe-
cially useful for applying keyphrase extraction in
other tasks, e.g. text summarization (Goldstein et
4
http://projects.spring.io/spring-ws/
5
https://dkpro.ukp.informatik.tu-
darmstadt.de/DKProWebDemo/livedemo/3
34
al., 2000). Both demo experiments are frequently
used keyphrase extraction systems. The first one
is based on TextRank (Mihalcea and Tarau, 2004)
and the second one is based on the supervised sys-
tem KEA (Witten et al., 1999). Both configura-
tions do not require any additional installation of
software packages.
This module offers setters to configure param-
eters, e.g. the size of co-occurrence windows in
case of the TextRank extractor.
5 Related work
Most work on keyphrase extraction is not accom-
panied with free and open software. The tools
listed in this section allow users to combine differ-
ent configurations with respect to preprocessing,
keyphrase selection, filtering, and ranking. In the
following, we give an overview of software tools
for keyphrase extraction.
KEA (Witten et al., 1999) provides a Java API,
which offers automatic keyphrase extraction from
texts. They provide a supervised approach for
keyphrase extraction. For each keyphrase, KEA
computes frequency, position, and semantic relat-
edness as features. Thus, for using KEA, the user
needs to provide annotated training data. KEA
generates keyphrases from n-grams with length
from 1 to 3 tokens. A controlled vocabulary can
be used to filter keyphrases. The configuration for
keyphrase selection and filtering is limited com-
pared to DKPro Keyphrases, which offers capa-
bilities for changing the entire preprocessing or
adding filters.
Maui (Medelyan et al., 2009) enhances KEA
by allowing the computation of semantic related-
ness of keyphrases. It uses Wikipedia as a the-
saurus and computes the keyphraseness of each
keyphrase, which is the number of times a can-
didate was used as keyphrase in the training data
(Medelyan et al., 2009).
Although Maui provides training data along
with their software, this training data is highly
domain-specific. A shortcoming of KEA and
Maui is the lack of any evaluation capabilities or
the possibility to run parameter sweeping exper-
iments. DKPro Keyphrases provides evaluation
tools for automatic testing of many parameter set-
tings.
Besides KEA and Mau, which are Java sys-
tems, there are several modules in Python,
e.g. topia.termextract
6
, which offer capabili-
ties for tokenization, part-of-speech tagging and
keyphrase extraction. Keyphrase extraction from
topia.termextract is based on noun phrases and
ranks them according to their frequencies.
BibClassify
7
is a python module which auto-
matically extracts keywords from a text based on
the occurrence of terms in a thesaurus. The ranker
is frequency-based like topia.termextract. Bib-
Classify and topia.termextract do not provide eval-
uation capabilities or parameter sweeping experi-
ments.
Besides these software tools, there exist web
services for keyphrase extraction. AlchemyAPI
8
offers a web service for keyword extraction. It
may return keyphrases encoded in various markup
languages. TerMine
9
offers a SOAP service for
extracting keyphrases from documents and a web
demo. The input must be a String and the extracted
terms will be returned as a String. Although web
services can be integrated easily due to their proto-
col stacks, they are not extensible and replicability
cannot be guaranteed over time.
6 Conclusions and future work
We presented DKPro Keyphrases, a framework for
flexible and reusable keyphrase extraction experi-
ments. This helps researchers to effectively de-
velop new keyphrase extraction components with-
out the need to re-implement state-of-the-art ap-
proaches.
The UIMA-based architecture of DKPro
Keyphrases allows users to easily evaluate
keyphrase extraction configurations. Researchers
can integrate keyphrase extraction with different
existing linguistic preprocessing components of-
fered by the open-source community and evaluate
them in terms of all commonly used evaluation
metrics.
As future work, we plan to wrap further
third-party libraries with keyphrase extraction ap-
proaches in DKPro Keyphrases and to add a super-
vised system using the unsupervised components
as features. We expect that a supervised system us-
ing a large variety of features would improve the
state of the art in keyphrase extraction.
6
https://pypi.python.org/pypi/topia.termextract/
7
http://invenio-demo.cern.ch/help/admin/bibclassify-
admin-guide
8
http://www.alchemyapi.com/api/keyword-extraction/
9
http://www.nactem.ac.uk/software/termine/
35
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
by the Klaus Tschira Foundation under project No.
00.133.2008, and by the German Federal Min-
istry of Education and Research (BMBF) within
the context of the Software Campus project open
window under grant No. 01IS12054. The authors
assume responsibility for the content. We thank
Richard Eckart de Castilho and all contributors for
their valuable collaboration and the we thank the
anonymous reviewers for their helpful comments.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
Gram Corpus Version 1.1. Technical report, Google
Research.
Christian D?orner, Volkmar Pipek, and Markus Won.
2007. Supporting Expertise Awareness: Finding
Out What Others Know. In Proceedings of the 2007
Symposium on Computer Human Interaction for the
Management of Information Technology.
Richard Eckart de Castilho and Iryna Gurevych. 2009.
DKPro-UGD: A Flexible Data-Cleansing Approach
to Processing User-Generated Discourse. In Online-
proceedings of the First French-speaking meeting
around the framework Apache UIMA.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A Lightweight Framework for Reproducible Param-
eter Sweeping in Information Retrieval. In Pro-
ceedings of the 2011 Workshop on Data Infrastruc-
tures for Supporting Information Retrieval Evalua-
tion, pages 7?10.
G?unes Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research, 22:457?479.
David Ferrucci and Adam Lally. 2004. UIMA: An
Architectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-Document Summa-
rization By Sentence Extraction. In Proceedings of
the NAACL-ANLP 2000 Workshop: Automatic Sum-
marization, pages 40?48.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
Based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technol-
ogy.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 21?26.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling Question Answering to the Web. ACM
Transactions on Information Systems, 19(3):242?
262.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. An Introduction to Infor-
mation Retrieval. Cambridge University Press Cam-
bridge.
Olena Medelyan, Eibe Frank, and Ian H Witten.
2009. Human-competitive Tagging using Automatic
Keyphrase Extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404?411.
Min Song, Il Yeol Song, Robert B. Allen, and Zo-
ran Obradovic. 2006. Keyphrase Extraction-based
Query Expansion in Digital Libraries. In Proceed-
ings of the 6th ACM/IEEE-CS Joint Conference on
Digital Libraries, pages 202?209.
Simon Tucker and Steve Whittaker. 2009. Have A Say
Over What You See: Evaluating Interactive Com-
pression Techniques. In Proceedings of the 2009
International Conference on Intelligent User Inter-
faces, pages 37?46.
Ian H. Witten, Gordon W. Paynter, Eibe Frank,
Carl Andrew Gutwin, and Craig G . Nevill-
Manning. 1999. KEA: Practical Automatic
Keyphrase Extraction. In Proceedings of the 4th
ACM Conference on Digital Libraries, pages 254?
255.
Torsten Zesch and Iryna Gurevych. 2009. Approx-
imate Matching for Evaluating Keyphrase Extrac-
tion. In Proceedings of the 7th International Confer-
ence on Recent Advances in Natural Language Pro-
cessing, pages 484?489.
36
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 61?66,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
DKPro TC: A Java-based Framework for Supervised Learning
Experiments on Textual Data
Johannes Daxenberger
?
, Oliver Ferschke
??
, Iryna Gurevych
??
and Torsten Zesch
??
? UKP Lab, Technische Universit?t Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
We present DKPro TC, a framework for
supervised learning experiments on tex-
tual data. The main goal of DKPro TC is
to enable researchers to focus on the actual
research task behind the learning problem
and let the framework handle the rest. It
enables rapid prototyping of experiments
by relying on an easy-to-use workflow en-
gine and standardized document prepro-
cessing based on the Apache Unstruc-
tured Information Management Architec-
ture (Ferrucci and Lally, 2004). It ships
with standard feature extraction modules,
while at the same time allowing the user
to add customized extractors. The exten-
sive reporting and logging facilities make
DKPro TC experiments fully replicable.
1 Introduction
Supervised learning on textual data is a ubiquitous
challenge in Natural Language Processing (NLP).
Applying a machine learning classifier has be-
come the standard procedure, as soon as there is
annotated data available. Before a classifier can
be applied, relevant information (referred to as
features) needs to be extracted from the data. A
wide range of tasks have been tackled in this way
including language identification, part-of-speech
(POS) tagging, word sense disambiguation, sen-
timent detection, and semantic similarity.
In order to solve a supervised learning task,
each researcher needs to perform the same set of
steps in a predefined order: reading input data,
preprocessing, feature extraction, machine learn-
ing, and evaluation. Standardizing this process
is quite challenging, as each of these steps might
vary a lot depending on the task at hand. To com-
plicate matters further, the experimental process
is usually embedded in a series of configuration
changes. For example, introducing a new fea-
ture often requires additional preprocessing. Re-
searchers should not need to think too much about
such details, but focus on the actual research task.
DKPro TC is our take on the standardization of
an inherently complex problem, namely the imple-
mentation of supervised learning experiments for
new datasets or new learning tasks.
We will make some simplifying assumptions
wherever they do not harm our goal that the frame-
work should be applicable to the widest possible
range of supervised learning tasks. For example,
DKPro TC only supports a limited set of machine
learning frameworks, as we argue that differences
between frameworks will mainly influence run-
time, but will have little influence on the final con-
clusions to be drawn from the experiment. The
main goal of DKPro TC is to enable the researcher
to quickly find an optimal experimental configura-
tion. One of the major contributions of DKPro TC
is the modular architecture for preprocessing and
feature extraction, as we believe that the focus of
research should be on a meaningful and expressive
feature set. DKPro TC has already been applied to
a wide range of different supervised learning tasks,
which makes us confident that it will be of use to
the research community.
DKPro TC is mostly written in Java and freely
available under an open source license.
1
2 Requirements
In the following, we give a more detailed overview
of the requirements and goals we have identified
for a general-purpose text classification system.
These requirements have guided the development
of the DKPro TC system architecture.
1
http://dkpro-tc.googlecode.com
61
Single-label Multi-label Regression
Document Mode
? Spam Detection
? Sentiment Detection
? Text Categorization
? Keyphrase Assignment
? Text Readability
Unit/Sequence Mode
? Named Entity Recognition
? Part-of-Speech Tagging
? Dialogue Act Tagging ? Word Difficulty
Pair Mode
? Paraphrase Identification
? Textual Entailment
? Relation Extraction ? Text Similarity
Table 1: Supervised learning scenarios and feature modes supported in DKPro TC, with example NLP
applications.
Flexibility Users of a system for supervised
learning on textual data should be able to choose
between different machine learning approaches
depending on the task at hand. In supervised ma-
chine learning, we have to distinguish between ap-
proaches based on classification and approaches
based on regression. In classification, given a
document d ? D and a set of labels C =
{c
1
, c
2
, ..., c
n
}, we want to label each document
d with L ? C, where L is the set of relevant
or true labels. In single-label classification, each
document d is labeled with exactly one label, i.e.
|L| = 1, whereas in multi-label classification, a
set of labels is assigned, i.e. |L| ? 1. Single-
label classification can further be divided into bi-
nary classification (|C| = 2) and multi-class clas-
sification (|C| > 2). In regression, real numbers
instead of labels are assigned.
Feature extraction should follow a modular de-
sign in order to facilitate reuse and to allow seam-
less integration of new features. However, the way
in which features need to be extracted from the in-
put documents depends on the the task at hand.
We have identified several typical scenarios in su-
pervised learning on textual data and propose the
following feature modes:
? In document mode, each input document will
be used as its own entity to be classified, e.g.
an email classified as wanted or unwanted
(spam).
? In unit/sequence mode, each input document
contains several units to be classified. The
units in the input document cannot be divided
into separate documents, either because the
context of each unit needs to be preserved
(e.g. to disambiguate named entities) or be-
cause they form a sequence which needs to
be kept (in sequence tagging).
? The pair mode is intended for problems
which require a pair of texts as input, e.g.
a pair of sentences to be classified as para-
phrase or non-paraphrase. It represents a
special case of multi-instance learning (Sur-
deanu et al., 2012), in which a document con-
tains exactly two instances.
Considering the outlined learning approaches and
feature modes, we have summarized typical sce-
narios in supervised learning on textual data in Ta-
ble 1 and added example applications in NLP.
Replicability and Reusability As it has been
recently noted by Fokkens et al. (2013), NLP ex-
periments are not replicable in most cases. The
problem already starts with undocumented pre-
processing steps such as tokenization or sentence
boundary detection that might have heavy impact
on experimental results. In a supervised learning
setting, this situation is even worse, as e.g. fea-
ture extraction is usually only partially described
in the limited space of a research paper. For ex-
ample, a paper might state that ?n-gram features?
were used, which encompasses a very broad range
of possible implementations.
In order to make NLP experiments replicable, a
text classification framework should (i) encourage
the user to reuse existing components which they
can refer to in research papers rather than writ-
ing their own components, (ii) document all per-
formed steps, and (iii) make it possible to re-run
experiments with minimal effort.
Apart from helping the replicability of experi-
ments, reusing components allows the user to con-
centrate on the new functionality that is specific
to the planned experiment instead of having to
reinvent the wheel. The parts of a text classifi-
cation system which can typically be reused are
62
preprocessing components, generic feature extrac-
tors, machine learning algorithms, and evaluation.
3 Architecture
We now give an overview of the DKPro TC archi-
tecture that was designed to take into account the
requirements outlined above. A core design deci-
sion is to model each of the typical steps in text
classification (reading input data and preprocess-
ing, feature extraction, machine learning and eval-
uation) as separate tasks. This modular architec-
ture helps the user to focus on the main problem,
i.e. developing and selecting good features.
In the following, we describe each module in
more detail, starting with the workflow engine that
is used to assemble the tasks into an experiment.
3.1 Configuration and Workflow Engine
We rely on the DKPro Lab (Eckart de Castilho
and Gurevych, 2011) workflow engine, which al-
lows fine-grained control over the dependencies
between single tasks, e.g. the pre-processing of a
document obviously needs to happen before the
feature extraction. In order to shield the user
from the complex ?wiring? of tasks, DKPro TC
currently provides three pre-defined workflows:
Train/Test, Cross-Validation, and Prediction (on
unseen data). Each workflow supports the feature
modes described above: document, unit/sequence,
and pair.
The user is still able to control the behavior of
the workflow by setting parameters, most impor-
tantly the sources of input data, the set of feature
extractors, and the classifier to be used. Internally,
each parameter is treated as a single dimension
in the global parameter space. Users may pro-
vide more than one value for a certain parame-
ter, e.g. specific feature sets or several classifiers.
The workflow engine will automatically run all
possible parameter value combinations (a process
called parameter sweeping).
3.2 Reading Input Data
Input data for supervised learning tasks comes in
myriad different formats which implies that read-
ing data cannot be standardized, but needs to be
handled individually for each data set. However,
the internal processing should not be dependent on
the input format. We therefore use the Common
Analysis Structure (CAS), provided by the Apache
Unstructured Information Management Architec-
ture (UIMA), to represent input documents and
annotations in a standardized way.
Under the UIMA model, reading input data
means to transform arbitrary input data into a
CAS representation. DKPro TC already provides
a wide range of readers from UIMA component
repositories such as DKPro Core.
2
The reader
also needs to assign to each classification unit an
outcome attribute that represents the relevant label
(single-label), labels (multi-label), or a real value
(regression). In unit/sequence mode, the reader
additionally needs to mark the units in the CAS.
In pair mode, a pair of texts (instead of a single
document) is stored within one CAS.
3.3 Preprocessing
In this step, additional information about the docu-
ment is added to the CAS, which efficiently stores
large numbers of stand-off annotations. In pair
mode, the preprocessing is automatically applied
to both documents.
DKPro TC allows the user to run arbitrary
UIMA-based preprocessing components as long
as they are compatible with the DKPro type sys-
tem that is currently used by DKPro Core and
EOP.
3
Thus, a large set of ready-to-use prepro-
cessing components for more than ten languages
is available, containing e.g. sentence boundary de-
tection, lemmatization, POS-tagging, or parsing.
3.4 Feature Extraction
DKPro TC ships a constantly growing number of
feature extractors. Feature extractors have access
to the document text as well as all the additional
information that has been added in the form of
UIMA stand-off annotations during the prepro-
cessing step. Users of DKPro TC can add cus-
tomized feature extractors for particular use cases
on demand.
Among the ready-to-use feature extractors con-
tained in DKPro TC, there are several ones ex-
tracting grammatical information, e.g. the plural-
singular ratio or the ratio of modal to all verbs.
Other features collect information about stylistic
cues of a document, e.g. the number of exclama-
tions or the type-token-ratio. DKPro TC is able to
extract n-grams or skip n-grams of tokens, charac-
ters, and POS tags.
Some feature extractors need access to informa-
tion about the entire document collection, e.g. in
2
http://dkpro-core-asl.googlecode.com
3
http://hltfbk.github.io/Excitement-Open-Platform/
63
order to weigh lexical features with tf.idf scores.
Such extractors have to declare that they depend
on collection level information and DKPro TC
will automatically include a special task that is
executed before the actual features are extracted.
Depending on the feature mode which has been
configured, DKPro TC will extract information
on document level, unit- and/or sequence-level, or
document pair level.
DKPro TC stores extracted features in its inter-
nal feature store. When the extraction process is
finished, a configurable data writer converts the
content from the feature store into a format which
can be handled by the utilized machine learning
tool. DKPro TC currently ships data writers for
the Weka (Hall et al., 2009), Meka
4
, and Mallet
(McCallum, 2002) frameworks. Users can also
add dedicated data writers that output features in
the format used by the machine learning frame-
work of their choice.
3.5 Supervised Learning
For the actual machine learning, DKPro TC cur-
rently relies on Weka (single-label and regres-
sion), Meka (multi-label), and Mallet (sequence
labeling). It contains a task which trains a freely
configurable classifier on the training data and
evaluates the learned model on the test data.
Before training and evaluation, the user may ap-
ply dimensionality reduction to the feature set, i.e.
select a limited number of (expectedly meaning-
ful) features to be included for training and eval-
uating the classifier. DKPro TC uses the feature
selection capabilities of Weka (single-label and re-
gression) and Mulan (multi-label) (Tsoumakas et
al., 2010).
DKPro TC can also predict labels on unseen
(i.e. unlabeled) data, using a trained classifier. In
that case, no evaluation will be carried out, but the
classifier?s prediction for each document will be
written to a file.
3.6 Evaluation and Reporting
DKPro TC calculates common evaluation scores
including accuracy, precision, recall, and F
1
-
score. Whenever sensible, scores are reported for
each individual label as well as aggregated over
all labels. To support users in further analyz-
ing the performance of a classification workflow,
DKPro TC outputs the confusion matrix, the ac-
4
http://meka.sourceforge.net
tual predictions assigned to each document, and a
ranking of the most useful features based on the
configured feature selection algorithm. Additional
task-specific reporting can be added by the user.
As mentioned before, a major goal of
DKPro TC is to increase the replicability of NLP
experiments. Thus, for each experiment, all con-
figuration parameters are stored and will be re-
ported together with the classification results.
4 Tweet Classification: A Use Case
We now give a brief summary of what a supervised
learning task might look like in DKPro TC using
a simple Twitter sentiment classification example.
Assuming that we want to classify a set of tweets
either as ?emotional? or ?neutral?, we can use the
setup shown in Listing 1. The example uses the
Groovy programming language which yields bet-
ter readable code, but pure Java is also supported.
Likewise, a DKPro TC experiment can also be set
up with the help of a configuration file, e.g. in
JSON or via Groovy scripts.
First, we create a workflow as a BatchTask-
CrossValidation which can be used to run
a cross-validation experiment on the data (using
10 folds as configured by the corresponding pa-
rameter). The workflow uses LabeledTweet-
Reader in order to import the experiment data
from source text files into the internal document
representation (one document per tweet). This
reader adds a UIMA annotation that specifies the
gold standard classification outcome, i.e. the rel-
evant label for the tweet. In this use case, pre-
processing consists of a single step: running the
ArkTweetTagger (Gimpel et al., 2011), a spe-
cialized Twitter tokenizer and POS-tagger that is
integrated in DKPro Core. The feature mode is set
to document (one tweet per CAS), and the learning
mode to single-label (each tweet is labeled with
exactly one label), cf. Table 1.
Two feature extractors are configured: One for
returning the number of hashtags and another one
returning the ratio of emoticons to tokens in the
tweet. Listing 2 shows the Java code for the sec-
ond extractor. Two things are noteworthy: (i) doc-
ument text and UIMA annotations are readily
available through the JCas object, and (ii) this is
really all that the user needs to write in order to
add a new feature extractor.
The next item to be configured is the Weka-
DataWriter which converts the internal fea-
64
BatchTaskCrossValidation batchTask = [
experimentName: "Twitter-Sentiment",
preprocessingPipeline: createEngineDescription(ArkTweetTagger), // Preprocessing
parameterSpace: [ // multi-valued parameters in the parameter space will be swept
Dimension.createBundle("reader", [
readerTrain: LabeledTweetReader,
readerTrainParams: [LabeledTweetReader.PARAM_CORPUS_PATH, "src/main/resources/tweets.txt"]]),
Dimension.create("featureMode", "document"),
Dimension.create("learningMode", "singleLabel"),
Dimension.create("featureSet", [EmoticonRatioExtractor.name, NumberOfHashTagsExtractor.name]),
Dimension.create("dataWriter", WekaDataWriter.name),
Dimension.create("classificationArguments", [NaiveBayes.name, RandomForest.name])],
reports: [BatchCrossValidationReport], // collects results from folds
numFolds: 10];
Listing 1: Groovy code to configure a DKPro TC cross-validation BatchTask on Twitter data.
public class EmoticonRatioFeatureExtractor
extends FeatureExtractorResource_ImplBase implements DocumentFeatureExtractor
{
@Override
public List<Feature> extract(JCas annoDb) throws TextClassificationException {
int nrOfEmoticons = JCasUtil.select(annoDb, EMO.class).size();
int nrOfTokens = JCasUtil.select(annoDb, Token.class).size();
double ratio = (double) nrOfEmoticons / nrOfTokens;
return new Feature("EmoticonRatio", ratio).asList();
}
}
Listing 2: A DKPro TC document mode feature extractor measuring the ratio of emoticons to tokens.
ture representation into the Weka ARFF format.
For the classification, two machine learning algo-
rithms will be iteratively tested: a Naive Bayes
classifier and a Random Forest classifier. Pass-
ing a list of parameters into the parameter space
will automatically make DKPro TC test all pos-
sible parameter combinations. The classification
task automatically trains a model on the training
data and stores the results of the evaluation on
the test data for each fold on the disk. Finally,
the evaluation scores for each fold are collected
by the BatchCrossValidationReport and
written to a single file using a tabulated format.
5 Related Work
This section will give a brief overview about tools
with a scope similar to DKPro TC. We only list
freely available software, most of which is open-
source. Unless otherwise indicated, all of the tools
are written in Java.
ClearTK (Ogren et al., 2008) is conceptually
closest to DKPro TC and shares many of its dis-
tinguishing features like the modular feature ex-
tractors. It provides interfaces to machine learn-
ing libraries such as Mallet or libsvm, offers wrap-
pers for basic NLP components, and comes with
a feature extraction library that facilitates the de-
velopment of custom feature extractors within the
UIMA framework. In contrast to DKPro TC, it is
rather designed as a programming library than a
customizable research environment for quick ex-
periments and does not provide predefined text
classification setups. Furthermore, it does not sup-
port parameter sweeping and has no explicit sup-
port for creating experiment reports.
Argo (Rak et al., 2013) is a web-based work-
bench with support for manual annotation and au-
tomatic analysis of mainly bio-medical data. Like
DKPro TC, Argo is based on UIMA, but focuses
on sequence tagging, and it lacks DKPro TC?s pa-
rameter sweeping capabilities.
NLTK (Bird et al., 2009) is a general-purpose
NLP toolkit written in Python. It offers com-
ponents for a wide range of preprocessing tasks
and also supports feature extraction and machine
learning for supervised text classification. Like
DKPro TC, it can be used to quickly setup baseline
experiments. As opposed to DKPro TC, NLTK
lacks a modular structure with respect to prepro-
cessing and feature extraction and does not sup-
port parameter sweeping.
Weka (Hall et al., 2009) is a machine learning
framework that covers only the last two steps of
DKPro TC?s experimental process, i.e. machine
learning and evaluation. However, it offers no ded-
icated support for preprocessing and feature gener-
ation. Weka is one of the machine learning frame-
works that can be used within DKPro TC for ac-
tual machine learning.
Mallet (McCallum, 2002) is another machine
65
learning framework implementing several super-
vised and unsupervised learning algorithms. As
opposed to Weka, is also supports sequence tag-
ging, including Conditional Random Fields, as
well as topic modeling. Mallet can be used as ma-
chine learning framework within DKPro TC.
Scikit-learn (Pedregosa et al., 2011) is a ma-
chine learning framework written in Python. It
offers basic functionality for preprocessing, fea-
ture selection, and parameter tuning. It provides
some methods for preprocessing such as convert-
ing documents to tf.idf vectors, but does not offer
sophisticated and customizable feature extractors
for textual data like DKPro TC.
6 Summary and Future Work
We have presented DKPro TC, a comprehensive
and flexible framework for supervised learning on
textual data. DKPro TC makes setting up exper-
iments and creating new features fast and simple,
and can therefore be applied for rapid prototyp-
ing. Its extensive logging capabilities emphasize
the replicability of results. In our own research
lab, DKPro TC has successfully been applied to a
wide range of tasks including author identification,
text quality assessment, and sentiment detection.
There are some limitations to DKPro TC which
we plan to address in future work. To reduce the
runtime of experiments with very large document
collections, we want to add support for parallel
processing of documents. While the current main
goal of DKPro TC is to bootstrap experiments on
new data sets or new applications, we also plan to
make DKPro TC workflows available as resources
to other applications, so that a model trained with
DKPro TC can be used to automatically label tex-
tual data in different environments.
Acknowledgments
This work has been supported by the Volks-
wagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Hessian research excellence pro-
gram ?Landes-Offensive zur Entwicklung
Wissenschaftlich-?konomischer Exzellenz?
(LOEWE) as part of the research center ?Digital
Humanities?. The authors would like give special
thanks to Richard Eckhart de Castilho, Nicolai
Erbs, Lucie Flekova, Emily Jamison, Krish
Perumal, and Artem Vovk for their contributions
to the DKPro TC framework.
References
S. Bird, E. Loper, and E. Klein. 2009. Natural Lan-
guage Processing with Python. O?Reilly Media Inc.
R. Eckart de Castilho and I. Gurevych. 2011. A
Lightweight Framework for Reproducible Parame-
ter Sweeping in Information Retrieval. In Proc. of
the Workshop on Data Infrastructures for Support-
ing Information Retrieval Evaluation, pages 7?10.
D. Ferrucci and A. Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
A. Fokkens, M. van Erp, M. Postma, T. Pedersen,
P. Vossen, and N. Freire. 2013. Offspring from
Reproduction Problems: What Replication Failure
Teaches Us. In Proc. ACL, pages 1691?1701.
K. Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. Smith. 2011. Part-of-speech
tagging for Twitter: annotation, features, and exper-
iments. In Proc. ACL, pages 42?47.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. Witten. 2009. The WEKA Data Min-
ing Software: An Update. SIGKDD Explorations,
11(1):10?18.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
P. Ogren, P. Wetzler, and S. Bethard. 2008. ClearTK:
A UIMA toolkit for statistical natural language pro-
cessing. In Towards Enhanced Interoperability for
Large HLT Systems: UIMA for NLP workshop at
LREC, pages 32?38.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
R. Rak, A. Rowley, J. Carter, and S. Ananiadou.
2013. Development and Analysis of NLP Pipelines
in Argo. In Proc. ACL, pages 115?120.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Proc. EMNLP-CoNLL, pages
455?465.
G. Tsoumakas, I. Katakis, and I. Vlahavas. 2010. Min-
ing Multi-label Data. Transformation, 135(2):1?20.
66
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435?440,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UKP: Computing Semantic Textual Similarity by
Combining Multiple Content Similarity Measures
Daniel Ba?r?, Chris Biemann?, Iryna Gurevych??, and Torsten Zesch??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
We present the UKP system which performed
best in the Semantic Textual Similarity (STS)
task at SemEval-2012 in two out of three met-
rics. It uses a simple log-linear regression
model, trained on the training data, to combine
multiple text similarity measures of varying
complexity. These range from simple char-
acter and word n-grams and common sub-
sequences to complex features such as Ex-
plicit Semantic Analysis vector comparisons
and aggregation of word similarity based on
lexical-semantic resources. Further, we em-
ploy a lexical substitution system and statisti-
cal machine translation to add additional lex-
emes, which alleviates lexical gaps. Our final
models, one per dataset, consist of a log-linear
combination of about 20 features, out of the
possible 300+ features implemented.
1 Introduction
The goal of the pilot Semantic Textual Similarity
(STS) task at SemEval-2012 is to measure the de-
gree of semantic equivalence between pairs of sen-
tences. STS is fundamental to a variety of tasks
and applications such as question answering (Lin
and Pantel, 2001), text reuse detection (Clough et
al., 2002) or automatic essay grading (Attali and
Burstein, 2006). STS is also closely related to tex-
tual entailment (TE) (Dagan et al, 2006) and para-
phrase recognition (Dolan et al, 2004). It differs
from both tasks, though, insofar as those operate on
binary similarity decisions while STS is defined as
a graded notion of similarity. STS further requires a
bidirectional similarity relationship to hold between
a pair of sentences rather than a unidirectional en-
tailment relation as for the TE task.
A multitude of measures for computing similar-
ity between texts have been proposed in the past
based on surface-level and/or semantic content fea-
tures (Mihalcea et al, 2006; Landauer et al, 1998;
Gabrilovich and Markovitch, 2007). The exist-
ing measures exhibit two major limitations, though:
Firstly, measures are typically used in separation.
Thereby, the assumption is made that a single
measure inherently captures all text characteristics
which are necessary for computing similarity. Sec-
ondly, existing measures typically exclude similar-
ity features beyond content per se, thereby implying
that similarity can be computed by comparing text
content exclusively, leaving out any other text char-
acteristics. While we can only briefly tackle the sec-
ond issue here, we explicitly address the first one by
combining several measures using a supervised ma-
chine learning approach. With this, we hope to take
advantage of the different facets and intuitions that
are captured in the single measures.
In the following section, we describe the feature
space in detail. Section 3 describes the machine
learning setup. After describing our submitted runs,
we discuss the results and conclude.
2 Text Similarity Measures
We now describe the various features we have tried,
also listing features that did not prove useful.
2.1 Simple String-based Measures
String Similarity Measures These measures op-
erate on string sequences. The longest common
435
substring measure (Gusfield, 1997) compares the
length of the longest contiguous sequence of char-
acters. The longest common subsequence measure
(Allison and Dix, 1986) drops the contiguity re-
quirement and allows to detect similarity in case
of word insertions/deletions. Greedy String Tiling
(Wise, 1996) allows to deal with reordered text parts
as it determines a set of shared contiguous sub-
strings, whereby each substring is a match of maxi-
mal length. We further used the following measures,
which, however, did not make it into the final mod-
els, since they were subsumed by the other mea-
sures: Jaro (1989), Jaro-Winkler (Winkler, 1990),
Monge and Elkan (1997), and Levenshtein (1966).
Character/word n-grams We compare character
n-grams following the implementation by Barro?n-
Ceden?o et al (2010), thereby generalizing the orig-
inal trigram variant to n = 2, 3, . . . , 15. We also
compare word n-grams using the Jaccard coefficient
as previously done by Lyon et al (2001), and the
containment measure (Broder, 1997). As high n led
to instabilities of the classifier due to their high in-
tercorrelation, only n = 1, 2, 3, 4 was used.
2.2 Semantic Similarity Measures
Pairwise Word Similarity The measures for
computing word similarity on a semantic level op-
erate on a graph-based representation of words and
the semantic relations among them within a lexical-
semantic resource. For this system, we used the al-
gorithms by Jiang and Conrath (1997), Lin (1998a),
and Resnik (1995) on WordNet (Fellbaum, 1998).
In order to scale the resulting pairwise word sim-
ilarities to the text level, we applied the aggregation
strategy by Mihalcea et al (2006): The sum of the
idf -weighted similarity scores of each word with the
best-matching counterpart in the other text is com-
puted in both directions, then averaged. In our ex-
periments, the measure by Resnik (1995) proved to
be superior to the other measures and was used in all
word similarity settings throughout this paper.
Explicit Semantic Analysis We also used the vec-
tor space model Explicit Semantic Analysis (ESA)
(Gabrilovich and Markovitch, 2007). Besides Word-
Net, we used two additional lexical-semantic re-
sources for the construction of the ESA vector space:
Wikipedia and Wiktionary1.
Textual Entailment We experimented with using
the BIUTEE textual entailment system (Stern and
Dagan, 2011) for generating entailment scores to
serve as features for the classifier. However, these
features were not selected by the classifier.
Distributional Thesaurus We used similarities
from a Distributional Thesaurus (similar to Lin
(1998b)) computed on 10M dependency-parsed sen-
tences of English newswire as a source for pairwise
word similarity, one additional feature per POS tag.
However, only the feature based on cardinal num-
bers (CD) was selected in the final models.
2.3 Text Expansion Mechanisms
Lexical Substitution System We used the lexical
substitution system based on supervised word sense
disambiguation (Biemann, 2012). This system au-
tomatically provides substitutions for a set of about
1,000 frequent English nouns with high precision.
For each covered noun, we added the substitutions
to the text and computed the pairwise word similar-
ity for the texts as described above. This feature al-
leviates the lexical gap for a subset of words.
Statistical Machine Translation We used the
Moses SMT system (Koehn et al, 2007) to trans-
late the original English texts via three bridge lan-
guages (Dutch, German, Spanish) back to English.
Thereby, the idea was that in the translation pro-
cess additional lexemes are introduced which allevi-
ate potential lexical gaps. The system was trained on
Europarl made available by Koehn (2005), using the
following configuration which was not optimized for
this task: WMT112 baseline without tuning, with
MGIZA alignment. The largest improvement was
reached for computing pairwise word similarity (as
described above) on the concatenation of the origi-
nal text and the three back-translations.
2.4 Measures Related to Structure and Style
In our system, we also used measures which go
beyond content and capture similarity along the
structure and style dimensions inherent to texts.
However, as we report later on, for this content-
1www.wiktionary.org
20-5-grams, grow-diag-final-and alignment, msd-bidirec-
tional-fe reodering, interpolation and kndiscount
436
oriented task they were not selected by the classifier.
Nonetheless, we briefly list them for completeness.
Structural similarity between texts can be de-
tected by computing stopword n-grams (Sta-
matatos, 2011). Thereby, all content-bearing words
are removed while stopwords are preserved. Stop-
word n-grams of both texts are compared using the
containment measure (Broder, 1997). In our experi-
ments, we tested n-gram sizes for n = 2, 3, . . . , 10.
We also compute part-of-speech n-grams for
various POS tags which we then compare using the
containment measure and the Jaccard coefficient.
We also used two similarity measures between
pairs of words (Hatzivassiloglou et al, 1999): Word
pair order tells whether two words occur in the
same order in both texts (with any number of words
in between), word pair distance counts the number
of words which lie between those of a given pair.
To compare texts along the stylistic dimension,
we further use a function word frequencies mea-
sure (Dinu and Popescu, 2009) which operates on a
set of 70 function words identified by Mosteller and
Wallace (1964). Function word frequency vectors
are computed and compared by Pearson correlation.
We also include a number of measures which
capture statistical properties of texts, such as type-
token ratio (TTR) (Templin, 1957) and sequential
TTR (McCarthy and Jarvis, 2010).
3 System Description
We first run each of the similarity measures intro-
duced above separately. We then use the resulting
scores as features for a machine learning classifier.
Pre-processing Our system is based on DKPro3,
a collection of software components for natural
language processing built upon the Apache UIMA
framework. During the pre-processing phase, we to-
kenize the input texts and lemmatize using the Tree-
Tagger implementation (Schmid, 1994). For some
measures, we additionally apply a stopword filter.
Feature Generation We now compute similarity
scores for the input texts with all measures and for
all configurations introduced in Section 2. This re-
sulted in 300+ individual score vectors which served
as features for the following step.
3http://dkpro-core-asl.googlecode.com
Run Features
1 Greedy String Tiling
Longest common subsequence (2 normalizations)
Longest common substring
Character 2-, 3-, and 4-grams
Word 1- and 2-grams (Containment, w/o stopwords)
Word 1-, 3-, and 4-grams (Jaccard)
Word 2- and 4-grams (Jaccard, w/o stopwords)
Word Similarity (Resnik (1995) on WordNet
aggregated according to Mihalcea et al (2006);
2 variants: complete texts + difference only)
Explicit Semantic Analysis (Wikipedia, Wiktionary)
Distributional Thesaurus (POS: Cardinal numbers)
2 All Features of Run 1
Lexical Substitution for Word Sim. (complete texts)
SMT for Word Sim. (complete texts as above)
3 All Features of Run 2
Random numbers from [4.5, 5] for surprise datasets
Table 1: Feature sets of our three system configurations
Feature Combination The feature combination
step uses the pre-computed similarity scores, and
combines their log-transformed values using a linear
regression classifier from the WEKA toolkit (Hall et
al., 2009). We trained the classifier on the training
datasets of the STS task. During the development
cycle, we evaluated using 10-fold cross-validation.
Post-processing For Runs 2 and 3, we applied a
post-processing filter which stripped all characters
off the texts which are not in the character range
[a-zA-Z0-9]. If the texts match, we set their similar-
ity score to 5.0 regardless of the classifier?s output.
4 Submitted Runs
Run 1 During the development cycle, we identi-
fied 19 features (see Table 1) which achieved the
best performance on the training data. For each
of the known datasets, we trained a separate clas-
sifier and applied it to the test data. For the surprise
datasets, we trained the classifier on a joint dataset
of all known training datasets.
Run 2 For the Run 2, we were interested in the
effects of two additional features: lexical substitu-
tion and statistical machine translation. We added
the corresponding measures to the feature set of Run
1 and followed the same evaluation procedure.
Run 3 For the third run, we used the same feature
set as for Run 2, but returned random numbers from
[4.5, 5] for the sentence pairs in the surprise datasets.
437
Dim. Text Similarity Features PAR VID SE
Best Feature Set, Run 1 .711 .868 .735
Best Feature Set, Run 2 .724 .868 .742
Content Pairwise Word Similarity .564 .835 .527
Character n-grams .658 .771 .554
Explicit Semantic Analysis .427 .781 .619
Word n-grams .474 .782 .619
String Similarity .593 .677 .744
Distributional Thesaurus .494 .481 .365
Lexical Substitution .228 .554 .483
Statistical Machine Translation .287 .652 .516
Structure Part-of-speech n-grams .193 .265 .557
Stopword n-grams .211 .118 .379
Word Pair Order .104 .077 .295
Style Statistical Properties .168 .225 .325
Function Word Frequencies .179 .142 .189
Table 2: Best results for single measures, grouped by di-
mension, on the training datasets MSRpar, MSRvid, and
SMTeuroparl, using 10-fold cross-validation
5 Results on Training Data
Evaluation was carried out using the official scorer
which computes Pearson correlation of the human
rated similarity scores with the the system?s output.
In Table 2, we report the results achieved on
each of the training datasets using 10-fold cross-
validation. The best results were achieved for the
feature set of Run 2, with Pearson?s r = .724,
r = .868, and r = .742 for the datasets MSR-
par, MSRvid, and SMTeuroparl, respectively. While
individual classes of content similarity measures
achieved good results, a different class performed
best for each dataset. However, text similarity mea-
sures related to structure and style achieved only
poor results on the training data. This was to be ex-
pected due to the nature of the data, though.
6 Results on Test Data
Besides the Pearson correlation for the union of all
datasets (ALL), the organizers introduced two addi-
tional evaluation metrics after system submission:
ALLnrm computes Pearson correlation after the sys-
tem outputs for each dataset are fitted to the gold
standard using least squares, and Mean refers to the
weighted mean across all datasets, where the weight
depends on the number of pairs in each dataset.
In Table 3, we report the offical results achieved
on the test data. The best configuration of our system
was Run 2 which was ranked #1 for the evaluation
#1 #2 #3 Sys. r1 r2 r3 PAR VID SE WN SN
1 2 1 UKP2 .823 .857 .677 .683 .873 .528 .664 .493
2 3 5 TL .813 .856 .660 .698 .862 .361 .704 .468
3 1 2 TL .813 .863 .675 .734 .880 .477 .679 .398
4 4 4 UKP1 .811 .855 .670 .682 .870 .511 .664 .467
5 6 13 UNT .784 .844 .616 .535 .875 .420 .671 .403
...
...
...
...
...
...
...
...
...
...
...
...
87 85 70 B/L .311 .673 .435 .433 .299 .454 .586 .390
Table 3: Official results on the test data for the top 5
participating runs out of 89 which were achieved on the
known datasets MSRpar, MSRvid, and SMTeuroparl, as
well as on the surprise datasets OnWN and SMTnews. We
report the ranks (#1: ALL, #2: ALLnrm, #3: Mean) and
the corresponding Pearson correlation r according to the
three offical evaluation metrics (see Sec. 6). The provided
baseline is shown at the bottom of this table.
metrics ALL (r = .823)4 and Mean (r = .677), and
#2 for ALLnrm (r = .857). An exhaustive overview
of all participating systems can be found in the STS
task description (Agirre et al, 2012).
7 Conclusions and Future Work
In this paper, we presented the UKP system, which
performed best across the three official evaluation
metrics in the pilot Semantic Textual Similarity
(STS) task at SemEval-2012. While we did not
reach the highest scores on any of the single datasets,
our system was most robust across different data. In
future work, it would be interesting to inspect the
performance of a system that combines the output
of all participating systems in a single linear model.
We also propose that two major issues with the
datasets are tackled in future work: (a) It is unclear
how to judge similarity between pairs of texts which
contain contextual references such as on Monday
vs. after the Thanksgiving weekend. (b) For several
pairs, it is unclear what point of view to take, e.g. for
the pair An animal is eating / The animal is hopping.
Is the pair to be considered similar (an animal is do-
ing something) or rather not (eating vs. hopping)?
Acknowledgements This work has been sup-
ported by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No.
I/82806, and by the Klaus Tschira Foundation under
project No. 00.133.2008.
499% confidence interval: .807 ? r ? .837
438
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion, in conjunction with the 1st Joint Conference on
Lexical and Computational Semantics.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305?310.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
Alberto Barro?n-Ceden?o, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism Detection across
Distant Language Pairs. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 37?45.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. Proceedings of the Compres-
sion and Complexity of Sequences, pages 21?29.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
152?159.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. In Machine Learning Challenges, Lecture
Notes in Computer Science, pages 177?190. Springer.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62?66.
William B. Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News Sources.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 350?356.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combina-
tions via machine learning. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
203?212.
Matthew A. Jaro. 1989. Advances in record linkage
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Associa-
tion, 84(406):414?420.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the 10th International Conference
on Research in Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
10th Machine Translation Summit, pages 79?86.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2):259?284.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998a. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296?304.
Dekang Lin. 1998b. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 768?774.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of Conference
on Empirical Methods in Natural Language Process-
ing, pages 118?125.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophisti-
439
cated approaches to lexical diversity assessment. Be-
havior research methods, 42(2):381?92.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775?780.
Alvaro Monge and Charles Elkan. 1997. An efficient
domain-independent algorithm for detecting approxi-
mately duplicate database records. In Proceedings of
the SIGMOD Workshop on Data Mining and Knowl-
edge Discovery, pages 23?29.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and disputed authorship: The Federalist.
Addison-Wesley.
Philip Resnik. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence, pages 448?453.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512?2527.
Asher Stern and Ido Dagan. 2011. A Confidence
Model for Syntactically-Motivated Entailment Proofs.
In Proceedings of the International Conference on Re-
cent Advances in Natural Language Processing, pages
455?462.
Mildred C. Templin. 1957. Certain language skills in
children. University of Minnesota Press.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the Sec-
tion on Survey Research Methods, pages 354?359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE technical symposium
on Computer science education, pages 130?134.
440
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 39?47, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 5: Evaluating Phrasal Semantics
Ioannis Korkontzelos
National Centre for Text Mining
School of Computer Science
University of Manchester, UK
ioannis.korkontzelos@man.ac.uk
Torsten Zesch
UKP Lab, CompSci Dept.
Technische Universita?t Darmstadt
Germany
zesch@ukp.informatik.tu-darmstadt.de
Fabio Massimo Zanzotto
Department of Enterprise Engineering
University of Rome ?Tor Vergata?
Italy
zanzotto@info.uniroma2.it
Chris Biemann
FG Language Technology, CompSci Dept.
Technische Universita?t Darmstadt
Germany
biem@cs.tu-darmstadt.de
Abstract
This paper describes the SemEval-2013 Task
5: ?Evaluating Phrasal Semantics?. Its first
subtask is about computing the semantic simi-
larity of words and compositional phrases of
minimal length. The second one addresses
deciding the compositionality of phrases in a
given context. The paper discusses the impor-
tance and background of these subtasks and
their structure. In succession, it introduces the
systems that participated and discusses evalu-
ation results.
1 Introduction
Numerous past tasks have focused on leveraging the
meaning of word types or words in context. Exam-
ples of the former are noun categorization and the
TOEFL test, examples of the latter are word sense
disambiguation, metonymy resolution, and lexical
substitution. As these tasks have enjoyed a lot suc-
cess, a natural progression is the pursuit of models
that can perform similar tasks taking into account
multiword expressions and complex compositional
structure. In this paper, we present two subtasks de-
signed to evaluate such phrasal models:
a. Semantic similarity of words and compositional
phrases
b. Evaluating the compositionality of phrases in
context
For example, the first subtask addresses computing
how similar the word ?valuation? is to the compo-
sitional sequence ?price assessment?, while the sec-
ond subtask addresses deciding whether the phrase
?piece of cake? is used literally or figuratively in the
sentence ?Labour was a piece of cake!?.
The aim of these subtasks is two-fold. Firstly,
considering that there is a spread interest lately in
phrasal semantics in its various guises, they provide
an opportunity to draw together approaches to nu-
merous related problems under a common evalua-
tion set. It is intended that after the competition,
the evaluation setting and the datasets will comprise
an on-going benchmark for the evaluation of these
phrasal models.
Secondly, the subtasks attempt to bridge the
gap between established lexical semantics and full-
blown linguistic inference. Thus, we anticipate that
they will stimulate an increased interest around the
general issue of phrasal semantics. We use the no-
tion of phrasal semantics here as opposed to lexi-
cal compounds or compositional semantics. Bridg-
ing the gap between lexical semantics and linguis-
tic inference could provoke novel approaches to cer-
tain established tasks, such as lexical entailment and
paraphrase identification. In addition, it could ul-
39
timately lead to improvements in a wide range of
applications in natural language processing, such
as document retrieval, clustering and classification,
question answering, query expansion, synonym ex-
traction, relation extraction, automatic translation,
or textual advertisement matching in search engines,
all of which depend on phrasal semantics.
The remainder of this paper is structured as fol-
lows: Section 2 presents details about the data
sources and the variety of sources applicable to the
task. Section 3 discusses the first subtask, which
is about semantic similarity of words and compo-
sitional phrases. In subsection 3.1 the subtask is
described in detail together with some information
about its background. Subsection 3.2 discusses the
data creation process and subsection 3.3 discusses
the participating systems and their results. Section 4
introduces the second subtask, which is about eval-
uating the compositionality of phrases in context.
Subsection 4.1 explains the data creation process for
this subtask. In subsection 4.2 the evaluation statis-
tics of participating systems are presented. Section
5 is a discussion about the conclusions of the entire
task. Finally, in section 6 we summarize this presen-
tation and discuss briefly our vision about challenges
in distributional semantics.
2 Data Sources & Methodology
Data instances of both subtasks are drawn from the
large-scale, freely available WaCky corpora (Baroni
et al, 2009). The resource contains corpora in 4 lan-
guages: English, French, German and Italian. The
English corpus, ukWaC, consists of 2 billion words
and was constructed by crawling to the .uk domain
of the web and using medium-frequency words from
the BNC as seeds. The corpus is part-of-speech
(PoS) tagged and lemmatized using the TreeTagger
(Schmid, 1994). The French corpus, frWaC, con-
tains 1.6 billion word corpus and was constructed
by web-crawling the .fr domain and using medium-
frequency words from the Le Monde Diplomatique
corpus and basic French vocabulary lists as seeds.
The corpus was PoS tagged and lemmatized with
the TreeTagger. The French corpus, deWaC, con-
sists of 1.7 billion word corpus and was constructed
by crawling the .de domain and using medium-
frequency words from the SudDeutsche Zeitung cor-
pus and basic German vocabulary lists as seeds. The
corpus was PoS tagged and lemmatized with the
TreeTagger. The Italian corpus, itWaC, is a 2 billion
word corpus constructed from the .it domain of the
web using medium-frequency words from the Re-
pubblica corpus and basic Italian vocabulary lists as
seeds. The corpus was PoS tagged with the Tree-
Tagger, and lemmatized using the Morph-it! lexicon
(Zanchetta and Baroni, 2005). Several versions of
the WaCky corpora, with various extra annotations
or modifications are also available1.
We ensured that data instances occur frequently
enough in the WaCky corpora, so that participat-
ing systems could gather statistics for building dis-
tributional vectors or other uses. As the evalua-
tion data only contains very small annotated sam-
ples from freely available web documents, and the
original source is provided, we could provide them
without violating copyrights.
The size of the WaCky corpora is suitable for
training reliable distributional models. Sentences
are already lemmatized and part-of-speech tagged.
Participating approaches making use of distribu-
tional methods, part-of-speech tags or lemmas, were
strongly encouraged to use these corpora and their
shared preprocessing, to ensure the highest possi-
ble comparability of results. Additionally, this had
the potential to considerably reduce the workload of
participants. For the first subtask, data were pro-
vided in English, German and Italian and for the sec-
ond subtask in English and German.
The range of methods applicable to both subtasks
was deliberately not limited to any specific branch of
methods, such as distributional or vector models of
semantic compositionality. We believe that the sub-
tasks can be tackled from different directions and we
expect a great deal of the scientific benefit to lie in
the comparison of very different approaches, as well
as how these approaches can be combined. An ex-
ception to this rule is the fact that participants in the
first subtask were not allowed to use directly defini-
tions extracted from dictionaries or lexicons. Since
the subtask is considered fundamental and its data
were created from online knowledge resources, sys-
tems using the same tools to address it would be of
limited use. However, participants were allowed to
1WaCky website: wacky.sslmit.unibo.it
40
use other information residing in dictionaries, such
as Wordnet synsets or synset relations.
Participating systems were allowed to attempt one
or both subtasks, in one or all of the languages sup-
ported. However, it was expected that systems per-
forming well at the first basic subtask would pro-
vide a good starting point for dealing with the sec-
ond subtask, which is considered harder. Moreover,
language-independent models were of special inter-
est.
3 Subtask 5a: Semantic Similarity of
Words and Compositional Phrases
The aim of this subtask is to evaluate the compo-
nent of a semantic model that computes the simi-
larity between word sequences of different length.
Participating systems are asked to estimate the se-
mantic similarity of a word and a short sequence of
two words. For example, they should be able to fig-
ure out that contact and close interaction are similar
whereas megalomania and great madness are not.
This subtask addresses a core problem, since sat-
isfactory performance in computing the similarity of
full sentences depends on similarity computations
on shorter sequences.
3.1 Background and Description
This subtask is based on the assumption that we
first need a basic set of functions to compose the
meaning of two words, in order to construct more
complex models that compositionally determine the
meaning of sentences, as a second step. For compo-
sitional distributional semantics, the need for these
basic functions is discussed in Mitchell and Lapata
(2008). Since then, many models have been pro-
posed for addressing the task (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010; Guevara, 2010),
but still comparative analysis is in general based on
comparing sequences that consist of two words.
As in Zanzotto et al (2010), this subtask proposes
to compare the similarity of a 2-word sequence and
a single word. This is important as it is the basic
step to analyse models that can compare any word
sequences of different length.
The development and testing set for this subtask
were built based on the idea described in Zanzotto
et al (2010). Dictionaries were used as sources of
contact/[kon-takt]
1. the act or state of touching;
a touching or meeting, as of
two things or people.
2. close interaction
3. an acquaintance, colleague,
or relative through whom a
person can gain access to
information, favors, influ-
ential people, and the like.
Figure 1: The definition of contact in a sample dictionary
positive training examples. Dictionaries are natural
repositories of equivalences between words under
definition and sequences of words used for defining
them. Figure 1 presents the definition of the word
contact, from which the pair (contact, close interac-
tion) can be extracted. Such equivalences extracted
from dictionaries can be seen as natural and unbi-
ased data instances. This idea opens numerous op-
portunities:
? Since definitions in dictionaries are syntacti-
cally rich, we are able to create examples for
different syntactic relations.
? We have the opportunity to extract positive ex-
amples for languages for which dictionaries
with sufficient entries are available.
Negative examples were generated by matching
words under definition with randomly chosen defin-
ing sequences. In the following subsection, we pro-
vide details about the application of this idea to build
the development and testing set for subtask 5a.
3.2 Data Creation
Data for this subtask were provided in English, Ger-
man and Italian. Pairs of words under definitions and
defining sequences were extracted from the English,
German and Italian part of Wiktionary, respectively.
In particular, for each language, all Wiktionary en-
tries were downloaded and part-of-speech tagged us-
ing the Genia tagger (Tsuruoka et al, 2005). In
succession, definitions that start with noun phrases
41
Language Train set Test set Total
English 5,861 3,907 9,768
German 1,516 1,010 2,526
Italian 1,275 850 2,125
German - no names 1,101 733 1,834
Table 1: Quantitative characteristics of the datasets
were kept, only. For the purpose of extracting word
and sequence pairs for this subtask, we consider as
noun phrases, sequences that consist of adjectives
or noun and end with a noun. In cases where the
extracted noun phrase was longer than two words,
the right-most two sequences were kept, since in
most cases noun phrases are governed by their right-
most component. Subsequently, we discarded in-
stances whose words occur too infrequently in the
WaCky corpora (Baroni et al, 2009) of each lan-
guage. WaCky corpora are available freely and are
large enough for participating systems to extract dis-
tributional statistics. Taking the numbers of ex-
tracted instances into account, we set the frequency
thresholds at 10 occurrences for English and 5 for
German and Italian.
Data instances extracted following this process
were then checked by a computational linguist. Can-
didate pairs in which the definition sequence was not
judged to be a precise and adequate definition of the
word under definition were discarded. These cases
were very limited and mostly account for shortcom-
ings of the very simple pattern used for extraction.
For example, the pair (standard, transmission vehi-
cle) coming from the definition of ?standard? as ?A
manual transmission vehicle? was discarded. Simi-
larly in German, the pair (Fremde (Eng. stranger),
weibliche Person (Eng. female person)) was dis-
carded. ?Fremde?, which is of female grammat-
ical genre, was defined as ?weibliche Person, die
man nicht kennt (Eng. female person, one does not
know)?. In Italian, the pair (paese (Eng. land, coun-
try, region), grande estensione (Eng. large tract))
was discarded, since the original definition was
?grande estensione di terreno abitato e generalmente
coltivato (Eng. large tract of land inhabited and cul-
tivated in general)?.
The final data sets were divided into training and
held-out testing sets, according to a 60% and 40%
ratio, respectively. The first three rows of table 1
present the numbers of the train and test sets for the
three languages chosen. It was identified that a fair
percentage of the German instances (approximately
27%) refer to the definitions of first names or family
names. This is probably a flaw of the German part of
Wiktionary. In addition, the pattern used for extrac-
tion happens to apply to the definitions of names.
Name instances were discarded from the German
data set to produce the data set described in the last
row of table 1.
The training set was released approximately 3
months earlier than the test data. Instances in both
set ware annotated as positive or negative. Test set
annotations were not released to the participants, but
were used for evaluation, only.
3.3 Results
Participating systems were evaluated on their ability
to predict correctly whether the components of each
test instance, i.e. word-sequence pair, are semanti-
cally similar or distinct. Participants were allowed
to use or ignore the training data, i.e. the systems
could be supervised or unsupervised. Unsupervised
systems were allowed to use the training data for de-
velopment and parameter tuning. Since this is a core
task, participating systems were not be able to use
dictionaries or other prefabricated lists. Instead, they
were allowed to use distributional similarity models,
selectional preferences, measures of semantic simi-
larity etc.
Participating system responses were scored in
terms of standard information retrieval measures:
accuracy (A), precision (P), recall (R) and F1 score
(Radev et al, 2003). Systems were encouraged to
submit at most 3 solutions for each language, but
submissions for fewer languages were accepted.
Five research teams participated. Ten system runs
were submitted for English, one for German (on data
set: German - no names) and one for Italian. Table 2
illustrates the results of the evaluation process. The
teams of (HsH) (Wartena, 2013), CLaC (Siblini and
Kosseim, 2013), UMCC DLSI-(EPS) (Da?vila et al,
2013), and ITNLP, the Harbin Institute of Technol-
ogy, approached the task in a supervised way, while
MELODI (Van de Cruys et al, 2013) participated
with two unsupervised approaches. Interestingly,
42
Language Rank Participant Id run Id A R P rej. R rej. P F1
1 HsH 1 .803 .752 .837 .854 .775 .792
3 CLaC 3 .794 .707 .856 .881 .750 .774
2 CLaC 2 .794 .695 .867 .893 .745 .771
4 CLaC 1 .788 .638 .910 .937 .721 .750
English 5 MELODI lvw .748 .614 .838 .882 .695 .709
6 UMCC DLSI-(EPS) 1 .724 .613 .787 .834 .683 .689
7 ITNLP 3 .703 .501 .840 .904 .645 .628
8 MELODI dm .689 .481 .825 .898 .634 .608
9 ITNLP 1 .663 .392 .857 .934 .606 .538
10 ITNLP 2 .659 .427 .797 .891 .609 .556
German 1 HsH 1 .825 .765 .870 .885 .790 .814
Italian 1 UMCC DLSI-(EPS) 1 .675 .576 .718 .774 .646 .640
Table 2: Task 5a: Evaluation results. A, P, R, rej. and F1 stand for accuracy, precision, recall, rejection and F1 score,
respectively.
these approaches performed better than some super-
vised ones for this experiment. Below, we sum-
marise the properties of participating systems.
(HsH) (Wartena, 2013) used distributed similarity
and especially random indexing to compute similar-
ities between words and possible definitions, under
the hypothesis that a word and its definition are dis-
tributionally more similar than a word and an arbi-
trary definition. Considering all open-class words,
context vectors over the entire WaCky corpus were
computed for the word under definition, the defining
sequence, its component words separately, the ad-
dition and multiplication of the vectors of the com-
ponent words and a general context vector. Then,
various similarity measures were computed on the
vectors, including an innovative length-normalised
version of Jensen-Shannon divergence. The similar-
ity values are used to train a Support Vector Machine
(SVM) classifier (Cortes and Vapnik, 1995).
The first approach (run 1) of CLaC (Siblini and
Kosseim, 2013) is based on a weighted semantic
network to measure semantic relatedness between
the word and the components of the phrase. A
PART classifier is used to generate a partial decision
trained on the semantic relatedness information of
the labelled training set. The second approach uses
a supervised distributional method based on words
frequently occurring in the Web1TB corpus to cal-
culate relatedness. A JRip classifier is used to gen-
erate rules trained on the semantic relatedness infor-
mation of the training set. This approach was used
in conjunction with the first one as a backup method
(run 2). In addition, features generated by both ap-
proaches were used to train the JRIP classifier col-
lectively (run 3).
The first approach of MELODI (Van de Cruys
et al, 2013), called lvw, uses a dependency-based
vector space model computed over the ukWaC cor-
pus, in combination with Latent Vector Weighting
(Van de Cruys et al, 2011). The system computes
the similarity between the first noun and the head
noun of the second phrase, which was weighted ac-
cording to the semantics of the modifier. The second
approach, called dm, used a dependency-based vec-
tor space model, but, unlike the first approach, disre-
garded the modifier in the defining sequence. Since
both systems are unsupervised, the training data was
used to train a similarity threshold parameter, only.
UMCC DLSI-(EPS) (Da?vila et al, 2013) locates
the synsets of words in data instances and computes
the semantic distances between each synset of the
word under definition and each synsets of the defin-
ing sequence words. In succession, a classifier is
trained using features based on distance and Word-
Net relations.
The first attempt of ITNLP (run 1) consisted of an
SVM classifier trained on semantic similarity com-
putations between the word under definition and
43
the defining sequence in each instance. Their sec-
ond attempt also uses an SVM, however trained on
WordNet-based similarities. The third attempt of
ITNLP is a combination of the previous two; it com-
bines their features to train an SVM classifier.
4 Subtask 5b: Semantic Compositionality
in Context
An interesting sub-problem of semantic composi-
tionality is to decide whether a target phrase is used
in its literal or figurative meaning in a given con-
text. For example ?big picture? might be used lit-
erally as in Click here for a bigger picture or figura-
tively as in To solve this problem, you have to look at
the bigger picture. Another example is ?old school?
which can also be used literally or figuratively: He
will go down in history as one of the old school, a
true gentlemen. vs. During the 1970?s the hall of the
old school was converted into the library.
Being able to detect whether a phrase is used lit-
erally or figuratively is e.g. especially important for
information retrieval, where figuratively used words
should be treated separately to avoid false positives.
For example, the example sentence He will go down
in history as one of the old school, a true gentle-
men. should probably not be retrieved for the query
?school?. Rather, the insights generated from sub-
task 5a could be utilized to retrieve sentences using
a similar phrase such as ?gentleman-like behavior?.
The task may also be of interest to the related re-
search fields of metaphor detection and idiom iden-
tification.
There were no restrictions regarding the array of
methods, and the kind of resources that could be
employed for this task. In particular, participants
were allowed to make use of pre-fabricated lists of
phrases annotated with their probability of being
used figuratively from publicly available sources, or
to produce these lists from corpora. Assessing how
well the phrase suits its context might be tackled
using e.g. measures of semantic relatedness as well
as distributional models learned from the underlying
corpus.
Participants of this subtask were provided with
real usage examples of target phrases. For each us-
age example, the task is to make a binary decision
whether the target phrase is used literally or figu-
ratively in this context. Systems were tested in two
different disciplines: a known phrases task where all
target phrases in the test set were contained in the
training, and an unknown phrases setting, where all
target phrases in the test set were unseen.
4.1 Data Creation
The first step in creating the corpus was to compile
a list of phrases that can be used either literally or
metaphorically. Thus, we created an initial list of
several thousand English idioms from Wiktionary by
listing all entries under the category ENGLISH ID-
IOMS using the JWKTL Wiktionary API (Zesch et
al., 2008). We manually filtered the list removing
most idioms that are very unlikely to be ever used
literally (anymore), e.g. to knock on heaven?s door.
For each of the resulting list of phrases, we extracted
usage contexts from the ukWaC corpus (Baroni et
al., 2009). Each usage context contains 5 sentences,
where the sentence with the target phrase appears in
a randomized position. Due to segmentation errors,
some usage contexts actually might contain less than
5 sentences, but we manually filtered all usage con-
texts where the remaining context was insufficient.
This was done in the final cleaning step where we
also manually removed (near) duplicates, obvious
spam, encoding problems etc.
The target phrases in context were annotated for
figurative, literal, both or impossible to tell usage,
using the CrowdFlower2 crowdsourcing annotation
platform. We used about 8% of items as ?gold?
items for quality assurance, and had each example
annotated by three crowdworkers. The task was
comparably easy for crowdworkers, who reached
90%-94% pairwise agreement, and 95% success on
the gold items. About 5% of items with low agree-
ment and marked as impossible were removed. Ta-
ble 3 summarizes the quantitative characteristics of
all datasets resulting from this process. We took care
in sampling the data as to keep similar distributions
across the training, development and testing parts.
4.2 Results
Training and development datasets were made avail-
able in advance, test data was provided during the
evaluation period without labels. System perfor-
2www.crowdflower.com
44
Task Dataset # Phrases # Items Items per phrase # Liter. # Figur. # Both
known
train 10 1,424 68?188 702 719 3
dev 10 358 17?47 176 181 1
test 10 594 28?78 294 299 1
unseen
train 31 1,114 4?75 458 653 3
dev 9 342 4?74 141 200 1
test 15 518 8?73 198 319 1
Table 3: Quantitative characteristics of the datasets
Rank System Run Accuracy
1 IIRG 3 .779
2 UNAL 2 .754
3 UNAL 1 .722
5 IIRG 1 .530
4 Baseline MFC - .503
6 IIRG 2 .502
Table 4: Task 5b: Evaluation results for the known
phrases setting
Rank System Run Accuracy
1 UNAL 1 .668
2 UNAL 2 .645
3 Baseline MFC - .616
4 CLaC 1 .550
Table 5: Task 5b: Evaluation results for the unseen
phrases setting
mance was measured in accuracy. Since all partic-
ipants provided classifications for all test items, the
accuracy score is equivalent to precision/recall/F1.
Participants were allowed to enter up to three dif-
ferent runs for evaluation. We also provide baseline
accuracy scores, which are obtained by always as-
signing the most frequent class (figurative).
Table 4 provides the evaluation results for the
known phrases task, while Table 5 ranks participants
for the unseen phrases task. As expected, the un-
seen phrases setting is much harder than the known
phrases setting, as for unseen phrases it is not possi-
ble to learn lexicalised contextual clues. In both set-
tings, the winning entries were able to beat the MFC
baseline. While performance in the known phrases
setting is close to 80% and thus acceptable, the gen-
eral task of recognizing the literal or figurative use of
unseen phrases remains very challenging, with only
a small improvement over the baseline. We refer to
the system descriptions for more details on the tech-
niques used for this subtask: UNAL (Jimenez et al,
2013), IIRG (Byrne et al, 2013) and CLaC (Siblini
and Kosseim, 2013).
5 Task Conclusions
In this section, we further discuss the findings and
conclusion of the evaluation challenge in the task of
?Phrasal Semantics?.
Looking at the results of both subtasks, one ob-
serves that the maximum performance achieved is
higher for the first than the second subtask. For
this comparison to be fair, trivial baselines should be
taken into account. A system randomly assigning an
output value would be on average 50% correct in the
first subtask, since the numbers of positive and neg-
ative instances in the testing set are equal. Similarly,
a system assigning the most frequent class, i.e. the
figurative use of any phrase, would be 50.3% and
61.6% accurate in the second subtask for seen and
unseen test instances, respectively. It should also be
noted that the testing instances in the first subtask
are unseen in the respective training set. As a result,
in terms of baselines, the second subtask on unseen
data (Table 5) should be considered easier than the
first subtask (Table 2). However, the best perform-
ing systems achieved much higher accuracy in the
first than in the second subtask. This contradiction
confirms our conception that the first subtask is less
complex than the second.
In the first subtask, it is evident that no method
performs much better or much worse than the others.
45
Although the participating systems have employed a
wide variety of approaches and tools, the difference
between the best and worst accuracy achieved is
relatively limited, in particular approximately 14%.
Even more interestingly, unsupervised approaches
performed better than some supervised ones. This
observation suggests that no ?golden recipe? has
been identified so far for this task. Thus, probably
different processing tools take advantage of different
sources of information. It is a matter of future re-
search to identify these sources and the correspond-
ing tools, and then develop hybrid methods of im-
proved performance.
In the second subtask, the results of evaluation
on known phrases are much higher than on unseen
phrases. This was expected, as for unseen phrases it
is not possible to learn lexicalised contextual clues.
Thus, the second subtask has succeeded in identify-
ing the complexity threshold up to which the cur-
rent state-of-the-art can address the computational
problem. Further than this threshold, i.e. for unseen
phrases, current systems have not yet succeeded in
addressing it. In conclusion, the difficulty in eval-
uating the compositionality of previously unseen
phrases in context highlights the overall complexity
of the second subtask.
6 Summary and Future Work
In this paper we have presented the 5th task of Se-
mEval 2013, ?Evaluating Phrasal Semantics?, which
consists of two subtasks: (1) semantic similarity of
words and compositional phrases, and (2) compo-
sitionality of phrases in context. The former sub-
task, which focussed on the first step of composing
the meaning of phrases of any length, is less com-
plex than the latter subtask, which considers the ef-
fect of context to the semantics of a phrase. The
paper presents details about the background and im-
portance of these subtasks, the data creation process,
the systems that took part in the evaluation and their
results.
In the future, we expect evaluation challenges on
phrasal semantics to progress towards two direc-
tions: (a) the synthesis of semantics of sequences
longer than two words, and (b) aiming to improve
the performance of systems that determine the com-
positionality of previously unseen phrases in con-
text. The evaluation results of the first task sug-
gest that state-of-the-art systems can compose the
semantics of two word sequences with a promising
level of success. However, this task should be seen
as the first step towards composing the semantics
of sentence-long sequences. As far as subtask 5b
is concerned, the accuracy achieved by the partici-
pating systems on unseen testing data was low, only
slightly better than the most frequent class baseline,
which assigns the figurative use to all test phrases.
Thus, the subtask cannot be considered well ad-
dressed by the state-of-the-art and further progress
should be sought.
Acknowledgements
The work relevant to subtask 5a described in this pa-
per is funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
We would like to thank Tristan Miller for help-
ing with the subtleties of English idiomatic ex-
pressions, and Eugenie Giesbrecht for support
in the organization of subtask 5b. This work
has been supported by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Pro-
gram under grant No. I/82806, and by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-o?konomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities.
46
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA. Association for Compu-
tational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Lorna Byrne, Caroline Fenlon, and John Dunnion. 2013.
IIRG: A naive approach to evaluating phrasal seman-
tics. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
He?ctor Da?vila, Antonio Ferna?ndez Orqu??n, Alexander
Cha?vez, Yoan Gutie?rrez, Armando Collazo, Jose? I.
Abreu, Andre?s Montoyo, and Rafael Mun?oz. 2013.
UMCC DLSI-(EPS): Paraphrases detection based on
semantic distance. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?37, Uppsala, Sweden. Association for Com-
putational Linguistics.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. UNAL: Discriminating between literal
and figurative phrasal usage using distributional statis-
tics and POS tags. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio. As-
sociation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda C?elebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, ACL ?03, pages
375?382, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic Part-of-Speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Reda Siblini and Leila Kosseim. 2013. CLaC: Semantic
relatedness of words and phrases. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Atlanta, Georgia, USA.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Panayiotis
Bozanis and Elias N. Houstis, editors, Advances in In-
formatics, volume 3746, chapter 36, pages 382?392.
Springer Berlin Heidelberg, Berlin, Heidelberg.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1012?1022, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tim Van de Cruys, Stergos Afantenos, and Philippe
Muller. 2013. MELODI: Semantic similarity of words
and compositional phrases using latent vector weight-
ing. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.
Christian Wartena. 2013. HsH: Estimating semantic sim-
ilarity of words and short phrases with frequency nor-
malized distance measures. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Atlanta, Georgia, USA.
Eros Zanchetta and Marco Baroni. 2005. Morph-it!: A
free corpus-based morphological resource for the ital-
ian language. Corpus Linguistics 2005, 1(1).
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING).
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
Wikipedia and Wiktionary. Proceedings of the Confer-
ence on Language Resources and Evaluation (LREC),
15:60.
47
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 285?289, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis
Torsten Zesch? Omer Levy? Iryna Gurevych? Ido Dagan?
? Ubiquitous Knowledge Processing Lab ? Natural Language Processing Lab
Computer Science Department Computer Science Department
Technische Universita?t Darmstadt Bar-Ilan University
Abstract
Our system combines text similarity measures
with a textual entailment system. In the main
task, we focused on the influence of lexical-
ized versus unlexicalized features, and how
they affect performance on unseen questions
and domains. We also participated in the pi-
lot partial entailment task, where our system
significantly outperforms a strong baseline.
1 Introduction
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (Dzikovska
et al, 2013) brings together two important dimen-
sions of Natural Language Processing: real-world
applications and semantic inference technologies.
The challenge focuses on the domain of middle-
school quizzes, and attempts to emulate the metic-
ulous marking process that teachers do on a daily
basis. Given a question, a reference answer, and a
student?s answer, the task is to determine whether
the student answered correctly. While this is not
a new task in itself, the challenge focuses on em-
ploying textual entailment technologies as the back-
bone of this educational application. As a conse-
quence, we formalize the question ?Did the student
answer correctly?? as ?Can the reference answer be
inferred from the student?s answer??. This question
can (hopefully) be answered by a textual entailment
system (Dagan et al, 2009).
The challenge contains two tasks: In the main
task, the system must analyze each answer as a
whole. There are three settings, where each one de-
fines ?correct? in a different resolution. The highest-
resolution setting defines five different classes or
?correctness values?: correct, partially correct, con-
tradictory, irrelevant, non-domain. In the pilot task,
critical elements of the answer need to be analyzed
separately. Each such element is called a facet, and
is defined as a pair of words that are critical in an-
swering the question. As there is a substantial dif-
ference between the two tasks, we designed sibling
architectures for each task, and divide the main part
of the paper accordingly.
Our goal is to provide a robust architecture for stu-
dent response analysis, that can generalize and per-
form well in multiple domains. Moreover, we are
interested in evaluating how well general-purpose
technologies will perform in this setting. We there-
fore approach the challenge by combining two such
technologies: DKPro Similarity ?an extensive suite
of text similarity measures? that has been success-
fully applied in other settings like the SemEval 2012
task on semantic textual similarity (Ba?r et al, 2012a)
or reuse detection (Ba?r et al, 2012b).
BIUTEE, the Bar-Ilan University Textual Entail-
ment Engine (Stern and Dagan, 2011), which has
shown state-of-the-art performance on recognizing
textual entailment challenges. Our systems use both
technologies to extract features, and combine them
in a supervised model. Indeed, this approach works
relatively well (with respect to other entries in the
challenge), especially in unseen domains.
2 Background
2.1 Text Similarity
Text similarity is a bidirectional, continuous func-
tion which operates on pairs of texts of any length
and returns a numeric score of how similar one text
is to the other. In previous work (Mihalcea et al,
285
2006; Gabrilovich and Markovitch, 2007; Landauer
et al, 1998), only a single text similarity measure
has typically been applied to text pairs. However,
as recent work (Ba?r et al, 2012a; Ba?r et al, 2012b)
has shown, text similarity computation can be much
improved when a variety of measures are combined.
In recent years, UKP lab at TU Darmstadt has de-
veloped DKPro Similarity1, an open source toolkit
for analyzing text similarity. It is part of the
DKPro framework for natural language processing
(Gurevych et al, 2007). DKPro Similarity excels
at the tasks of measuring semantic textual simi-
larity (STS) and detecting text reuse (DTR), hav-
ing achieved the best performance in previous chal-
lenges (Ba?r et al, 2012a; Ba?r et al, 2012b).
2.2 Textual Entailment
The textual entailment paradigm is a generic frame-
work for applied semantic inference (Dagan et al,
2009). The most prevalent task of textual entailment
is to recognize whether the meaning of a target nat-
ural language statement (H for hypothesis) can be
inferred from another piece of text (T for text). Ap-
parently, this core task underlies semantic inference
in many text applications. The task of analyzing stu-
dent responses is one such example. By assigning
the student?s answer as T and the reference answer
as H , we are basically asking whether one can in-
fer the correct (reference) answer from the student?s
response. In recent years, Bar-Ilan University has
developed BIUTEE (Stern and Dagan, 2011), an ex-
tensive textual entailment recognition engine. BI-
UTEE tries to convert T (represented as a depen-
dency tree) to H . It does so by applying a series of
knowledge-based transformations, such as synonym
substitution, active-passive conversion, and more.
BIUTEE is publicly available as open source.2
3 Main Task
In this section, we explain how we approached the
main task, in which the system needs to analyze each
answer as a whole. After describing our system?s ar-
chitecture, we explain how we selected training data
for the different scenarios in the main task. We then
1code.google.com/p/dkpro-similarity-asl
2cs.biu.ac.il/?nlp/downloads/biutee
provide the details for each submitted run, and fi-
nally, our empirical results.
3.1 System Description
We build a system based on the Apache UIMA
framework (Ferrucci and Lally, 2004) and DKPro
Lab (Eckart de Castilho and Gurevych, 2011). We
use DKPro Core3 for preprocessing. Specifically,
we used the default DKPro segmenter, TreeTagger
POS tagger and chunker, Jazzy Spell Checker, and
the Stanford parser.4 We trained a supervised model
(Naive Bayes) using Weka (Hall et al, 2009) with
feature extraction based on clearTK (Ogren et al,
2008). The following features have been used:
BOW features Bag-of-word features are based on
the assumption that certain words need to appear in
a correct answer. We used a mixture of token uni-
grams, bigrams, and trigrams, where each n-gram is
a binary feature that can either be true or false for a
document.5 Additionally, we also used the number
of tokens in the student answer as another feature in
this group.
Syntactic Features We extend BOW features
with syntactic functions by adding dependency and
phrase n-grams. Dependency n-grams are combina-
tions of two tokens and their dependency relation.
Phrase n-grams are combinations of the main verb
and the noun phrase left and right of the verb. In
both cases, we use the 10 most frequent n-grams.
Basic Similarity Features This group of features
computes the similarity between the reference an-
swer and the student answer. In case there is more
than one reference answer, we compute all pairwise
similarity scores and add the minimum, maximum,
average, and median similarity.6
Semantic Similarity Features are very similar to
the basic similarity features, except that we use se-
mantic similarity measures in order to bridge a pos-
sible vocabulary gap between the student and refer-
ence answer. We use the ESA measure (Gabrilovich
3code.google.com/p/dkpro-core-asl/
4DKPro Core v1.4.0, TreeTagger models v20130204.0,
Stanford parser PCFG model v20120709.0.
5Using the 750 most frequent n-grams gave good results on
the training set, so we also used this number for the test runs.
6As basic similarity measures, we use greedy string tiling
(Wise, 1996) with n = 3, longest common subsequence and
longest common substring (Allison and Dix, 1986), and word
n-gram containment(Lyon et al, 2001) with n = 2.
286
and Markovitch, 2007) based on concept vectors
build from WordNet, Wiktionary, and Wikipedia.
Spelling Features As spelling errors might be in-
dicative of the answer quality, we use the number of
spelling errors normalized by the text length as an
additional feature.
Entailment Features We run BIUTEE (Stern and
Dagan, 2011) on the test instance (as T ) with each
reference answer (as H), which results in an array
of numerical entailment confidence values. If there
is more than one reference answer, we compute all
pairwise confidence scores and add the minimum,
maximum, average, and median confidence.
3.2 Data Selection Regime
There are three scenarios under which our system
is expected to perform. For each one, we chose (a-
priori) a different set of examples for training.
Unseen Answers Classify new answers to famil-
iar questions. Train on instances that have the same
question as the test instance.
Unseen Questions Classify new answers to un-
seen (but related) questions. Partition the questions
according to their IDs, creating sets of related ques-
tions, and then train on all the instances that share
the same partition as the test instance.
Unseen Domains Classify new answers to unseen
questions from unseen domains. Use all available
training data from the same dataset.
3.3 Submitted Runs
The runs represent the different levels of lexicaliza-
tion of the model which we expect to have strong
influence in each scenario:
Run 1 uses all features as described above. We
expect the BOW features to be helpful for the Un-
seen Answers setting, but to be misleading for un-
seen questions or domains, as the same word indi-
cating a correct answer for one question might cor-
respond to a wrong answer for another question.
Run 2 uses only non-lexicalized features, i.e. all
features except the BOW and syntactic features, in
order to assess the impact of the lexicalization that
overfits on the topic of the questions. We expect this
run to be less sensitive to the topic changes in the
Unseen Questions and Unseen Domains settings.
Run 3 uses only the basic similarity and the en-
tailment features. It should indicate the baseline per-
Unseen Unseen Unseen
Task Run Answers Questions Domains
2-way
1 .734 .678 .671
2 .665 .644 .677
3 .662 .625 .677
3-way
1 .670 .573 .572
2 .595 .561 .577
3 .574 .540 .576
5-way
1 .590 .376 .407
2 .495 .397 .371
3 .461 .394 .376
Table 1: Main task performance for the SciEntsBank test
set. We show weighted averageF1 for the three scenarios.
Cor. Par Con. Irr. Non.
Correct 903 463 164 309 78
Partially Correct 219 261 93 333 80
Contradictory 61 126 91 103 36
Irrelevant 209 229 119 476 189
Non-Domain 0 0 0 2 18
Table 2: Confusion matrix of Run 1 in the 5-way Unseen
Domains scenario. The vertical axis is the real class, the
horizontal axis is the predicted class.
formance that can be expected without targeting the
system towards a certain topic.
3.4 Empirical Results
Table 1 shows the F1-measure (weighted average)
of the three runs. As was expected for the Unseen
Answers scenario, Run 1 using a lexicalized model
outperformed the other two runs. However, in the
other scenarios Run 1 is not significantly better, as
lexicalized features do not have the same impact if
the question or the domain changes.
Table 2 shows the confusion matrix of Run 1 in
the 5-way Unseen Domains scenario. The Correct
category was classified quite reliably, but the Irrele-
vant category was especially hard. While the refer-
ence answer provides some clues for what is correct
or incorrect, the range of things that are ?irrelevant?
for a given question is potentially very big and thus
cannot be easily learned. We also see that the system
ability to distinguish Correct and Partially Correct
answers need to be improved.
It is difficult to provide an exact assessment of our
system?s performance (with respect to other systems
in the challenge), since there are multiple tasks, sce-
287
narios, datasets, and even metrics. However, we can
safely say that our system performed above average
in most settings, and showed competitive results in
the Unseen Domains scenario.
4 Pilot Task
In the pilot task each facet needs to be analyzed sep-
arately, which requires some changes in the system
architecture.
4.1 System Description
We segmented and lemmatized the input data using
the default DKPro segmenter and the TreeTagger
lemmatizer. The partial entailment system is com-
posed of three methods: Exact, WordNet, and BI-
UTEE. These were combined in different combina-
tions to form the different runs.
Exact In this baseline method, we represent a
student answer as a bag-of-words containing all to-
kens and lemmas appearing in the text. Lemmas
are used to account for minor morphological dif-
ferences, such as tense or plurality. We then check
whether both facet words appear in the set.
WordNet checks whether both facet words, or
their semantically related words, appear in the stu-
dent?s answer. We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term as matched if the similarity
score exceeds a certain threshold (0.9, empirically
determined on the training set).
BIUTEE processes dependency trees instead of
bags of words. We therefore added a pre-processing
stage that extracts a path in the dependency parse
that represents the facet. This is done by first pars-
ing the entire reference answer, and then locating the
two nodes mentioned in the facet. We then find their
lowest common ancestor (LCA), and extract the path
from the facet?s first word to the second through the
LCA. BIUTEE can now be given the student an-
swer and the pre-processed facet, and try to recog-
nize whether the former entails the latter.
4.2 Submitted Runs
In preliminary experiments using the provided train-
ing data, we found that the very simple Exact Match
baseline performed surprisingly well, with 0.96 pre-
cision and 0.32 recall on positive class instances (ex-
pressed facets). We therefore decided to use this fea-
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
Run 1 .756 .710 .760
Run 2 .782 .765 .816
Run 3 .744 .733 .770
Table 3: Pilot task performance across different scenar-
ios. The scores are F1-measures (weighted average).
ture as an initial filter, and employ the others for
classifying the ?harder? cases. Training BIUTEE
only on these cases, while dismissing easy ones, im-
proved our system?s performance significantly.
Run 1: Exact OR WordNet This is essentially
just the WordNet feature on its own, because every
instance that Exact classifies as positive is also pos-
itive by WordNet.
Run 2: Exact OR (BIUTEE AND WordNet) If
the instance is non-trivial, this configuration requires
that both BIUTEE and WordNet Match agree on pos-
itive classification. Equivalent to the majority rule.
Run 3: Exact OR BIUTEE BIUTEE increases
recall of expressed facets at the expense of precision.
4.3 Empirical Results
Table 3 shows the F1-measure (weighted average) of
each run in each scenario, including Exact Match as
a quite strong baseline. In the majority of cases, Run
2 that combines entailment and WordNet-based lex-
ical matching, significantly outperformed the other
two. It is interesting to note that the systems? perfor-
mance does not degrade in ?harder? scenarios; this is
a result of the non-lexicalized nature of our methods.
Unfortunately, our system was the only submission
in this track, so we do not have any means to perform
relative comparison.
5 Conclusion
We combined semantic textual similarity with tex-
tual entailment to solve the problem of student re-
sponse analysis. Though our features were not tai-
lored for this task, they proved quite indicative, and
adapted well to unseen domains. We believe that ad-
ditional generic features and knowledge resources
are the best way to improve on our results, while
retaining the same robustness and generality as our
current architecture.
288
Acknowledgements
This work has been supported by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant No.
I/82806, and by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant agreement no.
287923 (EXCITEMENT). We would like to thank the Minerva
Foundation for facilitating this cooperation with a short term
research grant.
References
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012a. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation and the 1st Joint Confer-
ence on Lexical and Computational Semantics, pages
435?440, June.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2012b.
Text reuse detection using a composition of text sim-
ilarity measures. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING 2012), pages 167?184, December.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rationale,
evaluation and approaches. Natural Language Engi-
neering, 15(4):i?xvii.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A lightweight framework for reproducible parame-
ter sweeping in information retrieval. In Proceed-
ings of the 2011 workshop on Data infrastructurEs for
supporting information retrieval evaluation (DESIRE
?11), New York, NY, USA. ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3-4):327?348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence (IJCAI 2007), pages 1606?1611.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
based on UIMA. In Proceedings of the 1st Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technology,
Tu?bingen, Germany, April.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2&3):259?284.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
6th Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 118?125,
Pittsburgh, PA USA.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775?780, Boston, MA.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA Toolkit for Statistical Nat-
ural Language Processing. In Towards Enhanced
Interoperability for Large HLT Systems: UIMA for
NLP workshop at Language Resources and Evaluation
Conference (LREC).
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence (IJCAI 1995), pages 448?453.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Processing
(RANLP 2011), pages 455?462.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE Technical Sympo-
sium on Computer Science Education (SIGCSE 1996),
pages 130?134, Philadelphia, PA.
289
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 30?39,
Dublin, Ireland, August 23-24 2014.
Sense and Similarity: A Study of Sense-level Similarity Measures
Nicolai Erbs
?
, Iryna Gurevych
??
and Torsten Zesch
?
? UKP Lab, Technische Universit?at Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we investigate the differ-
ence between word and sense similarity
measures and present means to convert
a state-of-the-art word similarity measure
into a sense similarity measure. In or-
der to evaluate the new measure, we cre-
ate a special sense similarity dataset and
re-rate an existing word similarity dataset
using two different sense inventories from
WordNet and Wikipedia. We discover
that word-level measures were not able
to differentiate between different senses
of one word, while sense-level measures
actually increase correlation when shift-
ing to sense similarities. Sense-level sim-
ilarity measures improve when evaluated
with a re-rated sense-aware gold standard,
while correlation with word-level similar-
ity measures decreases.
1 Introduction
Measuring similarity between words is a very im-
portant task within NLP with applications in tasks
such as word sense disambiguation, information
retrieval, and question answering. However, most
of the existing approaches compute similarity on
the word-level instead of the sense-level. Conse-
quently, most evaluation datasets have so far been
annotated on the word level, which is problem-
atic as annotators might not know some infrequent
senses and are influenced by the more probable
senses. In this paper, we provide evidence that this
process heavily influences the annotation process.
For example, when people are presented the word
pair jaguar - gamepad only few people know that
Jaguar
Gamepad
Zoo
.0070.0016
.0000
Figure 1: Similarity between words.
jaguar is also the name of an Atari game console.
1
People rather know the more common senses of
jaguar, i.e. the car brand or the animal. Thus, the
word pair receives a low similarity score, while
computational measures are not so easily fooled
by popular senses. It is thus likely that existing
evaluation datasets give a wrong picture of the true
performance of similarity measures.
Thus, in this paper we investigate whether sim-
ilarity should be measured on the sense level. We
analyze state-of-the-art methods and describe how
the word-based Explicit Semantic Analysis (ESA)
measure (Gabrilovich and Markovitch, 2007) can
be transformed into a sense-level measure. We
create a sense similarity dataset, where senses are
clearly defined and evaluate similarity measures
with this novel dataset. We also re-annotate an ex-
isting word-level dataset on the sense level in order
to study the impact of sense-level computation of
similarity.
2 Word-level vs. Sense-level Similarity
Existing measures either compute similarity (i) on
the word level or (ii) on the sense level. Similarity
on the word level may cover any possible sense of
the word, where on the sense level only the actual
sense is considered. We use Wikipedia Link Mea-
1
If you knew that it is a certain sign that you are getting
old.
30
Atari Jaguar Jaguar (animal)
Gamepad
Zoo
.0000.0321 .0341.0000
.0000
Figure 2: Similarity between senses.
sure (Milne, 2007) and Lin (Lin, 1998) as exam-
ples of sense-level similarity measures
2
and ESA
as the prototypical word-level measure.
3
The Lin measure is a widely used graph-based
similarity measure from a family of similar ap-
proaches (Budanitsky and Hirst, 2006; Seco et al.,
2004; Banerjee and Pedersen, 2002; Resnik, 1999;
Jiang and Conrath, 1997; Grefenstette, 1992). It
computes the similarity between two senses based
on the information content (IC) of the lowest com-
mon subsumer (lcs) and both senses (see For-
mula 1).
sim
lin
=
2 IC(lcs)
IC(sense1) + IC(sense2)
(1)
Another type of sense-level similarity measure
is based on Wikipedia that can also be considered a
sense inventory, similar to WordNet. Milne (2007)
uses the link structure obtained from articles to
count the number of shared incoming links of ar-
ticles. Milne and Witten (2008) give a more effi-
cient variation for computing similarity (see For-
mula 2) based on the number of links for each ar-
ticle, shared links |A ?B| and the total number of
articles in Wikipedia|W |.
sim
LM
=
logmax(|A| ,|B|)? log|A ?B|
log|W | ? logmin(|A| ,|B|)
(2)
All sense-level similarity measures can be con-
verted into a word similarity measure by comput-
ing the maximum similarity between all possible
sense pairs. Formula 3 shows the heuristic, with
S
n
being the possible senses for word n, sim
w
the
word similarity, and sim
s
the sense similarity.
sim
w
(w
1
, w
2
) = max
s
1
?S
1
,s
2
?S
2
sim
s
(s1, s2) (3)
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007) is a widely used word-level
2
We selected these measures because they are intuitive but
still among the best performing measures.
3
Hassan and Mihalcea (2011) classify these measures as
corpus-based and knowledge-based.
similarity measure based on Wikipedia as a back-
ground document collection. ESA constructs a n-
dimensional space, where n is the number of arti-
cles in Wikipedia. A word is transformed in a vec-
tor with the length n. Values of the vector are de-
termined by the term frequency in the correspond-
ing dimension, i.e. in a certain Wikipedia article.
The similarity of two words is then computed as
the inner product (usually the cosine) of the two
word vectors.
We now show how ESA can be adapted success-
fully to work on the sense-level, too.
2.1 DESA: Disambiguated ESA
In the standard definintion, ESA computes the
term frequency based on the number of times a
term?usually a word?appears in a document. In
order to make it work on the sense level, we will
need a large sense-disambiguated corpus. Such
a corpus could be obtained by performing word
sense disambiguating (Agirre and Edmonds, 2006;
Navigli, 2009) on all words. However, as this
is an error-prone task and we are more inter-
ested to showcase the overall principle, we rely on
Wikipedia as an already manually disambiguated
corpus. Wikipedia is a highly linked resource and
articles can be considered as senses.
4
We ex-
tract all links from all articles, with the link tar-
get as the term. This approach is not restricted
to Wikipedia, but can be applied to any resource
containing connections between articles, such as
Wiktionary (Meyer and Gurevych, 2012b). An-
other reason to select Wikipedia as a corpus is that
it will allow us to directly compare similarity val-
ues with the Wikipedia Link Measure as described
above.
After this more high-level introduction, we now
focus on the mathematical foundation of ESA and
disambiguated ESA (called ESA on senses). ESA
and ESA on senses count the frequency of each
term (or sense) in each document. Table 1 shows
the corresponding term-document matrix for the
example in Figure 1. The term Jaguar appears in
all shown documents, but the term Zoo appears in
the articles Dublin Zoo and Wildlife Park.
5
A man-
ual analysis shows that Jaguar appears with differ-
ent senses in the articles D-pad
6
and Dublin Zoo.
4
Wikipedia also contains pages with a list of possible
senses called disambiguation pages, which we filter.
5
In total it appears in 30 articles but we shown only few
example articles.
6
A D-pad is a directional pad for playing computer games.
31
Articles Terms
Jaguar Gamepad Zoo
# articles 3,496 30 7,553
Dublin Zoo 1 0 25
Wildlife Park 1 0 3
D-pad 1 0 0
Gamepad 4 1 0
... ... ... ...
Table 1: Term-document-matrix for frequencies in
a corpus if words are used as terms
Articles Terms
Atari
Gamepad
Jaguar
Zoo
Jaguar (animal)
# articles 156 86 578 925
Dublin Zoo 0 0 2 1
Wildlife Park 0 0 1 1
D-pad 1 1 0 0
Gamepad 1 0 0 0
... ... ... ... ...
Table 2: Term-document-matrix for frequencies in
a corpus if senses are used as terms
By comparing the vectors without any modifi-
cation, we see that the word pairs Jaguar?Zoo
and Jaguar?Gamepad have vector entries for the
same document, thus leading to a non-zero simi-
larity. Vectors for the terms Gamepad and Zoo do
not share any documents, thus leading to a simi-
larity of zero.
Shifting from words to senses changes term fre-
quencies in the term-document-matrix in Table 2.
The word Jaguar is split in the senses Atari Jaguar
and Jaguar (animal). Overall, the term-document-
matrix for the sense-based similarity shows lower
frequencies, usually zero or one because in most
cases one article does not link to another article or
exactly once. Both senses of Jaguar do not appear
in the same document, hence, their vectors are or-
thogonal. The vector for the term Gamepad dif-
fers from the vector for the same term in Table 1.
This is due to two effects: (i) There is no link from
the article Gamepad to itself, but the term is men-
tioned in the article and (ii) there exists a link from
the article D-pad to Gamepad, but using another
term.
The term-document-matrices in Table 1 and 2
show unmodified frequencies of the terms. When
comparing two vectors, both are normalized in a
prior step. Values can be normalized by the inverse
logarithm of their document frequency. Term fre-
quencies can also be normalized by weighting
them with the inverse frequency of links pointing
to an article (document or articles with many links
pointing to them receive lower weights as docu-
ments with only few incoming links.) We normal-
ize vector values with the inverse logarithm of ar-
ticle frequencies.
Besides comparing two vectors by measuring
the angle between them (cosine), we also experi-
ment with a language model variant. In the lan-
guage model variant we calculate for both vec-
tors the ratio of links they both share. The fi-
nal similarity value is the average for both vec-
tors. This is somewhat similar to the approach of
Wikipedia Link Measure by Milne (2007). Both
rely on Wikipedia links and are based on frequen-
cies of these links. We show that?although, ESA
and Link Measure seem to be very different?they
both share a general idea and are identical with a
certain configuration.
2.2 Relation to the Wikipedia Link Measure
Link Measure counts the number of incoming
links to both articles and the number of shared
links. In the originally presented formula by Milne
(2007) the similarity is the cosine of vectors for
incoming or outgoing links from both articles. In-
coming links are also shown in term-document-
matrices in Table 1 and 2, thus providing the same
vector information. In Milne (2007), vector values
are weighted by the frequency of each link normal-
ized by the logarithmic inverse frequency of links
pointing to the target. This is one of the earlier de-
scribed normalization approaches. Thus, we argue
that the Wikipedia Link Measure is a special case
of our more general ESA on senses approach.
3 Annotation Study I: Rating Sense
Similarity
We argue that human judgment of similarity be-
tween words is influenced by the most probable
sense. We create a dataset with ambiguous terms
and ask annotators to rank the similarity of senses
and evaluate similarity measures with the novel
dataset.
3.1 Constructing an Ambiguous Dataset
In this section, we discuss how an evaluation
dataset should be constructed in order to correctly
asses the similarity of two senses. Typically, eval-
uation datasets for word similarity are constructed
by letting annotators rate the similarity between
32
both words without specifying any senses for these
words. It is common understanding that anno-
tators judge the similarity of the combination of
senses with the highest similarity.
We investigate this hypothesis by constructing
a new dataset consisting of 105 ambiguous word
pairs. Word pairs are constructed by adding one
word with two clearly distinct senses and a second
word, which has a high similarity to only one of
the senses. We first ask two annotators
7
to rate the
word pairs on a scale from 0 (not similar at all) to 4
(almost identical). In the second round, we ask the
same annotators to rate 277 sense
8
pairs for these
word pairs using the same scale.
The final dataset thus consists of two levels:
(i) word similarity ratings and (ii) sense similarity
ratings. The gold ratings are the averaged ratings
of both annotators, resulting in an agreement
9
of
.510 (Spearman: .598) for word ratings and .792
(Spearman: .806) for sense ratings.
Table 3 shows ratings of both annotators for two
word pairs and ratings for all sense combinations.
In the given example, the word bass has the senses
of the fish, the instrument, and the sound. Anno-
tators compare the words and senses to the words
Fish and Horn, which appear only in one sense
(most frequent sense) in the dataset.
The annotators? rankings contradict the assump-
tion that the word similarity equals the similar-
ity of the highest sense. Instead, the highest
sense similarity rating is higher than the word
similarity rating. This may be caused?among
others?by two effects: (i) the correct sense is not
known or not recalled, or (ii) the annotators (un-
consciously) adjust their ratings to the probabil-
ity of the sense. Although, the annotation manual
stated that Wikipedia (the source of the senses)
could be used to get informed about senses and
that any sense for the words can be selected, we
see both effects in the annotators? ratings. Both
annotators rated the similarity between Bass and
Fish as very low (1 and 2). However, when asked
to rate the similarity between the sense Bass (Fish)
and Fish, both annotators rated the similarity as
high (4). Accordingly, for the word pair Bass and
7
Annotators are near-native speakers of English and have
university degrees in cultural anthropology and computer sci-
ence.
8
The sense of a word is given in parentheses but annota-
tors have access to Wikipedia to get information about those
senses.
9
We report agreement as Krippendorf ? with a quadratic
weight function.
Horn, word similarity is low (1) while the highest
sense frequency is medium to high (3 and 4).
3.2 Results & Discussion
We evaluated similarity measures with the previ-
ously created new dataset. Table 4 shows corre-
lations of similarity measures with human ratings.
We divide the table into measures computing sim-
ilarity on word level and on sense level. ESA
works entirely on a word level, Lin (WordNet)
uses WordNet as a sense inventory, which means
that senses differ across sense inventories.
10
ESA
on senses and Wikipedia Link Measure (WLM)
compute similarity on a sense-level, however, sim-
ilarity on a word-level is computed by taking the
maximum similarity of all possible sense pairs.
Results in Table 4 show that word-level mea-
sures return the same rating independent from the
sense being used, thus, they perform good when
evaluated on a word-level, but perform poorly
on a sense-level. For the word pair Jaguar?
Zoo, there exist two sense pairs Atari Jaguar?
Zoo and Jaguar (animal)?Zoo. Word-level mea-
sures return the same similarity, thus leading to
a very low correlation. This was expected, as
only sense-based similarity measures can discrim-
inate between different senses of the same word.
Somewhat surprisingly, sense-level measures per-
form also well on a word-level, but their per-
formance increases strongly on sense-level. Our
novel measure ESA on senses provides the best
results. This is expected as the ambiguous dataset
contains many infrequently used senses, which an-
notators are not aware of.
Our analysis shows that the algorithm for com-
paring two vectors (i.e. cosine and language
model) only influences results for ESA on senses
when computed on a word-level. Correlation for
Wikipedia Link Measure (WLM) differs depend-
ing on whether the overlap of incoming or outgo-
ing links are computed. WLM on word-level using
incoming links performs better, while the differ-
ence on sense-level evaluation is only marginal.
Results show that an evaluation on the level of
words and senses may influence performance of
measures strongly.
3.3 Pair-wise Evaluation
In a second experiment, we evaluate how well
sense-based measures can decide, which one of
10
Although, there exists sense alignment resources, we did
not use any alignment.
33
Annotator 1 Annotator 2
Word 1 Word 2 Sense 1 Sense 2 Words Senses Words Senses
Bass Fish
Bass (Fish)
Fish (Animal) 1
4
1
4
Bass (Instrument) 1 1
Bass (Sound) 1 1
Bass Horn
Bass (Fish)
Horn (Instrument) 2
1
1
1
Bass (Instrument) 3 4
Bass (Sound) 3 3
Table 3: Examples of ratings for two word pairs and all sense combinations with the highest ratings
marked bold
Word-level Sense-level
measure Spearman Pearson Spearman Pearson
Word measures
ESA .456 .239 -.001 .017
Lin (WordNet) .298 .275 .038 .016
Sense measures
ESA on senses (Cosine) .292 .272 .642 .348
ESA on senses (Lang. Mod.) .185 .256 .642 .482
WLM (out) .190 .193 .537 .372
WLM (in) .287 .279 .535 .395
Table 4: Correlation of similarity measures with a human gold standard of ambiguous word pairs.
two sense pairs for one word pair have a higher
similarity. We thus create for every word pair all
possible sense pairs
11
and count cases where one
measure correctly decides, which is the sense pair
with a higher similarity.
Table 5 shows evaluation results based on a
minimal difference between two sense pairs. We
removed all sense pairs with a lower difference
of their gold similarity. Column #pairs gives the
number of remaining sense pairs. If a measure
classifies two sense pairs wrongly, it may either
be because it rated the sense pairs with an equal
similarity or because it reversed the order.
Results show that accuracy increases with in-
creasing minimum difference between sense pairs.
Figure 3 emphasizes this finding. Overall, accu-
racy for this task is high (between .70 and .83),
which shows that all the measures can discrim-
inate sense pairs. WLM (out) performs best for
most cases with a difference in accuracy of up to
.06.
When comparing these results to results from
Table 4, we see that correlation does not imply
accurate discrimination of sense pairs. Although,
ESA on senses has the highest correlation to hu-
man ratings, it is outperformed by WLM (out) on
the task of discriminating two sense pairs. We see
that results are not stable across both evaluation
11
For one word pair with two senses for one word, there are
two possible sense pairs. Three senses result in three sense
pairs.
0.5 1 1.5 2 2.5 3 3.5 4
0.7
0.75
0.8
0.85
0.9
Min. judgement difference
A
c
c
u
r
a
c
y
ESA on senses
WLM (in)
WLM (out)
Figure 3: Accuracy distribution depending on
minimum difference of similarity ratings
scenarios, however, ESA on senses achieves the
highest correlation and performs similar to WLM
(out) when comparing sense pairs pair-wise.
4 Annotation Study II: Re-rating of
RG65
We performed a second evaluation study where we
asked three human annotators
12
to rate the similar-
ity of word-level pairs in the dataset by Rubenstein
and Goodenough (1965). We hypothesize that
measures working on the sense-level should have a
disadvantage on word-level annotated datasets due
to the effects described above that influence anno-
tators towards frequent senses. In our annotation
12
As before, all three annotators are near-native speakers of
English and have a university degree in physics, engineering,
and computer science.
34
Min. Wrong
diff. #pairs measure Correct Reverse Values equal Accuracy
0.5
420
ESA on senses 296 44 80 .70
WLM (in) 296 62 62 .70
WLM (out) 310 76 34 .74
1.0
390
ESA on senses 286 38 66 .73
WLM (in) 282 52 56 .72
WLM (out) 294 64 32 .75
1.5
360
ESA on senses 264 34 62 .73
WLM (in) 260 48 52 .72
WLM (out) 280 54 26 .78
2.0
308
ESA on senses 232 28 48 .75
WLM (in) 226 36 46 .73
WLM (out) 244 46 18 .79
2.5
280
ESA on senses 216 22 42 .77
WLM (in) 206 32 42 .74
WLM (out) 224 38 18 .80
3.0
174
ESA on senses 134 10 30 .77
WLM (in) 128 20 26 .74
WLM (out) 136 22 16 .78
3.50
68
ESA on senses 56 4 8 .82
WLM (in) 50 6 12 .74
WLM (out) 52 6 10 .76
4.0
12
ESA on senses 10 2 0 .83
WLM (in) 10 2 0 .83
WLM (out) 10 2 0 .83
Table 5: Pair-wise comparison of measures: Results for ESA on senses (language model) and ESA on
senses (cosine) do not differ
studies, our aim is to minimize the effect of sense
weights.
In previous annotation studies, human annota-
tors could take sense weights into account when
judging the similarity of word pairs. Addition-
ally, some senses might not be known by anno-
tators and, thus receive a lower rating. We min-
imize these effects by asking annotators to select
the best sense for a word based on a short summary
of the corresponding sense. To mimic this pro-
cess, we created an annotation tool (see Figure 4),
for which an annotator first selects senses for both
words, which have the highest similarity. Then the
annotator ranks the similarity of these sense pairs
based on the complete sense definition.
A single word without any context cannot be
disambiguated properly. However, when word
pairs are given, annotators first select senses based
on the second word, e.g. if the word pair is Jaguar
and Zoo, an annotator will select the wild animal
for Jaguar. After disambiguating, an annotator
assigns a similarity score based on both selected
senses. To facilitate this process, a definition of
each possible sense is shown.
As in the previous experiment, similarity is an-
notated on a five-point-scale from 0 to 4. Al-
though, we ask annotators to select senses for
word pairs, we retrieve only one similarity rating
for each word pair, which is the sense combination
with the highest similarity.
No sense inventory To compare our results with
the original dataset from Rubenstein and Goode-
nough (1965), we asked annotators to rate similar-
ity of word pairs without any given sense reposi-
tory, i.e. comparing words directly. The annota-
tors reached an agreement of .73. The resulting
gold standard has a high correlation with the orig-
inal dataset (.923 Spearman and .938 Pearson).
This is in line with our expectations and previous
work that similarity ratings are stable across time
(B?ar et al., 2011).
Wikipedia sense inventory We now use the full
functionality of our annotation tool and ask an-
notators to first, select senses for each word and
second, rate the similarity. Possible senses and
definitions for these senses are extracted from
Wikipedia.
13
The same three annotators reached
13
We use the English Wikipedia version from June 15
th
,
2010.
35
Figure 4: User interface for annotation studies: The example shows the word pair glass?tumbler with
no senses selected. The interface shows WordNet definitons of possible senses in the text field below the
sense selection. The highest similarity is selected as sense 4496872 for tumbler is a drinking glass.
an agreement of .66. The correlation to the orig-
inal dataset is lower than for the re-rating (.881
Spearman, .896 Pearson). This effect is due
to many entities in Wikipedia, which annotators
would typically not know. Two annotators rated
the word pair graveyard?madhouse with a rather
high similarity because both are names of music
bands (still no very high similarity because one is
a rock and the other a jazz band).
WordNet sense inventory Similar to the previ-
ous experiment, we list possible senses for each
word from a sense inventory. In this experiment,
we use WordNet senses, thus, not using any named
entity. The annotators reached an agreement of .73
and the resulting gold standard has a high correla-
tion with the original dataset (.917 Spearman and
.928 Pearson).
Figure 5 shows average annotator ratings in
comparison to similarity judgments in the origi-
nal dataset. All re-rating studies follow the general
tendency of having higher annotator judgments for
similar pairs. However, there is a strong fluctua-
tion in the mid-similarity area (1 to 3). This is due
to fewer word pairs with such a similarity.
4.1 Results & Discussion
We evaluate the similarity measures using Spear-
man and Pearson correlation with human similar-
0 1 2 3 4
0
2
4
Original similarity
S
i
m
i
l
a
r
i
t
y
j
u
d
g
e
m
e
n
t
s
None
Wikipedia
WordNet
Figure 5: Correlation curve of rerating studies
ity judgments. We calculate correlations to four
human judgments: (i) from the original dataset
(Orig.), (ii) from our re-rating study (Rerat.), (iii)
from our study with senses from Wikipedia (WP),
and (iv) with senses from WordNet (WN). Ta-
ble 6 shows results for all described similarity
measures.
ESA
14
achieves a Spearman correlation of .751
and a slightly higher correlation (.765) on our
re-rating gold standard. Correlation then drops
when compared to gold standards with senses
from Wikipedia and WordNet. This is expected
as the gold standard becomes more sense-aware.
Lin is based on senses in WordNet but still out-
14
ESA is used with normalized text frequencies, a constant
document frequency, and a cosine comparison of vectors.
36
Spearman Pearson
measure Orig. Rerat. WP WN Orig. Rerat. WP WN
ESA .751 .765 .704 .705 .647 .694 .678 .625
Lin .815 .768 .705 .775 .873 .840 .798 .846
ESA on senses (lang. mod.) .733 .765 .782 .751 .703 .739 .739 .695
ESA on senses (cosine) .775 .810 .826 .795 .694 .712 .736 .699
WLM (in) .716 .745 .754 .733 .708 .712 .740 .707
WLM (out) .583 .607 .652 .599 .548 .583 .613 .568
Table 6: Correlation of similarity measures with a human gold standard on the word pairs by Rubenstein
and Goodenough (1965). Best results for each gold standard are marked bold.
performs all other measures on the original gold
standard. Correlation reaches a high value for
the gold standard based on WordNet, as the same
sense inventory for human annotations and mea-
sure is applied. Values for Pearson correlation em-
phasizes this effect: Lin reaches the maximum of
.846 on the WordNet-based gold standard.
Correspondingly, the similarity measures ESA
on senses and WLM reach their maximum on
the Wikipedia-based gold standard. As for the
ambiguous dataset in Section 3 ESA on senses
outperforms both WLM variants. Cosine vector
comparison again outperforms the language model
variant for Spearman correlation but impairs it in
terms of Pearson correlation. As before WLM (in)
outperforms WLM (out) across all datasets and
both correlation metrics.
Is word similarity sense-dependent? In gen-
eral, sense-level similarity measures improve
when evaluated with a sense-aware gold standard,
while correlation with word-level similarity mea-
sures decreases. A further manual analysis shows
that sense-level measures perform good when rat-
ing very similar word pairs. This is very useful for
applications such as information retrieval where a
user is only interested in very similar documents.
Our evaluation thus shows that word similar-
ity should not be considered without considering
the effect of the used sense inventory. The same
annotators rate word pairs differently if they can
specify senses explicitly (as seen in Table 3). Cor-
respondingly, results for similarity measures de-
pend on which senses can be selected. Wikipedia
contains many entities, e.g. music bands or ac-
tors, while WordNet contains fine-grained senses
for things (e.g. narrow senses of glass as shown in
Figure 4). Using the same sense inventory as the
one, which has been used in the annotation pro-
cess, leads to a higher correlation.
5 Related Work
The work by Schwartz and Gomez (2011) is the
closest to our approach in terms of sense anno-
tated datasets. They compare several sense-level
similarity measures based on the WordNet taxon-
omy on sense-annotated datasets. For their ex-
periments, annotators were asked to select senses
for every word pair in three similarity datasets.
Annotators were not asked to re-rate the similar-
ity of the word pairs, or the sense pairs, respec-
tively. Instead, similarity judgments from the orig-
inal datasets are used. Possible senses are given by
WordNet and the authors report an inter-annotator
agreement of .93 for the RG dataset.
The authors then compare Spearman correlation
between human judgments and judgments from
WordNet-based similarity measures. They focus
on differences between similarity measures using
the sense annotations and the maximum value for
all possible senses. The authors do not report im-
provements across all measures and datasets. Of
ten measures and three datasets, using sense an-
notations, improved results in nine cases. In 16
cases, results are higher when using the maxi-
mum similarity across all possible senses. In five
cases, both measures yielded an equal correlation.
The authors do not report any overall tendency
of results. However, these experiments show that
switching from words to senses has an effect on
the performance of similarity measures.
The work by Hassan and Mihalcea (2011) is
the closest to our approach in terms of similarity
measures. They introduce Salient Semantic Anal-
ysis (SAS), which is a sense-level measure based
on links and disambiguated senses in Wikipedia
articles. They create a word-sense-matrix and
37
compute similarity with a modified cosine met-
ric. However, they apply additional normaliza-
tion factors to optimize for the evaluation metrics
which makes a direct comparison of word-level
and sense-level variants difficult.
Meyer and Gurevych (2012a) analyze verb sim-
ilarity with a corpus from Yang and Powers
(2006) based on the work by Zesch et al. (2008).
They apply variations of the similarity measure
ESA by Gabrilovich and Markovitch (2007) us-
ing Wikipedia, Wiktionary, and WordNet. Meyer
and Gurevych (2012a) report improvements us-
ing a disambiguated version of Wiktionary. Links
in Wiktionary articles are disambiguated and thus
transform the resource to a sense-based resource.
In contrast to our work, they focus on the simi-
larity of verbs (in comparison to nouns in this pa-
per) and it applies disambiguation to improve the
underlying resource, while we switch the level,
which is processed by the measure to senses.
Shirakawa et al. (2013) apply ESA for compu-
tation of similarities between short texts. Texts
are extended with Wikipedia articles, which is one
step to a disambiguation of the input text. They
report an improvement of the sense-extended ESA
approach over the original version of ESA. In con-
trast to our work, the text itself is not changed and
similarity is computed on the level of texts.
6 Summary and Future Work
In this work, we investigated word-level and
sense-level similarity measures and investigated
their strengths and shortcomings. We evaluated
how correlations of similarity measures with a
gold standard depend on the sense inventory used
by the annotators.
We compared the similarity measures ESA
(corpus-based), Lin (WordNet), and Wikipedia
Link Measure (Wikipedia), and a sense-enabled
version of ESA and evaluated them with a dataset
containing ambiguous terms. Word-level mea-
sures were not able to differentiate between dif-
ferent senses of one word, while sense-level mea-
sures could even increase correlation when shift-
ing to sense similarities. Sense-level measures ob-
tained accuracies between .70 and .83 when decid-
ing which of two sense pairs has a higher similar-
ity.
We performed re-rating studies with three an-
notators based on the dataset by Rubenstein and
Goodenough (1965). Annotators were asked to
first annotate senses from Wikipedia and Word-
Net for word pairs and then judge their similar-
ity based on the selected senses. We evaluated
with these new human gold standards and found
that correlation heavily depends on the resource
used by the similarity measure and sense reposi-
tory a human annotator selected. Sense-level sim-
ilarity measures improve when evaluated with a
sense-aware gold standard, while correlation with
word-level similarity measures decreases. Using
the same sense inventory as the one, which has
been used in the annotation process, leads to a
higher correlation. This has implications for cre-
ating word similarity datasets and evaluating sim-
ilarity measures using different sense inventories.
In future work we would like to analyze how
we can improve sense-level similarity measures by
disambiguating a large document collection and
thus retrieving more accurate frequency values.
This might reduce the sparsity of term-document-
matrices for ESA on senses. We plan to use
word sense disambiguation components as a pre-
processing step to evaluate whether sense simi-
larity measures improve results for text similarity.
Additionally, we plan to use sense alignments be-
tween WordNet and Wikipedia to enrich the term-
document matrix with additional links based on
semantic relations.
The datasets, annotation guidelines, and our ex-
perimental framework are publicly available in or-
der to foster future research for computing sense
similarity.
15
Acknowledgments
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
by the Klaus Tschira Foundation under project No.
00.133.2008, and by the German Federal Min-
istry of Education and Research (BMBF) within
the context of the Software Campus project open
window under grant No. 01IS12054. The au-
thors assume responsibility for the content. We
thank Pedro Santos, Mich`ele Spankus and Markus
B?ucker for their valuable contribution. We thank
the anonymous reviewers for their helpful com-
ments.
15
www.ukp.tu-darmstadt.de/data/
text-similarity/sense-similarity/
38
References
Eneko Agirre and Philip Edmonds. 2006. Word
Sense Disambiguation: Algorithms and Applica-
tions. Springer.
Satanjeev Banerjee and Ted Pedersen. 2002. An
Adapted Lesk Algorithm for Word Sense Disam-
biguation using WordNet. In Computational Lin-
guistics and Intelligent Text, pages 136?-145.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2011.
A Reflective View on Text Similarity. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 515?
520, Hissar, Bulgaria.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, pages 1606?1611.
Gregory Grefenstette. 1992. Sextant: Exploring Unex-
plored Contexts for Semantic Extraction from Syn-
tactic Analysis. In Proceedings of the 30th An-
nual Meeting of the Association for Computational
Linguistics, pages 324?-326, Newark, Delaware,
USA. Association for Computational Linguistics.
Samer Hassan and Rada Mihalcea. 2011. Semantic
Relatedness Using Salient Semantic Analysis. In
Proceedings of the 25th AAAI Conference on Artifi-
cial Intelligence, (AAAI 2011), pages 884?889, San
Francisco, CA, USA.
Jay J Jiang and David W Conrath. 1997. Seman-
tic Similarity based on Corpus Statistics and Lexi-
cal Taxonomy. In Proceedings of 10th International
Conference Research on Computational Linguistics,
pages 1?15.
Dekang Lin. 1998. An Information-theoretic Defini-
tion of Similarity. In In Proceedings of the Interna-
tional Conference on Machine Learning, volume 98,
pages 296?-304.
Christian M. Meyer and Iryna Gurevych. 2012a. To
Exhibit is not to Loiter: A Multilingual, Sense-
Disambiguated Wiktionary for Measuring Verb Sim-
ilarity. In Proceedings of the 24th International
Conference on Computational Linguistics, pages
1763?1780, Mumbai, India.
Christian M. Meyer and Iryna Gurevych. 2012b. Wik-
tionary: A new rival for expert-built lexicons? Ex-
ploring the possibilities of collaborative lexicogra-
phy. In Sylviane Granger and Magali Paquot, ed-
itors, Electronic Lexicography, chapter 13, pages
259?291. Oxford University Press, Oxford, UK,
November.
David Milne and Ian H Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the 17th ACM
Conference on Information and Knowledge Man-
agement, pages 509?-518.
David Milne. 2007. Computing Semantic Relatedness
using Wikipedia Link Structure. In Proceedings of
the New Zealand Computer Science Research Stu-
dent Conference.
Roberto Navigli. 2009. Word Sense Disambiguation:
A Survey. ACM Computing Surveys, 41(2):1?69.
Philip Resnik. 1999. Semantic Similarity in a Tax-
onomy: An Information-based Measure and its Ap-
plication to Problems of Ambiguity in Natural Lan-
guage. Journal of Artificial Intelligence Research,
11:95?130.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual Correlates of Synonymy. Communica-
tions of the ACM, 8(10):627?-633.
Hansen A Schwartz and Fernando Gomez. 2011. Eval-
uating Semantic Metrics on Tasks of Concept Simi-
larity. In FLAIRS Conference.
Nuno Seco, Tony Veale, and Jer Hayes. 2004. An
Intrinsic Information Content Metric for Semantic
Similarity in WordNet. In Proceedings of European
Conference for Artificial Intelligence, number Ic,
pages 1089?1093.
Masumi Shirakawa, Kotaro Nakayama, Takahiro Hara,
and Shojiro Nishio. 2013. Probabilistic Seman-
tic Similarity Measurements for Noisy Short Texts
using Wikipedia Entities. In Proceedings of the
22nd ACM International Conference on Information
& Knowledge Management, pages 903?908, New
York, New York, USA. ACM Press.
Dongqiang Yang and David MW Powers. 2006. Verb
Similarity on the Taxonomy of WordNet. In Pro-
ceedings of GWC-06, pages 121?-128.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, pages
861?867, Chicago, IL, USA.
39
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 302?306,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HOO 2012 Shared Task: UKP Lab System Description
Torsten Zesch?? and Jens Haase?
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universita?t Darmstadt
?Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
www.ukp.tu-darmstadt.de
Abstract
In this paper, we describe the UKP Lab system
participating in the HOO 2012 Shared Task on
preposition and determiner error correction.
Our focus was to implement a highly flexi-
ble and modular system which can be easily
augmented by other researchers. The system
might be used to provide a level playground
for subsequent shared tasks and enable further
progress in this important research field on top
of the state of the art identified by the shared
task.
1 Introduction
UKP Lab already participated in the previous HOO
Shared Task in 2011. Our knowledge-based system
(Zesch, 2011) was targeted towards detecting real-
word spelling errors, but performed also well on a
number of other error classes.1 However, it was not
competitive for article and preposition errors where
supervised systems based on confusion sets consti-
tute the state of the art. Thus, we tailor the HOO
2011 system towards correcting article and prepo-
sition errors, but also implement a supervised ap-
proach based on confusion sets (Golding and Sch-
abes, 1996; Jones and Martin, 1997; Carlson et al,
2001).
We decided to implement a basic system that
should be as flexible as possible and might serve as a
basis for experiments in future rounds of the shared
task. We also plan to model the most successful
1http://clt.mq.edu.au/research/projects/hoo/hoo2011/reports/
hoo2011-UDposter.pdf
systems in our framework as soon as the system de-
scriptions are made available.2 This might provide
a level playground for subsequent shared tasks and
enable real progress in this important field on top of
the state of the art identified by the HOO shared task.
2 Supervised Error Detection
We implement a generic framework for article and
preposition error detection based on the open-source
DKPro framework.3 DKPro is a collection of soft-
ware components for natural language processing
based on the Apache UIMA framework (Ferrucci
and Lally, 2004). It comes with a collection of
ready-made modules which can be combined to
form more complex applications.
Our goal is to develop a system which is as flex-
ible as possible with respect to (i) linguistic pre-
processing, (ii) the extraction of features, and (iii)
the applied classification method. We will make the
source code publicly available as part of the DKPro
infrastructure and hope that this will lower the ob-
stacles for participating in future rounds of the HOO
Shared Task.
We also provide a reference implementation of the
HOO 2012 experiments based on the DKPro Lab
framework (Eckart de Castilho and Gurevych, 2011)
which enables (i) parameter sweeping, (ii) modeling
of interdependent tasks (like e.g. training and test
cycles), (iii) generating performance reports, and
(iv) storing all experimental results in a convenient
manner.
2We invite other participating teams to help with this effort.
3http://code.google.com/p/dkpro-core-asl/
302
2.1 Linguistic Preprocessing
For our basic implementation, we only use a few
preprocessing steps. We tokenize and sentence split
the data with the default DKPro segmenter, and
then use TreeTagger (Schmid, 2004) to POS-tag and
chunk the sentences. However, the framework al-
lows the effortless addition of other preprocessing
components, e.g. parsing or named-entity recogni-
tion.
2.2 Feature Extraction
We implement a generic feature extraction process
based on the ClearTK project (Ogren et al, 2008).
ClearTK provides a set of highly flexible feature ex-
tractors that access the annotations (e.g. POS tags,
chunks, etc.) created by the linguistic preprocessing.
One important decision during training is to de-
cide which instances should be used for feature ex-
traction. In the simplest setting, each token is used to
generate an instance, but this would result in a very
high number of negative instances for every positive
instance. For the error classes RT/UT and RD/UD, a
more balanced distribution of instances can be easily
enforced by only creating a positive instance if the
token equals an element in the corresponding confu-
sion set. We create a negative instance by removing
or changing the article/preposition.
For articles, we use the confusion set:
{a, an, the, this}4
For prepositions, we use the confusion set:
{as, at, but, by, for, from, in, of, on, out,
over, since, than, to, up, with}
The confusion set is a parameter to the feature ex-
traction method and can be changed easily. This also
makes it possible to apply the framework to other er-
ror classes, e.g. for correcting frequently confused
words like (accept, except) or (than, then).
Table 1 lists the set of basic features implemented
in the reference system. As our goal was to imple-
ment a highly flexible system, we put more effort
in the overall architecture than in the feature engi-
neering. N-gram features are computed based on the
4In the official runs, an was not part of the confusion set, but
was specially handled in a post-processing step. In the current
version of the framework, we removed this heuristic and now
treat an as a normal part of the confusion set.
Google Web1T n-gram corpus (Brants and Franz,
2006) which is accessed using jWeb1T.5
The listed features can be improved in many
ways, e.g. the chunk feature could also encode the
type of the chunk. As the framework allows to easily
add new feature extractors, we are going to integrate
the most successful features from the shared task.
Due to the modular architecture of ClearTK, the im-
plemented feature extractors could even be re-used
for other classification tasks unrelated to spelling
correction.
2.3 Classification
ClearTK provides a wide range of adapters to well
known machine learning frameworks and classifica-
tion tools. As of April 2012, the following adapters
are supported:
? LIBSVM6
? MALLET7 (McCallum, 2002)
? OpenNLP Maxent8
? SVMlight9 (Joachims, 1999)
? SVMlight-TK10 (Moschitti, 2006)
? Weka11 (Hall et al, 2009)
As we can easily switch the classifier, we tried
a wide range of classifiers, but SVM worked gen-
erally best. For the official runs, we used SVM as
implemented in the Weka toolkit with the parameter
?BuildLogisticModels? which allows to base a de-
tection decision on the confidence of the classifier in
order to improve precision.
3 Knowledge-based Error Detection
Besides the supervised system described above, we
also apply our knowledge-based system from the
HOO 2011 Pilot Round (Zesch, 2011). We re-
implemented two state-of-the-art approaches: the
5code.google.com/p/jweb1t/
6http://www.csie.ntu.edu.tw/?cjlin/libsvm/
7http://mallet.cs.umass.edu/
8http://opennlp.apache.org/
9http://svmlight.joachims.org/
10http://disi.unitn.it/moschitti/Tree-Kernel.htm
11http://www.cs.waikato.ac.nz/ml/weka/
303
Name Description Range of Values / Examples
pos?2?1
The neighboring POS tags. For the example we
assume ?IN NNP DT NN VBD?.
IN-NNP
pos?2 IN
pos?1 NNP
pos+1 NN
pos+2 VBD
pos+1+2 NN-VBD
chunk?1
Whether the neighboring tokens are part of a chunk.
For the example, we assume ?in the [United States]?.
O
chunk+1 B
chunk+2 I
chunk+1+2 B-I
vowel+1 Whether the next token starts with a vowel or not. 0/1
cons+1 Whether the next token starts with a consonant or not. 0/1
sign+1 Any sign that is not an alphabetic character. 0/1
n-gram(t?1{x? y})
Let f(n-gram) be the frequency of the n-gram in a
certain corpus. All n-gram features are then computed
as f(xt+1t+2)f(t+1t+2) ?
f(yt+1t+2)
f(t+1t+2)
n-gram(?{the? a} big house?);
f(?the big house?) = 100;
f(?a big house?) = 50;
f(?big house?) = 1000;
100
1000 ?
50
1000 = 0.05
n-gram({x? y}t+1)
n-gram(t?1{x? y}t+1)
n-gram(t?2t?1{x? y})
n-gram({x? y}t+1t+2)
Table 1: List of features used for classification.
knowledge-based approach (Hirst and Budanitsky,
2005) and the statistical approach (Mays et al, 1991;
Wilcox-OHearn et al, 2008). Both approaches mea-
sure the contextual fitness of a word and the sur-
rounding context. For that purpose, the knowledge-
based approach computes the semantic relatedness
of a target word with all other words in a certain con-
text window. This approach is not suitable for cor-
recting article or preposition errors, as these word
classes are not linked to the context via lexical-
semantic relations. Thus, we only use the statisti-
cal approach that computes the probability of a sen-
tence based on a n-gram language model. We use
the Google Web1T n-gram data (Brants and Franz,
2006).
Although being generally applicable to article
and preposition errors, the statistical approach needs
some adaptations in order to achieve acceptable per-
formance. In the original definition, the approach
computes the probability of all alternative sentences
where the target word is replaced with a word from
the vocabulary that has low edit distance to the tar-
get word. This results in a very high false detection
rate. Thus, we (i) limit detections to positions where
an article or preposition is already present, and (ii)
select the substitution candidate not from all tokens
with low edit distance to the original token, but only
from the appropriate confusion set.
As the statistical approach is purely based on n-
gram frequencies, while this is only one feature of
the supervised approach, we expect the supervised
approach to outperform our adapted knowledge-
based system by a wide margin.
4 Experimental Setup
We model all experiment pipelines in the previously
described framework. As training data, we use the
publicly available Brown corpus (Francis W. Nelson
and Kuc?era, 1964), but limit training to 3,700 ran-
domly selected sentences in order to speed up the
training process.
4.1 Unofficial Runs
Due to technical problems, we were not able to sub-
mit all runs in time. We therefore report also unof-
ficial runs which we evaluated on the test data that
was available for participants for a limited amount
of time.12 Although we did not tailor the unofficial
runs in any way towards the test data, they have cer-
tainly a different status than the official runs. We do
not consider this as a major problem, as our basic
12HOO 2012 test data was subject to a strict license and
needed to be deleted after the evaluation period.
304
Detection Recognition Correction
Description Run P R F P R F P R F
Baselines
Always the - 7.09 6.13 6.58 7.09 6.13 6.58 2.00 1.69 1.81
Always of - 11.51 28.54 16.40 11.51 28.54 16.40 1.53 3.81 2.19
Unofficial
2011 Articles; ? = .005 - 9.62 8.47 9.00 9.62 8.47 9.00 0.96 0.85 0.90
2011 Prepositions; ? = .005 - 18.11 23.04 20.28 18.11 23.04 20.28 9.00 11.42 10.05
2012 Naive Bayes - 9.35 39.32 15.11 9.35 39.32 15.11 1.26 5.29 2.03
2012 SVM - 10.46 33.40 15.93 10.46 33.40 15.93 2.71 8.67 4.13
Official
?RD = 0.95; ?RT = 0.8 UD0 8.64 7.73 8.16 4.94 4.42 4.66 1.48 1.32 1.40
?RD = 0.8; ?RT = 0.7 UD1 8.36 15.45 10.85 4.18 7.73 5.43 1.19 2.21 1.55
?RD = 0.5; ?RT = 0.3 UD2 8.94 31.13 13.88 5.51 19.21 8.57 1.20 4.19 1.87
Table 2: HOO 2012 test data: Results (in %) for article and preposition errors combined.
feature set is not competitive with the best perform-
ing systems anyway.
We implemented two baseline systems, one for
articles and one for prepositions. The baselines re-
place every occurrence of an article/preposition with
the most frequent article/preposition from the confu-
sion set (the for articles, of for prepositions).
We also apply the adapted HOO 2011 statistical
approach in two versions as described above: one
adapted towards articles, and one adapted towards
prepositions.
Finally, we use the new framework for supervised
error correction based on the basic feature set de-
scribed above with two classifiers: Naive Bayes and
SVM as implemented in the Weka toolkit version
3.7.5. We treat the correction task as a multi-class
problem and only target the error classes RD, RT,
UD, UT. The remaining error classes MD and MT
(missing articles and prepositions) are more chal-
lenging, as it is less obvious how to create good
training data from a non-error annotated corpus.
4.2 Official Runs
The three runs that were officially submitted are
also based on the SVM implementation in Weka,
but we applied the parameter ?BuildLogisticMod-
els? which allows to base a detection decision on
the confidence of the classifier in order to improve
precision. We tuned parameters on the training data
and report three runs for the threshold combinations
(?RD, ?RT ) = (0.95, 0.8), (0.8, 0.7), and (0.5, 0.3).
5 Results
Table 2 summarizes the results of all runs. As ex-
pected, the basic feature set used in our experiments
is not competitive with the top-performing systems
in the shared task.13 However, some observations
can be made from the relative differences between
the scores. The thresholds applied in the official runs
are not working as expected, as precision is not in-
fluenced, while recall drops a lot. The HOO 2011
system based on the statistical approach performs
quite well for prepositions, but not for articles. Its
performance is comparable to the supervised runs,
but this is only due to the limited feature set used in
our experiment.
As mentioned above, our focus was to implement
a highly flexible and modular system for supervised
error correction which can be easily augmented by
other researchers. We plan to model the most suc-
cessful systems in our framework as soon as the sys-
tem descriptions are made available, and we invite
other participating teams to help with this effort. The
system might provide a level playground for subse-
quent shared tasks and enable further progress in this
important field of research.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806.
13The best performing systems achieve about 40% F-score
for detection, 35% for recognition, and 28% for correction. See
(Dale et al, 2012) for an overview of the results.
305
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Andrew J Carlson, Jeffrey Rosen, and Dan Roth. 2001.
Scaling Up Context-Sensitive Text Correction. In Pro-
ceedings of IAAI.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, Montreal,
Canada.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A lightweight framework for reproducible parame-
ter sweeping in information retrieval. In Proceed-
ings of the 2011 workshop on Data infrastructurEs for
supporting information retrieval evaluation (DESIRE
?11), New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information Pro-
cessing in the Corporate Research Environment. Nat-
ural Language Engineering, 10(3-4):327?348.
Francis W. Nelson and Henry Kuc?era. 1964. Man-
ual of information to accompany a standard corpus of
present-day edited American English, for use with dig-
ital computers.
Andrew R. Golding and Yves Schabes. 1996. Com-
bining Trigram-based and feature-based methods for
context-sensitive spelling correction. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 71?78, Morristown, NJ,
USA. Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Graeme Hirst and Alexander Budanitsky. 2005. Correct-
ing real-word spelling errors by restoring lexical cohe-
sion. Natural Language Engineering, 11(1):87?111,
March.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scholkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning.
Michael P Jones and James H Martin. 1997. Contex-
tual spelling correction using latent semantic analy-
sis. In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing, pages 166?173,
Morristown, NJ, USA. Association for Computational
Linguistics.
Eric Mays, Fred. J Damerau, and Robert L Mercer. 1991.
Context based spelling correction. Information Pro-
cessing & Management, 27(5):517?522.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proceedings of
the Eleventh International Conference on European
Association for Computational Linguistics, Trento,
Italy.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA Toolkit for Statistical Nat-
ural Language Processing. In Towards Enhanced
Interoperability for Large HLT Systems: UIMA for
NLP workshop at Language Resources and Evaluation
Conference (LREC).
Helmut Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit Vectors. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING 2004), Geneva,
Switzerland.
Amber Wilcox-OHearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction with
trigrams: A reconsideration of the Mays, Damerau,
and Mercer model. In Proceedings of the 9th inter-
national conference on Computational linguistics and
intelligent text processing (CICLing).
Torsten Zesch. 2011. Helping Our Own 2011: UKP
Lab System Description. In Proceedings of the Help-
ing Our Own Working Group Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 260?262.
306
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 143?148,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Automatic Generation of Challenging Distractors
Using Context-Sensitive Inference Rules
Torsten Zesch
Language Technology Lab
University of Duisburg-Essen
torsten.zesch@uni-due.de
Oren Melamud
Computer Science Department
Bar-Ilan University
melamuo@cs.biu.ac.il
Abstract
Automatically generating challenging dis-
tractors for multiple-choice gap-fill items
is still an unsolved problem. We propose
to employ context-sensitive lexical infer-
ence rules in order to generate distractors
that are semantically similar to the gap tar-
get word in some sense, but not in the par-
ticular sense induced by the gap-fill con-
text. We hypothesize that such distrac-
tors should be particularly hard to distin-
guish from the correct answer. We focus
on verbs as they are especially difficult to
master for language learners and find that
our approach is quite effective. In our test
set of 20 items, our proposed method de-
creases the number of invalid distractors in
90% of the cases, and fully eliminates all
of them in 65%. Further analysis on that
dataset does not support our hypothesis re-
garding item difficulty as measured by av-
erage error rate of language learners. We
conjecture that this may be due to limita-
tions in our evaluation setting, which we
plan to address in future work.
1 Introduction
Multiple-choice gap-fill items as illustrated in Fig-
ure 1 are frequently used for both testing lan-
guage proficiency and as a learning device. Each
item consists of a carrier sentence that provides
the context to a target word. The target word is
blanked and presented as one possible gap-fill an-
swer together with a certain number (usually 3)
of distractors. Given a desired target word, car-
rier sentences containing it can be automatically
selected from a corpus. Some methods even select
only sentences where the target word is used in a
certain sense (Liu et al., 2005). Then, the main
problem is to pick challenging distractors that are
Figure 1: Multiple-choice gap-fill item.
reasonably hard to distinguish from the correct an-
swer (i.e. the target word) on one hand, yet cannot
be considered as correct answers on the other.
In this paper we propose to generate distrac-
tors that are semantically similar to the gap tar-
get word in some sense, but not in the particu-
lar sense induced by the gap-fill context, thereby
making them difficult to distinguish from the tar-
get word. For example, the distractor gain in Fig-
ure 1 is semantically similar to acquire, but is not
appropriate in the particular context of purchasing
companies, and therefore has high distractive po-
tential. On the other hand, the distractor purchase
is a correct answer in this context and is therefore
an invalid distractor. To generate challenging dis-
tractors, we utilize context-sensitive lexical infer-
ence rules that can discriminate between appropri-
ate substitutes of a target word given its context
and other inappropriate substitutes.
In the next section, we give an overview of pre-
vious work in order to place our contribution into
context.
2 Previous Work
The process of finding good distractors involves
two steps: Candidate Selection controls the diffi-
culty of the items, while Reliability Checking en-
sures that the items remain solvable, i.e. it ensures
143
that there is only one correct answer. We note
that this work is focused on single-word distrac-
tors rather than phrases (Gates et al., 2011), and
only on target isolated carrier sentences rather than
longer texts as in (Mostow and Jang, 2012).
2.1 Candidates Selection
In some settings the set of possible distractors is
known in advance, e.g. the set of English prepo-
sitions in preposition exercises (Lee and Seneff,
2007) or a confusion set with previously known
errors like {two, too, to}. Sakaguchi et al. (2013)
use data from the Lang-8 platform (a corpus of
manually annotated errors
1
) in order to determine
typical learner errors and use them as distractors.
However, in the common setting only the target
word is known and the set of distractors needs to
be automatically generated.
Randomly selecting distractors is a valid strat-
egy (Mostow and Jang, 2012), but it is only suit-
able for the most beginner learners. More ad-
vanced learners can easily rule out distractors that
do not fit grammatically or are too unrelated se-
mantically. Thus, more advanced approaches usu-
ally employ basic strategies, such as choosing dis-
tractors with the same part-of-speech tag as the
target word, or distractors with a corpus frequency
comparable to the target word (Hoshino and Naka-
gawa, 2007) (based on the assumption that corpus
frequency roughly correlates with word difficulty).
Pino and Eskenazi (2009) use distractors that are
morphologically, orthographically, or phonetically
similar (e.g. bread ? beard).
Another approach used in previous works to
make distractors more challenging is utilizing the-
sauri (Sumita et al., 2005; Smith and Avinesh,
2010) or taxonomies (Hoshino and Nakagawa,
2007; Mitkov et al., 2009) to select words that are
semantically similar to the target word. In addi-
tion to the target word, some approaches also con-
sider the semantic relatedness of distractors with
the whole carrier sentence or paragraph (Pino et
al., 2008; Agarwal and Mannem, 2011; Mostow
and Jang, 2012), i.e. they pick distractors that are
from the same domain as the target word.
Generally, selecting more challenging distrac-
tors usually means making them more similar to
the target word. As this increases the probability
that a distractor might actually be another correct
answer, we need a more sophisticated approach for
1
http://cl.naist.jp/nldata/lang-8/
checking the reliability of the distractor set.
2.2 Reliability Checking
In order to make sure that there is only one correct
answer to a gap-fill item, there needs to be a way
to decide for each distractor whether it fits into the
context of the carrier sentence or not. In those
cases, where we have a limited list of potential tar-
get words and distractors, e.g. in preposition exer-
cises (Lee and Seneff, 2007), a supervised classi-
fier can be trained to do this job. Given enough
training data, this approach yields very high preci-
sion, but it cannot be easily applied to open word
classes like nouns or verbs, which are much larger
and dynamic in nature.
When we do not have a closed list of potential
distractors at hand, one way to perform reliabil-
ity checking is by considering collocations involv-
ing the target word (Pino et al., 2008; Smith and
Avinesh, 2010). For example, if the target word
is strong, we can find the collocation strong tea.
Then we can use powerful as a distractor because
it is semantically similar to strong, yet *powerful
tea is not a valid collocation. This approach is ef-
fective, but requires strong collocations to discrim-
inate between valid and invalid distractors. There-
fore it cannot be used with carrier sentences that
do not contain strong collocations, such as the sen-
tence in Figure 1.
Sumita et al. (2005) apply a simple web search
approach to judge the reliability of an item. They
check whether the carrier sentence with the target
word replaced by the distractor can be found on
the web. If such a sentence is found, the distrac-
tor is discarded. We note that the applicability of
this approach is limited, as finding exact matches
for such artificial sentences can be unlikely due
to sparseness of natural languages. Therefore not
finding an exact match does not necessarily rule
out the possibility of an invalid distractor.
3 Automatic Generation of Challenging
Distractors
Our goal is to automatically generate distractors
that are as ?close? to the target word as possible,
yet do not fit the carrier sentence context. To ac-
complish this, our strategy is to first generate a set
of distractor candidates, which are semantically
similar to the target word. Then we use context-
sensitive lexical inference rules to filter candidates
that fit the context, and thus cannot be used as dis-
144
tractors. In the remainder of this section we de-
scribe this procedure in more detail.
3.1 Context-Sensitive Inference Rules
A lexical inference rule ?LHS ? RHS?, such as
?acquire ? purchase?, specifies a directional in-
ference relation between two words (or terms). A
rule can be applied when its LHS matches a word
in a text T , and then that word is substituted for
RHS, yielding the modified text H . For example,
applying the rule above to ?Microsoft acquired
Skype?, yields ?Microsoft purchased Skype?. If the
rule is true then the meaning of H is inferred from
the meaning of T . A popular way to learn lex-
ical inference rules in an unsupervised setting is
by using distributional similarity models (Lin and
Pantel, 2001; Kotlerman et al., 2010). Under this
approach, target words are represented as vectors
of context features, and the score of a rule between
two target words is based on vector arithmetics.
One of the main shortcomings of such rules is
that they are context-insensitive, i.e. they have a
single score, which is not assessed with respect to
the concrete context T under which they are ap-
plied. However, the appropriateness of an infer-
ence rule may in fact depend on this context. For
example, ?Microsoft acquire Skype ? Microsoft
purchase Skype?, is an appropriate application of
the rule ?acquire ? purchase?, while ?Children
acquire skills ? Children purchase skills? is not.
To address this issue, additional models were in-
troduced that compute a different context-sensitive
score per each context T , under which it is applied
(Dinu and Lapata, 2010; Melamud et al., 2013).
In this work, we use the resource provided
by Melamud et al. (2013), which includes both
context-sensitive and context-insensitive rules for
over 2,000 frequent verbs.
2
We use these rules to
generate challenging distractors as we show next.
3.2 Distractor Selection & Reliability
We start with the following illustrative example to
motivate our approach. While the words purchase
and acquire are considered to be almost perfect
synonyms in sentences like Microsoft acquires
Skype and Microsoft purchases Skype, this is not
true for all contexts. For example, in Children
acquire skills vs. Children purchase skills, the
meaning is clearly not equivalent. These context-
dependent senses, which are particularly typical to
2
http://www.cs.biu.ac.il/nlp/downloads/wt-rules.html
Figure 2: Filtering context-insensitive substitu-
tions with context-sensitive ones in order to get
challenging distractors.
verbs, make it difficult for learners to understand
how to properly use these words.
Acquiring such fine-grained sense distinction
skills is a prerequisite for really competent lan-
guage usage. These skills can be trained and tested
with distractors, such as purchase in the exam-
ple above. Therefore, such items are good indi-
cators in language proficiency testing, and should
be specifically trained when learning a language.
To generate such challenging distractors, we
first use the context-insensitive rules, whose LHS
matches the carrier sentence target word, to create
a distractor candidate set as illustrated on the left-
hand side of Figure 2. We include in this set the
top-n inferred words that correspond to the high-
est rule scores. These candidate words are inferred
by the target word, but not necessarily in the par-
ticular context of the carrier sentence. Therefore,
we expect this set to include both correct answers,
which would render the item unreliable, as well
as good distractors that are semantically similar to
the target word in some sense, but not in the par-
ticular sense induced by the carrier sentence.
Next, we use context-sensitive rules to generate
a distractor black-list including the top-m words
that are inferred by the target word, but this time
taking the context of the carrier sentence into con-
sideration. In this case, we expect the words in
the list to comprise only the gap-fillers that fit the
given context as illustrated on the right-hand side
of Figure 2. Such gap-fillers are correct answers
and therefore cannot be used as distractors. Fi-
nally, we subtract the black-list distractors from
the initial distractor candidate set and expect the
remaining candidates to comprise only good dis-
tractors. We consider the candidates in this final
set as our generated distractors.
145
3.3 Distractor Ranking
In case our approach returns a large number of
good distractors, we should use ranking to select
the most challenging ones. A simple strategy is
to rely on the corpus frequency of the distractor,
where less frequent means more challenging as it
will not be known to the learner. However, this
tends to put a focus on the more obscure words
of the vocabulary while actually the more frequent
words should be trained more often. Therefore, in
this work we use the scores that were assigned to
the distractors by the context-insensitive inference
rules. Accordingly, the more similar a distractor is
to the target word, the higher rank it will get (pro-
vided that it was not in the distractor black-list).
4 Experiments & Results
In our experiments we wanted to test two hy-
potheses: (i) whether context-sensitive inference
rules are able to reliably distinguish between valid
and invalid distractors, and (ii) whether the gener-
ated distractors are more challenging for language
learners than randomly chosen ones.
We used the Brown corpus (Nelson Francis and
Kuc?era, 1964) as a source for carrier sentences and
selected medium-sized (5-12 tokens long) sen-
tences that contain a main verb. We then manu-
ally inspected this set, keeping only well-formed
sentences that are understandable by a general au-
dience without requiring too much context knowl-
edge. In a production system, this manual pro-
cess would be replaced by a sophisticated method
for obtaining good carrier sentences, but this is be-
yond the scope of this paper. Finally, for this ex-
ploratory study, we only used the first 20 selected
sentences from a much larger set of possible car-
rier sentences.
4.1 Reliability
Our first goal was to study the effectiveness of our
approach in generating reliable items, i.e. items
where the target word is the only correct answer.
In order to minimize impact of pre-processing and
lemmatization, we provided the context-sensitive
inference rules with correctly lemmatized carrier
sentences and marked the target verbs. We found
that we get better results when using a distractor
black-list that is larger than the distractor candi-
date set, as this more aggressively filters invalid
distractors. We used the top-20 distractor black-
list and top-10 distractor candidate set, which lead
Only valid distractors 13/20 (65%)
Mix of valid and invalid 5/20 (25%)
Only invalid distractors 2/20 (10%)
Table 1: Reliability of items after filtering
to generating on average 3.3 distractors per item.
All our generated distractors were checked by
two native English speakers. We count a distrac-
tor as ?invalid? if it was ruled out by at least one
annotator. Table 1 summarizes the results. We
found that in 13 of the 20 items (65%) all distrac-
tors generated by our approach were valid, while
only for 2 items all generated distractors were in-
valid. For the remaining 5 items, our approach re-
turned a mix of valid and invalid distractors. We
note that the unfiltered distractor candidate set al-
ways contained invalid distractors and in 90% of
the items it contained a higher proportion of in-
valid distractors than the filtered one. This sug-
gests that the context-sensitive inference rules are
quite effective in differentiating between the dif-
ferent senses of the verbs.
A main source of error are sentences that do not
provide enough context, e.g. because the subject
is a pronoun. In She [served] one four-year term
on the national committee, it would be acceptable
to insert sold in the context of a report on po-
litical corruption, but a more precise subject like
Barack Obama would render that reading much
more unlikely. Therefore, more emphasis should
be put on selecting better carrier sentences. Se-
lecting longer sentences that provide a richer con-
text would help to rule out more distractor candi-
dates and may also lead to better results when us-
ing the context-sensitive inference rules. However,
long sentences are also more difficult for language
learners, so there will probably be some trade-off.
A qualitative analysis of the results shows that
especially for verbs with clearly distinct senses,
our approach yields good results. For example
in He [played] basketball there while working to-
ward a law degree, our method generates the dis-
tractors compose and tune which are both related
to the ?play a musical instrument? sense. An-
other example is His petition [charged] mental
cruelty, where our method generates among oth-
ers the distractors pay and collect that are both re-
lated to the ?charge taxes? reading of the verb. The
ball [floated] downstream is an example where our
method did not work well. It generated the distrac-
tors glide and travel which also fit the context and
146
Group 1 Group 2
Control Items 0.24? 0.12 0.20? 0.12
Test Items 0.18? 0.17 0.18? 0.15
Table 2: Average error rates on our dataset
should thus not be used as distractors. The verb
float is different from the previous examples, as
all its dominant senses involve some kind of ?float-
ing? even if only metaphorically used. This results
in similar senses that are harder to differentiate.
4.2 Difficulty
Next, we wanted to examine whether our approach
leads to more challenging distractors. For that
purpose we removed the distractors that our an-
notators identified as invalid in the previous step.
We then ranked the remaining distractors accord-
ing to the scores assigned to them by the context-
sensitive inference rules and selected the top-3 dis-
tractors. If our method generated less than 3 dis-
tractors, we randomly generated additional dis-
tractors from the same frequency range as the tar-
get word.
We compared our approach with randomly se-
lected distractors that are in the same order of
magnitude with respect to corpus frequency as the
distractors generated by our method. This way we
ensure that a possible change in distractor diffi-
culty cannot simply be attributed to differences in
the learners? familiarity with the distractor verbs
due to their corpus frequency. We note that ran-
dom selection repeatedly created invalid distrac-
tors that we needed to manually filter out. This
shows that better methods for checking the relia-
bility of items like in our approach are definitely
required.
We randomly split 52 participants (all non-
natives) into two groups, each assigned with a dif-
ferent test version. Table 2 summarizes the results.
For both groups, the first 7 test items were identi-
cal and contained only randomly selected distrac-
tors. Average error rate for these items was 0.24
(SD 0.12) for the first group, and 0.20 (SD 0.12)
for the second group, suggesting that the results of
the two groups on the remaining items can be com-
pared meaningfully. The first group was tested
on the remaining 13 items with randomly selected
distractors, while the second group got the same
items but with distractors created by our method.
Contrary to our hypothesis, the average error
rate for both groups was equal (0.18, SD
1
=0.17,
SD
2
=0.15). One reason might be that the English
language skills of the participants (mostly com-
puter science students or faculty) were rather high,
close to the native level, as shown by the low error
rates. Furthermore, even if the participants were
more challenged by our distractors, they might
have been able to finally select the right answer
with no measurable effect on error rate. Thus, in
future work we want measure answer time instead
of average error rate, in order to counter this effect.
We also want to re-run the experiment with lower
grade students, who might not have mastered the
kind of sense distinctions that our approach is fo-
cused on.
5 Conclusions
In this paper we have tackled the task of generating
challenging distractors for multiple-choice gap-fill
items. We propose to employ context-sensitive
lexical inference rules in order to generate distrac-
tors that are semantically similar to the gap target
word in some sense, but not in the particular sense
induced by the gap-fill context.
Our results suggest that our approach is quite ef-
fective, reducing the number of invalid distractors
in 90% of the cases, and fully eliminating all of
them in 65%. We did not find a difference in aver-
age error rate between distractors generated with
our method and randomly chosen distractors from
the same corpus frequency range. We conjecture
that this may be due to limitations in the setup of
our experiment.
Thus, in future work we want to re-run the ex-
periment with less experienced participants. We
also wish to measure answer time in addition to
error rate, as the distractive powers of a gap-filler
might be reflected in longer answer times more
than in higher error rates.
Acknowledgements
We thank all participants of the gap-fill survey,
and Emily Jamison and Tristan Miller for their
help with the annotation study. This work was
partially supported by the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement no. 287923 (EXCITE-
MENT).
147
References
Manish Agarwal and Prashanth Mannem. 2011. Au-
tomatic Gap-fill Question Generation from Text
Books. In Proceedings of the Sixth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 56?64.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing Distributional Similarity in Context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1162?1172.
Donna Gates, Margaret Mckeown, Juliet Bey, Forbes
Ave, and Ross Hall. 2011. How to Generate
Cloze Questions from Definitions : A Syntactic Ap-
proach. In Proceedings of the AAAI Fall Symposium
on Question Generation, pages 19?22.
Ayako Hoshino and Hiroshi Nakagawa. 2007. As-
sisting Cloze Test Making with a Web Application.
In Proceedings of the Society for Information Tech-
nology and Teacher Education International Confer-
ence, pages 2807? 2814.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distribu-
tional Similarity for Lexical Inference. Natural Lan-
guage Engineering, 16(4):359?389.
John Lee and Stephanie Seneff. 2007. Automatic
Generation of Cloze Items for Prepositions. In
Proceedings of INTERSPEECH, pages 2173?2176,
Antwerp, Belgium.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining 2001.
Chao-lin Liu, Chun-hung Wang, and Zhao-ming Gao.
2005. Using Lexical Constraints to Enhance the
Quality of Computer-Generated Multiple-Choice
Cloze Items. Computational Linguistics and Chi-
nese Language Processing, 10(3):303?328.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A Two Level
Model for Context Sensitive Inference Rules. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1331?1340, Sofia, Bulgaria.
Ruslan Mitkov, Le An Ha, Andrea Varga, and Luz
Rello. 2009. Semantic Similarity of Distractors in
Multiple-choice Tests: Extrinsic Evaluation. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 49?56.
Jack Mostow and Hyeju Jang. 2012. Generating
Diagnostic Multiple Choice Comprehension Cloze
Questions. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 136?146, Stroudsburg, PA, USA.
W. Nelson Francis and Henry Kuc?era. 1964. Manual
of Information to Accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers.
Juan Pino and Maxine Eskenazi. 2009. Semi-
Automatic Generation of Cloze Question Distrac-
tors Effect of Students L1. In SLaTE Workshop on
Speech and Language Technology in Education.
Juan Pino, Michael Heilman, and Maxine Eskenazi.
2008. A Selection Strategy to Improve Cloze Ques-
tion Quality. In Proceedings of the Workshop on In-
telligent Tutoring Systems for Ill-Defined Domains
at the 9th Internationnal Conference on Intelligent
Tutoring Systems.
Keisuke Sakaguchi, Yuki Arase, and Mamoru Ko-
machi. 2013. Discriminative Approach to Fill-in-
the-Blank Quiz Generation for Language Learners.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 238?242, Sofia, Bulgaria.
Simon Smith and P V S Avinesh. 2010. Gap-fill Tests
for Language Learners: Corpus-Driven Item Gener-
ation. In Proceedings of ICON-2010: 8th Interna-
tional Conference on Natural Language Processing.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speakers?
Proficiency of English by Using a Test with
Automatically-generated Fill-in-the-blank Ques-
tions. In Proceedings of the second workshop on
Building Educational Applications Using NLP,
EdAppsNLP 05, pages 61?68, Stroudsburg, PA,
USA.
148
