Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 201?208,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning the Structure of Task-driven Human-Human Dialogs
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Giuseppe Di Fabbrizio
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
pino@research.att.com
Amanda Stent
Dept of Computer Science
Stony Brook University
Stony Brook, NY
stent@cs.sunysb.edu
Abstract
Data-driven techniques have been used
for many computational linguistics tasks.
Models derived from data are generally
more robust than hand-crafted systems
since they better reflect the distribution
of the phenomena being modeled. With
the availability of large corpora of spo-
ken dialog, dialog management is now
reaping the benefits of data-driven tech-
niques. In this paper, we compare two ap-
proaches to modeling subtask structure in
dialog: a chunk-based model of subdialog
sequences, and a parse-based, or hierarchi-
cal, model. We evaluate these models us-
ing customer agent dialogs from a catalog
service domain.
1 Introduction
As large amounts of language data have become
available, approaches to sentence-level process-
ing tasks such as parsing, language modeling,
named-entity detection and machine translation
have become increasingly data-driven and empiri-
cal. Models for these tasks can be trained to cap-
ture the distributions of phenomena in the data
resulting in improved robustness and adaptabil-
ity. However, this trend has yet to significantly
impact approaches to dialog management in dia-
log systems. Dialog managers (both plan-based
and call-flow based, for example (Di Fabbrizio and
Lewis, 2004; Larsson et al, 1999)) have tradition-
ally been hand-crafted and consequently some-
what brittle and rigid. With the ability to record,
store and process large numbers of human-human
dialogs (e.g. from call centers), we anticipate
that data-driven methods will increasingly influ-
ence approaches to dialog management.
A successful dialog system relies on the syn-
ergistic working of several components: speech
recognition (ASR), spoken language understand-
ing (SLU), dialog management (DM), language
generation (LG) and text-to-speech synthesis
(TTS). While data-driven approaches to ASR and
SLU are prevalent, such approaches to DM, LG
and TTS are much less well-developed. In on-
going work, we are investigating data-driven ap-
proaches for building all components of spoken
dialog systems.
In this paper, we address one aspect of this prob-
lem ? inferring predictive models to structure task-
oriented dialogs. We view this problem as a first
step in predicting the system state of a dialog man-
ager and in predicting the system utterance during
an incremental execution of a dialog. In particular,
we learn models for predicting dialog acts of ut-
terances, and models for predicting subtask struc-
tures of dialogs. We use three different dialog act
tag sets for three different human-human dialog
corpora. We compare a flat chunk-based model
to a hierarchical parse-based model as models for
predicting the task structure of dialogs.
The outline of this paper is as follows: In Sec-
tion 2, we review current approaches to building
dialog systems. In Section 3, we review related
work in data-driven dialog modeling. In Section 4,
we present our view of analyzing the structure of
task-oriented human-human dialogs. In Section 5,
we discuss the problem of segmenting and label-
ing dialog structure and building models for pre-
dicting these labels. In Section 6, we report ex-
perimental results on Maptask, Switchboard and a
dialog data collection from a catalog ordering ser-
vice domain.
2 Current Methodology for Building
Dialog systems
Current approaches to building dialog systems
involve several manual steps and careful craft-
ing of different modules for a particular domain
or application. The process starts with a small
scale ?Wizard-of-Oz? data collection where sub-
jects talk to a machine driven by a human ?behind
the curtains?. A user experience (UE) engineer an-
alyzes the collected dialogs, subject matter expert
interviews, user testimonials and other evidences
(e.g. customer care history records). This hetero-
geneous set of information helps the UE engineer
to design some system functionalities, mainly: the
201
semantic scope (e.g. call-types in the case of call
routing systems), the LG model, and the DM strat-
egy. A larger automated data collection follows,
and the collected data is transcribed and labeled by
expert labelers following the UE engineer recom-
mendations. Finally, the transcribed and labeled
data is used to train both the ASR and the SLU.
This approach has proven itself in many com-
mercial dialog systems. However, the initial UE
requirements phase is an expensive and error-
prone process because it involves non-trivial de-
sign decisions that can only be evaluated after sys-
tem deployment. Moreover, scalability is compro-
mised by the time, cost and high level of UE know-
how needed to reach a consistent design.
The process of building speech-enabled auto-
mated contact center services has been formalized
and cast into a scalable commercial environment
in which dialog components developed for differ-
ent applications are reused and adapted (Gilbert
et al, 2005). However, we still believe that ex-
ploiting dialog data to train/adapt or complement
hand-crafted components will be vital for robust
and adaptable spoken dialog systems.
3 Related Work
In this paper, we discuss methods for automati-
cally creating models of dialog structure using di-
alog act and task/subtask information. Relevant
related work includes research on automatic dia-
log act tagging and stochastic dialog management,
and on building hierarchical models of plans using
task/subtask information.
There has been considerable research on statis-
tical dialog act tagging (Core, 1998; Jurafsky et
al., 1998; Poesio and Mikheev, 1998; Samuel et
al., 1998; Stolcke et al, 2000; Hastie et al, 2002).
Several disambiguation methods (n-gram models,
hidden Markov models, maximum entropy mod-
els) that include a variety of features (cue phrases,
speaker ID, word n-grams, prosodic features, syn-
tactic features, dialog history) have been used. In
this paper, we show that use of extended context
gives improved results for this task.
Approaches to dialog management include
AI-style plan recognition-based approaches (e.g.
(Sidner, 1985; Litman and Allen, 1987; Rich
and Sidner, 1997; Carberry, 2001; Bohus and
Rudnicky, 2003)) and information state-based ap-
proaches (e.g. (Larsson et al, 1999; Bos et al,
2003; Lemon and Gruenstein, 2004)). In recent
years, there has been considerable research on
how to automatically learn models of both types
from data. Researchers who treat dialog as a se-
quence of information states have used reinforce-
ment learning and/or Markov decision processes
to build stochastic models for dialog management
that are evaluated by means of dialog simulations
(Levin and Pieraccini, 1997; Scheffler and Young,
2002; Singh et al, 2002; Williams et al, 2005;
Henderson et al, 2005; Frampton and Lemon,
2005). Most recently, Henderson et al showed
that it is possible to automatically learn good dia-
log management strategies from automatically la-
beled data over a large potential space of dialog
states (Henderson et al, 2005); and Frampton and
Lemon showed that the use of context informa-
tion (the user?s last dialog act) can improve the
performance of learned strategies (Frampton and
Lemon, 2005). In this paper, we combine the use
of automatically labeled data and extended context
for automatic dialog modeling.
Other researchers have looked at probabilistic
models for plan recognition such as extensions of
Hidden Markov Models (Bui, 2003) and proba-
bilistic context-free grammars (Alexandersson and
Reithinger, 1997; Pynadath and Wellman, 2000).
In this paper, we compare hierarchical grammar-
style and flat chunking-style models of dialog.
In recent research, Hardy (2004) used a large
corpus of transcribed and annotated telephone
conversations to develop the Amities dialog sys-
tem. For their dialog manager, they trained sepa-
rate task and dialog act classifiers on this corpus.
For task identification they report an accuracy of
85% (true task is one of the top 2 results returned
by the classifier); for dialog act tagging they report
86% accuracy.
4 Structural Analysis of a Dialog
We consider a task-oriented dialog to be the re-
sult of incremental creation of a shared plan by
the participants (Lochbaum, 1998). The shared
plan is represented as a single tree that encap-
sulates the task structure (dominance and prece-
dence relations among tasks), dialog act structure
(sequences of dialog acts), and linguistic structure
of utterances (inter-clausal relations and predicate-
argument relations within a clause), as illustrated
in Figure 1. As the dialog proceeds, an utterance
from a participant is accommodated into the tree in
an incremental manner, much like an incremental
syntactic parser accommodates the next word into
a partial parse tree (Alexandersson and Reithinger,
1997). With this model, we can tightly couple
language understanding and dialog management
using a shared representation, which leads to im-
proved accuracy (Taylor et al, 1998).
In order to infer models for predicting the struc-
ture of task-oriented dialogs, we label human-
human dialogs with the hierarchical information
shown in Figure 1 in several stages: utterance
segmentation (Section 4.1), syntactic annotation
(Section 4.2), dialog act tagging (Section 4.3) and
202
subtask labeling (Section 5).
Dialog
Task
Topic/SubtaskTopic/Subtask
Task Task
Clause
UtteranceUtteranceUtterance
Topic/Subtask
DialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?Args
Figure 1: Structural analysis of a dialog
4.1 Utterance Segmentation
The task of ?cleaning up? spoken language utter-
ances by detecting and removing speech repairs
and dysfluencies and identifying sentence bound-
aries has been a focus of spoken language parsing
research for several years (e.g. (Bear et al, 1992;
Seneff, 1992; Shriberg et al, 2000; Charniak and
Johnson, 2001)). We use a system that segments
the ASR output of a user?s utterance into clauses.
The system annotates an utterance for sentence
boundaries, restarts and repairs, and identifies
coordinating conjunctions, filled pauses and dis-
course markers. These annotations are done using
a cascade of classifiers, details of which are de-
scribed in (Bangalore and Gupta, 2004).
4.2 Syntactic Annotation
We automatically annotate a user?s utterance with
supertags (Bangalore and Joshi, 1999). Supertags
encapsulate predicate-argument information in a
local structure. They are composed with each
other using the substitution and adjunction oper-
ations of Tree-Adjoining Grammars (Joshi, 1987)
to derive a dependency analysis of an utterance
and its predicate-argument structure.
4.3 Dialog Act Tagging
We use a domain-specific dialog act tag-
ging scheme based on an adapted version of
DAMSL (Core, 1998). The DAMSL scheme is
quite comprehensive, but as others have also found
(Jurafsky et al, 1998), the multi-dimensionality
of the scheme makes the building of models from
DAMSL-tagged data complex. Furthermore, the
generality of the DAMSL tags reduces their util-
ity for natural language generation. Other tagging
schemes, such as the Maptask scheme (Carletta et
al., 1997), are also too general for our purposes.
We were particularly concerned with obtaining
sufficient discriminatory power between different
types of statement (for generation), and to include
an out-of-domain tag (for interpretation). We pro-
vide a sample list of our dialog act tags in Table 2.
Our experiments in automatic dialog act tagging
are described in Section 6.3.
5 Modeling Subtask Structure
Figure 2 shows the task structure for a sample di-
alog in our domain (catalog ordering). An order
placement task is typically composed of the se-
quence of subtasks opening, contact-information,
order-item, related-offers, summary. Subtasks can
be nested; the nesting structure can be as deep as
five levels. Most often the nesting is at the left or
right frontier of the subtask tree.
Opening
Order Placement
Contact Info
Delivery InfoShipping Info
ClosingSummaryPayment InfoOrder Item
Figure 2: A sample task structure in our applica-
tion domain.
Contact Info Order Item Payment Info Summary Closing
Shipping Info Delivery Info
Opening
Figure 3: An example output of the chunk model?s
task structure
The goal of subtask segmentation is to predict if
the current utterance in the dialog is part of the cur-
rent subtask or starts a new subtask. We compare
two models for recovering the subtask structure
? a chunk-based model and a parse-based model.
In the chunk-based model, we recover the prece-
dence relations (sequence) of the subtasks but not
dominance relations (subtask structure) among the
subtasks. Figure 3 shows a sample output from the
chunk model. In the parse model, we recover the
complete task structure from the sequence of ut-
terances as shown in Figure 2. Here, we describe
our two models. We present our experiments on
subtask segmentation and labeling in Section 6.4.
5.1 Chunk-based model
This model is similar to the second one described
in (Poesio and Mikheev, 1998), except that we
use tasks and subtasks rather than dialog games.
We model the prediction problem as a classifica-
tion task as follows: given a sequence of utter-
ances   in a dialog   	
 			
  and a
203
subtask label vocabulary  ffBootstrapping Spoken Dialog Systems with Data Reuse
Giuseppe Di Fabbrizio Gokhan Tur Dilek Hakkani-Tu?r
AT&T Labs-Research,
Florham Park, NJ, 07932
 
pino,gtur,dtur  @research.att.com
Abstract
Building natural language spoken dialog sys-
tems requires large amounts of human tran-
scribed and labeled speech utterances to reach
useful operational service performances. Fur-
thermore, the design of such complex systems
consists of several manual steps. The User
Experience (UE) expert analyzes and defines
by hand the system core functionalities: the
system semantic scope (call-types) and the di-
alog manager strategy which will drive the
human-machine interaction. This approach is
extensive and error prone since it involves sev-
eral non-trivial design decisions that can only
be evaluated after the actual system deploy-
ment. Moreover, scalability is compromised by
time, costs and the high level of UE know-how
needed to reach a consistent design. We pro-
pose a novel approach for bootstrapping spo-
ken dialog systems based on reuse of existing
transcribed and labeled data, common reusable
dialog templates and patterns, generic language
and understanding models, and a consistent de-
sign process. We demonstrate that our ap-
proach reduces design and development time
while providing an effective system without
any application specific data.
1 Introduction
Spoken dialog systems aim to identify intents of humans,
expressed in natural language, and take actions accord-
ingly, to satisfy their requests (Gorin et al, 2002). In a
natural spoken dialog system, typically, first the speaker?s
utterance is recognized using an automatic speech rec-
ognizer (ASR). Then, the intent of the speaker is identi-
fied from the recognized sequence, using a spoken lan-
guage understanding (SLU) component. This step can
be framed as a classification problem for goal-oriented
call routing systems (Gorin et al, 2002; Natarajan et al,
2002, among others). Then, the user would be engaged in
a dialog via clarification or confirmation prompts if nec-
essary. The role of the dialog manager (DM) is to interact
in a natural way and help the user to achieve the task that
the system is designed to support.
In our case we only consider automated call routing
systems where the task is to reach the right route in a
large call center, which could be either a live opera-
tor or an automated system. An example dialog from
a telephone-based customer care application is given in
Figure 1. Typically the design of such complex sys-
tems consists of several manual steps, including analy-
sis of existing IVR (Interactive Voice Response) systems,
customer service representative (CSR) interviews, cus-
tomers? testimonials, CSR training material, and, when
available, human-machine unconstrained speech record-
ings. Based on these heterogeneous requirements, the
User Experience (UE) expert analyzes and defines by
hand the system core functionalities: the system semantic
scope (call-types) and the dialog manager strategy which
will drive the human-machine interaction. Once the UE
expert designs the system, large amounts of transcribed
and labeled speech utterances are needed for building the
ASR and SLU models.
In our previous work, we have presented active and
unsupervised (or semi-supervised) learning algorithms
in order to reduce the amount of labeling effort needed
while building ASR and SLU systems (Tur et al, 2003;
Tur and Hakkani-Tu?r, 2003; Riccardi and Hakkani-Tu?r,
2003). There we focus on a single application, and only
the ASR and SLU components. In this study, we aim
to exploit the labeled and transcribed data and common
reusable dialog templates and patterns obtained from
similar previous applications to bootstrap the whole spo-
ken dialog system with ASR, SLU, and DM components.
The organization of this paper is as follows. Sec-
 System: How may I help you?
 User: Hello?
 Call-type: Hello
 System: Hello, how may I help you?
 User: I have a question.
 Call-type: Ask(Info)
 System: OK, What is your question?
 User: I would like to know my account balance.
 Call-type: Request(Account Balance)
 System: I can help you with that. What is your ac-
count number?
 User: ...
Figure 1: An example natural language dialog
tion 2 describes briefly the AT&T Spoken Dialog Sys-
tem, which we use in this study, and its main components,
ASR, SLU, and DM. In Section 3 we present our method
to bootstrap ASR, SLU, and DM for a new application.
Section 4 presents our experiments using real data from a
customer care application.
2 AT&T Spoken Dialog System
Once a phone call is established, the dialog manager
prompts the caller either with a pre-recorded or synthe-
sized greetings message. At the same time, it activates
the top level ASR grammar. The caller speech is then
translated into text and sent to the SLU which replies with
a semantic representation of the utterance. Based on the
SLU reply and the implemented dialog strategy, the DM
engages in a mixed initiative dialog to drive the user to-
wards the goal. The DM iterates the previously described
steps until the call reaches a final state (e.g. the call is
transferred to a CSR, an IVR or the caller hangs up).
2.1 ASR
Robust speech recognition is a critical component of
a spoken dialog system. The speech recognizer uses
trigram language models based on Variable N-gram
Stochastic Automata (Riccardi et al, 1996). The acous-
tic models are subword unit based, with triphone context
modeling and variable number of gaussians (4-24). The
output of the ASR engine (which can be the 1-best or a
lattice) is then used as the input of the SLU component.
2.2 SLU
In a natural spoken dialog system, the definition of ?un-
derstanding? depends on the application. In this work,
we focus only on goal-oriented call classification tasks,
where the aim is to classify the intent of the user into
one of the predefined call-types. As a call classification
example, consider the utterance in the previous example
dialog I would like to know my account balance, in a cus-
tomer care application. Assuming that the utterance is
recognized correctly, the corresponding intent or the call-
type would be Request(Account Balance) and the action
would be prompting for the account number and then
telling the balance to the user or routing this call to the
Billing Department.
Classification can be achieved by either a knowledge-
based approach which depends heavily on an expert writ-
ing manual rules or a data-driven approach which trains
a classification model to be used during run-time. In our
current system we consider both approaches. Data-driven
classification has long been studied in the machine learn-
ing community. Typically these classification algorithms
try to train a classification model using the features from
the training data. More formally, each object in the train-
ing data, 	
 , is represented in the form 

 ,
where 
ffProceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 33?40,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
A Data Driven Approach to Relevancy Recognition for
Contextual Question Answering
Fan Yang?
OGI School of Science & Engineering
Oregon Health & Science University
fly@cslu.ogi.edu
Junlan Feng and Giuseppe Di Fabbrizio
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ, 07932 - USA
junlan@research.att.com, pino@research.att.com
Abstract
Contextual question answering (QA), in
which users? information needs are satis-
fied through an interactive QA dialogue,
has recently attracted more research atten-
tion. One challenge of engaging dialogue
into QA systems is to determine whether
a question is relevant to the previous inter-
action context. We refer to this task as rel-
evancy recognition. In this paper we pro-
pose a data driven approach for the task
of relevancy recognition and evaluate it
on two data sets: the TREC data and the
HandQA data. The results show that we
achieve better performance than a previ-
ous rule-based algorithm. A detailed eval-
uation analysis is presented.
1 Introduction
Question Answering (QA) is an interactive
human-machine process that aims to respond
to users? natural language questions with exact
answers rather than a list of documents. In the
last few years, QA has attracted broader research
attention from both the information retrieval
(Voorhees, 2004) and the computational linguistic
fields (http://www.clt.mq.edu.au/Events/
Conferences/acl04qa/). Publicly ac-
cessible web-based QA systems, such as
AskJeeves (http://www.ask.com/) and START
(http://start.csail.mit.edu/), have scaled up
?The work was done when the first author was visiting
AT&T Labs - Research.
this technology to open-domain solutions. More
task-oriented QA systems are deployed as virtual
customer care agents addressing questions about
specific domains. For instance, the AT&T Ask
Allier agent (http://www.allie.att.com/) is
able to answer questions about the AT&T plans
and services; and the Ikea ?Just Ask Anna!? agent
(http://www.ikea.com/ms/en US/) targets ques-
tions pertaining the company?s catalog. Most of
these QA systems, however, are limited to answer
questions in isolation. The reality is that users often
ask questions naturally as part of contextualized
interaction. For instance, a question ?How do I
subscribe to the AT&T CallVantager service?? is
likely to be followed by other related questions
like ?How much will the basic plan cost?? and
so on. Furthermore, many questions that users
frequently want answers for cannot be satisfied with
a simple answer. Some of them are too complicated,
broad, narrow, or vague resulting that there isn?t a
simple good answer or there are many good answer
candidates, which entails a clarification procedure
to constrain or relax the search. In all these cases,
a question answering system that is able to answer
contextual questions is more favored.
Contextual question answering as a research chal-
lenge has been fostered by TREC (Text Retrieval
Conference) since 2001. The TREC 2001 QA track
made the first attempt to evaluate QA systems? abil-
ity of tracking context through a series of questions.
The TREC 2004 re-introduced this task and orga-
nized all questions into 64 series, with each series
focusing on a specific topic. The earlier questions
in a series provide context for the on-going ques-
tion. However, in reality, QA systems will not be
33
informed about the boundaries between series in ad-
vance.
One challenge of engaging dialogue into QA sys-
tems is to determine the boundaries between topics.
For each question, the system would need to deter-
mine whether the question begins a new topic or it
is a follow-up question related to the current exist-
ing topic. We refer to this procedure as relevancy
recognition. If a question is recognized as a follow-
up question, the next step is to make use of context
information to interpret it and retrieve the answer.
We refer to this procedure as context information fu-
sion. Relevancy recognition is similar to text seg-
mentation (Hearst, 1994), but relevancy recognition
focuses on the current question with the previous
text while text segmentation has the full text avail-
able and is allowed to look ahead.
De Boni and Manandhar (2005) developed a rule-
based algorithm for relevancy recognition. Their
rules were manually deduced by carefully analyzing
the TREC 2001 QA data. For example, if a question
has no verbs, it is a follow-up question. This rule-
based algorithm achieves 81% in accuracy when rec-
ognizing the question relevance in the TREC 2001
QA data set. The disadvantage of this approach is
that it involves a good deal of human effort to re-
search on a specific data set and summarize the rules.
For a new corpus from a different domain, it is very
likely that one would have to go over the data set and
modify the rules, which is time and human-effort
consuming. An alternative is to pursue a data driven
approach to automatically learn the rules from a data
set. In this paper, we describe our experiments of
using supervised learning classification techniques
for the task of relevancy recognition. Experiments
show that machine learning approach achieves better
recognition accuracy and can also be easily applied
to a new domain.
The organization of this paper is as follows. In
Section 2, we summarize De Boni and Manandhar?s
rule-based algorithm. We present our learning ap-
proach in Section 3. We ran our experiments on
two data sets, namely, the TREC QA data and the
HandQA data, and give the results in Section 4. In
section 5, we report our preliminary study on con-
text information fusion. We conclude this paper in
Section 6.
2 Rule-Based Approach
De Boni and Manandhar (2005) observed the fol-
lowing cues to recognize follow-up questions:
? Pronouns and possessive adjectives. For exam-
ple, if a question has a pronoun that does not re-
fer to an entity in the same sentence, this ques-
tion could be a follow-up question.
? Cue words, such as ?precisely? and ?exactly?.
? Ellipsis. For example, if a question is not syn-
tactically complete, this question could be a
follow-up question.
? Semantic Similarity. For example, if a ques-
tion bears certain semantic similarity to previ-
ous questions, this question might be a follow-
up question.
De Boni and Manandhar (2005) proposed an
algorithm of calculating the semantic similar-
ity between the current question Q and a pre-
vious question Q?. Supposed Q consists of a
list of words (w1, w2, ..., wn) and Q? consists
of (w?1, w?2, ..., w?m):
SentenceSimilarity(Q, Q?) (1)
=
?
1?j?n
( max
1?i?m
WordSimilarity(wj , w?i))
The value of WordSimilarity(w, w?) is the
similarity between two words, calculated from
WordNet (Fellbaum, 1998). It returns a value
between 0 (w and w? have no semantic rela-
tions) and 1 (w and w? are the same).
Motivated by these observations, De Boni and
Manandhar (2005) proposed the rule-based algo-
rithm for relevancy recognition given in Figure 1.
This approach can be easily mapped into an hand-
crafted decision tree. According to the algorithm,
a question follows the current existing topic if it (1)
contains reference to other questions; or (2) contains
context-related cue words; or (3) contains no verbs;
or (4) bears certain semantic similarity to previous
questions or answer. Evaluated on the TREC 2001
QA context track data, the recall of the algorithm
is 90% for recognizing first questions and 78% for
follow-up questions; the precision is 56% and 76%
respectively. The overall accuracy is 81%.
34
Given the current question Qi and a sequence of history ques-
tions Qi?n, ..., Qi?1:
1. If Qi has a pronoun or possessive adjective which has
no references in the current question, Qi is a follow-up
question.
2. If Qi has cue words such as ?precisely? or ?exactly?, Qi
is a follow-up question.
3. If Qi does not contain any verbs, Qi is a follow-up ques-
tion.
4. Otherwise, calculate the semantic similarity measure of
Qi as
SimilarityMeasure(Qi)
= max
1?j?n
f(j) ? SentenceSimilarity(Qi, Qi?j)
Here f(j) is a decay function. If the similarity measure is
higher than a certain threshold, Qi is a follow-up ques-
tion.
5. Otherwise, if answer is available, calculate the semantic
distance between Qi and the immediately previous an-
swer Ai?1: SentenceSimilarity(Qi, Ai?1). If it is
higher than a certain threshold, Qi is a follow-up ques-
tion that is related to the previous answer.
6. Otherwise, Qi begins a new topic.
Figure 1: Rule-based Algorithm
3 Data Driven Approach
3.1 Decision Tree Learning
As a move away from heuristic rules, in this paper,
we make an attempt towards the task of relevancy
recognition using machine learning techniques. We
formulate it as a binary classification problem: a
question either begins a new topic or follows the
current existing topic. This classification task can
be approached with a number of learning algorithms
such as support vector machines, Adaboost and arti-
ficial neural networks. In this paper, we present our
experiments using Decision Tree. A decision tree
is a tree in which each internal node represents a
choice between a number of alternatives, and each
leaf node represents a decision. Learning a decision
tree is fairly straightforward. It begins from the root
node which consists of all the training data, growing
the tree top-down by recursively splitting each node
based on maximum information gain until certain
criteria is met. Although the idea is simple, decision
tree learning is often able to yield good results.
3.2 Feature Extraction
Inspired by De Boni and Manandhar?s (2005) work,
we selected two categories of features: syntactic fea-
tures and semantic features. Syntactic features cap-
ture whether a question has certain syntactic compo-
nents, such as verbs or pronouns. Semantic features
characterize the semantic similarity between the cur-
rent question and previous questions.
3.2.1 Syntactic Features
As the first step, we tagged each question with
part-of-speech tags using GATE (Cunningham et al,
2002), a software tool set for text engineering. We
then extracted the following binary syntactic fea-
tures:
PRONOUN: whether the question has a pronoun
or not. A more useful feature would be to la-
bel whether a pronoun refers to an entity in the
previous questions or in the current question.
However, the performances of currently avail-
able tools for anaphora resolution are quite lim-
ited for our task. The tools we tried, includ-
ing GATE (Cunningham et al, 2002), Ling-
Pipe (http://www.alias-i.com/lingpipe/)
and JavaRAP (Qiu et al, 2004), tend to use
the nearest noun phrase as the referents for pro-
nouns. While in the TREC questions, pronouns
tend to refer to the topic words (focus). As a
result, unsupervised anaphora resolution intro-
duced more noise than useful information.
ProperNoun: whether the question has a proper
noun or not.
NOUN: whether the question has a noun or not.
VERB: whether the question has a verb or not.
DefiniteNoun: if a question has a definite noun
phrase that refers to an entity in previous ques-
tions, the question is very likely to be a follow-
up question. However, considering the diffi-
culty in automatically identifying definite noun
phrases and their referents, we ended up not us-
ing this feature in our training because it in fact
introduced misleading information.
3.3 Semantic Features
To compute the semantic similarity between two
questions, we modified De Boni and Manandhar?s
formula with a further normalization by the length
of the questions; see formula (2).
35
SentenceSimilarity(Q, Q?) (2)
= 1n
?
1?j?n
( max
1?i?m
WordSimilarity(wj , w?i))
This normalization has pros and cons. It removes
the bias towards long sentences by eliminating the
accumulating effect; but on the other hand, it might
cause the system to miss a related question, for ex-
ample, when two related sentences have only one
key word in common.1
Formula (2) shows that sentence level similarity
depends on word-word similarity. Researchers have
proposed a variety of ways in measuring the seman-
tic similarity or relatedness between two words (to
be exact, word senses) based on WordNet. For ex-
ample, the Path (path) measure is the inverse of
the shortest path length between two word senses
in WordNet; the Wu and Palmer?s (wup) measure
(Wu and Palmer, 1994) is to find the most spe-
cific concept that two word senses share as ances-
tor (least common subsumer), and then scale the
path length of this concept to the root note (sup-
posed that there is a virtual root note in WordNet)
by the sum of the path lengths of the individual
word sense to the root node; the Lin?s (lin) mea-
sure (Lin, 1998) is based on information content,
which is a corpus based measure of the specificity of
a word; the Vector (vector) measure associates each
word with a gloss vector and calculates the similar-
ity of two words as the cosine between their gloss
vectors (Patwardhan, 2003). It was unclear which
measure(s) would contribute the best information to
the task of relevancy recognition, so we just exper-
imented on all four measures, path, wup, lin, and
vector, in our decision tree training. We used Peder-
sen et al?s (2004) tool WordNet::Similarity to com-
pute these four measures. WordNet::Similarity im-
plements nine different measures of word similar-
ity. We here only used the four described above be-
cause they return a value between 0 and 1, which
is suitable for using formula (2) to calculate sen-
tence similarity, and we leave others as future work.
Notice that the WordNet::Similarity implementation
1Another idea is to feed the decision tree training both the
normalized and non-normalized semantic similarity informa-
tion and see what would come out. We tried it on the TREC data
and found out that the normalized features actually have higher
information gain (i.e. appear at the top levels of the learned tree.
can only measure path, wup, and lin between two
nouns or between two verbs, while it uses all the
content words for the vector measure. We thus have
the following semantic features:
path noun: sentence similarity is based on the
nouns2 similarity using the path measure.
path verb: sentence similarity is based on the non-
trivial verbs similarity using the path measure.
Trivial verbs include ?does, been, has, have,
had, was, were, am, will, do, did, would, might,
could, is, are, can, should, shall, being?.
wup noun: sentence similarity is based on the
nouns similarity using the Wu and Palmer?s
measure.
wup verb: sentence similarity is based on the
non-trivial verbs similarity using the Wu and
Palmer?s measure.
lin noun: sentence similarity is based on the nouns
similarity using the Lin?s measure.
lin verb: sentence similarity is based on the non-
trivial verbs similarity using the Lin?s measure.
vector: sentence similarity is based on all content
words (nouns, verbs, and adjectives) similarity
using the vector measure.
4 Results
We ran the experiments on two sets of data: the
TREC QA data and the HandQA data.
4.1 Results on the TREC data
TREC has contextual questions in 2001 context
track and 2004 (Voorhees, 2001; Voorhees, 2004).
Questions about a specific topic are organized into a
session. In reality, the boundaries between sessions
are not given. The QA system would have to rec-
ognize the start of a new session as the first step of
question answering. We used the TREC 2004 data
as training and the TREC 2001 context track data as
testing. The training data contain 286 factoid and list
questions in 65 sessions3; the testing data contain 42
questions in 10 sessions. Averagely each session has
about 4-5 questions. Figure 2 shows some example
questions (the first three sessions) from the TREC
2001 context track data.
2This is to filter out all other words but nouns from a sen-
tence for measuring semantic similarity.
3In the TREC 2004 data, each session of questions is as-
signed a phrase as the topic, and thus the first question in a ses-
sion might have pronouns referring to this topic phrase. In such
cases, we manually replaced the pronouns by the topic phrase.
36
CTX1a Which museum in Florence was damaged by a
major bomb explosion in 1993?
CTX1b On what day did this happen?
CTX1c Which galleries were involved?
CTX1d How many people were killed?
CTX1e Where were these people located?
CTX1f How much explosive was used?
CTX2a Which industrial sector supplies the most
jobs in Toulouse?
CTX2b How many foreign companies were based there
in 1994?
CTX2c Name a company that flies there.
CTX3a What grape variety is used in Chateau Petrus
Bordeaus?
CTX3b How much did the future cost for the 1989
Vintage?
CTX3c Where did the winery?s owner go to college?
CTX3d What California winery does he own?
Figure 2: Example TREC questions
4.1.1 Confusion Matrix
Table 1 shows the confusion matrix of the deci-
sion tree learning results. On the testing data, the
learned model performs with 90% in recall and 82%
in precision for recognizing first questions; for rec-
ognizing follow-up questions, the recall is 94% and
precision is 97%. In contrast, De Boni and Man-
andhar?s rule-based algorithm has 90% in recall and
56% in precision for recognizing first questions; for
follow-up questions, the recall is 78% and precision
is 96%. The recall and precision of our learned
model to recognize first questions and follow-up
questions are all better than or at least the same
as the rule-based algorithm. The accuracy of our
learned model is 93%, about 12% absolute improve-
ment from the rule-based algorithm, which is 81% in
accuracy. Although the size of the data is too small
to draw a more general conclusion, we do see that
the data driven approach has better performance.
Training Data
Predicted Class
True Class First follow-up Total
First 63 2 65
follow-up 1 220 221
Total 64 222 286
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 9 1 10 90%
follow-up 2 30 32 94%
Total 11 31 42
Precision 82% 97%
Table 1: Confusion Matrix for TREC Data
4.1.2 Trained Tree
Figure 3 shows the first top two levels of the tree
learned from the training data. Not surprisingly,
PRONOUN turns out to be the most important fea-
ture which has the highest information gain. In the
TREC data, when there is a pronoun in a question,
the question is very likely to be a follow-up ques-
tion. In fact, in the TREC 2004 data, the referent
of pronouns very often is the topic phrase. The fea-
ture path noun, on the second level of the trained
tree, turns out to contribute most information in this
recognition task among the four different semantic
similarity measures. The similarity measures using
wup, wup noun and wup verb, and the vector mea-
sure do not appear in any node of the trained tree.
Figure 3: Trained Tree on TREC Data
The following are rules generated from the train-
ing data whose confidence is higher than 90%. Con-
fidence is defined as out of the training records for
which the left hand side of the rule is true, the per-
centage of records for which the right hand side is
also true. This measures the accuracy of the rule.
- If PRONOUN=1 then follow-up question
- If path noun?0.31 then follow-up question
- If lin noun?0.43 then follow-up question
- If path noun<0.15 and PRONOUN=0 then first question
De Boni and Manandhar?s algorithm has this
rule:?if a question has no verb, the question is
follow-up question?. However, we did not learn this
rule from the data, nor the feature VERB appears in
any node of the trained tree. One possible reason
is that this rule has too little support in the training
set (support is defined as the percentage of which
the left hand side of the rule is true). Another pos-
sible reason is that this rule is not needed because
the combination of other features is able to provide
enough information for recognizing follow-up ques-
tions. In any case, the decision tree learns a (local)
37
optimized combination of features which captures
most cases, and avoids redundant rules.
4.1.3 Error Analysis
The trained decision tree has 3 errors in the test-
ing data. Two of the errors are mis-recognition of
follow-up questions to be first questions, and one is
the vice versa.
The first error is failure to recognize the ques-
tion ?which galleries were involved?? (CTX1c) as
a follow-up question (see Figure 2 for context). It
is a syntactically complete sentence, and there is no
pronoun or definite noun in the sentence. Seman-
tic features are the most useful information to rec-
ognize it as a follow-up question. However, the se-
mantic relatedness in WordNet between the words
?gallery? in the current question and ?museum? in
the first question of this session (CTX1a in Figure 2)
is not strong enough for the trained decision tree to
relate the two questions together.
The second error is failure to recognize the ques-
tion ?Where did the winery?s owner go to college??
(CTX3c) as a follow-up question. Similarly, part
of the reason for this failure is due to the insuffi-
cient semantic relatedness between the words ?win-
ery? and ?grape? (in CTX3a) to connect the ques-
tions together. However, this question has a definite
noun phrase ?the winery? which refers to ?Chateau
Petrus Bordeaux? in the first question in this session.
We did not make use of the feature DefiniteNoun in
our training, because it is not easy to automatically
identify the referents of a definite noun phrase, or
even whether it has a referent or not. A lot of def-
inite noun phrases, such as ?the sun?, ?the trees in
China?, ?the first movie?, and ?the space shuttles?,
do not refer to any entity in the text. This does not
mean that the feature DefiniteNoun is not important,
but instead that we just leave it as our future work to
better incorporate this feature.
The third error, is failure to recognize the question
?What does transgenic mean?? as the first question
that opens a session. This error is due to the over-
fitting of decision tree training.
4.1.4 Boosting
We tried another machine learning approach, Ad-
aboost (Schapire and Singer, 2000), which is resis-
tant (but not always) to over-fitting. It calls a given
weak learning algorithm repeatedly in a series of
rounds t = 1, ..., T . Each time the weak learning
algorithm generates a rough ?rule of thumb?, and
after many rounds Adaboost combines these weak
rules into a single prediction rule that, hopefully,
will be more accurate than any one of the weak
rules. Figure 2 shows the confusion matrix of Ad-
aboost learning results. It shows that Adaboost is
able to correctly recognize ?What does transgenic
mean?? as beginning a new topic. However, Ad-
aboost has more errors in recognizing follow-up
questions, which results in an overall accuracy of
88%, slightly lower than decision tree learning.
Training Data
Predicted Class
True Class First follow-up Total
First 64 1 65
follow-up 1 220 221
Total 65 221 286
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 10 0 10 100%
follow-up 5 27 32 84%
Total 15 27 42
Precision 67% 100%
Table 2: Confusion Matrix Using Adaboosting
4.2 Results on the HandQA data
We also conducted an experiment using real-world
customer-care related questions. We selected our
test data from the chat logs of a deployed online
QA system. We refer to this system as HandQA.
HandQA is built using a telecommunication ontol-
ogy database and 1600 pre-determined FAQ-answer
pairs. For every submitted customer question,
HandQA chooses one of these 1600 answers as the
response. Each chat session contains about 3 ques-
tions. We assume the questions in a session are
context-related.
The HandQA data are different from the TREC
data in two ways. First, HandQA questions are real
typed questions from motivated users. The HandQA
data contain some noisy information, such as typos
and bad grammars. Some users even treated this
system as a search engine and simply typed in the
keywords. Second, questions in a chat session ba-
sically asked for the same information. Very often,
when the system failed to get the correct answer to
38
the user?s question, the user would repeat or rephrase
the same question, until they gave up or the system
luckily found the answer. As an example, Figure 4
shows two chat sessions. Again, we did not use the
system?s answer in our relevancy recognition.
How to make number non published?
Non published numbers
How to make number non listed?
Is my number switched to Call Vantage yet?
When will my number be switched?
When is number transferred?
Figure 4: Example questions in HandQA
A subset of the HandQA data, 5908 questions in
2184 sessions are used for training and testing the
decision tree. The data were randomly divided into
two sets: 90% for training and 10% for testing.
4.2.1 Confusion Matrix
Table 3 shows the confusion matrix of the deci-
sion tree learning results. For recognizing first ques-
tions, the learned model has 73% in recall and 62%
in precision; for recognizing follow-up questions,
the recall is 75% and precision is 84%. The accuracy
is 74%. A base line model is to have all questions
except the first one as following up questions, which
results in the accuracy of 64% (380/590). Thus the
learned decision tree yields an absolute improve-
ment of 10%. However, the results on this data set
are not as good as those on the TREC data.
Training Data
Predicted Class
True Class First follow-up Total
First 1483 490 1973
follow-up 699 2646 3345
Total 2182 3136 5318
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 153 58 211 73%
follow-up 93 286 379 75%
Total 246 344 590
Precision 62% 84%
Table 3: Confusion Matrix for HandQA Data
4.2.2 Trained Tree
Table 5 shows the top two levels of the tree
learned from the training data, both of which are
on the semantic measure path. This again confirms
that path best fits the task of relevancy recognition
among the four semantic measures.
No syntactical features appear in any node of the
learned tree. This is not surprising because syntac-
tic information is noisy in this data set. Typos, bad
grammars, and mis-capitalization affect automatic
POS tagging. Keywords input also results in incom-
plete sentences, which makes it unreliable to recog-
nize follow-up questions based on whether a ques-
tion is a complete sentence or not. Furthermore,
because questions in a session rarely refer to each
other, but just repeat or rephrase each other, the fea-
ture PRONOUN does not help either. All these make
syntactic features not useful. Semantic features turn
out to be more important for this data set.
Figure 5: Trained Tree on HandQA Data
4.2.3 Error Analysis
There are two reasons for the decreased perfor-
mance in this data set. The first reason, as we ana-
lyzed above, is that syntactical features do not con-
tribute to the recognition task. The second reason is
that consecutive chat sessions might ask for the same
information. In the handQA data set, questions are
basically all about telecommunication service, and
questions in two consecutive chat sessions, although
by different users, could be on very similar topics or
even have same words. Thus, questions, although in
two separate chat sessions, could have high semantic
similarity measure. This would introduce confusing
information to the decision tree learning.
5 Making Use of Context Information
Relevancy recognition is the first step of contextual
question answering. If a question is recognized as
following the current existing topic, the next step is
to make use of the context information to interpret it
39
and retrieve the answers. To explore how context in-
formation helps answer retrieval, we conducted pre-
liminary experiments with the TREC 2004 QA data.
We indexed the TREC documents using the Lucene
search engine (Hatcher and Gospodnetic, 2004) for
document retrieval. The Lucene search engine takes
as input a query (a list of keywords), and returns a
ranked list of relevant documents, of which the first
50 were taken and analyzed in our experiments. We
tried different strategies for query formulation. Sim-
ply using the questions as the query, only 20% of
the follow-up questions find their answers in the first
50 returned documents. This percentage went up
to 85% when we used the topic words, provided in
TREC data for each section, as the query. Because
topic words are usually not available in real world
applications, to be more practical, we tried using the
noun phrases in the first question as the query. In
this case, 81% of the questions are able to find the
answers in the returned documents. When we com-
bined the (follow-up) question with the noun phrases
in the first question as the query, the retrieved rate
increases to 84%. Typically, document retrieval is a
crucial step for QA systems. These results suggest
that context information fusion has a big potential to
improve the performance of answer retrieval. How-
ever, we leave the topic of how to fuse context infor-
mation into the follow-up questions as future work.
6 Conclusion
In this paper, we present a data driven approach, de-
cision tree learning, for the task of relevancy recog-
nition in contextual question answering. Experi-
ments show that this approach achieves 93% accu-
racy on the TREC data, about 12% improvement
from the rule-based algorithm reported by De Boni
and Mananhar (2005). Moreover, this data driven
approach requires much less human effort on inves-
tigating a specific data set and less human exper-
tise to summarize rules from the observation. All
the features we used in the training can be automat-
ically extracted. This makes it straightforward to
train a model in a new domain, such as the HandQA.
Furthermore, decision tree learning is a white-box
model and the trained tree is human interpretable. It
shows that the path measure has the best information
gain among the other semantic similarity measures.
We also report our preliminary experiment results on
context information fusion for question answering.
7 Acknowledgement
The authors thank Srinivas Bangalore and Mazin E.
Gilbert for helpful discussion.
References
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of the 40th ACL.
Marco De Boni and Suresh Manandhar. 2005. Imple-
menting clarification dialogues in open domain ques-
tion answering. Natural Language Engineering. Ac-
cepted.
Christiane Fellbaum. 1998. WordNet:An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Erik Hatcher and Otis Gospodnetic. 2004. Lucene in
Action. Manning.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of 32nd ACL, pages
9?16.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the International Con-
ference on Machine Learning.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. master?s thesis, University of
Minnesota, Duluth.
Ted Pederson, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the re-
latedness of concepts. In Proceedings of the 9th AAAI.
Intelligent Systems Demonstration.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the rap anaphora
resolution algorithm. In Proceedings of LREC, pages
291?294.
Robert E. Schapire and Yoram Singer. 2000. BoosTex-
ter: A boosting-based system for text categorization.
Machine Learning, 39:135?168.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
question answering track. In Proceedings of TREC-10.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Proceedings of TREC-13.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proceedings of 32nd ACL,
pages 133?138.
40
Referring Expression Generation Using Speaker-based Attribute Selection
and Trainable Realization (ATTR)
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
AT&T Labs - Research, Inc.
180 Park Avenue
Florham Park, NJ 07932, USA
{pino,stent,srini}@research.att.com
Abstract
In the first REG competition, researchers
proposed several general-purpose algorithms
for attribute selection for referring expression
generation. However, most of this work did
not take into account: a) stylistic differences
between speakers; or b) trainable surface re-
alization approaches that combine semantic
and word order information. In this paper we
describe and evaluate several end-to-end re-
ferring expression generation algorithms that
take into consideration speaker style and use
data-driven surface realization techniques.
1 Introduction
There now exist numerous general-purpose algo-
rithms for attribute selection used in referring ex-
pression generation (e.g., (Dale and Reiter, 1995;
Krahmer et al, 2003; Belz and Gatt, 2007)). How-
ever, these algorithms by-and-large focus on the al-
gorithmic aspects of referring expression generation
rather than on psycholinguistic factors that influence
language production. For example, we know that
humans exhibit individual style differences during
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the lan-
guage production process is subject to lexical prim-
ing, which means that words and concepts that have
been used recently are likely to appear again (Levelt,
1989).
In this paper, we first explore the impact of indi-
vidual style and priming on attribute selection for
referring expression generation. To get an idea
of the potential improvement when modeling these
factors, we implemented a version of full brevity
search (Dale, 1992) that uses speaker-specific con-
straints, and another version that also uses recency
constraints. We found that using speaker-specific
constraints led to big performance gains for both
TUNA domains, while the use of recency constraints
was not as effective for TUNA-style tasks. We then
modified Dale and Reiter?s classic attribute selection
algorithm (Dale and Reiter, 1995) to model speaker-
specific constraints, and found performance gains in
this more greedy approach as well.
Then we looked at surface realization for referring
expression generation. There are several approaches
to surface realization described in the literature (Re-
iter and Dale, 2000) ranging from hand-crafted
template-based realizers to data-driven syntax-based
realizers (Langkilde and Knight, 2000; Bangalore
and Rambow, 2000). Template-based realization
involves the insertion of attribute values into pre-
determined templates. Data-driven syntax-based
methods use syntactic relations between words (in-
cluding long-distance relations) for word ordering.
Other data-driven techniques exhaustively generate
possible realizations with recourse to syntax in as
much as it is reflected in local n-grams. Such tech-
niques have the advantage of being robust although
they are inadequate to capture long-range depen-
dencies. In this paper, we explore three techniques
for the task of referring expression generation that
are different hybrids of hand-crafted and data-driven
methods.
The remainder of this paper is organized as fol-
lows: In Section 2, we present the algorithms for
attribute selection. The different methods for sur-
face realizers are presented in Section 3. The exper-
iments concerning the attribute selection and surface
realization are presented in Section 4 and Section 5.
The final remarks are discussed in Section 6.
2 Attribute Selection Algorithms
Full Brevity (FB) We implemented a version of
full brevity search (Dale, 1992). It does the follow-
211
ing: first, it constructs AS, the set of attribute sets
that uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu ? AS
based on a selection criterion. The minimality (FB-
m) criterion selects from among the smallest ele-
ments of AS at random. The frequency (FB-f) cri-
terion selects from among the elements of AS the
one that occurred most often in the training data.
The speaker frequency (FB-sf) criterion selects
from among the elements of AS the one used most
often by this speaker in the training data, backing off
to FB-f if necessary. This criterion models speaker-
specific constraints. Finally, the speaker recency
(FB-sr) criterion selects from among the elements
of AS the one used most recently by this speaker in
the training data, backing off to FB-sf if necessary.
This criterion models priming and speaker-specific
constraints.
Dale and Reiter We implemented two variants of
the classic Dale & Reiter attribute selection (Dale
and Reiter, 1995) algorithm. For Dale & Reiter
basic (DR-b), we first build the preferred list of
attributes by sorting the most frequently used at-
tributes in the training set. We keep separate lists
based upon the ?+LOC? and ?-LOC? conditions
and backoff to a global preferred frequency list in
case the attributes are not covered in the current list
(merge and sort by frequency). Next, we iterate over
the list of preferred attributes and select the next one
that rules out at least one entity in the contrast set
until no distractors are left. The Dale & Reiter
speaker frequency (DR-sf) uses a speaker-specific
preferred list, backing off to the DR-b preferred list
if an attribute is not in the current speaker?s preferred
list. For this task, we ignored any further attribute
knowledge base or taxonomy abstraction.
3 Surface Realization Approaches
We summarize our approaches to surface realization
in this section. All three surface realizers have the
same four stages: (a) lexical choice of words and
phrases for the attribute values; (b) generation of a
space of surface realizations (T ); (c) ranking the set
of realizations using a language model (LM ); (d)
selecting the best scoring realization.
T ? = BestPath(Rank(T, LM)) (1)
Template-Based Realizer To construct our
template-based realizer, we extract the annotated
word string from each trial in the training data
and replace each annotated text segment with the
attribute type with which it is annotated. The key
for each template is the lexicographically sorted list
of attribute types it contains. Consequently, any
attribute lists not found in the training data cannot
be realized by the template-based realizer; however,
if there is a template for an input attribute list it is
quite likely to be coherent.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
fill in each possible template with each combina-
tion of the attribute realizations. We report results
for two versions of this realizer: one with speaker-
specific lexicon and templates (Template-S), and
one without (Template).
Dependency-Based Realizer To construct our
dependency-based realizer, we first parse all the
word strings from the training data using the depen-
dency parser described in (Bangalore et al, 2005;
Nasr and Rambow, 2004). Then, for every pair
of words wi, wj that occur in the same referring
expression (RE) in the training data, we compute:
freq(i < j), the frequency with which wi pre-
cedes wj in any RE; freq(i = j ? 1), the fre-
quency with which wi immediately precedes wj in
any RE; freq(dep(wi, wj) ? i < j), the frequency
with which wi depends on and precedes wj in any
RE, and freq(dep(wi, wj) ? j < i), the frequency
with which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
for each combination of attribute realizations, we
find the most likely set of dependencies and prece-
dences given the training data.
Permute and Rank In this method, the lexical
items associated with each of the attribute value to
be realized are treated as a disjunctive set of tokens.
This disjunctive set is represented as a finite-state
automaton with two states and transitions between
them labeled with the tokens of the set. The transi-
tions are weighted by the negative logarithm of the
probability of the lexical token (w) being associated
with that attribute value (attr): (?log(P (w|attr))).
These sets are treated as unordered bags of tokens;
we create permutations of these bags of tokens to
represent the set of possible surface realizations. We
then use the language model to rank this set of possi-
ble realizations and recover the highest scoring RE.
212
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
Table 1: Results for attribute selection
Unfortunately, the number of states of the min-
imal permutation automaton of even a linear au-
tomata (finite-state machine representation of a
string) grows exponentially with the number of
words of the string. So, instead of creating a full
permutation automaton, we choose to constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al, 2005)).
4 Attribute Selection Experiments
Data Preparation The training data were used to
build the models outlined above. The development
data were then processed one-by-one. For our final
submissions, we use training and development data
to build our models.
Results Table 1 shows the results for variations of
full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both cor-
pora, we see a large performance jump when we
use speaker constraints. However, when we incor-
porate recency constraints as well performance de-
clines slightly. We think this is due to two factors:
first, the speakers are not in a conversation, and self-
priming may have less impact; and second, we do
not always have the most recent prior utterance for a
given speaker in the training data.
Table 1 also shows the results for variations of
Dale and Reiter?s algorithm. When we incorpo-
String-Edit Dist. Accuracy
Furniture
DEV FB-sf DR-sf DEV FB-sf DR-sf
Permute&Rank 4.39 4.60 4.74 0.07 0.04 0.03
Dependency 3.90 4.25 5.50 0.14 0.06 0.03
Template 4.36 4.33 5.39 0.07 0.05 0.03
Template-S 3.52 3.81 5.16 0.28 0.20 0.04
People
Permute&Rank 6.26 6.46 7.01 0.01 0.01 0.00
Dependency 3.96 4.32 7.03 0.06 0.06 0.00
Template 5.16 4.62 7.26 0.03 0.06 0.00
Template-S 4.25 4.31 7.04 0.18 0.13 0.00
Overall
Permute&Rank 5.25 5.45 5.78 0.05 0.03 0.01
Dependency 3.93 4.28 6.20 0.07 0.06 0.01
Template 4.73 4.46 6.25 0.05 0.05 0.01
Template-S 3.86 4.04 6.03 0.23 0.17 0.02
Table 2: Results for realization
rate speaker constraints, we again see a performance
jump, although compared to the best possible case
(full brevity) there is still room for improvement.
Discussion We have shown that by using speaker
and recency constraints in standard algorithms, it
is possible to achieve performance gains on the at-
tribute selection task.
The most relevant previous research is the work of
(Gupta and Stent, 2005), who modified Dale and Re-
iter?s algorithm to model speaker adaptation in dia-
log. However, this corpus does not involve dialog so
there are no cross-speaker constraints, only within-
speaker constraints (style and priming).
5 Surface Realization Experiments
Data Preparation We first normalize the training
data to correct misspellings and remove punctuation
and capitalization. We then extract a phrasal lexi-
con. For each attribute value we extract the count of
all realizations of that value in the training data. We
treat locations as a special case, storing separately
the realizations of x-y coordinate pairs and single
x- or y-coordinates. We add a small number of re-
alizations to the lexicon by hand to cover possible
attribute values not seen in the training data.
Results Table 2 shows the evaluation results for
string-edit distance and string accuracy on the devel-
opment set with three different attributes sets: DEV
? attributes selected by the human test; FB-sf ? at-
tributes generated by the full brevity algorithm with
speaker frequency; and DR-sf ? attributes selected
213
by the Dale & Reiter algorithm with speaker fre-
quency.
For the TUNA realization task (DEV attributes),
our approaches work better for the furniture domain,
where there are fewer attributes, than for the people
domain. For the furniture domain, the Template-S
approach achieves lowest string-edit distance, while
for the people domain, the Dependency approach
achieves lowest string-edit distance. The latter
method was submitted for human evaluation.
When we consider the ?end-to-end? referring
expression generation task (FB-sf and DR-sf at-
tributes), the best overall performing system is the
speaker-based template generator with full-brevity
and speaker frequency attribute selection. In terms
of generated sentence quality, a preliminary and
qualitative analysis shows that the combination Per-
mute & Rank and DR-sf produces more naturalistic
phrases.
Discussion Although the Template-S approach
achieves the best string edit distance scores over-
all, it is not very robust. If no examples were found
in the training data neither Template approach will
produce no output. (This happens twice for each of
the domains on the development data.) The Depen-
dency approach achieves good overall performance
with more robustness.
The biggest cause of errors for the Permute
and Reorder approach was missing determiners and
missing modifiers. The biggest cause of errors for
the Dependency approach was missing determiners
and reordered words. The Template approach some-
times had repeated words (e.g. ?middle?, where
?middle? referred to both x- and y-coordinates).
6 Conclusions
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ?small? generation
tasks, and confirmed that data-driven approaches to
surface realization can work well using a range of
lexical, syntactic and semantic information.
In addition to individual style and priming, an-
other potentially fruitful area for exploration with
TUNA-style tasks is human visual search strategies
(Rayner, 1998). We leave this idea for future work.
Acknowledgments
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
References
S. Bangalore and O. Rambow. 2000. Exploiting a prob-
abilistic hierarchical model for generation. In Proc.
COLING.
S. Bangalore, A. Emami, and P. Haffner. 2005. Factor-
ing global inference by enriching local representations.
Technical report, AT&T Labs-Research.
A. Belz and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proceedings of UCNLG+MT at MT Summit XI.
A. Belz. 2007. Probabilistic generation of weather fore-
cast texts. In Proceedings of NAACL/HLT.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2).
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge, MA.
S. Gupta and A. Stent. 2005. Automatic evaluation of
referring expression generation using corpora. In Pro-
ceedings of UCNLG.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
I. Langkilde and K. Knight. 2000. Forest-based statisti-
cal sentence generation. In Proc. NAACL.
W. Levelt, 1989. Speaking: From intention to articula-
tion, pages 222?226. MIT Press.
A. Nasr and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop
on Tree Adjoining Grammar and Related Formalisms
(TAG+7).
K. Rayner. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psychologi-
cal Bulletin, 124(3).
E. Reiter and R. Dale. 2000. Building Natural Language
Generation Systems. Cambridge University Press.
214
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 151?158
Manchester, August 2008
Trainable Speaker-Based Referring Expression Generation
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
AT&T Labs - Research, Inc.
180 Park Avenue
Florham Park, NJ 07932, USA
{pino,stent,srini}@research.att.com
Abstract
Previous work in referring expression gen-
eration has explored general purpose tech-
niques for attribute selection and surface
realization. However, most of this work
did not take into account: a) stylistic dif-
ferences between speakers; or b) trainable
surface realization approaches that com-
bine semantic and word order information.
In this paper we describe and evaluate sev-
eral end-to-end referring expression gener-
ation algorithms that take into considera-
tion speaker style and use data-driven sur-
face realization techniques.
1 Introduction
Natural language generation (NLG) systems have
typically decomposed the problem of generating
a linguistic expression from a conceptual specifi-
cation into three major steps: content planning,
text planning and surface realization (Reiter and
Dale, 2000). The task in content planning is to
select the information that is to be conveyed to
maximize communication efficiency. The task in
text planning and surface realization is to use the
available linguistic resources (words and syntax) to
convey the selected information using well-formed
linguistic expressions.
During a discourse (whether written or spoken,
monolog or dialog), a number of entities are in-
troduced into the discourse context shared by the
reader/hearer and the writer/speaker. Construct-
ing linguistic references to these entities efficiently
and effectively is a problem that touches on all
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
parts of an NLG system. Traditionally, this prob-
lem is split into two parts. The task of selecting
the attributes to use in referring to an entity is the
attribute selection task, performed during content
planning or sentence planning. The actual con-
struction of the referring expression is part of sur-
face realization.
There now exist numerous general-purpose al-
gorithms for attribute selection (e.g., (Dale and Re-
iter, 1995; Krahmer et al, 2003; Belz and Gatt,
2007; Siddharthan and Copestake, 2004)). How-
ever, these algorithms by-and-large focus on the
algorithmic aspects of referring expression gener-
ation rather than on psycholinguistic factors that
influence language production. For example, we
know that humans exhibit individual differences in
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the
language production process is subject to lexical
priming, which means that words and concepts that
have been used recently are likely to appear again
(Levelt, 1989).
In this paper, we look at attribute selection and
surface realization for referring expression gener-
ation using the TUNA corpus 1, an annotated cor-
pus of human-produced referring expressions that
describe furniture and people. We first explore
the impact of individual style and priming on at-
tribute selection for referring expression genera-
tion. To get an idea of the potential improvement
when modeling these factors, we implemented a
version of full brevity search that uses speaker-
specific constraints, and another version that also
uses recency constraints. We found that using
speaker-specific constraints led to big performance
gains for both TUNA domains, while the use of re-
1http://www.csd.abdn.ac.uk/research/tuna/
151
cency constraints was not as effective for TUNA-
style tasks. We then modified Dale and Reiter?s
classic attribute selection algorithm (Dale and Re-
iter, 1995) to model individual differences in style,
and found performance gains in this more greedy
approach as well.
Then, we look at surface realization for re-
ferring expression generation. There are sev-
eral approaches to surface realizations described
in the literature (Reiter and Dale, 2000) rang-
ing from hand-crafted template-based realizers to
data-driven syntax-based realizers (Langkilde and
Knight, 2000; Bangalore and Rambow, 2000).
Template-based realization provides a straightfor-
ward method to fill out pre-defined templates with
the current attribute values. Data-driven syntax-
based methods employ techniques that incorporate
the syntactic relations between words which can
potentially go beyond local adjacency relations.
Syntactic information also helps in eliminating un-
grammatical sentence realizations. At the other ex-
treme, there are techniques that exhaustively gen-
erate possible realizations with recourse to syntax
in as much as it is reflected in local n-grams. Such
techniques have the advantage of being robust al-
though they are inadequate to capture long-range
dependencies. We explore three techniques for
the task of referring expression generation that are
different hybrids of hand-crafted and data-driven
methods.
The layout of this paper is as follows: In Sec-
tion 2, we describe the TUNA data set and the task
of identifying target entities in the context of dis-
tractors. In Section 3, we present our algorithms
for attribute selection. Our algorithms for sur-
face realization are presented in Section 4. Our
evaluation of these methods for attribute selection
and surface realization are presented in Sections 5
and 6.
2 The TUNA Corpus
The TUNA corpus was constructed using a web-
based experiment. Participants were presented
with a sequence of web pages, on each of which
they saw displayed a selection of 7 pictures of ei-
ther furniture (e.g. Figure 1) or people (e.g. Fig-
ure 2) sparsely placed on a 3 row x 5 column
grid. One of the pictures (the target) was high-
lighted; the other 6 objects (the distractors) were
randomly selected from the object database. Par-
ticipants were told that they were interacting with a
computer system to remove all but the highlighted
picture from the screen. They entered a description
of the object using natural language to identify the
object to the computer system.
The section of the TUNA corpus we used was
that provided for the REG 2008 Challenge2. The
training data includes 319 referring expressions in
the furniture domain and 274 in the people domain.
The development data (which we used for testing)
includes 80 referring expressions in the furniture
domain and 68 in the people domain.
Figure 1: Example of data from the furniture do-
main (The red couch on top).
Figure 2: Example of data from the people domain
(The bald subject on the bottom with the white
beard).
3 Attribute Selection Algorithms
Given a set of entities with attributes appropriate
to a domain (e.g., cost of flights, author of a book,
2http://www.nltg.brighton.ac.uk/research/reg08/. Prelimi-
nary versions of these algorithms were used in this challenge
and presented at INLG 2008.
152
color of a car) that are in a discourse context, and a
target entity that needs to be identified, the task of
attribute selection is to select a subset of the at-
tributes that uniquely identifies the target entity.
(Note that there may be more than one such at-
tribute set.) The efficacy of attribute selection can
be measured based on the minimality of the se-
lected attribute set as well as its ability to deter-
mine the target entity uniquely. There are varia-
tions however in terms of what makes an attribute
set more preferable to a human. For example, in
a people identification task, attributes of faces are
generally more memorable than attributes pertain-
ing to outfits. In this paper, we demonstrate that
the attribute set is speaker dependent.
In this section, we present two different attribute
selection algorithms. The Full Brevity algorithm
selects the attribute set by exhaustively searching
through all possible attribute sets. In contrast, Dale
and Reiter algorithm orders the attributes based
on a heuristic (motivated by human preference)
and selects the attributes in that order until the tar-
get entity is uniquely determined. We elaborate on
these algorithms below.
Full Brevity (FB) We implemented a version of
full brevity search. It does the following: first,
it constructs AS, the set of attribute sets that
uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu ? AS
based on one of the following four criteria: 1) The
minimality (FB-m) criterion selects from among
the smallest elements of AS at random. 2) The
frequency (FB-f) criterion selects the element of
AS that occurred most often in the training data.
3) The speaker frequency (FB-sf) criterion se-
lects the element of AS used most often by this
speaker in the training data, backing off to FB-f if
necessary. This criterion models individual speak-
ing/writing style. 4) Finally, the speaker recency
(FB-sr) criterion selects the element of AS used
most recently by this speaker in the training data,
backing off to FB-sf if necessary. This criterion
models priming.
Dale and Reiter We implemented two variants
of the classic Dale & Reiter attribute selection
(Dale and Reiter, 1995) algorithm. For Dale &
Reiter basic (DR-b), we first build the preferred
list of attributes by sorting the attributes according
to frequency of use in the training data. We keep
separate lists based on the ?LOC? condition (if its
value was ?+LOC?, the participants were told that
they could refer to the target using its location on
the screen; if it was ?-LOC?, they were instructed
not to use location on the screen) and backoff to
a global preferred attribute list if necessary. Next,
we iterate over the list of preferred attributes and
select the next one that rules out at least one en-
tity in the contrast set until no distractors are left.
Dale & Reiter speaker frequency (DR-sf) uses
a different preferred attribute list for each speaker,
backing off to the DR-b preferred list if an attribute
has never been observed in the current speaker?s
preferred attribute list. For the purpose of this task,
we did not use any external knowledge (e.g. tax-
onomies).
4 Surface Realization Approaches
A surface realizer for referring expression genera-
tion transforms a set of attribute-value pairs into a
linguistically well-formed expression. Our surface
realizers, which are all data-driven, involve four
stages of processing: (a) lexical choice of words
and phrases to realize attribute values; (b) genera-
tion of a space of surface realizations (T ); (c) rank-
ing the set of realizations using a language model
(LM ); (d) selecting the best scoring realization.
In general, the best ranking realization (T?) is de-
scribed by equation 1:
T ? = Bestpath(Rank(T,LM)) (1)
We describe three different methods for creating
the search space of surface realizations ? Template-
based, Dependency-based and Permutation-based
methods. Although these techniques share the
same method for ranking, they differ in the meth-
ods used for generating the space of possible sur-
face realizations.
4.1 Generating possible surface realizations
In order to transform the set of attribute-value
pairs into a linguistically well-formed expression,
the appropriate words that realize each attribute
value need to be selected (lexical choice) and the
selected words need to be ordered according to
the syntax of the target language (lexical order).
We present different models for approximating the
syntax of the target language. All three models
tightly integrate the lexical choice and lexical re-
ordering steps.
153
4.1.1 Template-Based Realizer
In the template-based approach, surface realiza-
tions from our training data are used to infer a set
of templates. In the TUNA data, each attribute in
each referring expression is annotated with its at-
tribute type (e.g. in ?the large red sofa? the sec-
ond word is labeled ?size?, the third ?color? and
the fourth ?type?). We extract the annotated re-
ferring expressions from each trial in the training
data and replace each attribute value with its type
(e.g. ?the size color type?) to create a tem-
plate. Each template is indexed by the lexicograph-
ically sorted list of attribute types it contains (e.g.
color size type). If an attribute set is not
found in the training data (e.g. color size)
but a superset of that set is (e.g. color size
type), then the corresponding template(s) may be
used, with the un-filled attribute types deleted prior
to output.
At generation time, we find all possible realiza-
tions (l) (from the training data) of each attribute
value (a) in the input attribute set (AS), and fill in
each possible template (t) with each combination
of the attribute realizations. The space of possible
surface realizations is represented as a weighted
finite-state automaton. The weights are computed
from the prior probability of each template and
the prior probability of each lexical item realizing
an attribute (Equation 2). We have two versions
of this realizer: one with speaker-specific lexi-
cons and templates (Template-S), and one without
(Template). We report results for both.
P (T |AS) =
?
t
P (t|AS)?
?
a?t
?
l
P (l|a, t) (2)
4.1.2 Dependency-Based Realizer
To construct our dependency-based realizer, we
first parse all the word strings from the train-
ing data using the dependency parser described
in (Bangalore et al, 2005; Nasr and Rambow,
2004). Then, for every pair of words wi, wj that
occur in the same referring expression (RE) in the
training data, we compute: freq(i < j), the fre-
quency with which wi precedes wj in any RE;
freq(dep(wi, wj) ? i < j), the frequency with
which wi depends on and precedes wj in any RE,
and freq(dep(wi, wj)?j < i), the frequency with
which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute value in the input attribute
set, and for each combination of attribute realiza-
tions, we find the most likely set of dependencies
and precedences given the training data. In other
words, we bin the selected attribute realizations
according to whether they are most likely to pre-
cede, depend on and precede, depend on and fol-
low, or follow, the head word they are closest to.
The result is a set of weighted partial orderings on
the attribute realizations. As with the template-
based surface realizer, we implemented speaker-
specific and speaker-independent versions of the
dependency-based surface realizer. Once again,
we encode the space of possible surface realiza-
tions as a weighted finite-state automaton.
4.1.3 Permute and Rank Realizer
In this method, the lexical items associated with
each attribute value to be realized are treated as a
disjunctive set of tokens. This disjunctive set is
represented as a finite-state automaton with two
states and transitions between them labeled with
the tokens of the set. The transitions are weighted
by the negative logarithm of the probability of the
lexical token (l) being associated with that attribute
value (a): (?log(P (l|a))). These sets are treated
as bags of tokens; we create permutations of these
bags of tokens to represent the set of possible sur-
face realizations.
In general, the number of states of the minimal
permutation automaton of even a linear automaton
(finite-state representation of a string) grows expo-
nentially with the number of words of the string.
Although creating the full permutation automaton
for full natural language generation tasks could
be computationally prohibitive, most attribute sets
in our two domains contain no more than five at-
tributes. So we choose to explore the full permu-
tation space. A more general approach might con-
strain permutations to be within a local window of
adjustable size (also see (Kanthak et al, 2005)).
Figure 3 shows the minimal permutation au-
tomaton for an input sequence of 4 words and a
window size of 2. Each state of the automaton is
indexed by a bit vector of size equal to the number
of words/phrases of the target sentence. Each bit
of the bit vector is set to 1 if the word/phrase in
that bit position is used on any path from the initial
to the current state. The next word for permutation
from a given state is restricted to be within the win-
dow size (2 in our case) positions counting from
the first as-yet uncovered position in that state. For
example, the state indexed with vector ?1000? rep-
154
0000
10001
0100
2
1100
2
10103
1
1110
3
1101
4
1111
4
3
2
Figure 3: Locally constraint permutation automaton for a sentence with 4 positions and a window size
of 2.
resents the fact that the word/phrase at position 1
has been used. The next two (window=2) posi-
tions are the possible outgoing arcs from this state
with labels 2 and 3 connecting to state ?1100? and
?1010? respectively. The bit vectors of two states
connected by an arc differ only by a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
4.2 Ranking and Recovering a Surface
Realization
These three methods for surface realization create
a space of possible linguistic expressions given the
set of attributes to be realized. These expressions
are encoded as finite-state automata and have to be
ranked based on their syntactic well-formedness.
We approximate the syntactic well-formedness of
an expression by the n-gram likelihood score of
that expression. We use a trigram model trained
on the realizations in the training corpus. This
language model is also represented as a weighted
finite-state automaton. The automaton represent-
ing the space of possible realizations and the one
representing the language model are composed.
The result is an automaton that ranks the possible
realizations according to their n-gram likelihood
scores. We then produce the best-scoring realiza-
tion as the target realization of the input attribute
set.
We introduce a parameter ? which allows us
to control the importance of the prior score rela-
tive to the language model scores. We weight the
finite-state automata according to this parameter as
shown in Equation 3.
T ? = Bestpath(? ? T ? (1 ? ?) ? LM) (3)
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
Table 1: Results for attribute selection
5 Attribute Selection Experiments
Data Preparation The training data were used
to build the models outlined above. The develop-
ment data were then processed one-by-one.
Metrics We report performance using the met-
rics used for the REG 2008 competition. The
MASI metric is a metric used in summarization
that measures agreement between two annotators
(or one annotator and one system) on set-valued
items (Nenkova et al, 2007). Values range from
0 to 1, with 1 representing perfect agreement.
The DICE metric is also a measure of association
whose value varies from 0 (no association) to 1 (to-
tal association) (Dice, 1945). The Accuracy met-
ric is binary-valued: 1 if the attribute set is iden-
tical to that selected by the human, 0 otherwise.
The Uniqueness metric is also binary-valued: 1 if
the attribute set uniquely identifies the target refer-
ent among the distractors, 0 otherwise. Finally, the
Minimality metric is 1 if the selected attribute set
is as small as possible (while still uniquely identi-
fying the target referent), and 0 otherwise. We note
155
that attribute selection algorithms such as Dale &
Reiter?s are based on the observation that humans
frequently do not produce minimal referring ex-
pressions.
Results Table 1 shows the results for variations
of full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both
corpora, we see a large performance jump when
we use speaker constraints for all metrics other
than minimality. However, when we incorporate
recency constraints as well performance declines
slightly. We think this is due to two factors: first,
the speakers are not in a conversation, and self-
priming may have less impact than other-priming;
and second, we do not always have the most recent
prior utterance for a given speaker in the training
data.
Table 1 also shows the results for variations of
Dale & Reiter?s algorithm. When we incorporate
speaker constraints, we again see a performance
jump for most metrics, although compared to the
best possible case (full brevity) there is still room
for improvement.
We conclude that speaker constraints can be suc-
cessfully used in standard attribute selection algo-
rithms to improve performance on this task.
The most relevant previous research is the work
of (Gupta and Stent, 2005), who modified Dale
and Reiter?s algorithm to model speaker adaptation
in dialog. However, this corpus does not involve
dialog so there are no cross-speaker constraints,
only within-speaker constraints (speaker style and
priming).
6 Surface Realization Experiments
Data Preparation We first normalized the train-
ing data to correct misspellings and remove punc-
tuation and capitalization. We then extracted a
phrasal lexicon. For each attribute value we ex-
tracted the count of all realizations of that value in
the training data. We treated locations as a spe-
cial case, storing separately the realizations of x-
y coordinate pairs and single x- or y-coordinates.
We added a small number of realizations by hand
to cover possible attribute values not seen in the
training data.
Realization We ran two realization experiments.
In the first experiment, we used the human-
selected attribute sets in the development data as
the input to realization. If we want to maxi-
? SED ACC Bleu NIST
Furniture
Permute&Rank 0.01 3.54 0.14 0.311 3.87
Dependency 0.90 4.51 0.09 0.206 3.29
Dependency-S 0.60 4.30 0.11 0.232 3.91
Template 0.10 3.59 0.13 0.328 3.93
Template-S 0.10 2.80 0.28 0.403 4.67
People
Permute&Rank 0.04 4.37 0.10 0.227 3.15
Dependency 0.70 6.10 0.00 0.072 2.35
Dependency-S 0.50 5.84 0.02 0.136 3.05
Template 0.80 3.87 0.07 0.250 3.18
Template-S 0.70 3.79 0.15 0.265 3.59
Overall
Permute&Rank .01/.04 3.92 0.12 0.271 4.02
Dependency 0.9/0.7 5.24 0.05 0.146 3.23
Dependency-S 0.6/0.5 5.01 0.07 0.187 3.98
Template 0.1/0.8 3.77 0.10 0.285 4.09
Template-S 0.1/0.7 3.26 0.22 0.335 4.77
Table 2: Results for realization using speakers? at-
tribute selection (SED: String Edit Distance, ACC:
String Accuracy)
mize humanlikeness, then using these attribute sets
should give us an idea of the best possible perfor-
mance of our realization methods. In the second
experiment, we used the attribute sets output by
our best-performing attribute selection algorithms
(FB-sf and DR-sf) as the input to realization.
Metrics We report performance of our surface
realizers using the metrics used for the REG 2008
shared challenge and standard metrics used in the
natural language generation and machine trans-
lation communities. String Edit Distance (SED)
is a measure of the number of words that would
have to be added, deleted, or replaced in order to
transform the generated referring expression into
the one produced by the human. As used in the
REG 2008 shared challenge, it is unnormalized, so
its values range from zero up. Accuracy (ACC)
is binary-valued: 1 if the generated referring ex-
pression is identical to that produced by the hu-
man (after spelling correction and normalization),
and 0 otherwise. Bleu is an n-gram based met-
ric that counts the number of 1, 2 and 3 grams
shared between the generated string and one or
more (preferably more) reference strings (Papenini
et al, 2001). Bleu values are normalized and range
from 0 (no match) to 1 (perfect match). Finally,
the NIST metric is a variation on the Bleu met-
ric that, among other things, weights rare n-grams
higher than frequently-occurring ones (Dodding-
ton, 2002). NIST values are unnormalized.
156
SED ACC Bleu NIST
Furniture
FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf
Permute&Rank 3.97 4.22 0.09 0.06 .291 .242 3.82 3.32
Dependency 4.80 5.03 0.04 0.03 .193 .105 3.32 2.46
Dependency-S 4.71 4.88 0.06 0.04 .201 .157 3.74 3.26
Template 3.89 4.56 0.09 0.05 .283 .213 3.48 3.22
Template-S 3.26 3.90 0.19 0.12 .362 .294 4.41 4.07
People
Permute&Rank 4.75 5.82 0.09 0.03 .171 .110 2.70 2.31
Dependency 6.35 6.91 0.00 0.00 .068 .073 1.81 1.86
Dependency-S 5.94 6.18 0.01 0.00 .108 .113 2.73 2.41
Template 3.62 4.24 0.07 0.04 .231 .138 2.88 1.35
Template-S 3.76 4.38 0.12 0.06 .201 .153 2.76 1.88
Overall
Permute&Rank 4.33 4.96 0.09 0.05 .236 .235 3.73 3.72
Dependency 5.51 6.00 0.02 0.01 .136 .091 2.97 2.50
Dependency-S 5.36 5.67 0.04 0.02 .159 .136 3.77 3.25
Template 3.76 4.41 0.08 0.05 .258 .180 3.69 2.89
Template-S 3.48 4.12 0.16 0.09 .288 .229 4.15 3.58
Table 3: Results for realization with different attribute selection algorithms
Furniture People
FB-sf DR-sf FB-sf DR-sf
Permute&Rank .01 .05 .05 .04
Dependency .9 .9 .9 .1
Dependency-S .2 .2 .4 .4
Template .8 .8 .8 .8
Template-S .6 .8 .8 .8
Table 4: Optimal ? values with different attribute
selection algorithms
Results Our experimental results are shown in
Tables 2 and 3. (These results are the results
obtained with the language model weighting that
gives best performance; the weights are shown in
Tables 2 and 4.) Our approaches work better for
the furniture domain, where there are fewer at-
tributes, than for the people domain. For both
domains, for automatic and human attribute se-
lection, the speaker-dependent Template-based ap-
proach seems to perform the best, then the speaker-
independent Template-based approach, and then
the Permute&Rank approach. However, we find
automatic metrics for evaluating generation qual-
ity to be unreliable. We looked at the output of the
surface realizers for the two examples in Section 2.
The best output for the example in Figure 1 is from
the FB-sf template-based speaker-dependent algo-
rithm, which is the big red sofa. The worst out-
put is from the DR-sf dependency-based speaker-
dependent algorithm, which is on the left red chair
with three seats. The best output for the exam-
ple in Figure 2 is from the FB-sf template-based
speaker-independent algorithm, which is the man
with the white beard. The worst output is from the
FB-sf dependency-based speaker-dependent algo-
rithm, which is beard man white.
Discussion The Template-S approach achieves
the best string edit distance scores, but it is not very
robust. If no examples are found in the training
data that realize (a superset of) the input attribute
set, neither Template approach will produce any
output.
The biggest cause of errors for the Permute and
Reorder approach is missing determiners and miss-
ing modifiers. The biggest cause of errors for the
Dependency approach is missing determiners and
reordered words. The Template approach some-
times has repeated words (e.g. ?middle?, where
?middle? referred to both x- and y-coordinates).
Here we report performance using automatic
metrics, but we find these metrics to be unreliable
(particularly in the absence of multiple reference
texts). Also, we are not sure that people would ac-
cept from a computer system output that is very
human-like in this domain, as the human-like out-
put is often ungrammatical and telegraphic (e.g.
?grey frontal table?). We plan to do a human eval-
uation soon to better analyze our systems? perfor-
mance.
7 Conclusions
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
157
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ?small? generation
tasks, and confirmed that data-driven approaches
to surface realization can work well using a range
of lexical, syntactic and semantic information.
We plan to explore the impact of human visual
search strategies (Rayner, 1998) on the referring
expression generation task. In addition, we are
planning a human evaluation of the generation sys-
tems? output. Finally, we plan to apply our algo-
rithms to a conversational task.
Acknowledgments
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
References
Bangalore, S. and O. Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In
Proc. COLING.
Bangalore, S., A. Emami, and P. Haffner. 2005. Fac-
toring global inference by enriching local represen-
tations. Technical report, AT&T Labs-Research.
Belz, A. and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proc. UCNLG+MT at MT Summit XI.
Belz, A. 2007. Probabilistic generation of weather
forecast texts. In Proc. NAACL/HLT.
Dale, R. and E. Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2).
Dice, L. 1945. Measures of the amount of ecologic
association between species. Ecology, 26.
Doddington, G. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. HLT.
Gupta, S. and A. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proc. UCNLG.
Kanthak, S., D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
Krahmer, E., S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
Langkilde, I. and K. Knight. 2000. Forest-based statis-
tical sentence generation. In Proc. NAACL.
Levelt, W., 1989. Speaking: From intention to articu-
lation, pages 222?226. MIT Press.
Nasr, A. and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+7).
Nenkova, A., R. Passonneau, and K. McKeown. 2007.
The Pyramid method: incorporating human con-
tent selection variation in summarization evaluation.
ACM Transactions on speech and language process-
ing, 4(2).
Papenini, K., S. Roukos, T. Ward, andW.-J. Zhu. 2001.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proc. ACL.
Rayner, K. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psycholog-
ical Bulletin, 124(3).
Reiter, E. and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Siddharthan, A. and A. Copestake. 2004. Generat-
ing referring expressions in open domains. In Proc.
ACL.
158
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 10?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Emotion Detection in Email Customer Care
Narendra Gupta, Mazin Gilbert, and Giuseppe Di Fabbrizio
AT&T Labs - Research, Inc.
Florham Park, NJ 07932 - USA
{ngupta,mazin,pino}@research.att.com
Abstract
Prompt and knowledgeable responses to cus-
tomers? emails are critical in maximizing cus-
tomer satisfaction. Such emails often con-
tain complaints about unfair treatment due to
negligence, incompetence, rigid protocols, un-
friendly systems, and unresponsive personnel.
In this paper, we refer to these emails as emo-
tional emails. They provide valuable feedback
to improve contact center processes and cus-
tomer care, as well as, to enhance customer re-
tention. This paper describes a method for ex-
tracting salient features and identifying emo-
tional emails in customer care. Salient fea-
tures reflect customer frustration, dissatisfac-
tion with the business, and threats to either
leave, take legal action and/or report to au-
thorities. Compared to a baseline system us-
ing word ngrams, our proposed approach with
salient features resulted in a 20% absolute F-
measure improvement.
1 Introduction
Emails are becoming the preferred communication
channel for customer service. For customers, it is a
way to avoid long hold times on call centers phone
calls and to keep a record of the information ex-
changes with the business. For businesses, it of-
fers an opportunity to best utilize customer service
representatives by evenly distributing the work load
over time, and for representatives, it allows time to
research the issue and respond to the customers in
a manner consistent with business policies. Busi-
nesses can further exploit the offline nature of this
channel by automatically routing the emails involv-
ing critical issues to specialized representatives. Be-
sides concerns related to products and services, busi-
nesses ensure that emails complaining about unfair
treatment due to negligence, incompetence, rigid
protocols and unfriendly systems, are always han-
dled with care. Such emails, referred to as emotional
emails, are critical to reduce the churn i.e., retain-
ing customers who otherwise would have taken their
business elsewhere, and, at the same time, they are a
valuable source of information for improving busi-
ness processes.
In recurring service oriented businesses, a large
number of customer emails may contain routine
complaints. While such complaints are important
and are addressed by customer service represen-
tatives, our purpose here is to identify emotional
emails where severity of the complaints and cus-
tomer dissatisfaction are relatively high. Emotional
emails may contain abusive and probably emotion-
ally charged language, but we are mainly interested
in identifying messages where, in addition to the
flames, the customer includes a concrete descrip-
tion of the problem experienced with the company
providing the service. In the context of customer
service, customers express their concerns in many
ways. Sometimes they convey a negative emotional
component articulated by phrases like disgusted
and you suck. In other cases, there is a minimum
emotional involvement by enumerating factual sen-
tences such as you overcharged, or take my
business elsewhere. In many cases, both
the emotional and factual components are actually
present. In this work, we have identified eight dif-
10
ferent ways that customers use to express their emo-
tions in emails. Throughout this paper, these ways
will be referred to as Salient Features. We cast the
identification of emotional email as a text classifi-
cation problem, and show that using salient features
we can significantly improve the identification ac-
curacy. Compared to a baseline system which uses
Boosting (Schapire, 1999) withnword n-grams fea-
tures, our proposed system using salient features re-
sulted in improvement in f-measure from 0.52 to
0.72.
In section 2, we provide a summary of previous
work and its relationship with our contribution. In
section 3, we describe our method for emotion de-
tection and extraction of salient features. A series of
experiments demonstrating improvement in classifi-
cation performance is presented in section 4. We
conclude the paper by highlighting the main contri-
bution of this work in section 5.
2 Previous Work
Extensive work has been done on emotion detec-
tion. In the context of human-computer dialogs, al-
though richer features including acoustic and intona-
tion are available, there is a general consensus (Lit-
man and Forbes-Riley, 2004b; Lee and Narayanan,
2005) about the use of lexical features to signifi-
cantly improve the accuracy of emotion detection.
Research has also been done in predicting ba-
sic emotions (also referred to as affects) within text
(Alm et al, 2005; Liu et al, 2003). To render speech
with prosodic contour conveying the emotional con-
tent of the text, one of 6 types of human emotions
(e.g., angry, disgusted, fearful, happy, sad, and sur-
prised) are identified for each sentence in the run-
ning text. Deducing such emotions from lexical con-
structs is a hard problem evidenced by little agree-
ment among humans. A Kappa value of 0.24-0.51
was shown in Alm et al (2005). Liu et al (2003)
have argued that the absence of affect laden surface
features i.e., key words, from the text does not imply
absence of emotions, therefore they have relied more
on common-sense knowledge. Instead of deducing
types emotions in each sentence, we are interested
in knowing if the entire email is emotional or not.
Additionally we are also interested in the intensity
and the cause of those emotions.
There is also a body of work in areas of creating
Semantic Orientation (SO) dictionaries (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003; Esuli and Sebastiani, 2005) and their use in
identifying emotions laden sentences and polarity
(Yu and Hatzivassiloglou, 2003; Kim and Hovy,
2004; Hu and Liu, 2004) of those emotions. While
such dictionaries provide a useful starting point,
their use alone does not yield satisfactory results. In
Wilson et al (2005), classification of phrases con-
taining positive, negative or neutral emotions is dis-
cussed. For this problem they show high agreement
among human annotators (Kappa of 0.84). They
also show that labeling phrases as positive, negative
or neutral only on the basis of presence of key word
from such dictionaries yields a classification accu-
racy of 48%. An obvious reason for this poor per-
formance is that semantic orientations of words are
context dependent.
Works reported in Wilson et al (2005); Pang et al
(2002) and Dave et al (2003) have attempted to
mitigate this problem by using supervised meth-
ods. They report classification results using a num-
ber of different sets of features, including unigram
word features. Wilson et al (2005) reports an im-
provement (63% to 65.7% accuracy) in performance
by using a host of features extracted from syntac-
tic dependencies. Similarly, Gamon (2004) shows
that the use of deep semantic features along with
word unigrams improve performances. Pang et al
(2002) and Dave et al (2003) on the other hand
confirmed that word unigrams provide the best clas-
sification results. This is in line with our experi-
ence as well and could be due to sparseness of the
data. We also used supervised methods to predict
emotional emails. To train predictive models we
used word ngrams (uni-, bi- and tri-grams) and a
number of binary features indicating the presence of
words/phrases from specific dictionaries.
Spertus (1997) discusses a system called Smoky
which recognizes hostile messages and is quite sim-
ilar to our work. While Smoky is interested in iden-
tifying messages that contain flames, our research
on emotional emails looks deeper to discover the
reasons for such flames. Besides word unigrams,
Smoky uses rules to derive additional features for
classification. These features are intended to cap-
ture different manifestations of the flames. Simi-
11
larly, in our work we also use rules (in our case im-
plemented as table look-up) to derive additional fea-
tures of emotional emails.
3 Emotion detection in emails
We use supervised machine learning techniques to
detect emotional emails. In particular, our emotion
detector is a statistical classifier model trained using
hand labeled training examples. For each example,
a set of salient features is extracted. The major com-
ponents of our system are described below.
3.1 Classifier
For detecting emotional emails we used Boostex-
ter as text classification. Our choice of machine
learning algorithm was not strategic and we have no
reason to believe that SVMs or maximum entropy?
based classifiers will not perform equally well.
Boostexter, which is based on the boosting family of
algorithms, was first proposed by Schapire (1999). It
has been applied successfully to numerous text clas-
sification applications (Gupta et al, 2005) at AT&T.
Boosting builds a highly accurate classifier by com-
bining many ?weak? base classifiers, each one of
which may only be moderately accurate. Boost-
ing constructs the collection of base classifiers iter-
atively. On each iteration t, the boosting algorithm
supplies the base learner weighted training data and
the base learner generates a base classifier ht. Set
of nonnegative weights wt encode how important it
is that ht correctly classifies each email. Generally,
emails that were most often misclassified by the pre-
ceding base classifiers will be given the most weight
so as to force the base learner to focus on the ?hard-
est? examples. As described in Schapire and Singer
(1999), Boostexter uses confidence rated base clas-
sifiers h that for every example x (in our case it is the
customer emails) output a real number h(x) whose
sign (-1 or +1) is interpreted as a prediction(+1 indi-
cates emotional email), and whose magnitude |h(x)|
is a measure of ?confidence.? The output of the final
classifier f is f(x) =
?T
t=1 ht(x), i.e., the sum of
confidence of all classifiers ht. The real-valued pre-
dictions of the final classifier f can be mapped onto a
confidence value between 0 and 1 by a logistic func-
tion;
conf(x = emotional email) =
1
1 + e?f(x)
.
The learning procedure in boosting minimizes the
negative conditional log likelihood of the training
data under this model, namely:
?
i
ln(1 + e?yif(xi)).
Here i iterates over all training examples and yi is
the label of ith example.
3.2 Feature extraction
Emotional emails are a reaction to perceived exces-
sive loss of time and/or money by customers. Ex-
pressions of such reactions in emails are salient fea-
tures of emotional emails. For our data we have
identified the eight features listed below. While
many of these features are of general nature and can
be present in most customer service related emo-
tional emails, in this paper we make no claims about
their completeness.
1. Expression of negative emotions: Explic-
itly expressing customers affective states
by phrases like it upsets me, I am
frustrated;
2. Expression of negative opinions about
the company: by evaluative expres-
sions like dishonest dealings,
disrespectful. These could also be
insulting expressions like stink, suck,
idiots;
3. Threats to take their business elsewhere:
by expression like business elsewhere,
look for another provider. These
expressions are neither emotional or evaluative;
4. Threats to report to authorities: federal
agencies, consumer protection.
These are domain dependent names of agen-
cies. The mere presence of such names implies
customer threat;
5. Threats to take legal action: seek
retribution, lawsuit. These ex-
pressions may also not be emotional or
evaluative in nature;
6. Justification about why they should have been
treated better. A common way to do this is
12
to say things like long time customer,
loyal customer, etc. Semantic orienta-
tions of most phrases used to express this fea-
ture are positive;
7. Disassociate themselves from the company,
by using phrases like you people, your
service representative, etc. These
are analogous to rule class ?Noun Phrases used
as Appositions? in Spertus (1997).
8. State what was done wrong to them: grossly
overcharged, on hold for hours,
etc. These phrases may have negative or
neutral semantic orientations.
In addition to the word unigrams, salient features of
emotional emails are also used for training/testing
the emotional email classifier. While labeling the
training data, labelers look for salient features within
the email and also the severity of the loss perceived
by the customer. For example, email 1 in Fig. 1 is la-
beled as emotional because customer perception of
loss is severe to the point that the customer may can-
cel the service. On the other hand, email 2 is not
emotional because customer perceived loss is not se-
vere to the point of service cancellation. This cus-
tomer would be satisfied in this instant if he/she re-
ceives the requested information in a timely fashion.
To extract salient features from an email, eight
separate lists of phrases customers use to express
each of the salient features were manually created.
These lists were extracted from the training data
and can be considered as basic rules that identify
emotional emails. In the labeling guide for critical
emails labelers were instructed to look for salient
features in the email and keep a list of encountered
phrases. We further enriched these lists by: a) us-
ing general knowledge of English, we added vari-
ations to existing phrases and b) searching a large
body of email text (different from testing) for differ-
ent phrases in which key words from known phrases
participated. For example from the known phrase
lied to we used the word lied and found a
phrase blatantly lied. Using these lists we
extracted eight binary salient features for each email,
indicating presence/absence of phrases from the cor-
responding list in the email.
1. You are making this very difficult
for me. I was assured that
my <SERVICE> would remain at
<CURRENCY> per month. But you
raised it to <CURRENCY> per
month. If I had known you were
going to go back on your word,
I would have looked for another
Internet provider. Present
bill is <CURRENCY>, including
<CURRENCY> for <SERVICE>.
2. I cannot figure out my current
charges. I have called several
times to straighten out a problem
with my service for <PHONENO1>
and <PHONENO2>. I am tired of
being put on hold. I cannot get
the information from the automated
phone service.
Figure 1: Email samples: 1) emotional; 2) neutral
4 Experiments and evaluation
We performed several experiments to compare the
performance of our emotional email classifier with
that using a ngram based text classifier. For these
experiments we labeled 620 emails as training ex-
amples and 457 emails as test examples. Training
examples were labeled independently by two differ-
ent labelers1 with relatively high degree of agree-
ment among them. Kappa (Cohen, 1960) value of
0.814 was observed versus 0.5-0.7 reported for emo-
tion labeling tasks (Alm and Sproat, 2005; Litman
and Forbes-Riley, 2004a). Because of the relatively
high agreement among these labelers, with differ-
ent back ground, we did not feel the need to check
the agreement among more than 2 labelers. Table
1 shows that emotional emails are about 12-13% of
the total population.
Set Number of examples Critical Emails
Training 620 12%
Test 457 13%
Table 1: Distribution of emotional emails
1One of the labeler was one of the authors of this paper and
other had linguistic back ground.
13
Due to the limited size of the training data we
used cross validation (leave-one-out) technique on
the test set to evaluate outcomes of different exper-
iments. In this round robin approach, each example
from the test set is tested using a model trained on
all remaining 1076 (620 plus 456) examples. Test
results on all 457 test examples are averaged.
Throughout all of our experiments, we computed
the classification accuracy of detecting emotional
emails using precision, recall and F-measure. No-
tice for our test data a classifier with majority vote
has a classification accuracy of 87%, but since none
of the emotional emails are identified, recall and F-
measure are both zero. On the other hand, a clas-
sifier which generates many more false positives
for each true positive, will have a lower classifi-
cation accuracy but a higher (non-zero) F-measure
than the majority vote classifier. Fig. 2 shows pre-
cision/recall curves for different experiments. The
black circles represent the operating point corre-
sponding to the best F-measure for each curve. Ac-
tual values of these points are provided in Table 2.
As a baseline experiment we used word ngram
features to train a classifier model. The graph la-
beled as ?ngram features? in Fig. 2 shows the per-
formance of this classifier. The best F-measure in
this case is only 0.52. Obviously this low perfor-
mance can be attributed to the small training set and
the large feature space formed by word ngrams.
Recall Prec. F-Mes.
Ngram Features 0.45 0.61 0.52
Rule based:
Threshholding on
Salient Features counts
? 4 0.41 0.93 0.57
? 3 0.63 0.74 0.68
? 2 0.81 0.53 0.63
Salient Features 0.77 0.65 0.70
ngram &
Salient Features 0.65 0.81 0.72
Ngram &
Random Features 0.57 0.67 0.61
Table 2: Recall and precision corresponding to best F-
measure for different classifier models
Figure 2: Precision/Recall curves for different experi-
ments. Large black circles indicate the operating point
with best F-Measure
4.1 Salient features
The baseline system was compared with a similar
system using salient features. First, we used a sim-
ple classification rule that we formulated by look-
ing at the training data. According to this rule, if
an email contained three or more salient features it
was classified as an emotional email. We classified
the test data using this rule and obtained and an F-
measure of 0.68 (see row labeled as ? 3 in Table 2).
Since no confidence thresholding can be used with
the deterministic rule, its performance is indicated
by a single point marked by the gray circle in Fig. 2.
This result clearly demonstrates high utility of our
salient features. To verify that the salient features
threshold count of 3 used in our simple classification
rule is the best, we also evaluated the performance of
the rule for the salient features with threshold count
of 2 and 4 (row labeled as ? 2 and ? 4 in Table 2).
In our next set experiments, we trained a clas-
sifier model using salient features alone and with
word ngrams. Corresponding cross validation re-
sults on the test data are annotated in Table 2 and in
14
Fig. 2 as ?Salient Features? and ?N-grams & Salient
Features?, respectively. Incremental improvement in
best F-measure clearly shows: a) BoosTexter is able
to learn better rules than the simple rule of identify-
ing three or more salient features. b) Even though
salient features provide a significant improvement
in performance, there is still discriminative informa-
tion in ngram features. A direct consequence of the
second observation is that the detection accuracy can
be further improved by extending/refining the phrase
lists and/or by using more labeled data so that to
exploit the discriminative information in the word
ngram features.
Salient Features of emotional emails are the con-
sequence of our knowledge of how customers react
to their excessive loss. To empirically demonstrate
that eight different salient features used in identifi-
cation of emotional emails do provide complemen-
tary evidence, we randomly distributed the phrases
in eight lists. We then used them to extract eight
binary features in the same manner as before. Best
F-measure for this experiment is shown in the last
row of Table 2, and labeled as ?N-gram & Random
Features?. Degradation in performance of this ex-
periment clearly demonstrates that salient features
used by us provide complimentary and not redun-
dant information.
5 Conclusions
Customer emails complaining about unfair treat-
ment are often emotional and are critical for busi-
nesses. They provide valuable feedback for improv-
ing business processes and coaching agents. Fur-
thermore careful handling of such emails helps to
improve customer retention. In this paper, we pre-
sented a method for emotional email identification.
We introduced the notion of salient features for
emotional emails, and demonstrated high agreement
among two labelers in detecting emotional emails.
We also demonstrated that extracting salient fea-
tures from the email text and using them to train a
classifier model can significantly improve identifi-
cation accuracy. Compared to a baseline classifier
which uses only the word ngrams features, the addi-
tion of the salient features improved the F-measure
from 0.52 to 0.72. Our current research is focused
on improving the salient feature extraction process.
More specifically by leveraging publically available
Semantic orientation dictionaries, and by enriching
our dictionaries using phrases extracted from a large
corpus by matching syntactic patterns of some seed
phrases.
References
Alm, Cecilia and Richard Sproat. 2005. Emotional
sequencing and development in fairy tales. In
Proceedings of the First International Conference
on Affective Computing and Intelligent Interac-
tion.
Alm, Cecilia Ovesdotter, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine
learning for text-based emotion prediction. In
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Morristown, NJ,
USA, pages 579?586.
Cohen, J. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement 20(1):37?46.
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews. In Proceedings of WWW. pages 519?
528.
Esuli, A. and F. Sebastiani. 2005. Determin-
ing the semantic orientation of terms through
gloss classificaion. In Proceedings of CIKM-05,
14th ACM International Conference on Informa-
tion and Knowledge Management. Bremen, DE.,
pages 617?624.
Gamon, M. 2004. Sentiment classification on cus-
tomer feedback data: Noisy data large feature
vectors and the role of linguistic analysis. In Pro-
ceedings of COLING 2004. Geneva, Switzerland,
pages 841?847.
Gupta, Narendra, Gokhan Tur, Dilek Hakkani-Tu?r,
Srinivas Banglore, Giuseppe Riccardi, and Mazin
Rahim. 2005. The AT&T Spoken Language
Understanding System. IEEE Transactions on
Speech and Audio Processing 14(1):213?222.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1997. Predicting the semantic orientation of ad-
15
jectives. In Proceedings of the Joint ACL/EACL
Conference. pages 174?181.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD). pages 168?177.
Kim, Soo-Min and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the International Conference on Computational
Linguistics (COLING).
Lee, Chul Min and Shrikanth S. Narayanan. 2005.
Toward detecting emotions in spoken dialogs.
IEEE Transactions on Speech and Audio Process-
ing 13(2):293?303.
Litman, D. and K. Forbes-Riley. 2004a. Annotat-
ing student emotional states in spoken tutoring
dialogues. In Proceedings of the 5th SIGdial
Workshop on Discourse and Dialogue (SIGdial).
Boston, MA.
Litman, D. and K. Forbes-Riley. 2004b. Predicting
student emotions in computer-human tutoring di-
alogues. In Proceedings of the 42nd Annual Meet-
ing of the Association for Compuational Linguis-
tics (ACL). Barcelone, Spain.
Liu, Hugo, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In IUI ?03: Proceedings of the 8th
international conference on Intelligent user inter-
faces. ACM Press, Miami, Florida, USA, pages
125?132.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Sentiment
classification using machine learning techniques.
In Proceedings of the 2002 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Philadelphia, Pennsylvania, pages
79?86.
Schapire, R.E. 1999. A brief introduction to boost-
ing. In Proceedings of IJCAI.
Schapire, R.E. and Y. Singer. 1999. Improved
boosting algorithms using confidence-rated pre-
dictions. Machine Learning 37(3):297?336.
Spertus, Ellen. 1997. Smokey: Automatic recogni-
tion of hostile messages. In In Proc. of Innova-
tive Applications of Artificial Intelligence. pages
1058?1065.
Turney, P. and M. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation
from association. ACM Transactions on Informa-
tion Systems 21(4):315?346.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Morristown, NJ, USA, pages 347?
354.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
16
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 36?43,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Capturing the stars: predicting ratings for service and product reviews
Narendra Gupta, Giuseppe Di Fabbrizio and Patrick Haffner
AT&T Labs - Research, Inc.
Florham Park, NJ 07932 - USA
{ngupta,pino,haffner}@research.att.com
Abstract
Bloggers, professional reviewers, and con-
sumers continuously create opinion?rich web
reviews about products and services, with the
result that textual reviews are now abundant on
the web and often convey a useful overall rat-
ing (number of stars). However, an overall rat-
ing cannot express the multiple or conflicting
opinions that might be contained in the text,
or explicitly rate the different aspects of the
evaluated entity. This work addresses the task
of automatically predicting ratings, for given
aspects of a textual review, by assigning a nu-
merical score to each evaluated aspect in the
reviews. We handle this task as both a re-
gression and a classification modeling prob-
lem and explore several combinations of syn-
tactic and semantic features. Our results sug-
gest that classification techniques perform bet-
ter than ranking modeling when handling eval-
uative text.
1 Introduction
An abundance of service and products reviews are
today available on the Web. Bloggers, professional
reviewers, and consumers continuously contribute
to this rich content both by providing text reviews
and often by assigning useful overall ratings (num-
ber of stars) to their overall experience. However,
the overall rating that usually accompanies online
reviews cannot express the multiple or conflicting
opinions that might be contained in the text, or ex-
plicitly rate the different aspects of the evaluated
entity. For example, a restaurant might receive an
overall great evaluation, while the service might
be rated below average due to slow and discourte-
ous wait staff. Pinpointing opinions in documents,
and the entities being referenced, would provide a
finer?grained sentiment analysis and a solid foun-
dation to automatically summarize evaluative text,
but such a task becomes even more challenging
when applied to a generic domain and with unsu-
pervised methods. Some significant contributions
by Hu and Liu (2004), Popescu and Etzioni (2005),
and Carenini et al (2006) illustrate different tech-
niques to find and measure opinion orientation in
text documents. Other work in sentiment analysis
(often referred as opinion mining) has explored sev-
eral facets of the problem, ranging from predicting
binary ratings (e.g., thumbs up/down) (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Yu and Hatzivas-
siloglou, 2003; Pang and Lee, 2004; Yi and Niblack,
2005; Carenini et al, 2006), to more detailed opin-
ion analysis methods predicting multi?scale ratings
(e.g., number of stars) (Pang and Lee, 2005; Sny-
der and Barzilay, 2007; Shimada and Endo, 2008;
Okanohara and Tsujii, 2005).
This paper focuses on multi?scale multi?aspect
rating prediction for textual reviews. As mentioned
before, textual reviews are abundant, but when try-
ing to make a buy decision on a specific product
or service, getting sufficient and reliable informa-
tion can be a daunting and time consuming task.
On one hand, a single overall rating does not pro-
vide enough information and could be unreliable, if
not supported over a large number of independent
reviews/ratings. From another standpoint, reading
through a large number of textual reviews in order
to infer the aspect ratings could be quite time con-
36
suming, and, at the same time, the outcome of the
evaluation could be biased by the reader?s interpre-
tation. In this work, instead of a single overall rat-
ing, we propose to provide ratings for multiple as-
pects of the product/service. For example, in the
case of restaurant reviews, we consider ratings for
five aspects: food, atmosphere, value, service and
overall experience. In Lu et al (2009) such aspect
ratings are called rated aspect summaries, in Shi-
mada and Endo (2008) they have been referred to as
seeing stars and in Snyder and Barzilay (2007) they
are referred to as multi?aspect ranking. We use su-
pervised learning methods to train predictive models
and use a specific decoding method to optimize the
aspect rating assignment to a review.
In the rest of this paper, we overview the previous
work in this research area in Section 2. We describe
the corpus used in the experiments in Section 3. In
Section 4 we present various learning algorithms we
experimented with. Section 5 explains our experi-
mental setup, while in Section 6 we provide analy-
sis of our experimental results. Section 7 presents
details of modeling and exploiting interdependence
among aspect ratings to boost the predictive perfor-
mance. Finally, we describe the future work in Sec-
tion 8 and report the concluding remarks in Section
9.
2 Related work
Previous work in sentiment analysis (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Yu and Hatzivas-
siloglou, 2003; Pang and Lee, 2004; Yi and Niblack,
2005; Carenini et al, 2006) used different informa-
tion extraction and supervised classification meth-
ods to detect document opinion polarity (positive vs.
negative).
By conducting a limited experiment with two sub-
jects, Pang and Lee (2005) demonstrated that hu-
mans can discern more grades of positive or neg-
ative judgments by accurately detecting small dif-
ferences in rating scores by just looking at review
text. In a five?star schema, for instance, the subjects
were able to perfectly distinguish rating differences
of three notches or 1.5 stars and correctly perceive
differences of one star with an average of 83% accu-
racy. This insight confirms that a five?star scale im-
proves the evaluative information and is perceived
with the right discriminative strength by the users.
Pang and Lee applied supervised and semi?
supervised classification techniques, in addition to
linear, -insensitive SVM regression methods, to
predict the overall ratings of movie reviews in three
and four?class star rating schemes. In the books
review domain, Okanohara and Tsujii (2005) show
a similar approach with comparable results. Both
these contributions consider only overall ratings,
which could be sufficient to describe sentiment for
movie and book reviews. Two recent endeavors,
Snyder and Barzilay (2007) for the restaurants do-
main, and Shimada and Endo (2008) for video
games reviews, exploit multi?aspect, multiple rat-
ing modeling. Snyder and Barzilay (2007) assume
inter?dependencies among the aspect ratings and
capture the relationship between the ratings via the
agreement relation. The agreement relation de-
scribes the likelihood that the user will express the
same rating for all the rated aspects. Interestingly,
Snyder and Barzilay (2007) show that modeling as-
pect rating dependencies helps to reduce the rank
loss by keeping in consideration the contributions of
the opinion strength of the single aspects referred
to in the review. They incorporated information
about the aspect rating dependencies in a regression
model and minimized the loss (overall grief ) dur-
ing decoding. Shimada and Endo (2008) exploits
a more traditional supervised machine learning ap-
proach where features such as word unigrams and
frequency counts are used to train classification and
regression models. As detailed in Section 4, our ap-
proach is similar to (Snyder and Barzilay, 2007) in
terms of review domain and algorithms, but we im-
prove on their performances by optimizing classifi-
cation predictions.
3 Reviews corpus
Labeled data containing textual reviews and aspect
ratings are rarely available. For this work, reviews
were mined from the we8there.com websites
around the end of 2008. we8there.com is one
of the few websites, where, besides textual reviews,
numerical ratings for different aspects of restaurants
are also provided. Aspects used for rating on this
site are: food, service, atmosphere, value and over-
all experience. Ratings are given on a scale from 1
37
to 5; for example, reviewers posting opinions were
asked to rank their overall experience by the follow-
ing prompt: ?On a scale of 1 (poor) to 5 (excel-
lent), please rate your dining experience?, and then
enter a textual description by the prompt: ?Please
describe your experience (30 words minimum)?. At
the time of mining, this site had reviews of about
3,800 restaurants with an average of two reviews
per restaurant containing around eight sentences per
review. A more detailed description is reported in
Table 1. Table 2 shows review ratings distribution
over the aspects. Rating distributions are evidently
skewed toward high ratings with 70% or more re-
views appraised as excellent (rank 5) or above aver-
age (rank 4).
Restaurants 3,866
Reviewers 4,660
Reviews 6,823
Average reviews per restaurant 1.76
Number of sentences 58,031
Average sentences per review 8.51
Table 1: Restaurant review corpus
Rating 1 2 3 4 5
Atmosphere 6.96 7.81 14.36 23.70 47.18
Food 8.24 6.72 9.86 18.53 56.65
Value 9.37 7.57 13.61 23.27 46.18
Service 11.83 6.12 11.91 22.00 48.14
Overall 10.48 8.19 10.17 20.47 50.69
Table 2: Restaurant review ratings distribution per aspect
4 Learning algorithms
In this section we review machine learning ap-
proaches that can predict ordinal ratings from textual
data. The goal is ordinal regression, which differs
from traditional numeric regression because the tar-
gets belong to a discrete space, but also differs from
classification as one wants to minimize the rank loss
rather than the classification error. The rank loss is
the average difference between actual and predicted
ratings and is defined as
RankLoss =
1
N
N?
i
(|rai ? rpi |)
where rai and rpi are actual and predicted ratings
respectively for the instance i, and N is the number
of considered reviews. There are several possible
approaches to such a regression problem.
1. The most obvious approach is numeric regres-
sion. It is implemented with a neural network
trained using the back?propagation algorithm.
2. Ordinal regression can also be implemented
with multiple thresholds (r ? 1 thresholds are
used to split r ranks). This is implemented
with a Perceptron based ranking model called
PRank (Crammer and Singer, 2001).
3. Since rating aspects with values 1, 2, 3, 4 and
5 is an ordinal regression problem it can also
be interpreted as a classification problem, with
one class per possible rank. In this interpreta-
tion, ordering information is not directly used
to help classification. Our implementation uses
binary one-vs-all Maximum Entropy (MaxEnt)
classifiers. We will see that this very simple
approach can be extended to handle aspect in-
terdependency, as presented in section 7.
In order to provide us with a broad range of rating
prediction strategies, we experimented with a nu-
merical regression technique viz. neural network, an
ordinal regression technique viz. PRank algorithm,
and a classification technique viz. MaxEnt classi-
fiers. Their implementations are straightforward and
the run?time highly efficient. After selecting a strat-
egy from the previous list, one could consider more
advanced algorithms described in Section 8.
5 Experimental setup
To predict aspect ratings of restaurants from their
textual reviews we used the reviews mined from the
we8there.com website to train different regres-
sion and classification models as outlined in Sec-
tion 4. In each of our experiments, we randomly
partitioned the data into 90% for training and 10%
for testing. This ensures that the distributions in
training and test data are identical. All the results
quoted in this paper are averages of 10?fold cross?
validation over 6,823 review examples. We con-
ducted repeatedly the same experiment on 10 differ-
ent training/test partitions and computed the average
rank loss over all the test partitions.
38
Figure 1 illustrates the training process where
each aspect is described by a separate predictive
model.
Figure 1: Predictive model training
We introduce the following notation that will be
helpful in further discussion. There are m aspects.
For our data m is 5. Each aspect can have an inte-
ger rating from 1 to k. Once again, for our data k
is 5. Each review text document t can have ratings
r, which is a vector of m integers ranging 1 to k
(bold faced letters indicate vectors). Using the train-
ing data (t1, r1)..(ti, ri)..(tn, rn) we train m rating
predictors Rj(ti), one for each aspect j. Given text
ti predictor Rj outputs the most likely rating l for
the aspect j. In these experiments, we treated aspect
rating predictors as independent of each other. For
each rated aspect, predictor models were trained in-
dependently and were used independently to predict
ratings for each aspect.
5.1 Feature Selection
We experimented with different combinations of
features, including word unigrams, bigrams, word
chunks, and parts?of?speech (POS) chunks. The as-
sumption is that bag?of?unigrams capture the ba-
sic word statistic and that bigrams take into account
some limited word context. POS chunks and word
chunks discriminate the use of words in the con-
text (e.g., a simple form word sense disambigua-
tion) and, at the same time, aggregate co?occurring
words (e.g., collocations), such as saute?ed onions,
buffalo burger, etc.
Most of the web?based reviews do not usually
provide fine?grained aspect ratings of products or
services, however, they often give an overall rating
evaluation. We therefore also experimented with the
overall rating as an input feature to predict the more
specific aspect ratings. Results of our experiments
are shown in Table 3.
Aspects Uni- Bi- Word Word Uni
gram gram Chunks Chunks gram
POS Overall
Chunks Rating
Atmosphere 0.740 0.763 0.789 0.783 0.527
Food 0.567 0.571 0.596 0.588 0.311
Value 0.703 0.725 0.751 0.743 0.406
Service 0.627 0.640 0.651 0.653 0.377
overall 0.548 0.559 0.577 0.583
Average 0.637 0.652 0.673 0.670 0.405
Table 3: Average ranking losses using MaxEnt classifier
with different feature sets
Review sentences
<s>Poor service made the lunch unpleasant.</s>
<s>The staff was unapologetic about their mistakes they
just didn?t seem to care.</s>
<s>For example the buffalo burger I ordered with sauteed
onions and fries initially was served without either.</s>
<s> The waitress said she?d bring out the onions but had
I waited for them before eating the burger the meat would
have been cold.</s>
<s>Other examples of the poor service were that the
waitress forgot to bring out my soup when she brought out
my friend?s salad and we had to repeatedly ask to get our
water glasses refilled.</s>
<s> When asked how our meal was I did politely mention my
dissatisfaction with the service but the staff person?s
response was silence not even a simple I m sorry.</s>
<s>I won?t return. </s>
Word Chunks
poor service made lunch unpleasant
staff unapologetic mistakes n?t care
example buffalo burger ordered sauteed onions fries served
waitress said bring onions waited eating burger meat cold
other examples poor service waitress forgot bring
soup brought friend salad repeatedly ask to get water
glasses refilled
asked meal politely mention dissatisfaction service
staff person response silence not simple sorry
n?t return
Parts-of-speech Chunks
NNP NN VBD NN JJ
NN JJ NNS RB VB
NN NN NN VBD NN NNS NNS VBN
NN VBD VB NNS VBD VBG NN NN JJ
JJ NNS JJ NN NN NN VB NN VBD NN NN RB VB TO VB NN VBZ VBN
VBD NN RB VB NN NN NN NN NN NN RB JJ JJ
RB VB
Table 4: Example of reviews and extracted word chunks
Unigram and bigram features refer to unigram
words and bigram words occurring more than 3
times in the training corpus. Word chunks are ob-
tained by only processing Noun (NP), Verb (VP) and
Adjective (ADJP) phrases in the review text. We re-
moved modals and auxiliary verbs form VPs, pro-
nouns from NPs and we broke the chunks containing
conjunctions. Table 4 shows an example of extracted
word and parts?of?speech chunks from review text.
As can be seen, word chunks largely keep the infor-
mation bearing chunks phrases and remove the rest.
Parts?of?speech chunks are simply parts?of?speech
39
of word chunks.
In spite of richness of word and parts-of-speech,
chunks models using word unigrams perform the
best. We can attribute this to the data sparseness,
never?the?less, this results is in line with the find-
ings in Pang et al (2002). Last column of Table 3
clearly shows that use of overall rating as input fea-
ture significantly improves the performance. Clearly
this validates the intuition that aspect ratings are
highly co?related with overall ratings.
For the remaining experiments, we used only the
unigram words as features of the review text. Since
overall ratings given by reviewers may contain their
biases and since they may not always be available,
we did not use them as input features. Our hope
is that even though we train the predictors using re-
viewers provided aspect ratings, learned models will
be able to predict aspect ratings that depend only on
the review text and not on reviewer?s biases.
5.2 Results
Table 5 shows the results of our evaluation. Each
row in this table reports average rank loss of four
different models for each aspect. The baseline rank
loss is computed by setting the predicted rank for all
test examples to 5, as it is the most frequently occur-
ring rank in the training data (see also Table 2). As
shown in Table 5, the average baseline rank loss is
greater than one. The third column shows the results
from the neural network?based numeric regression.
The fourth column corresponds to the Perceptron?
based PRank algorithm. The MaxEnt classification
results appear in the last column. For these results,
we also detail the standard deviation over the 10
cross?validation trials.
Aspects Base- Back- Percep- MaxEnt
line Prop. tron
Atmosphere 1.036 0.772 0.930 0.740 ? 0.022
Food 0.912 0.618 0.739 0.567? 0.033
Value 1.114 0.740 0.867 0.703? 0.028
Service 1.116 0.708 0.851 0.627? 0.033
Overall 1.077 0.602 0.756 0.548? 0.026
Average 1.053 0.694 0.833 0.637? 0.020
Table 5: Average ranking losses using different predictive
models
6 Analysis
As can be seen in table Table 5, Atmosphere and
Value are the worst performers. This is caused by
the missing textual support for these aspects in the
training data. Using manual examination of small
number of examples, we found that only 62% of
user given ratings have supporting text for ratings
of these aspects in the reviews.
For example, in Figure 2 the first review clearly
expresses opinions about food, service and atmo-
sphere (under appall of cigarette smoke), but there is
no evidence about value which is ranked three, two
notches above the other aspects. Similarly, the sec-
ond review is all about food without any reference
to service rated two notches above the other aspects,
or atmosphere or value.
Because of this reason, we do not expect any pre-
dictive model to do much better than 62% accuracy.
Manual examination of a small number of examples
also showed that 55% of ratings predicted by Max-
Ent models are supported by the review text. This is
89% of 62% (a rough upper bound) and can be con-
sidered satisfactory given small data set and differ-
ences among reviewers rating preference. One way
to boost the predictive performance would be to first
determine if there is a textual support for an aspect
rating, and use only the supported aspect ratings for
training and evaluation of the models. This however,
will require labeled data that we tried to avoid in this
work.
Figure 2: Example of ratings with partial support in the
text review
To our surprise, MaxEnt classification, although it
minimizes a classification error, performs best even
40
when evaluated using rank loss. As can be noticed,
the performance difference over the second best ap-
proach (back?propagation) usually exceeds the stan-
dard deviation.
MaxEnt results are also comparable to those pre-
sented in Snyder and Barzilay (2007) using the
Good Grief algorithm. Snyder and Barzilay (2007)
also used data from the we8there.com website.
While we are using the same data source, note
the following differences: (i) Snyder and Barzilay
(2007) used only 4,488 reviews as opposed to the
6,823 reviews used in our work; (ii) our results are
averaged over a 10 fold cross validation. As shown
with the baseline results reported in Table 6, the im-
pact on performance that can be attributed to these
differences is small. The most significant number,
which should minimize the impact of data discrep-
ancy, is the improvement over baseline (labeled as
?gain over baseline? in Table 6). In that respect,
our MaxEnt classification?based approach outper-
forms Good Grief for every aspect. Note also that,
while we trained 5 independent predictors (one for
each aspect) using only word unigrams as features,
the Good Grief algorithm additionally modeled the
agreements among aspect ratings and used the pres-
ence/absence of opposing polarity words in reviews
as additional features.
Our results Snyder and Barzilay
(2007)
Aspects Base- Max Gain Base- Good Gain
line Ent. over line Grief over
Base- Base-
line line
Atmosphere 1.039 0.740 0.299 1.044 0.774 0.270
Food 0.912 0.567 0.344 0.848 0.534 0.314
Value 1.114 0.703 0.411 1.030 0.644 0.386
Service 1.116 0.627 0.489 1.056 0.622 0.434
Overall 1.077 0.548 0.529 1.028 0.632 0.396
Table 6: Comparison of rank loss obtained from MaxEnt
classification and those reported in Snyder and Barzilay
(2007)
7 Modeling interdependence among aspect
ratings
Inspired by these observations, we also trained Max-
Ent classifiers to predict pair?wise absolute differ-
ences in aspect ratings. Since the difference in rat-
ings of any two aspects can only be 0,1,2,3 or 4,
there are 5 classes to predict. For each test exam-
ple, MaxEnt classifiers output the posterior proba-
bility to observe a class given an input example. In
our approach, we use these probabilities to compute
the best joint assignment of ratings to all aspects.
More specifically, in our modified algorithm we use
2 types of classifiers.
? Rating predictors - Given the text ti, our clas-
sifiers Rj(ti) output vectors pi consisting of
probabilities pil for text ti having a rating l for
the aspect j.
? Difference predictors - These correspond to
classifiers Dj,k(ti) which output vectors pij,k .
Elements of these vectors are the probabilities
that the difference between ratings of aspects j
and k is 0,1,2,3 and 4, respectively. While j
ranges from 1 to m, k ranges from 1 to j ? 1.
Thus, we trained a total of m(m ? 1)/2 = 10
difference predictors.
To predict aspect ratings for a given review text
ti we use both rating predictors and difference pre-
dictors and generate output probabilities. We then
select the most likely values of ri for text ti that sat-
isfies the probabilistic constraints generated by the
predictors. More specifically:
ri = argmax
r?R
m?
j=1
log(pirj ) +
m?
j=1
j?
k=1
log(p
ij,k
|rj?rk|
)
R is the set of all possible ratings assignments to
all aspects. In our case it contains 55 (3,125) tuples.
tuples in our case. Like Snyder and Barzilay (2007),
we also experimented with additional features in-
dicating presence of positive and negative polarity
words in the review text. Besides unigrams in the
review text, we also used 3 features: the counts of
positive and negative polarity words and their dif-
ferences. Polarity labels are obtained from a dictio-
nary of about 700 words. This dictionary was cre-
ated by first collecting words used as adjectives in a
corpus of un?related review text. We then retained
only those words in the dictionary that, in a context
free manner generally conveyed positive or negative
evaluation of any object, event or situation. Some
41
examples of negative words are awful, bad, bor-
ing, crude, disappointing, horrible, worst, worth-
less, yucky and some examples of positive words
are amazing, beautiful, delightful, good, impecca-
ble, lovable, marvelous, pleasant, recommendable,
sophisticated, superb, wonderful, wow. Table 7 first
shows gains obtained from using difference predic-
tors, and then gains from using polarity word fea-
tures in addition to these difference predictors.
Aspects MaxEnt + Difference + Polarity
predictor features
Atmosphere 0.740 0.718 0.707
Food 0.567 0.552 0.547
Value 0.703 0.695 0.685
Service 0.627 0.627 0.617
Overall 0.548 0.547 0.528
Average 0.637 0.628 0.617
Table 7: Improved rank loss obtained by using difference
predictors and polarity word features
8 Future Work
We have presented 3 algorithms chosen for their
simplicity of implementation and run time effi-
ciency. The results suggest that our classification?
based approach performs better than numeric or or-
dinal regression approaches. Our next step is to ver-
ify these results with the more advanced algorithms
outlined below.
1. For many numeric regression problems,
(boosted) classification trees have shown good
performance.
2. Several multi?threshold implementations of
Support Vector Ordinal Regression are com-
pared in Chu and Keerthi (2005). While they
are more principled than the Perceptron?based
PRank, their implementation is significantly
more complex. A simpler approach that per-
forms regression using a single classifier ex-
tracts extended examples from the original ex-
amples (Li and Lin, 2007).
3. Among classification?based approaches,
nested binary classifiers have been pro-
posed (Frank and Hall, 2001) to take into
account the ordering information, but the
prediction procedure based on classifier score
difference is ad?hoc.
9 Conclusions
Textual reviews for different products and services
are abundant. Still, when trying to make a buy deci-
sion, getting sufficient and reliable information can
be a daunting task. In this work, instead of a sin-
gle overall rating we focus on providing ratings for
multiple aspects of the product/service. Since most
textual reviews are rarely accompanied by multiple
aspect ratings, such ratings must be deduced from
predictive models. Several authors in the past have
studied this problem using both classification and re-
gression models. In this work we show that even
though the aspect rating problem seems like a re-
gression problem, maximum entropy classification
models perform the best. Results also show a strong
inter?dependence in the way users rate different as-
pects.
Acknowledgments
We thank Remi Zajac and his team for their support.
References
Carenini, Giuseppe, Raymond T. Ng, and Adam
Pauls. 2006. Interactive multimedia summaries of
evaluative text. In Proceedings of Intelligent User
Interfaces (IUI). ACM Press, pages 124?131.
Chu, Wei and S. Sathiya Keerthi. 2005. New ap-
proaches to support vector ordinal regression. In
Proceedings of the 22nd International Conference
on Machine Learning. Bonn, Germany, pages
145?152.
Crammer, Koby and Yoram Singer. 2001. Prank-
ing with ranking. In Advances in Neural Infor-
mation Processing Systems 14. MIT Press, pages
641?647.
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews. In WWW ?03: Proceedings of the 12th
International Conference on World Wide Web.
ACM, New York, NY, USA, pages 519?528.
Frank, Eibe and Mark Hall. 2001. A simple ap-
proach to ordinal classification. In Proceedings
42
of the Twelfth European Conference on Machine
Learning. Springer-Verlag, Berlin, pages 145?
156.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Pro-
ceedings of the 10th ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining. ACM, New York, NY, USA, pages 168?
177.
Li, Ling and Hsuan-Tien Lin. 2007. Ordinal re-
gression by extended binary classification. In
B. Scho?lkopf, J. C. Platt, and T. Hofmann, edi-
tors, Advances in Neural Information Processing
Systems 19. MIT Press, pages 865?872.
Lu, Yue, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In WWW ?09: Proceedings of the 18th
International Conference on World Wide Web.
ACM, New York, NY, USA, pages 131?140.
Okanohara, Daisuke and Jun-ichi Tsujii. 2005. As-
signing polarity scores to reviews using machine
learning techniques. In Robert Dale, Kam-Fai
Wong, Jian Su, and Oi Yee Kwong, editors, IJC-
NLP. Springer, volume 3651 of Lecture Notes in
Computer Science, pages 314?325.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In Pro-
ceedings of the Association for Computational
Linguistics (ACL). pages 271?278.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment catego-
rization with respect to rating scales. In Proceed-
ings of the Association for Computational Lin-
guistics (ACL). pages 115?124.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP). pages 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of the Human Language
Technology Conference and the Conference on
Empirical Methods in Natural Language Process-
ing (HLT/EMNLP).
Shimada, Kazutaka and Tsutomu Endo. 2008. See-
ing several stars: A rating inference task for a doc-
ument containing several evaluation criteria. In
Advances in Knowledge Discovery and Data Min-
ing, 12th Pacific-Asia Conference, PAKDD 2008.
Springer, Osaka, Japan, volume 5012 of Lecture
Notes in Computer Science, pages 1006?1014.
Snyder, Benjamin and Regina Barzilay. 2007. Mul-
tiple aspect ranking using the Good Grief algo-
rithm. In Proceedings of the Joint Human Lan-
guage Technology/North American Chapter of the
ACL Conference (HLT-NAACL). pages 300?307.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proceedings
of the Association for Computational Linguistics
(ACL). pages 417?424.
Yi, Jeonghee and Wayne Niblack. 2005. Senti-
ment mining in WebFountain. In Proceedings of
the International Conference on Data Engineer-
ing (ICDE).
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
43
Proceedings of the 8th International Natural Language Generation Conference, pages 54?63,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Hybrid Approach to Multi-document Summarization of
Opinions in Reviews
Giuseppe Di Fabbrizio
Amazon.com?
Cambridge, MA - USA
pino@difabbrizio.com
Amanda J. Stent
Yahoo! Labs
New York, NY - USA
stent@labs.yahoo.com
Robert Gaizauskas
Department of Computer Science
University of Sheffield, Sheffield - UK
R.Gaizauskas@sheffield.ac.uk
Abstract
We present a hybrid method to gener-
ate summaries of product and services re-
views by combining natural language gen-
eration and salient sentence selection tech-
niques. Our system, STARLET-H, re-
ceives as input textual reviews with asso-
ciated rated topics, and produces as out-
put a natural language document summa-
rizing the opinions expressed in the re-
views. STARLET-H operates as a hybrid
abstractive/extractive summarizer: using
extractive summarization techniques, it se-
lects salient quotes from the input reviews
and embeds them into an automatically
generated abstractive summary to provide
evidence for, exemplify or justify posi-
tive or negative opinions. We demon-
strate that, compared to extractive meth-
ods, summaries generated with abstractive
and hybrid summarization approaches are
more readable and compact.
1 Introduction
Text summarization is a well-established area of
research. Many approaches are extractive, that
is, they select and stitch together pieces of text
from the input documents (Goldstein et al., 2000;
Radev et al., 2004). Other approaches are abstrac-
tive; they use natural language generation (NLG)
techniques to paraphrase and condense the con-
tent of the input documents (Radev and McKeown,
1998). Most summarization methods focus on dis-
tilling factual information by identifying the in-
put documents? main topics, removing redundan-
cies, and coherently ordering extracted phrases or
sentences. Summarization of sentiment-laden text
(e.g., product or service reviews) is substantially
different from the traditional text summarization
task: instead of presenting facts, the summarizer
must present the range of opinions and the con-
sensus opinion (if any), and instead of focusing
on one topic, the summarizer must present infor-
mation about multiple aspects of the target entity.
?This work was conducted when in AT&T Labs Research
In addition, traditional summarization techniques
discard redundancies, while for summarization of
sentiment-laden text, similar opinions mentioned
multiple times across documents are crucial indi-
cators of the overall strength of the sentiments ex-
pressed by the writers (Ku et al., 2006).
Extractive summaries are linguistically interest-
ing and can be both informative and concise. Ex-
tractive summarizers also require less engineer-
ing effort. On the other hand, abstractive sum-
maries tend to have better coverage for a particular
level of conciseness, and to be less redundant and
more coherent (Carenini et al., 2012). They also
can be constructed to target particular discourse
goals, such as summarization, comparison or rec-
ommendation. Although in theory, it is possible to
produce user-targeted extractive summaries, user-
specific review summarization has only been ex-
plored in the context of abstractive summarization
(Carenini et al., 2012).
Current systems for summarizing sentiment-
laden text use information about the attributes of
the target entity (or entities); the range, mean
and median of the ratings of each attribute; re-
lationships between the attributes; and links be-
tween ratings/attributes and text elements in the
input documents (Blair-Goldensohn et al., 2008).
However, there is other information that no sum-
marizer currently takes into account. This in-
cludes temporal features (in particular, depending
on how old the documents are, products and ser-
vices evaluated features may change over time)
and social features (in particular, social or demo-
graphic similarities or relationships between doc-
ument authors and the reader of the summary).
In addition, there is an essential contradiction at
the heart of current review summarization sys-
tems: the system is authoring the review, but the
opinions contained therein are really attributable
to one or more human authors, and those attribu-
tions are not retained in the review summary. For
example, consider the extractive summary gener-
ated with STARLET-E (Di Fabbrizio et al., 2013):
?Delicious. Can?t wait for my next trip to Buffalo.
GREAT WINGS. I have rearranged business trips
54
so that I could stop in and have a helping or two
of their wings?. We were seated promptly and the
staff was courteous.
The summary is generated by selecting sen-
tences from reviews to reflect topics and rating dis-
tributions contained in the input review set. Do the
two sentences about wings reflect one (repeated)
opinion from a single reviewer, or two opinions
from two separate reviewers? The ability to at-
tribute subjective statements to known sources can
make them more trustworthy; conversely, in the
absence of the ability to attribute, a reader may
become skeptical or confused about the content of
the review summary. We term this summarization
issue opinion holder attribution.
In this paper we present STARLET-H, a hybrid
review summarizer that combines the advantages
of the abstractive and extractive approaches to
summarization and implements a solution to the
opinion holder attribution problem. STARLET-H
takes as input a set of reviews, each review of
which is labeled with aspect ratings and author-
ship. It generates hybrid abstractive/extractive re-
views that: 1) are informative (achieve broad cov-
erage of the input opinions); 2) are concise and
avoid redundancy; 3) are readable and coherent (of
high linguistic quality); 4) can be targeted to the
reader; and 5) address the opinion holder attribu-
tion problem by directly referring to reviewers au-
thorship when embedding phrases from reviews.
We demonstrate through a comparative evalua-
tion of STARLET-H and other review summariz-
ers that hybrid review summarization is preferred
over extractive summarization for readability, cor-
rectness, completeness (achieving broad coverage
of the input opinions) and compactness.
2 Hybrid summarization
Most NLG research has converged around a ?con-
sensus architecture? (Reiter, 1994; Rambow and
Korelsky, 1992), a pipeline architecture including
the following modules: 1) text planning, which
determines how the presentation content is se-
lected, structured, and ordered; 2) sentence plan-
ning, which assigns content to sentences, inserts
discourse cues to communicate the structure of
the presentation, and performs sentence aggrega-
tion and optionally referring expression genera-
tion; and 3) surface realization, which performs
lexical selection, resolves syntactic issues such as
subject-verb and noun-determiner agreement, and
assigns morphological inflection to produce the fi-
nal grammatical sentence. An abstractive sum-
marizer requires the customization of these three
modules. Specifically, the text planner has to se-
lect and organize the information contained in the
input reviews to reflect the rating distributions over
the aspects discussed by the reviewers. The sen-
tence planner must perform aggregation in such a
way as to optimize summary length without con-
fusing the reader, and insert discourse cues that
reveal the discourse structure underlying the sum-
mary. And, finally, the surface realizer must select
the proper domain lexemes to express positive and
negative opinions.
Figure 1: STARLET-H hybrid review summarizer
architecture
Figure 1 shows the architecture we adopted for
our STARLET-H hybrid review summarizer. We
use a generate-and-select approach: the decisions
to be made at each stage of the NLG process just
outlined are complex, and because they are not
truly independent of each other, a generate-and-
rank approach may be best (allowing each com-
ponent to express alternative ?good? choices and
choosing the best combination of these choices
at the end). Our text planner is responsible for
analyzing the input text reviews, extracting per-
attribute rating distributions and other meta-data
from each review, and synthesizing this informa-
tion to produce one or more discourse plans. Our
sentence planner, JSPARKY ? a freely-available
toolkit (Stent and Molina, 2009) ? can produce
several candidate sentence plans and their corre-
sponding surface realizations through SimpleNLG
(Gatt and Reiter, 2009). The candidate summaries
are ranked by calculating their perplexity with a
language model trained over a large number of
sentences from additional restaurant reviews col-
lected over the Web.
2.1 Data
STARLET-H uses review data directly, as input
to summarization, and indirectly, as training data
for statistical models and for lexicons for various
stages of the summarization process.
For training data, we used two sets of la-
beled data: one for the restaurant domain and
the other for the hotel domain. Both corpora in-
clude manually created sentence-level annotations
55
that identify: 1) opinion targets ? phrases refer-
ring to domain-relevant aspects that are the tar-
gets of opinions expressed by the reviewer; 2)
opinion phrases ? phrases expressing an opinion
about an entity, and its polarity (positive or neg-
ative); and 3) opinion groups ? links between
opinion phrases and their opinion targets. Ad-
ditionally, sentences satisfying the properties of
quotable sentence mentioned in Section 3 were la-
beled as ?quotable?. Table 1 summarizes the over-
all statistics of the two corpora. The annotated cor-
pora included the following rated aspects: Atmo-
sphere, Food, Service, Value, and Overall for the
Restaurant domain, and Location, Rooms, Service,
Value, and Overall for the Hotel domain1.
Table 1: Quote-annotated dataset statistics
Dataset RQ4000 HQ4000
Domain Restaurant Hotel Total
Reviews 484 404 888
Sentences 4,007 4,013 8,020
Avg sentences / review 8.28 9.93 9.03
2.2 Text planning
Reviews present highly structured information:
each contains an (implicit or explicit) rating of one
or more aspects of a target entity, possibly with
justification or evidence in the form of examples.
The rich information represented in these ratings
? either directly expressed in reviews or extracted
by an automatic rating prediction model ? can be
exploited in several ways. Our text planner re-
ceives as input a set of text reviews with associated
per-aspect ratings, and for each review proceeds
through the following analysis steps:
Entity description Extracts basic information
to describe the reviewed entity, e.g., the name and
location of the business, number of total and recent
reviews, review dates and authors, etc.
Aspect distribution categorization Catego-
rizes the rating distribution for each aspect of the
reviewed entity as one of four types: 1) positive
? most of the ratings are positive; 2) negative ?
most of the ratings are negative; 3) bimodal ?
most of the ratings are equally distributed into
positive and negative values; 4) uniform ? ratings
are uniformly distributed across the rating scale.
1Some examples from the annotated corpus are avail-
able at the following address http://s286209735.
onlinehome.us/starlet/examples
Quote selection and attribution Classifies each
sentence from the reviews using a quote selec-
tion model (see Section 3), which assigns to
each sentence an aspect, a rating polarity (posi-
tive/negative) and a confidence score. The classi-
fied sentences are sorted by confidence score and
a candidate quote is selected for each aspect of the
target entity that is explicitly mentioned in the in-
put reviews. Each quote is stored with the name
of the reviewer for correct authorship attribution.
Note that when the quote selection module is ex-
cluded, the system is an abstractive summarizer,
which we call STARLET-A.
Lexical selection Selects a lexicon for each as-
pect based on its rating polarity and its assigned
rating distribution type. Lexicons are extracted
from the corpus of annotated opinion phrases de-
scribed in Di Fabbrizio et al. (2011).
Aspect ordering Assigns an order over aspects
using aspect ordering statistics from our training
data (see Section 2.4), and generates a discourse
plan, using a small set of rhetorical relations orga-
nized into summary templates (see below).
2.3 Sentence planning
The STARLET-H sentence planner relies on rhetor-
ical structure theory (RST) (Mann and Thomp-
son, 1989). RST is a linguistic framework that
describes the structure of natural language text
in terms of the rhetorical relationships organizing
textual units. Through a manual inspection of our
training data, we identified a subset of six RST re-
lations that are relevant to review summarization:
concession, contrast, example, justify, list, and
summary. We further identified four basic RST-
based summary templates, one for each per-aspect
rating distribution: mostly positive, mostly nega-
tive, uniform across all ratings, and bimodal (e.g.,
both positive and negative). These summary tem-
plates are composed by the text planner to build
summary discourse plans. The JSPARKY sen-
tence planner then converts input discourse plans
into sentence plans, performing sentence order-
ing, sentence aggregation, cross-sentence refer-
ence resolution, sentence tense and mode (passive
or active), discourse cue insertion, and the selec-
tion of some lexical forms from FrameNet (Baker
et al., 1998) relations.
Figure 2 illustrates a typical RST template rep-
resenting a positive review summary and corre-
sponding text output generated by JSPARKY. For
each aspect of the considered domain, the sentence
plan strategy covers a variety of opinion distribu-
56
Figure 2: Example of RST structure generated by the text planner for mostly positive restaurant reviews
tion conditions (e.g., positive, negative, bimodal,
and uniform), and provides alternative RST struc-
tures when the default relation is missing due to
lack of data (e.g., missing quotes for a specific as-
pect, missing information about review distribu-
tion over time, missing type of cuisine, and so on).
The sentence template can also manage lexical
variations by generating multiple options to qual-
ify a specific pair of aspect and opinion polarity.
For instance, in case of very positive reviews about
restaurant atmosphere, it can provide few alterna-
tive adjective phrases (e.g., great, wonderful, very
warm, terrific, etc.) that can be used to produce
more summary candidates (over-generate) during
the final surface realization stage.
2.4 Ordering aspects and polarities
The discourse structure of a typical review consists
of a summary opinion, followed by a sequence
of per-aspect ratings with supporting information
(e.g., evidence, justification, examples, and con-
cessions). The preferred sequence of aspects to
present in a summary depends on the specific re-
view domain, the overall polarity of the reviews,
and how opinion polarity is distributed across the
reviewed aspects. Looking at our training data, we
observed that when the review is overall positive,
positively-rated aspects are typically discussed at
the beginning, while negatively-rated aspects tend
to gather toward the end. The opposite order
seems predominant in the case of negative re-
views. When opinions are mixed, aspect ordering
strategies are unclear. To most accurately model
aspect ordering, we trained weighted finite state
transducers for the restaurant and hotel domains
using our training data. Weighted finite state
transducers (WFSTs) are an elegant approach to
search large feature spaces and find optimal paths
by using well-defined algebraic operations (Mohri
et al., 1996). To find the optimal ordering of rated
aspects in a domain, the text planner creates a
WFST with all the possible permutations of the
input sequence of aspects, and composes it with a
larger WFST trained from bigram sequences of as-
pects extracted from the relevant domain-specific
review corpus. The best path sequence is then de-
rived from the composed WFST by applying the
Viterbi decoding algorithm. For instance, the se-
quence of aspects and polarities represented by the
string: value-n service-p overall-n food-n
atmosphere-n2 is first permuted in all the dif-
ferent possible sequences and then converted into
a WFST. Then the permutation network is fully
composed with the larger, corpus-trained WFST.
The best path is extracted by dynamic program-
ming, producing the optimal sequence service-p
value-n overall-n atmosphere-n food-n.
2We postfix the aspect label with a ?-p? for positive and
with ?-n? for negative opinion
57
2.5 Lexical choice
It can be hard to choose the best opinion words,
especially when the summary must convey the
different nuances between ?good? and ?great? or
?bad? and ?terrible? for a particular aspect in a
particular domain. For our summarization task,
we adopted a simple approach. From our anno-
tated corpora, we mined both positive and negative
opinion phrases with their associated aspects and
rating polarities. We sorted the opinion phrases
by frequency and then manually selected from the
most likely phrases adjective phrases that may cor-
rectly express per-aspect polarities. We then split
positive and negative phrases into two levels of
polarity (i.e., strongly positive, weakly positive,
weakly negative, strongly negative) and use the
number of star ratings to select the right polarity
during content planning. For bimodal and uniform
polarity distributions, we manually defined a cus-
tomized set of terms. Sample lexical terms are re-
ported in Table 2.
3 Quote selection modeling
There are several techniques to extract salient
phrases from text, often related to summariza-
tion problems, but there is a relatively little work
on extracting quotable sentences from text (Sar-
mento and Nunes, 2009; De La Clergerie et al.,
2009) and none, to our knowledge, on extract-
ing quotes from sentiment-latent text. So, what
does make a phrase quotable? What is a proper
quote definition that applies to review summa-
rization? We define a sentiment-laden quotable
phrase as a text fragment with the following char-
acteristics: attributable ? clearly ascribable to the
author; compact and simple ? it is typically a
relatively short phrase (between two and twenty
words) which contains a statement with a simple
syntactic structure and independent clauses; self-
contained its meaning is clear and self-contained,
e.g., it does not include pronominal references to
entities outside its scope; on-topic ? it refers to
opinion targets (i.e., aspects) in a specific domain;
sentiment-laden ? it has one or two opinion tar-
gets and an unambiguous overall polarity. Exam-
ple quotable phrases are presented in Table 3.
To automatically detect quotes from reviews,
we adopted a supervised machine learning ap-
proach based on manually labeled data. The clas-
sification task consists of classifying both aspects
and polarity for the most frequent aspects defined
for each domain. Quotes for the aspect food, for
instance, are split into positive and negative classi-
Table 3: Example of quotes from restaurant and
hotel domains
?Everyone goes out of their way to make sure you
are happy with their service and food.?
?The stuffed mushrooms are the best I?ve ever had
as was the lasagna.?
?Service is friendly and attentive even during
the morning rush.?
?I?ve never slept so well away from home loved
the comfortable beds.?
?The price is high for substandard mattresses
when I pay this much for a room.?
fication labels: food-p and food-n, respectively.
We identify quotable phrases and associate them
with aspects and rating polarities all in one step,
but multi-step approaches could also be used (e.g.,
a configuration with binary classification to detect
quotable sentences followed by another classifica-
tion model for aspect and polarity detection).
3.1 Training quote selection models
We used the following features for automatic
quote selection: ngrams ? unigrams, bigrams, and
trigrams from the input phrases with frequency
higher than three; binned number of words ?
we assumed a maximum length of twenty words
per sentence and created six bins, five of them
uniformly distributed from one to twenty, and the
sixth including all the sentences of length greater
than twenty words; POS ? unigrams, bigrams, and
trigrams for part of speech tags; chunks ? uni-
grams, bigrams, and trigrams for shallow parsed
syntactic chunks; opinion phrases ? a binary fea-
ture to keep track of the presence of positive and
negative opinion phrases as defined in our anno-
tated review corpora. In our annotated data only
the most popular aspects are well represented.
For instance, food-p and overall-p are the most
popular positive aspects among the quotable sen-
tences for the restaurant domain, while quotes on
atmosphere-n and value-n are scarce. The dis-
tribution is even further skewed for the hotel do-
main; there are plenty of quotes for overall-p
and service-p and only 13 samples (0.43%) for
location-n. To compensate for the broad vari-
ation in the sample population, we used stratified
sampling methods to divide the data into more bal-
anced testing and training data We generated 10-
fold stratified training/test sets. We experimented
with three machine learning algorithms: MaxEnt,
SVMs with linear kernels, and SVMs with poly-
nomial kernels. The MaxEnt learning algorithm
produced statistically better classification results
than the other algorithms when used with uni-
58
Table 2: Summarizer lexicon for most frequent adjective phrases by aspect and polarity
Domain Restaurant Hotel
Aspect positive very positive negative very negative Aspect positive very positive negative very negative
atmosphere nice, good,
friendly, com-
fortable
great, wonder-
ful, very warm,
terrific
ordinary,
depressing
really bad location good, nice,
pleasant
amazing,
awesome,
excellent, great
bad, noisy,
gloomy
very bad, very
bleak, very
gloomy
food good, deli-
cious, pleasant,
nice, hearty,
enjoyable
great, ex-
cellent, very
good, to die
for, incredible
very basic, un-
original, unin-
teresting, unac-
ceptable, sub-
standard, poor
mediocre, ter-
rible, horrible,
absolutely hor-
rible
rooms comfortable,
decent, clean,
good
amazing,
awesome,
gorgeous
average, basic,
subpar
terrible, very
limited, very
average
overall good, quite en-
joyable, lovely
wonderful, ter-
rific, very nice
bad, unremark-
able, not so
good
absolutely ter-
rible, horrible,
pretty bad
overall great, nice,
welcoming
excellent,
superb, perfect
average, noth-
ing great, noisy
quite bad, aw-
ful, horrible
service attentive,
friendly, pleas-
ant, courteous
very atten-
tive, great,
excellent, very
friendly
inattentive,
poor, not
friendly, bad
extremely
poor, horrible,
so lousy, awful
service friendly, great,
nice, helpful,
good
very friendly,
great, ex-
cellent, very
nice
average, basic,
not that great
very bad,
dreadful
value reasonable,
fair, good
value
very reason-
able, great
not that good,
not worthy
terrible, outra-
geous
value great, nice,
good, decent
very good,
wonderful,
perfectly good
not good not very good
gram features. This confirmed a general trend we
have previously observed in other text classifica-
tion experiments: with relatively small and noisy
datasets, unigram features provide better discrimi-
native power than sparse bigrams or trigrams, and
MaxEnt methods are more robust when dealing
with noisy data.
3.2 Quote selection results
Table 4 reports precision, recall and F-measures
averaged across 10-fold cross-validated test sets
with relative standard deviation. The label nq
identifies non-quotable sentences, while the other
labels refer to the domain-specific aspects and
their polarities. For the quote selection task, pre-
cision is the most important metric: missing some
potential candidates is less important than incor-
rectly identifying the polarity of a quote or sub-
stituting one aspect with another. The text planner
in STARLET-H further prunes the quotable phrases
by considering only the quote candidates with the
highest scores.
4 Evaluation
Evaluating an abstractive review summarizer in-
volves measuring how accurately the opinion con-
tent present in the reviews is reflected in the sum-
mary and how understandable the generated con-
tent is to the reader. Traditional multi-document
summarization evaluation techniques utilize both
qualitative and quantitative metrics. The former
require human subjects to rate different evaluative
characteristics on a Likert-like scale, while the lat-
ter relies on automatic metrics such as ROUGE
(Lin, 2004), which is based on the common num-
ber of n-grams between a peer, and one or several
gold-standard reference summaries.
Table 4: Quote, aspect, and polarity classification
performances for the restaurant domain
Precision Recall F-measure
atmosphere-n 0.233 0.080 0.115
atmosphere-p 0.589 0.409 0.475
food-n 0.634 0.409 0.491
food-p 0.592 0.634 0.612
nq 0.672 0.822 0.740
overall-n 0.545 0.275 0.343
overall-p 0.555 0.491 0.518
service-n 0.699 0.393 0.498
service-p 0.716 0.563 0.626
value-n 0.100 0.033 0.050
value-p 0.437 0.225 0.286
Hotel Precision Recall F-measure
location-n - - -
location-p 0.572 0.410 0.465
nq 0.678 0.836 0.748
overall-n 0.517 0.233 0.305
overall-p 0.590 0.492 0.536
rooms-n 0.628 0.330 0.403
rooms-p 0.667 0.573 0.612
service-n 0.517 0.163 0.240
service-p 0.605 0.500 0.543
value-n - - -
value-p 0.743 0.300 0.401
4.1 Evaluation materials
To evaluate our abstractive summarizer, we used
a qualitative metric approach and compared four
review summarizers: 1) the open source MEAD
system, designed for extractive summarization of
general text (Radev et al., 2004); 2) STARLET-E,
an extractive summarizer based on KL-divergence
and language modeling features that is described
in Di Fabbrizio et al. (2011); 3) STARLET-A, the
abstractive summarizer presented in this paper,
without the quote selection module; and 4) the hy-
brid summarizer STARLET-H.
We used the Amazon Mechanical Turk3 crowd-
3http://www.mturk.com
59
sourcing system to post subjective evaluation
tasks, or HITs, for 20 restaurant summaries. Each
HIT consists of a set of ten randomly ordered re-
views for one restaurant, and four randomly or-
dered summaries of reviews for that restaurant,
each one accompanied by a set of evaluation wid-
gets for the different evaluation metrics described
below. To minimize reading order bias, both re-
views and summaries were shuffled each time a
task was presented.
4.2 Evaluation metrics
We chose to carry out a qualitative evaluation
in the first instance as n-gram metrics, such as
ROUGE, are not necessarily appropriate for as-
sessing abstractive summaries. We asked each par-
ticipant to evaluate each summary by rating (using
a Likert scale with the following rating values: 1)
Not at all; 2) Not very; 3) Somewhat; 4) Very; 5)
Absolutely) the following four summary criteria:
readability ? a summary is readable if it is easy to
read and understand; correctness ? a summary is
correct if it expresses the opinions in the reviews;
completeness ? a summary is complete if it cap-
tures the whole range of opinions in the reviews;
compactness ? a summary is compact if it does
not repeat information.
4.3 Evaluation procedure
We requested five evaluators for each HIT. To in-
crease the chances of getting accurate evaluations,
we required evaluators to be located in the USA
and have an approval rate of 90% or higher (i.e.,
have a history of 90% or more approved HITs).
Manual examinations of the evaluation responses
did not show evidence of tampered data, but statis-
tical analysis showed unusually widely spread rat-
ing ranges. We noticed that most evaluators only
evaluated one or two HITs; this may imply that
they tried a few HITs and then decided not to con-
tinue because they found the task too long or the
instructions unclear. We then re-opened the evalu-
ation and directly contacted three additional eval-
uators, explaining in detail the instructions and the
evaluation scales. For consistency, we asked these
evaluators to complete the evaluation for all HITs.
In our analysis, we only included the five evalu-
ators (two from the first round of evaluation, and
three from the second) who completed all HITs.
For each evaluation metric, the five workers eval-
uated each of the 20 summaries, for a total of 100
ratings. Table 5 shows an example output of the
four summarization methods for a single set of
restaurant review documents.
Table 5: Example of MEAD-based, extractive, ab-
stractive and hybrid summaries from the restaurant
domain
MEAD Summary
a truly fun resturant everyone who like spicy
food should try the rattoes and for a mixed drink
the worm burner really good food and a fun place
to meet your friends. We were attracted by the
great big frog on the exterior of the building
and the fun RAZZOO S logo during a trip to the
mall. it was great the waitress was excellent
very prompt and courteous and friendly to all a
real complement to razzoo ?s way of service her
name was Tabitha. The best spicy food restaurant
with great server and fast service.
Extractive summary
Eat there every chance i get. We ve been going
here for years. Their crawfish etoufee is the
BEST. And such an awesome value for under 10.
Excellent as always. Some of the best food in
the area. I use to work at Razzoo s. It was
hard to leave. The people are great and so is
the food. I still go in there and miss it more
everytime. I Love Loney. It was great. Our
server was great and very observant. Try the
Chicken Tchoupitoulas.
Abstractive summary
Razzoo?s Cajun Cafe in Concord, NC is an American
restaurant. It has nine reviews. It had three
very recent reviews. It is an awesome, American
restaurant. It has many very positive reviews.
It has an excellent atmosphere and and has always
exceptional service.
Hybrid summary
Razzoo?s Cajun Cafe in Concord, NC is an American
restaurant. It has nine reviews. It had three
very recent reviews. It is an awesome, American
restaurant. It has many very positive reviews.
First it has a great price. Angela Haithcock
says ??And such an awesome value for under 10??.
Second it has always exceptional service and for
instance Danny Benson says ??it was great the
waitress was excellent very prompt and courteous
and friendly to all a real complement to razzoo?s
way of service her name was Tabitha??. Third it
has an excellent atmosphere. Last it has amazing
food. Scott Kern says ??Some of the best food in
the area??.
4.4 Evaluation results and discussion
The evaluation results are presented in Table 6.
Each evaluation metric is considered separately.
Average values for STARLET-E, STARLET-A and
STARLET-H are better than for MEAD across the
board, suggesting a preference for summaries of
sentiment-laden text that take opinion into ac-
count. To validate this hypothesis, we first com-
puted the non-parametric Kruskal-Wallis statistic
for each evaluation metric, using a chi-square test
to establish significance. The results were not sig-
nificant for any of the metrics.
However, when we conducted pairwise
Wilcoxon signed-rank tests considering two
summarization methods at a time, we found some
significant differences (p < 0.05). As predicted,
60
Table 6: Qualitative evaluation results
MEAD Starlet-E Starlet-A Starlet-H
Readability 2.95 3.17 3.64 3.74
Completeness 2.88 3.29 3.290 3.58
Compactness 3.07 3.35 3.80 3.58
Correctness 3.26 3.48 3.59 3.72
MEAD perform substantially worse than both
STARLET-A and STARLET-H on readability,
correctness, completeness, and compactness.
STARLET-A and STARLET-H are also preferred
over STARLET-E for readability. While STARLET-
A is preferred over STARLET-E for compactness
(the average length of the abstractive reviews
was 45.05 words, and of the extractive,102.30),
STARLET-H is preferred over STARLET-E for
correctness, since the former better captures the
reviewers opinions by quoting them in the ap-
propriate context. STARLET-A and STARLET-H
achieve virtually indistinguishable performance
on all evaluation metrics. Our evaluation results
accord with those of Carenini et al. (2012); their
abstractive summarizer had superior performance
in terms of content precision and accuracy when
compared to summaries generated by an extractive
summarizer. Carenini et al. (2012) also found that
the differences between extractive and abstractive
approaches are even more significant in the case
of controversial content, where the abstractive
system is able to more effectively convey the full
range of opinions.
5 Related work
Ganesan et al. (2010) propose a method to extract
salient sentence fragments that are both highly fre-
quent and syntactically well-formed by using a
graph-based data structure to eliminate redundan-
cies. However, this approach assumes that the in-
put sentences are already selected in terms of as-
pect and with highly redundant opinion content.
Also, the generated summaries are very short and
cannot be compared to a full-length output of a
typical multi-document summarizer (e.g., 100-200
words). A similar approach is described in Gane-
san et al. (2012), where very short phrases (from
two to five words) are collated together to generate
what the authors call ultra-concise summaries.
The most complete contribution to evaluative
text summarization is described in Carenini et al.
(2012) and it closely relates to this work. Carenini
et al. (2012) compare an extractive summariza-
tion system, MEAD* ? a modified version of
the open source summarization system MEAD
(Radev et al., 2004) ? with SEA, an abstractive
summarization system, demonstrating that both
systems perform equally well. The SEA approach,
although better than traditional MEAD, has a few
drawbacks. Firstly, the sentence selection mecha-
nism only considers the most frequently discussed
aspects, leaving the decision about where to stop
the selection process to the maximum summary
length parameter. This could leave out interest-
ing opinions that do not appear with sufficient fre-
quency in the source documents. Ideally, all opin-
ions should be represented in the summary accord-
ing to the overall distribution of the input reviews.
Secondly, Carenini et al. (2012) use the absolute
value of the sum of positive and negative contri-
butions to determine the relevance of a sentence in
terms of opinion content. This flattens the aspect
distributions since sentences with very negative or
very positive polarity or with numerous opinions,
but with moderate polarity strengths, will get the
same score, regardless. Finally, it does not ad-
dress the opinion holder attribution problem leav-
ing the source of opinion undefined. In contrast,
STARLET-H follows reviews aspect rating distri-
butions both to select quotable sentences and to
summarize relevant aspects. Moreover, it explic-
itly mentions the opinion source in the embedded
quoted sentences.
6 Conclusions
In this paper, we present a hybrid summarizer for
sentiment-laden text that combines an overall ab-
stractive summarization method with an extrac-
tive summarization-based quote selection method.
This summarizer can provide the readability and
correctness of abstractive summarization, while
addressing the opinion holder attribution problem
that can lead readers to become confused or mis-
led about who is making claims that they read in
review summaries. We plan a more extensive eval-
uation of STARLET-H. Another potential area of
future research concerns the ability to personal-
ize summaries to the user?s needs. For instance,
the text planner can adapt its communicative goals
based on polarity orientation ? a user can be more
interested in exploring in detail negative reviews
? or it can focus more on specific (user-tailored)
aspects and change the order of the presentation
accordingly. Finally, it could be interesting to cus-
tomize the summarizer to provide an overview of
what is available in a specific geographic neigh-
borhood and compare and contrast the options.
61
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. The Berkeley FrameNet Project. In
Proceedings of the 17th International Con-
ference on Computational Linguistics - Vol-
ume 1, COLING ?98, pages 86?90, Strouds-
burg, PA, USA, 1998. Association for Com-
putational Linguistics. doi: 10.3115/980451.
980860. URL http://dx.doi.org/10.
3115/980451.980860.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan
McDonald, Tyler Neylon, George Reis, and Jeff
Reynar. Building a Sentiment Summarizer for
Local Service Reviews. In NLP in the Informa-
tion Explosion Era, 2008.
Giuseppe Carenini, Jackie Chi Kit Cheung, and
Adam Pauls. Multi-Document Summarization
of Evaluative Text. Computational Intelligence,
2012.
E?ric De La Clergerie, Beno??t Sagot, Rosa Stern,
Pascal Denis, Gae?lle Recource?, and Victor
Mignot. Extracting and Visualizing Quotations
from News Wires. In Language and Technol-
ogy Conference, Poznan, Pologne, 2009. Projet
Scribo (po?le de compe?titivite? System@tic).
Giuseppe Di Fabbrizio, Ahmet Aker, and Robert
Gaizauskas. STARLET: Multi-document Sum-
marization of Service and Product Reviews with
Balanced Rating Distributions. In Proceedings
of the 2011 IEEE International Conference on
Data Mining (ICDM) Workshop on Sentiment
Elicitation from Natural Text for Information
Retrieval and Extraction (SENTIRE), Vancou-
ver, Canada, december 2011.
Giuseppe Di Fabbrizio, Ahmet Aker, and Robert
Gaizauskas. Summarizing On-line Product and
Service Reviews Using Aspect RatingDistribu-
tions and Language Modeling. Intelligent Sys-
tems, IEEE, 28(3):28?37, May 2013. ISSN
1541-1672. doi: 10.1109/MIS.2013.36.
Kavita Ganesan, ChengXiang Zhai, and Jiawei
Han. Opinosis: A Graph-Based Approach to
Abstractive Summarization of Highly Redun-
dant Opinions. In Proceedings of the 23rd Inter-
national Conference on Computational Linguis-
tics, COLING ?10, pages 340?348, Strouds-
burg, PA, USA, 2010. Association for Compu-
tational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Evelyne
Viegas. Micropinion Generation: An Unsu-
pervised Approach to Generating Ultra-concise
Summaries of Opinions. In Proceedings of the
21st international conference on World Wide
Web, WWW ?12, pages 869?878, New York,
NY, USA, 2012. ACM.
Albert Gatt and Ehud Reiter. SimpleNLG: A Re-
alisation Engine for Practical Applications. In
Proceedings of the 12th European Workshop
on Natural Language Generation, ENLG ?09,
pages 90?93, Stroudsburg, PA, USA, 2009. As-
sociation for Computational Linguistics.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell,
and Mark Kantrowitz. Multi-document Sum-
marization by Sentence Extraction. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on
Automatic summarization - Volume 4, pages 40?
48, Stroudsburg, PA, USA, 2000. Association
for Computational Linguistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
Opinion Extraction, Summarization and Track-
ing in News and BlogCorpora. In Proceedings
of AAAI-2006 Spring Symposium on Computa-
tional Approaches to Analyzing Weblogs, 2006.
Chin-Yew Lin. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proc. ACL
workshop on Text Summarization Branches Out,
page 10, 2004.
William C. Mann and Sandra A. Thompson.
Rhetorical Structure Theory: A Theory of Text
Organization. In Livia Polanyi, editor, The
Structure of Discourse. Ablex, Norwood, NJ,
1989.
Mehryar Mohri, Fernando Pereira, and Michael
Riley. Weighted Automata in Text and Speech
Processing. In ECAI-96 Workshop, pages 46?
50. John Wiley and Sons, 1996.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Ho-
racio Saggion, Simone Teufel, Michael Top-
per, Adam Winkel, and Zhu Zhang. MEAD
? A Platform for Multidocument Multilingual
Text Summarization. In Conference on Lan-
guage Resources and Evaluation (LREC), Lis-
bon, Portugal, May 2004.
Dragomir R. Radev and Kathleen R. McKe-
own. Generating natural language summaries
from multiple on-line sources. Computational
Linguistiscs, 24(3):470?500, September 1998.
ISSN 0891-2017.
62
Owen Rambow and Tanya Korelsky. Applied Text
Generation. In Proceedings of the Third Confer-
ence on Applied Natural Language Processing,
pages 40?47, Trento, Italy, 1992. Association
for Computational Linguistics. 31 March - 3
April.
Ehud Reiter. Has a Consensus NL Generation Ar-
chitecture Appeared, and is it Psychologically
Plausible? In David McDonald and Marie
Meteer, editors, Proceedings of the 7th. Inter-
national Workshop on Natural Language gen-
eration (INLGW ?94), pages 163?170, Kenneb-
unkport, Maine, 1994.
Luis Sarmento and Se?rgio Nunes. Automatic Ex-
traction of Quotes and Topics from News Feeds.
In 4th Doctoral Symposium on Informatics En-
gineering (DSIE09), 2009.
Amanda Stent and Martin Molina. Evaluating Au-
tomatic Extraction of Rules for Sentence Plan
Construction. In Proceedings of the SIGDIAL
2009 Conference: The 10th Annual Meeting of
the Special Interest Group on Discourse and Di-
alogue, SIGDIAL ?09, pages 290?297, Strouds-
burg, PA, USA, 2009. Association for Compu-
tational Linguistics.
63
