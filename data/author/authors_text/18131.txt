Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 290?300,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Parsing low-resource languages using Gibbs sampling
for PCFGs with latent annotations
Liang Sun
1
Jason Mielens
2
1
Department of Mechanical Engineering
2
Department of Linguistics
The University of Texas at Austin The University of Texas at Austin
sally722@utexas.edu {jmielens,jbaldrid}@utexas.edu
Jason Baldridge
2
Abstract
PCFGs with latent annotations have been
shown to be a very effective model for phrase
structure parsing. We present a Bayesian
model and algorithms based on a Gibbs sam-
pler for parsing with a grammar with latent an-
notations. For PCFG-LA, we present an ad-
ditional Gibbs sampler algorithm to learn an-
notations from training data, which are parse
trees with coarse (unannotated) symbols. We
show that a Gibbs sampling technique is ca-
pable of parsing sentences in a wide variety
of languages and producing results that are
on-par with or surpass previous approaches.
Our results for Kinyarwanda and Malagasy in
particular demonstrate that low-resource lan-
guage parsing can benefit substantially from a
Bayesian approach.
1 Introduction
Despite great progress over the past two decades on
parsing, relatively little work has considered the prob-
lem of creating accurate parsers for low-resource lan-
guages. Existing work in this area focuses primarily on
approaches that use some form of cross-lingual boot-
strapping to improve performance. For instance, Hwa
et al. (2005) use a parallel Chinese/English corpus and
an English dependency grammar to induce an anno-
tated Chinese corpus in order to train a Chinese de-
pendency grammar. Kuhn (2004b) also considers the
benefits of using multiple languages to induce a mono-
lingual grammar, making use of a measure for data re-
liability in order to weight training data based on confi-
dence of annotation. Bootstrapping approaches such as
these achieve markedly improved results, but they are
dependent on the existence of a parallel bilingual cor-
pus. Very few such corpora are readily available, par-
ticularly for low-resource languages, and creating such
corpora obviously presents a challenge for many practi-
cal applications. Kuhn (2004a) shows some of the diffi-
culty in handling low-resource languages by examining
various tasks using Q?anjob?al as an example. Another
approach is that of Bender et al. (2002), who take a
more linguistically-motivated approach by making use
of linguistic universals to seed newly developed gram-
mars. This substantially reduces the effort by making
it unnecessary to learn the basic parameters of a lan-
guage, but it lacks the robustness of grammars learned
from data.
Recent work on Probabilistic Context-Free Gram-
mars with latent annotations (PCFG-LA) (Matsuzaki et
al., 2005; Petrov et al., 2006) have shown them to be
effective models for syntactic parsing, especially when
less training material is available (Liang et al., 2009;
Shindo et al., 2012). The coarse nonterminal symbols
found in vanilla PCFGs are refined by latent variables;
these latent annotations can model subtypes of gram-
mar symbols that result in better grammars and enable
better estimates of grammar productions. In this pa-
per, we provide a Gibbs sampler for learning PCFG-
LA models and show its effectiveness for parsing low-
resource languages such as Malagasy and Kinyawanda.
Previous PCFG-LA work focuses on the prob-
lem of parameter estimation, including expectation-
maximization (EM) (Matsuzaki et al., 2005; Petrov et
al., 2006), spectral learning (Cohen et al., 2012; Co-
hen et al., 2013), and variational inference (Liang et
al., 2009; Wang and Blunsom, 2013). Regardless of
inference method, previous work has used the same
method to parse new sentences: a Viterbi parse un-
der a new sentence-specific PCFG obtained from an
approximation of the original grammar (Matsuzaki et
al., 2005). Here, we provide an alternative approach to
parsing new sentences: an extension of the Gibbs sam-
pling algorithm of Johnson et al. (2007), which learns
rule probabilities in an unsupervised PCFG.
We use a Gibbs sampler to collect sampled trees
theoretically distributed from the true posterior distri-
bution in order to parse. Priors in a Bayesian model
can control the sparsity of grammars (which the inside-
outside algorithm fails to do), while naturally incorpo-
rating smoothing into the model (Johnson et al., 2007;
Liang et al., 2009). We also build a Bayesian model
for parsing with a treebank, and incorporate informa-
tion from training data as a prior. Moreover, we ex-
tend the Gibbs sampler to learn and parse PCFGs with
latent annotations. Learning the latent annotations is
a compute-intensive process. We show how a small
amount of training data can be used to bootstrap: af-
ter running a large number of sampling iterations on a
small set, the resulting parameters are used to seed a
smaller number of iterations on the full training data.
290
This allows us to employ more latent annotations while
maintaining reasonable training times and still making
full use of the available training data.
To determine the cross-linguistic applicability of
these methods, we evaluate on a wide variety of lan-
guages with varying amounts of available training data.
We use English and Chinese as examples of languages
with high data availability, while Italian, Malagasy, and
Kinyarwanda provide examples of languages with little
available data.
We find that our technique comes near state of the
art results on large datasets, such as those for Chinese
and English, and it provides excellent results on limited
datasets ? both artificially limited in the case of En-
glish, and naturally limited in the case of Italian, Mala-
gasy, and Kinyarwanda. This, combined with its abil-
ity to run off-the-shelf on new languages without any
supporting materials such as parallel corpora, make it a
valuable technique for the parsing of low-resource lan-
guages.
2 Gibbs sampling for PCFGs
Our starting point is a Gibbs Sampling algorithm for
vanilla PCFGs introduced by Johnson et al. (2007) for
estimating rule probabilities in an unsupervised PCFG.
We focus instead on using this algorithm for parsing
new sentences and then extending it to learn PCFGs
with latent annotations. We begin by summarizing the
Bayesian PCFG and Gibbs sampler defined by Johnson
et al. (2007).
Bayesian PCFG For a grammarG, each rule r in the
set of rules R has an associated probability ?
r
. The
probabilities for all the rules that expand the same non-
terminal A must sum to one:
?
A???R
?
A??
= 1.
Given an input corpusw=(w
(1)
, ? ? ? ,w
(n)
), we in-
troduce a latent variable t=(t
(1)
, ? ? ? , t
(n)
) for trees
generated by G for each sentence. The joint posterior
distribution of t and ? conditioned on w is:
p(t, ? | w) ? p(?)p(w | t)p(t | ?)
= p(?)(
?
n
i=1
p(w
(i)
| t
(i)
)p(t
(i)
| ?))
= p(?)(
?
n
i=1
p(w
(i)
| t
(i)
)
?
r?R
?
f
r
(t
(i)
r
)) (1)
Here f
r
(t) is the number of occurrences of rule r in the
derivation of t; p(w
(i)
| t
(i)
) = 1 if the yield of t
(i)
is
the sequence w
(i)
, and 0 otherwise.
We use a Dirichlet distribution parametrized by ?
A
:
Dir(?
A
) as the prior of the probability distribution for
all rules expanding non-terminal A (p(?
A
)). The prior
for all ?, p(?), is the product of all Dirichlet distri-
butions over all non-terminals A ? N : p(? | ?) =
?
A?N
p(?
A
| ?
A
).
Since the Dirichlet distribution is conjugate to the
Multinomial distribution, which we use to model the
likelihood of trees, the conditional posterior of ?
A
can
be updated as follows:
p
G
(? | t, ?) ? p
G
(t | ?)p(? | ?)
? (
?
r?R
?
f
r
(t)
r
)(
?
r?R
?
?
r
?1
r
)
=
?
r?R
?
f
r
(t)+?
r
?1
r
(2)
which is still a Dirichlet distribution with updated pa-
rameter f
r
(t) + ?
r
for each rule r ? R.
Gibbs sampler The parameters of the PCFG model
can be learned from an annotated corpus by simply
counting rules. However, parsing cannot be done di-
rectly with standard CKY as with standard PCFGs,
so we use the Gibbs sampling algorithm presented in
Johnson et al. (2007). An additional motivation for us-
ing this algorithm is that Johnson et al. use it for learn-
ing without annotated structures, and in future work we
seek to learn from fewer, and at times partial, annota-
tions.
An advantage of using Gibbs sampling for Bayesian
inference, as opposed to other approximation algo-
rithms such as Variational Bayesian inference (VB) and
Collapsed Variational Bayesian inference (CVB), is
that Markov Chain Monte Carlo (MCMC) algorithms
are guaranteed to converge to a sample from the true
posterior under appropriate conditions (Taddy, 2011).
Both VB and CVB converge to inaccurate and locally
optimal solutions, like EM. In some models, CVB can
achieve more accurate results due to weaker assump-
tions (Wang and Blunsom, 2013). Another advantage
of Gibbs sampling is that the sampler allows for parallel
computation by allowing each sentence to be sampled
entirely independently of the others. After each paral-
lel sampling stage, all model parameters are updated in
a single step, and the process then repeats (see ?2).
To sample the joint posterior p(t, ? | w), we sample
production probabilities ? and then trees t from these
conditional distributions:
p(t | ?,w, ?) =
?
n
i=1
p(t
i
| w
i
, ?) (3)
p(? | t,w, ?) =
?
A?N
Dir(?
A
| f
A
(t) + ?
A
) (4)
Step 1: Sample Rule Probabilities. Given trees t and
prior ?, the production probabilities ?
A
for each non-
terminal A?N are sampled from a Dirichlet distribu-
tion with parameters f
A
(t) + ?
A
. f
A
(t) is a vector,
and each component of f
A
(t), is the number of occur-
rences of one rule expanding nonterminal A.
Step 2: Sample Tree Structures. To sample trees from
p(t
i
| w
i
, ?), we use the efficient sampling scheme
used in previous work (Goodman, 1998; Finkel et al.,
2006; Johnson et al., 2007). There are two parts to this
algorithm. The first constructs an inside table as in the
Inside-Outside algorithm for PCFGs (Lary and Young,
1990). The second selects the tree by recursively sam-
pling productions from top to bottom.
291
Require: A is parent node of binary rule; w
i,k
is a
span of words: i+ 1 < k
function TREESAMPLER(A, i, k)
for i < j < k and pair of child nodes of
A:B,C do
P (j, B,C) =
?
A?BC
?p
B,i,j
?p
C,j,k
?
p
A,i,k
end for
Sample j?, B?, C? from multinomial distribution
for (j, B,C) with probabilities calculated above
return j?, B?, C?
end function
Algorithm 1: Sampling split position and rule to ex-
pand parent node
Consider a sentence w, with sub-spans w
i,k
=
(w
i+1
, ? ? ? , w
k
). Given ?, we construct the inside ta-
ble with entries p
A,i,k
for each nonterminal and each
word span w
i,k
: 0 ? i < k ? l, where p
A,i,k
=
P
G
A
(w
i,k
|?) is the probability that words i through k
were produced by the non-terminal A. The table is
computed recursively by
p
A,k?1,k
= ?
A?w
k
(5)
p
A,i,k
=
?
A?BC?R
?
i<j<k
?
A?BC
? p
B,i,j
? p
C,j,k
(6)
for all A,B,C ? N and 0 ? i < j < k ? l.
The resulting inside probabilities are then used to
generate trees from the distribution of all valid trees of
the sentence. The tree is generated from top to bottom
recursively with the function TreeSampler defined in
Algorithm 1.
In unsupervised PCFG learning, the rule probabil-
ities can be resampled using the sampled trees, then
used to reparse the corpus, and so on. We use this
property to refine latent annotations for the PCFG-LA
model described in the next section.
3 PCFG with latent annotations
When labeled trees are available, rule frequencies can
be directly extracted and used as priors for a PCFG.
However, when learning PCFG-LAs, we must learn the
fine-grained rules from the coarse trees, so we extend
the Gibbs sampler to assign latent annotations to unan-
notated trees. The resulting learned PCFG-LA parser
outputs samples of annotated trees so that we can ob-
tain unannotated trees after marginalizing.
3.1 Model
With the PCFG-LA model (Matsuzaki et al., 2005;
Petrov et al., 2006) fine-grained CFG rules are auto-
matically induced from training, effectively providing
a form of feature engineering without human interven-
tion. GivenH = {1, ? ? ? ,K}, a set of latent annotation
symbols, and x ? H:
? ?
A[x]?U
is the probability of rule A[x] ? U ,
where U ? N ?N ? T . The probabilities for all
rules that expand the same annotated non-terminal
must sum to one.
? ?
A[x],B,C?y,z
is the probability of assigning la-
tent annotation y, z to child nodes B,C of A[x].
?
y,z?H?H
?
A[x],B,C?y,z
= 1.
The inputs to the PCFG-LA are a CFG G with finite
number of latent annotations for each non-terminal, an
initial guess of probabilities of grammar rule ?
0
, and a
prior ?
?
is learned from training.
The joint posterior distribution of t and ?, ? condi-
tioned on w is:
p(t, ?, ? | w) ? p(?, ?)p(w | t)p(t | ?, ?)
= p(?)p(?)(
?
n
i=1
p(w
i
| t
i
)p(t
i
| ?, ?)) (7)
We assume that ? and ? are independent to get
P (?, ?) = P (?)P (?).
To learn parameters ?, ?, we use a Dirichlet distribu-
tion as a prior for both ? and ?. The distribution for all
rules expanding A[x] is:
P (? | ?
?
) =
?
A?N,x?H
P (?
A[x]
| ?
?
A[x]
) (8)
The distribution for latent annotations associated
with child nodes of A[x]? BC is:
P (? | ?
?
) =
?
y,z?H?H
P (?
A[x],B,C
| ?
?
A[x],B,C
).
(9)
With this setting, the conditional posterior of ?
A[x]
and ?
A[x],B,C
can be updated, as in ?2. For all unary
and binary rules r expanding A[x]:
?
A[x]
| t, ?
?
? Dir(f
r
(t) + ?
?
r
) (10)
Here, f
r
(t) is the number of occurrence of annotated
rule r in t. Also, for combination of latent annotations
y, z ? H ?H assigned to B,C in rule A[x]? B,C:
?
A[x],B,C
| t, ?
?
? Dir(f
d
(t) + ?
?
d
) (11)
Here, f
d
(t) is the number of occurrences of combina-
tion d in t.
3.2 Learning PCFG-LAs from raw text
To learn from raw text, we extend the sampler in ?2
to PCFG-LA. Given priors ?
?
, ?
?
and raw text, the al-
gorithm alternates between two steps. The first sam-
ples trees for the entire corpus; the second samples ?
and ? from Dirichlet distributions with updated param-
eters, combining priors and counts from sampled trees.
The algorithm then alternates between these steps un-
til convergence. The outputs are samples of ?, ? and
annotated trees.
The parsing process is specified in Algorithm 2. The
first step assigns a tree to a sentence, say w
0,l
. We first
292
Require: w
1
, ? ? ? , w
n
are raw sentences; ?
0
, ?
0
are
initial values; ?
?
, ?
?
are priors; M is the number
of iterations
function PARSE(w
1
, .., w
n
, ?
0
, ?
0
, ?
?
, ?
?
,M )
for iteration i = 1 to M do
for sentence s = 1 to n do
Calculate Inside Table
Sample tree nodes and associated latent
annotations, get tree structure t
(i)
s
end for
Sample ?
(i)
, ?
(i)
end for
for sentence s = 1 to n do
Marginalize the latent annotations to get
unannotated trees T
(1)
s
, ? ? ? , T
(M)
s
Find the mode of T
(1)
s
, ? ? ? , T
(M)
s
: T
s
end for
return T
1
, ? ? ? , T
n
end function
Algorithm 2: Parsing new sentences
construct an inside table (see ?2). Each entry in the ta-
ble stores the probability that a word span is produced
by a given annotated nonterminal. For root node S,
with ?, ? and inside table p
A[x],i,k
, we sample one an-
notation based on all p
S[x],0,l
, x ? H . Assume that
we sampled x for S, we further sample a rule to ex-
pand S[x] and possible splits of the span w
0,l
jointly.
Assume that we sampled nonterminals B,C to expand
S[x], where B is responsible for w
0,j
and C is respon-
sible for w
j,l
. We further sample annotations for B,C
together, say y, z. Then we sample rules and split po-
sitions to expand B[y] and C[z], and continue until
reaching the terminals.
This algorithm alone could be used for unsupervised
learning of PCFG-LA if we input a non-informed or
weakly-informed prior ?
?
and ?
?
. With access to
unannotated trees for training, we only need to assign
latent annotations to them and then use the frequen-
cies of these annotated rules as the prior when parsing.
The details of training when trees are available are il-
lustrated in ?3.3.
Once we have trees (with latent annotations), the
step of sampling ? and ? from a Dirichlet distribution
is direct. We need to count the number of occurrences
f
r
(t) for each rule r like A[x] ? U,U ? N ?N ? T
in updated annotated trees t, and draw ?
A[x]
from the
updated Dirichlet distribution Dir(f
A[x]
(t) + ?
?
A[x]
).
We also need to count the number of occurrences of
f
d
(t) for each combination of yz ? H?H assigned to
B,C givenA[x]? B,C in t, and draw ?
A[x],B,C
from
the updated Dirichlet distribution Dir(f
A[x],B,C
(t) +
?
?
A[x],B,C
) similarly.
To parse a sentence, we first calculate the inside table
and then sample the tree.
Calculate the inside table. Given ?,? and a string
w=w
0,l
, we construct a table with entries p
A[x],i,k
for
each A?N , x ? H and 0 ? i < k ? l, where
p
A[x],i,k
= P
G
A[x]
(w
i,k
|?, ?) is the probability that
words i through k were produced by the annotated non-
terminal A[x]. The table can be computed recursively,
for all A ? N , x ? H , by
p
A[x],k?1,k
= ?
A[x]?w
k
(12)
p
A[x],i,k
=
?
A[x]?BC:BC?N?N
?
j:i<j<k
?
yz?H?H
?
A[x]?BC
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
(13)
Sample the tree, top to bottom. First, from start sym-
bol S, sample latent annotation from multinomial with
probability pi
S[x]
p
S[x],0,l
for each x ? H . Next, given
annotated non-terminal A[x] and i, k, sample possible
child nodes and split positions from multinomial with
probability:
p(B,C, j) =
1
p
A[x],i,k
?
?
y,z?H
?
A[x]?BC
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
(14)
Here the probability is calculated by marginaliz-
ing all possible latent annotations for B,C, and
?
A[x]?BC
?
A[x]BC?yz
is the probability of choosing
B[y], C[z] to expandA[x], and p
B[y],i,j
p
C[z],j,k
are the
probabilities for B[y] and C[z] to be responsible for
word span w
i,j
and w
j,k
respectively. And p
A[x],i,k
is
the normalizing term.
Third, given A[x], B,C, i, j, k, sample annotations
for B,C from multinomial with probability:
p(y, z) =
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
?
y,z
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
(15)
A crucial aspect of this procedure is that all trees can
be sampled independently. This parallel process pro-
duces a substantial speed gain that is important partic-
ularly when using more latent annotations. After all
trees have been sampled (independently), the counts
from each individual tree are combined prior to the next
sampling iteration.
3.3 Learning from coarse training trees
In training, we need to learn the probabilities of fine-
grained rules given coarsely-labeled trees. We perform
Gibbs sampling on the training data by first iteratively
sampling probabilities and then assigning annotations
to tree nodes. We use the average counts of anno-
tated production rules from sampled trees to produce
the prior ?
?
and ?
?
incorporated into parsing raw sen-
tences.
We first index the non-terminal nodes of each tree T
by 1, 2, ? ? ? from top to bottom, and left to right. Then
the sampler iterates between two steps. The first sam-
ples ?, ? given annotated trees (as in ?3.2). The sec-
ond samples latent annotations for nonterminal nodes
293
Require: T
1
, ? ? ? , T
n
are fully parsed trees; ?
0
, ?
0
are initial values; ?
?
0
, ?
?
0
are priors; M is the
number of iterations
function ANNO(T
1
, ? ? ? , T
n
, ?
0
, ?
0
, ?
?
0
, ?
?
0
,M )
for iteration i = 1 to M do
for sentence s = 1 to n do
Calculate inside probability
Sample latent annotations for each node
in the tree, get tree with latent annotations t
(i)
s
end for
Sample ?
(i)
, ?
(i)
end for
return Mean of number of occurrences of
production rules and associated latent annotations
from all sampled annotated trees
end function
Algorithm 3: Learning prior from training
in parsed trees, which also takes two steps. The first
step is to, for each node in the tree, calculate and store
the probability that the node is annotated by x. The
second step is to jointly sample latent annotations for
child nodes of root nodes, and then continue this pro-
cess from top to bottom until reaching the pre-terminal
nodes.
Step one: inside probabilities. Given tree T , com-
pute b
i
T
[x] for each non-terminal i recursively:
1. If node N
i
is a pre-terminal node above terminal
symbol w, then for x?H
b
i
T
[x] = ?
N
i
[x]?w
(16)
2. Otherwise, let j, k be two child nodes of i, then
for x ? H
b
i
T
[x] =
?
y,z?H
?
N
i
[x]?N
j
N
k
?
N
i
[x]N
j
N
k
?y,z
b
j
T
[y]b
k
T
[z] (17)
Step two: outside sampling. Given inside probabil-
ity b
i
T
[x] for every non-terminal i and all latent annota-
tions x?H , we sample the latent annotations from top
to bottom:
1. If node i is the root node (i = 1), then sample x ?
H from a multinomial distribution with f
i
T
[x] =
pi(N
i
[x]).
2. For a parent node with sampled latent annotation
N
i
[x] with childrenN
j
, N
k
, sample latent annota-
tions for these two nodes from a multinomial dis-
tribution with
f
i
T
[y, z] =
1
b
i
T
[x]
?
?
N
i
[x]?N
j
N
k
?
N
i
[x]N
j
N
k
?y,z
b
j
T
[y]b
k
T
[z] (18)
After training, we take the average counts of sampled
annotated rules and combinations of latent annotations
as priors to parse raw sentences.
4 Experiments
1
Our goal is to understand parsing efficacy using sam-
pling and latent annotations for low-resource lan-
guages, so we perform experiments on five languages
with varying amount of training data. We compare
our results to a number of previously established base-
lines. First, for all languages, we use both a stan-
dard unsmoothed PCFG and the Bikel parser, trained
on the training corpus. Additionally, we compare to
state-of-the-art results for both English and Chinese,
which have an existing body of work in PCFGs using
a Bayesian framework. For Chinese, we compare to
Huang & Harper (2009), using their results that only
use the Chinese Treebank (CTB). For English, we com-
pare to Liang et al. (2009). Prior results for parsing
the constituency version of the Italian data are avail-
able from Alicante et al. (2012), but as they make use
of a different version of the treebank including extra
sentences, and additionally use the extensive functional
tags present in the corpus, we do not directly compare
our results to theirs.
2
4.1 Data
English (ENG) and Chinese (CHI) are the two main
languages used for this work; they are commonly used
in parser evaluation and have previous examples of sta-
tistical parsers using a Bayesian framework. And since
we primarily are interested in parsing low-resource lan-
guages, we include results for Kinyarwanda (KIN) and
Malagasy (MLG) as examples of languages without
substantial existing treebanks. Finally, as a middle-
ground language, we use Italian (ITL).
For English, we use the Wall-Street Journal section
of the Penn Treebank (WSJ) (Marcus et al., 1993). The
data split is sections 02-21 for training, section 22 for
development, and section 23 for testing. For Chinese,
the Chinese Treebank (CTB5) (Xue et al., 2005) was
used. The data split is files 81-899 for training, files 41-
80 for development, and files 1-40/900-931 for testing.
The ITL data is from the Turin University Treebank
(TUT) (Bosco et al., 2000) and consists of 2,860 Italian
sentences from a variety of domains. It was split into
training, development, and test sets with a 70/15/15
percentage split.
The KIN texts are transcripts of testimonies by sur-
vivors of the Rwandan genocide provided by the Ki-
gali Genocide Memorial Center, along with a few BBC
news articles. The MLG texts are articles from the
websites Lakroa and La Gazette and Malagasy Global
Voices. Both datasets are described in Garrette and
Baldridge (2013). The KIN and MLG data is very
small compared to ENG and CHI: the KIN dataset con-
1
Code available at github.com/jmielens/gibbs-pcfg-2014,
along with instructions for replicating experiments when pos-
sible
2
As part of a standardized pre-processing step, we strip
functional tags, which makes a direct comparison to their re-
sults inappropriate.
294
tains 677 sentences, while the MLG dataset has only
113. Also, we simulated a small training set for ENG
data by using only section 02 of the WSJ for training.
4.2 Experimental Setup
As a preprocessing step, all trees are converted into
Chomsky Normal-Form such that all non-terminal pro-
ductions are binary and all unary chains are removed.
Additional standard normalization is performed.
Functional tags (e.g. the SBJ part of NP-SBJ), empty
nodes (traces), and indices are removed. Our binariza-
tion is simple: given a parent, select the rightmost child
as the head and add a stand-in node that contains the
remainder of the original children; the process then re-
curses. This simple technique uses no explicit head-
finding rules, which eases cross-linguistic applicability.
From this normalized data, we train latent PCFGs
with K=1,2,4,8,16,32 (where K=1 is equivalent to the
plain PCFG described in section 2).
4.3 Practical refinements
Unknown word handling. We use a similar unknown
word handling procedure to Liang et al. (2009). From
our raw corpus we extract features associated with ev-
ery word, these features include surrounding context
words as well as substring suffix/prefix features. Using
these features we produce fifty clusters using k-means.
Then, as a pre-parsing step, we replace all words oc-
curring less than five times with their cluster label -
this simulates unknown words for training. Finally,
during evaluation, any word not seen in training was
also replaced with its corresponding cluster label. This
final step is simple because there are no ?unknown un-
knowns? in our corpus, as the clustering has been per-
formed over the entire corpus prior to training. This
approach is similar to methods for unsupervised POS-
tag induction that also utilize clusters in this manner
(Dasgupta & Ng, 2007).
We compare this unknown word handling method to
one in which the clustering and a classifier is trained
not on the corpus under consideration, but rather on a
separate corpus of unrelated data. This comparison was
made to understand the effects of including the eval-
uation set in the training data (without labels) versus
training on out-of-domain texts. This is a more real-
istic measurement of out-of-the-box performance of a
trained model.
Jump-starting sampling. In the basic setup, train-
ing high K-value models takes a prohibitively long
time, so we also consider a jump-start technique that
allows larger annotation values (such as K=16) to be
run in less time. We train these high-K value models
first on a highly reduced training set (5% of the full
training set) for a large number of iterations, and then
use the found ? values as the starting point for training
on the full training set for a small number of iterations.
Although many of the estimated parameters on the re-
duced set will be zero, the prior allows us to eventually
System K=1 K=2 K=4 K=8 K=16
Unsmoothed PCFG 40.2 ? ? ? ?
Bikel Parser 57.9 ? ? ? ?
Liang et al. 07 60.5 71.1 77.2 79.2 78.2
Berkeley Parser 60.8 74.4 78.4 79.1 78.7
Gibbs PCFG 61.0 71.3 76.6 78.7 78.0
Table 1: F1 scores for small English training data ex-
periments. ?K? is the number of latent annotations ?
K=1 represents a vanilla, unannotated PCFG.
recover this information in the full set. This allows us
to train on the full training set, which is desirable rela-
tive to training on a reduced set, while still allowing the
model sufficient iterations to burn in. The fact that we
are likely starting in a fairly good position within the
search space (due to estimating ? from the corpus) also
likely helps enable these lower iteration counts.
5 Results
We start with Tables 1 and 2, which show performance
when training on section 02 of the WSJ (pretending En-
glish is a ?low-resource? language). The results show
that the basic Gibbs PCFG (where K=1), with an F-
score of 61.0, substantially outperforms not only an
unsmoothed PCFG (the simplest baseline), but also the
Bikel parser (Bikel, 2004b) trained on the same amount
of data. Table 1 also shows further large gains are
obtained from using latent annotations?from 60.5 for
K=1 to 78.7 for K=8.
The Gibbs PCFG also compares quite favorably to
the PCFG-LA of Liang et al. (2009)?slightly better
for K=1 and K=2 and slightly worse for K=4 and K=8.
Table 2 shows that the Gibbs PCFG is able to produce
results with a smaller amount of variance relative to
the Berkeley Parser, even at low training sizes. This
trend is repeated in Table 3, which shows that the Gibbs
PCFG also produces less variance when training on dif-
ferent single sections of the WSJ relative to the Berke-
ley Parser, although it again produces slightly lower F1
scores.
We also use the small English corpus to determine
the effects of weighting the prior when sampling anno-
tations, varying ? between 0.1 and 10.0. Though per-
formance is not sensitive to varying ? for larger cor-
pora, Figure 1 shows it can make a substantial differ-
ence for smaller corpora (with an optimal value was
obtained with an ? value of 5 for this small training
set). This seems to indicate that the lower counts asso-
ciated with the smaller training sets should be compen-
sated for by weighting those counts more heavily when
processing the evaluation set, as we had anticipated.
System WSJ Sec. 02 KIN MLG
Berkeley Parser 78.3 ? 0.93 60.6 ? 1.1 52.2 ? 2.0
Gibbs PCFG 76.7 ? 0.63 67.2 ? 0.92 57.5 ? 1.1
Table 2: F1 scores with standard deviation over ten runs
of small training data, K=4.
295
System F1 / StDev
Berkeley Parser 77.5 ? 2.1
Gibbs PCFG 77.0 ? 1.4
Table 3: F1 scores with standard deviations over twenty
runs, training on individual WSJ sections (02-21).
Figure 1: Accuracy by varying ? levels for small En-
glish data.
To evaluate the effectiveness of the jump-start tech-
nique, we ran the full ENG data set with K=4 to com-
pare the results from the full training setup to jump-
starting. For this, we performed 100 training iterations
on the reduced training set (WSJ section 02) and then
used the resulting ? values to seed training on the full
training set. Those training runs varied between three
and nine iterations, and the results are shown in Figure
2. The full ENG K=4 F-score is 86.2, so these results
represent a slight step back. Nonetheless, the technique
is still valuable in that it allows for inferring latent an-
notations for higher K-values than would typically be
available to us in a reasonable timeframe.
Table 4 shows the results for the main experiments.
Sampling a vanilla PCFG (K=1) produces results that
are not state-of-the-art, but still good overall and al-
ways better than an unsmoothed PCFG. The benefits of
the latent annotations are further shown in the increase
Condition ENG CHI ITA KIN MLG
Unsmoothed PCFG 69.9 66.8 62.1 45.9 49.2
Liang et al. 07 87.1 ? ? ? ?
Huang & Harper09 ? 84.1 ? ? ?
Bikel Parser 86.9 81.1 74.5 55.7 49.5
Berkeley Parser 90.1 83.4 71.6 61.4 51.8
Gibbs PCFG,K=1 79.3 75.4 66.3 58.5 55.1
Gibbs PCFG,K=2 82.6 79.8 69.3 65.0 57.0
Gibbs PCFG,K=4 86.0 82.3 71.9 67.2 57.8
Gibbs PCFG,K=16 87.2 83.2 72.4 68.1 58.2
Gibbs PCFG,K=32 87.4 83.4 71.0 66.8 55.3
Table 4: F1 scores for experiments on sampled PCFGs.
Note that Wang and Blunsom (2013) obtain an ENG F-
score of 77.9% using collapsed VB for K=2. Though
they do not give exact numbers, their Fig. 7 indicates
an F-score of about 87% for K=16.
Figure 2: F-Score for K=4, varying full-set training it-
erations (with and without 100x jump start).
of F1 score in all languages, as compared to the vanilla
PCFG. Experiments were run up to K=32 primarily due
to time constraint. Although previous literature results
report increases up to the equivalent of K=64, it may
be the case that higher K values with no merge step
more easily lead to overfitting in our model ? reduc-
ing the effectiveness of those high values, as shown by
the overall poorer performance on several languages at
K=32 when compared to K=16 as well as the general
levelling-off seen at the high K values.
For English and Chinese, the previous Bayesian
framework parsers outperform our own, but only by
around two points. Additionally, our parsing of Chi-
nese improves on the Bikel parser (trained on our train-
ing data) despite the fact that the Bikel parser makes
use of language specific optimizations. Our parser
needs no changes to switch languages.
The Gibbs PCFG with K=16 is superior to the strong
Bikel and Berkeley Parser benchmarks for both KIN
and MLG, a promising result for future work on pars-
ing low-resource languages in general. Note also that
our parser exhibits less variance than Berkeley Parser
especially for KIN and MLG, which supports the fact
that the variance of Berkeley Parser is higher for mod-
els with few subcategories (Petrov et al., 2006).
Examples of the improvement across latent annota-
tions for a given tree are shown in Figure 3. The details
of the noun phrase ?no major bond offerings? were the
same for each tree, and are thus abstracted here. The
low K-value tree (K=2) is shown in 3a, and primarily
suffers from issues related to the prepositional phrase,
?in Europe friday?. In particular, the low K-value tree
incorrectly groups ?Europe friday? as a noun phrase ob-
ject of ?in?.
The higher K-value tree (K=8) is shown in 3b.
This tree manages to correctly analyze the preposi-
tional phrase, accurately separately the temporal loca-
tive ?Friday? from the actual prepositional phrase of
?in Europe?. However, the high K-value tree makes a
296
Figure 3: Examples of tree progression in the Gibbs PCFG with a) K=2, b) K=8, and c) gold tree.
different mistake that the low K-value tree did not; it
groups ?no major bond offerings in Europe Friday? as a
noun phrase, when it should be three separate phrases
(two noun phrases and a prepositional phrase). This er-
ror may be related to the additional latent annotations.
With more available noun phrase subtypes, it may be
the case that a more unusual noun phrase could be per-
mitted that would have been too low probability with
only a few subtypes.
To determine whether the substantial range in F1
scores across languages are primarily the result of the
much larger training corpora available for certain lan-
guages, two extreme training set reduction experiments
were conducted. The training sets for all languages
were reduced to a total of either 100 or 500 sen-
tences. This process was repeated 10 times in a cross-
validation setup, where 10 separate sets of sentences
were selected for each language. The results of these
experiments are shown in Table 5.
We conclude that while data availability is a major
factor in the higher performance of English and Chi-
nese in our original experiments, it is not the only is-
sue. Clearly, either the linguistic facts of particular
languages or perhaps choices of formalism and annota-
tion conventions in the corpora make some of the lan-
guages more difficult to parse than others. The primary
questions is why Gibbs-PCFG is able to achieve higher
relative performance on the KIN/MLG datasets when
compared to the other parsers, and why this advantage
does not necessarily transfer to the extreme small-scale
versions of the ENG/CHI/ITL data. Preliminary inves-
tigation into the properties of the corpora have revealed
a number of potential answers. For instance, the POS
tagsets for KIN/MLG are substantially reduced com-
pared to the other corpora, and there are differences
in the branching factor of the native versions of the
corpora as well: a typical maximum branching fac-
tor for a tree in ENG/CHI/ITL is around 4-5, while
for KIN/MLG it is almost always 2 (natively binary).
Branching factors above 5 essentially never occur in
KIN/MLG, while they are not rare in ENG/CHI/ITL.
The question of exactly why the Gibbs-PCFG seems to
perform well on these corpora remains an open ques-
tion, but these differences could provide a starting point
Condition In-Domain Out-of-Domain
Full English (K=4) 86.0 83.3
Small English (K=4) 76.6 75.7
Kinyarwanda (K=4) 67.2 65.1
Malagasy (K=4) 57.8 55.4
Table 6: Effect of differing regimes for handling un-
known words.
for future analysis.
In addition to the actual F1 scores, the relative uni-
formity of the standard deviation results indicates that
the individual parsers are not that much different in
terms of their ability to provide consistent results at
these small data extremes, as opposed to the slightly
higher training levels where the Gibbs-PCFG generated
smaller variances.
Considering the effects of unknown word handling,
Table 6 shows that using the evaluation set when creat-
ing the unknown word classifier does improve overall
parsing accuracy when compared to an unknown word
handler that is trained on out-of-domain texts. This
shows that results reported in previous work somewhat
overstate the accuracy of these parsers when used in the
wild?which matters greatly in the low-resource set-
ting.
6 Conclusion
Our experiments demonstrate that sampling vanilla
PCFGs, as well as PCFGs with latent annotations, is
feasible with the use of a Gibbs sampler technique
and produces results that are in line with previous
parsers on controlled test sets. Our results also show
that our methods are effective on a wide variety of
languages?including two low-resource languages?
with no language-specific model modifications needed.
Additionally, although not a uniform winner, the
Gibbs-PCFG shows a propensity for performing well
on naturally small corpora (here, KIN/MLG). The ex-
act reason for this remains slightly unclear, but the
fact that a similar advantage is not found for extremely
small versions of large corpora indicates that our ap-
proach may be particularly well-suited for application
in real low-resource environments as opposed to a sim-
297
Parser Size ENG CHI ITL KIN MLG
Bikel 100 54.7 ? 2.2 51.4 ? 3.0 51 ? 2.4 47.1 ? 2.3 44.4 ? 2.0
Berkeley 100 55.2 ? 2.6 53.9 ? 2.9 50 ? 2.8 47.8 ? 2.1 44.5 ? 2.3
Gibbs-PCFG 100 54.5 ? 2.0 51.7 ? 2.4 49.5 ? 3.6 50.3 ? 2.3 45.8 ? 1.8
Bikel 500 56.2 ? 2.0 54.1 ? 2.7 54.2 ? 2.4 ? ?
Berkeley 500 58.9 ? 2.2 56.4 ? 2.7 52.5 ? 2.7 ? ?
Gibbs-PCFG 500 58.1 ? 2.0 55.7 ? 2.3 51.1 ? 3.2 ? ?
Table 5: 100/500 sentence training set results, including st.dev over 10 runs. KIN/MLG did not have enough data
to run the 500 sentence version.
ulated environment.
Having established this procedure and its relative tol-
erance for low amounts of data, we would like to extend
the model to make use of partial bracketing information
instead of complete trees, perhaps in the form of Frag-
mentary Unlabeled Dependency Grammar annotations
(Schneider et al., 2013). This would allow the sam-
pling procedure to potentially operate using corpora
with lighter annotations than full trees, making initial
annotation effort not quite as heavy and potentially in-
creasing the amount of available data for low-resource
languages. Additionally, using the expert partial anno-
tations to help restrict the sample space could provide
good gains in terms of training time.
Acknowledgments
Supported by the U.S. Army Research Office un-
der grant number W911NF-10-1-0533. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of the U.S. Army
Research Office.
References
Anita Alicante, Cristina Bosco, Anna Corazza, and
Alberto Lavelli. 2012. A treebank-based study
on the influence of Italian word order on pars-
ing performance. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of LREC?12, Istanbul, Turkey. European
Language Resources Association (ELRA).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The Grammar Matrix: An Open-Source
Starter-Kit for the Rapid Development of Cross-
Linguistically Consistent Broad-Coverage Precision
Grammars. In John Carroll, Nelleke Oostdijk, and
Richard Sutcliffe, editors, Proceedings of the Work-
shop on Grammar Engineering and Evaluation at
the 19th International Conference on Computational
Linguistics, pages 8?14, Taipei, Taiwan.
Dan Bikel. 2004a. On The Parameter Space of Gener-
ative Lexicalized Statistical Parsing Models. Ph.D.
thesis, University of Pennsylvania.
Daniel M Bikel. 2004b. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Ezra Black, Fred Jelinek, John Lafferty, David M
Magerman, Robert Mercer, and Salim Roukos.
1992. Towards history-based grammars: Using
richer models for probabilistic parsing. In Proceed-
ings of the workshop on Speech and Natural Lan-
guage, pages 134?139. Association for Computa-
tional Linguistics.
Taylor L Booth and Richard A Thompson. 1973. Ap-
plying probability measures to abstract languages.
Computers, IEEE Transactions on, 100(5):442?450.
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a Treebank
for Italian: a Data-driven Annotation Schema. In In
Proceedings of the Second International Conference
on Language Resources and Evaluation LREC-2000
(pp. 99, pages 99?105.
Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Department of Computer Sci-
ence, Univ.
Eugene Charniak. 1996. Tree-bank grammars. In Pro-
ceedings of the National Conference on Artificial In-
telligence, pages 1031?1036.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139. Asso-
ciation for Computational Linguistics.
Noam Chomsky. 1956. Three models for the descrip-
tion of language. Information Theory, IRE Transac-
tions on, 2(3):113?124.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning
of latent-variable PCFGs. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
223?231. Association for Computational Linguis-
tics.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Pro-
ceedings of NAACL-HLT, pages 148?157.
298
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th annual meeting on Association for
Computational Linguistics, pages 184?191. Associ-
ation for Computational Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics, pages 16?23. Association for
Computational Linguistics.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Jenny Rose Finkel, Christopher D Manning, and An-
drew Y Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguis-
tic annotation pipelines. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 618?626. Association for
Computational Linguistics.
Dan Garrette and Jason Baldridge. 2013. Learning a
Part-of-Speech Tagger from Two Hours of Annota-
tion. In Proceedings of NAACL, Atlanta, Georgia.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-World Semi-Supervised Learning of
POS-Taggers for Low-Resource Languages. In Pro-
ceedings of the 51th annual meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, (6):721?741.
Joshua T Goodman. 1998. Parsing Inside-Out.
Ph.D. thesis, Harvard University Cambridge, Mas-
sachusetts.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 832?841.
Association for Computational Linguistics.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
Breaking the Resource Bottleneck for Multilingual
Parsing. In The Proceedings of the Workshop on Lin-
guistic Knowledge Acquisition and Representation:
Bootstrapping Annotated Language Data. Confer-
ence on Language Resources and Evaluation.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov Chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Jonas Kuhn. 2004a. Applying computational linguis-
tic techniques in a documentary project for Qanjobal
(Mayan, Guatemala). In In Proceedings of LREC
2004. Citeseer.
Jonas Kuhn. 2004b. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 470. Association for Computational
Linguistics.
Karim Lary and Steve J Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algrithm. Computer, Speech and Language,
4:35?56.
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Probabilistic grammars and hierarchical Dirichlet
processes. The handbook of applied Bayesian anal-
ysis.
David M Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, pages 276?283. Association for Computa-
tional Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic CFG with latent annotations.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 75?82.
Association for Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th annual meeting
on Association for Computational Linguistics, pages
128?135. Association for Computational Linguis-
tics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In HLT-NAACL, pages
404?411.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
867?876. Association for Computational Linguis-
tics.
299
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple Unsupervised Grammar Induction from Raw
Text with Cascaded Finite State Models. In ACL,
pages 1077?1086.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. arXiv preprint
arXiv:1306.2091.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers-
Volume 1, pages 440?448. Association for Computa-
tional Linguistics.
Matthew A Taddy. 2011. On estimation and selection
for topic models. arXiv preprint arXiv:1109.4518.
Pengyu Wang and Phil Blunsom. 2013. Collapsed
Variational Bayesian Inference for PCFGs. In Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning, pages 173?
182, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Nat. Lang.
Eng., 11(2):207?238, June.
300
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583?592,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Real-World Semi-Supervised Learning
of POS-Taggers for Low-Resource Languages
Dan Garrette1 Jason Mielens2
1Department of Computer Science 2Department of Linguistics
The University of Texas at Austin The University of Texas at Austin
dhg@cs.utexas.edu {jmielens,jbaldrid}@utexas.edu
Jason Baldridge2
Abstract
Developing natural language processing
tools for low-resource languages often re-
quires creating resources from scratch.
While a variety of semi-supervised meth-
ods exist for training from incomplete
data, there are open questions regarding
what types of training data should be used
and how much is necessary. We dis-
cuss a series of experiments designed to
shed light on such questions in the con-
text of part-of-speech tagging. We obtain
timed annotations from linguists for the
low-resource languages Kinyarwanda and
Malagasy (as well as English) and eval-
uate how the amounts of various kinds
of data affect performance of a trained
POS-tagger. Our results show that an-
notation of word types is the most im-
portant, provided a sufficiently capable
semi-supervised learning infrastructure is
in place to project type information onto
a raw corpus. We also show that finite-
state morphological analyzers are effective
sources of type information when few la-
beled examples are available.
1 Introduction
Low-resource languages present a particularly dif-
ficult challenge for natural language processing
tasks. For example, supervised learning meth-
ods can provide high accuracy for part-of-speech
(POS) tagging (Manning, 2011), but they per-
form poorly when little supervision is avail-
able. Good results in weakly-supervised tagging
have been obtained by training sequence models
such as hidden Markov models (HMM) using the
Expectation-Maximization algorithm (EM), how-
ever most work in this area has still relied on rel-
atively large amounts of data, both annotated and
unannotated, as well as an assumption that the an-
notations are very clean (Kupiec, 1992; Merialdo,
1994).
The ability to learn taggers using very little data
is enticing: only a tiny fraction of the world?s lan-
guages have enough data for standard supervised
models to work well. The collection or develop-
ment of resources is a time-consuming and expen-
sive process, creating a significant barrier for an
under-studied language where there are few ex-
perts and little funding. It is thus important to
develop approaches that achieve good accuracy
based on the amount of data that can be reasonably
obtained, for example, in just a few hours by a lin-
guist doing fieldwork on a non-native language.
Previous work explored learning taggers from
weak information, but the type, amount, quality,
and sources of data raise questions about the appli-
cability of those results to real-world low-resource
scenarios (Toutanova and Johnson, 2008; Ravi and
Knight, 2009; Hasan and Ng, 2009; Garrette and
Baldridge, 2012). Most research simulated weak
supervision with tag dictionaries extracted from
existing large, expertly-annotated corpora. These
resources have been developed over long periods
of time by trained annotators who collaborate to
produce high-quality analyses. They are also bi-
ased towards including only the most likely tag
for each word type, resulting in a cleaner dictio-
nary than one would find in a real scenario. As
such, these experiments do not reflect real-world
constraints.
One exception to this work is Goldberg et al
(2008): they use a manually-constructed lexicon
for Hebrew in order to learn an HMM tagger. How-
ever, this lexicon was constructed by trained lexi-
cographers over a long period of time and achieves
very high coverage of the language with very good
quality, much better than could be achieved by
our non-expert linguistics graduate student anno-
tators in just a few hours. Cucerzan and Yarowsky
583
(2002) learn a POS-tagger from existing linguis-
tic resources, namely a dictionary and a refer-
ence grammar, but these resources are not avail-
able, much less digitized, for most under-studied
languages. Haghighi and Klein (2006) develop a
model in which a POS-tagger is learned from a list
of POS tags and just three ?prototype? word types
for each tag, but their approach requires a vector
space to compute the distributional similarity be-
tween prototypes and other word types in the cor-
pus. Such distributional models are not feasible
for low-resource languages because they require
immense amounts of raw text, much more than is
available in these settings (Abney and Bird, 2010).
Further, they extracted their prototype lists directly
from a labeled corpus, something we are specif-
ically avoiding. Ta?ckstro?m et al (2013) evalu-
ate the use of mixed type and token constraints
generated by projecting information from a high-
resource language to a low-resource language via
a parallel corpus. However, large parallel corpora
are not available for most low-resource languages.
These are also expensive resources to create and
would take considerably more effort to produce
than the monolingual resources that our annotators
were able to generate in a four-hour timeframe.
Of course, if they are available, such parallel text
links could be incorporated into our approach.
In our previous work, we developed a differ-
ent strategy based on generalizing linguistic input
with a computational model: linguists annotated
either types or tokens for two hours, these anno-
tations are projected onto a corpus of unlabeled
tokens using label propagation and HMMs, and
a final POS-tagger is trained on this larger auto-
labeled corpus (Garrette and Baldridge, 2013).
That approach uses much more realistic types
and quantities of resources than previous work;
nonetheless, it leaves many open questions regard-
ing the effectiveness of incrementally more anno-
tation, the role of unannotated data, and whether
there is a good balance to be found using a combi-
nation of type- and token-supervision. We also did
not consider morphological analyzers as a form
of type supervision, as suggested by Merialdo
(1994).
This paper addresses these questions via a se-
ries of experiments designed to quantify the ef-
fect on performance given by the amount of time
spent finding or annotating training materials. We
specifically look at the impact of four types of data
collection:
1. Time annotating sentences (token supervision)
2. Time creating tag dictionary (type supervision)
3. Time constructing a finite state transducer
(FST) to analyze word-type morphology
4. Amount of raw data available for training
We explore these strategies in the context of POS-
tagging for Kinyarwanda and Malagasy. We also
include experiments for English, pretending as
though it is a low-resource language. The over-
whelming take away from our results is that type
supervision?when backed by an effective semi-
supervised learning approach?is the most impor-
tant source of linguistic information. Also, mor-
phological analyzers help for morphologically rich
languages when there are few labeled types or to-
kens (and, it never hurts to use them). Finally, per-
formance improves with more raw data, though we
see diminishing returns past 400,000 tokens. With
just four hours of type annotation, our system ob-
tains good accuracy across the three languages:
89.8% on English, 81.9% on Kinyarwanda, and
81.2% on Malagasy.
Our results compare favorably with previous
work despite using considerably less supervision
and a more difficult set of tags. For example, Li et
al. (2012) use the entirety of English Wiktionary
directly as a tag dictionary to obtain 87.1% accu-
racy on English, below our result. Ta?ckstro?m et al
(2013) average 88.8% across 8 major languages,
but for Turkish, a morphologically rich language,
they achieve only 65.2%, significantly below our
81.9% for morphologically-rich Kinyarwanda.
2 Data
Kinyarwanda (KIN) and Malagasy (MLG) are low-
resource, KIN is morphologically rich, and English
(ENG) is used for comparison. For each language,
sentences were divided into four sets: training data
to be labeled by annotators, raw training data, de-
velopment data, and test data.
Data sources The KIN texts are transcripts of
testimonies by survivors of the Rwandan geno-
cide provided by the Kigali Genocide Memorial
Center. The MLG texts are articles from the web-
sites1 Lakroa and La Gazette and Malagasy Global
Voices.2 Texts in both KIN and MLG were tok-
1www.lakroa.mg and www.lagazette-dgi.com
2mg.globalvoicesonline.org/
584
KIN MLG ENG - Experienced ENG - Novice
time type token type token type token type token
1:00 801 559 (1093) 660 422 (899) 910 522 (1124) 210 308 (599)
2:00 1814 948 (2093) 1363 785 (1923) 2660 1036 (2375) 631 646 (1429)
3:00 2539 1324 (3176) 2043 1082 (3064) 4561 1314 (3222) 1350 953 (2178)
4:00 3682 1651 (4119) 2773 1378 (4227) 6598 1697 (4376) 2185 1220 (2933)
Table 1: Annotations for each language and annotator as time increases. Shows the number of tag
dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses).
For brevity, the table only shows hourly progress.
enized and labeled with POS tags by two linguis-
tics graduate students, each of which was studying
one of the languages. The KIN and MLG data have
12 and 23 distinct POS tags, respectively.
The Penn Treebank (PTB) (Marcus et al, 1993)
is used as ENG data. Section 01 was used for
token-supervised annotation, sections 02-14 were
used as raw data, 15-18 for development of the
FST, 19-21 as a dev set and 22-24 as a test set.
The PTB uses 45 distinct POS tags.
Collecting annotations Linguists with non-
native knowledge of KIN and MLG produced anno-
tations for four hours (in 30-minute intervals) for
two tasks. In the first task, type-supervision, the
annotator was given a list of the words in the tar-
get language (ranked from most to least frequent),
and they annotated each word type with its poten-
tial POS tags. The word types and frequencies used
for this task were taken from the raw training data
and did not include the test sets. In the second
task, token-supervision, full sentences were anno-
tated with POS tags. The 30-minute intervals allow
us to investigate the incremental benefit of addi-
tional annotation of each type as well as how both
annotation types might be combined within a fixed
annotation budget.
Baldridge and Palmer (2009) found that anno-
tator expertise greatly influences effectiveness of
active learning for morpheme glossing, a related
task. To see how differences in annotator speed
and quality impact our task, we obtained ENG data
from an experienced annotator and a novice one.
Ngai and Yarowsky (2000) investigated the ef-
fectiveness of rule-writing versus annotation (us-
ing active learning) for chunking, and found the
latter to be far more effective. While we do not
explore a rule-writing approach to POS-tagging,
we do consider the impact of rule-based morpho-
logical analyzers as a component in our semi-
supervised POS-tagging system.
ENG - Exp. ENG - Nov.
time type tok type tok
1:00 0.05 0.03 0.01 0.02
2:00 0.15 0.05 0.03 0.03
3:00 0.24 0.06 0.07 0.05
4:00 0.32 0.08 0.11 0.06
Table 2: Tag dictionary recall against the test set
for ENG annotators on type and token annotations.
Annotations Table 1 gives statistics for all lan-
guages and annotators showing progress during
the 4-hour tasks. With token-annotation, tag
dictionary growth slows because high-frequency
words are repeatedly annotated, producing only
additional frequency and sequence information.
In contrast, every type-annotation label is a new
tag dictionary entry. For types, growth increases
over time, reflecting the fact that high-frequency
words (which are addressed first) tend to be more
ambiguous and thus require more careful thought
than later words. For ENG, we can compare the
tagging speed of the experienced annotator with
the novice: 50% more tokens and 3 times as many
types. The token-tagging speed stayed fairly con-
stant for the experienced annotator, but the novice
increased his rate, showing the result of practice.
Checking the annotators? output against the
gold tags in the PTB shows that both had good
tagging accuracy on tokens: 94-95%. Comparing
the tag dictionary entries versus the test data, pre-
cision starts in the high 80%s and falls to to the
mid-70%s in all cases. However, the differences
in recall, shown in Table 2, are more interesting.
On types, the experienced annotator maxed out at
32%, but the novice only reaches 11%. More-
over, the maximum for token annotations is much
lower due to high repeat-annotation. The discrep-
ancies between experienced and novice, and be-
tween type and token recall explain a great deal of
the performance disparity seen in the experiments.
585
3 Morphological Transducers
Finite-state transducers (FSTs) accept regular lan-
guages and can be constructed easily using regu-
lar expressions, which makes them quite useful for
phonology, morphology and limited areas of syn-
tax (Karttunen, 2001). Past work has used FSTs
for direct POS-tagging (Roche and Schabes, 1995),
but this requires tight coupling between the FST
and target tagset. We use FSTs for morphologi-
cal analysis: the FST accepts a word type and pro-
duces a set of morphological features. If there are
multiple possible analyses for a given word type,
the FST returns them all. For instance the Kin-
yarwanda verb sibatarazuka ?he is not yet resur-
rected? is analyzed in several ways:
? +NEG+CL2+1PL+V+arazuk+IMP
? +NEG+CL2+NOT.YET+PRES+zuk+IMP
? +NEG+CL2+NOT.YET+razuk+IMP
FSTs are particularly valuable for their ability
to analyze out-of-vocabulary items. By looking
for known affixes, FSTs can guess the stem of
a word and produce an analysis despite not hav-
ing knowledge of that stem. For morphologically
complex languages like KIN, this ability is espe-
cially useful. Other factors, such as a large num-
ber of morphologically-conditioned phonological
changes (seen in MLG) make out-of-vocabulary
guessing more challenging because of the large
number of potential stems (high ambiguity).
Development of the FSTs for all three languages
was done by iteratively adding rules and lexical
items with the goal of increasing coverage on a
raw dataset. To accomplish this on a fixed time
budget, the most frequently occurring unanalyzed
tokens were examined, and their stems plus any
observable morphological or phonological pat-
terns were added to the transducer. Addition-
ally, developers searched for known morpholog-
ical alternations to locate instances of phonolog-
ical change for inclusion. Coverage was checked
against a raw dataset which did not include the test
data used for the POS experiments.
The KIN and MLG FSTs were created by
English-speaking linguists who were familiar with
their respective language. They also used dictio-
naries and grammars. Each FST was developed
in 10 hours. To evaluate the benefits of more de-
velopment time, a version of the English FST was
saved every 30 minutes, as shown in Table 3.
elapsed
time
tokens types
count pct count pct
2:00 130k 61% 2.1k 12%
4:00 159k 75% 4.1k 24%
6:00 170k 80% 6.7k 39%
8:00 182k 86% 7.7k 44%
10:00 192k 91% 10.7k 62%
Table 3: Coverage of the English morphological
FST during development. For brevity, showing 2-
hour increments instead of 30-minute segments.
tokens types
cov. ambig. cov. ambig.
KIN 86% 2.62 82% 5.31
MLG 78% 2.98 37% 1.13
ENG 91% 1.19 62% 1.97
Table 4: Coverage and ambiguity of the final FST
for each language.
4 Approach
Learning under low-resource conditions is more
difficult than scenarios in most previous POS work
because the vast majority of the word types in the
training and test data are not covered by the an-
notations. When most words are unknown, learn-
ing algorithms such as EM struggle (Garrette and
Baldridge, 2012). Recall that most work on learn-
ing POS-taggers from tag dictionaries used tag dic-
tionaries culled from test sets (even when consid-
ering incomplete dictionaries). We thus build on
our previous approach, which exploits extremely
sparse, human-generated annotations that are pro-
duced without knowledge of which words appear
in the test set (Garrette and Baldridge, 2013).
This approach generalizes a small initial tag dic-
tionary to include unannotated word types appear-
ing in raw data. It estimates word/tag pair and
tag-transition frequency information using model-
minimization, which also reduces noise intro-
duced by automatic tag dictionary expansion. The
approach exploits type annotations effectively to
learn parameters for out-of-vocabulary words and
infer missing frequency and sequence informa-
tion. This pipeline is described in detail in the
previous work, so we give only a brief overview
and describe our additions.
The purpose of tag dictionary expansion is to es-
timate label distributions for tokens in a raw cor-
586
pus, including words missing in the annotations.
For this, a graph connecting annotated words to
unannotated words via features is constructed and
POS labels are pushed between these items using
label propagation (LP) (Talukdar and Crammer,
2009). LP has been used successfully for extend-
ing POS labels from high-resource languages to
low via parallel corpora (Das and Petrov, 2011;
Ta?ckstro?m et al, 2013; Ding, 2011) or high- to
low-resource domains (Subramanya et al, 2010),
among other tasks. These works have typically
used n-gram features (capturing basic syntax) and
character affixes (basic morphology).
The character n-gram affix-as-morphology ap-
proach produces many features, but only a fraction
of them represent actual morphemes. Incorrect
features end up pushing noise around the graph,
so affixes can lead to more false labels that drown
out the true labels. While affixes may be suffi-
cient for languages with limited morphology, their
effectiveness diminishes for morphology-rich lan-
guages, which have much higher type-to-token ra-
tios. More types means sparser word frequency
statistics and more out-of-vocabulary items, and
thus problems for EM. Here, we modify the LP
graph by supplementing or replacing generic af-
fix features with a focused set of morphological
features produced by an FST. These targeted mor-
phological features are effective during LP because
words that share them are much more likely to ac-
tually share POS tags.
FSTs produce multiple analyses, which is actu-
ally advantageous for LP. Ambiguities need not be
resolved since we just take the union of all mor-
phological features for all analyses and use them
as features in the graph. Note that each FST pro-
duces its own POS-tags as features, but these do
not correspond to the target POS tagset used by the
tagger. This is important because it decouples FST
development and the final POS task. Thus, any FST
for the language, regardless of its provenance, can
be used with any target POS tagset.
Since the LP graph contains a node for each cor-
pus token, and each node is labeled with a distri-
bution over POS tags, the graph provides a corpus
of sentences labeled with noisy tag distributions
along with an expanded tag dictionary. This out-
put is useful as input to EM because it contains
labels for all seen word types as well as sequence
and frequency information. There is a high degree
of noise in the LP output, so we employ the model
minimization strategy of Ravi et al (2010), which
finds a minimal set of tag bigrams needed to ex-
plain the sentences in the raw corpus. It outputs
a corpus of tagged sentences, which are used as
a good starting point for EM training of an HMM.
The expanded tag dictionary constrains the EM
search space by providing a limited tagset for each
word type, steering EM towards a desirable result.
Because the HMM trained by EM will con-
tain zero-probabilities for words that did not ap-
pear in the training corpus, we use the ?auto-
supervision? step from our previous work: a Max-
imum Entropy Markov Model tagger is trained
on a corpus that is noisily labeled by the HMM
(Garrette and Baldridge, 2012). While training
an HMM before the MEMM is not strictly neces-
sary, our tests have shown that this generative-
then-discriminative combination generally results
in around 3% accuracy improvement.
5 Experiments3
To better understand the effect that each type of
supervision has on tagger accuracy, we perform a
series of experiments, with KIN and MLG as true
low-resource languages. English experiments, for
which we had both experienced and novice an-
notators, allow for further exploration into issues
concerning data collection and preparation.
The overall best accuracies achieved by lan-
guage are 81.9% for KIN using all types, 81.2% for
MLG using half types and half tokens, and 89.8%
for ENG using all types and the maximal amount
of raw data. All of these best values were achieved
using both FST and affix LP features.
All results described in this section are averaged
over five folds of raw data.
5.1 Types versus tokens
Our primary question was the relationship be-
tween annotation type and time. Annotation must
be done by someone familiar with the target lan-
guage, linguistics, and the target POS tagset. For
many low-resource languages, such people, and
the time they have to spend, are likely to be in
short supply. To make the best use of their time,
we need to know which annotations are most use-
3Code and all MLG data available at github.com/
dhgarrette/low-resource-pos-tagging-2013
We are unable to provide the KIN or ENG data for down-
load due to licensing restrictions. However, ENG data may
be shared with those holding a license for the Penn Treebank
and KIN data may be shared on a case-by-case basis.
587
(a) KIN type annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
50
55
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(b) KIN token annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
50
55
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(c) MLG type annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(d) MLG token annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
Figure 1: Annotation time vs. tagger accuracy for type-only and token-only annotations.
Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
85
Experienced annotator ? TypesExperienced annotator ? TokensNovice annotator ? TypesNovice annotator ? Tokens
Figure 2: Annotation time vs. tagger accuracy for
ENG type-only and token-only annotations with
affix and FST LP features.
ful so that efforts can be concentrated there. Ad-
ditionally, it is useful to identify when returns on
annotation effort diminish so that annotators do
not spend time doing work that is unlikely to add
much value.
The annotators produced four hours each of
type and token annotations, each in 30-minute in-
crements. To assess the effects of annotation time,
we trained taggers cumulatively on each increment
and determine the value of each additional half-
hour of effort. Results are shown for KIN and MLG
in Figure 1 and ENG in Figure 2. In all scenarios,
the use of LP (and model minimization) delivers
huge performance gains. Additionally, the use of
FST features, usually along with affixes, yielded
better results than without. This indicates the LP
procedure makes effective use of the morpholog-
ical features produced by the FST and that the af-
fix features are able to capture missing information
without adding too much noise to the LP graph.
Furthermore, performance is considerably bet-
ter when type annotations are used than only to-
kens. Type annotations plateau much faster, so
a shorter amount of time must be spent annotat-
ing types than if token annotations are used. For
KIN it takes approximately 1.5 hours to reach near-
maximum accuracy for types, but 2.5 hours for to-
kens. This difference is due to the fact that the type
annotations started with the most frequent words
whereas the token annotations were on random
sentences. Thus, type annotations quickly cover a
significant portion of the language?s tokens. With
annotations directly on tokens, some of the highest
588
(a) KIN ? Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(b) MLG ? Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
70
72
74
76
78
80
No LPAffixes onlyFST onlyAffixes+FST
Figure 3: Annotation mixture vs. tagger accuracy. X-axis labels give annotation proportions, e.g. ?t2/s6?
indicates 2/8 of the time (1 hour) was spent annotating types and 6/8 (3 hours), full sentences.
Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
70
75
80
85
Exp. ? With LPNov. ? With LPExp. ? No LPNov. ? No LP
Figure 4: Annotation mixture vs. tagger accuracy
on ENG using affix and FST LP features for experi-
enced (Exp.) and novice (Nov.) annotators.
frequency types are covered, but annotation time
is also ineffectively used on low-frequency types
that happen to appear in those sentences.
Finally, the use of FST features yields the largest
gains for KIN, but only when small amounts of
annotation are available. This makes sense: KIN
is a morphologically rich language, so sparsity is
greater and crude affixes capture less actual mor-
phology. With little annotated data, LP relies heav-
ily on morphological features to make clean links
between words. But, with more annotations, the
gains of the FST over affix features alone dimin-
ishes: the affix features eventually capture enough
of the morphology to make up the difference.
Figure 2 shows the dramatic differences be-
tween the experienced and novice ENG annota-
tors.4 For the former, results using types and to-
4The ENG graph omits ?No LP? results since they fol-
lowed patterns similar to KIN and MLG. Additionally, the
results without FST features are not shown because they were
nearly identical (though slightly lower) than with the FST.
kens were similar after 30 minutes, but type an-
notations proved much more useful beyond that.
In contrast, the novice annotated types much more
slowly, so early on there were not enough anno-
tated types for the training to be as effective. Even
so, after three hours of annotation, type annota-
tions still win with the novice, and even beat the
experienced annotator labeling tokens.
5.2 Mixing type and token annotations
Because type and token annotations are each bet-
ter at providing different information ? a tag dic-
tionary of high-frequency words vs. sequence and
frequency information ? it is reasonable to ex-
pect that a combination of the two might yield
higher performance by each contributing differ-
ent but complementary information during train-
ing. This matters in low-resource settings because
type or token annotations will likely be produced
by the same people, so there is a tradeoff between
spending resources on one form of annotation over
the other. Understanding the best mixture of an-
notations can inform us on how to maximize the
benefit of a set annotation budget. To this end, we
ran experiments fixing the annotation time to four
hours while varying the mix of type and token an-
notations. Results are shown for KIN and MLG in
Figure 3 and ENG in Figure 4.
For KIN and ENG, tagger accuracy increases as
the proportion of type annotations increases for all
LP feature configurations. For MLG, however, as
the reliance on the FST increases, the optimal mix-
ture shifts toward higher type proportions. When
only affix features are used, the optimal mixture is
1 hour of types and 3 hours of tokens. When FST
and affix features are used, the optimum is 2 hours
589
each of types and tokens. When only FST features
are used, it is best to use 3.5 hours of types and
only 30 minutes of tokens. Because the FST op-
erates on word types, it is effective at exploiting
type annotations. Thus, when the LP focuses more
on FST features, it becomes more desirable to have
larger amounts of type annotations.
Types clearly win for ENG. The experienced an-
notator was much faster at annotating types and
the speed difference was less pronounced for to-
kens, so accuracy is most similar when only token
annotations are used. The performance disparity
grows with increasing the type proportion.
Ta?ckstro?m et al (2013) explore the use of
mixed type and token annotations in which a tag-
ger is learned by projecting information via par-
allel text. In their experiments, they?like us?
found that type information is more valuable than
token information. However, they were able to see
gains through the complementary effects of mix-
ing type and token annotations. It is likely that this
difference in our results is due to the amount of an-
notated data used. It seems that the amount of type
information collected in four hours is not sufficient
to saturate the system, meaning that switching to
annotating tokens tends to hurt performance.
5.3 FST development
The third set of experiments evaluate how the
amount of time spent developing an FST affects
the performance of trained tagger. To do this,
we had our ENG FST developer save progress af-
ter each hour (for ten hours). The results show
that, for ENG, the FST provided no value, regard-
less of how much time was spent on its develop-
ment. Moreover, since large gains in accuracy can
be achieved by spending a small amount of time
just annotating word types with POS tags, we are
led to conclude that time should be spent annotat-
ing types or tokens instead of developing an FST.
While it is likely that FST development time would
have a greater impact for morphologically rich
languages, we suspect that greater gains can still
be obtained by instead annotating types. Nonethe-
less, FSTs never seems to hurt performance, so if
one is readily available, it should be used.
5.4 The effect of more raw data
In addition to annotations, semi-supervised tagger
training requires a corpus of raw text. Raw data
can be easier to acquire since it does not need
the attention of a linguist. Even so, for many
Number of Raw Data Tokens
Acc
ura
cy
100k 200k 300k 400k 500k 600k
80
82
84
86
88
90
4hr types, FST, With LP4hr types, FST, No LP1hr types, No FST, With LP
Figure 5: Amount of raw data vs. tagger accuracy
for ENG using high vs. low amounts of annotation
and using LP vs. no LP., for experienced annotator
(novice results were similar).
low-resource languages, the amount of digitized
text, such as transcripts or websites, is very lim-
ited and may, in fact, require substantial effort
to accumulate, even with assistance from compu-
tational tools (Bird, 2011). Therefore, the col-
lection of raw data can be considered another
time-sensitive task for which the tradeoffs with
previously-discussed annotation efforts must con-
tend.
It could be the case that more raw data for train-
ing could make up for additional annotation and
FST development effort or make the LP proce-
dure unnecessary. Figure 5 shows that that in-
creased raw data does provide increasing gains,
but they diminish after 200k tokens. The best per-
formance is achieved by using more annotation
and LP. Most importantly, however, removing ei-
ther annotations or LP results in a significant de-
cline in accuracy, such that even with 600k train-
ing tokens, we are unable to achieve the results of
high annotation and LP using only 100k tokens.
5.5 Correcting existing annotations
For all of the ENG experiments, we also ran ?or-
acle? experiments using gold tags for the same
sentences or a tag dictionary containing the same
number of type/tag entries as the annotator pro-
duced, but containing only the most frequent
entries as determined by the gold-labeled cor-
pus. Using this simulated ?perfect annotator? data
shows we lose accuracy due to annotator mistakes:
for our experienced annotator and maximal FST,
using 4 hours of types the oracle accuracy is 90.5
vs. 88.5 while using only tokens we see 83.9 vs.
590
81.5. This indicates that there are gains to be made
by correcting mistakes in the annotations. This
is true even after the point of diminishing returns
on the learning curve, meaning that even when
adding more annotations no longer improves per-
formance, progress can still be made by correcting
errors, so it may be reasonable to ask annotators to
attempt to correct errors in their past annotations.
Automated techniques for facilitating error identi-
fication can be employed for this (Dickinson and
Meurers, 2003).
6 Conclusions and Future Work
Care must be taken when drawing conclusions
from small-scale annotation studies such as those
presented in this paper. Nonetheless, we have
explored realistic annotation scenarios for POS-
tagging for low-resource languages and found sev-
eral consistent patterns. Most importantly, it is
clear that type annotations are the most useful in-
put one can obtain from a linguist?provided a
semi-supervised algorithm for projecting that in-
formation reliably onto raw tokens is available. In
a sense, this result validates the research trajectory
of efforts of the past two decades put into learning
taggers from tag dictionaries: papers have succes-
sively removed layers of unrealistic assumptions,
and in doing so have produced pipelines for type-
supervision that easily beat token-supervision pre-
pared in comparable amounts of time.
The result of most immediate practical value is
that we show it is possible to train effective POS-
taggers on actual low-resource languages given
only a relatively small amount of unlabeled text
and a few hours of annotation by a non-native
linguist. Instead of having annotators label full
sentences as one might expect the natural choice
would be, it is much more effective to simply
extract a list of the most frequent word types in
the language and concentrate efforts on annotat-
ing these types with their potential parts of speech.
Furthermore, for languages with rich morphology,
a morphological transducer can yield significant
performance gains when large amounts of other
annotated resources are unavailable. (And it never
hurts performance.)
Finally, additional raw text does improve per-
formance. However, using substantial amounts of
raw text is unlikely to produce gains larger than
only a few hours spent annotating types. Thus,
when deciding whether to spend time locating
larger volumes of digitized text or to spend time
annotating types, choose types.
Despite the consistent superiority of type anno-
tations in our experiments, it of course may be the
case that techniques such as active learning may
better select sentences for token annotation, so this
should be explored in future work.
Acknowledgements
We thank Kyle Jerro, Vijay John, Jim Evans, Yoav
Goldberg, Slav Petrov, and the reviewers for their
assistance and feedback. This work was sup-
ported by the U.S. Department of Defense through
the U.S. Army Research Office (grant number
W911NF-10-1-0533) and through a National De-
fense Science and Engineering Graduate Fellow-
ship for the first author. Experiments were run
on the UTCS Mastodon Cluster, provided by NSF
grant EIA-0303609.
References
Steven Abney and Steven Bird. 2010. The human lan-
guage project: Building a universal corpus of the
worlds languages. In Proceedings of ACL.
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP, Singa-
pore.
Steven Bird. 2011. Bootstrapping the language
archive: New prospects for natural language pro-
cessing in preserving linguistic heritage. Linguistic
Issues in Language Technology, 6.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of CoNLL, Taipei, Tai-
wan.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT, Portland,
Oregon, USA.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of EACL.
Weiwei Ding. 2011. Weakly supervised part-of-
speech tagging for Chinese using label propagation.
Master?s thesis, University of Texas at Austin.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP, Jeju, Korea.
591
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL, Atlanta, Georgia.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings NAACL.
Kazi Saidul Hasan and Vincent Ng. 2009.
Weakly supervised part-of-speech tagging for
morphologically-rich, resource-scarce languages. In
Proceedings of EACL, Athens, Greece.
Lauri Karttunen. 2001. Applications of finite-state
transducers in natural language processing. Lecture
Notes in Computer Science, 2088.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of EMNLP, Jeju Island, Korea.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: Is it time for some linguis-
tics? In Proceedings of CICLing.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings ACL.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING.
Emmanuel Roche and Yves Schabes. 1995. Determin-
istic part-of-speech tagging with finite-state trans-
ducers. Computational Linguistics, 21(2).
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings EMNLP, Cambridge, MA.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. In Transactions of the ACL. Association for
Computational Linguistics.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In Proceedings of ECML-PKDD, Bled, Slove-
nia.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS.
592
