
Empirical Methods for Evaluating Dialog Systems
Tim Paek
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
timpaek@microsoft.com
Abstract
We examine what purpose a
dialog metric serves and then
propose empirical methods for
evaluating systems that meet that
purpose. The methods include a
protocol for conducting a wizard-
of-oz experiment and a basic set
of descriptive statistics for
substantiating performance claims
using the data collected from the
experiment as an ideal benchmark
or ?gold standard? for making
comparative judgments. The
methods also provide a practical
means of optimizing the system
through component analysis and
cost valuation.
1 Introduction
In evaluating the performance of dialog systems,
designers face a number of complicated issues.
On the one hand, dialog systems are ultimately
created for the user, so usability factors such as
satisfaction or likelihood of future use should be
the final criteria. On the other hand, because
usability factors are subjective, they can be
erratic and highly dependent on features of the
user interface (Kamm et al, 1999). So, designers
have turned to ?objective? metrics such as
dialog success rate or completion time.
Unfortunately, due to the interactive nature of
dialog, these metrics do not always correspond
to the most effective user experience (Lamel et
al., 2000). Furthermore, several different metrics
may contradict one another (Kamm et al, 1999),
leaving designers with the tricky task of
untangling the interactions or correlations
between metrics.
Instead of focusing on developing new
metrics that circumvents the problems above, we
maintain that designers need to make better use
of the ones that already exist. Toward that end,
we first examine what purpose a dialog metric
serves and then propose empirical methods for
evaluating systems that meet that purpose. The
methods include a protocol for conducting a
wizard-of-oz experiment and a basic set of
descriptive statistics for substantiating
performance claims using the data collected
from the experiment as an ideal benchmark or
?gold standard? for making comparative
judgments. The methods also provide a practical
means of optimizing the system through
component analysis and cost valuation.
2 Purpose
Performance can be measured in myriad ways.
Indeed, for evaluating dialog systems, the one
problem designers do not encounter is lack of
choice. Dialog metrics come in a diverse
assortment of styles. They can be subjective or
objective, deriving from questionnaires or log
files. They can vary in scale, from the utterance
level to the overall dialog (Glass et al, 2000).
They can treat the system as a ?black box,?
describing only its external behavior (Eckert et
al., 1998), or as a ?glass box,? detailing its
internal processing. If one metric fails to suffice,
dialog metrics can be combined. For example,
the PARADISE framework allows designers to
predict user satisfaction from a linear
combination of objective metrics such as mean
recognition score and task completion (Kamm et
al., 1999; Litman & Pan, 1999; Walker et al,
1997).
Why so many metrics? The answer has to do
with more than just the absence of agreed upon
standards in the research community,
notwithstanding significant efforts in that
direction (Gibbon et al, 1997). Part of the
reason deals with what purpose a dialog metric
serves. Designers often have multiple and
sometimes inconsistent needs. Four of the most
typical needs are:
? Provide an accurate estimation of how
well a system meets the goals of the
domain task.
? Allow for comparative judgments of one
system against another, and if possible,
across different domain tasks.
? Identify factors or components in the
system that can be improved.
? Discover tradeoffs or correlations between
factors.
The above list of course is not intended to be
exhaustive. The point of creating the list is to
highlight the kinds of obstacles designers are
likely to face in trying to satisfy just these
typical needs. Consider the first need.
Providing an accurate estimation of how well
a system meets the goals of the domain task
depends on how well the designers have
delineated all the possible goals of interaction.
Unfortunately, users often have finer goals than
those anticipated by designers, even for domain
tasks that seem well defined, such as airline
ticket reservation. For example, a user may be
leisurely hunting for a vacation and not care
about destination or time of travel, or the user
may be frantically looking for an emergency
ticket and not care about price. The
?appropriate? dialog metric should reflect this
kind of subtlety. While ?time to completion? is
more appropriate for emergency tickets,
?concept efficiency rate? is more appropriate for
the savvy vacationer. As psychologists have
long recognized, when people engage in
conversation, they make sure that they mutually
understand the goals, roles, and behaviors that
can be expected (Clark, 1996; Clark & Brennan,
1991; Clark & Schaefer, 1989; Paek & Horvitz,
1999, 2000). They evaluate the ?performance?
of the dialog based on their mutual
understanding and expectations.
Not only do different users have different
goals, they sometimes have multiple goals, or
more often, their goals change dynamically in
response to system behavior such as
communication failures (Danieli & Gerbino,
1995; Paek & Horvitz, 1999). Because goals
engender expectations that then influence
evaluation at different points of time, usability
ratings are notoriously hard to interpret,
especially if the system is not equipped to infer
and keep track of user goals (Horvitz & Paek,
1999; Paek & Horvitz, 2000).
The second typical need for a dialog metric ?
allowing for comparative judgments, introduces
yet further obstacles. In addition to
unanticipated, dynamically changing user goals,
different systems employ different dialog
strategies operating under different architectural
constraints, rendering the search for dialog
metrics that generalize across systems a lofty if
not unattainable pursuit. While the PARADISE
framework facilitates some comparison of
dialog systems in different domain tasks,
generalization is limited because different
architectural constraints obviate certain factors
in the statistical model (Kamm et al, 1997). For
example, although the ability to ?barge-in? turns
out to be a significant predictor of usability,
many systems do not support this. Task
completion based on the kappa statistic appears
to be a good candidate for a common measure,
but only if every dialog system represented the
domain task as an Attribute-Value Matrix
(AVM). Unfortunately, that requirement
excludes systems that use Bayesian networks or
other non-symbolic representations. This has
prompted some researchers to argue that a
?common inventory of concepts? is necessary to
have standard metrics for evaluation across
systems and domain tasks (Kamm et al, 1997;
Glass et al, 2000). As we discuss in the next
section, the argument is actually backwards; we
can use the metrics we already have to define a
common inventory of concepts. Furthermore,
with the proper set of descriptive statistics, we
can exploit these metrics to address the third and
fourth typical needs of designers, that of
identifying contributing factors, along with their
tradeoffs, and optimizing them.
This is not to say that comparative judgments
are impossible; rather, it takes some amount of
careful work to make them meaningful. When
research papers describe evaluation studies of
the performance of dialog systems, it is
imperative that they provide a baseline
comparison from which to benchmark their
systems. Even when readers understand the
scale of the metrics being reported, without a
baseline, the numbers convey very little about
the quality of experience users can expect of the
system. For example, suppose a paper reports
that a dialog system received an average
usability score of 9.5/10, a high concept
efficiency rate of 90%, and a low word error rate
of 5%. The numbers sound terrific, but they
could have resulted from low user expectations
resulting from a simplistic interface. Practically
speaking, to make sense of the numbers, readers
either have to experience interacting with the
system themselves, or have a baseline
comparison for the domain task. This is true
even if the paper reports a statistical model for
predicting one or more of the dialog metrics
from the others, which may reveal tradeoffs but
not how well the system performs relative to the
baseline.
To sum up, in considering the purpose a
dialog metric serves, we examined four typical
needs and discussed the kinds of obstacles
designers are likely to face in finding a dialog
metric that satisfies those needs. The obstacles
themselves present distinct challenges: first,
keeping track of user goals and performance
expectations based on the goals, and second,
establishing a baseline from which to benchmark
systems and make comparative judgments.
Assuming that designers equip their system to
handle the first challenge, we now propose
empirical methods that allow them to handle the
second. These methods do not require new
dialog metrics, but instead take advantage of
existing ones through experimental design and a
basic set of descriptive statistics. They also
provide a practical means of optimizing the
system.
3 Empirical methods
If designers want to make comparative
judgments about the performance of a dialog
system relative to another system so that readers
unacquainted with either system can understand
the reported metrics, they need a baseline.
Fortunately, in evaluating dialog between
humans and computers, the ?gold standard? is
oftentimes known; namely, human conversation.
The most intuitive and effective way to
substantiate performance claims is to compare a
dialog system on a particular domain task with
how human beings perform on the same task.
Because human performance constitutes an ideal
benchmark, readers can make sense of the
reported metrics by assessing how close the
system approaches the gold standard.
Furthermore, with a benchmark, designers can
Figure 1. Wizard-of-Oz study for the purpose of
establishing a baseline comparison.
optimize their system through component
analysis and cost valuation.
In this section, we outline an experimental
protocol for obtaining human performance data
that can serve as a gold standard. We then
highlight a basic set of descriptive statistics for
substantiating performance claims, as well as for
optimization.
3.1 Experimental protocol
Collecting human performance data for
establishing a gold standard requires conducting
a carefully controlled wizard-of-oz (WOZ)
experiment. The general idea is that users
communicate with a human ?wizard? under the
illusion that they are interacting with a
computational system. For spoken dialog
systems, maintaining the illusion usually
involves utilizing a synthetic voice to output
wizard responses, often through voice distortion
or a text-to-speech (TTS) generator.
The typical use of a WOZ study is to record
and analyze user input and wizard output. This
allows designers to know what to expect and
what they should try to support. User input is
especially critical for speech recognition
systems that rely on the collected data for
acoustic training and language modeling. In
iterative WOZ studies, previously collected data
is used to adjust the system so that as the
performance of the system improves, the studies
employ less of the wizard and more of the
system (Glass et al, 2000). In the process,
design constraints in the interface may be
revealed, in which case, further studies are
Users
Wizard
Dialog System
or
Experimentally Controlled Curtain
Controlled Input
Controlled Output
conducted until acceptable tradeoffs are found
(Bernsen et al, 1998).
In contrast to the typical use, a WOZ study
for establishing a gold standard prohibits
modifications to the interface or experimental
?curtain.? As shown in Figure 1, all input and
output through the interface must be carefully
controlled. If designers want to use previously
collected performance data as a gold standard,
they need to verify that all input and output have
remained constant. The protocol for establishing
a gold standard is straightforward:
? Select a dialog metric to serve as an
objective function for evaluation and
optimization.
? Vary the component or feature that best
matches the desired performance claim for
the dialog metric.
? Hold all other input and output through
the interface constant so that the only
unknown variable is who does the internal
processing.
? Repeat using different wizards, making
sure that each wizard follows strict
guidelines for interacting with subjects.
To motivate the above protocol, consider
how a WOZ study might be used to evaluate
spoken dialog systems. As almost every
designer has found, the ?Achilles? heel? of
spoken interaction is the fragility of the speech
recognizer. System performance depends highly
on the quality of the recognition. Suppose a
designer is interested in bolstering the
robustness of a dialog system by exploiting
different types of repair strategies. Using task
completion rate as an objective function, the
designer varies the repair strategies utilized by
the system. To make claims about the robustness
of particular types of repair strategies, the
designer must keep all other input and output
constant. In particular, the protocol demands that
the wizard in the experiment must receive
utterances through the same speech recognizer
as the dialog system. The performance of the
wizard on the same quality of input as the dialog
system constitutes the gold standard. The
designer may also wish to keep the set of repair
strategies constant while varying the use or
disuse of the speech recognizer to estimate how
much the recognizer alone degrades task
completion rate.
A deep intuition underlies the experimental
control of the speech recognizer. As researchers
have observed, people with impaired hearing or
non-native language skills still manage to
communicate effectively despite noisy or
uncertain input. Unfortunately, the same cannot
be said of computers with analogous
deficiencies. People overcome their deficiencies
by collaboratively working out the mutual belief
that their utterances have been understood
sufficiently for current purposes, a process
referred to as ?grounding? (Clark, 1996). Repair
strategies based on grounding indeed show
promise for improving the robustness of spoken
dialog systems (Paek & Horvitz, 1999; Paek &
Horvitz, 2000).
3.1.1 Precautions
In following the above protocol, we point out
a few precautions. First, WOZ studies for
establishing a gold standard work best with
dialog systems that are highly modular. The
more modular the architecture of the dialog
system, the easier it will be to test components
by replacing a particular module of interest with
the wizard. Without modularity, it will be harder
to guarantee that all other inputs and outputs
have remained constant because component
boundaries are blurred. Ironically, after a certain
point, a high degree of modularity may in fact
preclude the experimental protocol; components
may be so specialized and quickly accessed by a
system that it may not be feasible to replace that
component with a human.
A second precaution deals with the concept
of a gold standard. What allows the performance
of the wizard to be used as a gold standard is not
the wizard, but rather the fact that the
performance constitutes an upper bound. If an
upper bound of performance has already been
identified, then that is the gold standard. For
example, graphical user interfaces (GUI) or
touch-tone systems may represent a better gold
standard for task completion rate if users finish
their interactions with such systems ore often
than with human operators. With spoken dialog
systems, the question of when the use of speech
interaction is truly compelling is often ignored.
If a dialog designer runs the experimental
protocol and observes that even human wizards
cannot perform the domain task very well, that
suggests that perhaps a gold standard may be
found elsewhere.
Figure 2. Comparison of two dialog systems
with respect to the gold standard.
3.2 Descriptive statistics
After collecting data using the experimental
protocol, designers can make comparative
judgments about the performance of their system
relative to other systems with a basic set of
descriptive statistics. The statistics build on the
initial step of fitting a statistical model on the
data fro both wizards and the dialog system. We
discuss precautions later. Plotting the fitted
curves on the same graph sheds light on how
best to substantiate any performance claims. The
graph displays the performance of the dialog
system along a particular dimension of interest
with the wizard data constituting a gold standard
for comparison. Consider how this kind of
?benchmark graph? could benefit the evaluation
of spoken dialog systems.
Referring to previous example, suppose a
designer is interested in evaluating the
robustness of two dialog systems utilizing two
sets of repair strategies. The designer varies
which set is implemented, while holding
constant the use of the speech recognizer. In
general, as speech recognition errors increase,
task completion rate, or dialog success rate,
decreases. Not surprisingly, several researchers
have found an approximately linear relationship
in plotting task completion rate as a function of
word error rate (Lamel et al, 2000; Rudnicky,
2000). Keeping this in mind, Figure 2 displays a
benchmark graph for two dialog systems A and
B, utilizing different repair strategies. The fitted
curve for A is characteristically linear, while the
curve for B is polynomial. Because wizards are
presumably more capable of anticipating and
recovering from speech recognition errors, their
Figure 3. Distance in performance of the two
systems from the gold standard.
performance data comprise the gold standard.
As such, the fitted curve for the gold standard in
Figure 2 stays close to the upper right hand
corner of the graph in a monotonically
decreasing fashion; that is, task completion rate
remains relatively high as word error rate
increases and then gracefully degrades before
the error rate reaches its highest level.
Looking at the benchmark graph, readers
immediately get a handle on substantiating
performance claims about robustness. For
example, by noticing that task completion rate
for the gold standard rapidly drops from around
65% at the 80% mark to about 15% by 100%,
readers know that at 80% word error rate, even
wizards, with human level intelligence, cannot
recover from failures with better than 65% task
completion rate. In short, the task is not trivial.
This means that if A and B report low numbers
for task completion rate beyond the 80% mark
for word error rate, they may be still performing
relatively well compared to the gold standard.
Numbers themselves are deceptive, unless they
are put side by side with a benchmark.
Of course, a designer might not have access
to data all along the word error rate continuum
as in Figure 2. If this presents a problem, it may
be more appropriate to measure task completion
rate as a function of concept error rate. The
choice, as stated in the experimental protocol,
depends on the performance claim a designer is
interested in making. In spoken dialog, however,
where speech recognition errors abound, another
particularly useful benchmark graph is to plot
word or concept error rate against user
frustration. This experiment reveals any inherent
Benchmark Graph
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100
Word Error Rate (%)
Ta
sk
Co
m
pl
et
io
n
Ra
te
(%
)
System A System B Gold Standard
Gold ImpurityGraph
0
5
10
15
20
25
30
35
40
45
50
0 10 20 30 40 50 60 70 80 90 100
Word Error Rate (%)
Ta
sk
Co
m
pl
et
io
n
Ra
te
Di
ffe
re
n
ce
(%
)
|A - G| |B - G|
System B Mass
System A Mass
Ta
sk
Co
m
pl
et
io
n
Ra
te
Di
ffe
re
n
ce
(%
)
bias users may have towards speaking with a
computer in the first place.
In making comparative judgments, designers
can also benefit from plotting the absolute
difference in performance from the gold
standard as a function of the same independent
variable as the benchmark graph. Figure 3
displays the difference in task completion rate,
or ?gold impurity,? for systems A and B as a
function of word error rate. The closer a system
is to the gold standard, the smaller the ?mass? of
the gold impurity on the graph. Anomalies are
easier to see, as they noticeably show up as
bumps or peaks. If a dialog system reports low
numbers but evinces little gold impurity, reader
can be assured that the system is as good as it
can possibly be.
Any crosses in performance can be revealing
as well. For example, in Figure 3, although B
performs worse at lower word error rates than A,
after about the 35% mark, B stays closer to the
gold standard. Hence, the designer in this case
could not categorically prefer one system to the
other. In fact, assuming that the only difference
between A and B is the choice of repair
strategies, the designer should prefer A to B if
the average word error rate for the speech
recognizer is below 35%, and B to A, if the
average error rate is about 40%. Of course, other
cost considerations come into play, as we
describe later.
The final point to make about comparing
dialog systems to a gold standard is that readers
are able to substantiate performance claims
across different domain tasks. They need only to
look at how close each system approaches their
respective gold standard in a benchmark graph,
or how much mass each system puts out in a
gold impurity graph. They can even do this
without having the luxury of experiencing any
of the compared systems.
3.2.1 Complexity
Without a gold standard, making comparative
judgments of dialog systems across different
domain tasks poses a problem for two reasons:
task complexity and interaction complexity.
Tutoring physics is a generally more complex
domain task than retrieving email. On the other
hand, task complexity alone does not explain
what makes one dialog more complex than
another; interaction complexity also plays a
significant role. Tutoring physics can be less
challenging than retrieving email if the system
accepts few inputs, essentially constraining users
to follow a predefined script. Any dialog system
that engages in ?mixed initiative? will be more
complex than one that utilizes ?system-initiated?
prompts because users have more actions at their
disposal at any point in time.
The way to evaluate complexity in a
benchmark graph is to measure the distance of
the gold standard to the absolute upper bound of
performance. If wizards with human level
intelligence cannot themselves perform
reasonably close to the absolute upper bound,
then either the task is very complex, or the
interaction afforded by the dialog interface is too
restrictive for wizards, or perhaps both. Because
complexity is measured only in connection with
the gold standard ceteris paribus, ?benchmark
complexity? can be computed as:
?
=
??=
n
x
xgUnBC
0
)(
where U is the upper bound value of a
performance metric, n is the upper bound value
for an independent variable X, and g(x) is the
gold standard along that variable.
Designers can use benchmark complexity to
compare systems across different domain tasks
if they are not too concerned about
discriminating between task complexity and
interaction complexity. Otherwise, they can treat
benchmark complexity as an objective function
and vary the interaction complexity of the dialog
interface to scrutinize the effect of task
complexity on wizard performance, or vice
versa. In short, they need to conduct another
experimental study.
3.2.2 Precautions
Before substantiating performance claims with a
benchmark graph, designers must exercise
prudence in model fitting. One precaution is to
beware of insufficient data. Without collecting
enough data, designers cannot be certain that
differences in the performance of a dialog
system from the gold standard cannot be
explained simply by the variance in the fitted
models. To determine when there is enough data
to generate reliable models, designers can
conduct WOZ studies in an iterative fashion.
First, collect some data and fit a statistical
model. Second, plot the least squares distance,
or ? ?
i
ii xfy 2))(( , where f(x) is the fitted
model, against the iteration. Keep collecting
more data until the plot seems to asymptotically
converge. Designers may need to report R2s for
the curves in their benchmark graphs to inform
readers of the reliability of their models.
Another precaution is to use different
wizards, making sure that each wizard follows
strict guidelines for interacting with subjects.
The experimental protocol included this
precaution because designers need to consider
whether a consistent gold standard is even
possible with a given dialog interface. Indeed,
difference between wizards may uncover serious
design flaws in the interface. Furthermore, using
different wizards compels designers to collect
more data for the gold standard.
As a final precaution, designers need to
watch out for violations of model assumptions
regarding residual errors. These are typically
well covered in most statistics textbooks. For
example, because task completion rate as a
performance metric has an upper bound of
100%, it is unlikely that residual errors will be
equally spread out along the word error rate
continuum. In regression analysis, this is called
?heteroscedasticity.? Another common violation
occurs with the non-normality of the residual
errors. Designers would do well to take
advantage of corrective measures for both.
3.2.3 Component analysis
A gold standard naturally lends itself to
optimization. With a gold standard, designers
can identify which components are contributing
the most to a performance metric by examining
the gold impurity graph of the system with and
without a particular component. This kind of test
is similar to how dissociations are discovered in
neuroscience through ?lesion? experiments.
Carrying out stepwise comparisons of the
components, designers can check for tradeoffs,
and even use all or part of the gold impurity as
an optimization metric. For example, suppose a
designer endeavors to improve a dialog system
from its current average task completion rate of
70% to 80%. In Figure 2, suppose B
incorporates a component that A does not.
Looking at the corresponding word error rates in
the gold impurity graph for both systems, the
Figure 4. The cost a designer is willing to incur
for improvements to task completion rate.
mass under the curve for B is slightly greater
than that for A. The designer can optimize the
performance of the system by selecting
components that minimize that mass, in which
case, the component in B would be excluded.
Because components often interact with each
other in terms of their statistical effect on the
performance metric, designers may wish to carry
out a multi-dimensional analysis to weed out
those components with weak main and
interaction effects.
3.2.4 Cost valuation
Another optimization use of a gold standard is to
minimize the amount of ?gold? expended in
developing a dialog system. Gold here includes
more than just dollars, but time and effort as
well. Designers can determine where to invest
their research focus by calculating ?average
marginal cost.? To do this, they must first elicit
a cost function that conveys what they are
willing to pay, in terms of utility, to achieve
various levels of performance in a dialog metric
(Bell et al, 1988). Figure 4 displays what cost a
designer might be willing to incur for various
rates of task completion. The average marginal
cost can be computed by weighting gold
impurity by the cost function. In other words,
average marginal cost can be computed as:
?
=
??=
b
ax
xgxfxcAMC )()()(
Cost for Improvement
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100
Task Completion Rate (%)
Ut
ili
ty
Co
st
where f(x) is the performance of the system on a
particular dialog metric X, g(x) is the gold
standard on that metric, and c(x) is elicited cost
function.
Following the previous example, if the
designer endeavors to improve a system that is
currently operating at an average task
completion rate of 70% to 80%, then the average
marginal cost for that gain is simply the area
under the cost function for that interval
multiplied by the gold impurity for that interval.
In deciding between systems or components,
designers can exploit average marginal cost to
drive down their expenditure.
4 Discussion
Instead of focusing on developing new dialog
metrics that allow for comparative judgments
across different systems and domain tasks, we
proposed empirical methods that accomplish the
same purpose while taking advantage of dialog
metrics that already exist. In particular, we
outlined an experimental protocol for
conducting a WOZ study to collect human
performance data that can serve as a gold
standard. We then described how to substantiate
performance claims using both a benchmark
graph and a gold impurity graph. Finally, we
explained how to optimize a dialog system using
component analysis and value optimization.
Without a doubt, the greatest drawback to the
empirical methods proposed is the tremendous
cost of conducting WOZ studies, both in terms
of time and money. In special circumstances,
such as the Communicator Project, where
participants all work within the same domain
task, DARPA itself might finance WOZ studies
for evaluation on behalf of the participants. Non-
participants may resort to average marginal cost
to optimize their own expenditure.
References
Bell, D. E., Raiffa, H., & Tversky, A. (Eds.). (1988).
Decision making: Descriptive, normative, and
prescriptive interactions. New York: Cambridge
University Press.
Bersen, N. O., Dybkjaer, H. & Dybkjaer, L. (1998).
Designing interactive speech systems: From first
ideas to user testing. Springer-Verlag.
Clark, H.H. (1996). Using language. Cambridge
University Press.
Clark, H.H. & Brennan, S.A. (1991). Grounding in
communication. In Perspectives on Socially
Shared Cognition, APA Books, 127-149.
Clark, H.H. & Schaefer, E.F. (1989). Contributing to
discourse. Cognitive Science, 13, 259-294.
Danieli, M. & Gerbino, E. (1995). Metrics for
evaluating dialogue strategies in a spoken
language system. In Proc. AAAI Spring
Symposium on Empirical Methods in Discourse
Interpretation and Generation, 34-39.
Eckert, W., Levin, E. & Pieraccini, R. (1998).
Automatic evaluation of spoken dialogue systems.
In TWLT13: Formal semantics and pragmatics of
dialogue, 99-110.
Gibbon, D., Moore, R. & Winski, R. (Eds.) (1998).
Handbook of standards and resources for spoken
language systems. Spoken Language System
Assessment, 3, Walter de Gruyter, Berlin.
Glass, J., Polifroni, J., Seneff, S. & Zue, V. (2000).
Data collection and performance evaluation of
spoken dialogue systems: The MIT experience. In
Proc. of ICSLP.
Horvitz, E. & Paek, T. (1999). A computational
architecture for conversation. In Proc. of 7th User
Modeling, Springer Wien, 201-210.
Kamm, C., Walker, M.A. & Litman, D. (1999).
Evaluating spoken language systems. In Proc. of
AVIOS.
Lamel, L., Rosset S. & Gauvain, J.L. (2000).
Considerations in the design and evaluation of
spoken language dialog systems. In Proc. of
ICSLP.
Litman, D. & Pan, S. (1999). Empirically evaluating
an adaptable spoken dialogue system. In Proc. of
7th User Modeling, Springer Wien, 55-64.
Paek, T. & Horvitz, E. (2000). Conversation as
action under uncertainty. In Proc. of 16th UAI,
Morgan Kaufmann, 455-464.
Paek, T. & Horvitz, E. (1999). Uncertainty, utility,
and misunderstanding: A decision-theoretic
perspective on grounding in conversational
systems. In Proc. of AAAI Fall Symposium on
Psychological Models of Communication, 85-92.
Rudnicky, A. (2000). Understanding system
performance in dialog systems. MSR Invited Talk.
Walker, M.A., Litman, D., Kamm, C. & Abella, A.
(1997). PARADISE: A framework for evaluating
spoken dialogue agents. In Proceedings of the 35th
ACL.
Empirical Methods for Evaluating Dialog Systems
Tim Paek
Microsoft Research
One Microsoft Way
Redmond, WA 98052
timpaek@microsoft.com
Paper ID: SIGDIAL_TP
Keywords: dialog evaluation, dialog metric, descriptive statistics, gold standard, wizard-of-oz
Contact Author: Tim Paek
Abstract
We examine what purpose a dialog metric serves and then propose empirical methods
for evaluating systems that meet that purpose. The methods include a protocol for
conducting a wizard-of-oz experiment and a basic set of descriptive statistics for
substantiating performance claims using the data collected from the experiment as an
ideal benchmark or ?gold standard? for comparative judgments. The methods also
provide a practical means of optimizing the system through component analysis and
cost valuation.
Empirical Methods for Evaluating Dialog Systems
Abstract
We examine what purpose a
dialog metric serves and then
propose empirical methods for
evaluating systems that meet that
purpose. The methods include a
protocol for conducting a wizard-
of-oz experiment and a basic set
of descriptive statistics for
substantiating performance claims
using the data collected from the
experiment as an ideal benchmark
or ?gold standard? for
comparative judgments. The
methods also provide a practical
means of optimizing the system
through component analysis and
cost valuation.
1 Introduction
In evaluating the performance of dialog systems,
designers face a number of complicated issues.
On the one hand, dialog systems are ultimately
created for the user, so usability factors such as
satisfaction or likelihood of future use should be
the final criteria. On the other hand, because
usability factors are subjective, they can be
erratic and highly dependent on features of the
user interface (Kamm et al, 1999). So, designers
have turned to ?objective? metrics such as
dialog success rate or completion time.
Unfortunately, due to the interactive nature of
dialog, these metrics do not always correspond
to the most effective user experience (Lamel et
al., 2000). Furthermore, several different metrics
may contradict one another (Kamm et al, 1999),
leaving designers with the tricky task of
untangling the interactions or correlations
between metrics.
Instead of focusing on developing a new
metric that circumvents the problems above, we
maintain that designers need to make better use
of the ones that already exist. Toward that end,
we first examine what purpose a dialog metric
serves and then propose empirical methods for
evaluating systems that meet that purpose. The
methods include a protocol for conducting a
wizard-of-oz experiment and a basic set of
descriptive statistics for substantiating
performance claims using the data collected
from the experiment as an ideal benchmark or
?gold standard? for comparative judgments. The
methods also provide a practical means of
optimizing the system through component
analysis and cost valuation.
2 Purpose
Performance can be measured in myriad ways.
Indeed, for evaluating dialog systems, the one
problem designers do not encounter is lack of
choice. Dialog metrics come in a diverse
assortment of styles. They can be subjective or
objective, deriving from questionnaires or log
files. They can vary in scale, from the utterance
level to the overall dialog (Glass et al, 2000).
They can treat the system as a ?black box,?
describing only its external behavior (Eckert et
al., 1998), or as a ?glass box,? detailing its
internal processing. If one metric fails to suffice,
dialog metrics can be combined. For example,
the PARADISE framework allows designers to
predict user satisfaction from a linear
combination of objective metrics such as mean
recognition score and task completion (Kamm et
al., 1999; Litman & Pan, 1999; Walker et al,
1997).
Why so many metrics? The answer has to do
with more than just the absence of agreed upon
standards in the research community,
notwithstanding significant efforts in that
direction (Gibbon et al, 1997). Part of the
reason deals with the purpose a dialog metric
serves. Designers want a dialog metric to
address the multiple, sometimes inconsistent
needs. Here are four typical needs:
(1) Provide an accurate estimation of how well a
system meets the goals of the domain task.
(2) Allow for comparative judgments of one
system against another, and if possible, across
different domain tasks.
(3) Identify factors or components in the system
that can be improved.
(4) Discover tradeoffs or correlations between
factors.
While the above list is not intended to be
exhaustive, it is instructive. Creating such a list
can help designers to anticipate the kinds of
obstacles they are likely to face in trying to
satisfy all of the needs. Consider the first need
on the list.
Providing an accurate estimation of how well
a system meets the goals of the domain task
depends on how well the designers have
delineated all the possible goals of interaction.
Unfortunately, users often have finer goals than
those anticipated by designers, even for domain
tasks that seem well defined, such as airline
ticket reservation. For example, a user may be
leisurely hunting for a vacation and not care
about destination or time of travel, or the user
may be frantically looking for an emergency
ticket and not care about price. The
?appropriate? dialog metric should reflect even
these kinds of goals. While ?time to completion?
is more appropriate for the emergency ticket,
?concept efficiency rate? is more appropriate for
the savvy vacationer. As psychologists have
long recognized, when people engage in
conversation, they make sure that they mutually
understand the goals, roles, and behaviors that
can be expected (Clark, 1996; Clark & Brennan,
1991; Clark & Schaefer, 1987, 1989). They
evaluate the ?performance? of the dialog based
on their mutual understanding and expectations.
Not only do different users have different
goals, they sometimes have multiple goals, or
more often, their goals change dynamically in
response to system behavior such as
communication failures (Danieli & Gerbino,
1995; Paek & Horvitz, 1999). Because goals
engender expectations that then influence
evaluation at different points of time, usability
ratings are notoriously hard to interpret,
especially if the system is not equipped to infer
and keep track of user goals (Horvitz & Paek,
1999; Paek & Horvitz, 2000).
The second typical need for a dialog metric ?
allowing for comparative judgments, introduces
further obstacles. In addition to unanticipated,
dynamically changing user goals, different
systems employ different dialog strategies
operating under different architectural
constraints, making the search for a dialog
metric that generalizes across systems nearly
impossible. While the PARADISE framework
facilitates some comparison of dialog systems in
different domain tasks, generalization is limited
because different components can render factors
irrelevant in the statistical model (Kamm et al,
1997). For example, a common measure of task
completion would be possible if every system
represented the domain task as an Attribute-
Value Matrix (AVM). Unfortunately, that
requirement excludes systems that use Bayesian
networks or other non-symbolic representations.
This has prompted some researchers to argue
that a ?common inventory of concepts? is
necessary to have standard metrics for
evaluation across systems and domain tasks
(Kamm et al, 1997; Glass et al, 2000). As we
discuss in the next section, the argument is
actually backwards; we can use the metrics we
already have to define a common inventory of
concepts. Furthermore, with the proper set of
descriptive statistics, we can exploit these
metrics to address the third and fourth typical
needs of designers, that of identifying
contributing factors, along with their tradeoffs,
and optimizing them.
This is not to say that comparative judgments
are impossible; rather, it takes some amount of
careful work to make them meaningful. When
research papers describe evaluation studies of
the performance of dialog systems, it is
imperative that they provide a baseline
comparison from which to benchmark their
systems. Even when readers understand the
scale of the metrics being reported, without a
baseline, the numbers convey very little about
the quality of experience users of the system can
expect. For example, suppose a paper reports
that a dialog system received an average
usability score of 9.5/10, a high concept
efficiency rate of 90%, and a low word error rate
of 5%. These numbers sound terrific, but they
could have resulted from low user expectations
and a simplistic or highly constrained interface.
Practically speaking, readers must either
experience interacting with the system
themselves, or have a baseline comparison for
the domain task from which to make sense of
the numbers. This is true even if the paper
reports a statistical model for predicting one or
more of the metrics from the others, which may
reveal tradeoffs but not how well the system
performs relative to the baseline.
To sum up, in considering the purpose a
dialog metric serves, we examined four typical
needs and discussed the kinds of obstacles
designers are likely to face in finding a dialog
metric that satisfies those needs. The obstacles
themselves present distinct challenges: first,
keeping track of user goals and expectations for
performance based on the goals, and second,
establishing a baseline from which to benchmark
systems and make comparative judgments.
Assuming that designers equip their system to
handle the first challenge, we now propose
empirical methods that allow them to handle the
second, while at the same time providing a
practical means of optimizing the system. These
methods do not require new metrics, but instead
take advantage of existing ones through
experimental design and a basic set of
descriptive statistics.
3 Empirical methods
Before designers can make comparative
judgments about the performance of a dialog
system relative to another system, so that readers
unacquainted with either system can understand
the reported metrics, they need a baseline.
Fortunately, in evaluating dialog between
humans and computers, the ?gold standard? is
oftentimes known; namely, human conversation.
The most intuitive and effective way to
substantiate performance claims is to compare a
dialog system on a particular domain task with
how human beings perform on the same task.
Because human performance constitutes an ideal
benchmark, readers can make sense of the
reported metrics by assessing how close the
system approaches the gold standard.
Furthermore, with a benchmark, designers can
optimize their system through component
analysis and cost valuation.
In this section, we outline an experimental
protocol for obtaining human performance data
that can serve as a gold standard. We then
highlight a basic set of descriptive statistics for
substantiating performance claims, as well as for
optimization.
3.1 Experimental protocol
Collecting human performance data for
establishing a gold standard requires conducting
a carefully controlled wizard-of-oz (WOZ)
study. The general idea is that users
communicate with a human ?wizard? under the
illusion that they are interacting with a
computational system. For spoken dialog
Figure 1. Wizard-of-Oz study for the purpose of
establishing a baseline comparison.
systems, maintaining the illusion usually
involves utilizing a synthetic voice to output
wizard responses, often through voice distortion
or a text-to-speech (TTS) generator.
The typical use of a WOZ study is to record
and analyze user input and wizard output. This
allows designers to know what to expect and
what they should try to support. User input is
especially critical for speech recognition
systems that rely on the collected data for
acoustic training and language modeling. In
iterative WOZ studies, previously collected data
is used to adjust the system so that as the
performance of the system improves, the studies
employ less of the wizard and more of the
system (Glass et al, 2000). In the process,
design constraints in the interface may be
revealed, in which case, further studies are
conducted until acceptable tradeoffs are found
(Bernsen et al, 1998).
In contrast to the typical use, a WOZ study
for establishing a gold standard prohibits
modifications to the interface or experimental
?curtain.? As shown in Figure 1, all input and
output through the interface must be carefully
controlled. If designers want to use previously
collected performance data as a gold standard,
they need to verify that all input and output have
remained constant. The protocol for establishing
a gold standard is straightforward:
(1) Select a dialog metric to serve as an
objective function for evaluation.
(2) Vary the component or feature that best
matches the desired performance claim for the
dialog metric.
Users
Wizard
Dialog System
or
Experimentally Controlled Curtain
Controlled Input
Controlled Output
(3) Hold all other input and output through the
interface constant so that the only unknown
variable is who does the internal processing.
(4) Repeat using different wizards.
To motivate the above protocol, consider
how a WOZ study might be used to evaluate
spoken dialog systems. The Achilles? heel of
spoken interaction is the fragility of the speech
recognizer. System performance depends highly
on the quality of the recognition. Suppose a
designer is interested in bolstering the
robustness of a dialog system by exploiting
various repair strategies. Using task completion
rate as an objective function, the designer varies
the repair strategies utilized by the system. To
make claims about the robustness of these repair
strategies, the designer must keep all other input
and output constant. In particular, the wizard in
the experiment must receive utterances through
the same speech recognizer as the dialog system.
The performance of the wizard on the same
quality of input as the dialog system constitutes
the gold standard. The designer may also wish to
keep the set of repair strategies constant while
varying the use or disuse of the speech
recognizer to estimate how much the recognizer
degrades task completion.
A deep intuition underlies the experimental
control of the speech recognizer. As researchers
have observed, people with impaired hearing or
non-native language skills still manage to
communicate effectively despite noisy or
uncertain input. Unfortunately, the same cannot
be said of computers with analogous
deficiencies. People overcome their deficiencies
by collaboratively working out the mutual belief
that their utterances have been understood
sufficiently for current purposes ? a process
referred to as ?grounding? (Clark, 1996). Repair
strategies based on grounding indeed show
promise for improving the robustness of spoken
dialog systems (Paek & Horvitz, 1999; Paek &
Horvitz, 2000).
3.1.1 Precautions
A few precautions are in order. First, WOZ
studies for establishing a gold standard work
best with dialog systems that are highly
modular. Modularity makes it possible to test
components by replacing a module with the
wizard. Without modularity, it is harder to use
because the boundaries between components are
the performance of the wizard as a gold standard
Figure 2. Comparison of two dialog systems
with respect to the gold standard.
blurred. Second, what allows the performance of
the wizard to be used as a gold standard is not
the wizard, but rather the fact that the
performance constitutes an upper bound. For
example, the upper bound may be better
established by graphical user interfaces (GUI) or
touch-tone systems, in which case, those
systems should be the gold standard.
3.2 Descriptive statistics
After designers collect data from the WOZ
study, they can make comparative judgments
about the performance of their system relative to
other systems using a basic set of descriptive
statistics. The descriptive statistics rest on first
model fitting the data for both the wizard and
the dialog system. Plotting the fitted curves on
the same graph sheds light on how best to
substantiate any performance claims. In fact, we
advocate that designers present this ?benchmark
graph? to assist readers in interpreting dialog
metrics.
Using spoken dialog again as an example,
suppose a designer is evaluating the robustness
of two dialog systems utilizing two different
repair strategies. The designer varies the repair
strategies, while holding constant the use of the
speech recognizer. As speech recognition errors
increase, numerous researchers have shown that
task completion rate, or dialog success rate, not
surprisingly decreases. Plotting task completion
rate as a function of word error rate discloses an
approximately linear relationship (Lamel et al,
2000; Rudnicky, 2000).
Figure 2 displays a benchmark graph for two
Benchmark Graph
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100
Word Error Rate (%)
Ta
sk
Co
m
pl
et
io
n
Ra
te
(%
)
System A System B Gold Standard
Figure 3. Distance in performance of the two
systems from the gold standard.
dialog systems A and B, utilizing different repair
strategies. Suppose that the fitted curve for
System A is characteristically linear, while the
curve for System B is polynomial. Because
wizards are presumably more capable of
recovering from recognition errors, their
performance data make up the gold standard.
Figure 2 shows a fitted curve for the gold
standard staying close to the upper right hand
corner of the graph in a monotonically
decreasing fashion; that is, task completion rate
remains relatively high as word error rate
increases and then gracefully degrades before
the error rate reaches 100%.
Looking at the benchmark graph, readers
immediately get a sense of how to substantiate
performance claims about robustness. For
example, by noticing that task completion rate
for the gold standard rapidly drops from around
65% at the 80% mark to about 15% by 100%,
readers know that at 80% word error rate, even
wizards, with human level intelligence, cannot
recover from failures with better than 65% task
completion rate. In other words, the task is
difficult. So, even if System A and B report low
task completion rates after the 80% word error
rate, they may be performing relatively well
compared to the gold standard.
In making comparative judgments, it helps to
plot the absolute difference in performance from
the gold standard as a function of the same
independent variable as the benchmark graph.
Figure 3 displays such a ?gold impurity graph?
for Systems A and B as a function of word error
rate. The closer a system is to the gold standard,
the smaller the ?mass? of the gold impurity on
the graph. Anomalies are easier to see, as they
typically show up as bumps or peaks. The
advantage of the graph is that if a dialog system
reports terrible numbers on various performance
metrics but displays a splendidly small gold
impurity, the reader can be assured that the
system is as good as it can possibly be.
Looking at the gold impurity graph for
Systems A and B, without having experienced
either of the two systems, readers can make
comparative judgments. For example, although
B performs worse at lower word error rates than
A, after about the 35% mark, B stays closer to
the gold standard. With such crosses in
performance, designers cannot categorically
prefer one system to the other. In fact, assuming
that the only difference between A and B is the
choice of repair strategies, designers should
prefer A to B if the average word error rate for
the speech recognizer is below 35%, and B to A,
if the average error rate is about 40%.
With a gold standard, readers are even able to
substantiate performance claims about different
dialog systems across domain tasks. They need
only to look at how close each system is to their
respective gold standard in a benchmark graph,
and how much mass each system shows in a
gold impurity graph.
3.2.1 Complexity
One reason why comparative judgments,
without a gold standard, are so hard to make
across different domain tasks is task complexity.
For example, tutoring physics is generally more
complex than retrieving email. Another reason is
dialog complexity. A physics tutoring system
will be less complex if the system forces users to
follow a predefined script. An email system that
engages in ?mixed initiative? will always be
more complex because the user can take more
possible actions at any point in time.
The way to express complexity in a
benchmark graph is to measure the distance of
the gold standard to the absolute upper bound of
performance. If wizards with human level
intelligence cannot perform close to the absolute
upper bound, then the task is complex, or the
dialog interface is too restrictive for wizard, or
both. Because complexity is measured only in
connection with the gold standard ceteris
paribus, ?intellectual complexity? can be
defined as:
Gold Impurity Graph
0
5
10
15
20
25
30
35
40
45
50
0 10 20 30 40 50 60 70 80 90 100
Word Error Rate (%)
Ta
sk
Co
m
pl
et
io
n
Ra
te
Di
ffe
re
n
ce
(%
)
|A - G| |B - G|
System B Mass
System A Mass
Ta
sk
Co
m
pl
et
io
n
Ra
te
Di
ffe
re
n
ce
(%
)
?
=
??=
n
x
xgUnIC
0
)(
where U is the upper bound value of a
performance metric, n is the upper bound value
for an independent variable x, and g(x) is the
gold standard along that variable.
Designers can use intellectual complexity to
compare systems across different domain tasks
if they are not too concerned about
discriminating task complexity from dialog
complexity. Otherwise, they can use intellectual
complexity an objective function and vary the
complexity of the dialog interface to scrutinize
how much task complexity affects wizard
performance.
3.2.2 Precautions
Before substantiating performance claims with a
benchmark graph, designers should exercise a
few precautionary measures. First, in model
fitting a gold standard or the performance of a
dialog system, beware of insufficient data.
Without sufficient data, differences from the
gold standard may be due to variance in the
models. To guarantee that designers have
collected enough data, we recommend that they
go through an iterative process. First, run
subjects, collect data, and fit a model. Then plot
the least squares distance, or ? ?
i
ii xfy 2))(( ,
where f(x) is the fitted model, against the
iteration. Keep running more subjects until the
plot seems to approach convergence. To inform
readers of the reliability of the fitted models, we
suggest that designers either show the
convergence plot or report their R2s for their
curves (which relate how much of the variance
can be accounted for by the fitted models).
Second, to guarantee the reliability of the gold
standard, use different wizards. The
experimental protocol listed this as the last point
because it is important to know whether a
consistent gold standard is even possible with
the given interface. Difference between wizards
may reveal serious design flaws. Furthermore,
just as adding more subjects improves the fit of
the dialog performance models, the law of large
numbers applies equally to the gold standard.
Finally, designers may encounter problems
with residual errors in model fitting that are
typically well covered in most statistics
Figure 4. Dollar amount designer is willing to
pay for improvements to task completion rate.
textbooks. For example, because the
performance metric shown in Figure 2 and 3,
task completion rate, has an upper bound of
100%, it is unlikely that residual errors will be
equally spread out at all word error rates.
Another common problem is the non-normality
of the residual errors, which violates the model
assumption.
3.2.3 Component analysis
Designers can identify which components are
contributing the most to a performance metric
by examining the gold impurity graph of the
system with and without the component,
rendering this kind of test similar to a ?lesion?
experiment. Carrying out stepwise comparisons
of the components, designers can check for
tradeoffs, and even use all or part of the mass
under the curve as an optimization metric. For
example, a designer may wish to improve a
dialog system from its current average task
completion rate of 70% to 80%. Suppose that
System B in Figure 2 incorporates a particular
component that System A does not. Looking at
the corresponding word error rates in the gold
impurity graph for both systems, the mass under
the curve for B is slightly greater than that for A.
The designer can optimize the performance of
the system by selecting components that
minimize that mass, in which case, the
component in System B should be excluded.
Because components may interact with each
other, designers may want to carry out a multi-
dimensional component analysis for
optimization.
Cost for Improvement
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100
Task Completion Rate (%)
Do
lla
rs
in
Th
o
u
sa
n
ds
($)
3.2.4 Cost valuation
Suppose the main concern of the designer is to
optimize the monetary cost of the dialog system.
The designer can determine how much
improving the system is worth by calculating the
average marginal cost. To do this, a cost
function must be elicited that conveys what the
designer is willing to pay to achieve various
levels of performance. This is actually very
easy. Figure 4 displays what dollar amount a
designer might be willing to pay for various
rates of task completion. The average marginal
cost can be computed by using the cost function
as a weighting factor for the mass under the gold
impurity graph for the system. So, following the
previous example, if the designer wishes to
improve the system that is currently operating at
an average task completion rate of 70% to 80%,
then the average marginal cost for that gain is
simply:
?
=
??=
80
70
)()()(
t
tgtftcAMC
where f(t) is the task completion rate of the
system, g(t) is the task completion rate of the
gold standard, and c(t) is the cost function.
Average marginal cost is useful for
minimizing expenditure. For example, if the
goal is to improve task completion rate from
70% to 80%, and the designer must choose
between two systems, one with a particular
component and one without, the designer should
calculate the average marginal cost of both
systems as stated in the above equation and
select the cheaper system.
4 Discussion
Instead of focusing on developing new dialog
metrics that allow for comparative judgments
across different systems and domain tasks, we
proposed empirical methods that accomplish the
same purpose while taking advantage of dialog
metrics that already exist. In particular, we
outlined a protocol for conducting a WOZ
experiment to collect human performance data
that can be used as a gold standard. We then
described how to substantiate performance
claims using both a benchmark graph and a gold
impurity graph. Finally, we explained how to
optimize a dialog system using component
analysis and value optimization.
Without a doubt, the greatest drawback to the
empirical methods we propose is the tremendous
cost of running WOZ studies, both in terms of
time and money. In special cases, such as the
DARPA Communicator Project where
participants work within the same domain task, a
funding agency may wish to conduct the WOZ
studies on behalf of the participants. To defray
the cost of running the studies, the agency may
wish to determine its own cost function with
respect to a given performance metric and utilize
average marginal cost to decide which dialog
systems to continue sponsoring.
Because the focus of this paper has been on
how to apply the empirical methods,
hypothetical examples were considered. Work is
currently underway to collect data for evaluating
implemented dialog systems. We maintain that
without these empirical methods, readers of
reported dialog metrics cannot really make sense
of the numbers.
References
Bersen, N. O., Dybkjaer, H. & Dybkjaer, L.
(1998). Designing interactive speech systems: From
first ideas to user testing. Springer-Verlag.
Clark, H.H. (1996). Using language. Cambridge
University Press.
Clark, H.H. & Brennan, S.A. (1991). Grounding
in communication. In Perspectives on Socially
Shared Cognition, APA Books, pp.127-149.
Clark, H.H. & Schaefer, E.F. (1987).
Collaborating on contributions to conversations.
Language and Cognitive Processes, 2/1, pp.19-41.
Clark, H.H. & Schaefer, E.F. (1989).
Contributing to discourse. Cognitive Science, 13,
pp.259-294.
Danieli, M. & Gerbino, E. (1995). Metrics for
evaluating dialogue strategies in a spoken language
system. In Proc. of AAAI Spring Symposium on
Empirical Methods in Discourse Interpretation and
Generation, pp. 34-39.
Eckert, W., Levin, E. & Pieraccini, R. (1998).
Automatic evaluation of spoken dialogue systems. In
TWLT13: Formal semantics and pragmatics of
dialogue, pp. 99-110.
Gibbon, D., Moore, R. & Winski, R. (Eds.)
(1998). Handbook of standards and resources for
spoken language systems. Spoken Language System
Assessment, 3, Walter de Gruyter, Berlin.
Glass, J., Polifroni, J., Seneff, S. & Zue, V.
(2000). Data collection and performance evaluation
of spoken dialogue systems: The MIT experience. In
Proc. of ICSLP.
Horvitz, E. & Paek, T. (1999). A computational
architecture for conversation. In Proc. of 7th
International Conference on User Modeling, Springer
Wien, pp. 201-210.
Kamm, C., Walker, M.A. & Litman, D. (1999).
Evaluating spoken language systems. In Proc. of
AVIOS.
Lamel, L., Rosset S. & Gauvain, J.L. (2000).
Considerations in the design and evaluation of
spoken language dialog systems. In Proc. of ICSLP.
Litman, D. & Pan, S. (1999). Empirically
evaluating an adaptable spoken dialogue system. In
Proc. of 7th International Conference on User
Modeling, Springer Wien, pp. 55-64.
Paek, T. & Horvitz, E. (2000). Conversation as
action under uncertainty. In Proc. of 16th UAI,
Morgan Kaufmann, pp. 455-464.
Paek, T. & Horvitz, E. (1999). Uncertainty,
utility, and misunderstanding. In Proc. of AAAI Fall
Symposium on Psychological Models of
Communication, pp. 85-92.
Rudnicky, A. (2000). Understanding system
performance in dialog systems. MSR Invited Talk.
Walker, M.A., Litman, D., Kamm, C. & Abella,
A. (1997). PARADISE: A framework for evaluating
spoken dialogue agents. In Proc. of 35th ACL.
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 40?47,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
40
41
42
43
44
45
46
47
Proceedings of SPEECHGRAM 2007, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Handling Out-of-Grammar Commands in Mobile Speech Interaction  
Using Backoff Filler Models 
Tim Paek1, Sudeep Gandhe2, David Maxwell Chickering1, Yun Cheng Ju1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 USA 
2
 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292, USA 
{timpaek|dmax|yuncj}@microsoft.com, gandhe@usc.edu 
 
 
Abstract 
In command and control (C&C) speech in-
teraction, users interact by speaking com-
mands or asking questions typically speci-
fied in a context-free grammar (CFG). Un-
fortunately, users often produce out-of-
grammar (OOG) commands, which can re-
sult in misunderstanding or non-
understanding.  We explore a simple ap-
proach to handling OOG commands that 
involves generating a backoff grammar 
from any CFG using filler models, and util-
izing that grammar for recognition when-
ever the CFG fails.  Working within the 
memory footprint requirements of a mobile 
C&C product, applying the approach 
yielded a 35% relative reduction in seman-
tic error rate for OOG commands.  It also 
improved partial recognitions for enabling 
clarification dialogue. 
1 Introduction 
In command and control (C&C) speech interaction, 
users interact with a system by speaking com-
mands or asking questions.  By defining a rigid 
syntax of possible phrases, C&C reduces the com-
plexity of having to recognize unconstrained natu-
ral language.  As such, it generally affords higher 
recognition accuracy, though at the cost of requir-
ing users to learn the syntax of the interaction 
(Rosenfeld et al, 2001).  To lessen the burden on 
users, C&C grammars are authored in an iterative 
fashion so as to broaden the coverage of likely ex-
pressions for commands, while remaining rela-
tively simple for faster performance.  Nevertheless, 
users can, and often still do, produce OOG com-
mands.  They may neglect to read the instructions, 
or forget the valid expressions.  They may mistak-
enly believe that recognition is more robust than it 
really is, or take too long to articulate the right 
words.  Whatever the reason, OOG commands can 
engender misunderstanding (i.e., recognition of the 
wrong command) or non-understanding (i.e., no 
recognition), and aggravate users who otherwise 
might not realize that their commands were OOG.  
In this paper, we explore a simple approach to 
handling OOG commands, designed specifically to 
meet the memory footprint requirements of a C&C 
product for mobile devices.  This paper is divided 
into three sections.  First, we provide background 
on the C&C product and discuss the different types 
of OOG commands that occur with personal mo-
bile devices. Second, we explain the details of the 
approach and how we applied it to the product do-
main. Finally, we evaluate the approach on data 
collected from real users, and discuss possible 
drawbacks. 
2 Mobile C&C 
With the introduction of voice dialing on mobile 
devices, C&C speech interaction hit the wider 
consumer market, albeit with rudimentary pattern 
recognition. Although C&C has been 
commonplace in telephony and accessibility for 
many years, only recently have mobile devices 
have the memory and processing capacity to 
support not only automatic speech recognition 
(ASR), but a whole range of multimedia 
functionalities that can be controlled with speech.  
Leveraging this newfound computational capacity 
is Voice Command, a C&C application for high-
end mobile devices that allows users to look up 
contacts, place phone calls, retrieve appointments, 
obtain device status information, control 
multimedia and launch applications.  It uses an 
embedded, speaker-independent recognizer and 
operates on 16 bit, 16 kHz, Mono audio. 
33
OOG commands pose a serious threat to the us-
ability of Voice Command. Many mobile users ex-
pect the product to ?just work? without having to 
read the manual.  So, if they should say ?Dial Bob?, 
when the proper syntax for making a phone call is 
Call {Name}, the utterance will likely be mis-
recognized or dropped as a false recognition.  If 
this happens enough, users may abandon the prod-
uct, concluding that it or ASR in general, does not 
work. 
2.1 OOG frequency 
Given that C&C speech interaction is typically 
geared towards a relatively small number of words 
per utterance, an important question is, how often 
do OOG commands really occur in C&C?  In Pro-
ject54 (Kun & Turner, 2005), a C&C application 
for retrieving police information in patrol cars, 
voice commands failed on average 15% of the time, 
roughly 63% of which were due to human error.  
Of that amount, roughly 54% were from extrane-
ous words not found in the grammar, 12% from 
segmentation errors, and the rest from speaking 
commands that were not active. 
To examine whether OOG commands might be 
as frequent on personal mobile devices, we col-
lected over 9700 commands of roughly 1 to 3 sec-
onds each from 204 real users of Voice Command, 
which were recorded as sound (.wav) files.  We 
also logged all device data such as contact entries 
and media items.  All sound files were transcribed 
by a paid professional transcription service.  We 
ignored all transcriptions that did not have an asso-
ciated command; the majority of such cases came 
from accidental pressing of the push-to-talk button.  
Furthermore, we focused on user-initiated com-
mands, during which time the active grammar had 
the highest perplexity, instead of yes-no responses 
and clarification dialogue.  This left 5061 tran-
scribed utterances. 
2.2 Emulation method 
With the data transcribed, we first needed a me-
thod to distinguish between In-Grammar (ING) 
and OOG utterances.  We developed a simulation 
environment built around a desktop version of the 
embedded recognizer which could load the same 
Voice Command grammars and update them with 
user device data, such as contact entries, for each 
sound file.  It is important to note the desktop ver-
sion was not the engine that is commercially 
shipped and optimized for particular devices, but 
rather one that serves testing and research purposes. 
The environment could not only recognize sound 
files, but also parse string input using the dynami-
cally updated grammars as if that were the recog-
nized result.  We utilized the latter to emulate rec-
ognition of all transcribed utterances for Voice 
Command.  If the parse succeeded, we labeled the 
utterance ING, otherwise it was labeled OOG. 
Overall, we found that slightly more than one 
out of every four (1361 or 26.9%) transcribed ut-
terances were OOG.  We provide a complete 
breakdown of OOG types, including extraneous 
words and segmentation errors similar to Project54, 
in the next section.  It is important to keep in mind 
that being OOG by emulation does not necessarily 
entail that the recognizer will fail on the actual 
sound file.  For example, if a user states ?Call Bob 
at mobile phone? when the word ?phone? is OOG, 
the recognizer will still perform well.  The OOG 
percentage for Voice Command may also reflect 
the high perplexity of the name-dialing task.  Users 
had anywhere from 5 to over 2000 contacts, each 
of which could be expressed in multiple ways (e.g., 
first name, first name + last name, prefix + last 
name, etc.).  In summary, our empirical analysis of 
the data suggests that OOG utterances for mobile 
C&C on personal devices can indeed occur on a 
frequent basis, and as such, are worth handling. 
2.3 OOG type 
In order to explore how we might handle different 
types of OOG commands, we classified them ac-
cording to functional anatomy and basic edit op-
erations.  With respect to the former, a C&C utter-
ance consists of three functional components: 
 
1. Slot: A dynamically adjustable list repre-
senting a semantic argument, such as {Con-
tact} or {Date}, where the value of the ar-
gument is typically one of the list members. 
2. Keyword: A word or phrase that uniquely 
identifies a semantic predicate, such as Call 
or Battery, where the predicate corresponds 
in a one-to-one mapping to a type of com-
mand.  
3. Carrier Text: A word or phrase that is de-
signed to facilitate naturalistic expression of 
commands and carries no attached semantic 
content, such as ?What is? or ?Tell me?. 
 
34
For example, in the command ?Call Bob at mo-
bile?, the word ?Call? is the keyword, ?Bob? and 
?mobile? are slots, and ?at? is a carrier word.  
If we were to convert an ING command to 
match an OOG command, we could perform a se-
ries of edit operations: substitution, deletion, and 
insertion.  For classifying OOG commands, substi-
tution implies the use of an unexpected word, dele-
tion implies the absence of an expected word, and 
insertion implies the addition of a superfluous 
word. 
Starting with both functional anatomy and edit 
operations for classification, Table 1 displays the 
different types of OOG commands we labeled 
along with their relative frequencies.  Because 
more than one label might apply to an utterance, 
we first looked to the slot for an OOG type label, 
then keyword, then everything else.  
The most frequent OOG type, at about 60%, was 
OOG Slot, which referred to slot values that did 
not exist in the grammar.  The majority of these 
cases came from two sources: 1) contact entries 
that users thought existed but did not ? sometimes 
they did exist, but not in any normalized form (e.g., 
?Rich? for ?Richard?), and 2) mislabeling of most-
ly foreign names by transcribers.  Although we 
tried to correct as many names as we could, given 
the large contact lists that many users had, this 
proved to be quite challenging.  
The second most frequent OOG type was Inser-
tion at about 14%.  The majority of these insertions 
were single words.  Note that similar to Project54, 
segmentation errors occurred quite often at about 
9%, when the different segmentation types are 
added together. 
3 Backoff Approach 
Having identified the different types of OOG 
commands, we needed to devise an approach for 
handling them that satisfied the requirements of 
Voice Command for supporting C&C on mobile 
devices.  
3.1 Mobile requirements 
For memory footprint, the Voice Command team 
specified that our approach should operate with 
less than 100KB of ROM and 1MB of RAM.  Fur-
thermore, the approach could not require changes 
OOGType % Total Description Examples 
Insertion 14.2% adding a non-keyword, non-slot word call britney porter on mobile phone [?phone? is 
superfluous] 
Deletion 3.1% deleting a non-keyword, non-slot 
word my next appointments [?what are? missing] 
Substitution 2.5% replacing a non-keyword, non-slot 
word 
where is my next appointment  
[?where? is not supported] 
Segmentation 8.2% incomplete utterance show, call, start 
Keyword 
Substitution 4.6% replacing a keyword 
call 8 8 2 8 0 8 0 [?dial? is keyword] ,  
dial john horton [?call? is keyword] 
Keyword 
Segmentation 0.1% incomplete keyword what are my appoint 
Keyword  
Deletion 2.2% deleting the keyword marsha porter at home [?call? missing] 
Slot  
Substitution 0.4% replacing slot words 
call executive 5 on desk  
[?desk? is not slot value] 
Slot  
Segmentation 0.9% incomplete slot call alexander woods on mob 
Slot Deletion 1.0% deleted slot call tracy morey at 
Disfluencies 1.8% disfluencies - mostly repetitions start expense start microsoft excel 
Order  
Rearrangement 0.6% 
changing the order of words within a 
keyword 
what meeting is next [Should be ?what is my 
next meeting?] 
Noise 0.7% non primary speaker oregon state home coming call brandon jones 
on mobile phone 
OOG Slot 59.8% The slot associated with this utterance is out of domain 
Show Rich Lowry [?Richard? is contact entry] , 
dial 0 2 1 6 [Needs > 7 digits] 
 
Table 1. Different OOG command types and their relative frequencies for the Voice Command product. The brack-
eted text in the ?Examples? column explicates the cause of the error 
35
to the existing embedded Speech API (SAPI).  
Because the team also wanted to extend the func-
tionality of Voice Command to new domains, we 
could not assume that we would have any data for 
training models.  Although statistical language 
models (SLM) offer greater robustness to varia-
tions in phrasing than fixed grammars (Rosenfeld, 
2000), the above requirements essentially prohib-
ited them.  So, we instead focused on extending the 
use of the base grammar, which for Voice Com-
mand was a context-free grammar (CFG): a formal 
specification of rules allowing for embedded recur-
sion that defines the set of possible phrases (Man-
ning & Sch?tze, 1999).  
Despite the manual effort that CFGs often re-
quire, they are widely prevalent in industry (Knight 
et al, 2001) for several reasons. First, they are easy 
for designers to understand and author.  Second, 
they are easy to modify; new phrases can be added 
and immediately recognized with little effort.  And 
third, they produce transparent semantics without 
requiring a separate natural language understand-
ing component; semantic properties can be at-
tached to CFG rules and assigned during recogni-
tion.  By focusing on CFGs, our approach allows 
industry designers who are more accustomed to 
fixed grammars to continue using their skill set, 
while hopefully improving the handling of utter-
ances that fall outside of their grammar. 
3.2 Leveraging a backoff grammar 
As long as utterances remain ING, a CFG affords 
fast and accurate recognition, especially because 
engines are often tuned to optimize C&C recogni-
tion.  For example, in comparing recognition per-
formance in a statistical and a CFG-based recog-
nizer for the same domain, Knight et al (2001) 
found that the CFG outperformed the SLM.  In 
order to exploit the optimization of the engine for 
C&C utterances that are ING, we decided to utilize 
a two-pass approach where each command is ini-
tially submitted to the base CFG.  If the confidence 
score of the top recognition C1 falls below a rejec-
tion threshold RCFG, or if the recognizer declares a 
false recognition (based on internal engine fea-
tures), then the audio stream is passed to a backoff 
grammar which then attempts to recognize the 
command.  If the backoff grammar fails to recog-
nize the command, or the top recognition falls 
again below a rejection threshold RBG, then users 
experience the same outcome as they normally 
would otherwise, except with a longer delay.  Fig-
ure 1(a) summarizes the approach. 
In order to generate the backoff grammar and 
still stay within the required memory bounds of 
Voice Command, we explored the use of the built-
in filler or garbage model, which is a context-
independent, acoustic phone loop.  Expressed in 
the syntax as ?...?, filler models capture phones in 
whatever context they are placed.  The functional 
anatomy of a C&C utterance, as explained in Sec-
tion 2.3, sheds light on where to place them: before 
and/or after keywords and/or slots.  As shown Fig-
ure 1(b), to construct a backoff grammar from a 
CFG during design time, we simply parse each 
CFG rule for keywords and slots, remove all car-
rier phrases, and insert filler models before and/or 
after the keywords and/or slots.  Although it is 
straightforward to automatically identify keywords 
(words that uniquely map to a CFG rule) and slots 
(lists with semantic properties), developers may 
want to edit the generated backoff grammar for any 
keywords and slots they wish to exclude; for ex-
ample, in cases where more than one keyword is 
found for a CFG rule. 
For both slots and keywords, we could employ 
any number of different patterns for placing the 
filler models, if any.  Table 2 displays some of the 
patterns in SAPI 5 format, which is an XML for-
mat where question marks indicate optional use.  
Although the Table is for keywords, the same pat-
terns apply for slots.  As shown in k4, even the 
functional constituent itself can be optional.  Fur-
thermore, alternate lists of patterns can be com-
posed, as in kn.  Depending on the number and type 
 
 
Figure 1. (a) A two-pass approach which leverages a 
base CFG for ING recognition and a backoff grammar 
for failed utterances. (b) Design time procedure for 
generating a backoff grammar 
36
of functional constituents for a CFG rule, backoff 
rules can be constructed by adjoining patterns for 
each constituent.  We address the situation when a 
backoff rule corresponds to multiple CFG rules in 
Section 3.4. 
3.3 Domain feasibility 
Because every C&C utterance can be characterized 
by its functional constituents, the backoff filler ap-
proach generically applies to C&C domains, re-
gardless of the actual keywords and slots.  But the 
question remains, is this generic approach feasible 
for handling the different OOG types for Voice 
Command discussed in Section 2.3? 
The filler model is clearly suited for Insertions, 
which are the second most frequent OOG type, 
because it would capture the additional phones.  
However, the most frequent OOG type, OOG Slot, 
cannot be handled by the backoff approach.  That 
requires the developer to write better code for 
proper name normalization (e.g, ?Rich? from ?Ri-
chard?) as well as breaking down the slot value 
into further components for better partial matching 
of names.  Because new C&C domains may not 
utilize name slots, we decided to treat improving 
name recognition as separate research.  Fortu-
nately, opportunity for applying the backoff filler 
approach to OOG Slot types still exists. 
3.4 Clarification of partial recognitions 
As researchers have observed, OOG words con-
tribute to increased word-error rates (Bazzi & 
Glass, 2000) and degrade the recognition perform-
ance of surrounding ING words (Gorrell, 2003).  
Hence, even if a keyword surrounding an OOG 
slot is recognized, its confidence score and the 
overall phrase confidence score will often be de-
graded.  This is in some ways an unfortunate by-
product of confidence annotation, which might be 
circumvented if SAPI exposed word lattice prob-
abilities.  Because SAPI does not, we can instead 
generate partial backoff rules that comprise only a 
subset of the functional constituents of a CFG rule.  
For example, if a CFG rule contains both a key-
word and slot, then we can generate a partial back-
off rule with just one or the other surrounded by 
filler models.  Using partial backoff rules prevents 
degradation of confidence scores for ING constitu-
ents and improves partial recognitions, as we show 
in Section 4.  Partial backoff rules not only handle 
OOG Slot commands where, for example, the 
name slot is not recognized, but also many types of 
segmentation, deletion and substitution commands 
as well. 
Following prior research (Gorrell et al, 2002; 
Hockey et al, 2003), we sought to improve partial 
recognitions so that the system could provide feed-
back to users on what was recognized, and to en-
courage them to stay within the C&C syntax.  Cla-
rification dialogue with implicit instruction of the 
syntax might proceed as follows: If a partial recog-
nition only corresponded to one CFG rule, then the 
system could assume the semantics of that rule and 
remind the user of the proper syntax.  On the other 
hand, if a partial recognition corresponded to more 
than one rule, then a disambiguation dialogue 
could relate the proper syntax for the choices.  For 
example, suppose a user says ?Telephone Bob?, 
using the OOG word ?Telephone?.  Although the 
original CFG would most likely misrecognize or 
even drop this command, our approach would ob-
tain a partial recognition with higher confidence 
score for the contact slot.  If only one CFG rule 
contained the slot, then the system could engage in 
the confirmation, ?Did you mean to say, call 
Bob?? On the other hand, if more than one CFG 
rule contained the slot, then the system could en-
gage in a disambiguation dialogue, such as ?I 
heard 'Bob'. You can either call or show Bob?.  
Either way, the user is exposed to and implicitly 
taught the proper C&C syntax. 
3.5 Related research 
In related research, several researchers have inves-
tigated using both a CFG and a domain-trained 
SLM simultaneously for recognition (Gorrell et al, 
2002; Hockey et al, 2003).  To finesse the per-
formance of a CFG, Gorrell (2003) advocated a 
two-pass approach where an SLM trained on CFG 
Scheme Keyword Pattern 
k1 <keyword/> 
k2 (?)?  <keyword> 
k3 (?)?  <keyword/>  (?)? 
k4 (?)?  <keyword/>? (?)? 
kn 
<list> 
(?)?  <keyword/>? (?)? 
(?) 
</list> 
 
Table 2. Possible patterns in SAPI 5 XML format for 
placing the filler model, which appears as 
?...?.Question marks indicate optional use. 
37
data (and slightly augmented) is utilized as a back-
off grammar.  However, only the performance of 
the SLM on a binary OOG classification task was 
evaluated and not the two-pass approach itself.  In 
designing a multimodal language acquisition sys-
tem, Dusan & Flanagan (2002) developed a two-
pass approach where they utilized a dictation n-
gram as a backoff grammar and added words rec-
ognized in the second pass into the base CFG.  Un-
fortunately, they only evaluated the general usabil-
ity of their architecture. 
Because of the requirements outlined in Section 
3.1, we have focused our efforts on generating a 
backoff grammar from the original CFG, taking 
advantage of functional anatomy and filler models.  
The approach is agnostic about what the actual fil-
ler model is, and as such, the built-in phone loop 
can easily be replaced by word-level (e.g., Yu et 
al., 2006) and sub-word level filler models (e.g., 
Liu et al, 2005).  In fact, we did explore the word-
level filler model, though so far we have not been 
able to meet the footprint requirements.  We are 
currently investigating phone-based filler models. 
Outside of recognition with a CFG, researchers 
have pursued methods that directly model OOG 
words as sub-word units in the recognition search 
space of a finite state transducer (FST) (Bazzi & 
Glass, 2000).  OOG words can also be dynamically 
incorporated into the FST (Chung et al, 2004).  
Because this line of research depends on entirely 
different engine architecture, we could not apply 
the techniques. 
4 Evaluation 
In C&C speech interaction, what matters most is 
not word-error rate, but semantic accuracy and task 
completion.  Because task completion is difficult to 
evaluate without collecting new data, we evaluated 
the semantic accuracy of the two-pass approach 
against the baseline of using just the CFG on the 
data we collected from real users, as discussed in 
Section 2.1.  Furthermore, because partial 
recognitions can ultimately result in a successful 
dialogue, we carried out separate evaluations for 
the functional constituents of a command (i.e., 
keyword and slot) as well as the complete 
command (keyword + slot).  For Voice Command, 
no command contained more than one slot, and 
because the vast majority of single slot commands 
were commands to either call or show a contact 
entry, we focused on those two commands as a 
proof of concept. 
For any utterance, the recognizer can either ac-
cept or reject it.  If it is accepted, then the seman-
tics of the utterance can either be correct (i.e., it 
matches what the user intended) or incorrect.  The 
following metrics can now be defined: 
 
precision = CA / (CA + IA)   (1) 
recall = CA / (CA + R)    (2) 
accuracy = CA / (CA + IA + R)   (3) 
 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejected 
commands.  Although R could be decomposed into 
correct and incorrect rejections, for C&C, 
recognition failure is essentially perceived the 
same way by users: that is, as a non-understanding. 
4.1 Results 
For every C&C command in Voice Command, the 
embedded recognizer returns either a false 
recognition (based on internal engine parameters) 
or a recognition event with a confidence score.  As 
described in Section 3.2, if the confidence score 
falls below a rejection threshold RCFG, then the 
audio stream is processed by the backoff grammar 
which also enforces its own threshold RBG.  The 
RCFG for Voice Command was set to 45% by a 
proprietary tuning procedure for optimizing 
acoustic word-error rate.  For utterances that 
exceeded RCFG, 84.2% of them were ING and 
15.8% OOG.  For utterances below RCFG, 48.5% 
 
 
Figure 2. The semantic accuracies comparing the 
baseline CFG against both the BG (backoff grammar 
alone) and the two-pass approach (CFG + Backoff) 
separated into functional constituent groups and fur-
ther separated by ING and OOG commands. 
38
were ING and 51.5% OOG.  Because a 
considerable number of utterances may be ING in 
the second pass, as it was in our case, RBG requires 
tuning as well.  Instead of using a development 
dataset to tune RBG, we decided to evaluate our 
approach on the entire data with RBG set to the 
same proprietary threshold as RCFG.  In post-hoc 
analyses, this policy of setting the two thresholds 
equal and reverting to the CFG recognition if the 
backoff confidence score falls below RBG achieved 
results comparable to optimizing the thresholds. 
Figure 2 displays semantic accuracies separated 
by ING and OOG commands.  Keyword evalua-
tions comprised 3700 ING and 1361 OOG com-
mands.  Slot and keyword + slot evaluations com-
prised 2111 ING and 138 OOG commands.  Over-
all, the two-pass approach was significantly higher 
in semantic accuracy than the baseline CFG, using 
McNemar's test (p<0.001).  Not surprisingly, the 
largest gains were with OOG commands.  Notice 
that for partial recognitions (i.e., keyword or slot 
only), the approach was able to improve accuracy, 
which with further clarification dialogue, could 
result in task completions.  Interestingly, the ap-
proach performed the same for keyword + slot as it 
did for slot, which suggests that getting the slot 
correct is crucial to recognizing surrounding key-
words.  Despite the high percentage of OOG Slots, 
slot accuracy still increased due to better handling 
of other OOG types such as deletions, insertions 
and substitutions.   
Finally, as a comparison, for the keyword + slot 
task, an upper bound of 74.3% ? 1.1% (10-fold 
cross-validated standard error) overall semantic 
accuracy was achieved using a small footprint sta-
tistical language modeling technique that re-ranked 
CFG results (Paek & Chickering, 2007), though 
the comparison is not completely fair given that the 
technique was focused on predictive language 
modeling and not on explicitly handling OOG ut-
terances.  Also note that in all cases, the backoff 
grammar alone performed worse than either the 
CFG or the two-pass approach.   
Table 3 provides a more detailed view of the re-
sults for the just OOG commands as well as the 
relative reductions in semantic error rate (RER).  
Notice that the approach increases recall, which 
signifies less non-understandings. However, this 
comes at the price of a small increase in misunder-
standings, as seen in the decrease in precision.  
Overall, the best reduction in semantic error rate 
achieved by the approach was about 35%. 
Decomposing RER by OOG types, we found 
that for keyword evaluations, the biggest im-
provement (52% RER), came about for Deletion 
types, or commands with missing carrier words.  
This makes sense because the backoff grammar 
only cares about the keyword.  For slot and key-
word + slot evaluations, Insertion types maintained 
the biggest improvement at 38% RER. 
Note that the results presented are those ob-
tained without tuning.  If application developers 
wanted to find an optimal operating point, they 
would need to decide what is more important for 
their application: precision or recall, and adjust the 
thresholds until they reach acceptable levels of per-
formance.  Ideally, these levels should accord with 
what real users of the application would accept. 
4.2 Efficiency 
Given that the approach was aimed at satisfying 
the mobile requirements stated in Section 3.1, 
which it did, we also compared the processing time 
it takes to arrive at a recognition or false 
recognition between the CFG alone and the two-
pass approach.  Because of the filler models, the 
backoff grammar is a more relaxed version of CFG 
with a larger search space, and as such, takes 
slightly more processing time. The average 
processing time for the CFG in our simulation 
environment was about 395 milliseconds, whereas 
the average processing time for the two passes was 
about 986 milliseconds.  Hence, when the backoff 
grammar is used, the total computation time is 
approximately 2.5 times that of a single pass alone.  
In our experiments, a total of 1570 commands (i.e. 
31%) required the two passes, while 3491 of them 
were accepted after a single CFG pass. 
 CFG 2-PASS RER 
Prec 85.0% 79.0% -39.7% 
Recall 36.8% 58.6% 34.5% Keyword 
Acc 34.5% 50.7% 24.7% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Slot 
Acc 54.4% 70.3% 34.9% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Keyword 
+ Slot 
Acc 54.4% 70.3% 34.9% 
 
Table 3. Relative reductions in semantic error rate, or 
Relative Error Reduction (RER) for OOG commands 
grouped by keyword, slot and keyword + slot evalua-
tions. ?2-PASS? denotes the two-pass approach. 
39
4.3 Drawbacks 
In exploring the backoff filler approach, we 
encountered a few drawbacks that are worth 
considering when applying this approach to other 
domains.  The first issue dealt with false positives.  
In the data collection for Voice Command, a total 
of 288 utterances contained no discernable speech.  
If these were included in the data set, they would 
amount to about 5% of all utterances.  As 
mentioned previously, these were mostly cases 
when the push-to-talk button was accidentally 
triggered.  When we evaluated the approach on 
these utterances, we found that the CFG accepted 
36 or roughly 13% of them, while the proposed 
approach accepted 115 or roughly 40% of them.  
For our domain, this problem can be avoided by 
instructing users to lock their devices when not in 
use to prevent spurious initiations.  For other C&C 
domains where unintentional command initiations 
occur frequently, this may be a serious concern, 
though we suspect that users will be more 
forgiving of accidental errors than real errors. 
Another drawback dealt with generating the 
backoff grammar.  As we discussed in Section 3.2, 
various patterns for placing filler models can be 
utilized.  Although we did explore the possibility 
that perhaps certain patterns might generalize 
across domains, we found that it was better to 
hand-craft patterns to the application.  For Voice 
Command, we used the kn pattern specified in Ta-
ble 2 for keywords, and the identical sn pattern for 
slots because they proved to be best suited to the 
product grammars in pre-trial experiments. 
5 Conclusion & Future Direction 
In this paper, we classified the different types of 
OOG commands that might occur in a mobile 
C&C application, and presented a simple two-pass 
approach for handling them that leverages the base 
CFG for ING recognition and a backoff grammar 
OOG recognition.  The backoff grammar is gener-
ated from the original CFG by surrounding key-
words and/or slots with filler models.  Operating 
within the memory footprint requirements of a 
mobile C&C product, the approach yielded a 35% 
relative reduction in semantic error rate for OOG 
commands, and improved partial recognitions, 
which can facilitate clarification dialogue. 
We are now exploring small footprint, phone-
based filler models.  Another avenue for future 
research is to further investigate optimal policies 
for deciding when to pass to the backoff grammar 
and when to use the backoff grammar recognition. 
References 
I. Bazzi & J. Glass. 2000. Modeling out-of-vocabulary 
words for robust speech recognition. In Proc. ICSLP. 
G. Chung, S. Seneff, C.Wang, & I. Hetherington. 2004. 
A dynamic vocabulary spoken dialogue interface. In 
Proc. ICSLP. 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based 
upon multimodal language acquisition. In Proc. IC-
MI. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intel-
ligent help to mixed initiative spoken dialogue sys-
tems. In Proc. ICSLP. 
G. Gorrell. 2003. Using statistical language modeling to 
identify new vocabulary in a grammar-based speech 
recognition system. In Proc. Eurospeech. 
B. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist, J. 
Hieronymus, A. Gruenstein, & J. Dowding. 2003. 
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance. In 
Proc. EACL, pp. 147?154. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, & I. Lewin. 2001. Comparing grammar-based 
and robust approaches to speech understanding: A 
case study. In Proc. Eurospeech. 
A. Kun & L. Turner. 2005. Evaluating the project54 
speech user interface. In Proc. Pervasive. 
P. Liu, Y. Tian, J. Zhou, & F. Soong. 2005. Background 
model based posterior probability for measuring 
confidence. In Proc. Interspeech. 
C.D. Manning & H. Sch?utze. 1999. Foundations of 
Statistical Natural Language Processing. MIT Press, 
Cambridge,Massachusetts. 
Paek, T. & Chickering, D. 2007. Improving command 
and control speech recognition: Using predictive us-
er models for language modeling. UMUAI, 17(1):93-
117. 
Rosenfeld, R. 2000. Two decades of statistical language 
modeling: Where do we go from here? In Proc. of the 
IEEE, 88(8): 1270?1278. 
R. Rosenfeld, D. Olsen, & A. Rudnicky. 2001. Univer-
sal speech interfaces. Interactions, 8(6):34?44. 
D. Yu, Y.C. Ju, Y. Wang, & A. Acero. 2006. N-gram 
based filler model for robust grammar authoring. In 
Proc. ICASSP. 
40
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64?67,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Rapidly Deploying Grammar-Based Speech Applications 
with Active Learning and Back-off Grammars 
Tim Paek1, Sudeep Gandhe2, David Maxwel Chickering1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 
2 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292 
{timpaek|dmax}@microsoft.com, gandhe@usc.edu 
                                                          
2 Second author was partly sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opi-
nions expressed do not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred. 
Abstract 
Grammar-based approaches to spoken lan-
guage understanding are utilized to a great ex-
tent in industry, particularly when developers 
are confronted with data sparsity. In order to 
ensure wide grammar coverage, developers 
typically modify their grammars in an itera-
tive process of deploying the application, col-
lecting and transcribing user utterances, and 
adjusting the grammar. In this paper, we ex-
plore enhancing this iterative process by leve-
raging active learning with back-off 
grammars. Because the back-off grammars 
expand coverage of user utterances, develop-
ers have a safety net for deploying applica-
tions earlier. Furthermore, the statistics related 
to the back-off can be used for active learning, 
thus reducing the effort and cost of data tran-
scription. In experiments conducted on a 
commercially deployed application, the ap-
proach achieved levels of semantic accuracy 
comparable to transcribing all failed utter-
ances with 87% less transcriptions. 
1 Introduction 
Although research in spoken language understand-
ing is typically pursued from a statistical perspec-
tive, grammar-based approaches are utilized to a 
great extent in industry (Knight et al, 2001). 
Speech recognition grammars are often manually 
authored and iteratively modified as follows: Typi-
cally, context-free grammars (CFG) are written in 
a format such as Speech Recognition Grammar 
Specification (SRGS) (W3C, 2004) and deployed. 
Once user utterances are collected and transcribed, 
the grammars are then adjusted to improve their 
coverage. This process continues until minimal 
OOG utterances are observed. In this paper, we 
explore enhancing this iterative process of gram-
mar modification by combining back-off gram-
mars, which expand coverage of user utterances, 
with active learning, which reduces ?the number of 
training examples to be labeled by automatically 
processing unlabeled examples, and then selecting 
the most informative ones with respect to a speci-
fied cost function for a human to label? (Hakkani-
Tur et al, 2002). This paper comprises three sec-
tions. In Section 2, we describe our overall ap-
proach to rapid application development (RAD). In 
Section 3, we explain how data transcription can 
be reduced by leveraging active learning based on 
statistics related to the usage of back-off gram-
mars. Finally, in Section 4, we evaluate the active 
learning approach with simulation experiments 
conducted on data collected from a commercial 
grammar-based speech application. 
2 RAD Approach & Related Work 
Working under the assumption that developers in 
industry will continue to use CFGs for rapid appli-
cation development, our approach to grammar 
modification is as follows: 
1. Create a CFG (either manually or automatically). 
1.1 Generate a back-off grammar from the CFG. 
2. Deploy the application. 
2.1 Use the back-off grammar for OOG utterances. 
3. Gather data from users. 
4. Selectively transcribe data by using statistics re-
lated to the back-off for active learning; i.e., transcribe 
only those utterances that satisfy the active learning 
criterion. 
5. Modify CFG either manually or automatically and 
go to step 1.1. 
To begin with, developers start with a CFG in Step 
1. If they had access to a grammatical platform 
64
such as Regulus (Rayner et al, 2006), they could 
in principle construct a CFG automatically for any 
new domain, though most developers will probably 
manually author the grammar. Two steps are added 
to the typical iterative process. In Step 1.1, we 
generate a back-off grammar from the CFG. One 
way to accomplish this is by constructing a back-
off CFG using filler models (Paek et al, 2007), 
which when applied to the same command-and-
control task in Section 4 can result in a 35% rela-
tive reduction in semantic error rate for OOG ut-
terances. However, the back-off grammar could 
also be a SLM trained on artificial data created 
from the CFG (Galescu et al, 1998). Whatever 
back-off mechanism is employed, its coverage 
should be wider than the original CFG so that ut-
terances that fail to be recognized by the CFG, or 
fall below an acceptable confidence threshold, can 
be handled by the back-off in a second or simulta-
neous pass. That is the gist of Step 2.1, the second 
additional step. It is not only important to generate 
a back-off grammar, but it must be utilized for 
handling possible OOG utterances. 
Our approach attempts to reduce the usual cost 
associated with grammar modification after the 
application has been deployed and data collected in 
Step 4. The idea is simple: Exploit the fast and ac-
curate CFG recognition of in-grammar (ING) ut-
terances by making OOG utterances handled by 
the back-off grammar ING. In other words, expand 
CFG coverage to include whatever gets handled by 
the back-off grammar. This idea is very comple-
mentary with a two-pass recognition approach 
where the goal is to get utterances correctly recog-
nized by a CFG on the first pass so as to minimize 
computational expenses (Paek et al, 2007).  
All of this can be accomplished with reduced 
transcription effort by keeping track of and leve-
raging back-off statistics for active learning. If the 
back-off is a CFG, we keep track of statistics re-
lated to which CFG rules were utilized the most, 
whether they allowed the task to be successfully 
completed, etc. If the back-off is a SLM, we keep 
track of similar statistics related to the semantic 
alignment and mapping in spoken language under-
standing. Given an active learning criterion, these 
statistics can be used to selectively transcribe ut-
terances which can then be used to modify the 
CFG in Step 5 so that OOG utterances become 
ING. Section 3 covers this in more detail. 
Finally, in Step 5, the CFG grammar is mod-
ified using the selectively transcribed utterances. 
Although developers will probably want to do this 
manually, it is possible to automate much of this 
step by making grammar changes with minimal 
edit distance or Levenshtein distance. 
Leveraging a wider coverage back-off grammar 
is of course not new. For grammar-based applica-
tions, several researchers have investigated using a 
CFG along with a back-off grammar either simul-
taneously via a domain-trained SLM (Gorrell et 
a1., 2002), or in two-pass recognition using either 
an SLM trained on CFG data (Gorrell, 2003) or a 
dictation n-gram (Dusan & Flanagan, 2002). To 
our knowledge however, no prior research has con-
sidered leveraging statistics related to the back-off 
grammar for active learning, especially as part of a 
RAD approach. 
3 Active Learning 
Our overall approach utilizes back-off grammars to 
provide developers with a safety net for deploying 
applications earlier, and active learning to reduce 
transcription effort and cost. We now elaborate on 
active learning, demonstrate the concept with re-
spect to a CFG back-off. 
Active learning aims at reducing transcription 
of training examples by selecting utterances that 
are most likely to be informative according to a 
specified cost function (Hakkani-Tur et al, 2002). 
In the speech community, active learning has been 
successfully applied to reducing the transcription 
effort for ASR (Hakkani-Tur et al, 2002), SLU 
(Tur et al, 2003b), as well as finding labeling er-
rors (Tur et al, 2003). In our case, the examples 
are user utterances that need to be transcribed, and 
the learning involves modifying a CFG to achieve 
wider coverage of user expressions. Instead of pas-
sively transcribing everything and modifying the 
CFG as such, the grammar can ?actively? partici-
pate in which utterances are transcribed. 
The usual procedure for selecting utterances for 
grammar modification is to transcribe at least all 
failed utterances, such as those that fall below a 
rejection threshold. By leveraging a back-off 
grammar, developers have more information with 
which to select utterances for transcription. For a 
CFG back-off, how frequently a back-off rule fired 
can serve as an active learning criterion because 
that is where OOG utterances are handled. Given 
65
this active learning criterion, the algorithm would 
proceed as follows (where i denotes iteration, St 
denotes the set of transcribed utterances, and Su 
denotes the set of all utterances): 
[1] Modify CFGi using St and generate corresponding 
back-offi from the CFGi. 
[2] Recognize utterances in set Su using CFGi + back-
offi. 
[3] Compute statistics on what back-off rules fired 
when and how frequently. 
[4] Select the k utterances that were handled by the 
most frequently occurring back-off rule and tran-
scribe them. Call the new transcribed set as Si. 
[5] ;t t i u u iS S S S S S? ? ??  
[6] Stop when CFGi achieves a desired level of seman-
tic accuracy, or alternatively when back-off rules 
only handle a desired percentage of Su, otherwise 
go to Step 1. 
Note that the set Su grows with each iteration and 
follows as a result of deploying an application with 
a CFGi + back-offi. Step [1] corresponds to Step 5, 
1.1, and 2.1 of our approach. Steps [2-4] above 
constitute the active learning criterion and can be 
adjusted depending on what developers want to 
optimize. This algorithm currently assumes that 
runtime efficiency is the main objective (e.g., on a 
mobile device); hence, it is critical to move utter-
ances recognized in the second pass to the first 
pass. If developers are more interested in learning 
new semantics, in Step [4] above they could tran-
scribe utterances that failed in the back-off. With 
an active learning criterion in place, Step [6] pro-
vides a stopping criterion. This too can be adjusted, 
and may even target budgetary objectives. 
4 Evaluation 
For evaluation, we used utterances collected from 
204 users of Microsoft Voice Command, a gram-
mar-based command-and-control (C&C) applica-
tion for high-end mobile devices (see Paek et al, 
2007 for details). We partitioned 5061 transcribed 
utterances into five sets, one of which was used 
exclusively for testing. The remaining four were 
used for iterative CFG modification. For the first 
iteration, we started with a CFG which was a de-
graded version of the grammar currently shipped 
with the Voice Command product. It was obtained 
by using the mode, or the most frequent user utter-
ance, for each CFG rule. We compared two ap-
proaches: CFG_Full, where each iterative CFG 
was modified using the full set of transcribed utter-
ances that resulted in a failure state (i.e., when a 
false recognition event occurred or the phrase con-
fidence score fell below 45%, which was set by a 
proprietary tuning procedure for optimizing word-
error rate), and CFG_Active, where each iterative 
CFG was modified using only those transcribed 
utterances corresponding to the most frequently 
occurring CFG back-off rules. For both CFG_Full 
and CFG_Active, CFGi was modified using the 
same set of heuristics akin to minimal edit dis-
tance. In order to assess the value of using the 
back-off grammar as a safety net, we also com-
pared CFG_Full+Back-off, where a derived CFG 
back-off was utilized whenever a failure state oc-
curred with CFG_Full, and CFG_Active+Back-off, 
where again a CFG back-off was utilized, this time 
with the back-off derived from the CFG trained on 
selective utterances. 
As our metric, we evaluated semantic accuracy 
since that is what matters most in C&C settings. 
Furthermore, because recognition of part of an ut-
terance can increase the odds of ultimately achiev-
ing task completion (Paek et al, 2007), we carried 
out separate evaluations for the functional consti-
tuents of a C&C utterance (i.e., keyword and slot) 
as well as the complete phrase (keyword + slot). 
We computed accuracy as follows: For any single 
utterance, the recognizer can either accept or reject 
it. If it is accepted, then the semantics of the utter-
ance can either be correct (i.e., it matches what the 
user intended) or incorrect, hence: 
accuracy = CA / (CA + IA + R)   (1) 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejections. 
Table 2 displays semantic accuracies for both 
CFG_Full and CFG_Active. Standard errors about 
the mean were computed using the jacknife proce-
dure with 10 re-samples. Notice that both 
CFG_Full and CFG_Active initially have the same 
accuracy levels because they start off with the 
same degraded CFG. The highest accuracies ob-
tained almost always occurred in the second itera-
tion after modifying the CFG with the first batch of 
transcriptions. Thereafter, all accuracies seem to 
decrease. In order to understand why this would be 
case, we computed the coverage of the ith CFG on 
the holdout set. This is reported in the ?OOG%? 
column. Comparing CFG_Full to CFG_Active on 
66
keyword + slot accuracy, CFG_Full decreases in 
accuracy after the second iteration as does 
CFG_Active. However, the OOG% of CFG_Full is 
much lower than CFG_Active. In fact, it seems to 
level off after the second iteration, suggesting that 
perhaps the decrease in accuracies reflects the in-
crease in grammar perplexity; that is, as the gram-
mar covers more of the utterances, it has more 
hypotheses to consider, and as a result, performs 
slightly worse. Interestingly, after the last iteration, 
CFG_Active for keyword + slot and slot accuracies 
was slightly higher (69.06%) than CFG_Full 
(66.88%) (p = .05). Furthermore, this was done 
with 193 utterances as opposed to 1393, or 87% 
less transcriptions. For keyword accuracy, 
CFG_Active (64.09%) was slightly worse than 
CFG_Full (66.10%) (p < .05). 
With respect to the value of having a back-off 
grammar as a safety net, we found that both 
CFG_Full and CFG_Active achieved much higher 
accuracies with the back-off for keyword, slot, and 
keyword + slot accuracies. Notice also that the dif-
ferences between CFG_Full and CFG_Active after 
the last iteration were much closer to each other 
than without the back-off, suggesting applications 
should always be deployed with a back-off. 
5 Conclusion 
In this paper, we explored enhancing the usual 
iterative process of grammar modification by leve-
raging active learning with back-off grammars. 
Because the back-off grammars expand coverage 
of user utterances to handle OOG occurrences, de-
velopers have a safety net for deploying applica-
tions earlier. Furthermore, because statistics related 
to the back-off can be used for active learning, de-
velopers can reduce the effort and cost of data 
transcription. In our simulation experiments, leve-
raging active learning achieved levels of semantic 
accuracy comparable to transcribing all failed ut-
terances with 87% less transcriptions. 
References 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based upon multimod-
al language acquisition. In Proc. of ICMI. 
L. Galescu, E. Ringger, & J. Allen. 1998. Rapid language model de-
velopment for new task domains. In Proc. of LREC. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intelligent help to 
mixed initiative spoken dialogue systems. In Proc. of ICSLP. 
G.. Gorrell. 2003. Using statistical language modeling to identify new 
vocabulary in a grammar-based speech recognition system. In 
Proc. of Eurospeech. 
D. Hakkani-Tur, G. Riccardi & A. Gorin. 2002. Active learning for 
automatic speech recognition. In Proc. of ICASSP. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, & I. Le-
win. 2001. Comparing grammar-based and robust approaches to 
speech understanding: A case study. In Proc. of Eurospeech. 
T. Paek, S. Gandhe, D. Chickering & Y. Ju. 2007. Handling out-of-
grammar commands in mobile speech interaction using back-off 
filler models. In Proc. of ACL Workshop on Grammar-Based Ap-
proaches to Spoken Language Processing (SPEECHGRAM). 
M. Rayner, B.A. Hockey, & P. Bouillon. 2006. Putting Linguistics 
into Speech Recognition: The Regulus Grammar Compiler. CSLI. 
G. Tur, M. Rahim & D. Hakkani-Tur. 2003. Active labeling for spo-
ken language understanding. In Proc. of Eurospeech. 
G. Tur, R. Schapire, & D. Hakkani-Tur. 2003b. Active learning for 
spoken language understanding. In Proc. of ICASSP. 
W3C. 2004. Speech Recognition Grammar Specification Version 1.0. 
http://www.w3.org/TR/speech-grammar  
Approach i 
Utterances 
Transcribed 
Keyword  
Accuracy 
Slot  
Accuracy 
Keyword + Slot 
Accuracy 
Processing 
Time (ms) 
OOG% 
CFG_Full 
 
1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10% 
2 590 66.20% (0.12%) 71.02% (0.23%) 70.59% (0.23%) 401 (4.0586) 31.92% 
3 1000 65.80% (0.15%) 69.72% (0.19%) 69.06% (0.19%) 422 (4.5804) 31.30% 
4 1393 66.10% (0.13%) 67.54% (0.22%) 66.88% (0.21%) 433 (4.7061) 30.95% 
CFG_Full + 
Back-off 
1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10% 
2 590 73.32% (0.11%) 72.11% (0.22%) 71.68% (0.23%) 562 (10.4696) 31.92% 
3 1000 72.52% (0.12%) 72.11% (0.21%) 71.46% (0.22%) 584 (10.4985) 31.30% 
4 1393 73.02% (0.10%) 71.02% (0.23%) 70.37% (0.23%) 592 (10.6805) 30.95% 
CFG_Active 
1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10% 
2 87 64.09% (0.13%) 74.29% (0.21%) 74.07% (0.22%) 395 (4.1469) 42.09% 
3 138 64.29% (0.15%) 70.15% (0.22%) 69.50% (0.24%) 409 (4.3375) 38.02% 
4 193 64.09% (0.15%) 69.72% (0.23%) 69.06% (0.24%) 413 (4.4015) 37.93% 
CFG_Active 
+ Back-off 
1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10% 
2 87 72.52% (0.10%) 76.91% (0.19%) 76.47% (0.21%) 568 (10.3494) 42.09% 
3 138 71.72% (0.14%) 71.90% (0.24%) 71.24% (0.27%) 581 (10.6330) 38.02% 
4 193 71.21% (0.15%) 71.90% (0.25%) 71.24% (0.26%) 580 (10.5266) 37.93% 
Table 2. Semantic accuracies for partial (keyword or slot) and full phrase recognitions (keyword + slot) using a CFG trained on either 
?Full? or ?Active? transcriptions (i.e., selective transcriptions based on active learning). Parentheses indicate standard error about the mean.  
The ?i? column represents iteration.  The ?Utterances Transcribed? column is cumulative.  The ?OOG%? column represents coverage of the 
ith CFG on the hold-out set. Rows containing ?Back-off? evaluate 2-pass recognition using both the CFG and a derived CFG back-off. 
 
 
67
Proceedings of the ACL 2010 Conference Short Papers, pages 313?317,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Speech to Reply to SMS Messages While Driving: 
An In-Car Simulator User Study 
Yun-Cheng Ju, Tim Paek 
Microsoft Research 
Redmond, WA USA 
{yuncj|timpaek}@microsoft.com 
 
 
Abstract 
Speech recognition affords automobile 
drivers a hands-free, eyes-free method of 
replying to Short Message Service (SMS) 
text messages. Although a voice search 
approach based on template matching has 
been shown to be more robust to the chal-
lenging acoustic environment of automo-
biles than using dictation, users may have 
difficulties verifying whether SMS re-
sponse templates match their intended 
meaning, especially while driving. Using a 
high-fidelity driving simulator, we com-
pared dictation for SMS replies versus 
voice search in increasingly difficult driv-
ing conditions. Although the two ap-
proaches did not differ in terms of driving 
performance measures, users made about 
six times more errors on average using 
dictation than voice search. 
1 Introduction 
Users love Short Message Service (SMS) text 
messaging; so much so that 3 trillion SMS mes-
sages are expected to have been sent in 2009 
alone (Stross, 2008). Because research has 
shown that SMS messaging while driving results 
in 35% slower reaction time than being intox-
icated (Reed & Robbins, 2008), campaigns have 
been launched by states, governments and even 
cell phone carriers to discourage and ban SMS 
messaging while driving (DOT, 2009). Yet, au-
tomobile manufacturers have started to offer in-
fotainment systems, such as the Ford Sync, 
which feature the ability to listen to incoming 
SMS messages using text-to-speech (TTS). Au-
tomatic speech recognition (ASR) affords users a 
hands-free, eyes-free method of replying to SMS 
messages. However, to date, manufacturers have 
not established a safe and reliable method of le-
veraging ASR, though some researchers have 
begun to explore techniques. In previous re-
search (Ju & Paek, 2009), we examined three 
ASR approaches to replying to SMS messages: 
dictation using a language model trained on SMS 
responses, canned responses using a probabilistic 
context-free grammar (PCFG), and a ?voice 
search? approach based on template matching. 
Voice search proceeds in two steps (Natarajan et 
al., 2002): an utterance is first converted into 
text, which is then used as a search query to 
match the most similar items of an index using 
IR techniques (Yu et al, 2007). For SMS replies, 
we created an index of SMS response templates, 
with slots for semantic concepts such as time and 
place, from a large SMS corpus. After convolv-
ing recorded SMS replies so that the audio would 
exhibit the acoustic characteristics of in-car rec-
ognition, they compared how the three approach-
es handled the convolved audio with respect to 
the top n-best reply candidates. The voice search 
approach consistently outperformed dictation and 
canned responses, achieving as high as 89.7% 
task completion with respect to the top 5 reply 
candidates. 
Even if the voice search approach may be 
more robust to in-car noise, this does not guaran-
tee that it will be more usable. Indeed, because 
voice search can only match semantic concepts 
contained in the templates (which may or may 
not utilize the same wording as the reply), users 
must verify that a retrieved template matches the 
semantics of their intended reply. For example, 
suppose a user replies to the SMS message ?how 
about lunch? with ?can?t right now running er-
rands?. Voice search may find ?nope, got er-
rands to run? as the closest template match, in 
which case, users will have to decide whether 
this response has the same meaning as their re-
ply. This of course entails cognitive effort, which 
is very limited in the context of driving. On the 
other hand, a dictation approach to replying to 
SMS messages may be far worse due to misre-
cognitions. For example, dictation may interpret 
?can?t right now running errands? as ?can right 
313
now fun in errands?. We posited that voice 
search has the advantage because it always gene-
rates intelligible SMS replies (since response 
templates are manually filtered), as opposed to 
dictation, which can sometimes result in unpre-
dictable and nonsensical misrecognitions. How-
ever, this advantage has not been empirically 
demonstrated in a user study. This paper presents 
a user study investigating how the two approach-
es compare when users are actually driving ? that 
is, when usability matters most. 
2 Driving Simulator Study 
Although ASR affords users hands-free, eyes-
free interaction, the benefits of leveraging speech 
can be forfeit if users are expending cognitive 
effort judging whether the speech interface cor-
rectly interpreted their utterances. Indeed, re-
search has shown that the cognitive demands of 
dialogue seem to play a more important role in 
distracting drivers than physically handling cell 
phones (Nunes & Recarte, 2002; Strayer & 
Johnston, 2001). Furthermore, Kun et al (2007) 
have found that when in-car speech interfaces 
encounter recognition problems, users tend to 
drive more dangerously as they attempt to figure 
out why their utterances are failing. Hence, any 
approach to replying to SMS messages in auto-
mobiles must avoid distracting drivers with er-
rors and be highly usable while users are en-
gaged in their primary task, driving. 
2.1 Method 
To assess the usability and performance of both 
the voice search approach and dictation, we con-
ducted a controlled experiment using the STISIM 
Drive? simulator. Our simulation setup con-
sisted of a central console with a steering wheel 
and two turn signals, surrounded by three 47?? 
flat panels placed at a 45? angle to immerse the 
driver. Figure 1 displays the setup. 
We recruited 16 participants (9 males, 7 fe-
males) through an email sent to employees of our 
organization. The mean age was 38.8. All partic-
ipants had a driver?s license and were compen-
sated for their time.  
We examined two independent variables: SMS 
Reply Approach, consisting of voice search and 
dictation, and Driving Condition, consisting of 
no driving, easy driving and difficult driving. We 
included Driving Condition as a way of increas-
ing cognitive demand (see next section). Overall, 
we conducted a 2 (SMS Reply Approach) ? 3 
(Driving Condition) repeated measures, within-
subjects design experiment in which the order of 
SMS Reply for each Driving Condition was coun-
ter-balanced. Because our primary variable of 
interest was SMS Reply, we had users experience 
both voice search and dictation with no driving 
first, then easy driving, followed by difficult 
driving. This gave users a chance to adjust them-
selves to increasingly difficult road conditions. 
 
Driving Task: As the primary task, users were 
asked to drive two courses we developed with 
easy driving and difficult driving conditions 
while obeying all rules of the road, as they would 
in real driving and not in a videogame. With 
speed limits ranging from 25 mph to 55 mph, 
both courses contained five sequential sections 
which took about 15-20 minutes to complete: a 
residential area, a country highway, and a small 
city with a downtown area as well as a busi-
ness/industrial park. Although both courses were 
almost identical in the number of turns, curves, 
stops, and traffic lights, the easy course consisted 
mostly of simple road segments with relatively 
no traffic, whereas the difficult course had four 
times as many vehicles, cyclists, and pedestrians. 
The difficult course also included a foggy road 
section, a few busy construction sites, and many 
unexpected events, such as a car in front sudden-
ly breaking, a parked car merging into traffic, 
and a pedestrian jaywalking. In short, the diffi-
cult course was designed to fully engage the at-
tention and cognitive resources of drivers.  
 
SMS Reply Task: As the secondary task, we 
asked users to listen to an incoming SMS mes-
sage together with a formulated reply, such as: 
(1) Message Received: ?Are you lost?? Your 
Reply: ?No, never with my GPS? 
The users were asked to repeat the reply back to 
the system. For Example (1) above, users would 
have to utter ?No, never with my GPS?. Users 
 
Figure 1. Driving simulator setup. 
314
could also say ?Repeat? if they had any difficul-
ties understanding the TTS rendering or if they 
experienced lapses in attention. For each course, 
users engaged in 10 SMS reply tasks. SMS mes-
sages were cued every 3000 feet, roughly every 
90 seconds, which provided enough time to 
complete each SMS dialogue. Once users uttered 
the formulated reply, they received a list of 4 
possible reply candidates (each labeled as ?One?, 
?Two?, etc.), from which they were asked to ei-
ther pick the correct reply (by stating its number 
at any time) or reject them all (by stating ?All 
wrong?). We did not provide any feedback about 
whether the replies they picked were correct or 
incorrect in order to avoid priming users to pay 
more or less attention in subsequent messages. 
Users did not have to finish listening to the entire 
list before making their selection.  
 
Stimuli: Because we were interested in examin-
ing which was worse, verifying whether SMS 
response templates matched the meaning of an 
intended reply, or deciphering the sometimes 
nonsensical misrecognitions of dictation, we de-
cided to experimentally control both the SMS 
reply uttered by the user as well as the 4-best list 
generated by the system. However, all SMS rep-
lies and 4-best lists were derived from the logs of 
an actual SMS Reply interface which imple-
mented the dictation and the voice search ap-
proaches (see Ju & Paek, 2009). For each course, 
5 of the SMS replies were short (with 3 or fewer 
words) and 5 were long (with 4 to 7 words). The 
mean length of the replies was 3.5 words (17.3 
chars). The order of the short and long replies 
was randomized. 
We selected 4-best lists where the correct an-
swer was in each of four possible positions (1-4) 
or All Wrong; that is, there were as many 4-best 
lists with the first choice correct as there were 
with the second choice correct, and so forth. We 
then randomly ordered the presentation of differ-
ent 4-best lists. Although one might argue that 
the four positions are not equally likely and that 
the top item of a 4-best list is most often the cor-
rect answer, we decided to experimentally con-
trol the position for two reasons: first, our pre-
vious research (Ju & Paek, 2009) had already 
demonstrated the superiority of the voice search 
approach with respect to the top position (i.e., 1-
best), and second, our experimental design 
sought to identify whether the voice search ap-
proach was more usable than the dictation ap-
proach even when the ASR accuracy of the two 
approaches was the same. 
In the dictation condition, the correct answer 
was not always an exact copy of the reply in 0-2 
of the 10 SMS messages. For instance, a correct 
dictation answer for Example (1) above was ?no 
I?m never with my GPS?. On the other hand, the 
voice search condition had more cases (2-4 mes-
sages) in which the correct answer was not an 
exact copy (e.g., ?no I have GPS?) due to the 
nature of the template approach. To some degree, 
this could be seen as handicapping the voice 
search condition, though the results did not re-
flect the disadvantage, as we discuss later. 
 
Measures: Performance for both the driving task 
and the SMS reply tasks were recorded. For the 
driving task, we measured the numbers of colli-
sions, speeding (exceeding 10 mph above the 
limit), traffic light and stop sign violations, and 
missed or incorrect turns. For the SMS reply 
task, we measured duration (i.e., time elapsed 
between the beginning of the 4-best list and 
when users ultimately provided their answer) and 
the number of times users correctly identified 
which of the 4 reply candidates contained the 
correct answer. 
Originally, we had an independent rater verify 
the position of the correct answer in all 4-best 
lists, however, we considered that some partici-
pants might be choosing replies that are semanti-
cally sufficient, even if they are not exactly cor-
rect. For example, a 4-best list generated by the 
dictation approach for Example (1) had: ?One: 
no I?m never want my GPS. Two: no I?m never 
with my GPS. Three: no I?m never when my 
GPS. Or Four: no no I?m in my GPS.? Although 
the rater identified the second reply as being 
?correct?, a participant might view the first or 
third replies as sufficient. In order to avoid am-
biguity about correctness, after the study, we 
showed the same 16 participants the SMS mes-
sages and replies as well as the 4-best lists they 
received during the study and asked them to se-
lect, for each SMS reply, any 4-best list items 
they felt sufficiently conveyed the same mean-
ing, even if the items were ungrammatical. Par-
ticipants were explicitly told that they could se-
lect multiple items from the 4-best list. We did 
not indicate which item they selected during the 
experiment and because this selection task oc-
curred months after the experiment, it was un-
likely that they would remember anyway. Partic-
ipants were compensated with a cafeteria vouch-
er. 
In computing the number of ?correct? an-
swers, for each SMS reply, we counted an an-
315
swer to be correct if it was included among the 
participants? set of semantically sufficient 4-best 
list items. Hence, we calculated the number of 
correct items in a personalized fashion for every 
participant. 
2.2 Results 
We conducted a series of repeated measures 
ANOVAs on all driving task and SMS reply task 
measures. For the driving task, we did not find 
any statistically significant differences between 
the voice search and dictation conditions. In oth-
er words, we could not reject the null hypothesis 
that the two approaches were the same in terms 
of their influence on driving performance. How-
ever, for the SMS reply task, we did find a main 
effect for SMS Reply Approach (F1,47 = 81.28, p < 
.001, ?Dictation = 2.13 (.19), ?VoiceSearch = .38 (.10)). 
As shown in Figure 2, the average number of 
errors per driving course for dictation is roughly 
6 times that for voice search. We also found a 
main effect for total duration (F1,47 = 11.94, p < 
.01, ?Dictation = 113.75 sec (3.54) or 11.4 sec/reply, 
?VoiceSearch = 125.32 sec (3.37) or 12.5 sec/reply). 
We discuss our explanation for the shorter dura-
tion below. For both errors and duration, we did 
not find any interaction effects with Driving 
Conditions. 
3 Discussion 
We conducted a simulator study in order to ex-
amine which was worse while driving: verifying 
whether SMS response templates matched the 
meaning of an intended reply, or deciphering the 
sometimes nonsensical misrecognitions of dicta-
tion. Our results suggest that deciphering dicta-
tion results under the duress of driving leads to 
more errors. In conducting a post-hoc error anal-
ysis, we noticed that participants tended to err 
when the 4-best lists generated by the dictation 
approach contained phonetically similar candi-
date replies. Because it is not atypical for the dic-
tation approach to have n-best list candidates 
differing from each other in this way, we rec-
ommend not utilizing this approach in speech-
only user interfaces, unless the n-best list candi-
dates can be made as distinct from each other as 
possible, phonetically, syntactically and most 
importantly, semantically. The voice search ap-
proach circumvents this problem in two ways: 1) 
templates were real responses and manually se-
lected and cleaned up during the development 
phase so there were no grammatical mistakes, 
and 2) semantically redundant templates can be 
further discarded to only present the distinct con-
cepts at the rendering time using the paraphrase 
detection algorithms reported in (Wu et al, 
2010). 
Given that users committed more errors in the 
dictation condition, we initially expected that 
dictation would exhibit higher duration than 
voice search since users might be spending more 
time figuring out the differences between the 
similar 4-best list candidates generated by the 
dictation approach. However, in our error analy-
sis we observed that most likely users did not 
discover the misrecognitions, and prematurely 
selected a reply candidate, resulting in shorter 
durations. The slightly higher duration for the 
voice search approach does not constitute a prob-
lem if users are listening to all of their choices 
and correctly selecting their intended SMS reply. 
Note that the duration did not bring about any 
significant driving performance differences. 
Although we did not find any significant driv-
ing performance differences, users experienced 
more difficulties confirming whether the dicta-
tion approach correctly interpreted their utter-
ances than they did with the voice search ap-
proach. As such, if a user deems it absolutely 
necessary to respond to SMS messages while 
driving, our simulator study suggests that the 
most reliable (i.e., least error-prone) way to re-
spond may just well be the voice search ap-
proach. 
References  
Distracted Driving Summit. 2009. Department of 
Transportation. Retrieved Dec. 1: 
http://www.rita.dot.gov/distracted_driving_summit 
 
Figure 2. Mean number of errors for the dictation 
and voice search approaches. Error bars represent 
standard errors about the mean. 
316
Y.C. Ju & T. Paek. 2009. A Voice Search Approach 
to Replying to SMS Messages in Automobiles. In 
Proc. of Interspeech. 
A. Kun, T. Paek & Z. Medenica. 2007. The Effect of 
Speech Interface Accuracy on Driving Perfor-
mance, In Proc. of Interspeech. 
P. Natarajan, R. Prasad, R. Schwartz, & J. Makhoul. 
2002. A Scalable Architecture for Directory Assis-
tance Automation. In Proc. of ICASSP, pp. 21-24. 
L. Nunes & M. Recarte. 2002. Cognitive Deamnds of 
Hands-Free-Phone Conversation While Driving. 
Transportation Research Part F, 5: 133-144. 
N. Reed & R. Robbins. 2008. The Effect of Text 
Messaging on Driver Behaviour: A Simulator 
Study. Transport Research Lab Report, PPR 367. 
D. Strayer & W. Johnston. 2001. Driven to Distrac-
tion: Dual-task Studies of Simulated Driving and 
Conversing on a Cellular Phone. Psychological 
Science, 12: 462-466. 
R. Stross. 2008. ?What carriers aren?t eager to tell you 
about texting?, New York Times, Dec. 26, 2008: 
http://www.nytimes.com/2008/12/28/business/28di
gi.html?_r=3  
D. Yu, Y.C. Ju, Y.-Y. Wang, G. Zweig, & A. Acero. 
2007. Automated Directory Assistance System: 
From Theory to Practice. In Proc. of Interspeech. 
Wei Wu, Yun-Cheng Ju, Xiao Li, and Ye-Yi Wang, 
Paraphrase Detection on SMS Messages in Auto-
mobiles, in ICASSP, IEEE, March 2010 
317
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 65?74,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis in Czech Social Media Using Supervised Machine
Learning
Ivan Habernal
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
habernal@kiv.zcu.cz
Toma?s? Pta?c?ek
Department of Computer
Science and Engineering,
Faculty of Applied Sciences
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
tigi@kiv.zcu.cz
Josef Steinberger
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
jstein@kiv.zcu.cz
Abstract
This article provides an in-depth research of
machine learning methods for sentiment ana-
lysis of Czech social media. Whereas in En-
glish, Chinese, or Spanish this field has a
long history and evaluation datasets for vari-
ous domains are widely available, in case of
Czech language there has not yet been any
systematical research conducted. We tackle
this issue and establish a common ground for
further research by providing a large human-
annotated Czech social media corpus. Fur-
thermore, we evaluate state-of-the-art super-
vised machine learning methods for sentiment
analysis. We explore different pre-processing
techniques and employ various features and
classifiers. Moreover, in addition to our newly
created social media dataset, we also report re-
sults on other widely popular domains, such
as movie and product reviews. We believe
that this article will not only extend the current
sentiment analysis research to another family
of languages, but will also encourage competi-
tion which potentially leads to the production
of high-end commercial solutions.
1 Introduction
Sentiment analysis has become a mainstream re-
search field in the past decade. Its impact can be
seen in many practical applications, ranging from
analyzing product reviews (Stepanov and Riccardi,
2011) to predicting sales and stock markets using so-
cial media monitoring (Yu et al, 2013). The users?
opinions are mostly extracted either on a certain po-
larity scale, or binary (positive, negative); various
levels of granularity are also taken into account, e.g.,
document-level, sentence-level, or aspect-based sen-
timent (Hajmohammadi et al, 2012).
Most of the research in automatic sentiment ana-
lysis of social media has been performed in English
and Chinese, as shown by several recent surveys,
i.e., (Liu and Zhang, 2012; Tsytsarau and Palpanas,
2012). For Czech language, there have been very
few attempts, although the importance of sentiment
analysis of social media became apparent, i.e., dur-
ing the recent presidential elections 1. Many Czech
companies also discovered a huge potential in social
media marketing and started launching campaigns,
contests, and even customer support on Facebook?
the dominant social network of the Czech online
community with approximately 3.5 million users.2
However, one aspect still eludes many of them: au-
tomatic analysis of customer sentiment of products,
services, or even a brand or a company name. In
many cases, sentiment is still labeled manually, ac-
cording to our information from one of the leading
Czech companies for social media monitoring.
Automatic sentiment analysis in the Czech envi-
ronment has not yet been thoroughly targeted by the
research community. Therefore it is necessary to
create a publicly available labeled dataset as well as
to evaluate the current state of the art for two rea-
sons. First, many NLP methods must deal with high
flection and rich syntax when processing the Czech
language. Facing these issues may lead to novel
1http://www.mediaguru.cz/2013/01/
analyza-facebook-rozhodne-o-volbe-prezidenta/ [in
Czech]
2http://www.czso.cz/csu/redakce.nsf/i/
uzivatele facebooku [in Czech]
65
approaches to sentiment analysis as well. Second,
freely accessible and well-documented datasets, as
known from many shared NLP tasks, may stimulate
competition which usually leads to the production of
cutting-edge solutions.3
This article focuses on document-level sentiment
analysis performed on three different Czech datasets
using supervised machine learning. As the first
dataset, we created a Facebook corpus consisting
of 10,000 posts. The dataset was manually la-
beled by two annotators. The other two datasets
come from online databases of movie and prod-
uct reviews, whose sentiment labels were derived
from the accompanying star ratings from users of
the databases. We provide all these labeled datasets
under Creative Commons BY-NC-SA licence4
at http://liks.fav.zcu.cz/sentiment ,
together with the sources for all the presented exper-
iments.
The rest of this article is organized as follows.
Section 2 examines the related work with a focus
on the Czech research and social media. Section 3
thoroughly describes the datasets and the annotation
process. In section 4, we list the employed features
and describe our approach to classification. Finally,
section 5 contains the results with a thorough discus-
sion.
2 Related work
There are two basic approaches to sentiment ana-
lysis: dictionary-based and machine learning-based.
While dictionary-based methods usually depend on
a sentiment dictionary (or a polarity lexicon) and a
set of handcrafted rules (Taboada et al, 2011), ma-
chine learning-based methods require labeled train-
ing data that are later represented as features and
fed into a classifier. Recent attempts have also in-
vestigated semi-supervised methods that incorporate
auxiliary unlabeled data (Zhang et al, 2012).
3E.g., named entity recognition based on Conditional Ran-
dom Fields emerged from CoNLL-2003 named entity recogni-
tion shared task.
4http://creativecommons.org/licenses/
by-nc-sa/3.0/
2.1 Supervised machine learning for sentiment
analysis
The key point of using machine learning for senti-
ment analysis lies in engineering a representative set
of features. Pang et al (2002) experimented with
unigrams (presence of a certain word, frequencies of
words), bigrams, part-of-speech (POS) tags, and ad-
jectives on a Movie Review dataset. Martineau and
Finin (2009) tested various weighting schemes for
unigrams based on TFIDF model (Manning et al,
2008) and proposed delta weighting for a binary sce-
nario (positive, negative). Their approach was later
extended by Paltoglou and Thelwall (2010) who pro-
posed further improvement in delta TFIDF weight-
ing.
The focus of the current sentiment analysis re-
search is shifting towards social media, mainly tar-
geting Twitter (Kouloumpis et al, 2011; Pak and
Paroubek, 2010) and Facebook (Go et al, 2009;
Ahkter and Soria, 2010; Zhang et al, 2011; Lo?pez et
al., 2012). Analyzing media with very informal lan-
guage benefits from involving novel features, such
as emoticons (Pak and Paroubek, 2010; Montejo-
Ra?ez et al, 2012), character n-grams (Blamey et al,
2012), POS and POS ratio (Ahkter and Soria, 2010;
Kouloumpis et al, 2011), or word shape (Go et al,
2009; Agarwal et al, 2011).
In many cases, the gold data for training and test-
ing the classifiers are created semi-automatically, as
in, e.g., (Kouloumpis et al, 2011; Go et al, 2009;
Pak and Paroubek, 2010). In the first step, random
samples from a large dataset are drawn according to
presence of emoticons (usually positive and nega-
tive) and are then filtered manually. Although large
high-quality collections can be created very quickly
using this approach, it makes a strong assumption
that every positive or negative post must contain an
emoticon.
Balahur and Tanev (2012) performed experiments
with Twitter posts as part of the CLEF 2012 Re-
pLab5. They classified English and Spanish tweets
by a small but precise lexicon, which contained also
slang, combined with a set of rules that capture the
manner in which sentiment is expressed in social
media.
5http://www.limosine-project.eu/events/
replab2012
66
Since the limited space of this paper does not al-
low us to present detailed evaluation from the related
work, we recommend an in-depth survey by Tsytsa-
rau and Palpanas (2012) for actual results obtained
from the abovementioned methods.
2.2 Sentiment analysis in Czech environment
Veselovska? et al (2012) presented an initial research
on Czech sentiment analysis. They created a corpus
which contains polarity categories of 410 news sen-
tences. They used the Naive Bayes classifier and
a classifier based on a lexicon generated from an-
notated data. The corpus is not publicly available,
moreover, due to the small size of the corpus no
strong conclusions can be drawn.
Steinberger et al (2012) proposed a semi-
automatic ?triangulation? approach to creating sen-
timent dictionaries in many languages, including
Czech. They first produced high-level gold-standard
sentiment dictionaries for two languages and then
translated them automatically into the third lan-
guage by a state-of-the-art machine translation ser-
vice. Finally, the resulting sentiment dictionaries
were merged by taking overlap from the two auto-
matic translations.
A multilingual parallel news corpus annotated
with opinions towards entities was presented in
(Steinberger et al, 2011). Sentiment annotations
were projected from one language to several others,
which saved annotation time and guaranteed compa-
rability of opinion mining evaluation results across
languages. The corpus contains 1,274 news sen-
tences where an entity (the target of the sentiment
analysis) occurs. It contains 7 languages including
Czech. Their research targets fundamentally differ-
ent objectives from our research as they focus on
news media and aspect-based sentiment analysis.
3 Datasets
3.1 Social media dataset
The initial selection of Facebook brand pages for our
dataset was based on the ?top? Czech pages, accord-
ing to the statistics from SocialBakers.6 We focused
on pages with a large Czech fan base and a sufficient
number of Czech posts. Using Facebook Graph API
6http://www.socialbakers.com/facebook-pages/
brands/czech-republic/
and Java Language Detector7 we acquired 10,000
random posts in the Czech language from nine dif-
ferent Facebook pages. The posts were then com-
pletely anonymized as we kept only their textual
contents.
Sentiment analysis of posts at Facebook brand
pages usually serves as a marketing feedback of user
opinions about brands, services, products, or current
campaigns. Thus we consider the sentiment target
to be the given product, brand, etc. Typically, users?
complaints hold negative sentiment, whereas joy or
happiness about the brand is taken as positive. We
also added another class called bipolar which rep-
resents both positive and negative sentiment in one
post.8 In some cases, the user?s opinion, although
being somehow positive, does not relate to the given
page.9 Therefore the sentiment is treated as neutral
in these cases, according to our above-mentioned as-
sumption.
The complete 10k dataset was independently an-
notated by two annotators. The inter-annotator
agreement (Cohen?s ?) between these two anno-
tators reaches 0.66 which represents a substantial
agreement level (Pustejovsky and Stubbs, 2013),
therefore the task can be considered as well-defined.
The gold data were created based on the agree-
ment of the two annotators. They disagreed in
2,216 cases. To solve these conflicts, we involved
a third super-annotator to assign the final sentiment
label. However, even after the third annotator?s la-
beling, there was still no agreement for 308 labels.
These cases were later solved by a fourth annotator.
We discovered that most of these conflicting cases
were classified as either neutral or bipolar. These
posts were often difficult to label because the author
used irony, sarcasm or the context or previous posts.
These issues remain open.
The Facebook dataset contains of 2,587 positive,
5,174 neutral, 1,991 negative, and 248 bipolar posts,
respectively. We ignore the bipolar class later in all
experiments. The sentiment distribution among the
7http://code.google.com/p/jlangdetect/
8For example ?to bylo moc dobry ,fakt jsem se nadlabla :-D
skoda ze uz neni v nabidce???It was very tasty, I really stuffed
myself :-D sad it?s not on the menu anymore?.
9Certain campaigns ask the fans for, i.e., writing a poem?
these posts are mostly positive (or funny, at least) but are irrele-
vant for the desired task.
67
source pages is shown in Figure 1. The statistics
reveal negative opinions towards cell phone oper-
ators and positive opinions towards, e.g., perfumes
and ZOO.
Figure 1: Social media dataset statistics
3.2 Movie review dataset
Movie reviews as a corpus for sentiment analysis
has been used in research since the pioneering re-
search conducted by Pang et al (2002). Therefore
we covered the same domain in our experiments as
well. We downloaded 91,381 movie reviews from
the Czech Movie Database10 and split them into 3
categories according to their star rating (0?2 stars as
negative, 3?4 stars as neutral, 5?6 stars as positive).
The dataset contains of 30,897 positive, 30,768 neu-
tral, and 29,716 negative reviews, respectively.
3.3 Product review dataset
Another very popular domain for sentiment analy-
sis deals with product reviews (Hu and Liu, 2004).
We crawled all user reviews from a large Czech e-
shop Mall.cz11 which offers a wide range of prod-
ucts. The product reviews are accompanied with star
ratings on the scale 0?5. We took a different strat-
egy for assigning sentiment labels. Whereas in the
movie dataset the distribution of stars was rather uni-
form, in the product review domain the ratings were
skewed towards the higher values. After a manual
inspection we discovered that 4-star ratings mostly
correspond to neutral opinions and 3 or less stars de-
note mostly negative comments. Thus we split the
10http://www.csfd.cz/
11http://www.mall.cz
dataset into three categories according to this obser-
vation. The final dataset consists of 145,307 posts
(102,977 positive, 31,943 neutral, and 10,387 nega-
tive).
4 Classification
4.1 Preprocessing
As pointed out by Laboreiro et al (2010), tokeniza-
tion significantly affects sentiment analysis, espe-
cially in case of social media. Although Ark-tweet-
nlp tool (Gimpel et al, 2011) was developed and
tested in English, it yields satisfactory results in
Czech as well, according to our initial experiments
on the Facebook corpus. Its significant feature is
proper handling of emoticons and other special char-
acter sequences that are typical for social media.
Furthermore, we remove stopwords using the stop-
word list from Apache Lucene project.12
In many NLP applications, a very popular pre-
processing technique is stemming. We tested Czech
light stemmer (Dolamic and Savoy, 2009) and High
Precision Stemmer13. Another widely-used method
for reducing the vocabulary size, and thus the feature
space, is lemmatization. For Czech language the
only currently available lemmatizer is shipped with
Prague Dependency Treebank (PDT) toolkit (Hajic?
et al, 2006). However, we use our in-house Java
HMM-based implementation using the PDT train-
ing data as we need a better control over each pre-
processing step.
Part-of-speech tagging is done using our in-house
Java solution that exploits Prague Dependency Tree-
bank (PDT) data as well. However, since PDT is
trained on news corpora, we doubt it is suitable for
tagging social media that are written in very infor-
mal language (consult, i.e., (Gimpel et al, 2011)
where similar issues were tackled in English).
Since the Facebook dataset contains a huge num-
ber of grammar mistakes and misspellings (typ-
ically ?i/y?,?e?/je/ie?, and others), we incorporated
phonetic transcription to International Phonetic Al-
phabet (IPA) in order to reduce the effect of these
mistakes. We rely on eSpeak14 implementation. An-
12http://lucene.apache.org/core/
13Publication pending; please visit
http://liks.fav.zcu.cz/HPS/.
14http://espeak.sourceforge.net
68
Pipe 1 Pipe 2 Pipe 3
Tokenizing
ArkTweetNLP
POS tagging
PDT
Stem (S) Lemma (L)
none (n) PDT (p)
light (l)
HPS (h)
Stopwords
remove
Casing (C) Phonetic (P) ?
keep (k) eSpeak (e)
lower (l)
Table 1: The preprocessing pipes (top-down). Various
combinations of methods can be denoted using the ap-
propriate labels, e.g. ?SnCk? means 1. tokenizing, 2.
POS-tagging, 3. no stemming, 4. removing stopwords,
and 5. no casing, or ?Lp? means 1. tokenizing, 2. POS-
tagging, 3. lemmatization using PDT, and 4. removing
stopwords.
other preprocessing step might involve removing di-
acritics, as many Czech users type only using unac-
cented characters. However, posts without diacritics
represent only about 8% of our datasets, thus we de-
cided to keep diacritics unaffected.
The complete preprocessing diagram and its vari-
ants is depicted in Table 1. Overall, there are 10
possible preprocessing ?pipe? configurations.
4.2 Features
N-gram features We use presence of unigrams
and bigrams as binary features. The feature space is
pruned by minimum n-gram occurrence which was
empirically set to 5. Note that this is the baseline
feature in most of the related work.
Character n-gram features Similarly to the word
n-gram features, we added character n-gram fea-
tures, as proposed by, e.g., (Blamey et al, 2012). We
set the minimum occurrence of a particular charac-
ter n-gram to 5, in order to prune the feature space.
Our feature set contains 3-grams to 6-grams.
POS-related features Direct usage of part-of-
speech n-grams that would cover sentiment patterns
has not shown any significant improvement in the re-
lated work. Still, POS tags provide certain character-
istics of a particular post. We implemented various
POS features that include, e.g., the number of nouns,
verbs, and adjectives (Ahkter and Soria, 2010), the
ratio of nouns to adjectives and verbs to adverbs
(Kouloumpis et al, 2011), and number of negative
verbs.
Emoticons We adapted the two lists of emoticons
that were considered as positive and negative from
(Montejo-Ra?ez et al, 2012). The feature captures
number of occurrences of each class of emoticons
within the text.
Delta TFIDF variants for binary scenarios Al-
though simple binary word features (presence of a
certain word) reach surprisingly good performance,
they have been surpassed by various TFIDF-based
weighting, such as Delta TFIDF (Martineau and
Finin, 2009), or Delta BM25 TFIDF (Paltoglou and
Thelwall, 2010). Delta-TFIDF still uses traditional
TFIDF word weighting but treats positive and nega-
tive documents differently. However, all the exist-
ing related works which use this kind of features
deal only with binary decisions (positive/negative),
thus we filtered out neutral documents from the
datasets.15 We implemented the most promising
weighting schemes from (Paltoglou and Thelwall,
2010), namely Augmented TF, LogAve TF, BM25
TF, Delta Smoothed IDF, Delta Prob. IDF, Delta
Smoothed Prob. IDF, and Delta BM25 IDF.
4.3 Classifiers
All evaluation tests were performed using two clas-
sifiers, Maximum Entropy (MaxEnt) and Support
Vector Machines (SVM). Although Naive Bayes
classifier is also widely used in the related work, we
did not include it as it usually performs worse than
SVM or MaxEnt. We used a pure Java framework
for machine learning16 with default settings (linear
kernel for SVM).
5 Results
For each combination from the preprocessing
pipeline (refer to Table 1) we assembled various sets
of features and employed two classifiers. In the first
15Opposite to leave-one-out cross validation in (Paltoglou
and Thelwall, 2010), we still use 10-fold cross validation in all
experiments.
16http://liks.fav.zcu.cz/ml
69
scenario, we classify into all three classes (positive,
negative, and neutral).17 In the second scenario,
we follow a strand of related research, e.g., (Mar-
tineau and Finin, 2009; Celikyilmaz et al, 2010),
that deals only with positive and negative classes.
For these purposes we filtered out all the neutral doc-
uments from the datasets. Furthermore, in this sce-
nario we evaluate only features based on weighted
delta-TFIDF, as, e.g., in (Paltoglou and Thelwall,
2010). We also involved only MaxEnt classifier into
the second scenario.
All tests were conducted in the 10-fold cross val-
idation manner. We report macro F-measure, as
it allows comparing classifier results on different
datasets. Moreover, we do not report micro F-
measure (accuracy) as it tends to prefer performance
on dominant classes in highly unbalanced datasets
(Manning et al, 2008), which is, e.g., the case of
our Product Review dataset where most of the labels
are positive.
5.1 Social media
Table 2 shows the results for the 3-class classifica-
tion scenario on the Facebook dataset. The row la-
bels denote the preprocessing configuration accord-
ing to Table 1. In most cases, maximum entropy
classifier significantly outperforms SVM. The com-
bination of all features (the last column) yields the
best results regardless to the preprocessing steps.
The reason might be that the involved character n-
gram feature captures subtle sequences which repre-
sent subjective punctuation or emoticons, that were
not covered by the emoticon feature. On average,
the best results were obtained when HPS stemmer
and lowercasing or phonetic transcription were in-
volved (lines ShCl and ShPe). This configuration
significantly outperforms other preprocessing tech-
niques for token-based features (see column Unigr
+ bigr + POS + emot.).
In the second scenario we evaluated various
TFIDF weighting schemes for binary sentiment
classification. The results are shown in Table 3.
The three-character notation consists of term fre-
quency, inverse document frequency, and normal-
ization. Due to a large number of possible combi-
nations, we report only the most successful ones,
17We ignore the bipolar posts in the current research.
namely Augmented?a and LogAve?L term fre-
quency, followed by Delta Smoothed??(t?), Delta
Smoothed Prob.??(p?), and Delta BM25??(k)
inverse document frequency; normalization was not
involved. We can see that the baseline (the first col-
umn bnn) is usually outperformed by any weighted
TFIDF technique. Moreover, using any kind of
stemming (the row entitled various*) significantly
improves the results. For the exact formulas of the
delta TFIDF variants please refer to (Paltoglou and
Thelwall, 2010).
We also tested the impact of TFIDF word fea-
tures when added to other features from the first sce-
nario (refer to Table 2). Column FS1 in Table 3 dis-
plays results for a feature set with the simple binary
presence-of-the-word feature (binary unigrams). In
the last column FS2 we replaced this binary feature
with TFIDF weighted feature a?(t?)n. It turned out
that the weighed form of word feature does not im-
prove the performance, when compared with sim-
ple binary unigram feature. Furthermore, a set of
different features (words, bigrams, POS, emoticons,
character n-grams) significantly outperforms a sin-
gle TFIDF weighted feature.
We also report the effect of the dataset size on
the performance. We randomly sampled 10 subsets
from the dataset (1k, 2k, etc.) and tested the per-
formance; still using 10-fold cross validation. We
took the most promising preprocessing configura-
tion (ShCl) and MaxEnt classifier. As can be seen in
Figure 2, while the dataset grows to approx 6k?7k
items, the performance rises for most combinations
of features. At 7k-items dataset, the performance
begins to reach its limits for most combinations of
features and hence adding more data does not lead
to a significant improvement.
5.1.1 Upper limits of automatic sentiment
analysis
To see the upper limits of the task itself, we also
evaluate the annotator?s judgments. Although the
gold labels were chosen after a consensus of at least
two people, there were many conflicting cases that
must have been solved by a third or even a fourth
person. Thus even the original annotators do not
achieve 1.00 F-measure on the gold data.
We present ?performance? results of both annota-
tors and of the best system as well (MaxEnt classi-
70
Facebook dataset, 3 classes
Unigrams Unigr + bigrams Unigr + bigr + Unigr + bigr + Unigr + bigr + POS +
POS features POS + emot. emot. + char n-grams
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
SnCk 0.63 0.64 0.63 0.64 0.66 0.64 0.66 0.64 0.69 0.67
SnCl 0.63 0.64 0.63 0.64 0.66 0.63 0.66 0.63 0.69 0.68
SlCk 0.65 0.67 0.66 0.67 0.68 0.66 0.67 0.66 0.69 0.67
SlCl 0.65 0.67 0.65 0.67 0.68 0.66 0.69 0.66 0.69 0.67
ShCk 0.66 0.67 0.66 0.67 0.68 0.67 0.67 0.67 0.69 0.67
ShCl 0.66 0.66 0.66 0.67 0.69 0.67 0.69 0.67 0.69 0.67
SnPe 0.64 0.65 0.64 0.65 0.67 0.65 0.67 0.65 0.68 0.68
SlPe 0.65 0.67 0.65 0.67 0.68 0.67 0.67 0.66 0.68 0.67
ShPe 0.66 0.67 0.66 0.67 0.69 0.66 0.69 0.66 0.68 0.67
Lp 0.64 0.65 0.63 0.65 0.67 0.64 0.67 0.65 0.68 0.67
Table 2: Results on the Facebook dataset, classification into 3 classes. Macro F-measure, 95% confidence interval
= ?0.01. Bold numbers denote the best results.
Facebook dataset, positive and negative classes only
bnn a?(t?)n a?(p?)n a?(k)n L?(t?)n L?(p?)n L?(k)n FS1 FS2
SnCk 0.83 0.86 0.86 0.86 0.85 0.86 0.86 0.90 0.89
SnCl 0.84 0.86 0.86 0.86 0.86 0.86 0.86 0.90 0.90
various* 0.85 0.88 0.88 0.88 0.88 0.88 0.88 0.90 0.90
SnPe 0.84 0.86 0.86 0.86 0.86 0.86 0.86 0.90 0.90
Lp 0.84 0.86 0.85 0.85 0.86 0.86 0.86 0.88 0.88
* same results for ShCk, ShCl, SlCl, SlPe, SlCk, and ShPe
FS1: Unigr + bigr + POS + emot. + char n-grams
FS2: a?(t?)n + bigr + POS + emot. + char n-grams
Table 3: Results on the Facebook dataset for various TFIDF-weighted features, classification into 2 classes. Macro F-
measure, 95% confidence interval = ?0.01. Underlined numbers show the best results for TFIDF-weighted features.
Bold numbers denote the best overall results.
Figure 2: Performance wrt. data size. Using ShCl pre-
processing and MaxEnt classifier.
fier, all features, ShCl preprocessing). Table 4 shows
the results as confusion matrices. For each class
(p?positive, n?negative, 0?neutral) we also re-
port precision, recall, and F-measure. The row head-
ings denote gold labels, the column headings repre-
sent values assigned by the annotators or the sys-
tem.18 The annotators? results show what can be ex-
pected from a ?perfect? system that would solve the
task the way a human would.
In general, both annotators judge all three classes
with very similar F-measure. By contrast, the sys-
tem?s F-measure is very low for negative posts (0.54
vs. ? 0.75 for neutral and positive). We offer the
following explanation. First, many of the negative
posts surprisingly contain happy emoticons, which
18Even though the task has three classes, the annotators also
used ?b? for ?bipolar and ??? for ?cannot decide?.
71
Annotator 1
0 n p ? b P R Fm
0 4867 136 115 2 54 .93 .94 .93
n 199 1753 6 0 33 .93 .88 .90
p 175 6 2376 0 30 .95 .92 .93
Macro Fm: .92
Annotator 2
0 n p ? b P R Fm
0 4095 495 573 3 8 .95 .79 .86
n 105 1878 6 0 2 .79 .94 .86
p 100 12 2468 3 4 .81 .95 .88
Macro Fm: .86
Best system
0 n p P R Fm
0 4014 670 490 .74 .78 .76
n 866 1027 98 .57 .52 .54
p 563 102 1922 .77 .74 .75
Macro Fm: .69
Table 4: Confusion matrices for three-class classification.
?Best system? configuration: all features (unigram, bi-
gram, POS, emoticons, character n-grams), ShCl prepro-
cessing, and MaxEnt classifier. 95% confidence interval
= ?0.01.
could be a misleading feature for the classifier. Sec-
ond, the language of the negative posts in not as ex-
plicit as for the positive ones in many cases; the neg-
ativity is ?hidden? in irony, or in a larger context (i.e.,
?Now I?m sooo satisfied with your competitor :))?).
This remains an open issue for the future research.
5.2 Product and movie reviews
For the other two datasets, the product reviews and
movie reviews, we slightly changed the configura-
tion. First, we removed the character n-grams from
the feature sets, otherwise the feature space would
become too large for feasible computing. Second,
we abandoned SVM as it became computationally
infeasible for such a large datasets.
Table 5 (left-hand part) presents results on the
product reviews. The combination of unigrams and
bigrams works best, almost regardless of the prepro-
cessing. By contrast, POS features rapidly decrease
the performance. We suspect that POS features do
not carry any useful information in this case and by
introducing a lot of ?noise? they cause that the op-
timization function in the MaxEnt classifier fails to
find a global minimum.
In the right-hand part of Table 5 we can see the
results on the movie reviews. Again, the bigram fea-
ture performs best, paired with combination of HPS
stemmer and phonetic transcription (ShPe). Adding
POS-related features causes a large drop in perfor-
mance. We can conclude that for larger texts, the
bigram-based feature outperforms unigram features
and, in some cases, a proper preprocessing may fur-
ther significantly improve the results.
6 Conclusion
This article presented an in-depth research of super-
vised machine learning methods for sentiment ana-
lysis of Czech social media. We created a large
Facebook dataset containing 10,000 posts, accom-
panied by human annotation with substantial agree-
ment (Cohen?s ? 0.66). The dataset is freely avail-
able for non-commercial purposes.19 We thoroughly
evaluated various state-of-the-art features and clas-
sifiers as well as different language-specific prepro-
cessing techniques. We significantly outperformed
the baseline (unigram feature without preprocess-
ing) in three-class classification and achieved F-
measure 0.69 using a combination of features (un-
igrams, bigrams, POS features, emoticons, charac-
ter n-grams) and preprocessing techniques (unsu-
pervised stemming and phonetic transcription). In
addition, we reported results in two other domains
(movie and product reviews) with a significant im-
provement over the baseline.
To the best of our knowledge, this article is the
only of its kind that deals with sentiment analysis
in Czech social media in such a thorough manner.
Not only it uses a dataset that is magnitudes larger
than any from the related work, but also incorporates
state-of-the-art features and classifiers. We believe
that the outcomes of this article will not only help
to set the common ground for sentiment analysis for
the Czech language but also help to extend the re-
search outside the mainstream languages in this re-
search field.
Acknowledgement
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
19We encourage other researchers to download our dataset
for their research in the sentiment analysis field.
72
Product reviews, 3 classes Movie reviews, 3 classes
FS1 FS2 FS3 FS4 FS1 FS2 FS3 FS4
SnCk 0.70 0.74 0.52 0.49 0.76 0.77 0.71 0.61
SnCl 0.71 0.75 0.51 0.52 0.76 0.77 0.71 0.70
SlCk 0.67 0.75 0.59 0.55 0.78 0.78 0.73 0.72
SlCl 0.67 0.75 0.56 0.57 0.78 0.78 0.71 0.71
ShCk 0.67 0.75 0.57 0.57 0.78 0.78 0.74 0.72
ShCl 0.67 0.74 0.55 0.57 0.77 0.78 0.73 0.73
SnPe 0.69 0.74 0.50 0.55 0.77 0.78 0.69 0.72
SlPe 0.67 0.75 0.55 0.57 0.78 0.78 0.73 0.73
ShPe 0.68 0.74 0.56 0.59 0.78 0.79 0.74 0.73
Lp 0.66 0.75 0.56 0.57 0.77 0.77 0.68 0.70
Table 5: Results on the product and movie review datasets, classification into 3 classes. FSx denote different feature
sets. FS1 = Unigrams; FS2 = Uni + bigrams; FS3 = Uni + big + POS features; FS4 = Uni + big + POS + emot. Macro
F-measure, 95% confidence interval ?0.002 (products), ?0.003 (movies). Bold numbers denote the best results.
systems and by the European Regional Develop-
ment Fund (ERDF), project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
ter of Excellence, CZ.1.05/1.1.00/02.0090. The
access to computing and storage facilities owned
by parties and projects contributing to the Na-
tional Grid Infrastructure MetaCentrum, provided
under the programme ?Projects of Large Infrastruc-
ture for Research, Development, and Innovations?
(LM2010005) is highly acknowledged.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Julie Kane Ahkter and Steven Soria. 2010. Sentiment
analysis: Facebook status messages. Technical report,
Stanford University. Final Project CS224N.
Alexandra Balahur and Hristo Tanev. 2012. Detecting
entity-related events and sentiments from tweets us-
ing multilingual resources. In Proceedings of the 2012
Conference and Labs of the Evaluation Forum Infor-
mation Access Evaluation meets Multilinguality, Mul-
timodality, and Visual Analytics.
Ben Blamey, Tom Crick, and Giles Oatley. 2012. R U
: -) or : -( ? character- vs. word-gram feature selec-
tion for sentiment classification of OSN corpora. In
Proceedings of AI-2012, The Thirty-second SGAI In-
ternational Conference on Innovative Techniques and
Applications of Artificial Intelligence, pages 207?212.
Springer.
A. Celikyilmaz, D. Hakkani-Tu?r, and Junlan Feng. 2010.
Probabilistic model-based sentiment analysis of twit-
ter messages. In Spoken Language Technology Work-
shop (SLT), 2010 IEEE, pages 79?84. IEEE.
Ljiljana Dolamic and Jacques Savoy. 2009. Indexing and
stemming approaches for the czech language. Infor-
mation Processing and Management, 45(6):714?720,
November.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague de-
pendency treebank 2.0. Linguistic Data Consortium,
Philadelphia.
Mohammad Sadegh Hajmohammadi, Roliana Ibrahim,
and Zulaiha Ali Othman. 2012. Opinion mining and
sentiment analysis: A survey. International Journal of
Computers & Technology, 2(3).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
73
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Media,
Barcelona, Catalonia, Spain, July 17-21, 2011. The
AAAI Press.
Gustavo Laboreiro, Lu??s Sarmento, Jorge Teixeira, and
Euge?nio Oliveira. 2010. Tokenizing micro-blogging
messages using a text classification approach. In Pro-
ceedings of the fourth workshop on Analytics for noisy
unstructured text data, AND ?10, pages 81?88, New
York, NY, USA. ACM.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415?463. Springer.
Roque Lo?pez, Javier Tejada, and Mike Thelwall. 2012.
Spanish sentistrength as a tool for opinion mining pe-
ruvian facebook and twitter. In Artificial Intelligence
Driven Solutions to Business and Engineering Prob-
lems, pages 82?85. ITHEA, Sofia, Bulgaria.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis. In
Proceedings of the Third International Conference on
Weblogs and Social Media, ICWSM 2009, San Jose,
California, USA. The AAAI Press.
A. Montejo-Ra?ez, E. Mart??nez-Ca?mara, M. T. Mart??n-
Valdivia, and L. A. Uren?a Lo?pez. 2012. Random
walk weighting over sentiwordnet for sentiment po-
larity detection on twitter. In Proceedings of the 3rd
Workshop in Computational Approaches to Subjectiv-
ity and Sentiment Analysis, WASSA ?12, pages 3?10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, LREC 2010. European Lan-
guage Resources Association.
Georgios Paltoglou and Mike Thelwall. 2010. A study of
information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 1386?1395, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing - Volume 10, EMNLP ?02, pages 79?
86, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
James Pustejovsky and Amber Stubbs. 2013. Natural
Language Annotation for Machine Learning. O?Reilly
Media, Sebastopol, CA 95472.
Josef Steinberger, Polina Lenkova, Mijail Alexandrov
Kabadjov, Ralf Steinberger, and Erik Van der Goot.
2011. Multilingual entity-centered sentiment analy-
sis evaluated by parallel corpora. In Proceedings of
the 8th International Conference on Recent Advances
in Natural Language Processing, RANLP?11, pages
770?775.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Alexandrov Kabadjov, Polina
Lenkova, Ralf Steinberger, Hristo Tanev, Silvia
Va?zquez, and Vanni Zavarella. 2012. Creating senti-
ment dictionaries via triangulation. Decision Support
Systems, 53:689??694.
E.A. Stepanov and G. Riccardi. 2011. Detecting gen-
eral opinions from customer surveys. In Data Mining
Workshops (ICDMW), 2011 IEEE 11th International
Conference on, pages 115?122.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Mikalai Tsytsarau and Themis Palpanas. 2012. Survey
on mining subjective data on the web. Data Mining
and Knowledge Discovery, 24(3):478?514, May.
Kater?ina Veselovska?, Jan Hajic? Jr., and Jana S?indlerova?.
2012. Creating annotated resources for polarity classi-
fication in Czech. In Proceedings of KONVENS 2012,
pages 296?304. O?GAI, September. PATHOS 2012
workshop.
Liang-Chih Yu, Jheng-Long Wu, Pei-Chann Chang, and
Hsuan-Shou Chu. 2013. Using a contextual entropy
model to expand emotion words and their intensity
for the sentiment classification of stock market news.
Knowledge Based Syst, 41:89?97.
Kunpeng Zhang, Yu Cheng, Yusheng Xie, Daniel Honbo,
Ankit Agrawal, Diana Palsetia, Kathy Lee, Wei keng
Liao, and Alok N. Choudhary. 2011. SES: Sentiment
elicitation system for social media data. In Data Min-
ing Workshops (ICDMW), 2011 IEEE 11th Confer-
ence on, Vancouver, BC, Canada, December 11, 2011,
pages 129?136. IEEE.
Dan Zhang, Luo Si, and Vernon J. Rego. 2012. Senti-
ment detection with auxiliary data. Information Re-
trieval, 15(3-4):373?390.
74
