Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 41?48
Manchester, August 2008
Verification and Implementation of Language-Based Deception 
Indicators in Civil and Criminal Narratives 
 
 
Joan Bachenko 
Deception Discovery Technologies 
Oxford, NJ 07863 
jbachenko@comcast.net 
 
Eileen Fitzpatrick 
Montclair State University 
Montclair, NJ 07043 
fitzpatricke@mail.montclair.edu
Michael Schonwetter 
Deception Discovery Technologies 
Minneapolis, MN 55416 
mschonwetter@synchronvideo.com 
 
Abstract 
Our goal is to use natural language proc-
essing to identify deceptive and non-
deceptive passages in transcribed narra-
tives.  We begin by motivating an analy-
sis of language-based deception that 
relies on specific linguistic indicators to 
discover deceptive statements.  The indi-
cator tags are assigned to a document us-
ing a mix of automated and manual 
methods.  Once the tags are assigned, an 
interpreter automatically discriminates 
between deceptive and truthful state-
ments based on tag densities.  The texts 
used in our study come entirely from 
?real world? sources?criminal state-
ments, police interrogations and legal tes-
timony.  The corpus was hand-tagged for 
the truth value of all propositions that 
could be externally verified as true or 
false. Classification and Regression Tree 
techniques suggest that the approach is 
feasible, with the model able to identify 
74.9% of the T/F propositions correctly. 
Implementation of an automatic tagger 
with a large subset of tags performed 
well on test data, producing an average 
score of 68.6% recall and 85.3% preci-
                                                          
 ? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
sion when compared to the performance 
of human taggers on the same subset.   
1. Introduction 
 
The ability to detect deceptive statements in text 
and speech has broad applications in law en-
forcement and intelligence gathering. The scien-
tific study of deception in language dates at least 
from Undeutsch (1954, 1989), who hypothesized 
that it is ?not the veracity of the reporting person 
but the truthfulness of the statement that matters 
and there are certain relatively exact, definable, 
descriptive criteria that form a key tool for the 
determination of the truthfulness of statements?. 
Reviews by Shuy (1998), Vrij (2000), and De-
Paulo et al (2003) indicate that many types of 
deception can be identified because the liar?s 
verbal and non-verbal behavior varies considera-
bly from that of the truth teller?s.  Even so, the 
literature reports that human lie detectors rarely 
perform at a level above chance. Vrij (2000) 
gives a summary of 39 studies of human ability 
to detect lies. The majority of the studies report 
accuracy rates between 45-60%, with the mean 
accuracy rate at 56.6%.  
The goal of our research is to develop and 
implement a system for automatically identifying 
deceptive and truthful statements in narratives 
and transcribed interviews. We focus exclusively 
on verbal cues to deception for this initial 
experiment,  ignoring at present potential 
prosodic cues (but see Hirschberg et al).   
 
41
In this paper, we describe a language-based 
analysis of deception that we have constructed 
and tested using ?real world? sources?criminal 
narratives, police interrogations and legal 
testimony.  Our analysis comprises two 
components:  a set of deception indicators that 
are used for tagging a document and an 
interpreter that associates tag clusters with a 
deception likelihood.   We tested the analysis by 
identifying propositions in the corpus that could 
be verified as true or false and then comparing 
the predictions of our model against this corpus 
of ground truth. Our analysis acheived an 
accuracy rate of 74.9%. In the remainder of this 
paper, we will present the analysis and a detailed 
description of our test results.  Implementation of 
the analysis will also be discussed.  
2. Studying Deception 
The literature on deception comes primarily from 
experimental psychology where much of the 
concentration is on lies in social life and much of 
the experimentation is done in laboratory settings 
where subjects are prompted to lie1. These stud-
ies lack the element of deception under stress. 
Because of the difficulties of collecting and cor-
roborating testimony in legal settings, analysis of 
so-called ?high stakes? data is harder to come by. 
To our knowledge, only two studies (Smith, 
2001; Adams, 2002) correlate linguistic cues 
with deception using high stakes data.  For our 
data we have relied exclusively on police de-
partment transcripts and high profile cases where 
the ground truth facts of the case can be estab-
lished. 
Previous studies correlating linguistic fea-
tures with deceptive behavior (Smith, 2001; Ad-
ams, 2002; Newman et al 2003, and studies cited 
in DePaulo et al 2003) have classified narrators 
as truth-tellers or liars according to the presence, 
number and distribution of deception indicators 
in their narratives. Newman, et al (2003), for 
example, proposes an analysis based on word 
likelihoods for semantically defined items such 
as action verbs, negative emotion words and pro-
nouns. Narratives for their study were generated 
in the laboratory by student subjects.  The goals 
of the project were to determine how well their 
word likelihood analysis classified the presumed 
author of each narrative as a liar or truth-teller 
and to compare their system's performance to 
that of human subjects.  The analysis correctly 
                                                          
 
 statements.    
ive load.  
d Stiff, 1993). 
1 We define deception as a deliberate attempt to mislead. 
We use the terms lying and deceiving interchangeably. 
achieved an overall distinction between liars and 
truth tellers 61% of the time.   
Our research on deception detection differs 
from most previous work in two important ways. 
First, we analyze naturally occurring data, i.e. 
actual civil and criminal narratives instead of 
laboratory generated data.  This gives us access 
to productions that cannot be replicated in 
laboratory experiments for ethical reasons.  
Second, we focus on the classification of specific 
statements within a narrative rather than 
characterizing an entire narrative or speaker as 
truthful or deceptive.  We assume that narrators 
are neither always truthful nor always deceptive. 
Rather, every narrative consists of declarations, 
or assertions of fact, that retain a constant value 
of truth or falsehood. In this respect, we are close 
to Undeutsch?s hypothesis in that we are not 
testing the veracity of the narrator but the 
truthfulness of the narrator?s
The purpose of our analysis is to assist 
human evaluators (e.g. legal professionals, 
intelligence analysts, employment interviewers) 
in assessing a text?s contents.  Hence the 
questions that we must answer are whether it is 
possible to classify specific declarations as true 
or deceptive using only linguistic cues and, if so, 
then how successfully an automated system can 
perform the task.  Our research makes no claim 
as to the cause of a speaker?s behavior, e.g. 
whether deception cues emerge as a function of 
emotional stress or excessive cognit
3. Linguistic Markers of Deception  
The literature on verbal cues to deception 
indicates that fabricated narrative may differ 
from truthful narrative at all levels from global 
discourse to individual word choice. Features of 
narrative structure and length, text coherence, 
factual and sensory detail, filled pauses, syntactic 
structure choice, verbal immediacy, negative 
expressions, tentative constructions, referential 
expressions, and particular phrasings have all 
been shown to differentiate truthful from 
deceptive statements in text (Adams, 2002; 
DePaulo et al, 2003; Miller an
In the area of forensic psychology, Statement 
Validity Assessment is the most commonly used 
technique for measuring the veracity of verbal 
statements. SVA examines a transcribed inter-
view for 19 criteria such as quantity of detail, 
embedding of the narrative in context, descrip-
tions of interactions and reproduction of conver-
sations (Steller & K?hnken, 1989). Tests of SVA 
 
42
show that users are able to detect deception 
above the level of chance -- the level at which 
the lay person functions in identifying deception 
? with some criteria performing considerably 
better (Vrij, 2000). An SVA analysis is admissi-
ble as court evidence in Germany, the Nether-
lands, and Sweden. 
In the criminal justice arena, another tech-
nique, Statement Analysis, or Scientific Content 
Analysis (SCAN), (Sapir, 1987) examines open-
ended written accounts in which the writers 
choose where to begin and what to include in the 
statements. According to Sapir (1995) ?when 
people are given the choice to give their own 
explanation in their own words, they would 
choose to be truthful . . . . it is very difficult to lie 
with commitment.? 
SCAN ?claims to be able to detect instances of 
potential deception within the language behav-
iour of an individual; it does not claim to identify 
whether the suspect is lying? (Smith, 2001). As 
such, its goal is the one we have adopted: to 
highlight areas of a text that require clarification 
as part of an interview strategy. 
Despite SCAN?s claim that it does not aim to 
classify a suspect as truthful or deceptive, the 
validations of SCAN cues to deception to date 
(Smith, 2001; Adams, 2002) evaluate the tech-
nique against entire statements classified as T or 
F. Our approach differs in that we evaluate sepa-
rately portions of the statement as true or decep-
tive based on the density of cues in that portion.  
4. Deception Analysis for an NLP System 
Our analysis is produced by two passes over the 
input text.  In the first pass the text is tagged for 
deception indicators using a mix of automated 
and manual techniques.  In the second pass the 
text is sent to an automated interpreter that calcu-
lates tag density using moving average and word 
proximity measures.    The output of the inter-
preter is a segmentation of the text into truthful 
and deceptive areas. 
4.1 Deception Indicators 
We have selected 12 linguistic indicators of de-
ception cited in the psychological and criminal 
justice literature that can be formally represented 
and automated in an NLP system.  The indicators 
fall into three classes.  
(1) Lack of commitment to a statement or dec-
laration.  The speaker uses linguistic devices to 
avoid making a direct statement of fact.  Five of 
the indicators fit into this class: (i) linguistic 
hedges (described below) including non-factive 
verbs and nominals; (ii) qualified assertions, 
which leave open whether an act was performed, 
e.g. I needed to get my inhaler; (iii) unexplained 
lapses of time, e.g. later that day; (iv) overzeal-
ous expressions, e.g. I swear to God, and (v) ra-
tionalization of an action, e.g. I was unfamiliar 
with the road. 
(2)  Preference for negative expressions in 
word choice, syntactic structure and semantics.  
This class comprises three indicators: (i) negative 
forms, either complete words such as never or 
negative morphemes as in inconceivable; (ii) 
negative emotions, e.g. I was a nervous wreck; 
(iii) memory loss, e.g. I forget. 
(3)  Inconsistencies with respect to verb and 
noun forms. Four of the indicators make up this 
class: (i) verb tense changes (described below); 
(ii) thematic role changes, e.g. changing the the-
matic role of a NP from agent in one sentence to 
patient in another; (iii) noun phrase changes, 
where different NP forms are used for the same 
referent or to change the focus of a narrative; (iv) 
pronoun changes (described below) which are 
similar to noun phrase changes  
To clarify our exposition, three of the indica-
tors are described in more detail below. It is im-
portant to note with respect to these indicators of 
deception that deceptive passages vary consid-
erably in the types and mix of indicators used, 
and the particular words used within an indicator 
type vary depending on factors such as race, 
gender, and socioeconomic status. 
Verb Tense 
The literature assumes that past tense narrative is 
the norm for truthful accounts of past events 
(Dulaney, 1982; Sapir, 1987; Rudacille, 1994). 
However, as Porter and Yuille (1996) demon-
strate, it is deviations from the past tense that 
correlate with deception. Indeed, changes in 
tense are often more indicative of deception than 
the overall choice of tense. The most often cited 
example of tense change in a criminal statement 
is that of Susan Smith, who released the brake on 
her car letting her two small children inside 
plunge to their deaths. "I just feel hopeless," she 
said. "I can't do enough. My children wanted me. 
They needed me. And now I can't help them. I 
just feel like such a failure." While her state-
ments about herself were couched in the present 
tense, those about her children were already in 
the past.
 
43
Hedges 
The terms ?hedge? and ?hedging? were intro-
duced by Lakoff (1972) to describe words 
?whose meaning implicitly involves fuzziness?, 
e.g., maybe, I guess, and sort of. The use of 
hedges has been widely studied in logic and 
pragmatics, and for practical applications like 
translation and language teaching (for a review, 
see Schr?der & Zimmer, 1997). In the forensic 
psychology literature, it has been correlated with 
deception (Knapp et al, 1974; Porter & Yuille, 
1996; Vrij & Heaven, 1999). 
Hedge types in our data include non-factive 
verbs like think and believe, non-factive NPs like 
my understanding and my recollection, epistemic 
adjectives and adverbs like possible and ap-
proximately, indefinite NPs like something and 
stuff, and miscellaneous phrases like a glimpse 
and between 9 and 9:30. 
The particular types of hedging that appear in 
our data depend heavily on the socioeconomic 
status of the speaker and the type of crime. The 
285 hedges in Jeffrey Skilling?s 7562 word En-
ron testimony include 21 cases of my recollec-
tion, 9 of my understanding, and 7 of to my 
knowledge while the 42 hedges in the car thief?s 
2282 word testimony include 6 cases of shit (do-
ing a little painting, and roofing, and shit), 6 of 
just and 4 of probably.  Despite the differences in 
style, however, the deceptive behavior in both 
cases is similar. 
Changes in Referential Expressions 
Laboratory studies of deception have found that 
deceivers tend to use fewer self-referencing ex-
pressions (I, my, mine) than truth-tellers and 
fewer references to others (Knapp et al, 1974; 
Dulaney, 1982; Newman et al, 2003). In exam-
ining a specific real world narrative, however, it 
is impossible to tell what a narrator?s truthful 
baseline use of referential expressions is, so the 
laboratory findings are hard to carry over to ac-
tual criminal narratives.  
On the other hand, changes in the use of refer-
ential expressions, like changes in verb tense, 
have also been cited as indicative of deception 
(Sapir, 1987; Adams, 1996), and these changes 
can be captured formally. Such changes in refer-
ence often involve the distancing of an item; for 
example, in the narrative of Captain McDonald, 
he describes ?my wife? and ?my daughter? sleep-
ing, but he reports the crime to an emergency 
number as follows, with his wife and daughter 
referred to as some people: 
 
So I told him that I needed a doctor and an 
ambulance and that some people had been 
stabbed. 
 
Deceptive statements may also omit refer-
ences entirely. Scott Peterson?s initial police in-
terview is characterized by a high number of 
omitted first person references: 
 
BROCCHINI: You drive straight home? 
PETERSON: To the warehouse, dropped 
off the boat. 
4.2 Identifying a Text Passage as Deceptive or 
Non-deceptive 
The presence or absence of a cue is not in itself 
sufficient to determine whether the language is 
deceptive or truthful.  Linguistic hedges and 
other deception indicators often occur in normal 
language use.  We hypothesize, however, that the 
distribution and density of the indicators would 
correlate with deceptive behavior.2  Areas of a 
narrative that contain a clustering of deceptive 
material may consist of outright lies or they may 
be evasive or misleading, while areas lacking in 
indicator clusters are likely to be truthful. 
   We use a moving average (MA) program to 
find clusters of indicators in a text.  Initially, the 
MA assigns each word in the text a proximity 
score based on its distance, measured in word 
count, to the nearest deception indicator.  Each 
score is then recalculated by applying a MA 
window of N words.  The MA sums the scores 
for N/2 words to the left and right of the current 
word and divides the result by N to obtain the 
revised score.  Clusters of low word scores indi-
cate deceptive areas of the text, high scoring 
clusters indicate truthful areas.  Hence, when 
applied to a text, the MA allows us to segment an 
entire text automatically into non-overlapping 
regions that are identified as likely true, likely 
deceptive or somewhere in between. 
   Our approach assumes that the input text will 
contain sufficient language to display scoring 
patterns. This rules out, for example, polygraph 
tests where answers are confined to Yes or No as 
                                                          
2 Currently the density algorithm does not take into account 
the possibility that some indicators may be more important 
than others. We plan to use the results of this initial test to 
determine the relative contribution of each tag type to the 
accuracy of the identification of deception. 
 
44
well as short answer interviews that focus on 
simple factual statements such as names and ad-
dresses.  Based on the data  examined so far, we 
estimate the analysis requires a minimum 100 
words to produce useful results. 
5. Corpora and Annotation 
The corpus used for developing our approach to 
deception detection was assembled from criminal 
statements, police interrogations, depositions and 
legal testimony; the texts describe a mix of vio-
lent and property crimes, white collar crime and 
civil litigation.  Because of the difficulty in ob-
taining corpora and ground truth information, the 
total corpus size is small--slightly over 30,000 
words. 
For this experiment, we selected a corpus sub-
set of 25,687 words.  Table 1 summarizes the 
corpus subset: 
 
Source 
 
Word Count 
 
Criminal statements (3) 1,527
Police interrogations (2) 3,922
Tobacco lawsuit deposition 12,762
Enron congress. testimony 7,476
 
Total 
 
25,687
 
Table 1. Corpora Used in the Experiment 
 
Each document in the experimental corpus 
was tagged for two factors: (1) linguistic decep-
tion indicators marked words and phrases associ-
ated with deception, and (2) True/False tags 
marked propositions that were externally veri-
fied. 
5.1. Linguistic Annotation (Tagging) 
A team of linguists tagged the corpus for the 
twelve linguistic indicators of deception de-
scribed above. For each document in the corpus, 
two people assigned the deception tags inde-
pendently.  Differences in tagging were then ad-
judicated by the two taggers and a third linguist.  
Because the original tagging work was focused 
on research and discovery, inter-rater reliability 
statistics are not very revealing.  However, cur-
rent work on new corpora more closely resem-
bles other tagging tasks.  In this case we have 
found inter-rater reliability at 96%. 
Tagging decisions were guided by a tagging 
manual that we developed.  The manual provides 
extensive descriptions and examples of each tag 
type.  Taggers did not have access to ground 
truth facts that could have influenced their tag 
assignments.   
5.2. True/False Annotation  
We then examined separate copies of each narra-
tive for propositions that could be externally 
verified. The following is a single proposition 
that asserts, despite its length, one verifiable 
claim?the birthrate went down: 
 
The number of births peaked in about 1955 
and from there on each year there were fewer 
births. As a result of that each year after 1973 
fewer people turned 18 so the company could 
no longer rely on this tremendous number of 
baby boomers reaching smoking age.  
  
Only propositions that could be verified were 
used. Verification came from supporting material 
such as police reports and court documents and 
from statements internal to the narrative, e.g. a 
confession at the end of an interview could be 
used to support or refute specific claims within 
the interview. The initial verification tagging was 
done by technical and legal researchers on the 
project.  The T/F tags were later reviewed by at 
least one other technical researcher. 
The experimental corpus contains 275 verifi-
able propositions. Table 2 gives examples of 
verified propositions in the corpus. 
 
Example True False 
I didn't do work specifically on 
teenage smoking 
 ? 
All right, man, I did it, the 
damage 
?  
 
Black male wearing a coat.  ? 
 
Table 2. Examples of Verified Propositions 
6. Results 
The dataset contained 275 propositions, of which 
164, or 59.6%, were externally verified as False 
and the remainder verified as True.  We tested 
the ability of the model to predict T/F using 
Classification and Regression Tree (CART) 
analysis (Breiman, et al 1984)3 with 25-fold 
cross-validation and a misclassification cost that 
penalizes True misclassified as False. Table 3 
shows the results of the CART analysis: 
                                                          
3 We used the QUEST program described in Loh and Shih 
(1997) for the modeling. QUEST is available at 
http://www.stat.wisc.edu/~loh/quest.html.
 
45
 
Predicted Class 
 False True % Correct 
False 124 40 75.6
  
Actual 
Class 
True 29 82 73.8
 
Table 3. T/F Classification Based on Cue Den-
sity 
 
We can conclude that the model identifies de-
ceptive language at a rate significantly better 
than chance.  Moreover, by tuning the scores to 
favor high recall for false propositions, it be-
comes possible to adapt the model to applications 
where low precision on true propositions is not a 
drawback, e.g. pre-trial interviews where investi-
gators are looking for leads.  The results in Table 
4 show how we might gear the analysis to this 
class of applications. 
 
 
Predicted Class 
 False True % Correct 
False 151 13 92.6
 
Actual 
Class 
True 66 45 40.5
 
Table 4. Penalizing F Misclassified as T  
 
 Finally, it should be noted that input to 
the analysis consisted of individual files with 
some files marked for topic changes.  In prepar-
ing the data for this test, we found that, in many 
cases, the moving average allowed the low 
scores assigned to deceptive language to influ-
ence the scores of nearby truthful language.  This 
typically occurs when the narrative contains a 
change in topic.  For example, in the deposition 
excerpt below, there is a topic change from teen-
age smokers to the definition of psychographic 
studies.  The hedge so far as I know belongs with 
the first topic but not the second.  However, the 
moving average allows the low scores triggered 
by the hedge to improperly affect scores in the 
new topic:  
 
Q:   Do you know anybody who did have 
data that would allow a market penetra-
tion study of the type I've asked about to 
be performed. 
A:  {So far as I know%HEDGE} only the 
federal government. 
Q:   Are you familiar with the phrase 
psychographic study from your work at 
Philip Morris? 
A:  Yes. 
Q:   What is a psychographic study? 
 
To mitigate the effect of topic change, we in-
serted eleven topic change boundaries. The re-
sults suggest that language is "reset" when a new 
topic is introduced by the interviewer or inter-
viewee.   
 
7. A Deception Indicator Tagger 
The results described in the previous section pro-
vide support for the deception indicator (DI) ap-
proach we have developed.  For the 
implementation, we selected a subset of tags 
whose contextual conditions were well estab-
lished by the literature and our own investiga-
tion.  In these cases we were able to formalize 
the rules for automatic assignment of the tags.  
We excluded tags whose contextual conditions 
are still being researched, i.e., tag assignments 
that require human judgment. 
The tagger was constructed as a rule-based 
system that uses a combination of context-free 
and context sensitive substitutions.  An example 
of a context free substitution is ?Mark all occur-
rences of Oh, God as an overzealous statement?.  
A context sensitive substitution is the rule that 
interprets something as a hedge if it is not modi-
fied, i.e., followed by a relative clause or prepo-
sitional phrase.   
In some cases the tagger refers to structure 
and part of speech.  For example, may as a modal 
verb (may_MD) is a hedge.  Certain verb+ infini-
tive complement constructions, e.g. I attempted 
to open the door, make up a qualified assertion.  
Syntactic structure is assigned by the CASS 
chunk parser (Abney, 1990).  Part of speech tags 
are assigned by Brill?s tagger (Brill, 1992).   The 
DI tag rules apply to the output of the parser and 
POS tagger.  
The subset of tags implemented in the tagger 
comprises 86% of all tags that occur in the train-
ing corpus.  To see how well the DI tagger cov-
ered the subset, we first ran the tagger on the 
training corpus.  70% of the subset tags were cor-
rectly identified in that corpus, with 76% preci-
sion.  We then tested the tagger on a test corpus 
of three files.  Each file was also handtagged by 
linguistic researchers on this project.  The results 
of the test are given in Table 5.  Tag amounts 
refer to the number of tags belonging to the sub-
set that was implemented.   
 
 
46
File name Handtags Autotags Correct 
Tags 
confession 31 20 19 
peterson 186 160 108 
deposition 720 665 625 
Total 937 845 752 
 
Table 5. DI Tagger Results on Three Test Files 
 
Table 6 provides a summary of the tagger?s 
performance. 
 
File name Recall Precision 
confession .61 .95 
peterson .58 .675 
deposition .868 .939 
Average .686 .853 
 
Table 6. Summary of DI Tagger Results 
 
These results may reflect a bias in our training 
data towards legal testimony?depositions are 
strongly represented in the corpus, police and 
criminal data less so.  Our test corpus consists of 
a police interview (?peterson?), a criminal state-
ment (?confession?) and a deposition (?deposi-
tion?).  The tagger?s best performance is 
associated with the deposition. 
8. Conclusion 
This paper has presented new results in the study 
of language-based cues to deception and truth-
fulness; these results come entirely from ?real 
world? sources?criminal narratives, interroga-
tions, and legal testimony.  Our goal is to provide 
a method of evaluating declarations within a sin-
gle narrative or document rather than deeming an 
entire narrative (or narrator) as truthful or decep-
tive.   
We first compared the predictions of linguistic 
cues that we adapted from the literature on de-
ception against actual True/False values that 
were manually determined for 275 propositions 
in our corpus.  Predictions from the linguistic 
indicators were determined by scoring the den-
sity of indicators in text areas that contain the 
propositions and using classification and regres-
sion to determine cut-off values for truth prob-
abilities.   
We then evaluated the performance of an 
automated tagger that implements a large subset 
of the linguistic indicators verified in our first 
experiment.  The automated tagger performed 
well on test data, averaging 80.2% correct when 
compared with human performance on the same 
data. 
The results strongly suggest that linguistic 
cues provide a guide to deceptive areas of a text.  
The predictions based on linguistic cues were 
correct in distinguishing False propositions over 
75% of the time, and over 90% for applications 
where recall of False, but not True, is required.  
Results of the automatic tagger?s performance 
suggest that we will eventually achieve a fully 
automated system for processing depositions and 
other documents in which veracity is an impor-
tant issue.  
References  
Abney, S.  1990.  Rapid incremental parsing with 
repair.  In Proceedings of the 6th New OED 
Conference: Electronic Text Research, pp. 1-
9.  University of Waterloo, Waterloo, Ontario. 
Adams, S. 1996. Statement analysis: What do 
suspects words really reveal? The FBI Law 
Enforcement Bulletin. 65(10). 
www.fbi.gov/publications/leb/1996/oct964.txt 
Adams, S. 2002. Communication under stress: 
indicators of veracity and deception in written 
narratives. Ph.D. dissertation, Virginia Poly-
technic Institute and State University 
Brill, E.  1992.  A simple rule-based part-of-
speech tagger.  In Proceedings of the Third 
Conference on Applied Natural Language 
Processing, pp. 152-155.  Trento, Italy. 
DePaulo, B. M., J.J. Lindsay, B.E. Malone, L. 
Muhlenbruck, K. Charlton, and H. Cooper. 
2003. Cues to deception. Psychological Bulle-
tin, 129(1), 74-118. 
Dulaney, E.F. Jr. 1982. Changes in language be-
havior as a function of veracity. Human Com-
munication Research 9, 75-82. 
Hirschberg, J., S. Benus, J. Brenier, F. Enos, S. 
Friedman, S. Gilman, C. Girand, M. Graci-
arena, A. Kathol, L. Michaelis, B. Pellom, E. 
Shriberg and A. Stolcke. 2005. 
INTERSPEECH 2005. Sept. 408, Lisbon, Por-
tugal. 
Knapp, M.L., Hart, R.P., and Dennis, H.S. 1974. 
An exploration of deception as a communica-
tion construct. Human Communication Re-
search, 1, 15-29. 
Lakoff, G.  1972.  Hedges: A study in meaning 
criteria and the logic of fuzzy concepts.  In 
 
47
Papers from the 8th Regional Meeting, Chi-
cago Linguistic Society.   
Loh, W.-Y. and Shih, Y.-S. 1997. Split selection 
methods for classification trees. Statistica 
Sinica 7:815-840. 
Miller, G. R. and J. B. Stiff.  1993. Deceptive 
Communication.  Sage Publications. Thousand 
Oaks, CA. 
Newman, M. L., Pennebaker, J. W., Berry, D. S. 
and J. M. Richards.  2003.  Lying words: pre-
dicting deception from linguistic styles.  Per-
sonality and Social Psychology Bulletin. 29, 
665-675. 
Porter, S. & Yuille, J. (1996). The language of 
deceit: An investigation of the verbal clues in 
the interrogation context. Law & Human Be-
havior, 20(4) 443-458. 
Rudacille, W.C. 1994. Identifying Lies in Dis-
guise. Kendall Hunt. Dubuque, IO. 
Sapir, A. 1987. Scientific Content Analysis 
(SCAN). Laboratory of Scientific Interroga-
tion. Phoenix, AZ. 
Sapir, A. 1995. The View Guidebook: Verbal 
Inquiry ? the Effective Witness. Laboratory of 
Scientific Interrogation. Phoenix, AZ. 
Schr?der, H. and D. Zimmer. 1997. Hedging re-
search in pragmatics: A bibliographical re-
search guide to hedging. In R. Markkanen and 
H. Schroder (eds.) Hedging and Discourse: 
Approaches to the Analysis of a Pragmatic 
Phenomenon in Academic Text. Walter de 
Gruyter, Berlin. 
Shuy, R.  1998.  The Language of Confession, 
Interrogation and Deception. Sage Publica-
tions, Thousand Oaks, CA. 
Smith, N. 2001. Reading between the lines: An 
evaluation of the scientific content analysis 
technique (SCAN). Police Research Series.  
London,UK. 
www.homeoffice.gov.uk/rds/prgpdfs/prs135.pdf 
Steller, M. and G. Kohnken. 1989. Criteria-
Based Content Analysis. In D.C. Raskin (ed.) 
Psychological Methods in Criminal Investiga-
tion and Evidence. Springer-Verlag, New 
York, 217-245. 
Undeutsch, U. 1989. The development of state-
ment reality analysis. In J.C. Yuille (ed.) 
Credibility Assessment. Dordrecht: Kluwer, 
101-121. 
Undeutsch, U. (1954). Die Entwicklung der ge-
richtspsychologischen Gutachtertatigkeit. In 
A. Wellek (Ed.), Bericht uber den 19, Kon-
gress der Deutschen Gesellschaft fur Psy-
chologie (pp. 1132-154). Gottingen: Verlag 
fur Psychologie. 
Vrij, A. 2000. Detecting Lies and Deceit. John 
Wiley & Sons, Chichester, UK. 
Vrij, A. and Heaven, S. 1999. Vocal and verbal 
indicators of deception as a function of lie 
complexity. Psychology, Crime, and Law 5, 
203-215. 
 
48
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 31?38,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Building a Data Collection for Deception Research 
 
Eileen Fitzpatrick Joan Bachenko 
Montclair State University Linguistech Consortium, Inc. 
Montclair, NJ 07043 Oxford, NJ 07863 
fitzpatricke@mail.montclair.edu jbachenko@linguistech.com 
 
 
 
 
 
Abstract 
Research in high stakes deception has been 
held back by the sparsity of ground truth 
verification for data collected from real world 
sources. We describe a set of guidelines for 
acquiring and developing corpora that will 
enable researchers to build and test models of 
deceptive narrative while avoiding the 
problem of sanctioned lying that is typically 
required in a controlled experiment. Our 
proposals are drawn from our experience in 
obtaining data from court cases and other 
testimony, and uncovering the background 
information that enabled us to annotate 
claims made in the narratives as true or false.  
 
1 Introduction 
The ability to spot deception is an issue in many 
important venues: in police, security, border 
crossing, customs, and asylum interviews; in 
congressional hearings; in financial reporting; in 
legal depositions; in human resource evaluation; 
and in predatory communications, including 
Internet scams, identity theft, and fraud.  The 
need for rapid, reliable deception detection in 
these high stakes venues calls for the 
development of computational applications that 
can distinguish true from false claims. 
Our ability to test such applications is, 
however, hampered by a basic issue: the ground 
truth problem. To be able to recognize the lie, the 
researcher must not only identify distinctive 
behavior when someone is lying but must 
ascertain whether the statement being made is 
true or not.  
The prevailing method for handling the 
ground truth problem is the controlled 
experiment, where truth and lies can be 
managed. While controlled laboratory 
experiments have yielded important insights into 
deceptive behavior, ethical and proprietary issues 
have put limits on the extent to which controlled 
experiments can model deception in the "real 
world". High stakes deception cannot be 
simulated in the laboratory without serious ethics 
violations. Hence the motivation to lie is weak 
since subjects have no personal loss or gain at 
stake. Motivation is further compromised when 
the lies are sanctioned by the experimenter who 
directs and condones the lying behavior (Stiff et 
al., 1994). With respect to the studies 
themselves, replication of laboratory deception 
research is rarely done due to differences in data 
sets and subjects used by different research 
groups. The result, as Vrij (2008) points out, is a 
lack of generalizability across studies. 
We believe that many of the issues holding 
back deception research could be resolved 
through the construction of standardized corpora 
that would provide a base for expanding 
deception studies, comparing different 
approaches and testing new methods.  As a first 
step towards standardization, we offer a set of 
practical guidelines for building corpora that are 
customized for studies of high stakes deception.    
The guidelines are based on our experiences in 
creating a corpus of real world language data that 
we used for testing the deception detection 
approach described in Bachenko et al (2008), 
Fitzpatrick and Bachenko (2010).  We hope that 
our experience will encourage other researchers 
to build and contribute corpora with the goal of 
establishing a shared resource that passes the test 
of ecological validity. 
Section 2 of the paper describes the data 
collection initiative we are engaged in, section 3 
describes the methods used to corroborate the 
claims in the data, section 4 concludes our 
account and covers lessons learned.  
We should point out that the ethical 
considerations that govern our data collection are 
subject to the United States Code of Federal 
31
Regulations (CFRs) for the protection of human 
subjects and may differ in some respects from 
those in other countries.  
 
2 Collecting High-Stakes Data 
We are building a corpus of spoken and written 
narrative data used in real world high stakes 
cases in which many of the claims in the corpus 
have been corroborated as True or False.  We 
have corroborated claims in almost 35,090 words 
of narrative. These narratives include statements 
to police, a legal deposition, and congressional 
testimony.   
In assembling and managing our corpus, two 
issues have been paramount: the availability of 
data and constraints on its use. Several types of 
information must be publicly available, including 
the primary linguistic data, background 
information used to determine ground truth, and 
general information about the case or situation 
from which the data is taken. In addition, the 
data must be narrative intensive. There are also 
several considerations about the data that must 
be taken into account, including the mode 
(written or spoken) of the narrative, and 
considerations involving the needs of the users of 
the data.  
To ensure unconstrained access, data 
collection must be exempt from human 
participant restrictions. The restrictions we must 
adhere to are the regulations of Title 46 of the 
CFRs.1 46 CFR 102 lists the data that is exempt 
from human participant restrictions. Exempt data 
includes ?[r]esearch involving the collection or 
study of existing data, documents, records, 
pathological specimens, or diagnostic specimens, 
if these sources are publicly available or if the 
information is recorded by the investigator in 
such a manner that subjects cannot be identified, 
directly or through identifiers linked to the 
subjects.? 
46 CFR 111, section 7 covers protection of 
privacy: ?When appropriate, there are adequate 
provisions to protect the privacy of subjects and 
to maintain the confidentiality of data.?  
It is conceivable that a ?real world? high 
stakes study could involve subjects whose 
identifiable data would be removed from the 
collection, but it is highly unlikely that the 
                                                 
1 These regulations are enforced either by the Institutional 
Review Board (IRB) of the institution where the research 
takes place or by an independent IRB contracted by the 
researchers if there is no housing institution.   
subjects would consent to having their data ? 
even if sanitized ? made available on the 
Internet. We have therefore used only exempt 
data, i.e., data that is publicly available with no 
expectation of privacy on the part of the people 
involved.   
 
2.1 Public availability of data 
There is a large body of narrative data in the 
public domain, data that is also likely to have a 
rich source of ground truth evidence and general 
background information. Typical public sources 
for this data would be crime investigation 
websites, published police interviews, legal 
websites, including findlaw.com and justice.gov, 
quarterly earnings conference calls, and the U.S. 
Congressional Record. Such data includes 
publicly available 
? Face-to-face interviews 
? Depositions 
? Court and other public testimony 
? Phone conversations2 
? Recorded statements to police 
? Written statements to police 
? Debates of political figures and candidates 
for public office 
? Online product endorsements 
? Blogs 
? Webpages 
 
High profile cases are particularly well 
represented on websites. In the U.S., police 
reports, which are a matter of public record, may 
also be obtained for a small fee from local police 
departments. Other data aggregators, like 
FactSet.com, provide data for higher fees. 
2.2 Types of Data 
2.2.1 Primary linguistic data 
The narrative data is the data to be analyzed for 
cues to deception. Written data is, of course, 
available as text, but spoken data may also only 
be available as transcripts. Our current dataset 
includes recorded data only from the Enron 
testimony, but ideally speech data would include 
high quality recorded speech to enable analysis 
of the prosodic qualities of the speech.  
To support robust analysis, it is important that 
the data be narrative intense. The ?yes?/?no? 
                                                 
2 For example, the quarterly earnings conference calls 
analyzed in Larcker and Zakolyukina (2010). 
32
responses of a polygraph interview are not usable 
for language analysis.  
Additionally, we have so far limited our 
collection to spontaneously produced data. 
Prepared, rehearsed narrative provides the 
opportunity to carefully craft the narrative 
putting the narrator in control not only of the 
story but of the language used to convey the 
story. This enables the speaker/writer to avoid 
the cues that we are looking for. We would be 
open to adding prepared data to the collection, 
but have not considered the guidelines for it. 
2.2.2 Background data 
Background information on the primary data is 
the basis for the ground truth annotation of the 
claims made in the primary data. Ground truth 
investigation can use various types of 
information, including that coming from 
interviews, police reports, public records posted 
on local and national government web sites, fact 
checking sites like FactCheck.org 3 and 
PolitiFact.com4 that analyze political claims and 
provide sources for the information they use in 
their own judgments, and websites such as 
truTV.com that offer the facts of a case, the final 
court judgment, and interviews with the people 
involved in the case.  
Many of these sources are available on the 
web ? an advantage of using data where there is 
no expectation of privacy.5 Some data requires 
filing for a police report or a court document. 
The sources for our current data set are given in 
Appendix A. 
Another source of verification can be the 
narrative itself in situations where the narrator 
contradicts a prior claim. For example, one 
narrator, after denying a theft for most of the 
interview, says ?All right, man, I did it,? 
enabling us to mark his previous denials as False.   
2.2.3 General information about the 
case/situation 
Ideally, the corpus will include background 
information on the situation covered by the 
narrative. If the situation is a legal case, the 
background information should include the 
verdict of the judge or jury, the judgment of 
                                                 
3 FactCheck is a project of the Annenberg Public Policy 
Center of the University of Pennsylvania. 
4 PolitiFact is sponsored by the Tampa Bay Times. 
5 Information may be withdrawn from the web, however, if 
there are changes in a case, such as the filing of an appeal or 
simply fading interest in the case.  
conviction given by the judge, and the sentence. 
If the case is on appeal, then that should be 
noted.  
Information on the amount of control the 
narrator has over the story is also valuable. Is the 
narrative elicited or freely given? The former 
gives the narrator less control over the narrative, 
possibly increasing the odds for the appearance 
of cues to deception. Is the narrator offering a 
monologue or a written statement, both of which 
give the author more control of the narrative than 
an interview. 
2.2.4 Speaker information 
General information on the speaker can be 
valuable in gauging the performance of a 
deception model, including information on 
gender, age, and education. We found 
information on first language background and 
culture to be useful in analyzing the speech of 
non-native speakers of English, whose second 
language speech characteristics sometimes align 
with deception cues. Other sociolinguistic traits 
may also be important, although we have found 
that, while sociolinguistic background may 
determine word choice, the deceptive behavior is 
invariant. We have not encountered issues of 
competency to stand trial in the criminal cases 
we have included, but such evaluations should be 
noted if the issue arises in a legal case. 
2.2.5 Spoken and written data 
Two of the narratives in our current collection 
are written; the others are spoken. Both written 
statements were produced as parts of a police 
interview. The purpose of requesting the 
statement is to obtain an account in the 
interviewee's own words and to do this before 
time and questioning affect the interviewee's 
thinking.  Hence the written statement is 
analogous to a lengthy interview answer, and the 
language used is much closer to speech than 
writing, as the opening of the Routier statement 
illustrates: 
Darin and my sister Dana came home from 
working at the shop.  The boys were playing with 
the neighborhood kids outside.  I was finishing 
up dinner. 
2.3 Other considerations 
In providing data for general use by researchers, 
the collector must be aware of varying needs of 
researchers using the data. The general needs we 
33
consider are the ground truth yield and the 
question of the scope of the True/False label.  
2.3.1 Ground truth yield 
The amount of background data that can be 
gathered to yield ground truth judgments can 
vary widely depending on the type of narrative 
data collected. We have worked with private 
criminal data where the ratio of verified 
propositions to words in the primary data is as 
high as .049 and with private job interview data 
where the ratio is as low as .00043. The low 
yield may be problematic for some types of 
experiment, as well as frustrating for the data 
collector. It is important to have some assurance 
that there are a reasonable number of resources 
that can provide ground truth data before 
collecting the narrative data, particularly if the 
narrative data is difficult to collect. 
2.3.2 The Scope of the T/F label 
With the exception of Fornaciari and Poesio 
(2011), Hirschberg et al (2005), Bachenko et al 
(2008) and Fitzpatrick and Bachenko (2010), the 
ML/NLP deception literature distinguishes True 
from False at the level of the narrative, not the 
proposition. In other words, most of the studies 
identify the liar, not the lie. For real world data, 
the choice to label the full narrative as True or 
False usually depends on the length of the 
narrative; a narrator giving trial testimony or a 
job interview will have many claims, while 
someone endorsing a product may have just one: 
this product is good.  
There are high stakes narratives that are short, 
such as TSA airport interviews. However, the 
computational models of such data will be 
different from those of longer narratives where 
true and false statements are interspersed 
throughout. We currently have no data of this 
type. 
3 Providing Ground Truth 
In longer real-world narratives people lie 
selectively and the interviewer usually needs to 
figure out which statements, or propositions, are 
lies. To enable the capture of this situation in a 
model, we engage in a two-step process: the 
scope of selected verifiable propositions in the 
data is marked, and then the claim in each 
proposition is verified or refuted in the 
background investigation.  
3.1 Marking the scope of each proposition 
We currently mark the scope of verifiable 
propositions in the narrative that are likely to 
have supporting background ground truth 
information before we establish the ground truth. 
For example, statements made about a domestic 
disturbance that involved the police are likely to 
have a police report to supply background 
information, while ?my mother walked me to 
school every day,? while technically verifiable, 
will not. 
A verifiable proposition, or claim, is any 
linguistic form that can be assigned a truth value. 
Propositions can be short; the transcribed 
answers below are all fragmented ground truth 
units:   
{my neck%T} 
{Correct%T} 
{Yep%T} 
 
Examples such as these are common in spoken 
dialogue.  Although they do not correspond 
syntactically to a full proposition, they have 
propositional content.   
Propositions can also be quite long. For 
example, in the 34 words of the sentence 
 
Any LJM transaction that involved a cash 
disbursement that would have been within my 
signing authority either had to be signed by me 
or someone else higher in the hierarchical chain 
of the company.  
 
there is only a single claim: I or someone above 
me had to sign LJM transactions that involved 
cash disbursements.  
Some material is excluded from proposition 
tagging. Utterances that attest only to the frame 
of mind of the narrator, e.g. expressions such as I 
think, it?s my belief, cannot be refuted or 
confirmed empirically. Similarly, a sentence like 
Ms. Watkins said that rumor had it contains an 
assertion (rumor had it) not made by the narrator 
and therefore has no value in testing a verbal 
deception hypothesis. For the same reason, direct 
quotes are excluded from verification.  
3.2 Marking the Ground Truth 
Once the scope of the propositions in a narrative 
is marked, the annotated narrative is checked 
against the background ground truth information, 
and each proposition that can be verified is 
marked as T or F. We represent this judgment as 
follows: 
34
But as far as the relationship between {Jeff 
McMahon moving from the finance group into 
the industrial products group%T}, {there was no 
connection whatsoever%F} (Enron) 
 
{At that time Philip Morris owned the Clark 
Gum Company%T} and {we were trying to get 
into the candy business%T} (Johnston) 
3.2.1 The fact checker 
It is critical that the person who marks the 
ground truth has no contact with the persons who 
are checking the narrative for markers of 
deception ? to the extent that the latter task is 
done by hand.  
We have employed a law student to fact check 
the claims in the one legal deposition (Johnston) 
we have in our current data set. We plan to 
employ an accounting student with a background 
in forensic accounting to fact check Lehmann 
Bros. quarterly earnings conference calls (see 
Larcker and Zakolyukina (2010) for similar 
data). For the other data, we have employed 
graduate assistants in linguistics who do not 
work on the deception markers. 
3.2.2 Sources of background information 
At a minimum, the background information used 
to mark the ground truth should include the 
source of the data used to establish the truth. 
That said, no data source is perfect. A confession 
may be coerced, an eyewitness may forget, a 
judgment may be faulty. However, at some point, 
we have to make a decision as to what a credible 
source is. We have assumed that the sources 
given in section 2.2.2 above, as well as claims 
made by the narrator that refute prior claims, all 
function as reliable sources of background 
information upon which to make decisions about 
the truth of a claim. 
3.2.3 Verifying a claim 
To verify a claim, we use both direct and 
circumstantial evidence. However, the latter is 
used only to direct us to a potentially false claim 
and must be supported by additional, direct facts. 
Direct evidence requires no additional 
inferencing. In a narrative we have studied but 
not marked for ground truth, the police return to 
the apartment from which the suspect?s wife has 
gone missing to find her body in the closet, at 
which point the suspect admits to suffocating his 
wife and describes the events leading up to the 
murder. His narrative prior to the confession 
described contrasting events that occurred in the 
same timeframe; this will enable us to mark these 
as False based on the direct evidence of the body 
and the confession. 
Circumstantial evidence requires that a fact be 
inferred. For example, in his testimony before 
the U.S. Congress, Jeffrey Skilling claims that 
when he left Enron four months before the 
company collapsed, he thought ?the company 
was in good shape.? Circumstantial evidence of 
Skilling?s reputation as an astute businessman 
and the well-known knowledge of his deep 
involvement with the company make this 
unlikely, as the interviewing congressman points 
out. However, we relied as well on direct 
testimony from other members of the Enron 
Board of Directors to affirm that Skilling knew 
the disastrous state of Enron when he left.  
Verifying claims is a difficult, time consuming 
and sometimes tedious process. For the 35,090 
words of narrative data currently in our 
collection, we have been able to verify 184 
propositions, 110 as True and 74 as False. 
Appendix B gives the T/F counts for each of our 
narratives. 
3.3 Enron: Examples of verification 
Jeffrey Skilling was the Chief Operating Officer 
of the Enron Corporation as it was failing in 
2001; he left the company in August 2001. In his 
testimony before the U.S. Congress the following 
year, which we used as our primary narrative 
data, Skilling made several important claims that 
were contradicted either by multiple parties 
involved in the case or by facts on record. This 
section illustrates how we apply the evidence to 
several of Skilling?s claims. 
 
1. The financial condition of Enron at the time 
of Skilling?s departure. 
 
MR. SKILLING: Congressman, I can just say it 
again ? {on the date I left I absolutely, 
unequivocally thought the company was in good 
shape.F%}  
 
Congressman Edward Markey provides 
circumstantial evidence that this claim is false, 
stating that Skilling?s reputation, competence and 
hands-on knowledge makes this claim hard to 
believe. Direct evidence comes from Jeffrey 
McMahon, a former Enron treasurer, and Jordan 
Mintz, a senior attorney, who testified that they 
had told Skilling their concerns that limited 
35
partnerships that the company was involved in 
created a conflict of interest for certain Enron 
board members, and were damaging Enron itself. 
 
2. The presence of Mr. Skilling at a critical 
meeting to discuss these limited partnerships, 
which enabled Enron to hide its losses. 
 
MR. SKILLING: Well, {there?s an issue as to 
whether I was actually at a%F} -- the particular 
meeting that you're talking about was in Florida, 
Palm Beach, Florida. . . . 
 
But when Greenwood brandished a copy of the 
meeting's minutes, which confirmed Skilling's 
presence, the former COO hedged his answer, 
saying,  
 
MR. SKILLING: "I could have been there for a 
portion of the meeting. Was I there for the entire 
meeting? I don't know." 
 
3. The issue of whether Skilling, as Enron?s 
Chief Operating Officer, was required to approve 
Enron-LJM limited partnership transactions.  
 
Mr. SKILLING: {I was not required to approve 
those transactions.%F} 
 
Minutes of the Finance Committee of Enron?s 
Board of Directors, October 6, 2000 (referenced 
in the congressional testimony) show that 
?Misters Buy, Causey, and Skilling approve all 
transactions between the company and LJM 
funds.? 
4 Conclusion and lessons learned  
Research in high stakes deception has been held 
back by the difficulty of ground truth 
verification. Finding suitable data "in the wild" 
and conducting the fact checks to obtain ground 
truth is costly, time-consuming and labor 
intensive.  This is not an unknown problem in 
computational linguistics.  Other research efforts 
that rely on fact checking, such as Sauri and 
Pustejovsky (2009), face similar ground truth 
challenges. 
We have described our work in building a 
corpus customized for high stakes deception 
studies in hopes of encouraging other researchers 
to build and share similar corpora.  We envision 
the eventual goal as a multi-language resource 
with standardized methods and corpora available 
to the community at little or no cost. 
We have made several mistakes that we hope 
we and others can avoid in collecting high stakes 
data. Some errors cost us time and others 
aggravating work trying to correct them. 
Our first lesson was to establish a strict 
separation between the people who annotate the 
data for ground truth and those who mark it for 
deception ? if any portion of the latter is being 
done manually. It is important that the fact 
checkers are not influenced by anything in the 
language of the narrator that might skew them 
toward marking a claim one way or the other. 
With respect to the narrative data, it is 
important in selecting new data for annotating 
and ground truth checking to establish that the 
data is of the types approved by the research 
institution?s compliance board; in the United 
States, this is the Institutional Review Board of 
the housing institution.  
It is also important to have assurance that 
there is a robust body of background data with 
which to establish ground truth. While it is 
impressive to be able to find 13 of the 15 
verifiably false statements in 240,000 words of 
narrative?a situation we experienced with a 
private data set?it does not give us the statistical 
robustness we would hope for. 
We also found it important to save the data 
sources locally. Websites disappear and the 
possibility of further fact checking goes with 
them. 
Finally, it is important to provide formal 
training for proposition tagging and ground truth 
tagging to ensure consistency and quality. 
Tutorials, user manuals and careful supervision 
should be available at all times.  
Acknowledgments 
We are thankful to the anonymous EACL 
reviewers for their incisive and helpful 
comments.  Any errors or oversights are strictly 
the responsibility of the authors.  
References  
Joan Bachenko, Eileen Fitzpatrick and Michael 
Schonwetter. 2008. Verification and 
Implementation of Language-based Deception 
Indicators in Civil and Criminal Narratives.   
Proceedings of the 22nd International Conference 
on Computational Linguistics (COLING 2008).  
University of Manchester, Manchester, UK. 
Eileen Fitzpatrick and Joan Bachenko. 2010. Building 
a Forensic Corpus to Test Language-based 
Indicators of Deception. Corpus Linguistics in 
36
North America 2008: Selections from the Seventh 
North American Symposium of the American 
Association for Corpus Linguistics. Gries, S., S. 
Wulff and M. Davies (eds.). Series in Language 
and Computers. Rodopi.   
Tommaso Fornaciari and Massimo Poesio. 2011. 
Lexical vs. Surface Features in Deceptive 
Language Analysis, Workshop: Legal Applications 
of Human Language Technology. 13th 
International Conference on Artificial Intelligence 
and Law. June 6-10. University of Pittsburgh. 
Julia Hirschberg, Stefan Benus, Jason M. Brenier, 
Frank Enos, Sarah Friedman, Sarah Gilman, 
Cynthia Girand, Martin Graciarena, Andreas 
Kathol, Laura Michaelis, Bryan L. Pellom, 
Elizabeth Shriberg, Andreas Stolcke. 2005. 
?Distinguishing Deceptive from Non-Deceptive 
Speech,? INTERSPEECH 2005, Lisbon, 
September. 
David F. Larcker and Anastasia A. Zakolyukina. 
2010. Detecting deceptive discussions in 
conference calls. Rock Center for Corporate 
Governance. Working Paper Series No. 83. 
Roser Sauri and James Pustejovsky. 2009. FactBank 
1.0. Linguistic Data Consortium, Philadelphia. 
James B. Stiff, Steve Corman, Robert Krizek, and 
Eric Snider. 1994. Individual differences and 
changes in nonverbal behavior; Unmasking the 
changing faces of deception. Communication 
Research, 21, 555-581. 
Aldert Vrij. 2008. Detecting Lies and Deceit: Pitfalls 
and Opportunities, 2nd. Edition. Wiley-Interscience. 
Code of Federal Regulations. Retrieved Jan. 26, 2012 
http://www.hhs.gov/ohrp/humansubjects/guidance/
45cfr46.html#46.102 
 
Appendix A. Sources of Background Data that has been verified6 
Case Source 
Johnston Documents available from the State of Minnesota and Blue Cross and Blue Shield of 
Minnesota v Philip Morris Inc et alduring the discovery process of the trial. 
Routier Police report from first responder, Sgt. Matthew Walling. No longer available online 
Enron7 Kenneth L. Lay and Jeffrey K. Skilling Jury Trial ? Govt. Exhibits8 
Enron Special Investigations Report (The Powers Report) 
Employee letters and emails 
Kennedy Police report from Edgartown MA, and transcript of the inquest 
Peterson 
 
Modesto Police Dept. website 
Gomez Peterson interview 
Sawyer Peterson interview 
Findlaw.com 
International call code database 
Mobile number lookup 
Mapquest 
U.S. Time Zones 
Livermore Chevron Station 
 
Appendix B. Distribution of T and F Propositions in Collection 
Case Words Trues Falses 
Johnston 12,762 34 48 
Routier 1,026 8 2 
Enron 7,476 23 21 
Kennedy 245 8 2 
Peterson 13,581 37 1 
TOTAL 35,090 110 74 
 
 
                                                 
6 We included data from two cases of theft in the original set, which was collected prior to the creation of an IRB at our 
university. Incomplete documentation requires us to exclude these cases.  Another case, which we called ?Guilty Nurse,? was 
not sufficiently sourced to be included. 
7 http://news.findlaw.com/legalnews/lit/enron/#documents 
8 http://www.justice.gov/enron/ 
37
Appendix C. Attributes of the Data Set  
S=spoken; W=written 
 
Case Case Type Mode Narrator 
Johnston Civil; sale 
of tobacco 
to teens 
S Male 60+; retired tobacco CEO  
Routier Criminal; 
murder 
W Female 26; homemaker 
Enron (Skilling) Criminal; 
fraud 
S Male 53; former Enron COO 
Kennedy Criminal; 
leaving the 
scene of an 
accident 
W Male 37; former US Senator, deceased 
Peterson 
 
Criminal; 
murder 
S Male 30; agriculture chemical salesman 
 
38
