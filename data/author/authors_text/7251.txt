Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 205?208,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
FastSum:
Fast and accurate query-based multi-document summarization
Frank Schilder and Ravikumar Kondadadi
Research & Development
Thomson Corp.
610 Opperman Drive, Eagan, MN 55123, USA
FirstName.LastName@Thomson.com
Abstract
We present a fast query-based multi-document
summarizer called FastSum based solely on
word-frequency features of clusters, docu-
ments and topics. Summary sentences are
ranked by a regression SVM. The summa-
rizer does not use any expensive NLP tech-
niques such as parsing, tagging of names or
even part of speech information. Still, the
achieved accuracy is comparable to the best
systems presented in recent academic com-
petitions (i.e., Document Understanding Con-
ference (DUC)). Because of a detailed fea-
ture analysis using Least Angle Regression
(LARS), FastSum can rely on a minimal set of
features leading to fast processing times: 1250
news documents in 60 seconds.
1 Introduction
In this paper, we propose a simple method for effec-
tively generating query-based multi-document sum-
maries without any complex processing steps. It
only involves sentence splitting, filtering candidate
sentences and computing the word frequencies in
the documents of a cluster, topic description and the
topic title. We use a machine learning technique
called regression SVM, as proposed by (Li et al,
2007). For the feature selection we use a new model
selection technique called Least Angle Regression
(LARS) (Efron et al, 2004).
Even though machine learning approaches dom-
inated the field of summarization systems in recent
DUC competitions, not much effort has been spent
in finding simple but effective features. Exceptions
are the SumBasic system that achieves reasonable
results with only one feature (i.e., word frequency
in document clusters) (Nenkova and Vanderwende,
2005). Our approach goes beyond SumBasic by
proposing an even more powerful feature that proves
to be the best predictor in all three recent DUC cor-
pora. In order to prove that our feature is more pre-
dictive than other features we provide a rigorous fea-
ture analysis by employing LARS.
Scalability is normally not considered when dif-
ferent summarization systems are compared. Pro-
cessing time of more than several seconds per sum-
mary should be considered unacceptable, in partic-
ular, if you bear in mind that using such a system
should help a user to process lots of data faster. Our
focus is on selecting the minimal set of features that
are computationally less expensive than other fea-
tures (i.e., full parse). Since FastSum can rely on
a minimal set of features determined by LARS, it
can process 1250 news documents in 60 seconds.1
A comparison test with the MEAD system2 showed
that FastSum is more than 4 times faster.
2 System description
We use a machine learning approach to rank all sen-
tences in the topic cluster for summarizability. We
use some features from Microsoft?s PYTHY system
(Toutonova et al, 2007), but added two new fea-
tures, which turned out to be better predictors.
First, the pre-processing module carries out tok-
enization and sentence splitting. We also created
a sentence simplification component which is based
14-way/2.0GHz PIII Xeon 4096Mb Memory
2http://www.summarization.com/mead/
205
on a few regular expressions to remove unimportant
components of a sentence (e.g., As a matter of fact,).
This processing step does not involve any syntac-
tic parsing though. For further processing, we ig-
nore all sentences that do not have at least two exact
word matches or at least three fuzzy matches with
the topic description.3
Features are mainly based on word frequencies of
words in the clusters, documents and topics. A clus-
ter contains 25 documents and is associated with a
topic. The topic contains a topic title and the topic
descriptions. The topic title is list of key words or
phrases describing the topic. The topic description
contains the actual query or queries (e.g., Describe
steps taken and worldwide reaction prior to intro-
duction of the Euro on January 1, 1999.).
The features we used can be divided into two sets;
word-based and sentence-based. Word-based fea-
tures are computed based on the probability of words
for the different containers (i.e., cluster, document,
topic title and description). At runtime, the different
probabilities of all words in a candidate sentence are
added up and normalized by length. Sentence-based
features include the length and position of the sen-
tence in the document. The starred features 1 and
4 are introduced by us, whereas the others can be
found in earlier literature.4
*1 Topic title frequency (1): ratio of number of
words ti in the sentence s that also appear in
the topic title T to the total number of words
t1..|s| in the sentence s:
?|s|
i=1
fT (ti)
|s| , where
fT =
{
1 : ti ? T
0 : otherwise
2 Topic description frequency (2): ratio of number
of words ti in the sentence s that also appear
in the topic description D to the total number
of words t1..|s| in the sentence s:
?|s|
i=1
fD(ti)
|s| ,
where fD =
{
1 : ti ? D
0 : otherwise
3 Content word frequency(3): the average content
word probability pc(ti) of all content words
3Fuzzy matches are defined by the OVERLAP similarity
(Bollegala et al, 2007) of at least 0.1.
4The numbers are used in the feature analysis, as in figure 2.
t1..|s| in a sentence s. The content word proba-
bility is defined as pc(ti) = nN , where n is the
number of times the word occurred in the clus-
ter and N is the total number of words in the
cluster:
?|s|
i=1
pc(ti)
|s|
*4 Document frequency (4): the average document
probability pd(ti) of all content words t1..|s| in
a sentence s. The document probability is de-
fined as pd(ti) = dD , where d is the number of
documents the word ti occurred in for a given
cluster and D is the total number of documents
in the cluster:
?|s|
i=1
pd(ti)
|s|
The remaining features are Headline frequency (5),
Sentence length (6), Sentence position (binary) (7),
and Sentence position (real) (8)
Eventually, each sentence is associated with a
score which is a linear combination of the above
mentioned feature values. We ignore all sentences
that do not have at least two exact word matches.5
In order to learn the feature weights, we trained a
SVM on the previous year?s data using the same fea-
ture set. We used a regression SVM. In regression,
the task is to estimate the functional dependence of
a dependent variable on a set of independent vari-
ables. In our case, the goal is to estimate the score
of a sentence based on the given feature set. In order
to get training data, we computed the word overlap
between the sentences from the document clusters
and the sentences in DUC model summaries. We
associated the word overlap score to the correspond-
ing sentence to generate the regression data. As a
last step, we use the pivoted QR decomposition to
handle redundancy. The basic idea is to avoid redun-
dancy by changing the relative importance of the rest
of the sentences based on the currently selected sen-
tence. The final summary is created from the ranked
sentence list after the redundancy removal step.
3 Results
We compared our system with the top performing
systems in the last two DUC competitions. With our
best performing features, we get ROUGE-2 (Lin,
2004) scores of 0.11 and 0.0925 on 2007 and 2006
5This threshold was derived experimentally with previous
data.
206
IIIT MS LIP6 IDA Peking FastSum Catalonia gen. Baseline
FastSum, 6 Top Systems and generic baseline for DUC 2007
ROU
GE?2
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
Figure 1: ROUGE-2 results including 95%-confidence
intervals for the top 6 systems, FastSum and the generic
baseline for DUC 2007
DUC data, respectively. These scores correspond
to rank 6th for DUC 2007 and the 2nd rank for
DUC 2006. Figure 1 shows a graphical compari-
son of our system with the top 6 systems in DUC
2007. According to an ANOVA test carried out by
the DUC organizers, these 6 systems are significant
better than the remaining 26 participating systems.
Note that our system is better than the PYTHY
system for 2006, if no sentence simplification was
carried out (DUC 2006: 0.089 (without simplifica-
tion); 0.096 (with simplification)). Sentence simpli-
fication is a computationally expensive process, be-
cause it requires a syntactic parse.
We evaluated the performance of the FastSum al-
gorithm using each of the features separately. Ta-
ble 1 shows the ROUGE score (recall) of the sum-
maries generated when we used each of the features
by themselves on 2006 and 2007 DUC data, trained
on the data from the respective previous year. Using
only the Document frequency feature by itself leads
to the second best system for DUC 2006 and to the
tenth best system for DUC 2007.
This first simple analysis of features indicates that
a more rigorous feature analysis would have bene-
fits for building simpler models. In addition, feature
selection could be guided by the complexity of the
features preferring those features that are computa-
tionally inexpensive.
Feature name 2007 2006
Title word frequency 0.096 0.0771
Topic word frequency 0.0996 0.0883
Content word frequency 0.1046 0.0839
Document frequency 0.1061 0.0903
Headline frequency 0.0938 0.0737
Sentence length 0.054 0.0438
Sentence position(binary) 0.0522 0.0484
Sentence position (real-valued) 0.0544 0.0458
Table 1: ROUGE-2 scores of individual features
We chose a so-called model selection algorithm
to find a minimal set of features. This problem can
be formulated as a shrinkage and selection method
for linear regression. The Least Angle Regres-
sion (LARS) (Efron et al, 2004) algorithm can be
used for computing the least absolute shrinkage and
selection operator (LASSO) (Tibshirani, 1996).At
each stage in LARS, the feature that is most corre-
lated with the response is added to the model. The
coefficient of the feature is set in the direction of the
sign of the feature?s correlation with the response.
We computed LARS on the DUC data sets from
the last three years. The graphical results for 2007
are shown in figure 2. In a LARS graph, features
are plotted on the x-axis and the corresponding co-
efficients are shown on y-axis. The value on the x-
axis is the ratio of norm of the coefficent vector to
the maximal norm with no constraint. The earlier a
feature appears on the x-axis, the better it is. Table
2 summarizes the best four features we determined
with LARS for the three available DUC data sets.
Year Top Features
2005 4 2 5 1
2006 4 3 2 1
2007 4 3 5 2
Table 2: The 4 top features for the DUC 2005, 2006 and
2007 data
Table 2 shows that feature 4, document frequency,
is consistently the most important feature for all
three data sets. Content word frequency (3), on the
other hand, comes in as second best feature for 2006
and 2007, but not for 2005. For the 2005 data, the
Topic description frequency is the second best fea-
ture. This observation is reflected by our single fea-
207
* * * * * * * * * ** *
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
2007
|beta|/max|beta|
Stand
ardize
d Coe
fficien
ts
* *
* * * * ** *
* * * * * * * ** *
*
* *
* * * * * ** *
* *
* * ** *
*** ** * * * ** ** * ** *
* * ** *
LASSO
108
23
4
Figure 2: Graphical output of LARS analysis:
Top features for 2007: 4 Document frequency, 3 Content word
frequency, 5 Headline frequency, 2 Topic description frequency
ture analysis for DUC 2006, as shown in table 1.
Similarly, Vanderwende et al (2006) report that they
gave the Topic description frequency a much higher
weight than the Content word frequency.
Consequently, we have shown that our new fea-
ture Document frequency is consistently the best
feature for all three past DUC corpora.
4 Conclusions
We proposed a fast query-based multi-document
summarizer called FastSum that produces state-of-
the-art summaries using a small set of predictors,
two of those are proposed by us: document fre-
quency and topic title frequency. A feature anal-
ysis using least angle regression (LARS) indicated
that the document frequency feature is the most use-
ful feature consistently for the last three DUC data
sets. Using document frequency alone can produce
competitive results for DUC 2006 and DUC 2007.
The two most useful feature that takes the topic de-
scription (i.e., the queries) into account is based on
the number of words in the topic description and the
topic title. Using a limited feature set of the 5 best
features generates summaries that are comparable to
the top systems of the DUC 2006 and 2007main task
and can be generated in real-time, since no compu-
tationally expensive features (e.g., parsing) are used.
From these findings, we draw the following con-
clusions. Since a feature set mainly based on word
frequencies can produce state-of-the-art summaries,
we need to analyze further the current set-up for the
query-based multi-document summarization task. In
particular, we need to ask the question whether the
selection of relevant documents for the DUC top-
ics is in any way biased. For DUC, the document
clusters for a topic containing relevant documents
were always pre-selected by the assessors in prepa-
ration for DUC. Our analysis suggests that simple
word frequency computations of these clusters and
the documents alone can produce reasonable sum-
maries. However, the human selecting the relevant
documents may have already influenced the way
summaries can automatically be generated. Our sys-
tem and systems such as SumBasic or SumFocus
may just exploit the fact that relevant articles pre-
screened by humans contain a high density of good
content words for summarization.6
References
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring Semantic Similarity between Words Using Web
Search Engines. In Proc. of 16th International World
Wide Web Conference (WWW 2007), pages 757?766,
Banff, Canada.
B. Efron, T. Hastie, I.M. Johnstone, and R. Tibshirani.
2004. Least angle regression. Annals of Statistics,
32(2):407?499.
S. Gupta, A. Nenkova, and D. Jurafsky. 2007. Measur-
ing Importance and Query Relevance in Topic-focused
Multi-document Summarization. In Proc. of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 193?196, Prague, Czech Republic.
S. Li, Y. Ouyang, W. Wang, and B. Sun. 2007. Multi-
document summarization using support vector regres-
sion. In Proceedings of DUC 2007, Rochester, USA.
C. Lin. 2004. Rouge: a package for automatic evaluation
of summaries. In Proceedings of the Workshop on Text
Summarization Branches Out (WAS 2004).
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. In MSR-TR-2005-101.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. J. Royal. Statist. Soc B., 58(1):267?288.
K. Toutonova, C. Brockett, J. Jagarlamudi, H. Suzuko,
and L. Vanderwende. 2007. The PYTHY Summa-
rization System: Microsoft Research at DUC2007. In
Proc. of DUC 2007, Rochester, USA.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft Research at DUC 2006: Task-focused summa-
rization with sentence simplification and lexical ex-
pansion. In Proc. of DUC 2006, New York, USA.
6Cf. Gupta et al (2007) who come to a similar conclusion
by comparing between word frequency and log-likelihood ratio.
208
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 15: TempEval Temporal Relation Identification
Marc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,
Graham Katz? and James Pustejovsky?
? Brandeis University, {marc,jamesp}@cs.brandeis.edu
? University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
? Thomson Legal & Regulatory, frank.schilder@thomson.com,
? Stanford University, egkatz@stanford.edu
Abstract
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
1 Introduction
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al, 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al, 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al, 2006; Boguraev et al, forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
2 Task Description
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks ? A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
75
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified ?
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
3 Data Description and Data Preparation
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
? TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
? EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
? TLINK. This is a simplified version of the
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen?s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
76
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
4 Evaluating Temporal Relations
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
Precision = Rc/R
Recall = Rc/K
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
Precision = Rcw/R
Recall = Rcw/K
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
Table 1: Evaluation weights
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
77
5 Participants
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs ? a TIMEX3 and an EVENT
? are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI?s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
6 Results
The results for the six teams are presented in tables
2, 3, and 4.
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
Table 2: Results for Task A
78
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
Table 3: Results for Task B
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
Table 4: Results for Task C
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI?s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
elements that needed to be linked for the TempEval
task.
7 Conclusion: the Future of Temporal
Evaluation
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ?...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...?. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
79
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
8 Acknowledgements
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Saur??, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
Hageg`e and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus?cas?u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, BenWellner, Marc Verhagen, ChongMin
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
80
From Temporal Expressions to Temporal Information:
Semantic Tagging of News Messages
Frank Schilder and Christopher Habel
Department for Informatics
University of Hamburg
Vogt-Ko?lln-Str. 30
22527 Hamburg
Germany
 
schilder habel  @informatik.uni-hamburg.de
Abstract
We present a semantic tagging system
for temporal expressions and discuss
how the temporal information conveyed
by these expressions can be extracted.
The performance of the system was
evaluated wrt. a small hand-annotated
corpus of news messages.
1 Introduction
This paper describes a semantic tagging sys-
tem that extracts temporal information from news
messages. Temporal expressions are defined for
this system as chunks of text that express some
sort of direct or inferred temporal information.
The set of these expressions investigated in the
present paper includes dates (e.g. 08.04.2001),
prepositional phrases (PPs) containing some time
expression (e.g. on Friday), and verbs referring
to a situation (e.g. opened). Related work by
Mani and Wilson (2000) focuses only on the core
temporal expressions neglecting the temporal in-
formation conveyed by prepositions (e.g. Friday
vs. by Friday).
The main part of the system is a temporal ex-
pression tagger that employs finite state trans-
ducers based on hand-written rules. The tag-
ger was trained on economic news articles ob-
tained from two German news papers and an on-
line news agency (Financial Times Deutschland,
die tageszeitung and www.comdirect.de).
Based on the syntactic classification of tempo-
ral expressions a semantic representation of the
extracted chunks is proposed. A clear-cut distinc-
tion between the syntactic tagging process and the
semantic interpretation is maintained. The advan-
tage of this approach is that a second level is cre-
ated that represents the meaning of the extracted
chunks. Having defined the semantic represen-
tation of the temporal expressions, further infer-
ences, in particular on temporal relations, can be
drawn. Establishing the temporal relations be-
tween all events mentioned by a news article is the
ultimate goal of this enterprise. However, at the
current stage of this work the semantic analysis is
still in progress. For the time being, we focus on
the anchoring of the temporal expressions in the
absolute time line and present an already substan-
tial subset of a full semantics that will eventually
cover the entire set of temporal expressions ex-
tracted.
Finally, the evaluation of the temporal expres-
sion tagger provides precision and recall rates for
tagging temporal expressions and drawing tempo-
ral inferences.
2 Representing time in news articles
Since we focus on a particular text domain
(i.e. news articles), the classification of temporal
expressions can be kept to a manageable set of
classes.
2.1 Classification of temporal expressions
The main distinction we make is between time-
denoting and event-denoting expressions. The
first group comprises chunks expressing temporal
information that can be stated with reference to a
calendar or clock system. Syntactically speaking,
these expressions are mainly expressed by prepo-
sitional, adverbial or noun phrases (e.g. on Friday
or today or the fourth quarter).
The second group, event-denoting expressions,
refers to events. These expressions have an im-
plicit temporal dimension, since all situations
possess a temporal component. For these expres-
sions, however, there is no direct or indirect link
to the calendar or clock system. These expres-
sions are verb or noun phrases (e.g. increased or
the election).
2.1.1 Time-denoting expressions
Temporal reference can be expressed in three
different ways:
Explicit reference. Date expressions such as
08.04.2001 refer explicitly to entries of a calen-
dar system. Also time expressions such as 3 p.m.
or Midnight denote a precise moment in our tem-
poral representation system.
Indexical reference. All temporal expressions
that can only be evaluated via a given index time
are called indexical. Expressions such as today,
by last week or next Saturday need to be evaluated
wrt. the article?s time stamp.
Vague reference. Some temporal expressions
express only vague temporal information and it
is rather difficult to precisely place the informa-
tion expressed on a time line. Expressions such
as in several weeks, in the evening or by Saturday
the latest cannot be represented by points or exact
intervals in time.
For the given domain of news article, the ex-
traction of a time stamp for the given article is
very important. This time stamp represents the
production time of the news information and is
used by the other temporal expressions as an in-
dex time to compute the correct temporal mean-
ing of the expression. Note that an explicit date
expression such as 24.12. can only be evaluated
wrt. the year that the article was written. This
means that even an explicit temporal expression
can contain some degree of indexicality.
2.1.2 Event-denoting expressions
Two types of event-denoting expressions have
to be distinguished, on the one hand, sentences,
and, on the other, specific noun phrases. In the
former case, the verb is the lexical bearer of in-
formation about the event in question, in the lat-
ter case, specific nouns, especially those created
by nominalisation, refer to an event.
Since temporal information is the topic of the
system described in this paper, only a subset
of event-denoting nouns have to be considered.
These expressions ? as election in the phrase af-
ter the election ? which serve as temporal ref-
erence pointers in building the temporal structure
of a news, can be marked by a specific attribute in
their lexical entry. Furthermore, in the text classes
we have investigated, there is a small number of
event nouns, which are used as domain dependent
pointers to elements of temporal structures. For
the domain of business and stock market news,
phrases such as opening of the stock exchange,
opening bell, or the close are examples of domain
specific event expressions.
2.2 Representation of temporal information:
the time domain
The primary purpose of the present paper is to
anchor the temporal information obtained from
natural language expressions in news messages
in absolute time, i.e. in a linearly ordered set of
abstract time-entities, which we call time-set in
the following. One of the major tasks in this an-
choring process is to augment the temporal in-
formation in case of indexical and vague tempo-
ral descriptions (see section 4.3 for more details).
Since these expressions do not specify an individ-
ual time-entity of the time-set, it is necessary to
add temporal information until the temporal en-
tity build up from natural language is fully speci-
fied, i.e. can be anchored in the time-set.
2.2.1 The granular system of temporal
entities
The temporal information obtained from news
messages is organised in a granular system of
temporal entities including such granularity lev-
els as GL-day, GL-week, GL-month and GL-
year.1 Individual days are anchored by a
1In the present paper we focus on the conception of gran-
ularity level in semantic and pragmatic inferences. There-
fore, we do not discuss the formal notions of granular sys-
tems for temporal entities here. Compare, e.g. Bettini et
al. (2000), for a framework of temporal granularity, which
could be used for the purposes we discuss here.
date, e.g. date(2001,3,23), on the time line,
i.e. the time-set. Further information, for exam-
ple, the day of the week, can also be included
by an additional slot of the time entity: time
= [?Fri?, date(2001,3,23)]. Time en-
tities of coarser granularity levels, e.g. weeks, are
represented on the basis of intervals, which can be
determined by a start, that is an entity of GL-day,
and a specific duration: time = [?Mon?,
date(2001,4,2), ?7 days? ]. 2
The concept of temporal granularity is reflected
linguistically, for example, in the use of demon-
stratives as determiners of time expressions in
German: dieser Freitag (?this Friday?) refers to
that Friday which is located in the current week
(i.e. the time entity of the next coarser level of
temporal granularity). The same phenomenon
holds with dieser Monatserste (?this first day of
the month?)
In the following we will apply the granular-
ity structure of temporal expressions only with
respect to the finer than - coarser than relation
between levels of granularity, which is differ-
ent from the is part of relation between tempo-
ral entities. For example, whereas between days
and weeks there is a unique functional relation-
ship, namely that there is exactly one week (as
standard calendar unit) that an individual day is
a part of, a week can temporally overlap with
one or two months (Technically, overlap can
be realized by temporal relations of Allen-style;
see Allen (1983)). Nevertheless, GL-week
finer than GL-month holds in the granular-
ity system.3
2Whether the GL-week information remains implicit,
i.e. is inferable from duration, or is made explicit, i.e. coded
by a GL-week-stamp, depends on some design decisions
dependent on the conceptual richness of domain modelling.
For example, in a standardised world of ISO-weeks, which
start on Monday, only, it is not necessary to use GL-week-
stamps. On the other hand, if ISO-weeks, and business
weeks?of five-day length? are conceptual alternatives,
then it is appropriate to use explicit granularity-level stamps.
3The phenomena of overlapping temporal entities of dif-
ferent granularity systems, for example the system of calen-
dar time-entities vs. the system of business time-entities, or
the astronomical system of seasons of the year vs. the me-
teorological seasons of the year are especially relevant for
processing vague and ambiguous temporal expressions. Due
to the temporal and spatial limitations of this paper, we can
not go into the details here.
2.2.2 Definition of temporal relations
Temporal relations are explicitely marked
by temporal prepositions (e.g. before, on or
by). We use the following seven tempo-
ral relation: before, after, incl, at,
starts, finishes, excl. The preposi-
tion on as in on Friday, for instance, denotes the
inclusion relation incl, whereas the preposition
by as in by Friday is represented as finishes.
Note that the seven temporal relations em-
ployed by the current version are equivalent to
sets of Allen?s interval relations (Allen, 1983).4
before 	

after 		

incl 

at Proceedings of the ACL 2010 System Demonstrations, pages 54?59,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Hunting for the Black Swan: Risk Mining from Text
Jochen L. Leidner and Frank Schilder
Thomson Reuters Corporation
Research & Development
610 Opperman Drive, St. Paul, MN 55123 USA
FirstName.LastName@ThomsonReuters.com
Abstract
In the business world, analyzing and dealing with
risk permeates all decisions and actions. However,
to date, risk identification, the first step in the risk
management cycle, has always been a manual activ-
ity with little to no intelligent software tool support.
In addition, although companies are required to list
risks to their business in their annual SEC filings
in the USA, these descriptions are often very high-
level and vague.
In this paper, we introduce Risk Mining, which is
the task of identifying a set of risks pertaining to a
business area or entity. We argue that by combining
Web mining and Information Extraction (IE) tech-
niques, risks can be detected automatically before
they materialize, thus providing valuable business
intelligence.
We describe a system that induces a risk taxonomy
with concrete risks (e.g., interest rate changes) at its
leaves and more abstract risks (e.g., financial risks)
closer to its root node. The taxonomy is induced
via a bootstrapping algorithms starting with a few
seeds. The risk taxonomy is used by the system as
input to a risk monitor that matches risk mentions in
financial documents to the abstract risk types, thus
bridging a lexical gap. Our system is able to au-
tomatically generate company specific ?risk maps?,
which we demonstrate for a corpus of earnings re-
port conference calls.
1 Introduction
Any given human activity with a particular in-
tended outcome is bound to face a non-zero like-
lihood of failure. In business, companies are ex-
posed to market risks such as new competitors,
disruptive technologies, change in customer at-
titudes, or a changes in government legislation
that can dramatically affect their profitability or
threaten their business model or mode of opera-
tion. Therefore, any tool to assist in the elicita-
tion of otherwise unforeseen risk factors carries
tremendous potential value.
However, it is very hard to identify risks ex-
haustively, and some types (commonly referred
to as the unknown unknowns) are especially elu-
sive: if a known unknown is the established knowl-
edge that important risk factors are known, but it is
unclear whether and when they become realized,
then an unknown unknown is the lack of aware-
ness, in practice or in principle, of circumstances
that may impact the outcome of a project, for ex-
ample. Nassim Nicholas Taleb calls these ?black
swans? (Taleb, 2007).
Companies in the US are required to disclose
a list of potential risks in their annual Form 10-K
SEC fillings in order to warn (potential) investors,
and risks are frequently the topic of conference
phone calls about a company?s earnings. These
risks are often reported in general terms, in par-
ticular, because it is quite difficult to pinpoint the
unknown unknown, i.e. what kind of risk is con-
cretely going to materialize. On the other hand,
there is a stream of valuable evidence available on
the Web, such as news messages, blog entries, and
analysts? reports talking about companies? perfor-
mance and products. Financial analysts and risk
officers in large companies have not enjoyed any
text analytics support so far, and risk lists devised
using questionnaires or interviews are unlikely to
be exhaustive due to small sample size, a gap
which we aim to address in this paper.
To this end, we propose to use a combination
of Web Mining (WM) and Information Eextrac-
tion (IE) to assist humans interested in risk (with
respect to an organization) and to bridge the gap
between the general language and concrete risks.
We describe our system, which is divided in two
main parts: (a) an offline Risk Miner that facili-
tates the risk identification step of the risk manage-
ment process, and an online (b)RiskMonitor that
supports the risk monitoring step (cf. Figure 2). In
addition, a Risk Mapper can aggregate and visu-
alize the evidence in the form of a risk map. Our
risk mining algorithm combines Riloff hyponym
patterns with recursive Web pattern bootstrapping
and a graph representation.
We do not know of any other implemented end-
to-end system for computer-assisted risk identifi-
cation/visualization using text mining technology.
54
2 Related Work
Financial IE. IE systems have been applied to the
financial domain on Message Understanding Con-
test (MUC) like tasks, ranging from named en-
tity tagging to slot filling in templates (Costantino,
1992).
Automatic Knowledge Acquisition. (Hearst,
1992) pioneered the pattern-based extraction of
hyponyms from corpora, which laid the ground-
work for subsequent work, and which included ex-
traction of knowledge from to the Web (e.g. (Et-
zioni et al, 2004)). To improve precision was the
mission of (Kozareva et al, 2008), which was de-
signed to extract hyponymy, but they did so at the
expense of recall, using longer dual anchored pat-
terns and a pattern linkage graph. However, their
method is by its very nature unable to deal with
low-frequency items, and their system does not
contain a chunker, so only single term items can
be extracted. De Saenger et al (De Saeger et al,
2008) describe an approach that extracts instances
of the ?trouble? or ?obstacle? relations from the
Web in the form of pairs of fillers for these bi-
nary relations. Their approach, which is described
for the Japanese language, uses support vector ma-
chine learning and relies on a Japanese syntac-
tic parser, which permits them to process nega-
tion. In contrast, and unlike their method, we pur-
sue a more general, open-ended search process,
which does not impose as much a priori knowl-
edge. Also, they create a set of pairs, whereas our
approach creates a taxonomy tree as output. Most
importantly though, our approach is not driven by
frequency, and was instead designed to work es-
pecially with rare occurrences in mind to permit
?black swan?-type risk discovery.
Correlation of Volatility and Text. (Kogan et al,
2009) study the correlation between share price
volatility, a proxy for risk, and a set of trigger
words occurring in 60,000 SEC 10-K filings from
1995-2006. Since the disclosure of a company?s
risks is mandatory by law, SEC reports provide
a rich source. Their trigger words are selected a
priori by humans; in contrast, risk mining as ex-
ercised in this paper aims to find risk-indicative
words and phrases automatically.
Kogan and colleagues attempt to find a regres-
sion model using very simple unigram features
based on whole documents that predicts volatility,
whereas our goal is to automatically extract pat-
terns to be used as alerts.
Speculative Language & NLP. Light et al (Light
et al, 2004) found that sub-string matching of
14 pre-defined string literals outperforms an SVM
classifier using bag-of-words features in the task
of speculative language detection in medical ab-
stracts. (Goldberg et al, 2009) are concerned with
automatic recognition of human wishes, as ex-
pressed in human notes for Year?s Eve. They use a
bi-partite graph-based approach, where one kind
of node (content node) represents things people
wish for (?world peace?) and the other kind of
node (template nodes) represent templates that ex-
tract them (e.g. ?I wish for ___?). Wishes
can be seen as positive Q, in our formalization.
3 Data
We apply the mined risk extraction patterns to a
corpus of financial documents. The data originates
from the StreetEvents database and was kindly
provided to us by Starmine, a Thomson Reuters
company. In particular, we are dealing with 170k
earning calls transcripts, a text type that contains
monologue (company executives reporting about
their company?s performance and general situa-
tion) as well as dialogue (in the form of ques-
tions and answers at the end of each conference
call). Participants typically include select business
analysts from investment banks, and the calls are
published afterwards for the shareholders? bene-
fits. Figure 1 shows some example excerpts. We
randomly took a sample of N=6,185 transcripts to
use them in our risk alerting experiments.1
4 Method
4.1 System
The overall system is divided into two core parts:
(a) Risk Mining and (b) Risk Monitoring (cf. Fig-
ure 2). For demonstration purposes, we add a (c)
Risk Mapper, a visualization component. We de-
scribe how a variety of risks can be identified given
a normally very high-level description of risks,
as one can find in earnings reports, other finan-
cial news, or the risk section of 10-K SEC filings.
Starting with rather abstract descriptions such as
operational risks and hyponym-inducing pattern
"< RISK > such as *", we use the Web to
mine pages from which we can harvest additional,
1We could also use this data for risk mining, but did not
try this due to the small size of the dataset compared to the
Web.
55
CEO: As announced last evening, during our third quarter, we will take the difficult but necessary step to seize [cease]
manufacturing at our nearly 100 year old Pennsylvania House casegood plant in Lewisburg, Pennsylvania as well as the nearby
Pennsylvania House dining room chair assembly facility in White Deer. Also, the three Lewisburg area warehouses will be
consolidated as we assess the logistical needs of the casegood group?s existing warehouse operations at an appropriate time in the
future to minimize any disruption of service to our customers. This will result in the loss of 425 jobs or approximately 15% of the
casegood group?s current employee base.
Analyst: Okay, so your comments ? and I guess I don?t know ? I can figure out, as you correctly helped me through, what
dollar contribution at GE. I don?t know the net equipment sales number last quarter and this quarter. But it sounded like from
your comments that if you exclude these fees, that equipment sales were probably flattish. Is that fair to say?
CEO: We?re not breaking out the origination fee from the equipment fee, but I think in total, I would say flattish to slightly up.
Figure 1: Example sentences from the earnings conference call dataset. Top: main part. Bottom: Q&A.
and eventually more concrete, candidates, and re-
late them to risk types via a transitive chain of bi-
nary IS-A relations. Contrary to the related work,
we use a base NP chunker and download the full
pages returned by the search engine rather than
search snippets in order to be able to extract risk
phrases rather than just terms, which reduces con-
textual ambiguity and thus increases overall preci-
sion. The taxonomy learning method described in
the following subsection determines a risk taxon-
omy and new risks patterns.
Web Miner
Taxonomy
Inducer
Seed Patterns
"* <RISK> such 
as *"
Search Engine Web Pages
Business 
Reports
Risk Alerting
Notification
Risk
Taxonomy
Risk Mining
for
Risk Identification
Information Extraction
for
Risk Monitoring
Figure 2: The risk mining and monitoring system
architecture
The second part of the system, the Risk Mon-
itor, takes the risks from the risk taxonomy and
uses them for monitoring financial text streams
such as news, SEC filings, or (in our use case)
earnings reports. Using this, an analyst is then able
to identify concrete risks in news messages and
link them to the high-level risk descriptions. He
or she may want to identify operational risks such
as fraud for a particular company, for instance.
The risk taxonomy can also derive further risks
in this category (e.g., faulty components, brakes)
for exploration and drill-down analysis. Thus,
news reports about faulty breaks in (e.g. Toyota)
or volcano outbreaks (e.g. Iceland) can be directly
linked to the risk as stated in earnings reports or
security filings.
Our Risk Miner and Risk Monitor are imple-
mented in Perl, with the graph processing of the
taxonomy implemented in SWI-Prolog, whereas
the Risk Mapper exists in two versions, a static
image generator for R2 and, alternatively, an in-
teractive Web page (DHTML, JavaScript, and us-
ing Google?s Chart API). We use the Yahoo Web
search API.
4.2 Taxonomy induction method
Using frequency to compute confidence in a pat-
tern does not work for risk mining, however, be-
cause mention of particular risks might be rare. In-
stead of frequency based indicators (n-grams, fre-
quency weights), we rely on two types of struc-
tural confidence validation, namely (a) previously
identified risks and (b) previously acquired struc-
tural patterns. Note, however, that we can still use
PageRank, a popularity-based graph algorithm,
because multiple patterns might be connected to
a risk term or phrase, even in the absence of fre-
quency counts for each (i.e., we interpret popular-
ity as having multiple sources of support).
1. Risk Candidate Extraction Step. The first
step is used to extract a list of risks based on high
precision patterns. However, it has been shown
that the use of such patterns (e.g., such as) quickly
lead to an decrease in precision. Ideally, we want
to retrieve specific risks by re-applying the the ex-
tract risk descriptions:
2http://www.r-project.org
56
Figure 3: A sample IS-A and Pattern network with
sample PageRank scores.
(a) Take a seed, instantiate "< SEED > such
as *" pattern with seed, extract candidates:
Input: risks
Method: apply pattern "< SEED > such
as < INSTANCE > ", where
< SEED > = risks
Output: list of instances (e.g., faulty compo-
nents)
(b) For each candidate from the list of instances,
we find a set of additional candidate hy-
ponyms.
Input: faulty components
Method: apply pattern "< SEED > such
as < INSTANCE > ", where
< SEED > = faulty components
Output: list of instances (e.g., brake)
2. Risk Validation. Since the Risk Candidate
extraction step will also find many false positives,
we need to factor in information that validates that
the extracted risk is indeed a risk. We do this by
constructing a possible pattern containing this new
risk.
(a) Append "* risks" to the output of 1(b) in
order to make sure that the candidate occurs
in a risk context.
Input: brake(s)
Pattern: "brake(s) * risk(s)"
Output: a list of patterns (e.g., minimize
such risks, raising the risk)
(b) extract new risk pattern by substituting the
risk candidate with < RISK > ; creating a
limited number of variations
Input: list of all patterns mined from step 2
(a)
Method: create more pattern variations,
such as "< RISK > minimize such
risks", "raising the risk
of < RISK > " etc.
Output: list of new potential risks (e.g., de-
flation), but also many false positives
(e.g., way, as in The best way to mini-
mize such risks).
In order to benefit from any human observations
of system errors in future runs, we also extended
the system so as to read in a partial list of pre-
defined risks at startup time, which can guide the
risk miner; while technically different from active
learning, this approach was somewhat inspired by
it (but our feedback is more loose).
3. Constructing Risk Graph. We have now
reached the point where we constructed a graph
with risks and patterns. Risks are connected via
IS-A links; risks and patterns are connected via
PATTERN links. Note that there are links from
risks to patterns and from patterns to risks; some
risks back-pointed by a pattern may actually not
be a risk (e.g., people). However, this node is also
not connected to a more abstract risk node and
will therefore have a low PageRank score. Risks
that are connected to patterns that have a high au-
thority (i.e., pointing to by many other links) are
highly ranked within PageRank (Figure 3). The
risk black Swan, for example, has only one pat-
tern it occurs in, but this pattern can be filled by
many other risks (e.g., fire, regulations). Hence,
the PageRank score of the black swan is high sim-
ilar to well known risks, such as fraud.
4.3 Risk alerting method
We compile the risk taxonomy into a trie automa-
ton, and create a second trie for company names
from the meta-data of our corpus. The Risk Mon-
itor reads the two tries and uses the first to de-
tect mentions of risks in the earning reports and
the second one to tag company names, both using
case-insensitive matching for better recall. Op-
tionally, we can use Porter stemming during trie
construction and matching to trade precision for
even higher recall, but in the experiments reported
here this is not used. Once a signal term or phrase
matches, we look up its risk type in a hash table,
take a note of the company that the current earn-
ings report is about, and increase the frequency
57
liquidity IS-A financial risks
credit IS-A financial risks
direct risks IS-A financial risks
fraud IS-A financial risks
irregular activity IS-A operational risks
process failure IS-A operational risks
human error IS-A operational risks
labor strikes IS-A operational risks
customer acceptance IS-A IT market risks
interest rate changes IS-A capital market risks
uncertainty IS-A market risks
volatility IS-A mean reverting market risks
copyright infringement IS-A legal risks
negligence IS-A other legal risks
an unfair dismissal IS-A the legal risks
Sarbanes IS-A legal risks
government changes IS-A global political risks
crime IS-A Social and political risks
state intervention IS-A political risks
terrorist acts IS-A geopolitical risks
earthquakes IS-A natural disaster risks
floods IS-A natural disaster risks
global climate change IS-A environmental risks
severe and extreme weather IS-A environmental risks
internal cracking IS-A any technological risks
GM technologies IS-A tech risks
scalability issues IS-A technology risks
viruses IS-A the technical risks
Figure 4: Selected financial risk tuples after Web
validation.
count for this ?company; risk type? tuple, which
we use for graphic rendering purposes.
4.4 Risk mapping method
To demonstrate the method presented here, we cre-
ated a visualization that displays a risk map, which
is a two dimensional table showing companies and
the types of risk they are facing, together with bub-
ble sizes proportional to the number of alerts that
the RiskMonitor could discover in the corpus. The
second option also permits the user to explore the
detected risk mentions per company and by risk
type.
5 Results
From the Web mining process, we obtain a set
of pairs (Figure 4), from which the taxonomy is
constructed. In one run with only 12 seeds (just
the risk type names with variants), we obtained a
taxonomy with 280 validated leave nodes that are
connected transitively to the risks root node.
Our resulting system produces visualizations
we call ?risk maps?, because they graphically
present the extracted risk types in aggregated
form. A set of risk types can be selected for pre-
sentation as well as a set of companies of interest.
A risk map display is then generated using either
R (Figure 5) or an interactive Web page, depend-
ing on the user?s preference.
Qualitative error analysis. We inspected the
output of the risk miner and observed the follow-
Figure 5: An Example Risk Map.
ing classes of issues: (a) chunker errors: if phrasal
boundaries are placed at the wrong position, the
taxonomy will include wrong relations. For exam-
ple, deictic determiners such as ?this? were a prob-
lem (e.g. that IS-A indirect risks) be-
fore we introduced a stop word filter that discards
candidate tuples that contain no content words.
Another prominent example is ?short term? in-
stead of the correct ?short term risk?; (b) seman-
tic drift3: due to polysemy, words and phrases
can denote risk and non-risk meanings, depend-
ing on context. Talking about risks even a spe-
cific pattern such as ?such as? [sic] is used by au-
thors to induce a variety of perspectives on the
topic of risk, and after several iterations negative
effects of type (a) errors compound; (c) off-topic
relations: the seeds are designed to induce a tax-
onomy specific to risk types. As a side effect,
many (correct or incorrect) irrelevant relations
are learned, e.g. credit and debit cards
is-a money transfer. We currently dis-
card these by virtue of ignoring all relations not
transitively connected with the root node risks,
so no formalized domain knowledge is required;
(d) overlap: the concept space is divided up dif-
ferently by different writers, both on the Web
and in the risk management literature, and this
is reflected by multiple category membership of
many risks (e.g. is cash flow primarily an oper-
ational risk or a financial risk?). Currently, we
do not deal with this phenomenon automatically;
(e) redundant relations: at the time of writing, we
do not cache all already extracted and validated
risks/non-risks. This means there is room for im-
provement w.r.t. runtime, because we make more
Web queries than strictly necessary. While we
have not evaluated this system yet, we found by in-
3to use a term coined by Andy Lauriston
58
specting the output that our method is particularly
effective for learning natural disasters and med-
ical conditions, probably because they are well-
covered by news sites and biomedical abstracts on
the Web. We also found that some classes contain
more noise than others, for example operational
risk was less precise than financial risk, proba-
bly due to the lesser specificity of the former risk
type.
6 Summary, Conclusions & Future Work
Summary of Contributions.
In this paper, we introduced the task of risk min-
ing, which produces patterns that are useful in an-
other task, risk alerting. Both tasks provide com-
putational assistance to risk-related decision mak-
ing in the financial sector. We described a special-
purpose algorithm for inducing a risk taxonomy
offline, which can then be used online to analyze
earning reports in order to signal risks. In do-
ing so, we have addressed two research questions
of general relevance, namely how to extract rare
patterns, for which frequency-based methods fail,
and how to use the Web to bridge the vocabulary
gap, i.e. how to match up terms and phrases in
financial news prose with the more abstract lan-
guage typically used in talking about risk in gen-
eral.
We have described an implemented demonstrator
system comprising an offline risk taxonomyminer,
an online risk alerter and a visualization compo-
nent that creates visual risk maps by company and
risk type, which we have applied to a corpus of
earnings call transcripts.
Future Work. Extracted negative and also pos-
itive risks can be used in many applications, rang-
ing from e-mail alerts to determinating credit rat-
ings. Our preliminary work on risk maps can be
put on a more theoretical footing (Hunter, 2000).
After studying further how output of risk alert-
ing correlates4 with non-textual signals like share
price, risk detection signals could inform human
or trading decisions.
Acknowledgments. We are grateful to Khalid Al-Kofahi,
Peter Jackson and James Powell for supporting this work.
Thanks to George Bonne, Ryan Roser, and Craig D?Alessio
at Starmine, a Thomson Reuters company, for sharing the
StreetEvents dataset with us, and to David Rosenblatt for dis-
cussions and to Jack Conrad for feedback on this paper.
4Our hypothesis is that risk patterns can outperform bag
of words (Kogan et al, 2009).
References
Marco Costantino. 1992. Financial information extrac-
tion using pre-defined and user-definable templates in the
LOLITA system. Proceedings of the Fifteenth Interna-
tional Conference on Computational Linguistics (COL-
ING 1992), vol. 4, pages 241?255.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi Kazama.
2008. Looking for trouble. In Proceedings of the 22nd
International Conference on Computational Linguistics
(COLING 2008), pages 185?192, Morristown, NJ, USA.
Association for Computational Linguistics.
Oren Etzioni, Michael J. Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2004. Web-scale
information extraction in KnowItAll: preliminary results.
In Stuart I. Feldman, Mike Uretsky, Marc Najork, and
Craig E. Wills, editors, Proceedings of the 13th interna-
tional conference on World Wide Web (WWW 2004), New
York, NY, USA, May 17-20, 2004, pages 100?110. ACM.
Andrew B. Goldberg, Nathanael Fillmore, David Andrzejew-
ski, Zhiting Xu, Bryan Gibson, and Xiaojin Zhu. 2009.
May all your wishes come true: A study of wishes and
how to recognize them. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Compu-
tational Linguistics, pages 263?271, Boulder, Colorado,
June. Association for Computational Linguistics.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the Fourteenth
International Conference on Computational Linguistics
(COLING 1992).
Anthony Hunter. 2000. Ramification analysis using causal
mapping. Data and Knowledge Engineering, 32:200?227.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Jacob S.
Sagi, and Noah A. Smith. 2009. Predicting risk from
financial reports with regression. In Proceedings of the
Joint International Conference on Human Language Tech-
nology and the Annual Meeting of the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL).
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings of ACL-HLT, pages
1048?1056, Columbus, OH, USA. Association for Com-
putational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and state-
ments in between. In BioLINK 2004: Linking Biological
Literature, Ontologies and Databases, pages 17?24. ACL.
Nassim Nicholas Taleb. 2007. The Black Swan: The Impact
of the Highly Improbable. Random House.
59
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1406?1415,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Statistical NLG Framework for Aggregated Planning and Realization
Ravi Kondadadi?, Blake Howald and Frank Schilder
Thomson Reuters, Research & Development
610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com
Abstract
We present a hybrid natural language gen-
eration (NLG) system that consolidates
macro and micro planning and surface re-
alization tasks into one statistical learn-
ing process. Our novel approach is based
on deriving a template bank automatically
from a corpus of texts from a target do-
main. First, we identify domain specific
entity tags and Discourse Representation
Structures on a per sentence basis. Each
sentence is then organized into semanti-
cally similar groups (representing a do-
main specific concept) by k-means cluster-
ing. After this semi-automatic processing
(human review of cluster assignments), a
number of corpus?level statistics are com-
piled and used as features by a ranking
SVM to develop model weights from a
training corpus. At generation time, a set
of input data, the collection of semanti-
cally organized templates, and the model
weights are used to select optimal tem-
plates. Our system is evaluated with au-
tomatic, non?expert crowdsourced and ex-
pert evaluation metrics. We also introduce
a novel automatic metric ? syntactic vari-
ability ? that represents linguistic variation
as a measure of unique template sequences
across a collection of automatically gener-
ated documents. The metrics for generated
weather and biography texts fall within ac-
ceptable ranges. In sum, we argue that our
statistical approach to NLG reduces the
need for complicated knowledge-based ar-
chitectures and readily adapts to different
domains with reduced development time.
?*Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
1 Introduction
NLG is the process of generating natural-sounding
text from non-linguistic inputs. A typical NLG
system contains three main components: (1) Doc-
ument (Macro) Planning - deciding what content
should be realized in the output and how it should
be structured; (2) Sentence (Micro) planning -
generating a detailed sentence specification and
selecting appropriate referring expressions; and
(3) Surface Realization - generating the final text
after applying morphological modifications based
on syntactic rules (see e.g., Bateman and Zock
(2003), Reiter and Dale (2000) and McKeown
(1985)). However, document planning is arguably
one of the most crucial components of an NLG
system and is responsible for making the texts ex-
press the desired communicative goal in a coher-
ent structure. If the document planning stage fails,
the communicative goal of the generated text will
not be met even if the other two stages are perfect.
While most traditional systems simplify develop-
ment by using a pipelined approach where (1-3)
are executed in a sequence, this can result in er-
rors at one stage propagating to successive stages
(see e.g., Robin and McKeown (1996)). We pro-
pose a hybrid framework that combines (1-3) by
converting data to text in one single process.
Most NLG systems fall into two broad
categories: knowledge-based and statistical.
Knowledge-based systems heavily depend on hav-
ing domain expertise to come up with hand-
crafted rules at each stage of a pipeline. Although
knowledge-based systems can produce high qual-
ity text, they are (1) very expensive to build, in-
volving a lot of discussion with the end users of the
system for the document planning stage alone; (2)
have limited linguistic coverage, as it is time con-
suming to capture linguistic variation; and (3) one
has to start from scratch for each new domain be-
cause the developed components cannot be reused.
1406
Statistical systems, on the other hand, are fairly
inexpensive, more adaptable and rely on having
historical data for the given domain. Coverage is
likely to be high if more historical data is avail-
able. The main disadvantage with statistical sys-
tems is that they are more prone to errors and the
output text may not be coherent as there are less
constraints on the generated text.
Our framework is a hybrid of statistical and
template-based systems. Many knowledge-based
systems use templates to generate text. A tem-
plate structure contains ?gaps? that are filled to
generate the output. The idea is to create a lot
of templates from the historical data and select
the right template based on some constraints. To
the best of our knowledge, this is the first hy-
brid statistical-template-based system that com-
bines all three stages of NLG. Experiments with
different variants of our system (for biography and
weather subject matter domains) demonstrate that
our system generates reasonable texts.
Also, in addition to the standard metrics used
to evaluate NLG systems (e.g., BLEU, NIST, etc.),
we present a unique text evaluation metric called
syntactic variability to measure the linguistic vari-
ation of generated texts. This metric applies to the
document collection level and is based on com-
puting the number of unique template sequences
among all the generated texts. A higher number
indicates the texts are more variable and natural-
sounding whereas a lower number shows they are
more redundant. We argue that this metric is use-
ful for evaluating template-based systems and for
any type of text generation for domains where lin-
guistic variability is favored (e.g., the user is ex-
pected to go through more than one document in
the same session).
The main contributions of this paper are (1) A
statistical NLG system that combines document
and sentence planning and surface realization into
one single process; and (2) A new metric ? syntac-
tic variability ? is proposed to measure the syntac-
tic and morphological variability of the generated
texts. We believe this is the first work to propose
an automatic metric to measure linguistic variabil-
ity of generated texts in NLG.
Section 2 provides an overview of related work
on NLG. We present our main system in Section 3.
The system is evaluated and discussed in Section
4. Finally, we conclude in Section 5 and point out
future directions of research.
2 Background
Typically, knowledge-based NLG systems are im-
plemented by rules and, as mentioned above, have
a pipelined architecture for the document and
sentence planning stages and surface realization
(Hovy, 1993; Moore and Paris, 1993). However,
document planning is arguably the most impor-
tant task (Sripada et al, 2001). It follows that ap-
proaches to document planning are rule-based as
well and, concomitantly, are usually domain spe-
cific. For example, Bouayad-Agha, et al (2011)
proposed document planning based on an ontol-
ogy knowledge base to generate football sum-
maries. For rule?based systems, rules exist for
selecting content to grammatical choices to post-
processing (e.g., pronoun generation). These rules
are often tailored to a given system, with input
from multiple experts; consequently, there is a
high associated development cost (e.g., 12 person
months for the SUMTIME-METEO system (Belz,
2007)).
Statistical approaches can reduce extensive de-
velopment time by relying on corpus data to
?learn? rules for one or more components of an
NLG system (Langkilde and Knight, 1998). For
example, Duboue and McKeown (2003) proposed
a statistical approach to extract content selection
rules for biography descriptions. Further, statisti-
cal approaches should be more adaptable to differ-
ent domains than their rule-based equivalents (An-
geli et al, 2012). For example, Barzilay and Lap-
ata (2005) formulated content selection as a clas-
sification task to produce football summaries and
Kelly et al (2009) extended Barzilay and Lapata?s
approach for generating match reports for cricket.
The present work builds on Howald et al
(2013) where, in a given corpus, a combination of
domain specific named entity tagging and cluster-
ing sentences (based on semantic predicates) were
used to generate templates. However, while the
system consolidated both sentence planning and
surface realization with this approach (described
in more detail in Section 3), the document plan
was given via the input data and sequencing infor-
mation was present in training documents. For the
present research, we introduce a similar method
that leverages the distributions of document?level
features in the training corpus to incorporate a
statistical document planning component. Con-
sequently, we are able to create a streamlined
statistical NLG architecture that balances natural
1407
human?like variability with appropriate and accu-
rate information.
3 Methodology
In order to generate text for a given domain our
system runs input data through a statistical ranking
model to select a sequence of templates that best
fit the input data (E). In order to build the rank-
ing model, our system takes historical data (cor-
pus) for the domain through four components: (A)
preprocessing; (B) ?conceptual unit? creation; (C)
collecting statistics; and (D) ranking model build-
ing (summarized in Figure 1). In this section, we
describe each component in detail.
Figure 1: System Architecture.
3.1 Preprocessing
The first component processes the given corpus to
extract templates. We assume that each document
in the corpus is classified to a specific domain.
Preprocessing involves uncovering the underlying
semantic structure of the corpus and using this as
a foundation for template creation (Lu et al, 2009;
Lu and Ng, 2011; Konstas and Lapata, 2012).
We first split each document in the corpus into
sentences and create a shallow Discourse Repre-
sentation Structure (following Discourse Repre-
sentation Theory (Kamp and Reyle, 1993)) of each
sentence. The DRS consists of semantic predi-
cates and named entity tags. We use Boxer se-
mantic analyzer (Bos, 2008) to extract semantic
predicates such as EVENT or DATE. In parallel,
domain specific named entity tags are identified
and, in conjunction with the semantic predicates,
are used to create templates. We developed the
named-entity tagger for the weather domain our-
selves. To tag entities in the biography domain,
we used OpenCalais (www.opencalais.com). For
example, in the biography in (1), the conceptual
meaning (semantic predicates and domain-specific
entities) of sentences (a-b) are represented in (c-d).
The corresponding templates are showing in (e-f).
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing Di-
rector of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor?s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING | TITLE | PERSON | COMPANY | DATE
d. HOLDS | DEGREE | SUBJECT | INSTITUTION| EVENT
Templates
e. [person] has been serving as [title] of the [company]
since [date].
f. [person] holds a [degree] in [subject] from [institution]
and a [degree] from [institution].
The outputs of the preprocessing stage are the tem-
plate bank and predicate information for each tem-
plate in the corpus.1
3.2 Creating Conceptual Units
The next step is to create conceptual units for the
corpus by clustering templates. This is a semi-
automatic process where we use the predicate in-
formation for each template to compute similar-
ity between templates. We use k-means clustering
with k (equivalent to the number of semantic con-
cepts in the domain) set to an arbitrarily high value
(100) to over-generate (using the WEKA toolkit
(Witten and Frank, 2005)). This allows for easier
manual verification of the generated clusters and
we merge them if necessary. We assign a unique
identifier called a CuId (Conceptual Unit Identi-
fier) to each cluster, which represents a ?concep-
tual unit?. We associate each template in the cor-
pus to a corresponding CuId. For example, in (2),
using the templates in (1e-f), the identified named
entities are assigned to a clustered CuId (2a-b).
(2) Conceptual Units
a. {CuId : 000} ? [person] has been serving as [title] of the
[company] since [date].
b. {CuId : 001} ? [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
At this stage, we will have a set of conceptual
units with corresponding template collections (see
Howald et al (2013) for a further explanation of
Sections 3.1-3.2).
1A similar approach to the clustering of semantic content
is found in Duboue and McKeown (2003), where text with
stopwords removed were used as semantic input. Boxer pro-
vides a similar representation with the addition of domain
general tags. However, to contrast our work from Duboue
and McKeown, which focused on content selection, we are
focused on learning templates from the semantic representa-
tions for the complete generation system (covering content
selection, aggregation, sentence and document planning).
1408
3.3 Collecting Corpus Statistics
After identifying the different conceptual units and
the template bank, we collect a number of statistics
from the corpus:
? Frequency distribution of templates overall and per po-
sition
? Frequency distribution of CuIds overall and per posi-
tion
? Average number of entity tags by CuId as well as the
entity distribution by CuId
? Average number of entity tags by position as well as
the entity distribution by position
? Average number of words per CuId.
? Average number of words per CuId and position com-
bination.
? Average number of words per position
? Frequency distribution of the main verbs by position
? Frequency distribution of CuId sequences (bigrams and
trigrams only) overall and per position
? Frequency distribution of template sequences (bigrams
and trigrams only) overall and per position
? Frequency distribution of entity tag sequences overall
and per position
? The average, minimum, maximum number of CuIds
across all documents
As discussed in the next section, these statistics
are turned into features used for building a ranking
model in the next component.
3.4 Building a ranking model
The core component of our system is a statistical
model that ranks a set of templates for a given
position (sentence 1, sentence 2, ..., sentence n)
based on the input data. The input data in our
tasks was extracted from a training document; this
serves as a temporary surrogate to a database. The
task is to learn the ranks of all the templates from
all CuIds at each position.
To generate the training data, we first filter the
templates that have named entity tags not specified
in the input data. This will make sure the gener-
ated text does not have any unfilled entity tags. We
then rank templates according to the Levenshtein
edit distance (Levenshtein, 1966) from the tem-
plate corresponding to the current sentence in the
training document (using the top 10 ranked tem-
plates in training for ease of processing effort). We
experimented with other ranking schemes such as
entity-based similarity (similarity between entity
sequences in the templates) and a combination of
edit-distance based and entity-based similarities.
We obtained better results with edit distance. For
each template, we generate the following features
to build the ranking model. Most of the features
are based on the corpus statistics mentioned above.
? CuId given position: This is a binary feature where
the current CuId is either the same as the most frequent
CuId for the position (1) or not (0).
? Overlap of named entities: Number of common enti-
ties between current CuId and most likely CuId for the
position
? Prior template: Probability of the sequence of tem-
plates selected at the previous position and the current
template (iterated for the last three positions).
? Prior CuId: Probability of the sequence of the CuId
selected at the previous position and the current CuId
(iterated for the last three positions).
? Difference in number of words: Absolute difference
between number of words for current template and av-
erage number of words for the CuId
? Difference in number of words given position: Ab-
solute difference between number of words for cur-
rent template and average number of words for CuId
at given position
? Percentage of unused data: This feature represents
the portion of the unused input so far.
? Difference in number of named entities: Absolute
difference between the number of named entities in the
current template and the average number of named en-
tities for the current position
? Most frequent verb for the position: Binary valued
feature where the main verb of the template belongs to
the most frequent verb group given the position is either
the same (1) or not (0).
? Average number of words used: Ratio of number of
words in the generated text so far to the average number
of words.
? Average number of entities: Ratio of number of
named entities in the generated text so far to the av-
erage number of named entities.
? Most likely CuId given position and previous CuId:
Binary feature indicating if the current CuId is most
likely given the position and the previous CuId.
? Similarity between the most likely template in CuId
and current template: Edit distance between the cur-
rent template and the most likely template for the cur-
rent CuId.
? Similarity between the most likely template in CuId
given position and current template: Edit distance
between the current template and the most likely tem-
plate for the current CuId at the current position.
We used a linear kernel for a ranking SVM
(Joachims, 2002) (cost set to total queries) to learn
the weights associated with each feature for the
different domains.
3.5 Generation
At generation time, our system has a set of in-
put data, a semantically organized template bank
(collection of templates organized by CuId) and a
model from training on the documents for a given
domain. We first filter out those templates that
contain a named entity tag not present in the in-
put data. Then, we compute a score for each of the
remaining templates from the feature values and
the feature weights from the model. The template
with the highest overall score is selected and filled
with matching entity tags from the input data and
1409
appended to the generated text.
Before generating the next sentence, we track
those entities used in the initial sentence gener-
ation and decide to either remove those entities
from the input data or keep the entity for one or
more additional sentence generations. For exam-
ple, in the biography discourses, the name of the
person may occur only once in the input data, but
it may be useful for creating good texts to have
that person?s name available for subsequent gen-
erations. To illustrate in (3), if we remove James
Smithton from the input data after the initial gen-
eration, an irrelevant sentence (d) is generated as
the input data will only have one company after
the removal of James Smithton and the model will
only select a template with one company. If we
keep James Smithton, then the generations in (a-b)
are more cohesive.
(3) Use more than once
a. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
b. Previously, Mr. Smithton was CFO of the Keyes
Development Group.
Use once and remove
c. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
d. Keyes Development Group is a venture capital firm.
Deciding on what type of entities and how to
remove them is different for each domain. For ex-
ample, some entities are very unique to a text and
should not be made available for subsequent gen-
erations as doing so would lead to unwanted re-
dundancies (e.g., mentioning the name of current
company in a biography discourse more than once
as in (3)) and some entities are general and should
be retained. Our system possesses the ability to
monitor the data usage from historical data and we
can set parameters (based on the distribution of en-
tities) on the usage to ensure coherent generations
for a given domain.
Once the input data has been modified (i.e., an
entity have been removed, replaced or retained),
it serves as the new input data for the next sen-
tence generation. This process repeats until reach-
ing the minimum number of sentences for the do-
main (determined from the training corpus statis-
tic) and then continues until all of the remaining
input data is consumed (and not to exceed the pre-
determined maximum number of sentences, also
determined from the training corpus statistic).
4 Evaluation and Discussion
In this section, we first discuss the corpus data
used to train and generate texts. Then, the re-
sults of both automatic and human evaluations of
our system?s generations against the original and
baseline texts are considered as a means of de-
termining performance. For all experiments re-
ported in this section, the baseline system selects
the most frequent conceptual unit at the given po-
sition, chooses the most likely template for the
conceptual unit, and fills the template with input
data. The above process is repeated until the num-
ber of sentences is less than or equal to the average
number of sentences for the given domain.
4.1 Data
We ran our system on two different domains: cor-
porate officer and director biographies and off-
shore oil rig weather reports from the SUMTIME-
METEO corpus ((Reiter et al, 2005)). The biogra-
phy domain includes 1150 texts ranging from 3-17
sentences and the weather domain includes 1045
weather reports ranging from 1-6 sentences.2 We
used a training-test(generation) split of 70/30.
(4) provides generation comparisons for the
system ( DocSys), baseline ( DocBase) and orig-
inal ( DocOrig) randomly selected text snippets
from each domain. The variability of the gener-
ated texts ranges from a close similarity to slightly
shorter - not an uncommon (Belz and Reiter,
2006), but not necessarily detrimental, observation
for NLG systems (van Deemter et al, 2005).
(4) Weather DocOrig
a. Another weak cold front will move ne to Cornwall by later
Friday.
Weather DocSys
b. Another weak cold front will move ne to Cornwall during
Friday.
Weather DocBase
c. Another weak cold front from ne through the Cornwall will
remain slow moving.
Bio DocOrig
d. He previously served as Director of Sales Planning and
Manager of Loan Center.
Bio DocSys
e. He previously served as Director of Sales in Loan Center
of the Company.
Bio DocBase
2The SUMTIME-METEO project is a common bench
mark in NLG. However, we provide no comparison between
our system and SUMTIME-METEO as our system utilized the
generated forecasts from SUMTIME-METEO?s system as the
historical data. We cannot compare with other statistical gen-
eration systems like (Belz, 2007) as they only focussed on the
part of the forecasts the predicts wind characteristics whereas
our system generates the complete forecasts.
1410
f. He previously served as Director of Sales of the Company.
The DocSys and DocBase generations are
largely grammatical and coherent on the surface
with some variance, but there are graded semantic
variations (e.g., Director of Sales Planning vs. Di-
rector of Sales (4g-h) and move ne to Cornwall vs.
from ne through the Cornwall). Both automatic
and human evaluations are required in NLG to de-
termine the impact of these variances on the under-
standability of the texts in general (non-experts)
and as they are representative of particular subject
matter domains (experts). The following sections
discuss the evaluation results.
4.2 Automatic Metrics
We used BLEU?4 (Papineni et al, 2002), METEOR
(v.1.3) (Denkowski and Lavie, 2011) to evaluate
the texts at document level. Both BLEU?4 and
METEOR originate from machine translation re-
search. BLEU?4 measures the degree of 4-gram
overlap between documents. METEOR uses a un-
igram weighted f?score less a penalty based on
chunking dissimilarity. These metrics only eval-
uate the text on a document level but fail to iden-
tify ?syntactic repetitiveness? across documents in
a document collection. This is an important char-
acteristic of a document collection to avoid banal-
ity. To address this issue, we propose a new auto-
matic metric called syntactic variability. In order
to compute this metric, each document should be
represented as a sequence of templates by associ-
ating each sentence in the document with a tem-
plate in the template bank. Syntactic variability is
defined as the percentage of unique template se-
quences across all generated documents. It ranges
between 0 and 1. A higher value indicates that
more documents in the collection are linguistically
different from each other and a value closer to zero
shows that most of documents have the similar
language despite different input data.3
As indicated in Figure 2, the BLEU-4 scores are
low for all DocSys and DocBase generations (as
compared to DocOrig) for each domain. How-
ever, the METEOR scores, while low overall (rang-
ing from .201-.437) are noticeably increased over
BLEU-4 (which ranges from .199-.320).
Given the nature of each metric, the results in-
dicate that the generated and baseline texts have
3Of course, syntactic and semantic repetitiveness could be
captured by syntactic variability, but only if this is the nature
of the underlying historical data - financial texts tend to be
fairly repetitive.
Figure 2: Automatic Evaluations.
very different surface realizations compared to the
originals (low BLEU-4), but are still capturing the
content of the originals (higher METEOR). Both
BLEU?4 and METEOR measure the similarity of
the generated text to the original text, but fail to
penalize repetitiveness across texts, which is ad-
dressed by the syntactic variability metric. There
is no statistically significant difference between
DocSys and DocBase generations for METEOR
and BLEU?4.4 However, there is a statistically
significant difference in the syntactic variability
metric for both domains (weather - ?2=137.16,
d.f.=1, p<.0001; biography - ?2=96.641, d.f.=1,
p<.0001) - the variability of the DocSys gener-
ations is greater than the DocBase generations,
which shows that texts generated by our system
are more variable than the baseline texts.
The use of automatic metrics is a common eval-
uation method in NLG, but they must be recon-
ciled against non?expert and expert level evalua-
tions.
4.3 Non-Expert Human Evaluations
Two sets of crowdsourced human evaluation tasks
(run on CrowdFlower) were constructed to com-
pare against the automatic metrics: (1) an under-
standability evaluation of the entire text on a three-
point scale: Fluent = no grammatical or infor-
mative barriers; Understandable = some gram-
matical or informative barriers; Disfluent = sig-
nificant grammatical or informative barriers; and
(2) a sentence?level preference between sentence
pairs (e.g., ?Do you prefer Sentence A (from Do-
cOrig) or the corresponding Sentence B (from
DocBase/DocSys)?).
4BLEU?4: weather - ?2=1.418, d.f.=1, p=.230; biography
- ?2=0.311, d.f.=1, p=.354. METEOR: weather - ?2=1.016,
d.f.=1, p=.313; biography - ?2=0.851, d.f.=1, p=.354.
1411
Over 100 native English speakers contributed,
each one restricted to providing no more than
50 responses and only after they successfully an-
swered 4 ?gold data? questions correctly. We also
omitted those evaluators with a disproportionately
high response rate. No other data was collected on
the contributors (although geographic data (coun-
try, region, city) and IP addresses were available).
For the sentence?level preference task, the pair or-
derings were randomized to prevent click bias.
For the text?understandability task, 40 docu-
ments were chosen at random from the DocOrig
test set alng with the corresponding 40 Doc-
Sys and DocBase generations (240 documents to-
tal/120 for each domain). 8 judgments per doc-
ument were solicited from the crowd (1920 to-
tal judgments, 69.51 average agreement) and are
summarized in Figures 3 and 4 (biography and
weather respectively).
If the system is performing well and the rank-
ing model is actually contributing to increased
performance, the accepted trend should be that
the DocOrig texts are more fluent and preferred
compared to both the DocSys and DocBase sys-
tems. However, the differences between DocOrig
and DocSys will not be significant, the differences
between DocOrig and DocBase and DocSys and
DocBase will be significantly different.
Figure 3: Biography Text Evaluations.
Focusing on fluency ratings, it is expected that
the DocOrig generations will have the highest flu-
ency (as they are human generated). Further, if the
DocSys is performing well, it is expected that the
fluency rating will be less than the DocOrig and
higher than DocBase. Figure 3, which shows the
biography text evaluations, demonstrates this ac-
ceptable distribution of performances.
For the weather discourses, as evident from
Figure 4, the acceptable trend holds between the
DocSys and DocBase generations, and the Doc-
Sys generation fluency is actually slightly higher
than DocOrig. This is possibly because the Do-
cOrig texts are from a particular subject matter -
weather forecasts for offshore oil rigs in the U.K.
- which may be difficult for people in general to
understand. Nonetheless, the demonstrated trend
is favorable to our system.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - ?2=.347,
d.f.=1, p=.555; DocOrig vs. DocBase - ?2=.090,
d.f.=1, p=.764; DocSys vs. DocBase - ?2=.790,
d.f.=1, p=.373). While this is a good result for
comparing DocOrig and DocSys generations, it is
not for the other pairs. However, numerically, the
trend is in the right direction despite not being
able to demonstrate significance. For biography,
the trend fits nicely both numerically and in terms
of statistical significance (DocOrig vs. DocSys -
?2=5.094, d.f.=1, p=.024; DocOrig vs. DocBase -
?2=35.171, d.f.=1, p<.0001; DocSys vs. DocBase
- ?2=14.000, d.f.=1, p<.0001).
Figure 4: Weather Text Evaluations.
For the sentence preference task, equivalent
sentences across the 120 documents were chosen
at random (80 sentences from biography and 74
sentences from weather). 8 judgments per com-
parison were solicited from the crowd (3758 to-
tal judgments, 75.87 average agreement) and are
summarized in Figures 5 and 6 (biography and
weather, respectively).
Similar to the text?understandability task, an
acceptable performance pattern should include the
DocOrig texts being preferred to both DocSys and
DocBase generations and the DocSys generation
preferred to the DocBase. The closer the Doc-
Sys generation is to the DocOrig, the better Doc-
Sys is performing. The biography domain illus-
1412
Figure 5: Biography Sentence Evaluations.
trates this scenario (Figure 5) where the results are
similar to the text-understandability experiments.
In contrast, for weather domain, sentences from
DocBase system were preferred to our system?s
(Figure 6). We looked at the cases where the
preferences were in favor of DocBase. It appears
that because of high syntactic variability, our sys-
tem can produce quite complex sentences where as
the non-experts seem to prefer shorter and simpler
sentences because of the complexity of the text.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - ?2=6.48,
d.f.=1, p=.011; DocOrig vs. DocBase - ?2=.720,
d.f.=1, p=.396; DocSys vs. DocBase - ?2=.720,
d.f.=1, p=.396). The trend is different compared to
the fluency metric above in that the DocBase sys-
tem is outperforming the DocOrig generations to
an almost statistically significant difference - the
remaining comparisons follow the trend. We be-
lieve that this is for similar reasons stated above
- i.e., the generation may be a more digestible
version of a technical document. More problem-
atic is the results of the biography evaluations.
Here there is a statistically significant difference
between the DocSys and DocOrig and no sta-
tistically significant difference between the Doc-
Sys and DocBase generations (DocOrig vs. Doc-
Sys - ?2=76.880, d.f.=1, p<.0001; DocOrig vs.
DocBase - ?2=38.720, d.f.=1, p<.0001; DocSys
vs. DocBase - ?2=.720, d.f.=1, p=.396). Again,
this distribution of preferences is numerically sim-
ilar to the trend we would like to see, but the sta-
tistical significance indicates that there is some
ground to make up. Expert evaluations are po-
tentially informative for identifying specific short-
comings and how best to address them.
Figure 6: Weather Sentence Evaluations.
4.4 Expert Human Evaluations
We performed expert evaluations for the biogra-
phy domain only as we do not have access to
weather experts. The four biography reviewers are
journalists who write short biographies for news
archives.
For the biography domain, evaluations of the
texts were largely similar to the evaluations of
the non-expert crowd (76.22 average agreement
for the sentence?preference task and 72.95 for the
text?understandability task). For example, the dis-
fluent ratings were highest for the DocBase gen-
erations and lowest for the DocOrig generations.
Also, the fluent ratings were highest for the Do-
cOrig generations, and while the combined flu-
ent and understandable are higher for DocSys as
compared to DocBase, the DocBase generations
had a 10% higher fluent score (58.22%) as com-
pared to the DocSys fluent score (47.97%). Based
on notes from the reviewers, the succinctness of
the the DocBase generations are preferred in some
ways as they are in keeping with certain editorial
standards. This is further reflected in the sentence
preferences being 70% in favor of the DocBase
generations as compared to the DocSys genera-
tions (all other sentence comparisons were consis-
tent with the non-expert crowd).
These expert evaluations provide much needed
clarity to the NLG process. Overall, our system
is generating clearly acceptable texts. Further,
there are enough parameters inherent in the system
to tune to different domain expectations. This is
an encouraging result considering that no experts
were involved in the development of the system -
a key contrast to many other existing (especially
rule-based) NLG systems.
1413
5 Conclusions and Future Work
We have presented a hybrid (template-based and
statistical), single?staged NLG system that gen-
erates natural sounding texts and is domain?
adaptable. Our experiments with both ex-
perts and non?experts demonstrate that the
system-generated texts are comparable to human?
authored texts. The development time to adapt
our system to new domains is small compared to
other NLG systems; around a week to adapt the
system to weather and biography domains. Most
of the development time was spent on creating the
domain-specific entity taggers for the weather do-
main. The development time would be reduced to
hours if the historical data for a domain is readily
available with the corresponding input data.
The main limitation of our system is that it re-
quires significant historical data. Our system does
consolidate many traditional components (macro-
and micro-planning, lexical choice and aggrega-
tion),5 but the system cannot be applied to the do-
mains with no historical data. The quality and the
linguistic variability of the generated text is di-
rectly proportional to the amount of historical data
available.
We also presented a new automatic metric to
evaluate generated texts at document collection
level to identify boilerplate texts. This metric
computes ?syntactic repetitiveness? by counting
the number of unique template sequences across
the given document collection.
Future work will focus on extending our frame-
work by adding additional features to the model
that could improve the quality of the generated
text. For example, most NLG pipelines have a
separate component responsible for referring ex-
pression generation (Krahmer and van Deemter,
2012). While we address the associated concern
of data consumption in Section 3.5, we currently
do not have any features that would handle refer-
ring expression generation. We believe that this
is possible by identifying referring expressions in
templates and adding features to the model to give
higher scores to the templates having relevant re-
ferring expressions. We also would like to inves-
tigate using all the top-scored templates instead
of the highest-scoring template. This would help
achieve better syntactic-variability scores by pro-
ducing more natural-sounding texts.
5Lexical choice and aggregation are ?handled? insofar as
their existence in the historical data.
Acknowledgments
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragment, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502?512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331?338.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284?304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL?06), pages 313?320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT?07), pages
164?171.
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2011. Content selection from an ontology-
based knowledge base for the generation of foot-
ball summaries. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), pages 72?81.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85?91.
1414
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of the
2003 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2003), pages 2003?
2007.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in
statistical nlg. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143?154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Colin Kelly, Ann Copestake, and Nikiforos Karama-
nis. 2009. Investigating content selection for lan-
guage generation using machine learning. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG), pages 130?137.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378.
Emiel Krahmer and Kees van Deemter. 2012. Com-
putational generation of referring expression: A sur-
vey. Computational Linguistics, 38(1):173?218.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?98),
pages 704?710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611?1622.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proceedings of the 2009 Conference
on Empirical Methods for Natural Language Pro-
cessing (EMNLP 2009), pages 400?409.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Johanna D. Moore and Cecile L. Paris. 1993. Planning
text for advisory dialogues: Capturing intentional
and rhetorical information. Computational Linguis-
tics, 19(4):651?694.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Jacques Robin and Kathy McKeown. 1996. Exmpiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and
Jin Yu. 2001. A two-stage model for content
determination. In Proceedings of the 8th Euro-
pean Workshop on Natural Language Generation
(ENLG), pages 1?8.
Kees van Deemter, Marie?t Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15?24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
1415
Proceedings of the 14th European Workshop on Natural Language Generation, pages 178?182,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
GenNext: A Consolidated Domain Adaptable NLG System
Frank Schilder, Blake Howald and Ravi Kondadadi?
Thomson Reuters, Research & Development
610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com
Abstract
We introduce GenNext, an NLG system
designed specifically to adapt quickly and
easily to different domains. Given a do-
main corpus of historical texts, GenNext
allows the user to generate a template bank
organized by semantic concept via derived
discourse representation structures in con-
junction with general and domain-specific
entity tags. Based on various features
collected from the training corpus, the
system statistically learns template rep-
resentations and document structure and
produces well?formed texts (as evaluated
by crowdsourced and expert evaluations).
In addition to domain adaptation, Gen-
Next?s hybrid approach significantly re-
duces complexity as compared to tradi-
tional NLG systems by relying on tem-
plates (consolidating micro-planning and
surface realization) and minimizing the
need for domain experts. In this descrip-
tion, we provide details of GenNext?s the-
oretical perspective, architecture and eval-
uations of output.
1 Introduction
NLG systems are typically tailored to very spe-
cific domains and tasks such as text summaries
from neonatal intensive care units (SUMTIME-
NEONATE (Portet et al, 2007)) or offshore oil
rig weather reports (SUMTIME-METEO (Reiter et
al., 2005)) and require significant investments in
development resources (e.g. people, time, etc.).
For example, for SUMTIME-METEO, 12 person
months were required for two of the system com-
ponents alone (Belz, 2007). Given the subject
matter of such systems, the investment is perfectly
?Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
reasonable. However, if the domains to be gener-
ated are comparatively more general, such as fi-
nancial reports or biographies, then the scaling of
development costs becomes a concern in NLG.
NLG in the editorial process for companies and
institutions where content can vary must be do-
main adaptable. Spending a year or more of devel-
opment time to produce high quality market sum-
maries, for example, is not a viable solution if it is
necessary to start from scratch to produce other re-
ports. GenNext, a hybrid system that statistically
learns document and sentence template represen-
tations from existing historical data, is developed
to be consolidated and domain adaptable. In par-
ticular, GenNext reduces complexity by avoiding
the necessity of having a separate document plan-
ner, surface realizer, etc., and extensive expert in-
volvement at the outset of system development.
Section 2 describes the theoretical background,
architecture and implementation of GenNext. Sec-
tion 3 discusses the results of a non?expert and ex-
pert crowdsourced sentence preference evaluation
task. Section 4 concludes with several future ex-
periments for system improvement.
2 Architecture of GenNext
In general, NLG systems follow a prototypical ar-
chitecture where some input data from a given do-
main is sent to a ?document planner? which de-
cides content and structuring to create a document
plan. That document plan serves as an input to
a ?micro planner? where the content is converted
into a syntactic expression (with associated con-
siderations of aggregation and referring expres-
sion generation) and a text specification is created.
The text specification then goes through the final
stage of ?surface realization? where everything is
put together into an output text (McKeown, 1985;
Reiter and Dale, 2000; Bateman and Zock, 2003).
In contrast, the architecture of GenNext (sum-
marized in Figure 1) is driven by a domain-specific
178
Figure 1: GenNext System Architecture.
corpus text. There is often a structured database
underlying the domains of corpus text, the fields
of which are used for domain specific entity tag-
ging (in addition to domain general entity tagging
[e.g. DATE, LOCATION, etc.]). An overview of
the different stages, which are a combination of
statistical (e.g., Langkilde and Knight (1998)) and
template?based (e.g., van Deemter, et al (2005))
approaches, follows in (A-E).1
A: Semantic Representation - We take a do-
main specific training corpus and reduce each
sentence to a Discourse Representation Structure
(DRS) - formal semantic representations of sen-
tences (and texts) from Discourse Representation
Theory (Kamp and Reyle, 1993; Basile and Bos,
2011). Each DRS is a combination of domain gen-
eral named entities, predicates (content words) and
relational elements (function words). In parallel,
domain specific named entity tags are identified
and are used to create templates that syntactically
represent some conceptual meaning; for example,
the short biography in (1):
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing
Director of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor?s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING | MANAGING | DIRECTOR | PERSON | ...
d. HOLDS | BACHELOR | FINANCE | MBA | HOLD | ...
Once the semantic representations are created,
they are organized and identified by semantic con-
cept (?CuId?) (described in (B)). Our assumption
is that each cluster equates with a CuId repre-
sented by each individual sentence in the cluster
and is contrastive with other CuIds (for similar ap-
1For more detail see Howald, et al (2013) - semantic
clustering and micro-planning and Kondadadi, et al (2013) -
document planning.
proaches, see Barzilay and Lapata (2005), Angeli,
et al (2010) and Lu and Ng (2011)).
B: Creating Conceptual Units - To create the
CuIds (a semi-automatic process), we cluster the
sentences using k-means clustering with k set ar-
bitrarily high to over-generate (Witten and Frank,
2005). This facilitates manual verification of the
generated clusters to merge (rather than split) them
if necessary. We assign a unique CuId to each
cluster and associate each template in the corpus to
a corresponding CuId. For example, in (2), using
the sentences in (1a-b), the identified named en-
tities are assigned to a clustered CuId (2a-b) and
then each sentence in the training corpus is re-
duced to a template (2c-d).
(2) Content Mapping
a. {CuId : 000} ? Information: person: Mr. Mitsutaka
Kambe; title: Managing Director; company: 77 Bank,
Ltd.; date: June 27, 2008
b. {CuId : 001} ? Information: person: he; degree:
Bachelor?s, MBA; subject: finance; institution: USC;
UCLA
Templates
c. {CuId : 000}: [person] has been serving as [title] of the
[company] since [date].
d. {CuId : 001}: [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
At this stage, we will have a set of CuIds with cor-
responding template collections which represent
the entire ?micro-planning? aspect of our system.
C: Collecting Statistics - For the ?document plan-
ning? stage, we collect a number of statistics for
each domain, for example:
? Frequency distribution of CuIds by position
? Frequency distribution of templates by position
? Frequency distribution of entity sequence
? Average number of entities by CuId and position
These statistics, in addition to entity tags and tem-
plates, are used in building different features used
by the ranking model (D).
D: Building a Ranking Model - The core compo-
nent of our system is a statistical model that ranks
a set of templates for a given position (e.g. sen-
tence 1, sentence 2, ..., sentence n) based on the
input data (see also Konstas and Lapata (2012).
The learning task is to find the rank for all the tem-
plates from all CuIds at each position. To gener-
ate the training data, we first exclude the templates
that have named entities not specified in the input
data (ensuring completeness). We then rank tem-
plates according to the edit distance (Levenshtein,
179
1966) from the template corresponding to the cur-
rent sentence in the training document. For each
template, we build a ranking model with features,
for example:
? Prior template and CuId
? Difference in number of words given position
? Most likely CuId given position and previous CuId
? Template 1-3grams given position and CuId
We use a linear kernel for a ranking SVM
(Joachims, 2002) to learn the weights associated
with each feature. Each domain has its own model
that is used when generating texts (E).
E: Generation: At generation time, our system
has a set of input data, a semantically organized
template bank and a model from training on a
given domain of texts. For each sentence, we first
exclude those templates that contain a named en-
tity not present in the input data. Then we cal-
culate the feature values times the model weight
for each of the remaining templates. The tem-
plate with the highest score is selected, filled
with matching entities from the input data and ap-
pended to the generated text. Example generations
for each domain are included in (3).
(3) Financial
a. First quarter profit per share for Brown-Forman
Corporation expected to be $0.91 per share by analysts.
b. Brown-Forman Corporation July first quarter profits will
be below that previously estimated by Wall Street with
a range between $0.89 and $0.93 per share and a projected
mean per share of $0.91 per share.
c. The consensus recommendation is Hold.
Biography
d. Mr. Satomi Mitsuzaki has been serving as Managing
Director of Mizuho Bank since June 27, 2008.
e. He was previously Director of Regional Compliance of
Kyoto Branch.
f. He is a former Managing Executive Officer and Chief
Executive Officer of new Industrial Finance Business
Group in Mitsubishi Corporation.
Weather
g. Complex low from southern Norway will drift slowly NNE
to the Lofoten Islands by early tomorrow.
h. A ridge will persist to the west of British Isles for Saturday
with a series of weak fronts moving east across
the North Sea.
i. A front will move ENE across the northern North Sea
Saturday.
3 Evaluation and Discussion
We have tested GenNext on three domains: Corpo-
rate Officer and Director Biographies (1150 texts
ranging from 3-10 period ended sentences), Fi-
nancial Texts (Mutual Fund Performances [162
texts, 2-4 sentences] and Broker Recommenda-
tions [905 texts, 8-20 sentences]), and Offshore
Oil Rig Weather Reports (1054 texts, 2-6 sen-
tences) from SUMTIME-METEO (Reiter et al,
2005). The total number of templates for the finan-
cial domain is 1379 distributed across 38 different
semantic concepts; 2836 templates across 19 con-
cepts for biography; and 2749 templates across 9
concepts for weather texts.
We have conducted several evaluation experi-
ments comparing two versions of GenNext, one
applying the ranking model (rank) and one with
random selection of templates (non-rank) (both
systems use the same template bank, CuId as-
signment and filtering) and the original texts from
which the data was extracted (original).
We used a combination of automatic (e.g.
BLEU?4 (Papineni et al, 2002), METEOR
(Denkowski and Lavie, 2011)) and human metrics
(using crowdsourcing) to evaluate the output (see
generally, Belz and Reiter (2006). However, in the
interest of space, we will restrict the discussion to
a human judgment task on output preferences. We
found this evaluation task to be most informative
for system improvement. The task asks an evalu-
ator to provide a binary preference determination
(100 sentence pairs/domain): ?Do you prefer Sen-
tence A (from original) or the corresponding Sen-
tence B (from rank or non-rank)?. This task was
performed for each domain.2 We also engaged 3
experts from the financial and 4 from the biogra-
phy domains to perform the same preference task
(average agreement was 76.22) as well as provide
targeted feedback.
For the preference results, summarized in Fig-
ure 2, we would like to see no statistically signifi-
cant difference between GenNext-rank and orig-
inal, but statistically significant differences be-
tween GenNext-rank and GenNext-non-rank, and
original and GenNext-non-rank. If this is the case,
then GenNext-rank is producing texts similar to
the original texts, and is providing an observ-
able improvement over not including the model at
all (GenNext-non-rank). This is exactly what we
see for all domains.3 However, in general, there
2Over 100 native English speakers contributed, each one
restricted to providing no more than 50 responses and only
after they successfully answered 4 initial gold data questions
correctly and continued to answer periodic gold data ques-
tions. The pair orderings were randomized to prevent click
bias. 8 judgments per sentence pair was collected (2400 judg-
ments) and average agreement was 75.87.
3Original vs. GenNext-rank : financial - ?2=.29, p?.59;
biography - ?2=3.01, p?.047; weather - ?2=.95, p?.32.
Original vs. GenNext-non-rank : financial - ?2=16.71,
p?.0001; biography - ?2=45.43, p?.0001; weather -
180
Figure 2: Cross-Domain Non-Expert Preference Evaluations.
is a greater difference between the original and
GenNext-rank biographies compared to the finan-
cial and weather texts. We take it as a goal to ap-
proach, as close as possible, the preferences for
the original texts.
The original financial documents were machine
generated from a different existing system. As
such, it is not surprising to see similarity in perfor-
mance compared to GenNext-rank and potentially
explains why preferences for the originals is some-
what low (assuming a higher preference rating for
well-formed human texts). Further, the original
weather documents are highly technical and not
easily understood by the lay person, so, again, it is
not surprising to see similar performance. Biogra-
phies were human generated and easy to under-
stand for the average reader. Here, both GenNext-
rank and GenNext-non-rank have some ground to
make up. Insights from domain experts are poten-
tially helpful in this regard.
Expert evaluations provided similar results and
agreements compared to the non?expert crowd.
Most beneficial about the expert evaluations was
the discussion of integrating certain editorial stan-
dards into the system. For example, shorter texts
were preferred to longer texts in the financial do-
main, but not the biographies. Consequently, we
could adjust weights to favor shorter templates.
Also, in biographies, sentences with subordinated
elaborations were not preferred because these con-
tained subjective comments (e.g. a leader in in-
dustry, a well respected individual, etc.). Here,
?2=24.27, p?.0001. GenNext-rank vs. GenNext-non-rank
: financial - ?2=12.81, p?.0003; biography - ?2=25.19,
p?.0001; weather - ?2=16.19, p?.0001.
we could manually curate or could automatically
detect templates with subordinated clauses and re-
move them. These types of comments are useful
to adjust the system accordingly to end user ex-
pectations.
4 Conclusion and Future Work
We have presented our system GenNext which is
domain adaptable, given adequate historical data,
and has a significantly reduced complexity com-
pared to other NLG systems (see generally, Robin
and McKeown (1996)). To the latter point, devel-
opment time for semantically processing the cor-
pus, applying domain general and specific tags,
and building a model is accomplished in days and
weeks as opposed to months and years.
Future experimentation will focus on being able
to automatically extract templates for different do-
mains to create preset banks of templates in the
absence of adequate historical data. We are also
looking into different ways to increase the vari-
ability of output texts from selecting templates
within a range of top scores (rather than just the
highest score) to providing additional generated
information from input data analytics.
Acknowledgments
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragement, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
181
References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502?512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331?338.
Valerio Basile and Johan Bos. 2011. Towards generat-
ing text from discourse representation structures. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation (ENLG), pages 145?150.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284?304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL?06), pages 313?320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT?07), pages
164?171.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85?91.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in sta-
tistical NLG. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143?154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical NLG framework for aggregated
planning and realization. In Proceedings of the An-
nual Conference for the Association of Computa-
tional Linguistics (ACL 2013). Association for Com-
putational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?98),
pages 704?710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611?1622.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318.
Franois Portet, Ehud Reiter, Jim Hunter, and Somaya-
julu Sripada. 2007. Automatic generation of tex-
tual summaries from neonatal intensive care data. In
In Proccedings of the 11th Conference on Artificial
Intelligence in Medicine (AIME 07). LNCS, pages
227?236.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Jacques Robin and Kathy McKeown. 1996. Empiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Kees van Deemter, Marie?t Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15?24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
182
