Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 114?121,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Opinion Mining of Spanish Customer Comments with Non-Expert
Annotations on Mechanical Turk
Bart Mellebeek, Francesc Benavent, Jens Grivolla,
Joan Codina, Marta R. Costa-jussa` and Rafael Banchs
Barcelona Media Innovation Center
Av. Diagonal, 177, planta 9
08018 Barcelona, Spain
{bart.mellebeek|francesc.benavent|jens.grivolla|joan.codina|
marta.ruiz|rafael.banchs}@barcelonamedia.org
Abstract
One of the major bottlenecks in the develop-
ment of data-driven AI Systems is the cost of
reliable human annotations. The recent ad-
vent of several crowdsourcing platforms such
as Amazon?s Mechanical Turk, allowing re-
questers the access to affordable and rapid re-
sults of a global workforce, greatly facilitates
the creation of massive training data. Most
of the available studies on the effectiveness of
crowdsourcing report on English data. We use
Mechanical Turk annotations to train an Opin-
ion Mining System to classify Spanish con-
sumer comments. We design three different
Human Intelligence Task (HIT) strategies and
report high inter-annotator agreement between
non-experts and expert annotators. We evalu-
ate the advantages/drawbacks of each HIT de-
sign and show that, in our case, the use of
non-expert annotations is a viable and cost-
effective alternative to expert annotations.
1 Introduction
Obtaining reliable human annotations to train data-
driven AI systems is often an arduous and expensive
process. For this reason, crowdsourcing platforms
such as Amazon?s Mechanical Turk1, Crowdflower2
and others have recently attracted a lot of attention
from both companies and academia. Crowdsourc-
ing enables requesters to tap from a global pool of
non-experts to obtain rapid and affordable answers
to simple Human Intelligence Tasks (HITs), which
1https://www.mturk.com
2http://crowdflower.com/
can be subsequently used to train data-driven appli-
cations.
A number of recent papers on this subject point
out that non-expert annotations, if produced in a suf-
ficient quantity, can rival and even surpass the qual-
ity of expert annotations, often at a much lower cost
(Snow et al, 2008), (Su et al, 2007). However, this
possible increase in quality depends on the task at
hand and on an adequate HIT design (Kittur et al,
2008).
In this paper, we evaluate the usefulness of MTurk
annotations to train an Opinion Mining System to
detect opinionated contents (Polarity Detection) in
Spanish customer comments on car brands. Cur-
rently, a large majority of MTurk tasks is designed
for English speakers. One of our reasons for partic-
ipating in this shared task was to find out how easy
it is to obtain annotated data for Spanish. In addi-
tion, we want to find out how useful these data are
by comparing them to expert annotations and using
them as training data of an Opinion Mining System
for polarity detection.
This paper is structured as follows. Section 2 con-
tains an explanation of the task outline and our goals.
Section 3 contains a description of three different
HIT designs that we used in this task. In Section
4, we provide a detailed analysis of the retrieved
HITs and focus on geographical information of the
workers, the correlation between the different HIT
designs, the quality of the retrieved answers and on
the cost-effectiveness of the experiment. In Section
5, we evaluate the incidence of MTurk-generated an-
notations on a polarity classification task using two
different experimental settings. Finally, we conclude
114
in Section 6.
2 Task Outline and Goals
We compare different HIT design strategies by eval-
uating the usefulness of resulting Mechanical Turk
(MTurk) annotations to train an Opinion Mining
System on Spanish consumer data. More specifi-
cally, we address the following research questions:
(i) Annotation quality: how do the different
MTurk annotations compare to expert annotations?
(ii) Annotation applicability: how does the per-
formance of an Opinion Mining classifier vary after
training on different (sub)sets of MTurk and expert
annotations?
(iii) Return on Investment: how does the use of
MTurk annotations compare economically against
the use of expert annotations?
(iv) Language barriers: currently, most MTurk
tasks are designed for English speakers. How easy
is it to obtain reliable MTurk results for Spanish?
3 HIT Design
We selected a dataset of 1000 sentences contain-
ing user opinions on cars from the automotive sec-
tion of www.ciao.es (Spanish). This website was
chosen because it contains a large and varied pool
of Spanish customer comments suitable to train an
Opinion Mining System and because opinions in-
clude simultaneously global numeric and specific
ratings over particular attributes of the subject mat-
ter. Section 5.1 contains more detailed information
about the selection of the dataset. An example of a
sentence from the data set can be found in (1):
(1) ?No te lo pienses ma?s, co?mpratelo!?
(= ?Don?t think twice, buy it!?)
The sentences in the dataset were presented to
the MTurk workers in three different HIT designs.
Each HIT design contains a single sentence to be
evaluated. HIT1 is a simple categorization scheme
in which workers are asked to classify the sentence
as being either positive, negative or neutral, as is
shown in Figure 1b. HIT2 is a graded categorization
template in which workers had to assign a score be-
tween -5 (negative) and +5 (positive) to the example
sentence, as is shown in Figure 1c. Finally, HIT3 is
a continuous triangular scoring template that allows
Figure 1: An example sentence (a) and the three HIT
designs used in the experiments: (b) HIT1: a simple
categorization scheme, (c) HIT2: a graded categoriza-
tion scheme, and (d) HIT3: a continuous triangular scor-
ing scheme containing both a horizontal positive-negative
axis and a vertical subjective-objective axis.
workers to use both a horizontal positive-negative
axis and a vertical subjective-objective axis by plac-
ing the example sentence anywhere inside the trian-
gle. The subjective-objective axis expresses the de-
gree to which the sentence contains opinionated con-
tent and was earlier used by (Esuli and Sebastiani,
2006). For example, the sentence ?I think this is a
wonderful car? clearly marks an opinion and should
be positioned towards the subjective end, while the
sentence ?The car has six cilinders? should be lo-
cated towards the objective end. Figure 1d contains
an example of HIT3. In order not to burden the
workers with overly complex instructions, we did
not mention this subjective-objective axis but asked
them instead to place ambiguous sentences towards
the center of the horizontal positive-negative axis
and more objective, non-opinionated sentences to-
wards the lower neutral tip of the triangle.
115
For each of the three HIT designs, we speci-
fied the requirement of three different unique as-
signments per HIT, which led to a total amount of
3 ? 3 ? 1000 = 9000 HIT assignments being up-
loaded on MTurk. Mind that setting the requirement
of unique assigments ensures a number of unique
workers per individual HIT, but does not ensure a
consistency of workers over a single batch of 1000
HITs. This is in the line with the philosophy of
crowdsourcing, which allows many different people
to participate in the same task.
4 Annotation Task Results and Analysis
After designing the HITs, we uploaded 30 random
samples for testing purposes. These HITs were com-
pleted in a matter of seconds, mostly by workers in
India. After a brief inspection of the results, it was
obvious that most answers corresponded to random
clicks. Therefore, we decided to include a small
competence test to ensure that future workers would
possess the necessary linguistic skills to perform the
task. The test consists of six simple categorisation
questions of the type of HIT1 that a skilled worker
would be able to perform in under a minute. In order
to discourage the use of automatic translation tools,
a time limit of two minutes was imposed and most
test sentences contain idiomatic constructions that
are known to pose problems to Machine Translation
Systems.
4.1 HIT Statistics
Table 1 contains statistics on the workers who com-
pleted our HITs. A total of 19 workers passed the
competence test and submitted at least one HIT. Of
those, four workers completed HITs belonging to
two different designs and six submitted HITs in all
three designs. Twelve workers are located in the US
(64%), three in Spain (16%), one in Mexico (5%),
Equador (5%), The Netherlands (5%) and an un-
known location (5%).
As to a comparison of completion times, it took
a worker on average 11 seconds to complete an in-
stance of HIT1, and 9 seconds to complete an in-
stance of HIT2 and HIT3. At first sight, this result
might seem surprising, since conceptually there is an
increase in complexity when moving from HIT1 to
HIT2 and from HIT2 to HIT3. These results might
Overall HIT1 HIT2 HIT3
ID C % # sec. # sec. # sec.
1 mx 29.9 794 11.0 967 8.6 930 11.6
2 us 27.6 980 8.3 507 7.8 994 7.4
3 nl 11.0 85 8.3 573 10.9 333 11.4
4 us 9.5 853 16.8 - - - -
5 es 9.4 - - 579 9.1 265 8.0
6 ec 4.1 151 9.4 14 16.7 200 13.0
7 us 3.6 3 15.7 139 8.5 133 11.6
8 us 2.2 77 8.2 106 7.3 11 10.5
9 us 0.6 - - - - 50 11.2
10 us 0.5 43 5.3 1 5 - -
11 us 0.4 - - 38 25.2 - -
12 us 0.4 - - 10 9.5 27 10.8
13 es 0.4 - - - - 35 15.1
14 es 0.3 - - 30 13.5 - -
15 us 0.3 8 24.7 18 21.5 - -
16 us 0.2 - - - - 22 8.9
17 us 0.2 - - 17 16.5 - -
18 ? 0.1 6 20 - - - -
19 us 0.1 - - 1 33 - -
Table 1: Statistics on MTurk workers for all three HIT
designs: (fictional) worker ID, country code, % of total
number of HITs completed, number of HITs completed
per design and average completion time.
suggest that users find it easier to classify items
on a graded or continuous scale such as HIT2 and
HIT3, which allows for a certain degree of flexibil-
ity, than on a stricter categorical template such as
HIT1, where there is no room for error.
4.2 Annotation Distributions
In order to get an overview of distribution of the re-
sults of each HIT, a histogram was plotted for each
different task. Figure 2a shows a uniform distribu-
tion of the three categories used in the simple cat-
egorization scheme of HIT1, as could be expected
from a balanced dataset.
Figure 2b shows the distribution of the graded cat-
egorization template of HIT2. Compared to the dis-
tribution in 2a, two observations can be made: (i)
the proportion of the zero values is almost identical
to the proportion of the neutral category in Figure
2a, and (ii) the proportion of the sum of the positive
values [+1,+5] and the proportion of the sum of the
negative values [-5,-1] are equally similar to the pro-
portion of the positive and negative categories in 2a.
This suggests that in order to map the graded annota-
tions of HIT2 to the categories of HIT1, an intuitive
partitioning of the graded scale into three equal parts
should be avoided. Instead, a more adequate alterna-
tive would consist of mapping [-5,-1] to negative, 0
116
Figure 2: Overview of HIT results: a) distribution of the three categories used in HIT1, b) distribution of results in the
scaled format of HIT2, c) heat map of the distribution of results in the HIT3 triangle, d) distribution of projection of
triangle data points onto the X-axis (positive/negative).
to neutral and [+1,+5] to positive. This means that
even slightly positive/negative grades correspond to
positive/negative categories.
Figure 2c shows a heat map that plots the distri-
bution of the annotations in the triangle of HIT3. It
appears that worker annotations show a spontaneous
tendency of clustering, despite the continuous nature
of the design. This suggests that this HIT design,
originally conceived as continuous, was transformed
by the workers as a simpler categorization task using
five labels: negative, ambiguous and positive at the
top, neutral at the bottom, and other in the center.
Figure 2d shows the distribution of all data-
points in the triangle of Figure 2c, projected onto
the X-axis (positive/negative). Although similar to
the graded scale in HIT2, the distribution shows a
slightly higher polarization.
These results suggest that, out of all three HIT de-
signs, HIT2 is the one that contains the best balance
between the amount of information that can be ob-
tained and the simplicity of a one-dimensional an-
notation.
4.3 Annotation Quality
The annotation quality of MTurk workers can be
measured by comparing them to expert annotations.
This is usually done by calculating inter-annotator
agreement (ITA) scores. Note that, since a single
HIT can contain more than one assignment and each
assignment is typically performed by more than one
annotator, we can only calculate ITA scores between
batches of assignments, rather than between individ-
ual workers. Therefore, we describe the ITA scores
in terms of batches. In Table 4.4, we present a com-
parison of standard kappa3 calculations (Eugenio
and Glass, 2004) between batches of assignments in
HIT1 and expert annotations.
We found an inter-batch ITA score of 0.598,
which indicates a moderate agreement due to fairly
consistent annotations between workers. When
comparing individual batches with expert annota-
tions, we found similar ITA scores, in the range be-
tween 0.628 and 0.649. This increase with respect
to the inter-batch score suggests a higher variability
among MTurk workers than between workers and
experts. In order to filter out noise in worker annota-
tions, we applied a simple majority voting procedure
in which we selected, for each sentence in HIT1, the
most voted category. This results in an additional
3In reality, we found that fixed and free margin Kappa values
were almost identical, which reflects the balanced distribution
of the dataset.
117
batch of annotations. This batch, refered in Table
4.4 as Majority, produced a considerably higher ITA
score of 0.716, which confirms the validity of the
majority voting scheme to obtain better annotations.
In addition, we calculated ITA scores between
three expert annotators on a separate, 500-sentence
dataset, randomly selected from the same corpus as
described at the start of Section 3. This collection
was later used as test set in the experiments de-
scribed in Section 5. The inter-expert ITA scores
on this separate dataset contains values of 0.725 for
?1 and 0.729 for ?2, only marginally higher than the
Majority ITA scores. Although we are comparing
results on different data sets, these results seem to
indicate that multiple MTurk annotations are able to
produce a similar quality to expert annotations. This
might suggest that a further increase in the number
of HIT assignments would outperform expert ITA
scores, as was previously reported in (Snow et al,
2008).
4.4 Annotation Costs
As explained in Section 3, a total amount of 9000
assignments were uploaded on MTurk. At a reward
of .02$ per assignment, a total sum of 225$ (180$
+ 45$ Amazon fees) was spent on the task. Work-
ers perceived an average hourly rate of 6.5$/hour for
HIT1 and 8$/hour for HIT2 and HIT3. These fig-
ures suggest that, at least for assignments of type
HIT2 and HIT3, a lower reward/assignment might
have been considered. This would also be consis-
tent with the recommendations of (Mason and Watts,
2009), who claim that lower rewards might have an
effect on the speed at which the task will be com-
pleted - more workers will be competing for the task
at any given moment - but not on the quality. Since
we were not certain whether a large enough crowd
existed with the necessary skills to perform our task,
we explicitly decided not to try to offer the lowest
possible price.
An in-house expert annotator (working at approx-
imately 70$/hour, including overhead) finished a
batch of 1000 HIT assignments in approximately
three hours, which leads to a total expert annotator
cost of 210$. By comparing this figure to the cost
of uploading 3 ? 1000 HIT assignments (75$), we
saved 210 ? 75 = 135$, which constitutes almost
65% of the cost of an expert annotator. These figures
do not take into account the costs of preparing the
data and HIT templates, but it can be assumed that
these costs will be marginal when large data sets are
used. Moreover, most of this effort is equally needed
for preparing data for in-house annotation.
?1 ?2
Inter-batch 0.598 0.598
Batch 1 vs. Expert 0.628 0.628
Batch 2 vs. Expert 0.649 0.649
Batch 3 vs. Expert 0.626 0.626
Majority vs. Expert 0.716 0.716
Experts4 0.725 0.729
Table 2: Interannotation Agreement as a measure of qual-
ity of the annotations in HIT1. ?1 = Fixed Margin
Kappa. ?2 = Free Margin Kappa.
5 Incidence of annotations on supervised
polarity classification
This section intends to evaluate the incidence of
MTurk-generated annotations on a polarity classifi-
cation task. We present two different evaluations.
In section 5.2, we compare the results of training
a polarity classification system with noisy available
metadata and with MTurk generated annotations of
HIT1. In section 5.3, we compare the results of
training several polarity classifiers using different
training sets, comparing expert annotations to those
obtained with MTurk.
5.1 Description of datasets
As was mentioned in Section 3, all sentences were
extracted from a corpus of user opinions on cars
from the automotive section of www.ciao.es
(Spanish). For conducting the experimental evalu-
ation, the following datasets were used:
1. Baseline: constitutes the dataset used for train-
ing the baseline or reference classifiers in Ex-
periment 1. Automatic annotation for this
dataset was obtained by using the following
naive approach: those sentences extracted from
comments with ratings5 equal to 5 were as-
signed to category ?positive?, those extracted
5The corpus at www.ciao.es contains consumer opinions
marked with a score between 1 (negative) and 5 (positive).
118
from comments with ratings equal to 3 were
assigned to ?neutral?, and those extracted from
comments with ratings equal to 1 were assigned
to ?negative?. This dataset contains a total of
5570 sentences, with a vocabulary coverage of
11797 words.
2. MTurk Annotated: constitutes the dataset that
was manually annotated by MTurk workers in
HIT1. This dataset is used for training the con-
trastive classifiers which are to be compared
with the baseline system in Experiment 1. It
is also used in various ways in Experiment 2.
The three independent annotations generated
by MTurk workers for each sentence within this
dataset were consolidated into one unique an-
notation by majority voting: if the three pro-
vided annotations happened to be different6,
the sentence was assigned to category ?neutral?;
otherwise, the sentence was assigned to the cat-
egory with at least two annotation agreements.
This dataset contains a total of 1000 sentences,
with a vocabulary coverage of 3022 words.
3. Expert Annotated: this dataset contains the
same sentences as the MTurk Annotated one,
but with annotations produced internally by
known reliable annotators7. Each sentence re-
ceived one annotation, while the dataset was
split between a total of five annotators.
4. Evaluation: constitutes the gold standard used
for evaluating the performance of classifiers.
This dataset was manually annotated by three
experts in an independent manner. The gold
standard annotation was consolidated by using
the same criterion used in the case of the pre-
vious dataset8. This dataset contains a total of
500 sentences, with a vocabulary coverage of
2004 words.
6This kind of total disagreement among annotators occurred
only in 13 sentences out of 1000.
7While annotations of this kind are necessarily somewhat
subjective, these annotations are guaranteed to have been pro-
duced in good faith by competent annotators with an excellent
understanding of the Spanish language (native or near-native
speakers)
8In this case, annotator inter-agreement was above 80%, and
total disagreement among annotators occurred only in 1 sen-
tence out of 500
Baseline Annotated Evaluation
Positive 1882 341 200
Negative 1876 323 137
Neutral 1812 336 161
Totals 5570 1000 500
Table 3: Sentence-per-category distributions for baseline,
annotated and evaluation datasets.
These three datasets were constructed by ran-
domly extracting sample sentences from an origi-
nal corpus of over 25000 user comments contain-
ing more than 1000000 sentences in total. The sam-
pling was conducted with the following constraints
in mind: (i) the three resulting datasets should not
overlap, (ii) only sentences containing more than
3 tokens are considered, and (iii) each resulting
dataset must be balanced, as much as possible, in
terms of the amount of sentences per category. Table
3 presents the distribution of sentences per category
for each of the three considered datasets.
5.2 Experiment one: MTurk annotations vs.
original Ciao annotations
A simple SVM-based supervised classification ap-
proach was considered for the polarity detection task
under consideration. According to this, two dif-
ferent groups of classifiers were used: a baseline
or reference group, and a contrastive group. Clas-
sifiers within these two groups were trained with
data samples extracted from the baseline and anno-
tated datasets, respectively. Within each group of
classifiers, three different binary classification sub-
tasks were considered: positive/not positive, nega-
tive/not negative and neutral/not neutral. All trained
binary classifiers were evaluated by computing pre-
cision and recall for each considered category, as
well as overall classification accuracy, over the eval-
uation dataset.
A feature space model representation of the data
was constructed by considering the standard bag-of-
words approach. In this way, a sparse vector was ob-
tained for each sentence in the datasets. Stop-word
removal was not conducted before computing vec-
tor models, and standard normalization and TF-IDF
weighting schemes were used.
Multiple-fold cross-validation was used in all
conducted experiments to tackle with statistical vari-
119
classifier baseline annotated
positive/not positive 59.63 (3.04) 69.53 (1.70)
negative/not negative 60.09 (2.90) 63.73 (1.60)
neutral/not neutral 51.27 (2.49) 62.57 (2.08)
Table 4: Mean accuracy over 20 independent simula-
tions (with standard deviations provided in parenthesis)
for each classification subtasks trained with either the
baseline or the annotated dataset.
ability of the data. In this sense, twenty independent
realizations were actually conducted for each exper-
iment presented and, instead of individual output re-
sults, mean values and standard deviations of evalu-
ation metrics are reported.
Each binary classifier realization was trained with
a random subsample set of 600 sentences extracted
from the training dataset corresponding to the clas-
sifier group, i.e. baseline dataset for reference sys-
tems, and annotated dataset for contrastive systems.
Training subsample sets were always balanced with
respect to the original three categories: ?positive?,
?negative? and ?neutral?.
Table 4 presents the resulting mean values of
accuracy for each considered subtask in classifiers
trained with either the baseline or the annotated
dataset. As observed in the table, all subtasks ben-
efit from using the annotated dataset for training
the classifiers; however, it is important to mention
that while similar absolute gains are observed for
the ?positive/not positive? and ?neutral/not neutral?
subtasks, this is not the case for the subtask ?neg-
ative/not negative?, which actually gains much less
than the other two subtasks.
After considering all evaluation metrics, the bene-
fit provided by human-annotated data availability for
categories ?neutral? and ?positive? is evident. How-
ever, in the case of category ?negative?, although
some gain is also observed, the benefit of human-
annotated data does not seem to be as much as for
the two other categories. This, along with the fact
that the ?negative/not negative? subtask is actually
the best performing one (in terms of accuracy) when
baseline training data is used, might suggest that
low rating comments contains a better representa-
tion of sentences belonging to category ?negative?
than medium and high rating comments do with re-
spect to classes ?neutral? and ?positive?.
In any case, this experimental work only verifies
the feasibility of constructing training datasets for
opinionated content analysis, as well as it provides
an approximated idea of costs involved in the gener-
ation of this type of resources, by using MTurk.
5.3 Experiment two: MTurk annotations vs.
expert annotations
In this section, we compare the results of training
several polarity classifiers on six different training
sets, each of them generated from the MTurk anno-
tations of HIT1. The different training sets are: (i)
the original dataset of 1000 sentences annotated by
experts (Experts), (ii) the first set of 1000 MTurk re-
sults (Batch1), (iii) the second set of 1000 MTurk
results (Batch2), (iv) the third set of 1000 MTurk
results (Batch3), (v) the batch obtained by major-
ity voting between Batch1, Batch2 and Batch3 (Ma-
jority), and (vi) a batch of 3000 training instances
obtained by aggregating Batch1, Batch2 and Batch3
(All). We used classifiers as implemented in Mal-
let (McCallum, 2002) and Weka (Hall et al, 2009),
based on a simple bag-of-words representation of
the sentences. As the objective was not to obtain
optimum performance but only to evaluate the dif-
ferences between different sets of annotations, all
classifiers were used with their default settings.
Table 5 contains results of four different clas-
sifiers (Maxent, C45, Winnow and SVM), trained
on these six different datasets and evaluated on the
same 500-sentence test set as explained in Section
5.1. Classification using expert annotations usu-
ally outperforms classification using a single batch
(one annotation per sentence) of annotations pro-
duced using MTurk. Using the tree annotations per
sentence available from MTurk, all classifiers reach
similar or better performance compared to the sin-
gle set of expert annotations, at a much lower cost
(as explained in section 4.4).
It is interesting to note that most classifiers bene-
fit from using the full 3000 training examples (1000
sentences with 3 annotations each), which intu-
itively makes sense as the unanimously labeled ex-
amples will have more weight in defining the model
of the corresponding class, whereas ambiguous or
unclear cases will have their impact reduced as their
characteristics are attributed to various classes.
On the contrary, Support Vector Machines show
120
System
E
xp
er
ts
B
at
ch
1
B
at
ch
2
B
at
ch
3
M
aj
or
it
y
A
ll
Winnow 44.2 43.6 40.4 47.6 46.2 50.6
SVM 57.6 53.0 55.4 54.0 57.2 52.8
C45 42.2 33.6 42.0 41.2 41.6 45.0
Maxent 59.2 55.8 57.6 54.0 57.6 58.6
Table 5: Accuracy figures of four different classifiers
(Winnow, SVM, C45 and Maxent) trained on six different
datasets (see text for details).
an important drop in performance when using mul-
tiple annotations, but perform well when using the
majority vote. As a first intuition, this may be due to
the fact that SVMs focus on detecting class bound-
aries (and optimizing the margin between classes)
rather than developing a model of each class. As
such, having the same data point appear several
times with the same label will not aid in finding ap-
propriate support vectors, whereas having the same
data point with conflicting labels may have a nega-
tive impact on the margin maximization.
Having only evaluated each classifier (and train-
ing set) once on a static test set it is unfortunately not
possible to reliably infer the significance of the per-
formance differences (or determine confidence in-
tervals, etc.). For a more in-depth analysis it might
be interesting to use bootstrapping or similar tech-
niques to evaluate the robustness of the results.
6 Conclusions
In this paper we have examined the usefulness of
non-expert annotations on Amazon?s Mechanical
Turk to annotate the polarity of Spanish consumer
comments. We discussed the advantages/drawbacks
of three different HIT designs, ranging from a sim-
ple categorization scheme to a continous scoring
template. We report high inter-annotator agree-
ment scores between non-experts and expert anno-
tators and show that training an Opinion Mining
System with non-expert MTurk annotations outper-
forms original noisy annotations and obtains com-
petitive results when compared to expert annotations
using a variety of classifiers. In conclusion, we
found that, in our case, the use of non-expert anno-
tations through crowdsourcing is a viable and cost-
effective alternative to the use of expert annotations.
In the classification experiments reported in this
paper, we have relied exclusively on MTurk anno-
tations from HIT1. Further work is needed to fully
analyze the impact of each of the HIT designs for
Opinion Mining tasks. We hope that the added rich-
ness of annotation of HIT2 and HIT3 will enable us
to use more sophisticated classification methods.
References
A. Esuli and F. Sebastiani. 2006. SentiWordNet: a pub-
licly available lexical resource for opinion mining. In
Proceedings of LREC, volume 6.
B. D Eugenio and M. Glass. 2004. The kappa statistic: A
second look. Computational linguistics, 30(1):95101.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
A. Kittur, E. H Chi, and B. Suh. 2008. Crowdsourcing
user studies with mechanical turk.
W. Mason and D. J Watts. 2009. Financial incentives
and the performance of crowds. In Proceedings of
the ACM SIGKDD Workshop on Human Computation,
pages 77?85.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y Ng. 2008.
Cheap and fastbut is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263.
Q. Su, D. Pavlov, J. H Chow, and W. C Baker. 2007.
Internet-scale collection of human-reviewed data. In
Proceedings of the 16th international conference on
World Wide Web, pages 231?240.
121
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46?52,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A hybrid framework for scalable Opinion Mining in Social Media: 
detecting polarities and attitude targets 
Carlos Rodr?guez-Penagos 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
carlos.rodriguez 
@barcelonamedia.org 
Jens Grivolla 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
jens.grivolla 
@barcelonamedia.org 
Joan Codina Fib? 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
joan.codina 
@barcelonamedia.org 
 
 
Abstract 
Text mining of massive Social Media 
postings presents interesting challenges for 
NLP applications due to sparse 
interpretation contexts, grammatical and 
orthographical variability as well as its very 
fragmentary nature. No single 
methodological approach can be expected to 
work across such diverse typologies as 
twitter micro-blogging, customer reviews, 
carefully edited blogs, etc. In this paper we 
present a modular and scalable framework 
to Social Media Opinion Mining that 
combines stochastic and symbolic 
techniques to structure a semantic space to 
exploit and interpret efficiently. We 
describe the use of this framework for the 
discovery and clustering of opinion targets 
and topics in user-generated comments for 
the Telecom and Automotive domains. 
1 Introduction 
Social Media (SM) postings constitute a messy 
and highly heterogeneous media that nonetheless 
represent a highly valuable source of information 
about the attitudes, interests and expectations of 
citizens and consumers everywhere. This fact has 
driven a trove of recent research and 
development efforts aimed at managing and 
interpreting such information for a wide 
spectrum of commercial applications, among 
them: reputation management, branding, 
marketing design, etc. A diverse array of 
techniques representing the state of the art run 
the gamut from knowledge-engineered rule-and 
lexicon-base approaches that (when carefully 
crafted) provide high precision in homogeneous 
contexts, to wide-coverage machine learning 
approaches that (when suitable development data 
is available) tackle noisy text with reasonable 
accuracies in some genres. 
 As SM channels are as different from each 
other as, say, spoken text from essay writing, we 
believe that no single technique, powerful as it 
may be, is capable of interpreting all domains, 
genres and channels in the vast universe of SM 
conversations. Faced with an industrial demand 
for simultaneous monitoring of heterogeneous 
opinion sources, our approach has evolved into 
combining diverse NLP technologies into a 
robust semantic analysis framework to create a 
high-granularity representation of user-generated 
commentaries amenable to machine 
interpretation. 
Analysis of Telecom-related social postings 
has shown how a modular and scalable analysis 
framework can combine a veritable arsenal of 
NLP and data mining techniques into a hybrid 
application that adapts well to the unique 
challenges and demands of different Social 
Media genres.  
Section 2 will present the UIMA-Solr 
framework and components used to process 
opinionated text, as well as discuss the 
representational choices made for analysis. 
Section 3 will frame our approach within the 
State-of-the-Art of Sentiment analysis and 
Opinion mining as we interpret it, while Sections 
4 and 5 describe data and results of the 
application of our proposed approach in the 
context of opinion topic detection and clustering 
of SM postings in the Telecoms and Automobile 
domains respectively, and with different textual 
genres. Finally, Section 6 will focus on the 
conclusions and future work that presents to us at 
this point.   
46
2 A modular toolset for SM processing 
For semantic processing of our data we use a 
UIMA 1  (Ferrucci & Lally, 2004) architecture  
plus Solr-based clustering and indexing 
capabilities. Our choice of UIMA is guided in 
part by our wish to achieve good scalability and 
robustness, and that all components can be 
implemented modularly and in a distributed 
manner using UIMA-AS (Asynchronous Scale 
out). Also, UIMA?s data representation as CAS 
objects allows preserving the documents integrity 
since annotations are added as standoff metadata, 
without modifying the original information. 
Under the UIMA architecture, a hybrid NLP 
analysis framework is possible, combining 
powerful Machine Learning modules like 
Maximum Entropy (ME, OpenNLP) 2  or 
Conditional Random Fields (CRF, JulieLab), 3 
with gazetteer and regular expression matchers 
and rule-based Noun Phrase chunkers. The basic 
linguistic processing has a sentence and token 
identifier, a POS tagger, a lemmatizer, a NP 
chunker and a dependency parser. In addition, we 
employ gazetteers to match products, companies, 
and other entities in text, as well as a hand-
crafted lexicon of polar terms created from 
corpus exploration of Telecom domain text, as 
well as a regular expression module to detect 
emoticons when available. Also, two models for 
Named-Entity recognition were applied using 
CRF: one trained on conventional ENAMEX 
Named Entity Recognition and Classification 
entities, and another trained using data from 
customer reviews from various domains (Cars, 
Banking, and Mobile service providers), in order 
to detect opinion targets and cues. One of the 
objectives of this relatively straightforward 
processing (although by no means the only one), 
was to select candidates for classifiers that could 
identify both the specific subject of each opinion 
expressed in text, as well as capture a more 
general topic of the whole conversation (which 
conceivably could coincide or not with one of the 
specific opinion targets). Targets and topics are 
usually expressed as entity names, concepts or 
attributes, and thus can appear in language as 
noun, adjectival, adverbial or even verbal 
phrases. Opinion cues (or Q-elements) are words, 
emoticons and phrases that convey the actual 
attitude of the speaker towards the topics and 
                                                 
1 Unstructured Information Management Architecture 
2 http://maxent.sourceforge.net 
3 http://www.julielab.de 
targets, and a strength and polarity can be 
attributed to them, both a priori and in context. 
Our modular processing approach allows 
customizing the annotation for each domain or 
genre, since, for example, regular expressions to 
detect emoticons will be useful for twitter micro-
blogging, but less so for more conventional blogs 
where such sentiment-expression devices are less 
frequent; Also pre-compiled lists of known 
entities can provide good target precision while 
customised distributional models will help 
discover unlisted names and concepts in text. 
The output of the semantic and syntactic 
processing pipeline is indexed using the Apache 
Solr framework,4 which is based on the Lucene 
engine. This setup allows the implementation of 
clustering and classification algorithms, allowing 
us to obtain reliable statistical correlations 
between documents and entities.  
We also developed or adapted a number of 
visualization components in order to present the 
data stored in Solr in an interactive page that is 
conducive to data exploration and discovery by 
the system?s corporate users. At the same time, 
Carrot2 is connected to Solr and is used to test 
clustering conditions and algorithms, providing a 
nice visualization interface. Carrot2 is an open 
source search results clustering engine (Osi?ski 
& Weiss, 2005). It can automatically organize 
collections of documents into thematic 
categories. 
3 Previous work 
Two good overviews of general Opinion Mining 
and Sentiment Analysis challenges are Pang & 
Lee (2008) and, focused specifically on customer 
reviews, Bhuiyan, Xu & Josang (2009). 
Detecting the subject or targets of opinions is one 
of the main lines of work within Opinion 
Mining, and considerable effort has been put into 
it, since it has been shown to be a highly-domain 
specific task (consumer reviews will focus on 
specific products and features, tweets have 
hashtags to identify topics, blogs can talk almost 
about anything, etc.).  
Outside of user-generated content, Coursey, 
Mihalcea, & Moen (2009) have suggested using 
indirect semantic resources, such as the 
Wikipedia, to identify document topics. For 
Opinion Mining genres, and extending on Hu & 
Liu (2004), Popescu & Etzioni (2005) use a 
combination of Pointwise Mutual Information, 
                                                 
4 http://lucene.apache.org/solr/ 
47
relaxation labeling and dependency analysis to 
extract possible targets and features in product 
reviews. Kim & Hovy (2006), for example, use 
thematic roles to establish a relation between 
candidate opinion holders and opinion topics, 
while exploiting clustering to improve coverage 
in their role-labeling. Recent approaches have 
included adaptation of NER techniques to noisy 
and irregular text, either by using learning 
algorithms or by doing text normalization (Locke 
& Martin, 2009; Ritter, Clark & Etzioni, 2011). 
4 Exploring the semantic space of 
Telecom-related online postings 
We collected close to 200,000 postings from 
various SM sources in a 4 month timeframe, 
including fairly carefully-written product-
oriented forums, blogs, etc., as well as more 
casually-drafted Facebook and twitter micro-
blogging, that discussed Spanish Telecom?s 
services and products. Of these, we randomly 
sub-selected a representative 190-document 
sample that was manually marked-up (for a test 
involving machine learning of cue-polarity-target 
relationships) by two different human annotators 
with a 20-document overlap, using simplified 
annotation guidelines focused on opinion targets, 
topics, cues and polarities. An interesting 
observation about the interannotator agreement 
(but one we can?t discuss in detail here) is that 
with regard to targets one of the human 
annotators tended more towards complete 
syntactic units (noun phrases), while the other 
chose more conceptual and semantic extensions 
as subjects for the opinions. The 20-document 
overlap was meant to help us evaluate this 
guideline development process, but the 
misalignment of guideline interpretation by the 
two human annotators made it very difficult to 
measure any kind of true interannotator 
agreement. Also, single annotation adjudication 
was made difficult due to the fact that both 
interpretations presented valid aspects, and we 
chose to use each set as an independent 
evaluation set to detect any unnoticed patterns 
that could emerge from using one of the other in 
our training and validation, but those results are 
inconclusive and merit further research. Since no 
adjudicator was incorporated in the process to 
resolve disagreements, the final annotated sets do 
not constitute a true Gold Standard, but each 
human-annotated set was used in turn as a 
benchmark against automatic annotators.  
Content elicitation was combined with activity 
and network mining for an enriched overview of 
the social conversation ecosystems, but the 
second aspect won?t be discussed here for the 
sake of brevity. For the same reason, although 
other aspects of sentiment analysis were 
performed on this data (cue and polarity 
detection, for example), we will also restrict the 
scope of these discussions on the detection and 
clustering of specific targets and general topics 
of the opinions expressed in such SM channels. 
Obviously, a deeper and more textured view of 
opinionated text is needed to be of any real use, 
but the overall features, shortcomings and 
advantages of our chosen approach are 
adequately discussed even if we restrict this 
paper to these very specific tasks. 
The first series of experiments about clustering 
using semantics explored the above-mentioned 
corpus of SM posting that discussed a Spanish 
Telecom, one of the aims being detecting and 
aggregating the topics and targets of online 
opinions. Different processing modules geared 
towards topic and target detection were 
compared against each human annotator?s 
choices, but also against each other and to the 
combined output of each. The main modules 
involved were: (A) generic NERC,  (B) a target 
and topic NERC model (StatTarg), (C) a Noun 
Phrase Chunker, and (D) a Gazetteer matcher 
(Taxonomy). Figures 1 through 4 show, 
respectively, recall (1) and precision (2) with 
regard to human annotated topics, and recall (3) 
and precision (4) with regard to human annotated 
targets. 
The results presented here are the overall 
performance across genres and domains, since 
the 190 documents annotated covered the whole 
range from forums to tweets. 
 
 Figure 1. Topic recall 
  
48
 Figure 2. Topic precision 
 
 Figure 3. Target recall 
 
 Figure 4. Target precision 
For this experiment, and as a guideline for the 
human annotators, targets were roughly defined 
as occurrences in the text of objects of opinion, 
whereas topics where to represent the main focus 
of the document or message. The annotators 
usually marked one topic per document, which 
was almost always also one of the targets. 
The customized taxonomy has a good precision 
with regard to target and topic identification, 
while the NERC and NP Chunk approaches 
improve the recall but suffer a bit on precision. 
Generic NER models have a moderately high 
precision (63%) with regard to manually 
annotated targets but rather low recall (specially 
in genres where capitalization is irregular which 
hinders NER detection), while NP Chunks 
present the opposite case: moderately (56%) high 
recall with low precision. This can be explained 
in part by the ?greediness? of each methodology, 
with the chunker annotating extensively while 
the NERC model being much more selective. 
Another noteworthy result is the strong domain 
bias of target annotators trained on a Ciao 
customer reviews for Banking, Automotive and 
Mobile Service markets. The models 
implemented through training from multi-domain 
review sites were found to have medium 
precision, but very low recall. 
The combination of all modules (AllTargets, a 
combination of NERC, Chunker, Taxonomy and 
StatTarget) had a very high recall of around 90%. 
With regard to topic detection, the combination 
of all modules had a recall of 94% and 83%, 
depending on which gold standard it is compared 
to (the one created by one expert human 
annotator or the other), which is an excellent 
recall level. The precision obtained on topic 
detection is very low. This, however, is expected 
as the evaluation is done using all candidates 
given by the different annotation layers, with no 
selection process. Since most of the topics are 
already identified as targets, the key issue here is 
to identify which of the comment targets is the 
main topic. 
It is important to note that merging the Chunker 
output with that of the rest of the modules 
improves the recall of the system but the 
precision becomes low. The main reason is that 
most targets and topics are noun phrases, but not 
all noun phrases are targets or topics.  
It is important to note that combining the output 
of different annotation layers (except for the NP 
chunker) does not reduce overall precision, while 
greatly increasing recall. 
For the clustering experiments, we chose 
Carrot2?s Lingo, a clustering algorithm based on  
Singular Value Decomposition. We envisioned 
the content-based clustering as an interactive 
exploratory tool, rather that providing a single 
?correct? and definitive set of groupings. Cluster 
analysis as such is not an automatic task, but an 
iterative process of knowledge discovery that 
involves trial and failure. It will often be 
necessary to modify the preprocessing and adjust 
parameters until the result achieves the desired 
properties. 
The  query ?problem?, for example, sent to 
some of the telecom forums in May produced 
groupings suggestive of complaints relating to 
rates, internet access, SIM chips, SMS, as well as 
with regard to specific terminal models and 
companies. Even this limited capability can be 
helpful for some of our user?s market analysis 
purposes. 
49
The visualization of query-based clustering 
with detection of target, cues and topics, and the 
possibility of tracking trends over time, provided 
a very powerful overview of how consumer 
attitudes, expectations and complaints about 
products and services are reflected in dynamic 
interchanges in various SM channels. These 
results are available through an online demo 6 
(Figure 5, shown for Facebook postings). 
5 Visualizing the evolution of customer 
opinion 
In addition to exploring SM data for the Telecom 
domain, we performed some experiments using 
clustering without directly using annotated 
semantics, but instead using the semantics only 
for data interpretation. We crawled more than 
10,000 customer reviews in the automotive 
domain in Spanish, along with some metadata 
that included the numerical ratings added by the 
reviewers themselves. Using our modular 
pipeline, we did shallow document clustering 
followed by linguistic processing that included 
lemmatization, POS tagging and Named Entity 
Recognition, in order to allow for analytical 
exploitation of the community-driven discussion 
on automobiles, product features and 
                                                 
6 http://webmining.barcelonamedia.org/Orange/ 
automakers. The most relevant nouns, adjectives, 
bigrams and named entities from a given query, 
are projected into a polarity versus time dynamic 
map. The clustering was performed by the 
combined use of vector space reduction 
techniques and the K-means classification 
paradigm in a completely unsupervised manner. 
Clusters thus obtained were represented by sets 
of words that best described them to obtain a 
view of the emerging terms, trends and features 
contained in the opinions, with the aim of 
providing a representation of their collective 
content. Since evaluating clustering techniques 
per se was not the objective of these 
experiments, and since a gold standard was not 
available, the purpose of the system was (A) to 
validate the coherence of the groupings 
according to the review?s content, and (B) assess 
if those clusters also aggregate as well along 
declared global polarity. Although inconclusive 
from a quantitative point of view, those 
experiments show the feasibility of leveraging 
existing Social Media resources in order to 
develop applications that can visualize and 
explore the semantic ecosystem of consumer 
opinions and attitudes, in a cost-effective and 
efficient manner. A demo of the functionalities 
of the system described here is also publicly 
Figure 5. Facebook's "Iphone" semantic exploration (screenshot) 
50
available. 7 . One cluster, a very positive one 
(based on the average user rating), is represented 
by the terms land-terreno-todoterreno-rover-
campo-4x4 (off-road, field, ground, land, Rover), 
while another one, aceite-garant?a-servicio-
problemas-a?os (oil-warranty-service-problems-
years), in the lower right side might indicate 
unhappy reviewers. 
6 Conclusion and future work 
The results obtained on the Telecom corpus with 
different automatic annotation layers suggest that 
a possible improvement in the system could 
come from researching which combinations of 
automatic annotators can enhance overall 
performance, as one module?s strength might 
complement another weaknesses and vice versa, 
so that what one is missing another one can 
catch. An additional option to increase overall 
recall is to implement a weighted voting scheme 
among the modules, allowing calculation of 
probabilities from the combinations of various 
annotations that overlap a textual segment. 
The fact that combination of annotation layers 
through simple merging of all annotations has 
such a great impact on recall while not reducing 
precision suggests that the different methods are 
very complementary. We expect to be able to 
trade off some of the gained recall for much 
improved precision by applying more 
sophisticated merging methods. 
Another possibility to be explored is using top 
level dependencies (such as SUBJECT, 
SENTENCE, etc.) to rank and select the main 
topic and target candidates using sentence 
structure configuration. This approach would 
also ensure that once a polarity-laden cue is 
identified, the corresponding target could be 
uniquely identified. This linguistics-heavy 
approach is feasible only in texts whose 
characteristics more closely resemble the data 
used to train the parser. 
Our work has helped us focus more clearly many 
of the challenges faced by any NLP system when 
used in a new user-generated content: scarce 
development data, novel pattern and form 
adaptability, tool robustness, and scalability to 
massive and noisy text.  
One of the lessons learned during these 
experiences is that keeping a modular hybrid 
analysis framework can improve matching by 
either customizing the pipeline to each genre and 
                                                 
7 http://webmining.barcelonamedia.org/cometa/index_dates 
task requirements, or by combining the results of 
different approaches to benefit from each one?s 
strengths while minimizing each one?s 
weaknesses. Extracting opinion centered 
information from highly heterogeneous text and 
from multitudes of authors will never be as 
straightforward as, say, doing IE on newswire or 
financial news, but it should be feasible and 
useful by using the right toolset. We are in the 
process of using crowdsourcing to fully annotate 
vast Spanish and English corpora of opinionated 
text, which will allow us to perform a better and 
more fine-grained quantitative analysis of our 
framework in the near future. 
Another lesson learned is that even if high-
precision opinion classification is not available 
(because not enough development data is 
available, or data is noisy, or for whatever other 
reason) doing even superficial semantic 
annotation of the text and unsupervised 
clustering can help industrial consumer of these 
technologies understand better what is being said 
in the Social Media ecosystems. Valuable 
objectives for a useful opinion mining system do 
not need to include all possible analyses or state-
of-the-art performance. 
Going forward, computational exploitation of 
Social Media and of community-based, data-
driven discussions on diverse topics and products 
is definitely an important facet of future market 
and business intelligence competencies, since 
more and more of our activities as citizens, 
friends and consumers take place in an online 
environment, where everything seems possible 
but where also everything we do leaves a trace 
and has a meaning. Extracting the semantics of 
collective action enables us to access that 
meaning. 
References 
 
Ritter A, Clark S, Mausam, and Etzioni O (2011). 
Named Entity Recognition in Tweets: An 
Experimental Study. Proceedings of the 2011 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2011) 
Bhuiyan, T., Xu, Y., & Josang, A. (2009). State-of-
the-Art Review on Opinion Mining from Online 
Customers? Feedback. Proceedings of the 9th Asia-
Pacific Complex Systems Conference (pp. 385?
390). 
Coursey, K., Mihalcea, R., & Moen, W. (2009). Using 
encyclopedic knowledge for automatic topic 
identification. Proceedings of the Thirteenth 
Conference on Computational Natural Language 
Learning, CoNLL  ?09 (pp. 210?218). Stroudsburg, 
51
PA, USA: Association for Computational 
Linguistics.  
Ferrucci, D., & Lally, A. (2004). UIMA: an 
architectural approach to unstructured information 
processing in the corporate research environment. 
Natural Language Engineering, 10(3-4), 327?348. 
Hu, M., & Liu, B. (2004). Mining and summarizing 
customer reviews. Proceedings of the tenth ACM 
SIGKDD international conference on Knowledge 
discovery and data mining (pp. 168-177). Seattle, 
WA, USA: ACM. doi:10.1145/1014052.1014073 
Kim, S. M., & Hovy, E. (2006). Extracting opinions, 
opinion holders, and topics expressed in online 
news media text. Proceedings of the Workshop on 
Sentiment and Subjectivity in Text (pp. 1?8). 
Locke, B., & Martin, J. (2009). Named entity 
recognition: Adapting to microblogging. University 
of Colorado.  
Osi?ski and D. Weiss (2005), ?Carrot 2: Design of a 
flexible and efficient web information retrieval 
framework,? Advances in Web Intelligence, pp. 
439?444, 2005. 
Pang, B., & Lee, L. (2008). Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2), 1?135. 
Popescu, A. M., & Etzioni, O. (2005). Extracting 
product features and opinions from reviews. 
Proceedings of HLT/EMNLP (Vol. 5, pp. 339?
346). 
 
 
 
 
52
