Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 17?24,
New York City, June 2006. c?2006 Association for Computational Linguistics
Term Generalization and Synonym Resolution for Biological Abstracts:
Using the Gene Ontology for Subcellular Localization Prediction
Alona Fyshe
Department of Computing Science
University of Alberta
Edmonton, Alberta T6G 2E8
alona@cs.ualberta.ca
Duane Szafron
Department of Computing Science
University of Alberta
Edmonton, Alberta T6G 2E8
duane@cs.ualberta.ca
Abstract
The field of molecular biology is growing
at an astounding rate and research findings
are being deposited into public databases,
such as Swiss-Prot. Many of the over
200,000 protein entries in Swiss-Prot 49.1
lack annotations such as subcellular lo-
calization or function, but the vast major-
ity have references to journal abstracts de-
scribing related research. These abstracts
represent a huge amount of information
that could be used to generate annotations
for proteins automatically. Training clas-
sifiers to perform text categorization on
abstracts is one way to accomplish this
task. We present a method for improving
text classification for biological journal
abstracts by generating additional text fea-
tures using the knowledge represented in
a biological concept hierarchy (the Gene
Ontology). The structure of the ontology,
as well as the synonyms recorded in it, are
leveraged by our simple technique to sig-
nificantly improve the F-measure of sub-
cellular localization text classifiers by as
much as 0.078 and we achieve F-measures
as high as 0.935.
1 Introduction
Can computers extract the semantic content of aca-
demic journal abstracts? This paper explores the use
of natural language techniques for processing bio-
logical abstracts to answer this question in a specific
domain. Our prototype method predicts the subcel-
lular localization of proteins (the part of the biolog-
ical cell where a protein performs its function) by
performing text classification on related journal ab-
stracts.
In the last two decades, there has been explosive
growth in molecular biology research. Molecular bi-
ologists organize their findings into a common set
of databases. One such database is Swiss-Prot, in
which each entry corresponds to a protein. As of
version 49.1 (February 21, 2006) Swiss-Prot con-
tains more than 200,000 proteins, 190,000 of which
link to biological journal abstracts. Unfortunately, a
much smaller percentage of protein entries are anno-
tated with other types of information. For example,
only about half the entries have subcellular localiza-
tion annotations. This disparity is partially due to
the fact that humans annotate these databases manu-
ally and cannot keep up with the influx of data. If a
computer could be trained to produce annotations by
processing journal abstracts, proteins in the Swiss-
Prot database could be curated semi-automatically.
Document classification is the process of cate-
gorizing a set of text documents into one or more
of a predefined set of classes. The classification
of biological abstracts is an interesting specializa-
tion of general document classification, in that sci-
entific language is often not understandable by, nor
written for, the lay-person. It is full of specialized
terms, acronyms and it often displays high levels
of synonymy. For example, the ?PAM complex?,
which exists in the mitochondrion of the biologi-
cal cell is also referred to with the phrases ?pre-
sequence translocase-associated import motor? and
17
?mitochondrial import motor?. This also illustrates
the fact that biological terms often span word bound-
aries and so their collective meaning is lost when
text is whitespace tokenized.
To overcome the challenges of scientific lan-
guage, our technique employs the Gene Ontology
(GO) (Ashburner et al 2000) as a source of expert
knowledge. The GO is a controlled vocabulary of
biological terms developed and maintained by biol-
ogists. In this paper we use the knowledge repre-
sented by the GO to complement the information
present in journal abstracts. Specifically we show
that:
? the GO can be used as a thesaurus
? the hierarchical structure of the GO can be used
to generalize specific terms into broad concepts
? simple techniques using the GO significantly
improve text classification
Although biological abstracts are challenging
documents to classify, solving this problem will
yield important benefits. With sufficiently accurate
text classifiers, the abstracts of Swiss-Prot entries
could be used to automatically annotate correspond-
ing proteins, meaning biologists could more effi-
ciently identify proteins of interest. Less time spent
sifting through unannotated proteins translates into
more time spent on new science, performing impor-
tant experiments and uncovering fresh knowledge.
2 Related Work
Several different learning algorithms have been ex-
plored for text classification (Dumais et al 1998)
and support vector machines (SVMs) (Vapnik,
1995) were found to be the most computationally ef-
ficient and to have the highest precision/recall break-
even point (BEP, the point where precision equals
recall). Joachims performed a very thorough evalu-
ation of the suitability of SVMs for text classifica-
tion (Joachims, 1998). Joachims states that SVMs
are perfect for textual data as it produces sparse
training instances in very high dimensional space.
Soon after Joachims? survey, researchers started
using SVMs to classify biological journal abstracts.
Stapley et al (2002) used SVMs to predict the sub-
cellular localization of yeast proteins. They created
a data set by mining Medline for abstracts contain-
ing a yeast gene name, which achieved F-measures
in the range [0.31,0.80]. F-measure is defined as
f =
2rp
r + p
where p is precision and r is recall. They expanded
their training data to include extra biological infor-
mation about each protein, in the form of amino acid
content, and raised their F-measure by as much as
0.05. These results are modest, but before Stapley
et al most localization classification systems were
built using text rules or were sequence based. This
was one of the first applications of SVMs to bio-
logical journal abstracts and it showed that text and
amino acid composition together yield better results
than either alone.
Properties of proteins themselves were again used
to improve text categorization for animal, plant and
fungi subcellular localization data sets (Ho?glund
et al 2006). The authors? text classifiers were
based on the most distinguishing terms of docu-
ments, and they included the output of four pro-
tein sequence classifiers in their training data. They
measure the performance of their classifier using
what they call sensitivity and specificity, though
the formulas cited are the standard definitions of
recall and precision. Their text-only classifier for
the animal MultiLoc data set had recall (sensitivity)
in the range [0.51,0.93] and specificity (precision)
[0.32,0.91]. The MultiLocText classifiers, which
include sequence-based classifications, have recall
[0.82,0.93] and precision [0.55,0.95]. Their overall
and average accuracy increased by 16.2% and 9.0%
to 86.4% and 94.5% respectively on the PLOC an-
imal data set when text was augmented with addi-
tional sequence-based information.
Our method is motivated by the improvements
that Stapley et al and Ho?glund et al saw when they
included additional biological information. How-
ever, our technique uses knowledge of a textual na-
ture to improve text classification; it uses no infor-
mation from the amino acid sequence. Thus, our ap-
proach can be used in conjunction with techniques
that use properties of the protein sequence.
In non-biological domains, external knowledge
has already been used to improve text categoriza-
tion (Gabrilovich and Markovitch, 2005). In their
18
research, text categorization is applied to news docu-
ments, newsgroup archives and movie reviews. The
authors use the Open Directory Project (ODP) as a
source of world knowledge to help alleviate prob-
lems of polysemy and synonymy. The ODP is a
hierarchy of concepts where each concept node has
links to related web pages. The authors mined these
web pages to collect characteristic words for each
concept. Then a new document was mapped, based
on document similarity, to the closest matching ODP
concept and features were generated from that con-
cept?s meaningful words. The generated features,
along with the original document, were fed into an
SVM text classifier. This technique yielded BEP as
high as 0.695 and improvements of up to 0.254.
We use Gabrilovich and Markovitch?s (2005) idea
to employ an external knowledge hierarchy, in our
case the GO, as a source of information. It has
been shown that GO molecular function annotations
in Swiss-Prot are indicative of subcellular localiza-
tion annotations (Lu and Hunter, 2005), and that GO
node names made up about 6% of a sample Medline
corpus (Verspoor et al 2003). Some consider GO
terms to be too rare to be of use (Rice et al 2005),
however we will show that although the presence of
GO terms is slight, the terms are powerful enough to
improve text classification. Our technique?s success
may be due to the fact that we include the synonyms
of GO node names, which increases the number of
GO terms found in the documents.
We use the GO hierarchy in a different way than
Gabrilovich et al use the ODP. Unlike their ap-
proach, we do not extract additional features from all
articles associated with a node of the GO hierarchy.
Instead we use synonyms of nodes and the names
of ancestor nodes. This is a simpler approach, as
it doesn?t require retrieving all abstracts for all pro-
teins of a GO node. Nonetheless, we will show that
our approach is still effective.
3 Methods
The workflow used to perform our experiments is
outlined in Figure 1.
3.1 The Data Set
The first step in evaluating the usefulness of GO as
a knowledge source is to create a data set. This pro-
Set of 
Proteins
Retrieve 
Abstracts
Set of 
Abstracts
Process 
Abstracts
Data Set 1 Data Set 2 Data Set 3
a
b
Figure 1: The workflow used to create data sets used
in this paper. Abstracts are gathered for proteins
with known localization (process a). Treatments are
applied to abstracts to create three Data Sets (pro-
cess b).
cess begins with a set of proteins with known sub-
cellular localization annotations (Figure 1). For this
we use Proteome Analyst?s (PA) data sets (Lu et al
2004; Szafron et al 2004). The PA group used these
data sets to create very accurate subcellular classi-
fiers based on the keyword fields of Swiss-Prot en-
tries for homologous proteins. Here we use PA?s
current data set of proteins collected from Swiss-
Prot (version 48.3) and impose one further crite-
rion: the subcellular localization annotation may not
be longer than four words. This constraint is in-
troduced to avoid including proteins where the lo-
calization category was incorrectly extracted from a
long sentence describing several aspects of localiza-
tion. For example, consider the subcellular anno-
tation ?attached to the plasma membrane by a lipid
anchor?, which could mean the protein?s functional
components are either cytoplasmic or extracellular
(depending on which side of the plasma membrane
the protein is anchored). PA?s simple parsing scheme
could mistake this description as meaning that the
protein performs its function in the plasma mem-
brane. Our length constraint reduces the chances of
including mislabeled training instances in our data.
19
Class Number of Number
Name Proteins of Abstracts
cytoplasm 1664 4078
endoplasmic
reticulum 310 666
extracellular 2704 5655
golgi a 41 71
lysosome 129 599
mitochondrion 559 1228
nucleus 2445 5589
peroxisome 108 221
plasma
membrane a 15 38
Total 7652 17175
aClasses with less than 100 abstracts were considered to
have too little training data and are not included in our experi-
ments.
Table 1: Summary of our Data Set. Totals are less
than the sum of the rows because proteins may be-
long to more than one localization class.
PA has data sets for five organisms (animal, plant,
fungi, gram negative bacteria and gram positive bac-
teria). The animal data set was chosen for our study
because it is PA?s largest and medical research has
the most to gain from increased annotations for an-
imal proteins. PA?s data sets have binary labeling,
and each class has its own training file. For exam-
ple, in the nuclear data set a nuclear protein appears
with the label ?+1?, and non-nuclear proteins ap-
pear with the label ??1?. Our training data includes
317 proteins that localize to more than one location,
so they will appear with a positive label in more than
one data set. For example, a protein that is both cyto-
plasmic and peroxisomal will appear with the label
?+1? in both the peroxisomal and cytoplasmic sets,
and with the label ??1? in all other sets. Our data
set has 7652 proteins across 9 classes (Table 1). To
take advantage of the information in the abstracts of
proteins with multiple localizations, we use a one-
against-all classification model, rather than a ?single
most confident class? approach.
3.2 Retrieve Abstracts
Now that a set of proteins with known localiza-
tions has been created, we gather each protein?s
abstracts and abstract titles (Figure 1, process a).
We do not include full text because it can be dif-
ficult to obtain automatically and because using
full text does not improve F-measure (Sinclair and
Webber, 2004). Abstracts for each protein are re-
trieved using the PubMed IDs recorded in the Swiss-
Prot database. PubMed (http://www.pubmed.
gov) is a database of life science articles. It should
be noted that more than one protein in Swiss-Prot
may point to the same abstract in PubMed. Because
the performance of our classifiers is estimated us-
ing cross-validation (discussed in Section 3.4) it is
important that the same abstract does not appear in
both testing and training sets during any stage of
cross-validation. To address this problem, all ab-
stracts that appear more than once in the complete
set of abstracts are removed. The distribution of the
remaining abstracts among the 9 subcellular local-
ization classes is shown in Table 1. For simplicity,
the fact that an abstract may actually be discussing
more than one protein is ignored. However, because
we remove duplicate abstracts, many abstracts dis-
cussing more than one protein are eliminated.
In Table 1 there are more abstracts than proteins
because each protein may have more than one asso-
ciated abstract. Classes with less than 100 abstracts
were deemed to have too little information for train-
ing. This constraint eliminated plasma membrane
and golgi classes, although they remained as nega-
tive data for the other 7 training sets.
It is likely that not every abstract associated with
a protein will discuss subcellular localization. How-
ever, because the Swiss-Prot entries for proteins in
our data set have subcellular annotations, some re-
search must have been performed to ascertain local-
ization. Thus it should be reported in at least one
abstract. If the topics of the other abstracts are truly
unrelated to localization than their distribution of
words may be the same for all localization classes.
However, even if an abstract does not discuss local-
ization directly, it may discuss some other property
that is correlated with localization (e.g. function).
In this case, terms that differentiate between local-
ization classes will be found by the classifier.
3.3 Processing Abstracts
Three different data sets are made by processing our
retrieved abstracts (Figure 1, process b). An ex-
20
   We studied the 
effect of p123 on 
the regulation of 
osmotic pressure.
"studi?:1, 
?effect?:1,
?p123?:1,
?regul?:1,
"osmot?:1,
"pressur?:1
"studi?:1, 
?effect?:1,
?p123?:1,
?regul?:1,
"osmot?:1,
"pressur?:1,
"osmoregulation":1
"studi?:1, 
?effect?:1,
?p123?:1,
?regul?:1,
"osmot?:1,
"pressur?:1,
"osmoregulation":1,
"GO_homeostasis":1,
"GO_physiological 
process":1,
"GO_biological process":1
D
a
t
a
s
e
t
 
1
D
a
t
a
s
e
t
 
2
D
a
t
a
s
e
t
 
3
Figure 2: A sentence illustrating our three meth-
ods of abstract processing. Data Set 1 is our base-
line, Data Set 2 incorporates synonym resolution
and Data Set 3 incorporates synonym resolution and
term generalization. Word counts are shown here for
simplicity, though our experiments use TFIDF.
ample illustrating our three processing techniques is
shown in Figure 2.
In Data Set 1, abstracts are tokenized and each
word is stemmed using Porter?s stemming algo-
rithm (Porter, 1980). The words are then trans-
formed into a vector of <word,TFIDF> pairs.
TFIDF is defined as:
TFIDF (wi) = f(wi) ? log(
n
D(wi)
)
where f(wi) is the number of times word wi ap-
pears in documents associated with a protein, n is
the total number of training documents and D(wi)
is the number of documents in the whole training
set that contain the word wi. TFIDF was first pro-
posed by Salton and Buckley (1998) and has been
used extensively in various forms for text catego-
rization (Joachims, 1998; Stapley et al 2002). The
words from all abstracts for a single protein are
amalgamated into one ?bag of words? that becomes
the training instance which represents the protein.
3.3.1 Synonym Resolution
The GO hierarchy can act as a thesaurus for
words with synonyms. For example the GO encodes
the fact that ?metabolic process? is a synonym for
?metabolism?(see Figure 3). Data Set 2 uses GO?s
?exact synonym? field for synonym resolution and
adds extra features to the vector of words from Data
Set 1. We search a stemmed version of the abstracts
regulation of 
osmotic pressure
biological 
process
physiological 
process
homeostasis metabolism
growth
thermo-
regulation
osmo-
regulation
metabolic 
process
Figure 3: A subgraph of the GO biological process
hierarchy. GO nodes are shown as ovals, synonyms
appear as grey rectangles.
for matches to stemmed GO node names or syn-
onyms. If a match is found, the GO node name
(deemed the canonical representative for its set of
synonyms) is associated with the abstract. In Fig-
ure 2 the phrase ?regulation of osmotic pressure?
appears in the text. A lookup in the GO synonym
dictionary will indicate that this is an exact synonym
of the GO node ?osmoregulation?. Therefore we as-
sociated the term ?osmoregulation? with the training
instance. This approach combines the weight of sev-
eral synonyms into one representative, allowing the
SVM to more accurately model the author?s intent,
and identifies multi-word phrases that are otherwise
lost during tokenization. Table 2 shows the increase
in average number of features per training instance
as a result of our synonym resolution technique.
3.3.2 Term Generalization
In order to express the relationships between
terms, the GO hierarchy is organized in a directed
acyclic graph (DAG). For example, ?thermoregula-
tion? is a type of ?homeostasis?, which is a ?phys-
iological process?. This ?is a? relationship is ex-
pressed as a series of parent-child relationships (see
Figure 3). In Data Set 3 we use the GO for synonym
resolution (as in Data Set 2) and we also use its hi-
erarchical structure to generalize specific terms into
broader concepts. For Data Set 3, if a GO node name
(or synonym) is found in an abstract, all names of
ancestors to the match in the text are included in the
21
Class Data Data Data
Set 1 Set 2 Set 3
cytoplasm 166 177 203
endoplasmic
reticulum 162 171 192
extracellular 148 155 171
lysosome 244 255 285
mitochondrion 155 163 186
nucleus 147 158 183
peroxisome 147 156 182
Overall Average 167 176 200
Table 2: Average number of features per training
instance for 7 subcellular localization categories in
animals. Data Set 1 is the baseline, Data Set 2 in-
corporates synonym resolution and Data Set 3 uses
synonym resolution and term generalization.
training instance along with word vectors from Data
Set 2 (see Figure 2). These additional node names
are prepended with the string ?GO ? which allows
the SVM to differentiate between the case where a
GO node name appears exactly in text and the case
where a GO node name?s child appeared in the text
and the ancestor was added by generalization. Term
generalization increases the average number of fea-
tures per training instance (Table 2).
Term generalization gives the SVM algorithm the
opportunity to learn correlations that exist between
general terms and subcellular localization even if
the general term never appears in an abstract and
we encounter only its more specific children. With-
out term generalization the SVM has no concept of
the relationship between child and parent terms, nor
between sibling terms. For some localization cate-
gories more general terms may be the most informa-
tive and in other cases specific terms may be best.
Because our technique adds features to training in-
stances and never removes any, the SVM can as-
sign lower weights to the generalized terms in cases
where the localization category demands it.
3.4 Evaluation
Each of our classifiers was evaluated using 10 fold
cross-validation. In 10 fold cross-validation each
Data Set is split into 10 stratified partitions. For the
first ?fold?, a classifier is trained on 9 of the 10 par-
titions and the tenth partition is used to test the clas-
sifier. This is repeated for nine more folds, holding
out a different tenth each time. The results of all
10 folds are combined and composite precision, re-
call and F-measures are computed. Cross-validation
accurately estimates prediction statistics of a classi-
fier, since each instance is used as a test case at some
point during validation.
The SVM implementation libSVM (Chang and
Lin, 2001) was used to conduct our experiments. A
linear kernel and default parameters were used in all
cases; no parameter searching was done. Precision,
recall and F-measure were calculated for each ex-
periment.
4 Results and Discussion
Results of 10 fold cross-validation are reported in
Table 3. Data Set 1 represents the baseline, while
Data Sets 2 and 3 represent synonym resolution and
combined synonym resolution/term generalization
respectively. Paired t-tests (p=0.05) were done be-
tween the baseline, synonym resolution and term
generalization Data Sets, where each sample is one
fold of cross-validation. Those classifiers with sig-
nificantly better performance over the baseline ap-
pear in bold in Table 3. For example, the lysosome
classifiers trained on Data Set 2 and 3 are both sig-
nificantly better than the baseline, and results for
Data Set 3 are significantly better than results for
Data Set 2, signified with an asterisk. In the case
of the nucleus classifier no abstract processing tech-
nique was significantly better, so no column appears
in bold.
In six of the seven classes, classifiers trained on
Data Set 2 are significantly better than the base-
line, and in no case are they worse. In Data Set
3, five of the seven classifiers are significantly bet-
ter than the baseline, and in no case are they worse.
For the lysosome and peroxisome classes our com-
bined synonym resolution/term generalization tech-
nique produced results that are significantly better
than synonym resolution alone. The average results
of Data Set 2 are significantly better than Data Set
1 and the average results of Data Set 3 are signifi-
cantly better than Data Set 2 and Data Set 1. On av-
erage, synonym resolution and term generalization
combined give an improvement of 3%, and synonym
22
Class
Data Set 1 Data Set 2 Data Set 3
Baseline Synonym Resolution Term Generalization
F-measure F-Measure ? F-Measure ?
cytoplasm 0.740 (?0.049) 0.758 (?0.042) +0.017 0.761 (?0.042) +0.021
endoplasmic
reticulum 0.760 (?0.055) 0.779 (?0.068) +0.019 0.786 (?0.072) +0.026
extracellular 0.931 (?0.009) 0.935 (?0.009) +0.004 0.935 (?0.010) +0.004
lysosome 0.746 (?0.107) 0.787 (? 0.100) +0.041 0.820* (?0.089) +0.074
mitochondrion 0.840 (?0.041) 0.848 (?0.038) +0.008 0.852 (?0.039) +0.012
nucleus 0.885 (?0.014) 0.885 (? 0.016) +0.001 0.887 (?0.019) +0.003
peroxisome 0.790 (?0.054) 0.823 (?0.042) +0.033 0.868* (?0.046) +0.078
Average 0.815 (?0.016) 0.832 (?0.012) +0.017 0.845* (?0.009) +0.030
Table 3: F-measures for stratified 10 fold cross-validation on our three Data Sets. Results deemed signifi-
cantly improved over the baseline (p=0.05) appear in bold, and those with an asterisk (*) are significantly
better than both other data sets. Change in F-measure compared to baseline is shown for Data Sets 2 and 3.
Standard deviation is shown in parentheses.
resolution alone yields a 1.7% improvement. Be-
cause term generalization and synonym resolution
never produce classifiers that are worse than syn-
onym resolution alone, and in some cases the result
is 7.8% better than the baseline, Data Set 3 can be
confidently used for text categorization of all seven
animal subcellular localization classes.
Our baseline SVM classifier performs quite well
compared to the baselines reported in related
work. At worst, our baseline classifier has F-
measure 0.740. The text only classifier reported
by Ho?glund et al has F-measure in the range
[0.449,0.851] (Ho?glund et al 2006) and the text
only classifiers presented by Stapley et al begin with
a baseline classifier with F-measure in the range
[0.31,0.80] (Stapley et al 2002). Although their
approaches gave a greater increase in performance
their low baselines left more room for improvement.
Though we use different data sets than Ho?glund
et al (2006), we compare our results to theirs on a
class by class basis. For those 7 localization classes
for which we both make predictions, the F-measure
of our classifiers trained on Data Set 3 exceed the F-
measures of the Ho?glund et al text only classifiers
in all cases, and our Data Set 3 classifier beats the F-
measure of the MutliLocText classifier for 5 classes
(see supplementary material http://www.cs.
ualberta.ca/?alona/bioNLP). In addition,
our technique does not preclude using techniques
presented by Ho?glund et al and Stapley et al, and
it may be that using a combination of our approach
and techniques involving protein sequence informa-
tion may result in an even stronger subcellular local-
ization predictor.
We do not assert that using abstract text alone is
the best way to predict subcellular localization, only
that if text is used, one must extract as much from
it as possible. We are currently working on incorpo-
rating the classifications given by our text classifiers
into Proteome Analyst?s subcellular classifier to im-
prove upon its already strong predictors (Lu et al
2004), as they do not currently use any information
present in the abstracts of homologous proteins.
5 Conclusion and Future work
Our study has shown that using an external informa-
tion source is beneficial when processing abstracts
from biological journals. The GO can be used as a
reference for both synonym resolution and term gen-
eralization for document classification and doing so
significantly increases the F-measure of most sub-
cellular localization classifiers for animal proteins.
On average, our improvements are modest, but they
indicate that further exploration of this technique is
warranted.
We are currently repeating our experiments for
PA?s other subcellular data sets and for function pre-
diction. Though our previous work with PA is not
23
text based, our experience training protein classifiers
has led us to believe that a technique that works well
for one protein property often succeeds for others
as well. For example our general function classifier
has F-measure within one percent of the F-measure
of our Animal subcellular classifier. Although we
test the technique presented here on subcellular lo-
calization only, we see no reason why it could not be
used to predict any protein property (general func-
tion, tissue specificity, relation to disease, etc.). Fi-
nally, although our results apply to text classification
for molecular biology, the principle of using an on-
tology that encodes synonyms and hierarchical re-
lationships may be applicable to other applications
with domain specific terminology.
The Data Sets used in these experiments are
available at http://www.cs.ualberta.ca/
?alona/bioNLP/.
6 Acknowledgments
We would like to thank Greg Kondrak, Colin Cherry,
Shane Bergsma and the whole NLP group at the
University of Alberta for their helpful feedback and
guidance. We also wish to thank Paul Lu, Rus-
sell Greiner, Kurt McMillan and the rest of the
Proteome Analyst team. This research was made
possible by financial support from the Natural Sci-
ences and Engineering Research Council of Canada
(NSERC), the Informatics Circle of Research Excel-
lence (iCORE) and the Alberta Ingenuity Centre for
Machine Learning (AICML).
References
Michael Ashburner et al 2000. Gene ontology: tool for
the unification of biology the gene ontology consor-
tium. Nature Genetics, 25(1):25?29.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Susan T. Dumais et al 1998. Inductive learning al-
gorithms and representations for text categorization.
In Proc. 7th International Conference on Information
and Knowledge Management CIKM, pages 148?155.
Evgeniy Gabrilovich and Shaul Markovitch. 2005. Fea-
ture generation for text categorization using world
knowledge. In IJCAI-05, Proceedings of the Nine-
teenth International Joint Conference on Artificial In-
telligence, pages 1048?1053.
Annette Ho?glund et al 2006. Significantly improved
prediction of subcellular localization by integrating
text and protein sequence data. In Pacific Symposium
on Biocomputing, pages 16?27.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML ?98: Proceedings of the 10th Eu-
ropean Conference on Machine Learning, pages 137?
142.
Zhiyong Lu and Lawrence Hunter. 2005. GO molecular
function terms are predictive of subcellular localiza-
tion. volume 10, pages 151?161.
Zhiyong Lu et al 2004. Predicting subcellular local-
ization of proteins using machine-learned classifiers.
Bioinformatics, 20(4):547?556.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Simon B Rice et al 2005. Mining protein function from
text using term-based support vector machines. BMC
Bioinformatics, 6:S22.
Gail Sinclair and Bonnie Webber. 2004. Classification
from full text: A comparison of canonical sections
of scientific papers. In COLING 2004 International
Joint workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA/BioNLP)
2004, pages 69?72.
B. J. Stapley et al 2002. Predicting the sub-cellular lo-
cation of proteins from text using support vector ma-
chines. In Pacific Symposium on Biocomputing, pages
374?385.
Duane Szafron et al 2004. Proteome analyst: Custom
predictions with explanations in a web-based tool for
high-throughput proteome annotations. Nucleic Acids
Research, 32:W365?W371.
Vladimir N Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Cornelia M. Verspoor et al 2003. The gene ontology as a
source of lexical semantic knowledge for a biological
natural language processing application. Proceedings
of the SIGIR?03 Workshop on Text Analysis and Search
for Bioinformatics.
24
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489?499,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Interpretable Semantic Vectors from a Joint Model of Brain- and Text-
Based Meaning
Alona Fyshe
1
, Partha P. Talukdar
1
, Brian Murphy
2
, Tom M. Mitchell
1
1
Machine Learning Department, Carnegie Mellon University
2
School of Electronics, Electrical Engineering and Computer Science
Queen?s University Belfast
[afyshe,partha.talukdar,tom.mitchell]@cs.cmu.edu
brian.murphy@qub.ac.uk
Abstract
Vector space models (VSMs) represent
word meanings as points in a high dimen-
sional space. VSMs are typically created
using a large text corpora, and so repre-
sent word semantics as observed in text.
We present a new algorithm (JNNSE) that
can incorporate a measure of semantics
not previously used to create VSMs: brain
activation data recorded while people read
words. The resulting model takes advan-
tage of the complementary strengths and
weaknesses of corpus and brain activation
data to give a more complete representa-
tion of semantics. Evaluations show that
the model 1) matches a behavioral mea-
sure of semantics more closely, 2) can
be used to predict corpus data for unseen
words and 3) has predictive power that
generalizes across brain imaging technolo-
gies and across subjects. We believe that
the model is thus a more faithful represen-
tation of mental vocabularies.
1 Introduction
Vector Space Models (VSMs) represent lexical
meaning by assigning each word a point in high di-
mensional space. Beyond their use in NLP appli-
cations, they are of interest to cognitive scientists
as an objective and data-driven method to discover
word meanings (Landauer and Dumais, 1997).
Typically, VSMs are created by collecting word
usage statistics from large amounts of text data and
applying some dimensionality reduction technique
like Singular Value Decomposition (SVD). The
basic assumption is that semantics drives a per-
son?s language production behavior, and as a result
co-occurrence patterns in written text indirectly
encode word meaning. The raw co-occurrence
statistics are unwieldy, but in the compressed
VSM the distance between any two words is con-
ceived to represent their mutual semantic similar-
ity (Sahlgren, 2006; Turney and Pantel, 2010), as
perceived and judged by speakers. This space then
reflects the ?semantic ground truth? of shared lex-
ical meanings in a language community?s vocab-
ulary. However corpus-based VSMs have been
criticized as being noisy or incomplete representa-
tions of meaning (Glenberg and Robertson, 2000).
For example, multiple word senses collide in the
same vector, and noise from mis-parsed sentences
or spam documents can interfere with the final se-
mantic representation.
When a person is reading or writing, the se-
mantic content of each word will be necessarily
activated in the mind, and so in patterns of ac-
tivity over individual neurons. In principle then,
brain activity could replace corpus data as input
to a VSM, and contemporary imaging techniques
allow us to attempt this. Functional Magnetic Res-
onance Imaging (fMRI) and Magnetoencephalog-
raphy (MEG) are two brain activation recording
technologies that measure neuronal activation in
aggregate, and have been shown to have a pre-
dictive relationship with models of word mean-
ing (Mitchell et al, 2008; Palatucci et al, 2009;
Sudre et al, 2012; Murphy et al, 2012b).
1
If brain activation data encodes semantics, we
theorized that including brain data in a model of
semantics could result in a model more consistent
with semantic ground truth. However, the inclu-
sion of brain data will only improve a text-based
model if brain data contains semantic information
not readily available in the corpus. In addition,
if a semantic test involves another subject?s brain
activation data, performance can improve only if
the additional semantic information is consistent
across brains. Of course, brains differ in shape,
size and in connectivity, so additional information
encoded in one brain might not translate to an-
1
For more details on fMRI and MEG, see Section 4.2
489
other. Furthermore, different brain imaging tech-
nologies measure very different correlates of neu-
ronal activity. Due to these differences, it is possi-
ble that one subject?s brain activation data cannot
improve a model?s performance on another sub-
ject?s brain data, or for brain data collected using
a different recording technology. Indeed, inter-
subject models of brain activation is an open re-
search area (Conroy et al, 2013), as is learning the
relationship between recording technologies (En-
gell et al, 2012; Hall et al, 2013). Brain data
can also be corrupted by many types of noise (e.g.
recording room interference, movement artifacts),
another possible hindrance to the use of brain data
in VSMs.
VSMs are interesting from both engineering
and scientific standpoints. In this work we fo-
cus on the scientific question: Can the inclusion
of brain data improve semantic representations
learned from corpus data? What can we learn from
such a model? From an engineering perspective,
brain activation data will likely never replace text
data. Brain activation recordings are both expen-
sive and time consuming to collect, whereas tex-
tual data is vast and much of it is free to download.
However, from a scientific perspective, combining
text and brain data could lead to more consistent
semantic models, in turn leading to a better un-
derstanding of semantics and semantic modeling
generally.
In this paper, we leverage both kinds of data to
build a hybrid VSM using a new matrix factor-
ization method (JNNSE). Our hypothesis is that
the noise of brain and corpus derived statistics
will be largely orthogonal, and so the two data
sources will have complementary strengths as in-
put to VSMs. If this hypothesis is correct, we
should find that the resulting VSM is more suc-
cessful in modeling word semantics as encoded in
human judgements, as well as separate corpus and
brain data that was not used in the derivation of the
model. We will show that our method:
1. creates a VSM that is more correlated to an
independent measure of word semantics.
2. produces word vectors that are more pre-
dictable from the brain activity of different
people, even when brain data is collected
with a different recording technology.
3. predicts corpus representations of withheld
words more accurately than a model that does
not combine data sources.
4. directly maps semantic concepts onto the
brain by jointly learning neural representa-
tions.
Together, these results suggest that corpus and
brain activation data measure semantics in com-
patible and complimentary ways. Our results
are evidence that a joint model of brain- and
text-based semantics may be closer to seman-
tic ground truth than text-only models. Our
findings also indicate that there is additional se-
mantic information available in brain activation
data that is not present in corpus data, and that
there are elements of semantics currently lack-
ing in text-based VSMs. We have made avail-
able the top performing VSMs created with brain
and text data (http://www.cs.cmu.edu/
?
afyshe/papers/acl2014/).
In the following sections we will review NNSE,
and our extension, JNNSE. We will describe the
data used and the experiments to support our posi-
tion that brain data is a valuable source of semantic
information that compliments text data.
2 Non-Negative Sparse Embedding
Non-Negative Sparse Embedding (NNSE) (Mur-
phy et al, 2012a) is an algorithm that produces
a latent representation using matrix factorization.
Standard NNSE begins with a matrix X ? R
w?c
made of c corpus statistics for w words. NNSE
solves the following objective function:
argmin
A,D
w
?
i=1
?
?
X
i,:
?A
i,:
?D
?
?
2
+ ?
?
?
A
?
?
1
(1)
subject to: D
i,:
D
T
i,:
? 1,? 1 ? i ? ` (2)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? ` (3)
The solution will find a matrix A ? R
w?`
that is
sparse, non-negative, and represents word seman-
tics in an `-dimensional latent space. D ? R
`?c
gives the encoding of corpus statistics in the la-
tent space. Together, they factor the original cor-
pus statistics matrix X in a way that minimizes
the reconstruction error. TheL
1
constraint encour-
ages sparsity in A; ? is a hyperparameter. Equa-
tion 2 constrains D to eliminate solutions where
A is made arbitrarily small by making D arbi-
trarily large. Equation 3 ensures that A is non-
negative. We may increase ` to give more dimen-
sional space to represent word semantics, or de-
crease ` for more compact representations.
490
The sparse and non-negative representation in
A produces a more interpretable semantic space,
where interpretability is quantified with a behav-
ioral task (Chang et al, 2009; Murphy et al,
2012a). To illustrate the interpretability of NNSE,
we describe a word by selecting the word?s top
scoring dimensions, and selecting the top scoring
words in those dimensions. For example, the word
chair has the following top scoring dimensions:
1. chairs, seating, couches;
2. mattress, futon, mattresses;
3. supervisor, coordinator, advisor.
These dimensions cover two of the distinct mean-
ings of the word chair (furniture and person of
power).
NNSE?s sparsity constraint dictates that each
word can have a non-zero score in only a few di-
mensions, which aligns well to previous feature
elicitation experiments in psychology. In feature
elicitation, participants are asked to name the char-
acteristics (features) of an object. The number of
characteristics named is usually small (McRae et
al., 2005), which supports the requirement of spar-
sity in the learned latent space.
3 Joint Non-Negative Sparse Embedding
We extend NNSEs to incorporate an additional
source of data for a subset of the words in X ,
and call the approach Joint Non-Negative Sparse
Embeddings (JNNSEs). The JNNSE algorithm
is general enough to incorporate any new infor-
mation about the a word w, but for this study
we will focus on brain activation recordings of
a human subject reading single words. We
will incorporate either fMRI or MEG data, and
call the resulting models JNNSE(fMRI+Text) and
JNNSE(MEG+Text) and refer to them generally
as JNNSE(Brain+Text). For clarity, from here
on, we will refer to NNSE as NNSE(Text), or
NNSE(Brain) depending on the single source of
input data used.
Let us order the rows of the corpus data X so
that the first 1 . . . w
?
rows have both corpus statis-
tics and brain activation recordings. Each brain
activation recording is a row in the brain data ma-
trix Y ? R
w
?
?v
where v is the number of features
derived from the recording. For MEG recordings,
v =sensors ? time points= 306? 150. For fMRI
v = grey-matter voxels =' 20, 000 depending on
the brain anatomy of each individual subject. The
new objective function is:
argmin
A,D
(c)
,D
(b)
w
?
i=1
?
?
X
i,:
?A
i,:
?D
(c)
?
?
2
+
w
?
?
i=1
?
?
Y
i,:
?A
i,:
?D
(b)
?
?
2
+ ?
?
?
A
?
?
1
(4)
subject to: D
(c)
i,:
D
(c)
i,:
T
? 1, ? 1 ? i ? ` (5)
D
(b)
i,:
D
(b)
i,:
T
? 1,? 1 ? i ? ` (6)
A
i,j
? 0, 1 ? i ? w, 1 ? j ? `
(7)
We have introduced an additional constraint on the
rows 1 . . . w
?
, requiring that some of the learned
representations in A also reconstruct the brain ac-
tivation recordings (Y ) through representations in
D
(b)
? R
`?v
. Let us use A
?
to refer to the brain-
constrained rows of A. Words that are close in
?brain space? must have similar representations in
A
?
, which can further percolate to affect the rep-
resentations of other words in A via closeness in
?corpus space?.
With A or D fixed, the objective function for
NNSE(Text) and JNNSE(Brain+Text) is convex.
However, we are solving forA andD, so the prob-
lem is non-convex. To solve for this objective, we
use the online algorithm of Section 3 from Mairal
et al (Mairal et al, 2010). This algorithm is
guaranteed to converge, and in practice we found
that JNNSE(Brain+Text) converged as quickly as
NNSE(Text) for the same `. We used the SPAMS
package
2
to solve, and set ? = 0.025. This al-
gorithm was a very easy extension to NNSE(Text)
and required very little additional tuning.
We also consider learning shared representa-
tions in the case where data X and Y contain the
effects of known disjoint features. For example,
when a person reads a word, the recorded brain
activation data Y will contain the physiological
response to viewing the stimulus, which is unre-
lated to the semantics of the word. These sig-
nals can be attributed to, for example, the num-
ber of letters in the word and the number of white
pixels on the screen (Sudre et al, 2012). To ac-
count for such effects in the data, we augment
A
?
with a set of n fixed, manually defined fea-
tures (e.g. word length) to create A
?
percept
?
R
w?(`+n)
. D
(b)
? R
(`+n)?v
is used withA
?
percept
,
2
SPAMS Package: http://spams-devel.gforge.inria.fr/
491
to reconstruct the brain data Y . More gener-
ally, one could instead allocate a certain num-
ber of latent features specific to X or Y, both of
which could be learned, as explored in some re-
lated work (Gupta et al, 2013). We use 11 per-
ceptual features that characterize the non-semantic
features of the word stimulus (for a list, see sup-
plementary material at http://www.cs.cmu.
edu/
?
afyshe/papers/acl2014/).
The JNNSE algorithm is advantageous in that
it can handle partially paired data. That is, the
algorithm does not require that every row in X
also have a row in Y . Fully paired data is a re-
quirement of many other approaches (White et al,
2012; Jia and Darrell, 2010). Our approach al-
lows us to leverage the semantic information in
corpus data even for words without brain activa-
tion recordings.
JNNSE(Brain+Text) does not require brain data
to be mapped to a common average brain, which
is often the case when one wants to generalize be-
tween human subjects. Such mappings can blur
and distort data, making it less useful for subse-
quent prediction steps. We avoid these mappings,
and instead use the fact that similar words elicit
similar brain activation within a subject. In the
JNNSE algorithm, it is this closeness in ?brain
space? that guides the creation of the latent space
A. Leveraging intra-subject distance measures
to study inter-subject encodings has been studied
previously (Kriegeskorte et al, 2008a; Raizada
and Connolly, 2012), and has even been used
across species (humans and primates) (Kriegesko-
rte et al, 2008b).
Though we restrict ourselves to using one sub-
ject per JNNSE(Brain+Text) model, the JNNSE
algorithm could easily be extended to include
data from multiple brain imaging experiments by
adding a new squared loss term for additional
brain data.
3.1 Related Work
Perhaps the most well known related approach
to joining data sources is Canonical Correlation
Analysis (CCA) (Hotelling, 1936), which has been
applied to brain activation data in the past (Rus-
tandi et al, 2009). CCA seeks two linear trans-
formations that maximally correlate two data sets
in the transformed form. CCA requires that the
data sources be paired (all rows in the corpus data
must have a corresponding brain data), as corre-
lation between points is integral to the objective.
To apply CCA to our data we would need to dis-
card the vast majority of our corpus data, and use
only the 60 rows of X with corresponding rows
in Y. While CCA holds the input data fixed and
maximally correlates the transformed form, we
hold the transformed form fixed and seek a solu-
tion that maximally correlates the reconstruction
(AD
(c)
or A
?
D
(b)
) with the data (X and Y respec-
tively). This shift in error compensation is what
allows our data to be only partially paired. While
a Bayesian formulation of CCA can handle miss-
ing data, our model has missing data for> 97% of
the full w ? (v + c) brain and corpus data matrix.
To our knowledge, this extreme amount of missing
data has not been explored with Bayesian CCA.
One could also use a topic model style formula-
tion to represent this semantic representation task.
Supervised topic models (Blei and McAuliffe,
2007) use a latent topic to generate two observed
outputs: words in a document and a categorical la-
bel for the document. The same idea could be ap-
plied here: the latent semantic representation gen-
erates the observed brain activity and corpus statis-
tics. Generative and discriminative models both
have their own strengths and weaknesses, gener-
ative models being particularly strong when data
sources are limited (Ng and Jordan, 2002). Our
task is an interesting blend of data-limited and
data-rich problem scenarios.
In the past, various pieces of additional informa-
tion have been incorporated into semantic models.
For example, models with behavioral data (Sil-
berer and Lapata, 2012) and models with visual
information (Bruni et al, 2011; Silberer et al,
2013) have both shown to improve semantic rep-
resentations. Other works have correlated VSMs
built with text or images with brain activation
data (Murphy et al, 2012b; Anderson et al, 2013).
To our knowledge, this work is the first to integrate
brain activation data into the construction of the
VSM.
4 Data
4.1 Corpus Data
The corpus statistics used here are the download-
able vectors from Fyshe et al (2013)
3
. They
are compiled from a 16 billion word subset of
ClueWeb09 (Callan and Hoy, 2009) and contain
two types of corpus features: dependency and doc-
ument features, found to be complimentary for
3
http://www.cs.cmu.edu/
?
afyshe/papers/
conll2013/
492
most tasks. Dependency statistics were derived
by dependency parsing the corpus and compil-
ing counts for all dependencies incident on the
word. Document statistics are word-document
co-occurrence counts. Count thresholding was
applied to reduce noise, and positive pointwise-
mutual-information (PPMI) (Church and Hanks,
1990) was applied to the counts. SVD was ap-
plied to the document and dependency statistics
and the top 1000 dimensions of each type were
retained. We selected the rows corresponding to
noun-tagged words (approx. 17000 words).
4.2 Brain Activation Data
We have MEG and fMRI data at our disposal.
MEG measures the magnetic field caused by many
thousands of neurons firing together, and has good
time resolution (1000 Hz) but poor spatial reso-
lution. fMRI measures the change in blood oxy-
genation that results from differential neural ac-
tivity, and has good spatial resolution but poor
time resolution (0.5-1 Hz). We have fMRI data
and MEG data for 18 subjects (9 in each imaging
modality) viewing 60 concrete nouns (Mitchell et
al., 2008; Sudre et al, 2012). The 60 words span
12 word categories (animals, buildings, tools, in-
sects, body parts, furniture, building parts, uten-
sils, vehicles, objects, clothing, food). Each of the
60 words was presented with a line drawing, so
word ambiguity is not an issue. For both record-
ing modalities, all trials for a particular word were
averaged together to create one training instance
per word, with 60 training instances in all for each
subject and imaging modality. More preprocess-
ing details appear in the supplementary material.
5 Experimental Results
Here we explore several variations of JNNSE and
NNSE formulations. For a comparison of the
models used, see Table 1.
5.1 Correlation to Behavioral Data
To test if our joint model of Brain+Text is closer
to semantic ground truth we compared the latent
representation A learned via JNNSE(Brain+Text)
or NNSE(Text) to an independent behavioral mea-
sure of semantics. We collected behavioral data
for the 60 nouns in the form of answers to 218
semantic questions. Answers were gathered with
Mechanical Turk. The full list of questions ap-
pear in the supplementary material. Some exam-
ple questions are:?Is it alive??, and ?Can it bend??.
Mechanical Turk users were asked to respond to
each question for each word on a scale of 1-5. At
least 3 respondents answered each question and
the median score was used. This gives us a se-
mantic representation of each of the 60 words in
a 218-dimensional behavioral space. Because we
required answers to each of the questions for all
words, we do not have the problems of sparsity
that exist for feature production norms from other
studies (McRae et al, 2005). In addition, our an-
swers are ratings, rather than binary yes/no an-
swers.
For a given value of ` we solve the NNSE(Text)
and JNNSE(Brain+Text) objective function as de-
tailed in Equation 1 and 4 respectively. We com-
pared JNNSE(Brain+Text) and NNSE(Text) mod-
els by measuring the correlation of all pairwise
distances in JNNSE(Brain+Text) and NNSE(Text)
space to the pairwise distances in the 218-
dimensional semantic space. Distances were
calculated using normalized Euclidean distance
(equivalent in rank-ordering to cosine distance,
but more suitable for sparse vectors). Figure 1
shows the results of this correlation test. The er-
ror bars for the JNNSE(Brain+Text) models rep-
resent a 95% confidence interval calculated using
the standard error of the mean (SEM) over the 9
person-specific JNNSE(Brain+Text) models. Be-
cause there is only one NNSE(Text) model for
each dimension setting, no SEM can be calculated,
but it suffices to show that the NNSE(Text) corre-
lation does not fall into the 95% confidence inter-
val of the JNNSE(Brain+Text) models. The SVD
matrix for the original corpus data has correlation
0.4279 to the behavioral data, also below the 95%
confidence interval for all JNNSE models. The re-
sults show that a model that incorporates brain ac-
tivation data is more faithful to a behavioral mea-
sure of semantics.
5.2 Word Prediction from Brain Activation
We now show that the JNNSE(Brain+Text) vec-
tors are more consistent with independent sam-
ples of brain activity collected from different sub-
jects, even when recorded using different record-
ing technologies. As previously mentioned, be-
cause there is a large degree of variation between
brains and because MEG and fMRI measure very
different correlates of neuronal activity, this type
of generalization has proven to be very challeng-
ing and is an open research question in the neuro-
science community.
The output A of the JNNSE(Brain+Text) or
493
Table 1: A Comparison of the models explored in this paper, and the data upon which they operate.
Model Name Section(s) Text Data Brain Data Withheld Data
NNSE(Text) 2, 5 X x -
NNSE(Brain) 2, 5.2.1, 5.3 x X -
JNNSE(Brain+Text) 3, 5 X X -
JNNSE(Brain+Text): Dropout task 5.2.2 X X subset of brain data
JNNSE(Brain+Text): Predict corpus 5.3 X X subset of text data
250 500 10000.4
0.42
0.44
0.46
0.48
0.5
Correlation of Semantic Question Distances to JNNSE(fMRI)
Number of Latent Dimensions
Cor
rela
tion
 
 JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 1: Correlation of JNNSE(Brain+Text) and
NNSE(Text) models with the distances in a se-
mantic space constructed from behavioral data.
Error bars indicate SEM.
NNSE(Text) algorithm can be used as a VSM,
which we use for the task of word prediction from
fMRI or MEG recordings. A JNNSE(Brain+Text)
created with a particular human subject?s data is
never used in the prediction framework with that
same subject. For example, if we use fMRI data
from subject 1 to create a JNNSE(fMRI+Text), we
will test it with the remaining 8 fMRI subjects, but
all 9 MEG subjects (fMRI and MEG subjects are
disjoint).
Let us call the VSM learned with
JNNSE(Brain+Text) or NNSE(Text) the se-
mantic vectors. We can train a weight matrix W
that predicts the semantic vector a of a word from
that word?s brain activation vector x: a = Wx.
W can be learned with a variety of methods, we
will use L
2
regularized regression. One can also
train regressors that predict the brain activation
data from the semantic vector: x = Wa, but we
have found this to give lower predictive accuracy.
Note that we must re-train our weight matrix W
for each subject (instead of re-using D
(b)
from
Equation 4) because testing always occurs on a
different subject, and the brain activation data is
not inter-subject aligned.
We train ` independent L
2
regularized regres-
sors to predict the `-dimensional vectors a =
{a
1
. . . a
`
}. The predictions are concatenated
to produce a predicted semantic vector: a? =
{a?
1
, . . . , a?
`
}. We assess word prediction perfor-
mance by testing if the model can differentiate be-
tween two unseen words, a task named 2 vs. 2 pre-
diction (Mitchell et al, 2008; Sudre et al, 2012).
We choose the assignment of the two held out se-
mantic vectors (a
(1)
,a
(2)
) to predicted semantic
vectors (a?
(1)
, a?
(2)
) that minimizes the sum of the
two normalized Euclidean distances. 2 vs. 2 ac-
curacy is the percentage of tests where the correct
assignment is chosen.
The 60 nouns fall into 12 word categories.
Words in the same word category (e.g. screw-
driver and hammer) are closer in semantic space
than words in different word categories, which
makes some 2 vs. 2 tests more difficult than oth-
ers. We choose 150 random pairs of words (with
each word represented equally) to estimate the dif-
ficulty of a typical word pair, without having to
test all
(
60
2
)
word pairs. The same 150 random
pairs are used for all subjects and all VSMs. Ex-
pected chance performance on the 2 vs. 2 test is
50%.
Results for testing on fMRI data in the
2 vs. 2 framework appear in Figure 2.
JNNSE(fMRI+Text) data performed on aver-
age 6% better than the best NNSE(Text), and
exceeding even the original SVD corpus represen-
tations while maintaining interpretability. These
results generalize across brain activity recording
types; JNNSE(MEG+Text) performs as well as
JNNSE(fMRI+Text) when tested on fMRI data.
The results are consistent when testing on MEG
data: JNNSE(MEG+Text) or JNNSE(fMRI+Text)
outperforms NNSE(Text) (see Figure 3).
494
250 500 1000
64
66
68
70
72
74
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on fMRI data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 2: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
fMRI data. Models created with one subject?s
fMRI data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
250 500 1000
66
68
70
72
74
76
78
80
82
Number of Latent Dimensions
2 vs
. 2 A
ccu
racy
2 vs. 2 Acc. for JNNSE and NNSE, tested on MEG data
 
 
JNNSE(fMRI+Text)JNNSE(MEG+Text)NNSE(Text)SVD(Text)
Figure 3: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
MEG data. Models created with one subject?s
MEG data were not used to compute 2 vs. 2 ac-
curacy for that same subject.
NNSE(Text) performance decreases as the
number of latent dimension increases. This im-
plies that without the regularizing effect of brain
activation data, the extra NNSE(Text) dimensions
are being used to overfit to the corpus data, or
possibly to fit semantic properties not detectable
with current brain imaging technologies. How-
ever, when brain activation data is included, in-
creasing the number of latent dimensions strictly
increases performance for JNNSE(fMRI+Text).
JNNSE(MEG+Text) has peak performance with
500 latent dimensions, with ? 1% decrease in
performance at 1000 latent dimensions. In previ-
ous work, the ability to decode words from brain
activation data was found to improve with added
latent dimensions (Murphy et al, 2012a). Our
results may differ because our words are POS
tagged, and we included only nouns for the final
NNSE(Text) model. We found that with the orig-
inal ? = 0.05 setting from Murphy et al (Mur-
phy et al, 2012a) produced vectors that were too
sparse; four of the 60 test words had all-zero vec-
tors (JNNSE(Brain+Text) models did have any all-
zero vectors). To improve the NNSE(Text) vectors
for a fair comparison, we reduced ? = 0.025, un-
der which NNSE(Text) did not produce any all-
zero vectors for the 60 words.
Our results show that brain activation data con-
tributes additional information, which leads to an
increase in performance for the task of word pre-
diction from brain activation data. This suggests
that corpus-only models may not capture all rel-
evant semantic information. This conflicts with
previous studies which found that semantic vec-
tors culled from corpus statistics contain all of the
semantic information required to predict brain ac-
tivation (Bullinaria and Levy, 2013).
5.2.1 Prediction from a Brain-only Model
How much predictive power does the corpus data
provide to this word prediction task? To test
this, we calculated the 2 vs. 2 accuracy for a
NNSE(Brain) model trained on brain activation
data only. We train NNSE(Brain) with one sub-
ject?s data and use the resulting vectors to calculate
2 vs. 2 accuracy for the remaining subjects. We
have brain data for only 60 words, so using ` ? 60
latent dimensions leads to an under-constrained
system and a degenerate solution wherein only one
latent dimension is active for any word (and where
the brain data can be perfectly reconstructed). The
degenerate solution makes it impossible to gen-
eralize across words and leads to performance at
chance levels. An NNSE(MEG) trained on MEG
data gave maximum 2 vs. 2 accuracy of 67% when
` = 20. The reduced performance may be due to
the limited training data and the low SNR of the
data, but could also be attributed to the lack of cor-
pus information, which provides another piece of
semantic information.
495
5.2.2 Effect on Rows Without Brain Data
It is possible that some JNNSE(Brain+Text) di-
mensions are being used exclusively to fit brain
activation data, and not the semantics represented
in both brain and corpus data. If a particular
dimension j is solely used for brain data, the
sparsity constraint will favor solutions that sets
A
(i,j)
= 0 for i > w
?
(no brain data constraint),
and A
(i,j)
> 0 for some 0 ? i ? w
?
(brain data
constrained). We found that there were no such
dimensions in the JNNSE(Brain+Text). In fact for
the ` = 1000 JNNSE(Brain+Text), all latent di-
mensions had greater than ? 25% non-zero en-
tries, which implies that all dimensions are being
shared between the two data inputs (corpus and
brain activation), and are used to reconstruct both.
To test that the brain activation data is truly in-
fluencing rows of A not constrained by brain acti-
vation data, we performed a dropout test. We split
the original 60 words into two 30 word groups (as
evenly as possible across word categories). We
trained JNNSE(fMRI+Text) with 30 words, and
tested word prediction with the remaining 8 sub-
jects and the other 30 words. Thus, the training
and testing word sets are disjoint. Because of the
reduced size of the training data, we did see a drop
in performance, but JNNSE(fMRI+Text) vectors
still gave word prediction performance 7% higher
than NNSE(Text) vectors. Full results appear in
the supplementary material.
5.3 Predicting Corpus Data
Here we ask: can an accurate latent representa-
tion of a word be constructed using only brain
activation data? This task simulates the scenario
where there is no reliable corpus representation of
a word, but brain data is available. This scenario
may occur for seldom-used words that fall below
the thresholds used for the compilation of corpus
statistics. It could also be useful for acronym to-
kens (lol, omg) found in social media contexts
where the meaning of the token is actually a full
sentence.
We trained a JNNSE(fMRI+Text) with brain
data for all 60 words, but withhold the corpus data
for 30 of the 60 words (as evenly distributed as
possible amongst the 12 word categories). The
brain activation data for the 30 withheld words
will allow us to create latent representations in
A for withheld words. Simultaneously, we will
learn a mapping from the latent representation to
the corpus data (D
(c)
). This task cannot be per-
Table 2: Mean rank accuracy over 30 words
using corpus representations predicted by a
JNNSE(MEG+Text) model trained with some
rows of the corpus data withheld. Significance
is calculated using Fisher?s method to combine p-
values for each of the subject-dependent models.
Latent Dim size Rank Accuracy p-value
250 65.30 < 10
?19
500 67.37 < 10
?24
1000 63.47 < 10
?15
formed with a NNSE(Text) model because one
cannot learn a latent representation of a word with-
out data of some kind. This further emphasizes the
impact of brain imaging data, which will allow us
to generalize to previously unseen words in corpus
space.
We use the latent representations in A for each
of the words without corpus data and the mapping
to corpus space D
(c)
to predict the withheld cor-
pus data in X . We then rank the withheld rows of
X by their distance to the predicted row of X and
calculate the mean rank accuracy of the held out
words. Results in Table 2 show that we can recre-
ate the withheld corpus data using brain activation
data. Peak mean rank accuracy (67.37) is attained
at ` = 500 latent dimensions. This result shows
that neural semantic representations can create a
latent representation that is faithful to unseen cor-
pus statistics, providing further evidence that the
two data sources share a strong common element.
How much power is the remaining corpus data
supplying in scenarios where we withhold cor-
pus data? To answer this question, we trained an
NNSE(Brain) model on 30 words of brain activa-
tion, and then trained a regressor to predict cor-
pus data from those latent brain-only representa-
tions. We use the trained regressor to predict the
corpus data for the remaining 30 words. Peak per-
formance is attained at ` = 10 latent dimensions,
giving mean rank accuracy of 62.37, significantly
worse than the model that includes both corpus
and brain activation data (67.37).
5.4 Mapping Semantics onto the Brain
Because our method incorporates brain data into
an interpretable semantic model, we can directly
map semantic concepts onto the brain. To do
this, we examined the mappings from the latent
space to the brain space via D
(b)
. We found that
the most interpretable mappings come from mod-
496
!"#$%&'()
(a) D
(b)
matrix, subject P3, dimension with top words bath-
room, balcony, kitchen. MNI coordinates z=-12 (left) and z=-18
(right). Fusiform is associated with shelter words.
!"#$%&'$()*+
!(&%&'$()*+
(b) D
(b)
matrix; subject P1; dimension with top words ankle,
elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Pre-
and post-central areas are activated for body part words.
!"#$%&'(#)*+"#,$%
(c) D
(b)
matrix; subject P1; dimension with top scoring words
buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24
(right). Pars opercularis is believed to be part of the gustatory
cortex, which responds to food related words.
Figure 4: The mappings (D
(b)
) from latent se-
mantic space (A) to brain space (Y ) for fMRI and
words from three semantic categories. Shown are
representations of the fMRI slices such that the
back of the head is at the top of the image, the
front of the head is at the bottom.
els where the perceptual features had been scaled
down (divided by a constant factor), which en-
courages more of the data to be explained by
the semantic features in A. Figure 4 shows the
mappings (D
(b)
) for dimensions related to shel-
ter, food and body parts. The red areas align
with areas of the brain previously known to be
activated by the corresponding concepts (Mitchell
et al, 2008; Just et al, 2010). Our model
has learned these mappings in an unsupervised
setting by relating semantic knowledge gleaned
from word usage to patterns of activation in the
brain. This illustrates how the interpretability of
JNNSE can allow one to explore semantics in
the human brain. The mappings for one subject
are available for download (http://www.cs.
cmu.edu/
?
afyshe/papers/acl2014/).
6 Future Work and Conclusion
We are interested in pursuing many future projects
inspired by the success of this model. We would
like to extend the JNNSE algorithm to incorporate
data from multiple subjects, multiple modalities
and multiple experiments with non-overlapping
words. Including behavioral data and image data
is another possibility.
We have explored a model of semantics that in-
corporates text and brain activation data. Though
the number of words for which we have brain acti-
vation data is comparatively small, we have shown
that including even this small amount of data has
a positive impact on the learned latent representa-
tions, including for words without brain data. We
have provided evidence that the latent representa-
tions are closer to the neural representation of se-
mantics, and possibly, closer to semantic ground
truth. Our results reveal that there are aspects of
semantics not currently represented in text-based
VSMs, indicating that there may be room for im-
provement in either the data or algorithms used to
create VSMs. Our findings also indicate that using
the brain as a semantic test can separate models
that capture this additional semantic information
from those that do not. Thus, the brain is an im-
portant source of both training and testing data.
Acknowledgments
This work was supported in part by NIH un-
der award 5R01HD075328-02, by DARPA under
award FA8750-13-2-0005, and by a fellowship to
Alona Fyshe from the Multimodal Neuroimag-
ing Training Program funded by NIH awards
T90DA022761 and R90DA023420.
References
Andrew J Anderson, Elia Bruni, Ulisse Bordignon,
Massimo Poesio, and Marco Baroni. 2013. Of
words , eyes and brains : Correlating image-based
distributional semantic models with neural represen-
tations of concepts. In Proceedings of the Confer-
ence on Empirical Methods on Natural Language
Processing.
David M Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In Advances in Neural Information
Processing Systems, pages 1?22.
497
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP 2011 Geometrical Mod-
els for Natural Language Semantics (GEMS).
John A Bullinaria and Joseph P Levy. 2013. Limiting
factors for mapping corpus-based semantic repre-
sentations to brain activity. PloS one, 8(3):e57191,
January.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09
Dataset.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2009. Reading
Tea Leaves : How Humans Interpret Topic Models.
In Advances in Neural Information Processing Sys-
tems, pages 1?9.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Bryan R Conroy, Benjamin D Singer, J Swaroop Gun-
tupalli, Peter J Ramadge, and James V Haxby. 2013.
Inter-subject alignment of human cortical anatomy
using functional connectivity. NeuroImage, 81:400?
11, November.
Andrew D Engell, Scott Huettel, and Gregory Mc-
Carthy. 2012. The fMRI BOLD signal tracks elec-
trophysiological spectral perturbations, not event-
related potentials. NeuroImage, 59(3):2600?6,
February.
Alona Fyshe, Partha Talukdar, Brian Murphy, and Tom
Mitchell. 2013. Documents and Dependencies : an
Exploration of Vector Space Models for Semantic
Composition. In Computational Natural Language
Learning, Sofia, Bulgaria.
Arthur M Glenberg and David a Robertson. 2000.
Symbol Grounding and Meaning: A Compari-
son of High-Dimensional and Embodied Theories
of Meaning. Journal of Memory and Language,
43(3):379?401, October.
Sunil Kumar Gupta, Dinh Phung, Brett Adams, and
Svetha Venkatesh. 2013. Regularized nonnegative
shared subspace learning. Data Mining and Knowl-
edge Discovery, 26(1):57?97.
Emma L Hall, Si?an E Robson, Peter G Morris, and
Matthew J Brookes. 2013. The relationship be-
tween MEG and fMRI. NeuroImage, November.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
Yangqing Jia and Trevor Darrell. 2010. Factorized La-
tent Spaces with Structured Sparsity. In Advances in
Neural Information Processing Systems, volume 23.
Marcel Adam Just, Vladimir L Cherkassky, Sandesh
Aryal, and Tom M Mitchell. 2010. A neuroseman-
tic theory of concrete noun representation based on
the underlying brain codes. PloS one, 5(1):e8622,
January.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008a. Representational similarity analysis
- connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2(November):4,
January.
Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff,
Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky,
Keiji Tanaka, and Peter A Bandettin. 2008b. Match-
ing Categorical Object Representations in Inferior
Temporal Cortex of Man and Monkey. Neuron,
60(6):1126?1141.
TK Landauer and ST Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review, 1(2):211?240.
Julien Mairal, Francis Bach, J Ponce, and Guillermo
Sapiro. 2010. Online learning for matrix factor-
ization and sparse coding. The Journal of Machine
Learning Research, 11:19?60.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547?59,
November.
Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L Malave, Robert A
Mason, and Marcel Adam Just. 2008. Pre-
dicting human brain activity associated with the
meanings of nouns. Science (New York, N.Y.),
320(5880):1191?5, May.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012a. Learning Effective and Interpretable Se-
mantic Models using Non-Negative Sparse Embed-
ding. In Proceedings of Conference on Computa-
tional Linguistics (COLING).
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012b. Selecting Corpus-Semantic Models for Neu-
rolinguistic Decoding. In First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 114?123, Montreal, Quebec, Canada.
Andrew Y. Ng and Michael I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. In Ad-
vances in neural information processing systems,
volume 14.
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau,
and Tom M Mitchell. 2009. Zero-Shot Learning
with Semantic Output Codes. Advances in Neural
Information Processing Systems, 22:1410?1418.
Rajeev D S Raizada and Andrew C Connolly. 2012.
What Makes Different People?s Representations
Alike : Neural Similarity Space Solves the Problem
of Across-subject fMRI Decoding. Journal of Cog-
nitive Neuroscience, 24(4):868?877.
498
Indrayana Rustandi, Marcel Adam Just, and Tom M
Mitchell. 2009. Integrating Multiple-Study
Multiple-Subject fMRI Datasets Using Canonical
Correlation Analysis. In MICCAI 2009 Workshop:
Statistical modeling and detection issues in intra-
and inter-subject functional MRI data analysis.
Magnus Sahlgren. 2006. The Word-Space Model Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words. Doctor
of philosophy, Stockholm University.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of Semantic Representation with Vi-
sual Attributes. In Association for Computational
Linguistics 2013, Sofia, Bulgaria.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking Neural Coding of Per-
ceptual and Semantic Features of Concrete Nouns.
NeuroImage, 62(1):463?451, May.
Peter D Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning : Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Martha White, Yaoliang Yu, Xinhua Zhang, and Dale
Schuurmans. 2012. Convex multi-view subspace
learning. In Advances in Neural Information Pro-
cessing Systems, pages 1?14.
499
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 84?93,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Documents and Dependencies: an Exploration of
Vector Space Models for Semantic Composition
Alona Fyshe, Partha Talukdar, Brian Murphy and Tom Mitchell
Machine Learning Department &
Center for the Neural Basis of Cognition
Carnegie Mellon University, Pittsburgh
{afyshe|ppt|bmurphy|tom.mitchell}@cs.cmu.edu
Abstract
In most previous research on distribu-
tional semantics, Vector Space Models
(VSMs) of words are built either from
topical information (e.g., documents in
which a word is present), or from syntac-
tic/semantic types of words (e.g., depen-
dency parse links of a word in sentences),
but not both. In this paper, we explore the
utility of combining these two representa-
tions to build VSM for the task of seman-
tic composition of adjective-noun phrases.
Through extensive experiments on bench-
mark datasets, we find that even though
a type-based VSM is effective for seman-
tic composition, it is often outperformed
by a VSM built using a combination of
topic- and type-based statistics. We also
introduce a new evaluation task wherein
we predict the composed vector represen-
tation of a phrase from the brain activity of
a human subject reading that phrase. We
exploit a large syntactically parsed corpus
of 16 billion tokens to build our VSMs,
with vectors for both phrases and words,
and make them publicly available.
1 Introduction
Vector space models (VSMs) of word semantics
use large collections of text to represent word
meanings. Each word vector is composed of fea-
tures, where features can be derived from global
corpus co-occurrence patterns (e.g. how often a
word appears in each document), or local corpus
co-occurrence patterns patterns (e.g. how often
two words appear together in the same sentence,
or are linked together in dependency parsed sen-
tences). These two feature types represent dif-
ferent aspects of word meaning (Murphy et al,
2012c), and can be compared with the paradig-
matic/syntagmatic distinction (Sahlgren, 2006).
Global patterns give a more topic-based mean-
ing (e.g. judge might appear in documents also
containing court and verdict). Certain local pat-
terns give a more type-based meaning (e.g. the
noun judge might be modified by the adjective
harsh, or be the subject of decide, as would related
and substitutable words such as referee or con-
ductor). Global patterns have been used in Latent
Semantic Analysis (Landauer and Dumais, 1997)
and LDA Topic models (Blei et al, 2003). Local
patterns based on word co-occurrence in a fixed
width window were used in Hyperspace Analogue
to Language (Lund and Burgess, 1996). Subse-
quent models added increasing linguistic sophisti-
cation, up to full syntactic and dependency parses
(Lin, 1998; Pado? and Lapata, 2007; Baroni and
Lenci, 2010).
In this paper we systematically explore the util-
ity of a global, topic-based VSM built from what
we call Document features, and a local, type-based
VSM built from Dependency features. Our Doc-
ument VSM represents each word w by a vector
where each feature is a specific document, and the
feature value is the number of mentions of word
w in that document. Our Dependency VSM rep-
resents word w with a vector where each feature
is a dependency parse link (e.g., the word w is the
subject of the verb ?eat?), and the feature value is
the number of instances of this dependency fea-
ture for word w across a large text corpus. We
also consider a third Combined VSM in which
the word vector is the concatenation of its Doc-
ument and Dependency features. All three mod-
els subsequently normalize frequencies using pos-
itive pointwise mutual-information (PPMI), and
84
are dimensionality reduced using singular value
decomposition (SVD). This is the first systematic
study of the utility of Document and Dependency
features for semantic composition. We construct
all three VSMs (Dependencies, Documents, Com-
bined) using the same text corpus and preprocess-
ing pipeline, and make the resulting VSMs avail-
able for download (http://www.cs.cmu.
edu/?afyshe/papers/conll2013/). To
our knowledge, this is the first freely available
VSM that includes entries for both words and
adjective-noun phrases, and it is built from a much
larger corpus than previously shared resources (16
billion words, 50 million documents). Our main
contributions include:
? We systematically study complementarity of
topical (Document) and type (Dependency)
features in Vector Space Model (VSM)
for semantic composition of adjective-noun
phrases. To the best of our knowledge, this is
one of the first studies of this kind.
? Through extensive experiments on standard
benchmark datasets, we find that a VSM built
from a combination of topical and type fea-
tures is more effective for semantic compo-
sition, compared to a VSM built from Docu-
ment and Dependency features alone.
? We introduce a novel task: to predict the vec-
tor representation of a composed phrase from
the brain activity of human subjects reading
that phrase.
? We explore two composition methods, addi-
tion and dilation, and find that while addition
performs well on corpus-only tasks, dilation
performs best on the brain activity task.
? We build our VSMs, for both phrases and
words, from a large syntactically parsed text
corpus of 16 billion tokens. We also make
the resulting VSM publicly available.
2 Related Work
Mitchell and Lapata (2010) explored several
methods of combining adjective and noun vec-
tors to estimate phrase vectors, and compared
the similarity judgements of humans to the sim-
ilarity of their predicted phrase vectors. They
found that for adjective-noun phrases, type-based
models outperformed Latent Dirichlet Allocation
(LDA) topic models. For the type-based mod-
els, multiplication performed the best, followed
by weighted addition and a dilation model (for de-
tails on composition functions see Section 4.2).
However, Mitchell and Lapata did not combine
the topic- and type-based models, an idea we ex-
plore in detail in this paper.
Baroni and Zamparelli (2010) extended the typ-
ical vector representation of words. Their model
used matrices to represent adjectives, while nouns
were represented with column vectors. The vec-
tors for nouns and adjective-noun phrases were
derived from local word co-occurrence statistics.
The matrix to represent the adjective was esti-
mated with partial least squares regression where
the product of the learned adjective matrix and
the observed noun vector should equal the ob-
served adjective-noun vector. Socher et al (2012)
also extended word representations beyond sim-
ple vectors. Their model assigns each word a vec-
tor and a matrix, which are composed via an non-
linear function (e.g. tanh) to create phrase rep-
resentations consisting of another vector/matrix
pair. This process can proceed recursively, follow-
ing a parse tree to produce a composite sentence
meaning. Other general semantic composition
frameworks have been suggested, e.g. (Sadrzadeh
and Grefenstette, 2011) who focus on the opera-
tional nature of composition, rather than the rep-
resentations that are supplied to the framework.
Here we focus on creating word representations
that are useful for semantic composition.
Turney (2012) published an exploration of the
impact of domain- and function-specific vector
space models, analogous to the topic and type
meanings encoded by our Document and Depen-
dency models respectively. In Turney?s work,
domain-specific information was represented by
noun token co-occurrence statistics within a lo-
cal window, and functional roles were repre-
sented by generalized token/part-of-speech co-
occurrence patterns with verbs - both of which
are relatively local and shallow when compared
with this work. Similar local context-based fea-
tures were used to cluster phrases in (Lin and Wu,
2009). Though the models discussed here are
not entirely comparable to it, a recent comparison
suggested that broader, deeper features such as
ours may result in representations that are superior
for tasks involving neural activation data (Murphy
et al, 2012b).
85
In contrast to the composite model in (Griffiths
et al, 2005), in this paper we explore the com-
plementarity of semantics captured by topical in-
formation and syntactic/semantic types. We fo-
cus on learning VSMs (involving both words and
phrases) for semantic composition, and use more
expressive dependency-based features in our type-
based VSM. A comparison of vector-space repre-
sentations was recently published (Blacoe and La-
pata, 2012), in which the authors compared sev-
eral methods of combining single words vectors
to create phrase vectors. They found that the best
performance for adjective-noun composition used
point-wise multiplication and a model based on
type-based word co-occurrence patterns.
3 Creating a Vector-Space
To create the Dependency vectors, a 16 billion
word subset of ClueWeb09 (Callan and Hoy,
2009) was dependency parsed using the Malt
parser (Hall et al, 2007). Dependency statistics
were then collected for a predetermined list of
target words and adjective-noun phrases, and for
arbitrary adjective-noun phrases observed in the
corpus. The list was composed of the 40 thou-
sand most frequent single tokens in the Ameri-
can National Corpus (Ide and Suderman, 2006),
and a small number of words and phrases used
as stimuli in our brain imaging experiments. Ad-
ditionally, we included any phrase found in the
corpus whose maximal token span matched the
PoS pattern J+N+, where J and N denote adjec-
tive and noun PoS tags respectively. For each
unit (i.e., word or phrase) in this augmented list,
counts of all unit-external dependencies incident
on the head word were aggregated across the cor-
pus, while unit-internal dependencies were ig-
nored. Each token was appended with its PoS tag,
and the dependency edge label was also included.
This resulted in the extraction of 498 million de-
pendency tuples. For example, the dependency tu-
ple (a/DT, NMOD, 27-inch/JJ television/NN,14),
indicates that a/DT was found as a child of 27-
inch/JJ television/NN with a frequency of 14 in
the corpus.
To create Document vectors, word-document
co-occurrence counts were taken from the same
subset of Clueweb, which covered 50 million doc-
uments. We applied feature-selection for compu-
tational efficiency reasons, ranking documents by
the number of target word/phrase types they con-
tained and choosing the top 10 million.
A series of three additional filtering steps
selected target words/phrases, and Docu-
ment/Dependency features for which there was
adequate data.1 First, a co-occurrence frequency
cut-off was used to reduce the dimensionality
of the matrices, and to discard noisy estimates.
A cutoff of 20 was applied to the dependency
counts, and of 2 to document counts. Positive
pointwise-mutual-information (PPMI) was used
as an association measure to normalize the
observed co-occurrence frequency for the varying
frequency of the target word and its features,
and to discard negative associations. Second, the
target list was filtered to the 57 thousand words
and phrases which had at least 20 non-?stop
word? Dependency co-occurrence types, where
a ?stop word? was one of the 100 most frequent
Dependency features observed (so named be-
cause the dependencies were largely incident on
function words). Third, features observed for
no more than one target were removed, as were
empty target entries. The result was a Document
co-occurrence matrix of 55 thousand targets by
5.2 million features (total 172 million non-zero
entries), and a Dependency matrix of 57 thousand
targets by 1.25 million features (total 35 million
non-zero entries).
A singular value decomposition (SVD) matrix
factorization was computed separately on the De-
pendency and Document statistics matrices, with
1000 latent dimensions retained. For this step
we used Python/Scipy implementation of the Im-
plicitly Restarted Arnoldi method (Lehoucq et al,
1998; Jones et al, 2001). This method is com-
patible with PPMI normalization, since a zero
value represents both negative target-feature asso-
ciations, and those that were not observed or fell
below the frequency cut-off. To combine Docu-
ment and Dependency information, we concate-
nate vectors.
4 Experiments
To evaluate how Document and Dependency di-
mensions can interact and compliment each other,
1In earlier experiments with more than 500 thousand
phrasal entries, we found that the majority of targets were
dominated by non-distinctive stop word co-occurrences, re-
sulting in semantically vacuous representations.
86
Table 1: The nearest neighbors of three queries under three VSMs: all 2000 dimensions (Deps & Docs);
1000 Document dimensions (Docs); 1000 Dependency dimensions (Deps).
Query Deps & Docs Docs Deps
beautiful/JJ wonderful/JJ wonderful/JJ lovely/JJ
lovely/JJ fantastic/JJ gorgeous/JJ
excellent/JJ unspoiled/JJ wonderful/JJ
dog/NN cat/NN dogs/NNS cat/NN
dogs/NNS vet/NN the/DT dog/NN
pet/NN leash/NN dogs/NNS
bad/JJ publicity/NN negative/JJ publicity/NN fast/JJ cash/NN loan/NN negative/JJ publicity/NN
bad/JJ press/NN small/JJ business/NN loan/NN bad/JJ press/NN
unpleasantness/NN important/JJ cities/NNS unpleasantness/NN
Concrete Cats Mixed Cats Concrete Sim Mixed Sim Mixed Related0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1Performance of Documents and Dependency Dimensions for Single Word Tasks
Task
Per
form
anc
e
 
 Docs onlyDeps onlyDocs & Deps
Figure 1: Performance of VSMs for single word
behavioral tasks as we vary Document and Depen-
dency inclusion.
we can perform a qualitative comparison between
the nearest neighbors (NNs) of words and phrases
in the three VSMs ? Dependency, Document, and
Combined (Dependency & Document). Results
appear in Table 1. Note that single words and
phrases can be neighbors of each other, demon-
strating that our VSMs can generalize across syn-
tactic types. In the Document VSM, we get more
topically related words as NNs (e.g., vet and leash
for dog); and in the Dependency VSM, we see
words that might substitute for one another in a
sentence (e.g., gorgeous for beautiful). The two
feature sets can work together to up-weight the
most suitable NNs (as in beautiful), or help to
drown out noise (as in the NNs for bad publicity
in the Document VSM).
4.1 Judgements of Word Similarity
As an initial test of the informativeness of Doc-
ument and Dependency features, we evaluate
the representation of single words. Behavioral
judgement benchmarks have been widely used to
evaluate vector space representations (Lund and
Burgess, 1996; Rapp, 2003; Sahlgren, 2006).
Here we used five such tests. Two tests are catego-
rization tests, where we evaluate how well an au-
tomatic clustering of our word vectors correspond
to pre-defined word categories. The first ?Con-
crete Categories? test-set consists of 82 nouns,
each assigned to one of 10 concrete classes (Battig
and Montague, 1969). The second ?Mixed Cat-
egories? test-set contains 402 nouns in a range
of 21 concrete and abstract classes from Word-
Net (Almuhareb and Poesio, 2004; Miller et al,
1990). Both categorization tests were performed
with the Cluto clustering package (Karypis, 2003)
using cosine distances. Success was measured as
percentage purity over clusters based on their plu-
rality class, with chance performance at 10% and
5% respectively for the ?Concrete Categories? and
?Mixed Categories? tests.
The remaining three tests use group judgements
of similarity: the ?Concrete Similarity? set of
65 concrete word pairs (Rubenstein and Goode-
nough, 1965); and two variations on the Word-
Sim353 test-set (Finkelstein et al, 2002), par-
titioned into subsets corresponding to strict at-
tributional similarity (?Mixed Similarity?, 203
noun pairs), and broader topical ?relatedness?
(?Mixed Relatedness?, 252 noun pairs) (Agirre et
al., 2009). Performance on these benchmarks is
Spearman correlation between the aggregate hu-
man judgements and pairwise cosine distances of
word vectors in a VSM.
The results in Figure 1 show that the Depen-
dency VSM substantially outperforms the Docu-
ment VSM when predicting human judgements of
strict attributional (categorial) similarity (?Simi-
larity? as opposed to ?Relatedness?) for concrete
nouns. Conversely the Document VSM is compet-
87
Figure 2: The performance of three phrase representations for predicting the behavioral phrasal similar-
ity scores from Mitchell and Lapata (2010). The highest correlation is 0.5033 and uses 25 Document
dimensions, 600 Dependency dimensions and the addition composition function.
itive for less concrete word types, and for judge-
ments of broader topical relatedness.
4.2 Judgements of Phrase Similarity
We also evaluated our system on behavioral data
of phrase similarity judgements gathered from 18
human informants. The adjective-noun phrase
pairs are divided into 3 groups: high, medium
and low similarity (Mitchell and Lapata, 2010).
For each pair of phrases, informants rated phrase
similarity on a Likert scale of 1-7. There are 36
phrase pairs in each of the three groups for a to-
tal of 108 phrase pairs. Not all of the phrases oc-
curred frequently enough in our corpus to pass our
thresholds, and so were omitted from our analy-
sis. In several cases we also used pluralizations
of the test phrases (e.g.?dark eyes?) where the
singular form was not found in our VSM. After
these changes we were left with 28, 24 and 28
in the high, medium and low groups respectively.
In total we have 80 observed vectors for the 108
phrase pairs. These adjective-noun phrases were
included in the list of targets, so their statistics
were gathered in the same way as for single words.
This does not impact results for composed vectors,
as all of the single words in the phrases do appear
in our VSMs. A full list of the phrase pairs can be
found in Mitchell and Lapata (2010).
To evaluate, we used three different representa-
tions of phrases. For phrase pairs that passed our
thresholds, we can test the similarity of observed
representations by comparing the VSM represen-
tation of the phrase (no composition function).
For all 108 phrase pairs we can test the composed
phrase representations, derived by applying addi-
tion and dilation operations to word vectors. Mul-
tiplication is not used as SVD representations in-
clude negative values, and so the product of two
negative values would be positive.
Addition is the element-wise sum of two se-
mantic feature vectors saddi = sadji +snouni , where
snouni , sadji , and saddi are the ith element of the
noun, adjective, and predicted phrase vectors, re-
spectively. Dilation of two semantic feature vec-
tors sadj and snoun is calculated by first decom-
posing the noun into a component parallel to the
adjective (x) and a component perpendicular to
the adjective (y) so that snoun = x + y. Dilation
then enhances the adjective component by multi-
plying it by a scalar (?): sdilate = ?x+y. This can
be viewed as taking the representation of the noun,
and up-weighting the elements it shares with the
adjective, which is coherent with the notion of co-
composition (Pustejovsky, 1995). Previous work
(Mitchell and Lapata, 2010) tuned the ? parame-
ter (? = 16.7). We use that value here, though
further optimization might increase performance.
For our evaluation we calculated the cosine dis-
tance between pairs of phrases in the three dif-
ferent representation spaces: observed, addition
and dilation. Results for a range of dimension-
ality settings appear in Figure 2. In the observed
space, we maximized performance when we in-
88
cluded all 1000 of the Document and 350 Depen-
dency dimensions. For consistency the y axis in
Figure 2 extends only to 100 Document dimen-
sions: changes beyond 100 dimensions for ob-
served vectors were minimal. By design, SVD
will tend to use lower dimensions to represent the
strongest signals in the input statistics, which typ-
ically originate in the types of targets that are most
frequent ? in this case single words. We have ob-
served that less frequent and noisier counts, as
might be found for many phrases, are displaced
to the higher dimensions. Consistent with this ob-
servation, maximum performance occurs using a
high number of dimensions (correlation of 0.37 to
human judgements of phrase similarity).
Interestingly, using the single word vectors to
predict the phrase vectors via the addition function
gives the best correlation of any of the represen-
tations, outperforming even the observed phrase
representations. When using 25 Document di-
mensions and 600 Dependency dimensions the
correlation is 0.52, compared to the best per-
formance of 0.51 using Dependency dimensions
only. We speculate that the advantage of com-
posed vectors over observed vectors is due to
sparseness and resulting noise/variance in the ob-
served phrase vectors, as phrases are necessarily
less frequent than their constituent words.
The dilation composition function performs
slightly worse than addition, but shows best per-
formance at the same point as addition. Here, the
highest correlation (0.46) is substantially lower
than that attained by addition, and uses 25 dimen-
sions of the Document, and 600 dimensions of the
Dependency VSM.
To summarize, without documents, {observed,
addition and dilation} phrase vectors have maxi-
mal correlations {0.37, 0.51 and 0.46}. With doc-
uments, {observed, addition and dilation} phrase
vectors have maximal correlations {0.37, 0.52 and
0.50}. Our results using the addition function
(0.52) outperform the results in two previous stud-
ies (Mitchell and Lapata, 2010; Blacoe and Lap-
ata, 2012): (0.46 and 0.48 respectively). This is
evidence that a VSM built from a larger corpus,
and with both Document and Dependency infor-
mation can yield superior results.
4.3 Composed vs Observed Phrase Vectors
Next we tested how well our representations and
semantic composition functions could predict the
observed vector statistics for phrases from the
vectors of their component words. Again, we
explored addition and dilation composition func-
tions. For testing we have 13, 575 vectors for
which both the adjective and noun passed our
thresholds. We predicted a composed phrase vec-
tor using the statistics of the single words and
one of the two composition functions (addition
or dilation). We then sorted the list of observed
phrase vectors by their distance to the composed
phrase vector and recorded the position of the
corresponding observed vector in the list. From
this we calculated percentile rank, the percent of
phrases that are further from the predicted vec-
tor than the observed vector. Percentile rank is:
100 ? (1 ? ?rank/N) where ?rank is the aver-
age position of the correct observed vector in the
sorted list and N = 13, 575 is the size of the list.
Figure 3 shows the changes in percentile rank
in response to varying dimensions of Documents
and Dependencies for the addition function. Di-
lation results are not shown, but the pattern of
performance is very similar. In general, when
one includes more Document dimensions, the per-
centile rank increases. For both the dilation and
addition composition functions the peak perfor-
mance is with 750 Dependency dimensions and
1000 Document dimensions. Dilation?s peak per-
formance is 97.87; addition peaks at 98.03 per-
centile rank. As in Section 4.2, we see that the
accurate representation of phrases requires higher
SVD dimensions.
To evaluate when composition fails, we ex-
amined the cases where the percentile rank was
< 25%. Amongst these words we found an over-
representation of operational adjectives like ?bet-
ter? and ?more?. As observed previously, it is
possible that such adjectives could be better rep-
resented with a matrix or function (Socher et al,
2012; Baroni and Zamparelli, 2010). Composi-
tion may also be failing when the adjective-noun
phrase is non-compositional (e.g. lazy susan); fil-
tering such phrases could improve performance.
4.4 Brain Activity Data
Here we explore the relationship between the neu-
ral activity observed when a person reads a phrase,
89
100 250 500 750 100093
93.5
94
94.5
95
95.5
96
96.5
97
97.5
98
Number of Dependency Dimensions
Per
cen
tile 
Ran
k
Percentile Rank for Varing Doc. and Dep. Dimensions (Addition)
 
 
0 Doc Dims25501005007501000
Figure 3: The percentile rank of observed phrase
vectors compared to vectors created using the ad-
dition composition function.
and our predicted composed VSM for that phrase.
We collected brain activity data using Magnetoen-
cephalography (MEG). MEG is a brain imaging
method with much higher temporal resolution (1
ms) than fMRI (?2 sec). Since words are natu-
rally read at a rate of about 2 per second, MEG is a
better candidate for capturing the fast dynamics of
semantic composition in the brain. Some previous
work has explored adjective-noun composition in
the brain (Chang et al, 2009), but used fMRI and
corpus statistics based only on co-occurrence with
5 hand-selected verbs.
Our MEG data was collected while 9 partici-
pants viewed 38 phrases, each repeated 20 times
(randomly interleaved). The stimulus nouns were
chosen because previous research had shown them
to be decodable from MEG recordings, and the ad-
jectives were selected to modulate their most de-
codable semantic properties (e.g. edibility, ma-
nipulability) (Sudre et al, 2012). The 8 adjec-
tives selected are (?big?, ?small?, ?ferocious?,
?gentle?, ?light?, ?heavy?, ?rotten?, ?tasty?), and
the 6 nouns are (?dog?, ?bear?, ?tomato?, ?car-
rot?, ?hammer?, ?shovel?). The words ?big? and
?small? are paired with every noun, ?ferocious?
and ?gentle? with animals, ?light? and ?heavy?
with tools and ?rotten? and ?tasty? with foods.
We also included the words ?the? and the word
?thing? as semantically neutral fillers, to present
each of the words in a condition without seman-
tic modulation. In total there are 38 phrases (e.g.
?rotten carrot?, ?big hammer?).
In the MEG experiment, the adjective and
paired noun were each shown for 500ms, with a
300ms interval between them, and there were 3
Figure 4: Results for predicting composed phrase
vectors (addition [4a] and dilation [4b]) from
MEG recordings. Results shown are the aver-
age over 9 subjects viewing 38 adjective-noun
phrases. This is the one task on which dilation
outperforms addition.
(a) Addition composition function results.
(b) Dilation composition function results.
seconds in total time between the onset of subse-
quent phrases. Data was preprocessed to maxi-
mize the signal/noise ratio as is common practice
? see Gross et al, (2012). The 20 repeated trials
for each phrase were averaged together to create
one average brain image per phrase.
To determine if the recorded MEG data can be
used to predict our composed vector space rep-
resentations, we devised the following classifica-
tion framework.2 The training data is comprised
of the averaged MEG signal for each of the 38
phrases for one subject, and the labels are the 38
phrases. We use our VSMs and composition func-
tions to form a mapping of the 38 phrases to com-
2Predicting brain activity from VSM representations is
also possible, but provides additional challenges, as parts of
the observed brain activity are not driven by semantics.
90
posed semantic feature vectors w ? {s1 . . . sm}.
The mapping allows us to use Zero Shot Learn-
ing (Palatucci et al, 2009) to predict novel phrases
(not seen during training) from a MEG record-
ing. This is a particularly attractive characteris-
tic for the task of predicting words, as there are
many words and many more phrases in the En-
glish language, and one cannot hope to collect
MEG recordings for all of them.
Formally, let us define the semantic represen-
tation of a phrase w as semantic feature vector
~sw = {s1...sm}, where the semantic space has
dimensionm that varies depending on the number
of Document and/or Dependency dimensions we
include. We utilize the mapping w ? {s1 . . . sm}
to train m independent functions f1(X) ?
s?1, . . . , fm(X) ? s?m where s? represents the
value of a predicted composed semantic feature.
We combine the output of f1 . . . fm to create the
final predicted semantic vector ~s? = {s?1 . . . s?m}.
We use cosine distance to quantify the distance be-
tween true and predicted semantic vectors.
To measure performance we use the 2 vs. 2 test.
For each test we withhold two phrases and train
regressors on the remaining 36. We use the re-
gressors f and MEG data from the two held out
phrases to create two predicted semantic vectors.
We then choose the assignment of predicted se-
mantic vectors (~s?i and ~s?j) to true semantic vec-
tors (~si and ~sj) that minimizes the sum of cosine
distances. If we choose the correct assignment
(~s?i 7? ~si and ~s?j 7? ~sj) we mark the test as cor-
rect. 2 vs. 2 accuracy is the number of 2 vs. 2
tests with correct assignments divided by the total
number of tests. There are (38 choose 2) = 703
distinct 2 vs. 2 tests, and we evaluate on the subset
for which neither the adjective nor noun are shared
(540 pairs). Chance performance is 0.50.
For each f we trained a regressor with L2
penalty. We tune the regularization parame-
ter with leave-one-out-cross-validation on training
data. We train regressors using the first 800 ms of
MEG signal after the noun stimulus appears, when
we assume semantic composition is taking place.
Results appear in Figure 4. The best perfor-
mance (2 vs. 2 accuracy of 0.9440) is achieved
with dilation, 800 dimensions of Dependencies
and zero Document dimensions. When we use
the addition composition function, optimal per-
formance is 0.9212, at 600 Dependency and zero
Document dimensions. Note, however, that the
parameter search here was much coarser that in
Sections 4.2 and 4.3, due to the computation re-
quired. We used a finer grid around the peaks in
performance for addition and dilation and found
minimal improvement (?0.5%) with the addition
of a small number of Document dimensions.
It is intriguing that this neurosemantic task is
the only task for which dilation outperforms addi-
tion. All other composition tasks explored in this
study were concerned with matching composed
word vectors to observed or composed word vec-
tors, whereas here we are interested in matching
composed word vectors to observed brain activity.
Perhaps the brain works in a manner more akin to
the emphasis of elements as modeled by dilation,
rather than a summing of features. Further work
is required to fully understand this phenomenon,
but this is surely a thought-provoking result.3
5 Conclusion
We have performed a systematic study of comple-
mentarity of topical (Document) and type (Depen-
dency) features in Vector Space Model (VSM) for
semantic composition of adjective-noun phrases.
To the best of our knowledge, this is one of the
first such studies of this kind. Through experi-
ments on multiple real world benchmark datasets,
we demonstrated the benefit of combining topic-
and type-based features in a VSM. Additionally,
we introduced a novel task of predicting vec-
tor representations of composed phrases from the
brain activity of human subjects reading those
phrases. We exploited a large syntactically parsed
corpus to build our VSM models, and make them
publicly available. We hope that the findings and
resources from this paper will serve to inform fu-
ture work on VSMs and semantic composition.
Acknowledgment
We are thankful to the anonymous reviewers for their con-
structive comments. We thank CMUs Parallel Data Labo-
ratory (PDL) for making the OpenCloud cluster available,
Justin Betteridge (CMU) for his help with parsing the corpus,
and Yahoo! for providing the M45 cluster. This research has
been supported in part by DARPA (under contract number
FA8750-13-2-0005), NIH (NICHD award 1R01HD075328-
01), Keck Foundation (DT123107), NSF (IIS0835797), and
Google. Any opinions, findings, conclusions and recommen-
dations expressed in this paper are the authors and do not
necessarily reflect those of the sponsors.
3No pun intended.
91
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-
ova, and Marius Pas. 2009. A study on similarity and
relatedness using distributional and WordNet-based ap-
proaches. Proceedings of NAACL-HLT 2009.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP, pages 158?165.
Marco Baroni and Alessandro Lenci. 2010. Distributional
memory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1183?1193. Association
for Computational Linguistics.
W F Battig and W E Montague. 1969. Category Norms
for Verbal Items in 56 Categories: A Replication and Ex-
tension of the Connecticut Category Norms. Journal of
Experimental Psychology Monographs, 80(3):1?46.
William Blacoe and Mirella Lapata. 2012. A Comparison of
Vector-based Representations for Semantic Composition.
In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 546?556, Jeju
Island, Korea.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine Learning
Research, 3(4-5):993?1022.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09 Dataset.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
Kai-min Chang, Vladimir L. Cherkassky, Tom M Mitchell,
and Marcel Adam Just. 2009. Quantitative modeling of
the neural representation of adjective-noun phrases to ac-
count for fMRI activation. In Proceedings of the Annual
Meeting of the ACL and the 4th IJCNLP of the AFNLP,
pages 638?646.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: the concept revisited.
ACM Transactions on Information Systems, 20(1):116?
131.
Thomas L Griffiths, Mark Steyvers, David M Blei, and
Joshua B Tenenbaum. 2005. Integrating topics and syn-
tax. Advances in neural information processing systems,
17.
Joachim Gross, Sylvain Baillet, Gareth R. Barnes,
Richard N. Henson, Arjan Hillebrand, Ole Jensen, Karim
Jerbi, Vladimir Litvak, Burkhard Maess, Robert Oost-
enveld, Lauri Parkkonen, Jason R. Taylor, Virginie van
Wassenhove, Michael Wibral, and Jan-Mathijs Schoffe-
len. 2012. Good-practice for conducting and reporting
MEG research. NeuroImage, October.
J Hall, J Nilsson, J Nivre, G Eryigit, B Megyesi, M Nilsson,
and M Saers. 2007. Single Malt or Blended? A Study
in Multilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLPCoNLL 2007,
volume s. 19-33, pages 933?939. Association for Compu-
tational Linguistics.
Nancy Ide and Keith Suderman. 2006. The American Na-
tional Corpus First Release. Proceedings of the Fifth Lan-
guage Resources and Evaluation Conference (LREC).
Eric Jones, Travis Oliphant, Pearu Peterson, and others.
2001. SciPy: Open source scientific tools for Python.
George Karypis. 2003. CLUTO: A Clustering Toolkit.
Technical Report 02-017, Department of Computer Sci-
ence, University of Minnesota.
T Landauer and S Dumais. 1997. A solution to Plato?s prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological
Review, 104(2):211?240.
R B Lehoucq, D C Sorensen, and C Yang. 1998. Arpack
users? guide: Solution of large scale eigenvalue problems
with implicitly restarted Arnoldi methods. SIAM.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for
discriminative learning. In Proceedings of the ACL.
Dekang Lin. 1998. Automatic Retrieval and Clustering of
Similar Words. In COLING-ACL, pages 768?774.
K Lund and C Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and Computers, 28:203?
208.
George A Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduction
to WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4):235?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive science,
34(8):1388?429, November.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012a.
Comparing Abstract and Concrete Conceptual Represen-
tations using Neurosemantic Decoding. In NAACL Work-
shop on Cognitive Modelling and Computational Linguis-
tics.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012b.
Selecting Corpus-Semantic Models for Neurolinguistic
Decoding. In First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pages 114?123, Montreal,
Quebec, Canada.
Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell.
2012c. Learning Effective and Interpretable Semantic
Models using Non-Negative Sparse Embedding. In Inter-
national Conference on Computational Linguistics (COL-
ING 2012), Mumbai, India.
S Pado? and M Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Linguis-
tics, 33(2):161?199.
92
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau, and
Tom M Mitchell. 2009. Zero-Shot Learning with Se-
mantic Output Codes. Advances in Neural Information
Processing Systems, 22:1410?1418.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
Reinhard Rapp. 2003. Word Sense Discovery Based on
Sense Descriptor Dissimilarity. Proceedings of the Ninth
Machine Translation Summit, pp:315?322.
Herbert Rubenstein and John B. Goodenough. 1965. Con-
textual correlates of synonymy. Communications of the
ACM, 8(10):627?633, October.
Mehrnoosh Sadrzadeh and Edward Grefenstette. 2011. A
Compositional Distributional Semantics Two Concrete
Constructions and some Experimental Evaluations. Lec-
ture Notes in Computer Science, 7052:35?47.
Magnus Sahlgren. 2006. The Word-Space Model: Using dis-
tributional analysis to represent syntagmatic and paradig-
matic relations between words in high-dimensional vector
spaces. Dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositionality
through Recursive Matrix-Vector Spaces. In Conference
on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila We-
hbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell.
2012. Tracking Neural Coding of Perceptual and Seman-
tic Features of Concrete Nouns. NeuroImage, 62(1):463?
451, May.
Peter D Turney. 2012. Domain and Function : A Dual-Space
Model of Semantic Relations and Compositions. Journal
of Artificial Intelligence Research, 44:533?585.
93
