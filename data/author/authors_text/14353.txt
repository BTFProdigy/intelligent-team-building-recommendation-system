Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 21?30,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Enhancing Medical Named Entity Recognition
with Features Derived from Unsupervised Methods
Maria Skeppstedt
Dept. of Computer and Systems Sciences (DSV)
Stockholm University, Forum 100, 164 40 Kista, Sweden
mariask@dsv.su.se
Abstract
A study of the usefulness of features ex-
tracted from unsupervised methods is pro-
posed. The usefulness of these features
will be studied on the task of performing
named entity recognition within one clin-
ical sub-domain as well as on the task of
adapting a named entity recognition model
to a new clinical sub-domain. Four named
entity types, all very relevant for clini-
cal information extraction, will be studied:
Disorder, Finding, Pharmaceutical Drug
and Body Structure. The named entity
recognition will be performed using con-
ditional random fields. As unsupervised
features, a clustering of the semantic rep-
resentation of words obtained from a ran-
dom indexing word space will be used.
1 Introduction
Creating the annotated corpus needed for training
a NER (named entity recognition) model is costly.
This is particularly the case for texts in specialised
domains, for which expert annotators are often re-
quired. In addition, the need for expert annotators
also limits the possibilities of using crowdsourcing
approaches (e.g. Amazon Mechanical Turk). Fea-
tures from unsupervised machine-learning meth-
ods, for which no labelled training data is required,
have, however, been shown to improve the per-
formance of NER systems (Jonnalagadda et al.,
2012). It is therefore likely that by incorporating
features from unsupervised methods, it is possible
to reduce the amount of training data needed to
achieve a fixed level of performance.
Due to differences in the use of language, an
NLP system developed for, or trained on, text
from one sub-domain often shows a drop in per-
formance when applied on texts from another sub-
domain (Martinez et al., 2013). This has the ef-
fect that when performing NER on a new sub-
domain, annotated text from this new targeted sub-
domain might be required, even when there are
annotated corpora from other domains. It would,
however, be preferable to be able to apply a NER
model trained on text from one sub-domain on an-
other sub-domain, with only a minimum of addi-
tional data from this other targeted sub-domain.
Incorporating features from unsupervised meth-
ods might limit the amount of additional annotated
data needed for adapting a NER model to a new
sub-domain.
The proposed study aims at investigating the
usefulness of unsupervised features, both for NER
within one sub-domain and for domain adaptation
of a NER model. The study has two hypotheses.
? Within one subdomain:
For reaching the same level of performance
when training a NER model, less training
data is required when unsupervised features
are used.
? For adapting a model trained on one subdo-
main to a new targeted subdomain:
For reaching the same level of performance
when adapting a NER model to a new subdo-
main, less additional training data is required
in the new targeted subdomain when unsu-
pervised features are used.
For both hypotheses, the level of performance is
defined in terms of F-score.
The proposed study will be carried out on dif-
ferent sub-domains within the specialised text do-
main of clinical text.
2 Related research
There are a number of previous studies on named
entity recognition in clinical text. For instance,
a corpus annotated for the entities Condition,
21
Drug/Device and Locus was used for training
a support vector machine with uneven margins
(Roberts et al., 2008) and a corpus annotated for
the entities Finding, Substance and Body was used
for training a conditional random fields (CRF) sys-
tem (Wang, 2009) as well as for training an en-
semble of different classifiers (Wang and Patrick,
2009). Most studies have, however, been con-
ducted on the i2b2 medication challenge corpus
and the i2b2 challenge on concepts, assertions,
and relations corpus. Conditional random fields
(Patrick and Li, 2010) as well as an ensemble clas-
sifier (Doan et al., 2012) has for instance been used
for extracting the entity Medication names from
the medication challenge corpus, while all but the
best among the top-performing systems used CRF
for extracting the entities Medical Problem, Test
and Treatment from the i2b2 challenge on con-
cepts, assertions, and relations corpus (Uzuner et
al., 2011). The best system (de Bruijn et al., 2011)
used semi-Markov HMM, and in addition to the
features used by most of the other systems (e.g.
tokens/lemmas/stems, orthographics, affixes, part-
of-speech, output of terminology matching), this
system also used features extracted from hierarchi-
cal word clusters on un-annotated text. For con-
structing the clusters, they used Brown clustering,
and represented the feature as a 7-bit showing to
what cluster a word belonged.
Outside of the biomedical domain, there are
many studies on English corpora, which have
shown that using features extracted from clusters
constructed on unlabelled corpora improves per-
formance of NER models, especially when using
a smaller amount of training data (Miller et al.,
2004; Freitag, 2004). This approach has also been
shown to be successful for named entity recogni-
tion in other languages, e.g. German, Dutch and
Spanish (T?ackstr?om et al., 2012), as well as on
related NLP tasks (Biemann et al., 2007), and
there are NER tools that automatically incorpo-
rate features extracted from unsupervised methods
(Stanford, 2012). There are a number of addi-
tional studies within the biomedical domain, e.g.
using features from Brown and other clustering
approaches (Stenetorp et al., 2012) or from k-
means clustered vectors from a neural networks-
based word space implementation (Pyysalo et al.,
2014). Jonnalagadda et al. (2012) also present a
study in which unsupervised features are used for
training a model on the i2b2 challenge on con-
cepts, assertions, and relations corpus. As un-
annotated corpus, they used a corpus created by
extracting Medline abstracts that are indexed with
the publication type ?clinical trials?. They then
built a semantic representation of this corpus in
the form of a random indexing-based word space.
This representation was then used for extracting a
number of similar words to each word in the i2b2
challenge on concepts, assertions, and relations
corpus, which were used as features when training
a CRF system. The parameters of the random in-
dexing model were selected by letting the nearest
neighbours of a word vote for one of the UMLS
categories Medical Problem, Treatment and Test
according to the category of the neighbour, and by
comparing the category winning the vote to the ac-
tual category of the word. The authors motivate
their choice of using random indexing for creat-
ing features with that this method is scalable to
very large corpora without requiring large compu-
tational resources.
The method proposed here is similar to the
method used by Jonnalagadda et al. (2012). How-
ever, the focus of the proposed study is to explore
to what extent unsupervised features can help a
machine learning system trained only on very lit-
tle data. It is therefore not feasible to use the large
number of features that would be generated by
using neighbouring words, as that would require
a large training data set to ensure that there are
enough training examples for each generated fea-
ture. Therefore, the proposed method instead fur-
ther processes the word space model by construct-
ing clusters of semantically related words, thereby
reducing the number of generated features, similar
to the approach by Pyysalo et al. (2014).
3 Materials and previous results
Texts from three different clinical sub-domains:
cardiac ICU (intensive care unit), orthopaedic ER
(emergency room), and internal medicine ER have
been annotated (Tables 1-3).
1
All texts are written
in Swedish, and they all share the characteristics
of text types written under time pressure; all of
them containing many abbreviations and incom-
plete sentences. There are, however, also differ-
ences in e.g. what abbreviations are used and what
1
Research on these texts aiming at extracting informa-
tion related to Disorders/Findings and Pharmaceutical Drugs
has been approved by the Regional Ethical Review Board
in Stockholm (Etikpr?ovningsn?amnden i Stockholm), permis-
sion number 2012/834-31/5.
22
Data set: All
Entity category # entities (Unique)
Disorder 1088 (533)
Finding 1798 (1295)
Pharmaceuticals 1048 (497)
Body structure 461 (252)
Table 1: Annotated data, Cardiac ICU
Data set: All
Entity category # entities (Unique)
Disorder 1258 (541)
Finding 1439 (785)
Pharmaceuticals 880 (212)
Body structure 1324 (423)
Table 2: Annotated data, Orthopaedic ER
entities that are frequently mentioned.
The texts from cardiac ICU and orthopaedic ER
will be treated as existing annotations in a cur-
rent domain, whereas internal medicine ER will
be treated as the new target domain. Approxi-
mately a third of the texts from internal medicine
ER have been doubly annotated, and an evaluation
set has been created by manually resolving differ-
ences between the two annotators (Skeppstedt et
al., 2014). This evaluation subset will be used as
held-out data for evaluating the NER task.
The following four entity categories have been
annotated (Skeppstedt et al., 2014): (1) Disorder
(a disease or abnormal condition that is not mo-
mentary and that has an underlying pathological
process), (2) Finding (a symptom reported by the
patient, an observation made by the physician or
the result of a medical examination of the patient),
(3) Pharmaceutical Drug (not limited to generic
name or trade name, but includes also e.g. drugs
expressed by their effect, such as painkiller or
sleeping pill). (4) Body Structure (an anatomically
defined body part).
These three annotated corpora will be used in
the proposed study, together with a large corpus
of un-annotated text from which unsupervised fea-
tures will be extracted. This large corpus will be a
subset of the Stockholm EPR corpus (Dalianis et
al., 2009), which is a large corpus of clinical text
written in Swedish.
Named entity recognition on the internal
medicine ER part of the annotated corpus has al-
ready been studied, and results on the evaluation
set were an F-score of 0.81 for the entity Dis-
order, 0.69 for Finding, 0.88 for Pharmaceutical
Drug, 0.85 for Body Structure and 0.78 for the
combined category Disorder + Finding (Skeppst-
edt et al., 2014). Features used for training the
model on the development/training part of the in-
ternal medicine ER corpus were the lemma forms
of the words, their part of speech, their semantic
category in used vocabulary lists, their word con-
stituents (if the words were compounds) as well
as the orthographics of the words. A narrow con-
text window was used, as shown by the entries
marked in boldface in Figure 1. As terminologies,
the Swedish versions of SNOMED CT
2
, MeSH
3
,
ICD-10
4
, the Swedish medical list FASS
5
were
used, as well as a vocabulary list of non-medical
words, compiled from the Swedish Parole corpus
(Gellerstam et al., 2000).
4 Methodological background
The proposed method consists of using the train-
ing data first for parameter setting (through n-
fold cross-validation) and thereafter for training a
model using the best parameters. This model is
then to be evaluated on held-out data. A number
of rounds with parameter setting and training will
be carried out, where each new round will make
use of an increasingly larger subset of the training
data. Two versions of parameter setting and model
training will be carried out for each round; one
using features obtained from unsupervised meth-
ods on un-annotated text and one in which such
features are not used. The results of the two ver-
sions are then to be compared, with the hypothesis
that the model incorporating unsupervised meth-
ods will perform better, at least for small training
data sizes.
To accomplish this, the proposed method makes
use of four main components: (1) A system for
training a NER model given features extracted
from an annotated corpus. As this component, a
conditional random fields (CRF) system will be
used. (2) A system for automatic parameter set-
ting. As a large number of models are to be con-
structed on different sizes of the training data, for
which optimal parameters are likely to differ, pa-
rameters for each set of training data has to be
determined automatically for it to be feasible to
2
www.ihtsdo.org
3
mesh.kib.ki.se
4
www.who.int/classifications/icd/en/
5
www.fass.se
23
Data set: Development Final evaluation
Entity category # entities (Unique) # entities
Disorder 1,317 (607) 681
Finding 2,540 (1,353) 1282
Pharmaceuticals 959 (350) 580
Body structure 497 (197) 253
Tokens in corpus 45,482 25,370
Table 3: Annotated entities, internal medicine ER
Token    Lemma    POS    Termi-  Compound   Ortho-   Cluster member-  .. Cluster member-  Category                nology         graphics  ship level 1    ship level n
DVT    dvt    noun   disorder  -   -   all upper  #40     .. #39423    B-Disorderpatient    patient   noun   person  -   -   -    #3      .. #23498    Owith     with    prep.   parole  -   -   -    #14     .. #30892    Ochestpain	 	 chestpain	 	 noun		 	 finding	 	 chest		 pain	 	 -							 	 	 #40     .. #23409    B-Finding  Currentand    and    conj.   parole  -   -   -    -      .. -      Oproblems	 	 problem	 	 	 noun	 	 	 finding	 	 -   -   -    #40     .. #23409    B-Findingto	 	 	 	 	 to	 	 	 	 	 prep.	 	 	 finding	 	 -   -   -    -      .. -      I-Findingbreathe	 	 	 breathe	 	 	 verb	 	 	 finding	 	 -   -   -    #90     .. #23409    I-Finding
Figure 1: A hypothetical example sentence, with hypothetical features for training a machine learning
model. Features used in a previous medical named entity recognition study (Skeppstedt et al., 2014) on
this corpus are shown in boldface. The last column contains the entity category according to the manual
annotation.
carry out the experiments. (3) A system for rep-
resenting semantic similarity of the words in the
un-annotated corpus. As this component, a ran-
dom indexing based word space model will used.
(4) A system for turning the semantic representa-
tion of the word space model into features to use
for the NER model. As this component, clustering
will be used.
To give a methodological background, the theo-
retical foundation for the four components will be
described.
4.1 Conditional random fields
Conditional random fields (CRF or CRFs), intro-
duced by Lafferty et al. (2001), is a machine learn-
ing method suitable for segmenting and labelling
sequential data and therefore often used for e.g.
named entity recognition. As described in the re-
lated research section, CRFs have been used in a
number of studies for extracting entities from clin-
ical text. In contrast to many other types of data,
observed data points for sequential data, such as
text, are dependent on other observed data points.
Such dependences between data points are prac-
tical to describe within the framework of graphi-
cal models (Bishop, 2006, p. 359), to which CRF
belongs (Sutton and McCallum, 2006, p. 1). In
the special, but frequently used, case of linear
chain CRF, the output variables are linked in a
chain. Apart from being dependent on the input
variables, each output variable is then condition-
ally independent on all other output variables, ex-
cept on the previous and following output variable,
given these two neighbouring output variables. In
a named entity recognition task, the output vari-
ables are the named entity classes that are to be
predicted and the observed input variables are ob-
served features of the text, such as the tokens or
their part-of-speech.
CRF is closely related to Hidden Markov Mod-
els, which is also typically described as a graph-
ical model. A difference, however, is that Hid-
den Markov Models belongs to the class of gener-
ative models, whereas CRF is a conditional model
(Sutton and McCallum, 2006, p. 1). Generative
models model the joint distribution between input
variables and the variables that are to be predicted
(Bishop, 2006, p. 43). In contrast, CRF and other
conditional models instead directly model the con-
ditional distribution, enabling the use of a larger
24
feature set (Sutton and McCallum, 2006, p. 1).
For named entity recognition, the IOB-
encoding is typically used for encoding the out-
put variables. Tokens not annotated as an entity
are then encoded with the label O, whereas labels
for annotated tokens are prefixed with a B, if it
is the first token in the annotated chunk, and an
I otherwise (Jurafsky and Martin, 2008, pp. 763?
764). An example of this encoding is shown in the
last column in Figure 1. In this case, where there
are four types of entities, the model thus learns to
classify in 8+1 different classes: B-Disorder, I-
Disorder, B-Finding, I-Finding, B-Drug, I-Drug,
B-BodyStructure, I-BodyStructure and O.
The dependencies are defined by a large number
of (typically binary) feature functions of input and
output variables. E.g. is all of the following true?
? Output: The output at the current position is
I-Disorder
? Output: The output at the previous position is
B-Disorder
? Input: The token at the current position is
chest-pain
? Input: The token at the previous position is
experiences
A feature function in a linear chain CRF can
only include the values of the output variable in
current position and in the immediate previous po-
sition, whereas it can include, and thereby show a
dependence on, input variables from any position.
The CRF model is trained through setting
weights for the feature functions, which is carried
out by penalised maximum likelihood. Penalised
means that regularisation is used, and regularisa-
tion is performed by adding a penalty term, which
prevents the weights from reaching too large val-
ues, and thereby prevents over-fitting (Bishop,
2006, p. 10). The L1-norm and the L2-norm are
frequently used for regularisation (Tsuruoka et al.,
2009), and a variable C governs the importance
of the regularisation. Using the L1-norm also re-
sults in that if C is large enough, some of the
weights are driven to zero, resulting in a sparse
model and thereby the feature functions that those
weights control will not play any role in the model.
Thereby, complex models can be trained also on
data sets with a limited size, without being over-
fitted. However, a suitable value of C must still be
determined (Bishop, 2006, p. 145).
The plan for the proposed study is to use the
CRF package CRF++
6
, which has been used in a
number of previous NER studies, also in the med-
ical domain. The CRF++ package automatically
generates feature functions from user-defined tem-
plates. When using CRF++ as a linear chain CRF,
it generates one binary feature function for each
combination of output class, previous output class
and unique string in the training data that is ex-
panded by a template. This means that L * L *
M feature functions are generated for each tem-
plate, where L = the number of output classes and
M = the number of unique expanded strings. If
only the current token were to be used as a fea-
ture, the number of feature functions would be
9?9?|unique tokens in the corpus|. In practice,
a lot of other features are, however, used. Most of
these features will be of no use to the classifier,
which means that it is important to use an infer-
ence method that sets the weights of the feature
functions with irrelevant features to zero, thus an
inference method that promotes sparsity.
4.2 Parameter setting
As previously explained, a large number of mod-
els are to be constructed, which requires a simple
and efficient method for parameter setting. An ad-
vantage with using the L1-norm is that only one
parameter, the C-value, has to be optimised, as the
weights for feature functions are driven to zero for
feature functions that are not useful. The L1-norm
will therefore be used in the proposed study. A
very large feature set can then be used, without
running the risk of over-fitting the model. Features
will include those that have been used in previous
clinical NER studies (Jonnalagadda et al., 2012;
de Bruijn et al., 2011; Skeppstedt et al., 2014),
with a context window of four previous and four
following tokens.
When maximising the conditional log likeli-
hood of the parameters, the CRF++ program will
set parameters that are optimal for training the
model for the best micro-averaged results for the
four classes Disorder, Finding, Pharmaceutical
drug and Body structure. A hill climbing search
(Marsland, 2009, pp. 262?264) for finding a good
C-value will be used, starting with a value very
close to zero and thereafter changing it in a direc-
tion that improves the NER results. A decreas-
ingly smaller step size will be used for changing
6
crfpp.sourceforge.net
25
Lemmatised and stop word filtered with a window size of 2 (1+1):
complain dermatitis eczema itch patient 
complain: [0 0 0 2 2]
dermatitis: [0 0 0 1 0]
eczema: [0 0 0 1 0]
itch: [2 1 1 0 0]
patient: [2 0 0 0 0]
Figure 2: Term-by-term co-occurrence matrix for
the small corpus ?Patient complains of itching der-
matitis. Patient complains of itching eczema.?
the C-value, until only small changes in the results
can be observed.
4.3 Random indexing
Random indexing is one version of the word space
model, and as all word space models it is a method
for representing distributional semantics. The ran-
dom indexing method was originally devised by
Kanerva et al. (2000), to deal with the performance
problems (in terms of memory and computation
time) that were associated with the LSA/LSI im-
plementations at that time. Due to its computa-
tional efficiency, random indexing remains to be
a popular method when building distributional se-
mantics models on very large corpora, e.g. large
web corpora (Sahlgren and Karlgren, 2009) or
Medline abstracts (Jonnalagadda et al., 2012).
Distributional semantics is built on the distribu-
tional hypothesis, which states that ?Words with
similar meanings tend to occur in similar con-
texts?. If dermatitis and eczema often occur in
similar contexts, e.g. ?Patient complains of itch-
ing dermatitis? and ?Patient complains of itching
eczema?, it is likely that dermatitis and eczema
have a similar meaning. One possible method
of representing word co-occurrence information is
to construct a term-by-term co-occurrence matrix,
i.e. a matrix of dimensionality w ? w, in which w
is the number of terms (unique semantic units, e.g.
words) in the corpus. The elements of the matrix
then contain the number of times each semantic
unit occurs in the context of each other semantic
unit (figure 2).
The context vectors of two semantic units can
then be compared as a measure of semantic sim-
ilarity between units, e.g. using the the euclid-
ian distance between normalised context vectors
or the cosine similarity.
1 2 3 ... d
    ... [0 0 1 ... 0]
 
complain: [0 0 0 ... 1]
itch: [0 1 1 ... 0]
patient: [-1 0 0 ... 0]
... [... ... ... ... ..]
     word w [0 0 -1 ... 0]
Figure 3: Index vectors.
The large dimension of a term-by-term ma-
trix leads, however, to scalability problems, and
the typical solution to this is to apply dimen-
sionality reduction on the matrix. In a semantic
space created by latent semantic analysis, for in-
stance, dimensionality reduction is performed by
applying the linear algebra matrix operation sin-
gular value decomposition (Landauer and Dutnais,
1997). Random indexing is another solution, in
which a matrix with a smaller dimension is created
from start, using the following method (Sahlgren
et al., 2008):
Each term in the data is assigned a unique rep-
resentation, called an index vector. The index vec-
tors all have the dimensionality d (where d ? 1000
but w). Most of the elements of the index vec-
tors are set to 0, but a few, randomly selected, el-
ements are set to either +1 or -1. (Usually around
1-2% of the elements.) Instead of having orthogo-
nal vectors, as is the case for the term-by-term ma-
trix, the index vectors are nearly orthogonal. (See
Figure 3.)
Each term in the data is also assigned a context
vector, also of the dimensionality d. Initially, all
elements in the context vectors are set to 0. The
context vector of each term is then updated by, for
every occurrence of the term in the corpus, adding
the index vectors of the neighboring words. The
neighboring words are called the context window,
and this can be both narrow or wide, depending on
what semantic relations the word space model is
intended to capture. The size of the context win-
dow can have large impact on the results (Sahlgren
et al., 2008), and for detecting paradigmatic re-
lations (i.e. words that occur in similar contexts,
rather than words that occur together) a fairly nar-
row context window has been shown to be most
effective.
The resulting context vectors form a matrix of
dimension w?d. This matrix is an approximation
of the term-by-term matrix, and the same similar-
26
Index vectors (never change)
1 2 3 ... d 
...
itching: [0 1 1 ... 0]
patient: [-1 0 0 ... 0]
...
___________________________________________________________
Context vectors 
1 2 3 ... d
...
complain: [-1 1 1 ... 0]
...
Figure 4: The updated context vectors.
0
Known term from an other entity category
Known term from one entity category
Unknown term
0
?
dermatitis
eczema
Measure similarity between two terms by e.g. cosine  ?
C1
C2
C3
C4
Figure 5: Context vectors for terms in a hypothet-
ical word space with d=2. The context vectors for
the semantically similar words eczema and der-
matitis are close in the word space, in which close-
ness is measured as the cosine of the angle be-
tween the vectors. Four hypothetical clusters (C1-
C4) of context vectors are also shown; clusters that
contain a large proportion of known terms.
ity measures can be applied.
A hypothetical word space with d=2 is shown in
Figure 5.
4.4 Clustering
As mentioned earlier, for the word space informa-
tion to be useful for training a CRF model on a
small data set, it must be represented as a feature
that can only take a limited number of different
values. The proposed methods for achieving this
is to cluster the context vectors of the word space
model, similar to what has been done in previous
research (Pyysalo et al., 2014). Also similar to
previous research, cluster membership for a word
in the NER training and test data will be used as a
feature. Four named hypothetical clusters of con-
text vectors are shown in the word space model
in Figure 5 to illustrate the general idea, and an
example of how to use cluster membership as a
feature is shown Figure 1.
Different clustering techniques will be evalu-
ated, for the quality of the created clusters, as well
as for their computational efficiency. Having hi-
erarchical clusters might be preferable, as clus-
ter membership to clusters of different granular-
ity then can be offered as features for training the
CRF model. Which granularity that is most suit-
able might vary depending on the entity type and
also depending on the size of the training data.
However, e.g. performing hierarchical agglomera-
tive clustering (Jurafsky and Martin, 2008, p. 700)
on the entire unlabelled corpus might be computa-
tionally intractable (thereby defeating the purpose
of using random indexing), as it requires pairwise
comparisons between the words in the corpus. The
pairwise comparison is a part of the agglomera-
tive clustering algorithm, in which each word is
first assigned its own cluster and then each pair of
clusters is compared for similarity, resulting in a
merge of the most similar clusters. This process
is thereafter iteratively repeated, having the dis-
tance between the centroids of the clusters as sim-
ilarity measure. An alternative, which requires a
less efficient clustering algorithm, would be to not
create clusters of all the words in the corpus, but
to limit initially created clusters to include those
words that occur in available terminologies. Clus-
ter membership of unknown words in the corpus
could then be determined by measuring similarity
to the centroids of these initially created clusters.
Regardless of what clustering technique that is
chosen, the parameters of the random indexing
models, as well as of the clustering, will be deter-
mined by evaluating to what extent words that be-
long to one of the studied semantic categories (ac-
cording to available terminologies) are clustered
together. This will be measured using purity and
inverse purity (Amig?o et al., 2009). However, if
clusters are to be created from all words in the cor-
pus, the true semantic category will only be known
for a very small subset of clustered words. In that
case, the two measures have to be defined as purity
being to what extent a cluster only contains known
words of one category and inverse purity being the
extent to which known words of the same category
are grouped into the same cluster.
27
5 Proposed experiments
The first phase of the experiments will consist of
finding the best parameters for the random index-
ing model and the clustering, as described above.
The second phase will consist of evaluating the
usefulness of the clustered data for the NER task.
Three main experiments will be carried out in this
phase (I, II and III), using data set(s) from the fol-
lowing sources:
I: Internal medicine ER
II: Internal medicine ER + Cardiac ICU
III: Internal medicine ER + Orthopaedic ER
In each experiment, the following will be car-
ried out:
1. Divide internal medicine ER training data
into 5 partitions (into a random division, to
better simulate the situation when not all data
is available, using the same random division
for all experiments).
2. Run step 3-5 in 5 rounds. Each new round
uses one additional internal medicine ER par-
tition: (Experiments II and III always use the
entire data set from the other domain). In
each round, two versions of step 3-5 will be
carried out:
(a) With unsupervised features.
(b) Without unsupervised features.
3. Use training data for determining C-value (by
n-fold cross-validation).
4. Use training data for training a model with
this C-value.
5. Evaluate the model on the held-out internal
medicine ICU data.
6 Open issues
What clustering technique to use has previously
been mentioned as one important open issue. The
following are examples of other open issues:
? Could the information obtained from random
indexing be used in some other way than as
transformed to cluster membership features?
Jonnalagadda et al. (2012) used the terms
closest in the semantic space as a feature.
Could this method be adapted in some way
to models constructed with a small amount
of training data? For instance by restricting
what terms are allowed to be used as such a
feature, and thereby limiting the number of
possible values this feature can take.
? Would it be better to use other approaches (or
compare different approaches) for obtaining
features from unlabelled data? A possibil-
ity could be to use a more standard cluster-
ing approach, such as Brown clustering used
in previous clinical NER studies (de Bruijn
et al., 2011). Another possibility could be to
keep the idea of creating clusters from vec-
tors in a word space model, but to use other
methods than random indexing for construct-
ing the word space; e.g. the previously men-
tioned latent semantic analysis (Landauer and
Dutnais, 1997), or a neural networks-based
word space implementation (Pyysalo et al.,
2014).
? Many relevant terms within the medical do-
main are multi-word terms (e.g. of the type
diabetes mellitus), and there are studies on
how to construct semantic spaces with such
multiword terms as the smallest semantic
unit (Henriksson et al., 2013). Should the
whitespace segmented token be treated as the
smallest semantic unit in the proposed study,
or should the use of larger semantic units be
considered?
Acknowledgements
Many thanks to Aron Henriksson, Alyaa Alfalahi,
Maria Kvist, Gunnar Nilsson and Hercules Dalia-
nis for taking a very active part in the planing of
the proposed study, as well as to the three anony-
mous reviewers for their constructive and detailed
comments.
References
Enrique Amig?o, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Inf. Retr., 12(4):461?486, aug.
Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.
2007. Unsupervised part of speech tagging support-
ing supervised methods. In RANLP.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning. Springer, New York, NY.
28
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus -
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR 2009, Evaluation and imple-
mentation of e-health and health information initia-
tives: international perspectives. 14th International
Symposium for Health Information Management Re-
search, Kalmar, Sweden, pages 243?249.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko,
Joel D. Martin, and Xiaodan Zhu. 2011. Machine-
learned solutions for three stages of clinical infor-
mation extraction: the state of the art at i2b2 2010.
J Am Med Inform Assoc, 18(5):557?562.
Son Doan, Nigel Collier, Hua Xu, Hoang Duy Pham,
and Minh Phuong Tu. 2012. Recognition of medi-
cation information from discharge summaries using
ensembles of classifiers. BMC Med Inform Decis
Mak, 12:36.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In EMNLP, pages
262?269.
M Gellerstam, Y Cederholm, and T Rasmark. 2000.
The bank of Swedish. In LREC 2000. The 2nd In-
ternational Conference on Language Resources and
Evaluation, pages 329?333, Athens, Greece.
Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying syn-
onymy between SNOMED clinical terms of vary-
ing length using distributional analysis of electronic
health records. In Proceedings of the Annual Sym-
posium of the American Medical Informatics Asso-
ciation (AMIA 2013), Washington DC, USA.
Siddhartha Jonnalagadda, Trevor Cohen, Stephen Wu,
and Graciela Gonzalez. 2012. Enhancing clinical
concept extraction with distributional semantics. J
Biomed Inform, 45(1):129?40, Feb.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition, February.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In L. R. Gleitman and A. K.
Joshi, editors, Proceedings of the 22nd Annual Con-
ference of the Cognitive Science Society, Mahwah,
NJ.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Thomas K Landauer and Susan T. Dutnais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Stephen Marsland. 2009. Machine learning : an algo-
rithmic perspective. Chapman & Hall/CRC, Boca
Raton, FL.
David Martinez, Lawrence Cavedon, and Graham Pit-
son. 2013. Stability of text mining techniques for
identifying cancer staging. In Proceedings of the 4th
International Louhi Workshop on Health Document
Text Mining and Information Analysis - Louhi 2013,
Sydney, Australia, February.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT, pages
337?342.
Jon Patrick and Min Li. 2010. High accuracy infor-
mation extraction of medication information from
clinical notes: 2009 i2b2 medication extraction chal-
lenge. J Am Med Inform Assoc, 17(5):524?527,
Sep-Oct.
Sampo Pyysalo, Filip Ginter, Hans Moen, Tapio
Salakoski, and Sophia Ananiadou. 2014. Distribu-
tional semantics resources for biomedical text pro-
cessing. In Proceedings of Languages in Biology
and Medicine.
Angus Roberts, Robert Gaizasukas, Mark Hepple, and
Yikun Guo. 2008. Combining terminology re-
sources and statistical methods for entity recogni-
tion: an evaluation. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation (LREC?08), pages 2974?2979, Mar-
rakech, Morocco, may. European Language Re-
sources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Magnus Sahlgren and Jussi Karlgren. 2009. Termi-
nology mining in social media. In Proceedings of
the 18th ACM conference on Information and knowl-
edge management, CIKM ?09.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order
in word space. In Proceedings of the 30th An-
nual Meeting of the Cognitive Science Society, pages
1300?1305.
Maria Skeppstedt, Maria Kvist, Gunnar H Nilsson, and
Hercules Dalianis. 2014. Automatic recognition of
disorders, findings, pharmaceuticals and body struc-
tures from clinical text: An annotation and machine
learning study. J Biomed Inform, Feb (in press).
NLP Group Stanford. 2012. Stanford Named
Entity Recognizer (NER). http://www-
nlp.stanford.edu/software/CRF-NER.shtml. Ac-
cessed 2012-03-29.
29
Pontus Stenetorp, Hubert Soyer, Sampo Pyysalo,
Sophia Ananiadou, and Takashi Chikayama. 2012.
Size (and domain) matters: Evaluating semantic
word space representations for biomedical text. In
Proceedings of the 5th International Symposium on
Semantic Mining in Biomedicine.
Charles. Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477?
487, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Fast full parsing by linear-chain con-
ditional random fields. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ?09,
pages 790?798, Stroudsburg, PA, USA. Association
for Computational Linguistics.
?
Ozlem. Uzuner, Brett R. South, Shuying Shen, and
Scott L. DuVall. 2011. 2010 i2b2/va challenge on
concepts, assertions, and relations in clinical text. J
Am Med Inform Assoc, 18(5):552?556.
Yefeng Wang and Jon Patrick. 2009. Cascading clas-
sifiers for named entity recognition in clinical notes.
In Proceedings of the Workshop on Biomedical In-
formation Extraction, pages 42?49.
Yefeng Wang. 2009. Annotating and recognising
named entities in clinical notes. In Proceedings of
the ACL-IJCNLP Student Research Workshop, pages
18?26, Singapore.
30
Proceedings of the ACL Student Research Workshop, pages 74?80,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Annotating named entities in clinical text
by combining pre-annotation and active learning
Maria Skeppstedt
Dept. of Computer and Systems Sciences (DSV)
Stockholm University, Forum 100, 164 40 Kista, Sweden
mariask@dsv.su.se
Abstract
For expanding a corpus of clinical text, an-
notated for named entities, a method that
combines pre-tagging with a version of ac-
tive learning is proposed. In order to fa-
cilitate annotation and to avoid bias, two
alternative automatic pre-taggings are pre-
sented to the annotator, without reveal-
ing which of them is given a higher con-
fidence by the pre-tagging system. The
task of the annotator is to select the cor-
rect version among these two alternatives.
To minimise the instances in which none
of the presented pre-taggings is correct,
the texts presented to the annotator are ac-
tively selected from a pool of unlabelled
text, with the selection criterion that one
of the presented pre-taggings should have
a high probability of being correct, while
still being useful for improving the result
of an automatic classifier.
1 Introduction
One of the key challenges for many NLP appli-
cations is to create the annotated corpus needed
for development and evaluation of the application.
Such a corpus is typically created through man-
ual annotation, which is a time-consuming task.
Therefore, there is a need to explore methods for
simplifying the annotation task and for reducing
the amount of data that must be annotated.
Annotation can be simplified by automatic pre-
annotation, in which the task of the annotator is
to improve or correct annotations provided by an
existing system. The amount of data needed to be
annotated can be reduced by active learning, i.e.
by actively selecting data to annotate that is useful
to a machine learning system. When using pre-
tagged data, the annotator might, however, be bi-
ased to choose the annotation provided by the pre-
tagger. Also, if the produced pre-taggings are not
good enough, it is still a time-consuming task to
correct them or select the correct tagging among
many suggestions.
Consequently, there is a need to further explore
how an annotated corpus can be expanded with
less effort and using methods that will not bias the
annotators.
2 Background
The background discusses basic ideas of pre-
annotation and active learning, as well as the parti-
cular challenges associated with annotating clini-
cal text.
2.1 Annotating clinical text
A number of text annotation projects have been
carried out in the clinical domain, some of them
including annotations of clinical named entities,
such as mentions of symptoms, diseases and med-
ication. Such studies have for example been
described by Ogren et al (2008), Chapman et
al. (2008), Roberts et al (2009), Wang (2009),
Uzuner et al (2010), Koeling et al (2011) and Al-
bright et al (2013).
As in many specialised domains, expert annota-
tors are typically required to create a reliable an-
notated clinical corpus. These expert annotators
are often more expensive than annotators without
the required specialised knowledge. It is also diffi-
cult to use crowdsourcing approaches, such as us-
ing e.g. Amazon?s Mechanical Turk to hire on-
line annotators with the required knowledge (Xia
and Yetisgen-Yildiz, 2012). A further challenge
is posed by the content of the clinical data, which
is often sensitive and should therefore only be ac-
cessed by a limited number of people. Research
community annotation is consequently another op-
tion that is not always open to annotation projects
in the clinical domain, even if there are examples
of such community annotations also for clinical
text, e.g. described by Uzuner et al (2010).
74
To simplify the annotation process, and to min-
imise the amount of annotated data is therefore
even more important for annotations in the clini-
cal domain than for annotation in general.
2.2 Pre-annotation
A way to simplify annotation is automatic pre-
annotation (or pre-tagging), in which a text is auto-
matically annotated by an existing system, before
it is given to the annotator. Instead of annotating
unlabelled data, the annotator either corrects mis-
takes made by this existing system (Chou et al,
2006), or chooses between different taggings pro-
vided by the system (Brants and Plaehn, 2000).
The system providing the pre-annotations could
be rule- or terminology based, not requiring an-
notated data (Mykowiecka and Marciniak, 2011),
as well as a machine learning/hybrid system that
uses the annotations provided by the annotator to
constantly improve the pre-annotation (Tomanek
et al, 2012). There exist several annotation tools
that facilitate the use of pre-annotation by allow-
ing the user to import pre-annotations or by pro-
viding pre-annotation included in the tools (Neves
and Leser, 2012).
A condition for pre-annotation to be useful is
that the produced annotations are good enough, or
the effect can be the opposite, slowing the annota-
tors down (Ogren et al, 2008). Another potential
problem with pre-annotation is that it might bias
towards the annotations given by the pre-tagging,
for instance if a good pre-tagger reduces the atten-
tion of the annotators (Fort and Sagot, 2010).
2.3 Active learning
Active learning can be used to reduce the amount
of annotated data needed to successfully train a
machine learning model. Instead of randomly se-
lecting annotation data, instances in the data that
are highly informative, and thereby also highly
useful for the machine learning system, are then
actively selected. (Olsson, 2008, p. 27).
There are several methods for selecting the
most informative instances among the unlabelled
ones in the available pool of data. A frequently
used method is uncertainty sampling, in which in-
stances that the machine learner is least certain
how to classify are selected for annotation. For
a model learning to classify into two classes, in-
stances, for which the classifier has no clear pref-
erence for one of the two alternatives, are chosen
for annotation. If there are more than two classes,
the confidence for the most probable class can be
used as the measure of uncertainty. Only using the
certainty level for the most probable classification
means that not all available information is used,
i.e. the information of the certainty levels for the
less probable classes. (Settles, 2009)
An alternative for a multi-class classifier is
therefore to instead use the difference of the cer-
tainty levels for the two most probable classes. If
cp1 is the most probable class and cp2 is the sec-
ond most probable class for the observation xn,
the margin used for measuring uncertainty for that
instance is:
Mn = P (cp1|xn)? P (cp2|xn) (1)
An instance with a large margin is easy to clas-
sify because the classifier is much more certain of
the most probable classification than on the second
most probable. Instances with a small margin, on
the other hand, are difficult to classify, and there-
fore instances with a small margin are selected for
annotation (Schein and Ungar, 2007). A common
alternative is to use entropy as an uncertainty mea-
sure, which takes the certainty levels of all possi-
ble classes into account (Settles, 2009).
There are also a number of other possible meth-
ods for selecting informative instances for anno-
tation, for instance to use a committee of learners
and select the instances for which the committee
disagrees the most, or to search for annotation in-
stances that would result in the largest expected
change to the current model (Settles, 2009).
There are also methods to ensure that the se-
lected data correctly reflects the distribution in the
pool of unlabelled data, avoiding a selection of
outliers that would not lead to a correct model of
the available data. Such methods for structured
prediction have been described by Symons et al
(2006) and Settles and Craven (2008).
Many different machine learning methods have
been used together with active learning for solving
various NLP tasks. Support vector machines have
been used for text classification (Tong and Koller,
2002), using properties of the support vector ma-
chine algorithm for determining what unlabelled
data to select for classification. For structured out-
put tasks, such as named entity recognition, hid-
den markov models have been used by Scheffer
et al (2001) and conditional random fields (CRF)
by Settles and Craven (2008) and Symons et al
(2006).
75
Olsson (2008) suggests combining active learn-
ing and pre-annotation for a named entity recogni-
tion task, that is providing the annotator with pre-
tagged data from an actively learned named entity
recogniser. It is proposed not to indiscriminately
pre-tagg the data, but to only provide those pre-
annotated labels to the human annotator, for which
the pre-tagger is relatively certain.
3 Method
Previous research on pre-annotation shows two
seemingly incompatible desirable properties in a
pre-annotation system. A pre-annotation that is
not good enough might slow the human annota-
tor down, whereas a good pre-annotation might
make the annotator lose concentration, trusting the
pre-annotation too much, resulting in a biased an-
notation. One possibility suggested in previous
research, is to only provide pre-annotations for
which the pre-annotation system is certain of its
classification. For annotations of named entities in
text, this would mean to only provide pre-tagged
entities for which the pre-annotations system is
certain. Such a high precision pre-tagger might,
however, also bias the human annotator towards
not correcting the pre-annotation.
Even more incompatible seems a combination
between pre-annotation and active learning, that
is to provide the human annotator with pre-tagged
data that has been selected for active learning.
The data selected for annotation when using active
learning, is the data for which the pre-annotator is
most uncertain and therefore the data which would
be least suitable for pre-annotation.
The method proposed here aims at finding a
way of combining pre-annotation and active learn-
ing while reducing the risk of annotation bias.
Thereby decreasing the amount of data that needs
to be annotated as well as facilitating the annota-
tion, without introducing bias. A previous version
of this idea has been outlined by Skeppstedt and
Dalianis (2012).
The method is focused on the annotation of
named entities in clinical text, that is marking of
spans of text as well as classification of the spans
into an entity class.
3.1 Pre-annotation
As in standard pre-annotation, the annotator will
be presented with pre-tagged data, and does not
have to annotate the data from scratch.
To reduce the bias problem that might be asso-
ciated with pre-tagging, the mode of presentation
will, however, be slightly different in the method
proposed here. Instead of presenting the best tag-
ging for the human annotator to correct, or to
present the n best taggings, the two best taggings
produced by a pre-tagger will be presented, with-
out informing the annotator which of them that the
pre-tagger considers most likely.
When being presented with two possible anno-
tations of the same text without knowing which of
them that the pre-annotation system considers as
most likely, the annotator always has to make an
active choice of which annotation to choose. This
reduces the bias to one particular pre-annotation,
thereby eliminating a drawback associated with
standard pre-annotation. Having to consider two
alternatives might add cognitive load to the anno-
tator compared to correcting one alternative, but
ought to be easier than annotating a text that is not
pre-tagged.
The reason for presenting two annotations, as
opposed to three or more, is that it is relatively
easy to compare two texts, letting your eyes wan-
der from one text to the other, when you have one
comparison to make. Having three optional an-
notations would result in three comparisons, and
having four would result in six comparisons, and
so on. Therefore, having two optional annotations
to choose from, reduces the bias problem while at
the same time still offering a method for speeding
up the annotation.
A simple Java program for choosing between
two alternative pre-annotated sentences has been
created (Figure 1). The program randomly
chooses in which of the two text boxes to place
which pre-annotation. The user can either choose
the left or the right annotation, or that none of them
is correct.
The data will be split into sentences, and one
sentence at time will be presented to the annotator
for annotation.
3.2 Active learning
To choose from two presented annotations might
also potentially be faster than making corrections
to one presented annotation. For this to be the
case, however, one of the presented annotations
has to be a correct annotation. In order to achieve
that, the proposed method is to use a version of
active learning.
76
Figure 1: A simple program for choosing between two alternative annotations, showing a constructed
example in English.
The standard use of active learning is to actively
select instances to annotate that are useful to a ma-
chine learner. Instances for which the machine
learning model can make a confident classifica-
tion are not presented to the annotator, as these
instances will be of little benefit for improving the
machine learning system.
The version of active learning proposed here is
retaining this general idea of active learning, but
is also adding an additional constraint to what in-
stances that are actively selected for annotation.
This constraint is to only select text passages for
which it is probable that one of the two best
pre-taggings is correct, i.e. the pre-tagger has to
be confident that one of the two presented pre-
annotations is correct, but it should be uncertain
as to which one of them is correct.
For ensuring that the sentences selected for an-
notation are informative enough, the previously
described difference of the certainty level of the
two most probable classes will be used. The same
standard for expressing margin as used in (1), can
be used here, except that in (1), cp1 and cp2 stand
for classification of one instance, whereas in this
case the output is a sequence of labels, labelling
each token in a sentence. Therefore, cp1 and cp2
stand for the classification of a sequence of labels.
Let cp1 be the most probable labelling sequence,
cp2 the second most probable labelling sequence
and cp3 the third most probable labelling sequence.
Moreover, let xn be the observations in sentence
n, then the following margins can be defined for
that sentence:
MtoSecond n = P (cp1|xn)? P (cp2|xn) (2)
MtoThird n = P (cp1|xn)? P (cp3|xn) (3)
To make the probability high that one of the
two presented pre-annotations is correct, the same
method that is used for determining that an an-
notation instance is informative enough could be
used. However, instead of minimising the margin
between two classification instances, it is ensured
that the margin in high enough. That is, the differ-
ence in certainty level between the two most prob-
able annotations and the third most probable must
be high enough to make it probable that one of the
two best classification candidates is correct. This
can be achieved by forcing MtoThird to be above a
threshold, t.
The criteria for selecting the next candidate sen-
tence to annotate can then be described as:
x? = argmin
x
P (cp1|x)? P (cp2|x) (4)
where
P (cp1|x)? P (cp3|x) > t
As instances with the highest possible P (cp2|x)
in relation to P (cp1|x) are favoured, no threshold
for the margin between P (cp2|x) and P (cp3|x) is
needed.
It might be difficult to automatically determine
an appropriate value of the threshold t. Therefore,
the proposed method for finding a good threshold,
is to adapt it to the behaviour of the annotator. If
the annotator often rejects the two presented pre-
taggings, text passages for which the pre-tagger is
more certain ought to be selected, that is the value
of t ought to be increased. On the other hand,
if one of the presented pre-taggings often is se-
lected by the annotator as the correct annotation,
the value of t can be decreased, possibly allowing
for annotation instances with a smaller MtoSecond.
3.3 Machine learning system
As machine learning system, the conditional ran-
dom fields system CRF++ (Kudo, 2013) will be
77
used. This system uses a combination of forward
Viterbi and backward A* search for finding the
best classification sequence for an input sentence,
given the trained model. It can also produce the
n-best classification sequences for each sentence,
which is necessary for the proposed pre-tagger that
presents the two best pre-taggings to the human
annotator.
CRF++ can also give the conditional probably
for the output, that is for the entire classification
sequence of a sentence, which is needed in the pro-
posed active learning algorithm.
3.4 Materials
There is a corpus of Swedish clinical text, i.e.
the text in the narrative part of the health record,
that contains clinical text from the Stockholm area,
from the years 2006-2008 (Dalianis et al, 2009).
A subset of this corpus, containing texts from an
emergency unit of internal medicine, has been an-
notated for four types of named entities: disorder,
finding, pharmaceutical drug and body structure
(Skeppstedt et al, 2012). For approximately one
third of this annotated corpus, double annotation
has been performed, and the instances, for which
there were a disagreement, have been resolved by
one of the annotators.
The annotated corpus will form the main source
of materials for the study proposed here, and addi-
tional data to annotate will be selected from a pool
of unlabelled data from internal medicine emer-
gency notes.
The larger subset of the annotated data, only
annotated by one annotator, will be referred to
as Single (containing 45 482 tokens), and the
smaller subset, annotated by two annotators, will
be referred to as Double (containing 25 370 to-
kens). The Single subset will be the main source
for developing the pre-annotation/active learning
method, whereas the Double subset will be used
for a final evaluation.
3.5 Step-by-step explanation
The proposed method can be divided into 8 steps:
1. Train a CRF model with a randomly selected
subset of the Single part of the annotated cor-
pus, the seed set. The size of this seed set, as
well as suitable features for the CRF model
will be evaluated using cross validation on
the seed set. The size should be as small as
possible, limiting the amount of initial anno-
tation needed, but large enough to have re-
sults in line with a baseline system using ter-
minology matching for named entity recog-
nition (Skeppstedt et al, 2012).
2. Apply the constructed CRF model on unla-
belled data from the pool of data from in-
ternal medicine emergency notes. Let the
model, which operates on a sentence level,
provide the three most probable label se-
quences for each sentence, together with its
level of certainty.
3. Calculate the difference in certainty be-
tween the most probable and the third most
probable suggestion sequence for each sen-
tence, that is MtoThird. Start with a low
threshold t and place all sentences with
MtoThird above the threshold t in a list of
candidates for presenting to the annotator
(that is the sentences fulfilling the criterion
P (cp1|x)? P (cp3|x) > t).
4. Order the sentences in the list of se-
lected candidates in increasing order of
MtoSecond. Present the sentence with the
lowest MtoSecond to the annotator. This is the
sentence, for which the pre-tagger is most un-
certain of which one of the two most probable
pre-taggings is correct.
Present the most probable pre-annotation
as well as the second most probable pre-
annotation, as shown in Figure 1.
5. If the annotator chooses that none of the pre-
sented pre-annotations is correct, discard the
previous candidate selection and make a new
one from the pool with a higher threshold
value t. Again, order the sentences in increas-
ing order of MtoSecond, and present the sen-
tence with the lowest MtoSecond to the anno-
tator.
Repeat step 3., 4. and 5., gradually increasing
the threshold until the annotator accepts one
of the presented pre-annotations.
6. Continue presenting the annotator with the
two most probable pre-annotations for the
sentences in the list of selected candidate
sentences, and allow the human annotator to
choose one of the pre-annotations.
78
The threshold t could be further adjusted ac-
cording to how often the option ?None? is
chosen.
7. Each selected annotation is added to a set
of annotated data. When a sufficiently large
amount of new sentences have been added to
this set, the model needs to be retrained with
the new data. The retraining of the model can
be carried out as a background process while
the human annotator is annotating. In or-
der to use the annotator time efficiently, there
should not be any waiting time while retrain-
ing.
8. When the model has been retrained, the pro-
cess starts over from step 2.
3.6 Evaluation
The text passages chosen in the selection process
will, as explained above, be used to re-train the
machine learning model, and used when select-
ing new text passages for annotation. The effect
of adding additional annotations will also be con-
stantly measured, using cross validation on the
seed set. The additional data added by the active
learning experiments will, however, not be used
in the validation part of the cross validation, but
only be used as additional training data, in order to
make sure that the results are not improved due to
easily classified examples being added to the cor-
pus.
When an actively selected corpus of the same
size as the entire Single subset of the corpus has
been created, this actively selected corpus will be
used for training a machine learning model. The
performance of this model will then be compared
to a model trained on the single subset. Both mod-
els will be evaluated on the Double subset of the
corpus. The hypothesis is that the machine learn-
ing model trained on the corpus partly created by
pre-tagging and active learning will perform bet-
ter than the model created on the original Single
subset.
4 Conclusion
A method that combines pre-annotation and active
learning, while reducing annotation bias, is pro-
posed. A program for presenting pre-annotated
data to the human annotator for selection has been
constructed, and a corpus of annotated data suit-
able as a seed set and as evaluation data has
been constructed. The active learning part of the
proposed method remains, however, to be imple-
mented.
Applying the proposed methods aims at creat-
ing a corpus suitable for training a machine learn-
ing system to recognise the four entities Disorder,
Finding, Pharmaceutical drug and Body struc-
ture. Moreover, methods for facilitating annotated
corpus construction will be explored, potentially
adding new knowledge to the science of annota-
tion.
Acknowledgements
I am very grateful to the reviewers and the pre-
submission mentor for their many valuable com-
ments. I would also like to thank Hercules Dalia-
nis and Magnus Ahltorp as well as the participants
of the ?Southern California Workshop on Medical
Text Analysis and Visualization? for fruitful dis-
cussions on the proposed method.
References
Daniel Albright, Arrick Lanfranchi, Anwen Fredrik-
sen, William F 4th Styler, Colin Warner, Jena D
Hwang, Jinho D Choi, Dmitriy Dligach, Rod-
ney D Nielsen, James Martin, Wayne Ward, Martha
Palmer, and Guergana K Savova. 2013. Towards
comprehensive syntactic and semantic annotations
of the clinical narrative. J Am Med Inform Assoc,
Jan.
Thorsten Brants and Oliver Plaehn. 2000. Interactive
corpus annotation. In LREC. European Language
Resources Association.
Wendy W Chapman, John N Dowling, and George
Hripcsak. 2008. Evaluation of training with an an-
notation schema for manual annotation of clinical
conditions from emergency department reports. Int
J Med Inform, Epub 2007 Feb 20, 77(2):107?113,
February.
Wen-chi Chou, Richard Tzong-han Tsai, and Ying-
shan Su. 2006. A semi-automatic method for anno-
tating a biomedical proposition bank. In FLAC?06.
ACL.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus -
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR 2009, Evaluation and imple-
mentation of e-health and health information initia-
tives: international perspectives. 14th International
Symposium for Health Information Management Re-
search, Kalmar, Sweden, pages 243?249.
Kare?n Fort and Beno??t Sagot. 2010. Influence of
pre-annotation on pos-tagged corpus development.
79
In Proceedings of the Fourth Linguistic Annotation
Workshop, LAW IV ?10, pages 56?63, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rob Koeling, John Carroll, Rosemary Tate, and
Amanda Nicholson. 2011. Annotating a corpus of
clinical text records for learning to recognize symp-
toms automatically. In Proceedings of the LOUHI
2011, Third International Workshop on Health Doc-
ument Text Mining and Information Analysis.
Taku Kudo. 2013. CRF++: Yet Another CRF toolkit.
http://crfpp.sourceforge.net/. Accessed 2013-05-21.
Agnieszka Mykowiecka and Ma?gorzata Marciniak.
2011. Some remarks on automatic semantic an-
notation of a medical corpus. In Proceedings of
the LOUHI 2011, Third International Workshop
on Health Document Text Mining and Information
Analysis.
Mariana Neves and Ulf Leser. 2012. A survey on an-
notation tools for the biomedical literature. Brief-
ings in Bioinformatics.
Philip Ogren, Guergana Savova, and Christopher
Chute. 2008. Constructing evaluation corpora for
automated clinical named entity recognition. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), pages 3143?
3149, Marrakech, Morocco, May. European Lan-
guage Resources Association (ELRA).
Fredrik Olsson. 2008. Bootstrapping Named Entity
Annotation by Means of Active Machine Learning.
Ph.D. thesis, University of Gothenburg. Faculty of
Arts.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J. of Biomedical In-
formatics, 42:950?966, October.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for in-
formation extraction. In Proceedings of the 4th In-
ternational Conference on Advances in Intelligent
Data Analysis, IDA ?01, pages 309?318, London,
UK, UK. Springer-Verlag.
Andrew I. Schein and Lyle H. Ungar. 2007. Ac-
tive learning for logistic regression: an evaluation.
Mach. Learn., 68(3):235?265, October.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 1070?1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin?Madison.
Maria Skeppstedt and Hercules Dalianis. 2012. Using
active learning and pre-tagging for annotating clin-
ical findings in health record text. In Proceedings
of SMBM 2012 - The 5th International Symposium
on Semantic Mining in Biomedicine, pages 98?99,
Zurich, Switzerland, September 3-4.
Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based entity recognition and coverage of
SNOMED CT in Swedish clinical text. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), pages
1250?1257, Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Christopher T. Symons, Nagiza F. Samatova, Ramya
Krishnamurthy, Byung H. Park, Tarik Umar, David
Buttler, Terence Critchlow, and David Hysom.
2006. Multi-criterion active learning in conditional
random fields. In Proceedings of the 18th IEEE In-
ternational Conference on Tools with Artificial In-
telligence, ICTAI ?06, pages 323?331, Washington,
DC, USA. IEEE Computer Society.
Katrin Tomanek, Philipp Daumke, Frank Enders, Jens
Huber, Katharina Theres, and Marcel Mu?ller. 2012.
An interactive de-identification-system. In Proceed-
ings of SMBM 2012 - The 5th International Sympo-
sium on Semantic Mining in Biomedicine, pages 82?
86, Zurich, Switzerland, September 3-4.
Simon Tong and Daphne Koller. 2002. Support
vector machine active learning with applications to
text classification. J. Mach. Learn. Res., 2:45?66,
March.
O?zlem Uzuner, Imre Solti, Fei Xia, and Eithon
Cadag. 2010. Community annotation experiment
for ground truth generation for the i2b2 medication
challenge. J Am Med Inform Assoc, 17(5):519?523.
Yefeng Wang. 2009. Annotating and recognising
named entities in clinical notes. In Proceedings of
the ACL-IJCNLP Student Research Workshop, pages
18?26, Singapore.
Fei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical
corpus annotation: Challenges and strategies. In
The Third Workshop on Building and Evaluating Re-
sources for Biomedical Text Mining (BioTxtM), an
LREC Workshop. Turkey.
80
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 15?21,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Negation Detection in Swedish Clinical Text
Maria Skeppstedt
DSV/Stockholm University
Forum 100
SE-164 40 Kista, Sweden
mariask@dsv.su.se
Abstract
NegEx, a rule-based algorithm that detects
negations in English clinical text, was trans-
lated into Swedish and evaluated on clini-
cal text written in Swedish. The NegEx al-
gorithm detects negations through the use of
trigger phrases, which indicate that a preced-
ing or following concept is negated. A list
of English trigger phrases was translated into
Swedish, taking grammatical differences be-
tween the two languages into account. This
translation was evaluated on a set of 436 man-
ually classified sentences from Swedish health
records. The results showed a precision of
70% and a recall of 81% for sentences con-
taining the trigger phrases and a negative pre-
dictive value of 96% for sentences not con-
taining any trigger phrases. The precision
was significantly lower for the Swedish adap-
tation than published results on the English
version, but since many negated propositions
were identified through a limited set of trigger
phrases, it could nevertheless be concluded
that the same trigger phrase approach is possi-
ble in a Swedish context, even though it needs
to be further developed.
1 Introduction
Medical documentation, such as patient records, is
today often stored in a digital, searchable format.
This opens the possibility of extracting information,
which for example could be used for disease surveil-
lance or to find new, unknown connections between
patient background, symptoms and diseases. When
extracting information from a text, it is not only the
words that occur in the text that are important, but
also whether these words are negated or not. This
is especially true when it comes to patient records,
since when describing the status of a patient, the
physician often reasons by excluding various pos-
sible diagnoses and symptoms.
Most work on detecting negations in medical lan-
guage has been carried out for English, and very lit-
tle has been carried out for other languages, as for
example Swedish. This article will therefore focus
on the task of finding whether a concept in a clinical
text written in Swedish is negated or not.1
2 Related research
There are many different methods for detecting
whether a concept is negated. Rokach et al (2008)
give a good overview of some approaches for de-
tecting negations. The methods can be divided into
two main groups; knowledge engineering methods
and machine learning methods. Knowledge engi-
neering methods have the advantage that a large an-
notated corpus is not needed, but the disadvantage
that rules have to be written manually, which is often
time-consuming. Negation detection based on ma-
chine learning methods, on the other hand, is faster
to implement and often works better when a text is
not completely grammatical, which is often the case
with clinical texts. (Rokach et al, 2008)
Since little previous work has been done on nega-
tion detection in Swedish medical text, the first step
1This research has been carried out after approval
from the Regional Ethical Review Board in Stockholm
(Etikpro?vningsna?mnden i Stockholm), permission number
2009/1742-31/5.
15
for Swedish negation detection is to adapt a sim-
ple knowledge engineering method that is used for
detecting negations in English, an algorithm called
NegEx. (Chapman et al, 2001b)
2.1 The NegEx algorithm
NegEx detects pertinent negatives in English patient
records, that is ?findings and diseases explicitly or
implicitly described as absent in a patient?. Given a
sentence and a chosen proposition in this sentence,
NegEx determines if that proposition is negated or
not. An example would be ?Extremities showed
no cyanoses.?, in which the proposition is cyanoses.
(Chapman et al, 2001b)
The NegEx algorithm uses regular expressions
and three lists of phrases. The first list, the pre-
negation list, consists of trigger phrases which indi-
cate that a proposition that follows them is negated
in the sentence, for example no signs of. The second
list, the post-negation list, consists of trigger phrases
that indicate that a proposition preceding them is
negated, as the phrase unlikely. Finally, the third
list consists of pseudo-negation phrases, phrases that
are similar to negation triggers, but that do not trig-
ger negation, for example not certain if. The algo-
rithm judges the proposition to be negated if it is in
the range of one to six words from a post- or pre-
negation trigger. (Chapman et al, 2001b)
NegEx has later been further developed into
NegEx version 22, for example through the addition
of more triggers and by limiting the scope of the
negation through a list of conjunctions.
In the evaluation of NegEx, the propositions con-
sisted of UMLS3 phrases that belonged to any of
the UMLS categories finding, disease or syndrome
or mental or behavioral dysfunction and that could
also be found in the describing text of an ICD-10
code4. Sentences containing these UMLS phrases
were extracted from discharge summaries. There-
after, 500 of the extracted sentences that contained
at least one negation trigger and 500 sentences that
did not contain a negation trigger were randomly se-
lected. A few sentences that contained phrases that
were suspected to sometimes indicate a negation, but
that were not in the three lists, were included in the
2http://www.dbmi.pitt.edu/chapman/negex.html
3See Bodenreider (2004) for a description of UMLS
4http://www.who.int/classifications/icd/en/
first group. The sentences were then categorised by
physicians into containing an affirmed proposition,
a negated proposition or an ambiguous proposition.
The inter-rater agreement was almost 100%. For the
NegEx evaluation, the categories affirmed and am-
biguous were grouped into the category not negated.
The results showed a precision of 84% and a recall
of 82% for sentences in the group with negation trig-
gers and a negative predictive value of 97% for sen-
tences in the group without triggers. Of the correctly
found negations, 82% were triggered by only three
negation triggers; no, without and no evidence of.
Moreover, only 15 of the 35 negation triggers were
found in the test set. The trigger not had a preci-
sion of 58%, which was much lower than the preci-
sion for the other common triggers. (Chapman et al,
2001b)
An evaluation of the NegEx algorithm on ten
other kinds of reports has also been carried out. The
average precision of NegEx was 97%, and 90% of
the detected negations were triggered by only seven
negation phrases, with the four most frequent being
no, denies, without and no evidence. (Chapman et
al., 2001a)
In a later study by Goldin and Chapman (2003), a
Naive Bayes classifier and a decision tree were used
to classify which occurrences of the trigger not that
indicated a negation, based on features such as sur-
rounding words and their part of speech. Both these
methods resulted in an increased precision.
3 Research Question
An evaluation was carried out on how the NegEx
algorithm performs on health records written in
Swedish, compared to health records written in
English. The hypothesis was that the results for
Swedish would be similar to the results for En-
glish, since the two languages are grammatically
close. This comparison could give an indication of
whether it is possible to adapt more advanced meth-
ods for negation detection into Swedish, and the re-
sults could also be used as a baseline for comparing
the results of other methods.
4 Translation and adaption method
In order to use NegEx on a Swedish text, there must
be a list of Swedish phrases that trigger negation.
16
4.1 Translating trigger phrases
The triggers for Swedish were obtained by translat-
ing the phrases forNegEx version 2. The translations
were made with the help of a web-based English-
Swedish dictionary5 and with the help of Google
translate6. In the cases where there was a good trans-
lation neither in the dictionary nor in the Google
translation, the negation was translated by the au-
thor of this article. When it was not possible to find
a good Swedish translation, the phrase was omitted.
A total of 148 phrases were translated. Almost all
negation phrases were general English terms. How-
ever, in a few cases they consisted of specific med-
ical terms, and in these cases the translation was
made by a physician. In many instances the dictio-
nary offered many translations, and in other cases
the same translation was offered for different En-
glish phrases. In the cases where several translations
were offered, all of them were added to the list of
Swedish negations.
4.2 Expanding the translated trigger phrases
English and Swedish are both Germanic languages
(Crystal, 1997) and they have a similar grammar.
Nevertheless, there are some grammatical differ-
ences that have to be taken into account through an
expansion of the list of translated trigger phrases.
Swedish has two grammatical genders (common
gender and neuter gender), whereas the English lan-
guage lacks grammatical gender. Adjectives and
some quantifiers in Swedish have a gender concord,
as well as a number concord (Dahl, 1982). To com-
pensate for this, the English negative quantifier no
was translated into three different forms of the corre-
sponding Swedish negative quantifier, namely inga,
ingen and inget. Inflections of all adjectives in the
trigger phrases were also generated. This was ac-
complished by invoking the Granska inflector7.
The English combinations of aspect and tense do
not always correspond directly to a Swedish verb
form (Dahl, 1982). Therefore, a direct translation
of the different forms of a verb in the trigger phrase
list was not performed. The lemma form of the verb
was instead added to the list of negation triggers in
5http://www.norstedtsord.se
6http://translate.google.com
7http://www.csc.kth.se/tcs/humanlang/tools.html
Swedish, and from this all inflections of the verb
were generated, again using the Granska inflector.
The difference connected with the do-
construction did not need to be taken into account.
When negating a non-auxiliary verb in English, the
do-construction is used. This type of construction
does not exist in Swedish. The phrase han vet (he
knows) would for example be negated as han vet
inte (he knows not) (Svartvik and Sager, 1996).
However, the NegEx algorithm only checks if
the proposition is less than six words to the right
of the word inte (not), and when it is, it will
consider the proposition to be negated. The lack
of a do-construction should therefore not affect the
results.8
Swedish has a word order inversion in subordi-
nate clauses. The position of the negating adverb
is changed, and it is instead positioned immedi-
ately before the verb (Holmes and Hinchliffe, 2008).
When stressing the negation, there is also the possi-
bility of using this word order in the main clause
(Sells, 2000). A version with reversed word order
was therefore generated for trigger phrases contain-
ing some of the most common adverbs. From the
translation of the trigger phrase has not, a version
with the word order not has was for example gener-
ated.
The frequency of the Swedish trigger phrases was
counted on a text other than the test set, and the
most frequent trigger phrases were selected. The
number of selected phrases was two more than used
in the English NegEx evaluation, to compensate for
Swedish gender and number concord9.
5 Evaluation method
5.1 Construction of test data
Propositions to use for evaluating the performance
of the Swedish version of NegEx were taken from
the Swedish translation of the ICD-10 codes. How-
ever, the description in the ICD-10 code list often
contains both the name of a symptom or disease and
a clarification or specification of it, which has the
8When negating the actual verb on the other hand, the posi-
tion of the word not is different in English and Swedish. In order
for the Swedish NegEx to handle verb phrase propositions, this
difference has to be accounted for.
9The triggers that were used can be downloaded from
http://people.dsv.su.se/?mariask/resources/triggers.txt
17
effect that simple string matching would not find
some of the most common symptoms and diseases.
An automatic pre-processing of the ICD-10 code list
was therefore first accomplished, where for exam-
ple text within parenthesis and clarifications such
as not specified or other specified forms were re-
moved. To find more names of symptoms and dis-
eases, additional lists were also added, including the
KSH97-P10, an adaption of the ICD-10 codes for
primary care, and the MeSH terms under the sec-
tions diseases and mental disorders.
The test data was extracted from a set of sen-
tences randomly chosen from the assessment part
of Swedish health records from the Stockholm EPR
Corpus (Dalianis et al, 2009). From this set, sen-
tences that contained any of the propositions in the
proposition list were extracted, also when the propo-
sition was part of a compound word. Neither the pre-
processing of the ICD-10 code list nor the detection
of a proposition in a compound word was perfect
and therefore some words that were not compara-
ble with findings, diseases or syndromes or mental
or behavioral dysfunctions, were added to the list of
propositions. Sentences containing these were man-
ually filtered out from the test data.
The chosen sentences were ordered in a list of
pairs, consisting of the sentence and the proposition.
If a sentence contained more than one proposition,
the sentence was added to the list one time for each
proposition.
In order to be able to compare the English and
Swedish versions of NegEx, the same evaluation
method was used, and two groups of test sentences
were constucted. The first group, Trig, contained
202 sentences with at least one of the trigger phrases.
The second group, Non-Trig, contained 234 sen-
tences without any of the trigger phrases.
5.2 Classification of test data
The propositions were manually classified into the
categories affirmed, negated and ambiguous by a
rater without medical education. The categories af-
firmed and ambiguous were thereafter collapsed into
the category not negated. The results are presented
in Table 1.
Of the 202 sentences in group Trig, 70 were also
10http://www.socialstyrelsen.se/publikationer1996/1996-4-1
Negated Not negated Total
Trig 90 112 202
Non-Trig 10 224 234
Table 1: Number of sentences manually classified as
negated and not negated for each of the groups Trig and
Non-Trig. Group Trig only contains sentences with trig-
ger phrases and Group Non-Trig only contains sentences
without trigger phrases.
classified by a physician. The inter-rater agreement
between the physician and the other rater with re-
spect to the two groups negated and not negated was
80%.
The majority of the sentences where there was
disagreement were judged as negated by the physi-
cian rater and ambiguous by the other rater, or am-
biguous by the physician rater and negated by the
other rater. There was no evident systematic ten-
dency to judge the propositions as more or less am-
biguous by either of the two raters.
When there was a difference in opinion of how to
classify the proposition, the classification made by
the physician was chosen. Also sentences that were
subjectively judged by the rater as not possible to
rate without deep medical knowledge, were rated by
the physician.
6 Results
The Swedish version of NegEx was executed with
the sentences in group Trig and the sentences in
group Non-Trig as input sentences.11 As shown in
Table 2, group Trig had a precision of 70% and a
recall of 81%. Group Non-Trig had a negative pre-
dictive value of 96%, as shown in Table 3.
When comparing Swedish and English results for
recall using the ?2-test, no significant difference was
found between them. (p-value>> 0.1). When com-
paring the results for precision using the ?2-test, it
was significantly lower for Swedish. (p < 0.001).
The precision of each trigger was also counted
and the results are shown in Table 4.
11http://code.google.com/p/negex/updates/list is the web lo-
cation of NegEx (negex.python.zip, 2009). NegEx could be
used in a Swedish context without any major modifications.
18
Group Trig English Swedish
recall (sensitivity) 82.00 % 81 %
specificity 82.50 % 71 %
precision (ppv) 84.49 % 70 %
npv 80.21 % 82 %
Table 2: Group Trig, 500 English sentences and 202
Swedish sentences. Recall: No. of correctly detected
negated propositions divided by no. of manually rated
negated propositions. Specificity: No. of propositions
correctly detected as not negated divided by no. propo-
sitions that were manually rated as not negated. Preci-
sion: No. of correctly detected negated propositions di-
vided by total no. of propositions that NegEx classified
as negated. Negative predictive value: No. of proposi-
tions that NegEx correctly did not classify as negated di-
vided by total no. of propositions that NegEx did not clas-
sify as negated. (Figures for English from Chapman et al
(2001b).)
Group Non-Trig English Swedish
npv 96.99 % 96 %
Table 3: Group Non-Trig, 500 English sentences and 234
Swedish sentences. (Figures for English from Chapman
et al (2001b).)
7 Discussion
The comparison between the English and Swedish
evaluations is complicated by the fact that the
Swedish test data had lower inter-rater agreement,
which adds uncertainty to the Swedish results. This
difference could perhaps be partly explained by the
different types of health records; the English version
was evaluated on discharge summaries, whereas the
Swedish version was evaluated on the assessment
part of a health record, which possibly contains more
reasoning and thereby perhaps more ambiguous ex-
pressions.
Also, the fact that group Trig in the evaluation
of the English version also included some sentences
not containing trigger phrases complicates the com-
parison.
It could, however, be concluded that the preci-
sion is lower for Swedish. The following error types
could at least account for some of this difference:
It is difficult to draw a line between what is an
ambiguous expression and what is a negation, both
for the raters and for the NegEx program. The
Phrase Precision Occur.
inga tecken (no signs of) 89 % 9
ingen (no) 89 % 27
ej (not) 75 % 8
inga (no, plural) 67 % 15
utan (without) 63 % 8
inte har (not have) 60 % 5
inte (not) 57 % 21
icke (non-, not) 0 % 4
Table 4: The most frequent triggers, their precision and
the number of times they occur in the sentences.
above-mentioned difference in type of evaluation
data could have resulted in lower precision and re-
call for the Swedish version.
It is a common construction for a name of a dis-
ease, or a version of a disease, to have a name that
starts with the word icke (non-, not), for example
icke allergisk astma. The disease is present in the
patient, even though the word icke is interpreted as
a negation trigger by NegEx. In the test data, all the
occurrences of the word icke are constructions like
this, thus having a negative impact on precision.
The Swedish word for without (utan) has a double
meaning. It is also a conjunction meaning but. This
gives rise to a few instances where the program in-
correctly classifies a proposition as negated, result-
ing in lower precision.
Other error types were also identified. These
were, however, not specific for Swedish or for the
type of test data, and could therefore not account for
the difference in precision between the English and
Swedish versions of NegEx. Examples are when the
negation of the proposition occurs in a conditional
clause or when the scope of the trigger should be
less than the NegEx scope of six words, for example
when the scope is limited by a conjunction.
7.1 Identified negation triggers
In the test set, only 16 of the 39 negation triggers
were found, and among them, only 12 correctly
negated a proposition. This is close to the English
version where 15 of 37 triggers were found. None of
the post-negation triggers were found in the Swedish
test data.
In the English version of NegEx, 82% of the cor-
19
rectly found negations were triggered by the three
negation phrases no, without and no evidence of. In
the Swedish version, the three most common trig-
gers were the common gender version of no (ingen),
not (inte) and the plural form of no (inga). Together,
they constitute 63% of the total number of correctly
identified negations. If the trigger in fourth place, no
signs of, is also counted, they make up 75% of the
correctly negated propositions. In both English and
Swedish there are thus a small number of negation
triggers that are very common.
It can also be noted that both in Swedish and En-
glish, the precision of the trigger not (inte) is low.
No other common negation triggers were found in
the test data. The only re-occurring trigger that was
not included in any of the three lists were different
forms of the phrase rule out.
8 Conclusion
The Swedish version of the NegEx algorithm had a
significantly lower precision than the English ver-
sion, and for the recall no significant conclusions
could be drawn. Not taking the uncertainty of the
low inter-rater agreement into account, the Swedish
version has a precision of 70% and a recall of 81%
for sentences containing the trigger phrases and a
negative predictive value of 96% for sentences not
containing any trigger phrases. As for the English
version, a small number of trigger phrases accounted
for the majority of detected negations.
Since a limited set of triggers can be used to iden-
tify many negations also in Swedish, this simple ap-
proach of the NegEx algorithm can be used as a base
method for identifying negations in Swedish. How-
ever, even for use in a system without high demands
on robustness, the method needs to be further devel-
oped.
From the relatively low inter-rater agreement, es-
pecially with respect to concepts that might be clas-
sified as either ambiguous or negated, it can be con-
cluded that it is a difficult task also for a human rater
to determine what is an ambiguity expressed as a
negation or an actual negation.
9 Limitations
The most important limitation of this study is the rel-
atively low inter-rater agreement, and the fact that
most of the sentences were rated by a person who
did not have a medical education. The lack of medi-
cal knowledge may have lead to mistakes when clas-
sifying the test data and could probably also partly
explain the low inter-rater agreement.
Another limitation is that errors in the module for
selecting sentences lead to that a few test sentences
did not contain a symptom, disease or equivalent.
Consequently, these sentences had to be filtered out
manually.
As in the study by Chapman et al (2001a), no
analysis has been made of the occurrences of nega-
tions that stretch over sentence boundaries.
10 Future work
To automatically distinguish an ambiguity from a
negation is not always trivial. However, the er-
rors originating from the other error types mentioned
could be limited through the use of more advanced
natural language processing methods. The cases
where the phrase icke does not trigger a negation,
could probably be detected by a simple regular ex-
pression rule. Which meaning of the phrase utan
that is intended could perhaps be detected by the ma-
chine learning methods used by Goldin and Chap-
man (2003). A list of conjunctions that limit the
scope of the negations, as in NegEx version 2, could
also be used to increase the precision, and a similar
method could be used to detect when the proposition
is negated in a conditional phrase.
It would also be interesting to use the complete
list of negation triggers that was constructed for
this study, instead of limiting the size to that of
the NegEx trigger list, and to evaluate this list on
a larger test set. This evaluation could also deter-
mine whether there are any common Swedish nega-
tion triggers that were not obtained by translating the
English trigger list.
Acknowledgments
I would like to thank my supervisors Hercules Dalia-
nis and Gunnar Nilsson for valuable comments on
this paper, and specifically Gunnar for the help with
the classification of the sentences. I would also
like to thank Birgitta Melin Skeppstedt for initial
help with the statistical calculations and Sumithra
Velupillai for the support on the early stages of the
20
work. Many thanks also to the three anonymous re-
viewers of the paper.
References
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical terminol-
ogy. Nucleic Acids Res, 1;32(Database issue).
Wendy W Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001a. Eval-
uation of negation phrases in narrative clinical reports.
Proc AMIA Symp, pages 105?109.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001b. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. J Biomed Inform,
34(5):301?310, Oct.
David Crystal. 1997. The Cambridge encyclopedia of
language. Cambridge University Press, second edi-
tion.
O?sten Dahl. 1982. Grammatik. Studentlitteratur.
Hercules Dalianis, Martin Hassel, and Sumithra Velupil-
lai. 2009. The Stockholm EPR Corpus - Charac-
teristics and Some Initial Findings. In Proceedings
of ISHIMR 2009, Evaluation and implementation of
e-health and health information initiatives: interna-
tional perspectives. 14th International Symposium for
Health Information Management Research, Kalmar,
Sweden, pages 243?249.
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to detect negation with ?not? in medical texts. ACM
SIGIR ?03 Workshop on Text Analysis and Search for
Bioinformatics: Participant Notebook, Acknowledge-
ments Toronto, Canada: Association for Computing
Machinery;.
Philip Holmes and Ian Hinchliffe. 2008. Swedish: An
Essential grammar. Routledge.
Lior Rokach, Roni Romano, and Oded Maimon. 2008.
Negation recognition in medical narrative reports. In-
formation Retrieval, 11(6):499?538, December.
Peter Sells. 2000. Negation in Swedish: Where it?s
not at. In Online Proceedings of the LFG-00 Con-
ference. Stanford: CSLI Publications. (At http://csli-
publications.stanford.edu/LFG/5/lfg00.html).
Jan Svartvik and Olof Sager. 1996. Engelsk universitets-
grammatik. Liber.
21
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 53?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Characteristics and Analysis of Finnish and Swedish  Clinical Intensive Care Nursing Narratives   Helen Allvinf, Elin Carlssonf, Hercules Dalianisf, Riitta Danielsson-Ojalaa, Vidas Daudaravi?iusb, Martin Hasself, Dimitrios Kokkinakisc, Helj? Lund-gren-Lainea, Gunnar Nilssonf, ?ystein Nytr?d, Sanna Salanter?a, Maria Skeppstedtf, Hanna Suominene, Sumithra Velupillaif aDepartment of Nursing Science, University of Turku, VSSHP, Turku, Finland, bVytautas Magnus University, Lithuania, cDepartment of Swedish, University of Gothenburg, Sweden, dIDI, The Norwegian University of Science and Technology, Norway, eNICTA Canberra Research Laboratory and Australian National University, Australia fDepartment of Computer and Systems Sciences/Stockholm University Forum 100 SE-164 40 Kista, Sweden http://www.dsv.su.se/hexanord 
Abstract 
We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothe-sis is that there are similarities that are impor-tant and interesting from a language technology point of view. This may have im-plications when building tools to support pro-ducing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nurs-ing narratives. Our findings are that ICU nurs-ing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to devel-oping language technology tools. 
1 Introduction The purpose of this study1 is to do content and lexical analysis of nursing narratives written in an                                                            1 Our research on the Stockholm EPR Corpus (Dalianis et al, 2009) has been approved by Etikpr?vningsn?mnden i Stock-holm, (the Regional Vetting Board), reference number 
Intensive Care Unit (ICU). The ultimate goal of our research is to define linguistic similarities and language-specific aspects that differentiate clinical narratives in Finnish and Swedish in order to lay groundwork for developing internationally appli-cable language technology solutions and create a framework for characterising and comparing clini-cal narratives. Free text is handy for information entry but a challenge for information extraction, care handover and other uses of gathered informa-tion. Language technology can alleviate some of these problems in retrospective analysis by offer-ing a more semantically informed interpretation and abstraction. However, the most promising po-tential of language technology is to interactively improve, interpret and code during text entry so that the resulting structured, coded, free text can be validated immediately. The critical bottleneck to-day is namely information handover and reuse, and extensive text is simply not used nor is useful. In-teractively validated, semantically processed text could be more usable and support abstraction, visualization and query tools for the benefit of cli-nicians, patients, researchers and quality adminis-trators.                                                                                               2009/1742-31/5. Our research on the Finnish Corpus (Salan-ter? et al, 2009) has been approved by the Ethical committee of the Hospital District of South West Finland, reference number 2/2009, ?66. 
53
In this paper, we analyze Finnish and Swedish ICU nursing narratives from both qualitative and quantitative perspectives. Our data includes textual nursing documentation of adult patients with a protracted inpatient period. We have chosen ICUs because of their international similarity in decision making (Lauri & Salanter? 2002) and nursing documentation because it covers the entire inpa-tient period. 2 Background 
2.1 Clinical text Clinical text covers the text documents produced for clinical work by clinicians and occurs in clini-cal information systems. It is written by clinicians, that is, professionals (physicians, nurses, therapists and other specialist) responsible for patient care. Its primary purpose is to serve patient care as a summary or hand-over note. However, clinical text is also written for legal requirements, care continu-ity and purposes of reimbursement, management and research. Clinical text covers every care phase and, depending on the purpose, documents differ. Documents that describe the patient?s state, current health problems and socio-medical history are very different from those describing a care plan, its ac-tualization and evaluation of care outcomes. Again, these differ from diagnostic notes, lab results, ra-diography readings, pathology reports and dis-charge documents that plan further care at discharge.  Finally, clinical text may have been entered in ?real time?, in retrospect, or as a sum-mary, by the bedside or elsewhere. The enterer can be a clinician, secretary who transcribes a dictate, speech recognition software or another system that generates or synthesizes text, (McDonald 1997, Thoroddsen et al, 2009.).  2.2 Legal requirements for clinical documen-tation in different countries In several countries clinical documentation is based on law. In Finland, the Ministry of Social Affairs and Health (Statutes of Finland, 298/2009) defines that to ensure good care, all necessary and wide-ranging information has to be registered in patient records. In Sweden, the National Board of Health and Welfare has a similar approach (Pa-tientdatalagen 2008:355). Clinical text should be 
explicit and intelligible, and only generally well-known, accepted concepts and abbreviations are allowed to be used. It should detail adequately the patient?s conditions, care and recovery.  2.3 Special features of ICU and nursing An ICU is an essential component of most large hospitals with high quality care.  ICUs provide care for critically ill patients and focus on condi-tions that are life-threatening and require compre-hensive care and constant monitoring (Webster's 2010). This task is fairly similar universally. It is based on optional, international guidelines focus-ing on triage, admission, discharge and education. This international similarity was evident when nurses? decision making was studied in Canada, Finland, Northern Ireland, Switzerland, Norway and the USA (Lauri & Salanter? 2002); the study showed that decision making of ICU nurses was the most uniform in different countries when com-pared with nurses working in public health care, psychiatric care, and short and long term care. Clinical text written by nurses, that is, nursing narratives, both in Finland and in Sweden is based on the care process which stands for gathering in-formation from the patient, setting goals for care, implementing nursing interventions, and evaluat-ing the results of given care. In Finland, the na-tional standardized documentation model has been implemented with the Finnish care classification (assessment, interventions and outcomes of care) (Tanttu & Ikonen 2007). The Swedish VIPS model provides a structure for the documentation process with key words that reflect the nursing process (Ehrenberg et al, 1996).  ICU nursing narratives can be lengthy, espe-cially when the patient stay in the ICU is pro-longed. As much as 60 A4 pages equivalents of written text may be gathered during one period of care. However, clinicians have somewhat different opinion on how to organize the information they write. For example, headings are often inconsistent and text under headings can cover a lot of other issues than those directly concerning the given heading. (Suominen et al, 2009.)  2.4 Related studies Since most of the available clinical documents are in free-text form, a number of stylistically oriented efforts to characterize the data from various angles 
54
have taken place. This may include various topics, from viewing detailed information about specific items (e.g. readability, Kim et al, 2007) to identi-fying patterns and structures in order to provide better technology to automatically process the sublanguage (Pakhomov et al, 2006). The majority of such efforts investigate different aspects of lin-guistic features at a monolingual level, for in-stance, Hahn & Wermter (2004); Tomanek et al, (2007); Chung (2009); Harkema et al, (2009); while for a thorough review of various related is-sues see Meystre et al, (2008). In the Nordic con-text, Josefsson (1999) discusses Swedish clinical language and shows examples on how verb con-structions in a clinical setting differ from a non clinical setting.  One claim is that the physician unmarks the verb forms for agentivity when writ-ing about the patient and what actions she takes, for example, Patienten hallucinerar [The patient hallucinates] instead of the normal form  Patienten f?r hallucinationer [The patient experiences hallu-cinations].   Helles? (2005) describes nurses' general use of the language function in the nursing discharge notes. She finds that the text in the nursing dis-charge notes is information-dense and character-ized by technical terms, and that the use of standardised templates helped nurses improve the completeness, structure and content of the informa-tion. Comparisons at a monolingual level between written clinical text and lay text has been carried out by Dalianis et al, (2009). A contrastive com-putational linguistics study was carried out be-tween the Stockholm EPR Corpus (SEPR) and a general language corpus, both written Swedish text. The findings showed that SEPR contained longer words and that the vocabulary was highly domain-specific. Other work is described in Ownby, (2005). Comparing clinical text at a crosslingual level has, to our knowledge, only been done by Borin et al, (2007).  3 Analysis of Finnish and Swedish ICU nursing narratives  The analyzed nursing narratives origin from one ICU in a university-affiliated hospital both in Fin-land and Sweden. Our inclusion criterion was an ICU inpatient period of at least 5 days and patient's age of at least 16 years. The Finnish data includes nursing narratives from 514 patient records (496 
unique patients, 18 rebounds, a patient record is defined as each inpatient period of at least 5 days per patient) between January 2005 and August 2006. The Swedish data includes nursing narra-tives from 379 patient records (333 unique pa-tients, 46 rebounds) between January 2006 and May 2008. Since we did not have complete admis-sion and discharge documents from both countries, our analysis is performed on daily nursing narra-tives. These documents are written by ICU nurses during the actual inpatient period from the patient admission to the discharge.    3.1 Qualitative analysis A manual content analysis was performed by four health care professionals (i.e., three native Finnish speakers knowing Swedish, one Swedish native speaker) and one native Swedish speaking lan-guage consultant. Three average-sized patient re-cords each from Finland and Sweden were chosen for our analysis (average size 2,389 words for Fin-land and 5,169 words for Sweden). In the analysis, we considered special features (Table 1) of daily notes both from the structural and content related points of view.  The style and context of both Finnish and Swed-ish text is very similar. For health care profession-als, and especially with an ICU background, all the texts are intelligible and the meaning of a writer becomes evident from the context even in the pres-ence of numerous linguistic and grammatical mis-takes; almost all the sentences are lacking both grammatical subjects and objects. It is evident that in both countries, the narratives are written from a professional to a professional in order to support information transfer, remind about important facts, and supplement numerical data.  A feature common for all the six records is that they rarely contain any subjects or objects when nurses are writing about patients. However, in the Swedish nursing narratives the word patient is used as a subject or object much more often than in the Finnish narratives. The abbreviation pat. is mostly used for this reference and she/he is never used for this purpose. In the whole data, pat. is 40 percent more common than she/he, which is the most common personal pronoun. It seems that the word patient or pat. is used more when the profes-sionals are writing about relatives. In general, pro-nouns are used infrequently in the narratives, and  
55
  Table 1. Special structural and contextual features of Finnish and Swedish daily ICU nursing narratives. The original examples are added in ().  I very rarely. If the reader is not a health care pro-fessional, a risk for confusing the subject (i.e., the patient or nurse) arises. However, the context makes it almost always clear who is referred to.  Approximately half of the narratives do not contain any verb. The most common tense is perfect, but 
without the auxiliary has. When the meaning does not contain a subject it becomes ?unnatural? to use has. Instead, the supine form is used, for example slept, lain, and eaten. Both present and past parti- ciples without be-verb are common, for example, Breathing: Ventilator parameters unchanged. 
Special features of Finnish narratives   Special features of Swedish narratives   Structure Examples Structure Examples 
Headings are used in 2 out of 3 patient records. Headings are typi-cally used as subjects or subjects are partially used.  
Diuresis: occasionally profuse. (Diureesi: ajoittain runsasta.)  Pupils move under eyelids but does not open eyes. (Pupillit liikkuvat luomien alla, mutta ei avaa silmi??n.)  
Headings are used in all daily narra-tives. In Swedish daily narratives, the structure of headings seems to be obligatory. The headings are used typically as subjects.  
Circulation: Stable with ino-trop. (Cirkulation: Stabil med ino-tropi.)  Reacts only for pain stimula-tion during the suction of intu-bation tube. (Reagerar enbart vid sm?rt-stimuli vid sugning i tuben.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Consciousness remained un-changed. (Tajunta pysynyt ennallaan.)  Blood pressure low. (Verenpaine matala.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Breathing: Ventilator parame-ters unchanged. (Andning: Ventilator paramet-rarna of?r?ndrade.) 
Complete sentences are rarely used.  
No spontaneous movements, rigidifies. (Ei spontaania liikett?, j?ykiste-lee.)  Husband and daughter have been staying a long time beside the patient. (Mies ja tyt?r olleet pitk??n potilaan vierell?.) 
Complete sentences are rarely used.  
Light sedation, looks up now and then. (L?tt sederad, tittar upp ibl-and.)  She took the wedding ring and the watch home. (Hon tog med sig vigselring och klocka hem.) 
Misspellings are found but the content or meaning is still clear.  Hemodynamic ? hemodynamic (Hemodynamiikka ? henody-namiikka) Misspellings are found but the content or meaning is still clear.  
The motther is informed. (Mammman ?r informered.)  Magnesium is addded. (Magnesium har tilllsatts.) Content Examples Content  Examples 
The word patient as a subject or object is infrequently mentioned. If this word is mentioned it is not abbreviated.  
Oxidates well or ventilates well. (Happeutuu hyvin tai ventiloituu hyvin.)  
The word patient is used more often than in Finnish narratives as a subject or object. It is also replaced with abbreviations of Pat or Pt.  
Patient got a percutanous tracheostomy today. (Patienten har f?tt en perkutan trakeostomi idag.)  Very worried about patient?s condition. (Mycket oroliga ?ver patien-tens tillst?nd.)  Pt. wakes up for talking and appears to be adequate. (Pt. vakner p? tilltal och up-plevs som adekvat.) 
Signs are typically used: e.g., >, <, -->, +, -.  
The height for the drain raised from 10 --> 20 mmHg. (Dreneerausrajaa nostettu 10  --> 20 mmHg.)  Got medicine --> good response. (Sai l??kett? -->hyv? vaste.) 
Many different abbreviations are used. The origin of entire word is Swedish, English, Latin, professional or ICU typical.  
em. [eftermiddag, afternoon], HR [heart rate], VF [Ven-tricula/Fibrillation, Ventricular Fibrillation]  
56
The use of headings is frequent and good ? most of the time the content matches the headings (Tables 1 and 3). In addition, headings are used similarly in the Swedish and Finnish documents. Most of the time the headings are considered as subjects of the sentence, for example, Consciousness: Unchanged. Liquor brighter than yesterday. However, in the use of headings there are two interesting findings: If the headings are to be cho-sen freely, as in the Finnish narratives, nurses tend to use their own headings and hence many syno-nyms or closely related concepts are used; for ex-ample, hemodynamics versus blood pressure and pulse or breathing versus oxidation. If the headings are obligatory, as in the Swedish narratives, nurses tend to write their observations under the heading which is somehow closest to the subject; for exam-ple, body temperature under circulation or level of sedation under sleep. For both languages the use of different abbrevia-tions is very common. Almost every daily nursing narrative included several abbreviations. Most of the abbreviations are typical for an ICU domain: CVP [central venous pressure], PEEP [Positive End-Expiratory Pressure], EN [Enteral Nutrition], TPN [Total Parenteral Nutrition], pO2 [partial pressure of oxygen], pCO2 [partial pressure of carbon dioxide], MV [Minute Ventilation] and MAP [Mean Arterial Pressure]. From a language technology point of view this means that ICU nurs-ing narratives contain language-independent vo-cabulary. However, nurses in both countries also use many language dependent abbreviations. 
3.2 Quantitative analysis The Finnish data set (n=514) was quantitatively analyzed using the morphological analyser FinT-WOL and the disambiguator FinCG, (Lingsoft 2010), and the Swedish data set (n=379) using the GTA, Granska Text Analyzer (Knutsson et al, 2003). Both data sets are rich in terms of amount of text and vocabulary (Table 2). It is also clear that the amount of text written per day and patient varies a lot in both data sets. More complex words were spelled in numerous ways. For example, the pharmacological substance Noradrenalin had ap-proximately 350 and 60 different spellings in the Finnish and Swedish data sets, respectively. This problem is part of a more general issue of refer-
ence resolution e.g. when mapping different lexical terms referring to the same concept. In our quantitative analysis, we have included punctuation characters. In the Swedish data there was a large amount of html-tags and other format-ting characters, which has a high impact on the total number of tokens (see Table 2). Moreover, as Finnish is highly inflective, FinCG produces alter-native lemmas, hence it is possible to reduce the sparseness of the data by processing the output by choosing only one alternative lemma (see total number of types in Table 2).  To further illustrate the richness of ICU nursing language, the number of unique bigrams (e.g., ?is not?, ?oxidate well? and ?night time? (note: a mis-spelled compound) are the most common ones for Finnish) and trigrams (e.g., ?oxygeneated and ven-tilated?, ?and ventilate well? are among the most common ones for Finnish) were 368,166 (275,205 after FinCG) and 745,407 (356,307 after FinCG) for Finnish patient records. For the Swedish data, the number of unique bigrams was 469,455 (344,127 after GTA) and 1,064,944 (905,539 after GTA). Examples of common Swedish ICU bi-grams and trigrams include ?circulation stabile?, ?during night?, ?in connection with?, and ?with good effect?. Of the content of Finnish nursing narratives, 11% are verbs, 7% nouns and less than 1% pronouns. For Swedish nursing narratives, the respective percentages are 11%, 27% and 2%. One reason for the high numbers for nouns in the Swed-ish data might be due to the large amount of (obligatory) headings relative to the Finnish data (see Table 3).  To support fluent information flow, language technology is needed to strengthen referential con-gruence. Much of this richness of vocabulary is explained by abbreviations and personal differ-ences in professional jargon. In particular, abbre-viations were common. Based on the analysis of the most common words, abbreviations were rela-tively established in Swedish data. For the Finnish data, abbreviations were less standard but RR, SR, CVP, h, ad, ml, ok, vas. [vasen, left] and oik. [oikea, right] were extremely common. Thus, ref-erential congruence can be strengthened by spell-ing out the most common abbreviations automatically. Adding topical content headings is another way to support information flow. Topical content head-ings were mandatory for Swedish data, but no de-
57
fault headings for Finnish existed. However, the headings for Finnish were established in terms of content. In Table 3, we see that the headings for both languages cover similar topics, which indi-cates that the clinical information need is similar for professionals in both countries (and languages). Thus, we recommend forming a standardized set of headings from which the user can voluntarily se-lect the ones to be used. This does not exclude add-ing other headings. Another alternative is to develop language technology for topic segmenta-tion and labeling. We have promising results from this approach (see, e.g., Suominen 2009).  Temporal expressions (e.g., time, evening, night) were often used in both data sets. This poses the question of tense analysis of verbs being unneces-sary and the time-related words being enough to imply the needed temporal information. It is also interesting to note that the negations inte [not, Swe], ingen [none, Swe], ej [not, Swe] and ei [no/not, Fin] are all among the most common types, which is an important property to take into account in information extraction applications. Furthermore, words regarding the oral cavity, such as breathing and mucus, as well as relations, such as daughter, son, wife, and husband are very com-mon in both data sets.  Inspired by the tf?idf-measure from information retrieval, we also analyzed the most common words in terms of a) the number of patients in whose documents the word was used and b) the number of daily nursing narratives in which the word was used. Here, we found, in both data sets, that those words that were used for all patients as well as all daily narratives, were very similar in both data sets, and were related to the most com-mon headings, temporal expressions, negations and monitoring (e.g., increase, continue, begin).  The amount of Protected Health Information (PHI) in form of person names was equal in both of the data sets: 1.5 person names per thousand tokens. This is notable, since this has implications when it comes to integrity issues and reuse of data for research purposes. FinCG did not recognize 36% of the content of Finnish nursing narratives. However, words marked as unrecognized by FinCG also included punctuation marks. In our previous study (see Suominen 2009 and references therein), we tai-lored FinCG by extending approximately 35,000 clinical terms. The extension not only substantially 
improved the applicability of FinCG to the health domain but also initiated piloting of our language technology components in an authentic healthcare environment in the fall 2008. This lead to the re-lease of commercial language technology for Fin-nish health records (Lingsoft 2010).   Data Finnish  Swedish  Total number of patients  514  379  Total number of  tokens,  types (unique tokens) and types after processing 
 1,227,909 63,328 38,649 
 1,959,271 - 41,883 Number of tokens per patient: Minimum Maximum Average Standard deviation 
 540 14,118 2,389 1,635   
 92 36,830 5,169 5,271 Total number of  daily documents and shifts  5,915 17,103  4,700 ? Number of tokens per daily document: Minimum Maximum Average Standard deviation 
  0 915 208 87 
  5 9,389 417 239  Table 2. Comparison of Finnish and Swedish ICU data sets: total amount of text per patient. A daily document, i.e. nursing narrative, contains all text written about a given patient during a calendar day.  Finnish n ? Swedish  n = Hemodynamics  7,800  Respiratory  11,301  Consciousness  6,900  Circulation 10,630  Relatives  5,700  Elimination  10,041  Diuresis  5,400  Nutrition  8,258  Breathing  4,500  Communication  5,880  Oxygenation  3,600  Event Time  5,681  Other  3,200  Pain  4,732  Excretion  590  Psychosocial  4,682  Hemodialysis  370  Sleep  4,438  Pulse  160  Skin  4,402  Skin  160  Activity  3,794   Table 3. Comparison of Finnish and Swedish ICU data sets: the most common headings. For the Finnish data, where default headings were not given, we approxi-mated the amount of heading by using an automated heuristics followed by manual combination of headings with the same meaning. 
58
For Swedish, GTA handles unknown words dif-ferently than FinCG. However, by comparing the ICU words with a Swedish general language cor-pus (PAROLE, Gellerstam et al (2000)), we found that 69% of the types are not included in PAROLE, which indicates a need for tailoring GTA (or simi-lar tools for Swedish) with domain-specific ICU terms.  4 Conclusions The purpose of this study was to do content and lexical analysis of nursing narratives written in an ICU. Our findings are that, even though the Fin-nish and Swedish languages are not linguistically closely related, the way of writing clinical nursing ICU narratives in both countries is very similar. Moreover, the written context made sentences clear for content experts, even though the texts were full of specialized jargon, misspellings, ab-breviations, and missing subjects and objects. However, these characteristics make clinical text challenging for language technology. For example coreference resolution as in the case of noradrena-lin. We have also shown that the content characteris-tics of Finnish and Swedish ICU nursing narratives are very similar. This implies that developing tools for documentation support in ICUs is not country or language dependant in that respect. Developing such tools may improve possibilities for informa-tion extraction and text mining, enabling the possi-bilities to reuse the vast amounts of important practice-based information and evidence captured in clinical narratives. The framework we have in-troduced here could easily be employed in other studies of clinical texts. 6 Future work In the future, we will use the results of this study in developing language technology for Finnish, Swe-dish and other Nordic ICU narratives. We will study how to identify abbreviations, misspellings and normalize and correct them, by using various distance measures and concept management tech-niques. We will also study how to automatically identify important parts of text and highlight them. Furthermore, we are interested in studying text provenance and pragmatics in this particular set-ting. In addition, we will evaluate the influence of 
these technology components in clinical practice. We will also address similarities and differences in clinical text written by various professional groups or at other hospital wards and health care units. Finally, we are eager to seek possibilities to incor-porate laymen's information needs and their inter-action with health care providers into our study. Acknowledgments We would like to thank Nordforsk and the Nordic Council of Ministers for the funding of our research network HEXAnord ? HEalth teXt Analysis network in the Nordic and Baltic countries and NICTA, funded by the Australian Government as represented by the De-partment of Broadband, Communications and the Digi-tal Economy and the Australian Research Council through the ICT Centre of Excellence program. We would also like to thank the Department of Information Technology and TUCS, University of Turku, Finland. References Lars Borin, Natalia Grabar, Catalina Hallett, Davis Hardcastle, Maria Toporowska Gronostaj, Dimitrios Kokkinakis, Sandra Williams and Alistair Willis. 2007. Empowering the patient with language tech-nology. SemanticMining NoE 507505: Deliverable D27.2. <http://gup.ub.gu.se/gup/record/index.xsql?pubid=53590>  Grace Yuet-Chee Chung. 2009. Towards identifying intervention arms in randomized controlled trials: ex-tracting coordinating constructions.  Journal of Bio-medical Informatics. 42(5):790?800  Hercules Dalianis, Martin Hassel and Sumithra Velupil-lai. 2009. The Stockholm EPR Corpus - Characteris-tics and Some Initial Findings. Proceedings of ISHIMR 2009, Evaluation and implementation of e-health and health information initiatives: interna-tional perspectives. 14th International Symposium for Health Information Management Research, Kal-mar, Sweden, 14-16 October, 2009, pp 243-249, pdf. Awarded best paper.  Anna Ehrenberg, Margareta Ehnfors and Ingrid Thorell-Ekstrand. 1996. Nursing documentation in patient re-cords: experience of the use of the VIPS model. Journal of Advanced Nursing 24, 853?867.  Martin Gellerstam, Yvonne Cederholm, and Torgny Rasmark. The bank of Swedish. In: Proceedings of LREC 2000 -- The 2nd International Conference on Language Resources and Evaluation, pages 329?333, Athens, Greece.  Udo Hahn and Joachim Wermter. 2004. High-performance tagging on medical texts. Proceedings of the 20th international conference on Computa-tional Linguistics. Geneva, Switzerland.  
59
Henk Harkema, Dowling JN, Thornblade T, Chapman WW. 2009. ConText: an algorithm for determining negation, experiencer, and temporal status from clin-ical reports. Journal of Biomedical Informatics 2009;42(5):839?51.  Ragnhild Helles?. 2005. Information handling in the nursing discharge notes, Journal of Clinical Nursing, Volume 15 Issue 1, 11 - 21. Blackwell publishing  Gunl?g Josefsson. 1999. F? feber eller tempa? N?gra tankar om agentivitet i medicinskt fackspr?k, Alla tiders spr?k: en v?nskrift till Gertrud Pettersson. Pages 127. Institutionen f?r nordiska spr?k. Lund. (In Swedish)  Hyeoneui Kim, Sergey Goryachev, Craciela Rosemblat, Allen Browne, Alla Keselman and Qing Zeng-Treitler. 2007. Beyond surface characteristics: a new health text-specific readability measurement. AMIA Annual Symp. 11:418-22.  Ola Knutsson, Johnny Bigert, and Vigg Kann. 2003. A robust shallow parser for Swedish. In Proceedings 14th Nordic Conf. on Comp. Ling. NODALIDA. Sirkka Lauri and Sanna Salanter?;. 2002. Developing an instrument to measure and describe clinical decision making in different nursing fields. Journal of Profes-sional Nursing. Mar-Apr;18(2), 93-100.  Lingsoft. 2010, Lingsoft Oy, http://www.lingsoft.fi/  Clement J. McDonald. 1997. The Barriers to Electronic Medical Record Systems and How to Overcome Them. JAMIA. 1997;4:213?221.   St?phane M. Meystre, Guergana K. Savova, Karin C. Kipper-Schuler and John E. Hurdle. 2008. Extracting Information from Textual Documents in the Elec-tronic Health Record: a Review of Recent Research. Yearbook Med Inform. 2008:128-44.  Raymond L. Ownby 2005. Influence of Vocabulary and Sentence Complexity and Passive Voice on the Readability of Consumer-Oriented Mental Health In-formation on the Internet. AMIA Annual Symposium Proceedings. 2005: 585?588. Serguei V. S. Pakhomov, Anni Coden and Christopher G. Chute. 2006. Developing a corpus of clinical notes manually annotated for part-of-speech. International Journal of Medical Informatics. 2006 Jun;75(6):418-29. Epub 2005 Sep 19. Patientdatalagen (2008:355) Svensk f?rfattnings-samling, Socialdepartementet, 2008, Stockholm. (In Swedish)  Hanna Suominen. 2009. Machine Learning and Clinical Text: Supporting Health Information Flow. TUCS Dissertations No 125, Turku Centre for Computer Science, 2009, Turku, Finland.  Hanna Suominen, Helj? Lundgr?n-Laine, Sanna Salan-ter?, Helena Karsten, and Tapio Salakoski. 2009. In-formation flow in intensive care narratives. In Chen J, Chen C, Ely J, Hakkani-Tr D, He J, Hsu H.-H, Liao L, Liu C, Pop  M, Ranganathan S, Reddy C.K, 
Ruan J, Song Y, Tseng V.S, Ungar L, Wu D, Wu Z, Xu K, Yu H, Zelikovsky A, editors. Proceedings IEEE International Conference on Bioinformatics and Biomedicine Workshops, BIBM 2009, pages 325?330. Institute of Electrical and Electronics Engi-neers, Los Alamitos, California, USA.  Kaarina Tanttu and Helena Ikonen. 2007. Nationally standardized electronic nursing documentation in Finland by the year 2007. Stud Health Technol In-form.122:540-1.  Asta Thoroddsen, Kaija Saranto, Anna Ehrenberg, Wal-ter Sermeus. 2009. Models, standards and structures of nursing documentation in European countries. Stud Health Technol Inform.146:327-31.  Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007. A Reappraisal of sentence and token splitting for life sciences documents. Stud Health Technol In-form. 129 (Pt 1):524-8. Webster?s 2010. Webster?s New World Medical Dic-tionary. http://www.medterms.com/script/main/hp.asp,last visited February 2, 2010. 
60
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 5?13,
Uppsala, July 2010.
Creating and Evaluating a Consensus for Negated and Speculative Words
in a Swedish Clinical Corpus
Hercules Dalianis, Maria Skeppstedt
Department of Computer and Systems Sciences (DSV)
Stockholm University
Forum 100
SE-164 40 Kista, Sweden
{hercules, mariask}@dsv.su.se
Abstract
In this paper we describe the creation
of a consensus corpus that was obtained
through combining three individual an-
notations of the same clinical corpus in
Swedish. We used a few basic rules that
were executed automatically to create the
consensus. The corpus contains nega-
tion words, speculative words, uncertain
expressions and certain expressions. We
evaluated the consensus using it for nega-
tion and speculation cue detection. We
used Stanford NER, which is based on the
machine learning algorithm Conditional
Random Fields for the training and detec-
tion. For comparison we also used the
clinical part of the BioScope Corpus and
trained it with Stanford NER. For our clin-
ical consensus corpus in Swedish we ob-
tained a precision of 87.9 percent and a re-
call of 91.7 percent for negation cues, and
for English with the Bioscope Corpus we
obtained a precision of 97.6 percent and a
recall of 96.7 percent for negation cues.
1 Introduction
How we use language to express our thoughts, and
how we interpret the language of others, varies be-
tween different speakers of a language. This is
true for various aspects of a language, and also
for the topic of this article; negations and spec-
ulations. The differences in interpretation are of
course most relevant when a text is used for com-
munication, but it also applies to the task of anno-
tation. When the same text is annotated by more
than one annotator, given that the annotating task
is non-trivial, the resulting annotated texts will not
be identical. This will be the result of differences
in how the text is interpreted, but also of differ-
ences in how the instructions for annotation are
interpreted. In order to use the annotated texts,
it must first be decided if the interpretations by the
different annotators are similar enough for the pur-
pose of the text, and if so, it must be decided how
to handle the non-identical annotations.
In the study described in this article, we have
used a Swedish clinical corpus that was anno-
tated for certainty and uncertainty, as well as for
negation and speculation cues by three Swedish-
speaking annotators. The article describes an eval-
uation of a consensus annotation obtained through
a few basic rules for combining the three different
annotations into one annotated text.
1
2 Related research
2.1 Previous studies on detection of negation
and speculation in clinical text
Clinical text often contains reasoning, and thereby
many uncertain or negated expressions. When,
for example, searching for patients with a specific
symptom in a clinical text, it is thus important to
be able to detect if a statement about this symptom
is negated, certain or uncertain.
The first approach to identifying negations in
Swedish clinical text was carried out by Skeppst-
edt (2010), by whom the well-known NegEx algo-
rithm (Chapman et al, 2001), created for English
clinical text, was adapted to Swedish clinical text.
Skeppstedt obtained a precision of 70 percent and
a recall of 81 percent in identifying negated dis-
eases and symptoms in Swedish clinical text. The
NegEx algorithm is purely rule-based, using lists
of cue words indicating that a preceding or follow-
ing disease or symptom is negated. The English
version of NegEx (Chapman et al, 2001) obtained
a precision of 84.5 percent and a recall of 82.0 per-
cent.
1
This research has been carried out after approval from
the Regional Ethical Review Board in Stockholm (Etikprvn-
ingsnmnden i Stockholm), permission number 2009/1742-
31/5.
5
Another example of negation detection in En-
glish is the approach used by Huang and Lowe
(2007). They used both parse trees and regu-
lar expressions for detecting negated expressions
in radiology reports. Their approach could de-
tect negated expressions both close to, and also
at some distance from, the actual negation cue (or
what they call negation signal). They obtained a
precision of 98.6 percent and a recall of 92.6 per-
cent.
Elkin et al (2005) used the terms in SNOMED-
CT (Systematized Nomenclature of Medicine-
Clinical Terms), (SNOMED-CT, 2010) and
matched them to 14 792 concepts in 41 health
records. Of these concepts, 1 823 were identified
as negated by humans. The authors used Mayo
Vocabulary Server Parsing Engine and lists of cue
words triggering negation as well as words in-
dicating the scope of these negation cues. This
approach gave a precision of 91.2 percent and
a recall of 97.2 percent in detecting negated
SNOMED-CT concepts.
In Rokach et al (2008), they used clinical nar-
rative reports containing 1 766 instances annotated
for negation. The authors tried several machine
learning algorithms for detecting negated findings
and diseases, including hidden markov models,
conditional random fields and decision trees. The
best results were obtained with cascaded decision
trees, with nodes consisting of regular expressions
for negation patterns. The regular expressions
were automatically learnt, using the LCS (longest
common subsequence) algorithm on the training
data. The cascaded decision trees, built with LCS,
gave a precision of 94.4 percent, a recall of 97.4
percent and an F-score of 95.9 percent.
Szarvas (2008) describes a trial to automatically
identify speculative sentences in radiology reports,
using Maximum Entropy Models. Advanced fea-
ture selection mechanisms were used to automat-
ically extract cue words for speculation from an
initial seed set of cues. This, combined with man-
ual selection of the best extracted candidates for
cue words, as well as with outer dictionaries of
cue words, yielded an F-score of 82.1 percent for
detecting speculations in radiology reports. An
evaluation was also made on scientific texts, and
it could be concluded that cue words for detecting
speculation were domain-specific.
Morante and Daelemans (2009) describe a ma-
chine learning system detecting the scope of nega-
tions, which is based on meta-learning and is
trained and tested on the annotated BioScope Cor-
pus. In the clinical part of the corpus, the au-
thors obtained a precision of 100 percent, a re-
call of 97.5 percent and finally an F-score of 98.8
percent on detection of cue words for negation.
The authors used TiMBL (Tilburg Memory Based
Learner), which based its decision on features
such as the words annotated as negation cues and
the two words surrounding them, as well as the
part of speech and word forms of these words.
For detection of the negation scope, the task was
to decide whether a word in a sentence contain-
ing a negation cue was either the word starting
or ending a negation scope, or neither of these
two. Three different classifiers were used: sup-
port vector machines, conditional random fields
and TiMBL. Features that were used included the
word and the two words preceding and following
it, the part of speech of these words and the dis-
tance to the negation cue. A fourth classifier, also
based on conditional random fields, used the out-
put of the other three classifiers, among other fea-
tures, for the final decision. The result was a pre-
cision of 86.3 percent and a recall of 82.1 percent
for clinical text. It could also be concluded that the
system was portable to other domains, but with a
lower result.
2.2 The BioScope Corpus
Annotated clinical corpora in English for nega-
tion and speculation are described in Vincze et al
(2008), where clinical radiology reports (a sub-
set of the so called BioScope Corpus) encompass-
ing 6 383 sentences were annotated for negation,
speculation and scope. Henceforth, when refer-
ring to the BioScope Corpus, we only refer to the
clinical subset of the BioScope Corpus. The au-
thors found 877 negation cues and 1 189 specu-
lation cues, (or what we call speculative cues) in
the corpora in 1 561 sentences. This means that
fully 24 percent of the sentences contained some
annotation for negation or uncertainty. However,
of the original 6 383 sentences, 14 percent con-
tained negations and 13 percent contained spec-
ulations. Hence some sentences contained both
negations and speculations. The corpus was anno-
tated by two students and their work was led by a
chief annotator. The students were not allowed to
discuss their annotations with each other, except at
regular meetings, but they were allowed to discuss
6
with the chief annotator. In the cases where the
two student annotators agreed on the annotation,
that annotation was chosen for the final corpus. In
the cases where they did not agree, an annotation
made by the chief annotator was chosen.
2.3 The Stanford NER based on CRF
The Stanford Named Entity Recognizer (NER) is
based on the machine learning algorithm Condi-
tional Random Fields (Finkel et al, 2005) and has
been used extensively for identifying named enti-
ties in news text. For example in the CoNLL-2003,
where the topic was language-independent named
entity recognition, Stanford NER CRF was used
both on English and German news text for train-
ing and evaluation. Where the best results for En-
glish with Stanford NER CRF gave a precision of
86.1 percent, a recall of 86.5 percent and F-score
of 86.3 percent, for German the best results had
a precision of 80.4 percent, a recall of 65.0 per-
cent and an F-score of 71.9 percent, (Klein et al,
2003). We have used the Stanford NER CRF for
training and evaluation of our consensus.
2.4 The annotated Swedish clinical corpus
for negation and speculation
A process to create an annotated clinical corpus
for negation and speculation is described in Dalia-
nis and Velupillai (2010). A total of 6 740 ran-
domly extracted sentences from a very large clin-
ical corpus in Swedish were annotated by three
non-clinical annotators. The sentences were ex-
tracted from the text field Assessment (Bed?omning
in Swedish). Each sentence and its context from
the text field Assessment were presented to the an-
notators who could use five different annotation
classes to annotate the corpora. The annotators
had discussions every two days on the previous
days? work led by the experiment leader.
As described in Velupillai (2010), the anno-
tation guidelines were inspired by the BioScope
Corpus guidelines. There were, however, some
differences, such as the scope of a negation or of
an uncertainty not being annotated. It was instead
annotated if a sentence or clause was certain, un-
certain or undefined. The annotators could thus
choose to annotate the entire sentence as belong-
ing to one of these three classes, or to break up the
sentence into subclauses.
Pairwise inter-annotator agreement was also
measured in the article by Dalianis and Velupillai
(2010) . The average inter-annotator agreement in-
creased after the first annotation rounds, but it was
lower than the agreement between the annotators
of the BioScope Corpus.
The annotation classes used were thus negation
and speculative words, but also certain expression
and uncertain expression as well as undefined. The
annotated subset contains a total of 6 740 sen-
tences or 71 454 tokens, including its context.
3 Method for constructing the consensus
We constructed a consensus annotation out of the
three different annotations of the same clinical cor-
pus that is described in Dalianis and Velupillai
(2010). The consensus was constructed with the
general idea of choosing, as far as possible, an an-
notation for which there existed an identical anno-
tation performed by at least two of the annotators,
and thus to find a majority annotation. In the cases
where no majority was found, other methods were
used.
Other options would be to let the annotators dis-
cuss the sentences that were not identically an-
notated, or to use the method of the BioScope
Corpus, where the sentences that were not iden-
tically annotated were resolved by a chief annota-
tor (Vincze et al, 2008). A third solution, which
might, however, lead to a very biased corpus,
would be to not include the sentences for which
there was not a unanimous annotation in the re-
sulting consensus corpus.
3.1 The creation of a consensus
The annotation classes that were used for annota-
tion can be divided into two levels. The first level
consisted of the annotation classes for classifying
the type of sentence or clause. This level thus in-
cluded the annotation classes uncertain, certain
and undefined. The second level consisted of
the annotation classes for annotating cue words
for negation and speculation, thus the annotation
classes negation and speculative words. The an-
notation classes on the first level were considered
as more important for the consensus, since if there
was no agreement on the kind of expression, it
could perhaps be said to be less important which
cue phrases these expressions contained. In the
following constructed example, the annotation tag
Uncertain is thus an annotation on the first level,
while the annotation tags Negation and Specula-
tive words are on the second level.
7
<Sentence>
<Uncertain>
<Speculative_words>
<Negation>Not</Negation>
really
</Speculative_words>
much worse than before
</Uncertain>
<Sentence>
When constructing the consensus corpus, the
annotated sentences from the first rounds of an-
notation were considered as sentences annotated
before the annotators had fully learnt to apply
the guidelines. The first 1 099 of the annotated
sentences, which also had a lower inter-annotator
agreement, were therefore not included when con-
structing the consensus. Thereby, 5 641 sentences
were left to compare.
The annotations were compared on a sentence
level, where the three versions of each sentence
were compared. First, sentences for which there
existed an identical annotation performed by at
least two of the annotators were chosen. This was
the case for 5 097 sentences, thus 90 percent of the
sentences.
For the remaining 544 sentences, only annota-
tion classes on the first level were compared for a
majority. For the 345 sentences where a majority
was found on the first level, a majority on the sec-
ond level was found for 298 sentences when the
scope of these tags was disregarded. The annota-
tion with the longest scope was then chosen. For
the remaining 47 sentences, the annotation with
the largest number of annotated instances on the
second level was chosen.
The 199 sentences that were still not resolved
were then once again compared on the first level,
this time disregarding the scope. Thereby, 77 sen-
tences were resolved. The annotation with the
longest scopes on the first-level annotations was
chosen.
The remaining 122 sentences were removed
from the consensus. Thus, of the 5 641 sentences,
2 percent could not be resolved with these basic
rules. In the resulting corpus, 92 percent of the
sentences were identically annotated by at least
two persons.
3.2 Differences between the consensus and
the individual annotations
Aspects of how the consensus annotation differed
from the individual annotations were measured.
The number of occurrences of each annotation
class was counted, and thereafter normalised on
the number of sentences, since the consensus an-
notation contained fewer sentences than the origi-
nal, individual annotations.
The results in Table 1 show that there are fewer
uncertain expressions in the consensus annotation
than in the average of the individual annotations.
The reason for this could be that if the annotation
is not completeley free of randomness, the class
with a higher probability will be more frequent in
a majority consensus, than in the individual anno-
tations. In the cases where the annotators are un-
sure of how to classify a sentence, it is not unlikely
that the sentence has a higher probability of being
classified as belonging to the majority class, that
is, the class certain.
The class undefined is also less common in
the consensus annotation, and the same reasoning
holds true for undefined as for uncertain, perhaps
to an even greater extent, since undefined is even
less common.
Also the speculative words are fewer in the con-
sensus. Most likely, this follows from the uncer-
tain sentences being less common.
The words annotated as negations, on the other
hand, are more common in the consensus anno-
tation than in the individual annotations. This
could be partly explained by the choice of the 47
sentences with an annotation that contained the
largest number of annotated instances on the sec-
ond level, and it is an indication that the consensus
contains some annotations for negation cues which
have only been annotated by one person.
Type of Annot. class Individ. Consens.
Negation 853 910
Speculative words 1 174 1 077
Uncertain expression 697 582
Certain expression 4 787 4 938
Undefined expression 257 146
Table 1: Comparison of the number of occurrences
of each annotation class for the individual annota-
tions and the consensus annotation. The figures
for the individual annotations are the mean of the
three annotators, normalised on the number of sen-
tences in the consensus.
Table 2 shows how often the annotators have
divided the sentences into clauses and annotated
each clause with a separate annotation class. From
the table we can see that annotator A and also an-
8
notator H broke up sentences into more than one
type of the expressions Certain, Uncertain or Un-
defined expressions more often than annotator F.
Thereby, the resulting consensus annotation has a
lower frequency of sentences that contained these
annotations than the average of the individual an-
notations. Many of the more granular annotations
that break up sentences into certain and uncertain
clauses are thus not included in the consensus an-
notation. There are instead more annotations that
classify the entire sentence as either Certain, Un-
certain or Undefined.
Annotators A F H Cons.
No. sentences 349 70 224 147
Table 2: Number of sentences that contained more
than one instance of either one of the annotation
classes Certain, Uncertain or Undefined expres-
sions or a combination of these three annotation
classes.
3.3 Discussion of the method
The constructed consensus annotation is thus dif-
ferent from the individual annotations, and it could
at least in some sense be said to be better, since 92
percent of the sentences have been identically an-
notated by at least two persons. However, since for
example some expressions of uncertainty, which
do not have to be incorrect, have been removed, it
can also be said that some information containing
possible interpretations of the text, has also been
lost.
The applied heuristics are in most cases specific
to this annotated corpus. The method is, however,
described in order to exemplify the more general
idea to use a majority decision for selecting the
correct annotations. What is tested when using the
majority method described in this article for de-
ciding which annotation is correct, is the idea that
a possible alternative to a high annotator agree-
ment would be to ask many annotators to judge
what they consider to be certain or uncertain. This
could perhaps be based on a very simplified idea
of language, that the use and interpretation of lan-
guage is nothing more than a majority decision by
the speakers of that language.
A similar approach is used in Steidl et al
(2005), where they study emotion in speech. Since
there are no objective criteria for deciding with
what emotion something is said, they use manual
classification by five labelers, and a majority vot-
ing for deciding which emotion label to use. If less
than three labelers agreed on the classification, it
was omitted from the corpus.
It could be argued that this is also true for un-
certainty, that if there is no possibility to ask the
author of the text, there are no objective criteria
for deciding the level of certainty in the text. It is
always dependent on how it is perceived by the
reader, and therefore a majority method is suit-
able. Even if the majority approach can be used for
subjective classifications, it has some problems.
For example, to increase validity more annotators
are needed, which complicates the process of an-
notation. Also, the same phenomenon that was
observed when constructing the consensus would
probably also arise, that a very infrequent class
such as uncertain, would be less frequent in the
majority consensus than in the individual annota-
tions. Finally, there would probably be many cases
where there is no clear majority for either com-
pletely certain or uncertain: in these cases, having
many annotators will not help to reach a decision
and it can only be concluded that it is difficult to
classify this part of a text. Different levels of un-
certainty could then be introduced, where the ab-
sence of a clear majority could be an indication of
weak certainty or uncertainty, and a very weak ma-
jority could result in an undefined classification.
However, even though different levels of cer-
tainty or uncertainty are interesting when study-
ing how uncertainties are expressed and perceived,
they would complicate the process of information
extraction. Thus, if the final aim of the annota-
tion is to create a system that automatically detects
what is certain or uncertain, it would of course be
more desirable to have an annotation with a higher
inter-annotator agreement. One way of achieving
a this would be to provide more detailed annota-
tion guidelines for what to define as certainty and
uncertainty. However, when it comes to such a
vague concept as uncertainty, there is always a thin
line between having guidelines capturing the gen-
eral perception of uncertainty in the language and
capturing a definition of uncertainty that is specific
to the writers of the guidelines. Also, there might
perhaps be a risk that the complex concept of cer-
tainty and uncertainty becomes overly simplified
when it has to be formulated as a limited set of
guidelines. Therefore, a more feasible method of
achieving higher agreement is probably to instead
9
Class Neg-Spec Relevant Retrieved Corpus Precision Recall F-score
Negation 782 890 853 0.879 0.917 0.897
Speculative words 376 558 1061 0.674 0.354 0.464
Total 1 158 1 448 1 914 0.800 0.605 0.687
Table 3: The results for negation and speculation on consensus when executing Stanford NER CRF using
ten-fold cross validation.
Class Cert-Uncertain Relevant Retrieved Corpus Precision Recall F-score
Certain expression 4 022 4 903 4 745 0.820 0.848 0.835
Uncertain expression 214 433 577 0.494 0.371 0.424
Undefined expression 2 5 144 0.400 0.014 0.027
Total 4 238 5 341 5 466 0.793 0.775 0.784
Table 4: The results for certain and uncertain on consensus when executing Stanford NER CRF using
ten-fold cross validation.
simplify what is being annotated, and not annotate
for such a broad concept as uncertainty in general.
Among other suggestions for improving the an-
notation guidelines for the corpus that the consen-
sus is based on, Velupillai (2010) suggests that the
guidelines should also include instructions on the
focus of the uncertainties, that is, what concepts
are to be annotated for uncertainty.
The task could thus, for example, be tailored to-
wards the information that is to be extracted, and
thereby be simplified by only annotating for un-
certainty relating to a specific concept. If diseases
or symptoms that are present in a patient are to be
extracted, the most relevant concept to annotate is
whether a finding is present or not present in the
patient, or whether it is uncertain if it is present or
not. This approach has, for example, achieved a
very high inter-annotator agreement in the anno-
tation of the evaluation data used by Chapman et
al. (2001). Even though this approach is perhaps
linguistically less interesting, not giving any infor-
mation on uncertainties in general, if the aim is to
search for diseases and symptoms in patients, it
should be sufficient.
In light of the discussion above, the question to
what extent the annotations in the constructed con-
sensus capture a general perception of certainty or
uncertainty must be posed. Since it is constructed
using a majority method with three annotators,
who had a relatively low pairwise agreement, the
corpus could probably not be said to be a precise
capture of what is a certainty or uncertainty. How-
ever, as Artstein and Poesio (2008) point out, it
cannot be said that there is a fixed level of agree-
ment that is valid for all purposes of a corpus, but
the agreement must be high enough for a certain
purpose. Therefore, if the information on whether
there was a unanimous annotation of a sentence or
not is retained, serving as an indicator of how typ-
ical an expression of certainty or uncertainty is,
the constructed corpus can be a useful resource.
Both for studying how uncertainty in clinical text
is constructed and perceived, and as one of the re-
sources that is used for learning to automatically
detect certainty and uncertainty in clinical text.
4 Results of training with Stanford NER
CRF
As a first indication of whether it is possible to use
the annotated consensus corpus for finding nega-
tion and speculation in clinical text, we trained the
Stanford NER CRF, (Finkel et al, 2005) on the an-
notated data. Artstein and Poesio (2008) write that
the fact that annotated data can be generalized and
learnt by a machine learning system is not an in-
dication that the annotations capture some kind of
reality. If it would be shown that the constructed
consensus is easily generalizable, this can thus not
be used as an evidence of its quality. However, if it
would be shown that the data obtained by the an-
notations cannot be learnt by a machine learning
system, this can be used as an indication that the
data is not easily generalizable and that the task
to learn perhaps should, if possible, be simplified.
Of course, it could also be an indication that an-
other learning algorithm should be used or other
features selected.
We created two training sets of annotated con-
sensus material.
The first training set contained annotations on
the second level, thus annotations that contained
the classes Speculative words and Negation. In 76
cases, the tag for Negation was inside an annota-
tion for Speculative words, and these occurrences
10
Class Neg-Spec Bio Relevant Retrieved Corpus Precision Recall F-score
Negation 843 864 872 0.976 0.967 0.971
Speculative words 1 021 1 079 1 124 0.946 0.908 0.927
Scope
1
1 295 1 546 1 595
2
0.838 0.812 0.825
Table 5: The results for negations, speculation cues and scopes on the BioScope Corpus when executing
Stanford NER CRF using ten-fold cross validation.
Class Neg-Spec Relevant Retrieved Corpus Precision Recall F-score
Negation A 791 1 005 896 0.787 0.883 0.832
Speculative words 684 953 1 699 0.718 0.403 0.516
Negation F 938 1097 1023 0.855 0.916 0.884
Speculative words 464 782 1 496 0.593 0.310 0.407
Negation H 722 955 856 0.756 0.843 0.797
Speculative words 552 853 1 639 0.647 0.336 0.443
Table 6: The results for negations and speculation cues and scopes for annotator A, F and H respectively
when executing Stanford NER CRF using ten-fold cross validation.
of the tag Negation were removed. It is detecting
this difference between a real negation cue and a
negation word inside a cue for speculation that is
one of the difficulties that distinguishes the learn-
ing task from a simple string matching.
The second training set only contained the con-
sensus annotations on the first level, thus the anno-
tation classes Certain, Uncertain and Undefined.
We used the default settings on Stanford NER
CRF. The results of the evaluation using ten-fold
cross validation (Kohavi, 1995) are shown in Table
3 and Table 4.
As a comparison, and to verify the suitabil-
ity of the chosen machine learning method, we
also trained and evaluated the BioScope Corpus
using Stanford NER CRF for negation, specula-
tion and scope. The results can be seen in Ta-
ble 5. When training the detection of scope, only
BioScope sentences that contained an annotation
for negation and speculation were selected for the
training and evaluation material for the Stanford
NER CRF. This division into two training sets fol-
lows the method used by Morante and Daelemans
(2009), where sentences containing a cue are first
detected, and then, among these sentences, the
scope of the cue is determined.
We also trained and evaluated the annotations
that were carried out by each annotator A, F and
H separately, i.e. the source of consensus. The re-
sults can be seen in Table 6.
We also compared the distribution of Negation
and Speculative words in the consensus versus the
BioScope Corpus and we found that the consen-
sus, in Swedish, used about the same number of
(types) for negation as the BioScope Corpus in
English (see Table 7), but for speculative words
the consensus contained many more types than the
BioScope Corpus. In the constructed consensus,
72 percent of the Speculative words occurred only
once, whereas in the BioScope Corpus this was the
case for only 24 percent of the Speculative words.
Type of word Cons. Bio
Unique words (Types)
annotated as Negation 13 19
Negations that
occurred only once 5 10
Unique words (Types)
annotated as Speculative 408 79
Speculative words that
occurred only once 294 19
Table 7: Number of unique words both in the Con-
sensus and in the BioScope Corpus that were an-
notated as Negation and as Speculative words, and
how many of these that occurred only once.
5 Discussion
The training results using our clinical consensus
corpus in Swedish gave a precision of 87.9 percent
and a recall of 91.7 percent for negation cues and a
precision of 67.4 percent and a recall of 35.4 per-
cent for speculation cues. The results for detecting
negation cues are thus much higher than for de-
tecting cues for speculation using Stanford NER
CRF. This difference is not very surprising, given
1
The scopes were trained and evaluated separetely from
the negations and speculations.
2
The original number of annotated scopes in the BioScope
Corpus is 1 981. Of these, 386 annotations for nested scopes
were removed.
11
the data in Table 7, which shows that there are only
a very limited number of negation cues, whereas
there exist over 400 different cue words for spec-
ulation. One reason why the F-score for negation
cues is not even higher, despite the fact that the
number of cues for negations is very limited, could
be that a negation word inside a tag for speculative
words is not counted as a negation cue. There-
fore, the word not in, for example, not really could
have been classified as a negation cue by Stanford
NER CRF, even though it is a cue for speculation
and not for negation. Another reason could be that
the word meaning without in Swedish (utan) also
means but, which only sometimes makes it a nega-
tion cue.
We can also observe in Table 4, that the results
for detection of uncertain expressions are very low
(F-score 42 percent). For undefined expressions,
due to scarce training material, it is not possible
to interpret the results. For certain expressions the
results are acceptable, but since the instances are
in majority, the results are not very useful.
Regarding the BioScope Corpus we can ob-
serve (see Table 5) that the training results both
for detecting cues for negation and for specula-
tions are very high, with an F-score of 97 and 93
percent, respectively. For scope detection, the re-
sult is lower but acceptable, with an F-score of
83 percent. These results indicate that the chosen
method is suitable for the learning task.
The main reason for the differences in F-score
between the Swedish consensus corpus and the
BioScope Corpus, when it comes to the detection
of speculation cues, is probably that the variation
of words that were annotated as Speculative word
is much larger in the constructed consensus than
in the BioScope Corpus.
As can be seen in Table 7, there are many more
types of speculative words in the Swedish consen-
sus than in the BioScope Corpus. We believe that
one reason for this difference is that the sentences
in the constructed consensus are extracted from
a very large number of clinics (several hundred),
whereas the BioScope Corpus comes from one ra-
diology clinic. This is supported by the findings of
Szarvas (2008), who writes that cues for specula-
tion are domain-specific. In this case, however, the
texts are still within the domain of clinical texts.
Another reason for the larger variety of cues for
speculation in the Swedish corpus could be that
the guidelines for annotating the BioScope Cor-
pus and the method for creating a consensus were
different.
When comparing the results for the individual
annotators with the constructed consensus, the fig-
ures in Tables 3 and 6 indicate that there are no
big differences in generalizability. When detecting
cues for negation, the precision for the consensus
is better than the precision for the individual an-
notations. However, the results for the recall are
only slightly better or equivalent for the consensus
than for the individual annotations. If we analyse
the speculative cues we can observe that the con-
sensus and the individual annotations have similar
results.
The low results for learning to detect cues for
speculation also serve as an indicator that the task
should be simplified to be more easily generaliz-
able. For example, as previously suggested for
increasing the inter-annotator agreement, the task
could be tailored towards the specific information
that is to be extracted, such as the presence of a
disease in a patient.
6 Future work
To further investigate if a machine learning algo-
rithm such as Conditional Random Fields can be
used for detecting speculative words, more infor-
mation needs to be provided for the Conditional
Random Fields, such as part of speech or if any
of the words in the sentence can be classified as a
symptom or a disease. One Conditional Random
Fields system that can treat nested annotations is
CRF++ (CRF++, 2010). CRF++ is used by several
research groups and we are interested in trying it
out for the negation and speculation detection as
well as scope detection.
7 Conclusion
A consensus clinical corpus was constructed by
applying a few basic rules for combining three in-
dividual annotations into one. Compared to the
individual annotations, the consensus contained
fewer annotations of uncertainties and fewer an-
notations that divided the sentences into clauses.
It also contained fewer annotations for speculative
words, and more annotations for negations. Of
the sentences in the constructed corpus, 92 percent
were identically annotated by at least two persons.
In comparison with the BioScope Corpus, the
constructed consensus contained both a larger
number and a larger variety of speculative cues.
12
This might be one of the reasons why the results
for detecting cues for speculative words using the
Stanford NER CRF are much better for the Bio-
Scope Corpus than for the constructed consensus
corpus; the F-scores are 93 percent versus 46 per-
cent.
Both the BioScope Corpus and the constructed
consensus corpus had high values for detection of
negation cues, F-scores 97 and 90 percent, respec-
tively.
As is suggested by Velupillai (2010), the guide-
lines for annotation should include instructions
on the focus of the uncertainties. To focus the
decision of uncertainty on, for instance, the dis-
ease of a patient, might improve both the inter-
annotator agreement and the possibility of auto-
matically learning to detect the concept of uncer-
tainty.
Acknowledgments
We are very grateful for the valuable comments by
the three anonymous reviewers.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
biomedical informatics, 34(5):301?310.
CRF++. 2010. CRF++: Yet another CRF toolkit, May
8. http://crfpp.sourceforge.net//.
Hercules Dalianis and Sumithra Velupillai. 2010.
How certain are clinical assessments? Annotating
Swedish clinical text for (un)certainties, specula-
tions and negations. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, May.
Peter L. Elkin, Steven H. Brown, Brent A. Bauer,
Casey S. Husser, William Carruth, Larry R.
Bergstrom, and Dietlind L. Wahner-Roedler. 2005.
A controlled trial of automated classification of
negation from clinical notes. BMC Medical Infor-
matics and Decision Making, 5(1):13.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 363?370.
Yang Huang and Henry J. Lowe. 2007. A novel hybrid
approach to automated negation detection in clinical
radiology reports. Journal of the American Medical
Informatics Association, 14(3):304.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003, pages 180?183. Association for
Computational Linguistics.
Ron Kohavi. 1995. A study of cross-validation and
bootstrap for accuracy estimation and model selec-
tion. In International Joint Conference on Artificial
Intelligence, volume 14, pages 1137?1145.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In CoNLL ?09: Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning, pages 21?29. Association for Com-
putational Linguistics.
Lior Rokach, Roni Romano, and Oded Maimo. 2008.
Negation recognition in medical narrative reports.
Information Retrieval, 11(6):499?538.
Maria Skeppstedt. 2010. Negation detection in
Swedish clinical text. In Louhi?10 - Second Louhi
Workshop on Text and Data Mining of Health Doc-
uments, held in conjunction with NAACL HLT 2010,
Los Angeles, June.
SNOMED-CT. 2010. Systematized nomen-
clature of medicine-clinical terms, May 8.
http://www.ihtsdo.org/snomed-ct/.
Stefan Steidl, Michael Levit, Anton Batliner, Elmar
N?oth, and Heinrich Niemann. 2005. ?Off all
things the measure is man? Automatic classification
of emotions and inter-labeler consistency. In Pro-
ceeding of the IEEE ICASSP,2005, pages 317?320.
Gy?orgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selec-
tion of keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Sumithra Velupillai. 2010. Towards a better un-
derstanding of uncertainties and speculations in
swedish clinical text ? analysis of an initial anno-
tation trial. To be published in the proceedings of
the Negation and Speculation in Natural Language
Processing Workshop, July 10, 2010, Uppsala, Swe-
den.
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas,
Gy?orgy M?ora, and J?anos Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(S-11).
13
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 36?44,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Corpus-Driven Terminology Development: Populating Swedish
SNOMED CT with Synonyms Extracted from Electronic Health Records
Aron Henriksson1, Maria Skeppstedt1, Maria Kvist1,2, Martin Duneld1, Mike Conway3
1Department of Computer and Systems Sciences (DSV), Stockholm University, Sweden
2Department of Learning, Informatics, Management and Ethics (LIME), Karolinska Institute, Sweden
3Division of Biomedical Informatics, University of California San Diego, USA
Abstract
The various ways in which one can re-
fer to the same clinical concept needs to
be accounted for in a semantic resource
such as SNOMED CT. Developing termi-
nological resources manually is, however,
prohibitively expensive and likely to re-
sult in low coverage, especially given the
high variability of language use in clinical
text. To support this process, distributional
methods can be employed in conjunction
with a large corpus of electronic health
records to extract synonym candidates for
clinical terms. In this paper, we exem-
plify the potential of our proposed method
using the Swedish version of SNOMED
CT, which currently lacks synonyms. A
medical expert inspects two thousand term
pairs generated by two semantic spaces ?
one of which models multiword terms in
addition to single words ? for one hundred
preferred terms of the semantic types dis-
order and finding.
1 Introduction
In recent years, the adoption of standardized ter-
minologies for the representation of clinical con-
cepts ? and their textual instantiations ? has en-
abled meaning-based retrieval of information from
electronic health records (EHRs). By identify-
ing and linking key facts in health records, the
ever-growing stores of clinical documentation now
available to us can more readily be processed
and, ultimately, leveraged to improve the qual-
ity of care. SNOMED CT1 has emerged as the
de facto international terminology for represent-
ing clinical concepts in EHRs and is today used
in more than fifty countries, despite only being
1http://www.ihtsdo.org/snomed-ct/
available in a handful of languages2. Translations
into several other languages are, however, under
way3. This translation effort is essential for more
widespread integration of SNOMED CT in EHR
systems globally.
Translating a comprehensive4 terminology such
as SNOMED CT to an additional language is,
however, a massive and expensive undertaking. A
substantial part of this process involves enrich-
ing the terminology with synonyms in the tar-
get language. SNOMED CT has, for instance,
recently been translated into Swedish; however,
the Swedish version does not as yet contain syn-
onyms. Methods and tools that can accelerate the
language porting process in general and the syn-
onym identification task in particular are clearly
needed, not only to lower costs but also to in-
crease the coverage of SNOMED CT in clinical
text. Methods that can account for real-world lan-
guage use in the clinical setting, then, as well as to
changes over time, are particularly valuable.
This paper evaluates a semi-automatic method
for the extraction of synonyms of SNOMED CT
preferred terms using models of distributional se-
mantics to induce semantic spaces from a large
corpus of clinical text. In contrast to most ap-
proaches that exploit the notion of distributional
similarity for synonym extraction, this method ad-
dresses the key problem of identifying synonymy
between terms of varying length: a simple solution
is proposed that effectively incorporates the notion
of paraphrasing in a distributional framework. The
semantic spaces ? and, by extension, the method ?
are evaluated for their ability to extract synonyms
of SNOMED CT terms of the semantic types dis-
order and finding in Swedish.
2SNOMED CT is currently available in US English, UK
English, Spanish, Danish and Swedish.
3http://www.ihtsdo.org/snomed-ct/
snomed-ct0/different-languages/
4SNOMED CT contains more than 300,000 active con-
cepts and over a million relations.
36
2 Background
Synonymy is an aspect of semantics that con-
cerns the fact that concepts can be instantiated
using multiple linguistic expressions, or, viewed
conversely, that multiple linguistic expressions
can refer to the same concept. As synonymous
expressions do not necessarily consist of single
words, we sometimes speak of paraphrasing rather
than synonymy (Androutsopoulos and Malakasio-
tis, 2010). This variability of language use needs
to be accounted for in order to build high-quality
natural language processing (NLP) and text min-
ing systems. This is typically achieved by us-
ing thesauri or encoding textual instantiations of
concepts in a semantic resource, e.g. an ontol-
ogy. Creating such resources manually is, how-
ever, prohibitively expensive and likely to lead
to low coverage, especially in the clinical genre
where language use variability is exceptionally
high (Meystre et al, 2008).
2.1 Synonym Extraction
As a result, the task of extracting synonyms ?
and other semantic relations ? has long been a
central challenge in the NLP research commu-
nity, not least in the biomedical (Cohen and Hersh,
2005) and clinical (Meystre et al, 2008) do-
mains. A wide range of techniques has been pro-
posed for relation extraction in general and syn-
onym extraction in particular ? lexico-syntactic
patterns (Hearst, 1992), distributional semantics
(Dumais and Landauer, 1997) and graph-based
models (Blondel et al, 2004) ? from a variety
of sources, including dictionaries (Blondel et al,
2004), linked data such as Wikipedia (Nakayama
et al, 2007), as well as both monolingual (Hindle,
1990) and multilingual (van der Plas and Tiede-
mann, 2006) corpora. In recent years, ensemble
methods have been applied to obtain better perfor-
mance on the synonym extraction task, combin-
ing models from different families (Peirsman and
Geeraerts, 2009), with different parameter settings
(Henriksson et al, 2012) and induced from differ-
ent data sources (Wu and Zhou, 2003).
In the context of biomedicine, the goal has of-
ten been to extract synonyms of gene and pro-
tein names from the biomedical literature (Yu and
Agichtein, 2003; Cohen et al, 2005; McCrae and
Collier, 2008). In the clinical domain, Conway
and Chapman (2012) used a rule-based approach
to generate potential synonyms from the BioPor-
tal ontology web service, verifying candidate syn-
onyms against a large clinical corpus. Zeng et
al. (2012) used three query expansion methods
for information retrieval of clinical documents and
found that a model of distributional semantics ?
LDA-based topic modeling ? generated the best
synonyms. Henriksson et al (2012) combined
models of distributional semantics ? random in-
dexing and random permutation ? to extract syn-
onym candidates for Swedish MeSH5 terms and
possible abbreviation-definition pairs. In the con-
text of SNOMED CT, distributional methods have
been applied to capture synonymous relations be-
tween terms of varying length: 16-24% of English
SNOMED CT synonyms present in a large clini-
cal corpus were successfully identified in a list of
twenty suggestions (Henriksson et al, 2013).
2.2 Distributional Semantics
Models of distributional semantics (see Cohen and
Widdows (2009) for an overview of methods and
their application in the biomedical domain) were
initially motivated by the inability of the vector
space model to account for synonymy, which had
a negative impact on recall in information retrieval
systems (Deerwester et al, 1990). The theoretical
foundation underpinning such models of seman-
tics is the distributional hypothesis (Harris, 1954),
according to which words with similar meanings
tend to appear in similar contexts. By exploiting
the availability of large corpora, the meaning of
terms can be modeled based on their distribution
in different contexts. An estimate of the semantic
relatedness between terms can then be quantified,
thereby, in some sense, rendering semantics com-
putable.
An obvious application of distributional seman-
tics is the extraction of semantic relations between
terms, such as synonymy, hyp(o/er)nymy and co-
hyponymy (Panchenko, 2013). As synonyms are
interchangeable in some contexts ? and thus have
similar distributional profiles ? synonymy is cer-
tainly a semantic relation that should be captured.
However, since hyp(o/er)nyms and co-hyponyms
? in fact, even antonyms ? are also likely to have
similar distributional profiles, such semantic rela-
tions will be extracted too.
Many models of distributional semantics dif-
fer in how context vectors, representing term
5Medical Subject Headings (http://www.nlm.nih.
gov/mesh).
37
meaning, are constructed. They are typically de-
rived from a term-context matrix that contains
the (weighted, normalized) frequency with which
terms occur in different contexts. Partly due
to the intractability of working with such high-
dimensional data, it is projected into a lower-
dimensional (semantic) space, while approxi-
mately preserving the relative distances between
data points. Methods that rely on computa-
tionally expensive dimensionality reduction tech-
niques suffer from scalability issues.
Random Indexing
Random indexing (RI) (Kanerva et al, 2000) is
a scalable and computationally efficient alterna-
tive in which explicit dimensionality reduction is
avoided: a lower dimensionality d is instead cho-
sen a priori as a model parameter and the d-
dimensional context vectors are then constructed
incrementally. Each unique term in the corpus is
assigned a static index vector, consisting of ze-
ros and a small number of randomly placed 1s
and -1s6. Each term is also assigned an initially
empty context vector, which is incrementally up-
dated by adding the index vectors of the surround-
ing words within a sliding window, weighted by
their distance to the target term. The semantic re-
latedness between two terms is then estimated by
calculating, for instance, the cosine similarity be-
tween their context vectors.
Random Permutation
Random permutation (RP) (Sahlgren et al, 2008)
is a modification of RI that attempts to take into
account term order information by simply permut-
ing (i.e. shifting) the index vectors according to
their direction and distance from the target term
before they are added to the context vector. RP
has been shown to outperform RI on the synonym
part of the TOEFL7 test.
Model Parameters
The model parameters need to be configured for
the task that the semantic space is to be used
for. For instance, with a document-level con-
text definition, syntagmatic relations are mod-
eled, i.e. terms that belong to the same topic
(<car, motor, race>), whereas, with a sliding win-
dow context definition, paradigmatic relations are
6By generating sparse vectors of a sufficiently high di-
mensionality in this way, the context representations will be
nearly orthogonal.
7Test Of English as a Foreign Language
modeled (<car, automobile, vehicle>) (Sahlgren,
2006). Synonymy is an instance of a paradigmatic
relation.
The dimensionality has also been shown to be
potentially very important, especially when the
size of the vocabulary and the number of contexts8
are large (Henriksson and Hassel, 2013).
3 Materials and Methods
The task of semi-automatically identifying syn-
onyms of SNOMED CT preferred terms is here
approached by, first, statistically identifying mul-
tiword terms in the data and treating them as com-
pounds; then, performing a distributional analysis
of a preprocessed clinical corpus to induce a se-
mantic term space; and, finally, extracting the se-
mantically most similar terms for each preferred
term of interest.
The experimental setup can be broken down
into the following steps: (1) data preparation, (2)
term recognition, (3) model parameter tuning and
(4) evaluation. Semantic spaces are induced with
different parameter configurations on two dataset
variants: one with unigram terms only and one that
also includes multiword terms. The model param-
eters are tuned using MeSH, which contains syn-
onyms for Swedish. The best parameter settings
for each of the two dataset variants are then em-
ployed in the final evaluation, where a medical ex-
pert inspects one hundred term lists extracted for
SNOMED CT preferred terms belonging to the se-
mantic types disorder and finding.
3.1 Data Preparation
The data used to induce the semantic spaces is ex-
tracted from the Stockholm EPR Corpus (Dalia-
nis et al, 2009), which contains Swedish health
records from the Karolinska University Hospital
in Stockholm9. The subset (?33 million tokens)
used in these experiments comprises all forms of
text-based records ? i.e., clinical notes ? from
a large variety of clinical practices. The docu-
ments in the corpus are initially preprocessed by
simply lowercasing tokens and removing punctu-
ation and digits. Lemmatization is not performed,
as we want to be able to capture morphological
8The vocabulary size and the number of contexts are
equivalent when employing a window context definition.
9This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpro?vningsna?mnden i Stock-
holm), permission number 2012/834-31/5.
38
variants of terms; stop-word filtering is not per-
formed, as traditional stop words ? for instance,
high-frequency function words ? could potentially
be constituents of multiword terms.
3.2 Term Recognition
Multiword terms are extracted statistically from
the corpus using the C-value statistic (Frantzi and
Ananiadou, 1996; Frantzi et al, 2000). This tech-
nique has been used successfully for term recog-
nition in the biomedical domain, largely due to
its ability to handle nested terms (Zhang et al,
2008). Using the C-value statistic for term recog-
nition first requires a list of candidate terms, for
which the C-value can then be calculated. Here,
this is simply produced by extracting n-grams ?
unigrams, bigrams and trigrams ? from the corpus
with TEXT-NSP (Banerjee and Pedersen, 2003).
The statistic is based on term frequency and term
length (number of words); if a candidate term is
part of a longer candidate term (as will be the case
for practically all unigram and bigram terms), the
number and frequency of those longer terms are
also taken into account (Figure 1).
In order to improve the quality of the extracted
terms, a number of filtering rules is applied to the
generated term list: terms that begin and/or end
with certain words, e.g. prepositions and articles,
are removed. The term list ? ranked according to
C-value ? is further modified by giving priority to
terms of particular interest, e.g. SNOMED CT dis-
order and finding preferred terms: these are moved
to the top of the list, regardless of their C-value.
As a result, the statistical foundation on which the
distributional method bases its semantic represen-
tation will effectively be strengthened.
The term list is then used to perform exact string
matching on the entire corpus: multiword terms
with a higher C-value than their constituents are
concatenated. We thereby treat multiword terms as
separate (term) types with distinct distributions in
the data, different from those of their constituents.
3.3 Model Parameter Tuning
Term spaces with different parameter configu-
rations are induced from the two dataset vari-
ants: one containing only unigram terms (Uni-
gram Word Spaces) and one containing also mul-
tiword terms (Multiword Term Spaces). The fol-
lowing model parameters are tuned:
? Distributional Model: Random indexing (RI)
vs. Random permutation (RP)
? Context Window Size: 2+2, 4+4, 8+8 sur-
rounding terms (left+right of the target term)
? Dimensionality: 1000, 2000, 3000
As the Swedish version of SNOMED CT cur-
rently does not contain synonyms, it cannot be
used to perform the parameter tuning automat-
ically. This is instead done with the Swedish
version of MeSH, which is one of the very few
standard terminologies that contains synonyms for
medical terms in Swedish. However, as the op-
timal parameter configurations for capturing syn-
onymy are not necessarily identical for all seman-
tic types, the parameter tuning is performed by
evaluating the semantic spaces for their ability to
identify synonyms of MeSH terms that belong to
the categories Disease or Syndrome and Sign or
Symptom. These particular categories are simply
chosen as they, to a reasonable extent, seem to
correspond to the SNOMED CT semantic types
studied in this paper, namely Disorder and Find-
ing. Only synonym pairs that appear at least fifty
times in each of the dataset variants are included
(155 for Unigram Word Spaces and 123 for Mul-
tiword Term Spaces), as the statistical foundation
for terms that only occur rarely in the data may
not be sufficiently solid. In these Multiword Term
Spaces, the MeSH terms ? but not the synonyms
? are given precedence in the term list. A term
is provided as input to a semantic space and the
twenty semantically most similar terms are out-
put, provided that they also appear at least fifty
times in the data. Recall Top 20 is calculated for
each input term: what proportion of the MeSH
synonyms are identified in a list of twenty sugges-
tions? Since each synonym pair must appear at
least fifty times in the corresponding dataset vari-
ant, it should be duly noted that the optimization
sets will not be identical, which in turn means that
the results of the Unigram Word Spaces and the
Multiword Term Spaces are not directly compara-
ble. The optimal parameter configuration, then,
may be different when also multiword terms are
modeled.
3.4 Evaluation
The optimal parameter configuration for each
dataset variant is employed in the final evaluation.
In this Multiword Term Space, the SNOMED CT
39
C-value(a) =
{
log2 |a| ? f(a) if a is not nested
log2 |a| ? (f(a)?
1
P (Ta)
?
bTa f(b)) otherwise
a = candidate term Ta = set of extracted candidate terms that contain a
b = longer candidate terms P (Ta) = number of candidate terms in Ta
f(a) = term frequency of a f(b) = term frequency of longer candidate term b
|a| = length of candidate term (number of words)
Figure 1: C-Value Formula. The formula for calculating C-value of candidate terms.
preferred terms of interest, rather than the MeSH
terms, are prioritized in the term list. The seman-
tic spaces ? and, in effect, the method ? are pri-
marily evaluated for their ability to identify syn-
onyms of SNOMED CT preferred terms, in this
case of concepts that belong to the semantic types
disorder and finding. The need to identify syn-
onyms for these semantic types is clear, as it has
been shown that the coverage of SNOMED CT
for mentions of disorders (38%) and, in particu-
lar, findings (23%) in Swedish clinical text is low
(Skeppstedt et al, 2012). Since the Swedish ver-
sion of SNOMED CT currently lacks synonyms,
the evaluation reasonably needs to be manual, as
there is no reference standard. One option, then,
could be to choose a random sample of preferred
terms to use in the evaluation. A potential draw-
back of such a(n) (unguided) selection is that many
concepts in the English version of SNOMED CT
do not have any synonymous terms, which might
lead to evaluators spending valuable time looking
for something which does not exist. An alterna-
tive approach, which is assumed here, is to inspect
concepts that have many synonyms in the English
version of SNOMED CT. The fact that some con-
cepts have many textual instantiations in one lan-
guage does not necessarily imply that they also
have many textual instantiations in another lan-
guage. This, however, seems to be the case when
comparing the English and Swedish versions of
MeSH: terms10 that have the most synonyms in the
English version tend to have at least one synonym
in the Swedish version to a larger extent than a ran-
dom selection of terms (60% and 62% of the terms
in the Swedish version have at least one synonym
when looking at the top 100 and top 50 terms with
the most synonyms in the English version, com-
pared to 41% overall in the Swedish version).
For the two dataset variants, we thus select 25
SNOMED CT preferred terms for each semantic
10These calculations are based on MeSH terms that belong
to the categories Disease or Syndrome and Sign or Symptom.
type ? disorder and finding ? that (1) have the most
synonyms in the English version and (2) occur at
least fifty times in the data. In total, fifty terms
are input to the Unigram Word Space and another
fifty terms (potentially with some overlap) are in-
put to the Multiword Term Space. A medical ex-
pert inspects the twenty semantically most simi-
lar terms for each input term. Synonymy is here
the primary semantic relation of interest, but the
semantic spaces are also evaluated for their abil-
ity, or tendency, to extract other semantic term re-
lations: hypernyms or hyponyms, co-hyponyms,
antonyms, as well as disorder-finding relations.
4 Results
The term recognition and concatenation of mul-
tiword terms naturally affect some properties of
the dataset variants, such as the vocabulary size
(number of types) and the type-token ratio. The
Unigram Word Space contains 381,553 types and
an average of 86.54 tokens/type, while the Mul-
tiword Term Space contains 2,223,953 types and
an average of 9.72 tokens/type. This, in turn, may
have an effect on which parameter configuration is
?optimal? for the synonym extraction task. In fact,
this seems to be the case when tuning the parame-
ters for the two dataset variants. For the Unigram
Word Spaces, random indexing with a sliding con-
text window of 8+8 terms and a dimensionality of
2000 seems to work best, whereas for the Mul-
tiword Term Spaces, random permutation with a
sliding window context of 4+4 terms and a dimen-
sionality of 3000 works better (Table 1).
When these parameter configurations are ap-
plied to the SNOMED CT terms, a total of 40 syn-
onyms are extracted by the Unigram Word Space
and 33 synonyms by the Multiword Term Space
(Table 2). On average, 0.80 and 0.66 synonyms
are extracted per preferred term, respectively. The
number of identified synonyms per input term
varies significantly: for some, none; for others, up
to ten. Other semantic relations are also extracted
40
Unigram Word Spaces Multiword Term Spaces
RI RP RI RP
Sliding Window? 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8
1000 dimensions 0.43 0.47 0.48 0.41 0.45 0.42 0.21 0.25 0.26 0.25 0.26 0.24
2000 dimensions 0.43 0.48 0.49 0.48 0.48 0.43 0.21 0.24 0.25 0.25 0.25 0.24
3000 dimensions 0.44 0.47 0.48 0.46 0.45 0.43 0.22 0.24 0.24 0.23 0.27 0.25
Table 1: Model Parameter Tuning. Results, reported as recall top 20, for MeSH synonyms that appear
at least 50 times in each of the dataset variants (unigram vs. multiword). Random indexing (RI) and
Random permutation (RP) term spaces were built with different context window sizes (2+2, 4+4, 8+8
surrounding terms) and dimensionality (1000, 2000, 3000).
by the semantic spaces: mainly co-hyponyms,
but also hypernyms and hyponyms, antonyms and
disorder-finding relations. The Unigram Word
Space extracts, on average, 0.52 hypernyms or hy-
ponyms, 1.8 co-hyponyms, 0.1 antonyms and 0.34
disorder-finding relations. The Multiword Term
Space extracts, on average, 0.16 hypernyms or
hyponyms, 1.1 co-hyponyms, 0.14 antonyms and
0.66 disorder-finding relations. In general, more
of the above semantic relations are extracted by
the Unigram Word Space than by the Multiword
Term Space (178 vs. 136). It is, however, inter-
esting to note that almost twice as many disorder-
finding relations are extracted by the latter com-
pared to the former. Of course, none of the re-
lations extracted by the Unigram Word Space in-
volve a multiword term; on the other hand, more
than half (around 57%) of the relations extracted
by the Multiword Term Space involve at least one
multiword term.
Both semantic spaces identify more synonyms
of preferred terms that belong to the semantic type
finding than disorder (in total 56 vs. 39). The same
holds true for hyp(er/o)nyms and co-hypnoyms;
however, the converse is true for antonyms and
disorder-finding relations.
5 Discussion
The results demonstrate that it is indeed possible
to extract synonyms of medical terms by perform-
ing a distributional analysis of a large corpus of
clinical text ? unigram-unigram relations, as well
as unigram-multiword and multiword-unigram re-
lations. It is also clear, however, that other se-
mantically related terms share distributional pro-
files to a similar degree as synonymous terms. The
predominance of the other semantic relations, ex-
cept for antonymy, in the term lists can reason-
ably be explained by the simple fact that there
exist more hypernyms, hyponyms, co-hyponyms
and disorder-finding relations than synonyms (or
antonyms).
It is also evident that more semantic relations,
and indeed more synonyms, are extracted by the
Unigram Word Space than the Multiword Term
Space. Again, it is important to underline that the
results cannot be compared without due qualifica-
tion since the evaluation sets are not identical: the
Unigram Word Space does not contain any mul-
tiword terms, for instance. The ability to model
multiword terms in a distributional framework and
to handle semantic composition ? i.e., how mean-
ing is, and sometimes is not, composed by the
meaning of its constituents ? has long been an en-
deavor in the NLP research community (Sag et al,
2002; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Mitchell, 2011). Treating
multiword terms as compound tokens is a simple
and rather straightforward approach, which also
makes intuitive sense: rather than treat individ-
ual words as clearly delineated bearers of mean-
ing, identify semantic units ? regardless of term
length ? and model their distributional profiles.
Unfortunately, there are problems with this ap-
proach. First, the attendant increase in vocabu-
lary size entails a lower tokens-type ratio, which
in turn means that the statistical foundation for
terms will weaken. In this case, the average token-
type ratio decreased from 86.54 to 9.72. This ap-
proach therefore requires access to a sufficiently
large corpus. Second, the inflation in vocabulary
size entails a corresponding increase in the num-
ber of vectors in the semantic space. This not only
requires more memory; to ensure that the crucial
near-orthogonality property11 of RI-based models
is maintained, the dimensionality has to be suffi-
11Random indexing assumes that the index vectors ? rep-
resenting distinct contexts ? are nearly orthogonal.
41
Unigram Word Space Multiword Term Space
DISORDER FINDING DISORDER FINDING
Synonyms
sum 18 22 16 17
average 0.72 0.88 0.64 0.68
? 1 / preferred term 12 12 8 6
involves mwe - - 10 13
Hyp(er/o)nyms
sum 12 14 4 4
average 0.48 0.56 0.16 0.16
? 1 / preferred term 6 8 4 3
involves mwe - - 3 3
Co-hyponyms
sum 34 56 22 33
average 1.36 2.24 0.88 1.32
? 1 / preferred term 14 17 10 13
involves mwe - - 19 15
Antonyms
sum 3 2 4 3
average 0.12 0.08 0.16 0.12
? 1 / preferred term 3 2 3 3
involves mwe - - 0 1
Disorder-Finding
sum 11 6 28 5
average 0.44 0.24 1.12 0.2
? 1 / preferred term 6 5 12 5
involves mwe - - 11 2
Table 2: Evaluation Results. The types of semantic relations extracted among the twenty most se-
mantically similar terms of 25 DISORDER and 25 FINDING SNOMED CT preferred terms from each
semantic space. Sum is the total number of identified relevant terms. Average is the average number of
relevant terms per preferred term. ? 1 / preferred term is the number of preferred terms for which at
least one relevant term is identified. Involves mwe is the number of relevant relations where either the
preferred term or the relevant term is a multiword expression.
ciently large in relation to the number of contexts
(represented by index vectors). In the Multiword
Term Space the vocabulary size is over two million
(compared to less than 400,000 in the Unigram
Word Space). A dimensionality of 3000 is likely
insufficient to ensure that each term type has an
initial distinct and uncorrelated representation. In
the evaluation, there were several examples where
two groups of terms ? semantically homogenous
within each group, but semantically heterogenous
across groups ? co-existed in the same term list:
these ?topics? had seemingly collapsed into the
same subspace. Despite these problems, it should
be recognized that the Multiword Term Space is, in
fact, able to retrieve 23 synonymous relations that
involve at least one multiword term. The Unigram
Word Space cannot retrieve any such relations.
The ability to extract high-quality terms would
seem to be an important prerequisite for this ap-
proach to modeling multiword terms in a distribu-
tional framework. However, despite employing a
rather simple means of extracting terms ? without
using any syntactic information ? the terms that
actually appeared in the lists of semantically re-
lated terms were mostly reasonable. This perhaps
indicates that the term recognition task does not
need to be perfect: terms of interest, of course,
need to be identified, but some noise in the form
of bad terms might be acceptable. A weakness
of the term recognition part is, however, that too
many terms were identified, which in turn led to
the aforementioned inflation in vocabulary size.
42
Limiting the number of multiword terms in the ini-
tial term list ? for instance by extracting syntactic
phrases as candidate terms ? could provide a pos-
sible solution to this problem.
Overall, more synonyms were identified for the
semantic type finding than for disorder. One pos-
sible explanation for this could be that there are
more ways of describing a finding than a disorder
? not all semantic types can be assumed to have
the same number of synonyms. The same holds
true for all other semantic relations except for dis-
order-finding, where disorders generated a much
larger number of distributionally similar findings
than vice versa. This could perhaps also be ex-
plained by the possible higher number of syn-
onyms for finding than disorder.
When this method was evaluated using the
English version of SNOMED CT, 16-24% of
known synonyms were identified (Henriksson et
al., 2013). In this case, however, we extracted
synonym candidates for terms that may or may
not have synonyms. This is thus a scenario that
more closely resembles how this method would
actually be used in a real-life setting to populate
a terminology with synonyms. Although the com-
parison with MeSH showed that terms with many
synonyms in English also tend to have at least one
synonym in Swedish, approximately 40% of them
did not have any synonyms. It is thus not certain
that the terms used in this evaluation all have at
least one synonym, which was also noted by the
evaluator in this study.
6 Conclusions
In this study, we have demonstrated a method
that could potentially be used to expedite the lan-
guage porting process of terminologies such as
SNOMED CT. With access to a large corpus of
clinical text in the target language and an initial
set of terms, this language-independent method is
able to extract and present candidate synonyms to
the lexicographer, thereby providing valuable sup-
port for semi-automatic terminology development.
A means to model multiword terms in a distri-
butional framework is an important feature of the
method and is crucial for the synonym extraction
task.
Acknowledgments
This work was partly supported by the Swedish
Foundation for Strategic Research through the
project High-Performance Data Mining for Drug
Effect Detection (ref. no. IIS11-0053) at Stock-
holm University, Sweden, and partly funded by
the Stockholm University Academic Initiative
through the Interlock project. Finally, we would
like to thank the reviewers for their constructive
feedback.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation, and Use of the Ngram Statis-
tic Package. In Proceedings of CICLing, pages 370?
381.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Representing
Adjective-Noun Constructions in Semantic Space.
In Proceedings of EMNLP, pages 1183?1193.
Vincent D. Blondel, Anah?? Gajardo, Maureen Hey-
mans, Pierre Senellart, and Paul Van Dooren.
2004. A Measure of Similarity between Graph Ver-
tices: Applications to Synonym Extraction and Web
Searching. SIAM Review, 46(4):647?666.
Aaron M. Cohen and William R. Hersh. 2005. A Sur-
vey of Current Work in Biomedical Text Mining.
Briefings in Bioinformatics, 6(1):57?71.
Trevor Cohen and Dominic Widdows. 2009. Empirical
Distributional Semantics: Methods and Biomedical
Applications. J Biomed Inform, 42(2):390?405.
AM Cohen, WR Hersh, C Dubay, and K Spackman.
2005. Using co-occurrence network structure to
extract synonymous gene and protein names from
medline abstracts. BMC Bioinformatics, 6(1):103.
Mike Conway and Wendy W. Chapman. 2012. Dis-
covering Lexical Instantiations of Clinical Con-
cepts using Web Services, WordNet and Corpus Re-
sources. In AMIA Fall Symposium, page 1604.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus:
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR, pages 243?249.
Scott Deerwester, Susan T. Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Susan T. Dumais and Thomas K. Landauer. 1997. A
Solution to Plato?s Problem: The Latent Semantic
43
Analysis Theory of Acquisition, Induction and Rep-
resentation of Knowledge. Psychological Review,
104(2):211?240.
Katerina Frantzi and Sophia Ananiadou. 1996. Ex-
tracting Nested Collocations. In Proceedings of
COLING, pages 41?46.
Katerina Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic Recognition of Multi-
Word Terms: The C-value/NC-value Method. In-
ternational Journal on Digital Libraries, 3(2):115?
130.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of EMNLP, pages 1394?1404.
Zellig S. Harris. 1954. Distributional Structure. Word,
10:146?162.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings
of COLING, pages 539?545.
Aron Henriksson and Martin Hassel. 2013. Optimiz-
ing the Dimensionality of Clinical Term Spaces for
Improved Diagnosis Coding Support. In Proceed-
ings of Louhi.
Aron Henriksson, Hans Moen, Maria Skeppstedt, Ann-
Marie Eklund, and Vidas Daudaravicius. 2012.
Synonym Extraction of Medical Terms from Clini-
cal Text Using Combinations of Word Space Mod-
els. In Proceedings of SMBM, pages 10?17.
Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying Syn-
onymy between SNOMED Clinical Terms of Vary-
ing Length Using Distributional Analysis of Elec-
tronic Health Records. In AMIA Annual Symposium
(submitted).
Donald Hindle. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings of
ACL, pages 268?275.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random Indexing of Text Samples for Latent
Semantic Analysis. In Proceedings CogSci, page
1036.
John McCrae and Nigel Collier. 2008. Synonym
Set Extraction from the Biomedical Literature by
Lexical Pattern Discovery. BMC Bioinformatics,
9(1):159.
Ste?phane M. Meystre, Guergana K. Savova, Karin C.
Kipper-Schuler, John F. Hurdle, et al 2008. Ex-
tracting Information from Textual Documents in the
Electronic Health Record: A Review of Recent Re-
search. Yearb Med Inform, 35:128?44.
Jeffrey Mitchell. 2011. Composition in Distributional
Models of Semantics. Ph.D. thesis, University of
Edinburgh.
Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2007. Wikipedia Mining for an Association Web
Thesaurus Construction. In Proceedings of WISE,
pages 322?334.
Alexander Panchenko. 2013. Similarity Measures for
Semantic Relation Extraction. Ph.D. thesis, PhD
thesis, Universite? catholique de Louvain & Bauman
Moscow State Technical University.
Yves Peirsman and Dirk Geeraerts. 2009. Predicting
Strong Associations on the Basis of Corpus Data. In
Proceedings of EACL, pages 648?656.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of CICLing, pages 1?15.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a Means to Encode Order
in Word Space. In Proceedings of CogSci, pages
1300?1305.
Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words in
High-Dimensional Vector Spaces. Ph.D. thesis, PhD
thesis, Stockholm University.
Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based Entity Recognition and Coverage
of SNOMED CT in Swedish Clinical Text. In Pro-
ceedings of LREC, pages 1250?1257.
Lonneke van der Plas and Jo?rg Tiedemann. 2006.
Finding Synonyms Using Automatic Word Align-
ment and Measures of Distributional Similarity. In
Proceedings of COLING/ACL, pages 866?873.
Hua Wu and Ming Zhou. 2003. Optimizing Syn-
onym Extraction Using Monolingual and Bilingual
Resources. In Proceedings of the Second Interna-
tional Workshop on Paraphrasing, pages 72?79.
Hong Yu and Eugene Agichtein. 2003. Extracting
Synonymous Gene and Protein Terms from Biolog-
ical Literature. Bioinformatics, 19(suppl 1):i340?
i349.
Qing T Zeng, Doug Redd, Thomas Rindflesch, and
Jonathan Nebeker. 2012. Synonym, Topic Model
and Predicate-Based Query Expansion for Retriev-
ing Clinical Documents. In Proceedings AMIA An-
nual Symposium, pages 1050?9.
Ziqi Zhang, Jose? Iria, Christopher Brewster, and Fabio
Ciravegna. 2008. A Comparative Evaluation of
Term Recognition Algorithms. In Proceedings of
LREC.
44
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 98?101,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adapting a parser to clinical text by simple pre-processing rules
Maria Skeppstedt
Dept. of Computer and Systems Sciences (DSV)
Stockholm University, Forum 100, 164 40 Kista, Sweden
mariask@dsv.su.se
Abstract
Sentence types typical to Swedish clini-
cal text were extracted by comparing sen-
tence part-of-speech tag sequences in clin-
ical and in standard Swedish text. Parsings
by a syntactic dependency parser, trained
on standard Swedish, were manually ana-
lysed for the 33 sentence types most typ-
ical to clinical text. This analysis re-
sulted in the identification of eight error
types, and for two of these error types, pre-
processing rules were constructed to im-
prove the performance of the parser. For
all but one of the ten sentence types af-
fected by these two rules, the parsing was
improved by pre-processing.
1 Introduction
Input speed is often prioritised over completeness
and grammatical correctness in health record nar-
ratives. This has the effect that lower results are
achieved when parsers trained on standard text are
applied on clinical text (Hassel et al, 2011).
Syntactic annotations to use for training a parser
on clinical text are, however, expensive (Albright
et al, 2013) and treebanking large clinical corpora
is therefore not always an option for smaller lan-
guages (Haverinen et al, 2009). There are studies
on adaptation of standard parsers to the biomedical
domain, focusing on overcoming difficulties due
to different vocabulary use (Candito et al, 2011).
How to overcome difficulties due to syntactic dif-
ferences between standard and clinical language
is, however, less studied. The aim of this study
was therefore to explore syntactic differences be-
tween clinical language and standard language and
to analyse errors made by the parser on sentence
types typical to the clinical domain. To exemplify
how this knowledge can be used, two simple pre-
processing rules for improving parser performance
on these typical sentences were developed.
2 Method
To find sentence types typical to the clinical do-
main, a comparison to standard text was con-
ducted. The used clinical corpus was: free-text
entries from assessment sections, thus mostly con-
taining diagnostic reasoning, that were randomly
selected from the Stockholm EPR corpus1 (Dalia-
nis et al, 2009); and the used standard corpus
was: La?kartidningen (Kokkinakis, 2012), a jour-
nal from the Swedish Medical Association.
The comparison was carried out on part-of-
speech sequences on a sentence level. The part-
of-speech tagger Granska (Carlberger and Kann,
1999), having an accuracy of 92% on clinical text
(Hassel et al, 2011), was applied on both cor-
pora, and the proportion of each sentence tag se-
quence was calculated. ?Sentence tag sequence?
refers here to the parts-of-speech corresponding to
each token in the sentence, combined to one unit,
e.g. ?dt nn vb nn mad? for the sentence ?The pa-
tient has headache.?. Pronouns, nouns and proper
names were collapsed into one class, as they often
play the same role in the sentence, and as terms
specific to the clinical domain are tagged inconsis-
tently as either nouns or proper names (Hassel et
al., 2011). As sentences from La?kartidningen not
ending with a full stop or a question mark are less
likely to be full sentences, they were not included,
in order to obtain a more contrasting corpus.
A 95% confidence interval for the proportion of
each sentence combination was computed using
the Wilson score interval, and the difference be-
tween the minimum frequency in the clinical cor-
pus and the maximum frequency in the standard
language corpus was calculated. Thereby, statis-
tics for the minimum difference between the two
domains was achieved.
1This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpro?vningsna?mnden i Stock-
holm), permission number 2012/834-31/5.
98
A total of 458 436 sentence types were found
in the clinical corpus. Of these, there were 1 736
types significantly more frequent in the clinical
corpus than in the standard corpus, not having
overlapping confidence interval for the propor-
tions. 33 sentence types, to which 10% of the sen-
tences in the corpus belonged, had more than 0.1
percentage points difference between minimum
frequency in the clinical corpus and maximum fre-
quency in the standard language corpus. For each
of these 33 sentence types, 30 sentences were ran-
domly extracted and the dependency parser Malt-
Parser (Nivre et al, 2009), pre-trained on Tal-
banken (Nivre et al, 2006) using the algorithm
stacklazy (Nivre et al, 2009), was applied to these
part-of-speech tagged sentences. Error categories
were manually identified, using MaltEval (Nilsson
and Nivre, 2008) for visualisation.
Given the identified error categories, two pre-
processing rules were constructed. These were
then evaluated by applying the same pre-trained
parser model on pre-processed sentences as on
original sentences. A manual analysis was per-
formed on a subset of the sentences that were dif-
ferently parsed after pre-processing.
3 Results
Although only one sentence type was a full sen-
tence (nn vb pp nn mad), most sentences were
correctly parsed. Omitted words could be inferred
from context, and therefore also the intended syn-
tax. Eight error types, to which most errors be-
longed, were identified: 1) Abbreviated words
ending with a full stop interpreted as the last word
in a sentence, resulting in an incorrect sentence
splitting. 2) Abbreviations incorrectly labelled as
nouns by Granska, resulting in sentences exclu-
sively containing nouns. 3) Adjectives not recog-
nised as such (often because they were abbrevi-
ated), resulting in AT relations being labelled as
DT relations. 4) A general adverbial relation in-
correctly assigned an adverb of place or time rela-
tion or vice versa. 5) The first word in compound
expressions parsed as a determiner to the second.
6) nn pp nn pp nn mad sentences for which a
preposition had been incorrectly attributed. 7) The
sentence type nn jj (noun adjective), for which
most evaluated sentences were incorrectly parsed.
8) An omitted initial subject, resulting in the ob-
ject incorrectly being parsed as the subject of the
sentence.
Pre-processing rules were constructed for error
types 7) and 8). As a verb in the middle of nn
jj-sentences (in most cases copula) was left out,
the first pre-processing rule added copula in the
middle of these sentences. The second rule added
the pronoun I as the first word in sentences starting
with a verb, as this was the most frequently left
out subject, along with the slightly less frequent
omission, patient. The rules were not applied on
sentences ending with a question mark.
10 (out of 33) sentence types were affected by
the two rules. The proportion of those receiving a
different parsing after pre-processing is shown in
the column Changed in Table 1. A subset of these
sentences, for which the parsing was changed, was
manually classified as either incorrect (= contain-
ing at least one parsing or labelling error) or com-
pletely correct.
For sentences classified as incorrect, a more
granular comparison between the original and the
modified parsing was carried out. For these sen-
tences, the difference in average unlabelled (UAS)
and labelled (LAS) attachment score between the
pre-processed and the original parsing was com-
puted. A positive value indicates that although the
pre-processing resulted in some incorrectly parsed
sentences, these sentences were improved by pre-
processing. The sentence types vb pp nn nn mad
and vb pp nn pp nn mad were thus slightly im-
proved by the pre-processing, although they had a
low proportion of correctly parsed sentences.
A negative value for attachment score differ-
ence, on the other hand, indicates that parsing for
the incorrectly parsed sentences was impaired by
pre-processing. As these figures only apply to sen-
tences incorrectly parsed after pre-processing, this
means that although e.g. the type vb ab nn mad
has negative UAS and LAS difference, this only
applies to the 3 sentences that were incorrectly
parsed by the pre-processed version.
With one important exception, sentences modi-
fied by pre-processing, were either a) given a com-
pletely correct parsing and labelling in between
64% and 100% of the cases, or were b) slightly
improved by pre-processing. A reasonable sim-
plification in this case is that there can only be
one correct parsing of a sentence, as although
there might be occurrences of syntactically am-
biguous sentences, it is unlikely that their inter-
pretation is not given by the context in the closed
domain of language used for diagnostic reasoning.
99
Given this simplification, this means that a sen-
tence was transformed from an incorrectly parsed
sentence to a correctly parsed sentence in 64% or
more of the cases, when pre-processing was ap-
plied. The difference in attachment score shows
that the parsing is not drastically degraded for the
rest of the sentences, although it mostly changed
to a worse parsing. The overall effect of apply-
ing pre-processing is therefore positive. Sentences
of the type vb nn pp nn mad is the important ex-
ception to this positive effect, important as 54% of
the sentences belonging to this type received a dif-
ferent parsing after pre-processing and as 0.39%
of the sentences in the corpus belong to this type.
Only 61% of the pre-processed sentences of this
type had a correct unlabelled parsing and only
32% had a correct labelled parsing. Many of these
sentences were similar to Writes a prescription of
Trombyl, for which of Trombyl incorrectly is given
the word write as the head after pre-processing.
Almost all of the sentences of the type nn jj mad
were correctly parsed when a copula was inserted
between the noun and the adjective. Of the other
types of sentences that improved, many improved
by an incorrectly labelled subject relation being
changed to an object relation. There were, how-
ever, also improvements because some adverbs of
place and time were correctly labelled after the
pre-processing rules had been applied.
4 Discussion
Even if quantitative data is given in Table 1, the
core of this study has been to use a qualitative ap-
proach: searching for different categories of errors
rather than determining accuracy figures, and in-
vestigating whether pre-processing has a positive
effect, rather than determining the final accuracy.
The next step is to apply the findings of this
study for developing a small treebank of clinical
text. A possible method for facilitating syntactic
annotation is to present pre-annotated data to the
annotator (Brants and Plaehn, 2000) for correction
or for selection among several alternatives. As the
overall effect of applying pre-processing were im-
proved parsings, the pre-annotation could be car-
ried out by applying a model trained on standard
language and improve it with the pre-processing
rules investigated here. The other identified error
types also give suggestions of how to improve the
parser, improvements that should be attempted be-
fore using a parser trained on standard language
for pre-annotation. Error types 1), 2) and partly
3) were due to abbreviations negatively affect-
ing part-of-speech tagging and sentence splitting.
Therefore, abbreviation expansion would be a pos-
sible way of improving the parser. That available
medical vocabularies also could be useful is shown
by error type 5), which was due to the parser fail-
ing to recognise compound expressions.
Of the sentences in the corpus, only 10%
belonged to the analysed sentence types, and
even fewer were affected by the evaluated pre-
processing rules. It is, however, likely that the two
developed pre-processing rules have effects on all
sentence types lacking a verb or starting with a
verb, thus effecting more sentence type than those
included in this study. This is worth studying,
as is also syntactic differences for shorter part-of-
speech sequences than sentence level sequences.
Another possible method for domain adaptation
would be to adapt the training data to construct a
model more suitable for parsing clinical text. In-
stead of applying pre-processing, sentences in the
training data could be modified to more closely re-
semble sentences in clinical text, e.g. by removing
words in the treebank corpus to achieve the incom-
plete sentences typical to clinical text. Differences
in vocabulary has not been included in this study,
but methods from previous studies for bridging
differences in vocabulary between the general and
medical domain could also be applied for improv-
ing parser performance.
For supplementing a treebank to also include
sentences typical to clinical text, some of the
methods investigated here for extracting such sen-
tence types, could be employed
5 Conclusion
Sentence types typical to clinical text were ex-
tracted, and eight categories of error types were
identified. For two of these error types, pre-
processing rules were devised and evaluated.
For four additional error types, techniques for
text-normalisation were suggested. As the pre-
processing rules had an overall positive effect on
the parser performance, it was suggested that a
model for syntactic pre-annotation of clinical text
should employ the evaluated text pre-processing.
Acknowledgements
Many thanks to Joakim Nivre and to the four re-
viewers for their many valuable comments.
100
Sentence # % # % Correct # Incorrect pp UAS(LAS)
type In test Changed Manually unlabelled unlabelled difference am-
classified (labelled) (labelled) ong incorrect
a) vb nn mad 1181 30% 40 100 (100)% 0 (0)
vb jj nn mad 317 13% 32 100 (94) % 0 (2)
nn jj mad 316 100% 200 94 (94) % 12 (12)
vb ab nn mad 256 33% 31 90 (90) % 3 (3) -25 (-25) pp
vb pp nn mad 674 5% 27 100 (85) % 0 (4) (-19) pp
vb ab pp nn mad 222 21% 30 100 (70) % 0 (9) (+7) pp
vb pp jj nn mad 207 7% 14 100 (64) % 0 (5) (-16) pp
b) vb pp nn nn mad 197 5% 9 22 (11) % 7 (8) 0 (+10) pp
vb pp nn pp nn mad 232 5% 12 75 (4) % 3 (12) 0 (+2) pp
c) vb nn pp nn mad 813 54% 28 61 (32) % 11 (19) -20 (-15) pp
Table 1: In test: Number of sentences in test set of this type. Changed: Proportion of sentences that
received a different parsing after pre-processing had been applied. Manually classified: Number of man-
ually classified sentences. Correct: Proportion of sentences that were correctly parsed (and labelled)
after pre-processing had been applied. # Incorrect: Number of incorrectly parsed (and labelled) sen-
tences after pre-processing. UAS (LAS) difference: For these incorrect sentences: The difference in UAS,
unlabelled attachment score, (and LAS, labelled attachment score) before and after pre-processing. (For
sentence types with more than 90% correct sentences, this difference was not calculated.)
References
Daniel Albright, Arrick Lanfranchi, Anwen Fredrik-
sen, William F Styler, 4th, Colin Warner, Jena D
Hwang, Jinho D Choi, Dmitriy Dligach, Rod-
ney D Nielsen, James Martin, Wayne Ward, Martha
Palmer, and Guergana K Savova. 2013. Towards
comprehensive syntactic and semantic annotations
of the clinical narrative. J Am Med Inform Assoc,
Jan.
Thorsten Brants and Oliver Plaehn. 2000. Interactive
corpus annotation. In LREC. European Language
Resources Association.
Marie Candito, Enrique H. Anguiano, and Djame? Sed-
dah. 2011. A Word Clustering Approach to Domain
Adaptation: Effective Parsing of Biomedical Texts.
In Proceedings of the 12th International Conference
on Parsing Technologies, pages 37?42, Dublin, Ire-
land, October. Association for Computational Lin-
guistics.
Johan Carlberger and Viggo Kann. 1999. Implement-
ing an efficient part-of-speech tagger. Software?
Practice and Experience, 29:815?832.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus -
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR 2009, Evaluation and imple-
mentation of e-health and health information initia-
tives: international perspectives. 14th International
Symposium for Health Information Management Re-
search, Kalmar, Sweden, pages 243?249.
Martin Hassel, Aron Henriksson, and Sumithra
Velupillai. 2011. Something Old, Something New
- Applying a Pre-trained Parsing Model to Clini-
cal Swedish. In Proceedings of NODALIDA?11 -
18th Nordic Conference on Computational Linguis-
tics, Riga, Latvia, May 11-13.
Katri Haverinen, Filip Ginter, Veronika Laippala, and
Tapio Salakoski. 2009. Parsing Clinical Finnish:
Experiments with Rule-Based and Statistical De-
pendency Parsers. In Kristiina Jokinen and Eck-
hard Bick, editors, Proceedings of NODALIDA?09,
Odense, Denmark, pages 65?72.
Dimitrios Kokkinakis. 2012. The journal of the
Swedish medical association - a corpus resource for
biomedical text mining in Swedish. In The Third
Workshop on Building and Evaluating Resources for
Biomedical Text Mining (BioTxtM), an LREC Work-
shop. Turkey.
Jens Nilsson and Joakim Nivre. 2008. Malteval:
An evaluation and visualization tool for dependency
parsing. In Proceedings of the Sixth International
Language Resources and Evaluation. LREC, pages
161?166.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
the fifth international conference on Language Re-
sources and Evaluation (LREC 2006), pages 24?26.
Joakim Nivre, Marco Kuhlmann, and Johan Hall.
2009. An improved oracle for dependency parsing
with online reordering. In Proceedings of the 11th
International Conference on Parsing Technologies,
IWPT ?09, pages 73?76, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
101
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 57?65,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Medical text simplification using synonym replacement:
Adapting assessment of word difficulty to a compounding language
Emil Abrahamsson
1
Timothy Forni
1
Maria Skeppstedt
1
Maria Kvist
1,2
1
Department of Computer and Systems Sciences (DSV)
Stockholm University, Sweden
{emab6827, tifo6794, mariask}@dsv.su.se
2
Department of Learning, Informatics, Management and Ethics (LIME)
Karolinska Institutet, Sweden
maria.kvist@karolinska.se
Abstract
Medical texts can be difficult to under-
stand for laymen, due to a frequent occur-
rence of specialised medical terms. Re-
placing these difficult terms with eas-
ier synonyms can, however, lead to im-
proved readability. In this study, we have
adapted a method for assessing difficulty
of words to make it more suitable to med-
ical Swedish. The difficulty of a word
was assessed not only by measuring the
frequency of the word in a general cor-
pus, but also by measuring the frequency
of substrings of words, thereby adapt-
ing the method to the compounding na-
ture of Swedish. All words having a
MeSH synonym that was assessed as eas-
ier, were replaced in a corpus of medical
text. According to the readability measure
LIX, the replacement resulted in a slightly
more difficult text, while the readability
increased according to the OVIX measure
and to a preliminary reader study.
1 Introduction
Our health, and the health of our family and
friends, is something that concerns us all. To be
able to understand texts from the medical domain,
e.g. our own health record or texts discussing sci-
entific findings related to our own medical prob-
lems, is therefore highly relevant for all of us.
Specialised terms, often derived from latin or
greek, as well as specialised abbreviations, are,
however, often used in medical texts (Kokkinakis
and Toporowska Gronostaj, 2006). This has the
effect that medical texts can be difficult to compre-
hend (Keselman and Smith, 2012). Comprehend-
ing medical text might be particularly challenging
for those laymen readers who are not used to look-
ing up unknown terms while reading. A survey of
Swedish Internet users showed, for instance, that
users with a long education consult medical infor-
mation available on the Internet to a much larger
extent than users with a shorter education (Find-
ahl, 2010, pp. 28?35). This discrepancy between
different user groups is one indication that meth-
ods for simplifying medical texts are needed, to
make the medical information accessible to every-
one.
Previous studies have shown that replacing dif-
ficult words with easier synonyms can reduce the
level of difficulty in a text. The level of diffi-
culty of a word was, in these studies, determined
by measuring its frequency in a general corpus of
the language; a measure based on the idea that
frequent words are easier than less frequent, as
they are more familiar to the reader. This syn-
onym replacement method has been evaluated on
medical English text (Leroy et al., 2012) as well
as on Swedish non-medical text (Keskis?arkk?a and
J?onsson, 2012). To the best of our knowledge, this
method has, however, not previously been evalu-
ated on medical text written in Swedish. In ad-
dition, as Swedish is a compounding language,
laymen versions of specialised medical terms are
often constructed by compounds of every-day
Swedish words. Whether a word consists of easily
understandable constituents, is a factor that also
ought to be taken into account when assessing the
difficulty of a word.
The aim of our study was, therefore, to in-
vestigate if synonym replacement based on term
frequency could be successfully applied also on
Swedish medical text, as well as if this method
could be further developed by adapting it to the
compounding nature of Swedish.
2 Background
The level of difficulty varies between different
types of medical texts (Leroy et al., 2006), but
studies have shown that even brochures intended
57
for patients, or websites about health issues, can be
difficult to comprehend (Kokkinakis et al., 2012;
Leroy et al., 2012). Bio-medical texts, such as
medical journals, are characterised by sentences
that have high informational and structural com-
plexity, thus containing a lot of technical terms
(Friedman et al., 2002). An abundance of med-
ical terminology and a frequent use of abbrevia-
tions form, as previously mentioned, a strong bar-
rier for comprehension when laymen read medical
text. Health literacy is a much larger issue than
only the frequent occurrence of specialised terms;
an issue that includes many socio-economic fac-
tors. The core of the issue is, however, the read-
ability of the text, and adapting word choice to the
reader group (Zeng et al., 2005; Leroy et al., 2012)
is a possible method to at least partly improve the
readability of medical texts.
Semi-automatic adaption of word choice has
been evaluated on English medical text (Leroy et
al., 2012) and automatic adaption on Swedish non-
medical text (Keskis?arkk?a and J?onsson, 2012).
Both studies used synonym lexicons and replaced
words that were difficult to understand with more
easily understandable synonyms. The level of dif-
ficulty of a word was determined by measuring its
frequency in a general corpus. The English study
based its figures for word frequency on the number
of occurrences of a word in Google?s index of En-
glish language websites, while the Swedish study
used the frequency of a word in the Swedish Pa-
role corpus (Gellerstam et al., 2000), which is a
corpus compiled from several sources, e.g. news-
paper texts and fiction.
The English study used English WordNet as the
synonym resource, and difficult text was trans-
formed by a medical librarian, who chose eas-
ier replacements for difficult words among candi-
dates that were presented by the text simplifica-
tion system. Also hypernyms from semantic cat-
egories in WordNet, UMLS and Wiktionary were
used, but as clarifications for difficult words (e.g.
in the form: ?difficult word, a kind of semantic cat-
egory?). A frequency cut-off in the Google Web
Corpus was used for distinguishing between easy
and difficult words. The study was evaluated by
letting readers 1) assess perceived difficulty in 12
sentences extracted from medical texts aimed at
patients, and 2) answer multiple choice questions
related to paragraphs of texts from the same re-
source, in order to measure actual difficulty. The
evaluations showed that perceived difficulty was
significantly higher before the transformation, and
that actual difficulty was significantly higher for
one combination of medical topic and test setting.
The Swedish study used the freely available
SynLex as the resource for synonyms, and one
of the studied methods was synonym replacement
based on word frequency. The synonym replace-
ment was totally automatic and no cut-off was
used for distinguishing between familiar and rare
words. The replacement algorithm instead re-
placed all words which had a synonym with a
higher frequency in the Parole corpus than the fre-
quency of the original word. The effect of the
frequency-based synonym replacement was auto-
matically evaluated by applying the two Swedish
readability measures LIX and OVIX on the orig-
inal and on the modified text. Synonym replace-
ment improved readability according to these two
measures for all of the four studied Swedish text
genres: newspaper texts, informative texts from
the Swedish Social Insurance Agency, articles
from a popular science magazine and academic
texts.
For synonym replacement to be a meaningful
method for text simplification, there must exist
synonyms that are near enough not to change the
content of what is written. Perfect synonyms are
rare, as there is typically at least one aspect in
which two separate words within a language dif-
fer; if it is not a small difference in meaning, it
might be in the context in which they are typi-
cally used (Saeed, 1997). For describing med-
ical concepts, there is, however, often one set
of terms that are used by health professionals,
whereas another set of laymen?s terms are used by
patients (Leroy and Chen, 2001; Kokkinakis and
Toporowska Gronostaj, 2006). This means that
synonym replacement could have a large poten-
tial for simplifying medical text, as there are many
synonyms within this domain, for which the dif-
ference mainly lies in the context in which they
are typically used.
The availability of comprehensive synonym re-
sources is another condition for making it possi-
ble to implement synonym replacement for text
simplification. For English, there is a consumer
health vocabulary initiative connecting laymen?s
expressions to technical terminology (Keselman
et al., 2008), as well as several medical termi-
58
Original Med r?ontgen kan man se en ?okad trabekulering, osteoporos
samt pseudofrakturer.
Transformed Med r?ontgen kan man se en ?okad trabekulering, bensk?orhet
samt pseudofrakturer.
Translated original With X-ray, one can see an increased trabeculation, osteoporosis
and pseudo-fractures.
Translated transformed With X-ray, one can see an increased trabeculation, bone-brittleness
and pseudo-fractures.
Table 1: An example of how the synonym replacement changes a word in a sentence.
nologies containing synonymic expressions, e.g.
MeSH
1
and SNOMED CT
2
. Swedish, with fewer
speakers, also has fewer lexical resources than En-
glish, and although SNOMED CT was recently
translated to Swedish, the Swedish version does
not contain any synonyms. MeSH on the other
hand, which is a controlled vocabulary for index-
ing biomedical literature, is available in Swedish
(among several other languages), and contains
synonyms and abbreviations for medical concepts
(Karolinska Institutet, 2012).
Swedish is, as previously mentioned, a com-
pounding language, with the potential to create
words expressing most of all imaginable concepts.
Laymen?s terms for medical concepts are typi-
cally descriptive and often consist of compounds
of words used in every-day language. The word
humerusfraktur (humerus fracture), for instance,
can also be expressed as ?overarmsbenbrott, for
which a literal translation would be upper-arm-
bone-break. That a compound word with many
constituents occurring in standard language could
be easier to understand than the technical terms
of medical terminology, forms the basis for our
adaption of word difficulty assessment to medical
Swedish.
3 Method
We studied simplification of one medical text
genre; medical journal text. The replacement
method, as well as the main evaluation method,
was based on the previous study by Keskis?arkk?a
and J?onsson (2012). The method for assessing
word difficulty was, however, further developed
compared to this previous study.
As medical journal text, a subset of the journal
L?akartidningen, the Journal of the Swedish Med-
ical Association (Kokkinakis, 2012), was used.
1
www.nlm.nih.gov/mesh/
2
www.ihtsdo.org
The subset consisted of 10 000 randomly selected
sentences from issues published in 1996. As syn-
onym lexicon, the Swedish version of MeSH was
used. This resource contains 10 771 synonyms,
near synonyms, multi-word phrases with a very
similar meaning and abbreviation/expansion pairs
(all denoted as synonyms here), belonging to 8 176
concepts.
Similar to the study by Keskis?arkk?a and J?onsson
(2012), the Parole corpus was used for frequency
statistics. For each word in the L?akartidningen
subset, it was checked whether the word had a syn-
onym in MeSH. If that was the case, and if the
synonym was more frequently occurring in Parole
than the original word, then the original word was
replaced with the synonym. An example of a sen-
tence changed by synonym replacement is shown
in Table 1.
There are many medical words that only rarely
occur in general Swedish, and therefore are not
present as independent words in a corpus of stan-
dard Swedish, even if constituents of the words
frequently occur in the corpus. The method used
by Keskis?arkk?a and J?onsson was further developed
to handle these cases. This development was built
on the previously mentioned idea that a compound
word with many constituents occurring in standard
language is easier to understand than a rare word
for which this is not the case. When neither the
original word, nor the synonym, occurred in Pa-
role, a search in Parole was therefore instead car-
ried out for substrings of the words. The original
word was replaced by the synonym, in cases when
the synonym consisted of a larger number of sub-
strings present in Parole than the original word.
To insure that the substrings were relevant words,
they had to consist of a least four characters.
Exemplified by a sentence containing the word
hemangiom (hemangioma), the extended replace-
ment algorithm would work as follows: The al-
59
gorithm first detects that hemangiom has the syn-
onym blodk?arlstum?or (blood-vessel-tumour) in
MeSH. It thereafter establishes that neither he-
mangiom nor blodk?arlstum?or is included in the
Parole corpus, and therefore instead tries to find
substrings of the two words in Parole. For he-
mangiom, no substrings are found, while four
substrings are found for blodk?arlstum?or (Table
2), and therefore hemangiom is replaced by
blodk?arlstum?or.
Word 1 2 3 4
hemangiom - - - -
blodk?arlstum?or blod k?arl blodk?arl tum?or
Table 2: Example of found substrings
As the main evaluation of the effect of the syn-
onym replacement, the two readability measures
used by Keskis?arkk?a and J?onsson were applied,
on the original as well as on the modified text.
LIX (l?asbarhetsindex, readability measure) is the
standard metric used for measuring readability of
Swedish texts, while OVIX (ordvariationsindex,
word variation index) measures lexical variance,
thereby reflecting the size of vocabulary in the text
(Falkenjack et al., 2013).
The two metrics are defined as follows
(M?uhlenbock and Johansson Kokkinakis, 2009):
LIX =
O
M
+
L ? 100
O
Where:
? O = number of words in the text
? M = number of sentences in the text
? L = number of long words in the text (more
than 6 characters)
OVIX =
log(O)
log(2?
log(U)
log(O)
)
Where:
? O = number of words in the text
? U = number of unique words in the text
The interpretation of the LIX value is shown in
Table 3, while OVIX scores ranging from 60 to 69
indicate easy-to-read texts (M?uhlenbock and Jo-
hansson Kokkinakis, 2009).
LIX-value Genre
less than 25 Children?s books
25-30 Easy texts
30-40 Normal text/fiction
40-50 Informative texts
50-60 Specialist literature
more than 60 Research, dissertations
Table 3: The LIX-scale, from M?uhlenbock and Jo-
hansson Kokkinakis (2009)
To obtain preliminary results from non-
automatic methods, a very small manual evalua-
tion of correctness and perceived readability was
also carried out. A randomly selected subset of
the sentences in which at least one term had been
replaced were classified into three classes by a
physician: 1) The original meaning was retained
after the synonym replacement, 2) The original
meaning was only slightly altered after the syn-
onym replacement, and 3) The original meaning
was altered more than slightly after the synonym
replacement. Sentences classified into the first cat-
egory by the physician were further categorised
for perceived readability by two other evaluators;
both with university degrees in non-life science
disciplines. The original and the transformed sen-
tence were presented in random order, and the
evaluators were only informed that the simplifica-
tion was built on word replacement. The follow-
ing categories were used for the evaluation of per-
ceived readability: 1) The two presented sentences
are equally easy/difficult to understand, 2) One of
the sentences is easier to understand than the other.
In the second case, the evaluator indicated which
sentence was easier.
4 Results
In the used corpus subset, which contained
150 384 tokens (26 251 unique), 4 909 MeSH
terms for which there exist a MeSH synonym were
found. Among these found terms, 1 154 were
replaced with their synonym. The 15 most fre-
quently replaced terms are shown in Table 4, many
of them being words typical for a professional lan-
guage that have been replaced with compounds
of every-day Swedish words, or abbreviations that
have been replaced by an expanded form.
The total number of words increased from
150 384 to 150 717 after the synonym replace-
60
Original term (English) Replaced with (Literal translation) n
aorta (aorta) kroppspuls?ader (body-artery) 34
kolestas (cholestasis) gallstas (biliary-stasis) 33
angio?odem (angioedema) angioneurotiskt ?odem (angio-neurotic-oedema) 29
stroke (stroke) slaganfall (strike-seizure) 29
TPN (TPN) parenteral n?aring, total (parenteral nutrition, total) 26
GCS (GCS) Glasgow Coma Scale (Glasgow Coma Scale) 20
mortalitet (mortality) d?odlighet (deathrate) 20
?odem (oedema) svullnad (swelling) 20
legitimation (licensure) licens (certificate) 18
RLS (RLS) rastl?osa ben-syndrom (restless legs-syndrome) 18
anemi (anemia) blodbrist (blood-shortage) 17
anh?origa (family) familj (family) 17
ekokardiografi (echocardiography) hj?artultraljuds- (heart-ultrasound 17
unders?okning -examination)
artrit (arthritis) ledinflammation (joint-inflammation) 16
MHC (MHC) histokompatibilitets- (histocompatibility- 15
komplex complex)
Table 4: The 15 most frequently replaced terms. As the most frequent synonym (or synonym with most
known substrings) is always chosen for replacement, the same choice among a number of synonyms, or
a number of abbreviation expansions, will always be made. The column n contains the number of times
the original term was replaced with this synonym.
ment. Also the number of long words (more than
six characters) increased from 51 530 to 51 851.
This resulted in an increased LIX value, as can be
seen in Table 5. Both before and after the transfor-
mation, the LIX-value lies on the border between
the difficulty levels of informative texts and non-
fictional texts. The replacement also had the effect
that the number of unique words decreased with
138 words, which resulted in a lower OVIX, also
to be seen in Table 5.
For the manual evaluation, 195 sentences, in
which at least one term had been replaced, were
randomly selected. For 17% of these sentences,
the original meaning was slightly altered, and for
10%, the original meaning was more than slightly
altered. The rest of the sentences, which re-
tained their original meaning, were used for mea-
suring perceived readability, resulting in the fig-
ures shown in Table 6. Many replaced terms oc-
curred more than once among the evaluated sen-
tences. Therefore, perceived difficulty was also
measured for a subset of the evaluation data, in
which it was ensured that each replaced term oc-
curred exactly once, by only including the sen-
tence in which it first appeared. These subset fig-
ures (denoted Unique in Table 6) did, however,
only differ marginally from the figures for the en-
tire set. Although there was a large difference be-
tween the two evaluators in how they assessed the
effect of the synonym replacement, they both clas-
sified a substantially larger proportion of the sen-
tences as easier to understand after the synonym
replacement.
LIX OVIX
Original text 50 87.2
After synonym replacement 51 86.9
Table 5: LIX and OVIX before and after synonym
replacement
5 Discussion
According to the LIX measure, the medical text
became slightly more difficult to read after the
transformation, which is the opposite result to
that achieved in the study by Keskis?arkk?a and
J?onsson (2012). Similar to this previous study,
however, the text became slightly easier to read
according to the OVIX measure, as the number
of unique words decreased. As words longer
than six characters result in a higher LIX value, a
very plausible explanation for the increased LIX-
value, is that short words derived from Greek or
Latin have been replaced with longer compounds
61
Perceived effect Evaluator 1 Evaluator 2
of replacement All (Unique) All (Unique)
No difference 51% (52%) 29% (28%)
Easier 42% (42%) 54% (52%)
More difficult 7% (7%) 17% (21%)
Table 6: Results for the manual classification
of perceived difficulty. Evaluator 1 classified
143 sentences and Evaluator 2 classified 140 sen-
tences. The (Unique) column contains results
when only the first a occurrence of a replacement
of a particular term is included. A binomial sign
test (Newbold et al., 2003, p. 532) was performed
on the effect of the replacement, with the null hy-
pothesis that the probability of creating a more dif-
ficult sentence was equal to that of creating an eas-
ier one. This hypothesis could be rejected for both
evaluators; when including all sentences and also
when only including the (Unique) subset, show-
ing that the differences were statistically signifi-
cant (p0.01).
of every-day words. Replacing an abbreviation or
an acronym with its expanded long form has the
same effect. Expanding acronyms also increases
the number of words per sentence, which also re-
sults in a higher LIX value.
Studies on English medical text indicate, how-
ever, that simple surface measures do not accu-
rately reflect the readability (Zeng-Treitler et al.,
2007; Wu et al., 2013), and user studies have been
performed to construct readability measures bet-
ter adapted to the domain of medical texts (Kim et
al., 2007; Leroy and Endicott, 2012). Therefore,
although the manual evaluation was very limited
in scope, the results from this evaluation might
give a better indication of the effects of the sys-
tem. This evaluation showed that the perceived
readability often improved with synonym replace-
ment, although there were also replacements that
resulted in a decrease of perceived readability.
Further studies are required to determine whether
these results are generalisable to a larger group of
readers. Such studies should also include an eval-
uation of actual readability, using methods similar
to those of Leroy et al. (2012). The cases, in which
the synonym replacement resulted in a perceived
decrease in readability should also be further stud-
ied. It might, for instance, be better to use a fre-
quency cut-off for distinguishing between rare and
frequent words, as applied by Leroy et al. (2012),
rather than always replacing a word with a more
frequent synonym.
The manual evaluation also showed that the
original semantic meaning had been at least
slightly altered in almost a third of the sentences,
which shows that the set of synonyms in Swedish
MeSH might need to be adapted to make the syn-
onyms suitable to use in a text simplification sys-
tem. The replacements in Table 4 show three
types of potential problems. First, there are also
distant synonyms, as exemplified by oedema and
swelling, where oedema means a specific type of
swelling in the form of increased amount of liq-
uid in the tissues, as opposed to e.g. increased
amount of fat. Second, the MeSH terms are not
always written in a form that is appropriate to use
in running text, such as the term parenteral nu-
trition, total. Such terms need to be transformed
to another format before they can be used for au-
tomatic synonym replacement. Third, although
the abbreviations included in the manual evalua-
tion were all expanded to the correct form, ab-
breviations within the medical domain are often
overloaded with a number of different meanings
(Liu et al., 2002). For instance, apart from be-
ing an acronym for restless legs syndrome, RLS
can also mean reaction level scale (Cederblom,
2005). Therefore, in order to include abbrevia-
tions and acronyms in the synonym replacement
method studied here, an abbreviation disambigua-
tion needs to be carried out first (Gaudan et al.,
2005; Savova et al., 2008). An alternative could
be to automatically detect which abbreviations and
acronyms that are defined in the text when they
first are mentioned (Dann?ells, 2006), and restrict
the replacement method to those.
The sentence in Table 1 shows an example of
a successful synonym replacement, replacing a
word typically used by health professionals (os-
teoporosis) with a word typically used in every-
day language (bone-brittleness). This sentence
also gives an example of when not enough is
replaced in the sentence for it to be easy to
understand. Neither trabeculation, nor pseudo-
fractures, are included in MeSH, which shows the
importance of having access to comprehensive ter-
minological resources for the method of synonym
replacement to be successful. Extracting terms
that are frequently occurring within the text genre
that is to be simplified, but which are neither in-
cluded in the used terminology, nor in a corpus
62
of standard language such as Parole, could be a
method for finding candidates for expanding the
terminological resources. Semi-automatic meth-
ods could be applied for finding synonyms to these
new candidate terms, as well as to existing terms
within the terminology for which no synonyms are
provided (Henriksson et al., 2013).
Table 1 also exemplifies a further issue not ad-
dressed here, namely the frequent occurrence of
inflected words in Swedish text. No morphologic
normalisation, e.g. lemmatisation, was performed
of the text that was to be simplified or of the
terms in MeSH (e.g. normalising pseudo-fractures
to pseudo-fracture). Such a normalisation would
have the potential of matching, and thereby replac-
ing, a larger number of words, but it would also re-
quire that the replaced word is inflected to match
the grammatical form of the original word.
An alternative to using frequency in the Parole
corpus, or occurrence of substrings in a word in
Parole, for determining when a synonym is to be
replaced, is to use the frequency in a medical cor-
pus. That corpus then has to be targeted towards
laymen, as word frequency in texts targeted to-
wards health professionals would favour word re-
placements with words typical to the professional
language. Examples of such patient corpora could
be health related web portals for patients (Kokki-
nakis, 2011). However, as also texts targeted to-
wards patients have been shown to be difficult to
understand, the method of searching for familiar
words in substrings of medical terms might be
relevant for assessing word difficulty also if easy
medical corpora would be used.
6 Future work
A number of points for future work have al-
ready been mentioned, among which evaluating
the method on a large set of target readers has
the highest priority. Adapting the method to han-
dle inflected words, studying how near synonyms
and ambiguity of abbreviations affect the content
of the transformed sentences, as well as studying
methods for semi-automatic expansion of termi-
nologies, are other topics that have already been
mentioned.
It might also be the case that what synonym re-
placements are suitable are dependent on the con-
text in which a word occurs. Methods for adapting
assessment of word difficulty to context have been
studied within the Semeval-2012 shared task on
English lexical simplification (Specia et al., 2012),
although it was shown that infrequent words are
generally perceived as more difficult, regardless of
context.
In addition to these points, it should be noted
that we in this study have focused on one type
medical text, i.e. medical journal text. As men-
tioned in the introduction, there is, however,
another medical text type on which applying
text simplification would also be highly relevant,
namely health record text (Kvist and Velupillai,
2013; Kandula et al., 2010). The electronic health
record is nowadays made available to patients via
e-services in a number of countries, and there is
also an on-going project constructing such a ser-
vice in Sweden. Apart from health record text
also containing many words derived from greek
and latin, there are additional challenges associ-
ated with this type of text. As health record text is
written under time pressure, it is often written in
a telegraphic style with incomplete sentences and
many abbreviations (Friedman et al., 2002; Aan-
taa, 2012). As was exemplified among the top 15
most frequently replaced words, abbreviations is
one of the large problems when using the synonym
replacement method for text simplification, as they
are often overloaded with a number of meanings.
Future work, therefore, also includes the eval-
uation of synonym replacement on health record
text. It also includes the study of writing tools for
encouraging health professionals to produce text
that is easier to understand for the patient, or at
least easier to transform into more patient-friendly
texts with methods similar to the method studied
here (Ahltorp et al., 2013).
7 Conclusion
A method used in previous studies for assess-
ing difficulty of words in Swedish text was fur-
ther developed. The difficulty of a word was as-
sessed not only by measuring the frequency of
the word in a general corpus, but also by measur-
ing the frequency of substrings of words, thereby
adapting the method to the compounding nature of
Swedish. The replacement was mainly evaluated
by the two readability measures LIX and OVIX,
showing a slightly decreased OVIX but a slightly
increased LIX. A preliminary study on readers
showed, however, an increased perceived readabil-
ity after the synonym replacement. Studies on a
larger reader group are required to draw any con-
63
clusions on the general effect of the method for as-
sessment of word difficult. The preliminary results
are, however, encouraging, showing that a method
that replaces specialised words derived from latin
and greek by compounds of every-day Swedish
words can result in a increase of the perceived
readability.
Acknowledgements
We would like to thank the three reviewers for
many useful comments. This work was partly sup-
ported by a grant from the V?ardal Foundation
References
Kirsi Aantaa. 2012. Mot patientv?anligare epikriser,
en kontrastiv unders?okning [towards more patient
friendly discharge letters, a contrastive study]. Mas-
ter?s thesis, Department of Nordic Languages, Uni-
versity of Turku.
Magnus Ahltorp, Maria Skeppstedt, Hercules Dalianis,
and Maria Kvist. 2013. Using text prediction for fa-
cilitating input and improving readability of clinical
text. Stud Health Technol Inform, 192:1149.
Staffan Cederblom. 2005. Medicinska f?orkortningar
och akronymer (In Swedish). Studentlitteratur,
Lund.
Dana Dann?ells. 2006. Automatic acronym recogni-
tion. In Proceedings of the 11th conference on Eu-
ropean chapter of the Association for Computational
Linguistics (EACL).
Johan Falkenjack, Katarina Heimann M?uhlenbock, and
Arne J?onsson. 2013. Features indicating readability
in Swedish text. In Proceedings of the 19th Nordic
Conference of Computational Linguistics (NODAL-
IDA 2013), pages 27?40.
Olle Findahl. 2010. Svenskarna och Internet. .se.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of zellig harris. J Biomed In-
form, 35(4):222?35, Aug.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses
in medline. Bioinformatics, 21(18):3658?3664,
September.
M Gellerstam, Y Cederholm, and T Rasmark. 2000.
The bank of Swedish. In LREC 2000. The 2nd In-
ternational Conference on Language Resources and
Evaluation, pages 329?333, Athens, Greece.
Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying syn-
onymy between SNOMED clinical terms of vary-
ing length using distributional analysis of electronic
health records. In Proceedings of the Annual Sym-
posium of the American Medical Informatics Asso-
ciation (AMIA 2013), Washington DC, USA.
Sasikiran Kandula, Dorothy Curtis, and Qing Zeng-
Treitler. 2010. A semantic and syntactic text sim-
plification tool for health content. AMIA Annu Symp
Proc, 2010:366?70.
Karolinska Institutet. 2012. Hur man anv?ander
den svenska MeSHen (In Swedish, trans-
lated as: How to use the Swedish MeSH).
http://mesh.kib.ki.se/swemesh/manual se.html.
Accessed 2012-03-10.
Alla Keselman and Catherine Arnott Smith. 2012. A
classification of errors in lay comprehension of med-
ical documents. Journal of Biomedical Informatics,
45(6):1151?1163.
Alla Keselman, Robert Logan, Catherine Arnott Smith,
Gondy Leroy, and Qing Zeng-Treitler. 2008. Devel-
oping informatics tools and strategies for consumer-
centered health communication. In J Am Med In-
form Assoc, volume 15:4, pages 473?483.
Robin Keskis?arkk?a and Arne J?onsson. 2012. Auto-
matic text simplification via synonym replacement.
In Proceedings of Swedish Language Technology
Conference 2012.
Hyeoneui Kim, Sergey Goryachev, Graciela Rosem-
blat, Allen Browne, Alla Keselman, and Qing Zeng-
Treitler. 2007. Beyond surface characteristics: a
new health text-specific readability measurement.
AMIA Annu Symp Proc, pages 418?422.
Dimitrios Kokkinakis and Maria Toporowska Gronos-
taj. 2006. Lay language versus professional lan-
guage within the cardiovascular subdomain - a con-
trastive study. In Proceedings of the 2006 WSEAS
Int. Conf. on Cellular & Molecular Biology, Bio-
physics & Bioengineering.
Dimitrios Kokkinakis, Markus Forsberg, Sofie Johans-
son Kokkinakis, Frida Smith, and Joakim
?
Ohl?en.
2012. Literacy demands and information to cancer
patients. In Proceedings of the 15th International
Conference on Text, Speech and Dialogue, pages
64?71.
Dimitrios Kokkinakis. 2011. Evaluating the coverage
of three controlled health vocabularies with focus on
findings, signs and symptoms. In NEALT Proceed-
ings Series, editor, NODALIDA, volume 12, pages
27?31.
Dimitrios Kokkinakis. 2012. The journal of the
Swedish medical association - a corpus resource for
biomedical text mining in Swedish. In The Third
Workshop on Building and Evaluating Resources for
Biomedical Text Mining (BioTxtM), an LREC Work-
shop. Turkey.
64
Maria Kvist and Sumithra Velupillai. 2013. Profes-
sional language in swedish radiology reports ? char-
acterization for patient-adapted text simplification.
In Scandinavian Conference on Health Informatics,
Copenhagen, Denmark, August.
Gondy Leroy and Hsinchun Chen. 2001. Meeting
medical terminology needs-the ontology-enhanced
medical concept mapper. IEEE Transactions on
Information Technology in Biomedicine, 5(4):261?
270.
Gondy Leroy and James E. Endicott. 2012. Com-
bining nlp with evidence-based methods to find text
metrics related to perceived and actual text difficulty.
In IHI, pages 749?754.
Gondy Leroy, Evren Eryilmaz, and Benjamin T.
Laroya. 2006. Health information text character-
istics. In AMIA Annu Symp Proc, pages 479?483.
Gondy Leroy, James E Endicott, Obay Mouradi, David
Kauchak, and Melissa L Just. 2012. Improving
perceived and actual text difficulty for health infor-
mation consumers using semi-automated methods.
AMIA Annu Symp Proc, 2012:522?31.
Hongfang Liu, Alan R Aronson, and Carol Friedman.
2002. A study of abbreviations in medline abstracts.
Proc AMIA Symp, pages 464?8.
Katarina M?uhlenbock and Sofie Johansson Kokkinakis.
2009. Lix 68 revisited - an extended readabil-
ity measure. In Proceedings of Corpus Linguistics
2009.
Paul Newbold, William L. Carlson, and Betty Thorne.
2003. Statistics for business and economics.
Prentice-Hall, Upper Saddle River, N. J., 5. ed. edi-
tion.
John I. Saeed. 1997. Semantics. Blackwell Publishers,
Oxford.
Guergana K. Savova, Anni Coden, Igor L. Sominsky,
Rie Johnson, Philip V. Ogren, Piet C. de Groen, and
Christopher G. Chute. 2008. Word sense disam-
biguation across two domains: Biomedical literature
and clinical notes. Journal of Biomedical Informat-
ics, 41(6):1088?1100.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In *SEM, First Joint Conference on
Lexical and Computational Semantics, pages 347?
355, Montr?eal, Canada.
Danny T Y Wu, David A Hanauer, Qiaozhu Mei, Pa-
tricia M Clark, Lawrence C An, Jianbo Lei, Joshua
Proulx, Qing Zeng-Treitler, and Kai Zheng. 2013.
Applying multiple methods to assess the readability
of a large corpus of medical documents. Stud Health
Technol Inform, 192:647?51.
Qing T. Zeng, Tony Tse, Jon Crowell, Guy Divita,
Laura Roth, and Allen C. Browne. 2005. Identify-
ing consumer-friendly display (cfd) names for health
concepts. In Proceedings of AMIA Annual Sympo-
sium, pages 859?863.
Qing Zeng-Treitler, Hyeoneui Kim, Sergey Goryachev,
Alla Keselman, Laura Slaughter, and Catherine. A.
Smith. 2007. Text characteristics of clinical reports
and their implications for the readability of personal
health records. Medinfo, 12(Pt 2):1117?1121.
65
