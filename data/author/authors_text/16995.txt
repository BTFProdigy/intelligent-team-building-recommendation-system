Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49?52,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Lightweight Terminology Verification Service for External Machine
Translation Engines
Alessio Bosca
?
, Vassilina Nikoulina
?
, Marc Dymetman
?
?
CELI, Turin, Italy
?
Xerox Research Centre Europe, Grenoble, France
?
alessio.bosca@celi.it,
?
{first.last}@xrce.xerox.com
Abstract
We propose a demonstration of a domain-
specific terminology checking service
which works on top of any generic black-
box MT, and only requires access to a
bilingual terminology resource in the do-
main. In cases where an incorrect trans-
lation of a source term was proposed by
the generic MT service, our service locates
the wrong translation of the term in the tar-
get and suggests a terminologically correct
translation for this term.
1 Introduction
Today there exist generic MT services for a large
number of language pairs, which allow relatively
easily to make your domain-specific portal mul-
tilingual, and allow access to its documents for a
broad international public. However, applying a
generic MT service to domain-specific texts often
leads to wrong results, especially relative to the
translation of domain-specific terminology. Table
1 illustrates an example of a terminology inconsis-
tent translation provided by a generic MT system.
English Source: Farmers tend to implement a
broad non-focused weed-control strategy, on
the basis of broad spectrum products and mix-
tures of different products.
Bing
1
: Los agricultores tienden a aplicar una
estrategia amplia para control de malezas no
centrado, sobre la base de productos de am-
plio espectro y las mezclas de diferentes pro-
ductos.
Table 1: Example of the translation produced by
a generic MT model for a domain-specific docu-
ment. Source term : weed-control, official Span-
ish term translation: control de malas hierbas.
The importance of domain-specific terminology
for Machine Translation has been mentioned in
several previous works (eg. (Carl and Langlais,
2002; Skadins et al., 2013)). However, most of
these works handle the case where the terminology
is tightly integrated into the translation process.
This requires both a good expertise in SMT and
a large amount of both in-domain and generic par-
allel texts, which is often difficult, especially for
low-resourced languages like Turkish or Estonian.
Here, we are targeting the situation where the con-
tent provider is not willing to train a dedicated
translation system, for some reason such as lack of
technical skills or lack of necessary resources (par-
allel data or computational resources), but has at
his disposal a multilingual in-domain terminology
which could be helpful for improving the generic
translation provided by an external translation ser-
vice. We propose a demonstration of a multilin-
gual terminology verification/correction service,
which detects the wrongly translated terms and
suggests a better translation of these terms. This
service can be seen as an aid for machine transla-
tion post-editing focused on in-domain terminol-
ogy and as a tool for supporting the workflow of
practicing translators.
2 Related Work
There has recently been a growing interest for ter-
minology integration into MT models. Direct in-
tegration of terminology into the SMT model has
been considered, either by extending SMT train-
ing data (Carl and Langlais, 2002), or via adding
an additional term indicator feature (Pinnis and
Skadins, 2012; Skadins et al., 2013) into the trans-
lation model. However none of the above is possi-
ble when we deal with an external black-box MT
service.
(Itagaki and Aikawa, 2008) propose a post-
processing step for an MT engine, where a
wrongly translated term is replaced with a user-
provided term translation. The authors claim that
translating the term directly often gives a different
49
translation from the one obtained when translating
the term in context: for English-Japanese the out-
of-context term translation matches exactly the in
context term translation in 62% of cases only. In
order to address this problem the authors propose
15 simple context templates that induce the same
term translation as the one obtained in the initial
sentence context. Such templates include ?This
is TERM? or ?TERM is a ...?. The main prob-
lem with this approach is that these templates are
both language-pair and MT engine/model specific.
Thus a certain human expertise is required to de-
velop such templates when moving to a new lan-
guage pair or underlying MT engine.
Our approach is close to the (Itagaki and
Aikawa, 2008) approach, but instead of devel-
oping specific templates we propose a generic
method for wrong terminology translation detec-
tion. We do not aim at producing the final trans-
lation by directly replacing the wrongly translated
term ? which can be tricky?, but rather perform
the term correction in an interactive manner, where
the user is proposed a better term translation and
may choose to use it if the suggestion is correct.
3 Terminology-checking service
We assume that the provider of the terminology-
checking service has a bilingual domain-specific
terminology D at his disposal, which he wishes
to use to improve the translation produced by a
generic MT service MT . Our method verifies
whether the terminology was translated correctly
by the MT service (terminology verification), and
if not, locates the wrong translation of the term and
suggests a better translation for it.
3.1 Terminology checking
The basic terminology verification procedure ap-
plied to the source sentence s and to its translation
MT (s) by the generic service is done through the
following steps:
1. For each term T = (T
s
, T
t
) in D check
whether its source part T
s
is present in the
source sentence s.
2. If s contains T
s
, check whether the target
part of the term T
t
is present in the transla-
tion MT (s). If yes, and the number of oc-
currences of T
s
in s is equal to that of T
t
in MT (s) : the term translation is consis-
tent with terminological base. Otherwise, we
attempt to locate the wrong term translation
and suggest a better translation to the user.
Both steps require a sub-string matching algo-
rithm which is able to deal with term detection
problems such as morphological variants or dif-
ferent term variants. We describe the approach we
take for efficient sub-string matching in more de-
tail in section 3.3.
3.2 Terminology correction
Once we have detected that there is a source term
T
s
which has been incorrectly translated we would
like to suggest a better translation for this term.
This requires not only knowing a correct transla-
tion T
t
of the source term T
s
, but also its position
in the target sentence. To do that, we need to iden-
tify what was the incorrect translation proposed by
the MT engine for the term and to locate it in the
translation MT (s).
This can be seen as a sub-problem of the word-
alignment problem, which is usually solved us-
ing bilingual dictionaries or by learning statistical
alignment models out of bilingual corpora. How-
ever, in practice, these resources are not easily
available, especially for low-resourced language
pairs. In order to be able to locate the wrong term
translation in the target sentence without resort-
ing to such resources, our approach is to rely in-
stead on the same external MT engine that was
used for translating the whole source sentence in
the first place, an approach also taken in (Itagaki
and Aikawa, 2008).
To overcome the problem mentioned by (Ita-
gaki and Aikawa, 2008) of non-matching out-of-
context terms translations we propose to com-
bine out-of context term translation (MT (T
s
)) and
context-extended term translation, as follows:
? Translate the term T
s
extended with
its left and/or right n-gram context:
s
i?n
s
i?n+1
...T
s
...s
j+n?1
s
j+n
, where
T
s
= s
i
...s
j
;
? Find a fuzzy match in MT (s) for the
translation of the context-extended term
MT (s
i?n
...T
s
...s
j+n
) using the same sub-
string matching algorithm as in the terminol-
ogy verification step.
Various combinations of out-of-context term
translation (MT (T
s
)) and n-extended term trans-
lation (MT (s
i?n
...T
s
...s
j+n
)) are possible.
50
The term location is performed in a sequential
way: if the wrong term translation was not located
after the first step (out-of-context translation), at-
tempt the following step, extending size of the
context (n) until the term is located.
3.3 Implementation
The implementation of the terminology-checking
service that we demonstrate exploits Bing Trans-
lator
2
as SMT service, refers to the Agricul-
ture domain and supports two terminology re-
sources: the multilingual ontology from the Or-
ganic.Edunet portal
3
and Agrovoc, a multilingual
theasurus from FAO
4
. The presented prototype en-
ables terminology checking for all the language
pairs involving English, French, German, Italian,
Portoguese, Spanish and Turkish.
The component for matching the textual input
(i.e. either the source or the translation from the
SMT service) with elements from domain termi-
nologies is based on the open source search engine
Lucene
5
and exploits its built-in textual search ca-
pabilities and indexing facilities. We created a
search index for each of the supported languages,
containing the textual representations of the ter-
minology elements in that language along with
their URI (unique for each terminology element).
The terms expressions are indexed in their origi-
nal form as well as in their lemmatized and POS
tagged ones; for Turkish, resources for morpho-
logical analysis were not available therefore stem-
ming has been used instead of lemmatization.
In order to find the terminological entries within
a textual input in a given language a two-steps pro-
cedure is applied:
? In a first step, the text is used as a query over
the search index (in that language) in order
to find a list of all the terminology elements
containing a textual fragment present in the
query.
? In a second step, in order to retain only the
domain terms with a complete match (no par-
tial matches) and locate them in the text, a
new search index is built in memory, con-
taining a single document, namely the orig-
inal textual input (lemmatized or stemmed
according to the resources available for that
2
http://www.bing.com/translator
3
http://organic-edunet.eu/
4
http://aims.fao.org/standards/agrovoc/about
5
https://lucene.apache.org
specific language). Then the candidate ter-
minology elements found in the first step are
used as queries over the in-memory index and
the ?highlighter? component of the search en-
gine is exploited to locate them in the text
(when found). A longest match criterion is
used when the terminology elements found
refer to overlapping spans of text.
Following this procedure a list with terminology
elements (along with their URIs and the position
within the text) is generated for both the source
text and its translation. A matching strategy based
on the URI allows to pair domain terms from the
two collections. For domain terms in the source
text without a corresponding terminology element
in the translated text, the ?wrong? translation is
located in the text according to the approach de-
scribed in 3.2. The domain term is retranslated
with the same SMT (with context extension, if
needed) in order to obtain the ?wrong? translation
and the translated string is located within the trans-
lation text with the same approach used in the sec-
ond step of the procedure used for locating termi-
nological entries (with an in-memory search index
over the full text and the fragment used as query).
The service outputs two lists: one containing
the pairs of terminology elements found both in
the source and in the translation and another one
with the terminology elements without a ?correct?
translation (according to the domain terminology
used) and for each of those an alternative transla-
tion from the domain terminology is proposed. In
our demonstration a web interface allows users to
access and test the service.
4 Proof of concept evaluation
In order to evaluate the quality of locating the
wrong term translation, we applied the terminol-
ogy verification service to an SMT model trained
with Moses (Hoang et al., 2007) on the Europarl
(Koehn, 2005) corpus. This SMT model was used
for translating a test set in the Agricultural domain
from Spanish into English. In these settings we
have access to the internal sub-phrase alignment
provided by Moses, thus we know the exact loca-
tion of the wrong term translation, which allows
us to evaluate how good our locating technique is.
The test set consists of 100 abstracts in Spanish
from a bibliographical database of scientific publi-
cations in the Agriculture domain. These abstracts
were translated into English with our translation
51
model, and we then applied terminology verifi-
cation and terminology correction procedures to
these translations.
When applying terminology verification we de-
tected in total 171 terms in Spanish, 71 of them
being correctly translated into English (consistent
with terminology), and 100 being wrongly trans-
lated (not consistent with terminology).
We then attempted to locate these wrongly
translated terms in the system translation MT (s).
Matching the out-of-context term translation
with initial translation allowed to find a match for
82 wrongly translated terms (out of 100); Match-
ing 1 left/right word extended term translation
(MT (w
i?1
T
s
w
j+1
)) allowed to find a match for
16 more terms (out of 18 left).
Using the internal word alignments provided by
Moses, we also evaluated how precisely the bor-
ders of the wrongly translated term were recovered
by our term location procedure. This precision is
measured as follows:
? The target tokens identified by our procedure
(as described in 3) are: g
T
= t
1
, . . . , t
j
;
? We then identify the reference target tokens
corresponding to the translation of the term
T
s
using the Moses word alignment : r
T
=
{r
t
1
, . . . , r
t
k
}.
We define term location precision p as p =
|t
j
?r
T
?g
T
|
|g
T
|
. The precision of term location with
out-of-context term translation is of 0.92; the pre-
cision of term location with context-extended term
translation is 0.91.
Overall, our approach allows to match 98% of
the wrongly translated terms, with an overall lo-
cation precision of 0.91. Although these numbers
may vary for other language pairs and other MT
systems, this performance is encouraging.
5 Conclusion
We propose a demonstration of a terminology ver-
ification system that can be used as an aid for post-
editing machine translations explicitly focused on
bilingual terminology consistency. This system re-
lies on an external black-box generic MT engine
extended with available domain-specific terminol-
ogy. The location of the wrong term translation is
located via re-translation of the original term with
the same MT engine. We show that we partially
overcome the situation where the out-of-context
translation of the term differs from the original
translation of this term (in the full sentence) by
extending the term context with surrounding n-
grams. The terminology verification method is
both MT engine and language independent, does
not require any access to the internals of the MT
engine used, and is easily portable.
Acknowledgments
This work has been partially supported by
the Organic.Lingua project (http://www.organic-
lingua.eu/), funded by the European Commission
under the ICT Policy Support Programme.
References
Michael Carl and Philippe Langlais. 2002. An intel-
ligent terminology database as a pre-processor for
statistical machine translation. In COLING-02: Sec-
ond International Workshop on Computational Ter-
minology, pages 1?7.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondej Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL 2007 Demo
and Poster Sessions, pages 177?180.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit X,
pages 79?86, Phuket Thailand.
Marcis Pinnis and Raivis Skadins. 2012. Mt adapta-
tion for under-resourced domains - what works and
what not. In Baltic HLT, volume 247, pages 176?
184.
Raivis Skadins, Marcis Pinnis, Tatiana Gornostay, and
Andrejs Vasiljevs. 2013. Application of online ter-
minology services in statistical machine translation.
In MT Summit XIV, pages 281?286.
52
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 696?700,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
CELI: An Experiment with Cross Language Textual Entailment
Milen Kouylekov
Celi S.R.L.
via San Quintino 31
Torino, Italy
kouylekov@celi.it
Luca Dini
Celi S.R.L.
via San Quintino 31
Torino, Italy
dini@celi.it
Alessio Bosca
Celi S.R.L.
via San Quintino 31
Torino, Italy
bosca@celi.it
Marco Trevisan
Celi S.R.L.
via San Quintino 31
Torino, Italy
trevisan@celi.it
Abstract
This paper presents CELI?s participation in the
SemEval Cross-lingual Textual Entailment for
Content Synchronization task.
1 Introduction
The Cross-Lingual Textual Entailment task (CLTE)
is a new task that addresses textual entailment (TE)
(Bentivogli et. al., 2011), targeting the cross-
lingual content synchronization scenario proposed
in (Mehdad et. al., 2011) and (Negri et. al., 2011).
The task has interesting application scenarios that
can be investigated. Some of them are content syn-
chronization and cross language query alignment.
The task is defined by the organizers as follows:
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments:
? Bidirectional: the two fragments entail each
other (semantic equivalence)
? Forward: unidirectional entailment from T1 to
T2
? Backward: unidirectional entailment from T2
to T1
? No Entailment: there is no entailment between
T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs.
Example for Spanish English pairs:
? bidirectional
Mozart naci en la ciudad de Salzburgo
Mozart was born in Salzburg.
? forward
Mozart naci en la ciudad de Salzburgo
Mozart was born on the 27th January 1756 in
Salzburg.
? backward
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born in 1756 in the city of Salzburg
? no entailment
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born to Leopold and Anna Maria
Pertl Mozart.
2 Our Approach to CLTE
In our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Negri et. al., 2012) we have developed
an approach based on cross-language text similarity.
We have modified our cross-language query similar-
ity system TLike to handle longer texts.
Our approach is based on four main resources:
? A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
? A set of word based bilingual translation mod-
ules.
696
? A semantic component able to associate a se-
mantic vectorial representation to words.
? We use Wikipedia as multilingual corpus.
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
compute mutual information scores between candi-
date terms.
More technically, Random Indexing exploits an
algebraic model in order to represent the seman-
tics of terms in a Nth dimensional space (a vector
of length N); approaches falling into this category,
actually create a Terms By Contexts matrix where
each cell represents the degree of memberships of a
given term to the different contexts. The algorithm
assigns a random signature to each context (a highly
sparse vector of length N , with few, randomly cho-
sen, non-zero elements) and then generates the vec-
tor space model by performing a statistical analysis
of the documents in the domain corpus and by ac-
cumulating on terms rows all the signatures of the
contexts where terms appear.
According to this approach if two different terms
have a similar meaning they should appear in similar
contexts (within the same documents or surrounded
by the same words), resulting into close coordinates
in the so generated semantic space.
In our case study semantic vectors have been gen-
erated taking as corpus the set of metadata available
via the CACAO project (Cacao Project, 2007) fed-
eration (about 6 millions records). After processing
for each word in the corpus we have:
? A vector of float from 0 to 1 representing its
contextual meaning;
? A set of neighbors terms selected among the
terms with a higher semantic similarity, calcu-
lated as cosine distance among vectors.
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The basic idea behind our algorithm is to detect
the probability for two texts to be one a translation
of the other. In the simple case we expect that if all
the words in text TS have a translation in text TT and
if TS and TT have the same number of terms, then
TS and TT are entailed. Things are of course more
complex than this, due to the following facts:
? The presence of compound words make the
constraints on cardinality of search terms not
feasible (e.g. the Italian Carta di Credito vs.
the German KreditCarte).
? One or more words in TS could be absent from
translation dictionaries.
? One or more words in TS could be present
in the translation dictionaries, but contextually
correct translation might be missing.
? There might be items which do not need to be
translated, notably Named Entities.
The first point, compounding, is only partially
an obstacle. NLP technology developed during
CACAO Project, which adopted translation dictio-
naries, deals with compound words both in terms
of identification and translation. Thus the Italian
?Carta di Credito? would be recognized and cor-
rectly translated into ?KreditCarte?. So, in an ideal
1http://lucene.apache.org
697
word, the cardinality principle could be considered
strict. In reality, however, there are many com-
pounding phenomena which are not covered by our
dictionaries, and this forces us to consider that a mis-
match in text term cardinality decrease the probabil-
ity that the two translations are translation of each
other, without necessarily setting it to zero.
Concerning the second aspect, the absence of
source (T1) words in translation dictionaries, it is
dealt with by accessing the semantic repository de-
scribed in the previous section. We first obtain the
list of neighbor terms for the untranslatable source
word. This list is likely to contain many words that
have one or more translations. For each translation,
again, we consult our semantic repository and we
obtain its semantic vector.
Finally, we compose all vectors of all available
translations and we search in the target text (T2) for
the word whose semantic vector best matches the
composed one (cosine distance). Of course we can-
not assume that the best matching vector is a transla-
tion of the original word, but we can use the distance
between the two vectors as a further weight for de-
ciding whether the two texts are translations one of
the other.
There are of course cases when the source word
is correctly missing in the source dictionary. This
is typically the case of most named entities, such
as geographical and person names. These entities
should be appropriately recognized and searched as
exact matches in the target text, thus by-passing any
dictionary look-up and any semantic based match-
ing. Notice that the recognition of named entities
it is not just a matter of generalizing the statement
according to which ?if something is not in the dic-
tionaries, it is a named entity?. Indeed there are well
known cases where the named entity is homograph
with common words (e.g. the French author ?La
Fontaine?), and in these cases we must detect them
in order to avoid the rejection of likely translation
pairs. In other words we must avoid that the two
texts ?La fontaine fables? and ?La Fontaine fav-
ole? are rejected as translation pairs, just by virtue
of the fact that ?La fontaine? is treated as a com-
mon word, thus generating the Italian translation?La
fontana?. Fortunately CACAO disposes of a quite
accurate subsystem for recognizing named entities
in texts, mixing standard NLP technologies with sta-
tistical processing and other corpus-oriented heuris-
tics.
We concentrated our work on handling cases
where two texts are candidates to be mutual trans-
lations, but one or more words receive a translation
which is not contained in the target text. Typically
these cases are a symptom of a non-optimal quality
in translation dictionaries: the lexicographer prob-
ably did not consider some translation candidate.
To address this problem we have created a solution
based on a weighting scheme. For each word of the
source language we assign a weight that reflects its
importance to the semantic interpretation of the text.
We define a matchweight of a word using the for-
mula represented in Figure 2.In this formula wis is
a word from the source text, wkt is a word from the
target text, w is a word in the source language and
trans is a boolean function that searches in the dic-
tionary for translations between two words.
The matchweight is relevant to the matching of a
translation of a word from the source with one of
the words of the target. If the system finds a direct
correspondence the weight is 0. If the match was
made using random indexing the weight is inverse
to the cosine similarity between the vectors.
In order to make an approximation of the signif-
icance of the word to the meaning of the phrase we
have used as its cost the inverse document frequency
(IDF) of the word calculated using Wikipedia as a
corpus. IDF is a most popular measure (a measure
commonly used in Information Retrieval) for calcu-
lating the importance of a word to a text. If N is the
number of documents in a text collection and Nw is
the number of documents of the collection that con-
tain w then the IDF of w is given by the formula:
weight(wis) = idf(w) = log(
N
Nw
) (2)
Using the matchweight and weight we define the
matchscore of a source target pair as:
matchscore(Ts, Tt) =
?
matchweigth(wis)
?
weight(wis)
(3)
If all the words of the source text have a transla-
tion in the target text the score is 0. If none is found
the score is 1. We have calculated the scores for each
698
matchweight(wis) =
?
??
??
0 ?wkt trans(wis) = wkt
w ? (wis) ? (1? d) ?w &wkt distance(wis, w) = d&trans(w) = wkt
w ? (wis) otherwise
(1)
Figure 1: Match Weight of a Word
pair taking t1 as a source and t2 as a target and vice
versa.
3 Systems
We have submitted four runs in the SemEval CLTE
challenge. We used the NaiveBayse algorithm im-
plemented in Mallet2 to create a classifier that will
produce the output for each of the four categories
Forward , Backward , Bidirectional and No Entail-
ment.
System 1 As our first system we have created a
binary classifier in the classical RTE (Bentivogli et.
al., 2011) classification (YES & NO) for each direc-
tion Forward and Backward. We assigned the Bidi-
rectional category if both classifiers returned YES.
As features the classifiers used only the match scores
obtained for the corresponding direction as one and
only numeric feature.
System 2 For the second system we trained a clas-
sifier using all four categories as output. Apart of the
scores obtained matching the texts in both directions
we have included also a set of eight simple surface
measures. Some of these are:
? The length of the two texts.
? The number of common words without transla-
tions.
? The cosine similarity between the tokens of the
two texts without translation.
? Levenshtein distance between the texts.
System 3 For the third system we trained a classi-
fier using all four categories as output. We used as
features scores obtained matching the texts in both
directions without the surface features used in the
System 2.
2http://mallet.cs.umass.edu/
System 4 In the last system we trained a classifier
using all four categories as output. We used as fea-
tures the simple surface measures used in System 2.
The results obtained are shown in Table 1.
4 Analysis
Analyzing the results of our participation we have
reached several important conclusions.
The dataset provided by the organizers presented
a significant challenge for our system which was
adapted from a query similarity approach. The re-
sults obtained demonstrate that only a similarity
based approach will not provide good results for this
task. This fact is also confirmed by the poor perfor-
mance of the simple similarity measures by them-
selves (System 4) and by their contribution to the
combined run (System 2).
The poor performance of our system can be par-
tially explained also by the small dimensions of the
cross-language dictionaries we used. Expanding
them with more words and phrases can potentially
increase our results.
The classifier with four categories clearly outper-
forms the two directional one (System 1 vs. System
3).
Overall we are not satisfied with our experi-
ment. A radically new approach is needed to address
the problem of Cross-Language Textual Entailment,
which our similarity based system could not model
correctly.
In the future we intend to integrate our approach
in our RTE open source system EDITS (Kouylekov
et. al., 2011) (Kouylekov and Negri, 2010) available
at http://edits.sf.net.
Acknowledgments
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
699
SPA-ENG ITA-ENG FRA-ENG DEU-ENG
System 1 0.276 0.278 0.278 0.280
System 2 0.336 0.336 0.300 0.352
System 3 0.322 0.334 0.298 0.350
System 4 0.268 0.280 0.280 0.274
Table 1: Results obtained.
References
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Cacao Project CACAO - project supported by the eCon-
tentplus Programme of the European Commission.
http://www.cacaoproject.eu/
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
700
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 592?597, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Celi: EDITS and Generic Text Pair Classification
Milen Kouylekov
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
kouylekov@celi.it
Luca Dini
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
dini@celi.it
Alessio Bosca
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
alessio.bosca@celi.it
Marco Trevisan
Celi S.R.L.
via San Quintino 31
Torino, Italy
trevisan@celi.it
Abstract
This paper presents CELI?s participation in the
SemEval The Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment
Challenge (Task7) and Cross-lingual Textual
Entailment for Content Synchronization task
(Task 8).
1 Introduction
Recognizing an existing relation between two text
fragments received a significant interest as NLP task
in the recent years. A lot of the approaches were
focused in the filed of Textual Entailment(TE). TE
has been proposed as as a comprehensive frame-
work for applied semantics (Dagan and Glickman,
2004), where the need for an explicit mapping be-
tween linguistic objects can be, at least partially,
bypassed through the definition of semantic infer-
ences at the textual level. In the TE framework, a
text (T ) is said to entail the hypothesis (H) if the
meaning of H can be derived from the meaning of
T . Initially defined as binary relation between texts
(YES/NO there is an entailment or there is not) the
TE evolved in the third RTE3 (Giampiccolo et al,
2007) challenge into a set of three relations between
texts: ENTAILMENT, CONTRADICTION and
UNKNOWN. These relations are interpreted as fol-
lows:
? ENTAILMENT - The T entails the H .
? CONTRADICTION - The H contradicts the T
? UNKNOWN - There is no semantic connection
between T and H .
With more and more applications available for
recognizing textual entailment the researches fo-
cused their efforts in finding practical applications
for the developed systems. Thus the Cross-Lingual
Textual Entailment task (CLTE) was created using
textual entailment (TE) to define cross-lingual con-
tent synchronization scenario proposed in (Mehdad
et. al., 2011), (Negri et. al., 2011) (Negri et. al.,
2012). The task is defined by the organizers as fol-
lows: Given a pair of topically related text fragments
(T1 and T2) in different languages, the CLTE task
consists of automatically annotating it with one of
the following entailment judgments:
? Bidirectional: the two fragments entail each
other (semantic equivalence)
? Forward: unidirectional entailment from T1 to
T2
? Backward: unidirectional entailment from T2
to T1
? No Entailment: there is no entailment between
T1 and T2
The textual entailment competition also evolved.
In this year SEMEVAL The Joint Student Response
Analysis and 8th Recognizing Textual Entailment
Challenge - JRSA-RTE8 (Task7) the textual entail-
ment was defined in three subtasks:
5-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? Correct, if the student answer is a complete and
correct paraphrase of the reference answer;
592
? Partially correct incomplete, if the student an-
swer is a partially correct answer containing
some but not all information from the reference
answer;
? Contradictory, if the student answer explicitly
contradicts the reference answer;
? Irrelevant, if the student answer is ?irrelevant?,
talking about domain content but not providing
the necessary information;
? Non domain, if the student answer expresses a
request for help, frustration or lack of domain
knowledge - e.g., ?I don?t know?, ?as the book
says?, ?you are stupid?.
3-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? correct
? contradictory
? incorrect, conflating the categories of par-
tially correct incomplete, irrelevant or
non domain in the 5-way classification
2-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? correct
? incorrect, conflating the categories of contra-
dictory and incorrect in the 3-way classifica-
tion.
Following the overall trend, we have decided to
convert our system for recognizing textual entail-
ment EDITS from a simple YES/NO recognition
system into a generic system capable of recognizing
multiple semantic relationships between two texts.
EDITS (Kouylekov and Negri, 2010) and
(Kouylekov et. al., 2011) is an open source pack-
age for recognizing textual entailment, which offers
a modular, flexible, and adaptable working environ-
ment to experiment with the RTE task over different
datasets. The package allows to: i) create an en-
tailment engine by defining its basic components ii)
train such entailment engine over an annotated RTE
corpus to learn a model; and iii) use the entailment
engine and the model to assign an entailment judg-
ments and a confidence score to each pair of an un-
annotated test corpus.
We define the recognition of semantic relations
between two texts as a classification task. In this
task the system takes as an input two texts and clas-
sifies them in one of a set of predefined relations.
We have modified EDITS in order to handle the so
defined task.
Having this in mind we have participated in
JRSA-RTE8 (task 7) and CLTE2 (task 8) with the
same approach. We have merged EDITS with some
features from the TLike system described in our last
participation in CLTE (Kouylekov et. al., 2011). For
each of the tasks we have created a specialized com-
ponents that are integrated in EDITS as one of the
system?s modules.
2 EDITS and Generic Text Pair
Classification
As in the previous versions, the core of EDITS im-
plements a distance-based framework. Within this
framework the system implements and harmonizes
different approaches to distance computation be-
tween texts, providing both edit distance algorithms,
and similarity algorithms. Each algorithm returns
a normalized distance score (a number between 0
and 1). Each algorithm algorithm depends on two
generic modules defined by the system?s user:
? Matcher - a module that is used to align text
fragments. This module uses semantic tech-
niques and entailment rules to find equivalent
textfragments.
? Weight Calculator - a module that is used to
give weight to text fragments. The weights are
used to determine the importance of a text por-
tion to the overall meaning of the text.
In the previous versions of the system at the train-
ing stage, distance scores calculated over annotated
T-H pairs are used to estimate a threshold that best
separates positive (YES) from negative (NO) exam-
ples. The calculated threshold was used at a test
stage to assign an entailment judgment and a con-
fidence score to each test pair. In the new version
593
of the system we used a machine learning classifier
to classify the T-H pairs in the appropriate category.
The overall architecture of the system is shown in
Figure 1.
The new architecture is divided in two sets of
modules: Machine Learning and Edit Distance. In
the Edit Distance set various distance algorithms are
used to calculate the distance between the two texts.
Each of these algorithms have a custom matcher and
weight calculator. The distances calculated by each
of these algorithms are used as features for the clas-
sifiers of the Machine Leaning modules. The ma-
chine learning modules are structured in two levels:
? Binary Classifiers - for each semantic relation
we create a binary classifier that distinguishes
between the members of the relation and the
members of the other relations. For example:
For 3way task (Task 7) the system created 3 bi-
nary classifiers one for each relation.
? Classifier - a module that makes final decision
for the text pair taking the output (decision and
confidence) of the binary classifiers as an input.
We have experimented with other configurations
of the machine leaning modules and selected this
one as the best performing on the available datasets
of the previous RTE competitions. In the version
of EDITS avalble online other configurations of the
machine leaning modules will be available using the
flexibility of the system configuration.
We have used the algorithms implemented in
WEKA (Hall et al, 2009) for the classification mod-
ules. The binary modules use SMO algorithm. The
top classifier uses NaiveBayes.
The input to the system is a corpus of text pairs
each classified with one semantic relation. We have
used the format of the previous RTE competitions
in order to be compliant. The goal of the system is
to create classifier that is capable of recognizing the
correct relation for an un-annotated pair of texts.
The new version of EDITS package allows to:
? Create an Classifier by defining its basic com-
ponents (i.e. algorithms, matchers, and weight
calculators);
? Train such Classifier over an annotated corpus
(containing T-H pairs annotated in terms of en-
tailment) to learn a Model;
? Use the Classifier and the Model to assign an
entailment judgment and a confidence score to
each pair of an un-annotated test corpus.
3 Resources
Like our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Kouylekov et. al., 2011), our approach is
based on four main resources:
? A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
? A set of word based bilingual translation mod-
ules.(Employed only for Task 8)
? A semantic component able to associate a se-
mantic vectorial representation to words.
? We use Wikipedia as multilingual corpus.
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
594
Figure 1: EDITS Architecture
compute mutual information scores between candi-
date terms.
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The semantic component and the translation2
modules are used as core components in the matcher
module. IDF calculated on Wikipedia is used as
weight for the words by the weight calculator model.
4 JRSA-RTE8
In the JRSA-RTE8 we consider the reference an-
swers as T (text) and the student answer as H (hy-
pothesis). As the reference answers are often more
than one, we considered as input to the machine
learning algorithms the distance between the student
answer and the closest reference answer. We define
the closest reference answer as the reference answer
with minimum distance according to the distance al-
gorithm.
1http://lucene.apache.org
2Translation module is used only for Task 8.
4.1 Systems
We have submitted two runs in the SemEval JRSA-
RTE8 challenge (Task 7). The systems were exe-
cuted on each of the sub tasks of the main task.
System 1 The distance algorithm used in the first
system is Word Overlap. The algorithm tries to find
the words of a source text between the words of the
target text. We have created two features for each
binary classifier: 1) Feature 1 - word overlap of H
into T (words of H are matched by the words in T;
2) Feature 2 - word overlap T into H (Words of T are
matched by the words in H).
System 2 In the second system the we have used
only Feature 1.
We have created separate models for the Beatle
dataset and the sciEntsBank dataset. The results ob-
tained are shown in Table 1.
4.2 Analysis
The results obtained are in line with our previous
participations in the RTE challenges (Kouylekov et.
al., 2011). Of course as we described before in our
papers (Kouylekov et. al., 2011) the potential of the
edit distance algorithm is limited. Still it provides a
595
Task Beatle Q Beatle A sciEntsBank Q sciEntsBank A sciEntsBank D
2way
run 1 0.6400 0.6570 0.5930 0.6280 0.6160
run 2 0.4620 0.4480 0.5560 0.5930 0.5710
3way
run 1 0.5510 0.4950 0.5240 0.5780 0.5490
run 2 0.4150 0.4400 0.4390 0.5030 0.4770
5way
run 1 0.4830 0.4470 0.4130 0.4340 0.4170
run 2 0.3850 0.4320 0.2330 0.2370 0.2540
Table 1: Task 7 Results obtained. (Accuracy)
good performance and provides a solid potential for
some close domain tasks as described in (Negri and
Kouylekov, 2009). We were quite content with the
new machine learning based core. The selected con-
figuration performed in an acceptable manner. The
results obtained were in line with the cross accuracy
obtained by our system on the training set which
shows that it is not susceptible to over-training.
5 CLTE
5.1 Systems
We have submitted two runs in the CLTE task (Task
8).
System 1 The distance algorithm used in the first
system is Word Overlap as we did for task 7. We
have created two features for each binary classifier:
1) Feature 1 - word overlap of H into T (words of H
are matched by the words in T; 2) Feature 2 - word
overlap T into H (Words of T are matched by the
words in H).
System 2 In the second system we have made a
slight modification of the matcher that handled num-
bers.
The matcher module for this task used the transla-
tion modules defined in Section 3. We have created
a model for each language pair.
The results obtained are shown in Table 2.
5.2 Analysis
The results obtained are quite disappointing. Our
system obtained on the test set of the last CLTE com-
petition (CLTE1) quite satisfactory results (clte1-
test). All the results obtained for this competition
are near or above the medium of the best systems.
Our algorithm did not show signs of over-training
(the accuracy of the system on the test and on the
training of CLTE1 were almost equal). Having this
in mind we expected to obtain scores at least in the
margins of 0.45 to 0.5. This does not happen ac-
cording us due to the fact that this year dataset has
characteristics quite different than the last year. To
test this hypothesis we have trained our system on
half of the dataset (clte2-half-training) ,given for test
this year, and test it on the rest (clte-half-test). The
results obtained demonstrate that the dataset given
is more difficult for our system than the last years
one. The results also prove that our system is prob-
ably too conservative when learning from examples.
If the test set is similar to the training it performs
in consistent manner on both, otherwise it demon-
strates severe over-training problems.
6 Conclusions
In this paper we have presented a generic system for
text pair classification. This system was evaluated
on task 7 and task 8 of Semeval 2013 and obtained
satisfactory results. The new machine learning mod-
ule of the system needs improvement and we plan to
focus our future efforts in it.
We plan to release the newly developed system as
version 4 of the open source package EDITS avail-
able at http://edits.sf.net.
Acknowledgments
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
596
Run Spanish Italian French German
run1 0.34 0.324 0.346 0.349
run2 0.342 0.324 0.34 0.349
clte2-half-training 0.41 0.43 0.40 0.44
clte2-half-test 0.43 0.44 0.41 0.43
clte1-test 0.52 0.51 0.54 0.55
Table 2: Task 8. Results obtained. (Accuracy)
References
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Dagan I. and Glickman O. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. Learning Methods for Text Understanding
and Mining Workshop.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Giampiccolo; Bernardo Magnini; Ido Dagan; Bill Dolan.
2007. The Third PASCAL Recognizing Textual
Entailment Challenge. Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing. June 2007, Prague, Czech Republic
Hall M., Frank E., Holmes G., Pfahringer B., Reute-
mann P., Witten I. 2009 The WEKA Data Mining
Software: An Update; SIGKDD Explorations, Vol-
ume 11, Issue 1.
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Kouylekov M., Bosca A., Dini L., Trevisan M. 2012.
CELI: An Experiment with Cross Language Textual
Entailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Kouylekov M., Mehdad Y. and Negri M. 2011 Is it Worth
Submitting this Run? Assess your RTE System with a
Good Sparring Partner Proceedings of the TextInfer
2011 Workshop on Textual Entailment
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Kouylekov M., 2009 Question Answer-
ing over Structured Data: an Entailment-Based Ap-
proach to Question Analysis. RANLP 2009 - Re-
cent Advances in Natural Language Processing, 2009
Borovets, Bulgaria
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
597
