Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 304?312,
Beijing, August 2010
Translation Model Generalization using Probability Averaging 
for Machine Translation 
Nan Duan1, Hong Sun 
School of Computer Science and Technology 
Tianjin University 
v-naduan@microsoft.com 
v-hongsun@microsoft.com 
Ming Zhou 
Microsoft Research Asia 
 
mingzhou@microsoft.com 
Abstract 
Previous methods on improving transla-
tion quality by employing multiple SMT 
models usually carry out as a second-
pass decision procedure on hypotheses 
from multiple systems using extra fea-
tures instead of using features in existing 
models in more depth. In this paper, we 
propose translation model generalization 
(TMG), an approach that updates proba-
bility feature values for the translation 
model being used based on the model it-
self and a set of auxiliary models, aiming 
to enhance translation quality in the first-
pass decoding. We validate our approach 
on translation models based on auxiliary 
models built by two different ways. We 
also introduce novel probability variance 
features into the log-linear models for 
further improvements. We conclude that 
our approach can be developed indepen-
dently and integrated into current SMT 
pipeline directly. We demonstrate BLEU 
improvements on the NIST Chinese-to-
English MT tasks for single-system de-
codings, a system combination approach 
and a model combination approach.1 
1 Introduction 
Current research on Statistical Machine Transla-
tion (SMT) has made rapid progress in recent 
decades. Although differed on paradigms, such 
as phrase-based (Koehn, 2004; Och and Ney, 
2004), hierarchical phrase-based (Chiang, 2007) 
and syntax-based (Galley et al, 2006; Shen et 
al., 2008; Huang, 2008), most SMT systems fol-
                                                 
1 This work has been done while the author was visiting 
Microsoft Research Asia. 
low the similar pipeline and share common 
translation probability features which constitute 
the principal components of translation models. 
However, due to different model structures or 
data distributions, these features are usually as-
signed with different values in different transla-
tion models and result in translation outputs with 
individual advantages and shortcomings. 
In order to obtain further improvements, many 
approaches have been explored over multiple 
systems: system combination based on confu-
sion network (Matusov et al, 2006; Rosti et al, 
2007; Li et al, 2009a) develop on multiple N-
best outputs and outperform primary SMT sys-
tems; consensus-based methods (Li et al, 2009b; 
DeNero et al, 2010), on the other hand, avoid 
the alignment problem between translations can-
didates and utilize n-gram consensus, aiming to 
optimize special decoding objectives for hypo-
thesis selection. All these approaches act as the 
second-pass decision procedure on hypotheses 
from multiple systems by using extra features. 
They begin to work only after the generation of 
translation hypotheses has been finished. 
In this paper, we propose translation model 
generalization (TMG), an approach that takes 
effect during the first-pass decoding procedure 
by updating translation probability features for 
the translation model being used based on the 
model itself and a set of auxiliary models. Baye-
sian Model Averaging is used to integrate values 
of identical features between models. Our con-
tributions mainly include the following 3 aspects: 
? Alleviate the model bias problem based on 
translation models with different paradigms.  
Because of various model constraints, trans-
lation models based on different paradigms 
could have individual biases. For instance, 
phrase-based models prefer translation pairs 
with high frequencies and assign them high 
304
probability values; yet such pairs could be 
disliked or even be absent in syntax-based 
models because of their violation on syntac-
tic restrictions. We alleviate such model bias 
problem by using the generalized probability 
features in first-pass decoding, which com-
puted based on feature values from all trans-
lation models instead of any single one. 
? Alleviate the over-estimation problem based 
on translation models with an identical pa-
radigm but different training corpora.  
In order to obtain further improvements by 
using an existing training module built for a 
specified model paradigm, we present a ran-
dom data sampling method inspired by bag-
ging (Breiman, 1996) to construct transla-
tion model ensembles from a unique data set 
for usage in TMG. Compared to results of 
TMG based on models with different para-
digms, TMG based on models built in such a 
way can achieve larger improvements. 
? Novel translation probability variance fea-
tures introduced. 
We present how to compute the variance for 
each probability feature based on its values 
in different involved translation models with 
prior model probabilities. We add them into 
the log-linear model as new features to make 
current SMT models to be more flexible. 
The remainder of this paper is organized as 
follows: we review various translation models in 
Section 2. In Section 3, we first introduce Baye-
sian Model Averaging method for SMT tasks 
and present a generic TMG algorithm based on it. 
We then discuss two solutions for constructing 
TM ensembles for usage in TMG. We next in-
troduce probability variance features into current 
SMT models as new features. We evaluate our 
method on four state-of-the-art SMT systems, a 
system combination approach and a model com-
bination approach. Evaluation results are shown 
in Section 4. In Section 5, we discuss some re-
lated work. We conclude the paper in Section 6. 
2 Summary of Translation Models 
Translation Model (TM) is the most important 
component in current SMT framework. It 
provides basic translation units for decoders with 
a series of probability features for model 
scoring. Many literatures have paid attentions to 
TMs from different aspects: DeNeefe et al 
(2007) compared strengths and weaknesses of a 
phrase-based TM and a syntax-based TM from 
the statistic aspect; Zollmann et al (2008) made 
a systematic comparison of three TMs, including 
phrasal, hierarchical and syntax-based, from the 
performance aspect; and Auli et al (2009) made 
a systematic analysis of a phrase-based TM and 
a hierarchical TM from the search space aspect. 
Given a word-aligned training corpus, we 
separate a TM training procedure into two phas-
es: extraction phase and parameterization phase. 
Extraction phase aims to pick out all valid 
translation pairs that are consistent with pre-
defined model constraints. We summarize cur-
rent TMs based on their corresponding model 
constraints into two categories below: 
? String-based TM (string-to-string): reserves 
all translation pairs that are consistent with 
word alignment and satisfy length limitation. 
SMT systems using such TMs can benefit 
from a large convergence of translation pairs. 
? Tree-based TM (string-to-tree, tree-to-string 
or tree-to-tree): needs to obey syntactic re-
strictions in one side or even both sides of 
translation candidates. The advantage of us-
ing such TMs is that translation outputs 
trend to be more syntactically well-formed. 
Parameterization phase aims to assign a series 
of probability features to each translation pair. 
These features play the most important roles in 
the decision process and are shared by most cur-
rent SMT decoders. In this paper, we mainly 
focus on the following four commonly used do-
minant probability features including: 
? translation probability features in two direc-
tions:         and         
? lexical weight features in two directions: 
           and            
Both string-based and tree-based TMs are 
state-of-the-art models, and each extraction ap-
proach has its own strengths and weaknesses 
comparing to others. Due to different predefined 
model constraints, translation pairs extracted by 
different models usually have different distribu-
tions, which could directly affect the resulting 
probability feature values computed in parame-
305
terization phase. In order to utilize translation 
pairs more fairly in decoding, it is desirable to 
use more information to measure the quality of 
translation pairs based on different TMs rather 
than totally believing any single one. 
3 Translation Model Generalization 
We first introduce Bayesian Model Averaging 
method for SMT task. Based on it, we then for-
mally present the generic TMG algorithm. We 
also provide two solutions for constructing TM 
ensembles as auxiliary models. We last intro-
duce probability variance features based on mul-
tiple TMs for further improvements. 
3.1 Bayesian Model Averaging for SMT 
Bayesian Model Averaging (BMA) (Hoeting et 
al., 1999) is a technique designed to solve uncer-
tainty inherent in model selection.  
Specifically, for SMT tasks,   is a source sen-
tence,  is the training data,   is the  
th SMT 
model trained on    ,            represents 
the probability score predicted by   that   can 
be translated into a target sentence  . BMA pro-
vides a way to combine decisions of all     
SMT models by computing the final translation 
probability score              as follows: 
                               
 
   
  (1) 
where          is the prior probability that 
   is a true model. For convenience, we will 
omit all symbols    in following descriptions. 
Ideally, if all involved models           
share the same search space, then translation 
hypotheses could only be differentiated in prob-
ability scores assigned by different SMT models. 
In such case, BMA can be straightly developed 
on the whole SMT models in either span level or 
sentence level to re-compute translation scores 
of hypotheses for better rankings. However, be-
cause of various reasons, e.g. different pruning 
methods, different training data used, different 
generative capabilities of SMT models, search 
spaces between different models are always not 
identical. Thus, it is intractable to develop BMA 
on the whole SMT model level directly. 
As a tradeoff, we notice that translation pairs 
between different TMs share a relatively large 
convergence because of word length limitation. 
So we instead utilize BMA method to multiple 
TMs by re-computing values of probability fea-
tures between them, and we name this process as 
translation model generalization. 
3.2 A Generic BMA-based TMG Algorithm 
For a translation model  , TMG aims to re-
compute its values of probability features based 
on itself and   collaborative TMs          . 
We describe the re-computation process for an 
arbitrary feature              as follows: 
                            
 
   
  (2) 
where            is the feature value assigned 
by  . We denote   as the main model, and 
other collaborative TMs as auxiliary models. 
Figure 1 describes an example of TMG on two 
TMs, where the main model is a phrasal TM. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Equation 2 is a general framework that can be 
applied to all TMs. The only limitation is that 
the segmentation (or tokenization) standards for 
source (or target) training sentences should be 
identical for all models. We describe the generic 
TMG procedure in Algorithm 12. 
                                                 
2 In this paper, since all data sets used have relative large 
sizes and all SMT models have similar performances, we 
heuristically set al       equally to        . 
 
Figure 1. TMG applied to a phrasal TM (main 
model) and a syntax-based TM (auxiliary mod-
el). The value of a translation probability feature 
           ??  in TM1 is de-valued (from 0.6 
to 0.3), in which ?join the? is absent in TM2 be-
cause of its bad syntactic structure. 
           ??  
=0.6 
Phrase-based TM1 
(Main model) 
Syntax-based TM2 
(Auxiliary model) 
      =0.5       =0.5 
 
Generalized TM1 
           ??  
=0.6*0.5+0.0*0.5=0.3 
           ??  
=0.0 
306
Algorithm 1: TMG for a main model   
1: for the  th auxiliary TM do 
2:          run training procedure on    with specified 
model constraints and generate   
3: end for 
4: for each translation pair         in   do 
5:  for each probability feature           do 
6:          for each translation model   do 
7:      if          is contained in   then 
8:                                         
9    end if 
10:   end for 
11:  end for 
12: end for 
13: return the generalized   for SMT decoding 
 
3.3 Auxiliary Model Construction 
In order to utilize TMG, more than one TM as 
auxiliary models is needed. Building TMs with 
different paradigms is one solution. For exam-
ple, we can build a syntax-based TM as an aux-
iliary model for a phrase-based TM. However, it 
has to re-implement more complicated TM train-
ing modules besides the existing one. 
In this sub-section, we present an alternative 
solution to construct auxiliary model ensembles 
by using the existing training module with dif-
ferent training data extracted from a unique data 
set. We describe the general procedure for con-
structing   auxiliary models as follows: 
1) Given a unique training corpus  , we ran-
domly sample    bilingual sentence pairs 
without replacement and denote them as   . 
  is a number determined empirically; 
2) Based on  , we re-do word alignment and 
train an auxiliary model   using the exist-
ing training module; 
3) We execute Step 1 and Step 2 iteratively for 
  times, and finally obtain   auxiliary mod-
els. The optimal setting of   for TMG is al-
so determined empirically. 
With all above steps finished, we can perform 
TMG as we described in Algorithm 1 based on 
the   auxiliary models generated already. 
The random data sampling process described 
above is very similar to bagging except for it not 
allowing replacement during sampling. By mak-
ing use of this process, translation pairs with low 
frequencies have relatively high probabilities to 
be totally discarded, and in resulting TMs, their 
probabilities could be zero; meanwhile, transla-
tion pairs with high frequencies still have high 
probabilities to be reserved, and hold similar 
probability feature values in resulting TMs com-
paring to the main model. Thus, after TMG pro-
cedure, feature values could be smoothed for 
translation pairs with low frequencies, and be 
stable for translation pairs with high frequencies. 
From this point of view, TMG can also be seen 
as a TM smoothing technique based on multiple 
TMs instead of single one such as Foster et al 
(2006). We will see in Section 4 that TMG based 
on TMs generated by both of these two solutions 
can improve translation quality for all baseline 
decoders on a series of evaluation sets. 
3.4 Probability Variance Feature 
The re-computed values of probability features 
in Equation 2 are actually the feature expecta-
tions based on their values from all involved 
TMs. In order to give more statistical meanings 
to translation pairs, we also compute their cor-
responding feature variances based on feature 
expectations and TM-specified feature values 
with prior probabilities. We introduce such va-
riances as new features into the log-linear model 
for further improvements. Our motivation is to 
quantify the differences of model preferences 
between TMs for arbitrary probability features. 
The variance for an arbitrary probability fea-
ture         can be computed as follows: 
                     
       
 
   
  (3) 
where        is the feature expectation computed 
by Equation 2,       is the feature value pre-
dicted by  , and        is the prior probabil-
ity for  . Each probability feature now corres-
ponds to a variance score. We extend the origi-
nal feature set of   with variance features add-
ed in and list the updated set below: 
? translation probability expectation features 
in two directions:           and           
? translation probability variance features in 
two directions:          and          
? lexical weight expectation features in two 
directions:           
   and         
     
? lexical weight variance features in two di-
rections:          
   and        
     
307
4 Experiments 
4.1 Data Condition 
We conduct experiments on the NIST Chinese-
to-English MT tasks. We tune model parameters 
on the NIST 2003 (MT03) evaluation set by 
MERT (Och, 2003), and report results on NIST 
evaluation sets including the NIST 2004 (MT04), 
the NIST 2005 (MT05), the newswire portion of 
the NIST 2006 (MT06) and 2008 (MT08). Per-
formances are measured in terms of the case-
insensitive BLEU scores in percentage numbers. 
Table 1 gives statistics over these evaluation sets. 
 
 MT03 MT04 MT05 MT06 MT08 
Sent 919 1,788 1,082 616 691 
Word 23,788 48,215 29,263 17,316 17,424 
Table 1. Statistics on dev/test evaluation sets 
We use the selected data that picked out from 
the whole data available for the NIST 2008 con-
strained track of Chinese-to-English machine 
translation task as the training corpora, including 
LDC2003E07, LDC2003E14, LDC2005T06, 
LDC2005T10, LDC2005E83, LDC2006E26, 
LDC2006E34, LDC2006E85 and LDC2006E92, 
which contain about 498,000 sentence pairs after 
pre-processing. Word alignments are performed 
by GIZA++ (Och and Ney, 2000) in both direc-
tions with an intersect-diag-grow refinement. 
A traditional 5-gram language model (LM) 
for all involved systems is trained on the English 
side of all bilingual data plus the Xinhua portion 
of LDC English Gigaword Version 3.0. A lexi-
calized reordering model (Xiong et al, 2006) is 
trained on the selected data in maximum entropy 
principle for the phrase-based system. A tri-
gram target dependency LM (DLM) is trained 
on the English side of the selected data for the 
dependency-based hierarchical system. 
 
 
 
 
 
 
 
 
4.2 MT System Description 
We include four baseline systems. The first one 
(Phr) is a phrasal system (Xiong et al, 2006) 
based on Bracketing Transduction Grammar 
(Wu, 1997) with a lexicalized reordering com-
ponent based on maximum entropy model. The 
second one (Hier) is a hierarchical phrase-based 
system (Chiang, 2007) based on Synchronous 
Context Free Grammar (SCFG). The third one 
(Dep) is a string-to-dependency hierarchical 
phrase-based system (Shen et al, 2008) with a 
dependency language model, which translates 
source strings to target dependency trees. The 
fourth one (Synx) is a syntax-based system (Gal-
ley et al, 2006) that translates source strings to 
target syntactic trees. 
4.3 TMG based on Multiple Paradigms 
We develop TMG for each baseline system?s 
TM based on the other three TMs as auxiliary 
models. All prior probabilities of TMs are set 
equally to 0.25 heuristically as their similar per-
formances. Evaluation results are shown in Ta-
ble 2, where gains more than 0.2 BLEU points 
are highlighted as improved cases. Compared to 
baseline systems, systems based on generalized 
TMs improve in most cases (18 times out of 20). 
We also notice that the improvements achieved 
on tree-based systems (Dep and Synx) are rela-
tively smaller than those on string-based systems 
(Phr and Hier). A potential explanation can be 
that with considering more syntactic restrictions, 
tree-based systems suffer less than string-based 
systems on the over-estimation problem. We do 
not present further results with variance features 
added because of their consistent un-promising 
numbers. We think this may be due to the consi-
derable portion of non-overlapping translation 
pairs between main model and auxiliary models, 
which cause the variances not so accurate. 
 
 
 
 
 
 
 
 
 
 
 
  MT03(dev) MT04 MT05 MT06 MT08 Average 
Phr 
Baseline 40.45 39.21 38.03 34.24 30.21 36.43 
TMG 41.19(+0.74) 39.74(+0.53) 38.39(+0.36) 34.71(+0.47) 30.69(+0.48) 36.94(+0.51) 
Hier 
Baseline 41.30 39.63 38.83 34.63 30.46 36.97 
TMG 41.67(+0.37) 40.25(+0.62) 39.11(+0.28) 35.78(+1.15) 31.17(+0.71) 37.60(+0.63) 
Dep 
Baseline 41.10 39.81 39.47 35.72 30.50 37.32 
TMG 41.37(+0.27) 39.92(+0.11) 39.91(+0.44) 35.99(+0.27) 31.07(+0.57) 37.65(+0.33) 
Synx 
Baseline 41.02 39.88 39.47 36.41 32.15 37.79 
TMG 41.26(+0.24) 40.09(+0.21) 39.90(+0.43) 36.77(+0.36) 32.15(+0.00) 38.03(+0.24) 
Table 2. Results of TMG based on TMs with different paradigms 
 
308
4.4 TMG based on Single Paradigm 
We then evaluate TMG based on auxiliary mod-
els generated by the random sampling method. 
We first decide the percentage of training data 
to be sampled. We empirically vary this number 
by 20%, 40%, 60%, 80% and 90% and use each 
sampled data to train an auxiliary model. We 
then run TMG on the baseline TM with different 
auxiliary model used each time. For time saving, 
we only evaluate on MT03 for Phr in Figure 2. 
 
Figure 2. Affects of different percentages of data 
The optimal result is achieved when the per-
centage is 80%, and we fix it as the default value 
in following experiments. 
We then decide the number of auxiliary mod-
els used for TMG by varying it from 1 to 5. We 
list different results on MT03 for Phr in Figure 3. 
 
Figure 3. Affects of different numbers of auxi-
liary models 
 
 
 
 
 
 
 
 
 
 
The optimal result is achieved when the num-
ber of auxiliary models is 4, and we fix it as the 
default value in following experiments. 
We now develop TMG for each baseline sys-
tem?s TM based on auxiliary models constructed 
under default settings determined above. Evalua-
tion results are shown in Table 3. We also inves-
tigate the affect of variance features for perfor-
mance, whose results are denoted as TMG+Var. 
From Table 3 we can see that, compared to 
the results on baseline systems, systems using 
generalized TMs obtain improvements on almost 
all evaluation sets (19 times out of 20). With 
probability variance features added further, the 
improvements become even more stable than the 
ones using TMG only (20 times out of 20). Simi-
lar to the trend in Table 2, we also notice that 
TMG method is more preferred by string-based 
systems (Phr and Hier) rather than tree-based 
systems (Dep and Synx). This makes our con-
clusion more solidly that syntactic restrictions 
can help to alleviate the over-estimation problem. 
4.5 Analysis on Phrase Coverage 
We next empirically investigate on the transla-
tion pair coverage between TM ensembles built 
by different ways, and use them to analyze re-
sults got from previous experiments. Here, we 
only focus on full lexicalized translation entries 
between models. Those entries with variables 
are out of consideration in comparisons because 
of their model dependent properties. 
Phrase pairs in the first three TMs have a 
length limitation in source side up to 3 words, 
and each source phrase can be translated to at 
most 20 target phrases.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
40.0
40.5
41.0
41.5
0% 20% 40% 60% 80% 90%
Phr
40.0
40.5
41.0
41.5
42.0
0 1 2 3 4 5
Phr
  MT03(dev) MT04 MT05 MT06 MT08 Average 
Phr 
Baseline 40.45 39.21 38.03 34.24 30.21 36.43 
TMG 41.77(+1.32) 40.28(+1.07) 39.13(+1.10) 35.38(+1.14) 31.12(+0.91) 37.54(+1.11) 
TMG+Var 41.77(+1.32) 40.31(+1.10) 39.43(+1.30) 35.61(+1.37) 31.62(+1.41) 37.74(+1.31) 
Hier 
Baseline 41.30 39.63 38.83 34.63 30.46 36.97 
TMG 42.28(+0.98) 40.45(+0.82) 39.61(+0.78) 35.67(+1.04) 31.54(+1.08) 37.91(+0.94) 
TMG+Var 42.42(+1.12) 40.55(+0.92) 39.69(+0.86) 35.55(+0.92) 31.41(+0.95) 37.92(+0.95) 
Dep 
Baseline 41.10 39.81 39.47 35.72 30.50 37.32 
TMG 41.49(+0.39) 40.20(+0.39) 40.00(+0.53) 36.13(+0.41) 31.24(+0.74) 37.81(+0.49) 
TMG+Var 41.72(+0.62) 40.57(+0.76) 40.44(+0.97) 36.15(+0.43) 31.31(+0.81) 38.04(+0.72) 
Synx 
Baseline 41.02 39.88 39.47 36.41 32.15 37.79 
TMG 41.18(+0.16) 40.30(+0.42) 39.90(+0.43) 36.99(+0.58) 32.45(+0.30) 38.16(+0.37) 
TMG+Var 41.42(+0.40) 40.55(+0.67) 40.17(+0.70) 36.89(+0.48) 32.51(+0.36) 38.31(+0.52) 
Table 3. Results of TMG based on TMs constructed by random data sampling 
 
309
For the fourth TM, these two limitations are 
released to 4 words and 30 target phrases. We 
treat phrase pairs identical on both sides but with 
different syntactic labels in the fourth TM as a 
unique pair for conveniences in statistics.  
We first make statistics on TMs with different 
paradigms in Table 4. We can see from Table 4 
that only slightly over half of the phrase pairs 
contained by the four involved TMs are common, 
which is also similar to the conclusion drawn in 
DeNeefe et al (2006). 
 
Models #Translation Pair #Percentage 
Phr 1,222,909 50.6% 
Hier 1,222,909 50.6% 
Dep 1,087,198 56.9% 
Synx 1,188,408 52.0% 
Overlaps 618,371 - 
Table 4. Rule statistics on TMs constructed by 
different paradigms 
We then make statistics on TMs with identical 
paradigm in Table 5. For each baseline TM and 
its corresponding four auxiliary models con-
structed by random data sampling, we count the 
number of phrase pairs that are common be-
tween them and compute the percentage num-
bers based on it for each TM individually. 
 
Models TM0 TM1 TM2 TM3 TM4 
Phr 61.8% 74.0% 74.1% 73.9% 74.1% 
Hier 61.8% 74.0% 74.1% 73.9% 74.1% 
Dep 60.8% 73.6% 73.6% 73.5% 73.7% 
Synx 57.2% 68.4% 68.5% 68.5% 68.6% 
Table 5. Rule statistics on TMs constructed by 
random sampling (TM0 is the main model) 
Compared to the numbers in Table 4, we find 
that the coverage between baseline TM and 
sampled auxiliary models with identical para-
digm is larger than that between baseline TM 
and auxiliary models with different paradigms 
(about 10 percents). It is a potential reason can 
explain why results of TMG based on sampled 
auxiliary models are more effective than those 
based on auxiliary models built with different 
paradigms, as we infer that they share more 
common phrase pairs each other and make the 
computation of feature expectations and va-
riances to be more reliable and accurate. 
4.6 Improvements on System Combination 
Besides working for single-system decoding, we 
also perform a system combination method on 
N-best outputs from systems using generalized 
TMs. We re-implement a state-of-the-art word-
level System Combination  (SC) approach based 
on incremental HMM alignment proposed by Li 
et al (2009a). The default number of N-best 
candidates used is set to 20. 
We evaluate SC on N-best outputs generated 
from 4 baseline decoders by using different TM 
settings and list results in Table 6, where Base 
stands for combination results on systems using 
default TMs; Paras stands for combination re-
sults on systems using TMs generalized based 
on auxiliary models with different paradigms; 
and Samp stands for combination results on sys-
tems using TMs generalized based on auxiliary 
models constructed by the random data sampling 
method. For the Samp setting, we also include 
probability variance features computed based on 
Equation 3 in the log-linear model.  
 
SC MT03 MT04 MT05 MT06 MT08 
Base 44.20 42.30 41.22 37.77 33.07 
Paras 44.40 42.69 41.53 38.05 33.31 
Samp 44.80 42.95 42.10 38.39 33.67 
Table 6. Results on system combination 
From Table 6 we can see that system combi-
nation can benefit from TMG method. 
4.7 Improvements on Model Combination 
As an alternative, model combination is another 
effective way to improve translation perfor-
mance by utilizing multiple systems. We re-
implement the Model Combination (MC) ap-
proach (DeNero et al, 2010) using N-best lists 
as its inputs and develop it on N-best outputs 
used in Table 6. Evaluation results are presented 
in Table 7.  
 
MC MT03 MT04 MT05 MT06 MT08 
Base 42.31 40.57 40.31 38.65 33.88 
Paras 42.87 40.96 40.77 38.81 34.47 
Samp 43.29 41.29 41.11 39.28 34.77 
Table 7. Results on model combination 
310
From Table 7 we can see that model combina-
tion can also benefit from TMG method. 
5 Related Work 
Foster and Kuhn (2007) presented an approach 
that resembles more to our work, in which they 
divided the training corpus into different com-
ponents and integrated models trained on each 
component using the mixture modeling. Howev-
er, their motivation was to address the domain 
adaption problem, and additional genre informa-
tion should be provided for the corpus partition 
to create multiple models for mixture. We in-
stead present two ways for the model ensemble 
construction without extra information needed: 
building models by different paradigms or by a 
random data sampling technique inspired by a 
machine learning technique. Compared to the 
prior work, our approach is more general, which 
can also be used for model adaptation. We can 
also treat TMG as a smoothing way to address 
the over-estimation problem existing in almost 
all TMs. Some literatures have paid attention to 
this issue as well, such as Foster et al (2006) 
and Mylonakis and Sima ?an (2008). However, 
they did not leverage information between mul-
tiple models as we did, and developed on single 
models only. Furthermore, we also make current 
translation probability features to contain more 
statistical meanings by introducing the probabili-
ty variance features into the log-linear model, 
which are completely novel to prior work and 
provide further improvements. 
6 Conclusion and Future Work 
In this paper, we have investigated a simple but 
effective translation model generalization me-
thod that benefits by integrating values of prob-
ability features between multiple TMs and using 
them in decoding phase directly. We also intro-
duce novel probability variance features into the 
current feature sets of translation models and 
make the SMT models to be more flexible. We 
evaluate our method on four state-of-the-art 
SMT systems, and get promising results not only 
on single-system decodings, but also on a system 
combination approach and a model combination 
approach. 
Making use of different distributions of trans-
lation probability features is the essential of this 
work. In the future, we will extend TMG method 
to other statistical models in SMT framework, 
(e.g. LM), which could be also suffered from the 
over-estimation problem. And we will make fur-
ther research on how to tune prior probabilities 
of models automatically as well, in order to 
make our method to be more robust and tunable. 
References 
Auli Michael, Adam Lopez, Hieu Hoang, and Philipp 
Koehn. 2009. A Systematic Analysis of Translation 
Model Search Spaces. In 4th Workshop on Statis-
tical Machine Translation, pages 224-232. 
Breiman Leo. 1996. Bagging Predictors. Machine 
Learning. 
Chiang David. 2007. Hierarchical Phrase Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
DeNero John, Shankar Kumar, Ciprian Chelba, and 
Franz Och. 2010. Model Combination for Machine 
Translation. To appear in Proc. of the North Amer-
ican Chapter of the Association for Computational 
Linguistic. 
DeNeefe Steve, Kevin Knight, Wei Wang, and Da-
niel Marcu. 2007. What Can Syntax-based MT 
Learn from Phrase-based MT? In Proc. of Empiri-
cal Methods on Natural Language Processing, 
pages 755-763. 
Foster George, Roland Kuhn, and Howard Johnson. 
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proc. of Empirical Methods 
on Natural Language Processing, pages 53-61. 
Foster George and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In 2th Workshop on 
Statistical Machine Translation, pages 128-135. 
Galley Michel, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang, and Igna-
cio Thayer. 2006. Scalable Inference and Training 
of Context-Rich Syntactic Translation Models. In 
Proc. of 44th Meeting of the Association for Com-
putational Linguistics, pages: 961-968. 
Huang Liang. 2008. Forest Reranking: Discrimina-
tive Parsing with Non-Local Features. In Proc. of 
46th Meeting of the Association for Computational 
Linguistics, pages 586-594. 
Hoeting Jennifer, David Madigan, Adrian Raftery, 
and Chris Volinsky. 1999. Bayesian Model Aver-
aging: A tutorial. Statistical Science, Vol. 14, pag-
es 382-417. 
311
He Xiaodong, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Out-
puts from Machine Translation Systems. In Proc. 
of Empirical Methods on Natural Language 
Processing, pages 98-107. 
Koehn Philipp. 2004. Phrase-based Model for SMT. 
Computational Linguistics, 28(1): 114-133. 
Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi. 
2009a. Incremental HMM Alignment for MT sys-
tem Combination. In Proc. of 47th Meeting of the 
Association for Computational Linguistics, pages 
949-957. 
Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009b. Collaborative Decoding: Par-
tial Hypothesis Re-Ranking Using Translation 
Consensus between Decoders. In Proc. of 47th 
Meeting of the Association for Computational Lin-
guistics, pages 585-592. 
Liu Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009. 
Joint Decoding with Multiple Translation Models. 
In Proc. of 47th Meeting of the Association for 
Computational Linguistics, pages 576-584. 
 
Mylonakis Markos and Khalil Sima ?an. 2008. 
Phrase Translation Probabilities with ITG Priors 
and Smoothing as Learning Objective. In Proc. of 
Empirical Methods on Natural Language 
Processing, pages 630-639. 
Matusov Evgeny, Nicola Ueffi ng, and Hermann 
Ney. 2006. Computing consensus translation from 
multiple machine translation systems using en-
hanced hypotheses alignment. In Proc. of Euro-
pean Charter of the Association for Computational 
Linguistics, pages 33-40. 
Och Franz and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. In Proc. of 38th Meeting of 
the Association for Computational Linguistics, 
pages 440-447. 
Och Franz. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of 41th 
Meeting of the Association for Computational Lin-
guistics, pages 160-167. 
Och Franz and Hermann Ney. 2004. The Alignment 
template approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(4): 417-449. 
Shen Libin, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. of 46th Meeting of the Association for 
Computational Linguistics, pages 577-585. 
Wu Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-404. 
Xiong Deyi, Qun Liu, and Shouxun Lin. 2006. Max-
imum Entropy based Phrase Reordering Model for 
Statistical Machine Translation. In Proc. of 44th 
Meeting of the Association for Computational Lin-
guistics, pages 521-528. 
Zollmann Andreas, Ashish Venugopal, Franz Och, 
and Jay Ponte. 2008. A Systematic Comparison of 
Phrase-Based, Hierarchical and Syntax-
Augmented Statistical MT. In 23rd International 
Conference on Computational Linguistics, pages 
1145-1152. 
 
 
312
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 38?42,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Learning of a Dual SMT System for Paraphrase Generation
Hong Sun?
School of Computer Science and Technology
Tianjin University
kaspersky@tju.edu.cn
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Abstract
SMT has been used in paraphrase generation
by translating a source sentence into another
(pivot) language and then back into the source.
The resulting sentences can be used as candi-
date paraphrases of the source sentence. Exist-
ing work that uses two independently trained
SMT systems cannot directly optimize the
paraphrase results. Paraphrase criteria espe-
cially the paraphrase rate is not able to be en-
sured in that way. In this paper, we propose
a joint learning method of two SMT systems
to optimize the process of paraphrase genera-
tion. In addition, a revised BLEU score (called
iBLEU ) which measures the adequacy and
diversity of the generated paraphrase sentence
is proposed for tuning parameters in SMT sys-
tems. Our experiments on NIST 2008 test-
ing data with automatic evaluation as well as
human judgments suggest that the proposed
method is able to enhance the paraphrase qual-
ity by adjusting between semantic equivalency
and surface dissimilarity.
1 Introduction
Paraphrasing (at word, phrase, and sentence levels)
is a procedure for generating alternative expressions
with an identical or similar meaning to the origi-
nal text. Paraphrasing technology has been applied
in many NLP applications, such as machine trans-
lation (MT), question answering (QA), and natural
language generation (NLG).
1This work has been done while the author was visiting Mi-
crosoft Research Asia.
As paraphrasing can be viewed as a transla-
tion process between the original expression (as in-
put) and the paraphrase results (as output), both
in the same language, statistical machine transla-
tion (SMT) has been used for this task. Quirk et al
(2004) build a monolingual translation system us-
ing a corpus of sentence pairs extracted from news
articles describing same events. Zhao et al (2008a)
enrich this approach by adding multiple resources
(e.g., thesaurus) and further extend the method by
generating different paraphrase in different applica-
tions (Zhao et al, 2009). Performance of the mono-
lingual MT-based method in paraphrase generation
is limited by the large-scale paraphrase corpus it re-
lies on as the corpus is not readily available (Zhao et
al., 2010).
In contrast, bilingual parallel data is in abundance
and has been used in extracting paraphrase (Ban-
nard and Callison-Burch, 2005; Zhao et al, 2008b;
Callison-Burch, 2008; Kok and Brockett, 2010;
Kuhn et al, 2010; Ganitkevitch et al, 2011). Thus
researchers leverage bilingual parallel data for this
task and apply two SMT systems (dual SMT system)
to translate the original sentences into another pivot
language and then translate them back into the orig-
inal language. For question expansion, Duboue? and
Chu-Carroll (2006) paraphrase the questions with
multiple MT engines and select the best paraphrase
result considering cosine distance, length, etc. Max
(2009) generates paraphrase for a given segment by
forcing the segment being translated independently
in both of the translation processes. Context features
are added into the SMT system to improve trans-
lation correctness against polysemous. To reduce
38
the noise introduced by machine translation, Zhao et
al. (2010) propose combining the results of multiple
machine translation engines? by performing MBR
(Minimum Bayes Risk) (Kumar and Byrne, 2004)
decoding on the N-best translation candidates.
The work presented in this paper belongs to
the pivot language method for paraphrase genera-
tion. Previous work employs two separately trained
SMT systems the parameters of which are tuned
for SMT scheme and therefore cannot directly op-
timize the paraphrase purposes, for example, opti-
mize the diversity against the input. Another prob-
lem comes from the contradiction between two cri-
teria in paraphrase generation: adequacy measuring
the semantic equivalency and paraphrase rate mea-
suring the surface dissimilarity. As they are incom-
patible (Zhao and Wang, 2010), the question arises
how to adapt between them to fit different applica-
tion scenarios. To address these issues, in this paper,
we propose a joint learning method of two SMT sys-
tems for paraphrase generation. The jointly-learned
dual SMT system: (1) Adapts the SMT systems so
that they are tuned specifically for paraphrase gener-
ation purposes, e.g., to increase the dissimilarity; (2)
Employs a revised BLEU score (named iBLEU , as
it?s an input-aware BLEU metric) that measures ad-
equacy and dissimilarity of the paraphrase results at
the same time. We test our method on NIST 2008
testing data. With both automatic and human eval-
uations, the results show that the proposed method
effectively balance between adequacy and dissimi-
larity.
2 Paraphrasing with a Dual SMT System
We focus on sentence level paraphrasing and lever-
age homogeneous machine translation systems for
this task bi-directionally. Generating sentential para-
phrase with the SMT system is done by first trans-
lating a source sentence into another pivot language,
and then back into the source. Here, we call these
two procedures a dual SMT system. Given an En-
glish sentence es, there could be n candidate trans-
lations in another language F , each translation could
havem candidates {e
?
}which may contain potential
paraphrases for es. Our task is to locate the candi-
date that best fit in the demands of paraphrasing.
2.1 Joint Inference of Dual SMT System
During the translation process, it is needed to select
a translation from the hypothesis based on the qual-
ity of the candidates. Each candidate?s quality can
be expressed by log-linear model considering dif-
ferent SMT features such as translation model and
language model.
When generating the paraphrase results for each
source sentence es, the selection of the best para-
phrase candidate e?? from e? ? C is performed by:
e??(es, {f}, ?M ) =
argmaxe??C,f?{f}
M?
m=1
?mhm(e
?|f)t(e?, f)(1)
where {f} is the set of sentences in pivot language
translated from es, hm is the mth feature value and
?m is the corresponding weight. t is an indicator
function equals to 1 when e? is translated from f and
0 otherwise.
The parameter weight vector ? is trained by
MERT (Minimum Error Rate Training) (Och, 2003).
MERT integrates the automatic evaluation metrics
into the training process to achieve optimal end-to-
end performance. In the joint inference method, the
feature vector of each e? comes from two parts: vec-
tor of translating es to {f} and vector of translating
{f} to e?, the two vectors are jointly learned at the
same time:
(??1, ?
?
2) = arg max
(?1,?2)
S?
s=1
G(rs, e
??(es, {f}, ?1, ?2))
(2)
where G is the automatic evaluation metric for para-
phrasing. S is the development set for training the
parameters and for each source sentence several hu-
man translations rs are listed as references.
2.2 Paraphrase Evaluation Metrics
The joint inference method with MERT enables the
dual SMT system to be optimized towards the qual-
ity of paraphrasing results. Different application
scenarios of paraphrase have different demands on
the paraphrasing results and up to now, the widely
mentioned criteria include (Zhao et al, 2009; Zhao
et al, 2010; Liu et al, 2010; Chen and Dolan, 2011;
Metzler et al, 2011): Semantic adequacy, fluency
39
and dissimilarity. However, as pointed out by (Chen
and Dolan, 2011), there is the lack of automatic met-
ric that is capable to measure all the three criteria in
paraphrase generation. Two issues are also raised
in (Zhao and Wang, 2010) about using automatic
metrics: paraphrase changes less gets larger BLEU
score and the evaluations of paraphrase quality and
rate tend to be incompatible.
To address the above problems, we propose a met-
ric for tuning parameters and evaluating the quality
of each candidate paraphrase c :
iBLEU(s, rs, c) = ?BLEU(c, rs)
? (1? ?)BLEU(c, s) (3)
where s is the input sentence, rs represents the ref-
erence paraphrases. BLEU(c, rs) captures the se-
mantic equivalency between the candidates and the
references (Finch et al (2005) have shown the ca-
pability for measuring semantic equivalency using
BLEU score); BLEU(c, s) is the BLEU score com-
puted between the candidate and the source sen-
tence to measure the dissimilarity. ? is a parameter
taking balance between adequacy and dissimilarity,
smaller ? value indicates larger punishment on self-
paraphrase. Fluency is not explicitly presented be-
cause there is high correlation between fluency and
adequacy (Zhao et al, 2010) and SMT has already
taken this into consideration. By using iBLEU , we
aim at adapting paraphrasing performance to differ-
ent application needs by adjusting ? value.
3 Experiments and Results
3.1 Experiment Setup
For English sentence paraphrasing task, we utilize
Chinese as the pivot language, our experiments are
built on English and Chinese bi-directional transla-
tion. We use 2003 NIST Open Machine Transla-
tion Evaluation data (NIST 2003) as development
data (containing 919 sentences) for MERT and test
the performance on NIST 2008 data set (containing
1357 sentences). NIST Chinese-to-English evalua-
tion data offers four English human translations for
every Chinese sentence. For each sentence pair, we
choose one English sentence e1 as source and use
the three left sentences e2, e3 and e4 as references.
The English-Chinese and Chinese-English sys-
tems are built on bilingual parallel corpus contain-
Joint learning BLEU
Self-
BLEU
iBLEU
No Joint 27.16 35.42 /
? = 1 30.75 53.51 30.75
? = 0.9 28.28 48.08 20.64
? = 0.8 27.39 35.64 14.78
? = 0.7 23.27 26.30 8.39
Table 1: iBLEU Score Results(NIST 2008)
Adequacy
(0/1/2)
Fluency
(0/1/2)
Variety
(0/1/2)
Overall
(0/1/2)
No Joint 30/82/88 22/83/95 25/117/58 23/127/50
? = 1 33/53/114 15/80/105 62/127/11 16/128/56
? = 0.9 31/77/92 16/93/91 23/157/20 20/119/61
? = 0.8 31/78/91 19/91/90 20/123/57 19/121/60
? = 0.7 35/105/60 32/101/67 9/108/83 35/107/58
Table 2: Human Evaluation Label Distribution
ing 497,862 sentences. Language model is trained
on 2,007,955 sentences for Chinese and 8,681,899
sentences for English. We adopt a phrase based MT
system of Chiang (2007). 10-best lists are used in
both of the translation processes.
3.2 Paraphrase Evaluation Results
The results of paraphrasing are illustrated in Table 1.
We show the BLEU score (computed against ref-
erences) to measure the adequacy and self-BLEU
(computed against source sentence) to evaluate the
dissimilarity (lower is better). By ?No Joint?, it
means two independently trained SMT systems are
employed in translating sentences from English to
Chinese and then back into English. This result is
listed to indicate the performance when we do not
involve joint learning to control the quality of para-
phrase results. For joint learning, results of ? from
0.7 to 1 are listed.
From the results we can see that, when the value
of ? decreases to address more penalty on self-
paraphrase, the self-BLEU score rapidly decays
while the consequence effect is that BLEU score
computed against references also drops seriously.
When ? drops under 0.6 we observe the sentences
become completely incomprehensible (this is the
reason why we leave out showing the results of ? un-
der 0.7). The best balance is achieved when ? is be-
tween 0.7 and 0.9, where both of the sentence qual-
ity and variety are relatively preserved. As ? value is
manually defined and not specially tuned, the exper-
40
Source
Torrential rains hit western india ,
43 people dead
No Joint
Rainstorms in western india ,
43 deaths
Joint(? = 1)
Rainstorms hit western india ,
43 people dead
Joint(? = 0.9)
Rainstorms hit western india
43 people dead
Joint(? = 0.8)
Heavy rain in western india ,
43 dead
Joint(? = 0.7)
Heavy rain in western india ,
43 killed
Table 3: Example of the Paraphrase Results
iments only achieve comparable results with no joint
learning when ? equals 0.8. However, the results
show that our method is able to effectively control
the self-paraphrase rate and lower down the score of
self-BLEU, this is done by both of the process of
joint learning and introducing the metric of iBLEU
to avoid trivial self-paraphrase. It is not capable with
no joint learning or with the traditional BLEU score
does not take self-paraphrase into consideration.
Human evaluation results are shown in Table 2.
We randomly choose 100 sentences from testing
data. For each setting, two annotators are asked to
give scores about semantic adequacy, fluency, vari-
ety and overall quality. The scales are 0 (meaning
changed; incomprehensible; almost same; cannot be
used), 1 (almost same meaning; little flaws; con-
taining different words; may be useful) and 2 (same
meaning; good sentence; different sentential form;
could be used). The agreements between the anno-
tators on these scores are 0.87, 0.74, 0.79 and 0.69
respectively. From the results we can see that human
evaluations are quite consistent with the automatic
evaluation, where higher BLEU scores correspond
to larger number of good adequacy and fluency la-
bels, and higher self-BLEU results tend to get lower
human evaluations over dissimilarity.
In our observation, we found that adequacy and
fluency are relatively easy to be kept especially for
short sentences. In contrast, dissimilarity is not easy
to achieve. This is because the translation tables
are used bi-directionally so lots of source sentences?
fragments present in the paraphrasing results.
We show an example of the paraphrase results
under different settings. All the results? sentential
forms are not changed comparing with the input sen-
tence and also well-formed. This is due to the short
length of the source sentence. Also, with smaller
value of ?, more variations show up in the para-
phrase results.
4 Discussion
4.1 SMT Systems and Pivot Languages
We have test our method by using homogeneous
SMT systems and a single pivot language. As the
method highly depends on machine translation, a
natural question arises to what is the impact when
using different pivots or SMT systems. The joint
learning method works by combining both of the
processes to concentrate on the final objective so it
is not affected by the selection of language or SMT
model.
In addition, our method is not limited to a ho-
mogeneous SMT model or a single pivot language.
As long as the models? translation candidates can
be scored with a log-linear model, the joint learning
process can tune the parameters at the same time.
When dealing with multiple pivot languages or het-
erogeneous SMT systems, our method will take ef-
fect by optimizing parameters from both the forward
and backward translation processes, together with
the final combination feature vector, to get optimal
paraphrase results.
4.2 Effect of iBLEU
iBLEU plays a key role in our method. The first
part of iBLEU , which is the traditional BLEU
score, helps to ensure the quality of the machine
translation results. Further, it also helps to keep
the semantic equivalency. These two roles unify the
goals of optimizing translation and paraphrase ade-
quacy in the training process.
Another contribution from iBLEU is its ability
to balance between adequacy and dissimilarity as the
two aspects in paraphrasing are incompatible (Zhao
and Wang, 2010). This is not difficult to explain be-
cause when we change many words, the meaning
and the sentence quality are hard to preserve. As
the paraphrasing task is not self-contained and will
be employed by different applications, the two mea-
sures should be given different priorities based on
the application scenario. For example, for a query
41
expansion task in QA that requires higher recall, va-
riety should be considered first. Lower ? value is
preferred but should be kept in a certain range as sig-
nificant change may lead to the loss of constraints
presented in the original sentence. The advantage
of the proposed method is reflected in its ability to
adapt to different application requirements by ad-
justing the value of ? in a reasonable range.
5 Conclusion
We propose a joint learning method for pivot
language-based paraphrase generation. The jointly
learned dual SMT system which combines the train-
ing processes of two SMT systems in paraphrase
generation, enables optimization of the final para-
phrase quality. Furthermore, a revised BLEU score
that balances between paraphrase adequacy and dis-
similarity is proposed in our training process. In the
future, we plan to go a step further to see whether
we can enhance dissimilarity with penalizing phrase
tables used in both of the translation processes.
References
Colin J. Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
EMNLP, pages 196?205.
David Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In ACL,
pages 190?200.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Pablo Ariel Duboue? and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In HLT-
NAACL.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level semantic
equivalence. In In IWP2005.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In EMNLP, pages 1168?
1179.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In HLT-NAACL, pages 145?
153.
Roland Kuhn, Boxing Chen, George F. Foster, and Evan
Stratford. 2010. Phrase clustering for smoothing
tm probabilities - or, how to extract paraphrases from
phrase tables. In COLING, pages 608?616.
Shankar Kumar and William J. Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In HLT-NAACL, pages 169?176.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Pem: A paraphrase evaluation metric exploiting paral-
lel texts. In EMNLP, pages 923?932.
Aurelien Max. 2009. Sub-sentential paraphrasing by
contextual pivot translation. In Proceedings of the
2009 Workshop on Applied Textual Inference, ACLI-
JCNLP, pages 18?26.
Donald Metzler, Eduard H. Hovy, and Chunliang Zhang.
2011. An empirical evaluation of data-driven para-
phrase generation techniques. In ACL (Short Papers),
pages 546?551.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In EMNLP, pages 142?149.
Shiqi Zhao and Haifeng Wang. 2010. Paraphrases and
applications. In COLING (Tutorials), pages 1?87.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
smt-based paraphrasing model. In ACL, pages 1021?
1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In ACL, pages 780?788.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In ACL/AFNLP, pages 834?842.
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging multiple mt engines for paraphrase
generation. In COLING, pages 1326?1334.
42
