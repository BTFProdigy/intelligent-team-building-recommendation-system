Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533?1541,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Phrase Dependency Parsing for Opinion Mining
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cn
Abstract
In this paper, we present a novel approach
for mining opinions from product reviews,
where it converts opinion mining task to
identify product features, expressions of
opinions and relations between them. By
taking advantage of the observation that a
lot of product features are phrases, a con-
cept of phrase dependency parsing is in-
troduced, which extends traditional depen-
dency parsing to phrase level. This con-
cept is then implemented for extracting re-
lations between product features and ex-
pressions of opinions. Experimental eval-
uations show that the mining task can ben-
efit from phrase dependency parsing.
1 Introduction
As millions of users contribute rich information
to the Internet everyday, an enormous number of
product reviews are freely written in blog pages,
Web forums and other consumer-generated medi-
ums (CGMs). This vast richness of content be-
comes increasingly important information source
for collecting and tracking customer opinions. Re-
trieving this information and analyzing this con-
tent are impossible tasks if they were to be manu-
ally done. However, advances in machine learning
and natural language processing present us with
a unique opportunity to automate the decoding of
consumers? opinions from online reviews.
Previous works on mining opinions can be di-
vided into two directions: sentiment classification
and sentiment related information extraction. The
former is a task of identifying positive and neg-
ative sentiments from a text which can be a pas-
sage, a sentence, a phrase and even a word (So-
masundaran et al, 2008; Pang et al, 2002; Dave
et al, 2003; Kim and Hovy, 2004; Takamura et
al., 2005). The latter focuses on extracting the el-
ements composing a sentiment text. The elements
include source of opinions who expresses an opin-
ion (Choi et al, 2005); target of opinions which
is a receptor of an opinion (Popescu and Etzioni,
2005); opinion expression which delivers an opin-
ion (Wilson et al, 2005b). Some researchers refer
this information extraction task as opinion extrac-
tion or opinion mining. Comparing with the for-
mer one, opinion mining usually produces richer
information.
In this paper, we define an opinion unit as a
triple consisting of a product feature, an expres-
sion of opinion, and an emotional attitude(positive
or negative). We use this definition as the basis for
our opinion mining task. Since a product review
may refer more than one product feature and ex-
press different opinions on each of them, the rela-
tion extraction is an important subtask of opinion
mining. Consider the following sentences:
1. I highly [recommend]
(1)
the Canon SD500
(1)
to
anybody looking for a compact camera that can take
[good]
(2)
pictures
(2)
.
2. This camera takes [amazing]
(3)
image qualities
(3)
and its size
(4)
[cannot be beat]
(4)
.
The phrases underlined are the product features,
marked with square brackets are opinion expres-
sions. Product features and opinion expressions
with identical superscript compose a relation. For
the first sentence, an opinion relation exists be-
tween ?the Canon SD500? and ?recommend?, but
not between ?picture? and ?recommend?. The ex-
ample shows that more than one relation may ap-
pear in a sentence, and the correct relations are not
simple Cartesian product of opinion expressions
and product features.
Simple inspection of the data reveals that prod-
uct features usually contain more than one word,
such as ?LCD screen?, ?image color?, ?Canon
PowerShot SD500?, and so on. An incomplete
product feature will confuse the successive anal-
ysis. For example, in passage ?Image color is dis-
1533
appointed?, the negative sentiment becomes ob-
scure if only ?image? or ?color? is picked out.
Since a product feature could not be represented
by a single word, dependency parsing might not be
the best approach here unfortunately, which pro-
vides dependency relations only between words.
Previous works on relation extraction usually use
the head word to represent the whole phrase and
extract features from the word level dependency
tree. This solution is problematic because the in-
formation provided by the phrase itself can not be
used by this kind of methods. And, experimental
results show that relation extraction task can ben-
efit from dependencies within a phrase.
To solve this issue, we introduce the concept
of phrase dependency parsing and propose an ap-
proach to construct it. Phrase dependency pars-
ing segments an input sentence into ?phrases? and
links segments with directed arcs. The parsing
focuses on the ?phrases? and the relations be-
tween them, rather than on the single words inside
each phrase. Because phrase dependency parsing
naturally divides the dependencies into local and
global, a novel tree kernel method has also been
proposed.
The remaining parts of this paper are organized
as follows: In Section 2 we discuss our phrase de-
pendency parsing and our approach. In Section 3,
experiments are given to show the improvements.
In Section 4, we present related work and Section
5 concludes the paper.
2 The Approach
Fig. 1 gives the architecture overview for our ap-
proach, which performs the opinion mining task
in three main steps: (1) constructing phrase de-
pendency tree from results of chunking and de-
pendency parsing; (2) extracting candidate prod-
uct features and candidate opinion expressions; (3)
extracting relations between product features and
opinion expressions.
2.1 Phrase Dependency Parsing
2.1.1 Overview of Dependency Grammar
Dependency grammar is a kind of syntactic the-
ories presented by Lucien Tesni`ere(1959). In de-
pendency grammar, structure is determined by the
relation between a head and its dependents. In
general, the dependent is a modifier or comple-
ment; the head plays a more important role in de-
termining the behaviors of the pair. Therefore, cri-
Phrase Dependency Parsing  
Review Crawler 
Review Database
 Chunking DependencyParsing
  
CandidateProduct FeaturesIdentification
CandidateOpinion ExpressionsExtraction
Relation ExtractionOpinionDatabase
Phrase Dependency Tree
Figure 1: The architecture of our approach.
teria of how to establish dependency relations and
how to distinguish the head and dependent in such
relations is central problem for dependency gram-
mar. Fig. 2(a) shows the dependency represen-
tation of an example sentence. The root of the
sentence is ?enjoyed?. There are seven pairs of
dependency relationships, depicted by seven arcs
from heads to dependents.
2.1.2 Phrase Dependency Parsing
Currently, the mainstream of dependency parsing
is conducted on lexical elements: relations are
built between single words. A major informa-
tion loss of this word level dependency tree com-
pared with constituent tree is that it doesn?t ex-
plicitly provide local structures and syntactic cat-
egories (i.e. NP, VP labels) of phrases (Xia and
Palmer, 2001). On the other hand, dependency
tree provides connections between distant words,
which are useful in extracting long distance rela-
tions. Therefore, compromising between the two,
we extend the dependency tree node with phrases.
That implies a noun phrase ?Cannon SD500 Pow-
erShot? can be a dependent that modifies a verb
phrase head ?really enjoy using? with relation type
?dobj?. The feasibility behind is that a phrase is a
syntactic unit regardless of the length or syntac-
tic category (Santorini and Kroch, 2007), and it is
acceptable to substitute a single word by a phrase
with same syntactic category in a sentence.
Formally, we define the dependency parsing
with phrase nodes as phrase dependency parsing.
A dependency relationship which is an asymmet-
ric binary relationship holds between two phrases.
One is called head, which is the central phrase in
the relation. The other phrase is called dependent,
which modifies the head. A label representing the
1534
enjoyed
We nsubj reallyadvmod using
partmod
SD500
thedet Canon PowerShotnn nn
dobj
enjoyed
nsubj really using
partmod
We 
VP
NP SD500
the
det
Canon PowerShot
nn nn
NP
advmod
dobj
(a)
(c)(b)
  NP SEGMENT:      [We] VP SEGMENT:      [really]      [enjoyed ]      [using] NP SEGMENT:      [the]      [Canon]      [PowerShot]      [SD500]
Figure 2: Example of Phrase Dependency Parsing.
relation type is assigned to each dependency rela-
tionship, such as subj (subject), obj (object), and
so on. Fig.2(c) shows an example of phrase de-
pendency parsing result.
By comparing the phrase dependency tree and
the word level dependency tree in Fig.2, the for-
mer delivers a more succinct tree structure. Local
words in same phrase are compacted into a sin-
gle node. These words provide local syntactic and
semantic effects which enrich the phrase they be-
long to. But they should have limited influences on
the global tree topology, especially in applications
which emphasis the whole tree structures, such as
tree kernels. Pruning away local dependency re-
lations by additional phrase structure information,
phrase dependency parsing accelerates following
processing of opinion relation extraction .
To construct phrase dependency tree, we pro-
pose a method which combines results from an
existing shallow parser and a lexical dependency
parser. A phrase dependency tree is defined as
T = (V ,E ), where V is the set of phrases,
E is the dependency relations among the phrases
in V representing by direct edges. To reserve
the word level dependencies inside a phrase, we
define a nested structure for a phrase T
i
in V :
T
i
= (V
i
, E
i
). V
i
= {v
1
, v
2
, ? ? ? , v
m
} is the inter-
nal words, E
i
is the internal dependency relations.
We conduct the phrase dependency parsing in
this way: traverses word level dependency tree
in preorder (visits root node first, then traverses
the children recursively). When visits a node R,
searches in its children and finds the node set D
which are in the same phrase with R according
Algorithm 1 Pseudo-Code for constructing the
phrase dependency tree
INPUT:
T
?
= (V
?
, E
?
) a word level dependency tree
P = phrases
OUTPUT:
phrase dependency tree T = (V , E ) where
V = {T
1
(V
1
, E
1
), T
2
(V
2
, E
2
), ? ? ? , T
n
(V
n
, E
n
)}
Initialize:
V ? {({v
?
}, {})|v
?
? V
?
}
E ? {(T
i
, T
j
)|(v
?
i
, v
?
j
) ? E
?
, v
?
i
? V
i
, v
?
j
? V
j
}
R = (V
r
, E
r
) root of T
PhraseDPTree(R, P )
1: Find p
i
? P where word[R] ? p
i
2: for each S = (V
s
, E
s
), (R,S) ? E do
3: if word[S] ? p
i
then
4: V
r
? V
r
? v
s
; v
s
? V
s
5: E
r
? E
r
? (v
r
, root[S]); v
r
? V
r
6: V ? V ? S
7: E ? E + (R, l); ?(S, l) ? E
8: E ? E ? (R,S)
9: end if
10: end for
11: for each (R,S) ? E do
12: PhraseDPTree(S,P )
13: end for
14: return (V , E )
to the shallow parsing result. Compacts D and R
into a single node. Then traverses all the remain-
ing children in the same way. The algorithm is
shown in Alg. 1.
The output of the algorithm is still a tree, for we
only cut edges which are compacted into a phrase,
the connectivity is keeped. Note that there will be
inevitable disagrees between shallow parser and
lexical dependency parser, the algorithm implies
that we simply follow the result of the latter one:
the phrases from shallow parser will not appear in
the final result if they cannot be found in the pro-
cedure.
Consider the following example:
?We really enjoyed using the Canon PowerShot SD500.?
Fig.2 shows the procedure of phrase depen-
dency parsing. Fig.2(a) is the result of the lex-
ical dependency parser. Shallow parsers result
is shown in Fig.2(b). Chunk phrases ?NP(We)?,
?VP(really enjoyed using)? and ?NP(the Canon
PowerShot SD500)? are nodes in the output phrase
dependency tree. When visiting node ?enjoyed? in
Fig.2(a), the shallow parser tells that ?really? and
?using? which are children of ?enjoy? are in the
same phrase with their parent, then the three nodes
are packed. The final phrase dependency parsing
tree is shown in the Fig. 2(c).
1535
2.2 Candidate Product Features and Opinion
Expressions Extraction
In this work, we define that product features
are products, product parts, properties of prod-
ucts, properties of parts, company names and re-
lated objects. For example,in consumer elec-
tronic domain, ?Canon PowerShot?, ?image qual-
ity?,?camera?, ?laptop? are all product features.
From analyzing the labeled corpus, we observe
that more than 98% of product features are in a
single phrase, which is either noun phrase (NP) or
verb phrase (VP). Based on it, all NPs and VPs
are selected as candidate product features. While
prepositional phrases (PPs) and adjectival phrases
(ADJPs) are excluded. Although it can cover
nearly all the true product features, the precision
is relatively low. The large amount of noise can-
didates may confuse the relation extraction clas-
sifier. To shrink the size of candidate set, we in-
troduce language model by an intuition that the
more likely a phrase to be a product feature, the
more closely it related to the product review. In
practice, for a certain domain of product reviews,
a language model is build on easily acquired unla-
beled data. Each candidate NP or VP chunk in the
output of shallow parser is scored by the model,
and cut off if its score is less than a threshold.
Opinion expressions are spans of text that ex-
press a comment or attitude of the opinion holder,
which are usually evaluative or subjective phrases.
We also analyze the labeled corpus for opinion ex-
pressions and observe that many opinion expres-
sions are used in multiple domains, which is iden-
tical with the conclusion presented by Kobayashi
et al (2007). They collected 5,550 opinion ex-
pressions from various sources . The coverage of
the dictionary is high in multiple domains. Moti-
vated by those observations, we use a dictionary
which contains 8221 opinion expressions to select
candidates (Wilson et al, 2005b). An assump-
tion we use to filter candidate opinion expressions
is that opinion expressions tend to appear closely
with product features, which is also used to extract
product features by Hu and Liu (2004). In our ex-
periments, the tree distance between product fea-
ture and opinion expression in a relation should be
less than 5 in the phrase dependency parsing tree.
2.3 Relation Extraction
This section describes our method on extracting
relations between opinion expressions and product
features using phrase dependency tree. Manually
built patterns were used in previous works which
have an obvious drawback that those patterns can
hardly cover all possible situations. By taking ad-
vantage of the kernel methods which can search a
feature space much larger than that could be repre-
sented by a feature extraction-based approach, we
define a new tree kernel over phrase dependency
trees and incorporate this kernel within an SVM to
extract relations between opinion expressions and
product features.
The potential relation set consists of the all
combinations between candidate product features
and candidate opinion expressions in a sentence.
Given a phrase dependency parsing tree, we
choose the subtree rooted at the lowest common
parent(LCP) of opinion expression and product
feature to represent the relation.
Dependency tree kernels has been proposed by
(Culotta and Sorensen, 2004). Their kernel is de-
fined on lexical dependency tree by the convolu-
tion of similarities between all possible subtrees.
However, if the convolution containing too many
irrelevant subtrees, over-fitting may occur and de-
creases the performance of the classifier. In phrase
dependency tree, local words in a same phrase are
compacted, therefore it provides a way to treat ?lo-
cal dependencies? and ?global dependencies? dif-
ferently (Fig. 3). As a consequence, these two
kinds of dependencies will not disturb each other
in measuring similarity. Later experiments prove
the validity of this statement.
B
A C
D
E
B
A
C
D E
Phrase Local dependencies
Global dependencies
Figure 3: Example of ?local dependencies? and
?global dependencies?.
We generalize the definition by (Culotta and
Sorensen, 2004) to fit the phrase dependency tree.
Use the symbols in Section 2.1.2, T
i
and T
j
are
two trees with root R
i
and R
j
, K(T
i
,T
j
) is the
kernel function for them. Firstly, each tree node
T
k
? T
i
is augmented with a set of features F ,
and an instance of F for T
k
is F
k
= {f
k
}. A
match function m(T
i
, T
j
) is defined on comparing
a subset of nodes? features M ? F . And in the
same way, a similarity function s(T
i
, T
j
) are de-
1536
fined on S ? F
m(T
i
, T
j
) =
{
1 if f
i
m
= f
j
m
?f
m
? M
0 otherwise
(1)
and
s(T
i
, T
j
) =
?
f
s
?S
C(f
i
s
, f
j
s
) (2)
where
C(f
i
s
, f
j
s
) =
{
1 if f
i
s
= f
j
s
0 otherwise
(3)
For the given phrase dependency parsing trees,
the kernel function K(T
i
,T
j
) is defined as fol-
low:
K(T
i
,T
j
) =
?
?
?
?
?
0 if m(R
i
, R
j
) = 0
s(R
i
, R
j
) +K
in
(R
i
, R
j
)
+K
c
(R
i
.C, R
j
.C) otherwise
(4)
where K
in
(R
i
, R
j
) is a kernel function over
R
i
= (V
i
r
, E
i
r
) and R
j
= (V
j
r
, E
j
r
)?s internal
phrase structures,
K
in
(R
i
, R
j
) = K(R
i
, R
j
) (5)
K
c
is the kernel function over R
i
and R
j
?s chil-
dren. Denote a is a continuous subsequence of in-
dices a, a+1, ? ? ? a+ l(a) for R
i
?s children where
l(a) is its length, a
s
is the s-th element in a. And
likewise b for R
j
.
K
c
(R
i
.C, R
j
.C) =
?
a,b,l(a)=l(b)
?
l(a)
K(R
i
.[a], R
j
.[b])
?
?
s=1..l(a)
m(R
i
.[a
s
], R
j
.[b
s
])
(6)
where the constant 0 < ? < 1 normalizes the ef-
fects of children subsequences? length.
Compared with the definitions in (Culotta and
Sorensen, 2004), we add term K
in
to handle the
internal nodes of a pharse, and make this exten-
sion still satisfy the kernel function requirements
(composition of kernels is still a kernel (Joachims
et al, 2001)). The consideration is that the local
words should have limited effects on whole tree
structures. So the kernel is defined on external
children (K
c
) and internal nodes (K
in
) separately,
Table 1: Statistics for the annotated corpus
Category # Products # Sentences
Cell Phone 2 1100
Diaper 1 375
Digital Camera 4 1470
DVD Player 1 740
MP3 Player 3 3258
as the result, the local words are not involved in
subsequences of external children for K
c
. After
the kernel computing through training instances,
support vector machine (SVM) is used for classi-
fication.
3 Experiments and Results
In this section, we describe the annotated corpus
and experiment configurations including baseline
methods and our results on in-domain and cross-
domain.
3.1 Corpus
We conducted experiments with labeled corpus
which are selected from Hu and Liu (2004), Jin-
dal and Liu (2008) have built. Their documents
are collected from Amazon.com and CNet.com,
where products have a large number of reviews.
They also manually labeled product features and
polarity orientations. Our corpus is selected
from them, which contains customer reviews of
11 products belong to 5 categories(Diaper, Cell
Phone, Digital Camera, DVD Player, and MP3
Player). Table 1 gives the detail statistics.
Since we need to evaluate not only the prod-
uct features but also the opinion expressions and
relations between them, we asked two annotators
to annotate them independently. The annotators
started from identifying product features. Then for
each product feature, they annotated the opinion
expression which has relation with it. Finally, one
annotator A
1
extracted 3595 relations, while the
other annotator A
2
extracted 3745 relations, and
3217 cases of them matched. In order to measure
the annotation quality, we use the following metric
to measure the inter-annotator agreement, which is
also used by Wiebe et al (2005).
agr(a||b) =
|A matches B|
|A|
1537
Table 2: Results for extracting product features
and opinion expressions
P R F
Product Feature 42.8% 85.5% 57.0%
Opinion Expression 52.5% 75.2% 61.8%
Table 3: Features used in SVM-1: o denotes an
opinion expression and t a product feature
1) Positions of o/t in sentence(start, end, other);
2) The distance between o and t (1, 2, 3, 4, other);
3) Whether o and t have direct dependency relation;
4) Whether o precedes t;
5) POS-Tags of o/t.
where agr(a||b) represents the inter-annotator
agreement between annotator a and b, A and B
are the sets of anchors annotated by annotators a
and b. agr(A
1
||A
2
) was 85.9% and agr(A
2
||A
1
)
was 89.5%. It indicates that the reliability of our
annotated corpus is satisfactory.
3.2 Preprocessing Results
Results of extracting product features and opin-
ion expressions are shown in Table 2. We use
precision, recall and F-measure to evaluate perfor-
mances. The candidate product features are ex-
tracted by the method described in Section 2.2,
whose result is in the first row. 6760 of 24414
candidate product features remained after the fil-
tering, which means we cut 72% of irrelevant can-
didates with a cost of 14.5%(1-85.5%) loss in true
answers. Similar to the product feature extraction,
the precision of extracting opinion expression is
relatively low, while the recall is 75.2%. Since
both product features and opinion expressions ex-
tractions are preprocessing steps, recall is more
important.
3.3 Relation Extraction Experiments
3.3.1 Experiments Settings
In order to compare with state-of-the-art results,
we also evaluated the following methods.
1. Adjacent method extracts relations between a
product feature and its nearest opinion expression,
which is also used in (Hu and Liu, 2004).
2. SVM-1. To compare with tree kernel based
Table 4: Features used in SVM-PTree
Features for match function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) Whether it is an opinion expression node
3) Whether it is a product future node.
Features for similarity function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) POS-Tag of the head word of node?s internal
phrases.
3) The type of phrase dependency edge linking
to node?s parent.
4) Feature 2) for the node?s parent
5) Feature 3) for the node?s parent
approaches, we evaluated an SVM
1
result with a
set of manually selected features(Table 3), which
are also used in (Kobayashi et al, 2007).
3. SVM-2 is designed to compare the effective-
ness of cross-domain performances. The features
used are simple bag of words and POS-Tags be-
tween opinion expressions and product features.
4. SVM-WTree uses head words of opinion ex-
pressions and product features in the word-level
dependency tree, as the previous works in infor-
mation extraction. Then conducts tree kernel pro-
posed by Culotta and Sorensen (2004).
5. SVM-PTree denotes the results of our tree-
kernel based SVM, which is described in the Sec-
tion 2.3. Stanford parser (Klein and Manning,
2002) and Sundance (Riloff and Phillips, 2004)
are used as lexical dependency parser and shallow
parser. The features in match function and simi-
larity function are shown in Table 4.
6. OERight is the result of SVM-PTree with
correct opinion expressions.
7. PFRight is the result of SVM-PTree with
correct product features.
Table 5 shows the performances of different
relation extraction methods with in-domain data.
For each domain, we conducted 5-fold cross val-
idation. Table 6 shows the performances of the
extraction methods on cross-domain data. We use
the digital camera and cell phone domain as train-
ing set. The other domains are used as testing set.
1
libsvm 2.88 is used in our experiments
1538
Table 5: Results of different methods
Cell Phone MP3 Player Digital Camera DVD Player Diaper
Methods P R F P R F P R F P R F P R F
Adjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1%
SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0%
SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9%
SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0%
SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6%
OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1%
PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4%
Table 6: Results for total performance with cross domain training data
Diaper DVD Player MP3 Player
Methods P R F P R F P R F
Adjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7%
SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4%
SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6%
SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2%
SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8%
3.3.2 Results Discussion
Table 5 presents different methods? results in five
domains. We observe that the three learning based
methods(SVM-1, SVM-WTree, SVM-PTree) per-
form better than the Adjacent baseline in the first
three domains. However, in other domains, di-
rectly adjacent method is better than the learning
based methods. The main difference between the
first three domains and the last two domains is the
size of data(Table 1). It implies that the simple Ad-
jacent method is also competent when the training
set is small.
A further inspection into the result of first 3
domains, we can also conclude that: 1) Tree
kernels(SVM-WTree and SVM-PTree) are better
than Adjacent, SVM-1 and SVM-2 in all domains.
It proofs that the dependency tree is important
in the opinion relation extraction. The reason
for that is a connection between an opinion and
its target can be discovered with various syntac-
tic structures. 2) The kernel defined on phrase
dependency tree (SVM-PTree) outperforms ker-
nel defined on word level dependency tree(SVM-
WTree) by 4.8% in average. We believe the main
reason is that phrase dependency tree provides a
more succinct tree structure, and the separative
treatment of local dependencies and global depen-
dencies in kernel computation can indeed improve
the performance of relation extraction.
To analysis the results of preprocessing steps?
influences on the following relation extraction,
we provide 2 additional experiments which the
product features and opinion expressions are all
correctly extracted respectively: OERight and
PFRight. These two results show that given an
exactly extraction of opinion expression and prod-
uct feature, the results of opinion relation extrac-
tion will be much better. Further, opinion expres-
sions are more influential which naturally means
the opinion expressions are crucial in opinion re-
lation extraction.
For evaluations on cross domain, the Adjacent
method doesn?t need training data, its results are
the same as the in-domain experiments. Note
in Table 3 and Table 4, we don?t use domain
related features in SVM-1, SVM-WTree, SVM-
PTree, but SVM-2?s features are domain depen-
dent. Since the cross-domain training set is larger
than the original one in Diaper and DVD domain,
the models are trained more sufficiently. The fi-
nal results on cross-domain are even better than
in-domain experiments on SVM-1, SVM-WTree,
and SVM-PTree with percentage of 4.6%, 8.6%,
10.3% in average. And the cross-domain train-
ing set is smaller than in-domain in MP3, but
it also achieve competitive performance with the
1539
in-domain. On the other hand, SVM-2?s result
decreased compared with the in-domain experi-
ments because the test domain changed. At the
same time, SVM-PTree outperforms other meth-
ods which is similar in in-domain experiments.
4 Related Work
Opinion mining has recently received consider-
able attention. Amount of works have been
done on sentimental classification in different lev-
els (Zhang et al, 2009; Somasundaran et al, 2008;
Pang et al, 2002; Dave et al, 2003; Kim and
Hovy, 2004; Takamura et al, 2005). While we
focus on extracting product features, opinion ex-
pressions and mining relations in this paper.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. Subject and aspect
belong to product features, while evaluation is the
opinion expression in our work. They converted
the task to two kinds of relation extraction tasks
and proposed a machine learning-based method
which combines contextual clues and statistical
clues. Their experimental results showed that the
model using contextual clues improved the perfor-
mance. However since the contextual information
in a domain is specific, the model got by their ap-
proach can not easily converted to other domains.
Choi et al (2006) used an integer linear pro-
gramming approach to jointly extract entities and
relations in the context of opinion oriented infor-
mation extraction. They identified expressions of
opinions, sources of opinions and the linking re-
lation that exists between them. The sources of
opinions denote to the person or entity that holds
the opinion.
Another area related to our work is opinion
expressions identification (Wilson et al, 2005a;
Breck et al, 2007). They worked on identify-
ing the words and phrases that express opinions
in text. According to Wiebe et al (2005), there are
two types of opinion expressions, direct subjective
expressions and expressive subjective elements.
5 Conclusions
In this paper, we described our work on min-
ing opinions from unstructured documents. We
focused on extracting relations between product
features and opinion expressions. The novelties
of our work included: 1) we defined the phrase
dependency parsing and proposed an approach
to construct the phrase dependency trees; 2) we
proposed a new tree kernel function to model
the phrase dependency trees. Experimental re-
sults show that our approach improved the perfor-
mances of the mining task.
6 Acknowledgement
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302. The authors would like to thank the
reviewers for their useful comments.
References
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings EMNLP.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In In Proceed-
ings of ACL 2004.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the ACM
SIGKDD 2004.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of WSDM ?08.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML ?01.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004. COLING.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
1540
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proc. of EMNLP
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP.
E. Riloff and W. Phillips. 2004. An introduction to
the sundance and autoslog systems. In University of
Utah School of Computing Technical Report UUCS-
04-015.
Beatrice Santorini and Anthony Kroch. 2007.
The syntax of natural language: An on-
line introduction using the Trees program.
http://www.ling.upenn.edu/ beatrice/syntax-
textbook.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING 2008.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of ACL?05.
L. Tesni`ere. 1959. El?ements de syntaxe structurale.
Editions Klincksieck.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjectiv-
ity analysis. In Demonstration Description in Con-
ference on Empirical Methods in Natural Language
Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT ?01:
Proceedings of the first international conference on
Human language technology research.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of SIGIR 2009.
1541
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 904?912,
Beijing, August 2010
2D Trie for Fast Parsing
Xian Qian, Qi Zhang, Xuanjing Huang, Lide Wu
Institute of Media Computing
School of Computer Science, Fudan University
{xianqian, qz, xjhuang, ldwu}@fudan.edu.cn
Abstract
In practical applications, decoding speed
is very important. Modern structured
learning technique adopts template based
method to extract millions of features.
Complicated templates bring about abun-
dant features which lead to higher accu-
racy but more feature extraction time. We
propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure
which takes advantage of relationship be-
tween templates: feature strings generated
by a template are prefixes of the features
from its extended templates. We apply
our technique to Maximum Spanning Tree
dependency parsing. Experimental results
on Chinese Tree Bank corpus show that
our 2D Trie is about 5 times faster than
traditional Trie structure, making parsing
speed 4.3 times faster.
1 Introduction
In practical applications, decoding speed is very
important. Modern structured learning technique
adopts template based method to generate mil-
lions of features. Such as shallow parsing (Sha
and Pereira, 2003), named entity recognition
(Kazama and Torisawa, ), dependency parsing
(McDonald et al, 2005), etc.
The problem arises when the number of tem-
plates increases, more features generated, mak-
ing the extraction step time consuming. Espe-
cially for maximum spanning tree (MST) depen-
dency parsing, since feature extraction requires
quadratic time even using a first order model. Ac-
cording to Bohnet?s report (Bohnet, 2009), a fast
FeatureGenerationTemplate:p .word+p .pos0 0 Feature:lucky/ADJ
Index:3228~3233
FeatureRetrievalParse Tree
Build lattice, inference etc.
Figure 1: Flow chart of dependency parsing.
p0.word, p0.pos denotes the word and POS tag
of parent node respectively. Indexes correspond
to the features conjoined with dependency types,
e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc.
feature extraction beside of a fast parsing algo-
rithm is important for the parsing and training
speed. He takes 3 measures for a 40X speedup,
despite the same inference algorithm. One impor-
tant measure is to store the feature vectors in file
to skip feature extraction, otherwise it will be the
bottleneck.
Now we quickly review the feature extraction
stage of structured learning. Typically, it consists
of 2 steps. First, features represented by strings
are generated using templates. Then a feature in-
dexing structure searches feature indexes to get
corresponding feature weights. Figure 1 shows
the flow chart of MST parsing, where p0.word,
p0.pos denote the word and POS tag of parent
node respectively.
We conduct a simple experiment to investi-
gate decoding time of MSTParser, a state-of-the-
art java implementation of dependency parsing 1.
Chinese Tree Bank 6 (CTB6) corpus (Palmer and
1http://sourceforge.net/projects/mstparser
904
Step Feature Index Other Total
Generation Retrieval
Time 300.27 61.66 59.48 421.41
Table 1: Time spent of each step (seconds) of
MSTParser on CTB6 standard test data (2660 sen-
tences). Details of the hardware and corpus are
described in section 5
Xue, 2009) with standard train/development/test
split is used for evaluation. Experimental results
are shown in Table 1. The observation is that time
spent of inference is trivial compared with feature
extraction. Thus, speeding up feature extraction is
critical especially when large template set is used
for high accuracy.
General indexing structure such as Hash and
Trie does not consider the relationships between
templates, therefore they could not speed up fea-
ture generation, and are not completely efficient
for searching feature indexes. For example, fea-
ture string s1 generated by template ?p0.word?
is prefix of feature s2 from template ?p0.word +
c0.word? (word pair of parent and child), hence
index of s1 could be used for searching s2. Fur-
ther more, if s1 is not in the feature set, then s2
must be absent, its generation can be skipped.
We propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure which
takes advantage of relationship between tem-
plates. We apply our technique to Maximum
Spanning Tree dependency parsing. Experimental
results on CTB6 corpus show that our 2D Trie is
about 5 times faster than traditional Trie structure,
making parsing speed 4.3 times faster.
The paper is structured as follows: in section 2,
we describe template tree which represents rela-
tionship between templates; in section 3, we de-
scribe our new 2D Trie structure; in section 4, we
analyze the complexity of the proposed method
and general string indexing structures for parsing;
experimental results are shown in section 5; we
conclude the work in section 6.
2 Template tree
2.1 Formulation of template
A template is a set of template units which are
manually designed: T = {t1, . . . , tm}. For con-
Unit Meaning
p?i/pi the ith node left/right to parent node
c?i/ci the ith node left/right to child node
r?i/ri the ith node left/right to root node
n.word word of node n
n.pos POS tag of node n
n.length word length of node n
|l conjoin current feature with linear distance
between child node and parent node
|d conjoin current feature with direction of de-
pendency (left/right)
Table 2: Template units appearing in this paper
venience, we use another formulation: T = t1 +
. . .+tm. All template units appearing in this paper
are described in Table 2, most of them are widely
used. For example, ?T = p0.word + c0.word|l ?
denotes the word pair of parent and child nodes,
conjoined with their distance.
2.2 Template tree
In the rest of the paper, for simplicity, let si be a
feature string generated by template Ti.
We define the relationship between templates:
T1 is the ancestor of T2 if and only T1 ? T2, and
T2 is called the descendant of T1. Recall that,
feature string s1 is prefix of feature s2. Suppose
T3 ? T1 ? T2, obviously, the most efficient way
to look up indexes of s1, s2, s3 is to search s3 first,
then use its index id3 to search s1, and finally use
id1 to search s2. Hence the relationship between
T2 and T3 can be neglected.
Therefore we define direct ancestor of T1: T2
is a direct ancestor of T1 if T2 ? T1, and there is
no template T ? such that T2 ? T ? ? T1. Corre-
spondingly, T1 is called the direct descendant of
T2.
Template graph G = (V,E) is a directed graph
that represents the relationship between templates,
where V = {T1, . . . , Tn} is the template set, E =
{e1, . . . , eN} is the edge set. Edge from Ti to Tj
exists, if and only if Ti is the direct ancestor of
Tj . For templates having no ancestor, we add an
empty template as their common direct ancestor,
which is also the root of the graph.
The left part of Figure 2 shows a template
graph for templates T1 =p0.word, T2 =p0.pos ,
T3 =p0.word + p0.pos. In this example, T3 has 2
direct ancestors, but in fact s3 has only one prefix
905
p .word0
p .word +p pos0 0.
root
p .word0
root
p .pos0
p .pos0 p .pos0
Figure 2: Left graph shows template graph for
T1 =p0.word, T2 =p0.pos , T3 =p0.word +
p0.pos. Right graph shows the corresponding tem-
plate tree, where each vertex saves the subset of
template units that do not belong to its father
which depends on the order of template units in
generation step. If s3 = s1 + s2, then its prefix is
s1, otherwise its prefix is s2. In this paper, we sim-
ply use the breadth-first tree of the graph for dis-
ambiguation, which is called template tree. The
only direct ancestor T1 of T2 in the tree is called
father of T2, and T2 is a child of T1. The right
part of Figure 2 shows the corresponding template
tree, where each vertex saves the subset of tem-
plate units that do not belong to its father.
2.3 Virtual vertex
Consider the template tree in the left part of Figure
3, red vertex and blue vertex are partially over-
lapped, their intersection is p0.word, if string s
from template T =p0.word is absent in feature set,
then both nodes can be neglected. For efficiently
pruning candidate templates, each vertex in tem-
plate tree is restricted to have exactly one template
unit (except root). Another important reason for
such restriction will be given in the next section.
To this end, virtual vertexes are created for
multi-unit vertexes. For efficient pruning, the new
virtual vertex should extract the most common
template unit. A natural goal is to minimize the
creation number. Here we use a simple greedy
strategy, for the vertexes sharing a common fa-
ther, the most frequent common unit is extracted
as new vertex. Virtual vertexes are iteratively cre-
ated in this way until all vertexes have one unit.
The final template tree is shown in the right part of
Figure 3, newly created virtual vertexes are shown
in dashed circle.
root
p .word+p .word+p .word-1 01
p .word+p pos0 0. c .word+c pos0 0.
root
p .word0
p .pos0 p .word-1
p .word1
c .word0
c .pos0
Figure 3: Templates that are partially overlapped:
Tred ? Tblue =p0.word, virtual vertexes shown in
dashed circle are created to extract the common
unit
root
p .word0
p .pos0
parse tag
VV NN... ... ... ...
.........
Level 0
Level 1
Level 2 VV ...
Figure 4: 2D Trie for single template, alphabets at
level 1 and level 2 are the word set, POS tag set
respectively
3 2D Trie
3.1 Single template case
Trie stores strings over a fixed alphabet, in our
case, feature strings are stored over several alpha-
bets, such as word list, POS tag list, etc. which are
extracted from training corpus.
To illustrate 2D Trie clearly, we first consider a
simple case, where only one template used. The
template tree degenerates to a sequence, we could
use a Trie like structure for feature indexing, the
only difference from traditional Trie is that nodes
at different levels could have different alphabets.
One example is shown in Figure 4. There are 3
feature strings from template ?p0.word + p0.pos?:
{parse/VV, tag/VV, tag/VV}. Alphabets at level
1 and level 2 are the word set, POS tag set re-
spectively, which are determined by correspond-
ing template vertexes.
As mentioned before, each vertex in template
tree has exactly one template unit, therefore, at
each level, we look up an index of a word or POS
906
HehadbeenasalesandmarketingexecutivewithChryslerfor20years
PRPVBDVBNDTNNSCCNNNNINNNPINCDNNS
2648273111210411506406313374192360231560220300566778
21272804130112120613060214
Figure 5: Look up indexes of words and POS tags
beforehand.
tag in sentence, not their combinations. Hence the
number of alphabets is limited, and all the indexes
could be searched beforehand for reuse, as shown
in Figure 5, the token table is converted to a in-
dex table. For example, when generating features
at position i of a sentence, template ?r0.word +
r1.word? requires index of i+1th word in the sen-
tence, which could be reused for generation at po-
sition i+ 1.
3.2 General case
Generally, for vertex in template tree with K chil-
dren, children of corresponding Trie node are ar-
ranged in a matrix of K rows and L columns, L
is the size of corresponding alphabet. If the vertex
is not virtual, i.e., it generates features, one more
row is added at the bottom to store feature indexes.
Figure 6 shows the 2D Trie for a general template
tree.
3.3 Feature extraction
When extracting features for a pair of nodes in a
sentence, template tree and 2D Trie are visited in
breath first traversal order. Each time, an alpha-
bet and a token index j from index table are se-
lected according to current vertex. For example,
POS tag set and the index of the POS tag of par-
ent node are selected as alphabet and token index
respectively for vertex ?p0.pos?. Then children in
the jth column of the Trie node are visited, valid
children and corresponding template vertexes are
saved for further retrieval or generate feature in-
dexes if the child is at the bottom and current Trie
node is not virtual. Two queues are maintained to
been......
... ......
...
VBNp .word+p .pos?been/VBN0 0...
... ......
p .word?been0... ...
root
root
p .word0p .pos0 c .word0
had ......
...
p .word?had0 ...
VBDp .word+p .pos?had/VBD0 0...
... ......
Hep .word+w .wordhad/He0 0?...
...
nmod vmodobj sub
Featureindex array-1 -13327 2510
nmod vmodobj sub-1 7821-1 -1............ ......
...... beenp .word+w .word?had/been0 0 ...
...
invalid
Figure 6: 2D trie for a general template tree.
Dashed boxes are keys of columns, which are not
stored in the structure
save the valid children and Trie nodes. Details of
feature extraction algorithm are described in Al-
gorithm 1.
3.4 Implementation
When feature set is very large, space complexity
of 2D Trie is expensive. Therefore, we use Double
Array Trie structure (Aoe, 1989) for implementa-
tion. Since children of 2D Trie node are arranged
in a matrix, not an array, so each element of the
base array has a list of bases, not one base in stan-
dard structure. For children that store features,
corresponding bases are feature indexes. One ex-
ample is shown in Figure 7. The root node has
3 bases that point to three rows of the child ma-
trix of vertex ?p0.word? respectively. Number of
bases in each element need not to be stored, since
it can be obtained from template vertex in extrac-
tion procedure.
Building algorithm is similarly to Double Array
Trie, when inserting a Trie node, each row of the
child matrix is independently insert into base and
check arrays using brute force strategy. The inser-
907
been
... been
...
...
...
had
...
had had...
... ...
...
...
been ... had had... ... ... ...... ... been hadroot base1 base2
base3
root base 2base
1
base3
...
VBD... ...
VBN...
...
...VBD
VBN
...
base1
base1 base1
base1
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
Basearray
Feature index array
Feature index array
Figure 7: Build base array for 2D Trie in Figure 6. String in the box represents the key of the child.
Blank boxes are the invalid children. The root node has 3 bases that point to three rows of the child
matrix of vertex ?p0.word? respectively
Algorithm 1 Feature extraction using 2D Trie
Input: 2D Trie that stores features, template
tree, template graph, a table storing token in-
dexes, parent and child positions
Output: Feature index set S of dependency
from parent to child.
Create template vertex queue Q1 and Trie
node queue Q2. Push roots of template tree
and Trie into Q1, Q2 respectively. S = ?
while Q1 is not empty, do
Pop a template vertex T from Q1 and a Trie
node N from Q2. Get token index j from
index table according to T .
for i = 1 to child number of T
if child of N at row i column j is valid,
push it into Q2 and push the ith child
of T into Q1.
else
remove decedents of ith child of T
from template tree
end if
end for
if T is not virtual and the last child of N in
column j is valid
Enumerate dependency types, add
valid feature indexes to S
end if
end while
Return S.
tion repeats recursively until all features stored.
4 Complexity analysis
Let
? |T | = number of templates
? |t| = number of template units
? |V | = number of vertexes in template tree,
i.e, |t|+ number of virtual vertexes
? |F | = number of features
? l = length of sentence
? |f | = average length of feature strings
The procedure of 2D Trie for feature extraction
consists of 2 steps: tokens in string table are
mapped to their indexes, then Algorithm 1 is car-
ried out for all node pairs of sentence. In the first
step, we use double array Trie for efficient map-
ping. In fact, time spent is trivial compared with
step 2 even by binary search. The main time spent
of Algorithm 1 is the traversal of the whole tem-
plate tree, in the worst case, no vertexes removed,
so the time complexity of a sentence is l2|V |,
which is proportional to |V |. In other words, mini-
mizing the number of virtual vertexes is important
for efficiency.
For other indexing structures, feature genera-
tion is a primary step of retrieval. For each node
908
Structure Generation Retrieval
2D Trie l2|V |
Hash / Trie l2|t| l2|f ||T |
Binary Search l2|t| l2|T | log |F |
Table 3: Time complexity of different indexing
structures.
pair of sentence, |t| template units are processed,
including concatenations of tokens and split sym-
bols (split tokens in feature strings), boundary
check ( e.g, p?1.word is out of boundary for be-
ginning node of sentence). Thus the generation
requires l2|t| processes. Notice that, time spent of
each process varies on the length of tokens.
For feature string s with length |s|, if perfect
hashing technique is adopted for index retrieval, it
takes |s| calculations to get hash value and a string
comparison to check the string at the calculated
position. So the time complexity is proportional to
|s|, which is the same as Trie. Hence the total time
for a sentence is l2|f ||T |. If binary search is used
instead, log |F | string comparisons are required,
complexity for a sentence is l2|T | log |F |.
Time complexity of these structures is summa-
rized in Table 3.
5 Experiments
5.1 Experimental settings
We use Chinese Tree Bank 6.0 corpus for evalua-
tion. The constituency structures are converted to
dependency trees by Penn2Malt 2 toolkit and the
standard training/development/test split is used.
257 sentences that failed in the conversion were
removed, yielding 23316 sentences for training,
2060 sentences for development and 2660 sen-
tences for testing respectively.
Since all the dependency trees are projective,
a first order projective MST parser is naturally
adopted. Online Passive Aggressive algorithm
(Crammer et al, 2006) is used for fast training, 2
parameters, i.e, iteration number and C, are tuned
on development data. The quality of the parser is
measured by the labeled attachment score (LAS),
i.e., the percentage of tokens with correct head and
dependency type.
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
Group IDs #Temp. #Vert. #Feat. LAS
1 1-2 72 91 3.23M 79.55%
2 1-3 128 155 10.4M 81.38%
3 1-4 240 275 25.0M 81.97%
4 1-5 332 367 34.8M 82.44%
Table 5: Parsing accuracy and number of tem-
plates, vertexes in template tree, features in decod-
ing stage (zero weighted features are excluded) of
each group.
We compare the proposed structure with Trie
and binary search. We do not compare with per-
fect hashing, because it has the same complex-
ity as Trie, and is often used for large data base
retrieval, since it requires only one IO opera-
tion. For easy comparison, all feature indexing
structures and the parser are implemented with
C++. All experiments are carried out on a 64bit
linux platform (CPU: Intel(R) Xeon(R) E5405,
2.00GHz, Memory: 16G Bytes). For each tem-
plate set, we run the parser five times on test data
and the averaged parsing time is reported.
5.2 Parsing speed comparison
To investigate the scalability of our method, rich
templates are designed to generate large feature
sets, as shown in Table 4. All templates are orga-
nized into 4 groups. Each row of Table 5 shows
the details of a group, including parsing accu-
racy and number of templates, vertexes in tem-
plate tree, and features in decoding stage (zero
weighted features are excluded).
There is a rough trend that parsing accuracy
increases as more templates used. Though such
trend is not completely correct, the clear conclu-
sion is that, abundant templates are necessary for
accurate parsing.
Though algorithm described in section 2.3 for
minimizing the number of virtual vertexes is
heuristic, empirical results are satisfactory, num-
ber of newly created vertexes is only 10% as orig-
inal templates. The reason is that complex tem-
plates are often extended from simple ones, their
differences are often one or two template units.
Results of parsing time comparison are shown
in Table 6. We can see that though time com-
plexity of dynamic programming is cubic, pars-
ing time of all systems is consistently dominated
909
ID Templates
1 pi.word pi.pos pi.word+pi.pos
ci.word ci.pos ci.word+ci.pos (|i| ? 2)
pi.length pi.length+pi.pos
ci.length ci.length+ci.pos (|i| ? 1)
p0.length+c0.length|ld p0.length+c0.length+c0.pos|ld p0.length+p0.pos+c0.length|ld
p0.length+p0.pos+c0.pos|ld p0.pos+c0.length+c0.pos|ld p0.length+p0.pos+c0.length+c0.pos|ld
pi.length+pj .length+ck.length+cm.length|ld (|i|+ |j|+ |k|+ |m| ? 2)r0.word r?1.word+r0.word r0.word+r1.word
r0.pos r?1.pos+r0.pos r0.pos+r1.pos
2 pi.pos+cj .pos|d pi.word+cj .word|d pi.pos+cj .word+cj .pos|d
pi.word+pi.pos+cj .pos|d pi.word+pi.pos+cj .word|d pi.word+cj .word+cj .pos|d
pi.word+pi.pos+cj .word+cj .pos|d (|i|+ |j| = 0)
Conjoin templates in the row above with |l
3 Similar with 2 |i|+ |j| = 1
4 Similar with 2 |i|+ |j| = 2
5 pi.word + pj .word + ck.word|d pi.word + cj .word + ck.word|d
pi.pos + pj .pos + ck.pos|d pi.pos + cj .pos + ck.pos|d (|i|+ |j|+ |k| ? 2)
Conjoin templates in the row above with |l
pi.word + pj .word + pk.word + cm.word|d pi.word + pj .word + ck.word + cm.word|d
pi.word + cj .word + ck.word + cm.word|d
pi.pos + pj .pos + pk.pos + cm.pos|d pi.pos + pj .pos + ck.pos + cm.pos|d
pi.pos + cj .pos + ck.pos + cm.pos|d (|i|+ |j|+ |k|+ |m| ? 2)
Conjoin templates in the row above with |l
Table 4: Templates used in Chinese dependency parsing.
by feature extraction. When efficient indexing
structure adopted, i.e, Trie or Hash, time index re-
trieval is greatly reduced, about 4-5 times faster
than binary search. However, general structures
search features independently, their results could
not guide feature generation. Hence, feature gen-
eration is still time consuming. The reason is that
processing each template unit includes a series of
steps, much slower than one integer comparison
in Trie search.
On the other hand, 2D Trie greatly reduces the
number of feature generations by pruning the tem-
plate graph. In fact, no string concatenation oc-
curs when using 2D Trie, since all tokens are con-
verted to indexes beforehand. The improvement
is significant, 2D Trie is about 5 times faster than
Trie on the largest feature set, yielding 13.4 sen-
tences per second parsing speed, about 4.3 times
faster.
Space requirement of 2D Trie is about 2.1 times
as binary search, and 1.7 times as Trie. One possi-
ble reason is that column number of 2D Trie (e.g.
size of words) is much larger than standard double
array Trie, which has only 256 children, i.e, range
of a byte. Therefore, inserting a 2D Trie node is
more strict, yielding sparser double arrays.
5.3 Comparison against state-of-the-art
Recent works on dependency parsing speedup
mainly focus on inference, such as expected
linear time non-projective dependency parsing
(Nivre, 2009), integer linear programming (ILP)
for higher order non-projective parsing (Martins
et al, 2009). They achieve 0.632 seconds per sen-
tence over several languages. On the other hand,
Goldberg and Elhadad proposed splitSVM (Gold-
berg and Elhadad, 2008) for fast low-degree poly-
nomial kernel classifiers, and applied it to transi-
tion based parsing (Nivre, 2003). They achieve
53 sentences per second parsing speed on En-
glish corpus, which is faster than our results, since
transition based parsing is linear time, while for
graph based method, complexity of feature ex-
traction is quadratic. Xavier Llu??s et al (Llu??s
et al, 2009) achieve 8.07 seconds per sentence
speed on CoNLL09 (Hajic? et al, 2009) Chinese
Tree Bank test data with a second order graphic
model. Bernd Bohnet (Bohnet, 2009) also uses
second order model, and achieves 610 minutes on
CoNLL09 English data (2399 sentences, 15.3 sec-
ond per sentence). Although direct comparison
of parsing time is difficult due to the differences
in data, models, hardware and implementations,
910
Group Structure Total Generation Retrieval Other Memory sent/sec
Trie 87.39 63.67 10.33 13.39 402M 30.44
1 Binary Search 127.84 62.68 51.52 13.64 340M 20.81
2D Trie 39.74 26.29 13.45 700M 66.94
Trie 264.21 205.19 39.74 19.28 1.3G 10.07
2 Binary Search 430.23 212.50 198.72 19.01 1.2G 6.18
2D Trie 72.81 53.95 18.86 2.5G 36.53
Trie 620.29 486.40 105.96 27.93 3.2G 4.29
3 Binary Search 982.41 484.62 469.44 28.35 2.9G 2.71
2D Trie 146.83 119.56 27.27 5.9G 18.12
Trie 854.04 677.32 139.70 37.02 4.9G 3.11
4 Binary Search 1328.49 680.36 609.70 38.43 4.1G 2.00
2D Trie 198.31 160.38 37.93 8.6G 13.41
Table 6: Parsing time of 2660 sentences (seconds) on a 64bit linux platform (CPU: Intel(R) Xeon(R)
E5405, 2.00GHz, Memory: 16G Bytes). Title ?Generation? and ?Retrieval? are short for feature gen-
eration and feature index retrieval steps respectively.
System sec/sent
(Martins et al, 2009) 0.63
(Goldberg and Elhadad, 2008) 0.019
(Llu??s et al, 2009) 8.07
(Bohnet, 2009) 15.3
(Galley and Manning, 2009) 15.6
ours group1 0.015
ours group2 0.027
ours group3 0.055
ours group4 0.075
Table 7: Comparison against state of the art, di-
rect comparison of parsing time is difficult due to
the differences in data, models, hardware and im-
plementations.
these results demonstrate that our structure can
actually result in a very fast implementation of a
parser. Moreover, our work is orthogonal to oth-
ers, and could be used for other learning tasks.
6 Conclusion
We proposed 2D Trie, a novel feature indexing
structure for fast template based feature extrac-
tion. The key insight is that feature strings gener-
ated by a template are prefixes of the features from
its extended templates, hence indexes of searched
features can be reused for further extraction. We
applied 2D Trie to dependency parsing task, ex-
perimental results on CTB corpus demonstrate the
advantages of our technique, about 5 times faster
than traditional Trie structure, yielding parsing
speed 4.3 times faster, while using only 1.7 times
as much memory.
7 Acknowledgments
The author wishes to thank the anonymous
reviewers for their helpful comments. This
work was partially funded by 973 Program
(2010CB327906), The National High Technol-
ogy Research and Development Program of China
(2009AA01A346), Shanghai Leading Academic
Discipline Project (B114), Doctoral Fund of Min-
istry of Education of China (200802460066), and
Shanghai Science and Technology Development
Funds (08511500302).
References
Aoe, Jun?ichi. 1989. An efficient digital
search algorithm by using a double-array struc-
ture. IEEE Transactions on software andengineer-
ing, 15(9):1066?1077.
Bohnet, Bernd. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task, pages 67?72, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Crammer, Koby, Joseph Keshet, Shai Shalev-Shwartz,
and Yoram Singer. 2006. Online passive-aggressive
algorithms. In JMLR 2006.
911
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773?781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Goldberg, Yoav and Michael Elhadad. 2008. splitsvm:
Fast, space-efficient, non-heuristic, polynomial ker-
nel computation for nlp applications. In Proceed-
ings of ACL-08: HLT, Short Papers, pages 237?240,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Kazama, Jun?ichi and Kentaro Torisawa. A new per-
ceptron algorithm for sequence labeling with non-
local features. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 315?324.
Llu??s, Xavier, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 79?84, Boulder, Colorado, June. Association
for Computational Linguistics.
Martins, Andre, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342?
350, Suntec, Singapore, August. Association for
Computational Linguistics.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 91?97. Association for Computa-
tional Linguistics.
Nivre, Joakim. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
the 11th International Conference on Parsing Tech-
niques, pages 149?160.
Nivre, Joakim. 2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 351?359, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Palmer, Martha and Nianwen Xue. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 134?141,
May.
912
