Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 333?338,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Document-level Automatic MT Evaluation
based on Discourse Representations
Jesu?s Gime?nez and
Llu??s Ma`rquez
TALP UPC
Barcelona, Spain
{jgimenez, lluism}
@lsi.upc.edu
Elisabet Comelles and
Irene Castello?n
Universitat de Barcelona
Barcelona, Spain
{elicomelles,
icastellon} @ub.edu
Victoria Arranz
ELDA/ELRA
Paris, France
arranz@elda.org
Abstract
This paper describes the joint submission of
Universitat Polite`cnica de Catalunya and Uni-
versitat de Barcelona to the Metrics MaTr
2010 evaluation challenge, in collaboration with
ELDA/ELRA. Our work is aimed at widening the
scope of current automatic evaluation measures
from sentence to document level. Preliminary ex-
periments, based on an extension of the metrics by
Gime?nez and Ma`rquez (2009) operating over dis-
course representations, are presented.
1 Introduction
Current automatic similarity measures for Ma-
chine Translation (MT) evaluation operate all,
without exception, at the segment level. Trans-
lations are analyzed on a segment-by-segment1
fashion, ignoring the text structure. Document
and system scores are obtained using aggregate
statistics over individual segments. This strategy
presents the main disadvantage of ignoring cross-
sentential/discursive phenomena.
In this work we suggest widening the scope
of evaluation methods. We have defined genuine
document-level measures which are able to ex-
ploit the structure of text to provide more informed
evaluation scores. For that purpose we take advan-
tage of two coincidental facts. First, test beds em-
ployed in recent MT evaluation campaigns include
a document structure grouping sentences related
to the same event, story or topic (Przybocki et al,
2008; Przybocki et al, 2009; Callison-Burch et al,
2009). Second, we count on automatic linguistic
processors which provide very detailed discourse-
level representations of text (Curran et al, 2007).
Discourse representations allow us to focus on
relevant pieces of information, such as the agent
1A segment typically consists of one or two sentences.
(who), location (where), time (when), and theme
(what), which may be spread all over the text.
Counting on a means of discerning the events, the
individuals taking part in each of them, and their
role, is crucial to determine the semantic equiva-
lence between a reference document and a candi-
date translation.
Moreover, the discourse analysis of a document
is not a mere concatenation of the analyses of its
individual sentences. There are some phenom-
ena which may go beyond the scope of a sen-
tence and can only be explained within the con-
text of the whole document. For instance, in a
newspaper article, facts and entities are progres-
sively added to the discourse and then referred
to anaphorically later on. The following extract
from the development set illustrates the impor-
tance of such a phenomenon in the discourse anal-
ysis: ?Among the current or underlying crises in
the Middle East, Rod Larsen mentioned the Arab-
Israeli conflict and the Iranian nuclear portfolio,
as well as the crisis between Lebanon and Syria.
He stated: ?All this leads us back to crucial val-
ues and opinions, which render the situation prone
at any moment to getting out of control, more so
than it was in past days.??. The subject pronoun
?he? works as an anaphoric pronoun whose an-
tecedent is the proper noun ?Rod Larson?. The
anaphoric relation established between these two
elements can only be identified by analyzing the
text as a whole, thus considering the gender agree-
ment between the third person singular masculine
subject pronoun ?he? and the masculine proper
noun ?Rod Larson?. However, if the two sen-
tences were analyzed separately, the identification
of this anaphoric relation would not be feasible
due to the lack of connection between the two ele-
ments. Discourse representations allow us to trace
links across sentences between the different facts
and entities appearing in them. Therefore, provid-
ing an approach to the text more similar to that of
333
a human, which implies taking into account the
whole text structure instead of considering each
sentence separately.
The rest of the paper is organized as follows.
Section 2 describes our evaluation methods and
the linguistic theory upon which they are based.
Experimental results are reported and discussed in
Section 3. Section 4 presents the metric submitted
to the evaluation challenge. Future work is out-
lined in Section 5.
As an additional result, document-level metrics
generated in this study have been incorporated to
the IQMT package for automatic MT evaluation2.
2 Metric Description
This section provides a brief description of our ap-
proach. First, in Section 2.1, we describe the un-
derlying theory and give examples on its capabili-
ties. Then, in Section 2.2, we describe the associ-
ated similarity measures.
2.1 Discourse Representations
As previously mentioned in Section 1, a document
has some features which need to be analyzed con-
sidering it as a whole instead of dividing it up
into sentences. The anaphoric relation between
a subject pronoun and a proper noun has already
been exemplified. However, this is not the only
anaphoric relation which can be found inside a
text, there are some others which are worth men-
tioning:
? the connection between a possessive adjec-
tive and a proper noun or a subject pro-
noun, as exemplified in the sentences ?Maria
bought a new sweater. Her new sweater is
blue.?, where the possessive feminine adjec-
tive ?her? refers to the proper noun ?Maria?.
? the link between a demonstrative pronoun
and its referent, which is exemplified in the
sentences ?He developed a new theory on
grammar. However, this is not the only the-
ory he developed?. In the second sentence,
the demonstrative pronoun ?this? refers back
to the noun phrase ?new theory on grammar?
which occurs in the previous sentence.
? the relation between a main verb and an aux-
iliary verb in certain contexts, as illustrated in
the following pair of sentences ?Would you
2http://www.lsi.upc.edu/
?
nlp/IQMT
like more sugar? Yes, I would?. In this ex-
ample, the auxiliary verb ?would? used in
the short answer substitutes the verb phrase
?would like?.
In addition to anaphoric relations, other features
need to be highlighted, such as the use of discourse
markers which help to give cohesion to the text,
link parts of a discourse and show the relations es-
tablished between them. Below, some examples
are given:
? ?Moreover?, ?Furthermore?, ?In addition?
indicate that the upcoming sentence adds
more information.
? ?However?, ?Nonetheless?, ?Nevertheless?
show contrast with previous ideas.
? ?Therefore?, ?As a result?, ?Consequently?
show a cause and effect relation.
? ?For instance?, ?For example? clarify or il-
lustrate the previous idea.
It is worth noticing that anaphora, as well as dis-
course markers, are key features in the interface
between syntax, semantics and pragmatics. Thus,
when dealing with these phenomena at a text level
we are not just looking separately at the different
language levels, but we are trying to give a com-
plete representation of both the surface and the
deep structures of a text.
2.2 Definition of Similarity Measures
In this work, as a first proposal, instead of elabo-
rating on novel similarity measures, we have bor-
rowed and extended the Discourse Representation
(DR) metrics defined by Gime?nez and Ma`rquez
(2009). These metrics analyze similarities be-
tween automatic and reference translations by
comparing their respective discourse representa-
tions over individual sentences.
For the discursive analysis of texts, DR met-
rics rely on the C&C Tools (Curran et al, 2007),
specifically on the Boxer component (Bos, 2008).
This software is based on the Discourse Represen-
tation Theory (DRT) by Kamp and Reyle (1993).
DRT is a theoretical framework offering a rep-
resentation language for the examination of con-
textually dependent meaning in discourse. A dis-
course is represented in a discourse representation
structure (DRS), which is essentially a variation of
first-order predicate calculus ?its forms are pairs
334
of first-order formulae and the free variables that
occur in them.
DRSs are viewed as semantic trees, built
through the application of two types of DRS con-
ditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
For instance, the DRS representation for the
sentence ?Every man loves Mary.? is as follows:
?y named(y,mary, per) ? (?x man(x) ?
?z love(z) ? event(z) ? agent(z, x) ?
patient(z, y)). DR integrates three different
kinds of metrics:
DR-STM These metrics are similar to the Syntac-
tic Tree Matching metric defined by Liu and
Gildea (2005), in this case applied to DRSs
instead of constituent trees. All semantic sub-
paths in the candidate and reference trees are
retrieved. The fraction of matching subpaths
of a given length (l=4 in our experiments) is
computed.
DR-Or(?) Average lexical overlap between dis-
course representation structures of the same
type. Overlap is measured according to the
formulae and definitions by Gime?nez and
Ma`rquez (2007).
DR-Orp(?) Average morphosyntactic overlap,
i.e., between grammatical categories ?parts-
of-speech? associated to lexical items, be-
tween discourse representation structures of
the same type.
We have extended these metrics to operate at
document level. For that purpose, instead of run-
ning the C&C Tools in a sentence-by-sentence
fashion, we run them document by document.
This is as simple as introducing a ?<META>? tag
at the beginning of each document to denote doc-
ument boundaries3 .
3Details on the advanced use of Boxer are avail-
able at http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/BoxerComplex.
3 Experimental Work
In this section, we analyze the behavior of the new
DR metrics operating at document level with re-
spect to their sentence-level counterparts.
3.1 Settings
We have used the ?mt06? part of the development
set provided by the Metrics MaTr 2010 organiza-
tion, which corresponds to a subset of 25 docu-
ments from the NIST 2006 Open MT Evaluation
Campaign Arabic-to-English translation. The to-
tal number of segments is 249. The average num-
ber of segments per document is, thus, 9.96. The
number of segments per document varies between
2 and 30. For the purpose of automatic evaluation,
4 human reference translations and automatic out-
puts by 8 different MT systems are available. In
addition, we count on the results of a process of
manual evaluation. Each translation segment was
assessed by two judges. After independently and
completely assessing the entire set, the judges re-
viewed their individual assessments together and
settled on a single final score. Average system ad-
equacy is 5.38.
In our experiments, metrics are evaluated in
terms of their correlation with human assess-
ments. We have computed Pearson, Spearman
and Kendall correlation coefficients between met-
ric scores and adequacy assessments. Document-
level and system-level assessments have been ob-
tained by averaging over segment-level assess-
ments. We have computed correlation coefficients
and confidence intervals applying bootstrap re-
sampling at a 99% statistical significance (Efron
and Tibshirani, 1986; Koehn, 2004). Since the
cost of exhaustive resampling was prohibitive, we
have limited to 1,000 resamplings. Confidence in-
tervals, not shown in the tables, are in all cases
lower than 10?3.
3.2 Metric Performance
Table 1 shows correlation coefficients at the docu-
ment level for several DR metric representatives,
and their document-level counterparts (DRdoc).
For the sake of comparison, the performance of
the METEOR metric is also reported4.
Contrary to our expectations, DRdoc variants
obtain lower levels of correlation than their DR
4We have used METEOR version 1.0 with default param-
eters optimized by its developers over adequacy and fluency
assessments. The METEOR metric is publicly available at
http://www.cs.cmu.edu/
?
alavie/METEOR/
335
Metric Pearson? Spearman? Kendall?
METEOR 0.9182 0.8478 0.6728
DR-Or(?) 0.8567 0.8061 0.6193
DR-Orp(?) 0.8286 0.7790 0.5875
DR-STM 0.7880 0.7468 0.5554
DRdoc-Or(?) 0.7936 0.7784 0.5875
DRdoc-Orp(?) 0.7219 0.6737 0.4929
DRdoc-STM 0.7553 0.7421 0.5458
Table 1: Meta-evaluation results at document level
Metric Pearson? Spearman? Kendall?
METEOR 0.9669 0.9151 0.8533
DR-Or(?) 0.9100 0.6549 0.5764
DR-Orp(?) 0.9471 0.7918 0.7261
DR-STM 0.9295 0.7676 0.7165
DRdoc-Or(?) 0.9534 0.8434 0.7828
DRdoc-Orp(?) 0.9595 0.9101 0.8518
DRdoc-STM 0.9676 0.9655 0.9272
DR-Or(?)? 0.9836 0.9594 0.9296
DR-Orp(?)? 0.9959 1.0000 1.0000
DR-STM? 0.9933 0.9634 0.9307
Table 2: Meta-evaluation results at system level
counterparts. There are three different factors
which could provide a possible explanation for
this negative result. First, the C&C Tools, like any
other automatic linguistic processor are not per-
fect. Parsing errors could be causing the metric
to confer less informed scores. This is especially
relevant taking into account that candidate transla-
tions are not always well-formed. Secondly, we
argue that the way in which we have obtained
document-level quality assessments, as an average
of segment-level assessments, may be biasing the
correlation. Thirdly, perhaps the similarity mea-
sures employed are not able to take advantage of
the document-level features provided by the dis-
course analysis. In the following subsection we
show some error analysis we have conducted by
inspecting particular cases.
Table 2 shows correlation coefficients at system
level. In the case of DR and DRdoc metrics, sys-
tem scores are computed by simple average over
individual documents. Interestingly, in this case
DRdoc variants seem to obtain higher correlation
than their DR counterparts. The improvement is
especially substantial in terms of Spearman and
Kendall coefficients, which do not consider ab-
solute values but ranking positions. However, it
could be the case that it was just an average ef-
fect. While DR metrics compute system scores as
an average of segment scores, DRdoc metrics av-
erage directly document scores. In order to clarify
this result, we have modified DR metrics so as to
compute system scores as an average of document
scores (DR? variants, the last three rows in the ta-
ble). It can be observed that DR? variants out-
perform their DRdoc counterparts, thus confirming
our suspicion about the averaging effect.
3.3 Analysis
It is worth noting that DRdoc metrics are able to
detect and deal with several linguistic phenomena
related to both syntax and semantics at sentence
and document level. Below, several examples il-
lustrating the potential of this metric are presented.
Control structures. Control structures (either
subject or object control) are always a
difficult issue as they mix both syntactic and
semantic knowledge. In Example 1 a couple
of control structures must be identified
and DRdoc metrics deal correctly with the
argument structure of all the verbs involved.
Thus, in the first part of the sentence, a
subject control verb can be identified being
?the minister? the agent of both verb forms
?go? and ?say?. On the other hand, in the
336
quoted question, the verb ?invite? works as
an object control verb because its patient
?Chechen representatives? is also the agent
of the verb visit.
Example 1: The minister went on to say,
?What would Moscow say if we were to invite
Chechen representatives to visit Jerusalem??
Anaphora and pronoun resolution. Whenever
there is a pronoun whose antecedent is a
named entity (NE), the metric identifies
correctly its antecedent. This feature is
highly valuable because a relationship be-
tween syntax and semantics is established.
Moreover, when dealing with Semantic
Roles the roles of Agent or Patient are given
to the antecedents instead of the pronouns.
Thus, in Example 2 the antecedent of the
relative pronoun ?who? is the NE ?Putin?
and the patient of the verb ?classified? is
also the NE ?Putin? instead of the relative
pronoun ?who?.
Example 2: Putin, who was not classified
as his country Hamas as ?terrorist organiza-
tions?, recently said that the European Union
is ?a big mistake? if it decided to suspend fi-
nancial aid to the Palestinians.
Nevertheless, although Boxer was expected
to deal with long-distance anaphoric relations
beyond the sentence, after analyzing several
cases, results show that it did not succeed in
capturing this type of relations as shown in
Example 3. In this example, the antecedent
of the pronoun ?he? in the second sentence
is the NE ?Roberto Calderoli? which ap-
pears in the first sentence. DRdoc metrics
should be capable of showing this connec-
tion. However, although the proper noun
?Roberto Calderoli? is identified as a NE, it
does not share the same reference as the third
person singular pronoun ?he?.
Example 3: Roberto Calderoli does not in-
tend to apologize. The newspaper Corriere
Della Sera reported today, Saturday, that
he said ?I don?t feel responsible for those
deaths.?
4 Our Submission
Instead of participating with individual metrics,
we have combined them by averaging their scores
as described in (Gime?nez and Ma`rquez, 2008).
This strategy has proven as an effective means of
combining the scores conferred by different met-
rics (Callison-Burch et al, 2008; Callison-Burch
et al, 2009). Metrics submitted are:
DRdoc an arithmetic mean over a heuristically-
defined set of DRdoc metric variants, respec-
tively computing lexical overlap, morphosyn-
tactic overlap, and semantic tree match-
ing (M = {?DRdoc-Or(?)?, ?DRdoc-Orp(?)?, ?DRdoc-
STM4?}). Since DRdoc metrics do not operate
over individual segments, we have assigned
each segment the score of the document in
which it is contained.
DR a measure analog to DRdoc but using the de-
fault version of DR metrics operating at the
segment level (M = {?DR-Or(?)?, ?DR-Orp(?)?,
?DR-STM4?}).
ULCh an arithmetic mean over a heuristically-
defined set of metrics operating at differ-
ent linguistic levels, including lexical met-
rics, and measures of overlap between con-
stituent parses, dependency parses, seman-
tic roles, and discourse representations (M =
{?ROUGEW ?, ?METEOR?, ?DP-HWCr?, ?DP-Oc(?)?,
?DP-Ol(?)?, ?DP-Or(?)?, ?CP-STM4?, ?SR-Or(?)?,
?SR-Orv?, ?DR-Orp(?)?}). This metric corre-
sponds exactly to the metric submitted in our
previous participation.
The performance of these metrics at the docu-
ment and system levels is shown in Table 3.
5 Conclusions and Future Work
We have presented a modified version of the DR
metrics by Gime?nez and Ma`rquez (2009) which,
instead of limiting their scope to the segment level,
are able to capture and exploit document-level fea-
tures. However, results in terms of correlation
with human assessments have not reported any im-
provement of these metrics over their sentence-
level counterparts as document and system quality
predictors. It must be clarified whether the prob-
lem is on the side of the linguistic tools, in the
similarity measure, or in the way in which we have
built document-level human assessments.
For future work, we plan to continue the er-
ror analysis to clarify why DRdoc metrics do not
outperform their DR counterparts at the document
level, and how to improve their behavior. This
337
Document level System level
Metric Pearson? Spearman? Kendall? Pearson? Spearman? Kendall?
ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145
ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435
ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638
Table 3: Meta-evaluation results at document and system level for submitted metrics
may imply defining new metrics possibly using
alternative linguistic processors. In addition, we
plan to work on the identification and analysis
of discourse markers. Finally, we plan to repeat
this experiment over other test beds with docu-
ment structure, such as those from the 2009 Work-
shop on Statistical Machine Translation shared
task (Callison-Burch et al, 2009) and the 2009
NIST MT Evaluation Campaign (Przybocki et al,
2009). In the case that document-level assess-
ments are not provided, we will also explore the
possibility of producing them ourselves.
Acknowledgments
This work has been partially funded by the
Spanish Government (projects OpenMT-2,
TIN2009-14675-C03, and KNOW, TIN-2009-
14715-C0403) and the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement numbers 247762 (FAUST
project, FP7-ICT-2009-4-247762) and 247914
(MOLTO project, FP7-ICT-2009-4-247914). We
are also thankful to anonymous reviewers for their
comments and suggestions.
References
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 33?36.
Bradley Efron and Robert Tibshirani. 1986. Bootstrap
Methods for Standard Errors, Confidence Intervals,
and Other Measures of Statistical Accuracy. Statis-
tical Science, 1(1):54?77.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A Smor-
gasbord of Features for Automatic MT Evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Dordrecht: Kluwer.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388?395.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2008. NIST Metrics for Machine Translation
2008 Evaluation (MetricsMATR08). Technical re-
port, National Institute of Standards and Technol-
ogy.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2009. NIST Open Machine Translation 2009
Evaluation (MT09). Technical report, National In-
stitute of Standards and Technology.
338
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 368?375,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
VERTa participation in the WMT14 Metrics Task 
 
 
Elisabet Comelles 
Universitat de Barcelona 
Barcelona, Spain 
elicomelles@ub.edu 
Jordi Atserias 
Yahoo! Labs 
Barcelona, Spain 
jordi@yahoo-inc.com 
 
 
Abstract 
In this paper we present VERTa, a lin-
guistically-motivated metric that com-
bines linguistic features at different lev-
els. We provide the linguistic motivation 
on which the metric is based, as well as 
describe the different modules in VERTa 
and how they are combined. Finally, we 
describe the two versions of VERTa, 
VERTa-EQ and VERTa-W, sent to 
WMT14 and report results obtained in 
the experiments conducted with the 
WMT12 and WMT13 data into English. 
1 Introduction 
In the Machine Translation (MT) process, the 
evaluation of MT systems plays a key role both 
in their development and improvement. From the 
MT metrics that have been developed during the 
last decades, BLEU (Papineni et al., 2002) is one 
of the most well-known and widely used, since it 
is fast and easy to use. Nonetheless, researchers 
such as (Callison-Burch et al., 2006) and (Lavie 
and Dekowski, 2009) have claimed its weak-
nesses regarding translation quality and its ten-
dency to favour statistically-based MT systems. 
As a consequence, other more complex metrics 
that use linguistic information have been devel-
oped. Some use linguistic information at lexical 
level, such as METEOR (Denkowski and Lavie, 
2011); others rely on syntactic information, ei-
ther using constituent (Liu and Hildea, 2005) or 
dependency analysis (Owczarzack et al., 2007a 
and 2007b; He et al., 2010); others use more 
complex information such as semantic roles 
(Gim?nez and M?rquez, 2007 and 2008a; Lo et 
al., 2012). All these metrics focus on partial as-
pects of language; however, other researchers 
have tried to combine information at different 
linguistic levels in order to follow a more holistic 
approach. Some of these metrics follow a ma-
chine-learning approach (Leusch and Ney, 2009; 
Albrecht and Hwa, 2007a and 2007b), others 
combine a wide variety of metrics in a simple 
and straightforward way (Gim?nez, 2008b; 
Gim?nez and M?rquez, 2010; Specia and Gim?-
nez, 2010). However, very little research has 
been performed on the impact of the linguistic 
features used and how to combine this informa-
tion from a linguistic point of view. Hence, our 
proposal is a linguistically-based metric, VERTa 
(Comelles et al., 2012), which uses a wide vari-
ety of linguistic features at different levels, and 
aims at combining them in order to provide a 
wider and more accurate coverage than those 
metrics working at a specific linguistic level. In 
this paper we provide a description of the lin-
guistic information used in VERTa, the different 
modules that form VERTa and how they are 
combined according to the language evaluated 
and the type of evaluation performed. Moreover, 
the two versions of VERTa participating in 
WMT14, VERTa-EQ and VERTa-W are de-
scribed. Finally, for the sake of comparison, we 
use the data available in WMT12 and WMT13 to 
compare both versions to the metrics participat-
ing in those shared tasks. 
2 Linguistic Motivation 
Before developing VERTa, we analysed those 
linguistic phenomena that an MT metric should 
cover. From this analysis, we decided to organise 
the information into the following groups: 
 Lexical information. The use of lexical 
semantics plays a key role when compar-
ing a hypothesis and reference segment, 
since it allows for identifying relations of 
synonymy, hypernymy and hyponymy. 
 Morphological information. This type of 
information is crucial when dealing with 
languages with a rich inflectional mor-
phology, such as Spanish, French or Cata-
368
lan because it helps in covering phenom-
ena related to tense, mood, gender, num-
ber, aspect or case. In addition, morphol-
ogy in combination with syntax (morpho-
syntax) is also important to identify 
agreement (i.e. subject-verb agreement). 
This type of information should be taken 
into account when evaluating the fluency 
of a segment. 
 Syntactic information. This type of in-
formation covers syntactic structure, syn-
tactic relations and word order.  
 Semantic information. Named Entities 
(NEs), sentence polarity and time expres-
sions are included here. 
All this information described above should be 
taken into account when developing a metric that 
aims at covering linguistic phenomena at differ-
ent levels and evaluate both adequacy and flu-
ency. 
3 Metric Description 
In order to cover the above linguistic features, 
VERTa is organised into different modules: 
Lexical similarity module, Morphological simi-
larity module, Dependency similarity module and 
Semantic similarity module. Likewise, an Ngram 
similarity module has also been added in order to 
account for similarity between chunks in the hy-
pothesis and reference segments. Each metric 
works first individually and the final score is the 
Fmean of the weighted combination of the Preci-
sion and Recall of each metric in order to get the 
results which best correlate with human assess-
ment. This way, the different modules can be 
weighted depending on their importance regard-
ing the type of evaluation (fluency or adequacy) 
and language evaluated. In addition, the modular 
design of this metric makes it suitable for all lan-
guages. Even those languages that do not have a 
wide range of NLP tools available could be 
evaluated, since each module can be used iso-
lated or in combination. 
All metrics use a weighted precision and recall 
over the number of matches of the particular 
element of each level (words, dependency triples, 
ngrams, etc) as shown below. 
 
)(
))((
h
hnmatchWP D
 
R
W nmatch
D
( (r))
(r) 
Where r is the reference, h is the hypothesis 
and ? is a function that given a segment will 
return the elements of each level (e.g. words at 
lexical level and triples at dependency level). D 
is the set of different functions to project the 
level element into the features associated to each 
level, such as word-form, lemma or partial-
lemma at lexical level. nmatch () is a function 
that returns the number of matches according to 
the feature ? (i.e. the number of lexical matches 
at the lexical level or the number of dependency 
triples that match at the dependency level). Fi-
nally, W is the set of weights ]0 1] associated to 
each of the different features in a particular level 
in order to combine the different kinds of 
matches considered in that level.  
All modules forming VERTa and the linguis-
tic features used are described in detail in the 
following subsections. 
3.1 Lexical module 
Inspired by METEOR, the lexical module 
matches lexical items in the hypothesis segment 
to those in the reference segment taking into ac-
count several linguistic features. However, while 
METEOR uses word-form, synonymy, stemming 
and paraphrasing, VERTa relies on word-form, 
synonymy1, lemma, partial lemma2, hypernyms 
and hyponyms. In addition, a set of weights is 
assigned to each type of match depending on 
their importance as regards semantics (see Table 
1). 
 W
  
Match Examples 
HYP REF 
1 1 Word-form east east 
2 1 Synonym believed considered 
3 1 Hypernym barrel keg 
4 1 Hyponym keg barrel 
5 .8 Lemma is_BE are_BE 
6 .6 Part-lemma danger dangerous 
Table 1. Lexical matches and examples. 
3.2 Morphological similarity module 
The morphological similarity module is based on 
the matches established in the lexical module 
(except for the partial-lemma match) in combina-
tion with Part-of-Speech (PoS) tags from the an-
notated corpus3. The aim of this module is to 
                                                 
1 Information on synonyms, lemmas, hypernyms and 
hyponyms is obtained from WordNet 3.0. 
2 Lemmas that share the first four letters. 
3 The corpus has been PoS tagged using the Stanford 
Parser (de Marneffe et al. 2006). 
369
compensate the broader coverage of the lexical 
module, preventing matches such as invites and 
invite, which although similar in terms of mean-
ing, do not coincide as for their morphological 
information. Therefore, this module turns more 
appropriate to assess the fluency of a segment 
rather than its adequacy. In addition, this module 
will be particularly useful when evaluating lan-
guages with a richer inflectional morphology (i.e. 
Romance languages). 
In line with the lexical similarity metric, the 
morphological similarity metric establishes 
matches between items in the hypothesis and the 
reference sentence and a set of weights (W) is 
applied. However, instead of comparing single 
lexical items as in the previous module, in this 
module we compare pairs of features in the order 
established in Table 2. 
 
W Match Examples 
HYP REF 
1 (Word-
form, PoS) 
(he, PRP) (he, PRP) 
1 (Synonym, 
PoS) 
(VIEW, 
NNS) 
(OPINON, 
NNS) 
1 (Hypern., 
PoS) 
(PUBLICA-
TION, NN) 
(MAGA-
ZINE, NN) 
1 (Hypon., 
PoS) 
(MAGA-
ZINE, NN) 
(PUBLI-
CATION, 
NN) 
.8 (LEMMA, 
PoS) 
can_(CAN, 
MD) 
Could_(C
AN, MD) 
Table 2. Morphological module matches. 
3.3 Dependency similarity module 
The dependency similarity metric helps in cap-
turing similarities between semantically compa-
rable expressions that show a different syntactic 
structure (see Example 1), as well as changes in 
word order (see Example 2). 
Example 1: 
HYP: ...the interior minister... 
REF: ...the minister of interior... 
In example 1 both hypothesis and reference 
chunks convey the same meaning but their syn-
tactic constructions are different. 
Example 2: 
HYP: After a meeting Monday night with the 
head of Egyptian intelligence chief Omar 
Suleiman Haniya said.... 
REF: Haniya said, after a meeting on Monday 
evening with the head of Egyptian Intelligence 
General Omar Suleiman... 
In example 2, the adjunct realised by the PP 
After a meeting Monday night with the head of 
Egyptian intelligence chief Omar Suleiman oc-
cupies different positions in the hypothesis and 
reference strings. In the hypothesis it is located at 
the beginning of the sentence, preceding the sub-
ject Haniya, whereas in the reference, it is placed 
after the verb. By means of dependencies, we can 
state that although located differently inside the 
sentence, both subject and adjunct depend on the 
verb. 
This module works at sentence level and fol-
lows the approach used by (Owczarzack et al., 
2007a and 2007b) and (He et al., 2010) with 
some linguistic additions in order to adapt it to 
our metric combination. Similar to the morpho-
logical module, the dependency similarity metric 
also relies first on those matches established at 
lexical level ? word-form, synonymy, hy-
pernymy, hyponymy and lemma ? in order to 
capture lexical variation across dependencies and 
avoid relying only on surface word-form. Then, 
by means of flat triples with the form La-
bel(Head, Mod) obtained from the parser4, four 
different types of dependency matches have been 
designed (see Table 3) and weights have been 
assigned to each type of match. 
 
W Match Type Match Descr. 
1 Complete Label1=Label2 
Head1=Head2 
Mod1=Mod2 
1 Partial_no_label Label1?Label2 
Head1=Head2 
Mod1=Mod2 
.9 Partial_no_mod Label1=Label2 
Head1=Head2 
Mod1?Mod2 
.7 Partial_no_head Label1=Label2 
Head1?Head2 
Mod1=Mod2 
Table 3. Dependency matches. 
 
In addition, dependency categories also re-
ceive a different weight depending on how in-
formative they are: dep, det and _5 which receive 
0.5, whereas the rest of categories are assigned 
the maximum weight (1). 
Finally, a set of language-dependent rules has 
been added with two goals: 1) capturing similari-
ties between different syntactic structures con-
                                                 
4 Both hypothesis and reference strings are annotated 
with dependency relations by means of the Stanford 
parser (de Marneffe et al. 2006). 
5 _ stands for no_dep_label 
370
veying the same meaning; and 2) restricting cer-
tain dependency relations (i.e. subject word order 
when translating from Arabic to English).  
3.4 Ngram similarity module 
The ngram similarity metric matches chunks in 
the hypothesis and reference segments and relies 
on the matches set by the lexical similarity met-
ric, which allows us to work not only with word-
forms but also with synonyms, lemmas, partial-
lemmas, hypernyms and hyponyms as shown in 
Example 3, where the chunks [the situation in 
the area] and [the situation in the region] do 
match, even though area and region do not share 
the same word-form but a relation of synonymy. 
Example 3: 
HYP: ? the situation in the area? 
REF: ? the situation in the region? 
3.5 Semantics similarity module 
As confirmed by the lexical module, semantics 
plays an important role in the evaluation of ade-
quacy. This has also been claimed by (Lo and 
Wu, 2010) who report that their metric based on 
semantic roles outperforms other well-known 
metrics when adequacy is assessed. With this 
aim in mind the semantic similarity module uses 
other semantic features at sentence level: NEs, 
time expressions and polarity. 
Regarding NEs, we use Named-Entity recog-
nition (NER) and Named-Entity linking (NEL). 
Following previous NE-based metrics (Reeder et 
al., 2011 and Gim?nez, 2008) the NER metric6 
aims at capturing similarities between NEs in the 
hypothesis and reference segments. On the other 
hand NEL7 focuses only on those NEs that ap-
pear on Wikipedia, which allows for linking NEs 
regardless of their external form. Thus, EU and 
European Union will be captured as the same 
NE, since both of them are considered as the 
same organisation in Wikipedia. 
As regards time expressions, the TIMEX met-
ric matches temporal expressions in the hypothe-
sis and reference segments regardless of their 
form. The tool used is the Stanford Temporal 
Tagger (Chang and Manning, 2012) which rec-
ognizes not only points in time but also duration. 
By means of this metric, different syntactic struc-
tures conveying the same time expression can be 
                                                 
6 In order to identify NEs we use the Supersense Tag-
ger (Ciaramita and Altun, 2006). 
7 The NEL module uses a graph-based NEL tool 
(Hachey, Radford and Curran, 2010) which links NEs 
in a text with those in Wikipedia pages. 
matched, such as on February 3rd and on the 
third of February. 
Finally, it has been reported that negation 
might pose a problem to SMT systems (Wetzel 
and Bond, 2012). In order to answer such need, a 
module that checks the polarity of the sentence 
has been added using the dictionary strategy de-
scribed (Atserias et al., 2012):  
 Adding 0.5 for each weak positive word. 
 Adding 1.0 for each strong positive word. 
 Subtracting 0.5 for each weak negative 
word. 
 Subtracting 1.0 for each strong negative 
word. 
For each query term score, the value is propa-
gated to the query term positions by reducing its 
strength in a factor of 1/n, where n is the distance 
between the query term and the polar term. 
According to the experiments performed, this 
module shows a low correlation with human 
judgements on adequacy, since only partial as-
pects of translation are considered, whereas hu-
man judges assess whole segments. However, 
regardless of how well/bad the module correlates 
with human judgements, it proves useful to 
check partial aspects of the segments translated, 
such as the correct translation of NEs or the cor-
rect translation of negation. 
3.6 Metrics combination 
The modular design of VERTa allows for pro-
viding different weights to each module depend-
ing on the type of evaluation and the language 
evaluated. Thus following linguistic criteria 
when evaluating adequacy, those modules which 
must play a key role are the lexical and depend-
ency module, since they are more related to se-
mantics; whereas, when evaluating fluency those 
related to morphology, morphosyntax and con-
stituent word order will be the most important. 
Moreover, metrics can also be combined depend-
ing on the type of language evaluated. If a lan-
guage with a rich inflectional morphology is as-
sessed, the morphology module should be given 
a higher weight; whereas if the language evalu-
ated does not show such a rich inflectional mor-
phology, the weight of the morphology module 
should be lower. 
4 Experiments and results 
Experiments were carried out on WMT data, 
specifically on WMT12 and WMT13 data, all 
languages into English. Languages ?all? include 
French, German, Spanish and Czech for WMT12 
371
and French, German, Spanish, Czech and Rus-
sian for WMT13. Both segment and system level 
evaluations were performed. Evaluation sets pro-
vided by WMT organizers were used to calculate 
both segment and system level correlations. 
Since VERTa has been mainly designed to as-
sess either adequacy or fluency separately, our 
goal for WMT14 was to find the best combina-
tion in order to evaluate whole translation qual-
ity. Firstly we decided to explore the influence of 
each module separately. To this aim, all modules 
described above, except for the semantics one 
were used and tested separately. Secondly, all 
modules were assigned the same weight and 
tested in combination (VERTa-EQ). The reason 
why the semantics module was disregarded is 
that it does not usually correlate well with human 
judgements, as stated above. Each module was 
set as follows: 
 Lexical module. As described above, ex-
cept for the use of hypernyms/hyponyms 
matches that were disregarded. 
 Morphological module. As described 
above, except for the lemma-PoS match 
and the hypernyms/hyponyms-PoS match. 
 Dependency module. As described above. 
 Ngram module. As described above, using 
a 2-gram length. 
 
Finally, we used the module combination 
aimed at evaluating adequacy, which is mainly 
based on the dependency and lexical modules, 
but with a stronger influence of the ngram mod-
ule in order to control word order (VERTa-W). 
Weights were manually assigned, based on re-
sults obtained in previous experiments conducted 
for adequacy and fluency (Comelles et al., 2012), 
as follows: 
 Lexical module:  0.41 
 Morphological module: 0 
 Dependency module: 0.40 
 Ngram module: 0.19 
 
Experiments aimed at evaluating the influence 
of each module (see Table 4 and Table 5) show 
that the dependency module, in the case of 
WMT12 data, and the lexical module in the case 
of WMT13 data, are the most effective ones. 
However, the influence of the ngram module and 
the morphological module varies depending on 
the source language. The fact that the depend-
ency module correlates better with human 
judgements than others might be due to its flexi-
bility to capture different syntactic constructions 
that convey the same meaning. In addition, the 
good performance of the lexical module is due to 
the use of lexical semantic relations. On the other 
hand, in general the morphological module 
shows a better performance than the ngram one, 
which might be due to the type of source lan-
guages and the possible translation mistakes. All 
source languages are highly-inflected languages 
and this might cause problems when translating 
into English, since its inflectional morphology is 
not as rich as theirs. As for the low performance 
of the ngram module in the cs-en (especially, in 
WMT12 data), it might be due to the fact that 
Czech word order is unrestricted, whereas Eng-
lish shows a stricter word order and this might 
cause translation issues. A longer ngram distance 
might have been more appropriate to control 
word order in this case. 
 
Module fr-en de-en es-en cs-en 
Lexical .16 .20 .18 .14 
Morph. .17 .19 .18 .12 
Depend. .18 .24 .20 .17 
Ngram .16 .17 .15 .08 
Table 4. Segment-level Kendall?s tau correla-
tion per module with WMT12 data. 
 
Module fr-
en 
de-
en 
es-
en 
cs-
en 
ru-
en 
Lexical .239 .254 .294 .227 .220 
Morph. .236 .243 .295 .214 .191 
Depend. .232 .247 .275 .220 .199 
Ngram .237 .245 .283 .213 .189 
Table 5. Segment-level Kendall?s tau correla-
tion per module with WMT13 data. 
 
Finally, two versions of VERTa were com-
pared: the unweighted combination (VERTa-EQ) 
and the weighted one (VERTa-W). These two 
versions were also compared to some of the best 
performing metrics in WMT12 (see Table 6 and 
Table 7) and WMT13 (see Table 8 and Table 9): 
Spede07-pP, METEOR, SEMPOR and AMBER 
(Callison-Burch et al., 2012); SIMPBLEU-
RECALL, METEOR and DEPREF-ALIGN 8 ).  
As regards WMT12 data at segment level, the 
unweighted version achieves similar results to 
those obtained by the best performing metrics. 
On the other hand, VERTa-W?s results are 
slightly worse, especially for fr-en and es-en 
pairs, which is due to the fact that the morpho-
logical module has been disregarded in this ver-
                                                 
8 http://www.statmt.org/wmt13/papers.html 
372
sion. Regarding system level correlation, neither 
VERTa-EQ nor VERTa-W achieves a high cor-
relation with human judgements. 
 
Metric fr-en de-en es-en cs-en 
Spede07-pP .26 .28 .26 .21 
METEOR .25 .27 .24 .21 
VERTa-EQ .26 .28 .26 .20 
VERTa-W .24 .28 .25 .20 
Table 6. Segment-level Kendall?s tau correla-
tion WMT12. 
 
Metric fr-en de-en es-en cs-en 
SEMPOR .80 .92 .94 .94 
AMBER .85 .79 .97 .83 
VERTa-EQ .83 .71 .89 .66 
VERTa-W .79 .73 .91 .66 
Table 7. System-level Spearman?s rho correla-
tion WMT12. 
 
As for segment level WMT13 results (see Ta-
ble 8), although both VERTa-EQ and VERTa-
W?s performance is worse than that of the two 
best-performing metrics, both versions achieve a 
third and fourth position for all language pairs, 
except for fr-en. As regards system level correla-
tions (see Table 9), both versions of VERTa 
show the best performance for de-en and ru-en 
pairs, as well as for the average score. 
5 Conclusions and Future Work 
In this paper we have presented VERTa, a lin-
guistically-based MT metric. VERTa allows for 
modular combination depending on the language 
and type of evaluation conducted. Although 
VERTa has been designed to evaluate adequacy 
and fluency separately, in order to evaluate 
whole MT quality, a couple of versions have 
been used: VERTa-EQ, an unweighted version 
that uses all modules, and VERTa-W a weighted 
version that uses the lexical, dependency and 
ngram modules. 
Experiments have shown that the modules that 
best correlate with human judgements are the 
dependency and lexical modules. In addition, 
both VERTa-EQ and VERTa-W have been com-
pared to the best performing metrics in WMT12 
and WMT13 shared tasks. VERTa-EQ has 
proved to be in line with results obtained by 
Spede07-pP and METEOR in WMT12 at seg-
ment level, while in WMT13, both VERTa and 
VERTa-W occupy the third and fourth position 
after METEOR and DEPREF-ALIGN as regards 
segment level and the first position at system 
level.  
In the future, we plan to continue working on 
the improvement of VERTa and use automatic 
tuning of module?s weight in order to achieve the 
final version that best correlates with human 
judgements on ranking. Likewise, we would like 
to explore the use of VERTa to evaluate other 
languages but English and how NLP tool errors 
may influence the performance of the metric. 
6 Acknowledgements 
We would like to acknowledge Victoria Arranz 
and Irene Castell?n for their valuable comments 
and sharing their knowledge. 
This work has been partially funded by the Span-
ish Government (projects SKATeR, TIN2012-
38584-C06-06 and Holopedia, TIN2010-21128-
C02-02).
 
Metric fr-en de-en es-en cs-en ru-en Average 
SIMPBLEU-RECALL .303 .318 .388 .260 .234 .301 
METEOR .264 .293 .324 .265 .239 .277 
VERTa-EQ .252 .280 .318 .239 .215 .261 
VERTa-W .253 .278 .314 .238 .222 .261 
DEPREF-ALIGN .257 .267 .312 .228 .200 .253 
Table 8. Segment-level Kendall?s tau correlation WMT13. 
 
Metric fr-en de-en es-en cs-en ru-en Average 
METEOR .984 .961 .979 .964 .789 .935 
DEPREF-ALIGN .995 .966 .965 .964 .768 .931 
VERTa-EQ .989 .970 .972 .936 .814 .936 
VERTa-W .989 .980 .972 .945 .868 .951 
Table 9. System-level Spearman?s rho correlation WMT13.
373
Reference 
J. S. Albrecth and R. Hwa. 2007. A Re-examination 
of Machine Learning Approaches for Sentence-
Level MT Evaluation. In The Proceedings of the 
45th Annual Meeting of the ACL, Prague, Czech 
Republic.  
J. S. Albrecth and R. Hwa. 2007. Regression for Sen-
tence-Level MT Evaluation with Pseudo Refer-
ences. In The Proceedings of the 45th Annual 
Meeting of the ACL, Prague, Czech Republic.  
J. Atserias, R. Blanco, J. M. Chenlo and C. 
Rodriquez. 2012. FBM-Yahoo at RepLab 2012, 
CLEF (Online Working Notes/Labs/Workshop) 
2012, September 20, 2012. 
C. Callison-Burch, M. Osborne and P. Koehn. 2006. 
Re-evaluating the role of BLEU in machine trans-
lation research. In Proceedings of the EACL 2006. 
C. Callison-Burch, P. Kohen, Ch. Monz, M. Post, R. 
Soricut and L. Specia. 2012. Findings of the 2012 
Workshop on Statistical Machine Translation. In 
Proceedings of the 7th Workshop on Statistical 
Machine Translation. Montr?al. Canada. 
A. X. Chang and Ch. D. Manning. 2012. SUTIME: A 
Library for Recognizing and Normalizing Time 
Expressions. 8th International Conference on Lan-
guage Resources and Evaluation (LREC 2012). 
M. Ciaramita and Y. Altun. 2006. Broad-coverage 
sense disambiguation and information extraction 
with a supersense sequence tagger. Empirical 
Methods in Natural Language Processing 
(EMNLP). 
E. Comelles, J. Atserias, V. Arranz and I. Castell?n. 
2012. VERTa: Linguistic features in MT evalua-
tion. Proceedings of the Eighth International Con-
ference on Language Resources and Evaluation 
(LREC'12), Istanbul, Turkey. 
M.C. de Marneffe, B. MacCartney and Ch. D. Man-
ning. 2006. Generating Typed Dependency Parses 
from Phrase Structure Parses in Proceedings of the 
5th Edition of the International Conference on 
Language Resources and Evaluation (LREC-
2006). Genoa, Italy. 
M. J. Denkowski and A. Lavie. 2011. METEOR 1.3: 
Automatic Metric for Reliable Optimization and 
Evaluation of Machine Translation Systems in 
Proceedings of the 6th Workshop on Statistical 
Machine Translation (ACL-2011). Edinburgh, 
Scotland, UK. 
J. Gim?nez and Ll. M?rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogeneous 
MT systems in Proceedings of the 2nd Workshop 
on Statistical Machine Translation (ACL), Prague, 
Czech Repubilc. 
 
J. Gim?nez and Ll. M?rquez. 2008. A smorgasbord of 
features for automatic MT evaluation in Proceed-
ings of the 3rd Workshop on Statistical Machine 
Translation (ACL). Columbus. OH. 
J. Gimenez. 2008. Empirical Machine Translation 
and its Evaluation. Doctoral Dissertation. UPC. 
J. Gim?nez and Ll. M?rquez. 2010. Linguistic Meas-
ures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4),77?86. 
Springer. 
B. Hachey, W. Radford and J. R. Curran. 2011. 
Graph-based named entity linking with Wikipedia 
in Proceedings of the 12th International confer-
ence on Web information system engineering, 
pages 213-226, Springer-Verlag, Berlin, Heidel-
berg. 
Y. He, J. Du, A. Way and J. van Genabith. 2010. The 
DCU Dependency-based Metric in WMT-Metrics 
MATR 2010. In Proceedings of the Joint Fifth 
Workshop on Statistical Machine Translation and 
Metrics MATR (WMT 2010),  Uppsala, Sweden. 
A. Lavie and M. J. Denkowski. 2009. The METEOR 
Metric for Automatic Evaluation of Machine 
Translation. Machine Translation, 23. 
G. Leusch and H. Ney. 2008. BLEUSP, INVWER, 
CDER: Three improved MT evaluation measures.  
In  NIST Metrics for Machine Translation 2008 
Evaluation (MericsMATR08), Waikiki, Honolulu, 
Hawaii,  October 2008. 
D. Liu and D. Hildea. 2005. Syntactic Features for 
Evaluation of Machine Translation in Proceedings 
of the ACL Workshop on Intrinsic and Extrinsic 
Evaluation Measures for Machine Translation 
and/or Summarization, Ann Arbor 
Ch.Lo and D. Wu. 2010. Semantic vs. Syntactic vs. 
Ngram Structure for Machine Translation Evalua-
tion.  In Proceedings of the 4th Workshop on Syntax 
Semantics and Structure in Statistical Translation. 
Beijing. China. 
Ch. Lo, A. K. Tumurulu and D. Wu. 2012. Fully 
Automatic Semantic MT Evaluation. Proceedings 
of the 7th Wrokshop on Statistical Machine Trans-
lation,  Montr?al, Canada, June 7-8. 
K. Owczarzak,  J. van Genabith  and A. Way. 2007. 
Dependency-Based Automatic Evaluation for Ma-
chine Translation in Proceedings of SSST, NAACL-
HLT/AMTA Workshop on Syntax and Structure I 
Statistical Translation, Rochester, New York. 
K. Owczarzak,  J. van Genabith  and A. Way. 2007. 
Labelled Dependencies in Machine Translation 
Evaluation in Proceedings of the ACL Workshop 
on Statistical Machine Translation, Prague, Czech 
Republic. 
374
K. Papineni, S. Roukos, T. Ward and W. Zhu. 2002. 
BLEU: A Method for Automatic Evaluation of 
Machine Translation. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL02). Philadelphia. PA. 
F. Reeder, K. Miller, J. Doyon and J. White. 2001. 
The Naming of Things and the Confusion of 
Tongues: an MT Metric. Proceedings of the Work-
shop on MT Evaluation ?Who did what to whom?? 
at Machine Translation Summit VIII. 
L. Specia and J. Gim?nez. 2010. Combining Confi-
dence Estimation and Reference-based Metrics for 
Segment-level MT Evaluation. The Ninth Confer-
ence of the Association for Machine Translation in 
the Americas (AMTA 2010), Denver, Colorado. 
D. Wetzel and F. Bond. 2012. Enriching Parallel Cor-
pora for Statistical Machine Translation with Se-
mantic Negation Rephrasing. Proceedings of SSST-
6, Sixth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Jeju, Republic of 
Korea. 
 
375
