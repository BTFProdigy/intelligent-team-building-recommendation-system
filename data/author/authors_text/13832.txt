Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 13?25,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting Thread Discourse Structure over Technical Web Forums
Li Wang,?? Marco Lui,?? Su Nam Kim,?? Joakim Nivre? and Timothy Baldwin??
? Dept. of Computer Science and Software Engineering, University of Melbourne
? NICTA Victoria Research Laboratory
? Dept. of Linguistics and Philology, Uppsala University
li.wang.d@gmail.com, saffsd@gmail.com,
sunamkim@gmail.com, joakim.nivre@lingfil.uu.se, tb@ldwin.net
Abstract
Online discussion forums are a valuable
means for users to resolve specific information
needs, both interactively for the participants
and statically for users who search/browse
over historical thread data. However, the com-
plex structure of forum threads can make it
difficult for users to extract relevant informa-
tion. The discourse structure of web forum
threads, in the form of labelled dependency re-
lationships between posts, has the potential to
greatly improve information access over web
forum archives. In this paper, we present the
task of parsing user forum threads to deter-
mine the labelled dependencies between posts.
Three methods, including a dependency pars-
ing approach, are proposed to jointly clas-
sify the links (relationships) between posts
and the dialogue act (type) of each link. The
proposed methods significantly surpass an in-
formed baseline. We also experiment with ?in
situ? classification of evolving threads, and es-
tablish that our best methods are able to per-
form equivalently well over partial threads as
complete threads.
1 Introduction
Web user forums (or simply ?forums?) are online
platforms for people to discuss information and ob-
tain information via a text-based threaded discourse,
generally in a pre-determined domain (e.g. IT sup-
port or DSLR cameras). With the advent of Web
2.0, there has been an explosion of web authorship in
this area, and forums are now widely used in various
areas such as customer support, community devel-
opment, interactive reporting and online eduction.
In addition to providing the means to interactively
participate in discussions or obtain/provide answers
to questions, the vast volumes of data contained in
forums make them a valuable resource for ?support
sharing?, i.e. looking over records of past user inter-
actions to potentially find an immediately applica-
ble solution to a current problem. On the one hand,
more and more answers to questions over a wide
range of domains are becoming available on forums;
on the other hand, it is becoming harder and harder
to extract and access relevant information due to the
sheer scale and diversity of the data.
This research aims at enhancing information ac-
cess and support sharing, by mining the discourse
structure of troubleshooting-oriented web user fo-
rum threads. Previous research has shown that sim-
ple thread structure information (e.g. reply-to struc-
ture) can enhance tasks such as forum information
retrieval (Seo et al, 2009) and post quality assess-
ment (Lui and Baldwin, 2009). We aim to move be-
yond simple threading, to predict not only the links
between posts, but also show the manner of each
link, in the form of the discourse structure of the
thread. In doing so, we hope to be able to perform
richer visualisation of thread structure (e.g. high-
lighting the key posts which appear to have led to
a successful resolution to a problem), and more fine-
grained weighting of posts in threads for search pur-
poses.
To illustrate the task, we use an example thread,
made up of 5 posts from 4 distinct participants, from
the CNET forum dataset of Kim et al (2010b), as
shown in Figure 1. The discourse structure of the
thread is modelled as a rooted directed acyclic graph
13
HTML Input Code...Please can someone tell me how to create an input box that asks the user to enter their ID, and then allows them to press go. It will then redirect to the page ...
User APost 1
User BPost 2
User CPost 3
Re: html input codePart 1: create a form with a text field. See ... Part 2: give it a Javascript action
asp.net c\# videoI?ve prepared for you video.link click ...
Thank You!Thanks a lot for that ... I have Microsoft Visual Studio 6, what program should I do this in? Lastly, how do I actually include this in my site? ...
A little more help... You would simply do it this way: ... You could also just ... An example of this is ...
User APost 4
User DPost 5
0+Question-Question
2+Answer-Answer
4+Answer-Answer
1+Answer-Answer
1+Answer-Confirmation
3+Question-Add
?
Figure 1: A snippeted and annotated CNET thread
(DAG) with a dialogue act label associated with each
edge of the graph. In this example, UserA initiates
the thread with a question (dialogue act = Question-
Question) in the first post, by asking how to create
an interactive input box on a webpage. In response,
UserB and UserC provide independent answers (di-
alogue act = Answer-Answer). UserA responds to
UserC to confirm the details of the solution (dia-
logue act = Answer-Confirmation), and at the same
time, adds extra information to his/her original ques-
tion (dialogue act = Question-Add); i.e., this one
post has two distinct dependency links associated
with it. Finally, UserD proposes a different solution
again to the original question.
To predict thread discourse structure of this type,
we jointly classify the links and dialogue acts be-
tween posts, experimenting with a variety of su-
pervised classification methods, namely dependency
parsing and linear-chain conditional random fields.
In this, we build on the earlier work of Kim et al
(2010b) who first proposed the task of thread dis-
course analysis, but only carried out experiments on
post linking and post dialogue act classification as
separate tasks. In addition to achieving state-of-the-
art accuracy over the task, we carry out in-depth
analysis of classification effectiveness at different
thread depths, and establish that the accuracy of our
method over partial threads is equivalent to that over
full threads, indicating that the method is applica-
ble to in-situ thread classification. Finally, we in-
vestigate the role of user-level features in discourse
structure analysis.
2 Related Work
This work builds directly on earlier work of a subset
of the authors (Kim et al, 2010b), whereby a novel
post-level dialogue act set was proposed, and used
as the basis for annotation of a set of threads taken
from CNET. In the original work, we proposed a set
of novel features, which we applied to the separate
tasks of post link classification and dialogue act clas-
sification. We later applied the same basic method-
ology to dialogue act classification over one-on-one
live chat data with provided message dependencies
(Kim et al, 2010a), demonstrating the generalisabil-
ity of the original method. In both cases, however,
we tackled only a single task, either link classifica-
tion (optionally given dialogue act tags) or dialogue
act classification, but never the two together. In this
paper, we take the obvious step of exploring joint
classification of post link and dialogue act tags, to
generate full thread discourse structures.
Discourse disentanglement (i.e. link classifica-
tion) and dialogue act tagging have been studied
largely as independent tasks. Discourse disentangle-
ment is the task of dividing a conversation thread
(Elsner and Charniak, 2008; Lemon et al, 2002)
or document thread (Wolf and Gibson, 2005) into
a set of distinct sub-discourses. The disentangled
discourse is sometimes assumed to take the form of
a tree structure (Grosz and Sidner, 1986; Lemon et
al., 2002; Seo et al, 2009), an acyclic graph struc-
ture (Rose? et al, 1995; Schuth et al, 2007; Elsner
and Charniak, 2008; Wang et al, 2008; Lin et al,
2009), or a more general cyclic chain graph struc-
ture (Wolf and Gibson, 2005). Dialogue acts are
used to describe the function or role of an utterance
in a discourse, and have been applied to the anal-
ysis of mediums of communication including con-
versational speech (Stolcke et al, 2000; Shriberg et
al., 2004; Murray et al, 2006), email (Cohen et al,
2004; Carvalho and Cohen, 2005; Lampert et al,
2008), instant messaging (Ivanovic, 2008; Kim et
al., 2010a), edited documents (Soricut and Marcu,
2003; Sagae, 2009) and online forums (Xi et al,
14
2004; Weinberger and Fischer, 2006; Wang et al,
2007; Fortuna et al, 2007; Kim et al, 2010b). For a
more complete review of models for discourse dis-
entanglement and dialogue act tagging, see Kim et
al. (2010b).
Joint classification has been applied in a number
of different contexts, based on the intuition that it
should be possible to harness interactions between
different sub-tasks to the mutual benefit of both.
Warnke et al (1997) jointly performed segmenta-
tion and dialogue act classification over a German
spontaneous speech corpus. In their approach, the
predictions of a multi-layer perceptron classifier on
dialogue act boundaries were fed into an n-gram
language model, which was used for the joint seg-
mentation and classification of dialogue acts. Sut-
ton and McCallum (2005) performed joint parsing
and semantic role labelling (SRL), using the results
of a probabilistic SRL system to improve the accu-
racy of a probabilistic parser. Finkel and Manning
(2009) built a joint, discriminative model for pars-
ing and named entity recognition (NER), address-
ing the problem of inconsistent annotations across
the two tasks, and demonstrating that NER bene-
fited considerably from the interaction with parsing.
Dahlmeier et al (2009) proposed a joint probabilis-
tic model for word sense disambiguation (WSD) of
prepositions and SRL of prepositional phrases (PPs),
and achieved state-of-the-art results over both tasks.
There has been a recent growth in user-level
research over forums. Lui and Baldwin (2009)
explored a range of user-level features, including
replies-to and co-participation graph analysis, for
post quality classification. Lui and Baldwin (2010)
introduced a novel user classification task where
each user is classified against four attributes: clar-
ity, proficiency, positivity and effort. User commu-
nication roles in web forums have also been studied
(Chan and Hayes, 2010; Chan et al, 2010).
Threading information has been shown to en-
hance retrieval effectiveness for post-level retrieval
(Xi et al, 2004; Seo et al, 2009), thread-level
retrieval (Seo et al, 2009; Elsas and Carbonell,
2009), sentence-level shallow information extrac-
tion (Sondhi et al, 2010), and near-duplicate thread
detection (Muthmann et al, 2009). These results
suggest that the thread structural representation used
in this research, which includes both linking struc-
ture and the dialogue act associated with each link,
could potentially provide even greater leverage in
these retrieval tasks.
Another related research area is post-level classi-
fication, such as general post quality classification
(Weimer et al, 2007; Weimer and Gurevych, 2007;
Wanas et al, 2008; Lui and Baldwin, 2009), and
post descriptiveness in particular domains (e.g. med-
ical forums: Leaman et al (2010)). It has been
demonstrated (Wanas et al, 2008; Lui and Bald-
win, 2009) that thread discourse structure can signif-
icantly improve the classification accuracy for post-
level tasks.
Initiation?response pairs (e.g. question?answer,
assessment?agreement, and blame?denial) from on-
line forums have the potential to enhance thread
summarisation or automatically generate knowledge
bases for Community Question Answering (cQA)
services such as Yahoo! Answers. While initiation?
response pair identification has been explored as a
pairwise ranking problem (Wang and Rose?, 2010),
question?answer pair identification has been ap-
proached via the two separate sub-tasks of ques-
tion classification and answer detection (Cong et al,
2008; Ding et al, 2008; Cao et al, 2009). Our
thread discourse structure prediction task includes
joint classification of post roles (i.e. dialogue acts)
and links, and could potentially be performed at the
sub-post sentence level to extract initiation?response
pairs.
3 Task Description and Data Set
The main task performed in this research is joint
classification of inter-post links (Link) and dialogue
acts (DA) within forum threads. In this, we assume
that a post can only link to an earlier post (or a vir-
tual root node), and that dialogue acts are labels on
edges. It is possible for there to be multiple edges
from a given post, e.g. if a post both confirms the va-
lidity of an answer and adds extra information to the
original question (as happens in Post4 in Figure 1).
We experiment with two different approaches to
joint classification: (1) a linear-chain CRF over
combined Link/DA post labels; and (2) a depen-
dency parser. The joint classification task is a nat-
ural fit for dependency parsing, in that the task is
intrinsically one of inferring labelled dependencies
15
between posts, but it has a number of special prop-
erties that distinguish it from standard dependency
parsing:
strict reverse-chronological directionality: the
head always precedes the dependent, in terms
of the chronological sequencing of posts.
non-projective dependencies: threads can contain
non-projective dependencies, e.g. in a 4-post
thread, posts 2 and 3 may be dependent on
post 1, and post 4 dependent on post 2; around
2% of the threads in our dataset contain non-
projective dependencies.
multi-headedness: it is possible for a given post to
have multiple heads, including the possibility
of multiple dependency links to the same post
(e.g. adding extra information to a question
[Question-Add] as well as retracting infor-
mation from the original question [Question-
Correction]); around 6% of the threads in our
dataset contain multi-headed dependencies.
disconnected sub-graphs: it is possible for there to
be disconnected sub-graphs, e.g. in instances
where a user hijacks a thread to ask their
own unrelated question, or submit an unrelated
spam post; around 2% of the threads in our
dataset contain disconnected sub-graphs.
The first constraint potentially simplifies depen-
dency parsing, and non-projective dependencies are
relatively well understood in the dependency parsing
community (Tapanainen and Jarvinen, 1997; Mc-
Donald et al, 2005). Multi-headedness and dis-
connected sub-graphs pose greater challenges to de-
pendency parsing, although there has been research
done on both (McDonald and Pereira, 2006; Sagae
and Tsujii, 2008; Eisner and Smith, 2005). The
combination of non-projectivity, multi-headedness
and disconnected sub-graphs in a single dataset,
however, poses a challenge for dependency parsing.
In addition to performing evaluation in batch
mode over complete threads, we consider the task of
?in situ thread classification?, whereby we predict
the discourse structure of a thread after each post.
This is intended to simulate the more realistic set-
ting of incrementally crawling/updating thread data,
but needing to predict discourse structure for partial
threads. We are interested in determining the rela-
tive degradation in accuracy for in situ classification
vs. batch classification.
As our dataset, we use the CNET forum dataset
of Kim et al (2010b),1 which contains 1332 an-
notated posts spanning 315 threads, collected from
the Operating System, Software, Hardware and Web
Development sub-forums of cnet.2 Each post is la-
belled with one or more links (including the possi-
bility of null-links, where the post doesn?t link to
any other post), and each link is labelled with a di-
alogue act. The dialogue act set is made up of 5
super-categories: Question, Answer, Resolution
(confirmation of the question being resolved), Re-
production (external confirmation of a proposed so-
lution working) and Other. The Question category
contains 4 sub-classes: Question, Add, Confirma-
tion and Correction. Similarly, the Answer cate-
gory contains 5 sub-classes: Answer, Add, Confir-
mation, Correction and Objection. For example,
the label Question-Add signifies the Question su-
perclass and Add subclass, i.e. addition of extra in-
formation to a question. For full details of the dia-
logue act tagset, see Kim et al (2010b).
Dependency links are represented by their relative
position in the chronologically-sorted list of posts,
e.g. 1 indicates a link back to the preceding post,
and 2 indicates a link back two posts.
Unless otherwise noted, evaluation is over the
combined link and dialogue act tag, including the
combination of superclass and subclass for the
Question and Answer dialogue acts. For ex-
ample, 1+Answer-Answer indicates a dependency
link back one post, which is an answer to a question.
The most common label in the dataset is 1+Answer-
answer (28.4%).
4 Learners and Features
4.1 Learners
To predict thread discourse structure, we use a struc-
tured classification approach ? based on the find-
ings of Kim et al (2010b) and Kim et al (2010a)
? and a dependency parser. The structured clas-
sification approach we experiment with is a linear-
1Available from http://www.csse.unimelb.edu.
au/research/lt/resources/conll2010-thread/
2http://forums.cnet.com/
16
chain conditional random field learner (CRF: Laf-
ferty et al (2001)), within which we explore two
simple approaches to joint classification, as is ex-
plained in Section 5.1. Dependency parsing (Ku?bler
et al, 2009) is the task of automatically predicting
the dependency structure of a token sequence, in
the form of binary asymmetric dependency relations
with dependency types.
Standardly, CRFs have been applied to tasks such
as part-of-speech tagging, named entity recognition,
semantic role labelling and supertagging, where the
individual tokens are single words. Similarly, de-
pendency parsing is conventionally applied to sen-
tences, with single-word tokens. In our case, our
tokens are thread posts, with much greater scope for
feature engineering than single words, and techni-
cal challenges in scaling the underlying implemen-
tations to handle potentially much larger feature sets.
As our learners, we deployed CRFSGD (Bot-
tou, 2011) to learn the CRF, and MaltParser (Nivre
et al, 2007) as our dependency parser. CRFSGD
uses stochastic gradient descent to efficiently solve
the convex optimisation problem, and scales well to
large feature sets. We used the default parameter set-
tings for CRFSGD, with feature templates includ-
ing all unigram features of the current token as well
as bigram features combining the previous output to-
ken with the current token.
MaltParser implements transition-based parsing,
where no formal grammar is considered, and a tran-
sition system, or state machine, is learned to map a
sentence onto its dependency graph. One feature of
MaltParser that makes it well suited to our task is
that it is possible to define feature models of arbi-
trary complexity for each token. In presenting the
thread data to MaltParser, we represent the null-
link from the initial post of each thread, as well as
any disconnected posts, as the root.
To the best of our knowledge, there is no past
work on using dependency parsing to learn thread
discourse structure. Based on extensive experimen-
tation, we determined that the MaltParser configu-
ration that obtains the best results for our task is the
Nivre algorithm in arc-standard mode (Nivre, 2003;
Nivre, 2004), using LIBSVM (Chang and Lin, 2011)
with a linear kernel as the learner, and a feature
model with exhaustive combinations of features re-
lating to the features and predictions of the first/top
three tokens from both ?Input? and ?Stack?.3 As
such, MaltParser is actually unable to predict any
non-projective structures, as experiments with algo-
rithms supporting non-projective structures invari-
ably led to lower results. In our choice of parsing al-
gorithm, we are also unable to detect posts with mul-
tiple heads, but can potentially detect disconnected
sub-graphs.
4.2 Features
The features used in our classifiers are as follows:
Structural Features:
Initiator a binary feature indicating whether the
current post?s author is the thread initiator.
Position the relative position of the current post,
as a ratio over the total number of posts in the
thread.
Semantic Features:
TitSim the relative location of the post which has
the most similar title (based on unweighted co-
sine similarity) to the current post.
PostSim the relative location of the post which
has the most similar content (based on un-
weighted cosine similarity) to the current post.
Punct the number of question marks (QuCount),
exclamation marks (ExCount) and URLs
(UrlCount) in the current post.
UserProf the class distribution (in the training
thread) of the author of the current post.
These features are drawn largely from the work
of Kim et al (2010b), with two major differences:
(1) we do not use post context features because our
learners (i.e. CRFSGD and MaltParser) inherently
capture Markov chains; and (2) our UserProf fea-
tures are customised to the class set associated with
the task at hand, e.g. the UserProf features for the
standalone linking task take the form of the link la-
bels (and not dialogue act labels) of the posts by the
relevant author in the training data. Table 1 shows
the feature representation of the third post in a thread
17
Feature Value Explanation
Initiator 1.0 post from the initiator
ExCount 4.0 4 exclamation marks
QuCount 0.0 0 question marks
UrlCount 0.0 0 URLs
Position 0.25 i?1n = 3?18PostSim 2.0 most similar to post 1
TitSim 2.0 most similar to post 1
UserProf ~x counts for posts of each
class from the same author
in the training data
Table 1: The feature presentation of the third post in a
thread of length 8
of length 8. The values of each feature are scaled to
the range [0, 1] before being fed into the learners.
We also experimented with other features,
including raw bag-of-words lexical features,
dimensionality-reduced lexical features (using
principal components analysis), and different post
similarity measures such as longest common subse-
quence (LCS) match. While we were able to obtain
gains in isolation, when combined with the other
features, these features had no impact, and are thus
not included in the results presented in this paper.
5 Classification Methodology
All our experiments were carried out based on strati-
fied 10-fold cross-validation, stratifying at the thread
level to ensure that all posts from a given thread
occur in a single fold. The results are primarily
evaluated using post-level micro-averaged F-score
(F?: ? = 1), and additionally with thread-level F-
score/classification accuracy (i.e. the proportion of
threads where all posts have been correctly classi-
fied4), where space allows. Statistical significance
is tested using randomised estimation (Yeh, 2000)
with p < 0.05. Initial experiments showed it is
hard for learners to discover which posts have multi-
ple links, largely due to the sparsity of multi-headed
posts (which account for less than 5% of the total
posts). Therefore, only the the most recent link for
3http://maltparser.org/userguide.html#
parsingalg
4Classification accuracy = F-score at the thread-level, as
each thread is assigned a single label of correct or incorrect.
each multi-headed post was included in training, but
evaluation still considers all links.
5.1 Joint classification
In our experiments, we test two basic approaches to
joint classification for the CRF: (1) classifying the
Link and DA separately, and composing the predic-
tions to form the joint classification (Composition);
and (2) combining the Link and DA labels into a sin-
gle class, and applying the learner over the posts
with the combined class (Combine). Note that
Composition has the potential for mismatches in
the number of Link and DA predictions it gener-
ates, causing complications in the class composition.
Even if the same number of labels is predicted for
both Link and DA, if multiple tags are predicted in
both cases, we are left with the problem of determin-
ing which link label to combine with which dialogue
act label. As such, we have our reservations about
Composition, but as the CRF performs strict 1-of-
n labelling, these are not issues in the experiments
reported herein.
MaltParser natively handles the combination of
Link and DA in its dependency parsing formulation.
5.2 In Situ Thread Classification
One of the biggest challenges in classifying the dis-
course structure of a forum thread is that threads
evolve over time, as new posts are posted. In or-
der to capture this phenomenon, and compare the
accuracy of different models when applied to partial
thread data (artificially cutting off a thread at post
N ) vs. complete threads.5 This is done in the fol-
lowing way: classification over the first two posts
only ([1, 2]), the first four posts ([1, 4]), the first six
posts ([1, 6]), the first eight posts ([1, 8]), and all
posts ([all]). In each case, we limit the test data
only, meaning that the only variable in play is the
extent of thread context used to learn the thread dis-
course structure for the given set of posts. We break
down the results in each case into the indicated sub-
threads, e.g. we take the predictions for [all], and
break them down into the results for [1, 2], [1, 4],
[1, 6], [1, 8] and [all], for direct comparison with the
predictions over the respective sub-thread data.
5In practice, completeness is defined at a given point in time,
when the crawl was done, and it is highly likely that some of the
?complete? threads had extra posts after the crawl.
18
Method Link DA
Kim et al (2010b) .863 / .676 .751 / .543
CRFSGD .891 / .727 .795 / .609
Table 2: Post/thread-level component-wise classification
F-scores for Link and DA classes
6 Experiments and Analysis
6.1 Joint classification
As our baseline for the task, we first use a sim-
ple majority class classifier in the form of the sin-
gle joint class of 1+Answer-Answer for all posts,
which has a post-level F-score of 0.284. A stronger
baseline is to classify all first posts as 0+Question-
Question and all subsequent posts as 1+Answer-
answer, which achieves a post-level F-score of
0.515 (labelled as Heuristic).
As described in Section 5.1, one approach to joint
classification with CRFSGD is to firstly conduct
component-wise classification over Link and DA
separately, and compose the predictions. The results
for the separate Link and DA classification tasks are
presented in Table 2, along with the best results for
Link and DA classification from Kim et al (2010b).
At the component-wise tasks, our method is superior
to Kim et al (2010b), based on a different learner
and slightly different feature set.
Next, we compose the component-wise clas-
sifications for the CRF into joint classifications
(Composition). We contrast this with the com-
bined class approach for CRFSGD and MaltParser
(jointly presented as Joint in Table 3). With the
combined class results, we additionally ablate each
of the feature types from Section 4.2, and also
present results for a dummy model, where no fea-
tures are provided and the prediction is based simply
on sequential priors (Dummy). The results are pre-
sented in Table 3, along with the Heuristic baseline
result.
Several interesting things can be observed from
the post-level F-score results in Table 3. First, with
no features (Dummy), while CRFSGD performs
slightly worse than the Heuristic baseline, Malt-
Parser significantly surpasses the baseline. This is
due to the richer sequential context model of Malt-
Parser. Second, the single feature with the greatest
impact on results is UserProf, i.e. user profile fea-
Method CRFSGD MaltParser
Heuristic .515?/ .311?
Dummy .508?/ .394? .533?/ .356?
Composition .728?/ .553? ?
Joint +ALL .756 / .578 .738 / .578
?Initiator .745 / .569 .708?/ .534?
?Position .750 / .565 .736 / .568
?PostSim .753 / .578 .737 / .568
?TitSim .760 / .587 .734 / .571
?Punct .745 / .571 .735 / .578
?UserProf .672?/ .527? .701?/ .536?
Table 3: Post/thread-level Link-DA joint classification F-
scores (??? signifies a significantly worse result than that
for the same learner with ALL features)
tures extracted from the training data; CRFSGD in
particular benefits from this feature. We return to ex-
plore this effect in Section 6.4. Third, although the
Initiator feature does not have much effect on CRF-
SGD, it affects the performance of MaltParser sig-
nificantly. Further experiments shown that the com-
bination of Initiator and UserProf is sufficient to
achieve a competitive result (i.e. 0.731). It therefore
seems that MaltParser is more robust than CRF-
SGD, whose performance relies crucially on user-
level features which must be learned from the train-
ing data (i.e. UserProf).
Looking to the thread-level F-scores, we observe
some interesting divergences from the post-level F-
score results. First, with no features (Dummy),
CRFSGD significantly outperforms both the base-
line and MaltParser. This appears to be because
CRFSGD performs particularly well over short
threads (e.g. of length 3 and 4), but worse over
longer threads. Second, the best thread-level F-
scores from CRFSGD (i.e. 0.587) and MaltParser
(i.e. 0.578) are not significantly different, despite the
discrepancy in post-level F-score (where CRFSGD
is markedly superior in this case). With the extra
features, the performance of MaltParser on short
threads appears to pick up noticeably, and the differ-
ence in post-level predictions is over longer threads.
If we evaluate the two models over DA super-
classes only (ignoring mismatches at the subclass
level for Question and Answer), the post-level F-
scores for joint classification with ALL features for
CRFSGD and MaltParser are 0.803 and 0.787, re-
spectively.
19
Approaches Link DA
Component-wise .891 / .727? .795 / .609
CRFSGD decomp .893 / .749 .785 / .603
MaltParser decomp .870?/ .730? .766?/ .571?
Table 4: Post/thread-level Link and DA F-scores from
component-wise classification, and from Link-DA clas-
sification decomposition (??? signifies a significantly
worse result than the best result in that column)
Looking at the performance of CRFSGD (in
Combine mode) and MaltParser on disconnected
sub-graphs, while both models did predict a small
number of non-initial posts with null-links (includ-
ing MaltParser predicting 5 out of 6 posts in a sin-
gle thread as having null-links), none were correct,
and neither model was able to correctly predict any
of the 6 actual non-initial instances of null-links in
the dataset.
Finally, we took the joint classification results
from CRFSGD and MaltParser using ALL fea-
tures, and decomposed the predictions into Link and
DA. The results are presented in Table 4, along with
the results for component-wise classification from
Table 2. Somewhat surprisingly, the decomposed
predictions are mostly slightly worse than the re-
sults for the component-wise classification, despite
achieving higher F-score for the joint classification
task. This is simply due to the combined method
tending to get both labels correct or both labels
wrong, for a given post.
6.2 Post Position-based Result Breakdown
One question in thread discourse structure classifica-
tion is how accurate the predictions are at different
depths in a thread (e.g. the first two posts vs. the sec-
ond two posts). A breakdown of results across posts
at different positions is presented in Figure 2.
The overall trend for both CRFSGD and Malt-
Parser is that it becomes increasingly hard to clas-
sify posts as we continue through a thread, due to
greater variability in discourse structure and greater
sparsity in the data. However, it is interesting to note
that the results for CRFSGD actually improve from
posts 7 and 8 ([7, 8]) to posts 9 and onwards ([9, ]).
To further investigate this effect, we performed class
decomposition over the joint classification predic-
tions, and performed a similar breakdown of posts
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posts
F ?
 
 CRFSGDMaltParser
Figure 2: Breakdown of post-level Link-DA results for
CRFSGD and MaltParser based on post position
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed Link
 
 CRFSGDMaltParser
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed DA
 
 CRFSGDMaltParser
Figure 3: Breakdown of post-level Link and DA F-score
based on the decomposition of CRFSGD and Malt-
Parser classifications
for Link and DA; the results are presented in Fig-
ure 3. It is clear that the anomaly for CRFSGD
comes from the DA component, due to there being
greater predictability in the dialogue for final posts
in a thread (users tend to confirm a successful reso-
lution of the problem, or report on successful exter-
nal reproduction of the solution). MaltParser seems
less adept at identifying that a post is at the end
of a thread, and predicting the dialogue act accord-
ingly. This observation is congruous with the find-
ings of McDonald and Nivre (2007) that errors prop-
agate, due to MaltParser?s greedy inference strat-
egy. The higher results for Link are to be expected,
as throughout the thread, most posts tend to link lo-
cally.
20
XXXXXXXXXTest
B/down [1, 2] [1, 4] [1, 6] [1, 8] [All]
[1, 2] .947/.947 ? ? ? ?
[1, 4] .946/.947 .836/.841 ? ?
[1, 6] .946/.947 .840/.841 .800/.794 ? ?
[1, 8] .946/.947 .840/.841 .800/.794 .780/.769 ?
[All] .946/.946 .840/.838 .800/.791 .776/.767 .756/.738
Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of
different lengths (indicated in the rows), broken down over different post extents (indicated in the columns)
6.3 In Situ Structure Prediction
As described in Section 5.2, we simulate in situ
thread discourse structure prediction by removing
differing numbers of posts from the tail of the thread,
and applying the trained model over the resultant
sub-threads. The results for in situ classification are
presented in Table 5, with the rows indicating the
size of the test sub-thread, and the columns being a
breakdown of results over different portions of the
classified thread. The reason that we do not pro-
vide numbers for all cells in the table is that the size
of the test sub-thread determines the post extents we
can breakdown the results into, e.g. we cannot return
results for posts 1?4 ([1, 4]) when the size of the test
thread was only two posts ([1, 2]).
From the results, we can see that both CRFSGD
and MaltParser are very robust when applied to par-
tial threads, to the extent that we actually achieve
higher results over shortened versions of the thread
than over the complete thread in some instances, al-
though the only difference that is statistically signif-
icant is over [1, 8] for CRFSGD, where the predic-
tion over the partial thread is actually superior to that
over the complete thread. From this, we can con-
clude that it is possible to apply our method to partial
threads without any reduction in effectiveness rela-
tive to classification over complete threads. As such,
our method is shown to be robust when applied to
real-time analysis of dynamically evolving threads.
6.4 User profile feature analysis
In our experiments, we noticed that the user profile
feature (UserProf) is the most effective feature for
both CRFSGD and MaltParser. To gain a deeper
insight into the behaviour of the feature, we binned
the posts according to the number of times the author
had posted in the training data, evaluated based on a
Bin uscore Posts Total Totalper user users posts
High 224.6 251 1 251
Medium 1?41.7 4?48 45 395
Low 0 2?4 157 377
Very Low 0 1 309 309
Table 6: Statistics for the 4 groups of users
user score (uscore) for each user:
uscorei =
?ni
j=1 spi,j
ni
where ni is the number of posts by user i, and spi,j is
the number of posts by user i that occur as training
instances for other posts by the same author. uscore
reflects the average training?test post ratio per user
in cross-validation. Note that as we include all posts
from a given thread in a single partition during cross-
validation, it is possible for an author to have posted
4 times, but have a uscore of 0 due to those posts all
occurring in the same thread.
We ranked the users in the dataset in descending
order of uscore, sub-ranking on ni in cases of a tie
in uscore. The users were binned into 4 groups
of roughly equal post size. The detailed statistics
are shown in Table 6, noting that the high-frequency
bin (?High?) contains posts from a single user. We
present the post-level micro-averaged F-score for
posts in each bin based on CRFSGD, with and with-
out user profile features, in Figure 4.
Contrary to expectation, the UserProf features
have the greatest impact for users with fewer posts.
In fact, a statistically significant difference was ob-
served only for users with no posts in the training
data (uscore = 0), where the F-score jumped over
10% in absolute terms for both the Low and Very
Low bins. Our explanation for this effect is that the
21
High Median Low Very Low0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
User Group
F ?
 
 With UserProfWithout UserProf
Figure 4: Post-level joint classification results for users
binned by uscore, based on CRFSGD with and without
UserProf features)
lack of user profile information is predictive of the
sort of posts we can expect from a user (i.e. they
tend to be newbie users, asking questions).
7 Conclusions and Future Work
In this research, we explored the joint classification
of web user forum thread discourse structure, in the
form of a rooted directed acyclic graph over posts,
with edges labelled with dialogue acts. Three classi-
fication approaches were proposed: separately pre-
dicting Link and DA labels, and composing them
into a joint class; predicting a combined Link-DA
class using a structured classifier; and applying de-
pendency parsing to the problem. We found the
combined approach based on CRFSGD to perform
best over the task, closely followed by dependency
parsing with MaltParser.
We also examined the task of in situ classification
of dialogue structure, in the form of predicting the
discourse structure of partial threads, as contrasted
with classifying only complete threads. We found
that there was no drop in F-score over different sub-
extents of the thread in classifying partial threads,
despite the relative lack of thread context.
In future work, we plan to delve further into de-
pendency parsing, looking specifically at the impli-
cations of multi-headedness and disconnected sub-
graphs on dependency parsing. We also intend to
carry out meta-classification, combining the predic-
tions of CRFSGD and MaltParser.
Our user profile features were found to be the
pick of our features, but counter-intuitively, to bene-
fit users with no posts in the training data, rather than
prolific users. We wish to explore this effect further,
including incorporating unsupervised user-level fea-
tures into our classifiers.
Acknowledgements
The authors wish to acknowledge the development
efforts of Johan Hall in configuring MaltParser to
handle numeric features, and be able to parse thread
structures. NICTA is funded by the Australian gov-
ernment as represented by Department of Broad-
band, Communication and Digital Economy, and the
Australian Research Council through the ICT Centre
of Excellence programme.
References
Le?on Bottou. 2011. CRFSGD software. http://
leon.bottou.org/projects/sgd.
Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, and
Ce Zhang. 2009. The use of categorization infor-
mation in language models for question retrieval. In
Proceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management (CIKM 2009), pages
265?274, Hong Kong, China.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ?speech acts?. In
Proceedings of 28th International ACM-SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR 2005), pages 345?352.
Jeffrey Chan and Conor Hayes. 2010. Decomposing dis-
cussion forums using user roles. In Proceedings of the
WebSci10: Extending the Frontiers of Society On-Line
(WebSci10), pages 1?8, Raleigh, USA.
Jeffrey Chan, Conor Hayes, and Elizabeth M. Daly.
2010. Decomposing discussion forums using user
roles. In Proceedings of the Fourth International AAAI
Conference on Weblogs and Social Media (ICWSM
2010), pages 215?8, Washington, USA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), pages 309?316, Barcelona, Spain.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
22
pairs from online forums. In Proceedings of 31st Inter-
national ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?08), pages
467?474, Singapore.
Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and seman-
tic roles of prepositional phrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2009), pages 450?458,
Singapore. Association for Computational Linguistics.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
text and answers of questions from online forums. In
Proceedings of the 46th Annual Meeting of the ACL:
HLT (ACL 2008), pages 710?718, Columbus, USA.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 30?41, Vancouver, Canada.
Jonathan L. Elsas and Jaime G. Carbonell. 2009. It
pays to be picky: An evaluation of thread retrieval
in online forums. In Proceedings of 32nd Interna-
tional ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?09), pages
714?715, Boston, USA.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of the 46th Annual
Meeting of the ACL: HLT (ACL 2008), pages 834?842,
Columbus, USA.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2009), pages 326?334, Boulder, Col-
orado. Association for Computational Linguistics.
Blaz Fortuna, Eduarda Mendes Rodrigues, and Natasa
Milic-Frayling. 2007. Improving the classification of
newsgroup messages through social network analysis.
In Proceedings of the 16th ACM Conference on In-
formation and Knowledge Management (CIKM 2007),
pages 877?880, Lisbon, Portugal.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010a. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010), pages 862?871, Boston, USA.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. In Proceedings
of the 14th Conference on Computational Natural Lan-
guage Learning (CoNLL-2010), pages 192?202, Upp-
sala, Sweden.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on Hu-
man Language Technologies, 2(1):1?127.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289, Williamstown, USA.
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2008.
The nature of requests and commitments in email mes-
sages. In Proceedings of the AAAI 2008 Workshop on
Enhanced Messaging, pages 42?47, Chicago, USA.
Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-
nie Skariah, Jian Yang, and Graciela Gonzalez. 2010.
Towards internet-age pharmacovigilance: Extracting
adverse drug reactions from user posts in health-
related social networks. In Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing (ACL 2010), pages 117?125, Uppsala, Sweden.
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL), Special Issue on Dialogue, 43(2):131?154.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
Wei Wang, and Lei Zhang. 2009. Modeling semantics
and structure of discussion threads. In Proceedings of
the 18th International Conference on the World Wide
Web (WWW 2009), pages 1103?1104, Madrid, Spain.
Marco Lui and Timothy Baldwin. 2009. You are what
you post: User-level features in threaded discourse. In
Proceedings of the 14th Australasian Document Com-
puting Symposium (ADCS 2009), Sydney, Australia.
Marco Lui and Timothy Baldwin. 2010. Classifying
user forum participants: Separating the gurus from the
hacks, and other tales of the internet. In Proceedings
of the 2010 Australasian Language Technology Work-
shop (ALTW 2010), pages 49?57, Melbourne, Aus-
tralia.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 122?131, Prague,
Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
23
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2006), pages 81?88, Trento,
Italy.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, Canada.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings
of the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 367?
374.
Klemens Muthmann, Wojciech M. Barczyn?ski, Falk
Brauer, and Alexander Lo?ser. 2009. Near-duplicate
detection for web-forums. In Proceedings of the 2009
International Database Engineering & Applications
Symposium (IDEAS 2009), pages 142?151, Cetraro,
Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95?135.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop Incremental Parsing: Bringing Engineer-
ing and Cognition Together (ACL-2004), pages 50?57,
Barcelona, Spain.
Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.
Levin, and Carol Van Ess-Dykema. 1995. Discourse
processing of dialogues with multiple threads. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 31?38,
Cambridge, USA.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING 2008), pages 753?760, Manchester,
UK.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT-09), pages 81?
84, Paris, France.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in comments
on news-articles. In Proceedings of the 9th Annual
ACM International Workshop on Web Information and
Data Management, pages 97?104, Lisboa, Portugal.
Jangwon Seo, W. Bruce Croft, and David A. Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 1907?1910, Hong Kong, China.
Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, USA.
Parikshit Sondhi, Manish Gupta, ChengXiang Zhai, and
Julia Hockenmaier. 2010. Shallow information ex-
traction from medical forum data. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), Posters Volume, pages
1158?1166, Beijing, China.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 149?156, Edmonton,
Canada.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Pail
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Charles Sutton and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 225?228, Ann
Arbor, Michigan. Association for Computational Lin-
guistics.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 64?71, Washington, USA.
Nayer Wanas, Motaz El-Saban, Heba Ashour, and
Waleed Ammar. 2008. Automatic scoring of online
discussion posts. In Proceeding of the 2nd ACM work-
shop on Information credibility on the web (WICOW
?08), pages 19?26, Napa Valley, USA.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Mak-
ing conversational structure explicit: identification of
initiation-response pairs within online discussions. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 673?676.
24
Yi-Chia Wang, Mahesh Joshi, and Carolyn Rose?. 2007.
A feature based approach to leveraging context for
classifying newsgroup style discussion segments. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions
(ACL 2007), pages 73?76, Prague, Czech Republic.
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, and
Carolyn Rose?. 2008. Recovering implicit thread
structure in newsgroup style conversations. In Pro-
ceedings of the Second International Conference on
Weblogs and Social Media (ICWSM 2008), pages 152?
160, Seattle, USA.
V. Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated dialog act segmentation and classification
using prosodic features and language models. In Proc.
Eurospeech, volume 1, pages 207?210.
Markus Weimer and Iryna Gurevych. 2007. Predicting
the perceived quality of web forum posts. In Proceed-
ings of the 2007 International Conference on Recent
Advances in Natural Language Processing (RANLP
2007), pages 643?648, Borovets, Bulgaria.
Markus Weimer, Iryna Gurevych, and Max Mu?hlha?user.
2007. Automatically assessing the post quality in on-
line discussions on software. In Proceedings of the
45th Annual Meeting of the ACL: Interactive Poster
and Demonstration Sessions, pages 125?128, Prague,
Czech Republic.
Armin Weinberger and Frank Fischer. 2006. A
framework to analyze argumentative knowledge con-
struction in computer-supported collaborative learn-
ing. Computers & Education, 46:71?95, January.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?287.
Wensi Xi, Jesper Lind, and Eric Brill. 2004. Learning
effective ranking functions for newsgroup search. In
Proceedings of 27th International ACM-SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR 2004), pages 394?401. Sheffield,
UK.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
25
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229?237,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Language Identification: The Long and the Short of the Matter
Timothy Baldwin and Marco Lui
Dept of Computer Science and Software Engineering
University of Melbourne, VIC 3010 Australia
tb@ldwin.net, saffsd@gmail.com
Abstract
Language identification is the task of identify-
ing the language a given document is written
in. This paper describes a detailed examina-
tion of what models perform best under dif-
ferent conditions, based on experiments across
three separate datasets and a range of tokeni-
sation strategies. We demonstrate that the task
becomes increasingly difficult as we increase
the number of languages, reduce the amount
of training data and reduce the length of docu-
ments. We also show that it is possible to per-
form language identification without having to
perform explicit character encoding detection.
1 Introduction
With the growth of the worldwide web, ever-
increasing numbers of documents have become
available, in more and more languages. This growth
has been a double-edged sword, however, in that
content in a given language has become more preva-
lent but increasingly hard to find, due to the web?s
sheer size and diversity of content. While the ma-
jority of (X)HTML documents declare their charac-
ter encoding, only a tiny minority specify what lan-
guage they are written in, despite support for lan-
guage declaration existing in the various (X)HTML
standards.1 Additionally, a single encoding can gen-
erally be used to render a large number of languages
such that the document encoding at best filters out
a subset of languages which are incompatible with
the given encoding, rather than disambiguates the
source language. Given this, the need for automatic
means to determine the source language of web doc-
1http://dev.opera.com/articles/view/
mama-head-structure/
uments is crucial for web aggregators of various
types.
There is widespread misconception of language
identification being a ?solved task?, generally as
a result of isolated experiments over homogeneous
datasets with small numbers of languages (Hughes
et al, 2006; Xia et al, 2009). Part of the motivation
for this paper is to draw attention to the fact that, as
a field, we are still a long way off perfect language
identification of web documents, as evaluated under
realistic conditions.
In this paper we describe experiments on lan-
guage identification of web documents, focusing on
the broad question of what combination of tokenisa-
tion strategy and classification model achieves the
best overall performance. We additionally evalu-
ate the impact of the volume of training data and
the test document length on the accuracy of lan-
guage identification, and investigate the interaction
between character encoding detection and language
identification.
One assumption we make in this research, follow-
ing standard assumptions made in the field, is that all
documents are monolingual. This is clearly an un-
realistic assumption when dealing with general web
documents (Hughes et al, 2006), and we plan to re-
turn to investigate language identification over mul-
tilingual documents in future work.
Our contributions in this paper are: the demon-
stration that language identification is: (a) trivial
over datasets with smaller numbers of languages
and approximately even amounts of training data per
language, but (b) considerably harder over datasets
with larger numbers of languages with more skew
in the amount of training data per language; byte-
based tokenisation without character encoding de-
tection is superior to codepoint-based tokenisation
229
with character encoding detection; and simple co-
sine similarity-based nearest neighbour classifica-
tion is equal to or better than models including sup-
port vector machines and naive Bayes over the lan-
guage identification task. We also develop datasets
to facilitate standardised evaluation of language
identification.
2 Background Research
Language identification was arguably established as
a task by Gold (1967), who construed it as a closed
class problem: given data in each of a predefined set
of possible languages, human subjects were asked
to classify the language of a given test document. It
wasn?t until the 1990s, however, that the task was
popularised as a text categorisation task.
The text categorisation approach to language
identification applies a standard supervised classi-
fication framework to the task. Perhaps the best-
known such model is that of Cavnar and Tren-
kle (1994), as popularised in the textcat tool.2
The method uses a per-language character frequency
model, and classifies documents via their relative
?out of place? distance from each language (see
Section 5.1). Variants on this basic method in-
clude Bayesian models for character sequence pre-
diction (Dunning, 1994), dot products of word fre-
quency vectors (Darnashek, 1995) and information-
theoretic measures of document similarity (Aslam
and Frost, 2003; Martins and Silva, 2005). More
recently, support vector machines (SVMs) and ker-
nel methods have been applied to the task of lan-
guage identification task with success (Teytaud and
Jalam, 2001; Lodhi et al, 2002; Kruengkrai et al,
2005), and Markov logic has been used for joint in-
ferencing in contexts where there are multiple evi-
dence sources (Xia et al, 2009).
Language identification has also been carried out
via linguistically motivated models. Johnson (1993)
used a list of stop words from different languages to
identify the language of a given document, choos-
ing the language with the highest stop word over-
lap with the document. Grefenstette (1995) used
word and part of speech (POS) correlation to de-
termine if two text samples were from the same
or different languages. Giguet (1995) developed a
2http://www.let.rug.nl/vannoord/TextCat/
cross-language tokenisation model and used it to
identify the language of a given document based
on its tokenisation similarity with training data.
Dueire Lins and Gonc?alves (2004) considered the
use of syntactically-derived closed grammatical-
class models, matching syntactic structure rather
than words or character sequences.
The observant reader will have noticed that some
of the above approaches make use of notions such
as ?word?, typically based on the naive assumption
that the language uses white space to delimit words.
These approaches are appropriate in contexts where
there is a guarantee of a document being in one of
a select set of languages where words are space-
delimited, or where manual segmentation has been
performed (e.g. interlinear glossed text). However,
we are interested in language identification of web
documents, which can be in any language, includ-
ing languages that do not overtly mark word bound-
aries, such as Japanese, Chinese and Thai; while
relatively few languages fall into this categories,
they are among the most populous web languages
and therefore an important consideration. There-
fore, approaches that assume a language is space-
delimited are clearly not suitable for our purposes.
Equally, approaches which make assumptions about
the availability of particular resources for each lan-
guage to be identified (e.g. POS taggers, or the ex-
istence of precompiled stop word lists) cannot be
used.
Language identification has been applied in a
number of contexts, the most immediate applica-
tion being in multilingual text retrieval, where re-
trieval results are generally superior if the language
of the query is known, and the search is restricted
to only those documents predicted to be in that lan-
guage (McNamee and Mayfield, 2004). It can also
be used to ?word spot? foreign language terms in
multilingual documents, e.g. to improve parsing per-
formance (Alex et al, 2007), or for linguistic corpus
creation purposes (Baldwin et al, 2006; Xia et al,
2009; Xia and Lewis, 2009).
3 Datasets
In the experiments reported in this paper, we em-
ploy three novel datasets, with differing properties
relevant to language identification research:
230
Corpus Documents Languages Encodings Document Length (bytes)
EUROGOV 1500 10 1 17460.5?39353.4
TCL 3174 60 12 2623.2?3751.9
WIKIPEDIA 4963 67 1 1480.8?4063.9
Table 1: Summary of the three language identification datasets
Figure 1: Distribution of languages in the three datasets
(vector of languages vs. the proportion of documents in
that language)
EUROGOV: longer documents, all in a single en-
coding, spread evenly across a relatively small num-
ber (10) of Western European languages; this dataset
is comparable to the datasets conventionally used in
language identification research. As the name would
suggest, the documents were sourced from the Euro-
GOV document collection, as used in the 2005 Web-
CLEF task.
TCL: a larger number of languages (60) across a
wider range of language families, with shorter docu-
ments and a range of character encodings (12). The
collection was manually sourced by the Thai Com-
putational Linguistics Laboratory (TCL) in 2005
from online news sources.
WIKIPEDIA: a slightly larger number of lan-
guages again (67), a single encoding, and shorter
documents; the distribution of languages is intended
to approximate that of the actual web. This col-
lection was automatically constructed by taking the
dumps of all versions of Wikipedia with 1000 or
more documents in non-constructed languages, and
randomly selecting documents from them in a bias-
preserving manner (i.e. preserving the document
distribution in the full collection); this is intended to
represent the document language bias observed on
the web. All three corpora are available on request.
We outline the characteristics of the three datasets
in Table 1. We further detail the language distri-
bution in Figure 1, using a constant vector of lan-
guages for all three datasets, based on the order of
languages in the WIKIPEDIA dataset (in descending
order of documents per language). Of note are the
contrasting language distributions between the three
datasets, in terms of both the languages represented
and the relative skew of documents per language. In
the following sections, we provide details of the cor-
pus compilation and document sampling method for
each dataset.
4 Document Representation
As we are interested in performing language iden-
tification over arbitrary web documents, we re-
quire a language-neutral document representation
which does not make artificial assumptions about the
source language of the document. Separately, there
is the question of whether it is necessary to deter-
mine the character encoding of the document in or-
der to extract out character sequences, or whether
the raw byte stream is sufficient. To explore this
question, we experiment with two document repre-
sentations: (1) byte n-grams, and (2) codepoint n-
grams. In both cases, a document is represented as a
feature vector of token counts.
Byte n-grams can be extracted directly without
explicit encoding detection. Codepoint n-grams, on
the other hand, require that we know the character
encoding of the document in order to perform to-
kenisation. Additionally, they should be based on a
common encoding to prevent: (a) over-fragmenting
the feature space (e.g. ending up with discrete fea-
ture spaces for euc-jp, s-jis and utf-8 in
the case of Japanese); and (b) spurious matches be-
tween encodings (e.g. Japanese hiragana and Ko-
rean hangul mapping onto the same codepoint in
euc-jp and euc-kr, respectively). We use uni-
231
code as the common encoding for all documents.
In practice, character encoding detection is an is-
sue only for TCL, as the other two datasets are in
a single encoding. Where a character encoding was
provided for a document in TCL and it was possi-
ble to transcode the document to unicode based on
that encoding, we used the encoding information. In
cases where a unique encoding was not provided,
we used an encoding detection library based on the
Mozilla browser.3 Having disambiguated the encod-
ing for each document, we transcoded it into uni-
code.
5 Models
In our experiments we use a number of different
language identification models, as outlined below.
We first describe the nearest-neighbour and nearest-
prototype models, and a selection of distance and
similarity metrics combined with each. We then
present three standalone text categorisation models.
5.1 Nearest-Neighbour and Nearest-Prototype
Models
The 1-nearest-neighbour (1NN) model is a common
classification technique, whereby a test document
D is classified based on the language of the clos-
est training document Di (with language l(Di)), as
determined by a given distance or similarity metric.
In nearest-neighbour models, each training doc-
ument is represented as a single instance, mean-
ing that the computational cost of classifying a test
document is proportional to the number of training
documents. A related model which aims to reduce
this cost is nearest-prototype (AM), where each lan-
guage is represented as a single instance, by merging
all of the training instances for that language into a
single centroid via the arithmetic mean.
For both nearest-neighbour and nearest-prototype
methods, we experimented with three similarity and
distance measures in this research:
Cosine similarity (COS): the cosine of the angle
between two feature vectors, as measured by the dot
product of the two vectors, normalised to unit length.
Skew divergence (SKEW): a variant of Kullback-
Leibler divergence, whereby the second distribution
3http://chardet.feedparser.org/
(y) is smoothed by linear interpolation with the first
(x) using a smoothing factor ? (Lee, 2001):
s?(x, y) = D(x || ?y + (1? ?)x)
where:
D(x || y) =
?
i
xi(log2 xi ? log2 yi)
In all our experiments, we set ? to 0.99.
Out-of-place (OOP): a ranklist-based distance
metric, where the distance between two documents
is calculated as (Cavnar and Trenkle, 1994):
oop(Dx, Dy) =
?
t?Dx?Dy
abs(RDx(t)?RDy(t))
RD(t) is the rank of term t in document D, based
on the descending order of frequency in document
D; terms not occurring in document D are conven-
tionally given the rank 1 + maxi RD(ti).
5.2 Naive Bayes (NB)
Naive Bayes is a popular text classification model,
due to it being lightweight, robust and easy to up-
date. The language of test document D is predicted
by:
l?(D) = arg max
li?L
P (li)
|V |
?
j=1
P (tj |li)ND,tj
ND,tj !
where L is the set of languages in the training data,
ND,tj is the frequency of the jth term in D, V is the
set of all terms, and:
P (t|li) =
1 +
?|D |
k=1 Nk,tP (li|Dk)
|V |+ ?|V |j=1
?|D |
k=1 Nk,tjP (li|Dk)
In this research, we use the rainbow imple-
mentation of multinominal naive Bayes (McCallum,
1996).
5.3 Support Vector Machines (SVM)
Support vector machines (SVMs) are one of the
most popular methods for text classification, largely
because they can automatically weight large num-
bers of features, capturing feature interactions in the
process (Joachims, 1998; Manning et al, 2008). The
basic principle underlying SVMs is to maximize the
232
margin between training instances and the calculated
decision boundary based on structural risk minimi-
sation (Vapnik, 1995).
In this work, we have made use of bsvm,4 an
implementation of SVMs with multiclass classifica-
tion support (Hsu et al, 2008). We only report re-
sults for multi-class bound-constrained support vec-
tor machines with linear kernels, as they were found
to perform best over our data.
6 Experimental Methodology
We carry out experiments over the cross-product of
the following options, as described above:
model (?7): nearest-neighbour (COS1NN,
SKEW1NN, OOP1NN), nearest-prototype
(COSAM, SKEWAM),5 NB, SVM
tokenisation (?2): byte, codepoint
n-gram (?3): 1-gram, 2-gram, 3-gram
for a total of 42 distinct classifiers. Each classi-
fier is run across the 3 datasets (EUROGOV, TCL
and WIKIPEDIA) based on 10-fold stratified cross-
validation.
We evaluate the models using micro-averaged
precision (P?), recall (R?) and F-score (F?), as well
as macro-averaged precision (PM ), recall (RM ) and
F-score (FM ). The micro-averaged scores indicate
the average performance per document; as we al-
ways make a unique prediction per document, the
micro-averaged precision, recall and F-score are al-
ways identical (as is the classification accuracy).
The macro-averaged scores, on the other hand, indi-
cate the average performance per language. In each
case, we average the precision, recall and F-score
across the 10 folds of cross validation.6
As a baseline, we use a majority class, or ZeroR,
classifier (ZEROR), which assigns the language with
highest prior in the training data to each of the test
documents.
4http://www.csie.ntu.edu.tw/?cjlin/bsvm/
5We do not include the results for nearest-prototype classi-
fiers with the OOP distance metric as the results were consid-
erably lower than the other methods.
6Note that this means that the averaged FM is not necessar-
ily the harmonic mean of the averaged PM andRM .
Model Token PM RM FM P?/R?/F?
ZEROR ? .020 .084 .032 .100
COS1NN byte .975 .978 .976 .975
COS1NN codepoint .968 .973 .970 .971
COSAM byte .922 .938 .926 .937
COSAM codepoint .908 .930 .913 .931
SKEW1NN byte .979 .979 .979 .977
SKEW1NN codepoint .978 .978 .978 .976
SKEWAM byte .974 .972 .972 .969
SKEWAM codepoint .974 .972 .973 .970
OOP1NN byte .953 .952 .953 .949
OOP1NN codepoint .961 .960 .960 .957
NB byte .975 .973 .974 .971
NB codepoint .975 .973 .974 .971
SVM byte .989 .985 .987 .987
SVM codepoint .988 .985 .986 .987
Table 2: Results for byte vs. codepoint (bigram) tokeni-
sation over EUROGOV
7 Results
In our experiments, we first compare the different
models for fixed n-gram order, then come back to
vary the n-gram order. Subsequently, we examine
the relative performance of the different models on
test documents of differing lengths, and finally look
into the impact of the amount of training data for
a given language on the performance for that lan-
guage.
7.1 Results for the Different Models and
Tokenisation Strategies
First, we present the results for each of the classifiers
in Tables 2?4, based on byte or codepoint tokenisa-
tion and bigrams. In each case, we present the best
result in each column in bold.
The relative performance over EUROGOV and
TCL is roughly comparable for all methods barring
SKEW1NN, with near-perfect scores over all 6 eval-
uation metrics. SKEW1NN is near-perfect over EU-
ROGOV and TCL, but drops to baseline levels over
WIKIPEDIA; we return to discuss this effect in Sec-
tion 7.2.
In the case of EUROGOV, the near-perfect re-
sults are in line with our expectations for the dataset,
based on its characteristics and results reported for
comparable datasets. The results for WIKIPEDIA,
however, fall off considerably, with the best model
achieving an FM of .671 and F? of .869, due to
233
Model Token PM RM FM P?/R?/F?
ZEROR ? .003 .017 .005 .173
COS1NN byte .981 .975 .975 .982
COS1NN codepoint .931 .930 .925 .961
COSAM byte .967 .975 .965 .965
COSAM codepoint .979 .977 .974 .964
SKEW1NN byte .984 .974 .976 .987
SKEW1NN codepoint .910 .210 .320 .337
SKEWAM byte .962 .959 .950 .972
SKEWAM codepoint .968 .961 .957 .967
OOP1NN byte .964 .945 .951 .974
OOP1NN codepoint .901 .892 .893 .933
NB byte .905 .905 .896 .969
NB codepoint .722 .711 .696 .845
SVM byte .981 .973 .977 .984
SVM codepoint .979 .970 .974 .980
Table 3: Results for byte vs. codepoint (bigram) tokeni-
sation over TCL
the larger number of languages, smaller documents,
and skew in the amounts of training data per lan-
guage. All models are roughly balanced in the rel-
ative scores they attain for PM , RM and FM (i.e.
there are no models that have notably higherPM rel-
ative to RM , for example).
The nearest-neighbour models outperform the
corresponding nearest-prototype models to varying
degrees, with the one exception of SKEW1NN over
WIKIPEDIA. The nearest-prototype classifiers were
certainly faster than the nearest-neighbour classi-
fiers, by roughly an order of 10, but this is more
than outweighed by the drop in classification per-
formance. With the exception of SKEW1NN over
WIKIPEDIA, all methods were well above the base-
lines for all three datasets.
The two methods which perform consistently well
at this point are COS1NN and SVM, with COS1NN
holding up particularly well under micro-averaged
F-score while NB drops away over WIKIPEDIA, the
most skewed dataset; this is due to the biasing effect
of the prior in NB.
Looking to the impact of byte- vs. codepoint-
tokenisation on classifier performance over the three
datasets, we find that overall, bytes outperform
codepoints. This is most notable for TCL and
WIKIPEDIA, and the SKEW1NN and NB models.
Given this result, we present only results for byte-
based tokenisation in the remainder of this paper.
Model Token PM RM FM P?/R?/F?
ZEROR ? .004 .013 .007 .328
COS1NN byte .740 .646 .671 .869
COS1NN codepoint .685 .604 .625 .835
COSAM byte .587 .634 .573 .776
COSAM codepoint .486 .556 .483 .725
SKEW1NN byte .005 .013 .008 .304
SKEW1NN codepoint .006 .013 .007 .241
SKEWAM byte .605 .617 .588 .844
SKEWAM codepoint .552 .575 .532 .807
OOP1NN byte .619 .518 .548 .831
OOP1NN codepoint .598 .486 .520 .807
NB byte .496 .454 .442 .851
NB codepoint .426 .349 .360 .798
SVM byte .667 .545 .577 .845
SVM codepoint .634 .494 .536 .818
Table 4: Results for byte vs. codepoint (bigram) tokeni-
sation over WIKIPEDIA
The results for byte tokenisation of TCL are par-
ticularly noteworthy. The transcoding into unicode
and use of codepoints, if anything, hurts perfor-
mance, suggesting that implicit character encoding
detection based on byte tokenisation is the best ap-
proach: it is both more accurate and simplifies the
system, in removing the need to perform encoding
detection prior to language identification.
7.2 Results for Differing n-gram Sizes
We present results with byte unigrams, bigrams and
trigrams in Table 5 for WIKIPEDIA.7 We omit re-
sults for the other two datasets, as the overall trend is
the same as for WIKIPEDIA, with lessened relative
differences between n-gram orders due to the rela-
tive simplicity of the respective classification tasks.
SKEW1NN is markedly different to the other meth-
ods in achieving the best performance with uni-
grams, moving from the worst-performing method
by far to one of the best-performing methods. This
is the result of the interaction between data sparse-
ness and heavy-handed smoothing with the ? con-
stant. Rather than using a constant ? value for all
n-gram orders, it may be better to parameterise it
using an exponential scale such as ? = 1??n (with
7The results for OOP1NN over byte trigrams are missing
due to the computational cost associated with the method, and
our experiment hence not having run to completion at the time
of writing. Extrapolating from the results for the other two
datasets, we predict similar results to bigrams.
234
Model n-gram PM RM FM P?/R?/F?
ZEROR ? .004 .013 .007 .328
COS1NN 1 .644 .579 .599 .816
COS1NN 2 .740 .646 .671 .869
COS1NN 3 .744 .656 .680 .862
COSAM 1 .526 .543 .487 .654
COSAM 2 .587 .634 .573 .776
COSAM 3 .553 .632 .545 .761
SKEW1NN 1 .691 .598 .625 .848
SKEW1NN 2 .005 .013 .008 .304
SKEW1NN 3 .005 .013 .004 .100
SKEWAM 1 .552 .569 .532 .740
SKEWAM 2 .605 .617 .588 .844
SKEWAM 3 .551 .631 .554 .825
OOP1NN 1 .519 .446 .468 .747
OOP1NN 2 .619 .518 .548 .831
NB 1 .576 .578 .555 .778
NB 2 .496 .454 .442 .851
NB 3 .493 .435 .432 .863
SVM 1 .585 .505 .523 .812
SVM 2 .667 .545 .577 .845
SVM 3 .717 .547 .594 .840
Table 5: Results for different n-gram orders over
WIKIPEDIA
? = 0.01, e.g.), based on the n-gram order. We
leave this for future research.
For most methods, bigrams and trigrams are bet-
ter than unigrams, with the one notable exception
of SKEW1NN. In general, there is little separating
bigrams and trigrams, although the best result for is
achieved slightly more often for bigrams than for tri-
grams.
For direct comparability with Cavnar and Tren-
kle (1994), we additionally carried out a preliminary
experiment with hybrid byte n-grams (all of 1- to 5-
grams), combined with simple frequency-based fea-
ture selection of the top-1000 features for each n-
gram order. The significance of this setting is that it
is the strategy adopted by textcat, based on the
original paper of Cavnar and Trenkle (1994) (with
the one exception that we use 1000 features rather
than 300, as all methods other than OOP1NN bene-
fitted from more features). The results are shown in
Table 6.
Compared to the results in Table 5, SKEW1NN and
SKEWAM both increase markedly to achieve the best
overall results. OOP1NN, on the other hand, rises
slightly, while the remaining three methods actually
Model PM RM FM P?/R?/F?
ZEROR .004 .013 .007 .328
COS1NN .735 .664 .682 .865
COSAM .592 .626 .580 .766
SKEW1NN .789 .708 .729 .902
SKEWAM .681 .718 .680 .870
OOP1NN .697 .595 .626 .864
SVM .669 .500 .544 .832
Table 6: Results for mixed n-grams (1?5) and feature se-
lection over WIKIPEDIA (a la? Cavnar and Trenkle (1994))
drop back slightly. Clearly, there is considerably
more experimentation to be done here with mixed
n-gram models and different feature selection meth-
ods, but the results indicate that some methods cer-
tainly benefit from n-gram hybridisation and feature
selection, and also that we have been able to sur-
pass the results of Cavnar and Trenkle (1994) with
SKEW1NN in an otherwise identical framework.
7.3 Breakdown Across Test Document Length
To better understand the impact of test document
size on classification accuracy, we divided the test
documents into 5 equal-size bins according to their
length, measured by the number of tokens. We then
computed F? individually for each bin across the 10
folds of cross validation. We present the breakdown
of results for WIKIPEDIA in Figure 2.
WIKIPEDIA shows a pseudo-logarithmic growth
in F? (= P? = R?) as the test document size in-
creases. This fits with our intuition, as the model
has progressively more evidence to base the classi-
fication on. It also suggests that performance over
shorter documents appears to be the dominating fac-
tor in the overall ranking of the different methods.
In particular, COS1NN and SVM appear to be able to
classify shorter documents most reliably, leading to
the overall result of them being the best-performing
methods.
While we do not show the graph for reasons of
space, the equivalent graph for EUROGOV displays
a curious effect: F? drops off as the test documents
get longer. Error analysis of the data indicates that
this is due to longer documents being more likely
to be ?contaminated? with either data from a sec-
ond language or extra-linguistic data, such as large
tables of numbers or chemical names. This sug-
gests that all the models are brittle when the assump-
235
Figure 2: Breakdown of F? over WIKIPEDIA for test
documents of increasing length
Figure 3: Per-language FM for COS1NN, relative to the
training data size (in MB) for that language
tion of strict monolingualism is broken, or when
the document is dominated by extra-linguistic data.
Clearly, this underlines our assumption of monolin-
gual documents, and suggests multilingual language
identification is a fertile research area even in terms
of optimising performance over our ?monolingual?
datasets.
7.4 Performance Relative to Training Data Size
As a final data point in our analysis, we calculated
the FM for each language relative to the amount of
training data available for that language, and present
the results in the form of a combined scatter plot for
the three datasets in Figure 3. The differing distri-
butions of the three datasets are self-evident, with
most languages in EUROGOV (the squares) both
having reasonably large amounts of training data and
achieving high FM values, but the majority of lan-
guages in WIKIPEDIA (the crosses) having very lit-
tle data (including a number of languages with no
training data, as there is a singleton document in that
language in the dataset). As an overall trend, we can
observe that the greater the volume of training data,
the higher the FM across all three datasets, but there
is considerable variation between the languages in
terms of their FM for a given training data size (the
column of crosses for WIKIPEDIA to the left of the
graph is particularly striking).
8 Conclusions
We have carried out a thorough (re)examination of
the task of language identification, that is predict-
ing the language that a given document is written
in, focusing on monolingual documents at present.
We experimented with a total of 7 models, and
tested each over two tokenisation strategies (bigrams
vs. codepoints) and three token n-gram orders (un-
igrams, bigrams and trigrams). At the same time
as reproducing results from earlier research on how
easy the task can be over small numbers of lan-
guages with longer documents, we demonstrated
that the task becomes much harder for larger num-
bers of languages, shorter documents and greater
class skew. We also found that explicit character
encoding detection is not necessary in language de-
tection, and that the most consistent model overall
is either a simple 1-NN model with cosine similar-
ity, or an SVM with a linear kernel, using a byte
bigram or trigram document representation. We also
confirmed that longer documents tend to be easier to
classify, but also that multilingual documents cause
problems for the standard model of language identi-
fication.
Acknowledgements
This research was supported by a Google Research
Award.
References
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
236
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,
Czech Republic.
Javed A. Aslam and Meredith Frost. 2003. An
information-theoretic measure for document similar-
ity. In Proceedings of 26th International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR 2003), pages 449?450, Toronto,
Canada.
Timothy Baldwin, Steven Bird, and Baden Hughes.
2006. Collecting low-density language materials on
the web. In Proceedings of the 12th Australasian Web
Conference (AusWeb06). http://www.ausweb.
scu.edu.au/ausweb06/edited/hughes/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, Las Vegas, USA.
Marc Darnashek. 1995. Gauging similarity with n-
grams: Language-independent categorization of text.
Science, 267:843?848.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128?1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Emmanuel Giguet. 1995. Categorization according to
language: A step toward combining linguistic knowl-
edge and statistic learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 5:447?474.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi Sta-
tistica dei Dati Testuali (JADT), pages 263?268.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2008. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence National Taiwan University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485?488, Genoa, Italy.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. In Proceedings of the 10th European Confer-
ence on Machine Learning, pages 137?142, Chemnitz,
Germany.
Stephen Johnson. 1993. Solving the problem of lan-
guage recognition. Technical report, School of Com-
puter Studies, University of Leeds.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896?899, Beijing, China.
Lillian Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Proceedings
of Artificial Intelligence and Statistics 2001 (AISTATS
2001), pages 65?72, Key West, USA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Bruno Martins and Ma?rio J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764?
768, Santa Fe, USA.
Andrew Kachites McCallum. 1996. Bow: A toolkit for
statistical language modeling, text retrieval, classifica-
tion and clustering. http://www.cs.cmu.edu/
?mccallum/bow.
Paul McNamee and JamesMayfield. 2004. CharacterN -
gram Tokenization for European Language Text Re-
trieval. Information Retrieval, 7(1?2):73?97.
Olivier Teytaud and Radwan Jalam. 2001. Kernel-
based text categorization. In Proceedings of the
International Joint Conference on Neural Networks
(IJCNN?2001), Washington DC, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin, Germany.
Fei Xia and William Lewis. 2009. Applying NLP tech-
nologies to the collection and enrichment of language
data on the web to aid linguistic research. In Pro-
ceedings of the EACL 2009 Workshop on Language
Technology and Resources for Cultural Heritage, So-
cial Sciences, Humanities, and Education (LaTeCH ?
SHELT&R 2009), pages 51?59, Athens, Greece.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proceedings of the 12th Conference of the
EACL (EACL 2009), pages 870?878, Athens, Greece.
237
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25?30,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
langid.py: An Off-the-shelf Language Identification Tool
Marco Lui and Timothy Baldwin
NICTA VRL
Department of Computing and Information Systems
University of Melbourne, VIC 3010, Australia
mhlui@unimelb.edu.au, tb@ldwin.net
Abstract
We present langid.py, an off-the-shelf lan-
guage identification tool. We discuss the de-
sign and implementation of langid.py, and
provide an empirical comparison on 5 long-
document datasets, and 2 datasets from the mi-
croblog domain. We find that langid.py
maintains consistently high accuracy across
all domains, making it ideal for end-users that
require language identification without want-
ing to invest in preparation of in-domain train-
ing data.
1 Introduction
Language identification (LangID) is the task of de-
termining the natural language that a document is
written in. It is a key step in automatic processing
of real-world data, where a multitude of languages
may be present. Natural language processing tech-
niques typically pre-suppose that all documents be-
ing processed are written in a given language (e.g.
English), but as focus shifts onto processing docu-
ments from internet sources such as microblogging
services, this becomes increasingly difficult to guar-
antee. Language identification is also a key compo-
nent of many web services. For example, the lan-
guage that a web page is written in is an important
consideration in determining whether it is likely to
be of interest to a particular user of a search engine,
and automatic identification is an essential step in
building language corpora from the web. It has prac-
tical implications for social networking and social
media, where it may be desirable to organize com-
ments and other user-generated content by language.
It also has implications for accessibility, since it en-
ables automatic determination of the target language
for automatic machine translation purposes.
Many applications could potentially benefit from
automatic language identification, but building a
customized solution per-application is prohibitively
expensive, especially if human annotation is re-
quired to produce a corpus of language-labelled
training documents from the application domain.
What is required is thus a generic language identi-
fication tool that is usable off-the-shelf, i.e. with no
end-user training and minimal configuration.
In this paper, we present langid.py, a LangID
tool with the following characteristics: (1) fast,
(2) usable off-the-shelf, (3) unaffected by domain-
specific features (e.g. HTML, XML, markdown),
(4) single file with minimal dependencies, and (5)
flexible interface
2 Methodology
langid.py is trained over a naive Bayes clas-
sifier with a multinomial event model (McCallum
and Nigam, 1998), over a mixture of byte n-grams
(1?n?4). One key difference from conventional
text categorization solutions is that langid.py
was designed to be used off-the-shelf. Since
langid.py implements a supervised classifier,
this presents two primary challenges: (1) a pre-
trained model must be distributed with the classi-
fier, and (2) the model must generalize to data from
different domains, meaning that in its default con-
figuration, it must have good accuracy over inputs
as diverse as web pages, newspaper articles and mi-
croblog messages. (1) is mostly a practical consid-
eration, and so we will address it in Section 3. In
order to address (2), we integrate information about
the language identification task from a variety of do-
mains by using LD feature selection (Lui and Bald-
win, 2011).
Lui and Baldwin (2011) showed that it is rela-
tively easy to attain high accuracy for language iden-
25
Dataset Documents Langs Doc Length (bytes)
EUROGOV 1500 10 1.7?104 ?3.9?104
TCL 3174 60 2.6?103 ?3.8?103
WIKIPEDIA 4963 67 1.5?103 ?4.1?103
EMEA 19988 22 2.9?105 ?7.9?105
EUROPARL 20828 22 1.7?102 ?1.6?102
T-BE 9659 6 1.0?102 ?3.2?101
T-SC 5000 5 8.8?101 ?3.9?101
Table 1: Summary of the LangID datasets
tification in a traditional text categorization setting,
where we have in-domain training data. The task be-
comes much harder when trying to perform domain
adaptation, that is, trying to use model parameters
learned in one domain to classify data from a dif-
ferent domain. LD feature selection addresses this
problem by focusing on key features that are relevant
to the language identification task. It is based on In-
formation Gain (IG), originally introduced as a split-
ting criteria for decision trees (Quinlan, 1986), and
later shown to be effective for feature selection in
text categorization (Yang and Pedersen, 1997; For-
man, 2003). LD represents the difference in IG with
respect to language and domain. Features with a
high LD score are informative about language with-
out being informative about domain. For practi-
cal reasons, before the IG calculation the candidate
feature set is pruned by means of a term-frequency
based feature selection.
Lui and Baldwin (2011) presented empirical evi-
dence that LD feature selection was effective for do-
main adaptation in language identification. This re-
sult is further supported by our evaluation, presented
in Section 5.
3 System Architecture
The full langid.py package consists of the
language identifier langid.py, as well as two
support modules LDfeatureselect.py and
train.py.
langid.py is the single file which packages the
language identification tool, and the only file needed
to use langid.py for off-the-shelf language iden-
tification. It comes with an embedded model which
covers 97 languages using training data drawn from
5 domains. Tokenization and feature selection are
carried out in a single pass over the input document
via Aho-Corasick string matching (Aho and Cora-
sick, 1975). The Aho-Corasick string matching al-
gorithm processes an input by means of a determin-
istic finite automaton (DFA). Some states of the au-
tomaton are associated with the completion of one
of the n-grams selected through LD feature selec-
tion. Thus, we can obtain our document represen-
tation by simply counting the number of times the
DFA enters particular states while processing our in-
put. The DFA and the associated mapping from state
to n-gram are constructed during the training phase,
and embedded as part of the pre-trained model.
The naive Bayes classifier is implemented using
numpy,1 the de-facto numerical computation pack-
age for Python. numpy is free and open source, and
available for all major platforms. Using numpy in-
troduces a dependency on a library that is not in the
Python standard library. This is a reasonable trade-
off, as numpy provides us with an optimized im-
plementation of matrix operations, which allows us
to implement fast naive Bayes classification while
maintaining the single-file concept of langid.py.
langid.py can be used in the three ways:
Command-line tool: langid.py supports an
interactive mode with a text prompt and line-by-line
classification. This mode is suitable for quick in-
teractive queries, as well as for demonstration pur-
poses. langid.py also supports language identi-
fication of entire files via redirection. This allows a
user to interactively explore data, as well as to inte-
grate language identification into a pipeline of other
unix-style tools. However, use via redirection is
not recommended for large quantities of documents
as each invocation requires the trained model to be
unpacked into memory. Where large quantities of
documents are being processed, use as a library or
web service is preferred as the model will only be
unpacked once upon initialization.
Python library: langid.py can be imported as
a Python module, and provides a function that ac-
cepts text and returns the identified language of the
text. This use of langid.py is the fastest in a
single-processor setting as it incurs the least over-
head.
Web service: langid.py can be started as a
web service with a command-line switch. This
1http://numpy.scipy.org
26
allows language identitication by means of HTTP
PUT and HTTP POST requests, which return JSON-
encoded responses. This is the preferred method of
using langid.py from other programming envi-
ronments, as most languages include libraries for in-
teracting with web services over HTTP. It also al-
lows the language identification service to be run as
a network/internet service. Finally, langid.py is
WSGI-compliant,2 so it can be deployed in a WSGI-
compliant web server. This provides an easy way to
achieve parallelism by leveraging existing technolo-
gies to manage load balancing and utilize multiple
processors in the handling of multiple concurrent re-
quests for a service.
LDfeatureselect.py implements the LD
feature selection. The calculation of term frequency
is done in constant memory by index inversion
through a MapReduce-style sharding approach. The
calculation of information gain is also chunked to
limit peak memory use, and furthermore it is paral-
lelized to make full use of modern multiprocessor
systems. LDfeatureselect.py produces a list
of byte n-grams ranked by their LD score.
train.py implements estimation of parameters
for the multinomial naive Bayes model, as well as
the construction of the DFA for the Aho-Corasick
string matching algorithm. Its input is a list of byte
patterns representing a feature set (such as that se-
lected via LDfeatureselect.py), and a corpus
of training documents. It produces the final model as
a single compressed, encoded string, which can be
saved to an external file and used by langid.py
via a command-line option.
4 Training Data
langid.py is distributed with an embedded
model trained using the multi-domain language
identification corpus of Lui and Baldwin (2011).
This corpus contains documents in a total of 97 lan-
guages. The data is drawn from 5 different do-
mains: government documents, software documen-
tation, newswire, online encyclopedia and an inter-
net crawl, though no domain covers the full set of
languages by itself, and some languages are present
only in a single domain. More details about this cor-
pus are given in Lui and Baldwin (2011).
2http://www.wsgi.org
We do not perform explicit encoding detection,
but we do not assume that all the data is in the same
encoding. Previous research has shown that explicit
encoding detection is not needed for language iden-
tification (Baldwin and Lui, 2010). Our training data
consists mostly of UTF8-encoded documents, but
some of our evaluation datasets contain a mixture
of encodings.
5 Evaluation
In order to benchmark langid.py, we carried out
an empirical evaluation using a number of language-
labelled datasets. We compare the empirical results
obtained from langid.py to those obtained from
other language identification toolkits which incor-
porate a pre-trained model, and are thus usable off-
the-shelf for language identification. These tools are
listed in Table 3.
5.1 Off-the-shelf LangID tools
TextCat is an implementation of the method of
Cavnar and Trenkle (1994) by Gertjan van Noord.
It has traditionally been the de facto LangID tool of
choice in research, and is the basis of language iden-
tification/filtering in the ClueWeb09 Dataset (Callan
and Hoy, 2009) and CorpusBuilder (Ghani et al,
2004). It includes support for training with user-
supplied data.
LangDetect implements a Naive Bayes classi-
fier, using a character n-gram based representation
without feature selection, with a set of normaliza-
tion heuristics to improve accuracy. It is trained on
data from Wikipedia,3 and can be trained with user-
supplied data.
CLD is a port of the embedded language identi-
fier in Google?s Chromium browser, maintained by
Mike McCandless. Not much is known about the
internal design of the tool, and there is no support
provided for re-training it.
The datasets come from a variety of domains,
such as newswire (TCL), biomedical corpora
(EMEA), government documents (EUROGOV, EU-
ROPARL) and microblog services (T-BE, T-SC). A
number of these datasets have been previously used
in language identification research. We provide a
3http://www.wikipedia.org
27
Test Dataset langid.py LangDetect TextCat CLDAccuracy docs/s ?Acc Slowdown ?Acc Slowdown ?Acc Slowdown
EUROGOV 0.987 70.5 +0.005 1.1? ?0.046 31.1? ?0.004 0.5?
TCL 0.904 185.4 ?0.086 2.1? ?0.299 24.2? ?0.172 0.5?
WIKIPEDIA 0.913 227.6 ?0.046 2.5? ?0.207 99.9? ?0.082 0.9?
EMEA 0.934 7.7 ?0.820 0.2? ?0.572 6.3? +0.044 0.3?
EUROPARL 0.992 294.3 +0.001 3.6? ?0.186 115.4? ?0.010 0.2?
T-BE 0.941 367.9 ?0.016 4.4? ?0.210 144.1? ?0.081 0.7?
T-SC 0.886 298.2 ?0.038 2.9? ?0.235 34.2? ?0.120 0.2?
Table 2: Comparison of standalone classification tools, in terms of accuracy and speed (documents/second), relative
to langid.py
Tool Languages URL
langid.py 97 http://www.csse.unimelb.edu.au/research/lt/resources/langid/
LangDetect 53 http://code.google.com/p/language-detection/
TextCat 75 http://odur.let.rug.nl/vannoord/TextCat/
CLD 64+ http://code.google.com/p/chromium-compact-language-detector/
Table 3: Summary of the LangID tools compared
brief summary of the characteristics of each dataset
in Table 1.
The datasets we use for evaluation are differ-
ent from and independent of the datasets from
which the embedded model of langid.py was
produced. In Table 2, we report the accuracy of
each tool, measured as the proportion of documents
from each dataset that are correctly classified. We
present the absolute accuracy and performance for
langid.py, and relative accuracy and slowdown
for the other systems. For this experiment, we used
a machine with 2 Intel Xeon E5540 processors and
24GB of RAM. We only utilized a single core, as
none of the language identification tools tested are
inherently multicore.
5.2 Comparison on standard datasets
We compared the four systems on datasets used in
previous language identification research (Baldwin
and Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), as
well as an extract from a biomedical parallel cor-
pus (Tiedemann, 2009) (EMEA) and a corpus of
samples from the Europarl Parallel Corpus (Koehn,
2005) (EUROPARL). The sample of EUROPARL
we use was originally prepared by Shuyo Nakatani
(author of LangDetect) as a validation set.
langid.py compares very favorably with other
language identification tools. It outperforms
TextCat in terms of speed and accuracy on all of
the datasets considered. langid.py is generally
orders of magnitude faster than TextCat, but this
advantage is reduced on larger documents. This is
primarily due to the design of TextCat, which re-
quires that the supplied models be read from file for
each document classified.
langid.py generally outperforms
LangDetect, except in datasets derived from
government documents (EUROGOV, EUROPARL).
However, the difference in accuracy between
langid.py and LangDetect on such datasets
is very small, and langid.py is generally faster.
An abnormal result was obtained when testing
LangDetect on the EMEA corpus. Here,
LangDetect is much faster, but has extremely
poor accuracy (0.114). Analysis of the results re-
veals that the majority of documents were classified
as Polish. We suspect that this is due to the early
termination criteria employed by LangDetect,
together with specific characteristics of the corpus.
TextCat also performed very poorly on this
corpus (accuracy 0.362). However, it is important
to note that langid.py and CLD both performed
very well, providing evidence that it is possible to
build a generic language identifier that is insensitive
to domain-specific characteristics.
langid.py also compares well with CLD. It is
generally more accurate, although CLD does bet-
ter on the EMEA corpus. This may reveal some
insight into the design of CLD, which is likely to
have been tuned for language identification of web
28
pages. The EMEA corpus is heavy in XML markup,
which CLD and langid.py both successfully ig-
nore. One area where CLD outperforms all other sys-
tems is in its speed. However, this increase in speed
comes at the cost of decreased accuracy in other do-
mains, as we will see in Section 5.3.
5.3 Comparison on microblog messages
The size of the input text is known to play a sig-
nificant role in the accuracy of automatic language
identification, with accuracy decreasing on shorter
input documents (Cavnar and Trenkle, 1994; Sibun
and Reynar, 1996; Baldwin and Lui, 2010).
Recently, language identification of short strings
has generated interest in the research community.
Hammarstrom (2007) described a method that aug-
mented a dictionary with an affix table, and tested it
over synthetic data derived from a parallel bible cor-
pus. Ceylan and Kim (2009) compared a number of
methods for identifying the language of search en-
gine queries of 2 to 3 words. They develop a method
which uses a decision tree to integrate outputs from
several different language identification approaches.
Vatanen et al (2010) focus on messages of 5?21
characters, using n-gram language models over data
drawn from UDHR in a naive Bayes classifier.
A recent application where language identifica-
tion is an open issue is over the rapidly-increasing
volume of data being generated by social media.
Microblog services such as Twitter4 allow users to
post short text messages. Twitter has a worldwide
user base, evidenced by the large array of languages
present on Twitter (Carter et al, to appear). It is es-
timated that half the messages on Twitter are not in
English. 5
This new domain presents a significant challenge
for automatic language identification, due to the
much shorter ?documents? to be classified, and is
compounded by the lack of language-labelled in-
domain data for training and validation. This has led
to recent research focused specifically on the task of
language identification of Twitter messages. Carter
et al (to appear) improve language identification in
Twitter messages by augmenting standard methods
4http://www.twitter.com
5http://semiocast.com/downloads/
Semiocast_Half_of_messages_on_Twitter_
are_not_in_English_20100224.pdf
with language identification priors based on a user?s
previous messages and by the content of links em-
bedded in messages. Tromp and Pechenizkiy (2011)
present a method for language identification of short
text messages by means of a graph structure.
Despite the recently published results on language
identification of microblog messages, there is no
dedicated off-the-shelf system to perform the task.
We thus examine the accuracy and performance of
using generic language identification tools to iden-
tify the language of microblog messages. It is im-
portant to note that none of the systems we test have
been specifically tuned for the microblog domain.
Furthermore, they do not make use of any non-
textual information such as author and link-based
priors (Carter et al, to appear).
We make use of two datasets of Twitter messages
kindly provided to us by other researchers. The first
is T-BE (Tromp and Pechenizkiy, 2011), which con-
tains 9659 messages in 6 European languages. The
second is T-SC (Carter et al, to appear), which con-
tains 5000 messages in 5 European languages.
We find that over both datasets, langid.py has
better accuracy than any of the other systems tested.
On T-BE, Tromp and Pechenizkiy (2011) report
accuracy between 0.92 and 0.98 depending on the
parametrization of their system, which was tuned
specifically for classifying short text messages. In
its off-the-shelf configuration, langid.py attains
an accuracy of 0.94, making it competitive with
the customized solution of Tromp and Pechenizkiy
(2011).
On T-SC, Carter et al (to appear) report over-
all accuracy of 0.90 for TextCat in the off-the-
shelf configuration, and up to 0.92 after the inclusion
of priors based on (domain-specific) extra-textual
information. In our experiments, the accuracy of
TextCat is much lower (0.654). This is because
Carter et al (to appear) constrained TextCat to
output only the set of 5 languages they considered.
Our results show that it is possible for a generic lan-
guage identification tool to attain reasonably high
accuracy (0.89) without artificially constraining the
set of languages to be considered, which corre-
sponds more closely to the demands of automatic
language identification to real-world data sources,
where there is generally no prior knowledge of the
languages present.
29
We also observe that while CLD is still the fastest
classifier, this has come at the cost of accuracy in an
alternative domain such as Twitter messages, where
both langid.py and LangDetect attain better
accuracy than CLD.
An interesting point of comparison between the
Twitter datasets is how the accuracy of all systems
is generally higher on T-BE than on T-SC, despite
them covering essentially the same languages (T-BE
includes Italian, whereas T-SC does not). This is
likely to be because the T-BE dataset was produced
using a semi-automatic method which involved a
language identification step using the method of
Cavnar and Trenkle (1994) (E Tromp, personal com-
munication, July 6 2011). This may also explain
why TextCat, which is also based on Cavnar and
Trenkle?s work, has unusually high accuracy on this
dataset.
6 Conclusion
In this paper, we presented langid.py, an off-the-
shelf language identification solution. We demon-
strated the robustness of the tool over a range of test
corpora of both long and short documents (including
micro-blogs).
Acknowledgments
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
string matching: an aid to bibliographic search. Com-
munications of the ACM, 18(6):333?340, June.
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In Pro-
ceedings of NAACL HLT 2010, pages 229?237, Los
Angeles, USA.
Jamie Callan and Mark Hoy, 2009. ClueWeb09
Dataset. Available at http://boston.lti.cs.
cmu.edu/Data/clueweb09/.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
to appear. Microblog language identification: Over-
coming the limitations of short, unedited and idiomatic
text. Language Resources and Evaluation Journal.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language
identification of search engine queries. In Proceedings
of ACL2009, pages 1066?1074, Singapore.
George Forman. 2003. An Extensive Empirical Study
of Feature Selection Metrics for Text Classification.
Journal of Machine Learning Research, 3(7-8):1289?
1305, October.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004.
Building Minority Language Corpora by Learning to
Generate Web Search Queries. Knowledge and Infor-
mation Systems, 7(1):56?83, February.
Harald Hammarstrom. 2007. A Fine-Grained Model for
Language Identication. In Proceedings of iNEWS07,
pages 14?20.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT summit, 11.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of 5th International Joint Conference on Nat-
ural Language Processing, pages 553?561, Chiang
Mai, Thailand.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classifi-
cation. In Proceedings of the AAAI-98 Workshop on
Learning for Text Categorization, Madison, USA.
J.R. Quinlan. 1986. Induction of Decision Trees. Ma-
chine Learning, 1(1):81?106, October.
Penelope Sibun and Jeffrey C. Reynar. 1996. Language
determination: Examining the issues. In Proceedings
of the 5th Annual Symposium on Document Analysis
and Information Retrieval, pages 125?135, Las Vegas,
USA.
Jo?rg Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. Recent Advances in Natural Language Process-
ing, V:237?248.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
Based N-gram Language Identification on Short Texts.
In Proceedings of Benelearn 2011, pages 27?35, The
Hague, Netherlands.
Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.
2010. Language identification of short text segments
with n-gram models. In Proceedings of LREC 2010,
pages 3423?3430.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML 97.
30
Automatic Detection and Language Identification of Multilingual Documents
Marco Lui??, Jey Han Lau? and Timothy Baldwin??
? Department of Computing and Information Systems
The University of Melbourne
? NICTA Victoria Research Laboratory
? Department of Philosophy
King?s College London
mhlui@unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net
Abstract
Language identification is the task of automat-
ically detecting the language(s) present in a
document based on the content of the docu-
ment. In this work, we address the problem
of detecting documents that contain text from
more than one language (multilingual docu-
ments). We introduce a method that is able to
detect that a document is multilingual, iden-
tify the languages present, and estimate their
relative proportions. We demonstrate the ef-
fectiveness of our method over synthetic data,
as well as real-world multilingual documents
collected from the web.
1 Introduction
Language identification is the task of automatically
detecting the language(s) present in a document
based on the content of the document. Language
identification techniques commonly assume that ev-
ery document is written in one of a closed set of
known languages for which there is training data,
and is thus formulated as the task of selecting the
most likely language from the set of training lan-
guages. In this work, we remove this monolingual
assumption, and address the problem of language
identification in documents that may contain text
from more than one language from the candidate set.
We propose a method that concurrently detects that a
document is multilingual, and estimates the propor-
tion of the document that is written in each language.
Detecting multilingual documents has a variety
of applications. Most natural language processing
techniques presuppose monolingual input data, so
inclusion of data in foreign languages introduces
noise, and can degrade the performance of NLP sys-
tems (Alex et al., 2007; Cook and Lui, 2012). Au-
tomatic detection of multilingual documents can be
used as a pre-filtering step to improve the quality of
input data. Detecting multilingual documents is also
important for acquiring linguistic data from the web
(Scannell, 2007; Abney and Bird, 2010), and has
applications in mining bilingual texts for statistical
machine translation from online resources (Resnik,
1999; Nie et al., 1999; Ling et al., 2013). There has
been particular interest in extracting text resources
for low-density languages from multilingual web
pages containing both the low-density language and
another language such as English (Yamaguchi and
Tanaka-Ishii, 2012; King and Abney, 2013). King
and Abney (2013, p1118) specifically mention the
need for an automatic method ?to examine a mul-
tilingual document, and with high accuracy, list the
languages that are present in the document?.
We introduce a method that is able to detect multi-
lingual documents, and simultaneously identify each
language present as well as estimate the propor-
tion of the document written in that language. We
achieve this with a probabilistic mixture model, us-
ing a document representation developed for mono-
lingual language identification (Lui and Baldwin,
2011). The model posits that each document is gen-
erated as samples from an unknown mixture of lan-
guages from the training set. We introduce a Gibbs
sampler to map samples to languages for any given
set of languages, and use this to select the set of lan-
guages that maximizes the posterior probability of
the document.
27
Transactions of the Association for Computational Linguistics, 2 (2014) 27?40. Action Editor: Kristina Toutanova.
Submitted 1/2013; Revised 7/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
Our method is able to learn a language identi-
fier for multilingual documents from monolingual
training data. This is an important property as there
are no standard corpora of multilingual documents
available, whereas corpora of monolingual docu-
ments are readily available for a reasonably large
number of languages (Lui and Baldwin, 2011). We
demonstrate the effectiveness of our method empir-
ically, firstly by evaluating it on synthetic datasets
drawn from Wikipedia data, and then by applying it
to real-world data, showing that we are able to iden-
tify multilingual documents in targeted web crawls
of minority languages (King and Abney, 2013).
Our main contributions are: (1) we present a
method for identifying multilingual documents, the
languages contained therein and the relative propor-
tion of the document in each language; (2) we show
that our method outperforms state-of-the-art meth-
ods for language identification in multilingual doc-
uments; (3) we show that our method is able to es-
timate the proportion of the document in each lan-
guage to a high degree of accuracy; and (4) we show
that our method is able to identify multilingual doc-
uments in real-world data.
2 Background
Most language identification research focuses on
language identification for monolingual documents
(Hughes et al., 2006). In monolingual LangID, the
task is to assign each documentD a unique language
Li ? L. Some work has reported near-perfect accu-
racy for language identification of large documents
in a small number of languages (Cavnar and Tren-
kle, 1994; McNamee, 2005). However, in order to
attain such accuracy, a large number of simplifying
assumptions have to be made (Hughes et al., 2006;
Baldwin and Lui, 2010a). In this work, we tackle
the assumption that each document is monolingual,
i.e. it contains text from a single language.
In language identification, documents are mod-
eled as a stream of characters (Cavnar and Trenkle,
1994; Kikui, 1996), often approximated by the cor-
responding stream of bytes (Kruengkrai et al., 2005;
Baldwin and Lui, 2010a) for robustness over vari-
able character encodings. In this work, we follow
Baldwin and Lui (2010a) in training a single model
for languages that naturally use multiple encodings
(e.g. UTF8, Big5 and GB encodings for Chinese), as
issues of encoding are not the focus of this research.
The document representation used for language
identification generally involves estimating the rel-
ative distributions of particular byte sequences, se-
lected such that their distributions differ between
languages. In some cases the relevant sequences
may be externally specified, such as function words
and common suffixes (Giguet, 1995) or grammati-
cal word classes (Dueire Lins and Gonc?alves, 2004),
though they are more frequently learned from la-
beled data (Cavnar and Trenkle, 1994; Grefenstette,
1995; Prager, 1999a; Lui and Baldwin, 2011).
Learning algorithms applied to language identi-
fication fall into two general categories: Bayesian
classifiers and nearest-prototype (Rocchio-style)
classifiers. Bayesian approaches include Markov
processes (Dunning, 1994), naive Bayes methods
(Grefenstette, 1995; Lui and Baldwin, 2011; Tiede-
mann and Ljubes?ic?, 2012), and compressive mod-
els (Teahan, 2000). The nearest-prototype methods
vary primarily in the distance measure used, includ-
ing measures based on rank order statistics (Cav-
nar and Trenkle, 1994), information theory (Bald-
win and Lui, 2010a), string kernels (Kruengkrai et
al., 2005) and vector space models (Prager, 1999a;
McNamee, 2005).
Language identification has been applied in do-
mains such as USENET messages (Cavnar and
Trenkle, 1994), web pages (Kikui, 1996; Mar-
tins and Silva, 2005; Liu and Liang, 2008), web
search queries (Ceylan and Kim, 2009; Bosca and
Dini, 2010), mining the web for bilingual text
(Resnik, 1999; Nie et al., 1999), building minor-
ity language corpora (Ghani et al., 2004; Scannell,
2007; Bergsma et al., 2012) as well as a large-
scale database of Interlinear Glossed Text (Xia et al.,
2010), and the construction of a large-scale multilin-
gual web crawl (Callan and Hoy, 2009).
2.1 Multilingual Documents
Language identification over documents that contain
text from more than one language has been identified
as an open research question (Hughes et al., 2006).
Common examples of multilingual documents are
web pages that contain excerpts from another lan-
guage, and documents from multilingual organiza-
tions such as the European Union.
28
English French Italian German Dutch Japanese
character the pour di auf voo ?
byte 74 68 65 20 70 6F 75 7 20 64 69 20 20 61 75 66 76 6F 6 E3 81 AF
Table 1: Examples of per-language byte sequences selected by information gain.
The Australiasian Language Technology Work-
shop 2010 hosted a shared task where participants
were required to predict the language(s) present in a
held-out test set containing monolingual and bilin-
gual documents (Baldwin and Lui, 2010b). The
dataset was prepared using data from Wikipedia, and
bilingual documents were produced using a segment
from a page in one language, and a segment from the
same page in another language. We use the dataset
from this shared task for our initial experiments.
To the authors? knowledge, the only other work to
directly tackle identification of multiple languages
and their relative proportions in a single document is
the LINGUINI system (Prager, 1999a). The system
is based on a vector space model, and cosine simi-
larity between a feature vector for the test document
and a feature vector for each language Li, computed
as the sum of feature vectors for all the documents
for language Li in the training data. The elements
in the feature vectors are frequency counts over
byte n-grams (2?n?5) and words. Language iden-
tification for multilingual documents is performed
through the use of virtual mixed languages. Prager
(1999a) shows how to construct vectors representa-
tive of particular combinations of languages inde-
pendent of the relative proportions, and proposes a
method for choosing combinations of languages to
consider for any given document.
Language identification in multilingual docu-
ments could also be performed by application of su-
pervised language segmentation algorithms. Given
a system that can segment a document into la-
beled monolingual segments, we can then extract
the languages present as well as the relative propor-
tion of text in each language. Several methods for
supervised language segmentation have been pro-
posed. Teahan (2000) proposed a system based on
text compression that identifies multilingual docu-
ments by first segmenting the text into monolingual
blocks. Rehurek and Kolkus (2009) perform lan-
guage segmentation by computing a relevance score
between terms and languages, smoothing across ad-
joining terms and finally identifying points of transi-
tion between high and low relevance, which are in-
terpreted as boundaries between languages. Yam-
aguchi and Tanaka-Ishii (2012) use a minimum de-
scription length approach, embedding a compressive
model to compute the description length of text seg-
ments in each language. They present a linear-time
dynamic programming solution to optimize the lo-
cation of segment boundaries and language labels.
3 Methodology
Language identification for multilingual documents
is a multi-label classification task, in which a doc-
ument can be mapped onto any number of labels
from a closed set. In the remainder of this paper,
we denote the set of all languages by L. We de-
note a document D which contains languages Lx
and Ly as D ? {Lx, Ly}, where Lx, Ly ? L.
We denote a document that does not contain a lan-
guage Lx by D ? {Lx}, though we generally omit
all the languages not contained in the document for
brevity. We denote classifier output using .; e.g.
D . {La, Lb} indicates that document D has been
predicted to contain text in languages La and Lb.
3.1 Document Representation and Feature
Selection
We represent each document D as a frequency dis-
tribution over byte n-gram sequences such as those
in Table 1. Each document is converted into a vector
where each entry counts the number of times a par-
ticular byte n-gram is present in the document. This
is analogous to a bag-of-words model, where the vo-
cabulary of ?words? is a set of byte sequences that
has been selected to distinguish between languages.
The exact set of features is selected from the
training data using Information Gain (IG), an
information-theoretic metric developed as a split-
ting criterion for decision trees (Quinlan, 1993). IG-
based feature selection combined with a naive Bayes
classifier has been shown to be particularly effective
for language identification (Lui and Baldwin, 2011).
29
3.2 Generative Mixture Models
Generative mixture models are popular for text mod-
eling tasks where a mixture of influences governs the
content of a document, such as in multi-label doc-
ument classification (McCallum, 1999; Ramage et
al., 2009), and topic modeling (Blei et al., 2003).
Such models normally assume full exchangeability
between tokens (i.e. the bag-of-words assumption),
and label each token with a single discrete label.
Multi-label text classification, topic modeling and
our model for language identification in multilingual
documents share the same fundamental representa-
tion of the latent structure of a document. Each la-
bel is modeled with a probability distribution over
tokens, and each document is modeled as a proba-
bilistic mixture of labels. As presented in Griffiths
and Steyvers (2004), the probability of the ith token
(wi) given a set of T labels z1? ? ?zT is modeled as:
P (wi) =
T?
j=1
P (wi|zi = j)P (zi = j) (1)
The set of tokens w is the document itself, which
in all cases is observed. In the case of topic model-
ing, the tokens are words and the labels are topics,
and z is latent. Whereas topic modeling is gener-
ally unsupervised, multi-label text classification is
a supervised text modeling task, where the labels
are a set of pre-defined categories (such as RUBBER,
IRON-STEEL, TRADE, etc. in the popular Reuters-
21578 data set (Lewis, 1997)), and the tokens are
individual words in documents. z is still latent, but
constrained in the training data (i.e. documents are
labeled but the individual words are not). Some ap-
proaches to labeling unseen documents require that
z for the training data be inferred, and methods for
doing this include an application of the Expectation-
Maximization (EM) algorithm (McCallum, 1999)
and Labeled LDA (Ramage et al., 2009).
The model that we propose for language identifi-
cation in multilingual documents is similar to multi-
label text classification. In the framework of Equa-
tion 1, each per-token label zi is a language and the
vocabulary of tokens is not given by words but rather
by specific byte sequences (Section 3.1). The key
difference with multi-label text classification is that
we use monolingual (i.e. mono-label) training data.
Hence, z is effectively observed for the training data
(since all tokens must share the same label). To infer
z for unlabeled documents, we utilize a Gibbs sam-
pler, closely related to that proposed by Griffiths and
Steyvers (2004) for LDA. The sampling probability
for a label zi for token w in a document d is:
P (zi = j|z?i, w) ? ?(w)j ? ?
(d)
j (2)
?(w)j = P (wi|zi = j, z?i, w?i)
?(d)j = P (zi = j|z?i)
In the LDA model, ?(d)j is assumed to have a Dirich-
let distribution with hyperparameter ?, and the word
distribution for each topic ?(w)j is also assumed to
have a Dirichlet distribution with hyperparameter
?. Griffiths (2002) describes a generative model for
LDA where both ?(w)j and ?(d)j are inferred from
the output of a Gibbs sampler. In our method, we
estimate ?(w)j using maximum likelihood estima-
tion (MLE) from the training data. Estimating ?(w)j
through MLE is equivalent to a multinomial Naive
Bayes model (McCallum and Nigam, 1998):
??(w)j =
n(w)j + ?
n(.)j +W?
(3)
where n(w)j is the number of times word w occurs
with label j, and n(.)j is the total number of words
that occur with label j. By setting ? to 1, we obtain
standard Laplacian smoothing. Hence, only ??(d)j is
updated at each step in the Gibbs sampler:
??(d)j =
n(d)?i,j + ?
n(d)?i + T?
(4)
where n(d)?i,j is the number of tokens in document d
that are currently mapped to language j, and n(d)?i is
the total number of tokens in document d. In both
cases, the current assignment of zi is excluded from
the count. T is the number of languages (i.e. the size
of the label set). For simplicity, we set ? to 0. We
note that in the LDA model, ? and ? influence the
sparsity of the solution, and so it may be possible
to tune these parameters for our model as well. We
leave this as an avenue for further research.
30
3.3 Language Identification in Multilingual
Documents
The model described in Section 3.2 can be used to
compute the most likely distribution to have gen-
erated an unlabeled document over a given set of
languages for which we have monolingual training
data, by letting the set of terms w be the byte n-gram
sequences we selected using per-language informa-
tion gain (Section 3.1), and allowing the labels z to
range over the set of all languages L. Using train-
ing data, we compute ??(w)j (Equation 3), and then
we infer P (Lj |D) for each Lj ? L for the unla-
beled document, by running the Gibbs sampler until
the samples for zi converge and then tabulating zi
over the whole d and normalizing by |d|. Naively,
we could identify the languages present in the doc-
ument by D . {Lx if ?(zi = Lx|D)}, but closely-
related languages tend to have similar frequency dis-
tributions over byte n-gram features, and hence it is
likely that some tokens will be incorrectly mapped to
a language that is similar to the ?correct? language.
We address this issue by finding the subset of lan-
guages ? from the training set L that maximizes
P (?|D) (a similar approach is taken in McCallum
(1999)). Through an application of Bayes? theorem,
P (?|D) ? P (D|?)?P (?), noting that P (D) is a
normalizing constant and can be dropped. We as-
sume that P (?) is constant (i.e. any subset of lan-
guages is equally likely, a reasonable assumption in
the absence of other evidence), and hence maximize
P (D|?). For any given D = w1? ? ?wn and ?, we
infer P (D|?) from the output of the Gibbs sampler:
P (D|?) =
N?
i=1
P (wi|?) (5)
=
N?
i=1
?
j??
P (wi|zi = j)P (zi = j) (6)
where both P (wi|zi = j) and P (zi = j) are esti-
mated by their maximum likelihood estimates.
In practice, exhaustive evaluation of the powerset
of L is prohibitively expensive, and so we greed-
ily approximate the optimal ? using Algorithm 1. In
essence, we initially rank all the candidate languages
by computing the most likely distribution over the
full set of candidate languages. Then, for each of
the top-N languages in turn, we consider whether
Algorithm 1 DetectLang(L,D)
LN ? top-N z ? L by P (z|D)
?? {Lu}
for each Lt ? LN do
?? ? ? ? Lt
if P (D|?) + t < P (D|??) then
?? ??
end if
end for
?? ? \ {Lu}
return D . ?
to add it to ?. ? is initialized with Lu, a dummy
language with a uniform distribution over terms (i.e.
P (w|Lu) = 1|w| ). A language is added if it improves
P (D|?) by at least t. The threshold t is required
to suppress the addition of spurious classes. Adding
languages gives the model additional freedom to fit
parameters, and so will generally increase P (D|?).
In the limit case, adding a completely irrelevant lan-
guage will result in no tokens being mapped to the
a language, and so the model will be no worse than
without the language. The threshold t is thus used to
control ?how much? improvement is required before
including the new language in ?.
3.4 Benchmark Approaches
We compare our approach to two methods for
language identification in multilingual documents:
(1) the virtual mixed languages approach (Prager,
1999a); and (2) the text segmentation approach (Ya-
maguchi and Tanaka-Ishii, 2012).
Prager (1999a) describes LINGUINI, a language
identifier based on the vector-space model com-
monly used in text classification and information re-
trieval. The document representation used by Prager
(1999a) is a vector of counts across a set of charac-
ter sequences. Prager (1999a) selects the feature set
based on a TFIDF-like approach. Terms with occur-
rence count m < n? k are rejected, where m is the
number of times the term occurs in the training data
(the TF component), n is the number of languages in
which the term occurred (the IDF component, where
?document? is replaced with ?language?), and k is a
parameter to control the overall number of terms se-
lected. In Prager (1999a), the value of k is reported
to be optimal in the region 0.3 to 0.5. In practice,
31
the value of k indirectly controls the number of fea-
tures selected. Values of k are not comparable across
datasets as m is not normalized for the size of the
training data, so in this work we do not report the
values of k and instead directly select the top-N fea-
tures, weighted by mn . In LINGUINI, each languageis modeled as a single pseudo-document, obtained
by concatenating all the training data for the given
language. A document is then classified according
to the vector with which it has the smallest angle;
this is implemented by finding the language vector
with the highest cosine with the document vector.
Prager (1999a) also proposes an extension to the
approach to allow identification of bilingual docu-
ments, and suggests how this may be generalized to
any number of languages in a document. The gist
of the method is simple: for any given pair of lan-
guages, the projection of a document vector onto
the hyperplane containing the language vectors of
the two languages gives the mixture proportions of
the two languages that minimizes the angle with the
document vector. Prager (1999a) terms this projec-
tion a virtual mixed language (VML), and shows
how to find the angle between the document vec-
tor and the VML. If this angle is less than that be-
tween the document vector and any individual lan-
guage vector, the document is labeled as bilingual in
the two languages from which the mixed vector was
derived. The practical difficulty presented by this
approach is that exhaustively evaluating all possible
combinations of languages is prohibitively expen-
sive. Prager (1999a) addresses this by arguing that in
multilingual documents, ?the individual component
languages will be close to d (the document vector)
? probably closer than most or all other languages?.
Hence, language mixtures are only considered for
combinations of the top m languages.
Prager (1999a) shows how to obtain the mixture
coefficients for bilingual VMLs, arguing that the
process generalizes. Prager (1999b) includes the
coefficients for 3-language VMLs, which are much
more complex than the 2-language variants. Us-
ing a computer algebra system, we verified the an-
alytic forms of the coefficients in the 3-language
VML. We also attempted to obtain an analytic form
for the coefficients in a 4-language VML, but these
were too complex for the computer algebra system
to compute. Thus, our evaluation of the VML ap-
proach proposed by Prager (1999a) is limited to 3-
language VMLs. Neither Prager (1999a) nor Prager
(1999b) include an empirical evaluation over mul-
tilingual documents, so to the best of our knowl-
edge this paper is the first empirical evaluation of
the method on multilingual documents. As no refer-
ence implementation of this method is available, we
have produced our own implementation, which we
have made freely available.1
The other benchmark we consider in this paper is
the method for text segmentation by language pro-
posed by Yamaguchi and Tanaka-Ishii (2012) (here-
after referred to as SEGLANG). The actual task ad-
dressed by Yamaguchi and Tanaka-Ishii (2012) is to
divide a document into monolingual segments. This
is formulated as the task of segmenting a document
D = x1, ? ? ? , x|D| (where xi denotes the ith char-
acter of D and |D| is the length of the document)
by finding a list of boundaries B = [B1, ? ? ? , B|B|]
where each Bi indicates the location of a language
boundary as an offset from the start of the document,
resulting in a list of segments X = [X0, ? ? ? , X|B|].
For each segment Xi, the system predicts Li, the
language associated with the segment, producing a
list of labellings L = [L0, ? ? ? , L|B|], with the con-
straint that adjacent elements in L must differ. Ya-
maguchi and Tanaka-Ishii (2012) solve the problem
of determining X and L for an unlabeled text us-
ing a method based on minimum description length.
They present a dynamic programming solution to
this problem, and analyze a number of parameters
that affect the overall accuracy of the system. Given
this method to determine X and L, it is then triv-
ial to label an unlabeled document according to
D . {Lx if ?Lx ? L}, and the length of each seg-
ment in X can then be used to determine the pro-
portions of the document that are in each language.
In this work, we use a reference implementation of
SEGLANG kindly provided to us by the authors.
Using the text segmentation approach of
SEGLANG to detect multilingual documents differs
from LINGUINI and our method primarily in that
LINGUINI and our method fragment the document
into small sequences of bytes, and discard informa-
tion about the relative order of the fragments. This
is in contrast to SEGLANG, where this information
1https://github.com/saffsd/linguini.py
32
System PM RM FM P? R? F?
Benchmark .497 .467 .464 .833 .826 .829
Winner .718 .703 .699 .932 .931 .932
SEGLANG .801 .810 .784 .866 .946 .905
LINGUINI .616 .535 .513 .713 .688 .700
Our method .753 .771 .748 .945 .922 .933
Table 2: Results on the ALTW2010 dataset.
?Benchmark? is the benchmark system proposed by
the shared task organizers. ?Winner? is the highest-
F? system submitted to the shared task.
is utilized in the sequential prediction of labels for
consecutive segments of text, and is thus able to
make better use of the locality of text (since there are
likely to be monolingual blocks of text in any given
multilingual document). The disadvantage of this is
that the underlying model becomes more complex
and hence more computationally expensive, as we
observe in Section 5.
3.5 Evaluation
We seek to evaluate the ability of each method:
(1) to correctly identify the language(s) present in
each test document; and (2) for multilingual doc-
uments, to estimate the relative proportion of the
document written in each language. In the first in-
stance, this is a classification problem, and the stan-
dard notions of precision (P), recall (R) and F-score
(F) apply. Consistent with previous work in lan-
guage identification, we report both the document-
level micro-average, as well as the language-level
macro-average. For consistency with Baldwin and
Lui (2010a), the macro-averaged F-score we report
is the average of the per-class F-scores, rather than
the harmonic mean of the macro-averaged precision
and recall; as such, it is possible for the F-score
to not fall between the precision and recall values.
As is common practice, we compute the F-score for
? = 1, giving equal importance to precision and
recall.2 We tested the difference in performance
for statistical significance using an approximate ran-
domization procedure (Yeh, 2000) with 10000 iter-
ations. Within each table of results (Tables 2, 3 and
2Intuitively, it may seem that the maximal precision and re-
call should be achieved when precision and recall are balanced.
However, because of the multi-label nature of the task and vari-
able number of labels assigned to a given document by our mod-
els, it is theoretically possible and indeed common in our results
for the maximal macro-averaged F-score to be achieved when
macro-averaged precision and recall are not balanced.
4), all differences between systems are statistically
significant at a p < 0.05 level.
To evaluate the predictions of the relative propor-
tions of a document D written in each detected lan-
guageLi, we compare the topic proportion predicted
by our model to the gold-standard proportion, mea-
sured as a byte ratio as follows:
gs(Li|D) =
length of Li part of D in bytes
length of D in bytes (7)
We report the correlation between predicted and ac-
tual proportions in terms of Pearson?s r coefficient.
We also report the mean absolute error (MAE) over
all document?language pairs.
4 Experiments on ALTW2010
Our first experiment utilizes the ALTW2010 shared
task dataset (Baldwin and Lui, 2010b), a synthetic
dataset of 10000 bilingual documents3 generated
from Wikipedia data, introduced in the ALTW2010
shared task,4 The dataset is organized into training,
development and test partitions. Following standard
machine learning practice, we train each system us-
ing the training partition, and tune parameters using
the development partition. We then report macro and
micro-averaged precision, recall and F-score on the
test partition, using the tuned parameters.
The results on the ALTW2010 shared task dataset
are summarized in Table 2. Each of the three sys-
tems we compare was re-trained using the training
data provided for the shared task, with a slight dif-
ference: in the shared task, participants were pro-
vided with multilingual training documents, but the
systems targeted in this research require monolin-
gual training data. We thus split the training doc-
uments into monolingual segments using the meta-
data provided with the dataset. The metadata was
only published after completion of the task and was
not available to task participants. For comparison,
we have included the benchmark results published
by the shared task organizers, as well as the score
attained by the winning entry (Tran et al., 2010).
3With a small number of monolingual documents, formed
by randomly selecting the two languages for a given docu-
ment independently, leaving the possibility of the same two lan-
guages being selected.
4http://comp.mq.edu.au/programming/task_
description/
33
We tune the parameters for each system using the
development partition of the dataset, and report re-
sults on the test partition. For LINGUINI, there is a
single parameter k to be tuned: the number of fea-
tures per language. We tested values between 10000
and 50000, and selected 46000 features as the opti-
mal value. For our method, there are two parameters
to be tuned: (1) the number of features selected for
each language, and (2) the threshold t for including
a language. We tested features-per-language counts
between 30 and 150, and found that adding features
beyond 70 per language had minimal effect. We
tested values of the threshold t from 0.01 to 0.15,
and found the best value was 0.14. For SEGLANG,
we introduce a threshold t on the minimum propor-
tion of a document (measured in bytes) that must
be labeled by a language before that language is in-
cluded in the output set. This was done because our
initial experiments indicate that SEGLANG tends to
over-produce labels. Using the development data,
we found the best value of t was 0.10.
We find that of the three systems tested, two out-
perform the winning entry to the shared task. This
is more evident in the macro-averaged results than
in the micro-averaged results. In micro-averaged
terms, our method is the best performer, whereas
on the macro-average, SEGLANG has the high-
est F-score. This suggests that our method does
well on higher-density languages (relative to the
ALTW2010 dataset), and poorly on lower-density
languages. This also accounts for the higher micro-
averaged precision but lower micro-averaged recall
for our method as compared to SEGLANG. The im-
proved macro-average F-score of SEGLANG comes
at a much higher computational cost, which in-
creases dramatically as the number of languages is
increased. In our testing on a 16-core worksta-
tion, SEGLANG took almost 24 hours to process the
ALTW2010 shared task test data, compared to 2
minutes for our method and 40 seconds for LIN-
GUINI. As such, SEGLANG is poorly suited to de-
tecting multilingual documents where a large num-
ber of candidate languages is considered.
The ALTW2010 dataset is an excellent starting
point for this research, but it predominantly contains
bilingual documents, making it difficult to assess the
ability of systems to distinguish multilingual docu-
ments from monolingual ones. Furthermore, we are
unable to use it to assess the ability of systems to
detect more than 2 languages in a document. To ad-
dress these shortcomings, we construct a new dataset
in a similar vein. The dataset and experiments per-
formed on it are described in the next section.
5 Experiments on WIKIPEDIAMULTI
To fully test the capabilities of our model, we gen-
erated WIKIPEDIAMULTI, a dataset that contains
a mixture of monolingual and multilingual docu-
ments. To allow for replicability of our results and
to facilitate research in language identification, we
have made the dataset publicly available.5 WIKI-
PEDIAMULTI is generated using excerpts from the
mediawiki sources of Wikipedia pages downloaded
from the Wikimedia foundation.6 The dumps we
used are from July?August 2010.
To generate WIKIPEDIAMULTI, we first normal-
ized the raw mediawiki documents. Mediawiki doc-
uments typically contain one paragraph per line, in-
terspersed with structural elements. We filtered each
document to remove all structural elements, and
only kept documents that exceeded 2500 bytes after
normalization. This yielded a collection of around
500,000 documents in 156 languages. From this
initial document set (hereafter referred to as WI-
KICONTENT), we only retained languages that had
more than 1000 documents (44 languages), and gen-
erated documents for WIKIPEDIAMULTI as follows:
1. randomly select the number of languages K
(1?K?5)
2. randomly select a set of K languages S =
{Li?L for i = 1? ? ?K} without replacement
3. randomly select a document for each Li?S
from WIKICONTENT without replacement
4. take the top 1K lines of the document5. join the K sections into a single document.
As a result of the procedure, the relative propor-
tion of each language in a multilingual document
tends not to be uniform, as it is conditioned on the
length of the original document from which it was
sourced, independent of the otherK?1 for the other
languages that it was combined with. Overall, the
average document length is 5500 bytes (standard de-
viation = 3800 bytes). Due to rounding up in taking
5http://www.csse.unimelb.edu.au/?tim/
6http://dumps.wikimedia.org
34
System PM RM FM P? R? F?
SEGLANG .809 .975 .875 .771 .975 .861
LINGUINI .853 .772 .802 .838 .774 .805
Our method .962 .954 .957 .963 .955 .959
Table 3: Results on the WIKIPEDIAMULTI dataset.
the top 1k lines (step 4), documents with higher Ktend to be longer (6200 bytes for K = 5 vs 5100
bytes for K = 1).
The WIKIPEDIAMULTI dataset contains training,
development and test partitions. The training parti-
tion consists of 5000 monolingual (i.e. K = 1) doc-
uments. The development partition consists of 5000
documents, 1000 documents for each value of K
where 1?K?5. The test partition contains 200 doc-
uments for each K, for a total of 1000 documents.
There is no overlap between any of the partitions.
5.1 Results over WIKIPEDIAMULTI
We trained each system using the monolingual train-
ing partition, and tuned parameters using the devel-
opment partition. For LINGUINI, we tested feature
counts between 10000 and 50000, and found that
the effect was relatively small. We thus use 10000
features as the optimum value. For SEGLANG, we
tested values for threshold t between 0.01 and 0.20,
and found that the maximal macro-averaged F-score
is attained when t = 0.06. Finally, for our method
we tested features-per-language counts between 30
and 130 and found the best performance with 120
features per language, although the actual effect of
varying this value is rather small. We tested values
of the threshold t for adding an extra language to
? from 0.01 to 0.15, and found that the best results
were attained when t = 0.02.
The results of evaluating each system on the
test partition are summarized in Table 3. In this
evaluation, our method clearly outperforms both
SEGLANG and LINGUINI. The results on WIKI-
PEDIAMULTI and ALTW2010 are difficult to com-
pare directly due to the different compositions of the
two datasets. ALTW2010 is predominantly bilin-
gual, whereas WIKIPEDIAMULTI contains docu-
ments with text in 1?5 languages. Furthermore, the
average document in ALTW2010 is half the length
of that in WIKIPEDIAMULTI. Overall, we observe
that SEGLANG has a tendency to over-label (despite
the introduction of the t parameter to reduce this ef-
fect), evidenced by high recall but lower precision.
LINGUINI is inherently limited in that it is only able
to detect up to 3 languages per document, causing
recall to suffer on WIKIPEDIAMULTI. However, it
also tends to always output 3 languages, regardless
of the actual number of languages in the document,
hurting precision. Furthermore, even on ALTW2010
it has lower recall than the other two systems.
6 Estimating Language Proportions
In addition to detecting multiple languages within
a document, our method also estimates the relative
proportions of the document that are written in each
language. This information may be useful for detect-
ing documents that are candidate bitexts for training
machine translation systems, since we may expect
languages in the document to be present in equal
proportions. It also allows us to identify the pre-
dominant language of a document.
A core element of our model of a document is
a distribution over a set of labels. Since each la-
bel corresponds to a language, as a first approxima-
tion, we take the probability mass associated with
each label as a direct estimate of the proportion of
the document written in that language. We examine
the results for predicting the language proportions
in the test partition of WIKIPEDIAMULTI. Mapping
label distributions directly to language proportions
produces excellent results, with a Pearson?s r value
of 0.863 and an MAE of 0.108.
Although labels have a one-to-one correspon-
dence with languages, the label distribution does
not actually correspond directly to the language pro-
portion, because the distribution estimates the pro-
portion of byte n-gram sequences associated with
a label and not the proportion of bytes directly.
The same number of bytes in different languages
can produce different numbers of n-gram sequences,
because after feature selection not all n-gram se-
quences are retained in the feature set. Hereafter,
we refer to each n-gram sequence as a token, and the
average number of tokens produced per byte of text
as the token emission rate.
We estimate the per-language token emission rate
(Figure 1) using the training partition of WIKIPE-
DIAMULTI. To improve our estimate of the lan-
guage proportions, we correct our label distribution
35
Original text the cat in the hat
n-gram features
?
???
???
he : 2 the : 2
hat : 1 in : 1
th : 1 the : 1
hat : 1 he c : 1
in t : 1 n th : 1
?
???
???
Emission rate #bytes#tokens = 1812 = 1.5 bytes/token
Figure 1: Example of calculating n-gram emission
rate for a text string.
using estimates of the per-language token emission
rate RLi in bytes per token for Li?L. Assume that
a document D of length |D| is estimated to contain
K languages in proportions Pi for i = 1? ? ?K. The
corrected estimate for the proportion of Li is:
Prop(Li) =
Pi ?RLi?K
j=1 (Pj ?RLj )
(8)
Note that the |D| term is common to the numerator
and denominator and has thus been eliminated.
This correction improves our estimates of lan-
guage proportions. After correction, the Pearson?s
r rises to 0.981, and the MAE is reduced to 0.024.
The improvement is most noticeable for language?
document pairs where the proportion of the docu-
ment in the given language is about 0.5 (Figure 2).
7 Real-world Multilingual Documents
So far, we have demonstrated the effectiveness of
our proposed approach using synthetic data. The
results have been excellent, and in this section we
validate the approach by applying it to a real-world
task that has recently been discussed in the lit-
erature. Yamaguchi and Tanaka-Ishii (2012) and
King and Abney (2013) both observe that in trying
to gather linguistic data for ?non-major? languages
from the web, one challenge faced is that documents
retrieved often contain sections in another language.
SEGLANG (the solution of Yamaguchi and Tanaka-
Ishii (2012)) concurrently detects multilingual doc-
uments and segments them by language, but the ap-
proach is computationally expensive and has a ten-
dency to over-label (Section 5). On the other hand,
the solution of King and Abney (2013) is incom-
plete, and they specifically mention the need for an
automatic method ?to examine a multilingual docu-
ment, and with high accuracy, list the languages that
are present in the document?. In this section, we
show that our method is able to fill this need. We
System P R F
Baseline 0.719 1.00 0.837
SEGLANG 0.779 0.991 0.872
LINGUINI 0.729 0.981 0.837
Our method 0.907 0.916 0.912
Table 4: Detection accuracy for English-language
inclusion in web documents from targeted web
crawls for low-density languages.
make use of manually-annotated data kindly pro-
vided to us by Ben King, which consists of 149 doc-
uments containing 42 languages retrieved from the
web using a set of targeted queries for low-density
languages. Note that the dataset described in King
and Abney (2013) was based on manual confirma-
tion of the presence of English in addition to the low-
density language of primary interest; our dataset
contains these bilingual documents as well as mono-
lingual documents in the low-density language of in-
terest. Our purpose in this section is to investigate
the ability of automatic systems to select this subset
of bilingual documents. Specifically, given a col-
lection of documents retrieved for a target language,
the task is to identify the documents that contain text
in English in addition to the target language. Thus,
we re-train each system for each target language, us-
ing only training data for English and the target lan-
guage. We reserve the data provided by Ben King
for evaluation, and train our methods using data sep-
arately obtained from the Universal Declaration of
Human Rights (UDHR). Where UDHR translations
for a particular language were not available, we used
data from Wikipedia or from a bible translation. Ap-
proximately 20?80 kB of data were used for each
language. As we do not have suitable development
data, we made use of the best parameters for each
system from the experiments on WIKIPEDIAMULTI.
We find that all 3 systems are able to detect that
each document contains the target language with
100% accuracy. However, systems vary in their abil-
ity to detect if a document also contains English in
addition to the target language. The detection accu-
racy for English-language inclusion is summarized
in Table 4.7 For comparison, we include a heuristic
baseline based on labeling all documents as contain-
7Note that Table 2 and Table 3 both report macro and micro-
averaged results across a number of languages. In contrast Ta-
ble 4 only reports results for English, and the values are not
directly comparable to our earlier evaluation.
36
0.2 0.4 0.6 0.8 1.0Actual Proportion
0.2
0.4
0.6
0.8
1.0
Pred
icted
 Prop
ortio
n
Pearson's r: 0.863MAE: 0.108
(a) without emission rate correction
0.2 0.4 0.6 0.8 1.0Actual Proportion
0.2
0.4
0.6
0.8
1.0
Pred
icted
 Prop
ortio
n
Pearson's r: 0.981MAE: 0.0241
(b) with emission rate correction
Figure 2: Scatterplot of the predicted vs. actual language proportions in a document for the test partition of
WIKIPEDIAMULTI (predictions are from our method; each point corresponds to a document-language pair).
ing English. We find that, like the heuristic base-
line, SEGLANG and LINGUINI both tend to over-
label documents, producing false positive labels of
English, resulting in increased recall at the expense
of precision. Our method produces less false pos-
itives (but slightly more false negatives). Overall,
our method attains the best F for detecting En-
glish inclusions. Manual error analysis suggests that
the false negatives for our method generally occur
where a relatively small proportion of the document
is written in English.
8 Future Work
Document segmentation by language could be ac-
complished by a combination of our method and the
method of King and Abney (2013), which could be
compared to the method of Yamaguchi and Tanaka-
Ishii (2012) in the context of constructing corpora
for low-density languages using the web. Another
area we have identified in this paper is the tuning
of the parameters ? and ? in our model (currently
? = 0 and ? = 1), which may have some effect on
the sparsity of the model.
Further work is required in dealing with cross-
domain effects, to allow for ?off-the-shelf? language
identification in multilingual documents. Previous
work has shown that it is possible to generate a docu-
ment representation that is robust to variation across
domains (Lui and Baldwin, 2011), and we intend to
investigate if these results are also applicable to lan-
guage identification in multilingual documents. An-
other open question is the extension of the genera-
tive mixture models to ?unknown? language identi-
fication (i.e. eliminating the closed-world assump-
tion (Hughes et al., 2006)), which may be possible
through the use of non-parametric mixture models
such as Hierarchical Dirichlet Processes (Teh et al.,
2006).
9 Conclusion
We have presented a system for language identifi-
cation in multilingual documents using a generative
mixture model inspired by supervised topic model-
ing algorithms, combined with a document represen-
tation based on previous research in language iden-
tification for monolingual documents. We showed
that the system outperforms alternative approaches
from the literature on synthetic data, as well as on
real-world data from related research on linguistic
corpus creation for low-density languages using the
web as a resource. We also showed that our system
is able to accurately estimate the proportion of the
document written in each of the languages identi-
fied. We have made a full reference implementation
of our system freely available,8 as well as the syn-
thetic dataset prepared for this paper (Section 5), in
order to facilitate the adoption of this technology and
further research in this area.
8https://github.com/saffsd/polyglot
37
Acknowledgments
We thank Hiroshi Yamaguchi for making a reference
implementation of SEGLANG available to us, and
Ben King for providing us with a collection of real-
world multilingual web documents. This work was
substantially improved as a result of the insightful
feedback received from the reviewers.
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and the
Australian Research Council through the ICT Cen-
tre of Excellence program.
References
Steven Abney and Steven Bird. 2010. The human
language project: building a universal corpus of the
world?s languages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 88?97. Association for Computational Lin-
guistics.
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,
Czech Republic.
Timothy Baldwin and Marco Lui. 2010a. Language
identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 229?237, Los Angeles,
USA.
Timothy Baldwin and Marco Lui. 2010b. Multilin-
gual language identification: ALTW 2010 shared task
dataset. In Proceedings of the Australasian Language
Technology Workshop 2010 (ALTW 2010), pages 5?7,
Melbourne, Australia.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings the Second Workshop on
Language in Social Media (LSM2012), pages 65?74,
Montre?al, Canada.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Alessio Bosca and Luca Dini. 2010. Language identi-
fication strategies for cross language information re-
trieval. In Working Notes of the Cross Language Eval-
uation Forum (CLEF).
Jamie Callan and Mark Hoy, 2009. ClueWeb09
Dataset. Available at http://boston.lti.cs.
cmu.edu/Data/clueweb09/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language
identification of search engine queries. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1066?1074, Singapore.
Paul Cook and Marco Lui. 2012. langid.py for bet-
ter language modelling. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2012, pages 107?112, Dunedin, New Zealand.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128?1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004.
Building minority language corpora by learning to
generate web search queries. Knowledge and Infor-
mation Systems, 7(1):56?83.
Emmanuel Giguet. 1995. Categorisation according to
language: A step toward combining linguistic knowl-
edge and statistical learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi
Statistica dei Dati Testuali (JADT), pages 263?268,
Rome, Italy.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas Griffiths. 2002. Gibbs sampling in the gener-
ative model of latent Dirichlet allocation. Technical
Report, Stanford University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485?488, Genoa, Italy.
38
Genitiro Kikui. 1996. Identifying the coding system
and language of on-line documents on the internet. In
Proceedings of the 16th International Conference on
Computational Linguistics (COLING ?96), pages 652?
657, Kyoto, Japan.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents using
weakly supervised methods. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119, At-
lanta, Georgia.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896?899, Beijing, China.
David D. Lewis. 1997. The Reuters-21578 data set.
available at http://www.daviddlewis.
com/resources/testcollections/
reuters21578/.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 176?186, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jicheng Liu and Chunyan Liang. 2008. Text Categoriza-
tion of Multilingual Web Pages in Specific Domain.
In Proceedings of the 12th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
PAKDD?08, pages 938?944, Osaka, Japan.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553?561, Chiang Mai, Thailand.
Bruno Martins and Ma?rio J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764?
768, Santa Fe, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classifi-
cation. In Proceedings of the AAAI-98 Workshop on
Learning for Text Categorization, pages Available as
Technical Report WS?98?05, AAAI Press., Madison,
USA.
Andrew Kachites McCallum. 1999. Multi-label text
classification with a mixture model trained by EM. In
Proceedings of AAAI 99 Workshop on Text Learning.
Paul McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. Jour-
nal of Computing Sciences in Colleges, 20(3):94?101.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings
of 22nd International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR?99), pages 74?81, Berkeley, USA.
John M. Prager. 1999a. Linguini: language identification
for multilingual documents. In Proceedings the 32nd
Annual Hawaii International Conference on Systems
Sciences (HICSS-32), Maui, Hawaii.
John M. Prager. 1999b. Linguini: Language identifica-
tion for multilingual documents. Journal of Manage-
ment Information Systems, 16(3):71?101.
John Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, USA.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009), pages 248?256, Singapore.
Radim Rehurek and Milan Kolkus. 2009. Language
Identification on the Web: Extending the Dictionary
Method. In Proceedings of Computational Linguis-
tics and Intelligent Text Processing, 10th International
Conference (CICLing 2009), pages 357?368, Mexico
City, Mexico.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 527?534,
College Park, USA.
Kevin P Scannell. 2007. The Cru?bada?n Project: Cor-
pus building for under-resourced languages. In Build-
ing and Exploring Web Corpora: Proceedings of the
3rd Web as Corpus Workshop, pages 5?15, Louvain-
la-Neuve, Belgium.
W. J. Teahan. 2000. Text Classification and Seg-
mentation Using Minimum Cross-Entropy. In Pro-
ceedings the 6th International Conference ?Recherche
d?Information Assistee par Ordinateur? (RIAO?00),
pages 943?961, Paris, France.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
Jo?rg Tiedemann and Nikola Ljubes?ic?. 2012. Efficient
discrimination between closely related languages. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
2619?2634, Mumbai, India.
Giang Binh Tran, Dat Ba Nguyen, and Bin Thanh
Kieu. 2010. N-gram based approach for mul-
tilingual language identification. poster. available
39
at http://comp.mq.edu.au/programming/
task_description/VILangTek.pdf.
Fei Xia, Carrie Lewis, and William D. Lewis. 2010. Lan-
guage ID for a thousand languages. In LSA Annual
Meeting Extended Abstracts, Baltimore,USA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 969?978, Jeju Is-
land, Korea.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
40
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UniMelb NLP-CORE: Integrating predictions from multiple domains and
feature sets for estimating semantic textual similarity
Spandana Gella,? Bahar Salehi,?? Marco Lui,??
Karl Grieser,? Paul Cook,? and Timothy Baldwin,??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.au
mhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.au
paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In this paper we present our systems for cal-
culating the degree of semantic similarity be-
tween two texts that we submitted to the Se-
mantic Textual Similarity task at SemEval-
2013. Our systems predict similarity using
a regression over features based on the fol-
lowing sources of information: string similar-
ity, topic distributions of the texts based on
latent Dirichlet alocation, and similarity be-
tween the documents returned by an informa-
tion retrieval engine when the target texts are
used as queries. We also explore methods for
integrating predictions using different training
datasets and feature sets. Our best system was
ranked 17th out of 89 participating systems.
In our post-task analysis, we identify simple
changes to our system that further improve our
results.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic similarity or equivalence between
a pair of short texts. STS is related to many natural
language processing applications such as text sum-
marisation (Aliguliyev, 2009), machine translation,
word sense disambiguation, and question answering
(De Boni and Manandhar, 2003; Jeon et al, 2005).
Two short texts are considered similar if they both
convey similar messages. Often it is the case that
similar texts will have a high degree of lexical over-
lap, although this isn?t always so. For example,
SC dismissed government?s review plea in Vodafone
tax case and SC dismisses govt?s review petition on
Vodafone tax verdict are semantically similar. These
texts have matches in terms of exact words (SC,
Vodafone, tax), morphologically-related words (dis-
missed and dismisses), and abbreviations (govern-
ment?s and govt?s). However, the usages (senses) of
plea and petition, and case and verdict are also sim-
ilar.
One straightforward way of estimating semantic
similarity of two texts is by using approaches based
on the similarity of the surface forms of the words
they contain. However, such methods are not capa-
ble of capturing similarity or relatedness at the lexi-
cal level, and moreover, they do not exploit the con-
text in which individual words are used in a target
text. Nevertheless, a variety of knowledge sources
? including part-of-speech, collocations, syntax,
and domain ? can be used to identify the usage or
sense of words in context (McRoy, 1992; Agirre and
Martinez, 2001; Agirre and Stevenson, 2006) to ad-
dress these issues.
Despite their limitations, string similarity mea-
sures have been widely used in previous seman-
tic similarity tasks (Agirre et al, 2012; Islam and
Inkpen, 2008). Latent variable models have also
been used to estimate the semantic similarity be-
tween words, word usages, and texts (Steyvers and
Griffiths, 2007; Lui et al, 2012; Guo and Diab,
2012; Dinu and Lapata, 2010).
In this paper, we consider three different ways of
measuring semantic similarity based on word and
word usage similarity:
1. String-based similarity to measure surface-
level lexical similarity, taking into account
morphology and abbreviations (e.g., dismisses
and dismissed, and government?s and govt?s);
207
2. Latent variable models of similarity to cap-
ture words that have different surface forms,
but that have similar meanings or that can be
used in similar contexts (e.g., petition and plea,
verdict and case); and
3. Topical/domain similarity of the texts with re-
spect to the similarity of documents in an ex-
ternal corpus (based on information-retrieval
methods) that are relevant to the target texts.
We develop features based on all three of these
knowledge sources to capture semantic similarity
from a variety of perspectives. We build a regres-
sion model, trained on STS training data which has
semantic similarity scores for pairs of texts, to learn
weights for the features and rate the similarity of test
instances. Our approach to the task is to explore the
utility of novel features or features that have not per-
formed well in previous research, rather than com-
bine these features with the myriad of features that
have been proposed by others for the task.
2 Text Similarity Measures
In this section we describe the various features used
in our system.
2.1 String Similarity Measures (SS)
Our first set of features contains various string simi-
larity measures (SS), which compare the target texts
in terms of the words they contain and the order
of the words (Islam and Inkpen, 2008). In the Se-
mEval 2012 STS task (Agirre et al, 2012) such
features were used by several participants (Biggins
et al, 2012; Ba?r et al, 2012; Heilman and Mad-
nani, 2012), including the first-ranked team (Ba?r et
al., 2012) who considered string similarity measures
alongside a wide range of other features.
For our string similarity features, the texts were
lemmatized using the implementation of Lancaster
Stemming in NLTK 2.0 (Bird, 2006), and all punc-
tuation was removed. Limited stopword removal
was carried out by eliminating the words a, and, and
the. The output of each string similarity measure
is normalized to the range of [0, 1], where 0 indi-
cates that the texts are completely different, while 1
means they are identical. The normalization method
for each feature is described in Salehi and Cook (to
appear), wherein the authors applied string similar-
ity measures successfully to the task of predicting
the compositionality of multiword expressions.
Identical Unigrams (IU): This feature measures
the number of words shared between the two texts,
irrespective of word order.
Longest Common Substring (LCS): This mea-
sures the longest sequence of words shared between
the two texts. For example, the longest common
substring between the following sentences is bolded:
A woman and man are dancing in the
rain.
A couple are dancing in the street.
Levenshtein (LEV1): Levenshtein distance (also
known as edit distance) calculates the number of
basic word-level edit operations (insertion, deletion
and substitution) to transform one text into the other:
Levenshtein with substitution penalty (LEV2):
This feature is a variant of LEV1 in which substi-
tution is considered as two edit operations: an inser-
tion and a deletion (Baldwin, 2009).
Smith Waterman (SW): This method is designed
to locally align two sequences of amino acids (Smith
and Waterman, 1981). The algorithm looks for
the longest similar regions by maximizing the num-
ber of matches and minimizing the number of in-
sertion/deletion/substitution operations necessary to
align the two sequences. In other words, it finds the
longest common sequence while tolerating a small
number of differences. We call this sequence, the
?aligned sequence?. It has length equal to or greater
than the longest common sequence.
Not Aligned Words (NAW): As mentioned
above, SW looks for similar regions in the given
texts. Our last string similarity feature shows the
number of identical words not aligned by the SW al-
gorithm. We used this feature to examine how simi-
lar the unaligned words are.
These six features (IU, LCS, LEV1, LEV2, SW,
and NAW) form our string similarity (SS) features.
LEV2, SW, and NAW have not been previously con-
sidered for STS.
208
2.2 Topic Modelling Similarity Measures (TM)
The topic modelling features (TM) are based on La-
tent Dirichlet Allocation (LDA), a generative prob-
abilistic model in which each document is mod-
eled as a distribution over a finite set of topics, and
each topic is represented as a distribution over words
(Blei et al, 2003). We build a topic model on a back-
ground corpus, and then for each target text we cre-
ate a topic vector based on the topic allocations of
its content words, based on the method developed
by Lui et al (2012) for predicting word usage simi-
larity.
The choice of the number of topics, T , can
have a big impact on the performance of this
method. Choosing a small T might give overly-
broad topics, while a large T might lead to un-
interpretable topics (Steyvers and Griffiths, 2007).
Moreover smaller numbers of topics have been
shown to perform poorly on both sentence simi-
larity (Guo and Diab, 2012) and word usage sim-
ilarity tasks (Lui et al, 2012). We therefore build
topic models for 33 values of T in the range
2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.
The background corpus used for generating the
topic models is similar to the COL-WTMF sys-
tem (Guo and Diab, 2012) from the STS-2012 task,
which outperformed LDA. In particular, we use
sense definitions from WordNet, Wiktionary and all
sentences from the Brown corpus. Similarity be-
tween two texts is measured on the basis of the simi-
larity between their topic distributions. We consider
three vector-based similarity measures here: Cosine
similarity, Jensen-Shannon divergence and KL di-
vergence. Thus for each target text pair we extract
99 features corresponding to the 3 similarity mea-
sures for each of the 33 T settings. These features
are used as the TM feature set in the systems de-
scribed below.
2.3 IR Similarity Measures (IR)
The information retrieval?based features (IR) were
based on a dump of English Wikipedia from Novem-
ber 2009. The entire dump was stripped of markup
and tokenised using the OpenNLP tokeniser. The
tokenised documents were then parsed into TREC
format, with each article forming an individual doc-
ument. These documents were indexed using the
Indri IR engine1 with stopword removal. Each
of the two target texts was issued as a full text
query (without any phrases) to Indri, and the first
1000 documents for each text were returned, based
on Okapi term weighting (Robertson and Walker,
1994). These resultant document lists were then
converted into features using a number of set- and
rank-based measures: Dice?s coefficient, Jaccard in-
dex, average overlap, and rank-biased overlap (the
latter two are described in Webber et al (2010)).
The first two are based on simple set overlap and
ignore the ranks; average overlap takes into account
the rank, but equally weights high- and low-ranking
documents; and rank-biased overlap weights higher-
ranked items higher.
In addition to comparisons of the document rank-
ings for a given target text pair, we also consid-
ered a method that compared the top-ranking doc-
uments themselves. To compare two texts, we ob-
tain the top-100 documents using each text as a
query as above. We then calculate the similarity be-
tween these two sets of resultant documents using
the ?2-based corpus similarity measure of Kilgarriff
(2001). In this method the ?2 statistic is calculated
for the 500 most frequent words in the union of the
two sets of documents (corpora), and is interpreted
as the similarity between the sets of documents.
These 5 IR features (4 rank-based, and 1
document-based) are novel in the context of STS,
and are used in the compound systems described be-
low.
3 Compound systems
3.1 Ridge regression
Each of our features represents a (potentially noisy)
measurement of the semantic textual similarity be-
tween two texts. However, the scale of our fea-
tures varies, e.g., [0, 1] for the string similarity fea-
tures vs. unbounded for KL divergence (one of the
topic modelling features). To learn the mapping be-
tween these features and the graded [0, 5] scale of
the shared task, we made use of a statistical tech-
nique known as ridge regression, as implemented in
scikit-learn.2 Ridge regression is a form of
linear regression where the loss function is the ordi-
1http://www.lemurproject.org/indri/
2http://scikit-learn.org
209
nary least squares, but with an additional L2 regular-
ization term. In our empirical evaluation, we found
that ridge regression outperformed linear regression
on our feature set. For brevity, we only present re-
sults from ridge regression.
3.2 Domain Adaptation
Domain adaptation (Daume? and Marcu, 2006) is the
general term applied to techniques for using labelled
data from a related distribution to label data from a
target distribution. For the 2013 Shared Task, no
training data was provided for the target datasets,
making domain adaptation an important considera-
tion. In this work, we assume that each dataset rep-
resents a different domain, and on this basis develop
approaches that are sensitive to inter-domain differ-
ences.
We tested two simple approaches to including do-
main information in our trained model. The first ap-
proach, which we will refer to as flagging, simply in-
volves appending a boolean vector to each training
instance to indicate which training dataset it came
from. The vector has length D, equal to the number
of training datasets (3 for this task, because we train
on the STS 2012 training data). All the values of the
vector are 0, except for a single 1 according to the
dataset that the training instance is drawn from. For
test data, the entire vector consists of 0s.
The second approach we considered is based on
metalearning, and we will refer to it as domain
stacking. In domain stacking, we train a regressor
for each domain (the level 0 regressors (Wolpert,
1992)). Each of these regressors is then applied
to a test instance to produce a predicted value (the
level 0 prediction). These predictions are then com-
bined using a second regressor (the level 1 regres-
sor), to produce a final prediction for each instance
(the level 1 prediction). This approach is closely
related to feature stacking (Lui, 2012) and stacked
generalization (Wolpert, 1992). A general princi-
ple of metalearning is to combine multiple weaker
(?less accurate?) predictors ? termed level 0 pre-
dictors ? to produce a stronger (?more accurate?)
predictor ? the level 1 predictor. In stacked gener-
alization, the level 0 predictors are different learning
algorithms. In feature stacking, they are the same
algorithm trained on different subsets of features, in
this work corresponding to different methods for es-
timating STS (Section 2). In domain stacking, the
level 0 predictions are obtained from subsets of the
training data, where each subset corresponds to all
the instances from a single dataset (e.g. MSRpar or
SMTeuroparl). In terms of subsampling the training
data, this technique is related to bagging (Breiman,
1996). However, rather than generating new train-
ing sets by uniform sampling across the whole pool
of training data, we treat each domain in the train-
ing dataset as a unique sample. Finally, we also ex-
periment with feature-domain stacking, in which the
level 0 predictions are obtained from the cross prod-
uct of subsets of the training data (as per domain
stacking) and subsets of the feature set (as per fea-
ture stacking). We report results for all 3 variants in
Section 5.
This framework of feature-domain stacking can
be applied with any regression or classification al-
gorithm (indeed, the level 0 and level 1 predictors
could be trained using different algorithms). In this
work, all our regressors are trained using ridge re-
gression (Section 3.1).
4 Submitted Runs
In this section we describe the three official runs we
submitted to the shared task.
4.1 Run1 ? Bahar
For this run we used just the SS feature set, aug-
mented with flagging for domain adaptation. Ridge
regression was used to train a regressor across the
three training datasets (MSRvid, MSRpar, SMTeu-
roparl). Each instance was then labelled using the
output of the regressor, and the output range was lin-
early re-scaled to [0, 5] as it occasionally produced
values outside of this range. Although this approach
approximates STS using only lexical textual similar-
ity, it was our best-performing system on the training
data (Table 1). Furthermore the SS features are ap-
pealing because of their simplicity and because they
do not make use of any external resources.
4.2 Run2 ? Concat
In this run, we concatenated the feature vectors
from all three of our feature sets (SS, TM and
IR), and again trained a regressor on the union of
the MSRvid, MSRpar and SMTeuroparl training
datasets. As in Run1, the output of the regression
210
FSet FL FS DS MSRpar MSRvid SMTeuroparl Ave
SS 0.522 0.537 0.526 0.528
(*) SS X 0.552 0.533 0.562 0.549
TM 0.270 0.479 0.425 0.391
TM X 0.250 0.580 0.427 0.419
IR 0.264 0.759 0.407 0.477
IR X 0.291 0.754 0.400 0.482
(+) ALL 0.401 0.543 0.513 0.485
ALL X 0.377 0.595 0.516 0.496
ALL X 0.385 0.587 0.520 0.497
ALL X 0.452 0.637 0.472 0.521
ALL X X 0.429 0.619 0.526 0.524
ALL X X 0.429 0.627 0.526 0.527
(?) ALL X X X 0.441 0.645 0.527 0.538
Table 1: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each training dataset, and the
micro-average over all training datasets. (*), (+),
and (?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
was also linearly re-scaled to the [0, 5] range. Un-
like the previous run, the flagging approach to do-
main adaptation was not used. This approach re-
flects a simple application of machine learning to in-
tegrating data from multiple feature sets and training
datasets, and provides a useful point of comparison
against more sophisticated approaches (i.e., Run3).
4.3 Run3 ? Stacking
In this run, we focused on an alternative method
to integrating information from multiple feature sets
and training datasets, namely feature-domain stack-
ing, as discussed in Section 3.2. In this approach, we
train nine regressors using ridge regression on each
combination of the three training datasets and three
feature sets. Thus, the level 1 representation for each
instance is a vector of nine predictions. For the train-
ing data, when computing the level 1 features for the
same training dataset from which a given instance is
drawn, 10-fold cross-validation is used. Ridge re-
gression is again used to combine the level 1 repre-
sentations and produce the final prediction for each
instance. In addition to this, we also simultaneously
apply the flagging approach to domain adaptation.
This approach incorporates all of our domain adap-
tation efforts, and in initial experiments on the train-
ing data (Table 1) it was our second-best system.
FSet FL FS DS OnWN FNWN Headlines SMT Ave
SS 0.340 0.366 0.688 0.325 0.453
(*) SS X 0.349 0.381 0.711 0.350 0.473
TM 0.648 0.358 0.516 0.209 0.433
TM X 0.701 0.368 0.614 0.287 0.506
IR 0.561 -0.006 0.610 0.228 0.419
IR X 0.596 0.002 0.621 0.256 0.441
(+) ALL 0.679 0.337 0.709 0.323 0.542
ALL X 0.704 0.365 0.718 0.344 0.560
ALL X 0.673 0.298 0.714 0.324 0.539
ALL X 0.618 0.264 0.717 0.357 0.534
ALL X X 0.658 0.309 0.721 0.330 0.540
ALL X X 0.557 0.142 0.694 0.280 0.475
(?) ALL X X X 0.614 0.186 0.706 0314 0.509
Table 2: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each test dataset, and the
micro-average over all test datasets. (*), (+), and
(?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
5 Results
For the STS 2013 task, the organisers advised par-
ticipants to make use of the STS 2012 data; we took
this to mean only the training data. In our post-task
analysis, we realised that the entire 2012 dataset, in-
cluding the testing data, could be used. All our of-
ficial runs were trained only on the training data for
the 2012 task (made up of MSRpar, MSRvid and
SMTeuroparl). We first discuss preliminary find-
ings training and testing on the (STS 2012) training
data, and then present results for the (2013) test data.
Post-submission, we re-trained our systems includ-
ing the 2012 test data.
5.1 Experiments on Training Data
We evaluated our models based on a leave-one-out
cross-validation across the 3 training datasets. Thus,
for each of the training datasets, we trained a sep-
arate model using features from the other two. We
considered approaches based on each individual fea-
ture set, with and without flagging. We further con-
sidered combinations of feature sets using feature
concatenation, as well as feature and domain stack-
ing, again with and without flagging.3 Results are
3We did not consider domain stacking with flagging.
211
FSet FL FS DS OnWN (?) FNWN (?) Headlines (?) SMT (?) Ave (?)
SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)
(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)
TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)
TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)
IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)
IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)
(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)
ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)
ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)
ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)
ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)
ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)
(?) ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)
Table 3: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adaptation
strategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012
data (test + train). (*), (+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the
shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system
performance after adding the additional training data.
reported in Table 1.
The best results on the training data were achieved
using only our SS feature set with flagging (Run1),
with an average Pearson?s ? of 0.549. This fea-
ture set alo gave the best performance on MSR-
par and SMTeuroparl, although the IR feature set
was substantially better on MSRvid. On the training
datasets, our approaches that combine feature sets
did not give an improvement over the best individ-
ual feature set on any dataset, or overall.
5.2 Test Set Results
STS 2013 included four different test sets. Table 2
presents the Pearson?s ? for the same methods as
Section 5.1 ? including our submitted runs ? on
the test data. Run1 drops in performance on the test
set as compared to the training set, where the other
two runs are more consistent, suggesting that lexi-
cal similarity does not generalise well cross-domain.
Table 4 shows that all of our systems performed
above the baseline on each dataset, except Run3 on
FNWN. Table 4 also shows that Run2 consistently
performed well on all the datasets when compared
to the median of all the systems submitted to the task
(Agirre et al, to appear).
Run2, which was based on the concatenation of
all the feature sets, performed well compared to the
stacking-based approaches on the test set, whereas
the stacking approaches all outperformed Run2 on
the training datasets. This is likely due to the
SS features being more effective for STS predic-
tion in the training datasets as compared to the test
datasets. Based on the training datasets, the stack-
ing approaches placed greater weight on the pre-
dictions from the SS feature set. This hypothe-
sis is supported by the result on Headlines, where
the SS feature set does relatively well, and thus the
stacking approaches tend to outperform the simple
concatenation-based method. Finally, an extension
of Run2 with flagging (not submitted to the shared
task) was the best of our methods on the test data.
5.3 Error Analysis
To better understand the behaviour of our systems,
we examined test instances and made the following
observations. Systems based entirely on the TM fea-
tures and domain adaptation consistently performed
well on sentence pairs for which all of our other sys-
tems performed poorly. One example is the follow-
ing OnWN pair, which corresponds to definitions of
newspaper: an enterprise or company that publishes
newsprint and a business firm that publishes news-
papers. Because these texts do not share many com-
mon words, the SS features cannot capture their se-
mantic similarity.
Stacking based approaches performed well on text
pairs which are complex to comprehend, e.g., Two
German tourists, two pilots killed in Kenya air crash
and Senator Reid involved in Las Vegas car crash,
where the individual methods tend to score lower
212
System Headlines OnWN FNWN SMT Ave
(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)
(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)
(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)
Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)
(?) Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)
(?) Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)
(?) Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)
(?) Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)
Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)
Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)
Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)
Table 4: Pearson?s ? (and projected ranking) of runs.
The upper 4 runs are trained only on STS 2012 train-
ing data. (+) denotes runs that were submitted for
evaluation. (?) denotes systems trained on STS 2012
training and test data. For comparison, we include
?Best?, the highest-scoring parametrization of our
system from our post-task analysis (Table 3). We
also include the organiser?s baseline, as well as the
median and best systems for each dataset across all
competitors.
than the human rating, but stacking was able to pre-
dict a higher score (presumably based on the fact
that no method predicted the text pair to be strongly
dissimilar; rather, all methods predicted there to be
somewhat low similarity).
In some cases, the texts are on a similar topic,
but semantically different, e.g., Nigeria mourns over
193 people killed in plane crash and Nigeria opens
probe into deadly air crash. In such cases, systems
based on SS features and stacking perform well.
Systems based on TM and IR features, on the other
hand, tend to predict overly-high scores because the
texts relate to similar topics and tend to have similar
relevant documents in an external corpus.
5.4 Results with the Full Training dataset
We re-trained all the above systems by extending the
training data to include the 2012 test data. Scores on
the 2013 test datasets and the change in Pearson?s ?
after adding the extra training data (denoted ?) are
presented in Table 3.
In general, the addition of the 2012 test data to
the training dataset improves the performance of the
system, though this is often not the case for the flag-
ging approach to domain adaptation, which in some
instances drops in performance after adding the ad-
ditional training data. The biggest improvements
were seen for feature-domain stacking, particularly
on FNWN. This suggests that feature-domain stack-
ing is more sensitive to the similarity between train-
ing data and test data than flagging, but also that it
is better able to cope with variety in training do-
mains than flagging. Given that the pool of anno-
tated data for the STS task continues to increase,
feature-domain stacking is a promising approach to
exploiting the differences between domains to im-
prove overall STS performance.
To facilitate comparison with the published re-
sults for the 2013 STS task, we present a condensed
summary of our results in Table 4, which shows the
absolute score as well as the projected ranking of
each of our systems. It also includes the median and
baseline results for comparison.
6 Conclusions and Future Work
In this paper we described our approach to the
STS SemEval-2013 shared task. While we did not
achieve high scores relative to the other submit-
ted systems on any of the datasets or overall, we
have identified some novel feature sets which we
show to have utility for the STS task. We have
also compared our proposed method?s performance
with a larger training dataset. In future work, we
intend to consider alternative ways for combining
features learned from different domains and training
datasets. Given the strong performance of our string
similarity features on particular datasets, we also in-
tend to consider combining string and distributional
similarity to capture elements of the texts that are not
currently captured by our string similarity features.
Acknowledgments
This work was supported by the European Erasmus
Mundus Masters Program in Language and Commu-
nication Technologies from the European Commis-
sion.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence program.
213
References
Eneko Agirre and David Martinez. 2001. Knowl-
edge sources for word sense disambiguation. In Text,
Speech and Dialogue, pages 1?10. Springer.
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation, volume 33 of
Text, Speech and Language Technology, pages 217?
251. Springer Netherlands.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. to appear. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Atlana, USA. Association for Computational Linguis-
tics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Sam Biggins, Shaabi Mohammed, Sam Oakley, Luke
Stringer, Mark Stevenson, and Judita Preiss. 2012.
University of sheffield: Two approaches to semantic
text similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 655?661, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 69?72, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Leo Breiman. 1996. Bagging predictors. Machine learn-
ing, 24(2):123?140.
Hal Daume?, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126, May.
Marco De Boni and Suresh Manandhar. 2003. The use
of sentence similarity as a semantic relevance metric
for question answering. In Proceedings of the AAAI
Symposium on New Directions in Question Answering,
Stanford, USA.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In *SEM 2012: The First Joint
Conference on Lexical and Computational Semantics
? Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 586?590, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Michael Heilman and Nitin Madnani. 2012. Ets: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ?05, pages 84?90, New York, NY,
USA. ACM.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):97?133.
214
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Association Workshop 2012, pages 33?41,
Dunedin, New Zealand, December.
Marco Lui. 2012. Feature stacking for sentence clas-
sification in evidence-based medicine. In Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop 2012, pages 134?138, Dunedin, New
Zealand, December.
Susan W McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. Computational
Linguistics, 18(1):1?30.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In *SEM 2013:
The Second Joint Conference on Lexical and Com-
putational Semantics, Atlana, USA. Association for
Computational Linguistics.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195?197.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424?440.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
215
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 15?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Intelligent Linux Information Access by Data Mining: the ILIAD Project
Timothy Baldwin,? David Martinez,? Richard B. Penman,? Su Nam Kim,?
Marco Lui,? Li Wang? and Andrew MacKinlay?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
Abstract
We propose an alternative to conventional in-
formation retrieval over Linux forum data,
based on thread-, post- and user-level analysis,
interfaced with an information retrieval engine
via reranking.
1 Introduction
Due to the sheer scale of web data, simple keyword
matching is an effective means of information ac-
cess for many informational web queries. There
still remain significant clusters of information access
needs, however, where keyword matching is less
successful. One such instance is technical web fo-
rums and mailing lists (collectively termed ?forums?
for the purposes of this paper): technical forums
are a rich source of information when troubleshoot-
ing, and it is often possible to resolve technical
queries/problems via web-archived data. The search
facilities provided by forums and web search en-
gines tend to be over-simplistic, however, and there
is a desperate need for more sophisticated search (Xi
et al, 2004; Seo et al, 2009), including: favour-
ing threads which have led to a successful resolu-
tion; reflecting the degree of clarity/reproducibility
of the proposed solution in a given thread; repre-
senting threads via their threaded rather than sim-
ple chronological structure; the ability to highlight
key aspects of the thread, in terms of the problem
description and solution which led to a successful
resolution; and ideally, the ability to represent the
problem and solution in normalised form via infor-
mation extraction.
This paper provides a brief outline of an attempt
to achieve these and other goals in the context of
Linux web user forum data, in the form of the IL-
IAD (Intelligent Linux Information Access by Data
Mining) project. Linux users and developers rely
particularly heavily on web user forums and mail-
ing lists, due to the nature of the community, which
is highly decentralised ? with massive proliferation
of packages and distributions? and notoriously bad
at maintaining up-to-date documentation at a level
suitable for newbie and even intermediate users.
2 Project Outline
Our proposed solution is as follows: (1) crawl data
from a variety of web user forums; (2) analyse each
thread, to identify named entities and generate meta-
data; (3) analyse post-level linkages; (4) predict
user-level features which are expected to impinge on
the quality of search results; and finally (5) draw to-
gether the features from (1) to (4) to enhance the
quality of a traditional ranked IR approach. We
briefly review each step below. Given space limi-
tations, we focus on outlining our interpretation of
the task in this paper. For further details and results,
the reader is referred to the key papers cited herein.
2.1 Crawling
The first step is to crawl data from a variety of fo-
rums and mailing lists, for which we have developed
open-source scraping software in the form of SITE-
SCRAPER.1 SITESCRAPER is designed such that the
user simply copies relevant content from a browser-
rendered version of a given set of pages, which it
interprets as a structured record, and translates into
a generalised XPATH query.
2.2 Thread-level analysis
Next, we perform named entity recognition (NER)
over each thread to identify entities such as package
and distribution names, version numbers and snip-
pets of code; as part of this, we perform version
1http://sitescraper.googlecode.com/
15
anchoring, in identifying what entity each version
number relates to.
To generate thread-level metadata, we classify
each thread for the following three features, based
on an ordinal scale of 1?5 (Baldwin et al, 2007):
Complete: Is the problem description complete?
Solved: Is a solution provided in the thread?
Task Oriented: Is the thread about a specific
problem?
We additionally automatically classify the nature
of the thread content, in terms of, e.g., whether it
contains documentation or installation details, or re-
lates to software, hardware or programming.
Our experiments on thread-level classification are
based on a set of 250 annotated threads from Lin-
uxQuestions and other forums, as well as a dataset
from CNET.
2.3 Post-level analysis
We automatically analyse the post-to-post discourse
structure of each thread, in terms of which (preced-
ing) post(s) each post relates to, and how, building
off the work of Rose? et al (1995) and Wolf and Gib-
son (2005). For example, a given post may refute
the solution proposed in an earlier post, and also
propose a novel solution in response to the initiat-
ing post.
Separately, we are developing techniques for
identifying whether a new post to a given forum
is sufficiently similar to other (ideally resolved)
threads that the author should be prompted to first
check the existing threads for redundancy before a
new thread is initiated.
Our experiments on post-level analysis are, once
again, based on data from LinuxQuestions and
CNET.
2.4 User-level analysis
We are also experimenting with profiling users vari-
ously, based on a 5-point ordinal scale across a range
of user characteristics. Our experiments are based
on data from LinuxQuestions (Lui, 2009).
2.5 IR ranking
The various features are interfaced with an ad hoc
information retrieval (IR) system via a learning-to-
rank approach (Cao et al, 2007). In order to carry
out IR evaluation, we have developed a set of queries
and relevance judgements over a large-scale set of
forum data.
Our experiments to date have been based on com-
bination over three IR engines (LUCENE, ZETTAIR
and LEMUR), and involved thread-level metadata
only, but we have achieved encouraging results, sug-
gesting that thread-level metadata can enhance IR
effectiveness.
3 Conclusions
This paper provides an outline of the ILIAD project,
focusing on the tasks of crawling, thread-level anal-
ysis, post-level analysis, user-level analysis and IR
reranking. We have designed a series of class sets
for the component tasks, and carried out experimen-
tation over a range of data sources, achieving en-
couraging results.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
T Baldwin, D Martinez, and RB Penman. 2007. Auto-
matic thread classification for Linux user forum infor-
mation access. In Proc of ADCS 2007.
Z Cao, T Qin, TY Liu, MF Tsai, and H Li. 2007. Learn-
ing to rank: from pairwise approach to listwise ap-
proach. In Proc of ICML 2007.
M Lui. 2009. Impact of user characteristics on online fo-
rum classification tasks. Honours thesis, University of
Melbourne. http://repository.unimelb.edu.
au/10187/5745.
CP Rose?, B Di Eugenio, LS Levin, and C Van Ess-
Dykema. 1995. Discourse processing of dialogues
with multiple threads. In Proc of ACL 1995.
J Seo, WB Croft, and DA Smith. 2009. Online commu-
nity search using thread structure. In Proc of CIKM
2009.
F Wolf and E Gibson. 2005. Representing discourse co-
herence: A corpus-based study. Comp Ling, 31(2).
W Xi, J Lind, and E Brill. 2004. Learning effective rank-
ing functions for newsgroup search. In Proc of SIGIR
2004.
16
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 17?25,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Accurate Language Identification of Twitter Messages
Marco Lui and Timothy Baldwin
NICTA VRL
Department of Computing and Information Systems
University of Melbourne, VIC 3010, Australia
mhlui@unimelb.edu.au, tb@ldwin.net
Abstract
We present an evaluation of ?off-the-
shelf? language identification systems as
applied to microblog messages from Twit-
ter. A key challenge is the lack of an ad-
equate corpus of messages annotated for
language that reflects the linguistic diver-
sity present on Twitter. We overcome this
through a ?mostly-automated? approach to
gathering language-labeled Twitter mes-
sages for evaluating language identifica-
tion. We present the method to con-
struct this dataset, as well as empirical
results over existing datasets and off-the-
shelf language identifiers. We also test
techniques that have been proposed in the
literature to boost language identification
performance over Twitter messages. We
find that simple voting over three specific
systems consistently outperforms any spe-
cific system, and achieves state-of-the-art
accuracy on the task.
1 Introduction
Twitter
1
has captured the attention of various re-
search communities as a potent data source, be-
cause of the immediacy of the information pre-
sented, the volume and variability of the data con-
tained, the potential to analyze networking effects
within the data, and the ability to (where GPS
data is available) geolocate messages (Krishna-
murthy et al., 2008). Although individual mes-
sages range from inane through mundane right up
to insane, the aggregate of these messages can lead
to profound insights in real-time. Examples in-
clude real-time detection of earthquakes (Sakaki
1
http://www.twitter.com
et al., 2010), analysis of the location and preva-
lence of flu epidemics (Lampos et al., 2010; Cu-
lotta, 2010), news event detection (Petrovi?c et al.,
2010), and prediction of sporting match outcomes
(Sinha et al., 2013).
Text analysis of social media has quickly be-
come one of the ?frontier? areas of Natural Lan-
guage Processing (NLP), with major conferences
opening entire tracks for it in recent years. The
challenges in NLP for social media are many,
stemming primarily from the ?noisy? nature of the
content. Research indicates that English Twitter
in particular is more dissimilar to the kinds of ref-
erence corpora used in NLP to date, compared
to other forms of social media such as blogs and
comments (Baldwin et al., 2013). This has led
to the development of techniques to ?normalize?
Twitter messages (Han et al., 2013), as well as
Twitter-specific approaches to conventional NLP
tasks such as part-of-speech tagging (Gimpel et
al., 2011) and information extraction (Bontcheva
et al., 2013). Even so, a precondition of NLP
techniques is that the language of the input data
is known, and this has led to interest in ?language
identification? (LangID) of Twitter messages. Re-
search has shown that ?off-the-shelf? LangID sys-
tems appear to perform fairly well on Twitter (Lui
and Baldwin, 2012), but Twitter-specific systems
seem to perform better (Carter et al., 2013; Tromp
and Pechenizkiy, 2011; Bergsma et al., 2012;
Goldszmidt et al., 2013).
Twitter recognizes the utility of language meta-
data in enabling new applications, and as of March
2013 includes language predictions with results
from its API (Roomann-Kurrik, 2013). These pre-
dictions are not perfect (see Section 3.2), and at
time of writing do not cover some languages (e.g.
Romanian). Furthermore, some research groups
17
have collected a substantial cache of Twitter data
from before the availability of built-in predictions.
Motivated by the need to work with monolingual
subsets of historical data, we investigate the most
practical means of carrying out LangID of Twitter
messages, balancing accuracy with ease of imple-
mentation. In this work, we present an evaluation
of ?off-the-shelf? language identifiers, combined
with techniques that have been proposed for boost-
ing accuracy on Twitter messages.
A major challenge that we have had to over-
come is the lack of annotated data for evaluation.
Bergsma et al. (2012) point out that in LangID
research on microblog messages to date, only a
small number of European languages has been
considered. Baldwin and Lui (2010) showed that,
when considering full documents, good perfor-
mance on just European languages does not nec-
essarily imply equally good performance when a
larger set of languages is considered. This does
not detract from work to date on European lan-
guages (Tromp and Pechenizkiy, 2011; Carter et
al., 2013), but rather highlights the need for fur-
ther research in LangID for microblog messages.
Manual annotation of Twitter messages is a
challenging and laborious process. Furthermore,
Twitter is highly multilingual, making it very dif-
ficult to obtain annotators for all of the languages
represented. Previous work has attempted to
crowdsource part of this process (Bergsma et al.,
2012), but such an approach requires substantial
monetary investment, as well as care in ensuring
the quality of the final annotations. In this paper,
we propose an alternative, ?mostly-automated?
approach to gathering language-labeled Twitter
messages for evaluating LangID. A corpus con-
structed by direct application of automatic LangID
to Twitter messages would obviously be unsuit-
able for evaluating the accuracy of LangID tools.
Even with manual post-filtering, the remaining
dataset would be biased towards messages that
are easy for automated systems to classify cor-
rectly. The novelty of our approach is to leverage
user identity, allowing us to construct a corpus of
language-labeled Twitter messages without using
automated tools to determine the languages of the
messages. This quality makes the corpus suitable
for use in the evaluation of automated LangID of
Twitter messages.
Our main contributions are: (1) we provide
a manually-labeled dataset of Twitter messages,
adding Chinese (zh) and Japanese (ja) to the set of
Twitter messages with human annotation for lan-
guage; (2) we provide a second dataset constructed
using a mostly-automated approach, covering 65
languages; (3) we detail the method for construct-
ing the dataset; (4) we provide a comprehensive
empirical evaluation of the accuracy of off-the-
shelf LangID systems on Twitter messages, using
published datasets in addition to the new datasets
we have introduced; and (5) we discuss and eval-
uate a simple voting-based ensemble for LangID,
and find that it outperforms any individual system
to achieve state-of-the-art results.
2 Background
LangID is the problem of mapping a document
onto the language(s) it is written in. The best-
known technique classifies documents according
to rank order statistics over character n-gram se-
quences between a document and a global lan-
guage profile (Cavnar and Trenkle, 1994). Other
statistical approaches applied to LangID include
Markov models over n-gram frequency profiles
(Dunning, 1994), dot products of word frequency
vectors (Darnashek, 1995), and string kernels
in support vector machines (Kruengkrai et al.,
2005). In contrast to purely statistical meth-
ods, linguistically-motivated models for LangID
have also been proposed, such as the use of stop
word lists (Johnson, 1993), where a document is
classified according to its degree of overlap with
lists for different languages. Other approaches
include word and part-of-speech (POS) corre-
lation (Grefenstette, 1995), cross-language tok-
enization (Giguet, 1995) and grammatical-class
models (Dueire Lins and Gonc?alves, 2004).
LangID of short strings has attracted recent
interest from the research community. Ham-
marstrom (2007) describes a method that aug-
ments a dictionary with an affix table, and tests
it over synthetic data derived from a parallel bible
corpus. Ceylan and Kim (2009) compare a num-
ber of methods for identifying the language of
search engine queries of 2 to 3 words. They de-
velop a method which uses a decision tree to in-
tegrate outputs from several different LangID ap-
proaches. Vatanen et al. (2010) focus on mes-
sages of 5?21 characters, using n-gram language
models over data drawn from UDHR in a naive
Bayes classifier. Carter et al. (2013) focus specifi-
cally on LangID in Twitter messages by augment-
18
ing standard methods with LangID priors based
on a user?s previous messages and the content
of links embedded in messages, and this is also
the method used in TwitIE (Bontcheva et al.,
2013). Tromp and Pechenizkiy (2011) present
a method for LangID of short text messages by
means of a graph structure, extending the stan-
dard ?bag? model of text to include information
about the relative order of tokens. Bergsma et
al. (2012) examine LangID for creating language-
specific twitter collections, finding that a compres-
sive method trained over out-of-domain data from
Wikipedia and standard text corpora performed
better than the off-the-shelf language identifiers
they tested. Goldszmidt et al. (2013) propose
a method based on rank-order statistics, using a
bootstrapping process to acquire in-domain train-
ing data from unlabeled Twitter messages. Recent
work has also put some emphasis on word-level
rather than document-level LangID (Yamaguchi
and Tanaka-Ishii, 2012; King and Abney, 2013),
including research on identifying the language of
each word in multilingual online communications
(Nguyen and Dogruoz, 2013; Ling et al., 2013).
In this paper, we focus on monolingual messages,
as despite being simpler, LangID of monolingual
Twitter messages is far from solved.
In Section 1, we discussed some work to date
on LangID on Twitter data. Some authors have re-
leased accompanying datasets: the dataset used by
Tromp and Pechenizkiy (2011) was made avail-
able in its entirety, consisting of 9066 messages
in 6 Western European languages. Other au-
thors have released message identifiers with as-
sociated language labels, including Carter et al.
(2013), with 5000 identifiers in 5 Western Euro-
pean languages, and Bergsma et al. (2012), pro-
viding 13190 identifiers across 9 languages from
3 language families (Arabic, Cyrillic and Devana-
gari). To date, only the dataset of Tromp and
Pechenizkiy (2011) has been used by other re-
searchers (Goldszmidt et al., 2013). With the kind
co-operation of the authors, we have obtained the
full datasets of Carter et al. (2013) and Bergsma
et al. (2012), allowing us to present the most ex-
tensive empirical evaluation of LangID of Twitter
messages to date. However, the total set of lan-
guages covered is still very small. In Section 2.1,
we present our own manually-annotated dataset,
adding Chinese (zh) and Japanese (ja) to the lan-
guages that have manually-annotated data.
English Chinese Japanese
Initial 0.906 0.773 0.989
Post-review 0.930 0.916 0.998
Table 1: Inter-annotator agreement measured us-
ing Fleiss? kappa (Fleiss, 1971) over annotations
for TWITTER.
2.1 Manual annotation of ZHENJA
A manual approach to constructing a LangID
dataset from Twitter data is difficult due to the
wide variety of languages present on Twitter ?
Bergsma et al. (2012) report observing 65 lan-
guages in a 10M message sample, and Baldwin
et al. (2013) report observing 97 languages in a
1M message sample. While this is encouraging
in terms of sourcing data for lower-density lan-
guages, the distribution of languages is Zipfian,
and the relative proportion of data in most lan-
guages is very small. Manually retrieving all avail-
able messages in a language would require a na-
tive speaker to view and reject a huge number
of messages in other languages in order to col-
lect the small number that are written in the tar-
get language. We initially attempted this, build-
ing ZHENJA, a dataset derived from a set of 5000
messages randomly sampled from a larger body
of 622192 messages collected from the Twitter
streaming API over a single 24-hour period in Au-
gust 2010. The messages are a 1% representative
sample of the total public messages posted on that
day. Each of the 5000 selected messages was an-
notated by speakers of three languages, English,
Japanese and Mandarin Chinese. For each mes-
sage, three annotators were asked if the message
contained any text in languages which they spoke,
as well as if it appeared to contain text in (unspeci-
fied) languages which they did not speak. The lat-
ter label was introduced in order to make a distinc-
tion between text in languages not spoken by our
annotators (e.g. Portuguese) and text with no lin-
guistic content (e.g. URLs). After the initial anno-
tation, annotators were asked to review messages
where there was disagreement, and messages were
assigned labels given by a majority of annotators
post-review. Inter-annotator agreement (Table 1)
is strong for the task: only 20 out of 5000 mes-
sages have less than 80% majority in annotations.
In many instances, the disagreement was due to
messages consisting entirely of a short sequence
of hanzi/kanji, which both Chinese and Japanese
speakers recognized as valid (these messages are
19
excluded from our set of labeled messages). Out
of the 5000 messages, 1953 (39.1%) were labeled
as English, 16 were labeled as Chinese (0.3%) and
1047 were labeled as Japanese (20.9%), for a total
of 3016 labeled messages.
A total of 8 annotators each invested 2?4 hours
in this annotation task, and the final dataset only
covers 3 languages (which includes the top-2
highest-density languages in Twitter). Obviously,
constructing a dataset of language-labeled Twit-
ter messages is a labor-intensive process, and the
lower density the language, the more expensive
our methodology becomes (as more and more doc-
uments need to be looked over to find documents
in the language of interest). Ideally, we would like
to be able to use some form of automated LangID
to accelerate the process without biasing the data
towards easy-to-classify messages.
2.2 A broad-coverage Twitter corpus
Based on our discussion so far, our desiderata for a
LangID dataset of Twitter messages are as follows:
(1) achieve broader coverage of languages than ex-
isting datasets; (2) minimize manual annotation;
and (3) avoid bias induced by selecting messages
using LangID. (2) and (3) may seem to be con-
flicting objectives, but we sidestep the problem by
first identifying monolingual users, then produce a
dataset by sampling messages by these users from
a held-out collection.
The overall workflow for constructing a dataset
is summarized in Algorithm 1. For each user we
consider, we divide all their messages into two dis-
joint sets. One set (M
main
u
) is used to determine
the language(s) spoken by the user. If only one
language is detected, the user is added to a pool
of candidate users (U
accept
). A fixed number of
users is sampled for each language (U
sample
), and
for each sampled user a fixed number of messages
is sampled from the held-out set (M
heldout
u
) and
added to the final dataset. We sample a fixed num-
ber of users per language to limit the amount of
data in the more-frequent languages, and we only
sample a small number of messages per user in
order to avoid biasing the dataset towards the lin-
guistic idiosyncrasies of any specific individual.
For both sampling steps, if the number of items
available is less than the number required, all the
available items are returned.
Algorithm 1 uses automated LangID to detect
the language of messages in M
main
u
(line 8). The
Algorithm 1 Procedure for building a Twitter
LangID dataset
1: U ? active users
2: L
accept
,M
accept
, U
accept
? {}, {}, {}
3: for each u ? U do
4: M
u
? all messages by user u
5: M
main
u
,M
heldout
u
? RandomSplit(M
u
)
6: L
u
? {}
7: for each m ?M
main
u
do
8: l
u
? LangID(m)
9: if l
u
6= unknown then
10: L
u
? L
u
? {l
u
}
11: end if
12: end for
13: if len(L
u
) = 1 then
14: U
accept
? U
accept
? {(u, L
u
)}
15: L
accept
? L
accept
? L
u
16: end if
17: end for
18: for each l ? L
accept
do
19: U
sample
? Sample(U
accept
l
,K)
20: for each u ? U
sample
do
21: M
sample
? Sample(M
heldout
u
, N)
22: M
accept
?M
accept
? {(M
sample
, l)}
23: end for
24: end for
25: return M
accept
accuracy of this identifier is not critical, as any
misclassifications for a monolingual user would
cause them to be rejected, as they would appear
multilingual. Hence, the risk of false positives at
the user-level LangID is very low. However, in-
correctly rejecting users reduces the pool of data
available for sampling, so a higher-accuracy solu-
tion is preferable. We compared the performance
of 8 off-the-shelf (i.e. pre-trained) LangID systems
to determine which would be the most suitable for
this role.
langid.py (Lui and Baldwin, 2012): an n-
gram feature set selected using data from multi-
ple sources, combined with a multinomial naive
Bayes classifier.
CLD2 (McCandless, 2010): the language iden-
tifier embedded in the Chrome web browser;
2
it
uses a naive Bayes classifier and script-specific to-
kenization strategies.
LangDetect (Nakatani, 2010): a naive Bayes
classifier, using a character n-gram based repre-
sentation without feature selection, with a set of
normalization heuristics to improve accuracy.
LDIG (Nakatani, 2012): a Twitter-specific
LangID tool, which uses a document representa-
tion based on tries, combined with normalization
2
http://www.google.com/chrome
20
heuristics and Bayesian classification, trained on
Twitter data.
whatlang (Brown, 2013): a vector-space
model with per-feature weighting over character
n-grams.
YALI (Majli
?
s, 2012): computes a per-language
score using the relative frequency of a set of byte
n-grams selected by term frequency.
TextCat (Scheelen, 2003); an implementation
of Cavnar and Trenkle (1994), which uses an ad-
hoc rank-order statistic over character n-grams.
MSR-LID (Goldszmidt et al., 2013): based on
rank-order statistics over character n-grams, and
Spearman?s ? to measure correlation. Twitter-
specific training data is acquired through a boot-
strapping approach. We use the 49-language
model provided by the authors, and the best pa-
rameters reported in the paper.
We investigated the performance of the systems
using manually-labeled datasets of Twitter mes-
sages (Table 2), including the ZHENJA set de-
scribed in Section 2.1.
3
We find that all the sys-
tems tested perform well on TROMP, with the
exception of TextCat. CARTER covers a very
similar set of languages to TROMP, yet all sys-
tems consistently perform worse on it. This sug-
gests that TROMP is biased towards messages that
LangID systems are likely to identify correctly
(also observed by Goldszmidt et al. (2013)). This
is due in part to the post-processing applied to the
messages, but also suggests a bias in how mes-
sages were selected. LDIG is the best performer
on TROMP and CARTER, albeit falling slightly
short of the 99.1% accuracy reported by the author
(Nakatani, 2012). However, it is only trained on
17 languages and thus is not able to fully support
BERGSMA and ZHENJA, and so we cannot draw
any conclusions on whether the method will gen-
eralize well to more languages. The system that
supports the most languages by far is whatlang,
but as a result its accuracy on Twitter messages
suffers. Manual analysis suggests this is due to
Twitter-specific ?noise? tipping the model in fa-
vor of lower-density languages. On BERGSMA,
LangDetect is the best performer, likely due
to its specific heuristics for distinguishing certain
language pairs (Nakatani, 2010), which happen to
be present in the BERGSMA dataset. Overall, in
3
We do not limit the comparison to languages supported
by each system as this would bias evaluation towards systems
that support few languages that are easy to discriminate.
their off-the-shelf configuration, only three sys-
tems (langid.py, CLD2, LangDetect) per-
form consistently well on LangID of Twitter mes-
sages. Even so, the macro-averaged F-Scores ob-
served were as low as 83%, indicating that whilst
performance is good, the problem of LangID of
Twitter messages is far from solved.
Given that the set of languages covered and ac-
curacy varies between systems, we investigated a
simple voting-based approach to combining the
predictions. For each dataset, we considered all
combinations of 3, 5, and 7 systems, combin-
ing the predictions using a simple majority vote.
The single-best combination for each dataset is re-
ported in Table 3. In all cases, the macro-averaged
F-score is improved upon, showing the effective-
ness of the voting approach. Hence, for purposes
of LangID in Algorithm 1, we chose to use a
majority-vote ensemble of langid.py, CLD2
and LangDetect, a combination that generally
performs well on all datasets.
4
Where all 3 sys-
tems disagree, the message is labeled as unknown,
which does not count as a separate language for
determining if a user is multilingual, mitigating
the risk of wrongly rejecting a monolingual user
due to misclassifying a particular message. This
ensemble is hereafter referred to as VOTING.
To build our final dataset, we collected all mes-
sages by active users from the 1% feed made avail-
able by Twitter over the course of 31 days, be-
tween 8 January 2012 and 7 February 2012. We
deemed users active if they had posted at least
5 messages in a single day on at least 7 differ-
ent days in the 31-day period we collected data
for. This gave us a set of approximately 2M
users. For each user, we partitioned their mes-
sages (RandomSplit in Algorithm 1) by selecting
one day at random. All of the messages posted
by the user on this day were treated as heldout
data (M
heldout
u
), and the remainder of the user?s
messages (M
main
u
) were used to determine the
language(s) spoken by the user. The day cho-
sen was randomly selected per-user to avoid any
bias that may be introduced by messages from
a particular day or date. Of the active users,
we identified 85.0% to be monolingual, cover-
ing a set of 65 languages. 50.6% of these users
spoke English (en), 14.1% spoke Japanese (ja),
and 13.0% spoke Portuguese (pt); this user-level
4
MSR-LID was excluded due to technical difficulties in
applying it to a large collection of messages because of its
oversized model.
21
Dataset langid.py CLD2 LangDetect LDIG whatlang YALI TextCat MSR-LID
TROMP 0.983 0.972 0.959 0.986 0.950 0.911 0.814 0.983
CARTER 0.917 0.902 0.891 0.943 0.834 0.824 0.510 0.927
BERGSMA 0.847 0.911 0.923 0.000 0.719 0.428 0.046 0.546
ZHENJA 0.871 0.884 0.831 0.315 0.622 0.877 0.313 0.848
Table 2: Macro-averaged F-Score on manually-annotated Twitter datasets. Italics denotes results where
the dataset contains languages not supported by the identifier.
Dataset
Single Best Voting 3-System
System F-Score Systems F-Score F-Score
TROMP LDIG 0.986 CLD2, MSR-LID, LDIG 0.992 0.986
CARTER LDIG 0.943 MSR-LID, langid.py, LDIG 0.948 0.927
BERGSMA LangDetect 0.923 CLD2, LangDetect, langid.py 0.935 0.935
ZHENJA CLD2 0.884 CLD2, MSR-LID, LDIG, YALI, langid.py 0.969 0.941
Table 3: System combination by majority voting. All combinations of 3, 5 and 7 systems were con-
sidered. For each dataset, we report the single-best system, the best combination, and F-score of the
majority-vote combination of langid.py, CLD2 and LangDetect.
language distribution largely mirrors the message-
level language distribution reported by Baldwin et
al. (2013) and others. From this set of users, we
randomly selected up to 100 users per language,
leaving us with a pool of 26011 held-out mes-
sages from 2914 users. Manual inspection of these
messages revealed a number of English messages
mislabeled with another language, indicating that
even predominantly monolingual users occasion-
ally introduce English into their online commu-
nications. Such messages are generally entirely
English, with code-switching (i.e. multiple lan-
guages in the same message) very rarely observed.
In order to eliminate mislabeled messages, we ap-
plied all 8 systems to this pool of 26011 messages.
Where at least 5 systems agree and the predicted
language does not match the user?s language, we
discarded the message. Where 3 or 4 systems
agree, we manually inspected the messages and
eliminated those that were clearly mislabeled (this
is the only manual step in the construction of this
dataset). Overall, we retained 24220 messages
(93.1%). From these, we sampled up to 5 mes-
sages per unique user, producing a final dataset of
14178 messages across 65 languages (hereafter re-
ferred to as the TWITUSER dataset).
3 Evaluating off-the-shelf language
identifiers on Twitter
Given TWITUSER, our broad-coverage Twitter
corpus, we return to the task of examining the
performance of the off-the-shelf LangID systems
we discussed in Section 2.2 (Table 4, left side).
In terms of macro-averaged F-Score across the
full set of 65 languages, CLD2 is the single best-
performing system. Unlike langid.py and
LangDetect, CLD2 does not always produce a
prediction, and instead has an in-built threshold
for it to output a prediction of ?unknown?. This
is reflected in the elevated precision, at the ex-
pense of decreased recall and message-level ac-
curacy. Systems like langid.py which always
make a prediction have reduced precision, bal-
anced by increased recall and message-level ac-
curacy. As with the manually-annotated datasets,
we experimented with a simple voting-based ap-
proach to combining multiple classifiers. We
again experimented with all possible combina-
tions of 3, 5 and 7 classifiers, and found that on
TWITUSER, a majority-vote ensemble of CLD2,
langid.py and LangDetect attains the best
macro-averaged F-Score, and also outperforms
any individual system on all of the metrics con-
sidered. We note that this is exactly the VOTING
ensemble of Section 2.2, validating its choice as
LangID(m) in Algorithm 1.
3.1 Adapting off-the-shelf LangID to Twitter
Tromp and Pechenizkiy (2011) propose to remove
links, usernames, hashtags and smilies before at-
tempting LangID, as they are Twitter specific. We
experimented with applying this cleaning proce-
dure to each message body before passing it to
our off-the-shelf systems (Table 4, right side). For
LDIG and MSR-LID, the results are exactly the
same with and without cleaning. These two sys-
tems are specifically targeted at Twitter messages,
and thus may include a similar normalization as
part of their processing pipeline. This also sug-
gests that the systems do not leverage this Twitter-
22
Tool
Without Cleaning With Cleaning
P R F Acc P R F Acc
langid.py 0.767 0.861 0.770 0.842 0.759 0.861 0.766 0.840
CLD2 0.852 0.814 0.806 0.775 0.866 0.823 0.820 0.780
LangDetect 0.618 0.680 0.626 0.839 0.623 0.687 0.634 0.854
LDIG 0.167 0.239 0.189 0.447 0.167 0.239 0.189 0.447
whatlang 0.749 0.655 0.663 0.624 0.739 0.667 0.663 0.623
YALI 0.441 0.564 0.438 0.710 0.449 0.560 0.443 0.705
TextCat 0.327 0.245 0.197 0.257 0.316 0.295 0.230 0.316
MSR-LID 0.533 0.609 0.536 0.848 0.533 0.609 0.536 0.848
VOTING 0.920 0.876 0.887 0.861 0.919 0.883 0.889 0.868
Table 4: Macro-averaged Precision/Recall/F-Score, as well as message-level accuracy for each system
on TWITUSER. The right side of the table reports results after applying message-level cleaning (Tromp
and Pechenizkiy, 2011).
specific content in making predictions. Other sys-
tems generally show a small improvement with
cleaning, except for langid.py. The VOTING
ensemble also benefits from cleaning, due to the
improvement in two of its component classifiers
(CLD2 and LangDetect). This cleaning pro-
cedure is trivial to implement, so despite the im-
provement being small, it may be worth imple-
menting if adapting off-the-shelf language identi-
fiers to Twitter messages.
Goldszmidt et al. (2013) suggest bootstrap-
ping a Twitter-specific language identifier using
an off-the-shelf language identifier and an unla-
beled collection of Twitter messages. We tested
this approach, using the 3 systems that provide
tools to generate new models from labeled data
(LangDetect, langid.py and TextCat).
We constructed bootstrap collections by: (1) us-
ing the off-the-shelf tools to directly identify
the language of messages; and (2) using Algo-
rithm 1. Overall, the bootstrapped identifiers are
not better than their off-the-shelf counterparts.
For TextCat there is an increase in accuracy
using bootstrapped models, but the accuracy of
TextCat with bootstrapped models is still infe-
rior to LangDetect and langid.py in their
off-the-shelf configuration. For LangDetect,
utilizing bootstrapped models does not always in-
crease the accuracy of LangID of Twitter mes-
sages. Where it does help, the bootstrap collec-
tions that are effective vary with the target dataset.
For langid.py, none of the bootstrapped mod-
els outperformed the off-the-shelf model. This
suggests that for LangID, the same features that
are predictive of language in other domains are
equally applicable to Twitter messages, and that
the cross-domain feature selection procedure pro-
posed utilized by langid.py (Lui and Baldwin,
Dataset Period Proportion
CARTER Jan ? Apr 2010 76.4%
BERGSMA May 2007 ? Feb 2012 92.2%
TWITUSER Jan ? Feb 2012 79.7%
Table 5: Proportion of messages from each dataset
that were still accessible as of August 2013.
2011) is able to identify these features effectively.
Bontcheva et al. (2013) report positive results
from the integration of LangID priors (Carter et
al., 2013), but we did not experiment with them,
as the calculation of priors is relatively expensive
compared to the other adaptations we have con-
sidered, in terms of both run time and developer
effort. Furthermore, there is a number of open is-
sues that are likely to affect the effectiveness of the
priors, such as the size and the scope of the mes-
sage collection used to determine the prior. This
is an interesting avenue of future work but is be-
yond the scope of this particular paper. However,
we observe that priors based on user identity (e.g
the ?Blogger? prior) are likely to be artificially ef-
fective on TWITUSER, because the messages have
been sampled from users that we have identified
as monolingual.
3.2 Twitter API predictions
For CARTER, BERGSMA and TWITUSER, we
have access to the original identifiers for each mes-
sage, which use used to download the messages
via the Twitter API.
5
Table 5 reports the propor-
tion of each dataset that is still accessible as of
August 2013. For the messages that we were able
to recover, the full response from the API now
includes language predictions. We do not report
quantitative results on the accuracy of the Twitter
API predictions as the Twitter API terms of ser-
5
http://dev.twitter.com
23
vice forbid benchmarking (?You will not attempt
... to ... use or access the Twitter API ... for ...
benchmarking or competitive purposes?). Further-
more, any results would be impossible to replicate:
the set of messages that are accessible is likely to
continue to decrease, and the accuracy of Twitter?s
predictions may vary as updates are made to the
API.
Error analysis of the language predictions pro-
vided by the Twitter API shows that at the time
of writing, for the languages supported the accu-
racy of the Twitter API is not substantially better
than the best off-the-shelf language identifiers we
examined in this paper. However, about a quarter
of the languages present in TWITUSER are never
offered as predictions. This has implications for
the precision of LangID in other languages: one
notable example is poor precision in Italian, due
to some Romanian messages being identified as
Italian (no messages are identified as Romanian).
This suggests that caution must be taken in tak-
ing the language predictions offered by the Twit-
ter API as goldstandard. The accuracy of the pre-
dictions is not perfect, and highlights the need for
further research into improving the scope and ac-
curacy of LangID for Twitter messages.
4 Conclusion
In this paper, we presented ZHENJA and TWIT-
USER, two novel datasets of language-labeled
Twitter messages. ZHENJA is constructed us-
ing a conventional manual annotation approach,
whereas TWITUSER is constructed using a novel
mostly-automated method that leverages user
identity. Using these new datasets alongside
three previously-published datasets, we com-
pared 8 off-the-shelf LangID systems over Twit-
ter messages, and found that a simple major-
ity vote across three specific systems (CLD2,
langid.py, LangDetect) consistently out-
performs any individual system. We also found
that removing Twitter-specific content from mes-
sages improves the performance of off-the-shelf
systems. We reported that the predictions provided
by the Twitter API are not better than state-of-the-
art off-the-shelf systems, and that a number of lan-
guages in use on Twitter appear to be unsupported
by the Twitter API, underscoring the need for fur-
ther research to broaden the scope and accuracy of
language identification from Twitter messages.
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program.
References
Timothy Baldwin and Marco Lui. 2010. Language identifi-
cation: The long and the short of the matter. In Proceed-
ings of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL HLT 2010),
pages 229?237, Los Angeles, USA.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKin-
lay, and Li Wang. 2013. How noisy social media text,
how diffrnt social media sources? In Proceedings of the
6th International Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton
Fink, and Theresa Wilson. 2012. Language identifica-
tion for creating language-specific Twitter collections. In
Proceedings the Second Workshop on Language in Social
Media (LSM2012), pages 65?74, Montr?eal, Canada.
Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark A.
Greenwood, Diana Maynard, and Niraj Aswani. 2013.
TwitIE: An open-source information extraction pipeline
for microblog text. In Proceedings of Recent Advances
in Natural Language Processing (RANLP 2013), Hissar,
Buglaria.
Ralf Brown. 2013. Selecting and weighting n-grams to
identify 1100 languages. In Proceedings of the 16th in-
ternational conference on text, speech and dialogue (TSD
2013), Plze?n, Czech Republic.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text. Lan-
guage Resources and Evaluation, pages 1?21.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of the Third
Symposium on Document Analysis and Information Re-
trieval, pages 161?175, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language iden-
tification of search engine queries. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 1066?1074,
Singapore.
Aron Culotta. 2010. Towards detecting influenza epidemics
by analyzing Twitter messages. In Proceedings of the
KDD Workshop on Social Media Analytics.
Marc Darnashek. 1995. Gauging similarity with n-grams:
Language-independent categorization of text. Science,
267:843?848.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Automatic
language identification of written texts. In Proceedings of
the 2004 ACM Symposium on Applied Computing (SAC
2004), pages 1128?1133, Nicosia, Cyprus.
Ted Dunning. 1994. Statistical identification of language.
Technical Report MCCS 940-273, Computing Research
Laboratory, New Mexico State University.
24
Joseph L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76(5):378?
382.
Emmanuel Giguet. 1995. Categorisation according to lan-
guage: A step toward combining linguistic knowledge and
statistical learning. In Proceedings of the 4th Interna-
tional Workshop on Parsing Technologies (IWPT-1995),
Prague, Czech Republic.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael Heil-
man, Dani Yogatama, Jeffrey Flanigan, and Noah A.
Smith. 2011. Part-of-speech tagging for Twitter: An-
notation, features, and experiments. In Proceedings of
the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies (ACL
HLT 2011), pages 42?47, Portland, USA.
Moises Goldszmidt, Marc Najork, and Stelios Paparizos.
2013. Boot-strapping language identifiers for short col-
loquial postings. In Proceedings of the European Confer-
ence on Machine Learning and Principles and Practice of
Knowledge Discovery in Databases (ECMLPKDD 2013),
Prague, Czech Republic.
Gregory Grefenstette. 1995. Comparing two language iden-
tification schemes. In Proceedings of Analisi Statistica dei
Dati Testuali (JADT), pages 263?268, Rome, Italy.
Harald Hammarstrom. 2007. A Fine-Grained Model for
Language Identication. In Proceedings of Improving Non
English Web Searching (iNEWS07), pages 14?20.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lexical
normalization for social media text. ACM Trans. Intell.
Syst. Technol., 4(1):5:1?5:27, February.
Stephen Johnson. 1993. Solving the problem of language
recognition. Technical report, School of Computer Stud-
ies, University of Leeds.
Ben King and Steven Abney. 2013. Labeling the languages
of words in mixed-language documents using weakly su-
pervised methods. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 1110?1119, Atlanta, Georgia.
Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt.
2008. A few chirps about Twitter. In Proceedings of the
First Workshop on Online Social Networks (WOSN 2008),
pages 19?24, Seattle, USA.
Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlert-
lamvanich, and Hitoshi Isahara. 2005. Language identifi-
cation based on string kernels. In Proceedings of the 5th
International Symposium on Communications and Infor-
mation Technologies (ISCIT-2005), pages 896?899, Bei-
jing, China.
Vasileios Lampos, Tijl De Bie, and Nello Cristianini. 2010.
Flu Detector ? tracking epidemics on Twitter. In Pro-
ceedings of the European Conference on Machine Learn-
ing and Principles and Practice of Knowledge Discov-
ery in Databases (ECML PKDD 2010), pages 599?602,
Barcelona, Spain.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Is-
abel Trancoso. 2013. Microblogs as parallel corpora. In
Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers),
pages 176?186, Sofia, Bulgaria.
Marco Lui and Timothy Baldwin. 2011. Cross-domain fea-
ture selection for language identification. In Proceedings
of the 5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2011), pages 553?561, Chiang
Mai, Thailand.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-
the-shelf language identification tool. In Proceedings of
the 50th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2012) Demo Session, pages 25?
30, Jeju, Republic of Korea.
Martin Majli?s. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at the 13th
Conference of the European Chapter of the Association
for Computational Linguistics, pages 46?54, Avignon,
France.
Michael McCandless. 2010. Accuracy and performance
of google?s compact language detector. blog post. avail-
able at http://blog.mikemccandless.com/
2011/10/accuracy-and-performance-of-
googles.html.
Shuyo Nakatani. 2010. Language detection library
(slides). http://www.slideshare.net/shuyo/
language-detection-library-for-java.
Retrieved on 21/06/2013.
Shuyo Nakatani. 2012. Short text language detec-
tion with infinity-gram. blog post. available at
http://shuyo.wordpress.com/2012/05/
17/short-text-language-detection-with-
infinity-gram/.
Dong Nguyen and A. Seza Dogruoz. 2013. Word level
language identification in online multilingual communi-
cation. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing, pages
857?862, Seattle, USA.
S. Petrovi?c, Miles Osborne, and Victor Lavrenko. 2010.
Streaming first story detection with application to twitter.
In Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL
HLT 2010), pages 181?189, Los Angeles, USA.
Arne Roomann-Kurrik. 2013. Introducing new meta-
data fro tweets. blog post. available at https:
//dev.twitter.com/blog/introducing-
new-metadata-for-tweets.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010.
Earthquake shakes Twitter users: real-time event detection
by social sensors. In Proceedings of the 19th International
Conference on the World Wide Web (WWW 2010), pages
851?860, Raleigh, USA.
Frank Scheelen, 2003. libtextcat. Software avail-
able at http://software.wise-guys.nl/
libtextcat/.
Shiladitya Sinha, Chris Dyer, Kevin Gimpel, and Noah A.
Smith. 2013. Predicting the NFL using Twitter. In
Proceedings of the ECML/PKDD Workshop on Machine
Learning and Data Mining for Sports Analytics, Prague,
Czech Republic.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-based
n-gram language identification on short texts. In Proceed-
ings of Benelearn 2011, pages 27?35, The Hague, Nether-
lands.
Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.
2010. Language identification of short text segments
with n-gram models. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evaluation
(LREC 2010), pages 3423?3430.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012. Text
segmentation by language using minimum description
length. In Proceedings the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1: Long
Papers), pages 969?978, Jeju Island, Korea.
25
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 129?138,
Dublin, Ireland, August 23 2014.
Exploring Methods and Resources for Discriminating Similar Languages
Marco Lui
??
, Ned Letcher
?
, Oliver Adams
?
,
Long Duong
??
, Paul Cook
?
and Timothy Baldwin
??
?
Department of Computing and Information Systems
The University of Melbourne
?
NICTA Victoria
mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au,
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici-
pants to build an automatic language identification system to discriminate between 13 languages
in 6 groups of highly-similar languages (or national varieties of the same language). In this
paper, we describe the submissions made by team UniMelb-NLP, which took part in both the
closed and open categories. We present the text representations and modeling techniques used,
including cross-lingual POS tagging as well as fine-grained tags extracted from a deep grammar
of English, and discuss additional data we collected for the open submissions, utilizing custom-
built web corpora based on top-level domains as well as existing corpora.
1 Introduction
Language identification (LangID) is the problem of determining what natural language a document is
written in. Studies in the area often report high accuracy (Cavnar and Trenkle, 1994; Dunning, 1994;
Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accu-
racy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further
work is accurate discrimination between closely-related languages (Ljube?si?c et al., 2007; Tiedemann
and Ljube?si?c, 2012). The problem has been explored for specific groups of confusable languages, such
as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and
Ljube?si?c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre,
2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task
(Zampieri et al., 2014) was hosted at the VarDial workshop at COLING 2014, and brings together the
work on these various language groups by proposing a task on a single dataset containing text from 13
languages in 6 groups, drawn from a variety of news text datasets (Tan et al., 2014).
In this paper, we describe the entries made by team UniMelb NLP to the DSL shared task. We took
part in both the closed and the open categories, submitting to the main component (Groups A-E) as well
as the separate English component (Group F). For our closed submissions, we focused on comparing a
conventional LangID methodology based on individual words and language-indicative letter sequences
(Section 2.1) to a methodology that uses a de-lexicalized representation of language (Section 2.3). For
Groups A-E we use cross-lingual POS-tagger adaptation (Section 2.3.1) to convert the raw text to a
POS stream using a per-group tagger, and use n-grams of POS tags as our de-lexicalized representation.
For English, we also use a de-lexicalized representation based on lexical types extracted from a deep
grammar (Section 2.3.2), which can be thought of as a very fine-grained tagset. For the open submissions,
we constructed new web-based corpora using a standard methodology, targeting per-language top-level
domains (Section 2.4.2). We also compiled additional training data from existing corpora (Section 2.4.1).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
129
2 Overview
Our main focus was to explore novel methods and sources of training data for discriminating similar
languages. In this section, we describe techniques and text representations that we tested, as well as the
external data sources that we used to build language identifiers for this task.
2.1 Language-Indicative Byte Sequences
Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is
robust to variation in languages across different sources of text. The LD feature set can be thought of as
language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly
characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin
(2012) present langid.py,
1
an off-the-shelf LangID system that utilizes the LD feature set. In this
work, we re-train langid.py using the training data provided by the shared task organizers, and use
this as a baseline result representative of the state-of-the-art in LangID.
2.2 Hierarchical LangID
In LangID research to date, systems generally do not take into account any form of structure in the
class space. In this shared task, languages are explicitly grouped into 6 disjoint groups. We make use
of this structure by introducing a two-level LangID model. The first level implements a single group-
level classifier, which takes an input sentence and identifies the language group (A?F) that the sentence
is from. The output of this group-level classifier is used to select a corresponding per-group classifier,
that is trained only on data for languages in the group. This per-group classifier is applied to the input
sentence and the output thereof is the final label for the sentence.
2.3 De-Lexicalized Text Representation for DSL
One of the challenges in a machine learning approach to discriminating similar languages is to learn
differences between languages that are truly representative of the distinction between varieties, rather
than differences that are merely representative of peculiarities of the training data (Kilgarriff, 2001). One
possible confounding factor is the topicality of the training data ? if the data for each variety is drawn
from different datasets, it is possible that a classifier will simply learn the topical differences between
datasets. Diwersy et al. (2014) carried out a study of colligations in French varieties, where the variation
in the grammatical function of noun lemmas was studied across French-language newspapers from six
countries. In their initial analysis the found that the characteristic features of each country included the
name of the country and other country-specific proper nouns, which resulted in near 100% classification
accuracy but do not provide any insight into national varieties from a linguistic perspective.
One strategy that has been proposed to mitigate the effect of such topical differences is the use of
a de-lexicalized text representation (Lui and Cook, 2013). The de-lexicalization is achieved through
the use of a Part-Of-Speech tagger, which labels each word in a sentence according to its word class
(such as Noun, Verb, Adjective etc). De-lexicalized text representations through POS tagging were first
considered for native language identification (NLI), where they were used as a proxy for syntax in order
to capture certain types of grammatical errors (Wong and Dras, 2009). Syntactic structure is known to
vary across national dialects (Trudgill and Hannah, 2008), so Lui and Cook (2013) investigated POS plus
function word n-grams as a proxy for syntactic structure, and used this representation to build classifiers
to discriminate between Canadian, British and American English. They found that classifiers using such a
representation achieved above-baseline results, indicating some systematic differences between varieties
could be captured through the use of such a de-lexicalized representation. In this work, we explore this
idea further ? in particular, we examine (1) the applicability of de-lexicalized text representations to
other languages using automatically-induced crosslingual POS taggers, and (2) the difference in accuracy
for discriminating English varieties between representations based on a coarse-grained universal tagset
(Section 2.3.1) as compared to a very fine-grained tagset used in deep parsing (Section 2.3.2).
1
http://github.com/saffsd/langid.py
130
Sandy quit on Tuesday Sandy quit Tuesday
UT NOUN VERB ADP NOUN NOUN VERB NOUN
LTT n - pn v np
*
p np i-tmp n - c-dow n - pn v np
*
n - c-dow
British English American English
Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical
type tagset (LTT).
2.3.1 Crosslingual POS Tagging
A key issue in generating de-lexicalized text representations based on POS tags is the lack of availability
of POS taggers for many languages. While some languages have some tools available for POS tagging
(e.g. Treaties (Schmid, 1994) has parameter files for Spanish and Portuguese), the availability of POS
taggers is far from universal. To address this problem for the purposes of discriminating similar lan-
guages, we draw on previous work in unsupervised cross-lingual POS tagging (Duong et al., 2013) to
build a POS tagger for each group of languages, a method which we will refer to hereafter as ?UMPOS?.
UMPOS employs a 12-tag Universal Tagset introduced by Petrov et al. (2012), which consists of the
tags NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner or article), ADP
(preposition or postposition), NUM (numeral), CONJ (conjunction), PRT (particle), PUNCT (punctua-
tion), and X (all other categories, e.g., foreign words or abbreviations). These twelve basic tags constitute
a ?universal? tagset in that they can be used to describe the morphosyntax of any language at a coarse
level.
UMPOS generates POS taggers for new languages in an unsupervised fashion, by making use of
parallel data and an existing POS tagger. The input for UMPOS is: (1) parallel data between the source
and target languages; and (2) a supervised POS tagger for the source language. The output will be the
tagger for the target language. The parallel data acts as a bridge to transfer POS annotation information
from the source language to the target language.
The steps used in UMPOS are as follow. First, we collect parallel data which has English as the source
language, drawing from Europarl (Koehn, 2005) and EUbookshop (Skadin??s et al., 2014). UMPOS word-
aligns the parallel data using the Giza++ alignment tool (Och and Ney, 2003). The English side is POS-
tagged using the Stanford POS tagger (Toutanova et al., 2003), and the POS tags are then projected from
English to the target language based solely on one-to-one mappings. Using the sentence alignment score,
UMPOS ranks the ?goodness? of projected sentences and builds a seed model for the target language on
a subset of the parallel data. To further improve accuracy, UMPOS builds the final model by applying
self-training with revision to the rest of the data as follows: (1) the parallel corpus data is divided into
different blocks; (2) the first block is tagged using the seed model; (3) the block is revised based on
alignment confidence; (4) a new tagger is trained on the first block and then used to tag the second block.
This process continues until all blocks are tagged. In experiments on a set of 8 languages, Duong et al.
(2013) report accuracy of 83.4%, which is state-of-the-art for unsupervised POS tagging.
2.3.2 English Tagging Using ERG Lexical Types
Focusing specifically on language Group F ? British English and American English ? we leveraged
linguistic information from the analyses produced by the English Resource Grammar (ERG: Flickinger
(2002)), a broad-coverage, handcrafted grammar of English in the HPSG framework (Pollard and Sag,
1994) and developed within the DELPH-IN
2
research initiative. In particular, we extracted the lexical
types assigned to tokens by the parser for the best analysis of each input string. In accordance with
the heavily lexicalized nature of HPSG, lexical types are the primary means of distinguishing between
different morphosyntactic contexts in which a given lexical entry can occur. They can be thought of as
fine-grained POS tags, containing subcategorisation information in addition to part of speech informa-
tion, and semantic information in cases that it directly impacts on morphosyntax. The version of the
ERG we used (the ?1212? release) has almost 1000 lexical types.
Table 1 illustrates an example of the type of syntactic variation that can be captured with the finer-
2
http://www.delph-in.net
131
Group Language Code
Web Corpora Existing Corpora
TLD # words # datasets # words
A Bosnian bs .ba 817383 4 715602
A Croatian hr .hr 43307311 5 1536623
A Serbian sr .rs 1374787 4 1204684
B Indonesian id .id 23812382 3 564824
B Malaysian my .my 2596378 3 535221
C Czech cz .cz 17103140 8 2181486
C Slovakian sk .sk 17253001 8 2308083
D Brazilian Portuguese pt-BR .br 27369673 4 860065
D European Portuguese pt-PT .pt 22620401 8 2860321
E Argentine Spanish es-AR .ar 45913651 2 619500
E Peninsular Spanish es-ES .es 30965338 9 3458462
F British English en-GB .uk 20375047 1 523653
F American English en-US .us 21298230 1 527915
Table 2: Word count of training data used for open submissions.
grained lexical types, that would be missed with the coarse-grained universal tagset. In American En-
glish, both Sandy resigned on Tuesday and Sandy resigned on Tuesday are acceptable whereas British
English does not permit the omission of the preposition before dates. In the coarse-grained tagset, the
American English form results in a sequence VERB : NOUN, which is not particularly interesting as we
expect this to occur in both English varieties, whereas the fine-grained lexical types allow us to capture
the sequence v np
*
ntr : n - c-dow (verb followed by count noun [day of week]), which we expect
to see in American English but not in British English.
Since the ERG models a sharp notion of grammaticality, not all inputs receive an analysis ? whether
due to gaps in the coverage of the grammar or genuinely ungrammatical input. The ERG achieved
a coverage of 86% over the training data across both British English and American English. Sentences
which failed to parse were excluded from use as input into the classifier. However the inability to classify
any sentence which we cannot parse is unsatisfactory. We solved this problem by generating lexical type
features for sentences which failed to parse using the ERG-trained ?ubertagger of Dridan (2013), which
performs both tokenisation and supertagging of lexical types and improves parser efficiency by reducing
ambiguity in the input lattice to the parser.
2.4 External Corpora
The DSL shared task invited two categories of participation: (1) Closed, using only training data provided
by the organizers (Tan et al., 2014); and (2) Open, using any training data available to participants. To
participate in the latter category, we sourced additional training data through: (1) collection of data
relevant to this task from existing text corpora; and (2) automatic construction of web corpora. The
information about the additional training data is shown in Table 2.
2.4.1 Existing Corpora
We collected training data from a number of existing corpora, as shown in Table 3. Many of the cor-
pora that we used are part of OPUS (Tiedemann, 2012), which is a collection of sentence-aligned text
corpora commonly used for research in machine translation. The exceptions are: (1) debian, which
was constructed using translations of message strings from the Debian operating system,
3
; (2) BNC ?
the British National Corpus (Burnard, 2000); (3) OANC ? the open component of the Second Release
of the American National Corpus (Ide and Macleod, 2001), and (4) Reuters Corpus Volume 2 (RCV2);
4
a corpus of news stories by local reporters in 13 languages. We sampled approximately 19000 sentences
from each of the BNC and OANC, which we used as training data to generate ERG lextype features (Sec-
tion 2.3.2) for British English (en-GB) and American English (en-US), respectively. From RCV2 we
3
http://www.debian.org
4
http://trec.nist.gov/data/reuters/reuters.html
132
bs hr sr pt-PT pt-BR id my cz sk es-ES es-AR en-US en-GB
BNC X
debian X X X X X X X X X X X
ECB X X X X
EMEA X X X X
EUconst X X X X
Europarl X X X X
hrenWaC X
KDE4 X X X X X X X X X
KDEdoc X X X X
OANC X
OpenSubtitles X X X X X X X X X X
RCV2 X X
SETIMES2 X X X
Tatoeba X X
Table 3: Training data compiled from existing corpora.
used the Latin American Spanish news stories as a proxy for Argentine Spanish (es-AR). Note that, for
a given text source, we didn?t necessarily use data for all available languages. For example, debian
contains British English and American English translations, which we did not use.
2.4.2 Web Corpus Construction
Each existing corpus we describe in Section 2.4.1 provides incomplete coverage over the set of languages
in the shared task dataset. In order to have a resource that covers all the languages in the shared task
drawn from a single source, we constructed web corpora for each language. Our approach was strongly
inspired by the approach used to create ukWaC (Ferraresi et al., 2008), and the creation of each sub-
language?s corpus involved crawling the top level domains of the primary countries associated with
those sub-languages. Based on the findings of Cook and Hirst (2012), the assumption underlying this
approach is that text found in the top-level domains (TLDs) of those countries will primarily be of
the sub-language dominant in that country. For instance, we assume that Portuguese text found when
crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will
be primarily Brazilian Portuguese.
The process of creating a corpus for each sub-language involved translating a sample of 200 of the
original ukWaC queries into each language using Panlex (Baldwin et al., 2010).
5
These queries were then
submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining
results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended
them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a
Heritrix 3.1.1
6
instance with default settings other than constraining the crawled content to the relevant
TLD.
Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach,
only documents with a MIME type of HTML and size between 5k and 200k bytes were used. Jus-
text (Pomik?alek, 2011) was used to extract text from the selected documents. langid.py (Lui and
Baldwin, 2012) was then used to discard documents whose text was not in the relevant language or lan-
guage group. The corpus was then refined through deduplication. First, near-deduplication was done at
the paragraph level using Onion (Pomik?alek, 2011) with its default settings. Then, exact-match sentence-
level deduplication, ignoring whitespace and case, was applied.
3 Results and Discussion
Table 4 summarizes the runs submitted by team UniMelb NLP to the VarDial DSL shared task. We
submitted the maximum number of runs allowed, i.e. 3 closed runs and 3 open runs, to both the ?general?
Groups A?E subtask as well as the English-specific Group F subtask. We applied different methods to
Group F, as some of the tools (the ERG) and resources (BNC/OANC) were specific to English. For clarity
in discussion, we have labeled each of our runs according to a 3-letter code: the first letter indicates the
5
A sample of the queries was used because of time and resource limitations.
6
https://webarchive.jira.com/wiki/display/Heritrix
133
Run Description
Macro-avg F-Score
dev tst
Grp A-E closed
AC1 langid.py 13-way 0.822 0.817
AC2 langid.py per-group 0.923 0.918
AC3 POS features 0.683 0.671
Grp F closed
FC1 Lextype features 0.559 0.415
FC2 langid.py per-group 0.548 0.403
FC3 POS features 0.545 0.435
Grp A-E open
AO1 Ext Corpora (word-level model) 0.705 0.703
AO2 Web Corpora (word-level model) 0.771 0.767
AO3 5-way voting 0.881 0.878
Grp F open
FO1 Lextype features using BNC/OANC training data 0.491 0.572
FO2 Web Corpora (word-level model) 0.490 0.581
FO3 5-way voting 0.574 0.442
Table 4: Summary of the official runs submitted by UniMelbNLP. ?dev? indicates scores from our
internal testing on the development partition of the dataset.
subtask (A for Groups A?E, F for Group F), the second indicates Closed (?C?) or Open (?O?), and the
final digit indicates the run number.
AC1 represents a benchmark result based on the LangID system (Lui and Baldwin, 2012). We used
the training tools provided with langid.py to generate a new model using the training data provided
by the shared task organizers, noting that as only data from a single source is used, we are not able to
fully exploit the cross-domain feature selection (Lui and Baldwin, 2011) implemented by langid.py.
The macro-averaged F-score across groups is substantially lower than that on standard LangID datasets
(Lui and Baldwin, 2012).
AC2 and FC2 are a straightforward implementation of hierarchical LangID (Section 2.2), using
mostly-default settings of langid.py. A 6-way group-level classifier is trained, and well as 6 different
per-group classifiers. We increase the number of features selected per class (i.e. group or language) to
500 from the default of 300, to compensate for the smaller number of classes (langid.py off-the-
shelf supports 97 languages). In our internal testing on the provided development data, the group-level
classifier achieved 100% accuracy in classifying sentences at the group level, essentially reducing the
problem to within-group disambiguation. Despite being one of the simplest approaches, overall this
was our best-performing submission for Groups A?E. It also represents a substantial improvement on
AC1, further emphasizing the need to implement hierarchical LangID in order to attain high accuracy in
discriminating similar languages.
AC3 and FC3 are based solely on POS-tag sequences generated by UMPOS, and implement a hierar-
chical LangID approach similar to AC2/FC2. Each sentence in the training data is mapped to a POS-tag
sequence in the 12-tag universal tagset, using the per-group POS tagger for the language group. Each tag
was represented using a single character, allowing us to make use of langid.py to train 6 per-group
classifiers based on n-grams of POS-tags. We used n-grams of order 1?6, and selected 5000 top-ranked
sequences per-language. To classify test data, the same group-level classifier used in AC2 was used to
map sentences to language groups, and then the per-group POS tagger was applied to derive the corre-
sponding stream of POS tags for each sentence. The corresponding per-group classifier trained on POS
tag sequences was then applied to produce the final label for the sentence. For Groups A?E, we find that
134
bs hr sr id my cz sk
T 53.0 23.2 60.0 VHN 0.9 1.3 .1.1 1.0 1.2
TV 32.4 13.2 43.3 DHN 0.1 0.1 1.1. 2.0 2.2
NT 31.5 13.9 43.2 N.1. 12.1 3.1 1.1 4.0 4.4
TVN 24.8 9.8 34.3 N.N 63.3 48.0 .N.1 0.5 0.7
VT 19.4 6.1 27.1 .DNV 1.8 1.1 .C 39.0 33.5
TN 29.1 10.9 29.6 DH 1.7 2.1 .1.. 0.7 1.0
NTV 18.6 8.4 29.4 N.DN 3.2 2.0 .P 51.2 41.8
TVNN 16.8 6.9 23.7 VH 11.3 14.9 1. 14.0 13.9
NVT 11.2 2.9 15.5 PNV1 0.5 0.4 1.. 1.2 1.6
VTV 11.0 3.2 17.0 .1. 13.2 3.8 .R 44.0 30.0
pt-BR pt-PT es-AR es-ES en-GB en-US
X 3.4 2.8 .. 22.6 43.3 NNN 48.2 43.2
N.NN 22.2 15.3 N.. 16.4 31.7 HV 41.5 46.4
.NN 29.9 22.9 .P 52.2 68.3 NN 86.3 83.0
XN 0.4 0.4 P. 6.6 16.8 H 61.8 65.9
NNNN 6.2 3.2 D. 4.4 12.6 R 61.5 65.5
D 99.2 99.5 ..$ 0.0 0.0 RR 7.2 9.4
NNN 28.3 18.6 J.. 5.0 12.6 NNNN 21.7 18.5
.NNN 6.7 4.0 ..VV 0.9 5.2 .C 15.8 18.8
N.D 58.6 47.8 DN.. 4.2 11.0 ... 0.8 0.3
NX 0.8 0.5 .PD 24.5 36.3 N.C 11.3 13.6
Table 5: Top 10 POS features per-group by Information Gain, along with percentage of sentences in each
language in which the feature appears. The notation used is as follows: . = punctuation, J = adjective,
P = pronoun, R = adverb, C = conjunction, D = determiner/article, N = noun, 1 = numeral, H = pronoun,
T = particle, V = verb, and X = others
the POS-tag sequence features are not as effective as the character n-grams used in AC2. Nonetheless,
the results attained are above baseline, indicating that there are systematic differences between languages
in each group that can be captured by an unsupervised approach to POS-tagging using a coarse-grained
tagset. This extends the similar observation made by Lui and Cook (2013) on varieties of English, show-
ing that the same is true for the other language groups in this shared task. Also of interest is the higher
accuracy attained by the POS-tag features on Groups A?E (i.e. AC3) than on English (Group F, FC3).
The top-10 sequences per-group are presented in Table 5, where it can be seen that the sequences are
often slightly more common in one language in the group than the other language(s). One limitation of
the Information Gain based feature selection used in langid.py is that each feature is scored inde-
pendently, and each language receives a binarized score. This can be seen in the features selected for
Group A, where all the top-10 features selected involve particles (labelled T). Overall, this indicates that
Croatian (hr) appears to use particles much less frequently than Serbian (sr) or Bosnian (bs), which is
an intriguing finding. However, most of the top-10 features are redundant in that they all convey very
similar information.
Similar to FC3, a hierarchical LangID approach is used in FC1, in conjunction with per-group classi-
fiers based on a sequence of tags derived from the original sentence. The difference between the taggers
used for FC3 and FC1 is that the FC3 tagger utilizes the 12-tag universal tagset, whereas the FC1 tagger
uses the English-specific lexical types from the ERG (Section 2.3.2), a set of approximately 1000 tags.
There is hence a trade-off to be made between the degree of distinction between tags, and the relative
sparsity of the data ? having a larger tagset means that any given sequence of tags is proportionally less
likely to occur. On the basis of the results of FC1 and FC3 on the dev data, the lexical type features
marginally outperform the coarse-grained universal tagset. However, this result is made harder to inter-
pret by the mismatch between the dev and tst partitions of the shared task dataset. We will discuss
this issue in more detail below, in the context of examining the results on Group F for the open category.
In the open category, we focused primarily on the effect of using different sources of training data.
AO1 and AO2 both implement a hierarchical LangID approach, again using the group-level classifier
from AC2. For the per-group classifiers, runs AO1 and AO2 use a naive Bayes model on a word-level
representation, with feature selection by Information Gain. The difference between the two is that A01
uses samples from existing text corpora (Section 2.4.1), whereas A02 uses web corpora that we prepared
specifically for this shared task (Section 2.4.2). In terms of accuracy, both types of corpora perform
135
substantially better than baseline, indicating that at the word level, there are differences between the
language varieties that are consistent across the different corpus types. This result is complementary to
Cook and Hirst (2012), who found that web corpora from specific top-level domains were representative
of national varieties of English. AO2 (web corpora) outperforms AO1 (existing corpora), further high-
lighting the relevance of web corpora as a source of training data for discriminating similar languages.
However, our models trained on external data were not able to outperform the models trained on the
official training data for Groups A?E. A03 consists of a 5-way majority vote between results AC1, AC2,
AC3, AO1 and AO2. Including the predictions from the closed submissions substantially improves the
result with respect to AO1/AO2, but overall our best result for Groups A?E was obtained by run AC2.
For Group F, FO1 utilizes ERG lexical type features in the same manner as FC1, the difference being
that FC1 uses the shared task trn partition, whereas FO1 uses sentences sampled from existing corpora,
specifically BNC for en-GB and OANC for en-US. FO2 implements the same concept as AO2, namely a
word-level naive Bayes model trained using web corpora. For the Group F (i.e. English) subtask, this was
our best-performing submission overall. FO3 is a 5-way vote between FC1, FC2, FC3, FO1 and FO2,
similar to AO3. Notably, our Group F submissions based on the supplied training data all performed
substantially better on the dev partition of the shared task dataset than on the tst partition. The inverse
is true for our submissions based on external corpora, where all our entries performed substantially better
on the tst partition than on the dev partition. Furthermore, the differences are fairly large, particularly
since Group F is a binary classification task with a 50% baseline. This implies that, at least under our
models, the en-GB portion of the trn partition is a better model of the en-US portion of the tst partition
than the en-GB portion thereof. This is likely due to the manual intervention that was only carried out
on the test portion of the dataset (Zampieri et al., 2014).
Our Group F results appear to be inferior to previous work on discriminating English varieties (Lui
and Cook, 2013). However, there are a number of differences that make it difficult to compare the
results: Lui and Cook (2013) studied differences between Australian, British and Canadian English,
whereas the shared task focused on differences between British and American English. Lui and Cook
(2013) also draw on training data from a variety of domains (national corpora, web corpora and Twitter
messages), whereas the shared task used a dataset collected from newspaper texts (Tan et al., 2014).
Consistent with Cook and Hirst (2012) and Lui and Cook (2013), we found that web corpora appear to be
representative of national varieties, and consistent with Lui and Cook (2013) we found that de-lexicalized
representations of text are able to provide better than baseline discrimination between national varieties.
Overall, these results highlight the need for further research into discriminating between varieties of
English.
4 Conclusion
Discriminating between similar languages is an interesting sub-problem in language identification, and
the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater
detail. Our most successful methods implement straightforward hierarchical LangID, firstly identifying
the language group that a sentence belongs to, before identifying the specific language. We examined a
number of text representations for the per-group language identifiers, including a standard representation
for language identification based on language-indicative byte sequences, as well as with de-lexicalized
text representations. We found that the performance of de-lexicalized representations was above baseline,
however we were not able to fully investigate approaches to integrating predictions from lexicalized
and de-lexicalized text representations due to time constraints. We also found that when using external
corpora, web corpora constructed by scraping per-country top-level domains performed as well as (if
not better than) data collected from existing text corpora, supporting the hypothesis that web corpora
are representative of national varieties of respective languages. Overall, our best result was obtained by
applying two-level hierarchical LangID, firstly identifying the language group that a sentence belongs
to, and then disambiguating within each group. Our best result was achieved by applying an existing
LangID method (Lui and Baldwin, 2012) to both the group-level and the per-group classification tasks.
136
Acknowledgments
The authors wish to thank Li Wang, Rebecca Dridan and Bahar Salehi for their kind assistance with
this research. NICTA is funded by the Australian Government as represented by the Department of
Broadband, Communications and the Digital Economy and the Australian Research Council through the
ICT Centre of Excellence program.
References
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL HLT 2010), pages 229?237, Los Angeles, USA.
Timothy Baldwin, Jonathan Pool, and Susan M Colowick. 2010. Panlex and lextract: Translating all words of
all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics:
Demonstrations, pages 37?40, Beijing, China.
Marco Baroni and Silvia Bernardini. 2004. BootCaT: Bootstrapping corpora and terms from the Web. In Pro-
ceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004).
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford University
Computing Services.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the Third
Symposium on Document Analysis and Information Retrieval, pages 161?175, Las Vegas, USA.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties of
English? In Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages
281?293, Li`ege, Belgium.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A weakly supervised multivariate approach to the study
of language variation. In Benedikt Szmrecsanyi and Bernhard W?alchli, editors, Aggregating Dialectology,
Typology, and Register Analysis. Linguistic Variation in Text and Speech. De Gruyter, Berlin.
Rebecca Dridan. 2013. Ubertagging. Joint segmentation and supertagging for English. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1201?1212, Seattle, USA.
Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS 940-273, Computing Research
Laboratory, New Mexico State University.
Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013. Simpler unsupervised POS tagging with bilin-
gual projections. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 634?639.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Morocco.
Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering. CSLI Publi-
cations, Stanford, USA.
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of Analisi Statistica
dei Dati Testuali (JADT), pages 263?268, Rome, Italy.
Nancy Ide and Catherine Macleod. 2001. The American National Corpus: A standardized resource of American
English. In Proceedings of Corpus Linguistics 2001, pages 274?280, Lancaster, UK.
Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):97?133.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth
Machine Translation Summit (MT Summit X), pages 79?86, Phuket, Thailand.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identification : how to distinguish similar
languages ? In 29th International Conference on Information Technology Interfaces, pages 541?546.
Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceed-
ings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 553?561,
Chiang Mai, Thailand.
137
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages
25?30, Jeju, Republic of Korea.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013, pages 5?15, Brisbane, Australia.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.
European Language Resources Association (ELRA).
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Jan Pomik?alek. 2011. Removing Boilerplate and Duplicate Content from Web Corpora. Ph.D. thesis, Masaryk
University.
John M. Prager. 1999. Linguini: language identification for multilingual documents. In Proceedings of the 32nd
Annual Hawaii International Conference on Systems Sciences (HICSS-32), Maui, Hawaii.
Bali Ranaivo-Malancon. 2006. Automatic Identification of Close Languages - Case study : Malay and Indonesian.
ECTI Transaction on Computer and Information Technology, 2(2):126?134.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Conference
on New Methods in Natural Language Processing, Manchester, 1994.
Raivis Skadin??s, J?org Tiedemann, Roberts Rozis, and Daiga Deksne. 2014. Billions of parallel words for free:
Building and using the EU Bookshop corpus. In Proceedings of the 9th International Conference on Language
Resources and Evaluation (LREC-2014), Reykjavik, Iceland.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
W. J. Teahan. 2000. Text Classification and Segmentation Using Minimum Cross-Entropy. In Proceedings the 6th
International Conference ?Recherche dInformation Assistee par Ordinateur? (RIAO00), pages 943?961, Paris,
France.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 2619?
2634, Mumbai, India.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th International
Conference on Language Resources and Evaluation (LREC 2012), pages 2214?2218, Istanbul, Turkey.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL
?03), pages 173?180, Edmonton, Canada.
Peter Trudgill and Jean Hannah. 2008. International English: A guide to varieties of Standard English. Hodder
Education, London, UK, 5th edition.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proceed-
ings of the Australasian Language Technology Workshop 2009 (ALTW 2009), pages 53?61, Sydney, Australia.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS 2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN 2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and J?org Tiedemann. 2014. A report on the DSL shared task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial), Dublin, Ireland.
138
