Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 35?40,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Natural Language Models for Predicting Programming Comments
Dana Movshovitz-Attias
Computer Science Department
Carnegie Mellon University
dma@cs.cmu.edu
William W. Cohen
Computer Science Department
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
Statistical language models have success-
fully been used to describe and analyze
natural language documents. Recent work
applying language models to program-
ming languages is focused on the task
of predicting code, while mainly ignoring
the prediction of programmer comments.
In this work, we predict comments from
JAVA source files of open source projects,
using topic models and n-grams, and we
analyze the performance of the models
given varying amounts of background data
on the project being predicted. We evalu-
ate models on their comment-completion
capability in a setting similar to code-
completion tools built into standard code
editors, and show that using a comment
completion tool can save up to 47% of the
comment typing.
1 Introduction and Related Work
Statistical language models have traditionally
been used to describe and analyze natural lan-
guage documents. Recently, software engineer-
ing researchers have adopted the use of language
models for modeling software code. Hindle et al
(2012) observe that, as code is created by humans
it is likely to be repetitive and predictable, similar
to natural language. NLP models have thus been
used for a variety of software development tasks
such as code token completion (Han et al, 2009;
Jacob and Tairas, 2010), analysis of names in code
(Lawrie et al, 2006; Binkley et al, 2011) and min-
ing software repositories (Gabel and Su, 2008).
An important part of software programming and
maintenance lies in documentation, which may
come in the form of tutorials describing the code,
or inline comments provided by the programmer.
The documentation provides a high level descrip-
tion of the task performed by the code, and may
include examples of use-cases for specific code
segments or identifiers such as classes, methods
and variables. Well documented code is easier to
read and maintain in the long-run but writing com-
ments is a laborious task that is often overlooked
or at least postponed by many programmers.
Code commenting not only provides a summa-
rization of the conceptual idea behind the code
(Sridhara et al, 2010), but can also be viewed as a
form of document expansion where the comment
contains significant terms relevant to the described
code. Accurately predicted comment words can
therefore be used for a variety of linguistic uses
including improved search over code bases using
natural language queries, code categorization, and
locating parts of the code that are relevant to a spe-
cific topic or idea (Tseng and Juang, 2003; Wan et
al., 2007; Kumar and Carterette, 2013; Shepherd
et al, 2007; Rastkar et al, 2011). A related and
well studied NLP task is that of predicting natural
language caption and commentary for images and
videos (Blei and Jordan, 2003; Feng and Lapata,
2010; Feng and Lapata, 2013; Wu and Li, 2011).
In this work, our goal is to apply statistical lan-
guage models for predicting class comments. We
show that n-gram models are extremely success-
ful in this task, and can lead to a saving of up
to 47% in comment typing. This is expected as
n-grams have been shown as a strong model for
language and speech prediction that is hard to im-
prove upon (Rosenfeld, 2000). In some cases how-
ever, for example in a document expansion task,
we wish to extract important terms relevant to the
code regardless of local syntactic dependencies.
We hence also evaluate the use of LDA (Blei et al,
2003) and link-LDA (Erosheva et al, 2004) topic
models, which are more relevant for the term ex-
traction scenario. We find that the topic model per-
formance can be improved by distinguishing code
and text tokens in the code.
35
2 Method
2.1 Models
We train n-gram models (n = 1, 2, 3) over source
code documents containing sequences of com-
bined code and text tokens from multiple training
datasets (described below). We use the Berkeley
Language Model package (Pauls and Klein, 2011)
with absolute discounting (Kneser-Ney smooth-
ing; (1995)) which includes a backoff strategy to
lower-order n-grams. Next, we use LDA topic
models (Blei et al, 2003) trained on the same data,
with 1, 5, 10 and 20 topics. The joint distribution
of a topic mixture ?, and a set of N topics z, for
a single source code document with N observed
word tokens, d = {wi}Ni=1, given the Dirichlet pa-
rameters ? and ?, is therefore
p(?, z, w|?, ?) = (1)
p(?|?)
?
w
p(z|?)p(w|z, ?)
Under the models described so far, there is no dis-
tinction between text and code tokens.
Finally, we consider documents as having a
mixed membership of two entity types, code and
text tokens, d = ({wcodei }Cni=1, {wtexti }Tni=1), where
the text words are tokens from comment and
string literals, and the code words include the pro-
gramming language syntax tokens (e.g., public,
private, for, etc? ) and all identifiers. In this
case, we train link-LDA models (Erosheva et al,
2004) with 1, 5, 10 and 20 topics. Under the link-
LDA model, the mixed-membership joint distribu-
tion of a topic mixture, words and topics is then
p(?, z, w|?, ?) = p(?|?)? (2)
?
wtext
p(ztext|?)p(wtext|ztext, ?)?
?
wcode
p(zcode|?)p(wcode|zcode, ?)
where ? is the joint topic distribution, w is the set
of observed document words, ztext is a topic asso-
ciated with a text word, and zcode a topic associ-
ated with a code word.
The LDA and link-LDA models use Gibbs sam-
pling (Griffiths and Steyvers, 2004) for topic infer-
ence, based on the implementation of Balasubra-
manyan and Cohen (2011) with single or multiple
entities per document, respectively.
2.2 Testing Methodology
Our goal is to predict the tokens of the JAVA class
comment (the one preceding the class definition)
in each of the test files. Each of the models de-
scribed above assigns a probability to the next
comment token. In the case of n-grams, the prob-
ability of a token word wi is given by considering
previous words p(wi|wi?1, . . . , w0). This proba-
bility is estimated given the previous n? 1 tokens
as p(wi|wi?1, . . . , wi?(n?1)).
For the topic models, we separate the docu-
ment tokens into the class definition and the com-
ment we wish to predict. The set of tokens of
the class comment wc, are all considered as text
tokens. The rest of the tokens in the document
wr, are considered to be the class definition, and
they may contain both code and text tokens (from
string literals and other comments in the source
file). We then compute the posterior probability
of document topics by solving the following infer-
ence problem conditioned on the wr tokens
p(?, zr|wr, ?, ?) = p(?, z
r, wr|?, ?)
p(wr|?, ?) (3)
This gives us an estimate of the document distri-
bution, ?, with which we infer the probability of
the comment tokens as
p(wc|?, ?) =
?
z
p(wc|z, ?)p(z|?) (4)
Following Blei et al (2003), for the case
of a single entity LDA, the inference problem
from equation (3) can be solved by considering
p(?, z, w|?, ?), as in equation (1), and by taking
the marginal distribution of the document tokens
as a continuous mixture distribution for the set
w = wr, by integrating over ? and summing over
the set of topics z
p(w|?, ?) =
?
p(?|?)? (5)
(?
w
?
z
p(z|?)p(w|z, ?)
)
d?
For the case of link-LDA where the document is
comprised of two entities, in our case code to-
kens and text tokens, we can consider the mixed-
membership joint distribution ?, as in equation (2),
and similarly the marginal distribution p(w|?, ?)
over both code and text tokens from wr. Since
comment words in wc are all considered as text
tokens they are sampled using text topics, namely
ztext, in equation (4).
36
3 Experimental Settings
3.1 Data and Training Methodology
We use source code from nine open source JAVA
projects: Ant, Cassandra, Log4j, Maven, Minor-
Third, Batik, Lucene, Xalan and Xerces. For each
project, we divide the source files into a training
and testing dataset. Then, for each project in turn,
we consider the following three main training sce-
narios, leading to using three training datasets.
To emulate a scenario in which we are predict-
ing comments in the middle of project develop-
ment, we can use data (documented code) from the
same project. In this case, we use the in-project
training dataset (IN). Alternatively, if we train a
comment prediction model at the beginning of the
development, we need to use source files from
other, possibly related projects. To analyze this
scenario, for each of the projects above we train
models using an out-of-project dataset (OUT) con-
taining data from the other eight projects.
Typically, source code files contain a greater
amount of code versus comment text. Since we are
interested in predicting comments, we consider a
third training data source which contains more En-
glish text as well as some code segments. We use
data from the popular Q&A website StackOver-
flow (SO) where users ask and answer technical
questions about software development, tools, al-
gorithms, etc?. We downloaded a dataset of all ac-
tions performed on the site since it was launched in
August 2008 until August 2012. The data includes
3,453,742 questions and 6,858,133 answers posted
by 1,295,620 users. We used only posts that are
tagged as JAVA related questions and answers.
All the models for each project are then tested
on the testing set of that project. We report results
averaged over all projects in Table 1.
Source files were tokenized using the Eclipse
JDT compiler tools, separating code tokens and
identifiers. Identifier names (of classes, methods
and variables), were further tokenized by camel
case notation (e.g., ?minMargin? was converted to
?min margin?). Non alpha-numeric tokens (e.g.,
dot, semicolon) were discarded from the code, as
well as numeric and single character literals. Text
from comments or any string literals within the
code were further tokenized with the Mallet sta-
tistical natural language processing package (Mc-
Callum, 2002). Posts from SO were parsed using
the Apache Tika toolkit1 and then tokenized with
the Mallet package. We considered as raw code
tokens anything labeled using a <code> markup
(as indicated by the SO users who wrote the post).
3.2 Evaluation
Since our models are trained using various data
sources the vocabularies used by each of them are
different, making the comment likelihood given by
each model incomparable due to different sets of
out-of-vocabulary tokens. We thus evaluate mod-
els using a character saving metric which aims at
quantifying the percentage of characters that can
be saved by using the model in a word-completion
settings, similar to standard code completion tools
built into code editors. For a comment word with
n characters, w = w1, . . . , wn, we predict the two
most likely words given each model filtered by the
first 0, . . . , n characters ofw. Let k be the minimal
ki for which w is in the top two predicted word to-
kens where tokens are filtered by the first ki char-
acters. Then, the number of saved characters for w
is n? k. In Table 1 we report the average percent-
age of saved characters per comment using each of
the above models. The final results are also aver-
aged over the nine input projects. As an example,
in the predicted comment shown in Table 2, taken
from the project Minor-Third, the token entity is
the most likely token according to the model SO
trigram, out of tokens starting with the prefix ?en?.
The saved characters in this case are ?tity?.
4 Results
Table 1 displays the average percentage of char-
acters saved per class comment using each of the
models. Models trained on in-project data (IN)
perform significantly better than those trained on
another data source, regardless of the model type,
with an average saving of 47.1% characters using
a trigram model. This is expected, as files from
the same project are likely to contain similar com-
ments, and identifier names that appear in the com-
ment of one class may appear in the code of an-
other class in the same project. Clearly, in-project
data should be used when available as it improves
comment prediction leading to an average increase
of between 6% for the worst model (26.6 for OUT
unigram versus 33.05 for IN) and 14% for the best
(32.96 for OUT trigram versus 47.1 for IN).
1http://tika.apache.org/
37
Model n-gram LDA Link-LDA
n / topics 1 2 3 20 10 5 1 20 10 5 1
IN 33.05 43.27 47.1 34.20 33.93 33.63 33.05 35.76 35.81 35.37 34.59
(3.62) (5.79) (6.87) (3.63) (3.67) (3.67) (3.62) (3.95) (4.12) (3.98) (3.92)
OUT 26.6 31.52 32.96 26.79 26.8 26.86 26.6 28.03 28 28 27.82
(3.37) (4.17) (4.33) (3.26) (3.36) (3.44) (3.37) (3.60) (3.56) (3.67) (3.62)
SO 27.8 33.29 34.56 27.25 27.22 27.34 27.8 28.08 28.12 27.94 27.9
(3.51) (4.40) (4.78) (3.67) (3.44) (3.55) (3.51) (3.48) (3.58) (3.56) (3.45)
Table 1: Average percentage of characters saved per comment using n-gram, LDA and link-LDA models
trained on three training sets: IN, OUT, and SO. The results are averaged over nine JAVA projects (with
standard deviations in parenthesis).
Model Predicted Comment
IN trigram ?Train a named-entity extractor?
IN link-LDA ?Train a named-entity extractor?
OUT trigram ?Train a named-entity extractor?
SO trigram ?Train a named-entity extractor?
Table 2: Sample comment from the Minor-Third
project predicted using IN, OUT and SO based
models. Saved characters are underlined.
Of the out-of-project data sources, models us-
ing a greater amount of text (SO) mostly out-
performed models based on more code (OUT).
This increase in performance, however, comes at
a cost of greater run-time due to the larger word
dictionary associated with the SO data. Note that
in the scope of this work we did not investigate the
contribution of each of the background projects
used in OUT, and how their relevance to the tar-
get prediction project effects their performance.
The trigram model shows the best performance
across all training data sources (47% for IN, 32%
for OUT and 34% for SO). Amongst the tested
topic models, link-LDA models which distinguish
code and text tokens perform consistently better
than simple LDA models in which all tokens are
considered as text. We did not however find a
correlation between the number of latent topics
learned by a topic model and its performance. In
fact, for each of the data sources, a different num-
ber of topics gave the optimal character saving re-
sults.
Note that in this work, all topic models are
based on unigram tokens, therefore their results
are most comparable with that of the unigram in
Dataset n-gram link-LDA
IN 2778.35 574.34
OUT 1865.67 670.34
SO 1898.43 638.55
Table 3: Average words per project for which each
tested model completes the word better than the
other. This indicates that each of the models is bet-
ter at predicting a different set of comment words.
Table 1, which does not benefit from the back-
off strategy used by the bigram and trigram mod-
els. By this comparison, the link-LDA topic model
proves more successful in the comment prediction
task than the simpler models which do not distin-
guish code and text tokens. Using n-grams without
backoff leads to results significantly worse than
any of the presented models (not shown).
Table 2 shows a sample comment segment for
which words were predicted using trigram models
from all training sources and an in-project link-
LDA. The comment is taken from the TrainEx-
tractor class in the Minor-Third project, a ma-
chine learning library for annotating and catego-
rizing text. Both IN models show a clear advan-
tage in completing the project-specific word Train,
compared to models based on out-of-project data
(OUT and SO). Interestingly, in this example the
trigram is better at completing the term named-
entity given the prefix named. However, the topic
model is better at completing the word extractor
which refers to the target class. This example indi-
cates that each model type may be more successful
in predicting different comment words, and that
combining multiple models may be advantageous.
38
This can also be seen by the analysis in Table 3
where we compare the average number of words
completed better by either the best n-gram or topic
model given each training dataset. Again, while
n-grams generally complete more words better, a
considerable portion of the words is better com-
pleted using a topic model, further motivating a
hybrid solution.
5 Conclusions
We analyze the use of language models for pre-
dicting class comments for source file documents
containing a mixture of code and text tokens. Our
experiments demonstrate the effectiveness of us-
ing language models for comment completion,
showing a saving of up to 47% of the comment
characters. When available, using in-project train-
ing data proves significantly more successful than
using out-of-project data. However, we find that
when using out-of-project data, a dataset based on
more words than code performs consistently bet-
ter. The results also show that different models
are better at predicting different comment words,
which motivates a hybrid solution combining the
advantages of multiple models.
Acknowledgments
This research was supported by the NSF under
grant CCF-1247088.
References
Ramnath Balasubramanyan and William W Cohen.
2011. Block-lda: Jointly modeling entity-annotated
text and entity-entity links. In Proceedings of the 7th
SIAM International Conference on Data Mining.
Dave Binkley, Matthew Hearn, and Dawn Lawrie.
2011. Improving identifier informativeness using
part of speech information. In Proc. of the Working
Conference on Mining Software Repositories. ACM.
David M Blei and Michael I Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research
and development in informaion retrieval. ACM.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proc. of the 48th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE transac-
tions on pattern analysis and machine intelligence.
Mark Gabel and Zhendong Su. 2008. Javert: fully au-
tomatic mining of general temporal properties from
dynamic traces. In Proceedings of the 16th ACM
SIGSOFT International Symposium on Foundations
of software engineering, pages 339?349. ACM.
Thomas L Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proc. of the National Academy of
Sciences of the United States of America.
Sangmok Han, David R Wallace, and Robert C Miller.
2009. Code completion from abbreviated input.
In Automated Software Engineering, 2009. ASE?09.
24th IEEE/ACM International Conference on, pages
332?343. IEEE.
Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the naturalness
of software. In Software Engineering (ICSE), 2012
34th International Conference on. IEEE.
Ferosh Jacob and Robert Tairas. 2010. Code template
inference using language models. In Proceedings
of the 48th Annual Southeast Regional Conference.
ACM.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., volume 1, pages 181?184. IEEE.
Naveen Kumar and Benjamin Carterette. 2013. Time
based feedback and query expansion for twitter
search. In Advances in Information Retrieval, pages
734?737. Springer.
Dawn Lawrie, Christopher Morrell, Henry Feild, and
David Binkley. 2006. Whats in a name? a study
of identifiers. In Program Comprehension, 2006.
ICPC 2006. 14th IEEE International Conference on,
pages 3?12. IEEE.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the
49th annual meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, volume 1, pages 258?267.
Sarah Rastkar, Gail C Murphy, and Alexander WJ
Bradley. 2011. Generating natural language sum-
maries for crosscutting source code concerns. In
Software Maintenance (ICSM), 2011 27th IEEE In-
ternational Conference on, pages 103?112. IEEE.
39
Ronald Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here?
Proceedings of the IEEE, 88(8):1270?1278.
David Shepherd, Zachary P Fry, Emily Hill, Lori Pol-
lock, and K Vijay-Shanker. 2007. Using natu-
ral language program analysis to locate and under-
stand action-oriented concerns. In Proceedings of
the 6th international conference on Aspect-oriented
software development, pages 212?224. ACM.
Giriprasad Sridhara, Emily Hill, Divya Muppaneni,
Lori Pollock, and K Vijay-Shanker. 2010. To-
wards automatically generating summary comments
for java methods. In Proceedings of the IEEE/ACM
international conference on Automated software en-
gineering, pages 43?52. ACM.
Yuen-Hsien Tseng and Da-Wei Juang. 2003.
Document-self expansion for text categorization. In
Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in
informaion retrieval, pages 399?400. ACM.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Single document summarization with document ex-
pansion. In Proc. of the National Conference on
Artificial Intelligence. Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999.
Roung-Shiunn Wu and Po-Chun Li. 2011. Video
annotation using hierarchical dirichlet process mix-
ture model. Expert Systems with Applications,
38(4):3040?3048.
40
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 11?19,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Bootstrapping Biomedical Ontologies for Scientific Text using NELL
Dana Movshovitz-Attias
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
dma@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
wcohen@cs.cmu.edu
Abstract
We describe an open information extraction
system for biomedical text based on NELL
(the Never-Ending Language Learner) (Carl-
son et al, 2010), a system designed for ex-
traction from Web text. NELL uses a cou-
pled semi-supervised bootstrapping approach
to learn new facts from text, given an initial
ontology and a small number of ?seeds? for
each ontology category. In contrast to previ-
ous applications of NELL, in our task the ini-
tial ontology and seeds are automatically de-
rived from existing resources. We show that
NELL?s bootstrapping algorithm is suscepti-
ble to ambiguous seeds, which are frequent in
the biomedical domain. Using NELL to ex-
tract facts from biomedical text quickly leads
to semantic drift. To address this problem, we
introduce a method for assessing seed qual-
ity, based on a larger corpus of data derived
from the Web. In our method, seed quality
is assessed at each iteration of the bootstrap-
ping process. Experimental results show sig-
nificant improvements over NELL?s original
bootstrapping algorithm on two types of tasks:
learning terms from biomedical categories,
and named-entity recognition for biomedical
entities using a learned lexicon.
1 Introduction
NELL (the Never-Ending Language Learner) is a
semi-supervised learning system, designed for ex-
traction of information from the Web. The system
uses a coupled semi-supervised bootstrapping app-
roach to learn new facts from text, given an initial
ontology and a small number of ?seeds?, i.e., labeled
examples for each ontology category. The new facts
are stored in a growing structured knowledge base.
One of the concerns about gathering data from the
Web is that it comes from various un-authoritative
sources, and may not be reliable. This is especially
true when gathering scientific information. In con-
trast to Web data, scientific text is potentially more
reliable, as it is guided by the peer-review process.
Open access scientific archives make this informa-
tion available for all. In fact, the production rate of
publicly available scientific data far exceeds the abil-
ity of researchers to ?manually? process it, and there
is a growing need for the automation of this process.
The biomedical field presents a great potential for
text mining applications. An integral part of life sci-
ence research involves production and publication of
large collections of data by curators, and as part of
collaborative community effort. Prominent exam-
ples include: publication of genomic sequence data,
e.g., by the Human Genome Project; online col-
lections of three-dimensional coordinates of protein
structures; and databases holding data on genes. An
important resource, initiated as a means of enforc-
ing data standardization, are ontologies describing
biological, chemical and medical terms. These are
heavily used by the research community. With this
wealth of available data the biomedical field holds
many information extraction opportunities.
We describe an open information extraction sys-
tem adapting NELL to the biomedical domain. We
present an implementation of our approach, named
BioNELL, which uses three main sources of infor-
mation: (1) a public corpus of biomedical scientific
text, (2) commonly used biomedical ontologies, and
11
High PMI Seeds Random Seeds
SoxN achaete cycA cac section 33 28
Pax-6 Drosomycin Zfh-1 crybaby hv Bob
BX-C Ultrabithorax GATAe ael LRS dip
D-Fos sine oculis FMRFa chm sht 3520
Abd-A dCtBP Antp M-2 AGI tou
PKAc huckebein abd-A shanti disp zen
Hmgcr Goosecoid knirps Buffy Gap Scm
fkh decapentaplegic Sxl lac Mercurio REPO
abdA naked cuticle BR-C subcosta mef Ferritin
zfh-1 Kruppel hmgcr Slam dad dTCF
tkv gypsy insulator Dichaete Cbs Helicase mago
CrebA alpha-Adaptin Abd-B Sufu ora Pten
D-raf doublesex gusA pelo vu sb
MtnA FasII AbdA sombre domain II TrpRS
Dcr-2 GAGA factor dTCF TAS CCK ripcord
fushi
tarazu
kanamycin
resistance
Ecdysone
receptor
GABAA
receptor
diazepam
binding
inhibitor
yolk
protein
Tkv dCBP Debcl arm
Table 1: Two samples of fruit-fly genes, taken from the
complete fly gene dictionary. High PMI Seeds are the top
50 terms selected using PMI ranking, and Random Seeds
are a random draw of 50 terms from the dictionary. These
are used as seeds for the Fly Gene category (Section 4.2).
Notice that the random set contains many terms that are
often not used as genes including arm, 28, and dad. Us-
ing these as seeds can lead to semantic drift. In contrast,
high PMI seeds exhibit much less ambiguity.
(3) a corpus of Web documents.
NELL?s ontology, including categories and seeds,
has been manually designed during the system de-
velopment. Ontology design involves assembling a
set of interesting categories, organized in a meaning-
ful hierarchical structure, and providing represen-
tative seeds for each category. Redesigning a new
ontology for a technical domain is difficult without
non-trivial knowledge of the domain. We describe a
process of merging source ontologies into one struc-
ture of categories with seed examples.
However, as we will show, using NELL?s boot-
strapping algorithm to extract facts from a biomed-
ical corpus is susceptible to noisy and ambiguous
terms. Such ambiguities are common in biomedi-
cal terminology (see examples in Table 1), and some
ambiguous terms are heavily used in the literature.
For example, in the sentence ?We have cloned an
induced white mutation and characterized the in-
sertion sequence responsible for the mutant pheno-
type?, white is an ambiguous term referring to the
name of a gene. In NELL, ambiguity is limited us-
ing coupled semi-supervised learning (Carlson et al,
2009): if two categories in the ontology are declared
mutually exclusive, instances of one category are
used as negative examples for the other, and the two
categories cannot share any instances. To resolve
the ambiguity of white with mutual exclusion, we
would have to include a Color category in the ontol-
ogy, and declare it mutually exclusive with the Gene
category. Then, instances of Color will not be able
to refer to genes in the KB. It is hard to estimate what
additional categories should be added, and building
a ?complete? ontology tree is practically infeasible.
NELL also includes a polysemy resolution com-
ponent that acknowledges that one term, for exam-
ple white, may refer to two distinct concepts, say
a color and a gene, that map to different ontology
categories, such as Color and Fly Gene (Krishna-
murthy and Mitchell, 2011). By including a Color
category, this component can identify that white is
both a color and a gene. The polysemy resolver per-
forms word sense induction and synonym resolution
based on relations defined between categories in the
ontology, and labeled synonym examples. However,
at present, BioNELL?s ontology does not contain re-
lation definitions (it is based only on categories),
so we cannot include this component in our exper-
iments. Additionally, it is unclear how to avoid the
use of polysemous terms as category seeds, and no
method has been suggested for selecting seeds that
are representative of a single specific category.
To address the problem of ambiguity, we intro-
duce a method for assessing the desirability of noun
phrases to be used as seeds for a specific target cat-
egory. We propose ranking seeds using a Point-
wise Mutual Information (PMI) -based collocation
measure for a seed and a category name. Colloca-
tion is measured based on a large corpus of domain-
independent data derived from the Web, accounting
for uses of the seed in many different contexts.
NELL?s bootstrapping algorithm uses the mor-
phological and semantic features of seeds to pro-
pose new facts, which are added to the KB and used
as seeds in the next bootstrapping iteration to learn
more facts. This means that ambiguous terms may
be added at any learning iteration. Since white really
is a name of a gene, it is sometimes used in the same
semantic context as other genes, and may be added
to the KB despite not being used as an initial seed.
12
To resolve this problem, we propose measuring seed
quality in a Rank-and-Learn bootstrapping method-
ology: after every iteration, we rank all the instances
that have been added to the KB by their quality
as potential category seeds. Only high-ranking in-
stances are used as seeds in the next iteration. Low-
ranking instances are stored in the KB and ?remem-
bered? as true facts, but are not used for learning
new information. This is in contrast to NELL?s ap-
proach (and most other bootstrapping systems), in
which there is no distinction between acquired facts,
and facts that are used for learning.
2 Related Work
Biomedical Information Extraction systems have
traditionally targeted recognition of few distinct bi-
ological entities, focusing mainly on genes (e.g.,
(Chang et al, 2004)). Few systems have been devel-
oped for fact-extraction of many biomedical predi-
cates, and these are relatively small scale (Wattaru-
jeekrit et al, 2004), or they account for limited sub-
domains (Dolbey et al, 2006). We suggest a more
general approach, using bootstrapping to extend ex-
isting biomedical ontologies, including a wide range
of sub-domains and many categories. The current
implementation of BioNELL includes an ontology
with over 100 categories. To the best of our knowl-
edge, such large-scale biomedical bootstrapping has
not been done before.
Bootstrap Learning and Semantic Drift. Carl-
son et al (2010) use coupled semi-supervised boot-
strap learning in NELL to learn a large set of cate-
gory classifiers with high precision. One drawback
of using iterative bootstrapping is the sensitivity of
this method to the set of initial seeds (Pantel et al,
2009). An ambiguous set of seeds can lead to se-
mantic drift, i.e., accumulation of erroneous terms
and contexts when learning a semantic class. Strict
bootstrapping environments reduce this problem by
adding boundaries or limiting the learning process,
including learning mutual terms and contexts (Riloff
and Jones, 1999) and using mutual exclusion and
negative class examples (Curran et al, 2007).
McIntosh and Curran (2009) propose a metric
for measuring the semantic drift introduced by a
learned term, favoring terms different than the recent
m learned terms and similar to the first n, (shown
for n=20 and n=100), following the assumption that
semantic drift develops in late bootstrapping itera-
tions. As we will show, for biomedical categories,
semantic drift in NELL occurs within a handful of
iterations (< 5), however according to the authors,
using low values for n produces inadequate results.
In fact, selecting effective n and m parameters may
not only be a function of the data being used, but
also of the specific category, and it is unclear how to
automatically tune them.
Seed Set Refinement. Vyas et al (2009) suggest
a method for reducing ambiguity in seeds provided
by human experts, by selecting the tightest seed
clusters based on context similarity. The method is
described for an order of 10 seeds, however, in an
ontology containing hundreds of seeds per class, it is
unclear how to estimate the correct number of clus-
ters to choose from. Another approach, suggested
by Kozareva et al (2010), is using only constrained
contexts where both seed and class are present in a
sentence. Extending this idea, we consider a more
general collocation metric, looking at entire docu-
ments including both the seed and its category.
3 Implementation
3.1 NELL?s Bootstrapping System
We have implemented BioNELL based on the sys-
tem design of NELL. NELL?s bootstrapping algo-
rithm is initiated with an input ontology structure of
categories and seeds. Three sub-components oper-
ate to introduce new facts based on the semantic and
morphological attributes of known facts. At every
iteration, each component proposes candidate facts,
specifying the supporting evidence for each candi-
date, and the candidates with the most strongly sup-
ported evidence are added to the KB. The process
and sub-components are described in detail by Carl-
son et al (2010) and Wang and Cohen (2009).
3.2 Text Corpora
PubMed Corpus: We used a corpus of 200K full-
text biomedical articles taken from the PubMed
Central Open Access Subset (extracted in October
2010)1, which were processed using the OpenNLP
package2. This is the main BioNELL corpus and it
1http://www.ncbi.nlm.nih.gov/pmc/
2http://opennlp.sourceforge.net
13
is used to extract category instances in all the exper-
iments presented in this paper.
Web Corpus: BioNELL?s seed-quality colloca-
tion measure (Section 3.4) is based on a domain-
independent Web corpus, the English portion of the
ClueWeb09 data set (Callan and Hoy, 2009), which
includes 500 million web documents.
3.3 Ontology
BioNELL?s ontology is composed of six base on-
tologies, covering a wide range of biomedical sub-
domains: the Gene Ontology (GO) (Ashburner et
al., 2000), describing gene attributes; the NCBI Tax-
onomy for model organisms (Sayers et al, 2009);
Chemical Entities of Biological Interest (ChEBI)
(Degtyarenko et al, 2008), a dictionary focused on
small chemical compounds; the Sequence Ontol-
ogy (Eilbeck et al, 2005), describing biological se-
quences; the Cell Type Ontology (Bard et al, 2005);
and the Human Disease Ontology (Osborne et al,
2009). Each ontology provides a hierarchy of terms
but does not distinguish concepts from instances.
We used an automatic process for merging base
ontologies into one ontology tree. First, we group
the ontologies under one hierarchical structure, pro-
ducing a tree of over 1 million entities, including
856K terms and 154K synonyms. We then separate
these into potential categories and potential seeds.
Categories are nodes that are unambiguous (have a
single parent in the ontology tree), with at least 100
descendants. These descendants are the category?s
Potential seeds. This results in 4188 category nodes.
In the experiments of this paper we selected only
the top (most general) 20 categories in the tree of
each base ontology. We are left with 109 final cate-
gories, as some base ontologies had less than 20 cat-
egories under these restrictions. Leaf categories are
given seeds from their descendants in the full tree of
all terms and synonyms, giving a total of around 1
million potential seeds. Seed set refinement is de-
scribed below. The seeds of leaf categories are later
extended by the bootstrapping process.
3.4 BioNELL?s Bootstrapping System
3.4.1 PMI Collocation with the Category Name
We define a seed quality metric based on a large
corpus of Web data. Let s and c be a seed and a tar-
get category, respectively. For example, we can take
s = ?white?, the name of a gene of the fruit-fly, and c
= ?fly gene?. Now, let D be a document corpus (Sec-
tion 3.2 describes the Web corpus used for ranking),
and let Dc be a subset of the documents contain-
ing a mention of the category name. We measure
the collocation of the seed and the category by the
number of times s appears in Dc, |Occur(s,Dc)|.
The overall occurrence of s in the corpus is given
by |Occur(s,D)|. Following the formulation of
Church and Hanks (1990), we compute the PMI-
rank of s and c as
PMI(s, c) =
|Occur(s,Dc)|
|Occur(s,D)|
(1)
Since this measure is used to compare seeds of the
same category, we omit the log from the original for-
mulation. In our example, as white is a highly am-
biguous gene name, we find that it appears in many
documents that do not discuss the fruit fly, resulting
in a PMI rank close to 0.
The proposed ranking is sensitive to the descrip-
tive name given to categories. For a more robust
ranking, we use a combination of rankings of the
seed with several of its ancestors in the ontology hi-
erarchy. In (Movshovitz-Attias and Cohen, 2012)
we describe this hierarchical ranking in more detail
and additionally explore the use of the binomial log-
likelihood ratio test (BLRT) as an alternative collo-
cation measure for ranking.
We further note that some specialized biomedical
terms follow strict nomenclature rules making them
easily identifiable as category specific. These terms
may not be frequent in general Web context, lead-
ing to a low PMI rank under the proposed method.
Given such a set of high confidence seeds from a
reliable source, one can enforce their inclusion in
the learning process, and specialized seeds can addi-
tionally be identified by high-confidence patterns, if
such exist. However, the scope of this work involves
selecting seeds from an ambiguous source, biomed-
ical ontologies, thus we do not include an analysis
for these specialized cases.
3.4.2 Rank-and-Learn Bootstrapping
We incorporate PMI ranking into BioNELL using
a Rank-and-Learn bootstrapping methodology. Af-
ter every iteration, we rank all the instances that have
been added to the KB. Only high-ranking instances
14
Learning System Bootstrapping
Algorithm
Initial
Seeds
Corpus
BioNELL Rank-and-Learn
with PMI
PMI
top 50
PubMed
NELL NELL?s
algorithm
Random
50
PubMed
BioNELL+Random Rank-and-Learn
with PMI
Random
50
PubMed
Table 2: Learning systems used in our evaluation, all us-
ing the PubMed biomedical corpus and the biomedical
ontology described in Sections 3.2 and 3.3.
are added to the collection of seeds that are used in
the next learning iteration. Instances with low PMI
rank are stored in the KB and are not used for learn-
ing new information. We consider a high-ranking
instance to be one with PMI rank higher than 0.25.
4 Experimental Evaluation
4.1 Experimental Settings
4.1.1 Configurations of the Algorithm
In our experiments, we ran BioNELL and NELL
with the following system configurations, all using
the biomedical corpus and the ontology described in
Sections 3.2 and 3.3, and all running 50 iterations,
in order to evaluate the long term effects of ranking.
Section 4.2 includes a discussion on the learning rate
of the tested systems which motivates the reason for
evaluating performance at the 50th iteration.
To expand a category we used the following sys-
tems, also summarized in Table 2: (1) the BioNELL
system, using Rank-and-Learn bootstrapping (Sec-
tion 3.4.2) initialized with the top 50 seeds using
PMI ranking, (2) the NELL system, using NELL?s
original bootstrapping algorithm (Section 3.1) ini-
tialized with 50 random seeds from the category?s
potential seeds (NELL does not provide a seed se-
lection method), and (3) in order to distinguish
the contribution of Rank-and-Learn bootstrapping
over ranking the initial seeds, we tested a third
system, BioNELL+Random, using Rank-and-Learn
bootstrapping initialized with 50 random seeds.
4.1.2 Evaluation Methodology
Using BioNELL we can learn lexicons, collec-
tions of category terms accumulated after running
the system. One evaluation approach is to select
a set of learned instances and assess their correct-
ness (Carlson et al, 2010). This is relatively easy
for data extracted for general categories like City or
Sports Team. For example, it is easy to evaluate the
statement ?London is a City?. This task becomes
more difficult when assessing domain-specific facts
such as ?Beryllium is an S-block molecular entity?
(in fact, it is). We cannot, for example, use the help
of Mechanical Turk for this task. A possible alter-
native evaluation approach is asking an expert. On
top of being a costly and slow approach, the range
of topics covered by BioNELL is large and a single
expert is not likely be able to assess all of them.
We evaluated lexicons learned by BioNELL by
comparing them to available resources. Lexicons of
gene names for certain species are available, and the
Freebase database (Google, 2011), an open repos-
itory holding data for millions of entities, includes
some biomedical concepts. For most biomedical
categories, however, complete lexicons are scarce.
4.1.3 Data Sets
We compared learned lexicons to category dictio-
naries, lists of concept terms taken from the follow-
ing sources, which we consider as a Gold Standard.
We used three lexicons of biomedical categories
taken from Freebase: Disease (9420 terms), Chemi-
cal Compound (9225 terms), and Drug (3896 terms).
To evaluate gene names we used data from the
BioCreative Challenge (Hirschman et al, 2005),
an evaluation competition focused on annotations
of genes and gene products. The data includes
a dictionary of genes of the fruit-fly, Drosophila
Melanogaster, which specifies a set of gene iden-
tifiers and possible alternative forms of the gene
name, for a total of 7151 terms, which we consider
to be the complete fly gene dictionary.
We used additional BioCreative data for a named-
entity recognition task. This includes 108 scientific
abstracts, manually annotated by BioCreative with
gene IDs of fly genes discussed in the text. The ab-
stracts contain either the gene ID or any gene name.
4.2 Extending Lexicons of Biomedical
Categories
4.2.1 Recovering a Closed Category Lexicon
We used BioNELL to learn the lexicon of a
closed category, representing genes of the fruit-fly,
15
10 20 30 40 500
0.2
0.4
0.6
0.8
1
Iteration
Precis
ion
 
 
BioNELLNELLBioNELL+Random
(a) Precision
10 20 30 40 500
50
100
150
200
250
IterationC
umula
tive co
rrect l
exicon
 items
 
 
BioNELLNELLBioNELL+Random
(b) Cumulative correct items
10 20 30 40 500
100
200
300
400
500
IterationC
umula
tive in
correc
t lexic
on ite
ms
 
 BioNELLNELLBioNELL+Random
(c) Cumulative incorrect items
Figure 1: Performance per learning iteration for gene lexicons learned using BioNELL and NELL.
Learning System Precision Correct Total
BioNELL .83 109 132
NELL .29 186 651
BioNELL+Random .73 248 338
NELL by size 132 .72 93 130
Table 3: Precision, total number of instances (Total),
and correct instances (Correct) of gene lexicons learned
with BioNELL and NELL. BioNELL significantly im-
proves the precision of the learned lexicon compared with
NELL. When examining only the first 132 learned items,
BioNELL has both higher precision and more correct in-
stances than NELL (last row, NELL by size 132).
D. Melanogaster, a model organism used to study
genetics and developmental biology. Two samples
of genes from the full fly gene dictionary are shown
in Table 1: High PMI Seeds are the top 50 dictio-
nary terms selected using PMI ranking, and Random
Seeds are a random draw of 50 terms. Notice that the
random set contains many seeds that are not distinct
gene names including arm, 28, and dad. In con-
trast, high PMI seeds exhibit much less ambiguity.
We learned gene lexicons using the test systems de-
scribed in Section 4.1.1 with the high-PMI and ran-
dom seed sets shown in Table 1. We measured the
precision, total number of instances, and correct in-
stances of the learned lexicons against the full dic-
tionary of genes. Table 3 summarizes the results.
BioNELL, initialized with PMI-ranked seeds, sig-
nificantly improved the precision of the learned
lexicon over NELL (29% for NELL to 83% for
BioNELL). In fact, the two learning systems us-
ing Rank-and-Learn bootstrapping resulted in higher
precision lexicons (83%, 73%), suggesting that con-
strained bootstrapping using iterative seed ranking
successfully eliminates noisy and ambiguous seeds.
BioNELL?s bootstrapping methodology is highly
restrictive and it affects the size of the learned lexi-
con as well as its precision. Notice, however, that
while NELL?s final lexicon is 5 times larger than
BioNELL?s, the number of correctly learned items in
it are less than twice that of BioNELL. Additionally,
BioNELL+Random has learned a smaller dictionary
than NELL (338 and 651 terms, respectively) with a
greater number of correct instances (248 and 186).
We examined the performance of NELL after the
7th iteration, when it has learned a lexicon of 130
items, similar in size to BioNELL?s final lexicon (Ta-
ble 3, last row). After learning 130 items, BioNELL
achieved both higher precision (83% versus 72%)
and higher recall (109 versus 93 correct lexicon
instances) than NELL, indicating that BioNELL?s
learning method is overall more accurate.
After running for 50 iterations, all systems re-
cover only a small portion of the complete gene dic-
tionary (109-248 instances out of 7151), suggesting
either that, (1) more learning iterations are required,
(2) the biomedical corpus we use is too small and
does not contain (frequent) mentions of some gene
names from the dictionary, or (3) some other limita-
tions exist that prevent the learning algorithm from
finding additional class examples.
Lexicons learned using BioNELL show persis-
tently high precision throughout the 50 iterations,
even when initiated with random seeds (Figure 1A).
By the final iteration, all systems stop accumulating
further significant amounts of correct gene instances
(Figure 1B). Systems that use PMI-based Rank-
and-Learn bootstrapping also stop learning incorrect
16
Learning System Precision Correct Total
CC Drug Disease CC Drug Disease CC Drug Disease
BioNELL .66 .52 .43 63 508 276 96 972 624
NELL .15 .40 .37 74 522 288 449 1300 782
NELL by size .58 .47 .37 58 455 232 100 968 623
Table 4: Precision, total number of instances (Total), and correct instances (Correct) of learned lexicons of Chemical
Compound (CC), Drug, and Disease. BioNELL?s lexicons have higher precision on all categories compared with
NELL, while learning a similar number of correct instances. When restricting NELL to a total lexicon size similar to
BioNELL?s, BioNELL has both higher precision and more correct instances (last row, NELL by size).
instances (BioNELL and BioNELL+Random; Fig-
ure 1C). This is in contrast to NELL which continues
learning incorrect examples.
Interestingly, the highest number of correct gene
instances was learned using Rank-and-Learn boot-
strapping with random initial seeds (248 items;
BioNELL+Random). This may happen when the
random set includes genes that are infrequent in
the general Web corpus, despite being otherwise
category-specific in the biomedical context. As
such, these would result in low PMI rank (see note
in Section 3.4.1). However, random seed selection
does not offer any guarantees on the quality of the
seeds used, and therefore will result in unstable per-
formance. Note that BioNELL+Random was initi-
ated with the same random seeds as NELL, but due
to the more constrained Rank-and-Learn bootstrap-
ping it achieves both higher recall (248 versus 186
correct instances) and precision (73% versus 29%).
4.2.2 Extending Lexicons of Open Categories
We evaluated learned lexicons for three open cat-
egories, Chemical Compound (CC), Drug, and Dis-
ease, using dictionaries from Freebase. Since these
are open categories ? new drugs are being devel-
oped every year, new diseases are discovered, and
varied chemical compounds can be created ? the
Freebase dictionaries are not likely to contain com-
plete information on these categories. For our evalu-
ation, however, we considered them to be complete.
We used BioNELL and NELL to learn these cat-
egories, and for all of them BioNELL?s lexicons
achieved higher precision than NELL (Table 4). The
number of correct learned instances was similar in
both systems (63 and 74 for CC, 508 and 522 for
Drug, and 276 and 288 for Disease), however in
BioNELL, the additional bootstrapping restrictions
assist in rejecting incorrect instances, resulting in a
smaller, more accurate lexicon.
We examined NELL?s lexicons when they reached
a size similar to BioNELL?s final lexicons (at the 8th,
42nd and 39th iterations for CC, Drug, and Disease,
respectively). BioNELL?s lexicons have both higher
precision and higher recall (more correct learned in-
stances) than the comparable NELL lexicons (Ta-
ble 4, NELL by size, last row).
4.3 Named-Entity Recognition using a
Learned Lexicon
We examined the use of gene lexicons learned with
BioNELL and NELL for the task of recognizing
concepts in free text, using a simple strategy of
matching words in the text with terms from the lex-
icon. We use data from the BioCreative challenge
(Section 4.1.3), which includes text abstracts and the
IDs of genes that appear in each abstract. We show
that BioNELL?s lexicon achieves both higher preci-
sion and recall in this task than NELL?s.
We implemented an annotator for predicting what
genes are discussed in text, which uses a gene lexi-
con as input. Given sample text, if any of the terms
in the lexicon appear in the text, the corresponding
gene is predicted to be discussed in the text. Follow-
ing BioCreative?s annotation format, the annotator
emits as output the set of gene IDs of the genes pre-
dicted for the sample text.
We evaluated annotators that were given as in-
put: the complete fly-genes dictionary, a filtered
version of that dictionary, or lexicons learned us-
ing BioNELL and NELL. Using these annotators we
predicted gene mentions for all text abstracts in the
data. We report the average precision (over 108 text
17
Lexicon Precision Correct Total
BioNELL .90 18 20
NELL .02 5 268
BioNELL+Random .03 3 82
Complete Dictionary .09 153 1616
Filtered Dictionary .18 138 675
Table 5: Precision, total number of predicted genes (To-
tal), and correct predictions (Correct), in a named-entity
recognition task using a complete lexicon, a filtered lex-
icon, and lexicons learned with BioNELL and NELL.
BioNELL?s lexicon achieves the highest precision, and
makes more correct predictions than NELL.
abstracts) and number of total and correct predic-
tions of gene mentions, compared with the labeled
annotations for each text (Table 5).
Many gene names are shared among multiple
variants. For example, the name Antennapedia may
refer to several gene variations, e.g., Dgua\Antp or
Dmed\Antp. Thus, in our precision measurements,
we consider a prediction of a gene ID as ?true? if it
is labeled as such by BioCreative, or if it shares a
synonym name with another true labeled gene ID.
First, we used the complete fly gene dictionary
for the recognition task. Any dictionary gene that
is mentioned in the text was recovered, resulting
in high recall. However, the full dictionary con-
tains ambiguous gene names that contribute many
false predictions to the complete dictionary annota-
tor, leading to a low precision of 9%.
Some ambiguous terms can be detected using
simple rules, e.g., short abbreviations and numbers.
For example, section 9 is a gene named after the
functional unit to which it belongs, and abbreviated
by the symbol 9. Clearly, removing 9 from the full
lexicon should improve precision without great cost
to recall. We similarly filtered the full dictionary, re-
moving one- and two-letter abbreviations and terms
composed only of non-alphabetical characters, leav-
ing 6253 terms. Using the filtered dictionary, pre-
cision has doubled (18%) with minor compromise
to recall. Using complete or manually refined gene
dictionaries for named-entity recognition has been
shown before to produce similar high-recall and
low-precision results (Bunescu et al, 2005).
We evaluated annotators on gene lexicons learned
with BioNELL and NELL. BioNELL?s lexicon
achieved significantly higher precision (90%) than
other lexicons (2%-18%). It is evident that this lexi-
con contains few ambiguous terms as it leads to only
2 false predictions. Note also, that BioNELL?s lexi-
con has both higher precision and recall than NELL.
5 Conclusions
We have proposed a methodology for an open infor-
mation extraction system for biomedical scientific
text, using an automatically derived ontology of cat-
egories and seeds. Our implementation is based on
constrained bootstrapping in which seeds are ranked
at every iteration.
The benefits of iterative seed ranking have been
demonstrated, showing that our method leads to sig-
nificantly less ambiguous lexicons for all the eval-
uated biomedical concepts. BioNELL shows 51%
increase over NELL in the precision of a learned
lexicon of chemical compounds, and 45% increase
for a category of gene names. Importantly, when
BioNELL and NELL learn lexicons of similar size,
BioNELL?s lexicons have both higher precision and
recall. We have demonstrated the use of BioNELL?s
learned gene lexicon as a high precision annotator
in an entity recognition task (with 90% precision).
The results are promising, though it is currently dif-
ficult to provide a similar quantitative evaluation for
a wider range of concepts.
Many interesting improvements could be made
in the current system, mainly discovery of relations
between existing ontology categories. In addition,
we believe that Rank-and-Learn bootstrapping and
iterative seed ranking can be beneficial in general,
domain-independent settings, and we would like to
explore further use of this method.
Acknowledgments
This work was funded by grant 1R101GM081293
from NIH, IIS-0811562 from NSF and by a gift from
Google. The opinions expressed in this paper are
solely those of the authors.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, et al 2000. Gene ontology: tool for the
unification of biology. Nature genetics, 25(1):25.
18
J. Bard, S.Y. Rhee, and M. Ashburner. 2005. An ontol-
ogy for cell types. Genome Biology, 6(2):R21.
R. Bunescu, R. Ge, R.J. Kate, E.M. Marcotte, and R.J.
Mooney. 2005. Comparative experiments on learning
information extractors for proteins and their interac-
tions. Artificial Intelligence in Medicine, 33(2).
J. Callan and M. Hoy. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/.
A. Carlson, J. Betteridge, E.R. Hruschka Jr, T.M.
Mitchell, and SP Sao Carlos. 2009. Coupling semi-
supervised learning of categories and relations. Semi-
supervised Learning for Natural Language Process-
ing, page 1.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hr-
uschka Jr, and T.M. Mitchell. 2010. Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Twenty-Fourth Conference on Artificial
Intelligence (AAAI 2010), volume 2, pages 3?3.
J.T. Chang, H. Schu?tze, and R.B. Altman. 2004. Gap-
score: finding gene and protein names one word at a
time. Bioinformatics, 20(2):216.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
J.R. Curran, T. Murphy, and B. Scholz. 2007. Minimis-
ing semantic drift with mutual exclusion bootstrap-
ping. In Proceedings of the 10th Conference of the
Pacific Association for Computational Linguistics.
K. Degtyarenko, P. De Matos, M. Ennis, J. Hastings,
M. Zbinden, A. McNaught, R. Alca?ntara, M. Darsow,
M. Guedj, and M. Ashburner. 2008. Chebi: a database
and ontology for chemical entities of biological inter-
est. Nucleic acids research, 36(suppl 1):D344.
A. Dolbey, M. Ellsworth, and J. Scheffczyk. 2006.
Bioframenet: A domain-specific framenet extension
with links to biomedical ontologies. In Proceedings
of KR-MED, pages 87?94. Citeseer.
K. Eilbeck, S.E. Lewis, C.J. Mungall, M. Yandell,
L. Stein, R. Durbin, and M. Ashburner. 2005. The se-
quence ontology: a tool for the unification of genome
annotations. Genome biology, 6(5):R44.
Google. 2011. Freebase data dumps.
http://download.freebase.com/datadumps/.
L. Hirschman, A. Yeh, C. Blaschke, and A. Valencia.
2005. Overview of biocreative: critical assessment of
information extraction for biology. BMC bioinformat-
ics, 6(Suppl 1):S1.
Z. Kozareva and E. Hovy. 2010. Not all seeds are equal:
measuring the quality of text mining seeds. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 618?626. Associa-
tion for Computational Linguistics.
J. Krishnamurthy and T.M. Mitchell. 2011. Which noun
phrases denote which concepts? In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
Association for Computational Linguistics.
T. McIntosh and J.R. Curran. 2009. Reducing seman-
tic drift with bagging and distributional similarity. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 1-Volume 1, pages 396?404. As-
sociation for Computational Linguistics.
D. Movshovitz-Attias and W.W. Cohen. 2012. Boot-
strapping biomedical ontologies for scientific text us-
ing nell. Technical report, Carnegie Mellon Univer-
sity, CMU-ML-12-101.
J. Osborne, J. Flatow, M. Holko, S. Lin, W. Kibbe,
L. Zhu, M. Danila, G. Feng, and R. Chisholm. 2009.
Annotating the human genome with disease ontology.
BMC genomics, 10(Suppl 1):S6.
P. Pantel, E. Crestan, A. Borkovsky, A.M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 938?947. As-
sociation for Computational Linguistics.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the National Conference on Artifi-
cial Intelligence (AAAI-99), pages 474?479.
E. W. Sayers, T. Barrett, D. A. Benson, S. H. Bryant,
K. Canese, V. Chetvernin, D. M. Church, M. DiCuc-
cio, R. Edgar, S. Federhen, M. Feolo, L. Y. Geer,
W. Helmberg, Y. Kapustin, D. Landsman, D. J.
Lipman, T. L. Madden, D. R. Maglott, V. Miller,
I. Mizrachi, J. Ostell, K. D. Pruitt, G. D. Schuler,
E. Sequeira, S. T. Sherry, M. Shumway, K. Sirotkin,
A. Souvorov, G. Starchenko, T. A. Tatusova, L. Wag-
ner, E. Yaschenko, and J. Ye. 2009. Database re-
sources of the National Center for Biotechnology In-
formation. Nucleic Acids Res., 37:5?15, Jan.
V. Vyas, P. Pantel, and E. Crestan. 2009. Helping edi-
tors choose better seed sets for entity set expansion. In
Proceeding of the 18th ACM conference on Informa-
tion and knowledge management. ACM.
R.C. Wang and W.W. Cohen. 2009. Character-level anal-
ysis of semi-structured documents for set expansion.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1503?1512. Association for Compu-
tational Linguistics.
T. Wattarujeekrit, P. Shah, and N. Collier. 2004. Pasbio:
predicate-argument structures for event extraction in
molecular biology. BMC bioinformatics, 5(1):155.
19
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 47?55,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Alignment-HMM-based Extraction of Abbreviations from Biomedical Text
Dana Movshovitz-Attias
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
dma@cs.cmu.edu
William W. Cohen
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
wcohen@cs.cmu.edu
Abstract
We present an algorithm for extracting abbre-
viation definitions from biomedical text. Our
approach is based on an alignment HMM,
matching abbreviations and their definitions.
We report 98% precision and 93% recall on
a standard data set, and 95% precision and
91% recall on an additional test set. Our re-
sults show an improvement over previously re-
ported methods and our model has several ad-
vantages. Our model: (1) is simpler and faster
than a comparable alignment-based abbrevia-
tion extractor; (2) is naturally generalizable to
specific types of abbreviations, e.g., abbrevia-
tions of chemical formulas; (3) is trained on a
set of unlabeled examples; and (4) associates a
probability with each predicted definition. Us-
ing the abbreviation alignment model we were
able to extract over 1.4 million abbreviations
from a corpus of 200K full-text PubMed pa-
pers, including 455,844 unique definitions.
1 Introduction
Abbreviations and acronyms are commonly used in
the biomedical literature for names of genes, dis-
eases and more (Ambrus, 1987). Abbreviation def-
initions are a source of ambiguity since they may
change depending on the context. The ability to rec-
ognize and extract abbreviations and map them to
a full definition can be useful for Information Ex-
traction tasks (Yu et al, 2007) and for the complete
understanding of scientific biomedical text.
Yu et al (2002) distinguish the two follow-
ing uses of abbreviations: (1) Common abbrevia-
tions are those that have become widely accepted as
synonyms, such as ?DNA, deoxyribonucleic acid?
or ?AIDS, acquired immunodeficiency syndrome?.
These represent common fundamental and impor-
tant terms and are often used, although not explic-
itly defined within the text (Fred and Cheng, 2003).
In contrast, (2) Dynamic abbreviations, are defined
by the author and used within a particular article.
Such definitions can often overlap, depending on
the context. For example, the term PBS most com-
monly abbreviates Phosphate Buffered Saline, but
in other contexts may refer to the following: Pain
Behavior Scale, Painful Bladder Syndrome, Paired
Domain-Binding Site, Particle Based Simulation,
Partitioned Bremer Support, Pharmaceutical Bene-
fits Scheme, and more. Some abbreviations fall be-
tween these two definitions in the sense that they are
normally defined in the text, however, they have be-
come widely used, and therefore they do not nor-
mally overlap with other abbreviations. An exam-
ple for this is the term ATP which, almost exclu-
sively, abbreviates adenosine triphosphate, and is
only rarely used in different contexts in biomedicine.
Gaudan et al (2005) define two similar con-
cepts, distinguishing Global and Local abbrevia-
tions. Global abbreviations are not defined within
the document, similar to common abbreviation. Lo-
cal abbreviations appear in the document alongside
the long form, similar to dynamic abbreviations.
The contextual ambiguity of dynamic, or local, ab-
breviations makes them an important target for ab-
breviation recognition tasks.
There is a great deal of variation in the way that
different authors produce abbreviations. Our defini-
tion of abbreviation is quite flexible and can best be
47
represented by the set of examples described in Ta-
ble 1. These include simple acronyms, in which the
first letter of every word from the long form is rep-
resented in the short form, as well as more complex
cases such as: inner letter matches, missing short
form characters, and specific substitutions (such as
of a chemical element and its symbol). We gener-
ally assume that the abbreviated form contains some
contraction of words or phrases from the full form.
This definition is consistent with the one defined by
many other extraction systems (see e.g., (Schwartz
and Hearst, 2002) and (Chang et al, 2002)).
We describe a method for extracting dynamic ab-
breviations, which are explicitly defined in biomed-
ical abstracts. For each of the input texts, the task
is to identify and extract ?short form, long form?
pairs of the abbreviations defined within the text. We
also provide a mapping, formed as an alignment, be-
tween the characters of the two forms, and the prob-
ability of this alignment according to our model.
Our approach is based on dividing the abbrevia-
tion recognition task into the following stages: (1)
Parsing the text and extracting candidate abbrevia-
tion pairs (long and short forms) based on textual
cues, such as parentheses; (2) Recovering a valid
alignment between the short and long form candi-
dates (valid alignments are defined in Section 3.2).
We perform a sequential alignment based on a pair-
HMM; (3) Extracting a final short and long form
from the alignment.
We will show that our approach is fast and accu-
rate: we report 98% precision and 93% recall on a
standard data set, and 95% precision and 91% recall
on a validation set. The alignment model: (1) is sim-
pler and faster than a comparable alignment-based
abbreviation extractor; (2) is naturally generalizable
to specific types of abbreviations; (3) is trained on a
set of unlabeled examples; and (4) associates a prob-
ability with each predicted definition.
2 Related Work
A wide variety of methods have been introduced
for recognizing abbreviations in biomedical context.
Many utilize one of the following techniques: rule-
based extraction, and extraction that relies on an
alignment of the abbreviation and full definition.
Abbreviation extraction methods have been used in
two main contexts: to create online collections of
abbreviations, normally extracted from PubMed ab-
stracts (Zhou et al, 2006; Gaudan et al, 2005; Adar,
2004), and as part of larger learning frameworks,
mainly for feature generation (Chowdhury et al,
2010; Huang et al, 2011).
Rule based extraction systems use a set of man-
ually crafted pattern-matching rules to recognize
and extract the pair of abbreviation and defini-
tion: Acrophile (Larkey et al, 2000) is an acronym
recognition system that exploits morphological rules
based on the case of the characters in the definitions.
Unlike many of the other available systems, it rec-
ognized acronyms that are defined without paren-
theses; The Alice system (Ao and Takagi, 2005) is
based on three extraction phases, each employing
an elaborate set of over 15 rules, patterns and stop
word lists. Liu and Friedman (2003) use a set of
statistical rules to resolve cases in which an abbre-
viation is defined more than once with several dif-
ferent definitions. While these methods normally
achieve high performance results, their main draw-
back is that they are difficult to implement and to
extend. Rule development is normally based on a
thorough investigation of the range of targeted ab-
breviations and the resulting heuristic patterns con-
tain subtleties that are hard to recreate or modify.
Several extraction methods have been developed
based on some variant of the Longest Common Sub-
sequence algorithm (LCS) (Schwartz and Hearst,
2002; Chang et al, 2002; Taghva and Gilbreth,
1999; Bowden et al, 1997). These systems search
for at least one possible alignment of an abbrevia-
tion and a full form definition.
The most widely used abbreviation extraction sys-
tem is that presented by Schwartz and Hearst (2002).
Their method scans the input text and extract pairs
of candidate abbreviations from text surrounding
parentheses. The algorithm scans the candidate defi-
nition from right to left, and searches for an implicit
alignment of the definition and abbreviation based
on few ad-hoc rules. This algorithm presents several
constraints on the type of recognized abbreviations,
the most restrictive being that every letter of the ab-
breviation must be matched during the process of
scanning the definition. Of the variety of available
extraction systems, this remains a popular choice
due to its simplicity and speed. However, as the au-
48
Short Long Type of Abbreviation
AMS Associated Medical Services Acronym using the first letter of each long-form word.
PS postsynaptic Inner letters are represented in the abbreviation.
NTx cross-linked N-telopeptides 1. Phonetic substitution (cross? x).
2. The short form is out-of-order.
3. Words from the long form are missing in the short form (linked).
EDI-2 Eating Disorders Inventory Characters from the short form are missing in the long form (-2).
NaB sodium butyrate Substitution of a chemical element by its symbol (sodium? Na).
MTIC 5-(3-N-methyltriazen-1-yl)-
imidazole-4-carboxamide
Chemical formula.
EBNA-1 Epstein-Barr virus (EBV) nuclear
antigen 1
Recursive definition, in which the long form contains another ab-
breviation definition.
3-D three-dimensional Substitution of a number name and symbol (three? 3).
A&E accident and emergency Substitution of a word and symbol (and? &).
anti-Tac antibody to the alpha subunit of the
IL-2 receptor
Synonym: the short form commonly represents the long form, al-
though it is not a direct abbreviation of it.
R.E.A.L. ?Revised European-American Clas-
sification of Lymphoid Neoplasms?
The long- and/or short-forms contain characters that are not di-
rectly related to the abbreviation (e.g., punctuation symbols).
Table 1: Examples of biomedical abbreviations.
thors report, this algorithm is less specific than other
approaches and consequently results in lower recall.
We will show that by performing an explicit align-
ment of the abbreviation using an alignment-HMM,
our model results in more accurate predictions, and
that the edit operations used in the alignment allow
for natural extensions of the abbreviations domain.
Another frequently used alignment based ap-
proach is that of Chang et al (2002), and it is closest
to our approach. After calculating an abbreviation
alignment, they convert the set of aligned terms into
a feature vector which is scored using a binary logis-
tic regression classifier. Using a correct threshold on
the alignment scores produces a high performance
abbreviation extractor. However this approach has
several drawbacks. The run-time of this algorithm
is fairly long (see Section 4.3), in part due to the
steps following the alignment recovery, i.e., calcu-
lating a feature vector, and generating an alignment
score. Additionally, choosing a score threshold may
depend on the genre of text, and different thresh-
olds lead to a variety of quality in the results. We
will show that presenting limitations on the range of
available alignments can produce correct alignments
more efficiently and quickly, maintaining high qual-
ity results, without the need for threshold selection.
Our alignment method distinguishes and penalizes
inner and leading gaps in the alignment, and it ap-
plies a set of constraints on the range of legal align-
ments. We will also show that relying solely on con-
strained alignments still allows for flexibility in the
definition of the range of desired abbreviations.
Ristad and Yianilos (1998) proposed a single state
alignment-HMM for learning string-edit distance
based on matched strings. In later work, Bilenko and
Mooney (2003) extend this model to include affine
gaps, by including in their model separate states
for Matches, Deletions and Insertions. McCallum
et al (2005) describe a discriminative string edit
CRF, following a similar approach to that of Bilenko
and Mooney. The CRF model includes two disjoint
sets of states, each representing either ?matching? or
?mismatching? string pairs. Each of the sets is sim-
ilar to the model described by Bilenko and Mooney.
All of these models require labeled training exam-
ples, and the CRF approach also requires negative
training examples, which train the ?mismatching?
states of the model. We describe an alignment HMM
that is suited for aligning abbreviation long and short
forms, and does not require any labeling of the input
text or training examples.
3 Method
In the following sections we describe a method for
extracting candidate abbreviation definitions from
text, and an alignment model with affine gaps for
49
Description Result
i. Input sentence: ?anti-sperm antibodies were studied by indirect mixed anti-globulin reaction test (MAR)?
ii. Candidate: ?MAR, by indirect mixed anti-globulin reaction test?
iii. Alignment:
HMM States
Short Form
Long Form
LG LG LG LG M M M M IG M M M IG
M A R
by indirect mixed anti - globulin reaction test
iv. Abbreviation: ?MAR, mixed anti-globulin reaction test?
Table 2: Example of the processing steps of a sample sentence. (i) Input sentence containing a single abbreviation.
(ii) Candidate ?short form, long form? pair extracted from the sentence (after truncating the long-form). (iii) The
most likely (Viterbi) alignment of the candidate pair, using our alignment model. Each state corresponds to a single
edit-operation, which consumed the corresponding short-form and long-form characters in the alignment. (iv) Final
abbreviation, extracted from the alignment by removing leading gaps.
matching the two forms of a candidate definition.
Finally we describe how to extract the final abbre-
viation prediction out of the alignment.
3.1 Extracting candidate abbreviations
The process described below scans the text for tex-
tual cues and extracts a list of candidate abbreviation
pairs, for every input document, in the form: ?short
form, long form?. The following text also describes
the restrictions and conditions of what we consider
to be valid candidate pairs. The assumptions made
in this work are generally less restrictive that those
introduced by previous extraction systems and they
lead to a larger pool of candidate definitions. We
will later show that false candidates normally pro-
duce invalid alignment of their short and long forms,
according to our alignment model, and so they are
removed and do not affect the final results.
The parsing process includes a search for both
single abbreviations, and abbreviation patterns. An
example of a sentence with a single abbreviation
can be seen in Table 2(i). We consider the fol-
lowing two cases of a single abbreviation defini-
tion: (1) ?long form (short form)?, and (2) ?short
form (long form)?. Note that in some cases, the
term within the parenthesis is parsed, e.g., in the
following text, ELISA is extracted from the paren-
thesis, by removing the text beyond the ?;? symbol:
?. . . human commercial enzyme-linked immunosor-
bent assay (ELISA; BioGen, Germany) . . . ?.
We also consider abbreviation patterns which
define multiple abbreviations simultaneously, as
demonstrated by these examples:
? ?anti-sperm (ASA), anti-phospholipid (APA),
and antizonal (AZA) antibodies? ? The main
noun (antibodies) follows the pattern.
? ?Epithelial-mesenchymal transition (EMT)
and interaction (EMI)? ? The main noun
(Epithelial-mesenchymal) is at the head of the
pattern.
Using textual cues (patterns and parentheses) we
extract candidate short and long forms. Whenever
possible, we consider the term within the parenthe-
sis as the short form, and the text to the left of the
parenthesis (until the beginning of the sentence) as
the candidate long form. We consider valid short
forms to be no longer than 3 words, having between
1 and 15 characters, and containing at least one let-
ter. In the case that the candidate short form was
found to be invalid by these definitions, we switch
the assignment of long and short forms. The long-
form string is truncated, following Park and Byrd
(2001), to a length of min(|A|+ 5, |A| ? 2), where
|A| is the length of the short form.
The length of the candidate long form is estimated
using the Park and Byrd formula, and it is therefore
normally the case that the resulting candidate long
form contains some leading characters that are not
part of the abbreviation definition. Next, we define
an alignment between short and long form strings
50
<?CRF-BP?, ?ligands for the corticotrophin-releasing factor binding protein?> 
       |  |   |  |   |  |C             | |R        | |F     | |-|B      | |P      | ligands|  |for|  |the|  |corticotrophin|-|releasing| |factor| | |binding| |protein| 
!"
#$"
%"
&$"
'"
Figure 1: Abbreviation alignment HMM model with
states: start (s), leading gaps (LG), match (M), inner gap
(IG) and end (e).
Edit
Operation
SF
Match
LF
Match
Valid
States
LF deletion  alpha-numeric
char
LG, IG
LF deletion  punct. symbol LG, M
LF deletion  word LG, IG
SF deletion digit or punct.  IG
Match char (partial) word M
Match char char M
Substitution ?&? ?and? M
Substitution ?1?-?9? ?one?-?nine? M
Substitution chem. symbol chemical name M
Table 3: Edit operations used in the alignment HMM
model including, long form (LF) and short form (SF)
deletions, matches and substitutions. We note the SF and
LF characters consumed by each edit operation, and the
HMM states in which it may be used.
which detects possible segments that are missing in
the alignment in either string (gaps).
3.2 Aligning candidate long and short forms
For each of the candidate pairs produced in the pre-
vious step, we find the best alignment (if any) be-
tween the short and the long form strings. We de-
scribe an alignment HMM that is suited for abbrevi-
ation alignments. The model is shown in Figure 1,
and Table 2 shows the parsing process of a sam-
ple sentence, including an alignment created for this
sample using the model.
3.3 Abbreviation Alignment with Affine
Leading and Inner Gaps
An alignment between a long and a short form of an
abbreviation can be modeled as a series of edit oper-
ations between the two strings, in which characters
from the short form may match a single or a series
of characters from the long form. In previous work,
Bilenko and Mooney (2003) describe a generative
model for string edit distance with affine gaps, and
an Expectation Maximization algorithm for learning
the model parameters using a labeled set of match-
ing strings. We propose a similar model for aligning
the short and long form of an abbreviation, using an
affine cost model for gaps
cost(g) = s+ e ? l (1)
where s is the cost of starting a gap, e is the cost of
extending a gap and l is the length of the gap. In our
method, we use extracted candidate pairs (candidate
short and long forms) as training examples.
As described above, candidate long forms are
formed by extracting text preceding parentheses and
truncating it to some length. This process may lead
to candidate long forms that contain leading charac-
ters that do not belong to the abbreviation, which
will result in leading gaps in the final alignment.
For example, the candidate long form presented in
Table 2(ii) contains the leading text ?by indirect ?.
While extra leading text is expected as an artifact of
our candidates extraction method, inner alignment
gaps are not expected to commonly appear in abbre-
viation alignments, and are usually an indication of a
bad alignment. The example presented in Table 2 is
of an abbreviation that does contain inner gaps (e.g.,
globulin) despite being a valid definition.
We distinguish leading and inner alignment gaps
using a model with five states: Leading Gap (LG),
Match (M), Inner Gap (IG), and two ?dummy? states
for the beginning and end of an alignment (Figure 1).
Since leading and inner gaps are represented by dif-
ferent states, their penalization is not coupled, i.e.,
they are associated with different s, e and l costs.
We use the EM algorithm to learn the model param-
eters, based on a set of unlabeled candidate pairs,
following the assumption that many false-candidates
will not produce a valid alignment, and will not af-
fect training. This is in contrast to previous string
edit distance models, which require labeled training
examples.
The main effort in developing a successful ab-
breviation alignment model involves generating a
meaningful set of edit operations. The edit opera-
tions used in our model,E = Ed?Em?Es, is shown
in Table 3 and includes: Ed, deletions of characters
or words from the long form, or of single characters
51
from the short form; Em, matches of a full of par-
tial word from the long form to a character in the
short form; and Es, word substitutions in which a
word from the long form is replaced by a symbol in
the short form. Note that: (1) while all types of dele-
tions from the long form are valid, deletions from the
short form are limited to digits and punctuation sym-
bols, and (2) deletion of non-alpha-numeric charac-
ters from the long form is not considered as opening
a gap but as a match, as it is common for non-alpha-
numeric characters to be missing in an abbreviation
(i.e., be ?matched? with the empty string, ).
Let x = x1 . . . xT be the short form candidate,
y = y1 . . . yV be the long form candidate, and
a = ?ap?np=1, ap = (ep, qp, ixp, jyp), be a pos-
sible alignment of the strings x and y. a repre-
sents as a sequence of HMM transitions, ap, where
ep ? E is an edit operation that consumes charac-
ters from x (deletion from the long form), y (dele-
tion from the short form), or both (match or substi-
tution), up to position ixp in x and jyp in y, and
is associated with a transition in the model to state
qp ? {LG,M, IG, e}. Let pi(q, q?) be the transition
probability between states q and q?, and let ?(q, e)
be the emission probability of the edit operation e at
state q. Given a candidate abbreviation pair ?x, y?,
and the model parameters pi and ? , the probability of
an alignment is given by
p(a|x, y, pi, ?) =
|a|?
p=1
pi(qp?1, qp) ? ?(qp, ep) (2)
where q0 is the start state. This probability can be
calculated efficiently using dynamic programming
with the forward-backward algorithm, and the most
likely alignment corresponds to the Viterbi distance
between x and y.
In our method, the model parameters, pi and ? ,
are estimated using the EM algorithm on an unla-
beled training set of candidate pairs that have been
extracted from the text, without any further process-
ing. At each EM iteration, we train on pairs that have
valid alignments (see below) with non-zero proba-
bility under the model parameters at that iteration.
3.3.1 Valid Alignments
Given the edit operations defined above, the only
valid way of matching a letter from the short form
to the long form is by matching that letter to the
beginning of a full or partial word, or by matching
that letter using a substitution operation. There is
no edit operation for deleting letters from the short
form (only digits and punctuation symbols can be
deleted). This means that for some candidate pairs
there are no valid alignments under this model, in
which case, no abbreviation will be predicted.
3.3.2 Extracting the Final Abbreviation
Given a valid alignment a between the candi-
date pair, x and y, we create a truncated alignment,
a?, by removing from a initial transitions in which
qp = LG. We consider a? valid if the number of
matches in a? = ?a?p?
n?
p=1 is greater than the number
of deletions,
n??
p=1
I(q?p = M) >
n??
p=1
I(q?p = IG) (3)
where I is an indicator function.
The final abbreviation prediction is given by the
portions of the x and y strings that are associated
with a?, named x? and y?, respectively. These may be
truncated compared to x and y, as leading alignment
gaps are removed. The final alignment probability is
given by p(a?|x?, y?, pi, ?).
3.4 Substitution Edit Operations
In contrast to rule-based extraction algorithms, in
our model, it is easy to introduce new types of edit
operations, and adjust the model to recognize a va-
riety of abbreviation types. As an example, we have
added a number of substitution operations (see Ta-
ble 3), including an operation for the commonly
used convention of replacing a chemical element
name (e.g., Sodium) with its symbol (Na). These
types of operations are not available using simpler
models, such as that presented by Schwartz and
Hearst (2002), making it impossible to recognize
some important biomedical entities, such as chem-
ical compounds (e.g., ?NaB, SodiumButyrate?).
In contrast, such additions are natural in our model.
4 Evaluation
4.1 Abbreviation Extraction Analysis
We evaluated the alignment abbreviation model over
two data sets (Table 4). The method was tuned using
52
Data Set Name Abstracts Abbreviations Testing Method
Development (D) Medstract 400 483 10-fold cross validation.
Validation (V) PubMed Sample 50 76 Training on set D and testing on set V.
Table 4: Evaluation Data Sets.
Model D (average %) V (%)
P R F1 P R F1
Alignment HMM 98 93 96 95 91 93
SH 96 88 91 97 83 89
Chang 0.88 99 46 62 97 47 64
Chang 0.14 94 89 91 95 91 93
Chang 0.03 92 91 91 88 93 90
Chang 0 49 92 64 53 93 67
Table 5: Results on validation (V) and development (D)
sets. Average results are shown for D set, which was
tested using 10-fold cross-validation (results rounded to
nearest percent, all standard deviations were < 0.1)
10 fold cross-validation over the publicly available
Medstract corpus (Pustejovsky et al, 2002) which
includes 400 Medline abstracts. The online version
of the corpus was missing the Gold Standard annota-
tions throughout the development of our algorithm,
nor was it possible to get them through communica-
tion with the authors. We therefore hand-annotated
the Medstract data, yielding 483 abbreviation defi-
nitions in the form of ?short form, long form? pairs.
In order to be consistent with previous evaluations
over Medstract, our annotations include only defini-
tions in which either the short or the long form ap-
pear in parenthesis, and it is assumed that there are
no trailing gaps in the term preceding the parenthe-
sis, although our model does detect such gaps.
We compare our results with two algorithms
available for download: the Schwartz and Hearst
(SH; (2002)) algorithm1, and the Chang et al (2002)
algorithm2 used at three score cutoffs reported in
their paper (0.88, 0.14, 0.03). We also use a fourth
score cutoff of 0 to account for any legal alignments
produced by the Chang model.
In Table 5 we report precision (P), recall (R) and
1Taken from http://biotext.berkeley.edu/software.html
2Taken from http://abbreviation.stanford.edu
F1 scores for all methods, calculated by
P =
correct predicted abbreviations
all predicted abbreviations
(4)
R =
correct predicted abbreviations
all correct abbreviations
(5)
On the development set, our alignment model
achieves 98% precision, 93% recall and 96% F1 (av-
erage values over cross-validation iterations, with
standard deviations all under 0.03).
To test the final model we used a validation
dataset consisting of 50 abstracts, randomly selected
out of a corpus of 200K full-text biomedical articles
taken from the PubMed Central Open Access Sub-
set (extracted in October 2010)3. These were hand-
annotated, yielding 76 abbreviation definitions.
On the validation set, we predicted 69 out of 76
abbreviations, with 4 false predictions, giving 95%
precision, 91% recall and 93% F1. Our alignment
model results in higher F1 score over all baselines
in both datasets (with Chang0.14 giving equal results
on the validation set). Our results are most compa-
rable with the Chang model at a score cutoff of 0.14,
though our model does not require selecting a score
cutoff, and as we will show, it is considerably faster.
Interestingly, our model results in lower recall than
precision on both data sets. This may be due to a
limited scope of edit operations.
In order to evaluate the usability of our method,
we used it to scan the 200K full-text documents of
the PubMed Central Open Access Subset corpus.
The process completed in under 3 hours, yielding
over 1.4 million abbreviations, including 455,844
unique definitions. A random sample of the ex-
tracted abbreviations suggests a low rate of false
positive predictions.
4.2 Error Analysis
Our model makes 4 incorrect predictions on the val-
idation set, 3 of which are partial matches to the
3http://www.ncbi.nlm.nih.gov/pmc/
53
Description D V
Letters in short form are missing (e.g., ?GlyRalpha2, glycine alpha2?) 5 3
Abbreviation missed due to extraction rules. 6 1
Abbreviation is a synonym (e.g., ?IRX-2, natural cytokine mixture?) 5 1
Abbreviation letters are out-of-order (e.g., ?VSV-G, G glycoprotein of vesicular stomatitis virus?) 4 1
Correct alignment was found but it is invalid due to many inner gaps (see Section 3.3.1). 5 0
Abbreviations of chemical formulas or compounds. 4 0
Table 6: Abbreviations missed in development (D) and validation (V) sets.
correct definitions, e.g., we predict the pair ?GlOx,
glutamate oxidase? instead of ?GlOx, L-glutamate
oxidase?. On the development set, 3 out of 5 incor-
rect predictions are partial matches.
Our model did not extract 7 of the abbreviations
from the validation set and 33 from the development
set. Many of these abbreviations (6 from the valida-
tion set and 29 from the development set) had one
of the properties described in Table 6. The remain-
ing 5 definitions have been missed due to miscel-
laneous issues. Note that while we added several
substitution operations for chemical formula recog-
nition, the elaborate set of operations required for
recovering the full range of chemical formulas was
not included in this work, leading to 4 chemical for-
mula abbreviations being missed.
4.3 Run-Time Analysis
We provide an estimated comparison of the run
time of our method and the baseline algorithms.
This analysis is especially interesting for cases in
which an abbreviation extraction model is included
within a larger learning framework (Chowdhury et
al., 2010; Huang et al, 2011), and may be used in
it in an online fashion. Run time was evaluated on
an Apple iMac with 4GB 1333 MHz RAM, and a
3.06 GHz Core i3, double-core processor, by run-
ning all models on a random set of 400 abstracts.
In order to evaluate the run time contribution of the
substitution operations introduced in our model we
ran it both with (88 docssec ) and without (98
docs
sec ) the
use of substitution operations. We find that using
substitutions did not have considerable effect on run
time, adding under 1 ms for processing each docu-
ment. We should note that the performance of the
substitution-less model on this test data was similar
to that of the original model, as substitutions were
relevant to only a smaller portion of the abbrevi-
ations. As expected, the SH algorithm is consid-
erably faster (6451 docssec ) than our model, as it is
based on only a number of simple rules. The Chang
model, however, is slower (4 docssec ) as it includes
processing steps following the discovery of an ab-
breviation alignment, which means that our model
provides comparable results to the Chang model and
runs an order-of-magnitude faster.
5 Conclusions and Discussion
We presented a method for extracting abbreviation
definitions with high precision and high recall (95%
precision, 91% recall and 93% F1 on a validation
set). Our model achieves higher F1 on both the de-
velopment and validation data sets, when compared
with two popular extraction methods.
Our approach is based on a sequential genera-
tive model, aligning the short and long form of an
abbreviation. Using the proposed method we ex-
tracted 1.4 million abbreviations from a corpus of
200K PubMed articles. This data can be valuable
for Information Extraction tasks and for the full un-
derstanding of biomedical scientific data.
The alignment abbreviation extractor can be eas-
ily extended by adding edit-operations over short
and long forms. This was demonstrated by including
substitutions of chemical elements and their sym-
bols, which facilitates recognition of chemical for-
mulas and compounds.
We have identified the main classes of abbrevia-
tion definitions missed by our approach. These in-
clude out-of-order matches, synonym-like abbrevia-
tions, and short forms with excess letters. It may be
possible to address some of these issues by includ-
ing ?global? information on abbreviations, such as
the occurrence of frequent definitions.
54
Acknowledgments
This work was funded by grant 1R101GM081293
from NIH, IIS-0811562 from NSF and by a gift from
Google. The opinions expressed in this paper are
solely those of the authors.
References
E. Adar. 2004. Sarad: A simple and robust abbreviation
dictionary. Bioinformatics, 20(4):527?533.
JL Ambrus. 1987. Acronyms and abbreviations. Journal
of medicine, 18(3-4):134.
H. Ao and T. Takagi. 2005. Alice: an algorithm to extract
abbreviations from medline. Journal of the American
Medical Informatics Association, 12(5):576?586.
M. Bilenko and R.J. Mooney. 2003. Adaptive duplicate
detection using learnable string similarity measures.
In Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 39?48. ACM.
P.R. Bowden, P. Halstead, and T.G. Rose. 1997. Dic-
tionaryless english plural noun singularisation using
a corpus-based list of irregular forms. LANGUAGE
AND COMPUTERS, 20:339?352.
J.T. Chang, H. Schu?tze, and R.B. Altman. 2002. Cre-
ating an online dictionary of abbreviations from med-
line. Journal of the American Medical Informatics As-
sociation, 9(6):612?620.
M. Chowdhury, M. Faisal, et al 2010. Disease mention
recognition with specific features. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, pages 83?90. Association for Computa-
tional Linguistics.
H.L. Fred and T.O. Cheng. 2003. Acronymesis: the
exploding misuse of acronyms. Texas Heart Institute
Journal, 30(4):255.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in med-
line. Bioinformatics, 21(18):3658?3664.
M. Huang, J. Liu, and X. Zhu. 2011. Genetukit: a soft-
ware for document-level gene normalization. Bioin-
formatics, 27(7):1032?1033.
L.S. Larkey, P. Ogilvie, M.A. Price, and B. Tamilio.
2000. Acrophile: an automated acronym extractor and
server. In Proceedings of the fifth ACM conference on
Digital libraries, pages 205?214. ACM.
H. Liu, C. Friedman, et al 2003. Mining terminological
knowledge in large biomedical corpora. In Pac Symp
Biocomput, pages 415?426.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In Conference on Uncer-
tainty in AI (UAI).
Y. Park and R.J. Byrd. 2001. Hybrid text mining for find-
ing abbreviations and their definitions. In Proceedings
of the 2001 conference on empirical methods in natu-
ral language processing, pages 126?133.
J. Pustejovsky, J. Castano, R. Sauri, A. Rumshinsky,
J. Zhang, and W. Luo. 2002. Medstract: creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the ACL-02 workshop
on Natural language processing in the biomedical
domain-Volume 3, pages 85?92. Association for Com-
putational Linguistics.
E.S. Ristad and P.N. Yianilos. 1998. Learning string-edit
distance. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 20(5):522?532.
A.S. Schwartz and M.A. Hearst. 2002. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing 2003: Kauai, Hawaii, 3-7 January 2003, page 451.
World Scientific Pub Co Inc.
K. Taghva and J. Gilbreth. 1999. Recognizing acronyms
and their definitions. International Journal on Docu-
ment Analysis and Recognition, 1(4):191?198.
H. Yu, G. Hripcsak, and C. Friedman. 2002. Map-
ping abbreviations to full forms in biomedical articles.
Journal of the American Medical Informatics Associa-
tion, 9(3):262?272.
H. Yu, W. Kim, V. Hatzivassiloglou, and W.J. Wilbur.
2007. Using medline as a knowledge source for dis-
ambiguating abbreviations and acronyms in full-text
biomedical journal articles. Journal of biomedical in-
formatics, 40(2):150?159.
W. Zhou, V.I. Torvik, and N.R. Smalheiser. 2006. Adam:
another database of abbreviations in medline. Bioin-
formatics, 22(22):2813.
55
