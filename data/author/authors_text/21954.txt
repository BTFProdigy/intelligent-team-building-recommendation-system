Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 333?336,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
Edinburgh-LTG: TempEval-2 System Description
Claire Grover, Richard Tobin, Beatrice Alex and Kate Byrne
University of Edinburgh
Edinburgh, United Kingdom
{grover, richard, balex, kbyrne3}@inf.ed.ac.uk
Abstract
We describe the Edinburgh information
extraction system which we are currently
adapting for analysis of newspaper text
as part of the SYNC3 project. Our most
recent focus is geospatial and temporal
grounding of entities and it has been use-
ful to participate in TempEval-2 to mea-
sure the performance of our system and to
guide further development. We took part
in Tasks A and B for English.
1 Background
The Language Technology Group (LTG) at Edin-
burgh has been active in the field of information
extraction (IE) for a number of years. Up until re-
cently our main focus has been in biomedical IE
(Alex et al, 2008) but we have also been pursuing
projects in other domains, e.g. digitised histori-
cal documents (Grover et al, 2010) and we are
currently participants in the EU-funded SYNC3
project where our role is to analyse news arti-
cles and establish spatio-temporal and other re-
lations between news events. As a step towards
this goal, we have been extending and adapting
our IE pipeline to ground spatial and temporal en-
tities. We have developed the Edinburgh Geop-
arser for georeferencing documents and have eval-
uated our system against the SpatialML corpus,
as reported in Tobin et al (2010). We are cur-
rently in the process of developing a rule-based
date and time grounding component and it is this
component that we used for Task A, which re-
quires systems to identify the extents of tempo-
ral named entities and provide their interpreta-
tion. The TempEval-2 data also contains event en-
tities and we have adapted the output of our in-
house chunker (Grover and Tobin, 2006) to iden-
tify events for Task B, which requires systems to
identify event denoting words and to compute a
range of attributes for them. In future work we will
adapt our machine-learning-based relation extrac-
tion component (Haddow, 2008) to recognise re-
lations between spatial and temporal entities and
event entities along the lines of the linking tasks.
2 The Edinburgh IE System
Our IE system is a modular pipeline system built
around the LT-XML2
1
and LT-TTT2
2
toolsets.
Documents are converted into our internal doc-
ument format and are then passed through a se-
quence of linguistic components which each add
XML mark-up. Early stages identify paragraphs,
sentences and tokens. Part-of-speech (POS) tag-
ging is done using the C&C tagger (Curran and
Clark, 2003a) and lemmatisation is done using
morpha (Minnen et al, 2000).
We use both rule-based and machine-learning
named entity recognition (NER) components, the
former implemented using LT-TTT2 and the lat-
ter using the C&C maximum entropy NER tagger
(Curran and Clark, 2003b). We are experiment-
ing to find the best combination of the two dif-
ferent NER views but this is not an issue in the
case of date and time entities since we have taken
the decision to use the rule-based output for these.
The main motivation for this decision arises from
the need to ground (provide temporal values for)
these entities and the rules for the grounding are
most naturally implemented as an elaboration of
the rules for recognition.
Our IE pipeline also uses the LT-TTT2 chun-
ker to provide a very shallow syntactic analysis.
Figure 1 shows an example of the results of pro-
cessing at the point where the rule-based NER
and chunker have both applied. As can be seen
from Figure 1, a positive feature for TempEval-
2 is that the verb group analysis provides in-
formation about tense, aspect, voice, modality
and polarity which translate relatively straightfor-
wardly into the Task B attributes. The noun group
analysis provides verbal stem information (e.g.
1
www.ltg.ed.ac.uk/software/ltxml2
2
www.ltg.ed.ac.uk/software/lt-ttt2
333
<s id="s1">
<ng>
<w p="DT" id="w13">The</w>
<w p="NN" id="w17" l="announcement" vstem="announce" headn="yes">announcement</w>
</ng>
<vg tense="pres" voice="pass" asp="simple" modal="yes" neg="yes">
<w p="MD" id="w30" pws="yes" l="must" neg="yes">must</w>
<w p="RB" id="w35" pws="yes" neg="yes">not</w>
<w p="VB" id="w39" pws="yes" l="be">be</w>
<w p="VBN" id="w42" pws="yes" l="make" headv="yes">made</w>
</vg>
<ng>
<timex unit="day" trel="same" type="date" id="rb1">
<w unit="day" trel="same" p="NN" id="w47" l="today">today</w>
</timex>
</ng>
<w p="." id="w52" sb="true">.</w>
</s>
Figure 1: Example of NER tagger and chunker output for the sentence ?The announcement must not be
made today.?
vstem="announce") about nominalisations.
Various attributes are computed for <timex>
elements and these are used by a temporal resolu-
tion component to provide a grounding for them.
The final output of the IE pipeline contains entity
mark-up in ?standoff? format where the entities
point at the word elements using ids. The date
and event entities for ?made? and ?today? are as
follows:
<ent tense="pres" voice="pass" neg="yes"
modal="yes" asp="simple" id="ev1"
subtype="make" type="event">
<parts>
<part ew="w39" sw="w39">made</part>
</parts>
</ent>
<ent wdaynum="5" day="Friday" date="16"
month="4" year="2010" unit="day"
day-number="733877" trel="same"
type="date" id="rb1">
<parts>
<part ew="w47" sw="w47">today</part>
</parts>
</ent>
The date entity has been grounded with respect
to the date of writing (16th April 2010). To do the
grounding we calculate a day-number value for
each date where the day number count starts from
1st January 1 AD. Using this unique day number
we are able to calculate the date for any given day
number as well as the day of the week. We use
the day number to perform simple arithmetic to
ground date expressions such as ?last Monday?,
?the day after tomorrow? etc. Grounding informa-
tion is spread across the attributes for day, date,
month and year. A fully grounded date has a
value for all of these while an underspecified date,
e.g. ?2009?, ?March 13th?, ?next year?, etc., only
has values for some of these attributes.
3 Adaptations for TempEval-2
Our system has been developed independently of
TimeML or TempEval-2 and there is therefore a
gap between what our system outputs and what is
contained in the TempEval-2 data. In order to run
our system over the data we needed to convert it
into our XML input format while preserving the
tokenisation decisions from the original. Certain
tokenisation mismatches required that we extend
various rules to allow for alternative token bound-
aries, for example, we tokenise ?wasn?t? as was +
n?t whereas the TempEval-2 data contains was
+ n + ?t or occasionally wasn + ?t.
Other adaptations fall broadly into two classes:
extension of our system to cover entities in
TempEval-2 that we didn?t previously recognise,
and mapping of our output to fit TempEval-2 re-
quirements.
3.1 Extensions
The date and time entities that our system recog-
nises are more like the MUC7 TIMEX entities
(Chinchor, 1998) than TIMEX3 ones. In partic-
ular, we have focused on dates which can either
be fully grounded or which, though underspeci-
fied, can be grounded to a precise range, e.g. ?last
month? can be grounded to a particular month and
year given a document creation date and it can be
precisely specified if we take it to express a range
from the first to last days of the month. TIMEX3
entities can be vaguer than this, for example, en-
tities of type DURATION such as ?twenty years?,
?some time?, etc. can be recognised as denoting
a temporal period but cannot easily be grounded.
To align our output more closely to TempEval-
2, we added NER rules to recognise examples
334
such as ?a long time?, ?recent years?, ?the past?,
?years?, ?some weeks?, ?10 minutes?. In addition
we needed to compute appropriate information to
allow us to create TempEval-2 values such as P1W
(period of 1 week).
For event recognition, our initial system created
an event entity for every head verb and for ev-
ery head noun which was a nominalisation. This
simple approach goes a long way towards captur-
ing the TempEval-2 events but results in too many
false positives and false negatives for nouns. In
addition our system did not calculate the informa-
tion needed to compute the TempEval-2 class at-
tribute. To help improve performance we added
attributes to potential event entities based on look-
up in lexicons compiled from the training data and
from WordNet (Fellbaum, 1998). These attributes
contribute to the decision as to whether a noun
or verb chunk head should be an event entity or
not
3
. The lexicons derived from the training data
contain the stems of all the nouns which acted
more than once as events as well as information
about those predicates which occurred more than
once as class ASPECTUAL, I STATE, REPORT-
ING or STATE in the training data. Where look-
up succeeds for event, if class look-up also suc-
ceeds then the class attribute is set accordingly. If
class look-up fails, the default, OCCURRENCE,
is used. The WordNet derived lexicon contains in-
formation about whether the first sense of a noun
has event or state as a hypernym. As a result of the
lexical look-up stage, the noun ?work?, for exam-
ple, is marked as having occurred in the training
data as an event and as having event as a hyper-
nym for its first sense. The conjunction of these
cause it to be considered to be an event entity. For
verbs, the only substantive change in our system
was to not consider as events all main verb uses
of ?be? (be happy), ?have? (have a meal) and ?do?
(do the dishes).
3.2 Mapping
For both timex and event entities the creation
of the extents files was a straightforward map-
ping. For the creation of the attributes files,
on the other hand, we used stylesheets to con-
struct appropriate values for the TempEval-2 at-
tributes based on the attributes in our output
XML. The construction of event attributes is not
overly complex: for example, where an event
entity is specified as tense="nonfin" and
3
Our system does not recognise adjective events. How-
ever, passive participles, which are sometimes treated as ad-
jectives in TempEval-2, are frequently treated as verbs in our
system and are therefore recognised.
voice="pass" the TempEval-2 tense attribute
is given the value PASTPART. For modality our
attribute only records whether a modal verb is
present or not, so it was necessary to set the
TempEval-2 modality attribute to the actual modal
verb inside the verb group.
For timex entities, a single value for the value
attribute had to be constructed from the values
of a set of attributes on our entity. For example,
the information in date="16", month="4"
year="2010" has to be converted to 2010-04-
16. For durations other attributes provide the rel-
evant information, for example for ?two days? the
attributes unit="day", quty="2" are used
to create the value P2D (period of 2 days).
4 Evaluation and Error Analysis
The recognition results for both timex and event
extents are shown in Table 1. For Task A (timex)
we achieved a close balance between precision and
recall, while for Task B (events) we erred towards
recall at some cost to precision.
Task Precision Recall F1
Task A 0.85 0.82 0.84
Task B 0.75 0.85 0.80
Table 1: Extent Results
For timex entities our false negatives were all
entities of the vaguest kind, for example, ?10-
hour?, ?currently?, ?third-quarter?, ?overnight?,
?the week?: these are ones which the original sys-
tem did not recognise and for which we added ex-
tra rules, though evidently we were not thorough
enough. The false positives were mostly of the
kind that would usually be a date entity but which
were not considered to be so in the key, for exam-
ple, ?1969?, ?Oct 25?, ?now?, ?the past?, ?a few
days?. In two cases the system mistakenly identi-
fied numbers as times (?1.02?, ?2.41?).
For event entities we had 73 false negatives.
Some of these were caused by verbs being
mistagged as nouns (?complies?, ?stretch?, ?suit?)
while others were nouns which didn?t occur in
the WordNet derived lexicon as events. There
were 143 event false positives. Some of these
are clearly wrong, for example, ?destruction? in
?weapons of mass destruction? while others are
a consequence of the subtle distinctions that the
TempEval-2 guidelines make and which our shal-
low approach cannot easily mimic.
Table 2 shows the results for attribute detec-
tion for both tasks. In the case of timex attributes
335
Task Attribute Score
Task A type 0.84
value 0.63
Task B polarity 0.99
pos 0.97
modality 0.99
tense 0.92
aspect 0.98
class 0.76
Table 2: Attribute Results
there was a set of entities which had systematically
wrong values for both type and value: these were
dates such as ?this week? and ?last week?. These
should have had DATE as their type and a value
such as 1998-W19 to indicate exactly which week
in which year they denote. Our date grounding
does not currently cover the numbering of weeks
in a year and so it would not have been possible
to create appropriate values. Instead we incor-
rectly treated these entities as being of type DU-
RATION with value P1W. Many of the remaining
errors were value errors where the system resolved
relative dates as past references when they should
have been future or vice versa. For example, the
value for ?Monday? in ?He and Palestinian leader
Yasser Arafat meet separately Monday with ...?
should have been 1998-05-04 but our system in-
terpreted it as the past Monday, 1998-04-27. There
were a few cases where the value was correct but
insufficient, for example for ?a year ago? the sys-
tem returned 1988 when it should have produced
1988-Q3.
Our scores for event attributes were high for all
attributes except for class. The high scoring at-
tributes were derived from the output of our chun-
ker and demonstrate the quality of this component.
There does not appear to be a particular pattern
behind the small number of errors for these at-
tributes except that errors for the pos attribute re-
flect POS tagger errors and there were some com-
bined tense and modality errors where ?will? and
?would? should have been interpreted as future
tense but were instead treated as modals. The class
attribute represents information that our system
had not previously been designed to determine.
We computed the class attribute in a relatively
minimal way. Since the class value is OCCUR-
RENCE in nearly 60% of events in the training
data, we use this as the default but, as described in
Section 3, we override this for events which are in
our training data-derived lexicon as REPORTING,
ASPECTUAL, I STATE or STATE. We do not at-
tempt to assign the I ACTION class value and
nearly half of our class errors result from this. An-
other set of errors comes from missing REPORT-
ING events such as ?alleging?, ?telegraphed? and
?acknowledged?.
Acknowledgements
The current phase of development of the Ed-
inburgh IE system is supported by the SYNC3
project (FP7-231854)
4
.
References
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Richard
Tobin, and Xinglong Wang. 2008. Automating cu-
ration using a natural language processing pipeline.
Genome Biology, 9(Suppl 2).
Nancy A. Chinchor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7).
Fairfax, Virginia.
James R. Curran and Stephen Clark. 2003a. Inves-
tigating GIS and smoothing for maximum entropy
taggers. In Proceedings of the 11th Meeting of the
European Chapter of the Association for Compu-
tational Linguistics (EACL-03), pages 91?98. Bu-
dapest, Hungary.
James R. Curran and Stephen Clark. 2003b. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, Edmonton, Alberta, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC 2006).
Claire Grover, Richard Tobin, Kate Byrne, Matthew
Woollard, James Reid, Stuart Dunn, and Julian Ball.
2010. Use of the Edinburgh geoparser for georefer-
encing digitised historical collections. Phil. Trans.
R. Soc. A.
Barry Haddow. 2008. Using automated feature op-
timisation to create an adaptable relation extraction
system. In Proc. of BioNLP 2008, Columbus, Ohio.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference, Mitzpe Ramon, Israel.
Richard Tobin, Claire Grover, Kate Byrne, James Reid,
and Jo Walsh. 2010. Evaluation of georeferencing.
In Proceedings of Workshop on Geographic Infor-
mation Retrieval (GIR?10).
4
http://www.sync3.eu/
336
LAW VIII - The 8th Linguistic Annotation Workshop, pages 59?63,
Dublin, Ireland, August 23-24 2014.
A Web-based Geo-resolution Annotation and Evaluation Tool
Beatrice Alex, Kate Byrne, Claire Grover and Richard Tobin
School of Informatics
University of Edinburgh
{balex,kbyrne3,grover,richard}@inf.ed.ac.uk
Abstract
In this paper we present the Edinburgh Geo-annotator, a web-based annotation tool for the manual
geo-resolution of location mentions in text using a gazetteer. The annotation tool has an inter-
linked text and map interface which lets annotators pick correct candidates within the gazetteer
more easily. The geo-annotator can be used to correct the output of a geoparser or to create
gold standard geo-resolution data. We include accompanying scoring software for geo-resolution
evaluation.
1 Introduction
Many kinds of digitised content have an important geospatial dimension. However not all geospatial
information is immediately accessible, particularly in the case where it is implicit in place names in text.
The process of geo-resolution (also often referred to as geo-referencing, geoparsing or geotagging) links
instances of textual geographic information to location coordinates, enabling searching and linking of
digital content using its geospatial properties.
Geo-resolution tools can never be completely accurate and their performance can vary significantly
depending on the type and quality of the input texts as well as on the gazetteer resources they consult.
For this reason, users of text collections are frequently disappointed in the results of geo-resolution and,
depending on their application and dataset size, they may decide to take remedial action to improve
the quality. The tool we describe here is a web-based, manual annotation tool which can be used to
correct the output of geo-resolution. It has been developed in step with our geo-resolution system, the
Edinburgh Geoparser (Grover et al., 2010), but it could also be used to correct the output of other tools.
In our work, we use the geo-annotator to create gold-standard material for geo-resolution evaluation and
have produced accompanying scoring software.
1
2 Related Work
Within the field of NLP, SpatialML is probably the best known work in the area of geo-referencing.
SpatialML is an annotation scheme for marking up natural language references to places and grounding
them to coordinates. The SpatialML corpus (Mani et al., 2008) instantiates this annotation scheme and
can be used as an evaluation corpus for geo-resolution (Tobin et al., 2010). Other researchers develop
their own geo-annotated corpora and evaluate against these, e.g. Clough (2005), Leidner (2007).
Within the field of Information Retrieval, there is an ACM special interest group on spatially-related
information, SIGSPATIAL
2
, with regular geographic IR conferences (GIR conferences) where geo-
referencing research is presented, see for example Purves et al. (2007).
There are currently several geoparsing tools available, such as GeoLocate
3
, and CLAVIN
4
, as well as
our own tool, the Edinburgh Geoparser. All of these enable users to geo-reference text collections but do
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The Edinburgh Geo-annotator will be available at http://www.ltg.ed.ac.uk.
2
http://www.sigspatial.org/
3
http://www.museum.tulane.edu/geolocate/
4
http://clavin.bericotechnologies.com/
59
not address the question of how to interact with the geo-annotations in order to correct them, nor do they
assist in creating evaluation materials for particular text collections.
The Edinburgh Geo-annotator has been developed in tandem with the Edinburgh Geoparser and ear-
lier versions have been used in the GeoDigRef project (Grover et al., 2010) to create evaluation data
for historical text collections as well as in the botanical domain (Llewellyn et al., 2012; Llewellyn et
al., 2011) where we adapted it to allow curators to geo-reference the textual metadata associated with
herbarium specimens. The current version has also been used to create gold standard data for Trading
Consequences, a historical text mining project on mining location-centric trading information relevant to
the nineteenth century (Klein et al., 2014). The Pelagios project, which deals with texts about the ancient
world, has recently developed Recogito
5
, a geo-resolution correction tool similar to our own.
3 Annotation Tool
The Edinburgh Geo-annotator is a geo-resolution annotation tool which can be used to correct geo-
resolution output or to create manually annotated gold standard data for evaluating geo-resolution al-
gorithms and tools. The geo-annotator has a web-based interface allowing easy off-site annotation in
inter-disciplinary projects by domain experts (who are not always necessarily the developers of the geo-
referencing software).
6
The interface allows users to select documents from a collection of prepared files
containing annotated location entity mentions. By selecting and loading a document, the user can see its
textual content and the location mentions highlighted within it.
The current tool is set up to select locations from a set of location candidates retrieved from GeoNames
and visualised by pins on a Google Maps (v3) window. However, it can be configured to use candidates
from a different location gazetteer. There are two files associated with each document: (1) an HTML
file which contains the text of the document and (2) an XML file which contains the candidates for each
location mention in the text and in which the annotations are stored. Candidates are linked to location
mentions via identifiers.
All location mentions displayed in the text interface are highlighted in colour (see Figure 1). Those in
red (e.g. Dublin) have one or more potential candidates in the gazetteer, while those in blue (e.g. British
Empire) do not have candidate entries in the gazetteer. There are a number of reasons why a mention
does not have a gazetteer entry. For example, the mention might be an old name of a location which is
not stored in the gazetteer, or the mention contains a misspelling. During the annotation phase, the user
is instructed to go through the red location mentions in the text and select the appropriate candidate.
In some cases there is only one candidate that can be selected (see Figure 2). The user can zoom to
the correct location pin which when selected shows a popup with the relevant gazetteer information for
that entry. The user can choose this candidate by pressing either ?Select for this mention? if the choice
is specific to the selected mention or ?Select for all mentions? if the selection can be propagated for all
mentions with the same string in the document. Once a pin is selected, it and the location mention in the
text turn green. To undo a selection, the user can click on a green pin and press either ?Deselect for this
mention? or ?Deselect for all mentions?.
In other cases, there are many candidates to choose from. For example, when clicking on the first
location mention (Dublin) shown in Figure 1, the map adjusts to the central point of all 42 candidate
locations. When reading a piece of text, human beings can often easily understand which location a
place name refers to based on the context it appears in, which means that choosing between multiple
candidates manually is not expected to be a difficult task. However, the number of location candidates
that are suggested by GeoNames and consequently displayed in the interface can be limited in the data
files, if for example the user only wants to choose between a small number of candidates.
In the case of Dublin (see Figure 1), the user would then zoom into the correct Dublin to select a
candidate and discover that there are two pins which are relevant, Dublin ? the capital, and Baile
?
Atha
Cliath ? the Gaelic name for Dublin and its gazetteer entry referring to the administrative division (see
Figure 3). The gazetteer information in the popup can assist the user to make a choice. In this case, it
is clear from the context that the text refers to the capital. It might not always be as clearcut to choose
5
http://pelagios-project.blogspot.co.at/2014/01/from-bordeaux-to-jerusalem-and-back.
html
6
The geo-annotator is run via a javascript programme which calls an update.cgi script on the server side to write the saved
data to file. We have tested it in Safari, Firefox and Chrome.
60
Figure 1: When an example location mention (e.g. Dublin) is clicked the map adjusts to show all potential
location candidates that exist in the gazetteer for this place name.
between multiple candidates. In such cases, it is important that the annotation guidelines provide detailed
instruction as to which type of gazetteer entry to prefer.
If none of the candidates displayed on the map are correct, then the user must mark this by pressing
?This mention? (or ?All mentions?) in the box located at the top of right corner of the map (see Figure 1).
Once there are only green or blue location mentions left in the text, the annotation for the selected docu-
ment is complete and the user should press ?Save Current Document? and move to the next document in
the collection.
4 Geo-resolution Evaluation
It is important to be able to report the quality of a geo-resolver?s performance in concrete and quantifi-
able terms. Along with the annotation tool, we are therefore also releasing an evaluation script which
compares the manually geo-resolved locations to those predicted by an automatic geoparser.
7
We follow
standard practice in comparing system output to hand-annotated gold standard evaluation data. The script
evaluates the performance of the geo-resolution independently from geo-tagging, meaning that it only
considers named entities which were tagged in the input to the manual geo-resolution annotation but not
those that were missed. It is therefore preferable to use input data which contains manually annotated or
corrected location mentions.
The evaluation script computes the number of correctly geo-resolved locations and accuracy in percent.
Both figures are presented for a strict evaluation of exact match against gazetteer identifier and for a lax
evaluation where the grid references of the gold and the system choice have to occur within a small
distance of one another to count as a match. For a pair of location candidates (gold vs. system), we
compute the Great-circle distance using a special case of the Vincenty formula which is most accurate
for all distances.
8
The lax evaluation is provided as even with clear annotation guidelines, annotators
7
We provide Mac and Linux binaries of the evaluation scripts.
8
For the exact formula, see: http://en.wikipedia.org/wiki/Great-circle_distance
61
Figure 2: Example candidate for the location mention River Liffey and its gazetteer entry information
shown in a popup.
Figure 3: Choosing between multiple candidates for the same location mention.
can find it difficult to chose between different location types for essentially the same place (e.g. see the
example for Dublin in Figure 3).
During the manual annotation, three special cases can arise. Some location mentions do not have a
candidate in the gazetteer (those appearing in blue), while others do have candidates in the gazetteer but
the annotator does not consider any of them correct. Occasionally there are location mentions with one
or more candidates in the gazetteer but an annotator neither chooses one of them nor selects ?none?. The
latter cases are considered to be annotation errors, usually because the annotator has forgotten to resolve
them. The evaluation excludes all three cases when computing accuracy scores but notes them in the
evaluation report in order to facilitate error analysis (see sample output in Figure 4).
total: 11 exact: 10 (90.9\%) within 6.0km 11 (100.0\%)
note: no gold choice for British Empire
note: annotator selected "none" for Irish Free State
Figure 4: Sample output of the geo-resolution evaluation script. When setting the lax evaluation to 6km,
one candidate selected by the system was close enough to the gold candidate to count as a match.
5 Summary
We have presented a web-based manual geo-resolution annotation and evaluation tool which we are
releasing to the research community to facilitate correction of automatic geo-resolution output and eval-
uation of geo-resolution algorithms and techniques. In this paper we introduce the annotation tool and its
main functionalities and describe two geo-resolution evaluation metrics with an example, namely strict
and lax accuracy scoring. The release will contain more detailed documentation of the configuration and
installation process and the document formats for the textual input and candidate gazetteer entries.
62
References
Paul Clough. 2005. Extracting metadata for spatially-aware information retrieval on the internet. In Proceedings
of Workshop on Geographic Information Retrieval (GIR?05).
Claire Grover, Richard Tobin, Kate Byrne, Matthew Woollard, James Reid, Stuart Dunn, and Julian Ball. 2010.
Use of the Edinburgh geoparser for georeferencing digitised historical collections. Phil. Trans. R. Soc. A.
Ewan Klein, Beatrice Alex, Claire Grover, Richard Tobin, Colin Coates, Jim Clifford, Aaron Quigley, Uta Hin-
richs, James Reid, Nicola Osborne, and Ian Fieldhouse. 2014. Digging Into Data White Paper: Trading Conse-
quences.
Jochen L. Leidner. 2007. Toponym Resolution in Text: Annotation, Evaluation and Applications of Spatial
Grounding of Place Names. Ph.D. thesis, School of Informatics, University of Edinburgh.
Clare Llewellyn, Elspeth Haston, and Claire Grover. 2011. Georeferencing botanical data using text analysis tools.
In Proceedings of the Biodiversity Information Standards Annual Conference (TDWG 2011).
Clare Llewellyn, Claire Grover, Jon Oberlander, and Elspeth Haston. 2012. Enhancing the curation of botan-
ical data using text analysis tools. In Panayiotis Zaphiris, George Buchanan, Edie Rasmussen, and Fernando
Loizides, editors, Theory and Practice of Digital Libraries, volume 7489 of Lecture Notes in Computer Science,
pages 480?485. Springer Berlin Heidelberg.
Inderjeet Mani, Janet Hitzeman, Justin Richer, Dave Harris, Rob Quimby, and Ben Wellner. 2008. SpatialML:
Annotation scheme, corpora, and tools. In Proceedings of the Sixth International Language Resources and
Evaluation (LREC?08).
Ross S. Purves, Paul Clough, Christopher B. Jones, Avi Arampatzis, Benedicte Bucher, David Finch, Gaihua Fu,
Hideo Joho, Awase Khirni Syed, Subodh Vaid, and Bisheng Yang. 2007. The design and implementation
of SPIRIT: a spatially-aware search engine for information retrieval on the internet. International Journal of
Geographic Information Systems (IJGIS), 21(7).
Richard Tobin, Claire Grover, Kate Byrne, James Reid, and Jo Walsh. 2010. Evaluation of georeferencing. In
Proceedings of Workshop on Geographic Information Retrieval (GIR?10).
63
