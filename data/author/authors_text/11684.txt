Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 146?149,
Paris, October 2009. c?2009 Association for Computational Linguistics
Guessing the Grammatical Function of a Non-Root F-Structure in LFG
Anton Bryl
CNGL,
Dublin City University,
Dublin 9, Ireland
Josef van Genabith
CNGL,
Dublin City University,
Dublin 9, Ireland
{abryl,josef,ygraham}@computing.dcu.ie
Yvette Graham
NCLT,
Dublin City University,
Dublin 9, Ireland
Abstract
Lexical-Functional Grammar (Kaplan and
Bresnan, 1982) f-structures are bilexical
labelled dependency representations. We
show that the Naive Bayes classifier is able
to guess missing grammatical function la-
bels (i.e. bilexical dependency labels) with
reasonably high accuracy (82?91%). In
the experiments we use f-structure parser
output for English and German Europarl
data, automatically ?broken? by replacing
grammatical function labels with a generic
UNKNOWN label and asking the classifier
to restore the label.
1 Introduction
The task of labeling unlabelled dependencies, a
sub-task of dependency parsing task, can occur
in transfer-based machine translation (when only
an inexact match can be found in the training
data for the given SL fragment) or in parsing
where the system produces fragmented output. In
such cases it is often reasonably straightforward
to guess which fragments are dependent on which
other fragments (e.g. in transfer-based MT). What
is harder to guess are the labels of the dependen-
cies connecting the fragments.
In this paper we systematically investigate the
labelling task by automatically deleting function
labels from Lexical-Functional Grammar-based
parser output for German and English Europarl
data, and then restoring them using a Naive Bayes
classifier trained on attribute names and attribute
values of the f-structure fragments. We achieve
82% (German) to 91% (English) accuracy for both
single and multiple missing function labels.
The paper is organized as follows: in Section 2
we define the problem and the proposed solution
more formally. Section 3 details the experimental
evaluations, and in Section 4 we present our con-
clusions.
?
????????
PRED ?adopt?
UNKNOWN f1
?
????
PERS 3
NUM sg
PRED ?resolution?
SPEC f2
[ DET f3 [PRED ?the?]
]
?
????
SUBJ f4
?
?
PERS 3
NUM sg
PRED ?Parliament?
?
?
?
????????
Figure 1: Example of a ?broken? f-structure (sim-
plified). The sentence is ?Parliament adopted the
resolution.? The missing function of f1 is OBJ.
2 Guessing Unknown Grammatical
Functions
Let us introduce some useful definitions. By de-
pendent f-structure of the parent f-structure fP we
mean an f-structure fd which bears a grammati-
cal function within fP , or belongs to a set which
bears a grammatical function within fP . E.g., in
Figure 1 f2 is a dependent f-structure of f1. In this
paper we will not distinguish between these two
situations, but simply refer to multiple f-structures
bearing the same function within the same parent
for set-valued grammatical functions. C(?, fP )
denotes the number of dependent f-structures of
fP which bear the grammatical function ? in fP
(either directly or as members of a set).
Let us formalize the simple case when the gram-
matical function of only one dependent f-structure
is missing. Let FP be the set of f-structures which
have a dependent f-structure with an UNKNOWN la-
bel instead of the grammatical function. Let ? be
the set of all grammatical functions of the given
grammar. We need a guessing function G : FP ?
?, such that G(fP ) is a meaningful replacement
for the UNKNOWN label in fP . As the set ? is fi-
nite, the problem is evidently a classification task.
F-structures are characterized by attributes
some of which potentially carry information about
the f-structure?s grammatical function, even if
146
Language N-GF N-DEP AVG-DEP MIN-DEP MAX-DEP
English 24 9724 1.57 1 5
German 39 10910 1.55 1 5
Table 1: Data used in the evaluation. N-GF is the number of different grammatical functions occurring
in the dataset. N-DEP is the number of dependent f-structures in the test set. AVG-DEP, MIN-DEP,
MAX-DEP is the average, min. and max. number of dependant structures per parent in the test set.
we observe these attributes completely separately
from each other. For example, it seems likely
that an f-structure with an ATYPE attribute is an
ADJUNCT, while an f-structure which has CASE
is probably a SUBJ or an OBJ. Given this, Naive
Bayes appears to be a promising solution here. Be-
low we describe a way to adapt this classifier to the
problem of grammatical function guessing.
Let ?P ? ? be the set of grammatical functions
which are already present in fP . Let ? = {?1..?n}
be the set of features, and let X = {x1..xn} be
the values of these features for the f-structure fd
for which the function should be guessed. Then
the answer ?d is chosen as follows:
?d = arg max
???
(
p(?)MP (?)
n?
i=1
p(?i = xi|?)
)
(1)
MP (?) =
{
p(C(?, fP ) > 1), if ? ? ?P
1, otherwise (2)
where the probabilities are estimated from the
training data. Equation (2) states that if ? is al-
ready present in the parent f-structure, the proba-
bility of ? being set-valued is considered.
We propose two ways of building the feature
set ?. First, it is possible to consider the pres-
ence/absence of each particular attribute in fd as
a binary feature. Second, it is possible to con-
sider atomic attribute values as features as well.
To give a motivating example, in many languages
the value of CASE is extremely informative when
distinguishing objects from subjects. We use only
those atomic attribute values which do not rep-
resent words. E.g., NUM, PRED or NUM=sg are
features, while PRED=?resolution? is not a
feature. This distinction prevents the feature set
from growing too large and thus the probability
estimates from being too inaccurate.
If grammatical functions are missing for sev-
eral dependent f-structures, it is possible to use
the same approach, guessing the missing func-
tions one by one. In general, however, these de-
cisions will not be independent. To illustrate this,
let us consider a situation when the functions are
to be guessed for two dependent f-structures of
the same parent f-structure, OBJ being the correct
answer for the first and SUBJ for the second. If
the guesser returns SUBJ for the first of the two,
this answer will not only be incorrect, but also de-
crease the probability of the correct answer for the
second by decreasing MP (SUBJ) in Equation (1).
This suggests that in such cases maximization of
the joint probability of the values of all the miss-
ing functions may be a better choice.
3 Experimental Evaluation
We present two experiments which assess the ac-
curacy of the proposed approach and compare dif-
ferent variants of it in order to select the best, and
an additional one which assesses the usefulness of
the approach for practical machine translation.
3.1 Data Used in the Evaluation
For our experiments we used sentences from
the German-English part of the Europarl cor-
pus (Koehn, 2005) parsed into f-structures with
the XLE parser (Kaplan et al, 2002) using En-
glish (Riezler et al, 2002) and German (Butt et
al., 2002) LFGs. We parsed only sentences of
length 5?15 words. For the first two experiments,
we picked 2000 sentences for training and 1000
for testing for both languages. We ignored robust-
ness features (FIRST, REST), functions related to
c-structure constraints (MOTHER, LEFT SISTER,
etc.), and TOPIC. Of the remaining functions, we
considered only those occurring in the PREDs-
only part of f-structure. If a dependent f-structure
has multiple functions within the same parent f-
structure, only the first function occurring in the
description is considered. This does not unduely
influence the results, as the grammatical function
of an f-structure, after exclusion of TOPIC, carries
multiple labels in only about 2% of the cases in the
English data and about 1% in the German data. In
Table 1 we provide some useful statistics to help
the reader interpret the results of the experiments.
147
Language MF NB-CASE NB-N NB-N&V
English 36.3% 56.7% 85.6% 91.6%
German 23.4% 51.0% 74.8% 82.5%
Table 2: Experiment 1: Guessing a Single Miss-
ing Grammatical Function. MF is the pick-most-
frequent classifier. NB-CASE is Naive Bayes
(NB) with only CASE values used as features. NB-
N is NB with only attribute names used as fea-
tures. NB-N&V is NB with both attribute names
and atomic attribute values used as features.
3.2 Experiment 1: Guessing a Single Missing
Grammatical Function
The goal of this experiment is to evaluate the ac-
curacy of the Bayesian guesser in the case when
the grammatical function is unknown only for one
dependent f-structure, and to assess whether the
inclusion of attribute values into the feature set
improves the results, and whether attributes other
than CASE are useful.
Procedure. As a baseline, we used a pick-most-
frequent algorithm MF which considers only the
function?s prior probability and the presence of
this function in the parent (returning to Equations
(1) and (2), MF is in fact Naive Bayes with an
empty feature set ?). The guesser was evaluated
in three variants: NB-CASE with the feature set
formed only from the values of CASE attributes
(if the f-structure has no CASE feature, the classi-
fier degenerates to MF), NB-N with the feature set
formed only from attribute names, and NB-N&V
with the feature set formed from both attribute
names and values. All grammatical functions in
the test set were used as test cases. At each step in
the evaluation, one function was removed and then
guessed by each algorithm. For both languages the
test set was split into 10 non-intersecting subsets
with approximately equal numbers of grammati-
cal functions in each, and the values obtained for
the 10 subsets were further used to assess the sta-
tistical significance of the differences in the results
with the paired Student?s t-test.
Results. Table 2 presents the results. For both
English and German all the three versions of the
classifier clearly outperform the baseline, and even
the advantage of NB-CASE over the baseline is
statistically significant at the 0.5% level for both
languages. However, NB-CASE performs much
worse than NB-N and NB-N&V (their advantage
over NB-CASE is statistically significant at the
0.5% level for both languages), confirming that
Language MF NB-S NB-J
English 22.0% 90.4% 91.2%
German 17.1% 81.4% 82.1%
Table 3: Experiment 2: Guessing Multiple Miss-
ing Functions. MF is the pick-most-frequent clas-
sifier. NB-S and NB-J are one-by-one and join-
probability-based Naive Bayesian guessers.
CASE is not the only feature which is useful in
our task. The increase in accuracy brought about
by including the atomic attribute values into the
feature space is visible and significant at the same
level. The increase is somewhat more pronounced
for German than for English. For English the in-
clusion of attribute values into the feature space
affects primarily the accuracy of SUBJ vs. OBJ
decisions. For German, the accuracy notably in-
creases for telling SUBJ, OBJ and ADJ-GEN from
one another.
3.3 Experiment 2: Guessing Multiple
Missing Grammatical Functions
The goal of this experiment is to assess the accu-
racy of the Bayesian guesser for multiple miss-
ing grammatical functions within one parent f-
structure, and to compare the accuracy of one-
by-one vs. joint-probability-based guessing. Our
evaluation procedure models the extreme case
when the functions are unknown for all the depen-
dent f-structures of a particular parent.
Procedure. As a baseline, we use the same al-
gorithm MF as in Experiment 1, applied to the
missing grammatical functions one by one. Two
Bayesian guessers are evaluated, NB-S guessing
the missing grammatical functions one by one, and
NB-J guessing them all at once by maximizing
the joint probability of the values. Both Bayesian
guessers use attribute names and values as fea-
tures. All grammatical functions in the test set
were used as test cases. At each step of the ex-
periment, the grammatical functions of all the de-
pendent f-structures of a particular parent were
removed simultaneously, and then guessed with
each of the algorithms considered in this experi-
ment. Statistical significance was assessed in the
same way as in Experiment 1.
Results. Table 3 presents the accuracy scores.
The one-by-one guesser and the joint-probability-
based guesser perform nearly equally well, result-
ing in accuracy levels very close to those obtained
in Experiment 1 for f-structures with a single
148
missing function. Joint-probability-based guess-
ing achieves an advantage which is statistically
significant at the 0.5% level for both languages
but is not exceeding 1% absolute improvement.
For both languages errors typically occur in distin-
guishing OBJ vs. SUBJ and ADJUNCT vs. MOD,
and additionally in XCOMP vs. OBJ for English.
3.3.1 Experiment 3: Postprocessing the
Output of an MT Decoder
The goal of this experiment is to see how the
method influences the results of an SMT system.
Procedure. For this experiment we use the Sulis
SMT system (Graham et al, 2009), and a decoder,
which selects the transfer rules by maximizing the
source-to-target probability of the complete trans-
lation. Such a decoder, though simple, allows us
to create a realistic environment for evaluation.
From the f-structures produced by the decoder,
candidate sentences are generated with XLE, and
then the one best translation is selected for each
sentence using a language model. The function
guesser is used to postprocess the output of the
decoder before sentence generation. In the ex-
periment, the function guesser uses both attribute
names and values to make a guess. Guessing of
multiple missing functions is performed one-by-
one, as joint guessing complicates the algorithm
and leads to a very small improvement in accuracy.
The function guesser is trained on 3000 sentences,
which are a subset of the set used for inducing the
transfer rules. The overall MT system is evaluated
both with and without function guessing on 500
held-out sentences, and the quality of the transla-
tion is measured using the BLEU metric (Papineni
et al, 2002). We also calculate the number of sen-
tences for which the generator output is unempty.
Results. The system without function guesser
produced results for 364 sentences out of 500,
with BLEU score equal to 5.69%; with function
guesser the number of successfully generated sen-
tences increases to 433, with BLEU improving to
6.95%. Thus, the absolute increase of BLEU score
brought about by the guesser is 1.24%. This sug-
gests that the algorithm succeeds on real data and
is useful in grammar-based machine translation.
4 Conclusion
In this paper we addressed the problem of restor-
ing unknown grammatical functions in automati-
cally generated f-structures. We proposed to view
this problem as a classification task and to solve
it with the Naive Bayes classifier, using the names
and the values of the attributes of the dependent
f-structure to construct the feature set.
The approach was evaluated on English and
German data, and showed reasonable accuracy,
restoring the missing functions correctly in about
91% of the cases for English and about 82% for
German. It is tempting to interpret the differences
in accuracy for English and German as reflecting
the complexity of grammatical function assign-
ment for the two languages. It is not clear, how-
ever, whether the differences are due to differences
in the grammars or in the underlying data.
The experiments reported here use LFG-type
representations. However, nothing much in the
method is specific to LFG, and therefore we are
confident that our method also applies to other
dependency-based representations.
Acknowledgments
The research presented here was supported by Sci-
ence Foundation Ireland grant 07/CE2/I1142 un-
der the CNGL CSET programme.
References
M. Butt, H. Dyvik, T. H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In COLING?02, Workshop on Grammar Engineer-
ing and Evaluation.
Y. Graham, A. Bryl, and J. van Genabith. 2009. F-
structure transfer-based statistical machine transla-
tion. In LFG?09 (To Appear).
R. Kaplan and J. Bresnan. 1982. Lexical functional
grammar, a formal system for grammatical represe-
nation. The Mental Representation of Grammatical
Relations, pages 173?281.
R. M. Kaplan, T. H. King, and J. T. Maxwell III. 2002.
Adapting existing grammars: the XLE experience.
In COLING?02, Workshop on Grammar Engineer-
ing and Evaluation.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT Summit X, pages
79?86.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL?02, pages 311?318.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques.
In ACL?02, pages 271?278.
149
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172?176,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Testing for Significance of Increased Correlation with Human Judgment
Yvette Graham Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
graham.yvette@gmail.com, tb@ldwin.net
Abstract
Automatic metrics are widely used in ma-
chine translation as a substitute for hu-
man assessment. With the introduction
of any new metric comes the question of
just how well that metric mimics human
assessment of translation quality. This is
often measured by correlation with hu-
man judgment. Significance tests are gen-
erally not used to establish whether im-
provements over existing methods such as
BLEU are statistically significant or have
occurred simply by chance, however. In
this paper, we introduce a significance test
for comparing correlations of two metrics,
along with an open-source implementation
of the test. When applied to a range of
metrics across seven language pairs, tests
show that for a high proportion of metrics,
there is insufficient evidence to conclude
significant improvement over BLEU.
1 Introduction
Within machine translation (MT), efforts are on-
going to improve evaluation metrics and find bet-
ter ways to automatically assess translation qual-
ity. The process of validating a new metric in-
volves demonstration that it correlates better with
human judgment than a standard metric such as
BLEU (Papineni et al., 2001). However, although
it is standard practice in MT evaluation to mea-
sure increases in automatic metric scores with sig-
nificance tests (Germann, 2003; Och, 2003; Ku-
mar and Byrne, 2004; Koehn, 2004; Riezler and
Maxwell, 2005; Graham et al., 2014), this has
not been the case in papers proposing new met-
rics. Thus it is possible that some reported im-
provements in correlation with human judgment
are attributable to chance rather than a systematic
improvement.
In this paper, we motivate and introduce a novel
significance test to assess the statistical signifi-
cance of differences in correlation with human
judgment for pairs of automatic metrics. We ap-
ply tests to the WMT-12 shared metrics task to
compare each of the participating methods, and
find that for a high proportion of metrics, there is
not enough evidence to conclude that they signifi-
cantly outperform BLEU.
2 Correlation with Human Judgment
A common means of assessing automatic MT
evaluation metrics is Spearman?s rank correlation
with human judgments (Melamed et al., 2003),
which measures the relative degree of monotonic-
ity between the metric and human scores in the
range [?1, 1]. The standard justification for cal-
culating correlations over ranks rather than raw
scores is to: (a) reduce anomalies due to absolute
score differences; and (b) focus evaluation on what
is generally the primary area of interest, namely
the ranking of systems/translations.
An alternative means of evaluation is Pearson?s
correlation, which measures the linear correlation
between a metric and human scores (Leusch et al.,
2003). Debate on the relative merits of Spear-
man?s and Pearson?s correlation for the evaluation
of automatic metrics is ongoing, but there is an in-
creasing trend towards Pearson?s correlation, e.g.
in the recent WMT-14 shared metrics task.
Figure 1 presents the system-level results for
two evaluation metrics ? AMBER (Chen et al.,
2012) and TERRORCAT (Fishel et al., 2012)
? over the WMT-12 Spanish-to-English metrics
task. These two metrics achieved the joint-highest
rank correlation (? = 0.965) for the task, but dif-
fer greatly in terms of Pearson?s correlation (r =
0.881 vs. 0.971, resp.). The largest contributor to
this artifact is the system with the lowest human
score, represented by the leftmost point in both
plots.
172
ll
ll
l ll
l
ll
l
l
?3 ?2 ?1 0 1 2 3?
3
?
2
?
1
0
1
2
3
Human
AMB
ER
Spearman: 0.965Pearson: 0.881
(a) AMBER
ll
ll
l l
ll
ll
l
l
?3 ?2 ?1 0 1 2 3?
3
?
2
?
1
0
1
2
3
Human
Terro
rCat
Spearman: 0.965Pearson: 0.971
(b) TERRORCAT
Figure 1: Scatter plot of human and automatic scores of WMT-12 Spanish-to-English systems for two
MT evaluation metrics (AMBER and TERRORCAT)
Consistent with the WMT-14 metrics shared
task, we argue that Pearson?s correlation is more
sensitive than Spearman?s correlation. There is
still the question, however, of whether an observed
difference in Pearson?s r is statistically significant,
which we address in the next section.
3 Significance Testing
Evaluation of a new automatic metric, M
new
,
commonly takes the form of quantifying the cor-
relation between the new metric and human judg-
ment, r(M
new
, H), and contrasting it with the cor-
relation for some baseline metric, r(M
base
, H). It
is very rare in the MT literature for significance
testing to be performed in such cases, however.
We introduce a statistical test which can be used
for this purpose, and apply the test to the evalua-
tion of metrics participating in the WMT-12 metric
evaluation task.
At first gloss, it might seem reasonable to per-
form significance testing in the following man-
ner when an increase in correlation with human
assessment is observed: apply a significance test
separately to the correlation of each metric with
human judgment, with the hope that the newly
proposed metric will achieve a significant correla-
tion where the baseline metric does not. However,
besides the fact that the correlation between al-
most any document-level metric and human judg-
ment will generally be significantly greater than
zero, the logic here is flawed: the fact that
one correlation is significantly higher than zero
(r(M
new
, H)) and that of another is not, does not
necessarily mean that the difference between the
two correlations is significant. Instead, a specific
test should be applied to the difference in corre-
lations on the data. For this same reason, con-
fidence intervals for individual correlations with
human judgment are also not particularly mean-
ingful.
In psychological studies, it is often the case that
samples that data are drawn from are independent,
and differences in correlations are computed on in-
dependent data sets. In such cases, the Fisher r
to z transformation is applied to test for signifi-
cant differences in correlations. In the case of au-
tomatic metric evaluation, however, the data sets
used are almost never independent. This means
that if r(M
base
, H) and r(M
new
, H) are both> 0,
the correlation between the metric scores them-
selves, r(M
base
,M
new
), must also be > 0. The
strength of this correlation, directly between pairs
of metrics, should be taken into account using a
significance test of the difference in correlation be-
tween r(M
base
, H) and r(M
new
, H).
3.1 Correlated Correlations
Correlations computed for two separate automatic
metrics on the same data set are not independent,
and for this reason in order to test the difference in
correlation between them, the degree to which the
pair of metrics correlate with each other should be
taken into account. The Williams test (Williams,
173
Terro
rCat
MET
EOR Saga
n
Sem
pos PosF
XEn
ErrC
ats
WBE
rrCa
ts
Amb
er
BErr
Cats
Simp
BLE
U
BLE
U.4c
c TER
TERBLEU?4cc
SimpBLEUBErrCats
AmberWBErrCats
XEnErrCatsPosF
SemposSagan
METEORTerrorCat
(a) Pearson?s correlation
Terro
rCat
MET
EOR Saga
n
Sem
pos PosF
XEn
ErrC
ats
WBE
rrCa
ts
Amb
er
BErr
Cats
Simp
BLE
U
BLE
U.4c
c TER
TERBLEU?4cc
SimpBLEUBErrCats
AmberWBErrCats
XEnErrCatsPosF
SemposSagan
METEORTerrorCat
(b) Statistical significance
Figure 2: (a) Pearson?s correlation between pairs of automatic metrics; and (b) p-value of Williams
significance tests, where a colored cell in row i (named on y-axis), col j indicates that metric i (named
on x-axis) correlates significantly higher with human judgment than metric j; all results are based on the
WMT-12 Spanish-to-English data set.
1959)
1
evaluates significance in a difference in de-
pendent correlations (Steiger, 1980). It is formu-
lated as follows, as a test of whether the population
correlation betweenX
1
andX
3
equals the popula-
tion correlation between X
2
and X
3
:
t(n? 3) =
(r
13
? r
23
)
?
(n? 1)(1 + r
12
)
?
2K
(n?1)
(n?3)
+
(r
23
+r
13
)
2
4
(1? r
12
)
3
,
where r
ij
is the Pearson correlation between X
i
and X
j
, n is the size of the population, and:
K = 1? r
12
2
? r
13
2
? r
23
2
+ 2r
12
r
13
r
23
The Williams test is more powerful than the
equivalent for independent samples (Fisher r to
z), as it takes the correlations between X
1
and
X
2
(metric scores) into account. All else being
equal, the higher the correlation between the met-
ric scores, the greater the statistical power of the
test.
4 Evaluation and Discussion
Figure 2a is a heatmap of the degree to which au-
tomatic metrics correlate with one another when
computed on the same data set, in the form of the
Pearson?s correlation between each pair of met-
rics that participated in the WMT-12 metrics task
for Spanish-to-English evaluation. Metrics are or-
dered in all tables from highest to lowest correla-
tion with human assessment. In addition, for the
1
Also sometimes referred to as the Hotelling?Williams
test.
purposes of significance testing, we take the abso-
lute value of all correlations, in order to compare
error-based metrics with non-error based ones.
In general, the correlation is high amongst all
pairs of metrics, with a high proportion of paired
metrics achieving a correlation in excess of r =
0.9. Two exceptions to this are TERRORCAT
(Fishel et al., 2012) and SAGAN (Castillo and Es-
trella, 2012), as seen in the regions of yellow and
white.
Figure 2b shows the results of Williams sig-
nificance tests for all pairs of metrics. Since we
are interested in not only identifying significant
differences in correlations, but ultimately ranking
competing metrics, we use a one-sided test. Here
again, the metrics are ordered from highest to low-
est (absolute) correlation with human judgment.
For the Spanish-to-English systems, approxi-
mately 60% of WMT-12 metric pairs show a sig-
nificant difference in correlation with human judg-
ment at p < 0.05 (for one of the two metric di-
rections).
2
As expected, the higher the correlation
with human judgment, the more metrics a given
method is superior to at a level of statistical signifi-
cance. Although TERRORCAT (Fishel et al., 2012)
achieves the highest absolute correlation with hu-
man judgment, it is not significantly better (p ?
0.05) than the four next-best metrics (METEOR
(Denkowski and Lavie, 2011), SAGAN (Castillo
and Estrella, 2012), SEMPOS (Mach?a?cek and Bo-
2
Correlation matrices (red) are maximally filled, in con-
trast to one-sided significance test matrices (green), where, at
a maximum, fewer than half of the cells can be filled.
174
BLEU
.4cc
Simp
BLEU Semp
os Ambe
r TER Saga
n
MET
EOR
Terro
rCat
BErr
Cats
XEnE
rrCat
s PosF
WBE
rrCat
s WBErrCats
PosFXEnErrCats
BErrCatsTerrorCat
METEORSagan
TERAmber
SemposSimpBLEU
BLEU?4cc
(a) Czech-to-English
Terro
rCat Semp
os
MET
EOR
Simp
BLEU BLEU
.4cc Amb
er PosF
XEnE
rrCat
s
BErr
Cats
WBE
rrCat
s TER
TERWBErrCats
BErrCatsXEnErrCats
PosFAmber
BLEU?4ccSimpBLEU
METEORSempos
TerrorCat
(b) French-to-English
Semp
os
MET
EOR
Terro
rCat Ambe
r
BErr
Cats PosF
WBE
rrCat
s
XEnE
rrCat
s
Simp
BLEU TER BLEU
.4cc
BLEU?4ccTER
SimpBLEUXEnErrCats
WBErrCatsPosF
BErrCatsAmber
TerrorCatMETEOR
Sempos
(c) German-to-English
Terro
rCat
EnXE
rrCat
s
Amb
er
BErr
Cats
WBE
rrCat
s
BLEU
.4cc PosF
Simp
BLEU TER MET
EOR
METEORTER
SimpBLEUPosF
BLEU?4ccWBErrCats
BErrCatsAmber
EnXErrCatsTerrorCat
(d) English-to-Spanish
EnXE
rrCat
s
BErr
Cats
Simp
BLEU MET
EOR
WBE
rrCat
s
Amb
er
BLEU
.4cc
Terro
rCat PosF TER
TERPosF
TerrorCatBLEU?4cc
AmberWBErrCats
METEORSimpBLEU
BErrCatsEnXErrCats
(e) English-to-French
Terro
rCat
Simp
BLEU PosF BErrC
ats
EnXE
rrCat
s
Amb
er TER
WBE
rrCat
s
BLEU
.4cc
MET
EOR
METEORBLEU?4cc
WBErrCatsTER
AmberEnXErrCats
BErrCatsPosF
SimpBLEUTerrorCat
(f) English-to-German
Figure 3: Significance results for pairs of automatic metrics for each WMT-12 language pair.
jar, 2011) and POSF (Popovic, 2012)). There is
not enough evidence to conclude, therefore, that
this metric is any better at evaluating Spanish-to-
English MT system quality than the next four met-
rics.
Figure 3 shows the results of significance tests
for the six other language pairs used in the WMT-
12 metrics shared task.
3
For no language pair
is there an outright winner amongst the met-
rics, with proportions of significant differences be-
tween metrics for a given language pair ranging
from 3% for Czech-to-English to 82% for English-
to-French (p < 0.05). The number of metrics that
significantly outperform BLEU for a given lan-
guage pair is only 34% (p < 0.05), and no method
significantly outperforms BLEU over all language
pairs ? indeed, even the best methods achieve sta-
tistical significance over BLEU for only a small
minority of language pairs. This underlines the
dangers of assessing metrics based solely on cor-
relation numbers, and emphasizes the importance
of statistical testing.
It is important to note that the number of com-
3
We omit English-to-Czech due to some metric scores be-
ing omitted from the WMT-12 data set.
peting metrics a metric significantly outperforms
should not be used as the criterion for ranking
competing metrics. This is due to the fact that
the power of the Williams test to identify signifi-
cant differences between correlations changes de-
pending on the degree to which the pair of met-
rics correlate with each other. Therefore, a metric
that happens to correlate strongly with many other
metrics would be at an unfair advantage, were
numbers of significant wins to be used to rank met-
rics. For this reason, it is best to interpret pairwise
metric tests in isolation.
As part of this research, we have made avail-
able an open-source implementation of statis-
tical tests tailored to the assessment of MT
metrics available at https://github.com/
ygraham/significance-williams.
5 Conclusions
We have provided an analysis of current method-
ologies for evaluating automatic metrics in ma-
chine translation, and identified an issue with re-
spect to the lack of significance testing. We in-
troduced the Williams test as a means of cal-
culating the statistical significance of differences
175
in correlations for dependent samples. Analysis
of statistical significance in the WMT-12 metrics
shared task showed there is currently insufficient
evidence for a high proportion of metrics to con-
clude that they outperform BLEU.
Acknowledgments
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported by funding from the Australian Research
Council.
References
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 52?58, Montr?eal, Canada.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving AMBER, an MT evaluation metric. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 59?63, Montr?eal,
Canada.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, UK.
Mark Fishel, Rico Sennrich, Maja Popovi?c, and Ond?rej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 64?70, Montr?eal, Canada.
Ulrich Germann. 2003. Greedy decoding for statis-
tical machine translation in almost linear time. In
Proceedings of the 2003 Conference of the North
American Chapter of the Assoc. Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 1?8, Edmonton, Canada.
Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2014. Randomized significance tests in machine
translation. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, pages
266?274, Baltimore, USA.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Empirical Methods in Natural Language Processing
2004 (EMNLP 2004), pages 388?395, Barcelona,
Spain.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 4th International Con-
ference on Human Language Technology Research
and 5th Annual Meeting of the NAACL (HLT-NAACL
2004), pages 169?176, Boston, USA.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distance measure
with applications to machine translation evaluation.
In Proceedings 9th Machine Translation Summit
(MT Summit IX), pages 240?247, New Orleans,
USA.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approx-
imating a deep-syntactic metric for MT evaluation
and tuning. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 92?98, Edin-
burgh, UK.
Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003) ? Short Papers, pages 61?63, Ed-
monton, Canada.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), IBM Research, Thomas J.
Watson Research Center.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71?75, Montr?eal, Canada.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for mt. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, USA.
James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Evan J. Williams. 1959. Regression Analysis, vol-
ume 14. Wiley, New York, USA.
176
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 443?451,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Is Machine Translation Getting Better over Time?
Yvette Graham Timothy Baldwin Alistair Moffat Justin Zobel
Department of Computing and Information Systems
The University of Melbourne
{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au
Abstract
Recent human evaluation of machine
translation has focused on relative pref-
erence judgments of translation quality,
making it difficult to track longitudinal im-
provements over time. We carry out a
large-scale crowd-sourcing experiment to
estimate the degree to which state-of-the-
art performance in machine translation has
increased over the past five years. To fa-
cilitate longitudinal evaluation, we move
away from relative preference judgments
and instead ask human judges to provide
direct estimates of the quality of individ-
ual translations in isolation from alternate
outputs. For seven European language
pairs, our evaluation estimates an aver-
age 10-point improvement to state-of-the-
art machine translation between 2007 and
2012, with Czech-to-English translation
standing out as the language pair achiev-
ing most substantial gains. Our method
of human evaluation offers an economi-
cally feasible and robust means of per-
forming ongoing longitudinal evaluation
of machine translation.
1 Introduction
Human evaluation provides the foundation for em-
pirical machine translation (MT), whether human
judges are employed directly to evaluate system
output, or via the use of automatic metrics ?
validated through correlation with human judg-
ments. Achieving consistent human evaluation
is not easy, however. Annual evaluation cam-
paigns conduct large-scale human assessment but
report ever-decreasing levels of judge consistency
? when given the same pair of translations to
repeat-assess, even expert human judges will wor-
ryingly often contradict both the preference judg-
ment of other judges and their own earlier prefer-
ence (Bojar et al., 2013). For this reason, human
evaluation has been targeted within the commu-
nity as an area in need of attention, with increased
efforts to develop more reliable methodologies.
One standard platform for human evaluation is
WMT shared tasks, where assessments have (since
2007) taken the form of ranking five alternate sys-
tem outputs from best to worst (Bojar et al., 2013).
This method has been shown to produce more con-
sistent judgments compared to fluency and ade-
quacy judgments on a five-point scale (Callison-
Burch et al., 2007). However, relative preference
judgments have been criticized for being a sim-
plification of the real differences between trans-
lations, not sufficiently taking into account the
large number of different types of errors of vary-
ing severity that occur in translations (Birch et al.,
2013). Relative preference judgments do not take
into account the degree to which one translation is
better than another ? there is no way of knowing if
a winning system produces far better translations
than all other systems, or if that system would have
ranked lower if the severity of its inferior transla-
tion outputs were taken into account.
Rather than directly aiming to increase human
judge consistency, some methods instead increase
the number of reference translations available to
automatic metrics. HTER (Snover et al., 2006)
employs humans to post-edit each system out-
put, creating individual human-targeted reference
translations which are then used as the basis for
computing the translation error rate. HyTER, on
the other hand, is a tool that facilitates creation
of very large numbers of reference translations
(Dreyer and Marcu, 2012). Although both ap-
proaches increase fairness compared to automatic
metrics that use a single generic reference transla-
tion, even human post-editors will inevitably vary
in the way they post-edit translations, and the pro-
cess of creating even a single new reference trans-
443
lation for each system output is often too resource-
intensive to be used in practice.
With each method of human evaluation, a trade-
off exists between annotation time and the number
of judgments collected. At one end of the spec-
trum, the WMT human evaluation collects large
numbers of quick judgments (approximately 3.5
minutes per screen, or 20 seconds per label) (Bojar
et al., 2013).
1
In contrast, HMEANT (Lo and Wu,
2011) uses a more time-consuming fine-grained
semantic-role labeling analysis at a rate of approx-
imately 10 sentences per hour (Birch et al., 2013).
But even with this detailed evaluation methodol-
ogy, human judges are inconsistent (Birch et al.,
2013).
Although the trend appears to be toward more
fine-grained human evaluation of MT output, it
remains to be shown that this approach leads to
more reliable system rankings ? with a main rea-
son to doubt this being that far fewer judgments
will inevitably be possible. We take a counter-
approach and aim to maintain the speed by which
assessments are collected in shared task evalua-
tions, but modify the evaluation set-up in two main
ways: (1) we structure the judgments as monolin-
gual tasks, reducing the cognitive load involved
in assessing translation quality; and (2) we ap-
ply judge-intrinsic quality control and score stan-
dardization, to minimize noise introduced when
crowd-sourcing is used to leverage numbers of as-
sessments and to allow for the fact that human
judges will vary in the way they assess transla-
tions. Assessors are regarded as reliable as long
as they demonstrate consistent judgments across a
range of different quality translations.
We elicit direct estimates of quality from
judges, as a quantitative estimate of the magni-
tude of each attribute of interest (Steiner and Nor-
man, 1989). Since we no longer look for rela-
tive preference judgments, we revert back to the
original fluency and adequacy criteria last used in
WMT 2007 shared task evaluation. Instead of five-
point fluency/adequacy scales, however, we use
a (100-point) continuous rating scale, as this fa-
cilitates more sophisticated statistical analyses of
score distributions for judges, including worker-
intrinsic quality control for crowd-sourcing. The
latter does not depend on agreement with ex-
perts, and is made possible by the reduction in
1
WMT 2013 reports 361 hours of labor to collect 61,695
labels, with approximately one screen of five pairwise com-
parisons each yielding a set of 10 labels.
information-loss when a continuous scale is used.
In addition, translations are assessed in isolation
from alternate system outputs, so that judgments
collected are no longer relative to a set of five
translations. This has the added advantage of elim-
inating the criticism made of WMT evaluations
that systems sometimes gain advantage from luck-
of-the-draw comparison with low quality output,
and vice-versa (Bojar et al., 2011).
Based on our proposed evaluation methodology,
human judges are able to work quickly, on average
spending 18 and 13 seconds per single segment ad-
equacy and fluency judgment, respectively. Addi-
tionally, when sufficiently large volumes of such
judgments are collected, mean scores reveal sig-
nificant differences between systems. Further-
more, since human evaluation takes the form of di-
rect estimates instead of relative preference judg-
ments, our evaluation introduces the possibility
of large-scale longitudinal human evaluation. We
demonstrate the value of longitudinal evaluation
by investigating the improvement made to state-
of-the-art MT over a five year time period (be-
tween 2007 and 2012) using the best participating
WMT shared task system output. Since it is likely
that the test data used for shared tasks has varied
in difficulty over this time period, we additionally
propose a simple mechanism for scaling system
scores relative to task difficulty.
Using the proposed methodology for measur-
ing longitudinal change in MT, we conclude that,
for the seven European language pairs we evalu-
ate, MT has made an average 10% improvement
over the past 5 years. Our method uses non-expert
monolingual judges via a crowd-sourcing portal,
with fast turnaround and at relatively modest cost.
2 Monolingual Human Evaluation
There are several reasons why the assessment of
MT quality is difficult. Ideally, each judge should
be a native speaker of the target language, while
at the same time being highly competent in the
source language. Genuinely bilingual people are
rare, however. As a result, judges are often peo-
ple with demonstrated skills in the target language,
and a working knowledge ? often self-assessed ?
of the source language. Adding to the complexity
is the discipline that is required: the task is cog-
nitively difficult and time-consuming when done
properly. The judge is, in essence, being asked to
decide if the supplied translations are what they
444
would have generated if they were asked to do the
same translation.
The assessment task itself is typically structured
as follows: the source segment (a sentence or
a phrase), plus five alternative translations and a
?reference? translation are displayed. The judge
is then asked to assign a rank order to the five
translations, from best to worst. A set of pairwise
preferences are then inferred, and used to generate
system rankings, without any explicit formation of
stand-alone system ?scores?.
This structure introduces the risk that judges
will only compare translations against the refer-
ence translation. Certainly, judges will vary in
the degree they rely on the reference translation,
which will in turn impact on inter-judge inconsis-
tency. For instance, even when expert judges do
assessments, it is possible that they use the ref-
erence translation as a substitute for reading the
source input, or do not read the source input at
all. And if crowd-sourcing is used, can we really
expect high proportions of workers to put the ad-
ditional effort into reading and understanding the
source input when a reference translation (proba-
bly in their native language) is displayed? In re-
sponse to this potential variability in how annota-
tors go about the assessment task, we trial assess-
ments of adequacy in which the source input is not
displayed to human judges. We structure assess-
ments as a monolingual task and pose them in such
a way that the focus is on comparing the meaning
of reference translations and system outputs.
2
We therefore ask human judges to assess the de-
gree to which the system output conveys the same
meaning as the reference translation. In this way,
we focus the human judge indirectly on the ques-
tion we wish to answer when assessing MT: does
the translation convey the meaning of the source?
The fundamental assumption of this approach is
that the reference translation accurately captures
the meaning of the source; once that assumption
is made, it is clear that the source is not required
during the evaluation.
Benefits of this change are that the task is both
easier to describe to novice judges, and easier
to answer, and that it requires only monolingual
speakers, opening up the evaluation to a vastly
larger pool of genuinely qualified workers.
With this set-up in place for adequacy, we also
2
This dimension of the assessment is similar but not iden-
tical to the monolingual adequacy assessment in early NIST
evaluation campaigns (NIST, 2002).
re-introduce a fluency assessment. Fluency rat-
ings can be carried out without the presence of a
reference translation, reducing any remnant bias
towards reference translations in the evaluation
setup. That is, we propose a judgment regime in
which each task is presented as a two-item fluency
and adequacy judgment, evaluated separately, and
with adequacy restructured into a monolingual
?similarity of meaning? task.
When fluency and adequacy were originally
used for human evaluation, each rating used a 5-
point adjective scale (Callison-Burch et al., 2007).
However, adjectival scale labels are problematic
and ratings have been shown to be highly depen-
dent on the exact wording of descriptors (Seymour
et al., 1985). Alexandrov (2010) provides a sum-
mary of the extensive problems associated with the
use of adjectival scale labels, including bias result-
ing from positively- and negatively-worded items
not being true opposites of one another, and items
intended to have neutral intensity in fact proving
to have specific conceptual meanings.
It is often the case, however, that the question
could be restructured so that the rating scale no
longer requires adjectival labels, by posing the
question as a statement such as The text is fluent
English and asking the human assessor to specify
how strongly they agree or disagree with that state-
ment. The scale and labels can then be held con-
stant across experimental set-ups for all attributes
evaluated ? meaning that if the scale is still biased
in some way it will be equally so across all set-ups.
3 Assessor Consistency
One way of estimating the quality of a human
evaluation regime is to measure its consistency:
whether or not the same outcome is achieved if
the same question is asked a second time. In
MT, annotator consistency is commonly measured
using Cohen?s kappa coefficient, or some variant
thereof (Artstein and Poesio, 2008). Originally de-
veloped as a means of establishing assessor inde-
pendence, it is now commonly used in the reverse
sense, with high numeric values being used as ev-
idence of agreement. Two different measurements
can be made ? whether a judge is consistent with
other judgments performed by themselves (intra-
annotator agreement), and whether a judge is con-
sistent with other judges (inter-annotator agree-
ment).
Cohen?s kappa is intended for use with categor-
445
ical judgments, but is also commonly used with
five-point adjectival-scale judgments, where the
set of categories has an explicit ordering. One
particular issue with five-point assessments is that
score standardization cannot be applied. As such,
a judge who assigns two neighboring intervals is
awarded the same ?penalty? for being ?different?
as the judge who chooses the extremities. The
kappa coefficient cannot be directly applied to
many-valued interval or continuous data.
This raises the question of how we should eval-
uate assessor consistency when a continuous rat-
ing scale is in place. No judge, when given the
same translation to judge twice on a continuous
rating scale, can be expected to give precisely the
same score for each judgment (where repeat as-
sessments are separated by a considerable number
of intervening ones). A more flexible tool is thus
required. We build such a tool by starting with two
core assumptions:
A: When a consistent assessor is presented with
a set of repeat judgments, the mean of the
initial set of assessments will not be signifi-
cantly different from the mean score of repeat
assessments.
B: When a consistent judge is presented with a
set of judgments for translations from two
systems, one of which is known to produce
better translations than the other, the mean
score for the better system will be signifi-
cantly higher than that of the inferior system.
Assumption B is the basis of our quality-control
mechanism, and allows us to distinguish between
Turkers who are working carefully and those who
are merely going through the motions. We use a
100-judgment HIT structure to control same-judge
repeat items and deliberately-degraded system
outputs (bad reference items) used for worker-
intrinsic quality control (Graham et al., 2013).
bad reference translations for fluency judgments
are created as follows: two words in the translation
are randomly selected and randomly re-inserted
elsewhere in the sentence (but not as the initial or
final words of the sentence).
Since adding duplicate words will not degrade
adequacy in the same way, we use an alternate
method to create bad reference items for adequacy
judgments: we randomly delete a short sub-string
of length proportional to the length of the origi-
nal translation to emulate a missing phrase. Since
total fltrd Assum A total fltrd
wrkrs wrkrs holds segs segs
F 557 321 (58%) 314 (98.8%) 122k 78k (64%)
A 542 283 (52%) 282 (99.6%) 102k 62k (61%)
Table 1: Total quality control filtered workers and
assessments (F = fluency; A = adequacy).
this is effectively a new degradation scheme, we
tested against experts. For low-quality transla-
tions, deleting just two words from a long sentence
often made little difference. The method we even-
tually settled on removes a sequence of k words,
as a function of sentence length n:
2 ? n ? 3 ? k = 1
4 ? n ? 5 ? k = 2
6 ? n ? 8 ? k = 3
9 ? n ? 15 ? k = 4
16 ? n ? 20 ? k = 5
n > 20 ? k =
?
n
5
?
To filter out careless workers, scores for
bad reference pairs are extracted, and a
difference-of-means test is used to calculate
a worker-reliability estimate in the form of a
p-value. Paired tests are then employed using the
raw scores for degraded and corresponding system
outputs, using a reliability significance threshold
of p < 0.05. If a worker does not demonstrate
the ability to reliably distinguish between a bad
system and a better one, the judgments from
that worker are discarded. This methodology
means that careless workers who habitually rate
translations either high or low will be detected,
as well as (with high probability) those that click
(perhaps via robots) randomly. It also has the
advantage of not filtering out workers who are
internally consistent but whose scores happen not
to correspond particularly well to a set of expert
assessments.
Having filtered out users who are unable to reli-
ably distinguish between better and worse sets of
translations (p ? 0.05), we can now examine how
well Assumption A holds for the remaining users,
i.e. the extent to which workers apply consistent
scores to repeated translations. We compute mean
scores for the initial and repeat items and look for
even very small differences in the two distribu-
tions for each worker. Table 1 shows numbers of
workers who passed quality control, and also that
446
Si
S
i+5
1 bad reference its corresponding system output
1 system output a repeat of it
1 reference its corresponding system output
Above in reverse for S
i
and S
i+5
4 system outputs 4 system outputs
Table 2: Control of repeat item pairs. S
i
denotes
the i
th
set of 10 translations assessed within a 100
translation HIT.
the vast majority (around 99%) of reliable work-
ers have no significant difference between mean
scores for repeat items.
4 Five Years of Machine Translation
To estimate the improvement in MT that took
place between 2007 and 2012, we asked work-
ers on Amazon?s Mechanical Turk (MTurk) to rate
the quality of translations produced by the best-
reported participating system for each of WMT
2007 and WMT 2012 (Callison-Burch et al., 2007;
Callison-Burch et al., 2012). Since it is likely that
the test set has changed in difficulty over this time
period, we also include in the evaluation the orig-
inal test data for 2007 and 2012, translated by a
single current MT system. We use the latter to cal-
ibrate the results for test set difficulty, by calcu-
lating the average difference in rating, ?, between
the 2007 and 2012 test sets. This is then added
to the difference in rating for the best-reported
systems in 2012 and 2007, to arrive at an over-
all evaluation of the 5-year gain in MT quality for
a given language pair, separately for fluency and
adequacy.
Experiments were carried out for each of Ger-
man, French and Spanish into and out of English,
and also for Czech-to-English. English-to-Czech
was omitted because of a low response rate on
MTurk. For language pairs where two systems tied
for first place in the shared task, a random selec-
tion of translations from both systems was made.
HIT structure
To facilitate quality control, we construct each
HIT on MTurk as an assessment of 100 trans-
lations. Each individual translation is rated in
isolation from other translations with workers re-
quired to iterate through 100 translations without
the opportunity to revisit earlier assessments. A
100-translation HIT contains the following items:
70 randomly selected system outputs made up of
roughly equal proportions of translations for each
evaluated system, 10 bad reference translations
(each based on one of the 70 system outputs), 10
exact repeats and 10 reference translations. We di-
vide a 100-translation HIT into 10 sets of 10 trans-
lations. Table 2 shows how the content of each set
is determined. Translations are then randomized
only within each set (of 10 translations), with the
original sequence order of the sets preserved. In
this way, the order of quality control items is un-
predictable but controlled so pairs are separated by
a minimum of 40 intervening assessments (4 sets
of translations). The HIT structure results in 80%
of assessed translations corresponding to genuine
outputs of a system (including exact repeat assess-
ments), which is ultimately what we wish to ob-
tain, with 20% of assessments belonging to quality
control items (bad reference or reference transla-
tions).
Assessment set-up
Separate HITs were provided for evaluation of flu-
ency and adequacy. For fluency, a single system
output was displayed per screen, with a worker re-
quired to rate the fluency of a translation on a 100-
point visual analog scale with no displayed point
scores. A similar set-up was used for adequacy but
with the addition of a reference translation (dis-
played in gray font to distinguish it from the sys-
tem output being assessed). The Likert-type state-
ment that framed the judgment was Read the text
below and rate it by how much you agree that:
? [for fluency] the text is fluent English
? [for adequacy] the black text adequately ex-
presses the meaning of the gray text.
In neither case was the source language string pro-
vided to the workers.
Tasks were published on MTurk, with no re-
gion restriction but the stipulation that only na-
tive speakers of the target language should com-
plete HITs, and with a qualification of an MTurk
prior HIT-approval rate of at least 95%. Instruc-
tions were always presented in the target language.
Workers were paid US$0.50 per fluency HIT, and
US$0.60 per adequacy HIT.
3
3
Since insufficient assessments were collected for French
and German evaluations in the initial run, a second and ulti-
mately third set of HITs were needed for these languages with
increased payment per HIT of US$1.0 per 100-judgment ade-
quacy HIT, US$0.65 per 100-judgment fluency HIT and later
again to US$1.00 per 100-judgment fluency HIT.
447
Close to one thousand individual Turkers con-
tributed to this experiment (some did both flu-
ency and adequacy assessments), providing a to-
tal of more than 220,000 translations, of which
140,000 were provided by workers meeting the
quality threshold.
In general, it cost approximately US$30 to as-
sess each system, with low-quality workers ap-
proximately doubling the cost of the annotation.
We rejected HITs where it was clear that random-
clicking had taken place, but did not reject solely
on the basis of having not met the quality control
threshold, to avoid penalizing well-intentioned but
low-quality workers.
Overall change in performance
Table 3 shows the overall gain made in five years,
from WMT 07 to WMT 12. Mean scores for the
two top-performing systems from each shared task
(BEST
07
, BEST
12
) are included, as well as scores
for the benchmark current MT system on the two
test sets (CURR
07
, CURR
12
). For each language
pair, a 100-translation HIT was constructed by
randomly selecting translations from the pool of
(3003+2007)?2 that were available, and this re-
sults in apparently fewer assessments for the 2007
test set. In fact, numbers of evaluated translations
are relative to the size of each test set. Average z
scores for each system are also presented, based on
the mean and standard deviation of all assessments
provided by an individual worker, with positive
values representing deviations above the mean of
workers. In addition, we include mean BLEU (Pa-
pineni et al., 2001) and METEOR (Banerjee and
Lavie, 2005) automatic scores for the same system
outputs.
The CURR benchmark shows fluency scores
that are 5.9 points higher on the 2007 data set than
they are on the 2012 test data, with a larger dif-
ference in adequacy of 8.3 points. As such, the
2012 test data is more challenging than the 2007
test data. Despite this, both fluency and adequacy
scores for the best system in 2012 have increased
by 4.5 and 2.0 points respectively, amounting to
estimated average gains of 10.4 points in fluency
and 10.3 points in adequacy for state-of-the-art
MT across the seven language pairs.
Looking at the standardized scores, it is appar-
ent that the presence of the CURR translations for
the 2007 test set pushes the mean score for the
2007 best systems below zero. The presence in
the HITs of reference translations also shifts stan-
dardized system evaluations below zero, because
they are not attributable to any of the systems be-
ing assessed.
4
Results for automatic metrics lead to similar
conclusions: that the test set has indeed increased
in difficulty; and that, in spite of this, substantial
improvements have been made according to auto-
matic metrics, +13.5 using BLEU, and +7.1 on
average using METEOR.
Language pairs
Table 4 shows mean fluency and adequacy scores
by language pair for translation into English. Rel-
ative gains in both adequacy and fluency for the to-
English language pairs are in agreement with the
estimates generated through the use of the two au-
tomatic metrics. Most notably, Czech-to-English
translation appears to have made substantial gains
across the board, achieving more than double the
gain made by some of the other language pairs; re-
sults for best participating 2007 systems show that
this may in part be caused by the fact that Czech-
to-English translation had a lower 2007 baseline
to begin with (BEST
07
F:40.8; A:41.7) in compar-
ison to, for example, Spanish-to-English transla-
tion (BEST
07
F:56.7; A:59.0).
Another notable result is that although the test
data for each year?s shared task is parallel across
five languages, test set difficulty increases by dif-
ferent degrees according to human judges and au-
tomatic metrics, with BLEU scores showing sub-
stantial divergence across the to-English language
pairs. Comparing BLEU scores achieved by the
benchmark system for Spanish to English and
Czech-to-English, for example, the benchmark
system achieves close scores on the 2007 test data
with a difference of only |52.3 ? 51.2| = 1.1,
compared to the score difference for the bench-
mark scores for translation of the 2012 test data of
|25.0 ? 38.3| = 13.3. This may indicate that the
increase in test set difficulty that has taken place
over the years has made the shared task dispro-
portionately more difficult for some language pairs
than for others. It does seem that some language
pairs are harder to translate than others, and the
differential change may be a consequence of the
fact that increasing test set complexity for all lan-
guages in parallel has a greater impact on transla-
tion difficulty for language pairs that are intrinsi-
cally harder to translate between.
4
Scores for reference translations can optionally be omit-
ted for score standardization.
448
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
fl
u
e
n
c
y
score 64.1 58.2 5.9 53.5 58.0 (+4.5) 10.4
z 0.18 0.00 0.18 ?0.16 0.00 (+0.16) 0.34
n 12,334 18,654 12,513 18,579
a
d
e
q
u
a
c
y
score 65.0 56.7 8.3 54.0 56.0 (+2.0) 10.3
z 0.18 ?0.07 0.25 ?0.16 ?0.09 (+0.07) 0.32
n 10,022 14,870 10,049 14,979
m
e
t
r
i
c
s
BLEU 41.5 30.0 11.4 25.6 27.7 (+2.1) 13.5
METEOR 49.2 41.1 8.1 41.1 40.1 (?1.0) 7.1
Table 3: Average human evaluation results for all language pairs; mean and standardized z scores are
computed in each case for n translations. In this table, and in Tables 4 and 5, all reported fluency and
adequacy values are in points relative to the 100-point assessment scale.
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
D
E
-
E
N
fluency
score 65.3
???
57.9 7.4 52.8 55.0
?
(+2.2) 9.6
n 2,164 3,381 2,242 3,253
adequacy
score 63.8
???
52.8 11.0 46.5 49.8
??
(+3.3) 14.3
n 1,458 2,175 1,454 2,193
metrics
BLEU 38.3 26.5 11.8 21.1 23.8 (+2.7) 14.5
METEOR 40.3 32.7 7.6 33.4 31.7 (?1.7) 5.9
F
R
-
E
N
fluency
score 65.9
???
58.0 7.9 57.8 60.2
??
(+2.4) 10.3
n 2,172 3,267 2,203 3,238
adequacy
score 61.0
???
52.3 8.7 52.7 51.5 (?1.2) 7.5
n 1,754 2,651 1,763 2,712
metrics
BLEU 39.4 32.0 7.4 28.6 31.5 (+2.9) 10.3
METEOR 39.8 34.6 5.2 35.9 34.3 (?1.6) 3.6
E
S
-
E
N
fluency
score 68.4
???
59.2 9.2 56.7 56.7 (+0.0) 9.2
n 1,514 2,234 1,462 2,230
adequacy
score 68.0
???
56.9 11.1 59.0
???
55.7 (?3.3) 7.8
n 1,495 2,193 1,492 2,180
metrics
BLEU 51.2 38.3 12.9 35.1 33.5 (?1.6) 11.3
METEOR 45.4 37.0 8.4 39.9 36.0 (?3.9) 4.5
C
S
-
E
N
fluency
score 62.3
???
49.9 12.4 40.8 50.5
???
(+9.7) 22.1
n 1,873 2,816 1,923 2,828
adequacy
score 62.4
???
47.5 14.9 41.7 47.4
???
(+5.7) 20.6
n 1,218 1,830 1,257 1,855
metrics
BLEU 52.3 25.0 27.3 25.1 22.4 (?2.7) 24.6
METEOR 44.7 31.6 13.1 34.3 30.8 (?3.5) 9.6
Table 4: Human evaluation of WMT 2007 and 2012 best systems for to-English language pairs. Mean
scores are computed in each case for n translations. In this table and in Table 5,
?
denotes significance at
p < 0.05;
??
significance at p < 0.01; and
???
significance at p < 0.001.
Table 5 shows results for translation out-of En-
glish, and once again human evaluation scores are
in agreement with automatic metrics with English-
to-Spanish translation achieving most substantial
gains for the three out-of-English language pairs,
an increase of 12.4 points for fluency, and 11.8
points with respect to adequacy, while English-
to-French translation achieves a gain of 8.8 for
449
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
E
N
-
E
S
fluency
score 77.2
???
73.4 3.8 63.3 71.9
???
(+8.6) 12.4
n 2,286 3,318 2,336 3,420
adequacy
score 75.2
???
68.1 7.1 62.5 67.2 (+4.7) 11.8
n 1,410 2,039 1,399 2,112
metrics
BLEU 48.2 38.7 9.5 29.1 35.3 (+6.2) 15.7
METEOR 69.9 59.6 10.3 57.0 58.1 (+1.1) 11.4
E
N
-
F
R
fluency
score 57.1 55.2 1.9 49.5 56.4 (+6.9) 8.8
n 1,008 1,645 1,039 1,588
adequacy
score 64.2
?
61.9 2.3 57.2 62.3 (+5.1) 7.4
n 1,234 1,877 1,274 1,775
metrics
BLEU 37.2 30.8 6.4 25.3 29.9 (+4.6) 11.0
METEOR 59.4 52.9 6.5 50.4 52.0 (+1.6) 8.1
E
N
-
D
E
fluency
score 52.3 54.1
?
?1.8 53.7 55.5 (+1.8) 0.0
n 1,317 1,993 1,308 2,022
adequacy
score 60.3
??
57.4 2.9 58.3 58.3 (+0.0) 2.9
n 1,453 2,105 1,410 2,152
metrics
BLEU 23.6 18.7 4.9 14.6 17.2 (+2.6) 7.5
METEOR 44.7 39.1 5.6 36.7 38.0 (+1.3) 6.9
Table 5: Human evaluation of WMT 2007 and 2012 best systems for out of English language pairs.
Mean scores are computed in each case for n translations.
fluency and 7.4 points for adequacy. English-to-
German translation achieves the lowest gain of
all languages, with apparently no improvement
in fluency, as the human fluency evaluation of
the benchmark system on the supposedly easier
2007 data receives a substantially lower score than
the same system over the 2012 data. This result
demonstrates why fluency, evaluated without a ref-
erence translation, should not be used to evalu-
ate MT systems without an adequacy assessment,
since it is entirely possible for a low-adequacy
translation to achieve a high fluency score.
For all language pairs, Figure 1 plots the net
gain in fluency, adequacy and F
1
against increase
in test data difficulty.
5 Conclusion
We carried out a large-scale human evaluation
of best-performing WMT 2007 and 2012 shared
task systems in order to estimate the improvement
made to state-of-the-art machine translation over
this five year time period. Results show significant
improvements have been made in machine trans-
lation of European language pairs, with Czech-
to-English recording the greatest gains. It is also
clear from our data that the difficulty of the task
has risen over the same period, to varying degrees
0 5 10 15
0
5
10
15
Best12 ? Best 07
? ( C
urr  07
 
?
 
Curr
12 ) de?enfr?en
es?en
cs?en
en?de
en?fr
en?es
F1
Fluency
Adequacy
Figure 1: Mean fluency, adequacy and combined
F
1
scores for language pairs.
for individual language pairs.
Researchers interested in making use of the
dataset are invited to contact the first author.
Acknowledgments This work was supported by
the Australian Research Council.
450
References
A. Alexandrov. 2010. Characteristics of single-item
measures in Likert scale format. The Electronic
Journal of Business Research Methods, 8:1?12.
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for mt evaluation with improved cor-
relation with human judgements. In Proc. Wkshp.
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65?
73, Ann Arbor, MI.
A. Birch, B. Haddow, U. Germann, M. Nadejde,
C. Buck, and P. Koehn. 2013. The feasibility of
HMEANT as a human MT evaluation metric. In
Proc. 8th Wkshp. Statistical Machine Translation,
pages 52?61, Sofia, Bulgaria. ACL.
O. Bojar, M. Ercegov?cevic, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evalua-
tion. In Proc. 6th Wkshp. Statistical Machine Trans-
lation, pages 1?11, Edinburgh, Scotland. ACL.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc.
8th Wkshp. Statistical Machine Translation, pages
1?44, Sofia, Bulgaria. ACL.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proc. 2nd Wkshp. Statistical Machine
Translation, pages 136?158, Prague, Czech Repub-
lic. ACL.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada. ACL.
M. Dreyer and D. Marcu. 2012. HyTER: Meaning-
equivalent semantics for translation evaluation. In
Proc. 2012 Conf. North American Chapter of the
ACL: Human Language Technologies, pages 162?
171, Montreal, Canada. ACL.
Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013.
Continuous measurement scales in human evalua-
tion of machine translation. In Proc. 7th Linguis-
tic Annotation Wkshp. & Interoperability with Dis-
course, pages 33?41, Sofia, Bulgaria. ACL.
C. Lo and D. Wu. 2011. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In Proc.
49th Annual Meeting of the ACL: Human Language
Techologies, pages 220?229, Portland, OR. ACL.
NIST. 2002. The 2002 NIST machine translation
evaluation plan. National Institute of Standards and
Technology. http://www.itl.nist.gov/
iad/894.01/tests/mt/2003/doc/mt03_
evalplan.v2.pdf.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2001. BLEU: A method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research, Thomas J. Watson
Research Center.
R. A. Seymour, J. M. Simpson, J. E. Charlton, and
M. E. Phillips. 1985. An evaluation of length and
end-phrase of visual analogue scales in dental pain.
Pain, 21:177?185.
M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and
L. Micciula. 2006. A study of translation error rate
with targeted human annotation. In Proc. 7th Bien-
nial Conf. of the Assoc. Machine Translaiton in the
Americas, pages 223?231, Boston, MA.
D. L. Steiner and G. R. Norman. 1989. Health Mea-
surement Scales, A Practical Guide to their Devel-
opment and Use. Oxford University Press, Oxford,
UK, fourth edition.
451
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 133?137, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Umelb: Cross-lingual Textual Entailment with Word Alignment and String
Similarity Features
Yvette Graham Bahar Salehi Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
{ygraham,bsalehi,tbaldwin}@unimelb.edu.au
Abstract
This paper describes The University of Mel-
bourne NLP group submission to the Cross-
lingual Textual Entailment shared task, our
first tentative attempt at the task. The ap-
proach involves using parallel corpora and au-
tomatic word alignment to align text fragment
pairs, and statistics based on unaligned words
as features to classify items as forward and
backward before a compositional combination
into the final four classes, as well as exper-
iments with additional string similarity fea-
tures.
1 Introduction
Cross-lingual Textual Entailment (CLTE) (Negri et
al., 2012) proposes the task of automatically iden-
tifying the kind of relation that exists between pairs
of semantically-related text fragments written in two
distinct languages, a variant of the traditional Rec-
ognizing Textual Entailment (RTE) task (Bentivogli
et al, 2009; Bentivogli et al, 2010). The task tar-
gets the cross-lingual content synchronization sce-
nario proposed in Mehdad et al (2010, 2011). Com-
positional classification can be used by training two
distinct binary classifiers for forward and backward
entailment classification, before combining labels
into the four final entailment categories that now in-
clude bidirectional and no entailment labels. The
most similar previous work to this work is the cross-
lingual approach of the FBK system (Mehdad et
al., 2012) from Semeval 2012 (Negri et al, 2012),
in which the entailment classification is obtained
without translating T1 into T2 for the Spanish?
English language pair. We apply the cross-lingual
approach to German?English and instead of cross-
lingual matching features, we use Giza++ (Och et
al., 1999) and Moses (Koehn et al, 2007) to auto-
matically word align text fragment pairs to compute
statistics of unaligned words. In addition, we in-
clude some additional experiments using string sim-
ilarity features.
2 Compositional Classification
Given a pair of topically related fragments, T1 (Ger-
man) and T2 (English), we automatically annotate it
with one of the following entailment labels: bidi-
rectional, forward, backward, no entailment. We
take the compositional approach and separately train
a forward, as well as a backward binary classifier.
Each classifier is run separately on the set of text
fragment pairs to produce two binary labels for for-
ward and backward entailment. The two sets of la-
bels are logically combined to produce a final clas-
sification for each test pair of forward, backward,
bidirectional or no entailment.
3 Word Alignment Features
The test set of topically-related text fragments, T1
(German) and T2 (English) were added to Europarl
German?English parallel text (Koehn, 2005) and
Giza++ was used for automatic word alignment in
both language directions. Moses (Koehn et al,
2007) was then used for symmetrization with the
grow diag final and algorithm. This produces a
many-to-many alignment between the words of the
133
German, T1, and English, T2, with words also re-
maining unaligned.
The following features are computed for each test
pair feature scores for the forward classifier:
? A1: count of unaligned words in T2
? A2: count of words comprised soley of digits
in T2 not in T1
? A3: count of unaligned words in T2 with low
probability of appearing unaligned in Europarl
(with threshold p=0.11)
The number of words in T2 (English) that are not
aligned with anything in T1 (German) should pro-
vide an indication that, for example, the English text
fragment contains information not present in the cor-
responding German text fragment and subsequently
evidence against the presence of forward entailment.
We there include the feature, A1, that is simply a
count of unaligned words in English T2. In addi-
tion, we hypothesize that the absence of a number
from T2 may be a more significant missing element
of T2 from T1. We therefore include as a feature
the count of tokens comprised of digits in T2 that
are not also present in T1. The final word align-
ment feature attempts to refine A1, by distinguishing
words that are rarely unaligned in German?English
translations. Statistics are computed for every lexi-
cal item from German?English Europarl translations
to produce a lexical unalignment probability, com-
puted for each lexical item based on its relative fre-
quency in the corpus when it is not aligned to any
other word.
The backward classifier uses the same features but
computed for each test pair on counts of unaligned
T1 words.
4 Results
Results for several combinations of features are
shown in Table 1 when the system is trained on
the 500-pair development set training corpus and
tested on the 500-pair held-out development test set
(DEV), in addition to results for feature combina-
tions when trained on the entire 1000-pair develop-
ment data and tested on the held-out 500-pair gold
standard (TEST) (Negri et al, 2011), when the sys-
tem is evaluated as two separate binary forward and
backward classifiers (2-CLASS) as well as the final
evaluation including all four entailment classes (4-
CLASS). The highest accuracy is achieved by the
classifier using the single feature of counts of un-
aligned words, A1, of 34.6%. As two separate bi-
nary classifiers, the alignment features, A1+A2+A3,
achieve a relatively high accuracy of 74.0% for for-
ward with somewhat less accurate for backward
(65.8%) classification (both over the DEV data).
When combined to the final four CLTE classes, how-
ever, accuracy drops significantly to an overall accu-
racy of 50% (also over DEV). A main cause is inac-
curate labeling of no entailment gold standard test
pairs, as the most severe decline is for recall of test
pairs for this label (38.4%).
Accuracy on the development set for the word
alignment features, A1+A2+A3, compared to the
test set shows a sever decline, from 50% to 32%. On
the test data, however, a main cause of inaccuracy
is that backward gold standard test pairs, although
achieving close accuracy to forward when evaluated
as binary classifiers, are inaccurately labeled in the
4-class evaluation, as recall for backward drops to
only 18.4% for this label.
Another insight revealed for the alignment fea-
tures, A1+A2+A3, in the 4-class evaluation is that
when run on the development set, the classes for-
ward and backward achieve significantly higher
f-scores compared to no entailment. However,
the contrary is observed for the test data, as
no entailment achieve higher results than both uni-
directional classes. This appears at first to be a
somewhat counter-intuitive result, but in this case,
the system is simply better at predicting forward and
backward when no entailment exists for a translation
pair compared to when a unidirectional entailment is
present.
4.1 String Similarity Features
In addition to the word alignment features, subse-
quent to submitting results to the shared task, we
have carried out additional experiments using string
similarity features, based on our recent success in
apply string similarity to both the estimation of com-
positionality of MWEs (Salehi and Cook, to appear)
and also the estimation of similarity between short
134
2-CLASS 4-CLASS
Acc. Prec Recall F1 Acc. Prec Recall F1
D
E
V
A1 + A2 + A3
bwrd 65.80 63.12 76.00 68.96 50.00 bwrd 54.80 59.20 56.90
fwrd 74.00 72.22 78.00 75.00 fwrd 54.80 45.60 49.80
none 50.50 38.40 43.60
bidir 42.80 56.80 48.80
S1 + S2 + S3
bwrd 58.20 57.75 61.20 59.42 27.40 bwrd 14.30 0.80 1.50
fwrd 47.00 47.17 50.00 59.42 fwrd 0.00 0.00 0.00
none 30.70 39.70 39.70
bidir 25.60 52.80 34.50
T
E
S
T
A1
bwrd 57.00 58.54 48.00 52.75 34.60 bwrd 25.50 19.20 21.90
fwrd 58.40 58.75 56.40 57.55 fwrd 34.90 36.00 35.40
none 36.70 48.80 41.90
bidir 38.70 34.40 36.40
A2
bwrd 50.00 0.00 0.00 0.00 33.60 bwrd 24.70 18.40 21.10
fwrd 51.60 50.85 95.20 66.29 fwrd 34.70 34.40 34.50
none 36.90 38.40 37.60
bidir 35.30 43.20 38.80
A3
bwrd 54.80 55.61 47.60 51.29 34.20 bwrd 32.70 26.40 29.20
fwrd 61.20 61.57 59.60 60.57 fwrd 33.30 34.40 33.90
none 36.90 46.40 41.10
bidir 32.70 29.60 31.10
A1+A2
bwrd 57.60 57.72 56.80 57.26 33.60 bwrd 24.70 18.40 21.10
fwrd 59.80 58.84 65.20 61.86 fwrd 34.70 34.40 34.50
none 36.90 38.40 37.60
bidir 35.30 43.20 38.80
A1+A3
bwrd 57.20 57.96 52.40 55.04 33.00 bwrd 26.60 20.00 22.80
fwrd 58.60 58.05 62.00 59.96 fwrd 31.90 34.40 33.10
none 36.70 40.80 38.60
bidir 34.80 36.80 35.80
A2+A3
bwrd 54.80 55.83 46.00 50.44 33.40 bwrd 32.30 25.60 28.60
fwrd 61.00 61.70 58.00 59.79 fwrd 32.80 33.60 33.20
none 34.90 46.40 39.90
bidir 32.70 28.00 30.20
A1 + A2 + A3
bwrd 57.60 57.72 56.80 57.26 32.00 bwrd 24.00 18.40 20.80
fwrd 59.20 58.39 64.00 61.07 fwrd 32.30 32.00 32.10
none 36.20 37.60 36.90
bidir 34.70 41.60 37.80
S1 + S2 + S3
bwrd 53.20 53.77 45.60 49.35 26.00 bwrd 20.00 1.50 29.50
fwrd 48.60 48.36 41.20 44.49 fwrd 16.70 0.80 31.50
none 28.00 63.20 38.80
bidir 23.70 39.20 29.50
A1 + A2 + A3 + S1
bwrd 57.40 58.30 52.00 54.97 33.00 bwrd 27.60 19.20 22.60
fwrd 59.80 58.84 65.20 61.86 fwrd 29.80 33.60 31.60
none 38.20 41.60 39.80
bidir 34.60 37.60 36.00
A1 + A2 + A3 + S2
bwrd 57.80 58.52 53.60 55.95 32.60 bwrd 26.70 19.20 22.30
fwrd 59.60 58.70 64.80 61.60 fwrd 30.70 33.60 32.10
none 37.30 40.00 38.60
bidir 33.80 37.60 35.60
A1 + A2 + A3 +S3
bwrd 58.20 58.51 56.40 57.44 32.80 bwrd 24.70 19.20 21.60
fwrd 59.60 58.82 64.00 61.30 fwrd 32.00 32.80 32.40
none 37.40 39.20 38.30
bidir 34.70 40.00 37.20
Table 1: Cross-lingual Textual Entailment Results for Word alignment Features and String Similarity Measures, A1
= count of unaligned words in T2, A2 = count of unaligned numbers in T2, A3 = count of unaligned words in T2
with unaligned probability < 0.11, S1 = Number of matched words in the aligned sequence given by Smith-Waterman
algorithm, S2 = Penalty of aligning sentences using Smith-Waterman algorithm, S3 = Levenshtein distance between
the sentences
135
texts in the *SEM 2013 Shared Task (Gella et al,
to appear). Using the alignments, we replace each
English word with its corresponding word in Ger-
man. The resulting German sentence is compared
with the actual one using string similarity measures.
As the structure of both English and German sen-
tences are usually SVO, we hypothesize that when
there is no entailment between the two given sen-
tences, the newly-made German sentence and the
original German sentence will differ a lot in word
order.
In order to compare the two German sentences,
we use the Levenshtein (Levenshtein, 1966) and the
Smith-Waterman (Smith and Waterman, 1981) al-
gorithm. The Levenshtein algorithm measures the
number of world-level edits to change one sentence
into another. The edit operators consist of insertion
and deletion. We consider substitution as two edits
(combination of insertion and deletion) based on the
findings of Baldwin (2009).
We also use Smith-Waterman (SW) algorithm,
which was originally developed to find the most sim-
ilar region between two proteins. The algorithm
looks for the longest common substring, except that
it permits small numbers of penalized editions con-
sisting of insertion, deletion and substitution. We
call the best found substring the ?SW aligned se-
quence?. In this experiment, we consider the number
of matched words and the number of penalties in the
SW aligned sequence as features.
Results for the string similarity features are shown
in Table 1. Since the string similarity feature scores
do not take the entailment direction into account,
i.e. there is a single set of feature scores for each
text fragment pair as there is no distinction between
forward and backward entailment, and they are not
suited for standalone use in compositional classifica-
tion. We do, however, include these scores in Table
1 to illustrate how with the compositional approach
using the same set of features for forward and back-
ward ultimately results in a classification of test pairs
as either bidirectional or no entailment.
When individual string similarity features are
added to the word alignment features, minor gains in
accuracy are achieved over the word alignment fea-
tures alone, +1% for S1, +0.6% for S2 and +0.8%
for S3 (= Levenstein).
5 Possible Additions: Dictionary Features
We hypothesize that when there is no entailment be-
tween the two sentences, the aligner may not accu-
rately align words. An on-line dictionary contain-
ing lemmatized words, such as Panlex (Baldwin and
Colowick, 2010), could be used to avoid errors in
such cases. Dictionary-based feature scores based
on the presence or absence of alignments in the dic-
tionary could then be applied.
6 Conclusions
This paper describes a compositional cross-lingual
approach to CLTE with experiments carried out
for the German-English language pair. Our results
showed that in the first stages of binary classification
as forward and backward, the word alignment fea-
tures alone achieved good accuracy but when com-
bined suffer severely. Accuracy of the approach
using word alignment features could benefit from
a more directional multi-class classification as op-
posed to the compositional approach we used. In
addition, results showed minor increases in accuracy
can be achieved using string similarity measures.
Acknowledgments
This work was supported by the Australian Research
Council.
References
Timothy Baldwin and Jonathan Pool Susan M. Colowick.
2010. Panlex and lextract: Translating all words of all
languages of the world. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 37?40.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In TAC 2009 Workshop
Proceedings, Gaithersburg, MD.
L. Bentivogli, P. Clark, I. Dagan, H. T. Dang, and D. Gi-
ampiccolo. 2010. The sixth PASCAL recognizing
textual entailment challenge. In TAC 2010 Workshop
Proceedings, Gaithersburg, MD.
Spandana Gella, Bahar Salehi, Marco Lui, Karl Grieser,
Paul Cook, and Timothy Baldwin. to appear. Integrat-
ing predictions from multiple domains and feature sets
136
for estimating semantic textual similarity. In Proceed-
ings of *SEM 2013 Shared Task STS.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan HerbstHieu Hoang. 2007. Moses:
Open Source Toolkit for Statistical Machine Transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), demonstration session,
Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit, Phuket, Thailand.
Vladimir I Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
cross-lingual textual entailment. In Proceedings of
NAACL-HLT.
Y. Mehdad, M. Negri, and M. Federico. 2011. Using par-
allel corpora for cross-lingual textual entailment. In
Proceedings of ACL-HLT 2011.
Yashar Mehdad, Matteo Negri, and Jose G. C. de Souza.
2012. Fbk: Cross-lingual textual entailment with-out
translation. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval2012).
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of EMNLP 2011.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entailment
for content synchronization. In First Joint Conference
on Lexical and Computational Semantics, pages 399?
407, Montreal, Canada.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
20?28, College Park, MD.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings of
the Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM 2013).
Temple F Smith and Michael S Waterman. 1981. The
identification of common molecular subsequences.
Journal of Molecular Biology, 147:195?197.
137
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 118?126,
COLING 2010, Beijing, August 2010.
Deep Syntax Language Models and Statistical Machine Translation
Yvette Graham
NCLT
Dublin City University
ygraham@computing.dcu.ie josef@computing.dcu.ie
Josef van Genabith
CNGL
Dublin City University
Abstract
Hierarchical Models increase the re-
ordering capabilities of MT systems
by introducing non-terminal symbols to
phrases that map source language (SL)
words/phrases to the correct position
in the target language (TL) translation.
Building translations via discontiguous
TL phrases increases the difficulty of lan-
guage modeling, however, introducing the
need for heuristic techniques such as cube
pruning (Chiang, 2005), for example.
An additional possibility to aid language
modeling in hierarchical systems is to use
a language model that models fluency of
words not using their local context in the
string, as in traditional language models,
but instead using the deeper context of
a word. In this paper, we explore the
potential of deep syntax language mod-
els providing an interesting comparison
with the traditional string-based language
model. We include an experimental evalu-
ation that compares the two kinds of mod-
els independently of any MT system to in-
vestigate the possible potential of integrat-
ing a deep syntax language model into Hi-
erarchical SMT systems.
1 Introduction
In Phrase-Based Models of Machine Translation
all phrases consistent with the word alignment
are extracted (Koehn et al, 2003), with shorter
phrases needed for high coverage of unseen data
and longer phrases providing improved fluency in
target language translations. Hierarchical Mod-
els (Chiang, 2007; Chiang, 2005) build on Phrase-
Based Models by relaxing the constraint that
phrases must be contiguous sequences of words
and allow a short phrase (or phrases) nested within
a longer phrase to be replaced by a non-terminal
symbol forming a new hierarchical phrase. Tra-
ditional language models use the local context of
words to estimate the probability of the sentence
and introducing hierarchical phrases that generate
discontiguous sequences of TL words increases
the difficulty of computing language model proba-
bilities during decoding and require sophisticated
heuristic language modeling techniques (Chiang,
2007; Chiang, 2005).
Leaving aside heuristic language modeling for
a moment, the difficulty of integrating a tradi-
tional string-based language model into the de-
coding process in a hierarchical system, highlights
a slight incongruity between the translation model
and language model in Hierarchical Models. Ac-
cording to the translation model, the best way to
build a fluent TL translation is via discontiguous
phrases, while the language model can only pro-
vide information about the fluency of contiguous
sequences of words. Intuitively, a language model
that models fluency between discontiguous words
may be well-suited to hierarchical models. Deep
syntax language models condition the probability
of a word on its deep context, i.e. words linked to
it via dependency relations, as opposed to preced-
ing words in the string. During decoding in Hi-
erarchical Models, words missing a context in the
string due to being preceded by a non-terminal,
might however be in a dependency relation with
a word that is already present in the string and
118
this context could add useful information about
the fluency of the hypothesis as its constructed.
In addition, using the deep context of a word
provides a deeper notion of fluency than the lo-
cal context provides on its own and this might be
useful to improve such things as lexical choice in
SMT systems. Good lexical choice is very im-
portant and the deeper context of a word, if avail-
able, may provide more meaningful information
and result in better lexical choice. Integrating
such a model into a Hierarchical SMT system is
not straightforward, however, and we believe be-
fore embarking on this its worthwhile to evalu-
ate the model independently of any MT system.
We therefore provide an experimental evaluation
of the model and in order to provide an interesting
comparison, we evaluate a traditional string-based
language model on the same data.
2 Related Work
The idea of using a language model based on deep
syntax is not new to SMT. Shen et al (2008) use
a dependency-based language model in a string
to dependency tree SMT system for Chinese-
English translation, using information from the
deeper structure about dependency relations be-
tween words, in addition to the position of the
words in the string, including information about
whether context words were positioned on the left
or right of a word. Bojar and Hajic? (2008) use a
deep syntax language model in an English-Czech
dependency tree-to-tree transfer system, and in-
clude three separate bigram language models: a
reverse, direct and joint model. The model in our
evaluation is similar to their direct bigram model,
but is not restricted to bigrams.
Riezler and Maxwell (2006) use a trigram deep
syntax language model in German-English depen-
dency tree-to-tree transfer to re-rank decoder out-
put. The language model of Riezler and Maxwell
(2006) is similar to the model in our evaluation,
but differs in that it is restricted to a trigram model
trained on LFG f-structures. In addition, as lan-
guage modeling is not the main focus of their
work, they provide little detail on the language
model they use, except to say that it is based on
?log-probability of strings of predicates from root
to frontier of target f-structure, estimated from
predicate trigrams in English f-structures? (Rie-
zler and Maxwell, 2006). An important prop-
erty of LFG f-structures (and deep syntactic struc-
tures in general) was possibly overlooked here.
F-structures can contain more than one path of
predicates from the root to a frontier that in-
clude the same ngram, and this occurs when the
underlying graph includes unary branching fol-
lowed by branching with arity greater than one.
In such cases, the language model probability as
described in Riezler and Maxwell (2006) is incor-
rect as the probability of these ngrams will be in-
cluded multiple times. In our definition of a deep
syntax language model, we ensure that such du-
plicate ngrams are omitted in training and testing.
In addition, Wu (1998) use a bigram deep syntax
language model in a stochastic inversion transduc-
tion grammar for English to Chinese. None of the
related research we discuss here has included an
evaluation of the deep syntax language model they
employ in isolation from the MT system, however.
3 Deep Syntax
The deep syntax language model we describe is
not restricted to any individual theory of deep
syntax. For clarity, however, we restrict our ex-
amples to LFG, which is also the deep syntax
theory we use for our evaluation. The Lexical
Functional Grammar (LFG) (Kaplan and Bres-
nan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrym-
ple, 2001) functional structure (f-structure) is an
attribute-value encoding of bi-lexical labeled de-
pendencies, such as subject, object and adjunct
for example, with morpho-syntactic atomic at-
tributes encoding information such as mood and
tense of verbs, and person, number and case for
nouns. Figure 1 shows the LFG f-structure for En-
glish sentence ?Today congress passed Obama?s
health care bill.?1
Encoded within the f-structure is a directed
graph and our language model uses a simplified
acyclic unlabeled version of this graph. Figure
1(b) shows the graph structure encoded within the
f-structure of Figure 1(a). We discuss the simpli-
fication procedure later in Section 5.
1Morpho-syntactic information/ atomic features are omit-
ted from the diagram.
119
(a) ?
??????????
PRED pass
SUBJ
[
PRED congress
]
OBJ
?
????
PRED bill
SPEC
[
POSS
[
PRED Obama
]]
MOD
[
PRED care
MOD
[
PRED health
]
]
?
????
ADJ
[
PRED today
]
?
??????????
(b) <s>
pass
today congress bill
</s> </s> obama care
</s> health
</s>
Figure 1: ?Today congress passed Obama?s health care bill.?
4 Language Model
We use a simplified approximation of the deep
syntactic structure, de, that encodes the unlabeled
dependencies between the words of the sentence,
to estimate a deep syntax language model prob-
ability. Traditional string-based language mod-
els combine the probability of each word in the
sentence, wi, given its preceding context, the se-
quence of words from w1 to wi?1, as shown in
Equation 1.
p(w1, w2, ..., wl) =
l?
i=1
p(wi|w1, ..., wi?1) (1)
In a similar way, a deep syntax language model
probability combines the probability of each word
in the structure, wi, given its context within the
structure, the sequence of words from wr, the
head of the sentence, to wm(i), as shown in Equa-
tion 2, with function m used to map the index of a
word in the structure to the index of its head. 2
p(de) =
l?
i=1
p(wi|wr, ..., wm(m(i))wm(i)) (2)
In order to combat data sparseness, we apply
the Markov assumption, as is done in traditional
string-based language modeling, and simplify the
probability by only including a limited length of
history when estimating the probability of each
2We refer to the lexicalized nodes in the dependency
structure as words, alternatively the term predicate can be
used.
word in the structure. For example, a trigram deep
syntax language model conditions the probability
of each word on the sequence of words consisting
of the head of the head of the word followed by
the head of the word as follows:
p(de) =
l?
i=1
P (wi|wm(m(i)) , wm(i)) (3)
In addition, similar to string-based language
modeling, we add a start symbol, <s>, at the
root of the structure and end symbols, </s>, at
the leaves to include the probability of a word be-
ing the head of the sentence and the probability
of words occurring as leaf nodes in the structure.
Figure 2(a) shows an example of how a trigram
deep syntax language model probability is com-
puted for the example sentence in Figure 1(a).
5 Simplified Approximation of the Deep
Syntactic Representation
We describe the deep syntactic structure, de, as
an approximation since a parser is employed to
automatically produce it and there is therefore no
certainty that we use the actual/correct deep syn-
tactic representation for the sentence. In addi-
tion, the function m requires that each node in the
structure has exactly one head, however, structure-
sharing can occur within deep syntactic structures
resulting in a single word legitimately having two
heads. In such cases we use a simplification of
the graph in the deep syntactic structure. Fig-
ure 3 shows an f-structure in which the subject
120
(a) Deep Syntax LM (b) Traditional LM
p(e) ? p( pass | <s>)? p(e) ? p( passed | today congress )?
p( today | <s> pass )? p( today | <s>)?
p(</s> | pass today )?
p( congress | <s> pass )? p( congress | <s> today )?
p(</s> | pass congress )?
p( bill | <s> pass )? p( bill | health care )?
p( obama | pass bill )? p( obama | congress passed )?
p(</s> | bill obama )?
p( care | pass bill )? p( care | s health )?
p( health | bill care )? p( health | ? s )?
p(</s> | care health )
p( ? | passed Obama )?
p( s | obama ? )?
p( . | care bill )?
p(</s> | bill . )
Figure 2: Example Comparison of Deep Syntax and Traditional Language Models
of both like, be and president is hillary. In our
simplified structure, the dependency relations be-
tween be and hillary and president and hillary are
dropped. We discuss how we do this later in Sec-
tion 6. Similar to our simplification for structure
sharing, we also simplify structures that contain
cycles by discarding edges that cause loops in the
structure.
6 Implementation
SRILM (Stolcke, 2002) can be used to compute
a language model from ngram counts (the -read
option of the ngram-count command). Implemen-
tation to train the language model, therefore, sim-
ply requires accurately extracting counts from the
deep syntax parsed training corpus. To simplify
the structures to acyclic graphs, nodes are labeled
with an increasing index number via a depth first
traversal. This allows each arc causing a loop in
the graph or argument sharing to be identified by
a simple comparison of index numbers, as the in-
dex number of its start node will be greater than
that of its end node. The algorithm we use to
extract ngrams from the dependency structures is
straightforward: we simply carry out a depth-first
traversal of the graph to construct paths of words
that stretch from the root of the graph to words
?
????????????????
PRED like
SUBJ 1:
[
PRED Hillary
]
XCOMP
?
???????????
PRED be
SUBJ 1
XCOMP-PRED
[
PRED president
SUBJ 1
]
ADJ
?
??
PRED at
OBJ
[
PRED U.N.
SPEC
[
PRED the
]
]
?
??
?
???????????
?
????????????????
<s>
like
hillary be
</s> president at
</s> U.N.
the
</s>
Figure 3: ?Hillary liked being president at the
U.N.?
121
?
????????????
PRED agree
SUBJ
[
PRED nobody
]
XCOMP
?
???????
PRED with
OBJ
?
?????
PRED point
ADJ
?
??
??
COORD and{[
PRED two
]
,[
PRED three
]
}
?
??
??
?
?????
?
???????
?
????????????
<s>
agree
nobody with
</s> point
and
two three
</s> </s>
Figure 4: ?Nobody agreed with points two and
three.?
at the leaves and then extract the required order
ngrams from each path. As mentioned earlier,
some ngrams can belong to more than one path.
Figure 4 shows an example structure containing
unary branching followed by binary branching in
which the sequence of symbols and words ?<s>
agree with point and? belong to the path ending
in two </s> and three </s>. In order to ensure
that only distinct ngrams are extracted we assign
each word in the structure a unique id number
and include this in the extracted ngrams. Paths
are split into ngrams and duplicate ngrams result-
ing from their occurrence in more than one path
are discarded. Its also possible for ngrams to le-
gitimately be repeated in a deep structure, and in
such cases we do not discard these ngrams. Legit-
imately repeating ngrams are easily identified as
the id numbers attached to words will be differ-
ent.
7 Deep Syntax and Lexical Choice in
SMT
Correct lexical choice in machine translation is
extremely important and PB-SMT systems rely
on the language model to ensure, that when two
phrases are combined with each other, that the
model can rank combined phrases that are flu-
ent higher than less fluent combinations. Con-
ditioning the probability of each word on its
deep context has the potential to provide a
more meaningful context than the local context
within the string. A comparison of the proba-
bilities of individual words in the deep syntax
model and traditional language model in Figure
2 clearly shows this. For instance, let us con-
sider how the language model in a German to
English SMT system is used to help rank the
following two translations today congress passed
... and today convention passed ... (the word
Kongress in German can be translated into ei-
ther congress or convention in English). In
the deep syntax model, the important compet-
ing probabilities are (i) p(congress|<s>pass)
and (ii) p(convention|<s>pass), where (i)
can be interpreted as the probability of the
word congress modifying pass when pass is
the head of the entire sentence and, simi-
larly (ii) the probability of the word conven-
tion modifying pass when pass is the head of
the entire sentence. In the traditional string-
based language model, the equivalent compet-
ing probabilities are (i) p(congress|<s>today),
the probability of congress following today when
today is the start of the sentence and (ii)
p(convention|<s>today), probability of con-
vention following today when today is the start
of the sentence, showing that the deep syntax
language model is able to use more meaningful
context for good lexical choice when estimating
the probability of words congress and convention
compared to the traditional language model.
In addition, the deep syntax language model
will encounter less data sparseness problems for
some words than a string-based language model.
In many languages words occur that can legiti-
mately be moved to different positions within the
string without any change to dependencies be-
tween words. For example, sentential adverbs
in English, can legitimately change position in
a sentence, without affecting the underlying de-
pendencies between words. The word today in
?Today congress passed Obama?s health bill?
122
can appear as ?Congress passed Obama?s health
bill today? and ?Congress today passed Obama?s
health bill?. Any sentence in the training cor-
pus in which the word pass is modified by today
will result in a bigram being counted for the two
words, regardless of the position of today within
each sentence.
In addition, some surface form words such as
auxiliary verbs for example, are not represented
as predicates in the deep syntactic structure. For
lexical choice, its not really the choice of auxiliary
verbs that is most important, but rather the choice
of an appropriate lexical item for the main verb
(that belongs to the auxiliary verb). Omitting aux-
iliary verbs during language modeling could aid
good lexical choice, by focusing on the choice of
a main verb without the effect of what auxiliary
verb is used with it.
For some words, however, the probability in the
string-based language model provides as good if
not better context than the deep syntax model, but
only for the few words that happen to be preceded
by words that are important to its lexical choice,
and this reinforces the idea that SMT systems can
benefit from using both a deep syntax and string-
based language model. For example, the proba-
bility of bill in Figures 2(a) and 2(b) is computed
in the deep syntax model as p(bill| <s> pass)
and in the string-based model using p(bill|health
care), and for this word the local context seems to
provide more important information than the deep
context when it comes to lexical choice. The deep
model nevertheless adds some useful information,
as it includes the probability of bill being an argu-
ment of pass when pass is the head of a sentence.
In traditional language modeling, the special
start symbol is added at the beginning of a sen-
tence so that the probability of the first word ap-
pearing as the first word of a sentence can be
included when estimating the probability. With
similar motivation, we add a start symbol to the
deep syntactic representation so that the probabil-
ity of the head of the sentence occurring as the
head of a sentence can be included. For exam-
ple, p(be| <s>) will have a high probability as
the verb be is the head of many sentences of En-
glish, whereas p(colorless| <s>) will have a low
probability since it is unlikely to occur as the head.
We also add end symbols at the leaf nodes in the
structure to include the probability of these words
appearing at that position in a structure. For in-
stance, a noun followed by its determiner such as
p(</s> |attorney a) would have a high probabil-
ity compared to a conjunction followed by a verb
p(</s> |and be).
8 Evaluation
We carry out an experimental evaluation to inves-
tigate the potential of the deep syntax language
model we describe in this paper independently of
any machine translation system. We train a 5-
gram deep syntax language model on 7M English
f-structures, and evaluate it by computing the per-
plexity and ngram coverage statistics on a held-
out test set of parsed fluent English sentences. In
order to provide an interesting comparison, we
also train a traditional string-based 5-gram lan-
guage model on the same training data and test
it on the same held-out test set of English sen-
tences. A deep syntax language model comes with
the obvious disadvantage that any data it is trained
on must be in-coverage of the parser, whereas a
string-based language model can be trained on any
available data of the appropriate language. Since
parser coverage is not the focus of our work, we
eliminate its effects from the evaluation by select-
ing the training and test data for both the string-
based and deep syntax language models on the ba-
sis that they are in fact in-coverage of the parser.
8.1 Language Model Training
Our training data consists of English sentences
from the WMT09 monolingual training corpus
with sentence length range of 5-20 words that are
in coverage of the parsing resources (Kaplan et al,
2004; Riezler et al, 2002) resulting in approxi-
mately 7M sentences. Preparation of training and
test data for the traditional language model con-
sisted of tokenization and lower casing. Parsing
was carried out with XLE (Kaplan et al, 2002)
and an English LFG grammar (Kaplan et al,
2004; Riezler et al, 2002). The parser produces
a packed representation of all possible parses ac-
cording to the LFG grammar and we select only
the single best parse for language model training
by means of a disambiguation model (Kaplan et
123
Corpus Tokens Ave. Tokens Vocab
per Sent.
strings 138.6M 19 345K
LFG lemmas/predicates 118.4M 16 280K
Table 1: Language model statistics for string-based and deep syntax language models, statistics are for
string tokens and LFG lemmas for the same set of 7.29M English sentences
al., 2004; Riezler et al, 2002). Ngrams were auto-
matically extracted from the f-structures and low-
ercased. SRILM (Stolcke, 2002) was used to com-
pute both language models. Table 1 shows statis-
tics on the number of words and lemmas used to
train each model.
8.2 Testing
The test set consisted of 789 sentences selected
from WMT09 additional development sets3 con-
taining English Europarl text and again was se-
lected on the basis of sentences being in-coverage
of the parsing resources. SRILM (Stolcke, 2002)
was used to compute test set perplexity and ngram
coverage statistics for each order model.
Since the deep syntax language model adds end
of sentence markers to leaf nodes in the structures,
the number of (so-called) end of sentence markers
in the test set for the deep syntax model is much
higher than in the string-based model. We there-
fore also compute statistics for each model when
end of sentence markers are omitted from training
and testing. 4 In addition, since the vast majority
of punctuation is not represented as predicates in
LFG f-structures, we also test the string-based lan-
guage model when punctuation has been removed.
8.3 Results
Table 2 shows perplexity scores and ngram cover-
age statistics for each order and type of language
model. Note that perplexity scores for the string-
based and deep syntax language models are not
directly comparable because each model has a dif-
ferent vocabulary. Although both models train on
an identical set of sentences, the data is in a dif-
ferent format for each model, as the string-based
3test2006.en and test2007.en
4When we include end of sentence marker probabilities
we also include them for normalization, and omit them from
normalization when their probabilities are omitted.
model is trained on surface form tokens, whereas
the deep syntax model uses lemmas. Ngram cov-
erage statistics provide a better comparison.
Unigram coverage for all models is high with
all models achieving close to 100% coverage on
the held-out test set. Bigram coverage is high-
est for the deep syntax language model when eos
markers are included (94.71%) with next high-
est coverage achieved by the string-based model
that includes eos markers (93.09%). When eos
markers are omitted bigram coverage goes down
slightly to 92.44% for the deep syntax model and
to 92.83% for the string-based model, and when
punctuation is also omitted from the string-based
model, coverage goes down again to 91.57%.
Trigram coverage statistics for the test set main-
tain the same rank between models as in the bi-
gram coverage, from highest to lowest as follows:
DS+eos at 64.71%, SB+eos at 58.75%, SB-eos
at 56.89%, DS-eos at 53.67%, SB-eos-punc at
53.45%. For 4-gram and 5-gram coverage a sim-
ilar coverage ranking is seen, but with DS-eos
(4gram at 17.17%, 5gram at 3.59%) and SB-eos-
punc (4gram at 20.24%, 5gram at 5.76%) swap-
ping rank position.
8.4 Discussion
Ngram coverage statistics for the DS-eos and
SB-eos-punc models provide the fairest com-
parison, with the deep syntax model achiev-
ing higher coverage than the string-based model
for bigrams (+0.87%) and trigrams (+0.22%),
marginally lower coverage coverage of unigrams
(-0.02%) and lower coverage of 4-grams (-3.07%)
and 5-grams (2.17%) compared to the string-
based model.
Perplexity scores for the deep syntax model
when eos symbols are included are low (79 for the
5gram model) and this is caused by eos markers
124
1-gram 2-gram 3-gram 4-gram 5-gram
cov. ppl cov. ppl cov. ppl cov. ppl cov. ppl
SB-eos 99.61% 1045 92.83% 297 56.89% 251 23.32% 268 7.19% 279
SB-eos-punc 99.58% 1357 91.57% 382 53.45% 327 20.24% 348 5.76% 360
DS-eos 99.56% 1005 92.44% 422 53.67% 412 17.17% 446 3.59% 453
SB+eos 99.63% 900 93.09% 227 58.75% 194 25.48% 207 8.35% 215
DS+eos 99.70% 211 94.71% 77 64.71% 73 29.86% 78 8.75% 79
Table 2: Ngram coverage and perplexity (ppl) on held-out test set. Note: DS = deep syntax, SB string-
based, eos = end of sentence markers
in the test set in general being assigned relatively
high probabilities by the model, and since several
occur per sentence, the perplexity increases when
the are omitted (453 for the 5gram model).
Tables 3 and 4 show the most frequently en-
countered trigrams in the test data for each type
of model. A comparison shows how different the
two models are and highlights the potential of the
deep syntax language model to aid lexical choice
in SMT systems. Many of the most frequently oc-
curring trigram probabilities for the deep syntax
model are for arguments of the main verb of the
sentence, conditioned on the main verb, and in-
cluding such probabilities in a system could im-
prove fluency by using information about which
words are in a dependency relation together ex-
plicitely in the model. In addition, a frequent tri-
gram in the held-out data is <s> be also, where
the word also is a sentential adverb modifying
be. Trigrams for sentential adverbs are likely to
be less effected by data sparseness in the deep
syntax model compared to the string-based model
which could result in the deep syntax model im-
proving fluency with respect to combinations of
main verbs and their modifying adverbs. The most
frequent trigram in the deep syntax test set is <s>
and be, in which the head of the sentence is the
conjunction and with argument be. In this type of
syntactic construction in English, its often the case
that the conjunction and verb will be distant from
each other in the sentence, for example: Nobody
was there except the old lady and without thinking
we quickly left. (where was and and are in a de-
pendency relation). Using a deep syntax language
model could therefore improve lexical choice for
such words, since they are too distant for a string-
3-gram No. Occ. Prob.
<s> and be 42 0.1251
<s> be this 21 0.0110
<s> must we 19 0.0347
<s> would i 19 0.0414
<s> be in 17 0.0326
<s> be that 14 0.0122
be debate the 13 0.0947
<s> be debate 13 0.0003
<s> can not 12 0.0348
<s> and president 11 0.0002
<s> would like 11 0.0136
<s> would be 11 0.0835
<s> be also 10 0.0075
Table 3: Most frequent trigrams in test set for deep
syntax model
based model.
9 Conclusions
We presented a comparison of a deep syntax
language and traditional string-based language
model. Results showed that the deep syntax lan-
guage model achieves similar ngram coverage to
the string-based model on a held out test set.
We highlighted the potential of integrating such
a model into SMT systems for improving lexical
choice by using a deeper context for probabilities
of words compared to a string-based model.
References
Bojar, Ondr?ej, Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the third Work-
125
3-gram No. Occ. Prob.
mr president , 40 0.5385
<s> this is 25 0.1877
by the european 20 0.0014
the european union 18 0.1096
<s> it is 16 0.1815
the european parliament 15 0.0252
would like to 15 0.4944
<s> i would 15 0.0250
<s> that is 14 0.1094
i would like 14 0.0335
and gentlemen , 13 0.1005
ladies and gentlemen 13 0.2834
<s> we must 12 0.0120
should like to 12 0.1304
i should like 11 0.0089
, ladies and 11 0.5944
, it is 10 0.1090
Table 4: Most frequent trigrams in test set for
string-based model
shop on Statistical Machine Translation, Columbus,
Ohio.
Bresnan, Joan. 2001. Lexical-Functional Syntax.,
Blackwell Oxford.
Chiang, David. 2007. Hierarchical Phrase-based
Models of Translation In Computational Linguis-
tics, No. 33:2.
Chiang, David. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263-270,
Ann Arbor, Michigan.
Dalrymple, Mary. 2001. Lexical Functional Gram-
mar, Academic Press, San Diego, CA; London.
Kaplan, Ronald, Stefan Riezler, Tracy H. King, John
T. Maxwell, Alexander Vasserman. 2004. Speed
and Accuracy in Shallow and Deep Stochastic Pars-
ing. In Proceedings of Human Language Tech-
nology Conference/North American Chapter of the
Association for Computational Linguistics Meeting,
Boston, MA.
Kaplan, Ronald M., Tracy H. King, John T. Maxwell.
2002. Adapting Existing Grammars: the XLE Ex-
perience. In Proceedings of the 19th International
Conference on Computational Linguistics (COL-
ING) 2002, Taipei, Taiwan.
Kaplan, Ronald M. 1995. The Formal Architecture of
Lexical Functional Grammar. In Formal Issues in
Lexical Functional Grammar, ed. Mary Dalrymple,
pages 7-28, CSLI Publications, Stanford, CA.
Kaplan, Ronald M., Joan Bresnan. 1982. Lexical
Functional Grammar, a Formal System for Gram-
matical Represenation. In J. Bresnan, editor, The
Mental Representation of Grammatical Relations,
173-281, MIT Press, Cambridge, MA.
Koehn, Philipp, Hieu Hoang. 2007. Factored Trans-
lation Models. Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, 868-876.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicoli Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics, demonstration session
Koehn, Philipp 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the tenth Machine Translation Summit.
Koehn, Philipp, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-based Translation. In Proceed-
ings of Human Language Technology and North
American Chapter of the Association for Computa-
tional Linguistics Conference, 48-54.
Riezler, Stefan, John T. Maxwell III. 2006. Grammat-
ical Machine Translation. In Proceedings of HLT-
ACL, pages 248-255, New York.
Riezler, Stefan, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, Mark Johnson.
2002. Parsing the Wall Street Journal using Lexical
Functional Grammar and Discriminitive Estimation
Techniques . (grammar version 2005) In Proceed-
ings of the 40th ACL, Philadelphia.
Shen, Libin, Jinxi Xu, Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. Proceedings of ACL-08: HLT, pages 577-
585.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, Denver, Colorado.
Dekai, Wu, Hongsing Wong. 1998. Machine Trans-
lation with a Stochastic Grammatical Channel. In
Proceedings of the 36th ACL and 17th COLING,
Montreal, Quebec.
126
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 26?34,
Beijing, August 2010
Multi-Word Expression-Sensitive Word Alignment
Tsuyoshi Okita1, Alfredo Maldonado Guerra2, Yvette Graham3, Andy Way1
{CNGL1, NCLT3} / School of Computing / Dublin City University,
CNGL / School of Computer Science and Statistics / Trinity College Dublin2
{tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ie
Abstract
This paper presents a new word align-
ment method which incorporates knowl-
edge about Bilingual Multi-Word Expres-
sions (BMWEs). Our method of word
alignment first extracts such BMWEs in
a bidirectional way for a given corpus and
then starts conventional word alignment,
considering the properties of BMWEs in
their grouping as well as their alignment
links. We give partial annotation of align-
ment links as prior knowledge to the word
alignment process; by replacing the max-
imum likelihood estimate in the M-step
of the IBM Models with the Maximum A
Posteriori (MAP) estimate, prior knowl-
edge about BMWEs is embedded in the
prior in this MAP estimate. In our exper-
iments, we saw an improvement of 0.77
Bleu points absolute in JP?EN. Except
for one case, our method gave better re-
sults than the method using only BMWEs
grouping. Even though this paper does
not directly address the issues in Cross-
Lingual Information Retrieval (CLIR), it
discusses an approach of direct relevance
to the field. This approach could be
viewed as the opposite of current trends
in CLIR on semantic space that incorpo-
rate a notion of order in the bag-of-words
model (e.g. co-occurences).
1 Introduction
Word alignment (Brown et al, 1993; Vogel et
al., 1996; Och and Ney, 2003a; Graca et al,
2007) remains key to providing high-quality trans-
lations as all subsequent training stages rely on its
performance. It alone does not effectively cap-
ture many-to-many word correspondences, but in-
stead relies on the ability of subsequent heuristic
phrase extraction algorithms, such as grow-diag-
final (Koehn et al, 2003), to resolve them.
Some aligned corpora include implicit partial
alignment annotation, while for other corpora a
partial alignment can be extracted by state-of-
the-art techniques. For example, implicit tags
such as reference number within the patent cor-
pus of Fujii et al (2010) provide (often many-to-
many) correspondences between source and tar-
get words, while statistical methods for extract-
ing a partial annotation, like Kupiec et al (1993),
extract terminology pairs using linguistically pre-
defined POS patterns. Gale and Church (1991)
extract pairs of anchor words, such as num-
bers, proper nouns (organization, person, title),
dates, and monetary information. Resnik and
Melamed (1997) automatically extract domain-
specific lexica. Moore (2003) extracts named-
entities. In Machine Translation, Lambert and
Banchs (2006) extract BMWEs from a phrase ta-
ble, which is an outcome of word alignment fol-
lowed by phrase extraction; this method does not
alter the word alignment process.
This paper introduces a new method of incorpo-
rating previously known many-to-many word cor-
respondences into word alignment. A well-known
method of incorporating such prior knowledge
in Machine Learning is to replace the likelihood
maximization in the M-step of the EM algorithm
with either the MAP estimate or the Maximum
Penalized Likelihood (MPL) estimate (McLach-
26
lan and Krishnan, 1997; Bishop, 2006). Then, the
MAP estimate allows us to incorporate the prior,
a probability used to reflect the degree of prior be-
lief about the occurrences of the events.
A small number of studies have been carried
out that use partial alignment annotation for word
alignment. Firstly, Graca et al (2007) introduce
a posterior regularization to employ the prior that
cannot be easily expressed over model parameters
such as stochastic constraints and agreement con-
straints. These constraints are set in the E-step to
discard intractable alignments contradicting these
constraints. This mechanism in the E-step is in a
similar spirit to that in GIZA++ for IBM Model
3 and 4 which only searches around neighbour-
ing alignments around the Viterbi alignment. For
this reason, this algorithm is not intended to be
used combined with IBM Models 3 and 4. Al-
though theoretically it is possible to incorporate
partial annotation with a small change in its code,
Graca et al do not mention it. Secondly, Tal-
bot (2005) introduces a constrained EM method
which constrains the E-step to incorporate par-
tial alignment into word alignment,1 which is in
a similar manner to Graca et al (2007). He con-
ducted experiments using partial alignment anno-
tation based on cognate relations, a bilingual dic-
tionary, domain-specific bilingual semantic anno-
tation, and numerical pattern matching. He did
not incorporate BMWEs. Thirdly, Callison-Burch
et al (2004) replace the likelihood maximization
in the M-step with mixed likelihood maximiza-
tion, which is a convex combination of negative
log likelihood of known links and unknown links.
The remainder of this paper is organized as fol-
lows: in Section 2 we define the anchor word
alignment problem. In Section 3 we include
a review of the EM algorithm with IBM Mod-
els 1-5, and the HMM Model. Section 4 de-
scribes our own algorithm based on the combina-
tion of BMWE extraction and the modified word
alignment which incorporates the groupings of
BMWEs and enforces their alignment links; we
explain the EM algorithm with MAP estimation
1Although the code may be similar in practice to our Prior
Model I, his explanation to modify the E-step will not be
applied to IBM Models 3 and 4. Our view is to modify the
M-step due to the same reason above, i.e. GIZA++ searches
only over the alignment space around the Viterbi alignment.
pair GIZA++(no prior) Ours(with prior)
EN-FR fin ini prior fin ini prior
is NULL 1 .25 0 0 .25 .25
rosy en 1 .5 0 0 .5 .2
that . 1 .25 0 0 .25 .25
life la 1 .25 0 0 .25 0
. c? 1 .25 0 0 .25 .25
that c? 0 .25 0 1 .25 .25
is est 0 .25 0 1 .25 .25
life vie 0 .5 0 1 .5 1
rosy rose 0 .25 0 1 .25 .2
Table 1: The benefit of prior knowledge of anchor
words.
with three kinds of priors. In Section 5 our exper-
imental results are presented, and we conclude in
Section 6.
2 Anchor Word Alignment Problem
The input to standard methods of word alignment
is simply the sentence-aligned corpus, whereas
our alignment method takes in additionally a par-
tial alignment. We assume, therefore, the avail-
ability of a partial alignment, for example via a
MWE extraction tool. Let e? denote an English
sentence, and e denote an English word, through-
out this paper. The anchor word alignment prob-
lem is defined as follows:
Definition 1 (Anchor Word Alignment Problem)
Let (e?, f?) = {(e?1, f?1), . . . , (e?n, f?n)} be a parallel
corpus. By prior knowledge we additionally
have knowledge of anchor words (e?, f?) =
{(senti, te1, tf1 , pose1, posf1 , lengthe, lengthf ),
. . ., (sentk, ten , tfn , posen , posfn , lengthe,
lengthf )} where senti denotes sentence ID,
posei denotes the position of tei in a sentence e?i,
and lengthe (and lengthf ) denotes the sentence
length of the original sentence which includes
ei. Under a given (e?, f?) and (e?, f?), our objective
is to obtain word alignments. It is noted that an
anchor word may include a phrase pair which
forms n-to-m mapping objects.
Table 1 shows two example phrase pairs for
French to English c?est la vie and that is life, and
la vie en rose and rosy life with the initial value
for the EM algorithm, the prior value and the fi-
27
Statistical MWE extraction method
97|||groupe socialiste|||socialist group|||26|||26
101|||monsieur poettering|||mr poettering|||1|||4
103|||monsieur poettering|||mr poettering|||1|||11
110|||monsieur poettering|||mr poettering|||1|||9
117|||explication de vote|||explanation of vote|||28|||26
Heuristic-based MWE extraction method
28|||the wheel 2|||?? ?||| 25||| 5
28|||the primary-side fixed armature 13|||? ? ? ?
? ?? ? ? ?||| 13||| 9
28|||the secondary-side rotary magnet 7|||? ? ? ?
? ????? ?||| 15||| 11
Table 2: Example of MWE pairs in Europarl cor-
pus (FR-EN) and NTCIR patent corpus (JP-EN).
There are 5 columns for each term: sentence num-
ber, source term, target term, source position, and
target position. The number appended to each
term from the patent corpus (lower half) is a ref-
erence number. In this corpus, all the important
technical terms have been identified and annotated
with reference numbers.
nal lexical translation probability for Giza++ IBM
Model 4 and that of our modified Giza++. Our
modified Giza++ achieves the correct result when
anchor words ?life? and ?vie? are used to assign a
value to the prior in our model.
3 Word Alignment
We review two models which address the prob-
lem of word alignment. The aim of word align-
ment is to obtain the model parameter t among
English and French words, ei and fj respectively.
We search for this model parameter under some
model M where M is chosen by IBM Models 1-
5 and the HMM model. We introduce the latent
variable a, which is an alignment function with
the hypothesis that each e and f correspond to this
latent variable. (e, f, a) is a complete data set, and
(e, f) is an incomplete data set.
3.1 EM Algorithm
We follow the description of the EM algorithm for
IBM Models of Brown et al (1993) but introduce
the parameter t explicitly. In this model, the pa-
rameter t represents the lexical translation proba-
bilities t(ei|fj). It is noted that we use e|f rather
than f |e following the notation of Koehn (2010).
One important remark is that the Viterbi align-
ment of the sentence pair (e?, f?) = (eJ1 , f I1 ), which
is obtained as in (1):
Eviterbi : a?J1 = argmaxaJ1
p??(f, a|e) (1)
provides the best alignment for a given log-
likelihood distribution p??(f, a|e). Instead of sum-
ming, this step simplifies the E-step. However, un-
der our modification of maximum likelihood esti-
mate with MAP estimate, this simplification is not
a correct approximation of the summation since
our surface in the E-step is greatly perturbed by
the prior. There is no guarantee that the Viterbi
alignment is within the proximity of the target
alignment (cf. Table 1).
Let z be the latent variable, t be the parameters,
and x be the observations. The EM algorithm is
an iterative procedure repeating the E-step and the
M-step as in (2):
EEXH : q(z;x) =p(z|x; ?) (2)
MMLE : t? = argmax
t
Q(t, told)
= argmax
t
?
x,z
q(z|x) log p(x, z; t)
In the E-step, our knowledge of the values of the
latent variables in a is given only by the poste-
rior distribution p(a|e, f, t). Hence, the (negative
log)-likelihood of complete data (e, f, a), which
we denote by ? log p(t|e, f, a), is obtained over
all possible alignments a. We use the current pa-
rameter values told to find the posterior distribu-
tion of the latent variables given by p(a|e, f, told).
We then use this posterior distribution to find the
expectation of the complete data log-likelihood
evaluated for parameter value t. This expectation
is given by
?
a p(a|e, f, told) log p(e, f, a|t).
In the M-step, we use a maximal likelihood es-
timation to minimize negative log-likelihood in
order to determine the parameter t; note that t is
a lexical translation probability. Instead of using
the log-likelihood log p(a, e, f |t), we use the ex-
pected complete data log-likelihood over all the
possible alignments a that we obtained in the E-
28
step, as in (3):
MMLE : t? = argmax
t
Q(t, told) (3)
= c(f |e; f, e)?
e c(f |e; f, e)
where an auxiliary function c(e|f ; e, f) for IBM
Model 1 introduced by Brown et al is defined as
c(f |e; f, e) =
?
a
p(a|e, f)
m?
j=1
?(f, fj)?(e, eaj )
and where the Kronecker-Delta function ?(x, y) is
1 if x = y and 0 otherwise. This auxiliary func-
tion is convenient since the normalization factor of
this count is also required. We note that if we use
the MAP estimate, the E-step remains the same as
in the maximum likelihood case, whereas in the
M-step the quantity to be minimized is given by
Q(t, told) + log p(t). Hence, we search for the
value of t which maximizes the following equa-
tion:
MMAP : t? = argmax
t
Q(t, told) + log p(t)
3.2 HMM
A first-order Hidden Markov Model (Vogel et al,
1996) uses the sentence length probability p(J |I),
the mixture alignment probability p(i|j, I), and
the translation probability, as in (4):
p(f |e) = p(J |I)
J?
j=1
p(fj|ei) (4)
Suppose we have a training set of R observation
sequences Xr, where r = 1, ? ? ? , R, each of which
is labelled according to its class m, where m =
1, ? ? ? ,M , as in (5):
p(i|j, I) = r(i? j
I
J )?I
i?=1 r(i? ? j IJ )
(5)
The HMM alignment probabilities p(i|i?, I) de-
pend only on the jump width (i ? i?). Using a set
of non-negative parameters s(i? i?), we have (6):
p(i|i?, I) = s(i ? i
?)
?I
l=1 s(l ? i?)
(6)
4 Our Approach
Algorithm 1 Overall Algorithm
Given: a parallel corpus,
1. Extract MWEs by Algorithm 2.
2. Based on the results of Step 1, specify a set
of anchor word alignment links in the format of
anchor word alignment problem (cf. Definition
1 and Table 2).
3. Group MWEs in source and target text.
4. Calculate the prior in order to embed knowl-
edge about anchor words.
5. Calculate lexical translation probabilities
with the prior.
6. Obtain alignment probabilities.
7. Ungroup of MWEs in source and target text.
Algorithm 1 consists of seven steps. We use the
Model I prior for the case where our prior knowl-
edge is sparse and evenly distributed throughout
the corpus, whereas we use the Model II prior
when our prior knowledge is dense in a partial
corpus. A typical example of the former case
is when we use partial alignment annotation ex-
tracted throughout a corpus for bilingual terminol-
ogy. A typical example of the latter case is when a
sample of only a few hundred lines from the cor-
pus have been hand-annotated.
4.1 MWE Extraction
Our algorithm of extracting MWEs is a statisti-
cal method which is a bidirectional version of Ku-
piec (1993). Firstly, Kupiec presents a method to
extract bilingual MWE pairs in a unidirectional
manner based on the knowledge about typical
POS patterns of noun phrases, which is language-
dependent but can be written down with some ease
by a linguistic expert. For example in French they
are N N, N prep N, and N Adj. Secondly, we take
the intersection (or union) of extracted bilingual
MWE pairs.2
2In word alignment, bidirectional word alignment by tak-
ing the intersection or union is a standard method which
improves its quality compared to unidirectional word align-
ment.
29
Algorithm 2 MWE Extraction Algorithm
Given: a parallel corpus and a set of anchor
word alignment links:
1. We use a POS tagger (Part-Of-Speech Tag-
ger) to tag a sentence on the SL side.
2. Based on the typical POS patterns for the SL,
extract noun phrases on the SL side.
3. Count n-gram statistics (typically n =
1, ? ? ? , 5 are used) on the TL side which jointly
occur with each source noun phrase extracted
in Step 2.
4. Obtain the maximum likelihood counts of
joint phrases, i.e. noun phrases on the SL side
and n-gram phrases on the TL side.
5. Repeat the same procedure from Step 1 to 4
reversing the SL and TL.
6. Intersect (or union) the results in both direc-
tions.
Let SL be the source language side and TL be
the target language side. The procedure is shown
in Algorithm 2. We informally evaluated the
MWE extraction tool following Kupiec (1993) by
manually inspecting the mapping of the 100 most
frequent terms. For example, we found that 93 of
the 100 most frequent English terms in the patent
corpus were correctly mapped to their Japanese
translation.
Depending on the corpus, we can use more
prior knowledge about implicit alignment links.
For example in some categories of patent and
technical documents corpora,3 we can use heuris-
tics to extract the ?noun phrase? + ?reference
number? from both sides. This is due to the fact
that terminology is often labelled with a unique
reference number, which is labelled on both the
SL and TL sides.
4.2 Prior Model I
Prior for Exhaustive Alignment Space IBM
Models 1 and 2 implement a prior for all possible
3Unlike other language pairs, the availability of
Japanese?English parallel corpora is quite limited: the NT-
CIR patent corpus (Fujii et al, 2010) of 3 million sentence
pairs (the latest NTCIR-8 version) for the patent domain and
JENAAD corpus (Utiyama and Isahara, 2003) of 150k sen-
tence pairs for the news domain. In this regard, the patent
domain is particularly important for this particular language
pair.
Algorithm 3 Prior Model I for IBM Model 1
Given: parallel corpus e?, f? ,
anchor words biTerm
initialize t(e|f ) uniformly
do until convergence
set count(e|f ) to 0 for all e,f
set total(f) to 0 for all f
for all sentence pairs (e?s,f?s)
prior(e|f)s = getPriorModelI(e?, f? , biT erm)
for all words e in e?s
totals(e) = 0
for all words f in f?s
totals(e) += t(e|f )
for all words e in e?s
for all words f in f?s
count(e|f )+=t(e|f)/totals(e)? prior(e|f)s
total(f) += t(e|f)/totals(e) ? prior(e|f)s
for all f
for all e
t(e|f ) = count(e|f)/total(f)
alignments exhaustively. Such a prior requires the
following two conditions. Firstly, partial knowl-
edge about the prior that we use in our context is
defined as follows. Let us denote a bilingual term
list T = {(s1, t1), . . . , (sm, tm)}. For example
with IBM Model 1: Let us define the following
prior p(e|f, e, f ;T ) from Equation (4):
p(e|f, e, f ;T ) =
?
?
?
1 (ei = si, fj = tj)
0 (ei = si, fj 6= tj)
0 (ei 6= si, fj = tj)
uniform (ei 6= si, fj 6= tj)
Secondly, this prior should be proper for the ex-
haustive case and non-proper for the sampled
alignment space where by proper we mean that the
probability is normalized to 1. Algorithm 3 shows
the pseudo-code for Prior Model I. Note that if
the prior is uniform in the MAP estimation, this is
equivalent to maximum likelihood estimation.
Prior for Sampled Alignment (Function) Space
Due to the exponential costs introduced by fertil-
ity, null token insertion, and distortion probability,
IBM Models 3 and 4 do not consider all (I + 1)J
alignments exhaustively, but rather a small subset
in the E-step. Each iteration only uses the sub-
set of all the alignment functions: this sampling
30
is not uniform, as it only includes the best possi-
ble alignment with all its neighbouring alignments
which differ from the best alignment by one word
(this can be corrected by a move operation) or two
words (this can be corrected by a swap operation).
If we consider the neighbouring alignment via
a move or a swap operation, two issues arise.
Firstly, the fact that these two neighbouring align-
ments are drawn from different underlying distri-
butions needs to be taken into account, and sec-
ondly, that the application of a move and a swap
operation alters a row or column of a prior ma-
trix (or indices of the prior) since either operation
involves the manipulation of links.
Algorithm 4 Pseudo-code for Prior Model II Ex-
haustive Alignment Space
def getPriorModelII(e?,f? ,biTerm):
for i in sentence:
for e in e?i:
allWordsi = length of sentence e?
for f in f?i:
if (e, f ) in biTerm:
n= num of anchor words in i
uni(e|f)i = allWordsi?nallWordsi
expSum(e|f) += uni(e|f)i ? n
else:
countSum(e|f)i += n
countSum(e|f) += count(e|f)i
for e in alle:
for f in allf :
prior(e|f) = expSum(e|f) + countSum(e|f)
return prior(e|f)
Prior for Jump Width i? One implementation
of HMM is to use the forward-backward algo-
rithm. A prior should be embedded within the
forward-backward algorithm. From Equation (6),
there are three cases which depend on whether
ai and its neighbouring alignment ai?1 are deter-
mined by our prior knowledge about anchor words
or not. When both ai and aj are determined, this
probability is expressed as in (7):
p(i? i?; I) =
?
?
?
0 (else) (7)
1 (ei = si, fj = tj for ai) and
(e?i = s?i, f ?j = t?j for aj)
When either ai or aj is determined, this probabil-
ity is expressed as in (8):4
p(i? i?; I) =
?
???
???
0 (condition 1) (8)
1 (condition 2)
1
(m?#eai?????#eai+m)
(else)
(uniform distribution)
When neither ai nor aj is determined, this proba-
bility is expressed as in (9): 5
p(i? i?; I) =
?
????
????
0 (condition 3) (9)
1 (condition 4)
m?i?
(m?#eai?????#eai+m)2
(else)
(Pascal?s triangle distribution)
4.3 Prior Model II
Prior Model II assumes that we have prior knowl-
edge only in some part of the training corpus. A
typical example is when a small part of the corpus
has a hand-crafted ?gold standard? annotation.
Prior for Exhaustive Alignment Space Prior
Model II is used to obtain the prior probability
p(e|f) over all possible combinations of e and f .
In contrast to Prior Model I, which computes the
prior probability p(e|f) for each sentence, Prior
Model II computes the prior probability globally
for all sentences in the corpus. Algorithm 4 shows
the pseudo-code for Prior Model II Exhaustive
Alignment Space.
4condition 1 is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
?condition 2? is as follows:
((ei = si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei 6= si, fj = tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or
((ei = si, fj = tj for ai) and (e?i = s?i, f ?j 6= t?j for aj))
5
?condition 3? is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
?condition 4? is as follows:
((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or
((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j 6= t?j for aj)) or
((ei 6= si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or
((ei = si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))
31
Prior for Sampled Alignment (Function) Space
This is identical to that of the Prior Model II ex-
haustive alignment space with only a difference in
the normalization process.
Prior for Jump Width i? This categorization of
Prior Model II is the same as that of Prior Model I
for for Jump Width i? (see Section 4.2). Note that
Prior Model II requires more memory compared
to the Prior Model I.6
5 Experimental Settings
The baseline in our experiments is a standard
log-linear phrase-based MT system based on
Moses. The GIZA++ implementation (Och and
Ney, 2003a) of IBM Model 4 is used as the base-
line for word alignment, which we compare to
our modified GIZA++. Model 4 is incrementally
trained by performing 5 iterations of Model 1, 5
iterations of HMM, 5 iterations of Model 3, and
5 iterations of Model 4. For phrase extraction the
grow-diag-final heuristics are used to derive the
refined alignment from bidirectional alignments.
We then perform MERT while a 5-gram language
model is trained with SRILM. Our implementa-
tion is based on a modified version of GIZA++
(Och and Ney, 2003a). This modification is on the
function that reads a bilingual terminology file,
the function that calculates priors, the M-step in
IBM Models 1-5, and the forward-backward algo-
rithm in the HMM Model. Other related software
tools are written in Python and Perl: terminol-
ogy concatenation, terminology numbering, and
so forth.
6 Experimental Results
We conduct an experimental evaluation on the
NTCIR-8 corpus (Fujii et al, 2010) and on Eu-
roparl (Koehn, 2005). Firstly, MWEs are ex-
tracted from both corpora, as shown in Table 3.
In the second step, we apply our modified version
of GIZA++ in which we incorporate the results of
6This is because it needs to maintain potentially an ??m
matrix, where ? denotes the number of English tokens in the
corpus and m denotes the number of foreign tokens, even if
the matrix is sparse. Prior Model I only requires an ?? ? m?
matrix where ?? is the number of English tokens in a sentence
and m? is the number of foreign tokens in a sentence, which
is only needed until this information is incorporated in a pos-
terior probability during the iterative process.
corpus language size #unique #all
MWEs MWEs
statistical method
NTCIR EN-JP 200k 1,121 120,070
europarl EN-FR 200k 312 22,001
europarl EN-ES 200k 406 16,350
heuristic method
NTCIR EN-JP 200k 50,613 114,373
Table 3: Statistics of our MWE extraction method.
The numbers of MWEs are from 0.08 to 0.6 MWE
/ sentence pair in our statistical MWE extraction
methods.
MWE extraction. Secondly, in order to incorpo-
rate the extracted MWEs, they are reformatted as
shown in Table 2. Thirdly, we convert all MWEs
into a single token, i.e. we concatenate them with
an underscore character. We then run the modi-
fied version of GIZA++ and obtain a phrase and
reordering table. In the fourth step, we split the
concatenated MWEs embedded in the third step.
Finally, in the fifth step, we run MERT, and pro-
ceed with decoding before automatically evaluat-
ing the translations.
Table 4 shows the results where ?baseline? in-
dicates no BMWE grouping nor prior, and ?base-
line2? represents a BMWE grouping but without
the prior. Although ?baseline2? (BMWE group-
ing) shows a drop in performance in the JP?EN
/ EN?JP 50k sentence pair setting, Prior Model I
results in an increase in performance in the same
setting. Except for EN?ES 200k, our Prior Model
I was better than ?baseline2?. For EN?JP NT-
CIR using 200k sentence pairs, we obtained an
absolute improvement of 0.77 Bleu points com-
pared to the ?baseline?; for EN?JP using 50k sen-
tence pairs, 0.75 Bleu points; and for ES?EN Eu-
roparl corpus using 200k sentence pairs, 0.63 Bleu
points. In contrast, Prior Model II did not work
well. The possible reason for this is the misspec-
ification, i.e. the modelling by IBM Model 4 was
wrong in terms of the given data. One piece of ev-
idence for this is that most of the enforced align-
ments were found correct in a manual inspection.
For EN?JP NTCIR using the same corpus of
200k, although the number of unique MWEs ex-
32
size EN-JP Bleu JP-EN Bleu
50k baseline 16.33 baseline 22.01
50k baseline2 16.10 baseline2 21.71
50k prior I 17.08 prior I 22.11
50k prior II 16.02 prior II 20.02
200k baseline 23.42 baseline 21.68
200k baseline2 24.10 baseline2 22.32
200k prior I 24.22 prior I 22.45
200k prior II 23.22 prior II 21.00
size FR-EN Bleu EN-FR Bleu
50k baseline 17.68 baseline 17.80
50k baseline2 17.76 baseline2 18.00
50k prior I 17.81 prior I 18.02
50k prior II 17.01 prior II 17.30
200k baseline 18.40 baseline 18.20
200k baseline2 18.80 baseline2 18.50
200k prior I 18.99 prior I 18.60
200k prior II 18.20 prior II 17.50
size ES-EN Bleu EN-ES Bleu
50k baseline 16.21 baseline 15.17
50k baseline2 16.61 baseline2 15.60
50k prior I 16.91 prior I 15.87
50k prior II 16.15 prior II 14.60
200k baseline 16.87 baseline 17.62
200k baseline2 17.40 baseline2 18.21
200k prior I 17.50 prior I 18.20
200k prior II 16.50 prior II 17.10
Table 4: Results. Baseline is plain GIZA++ /
Moses (without BMWE grouping / prior), base-
line2 is with BMWE grouping, prior I / II are with
BMWE grouping and prior.
tracted by the statistical method and the heuris-
tic method varies significantly, the total number
of MWEs by each method becomes comparable.
The resulting Bleu score for the heuristic method
(24.24 / 22.48 Blue points for 200k EN?JP / JP?
EN) is slightly better than that of the statistical
method. The possible reason for this is related
to the way the heuristic method groups terms in-
cluding reference numbers, while the statistical
method does not. As a result, the complexity of
the alignment model simplifies slightly in the case
of the heuristic method.
7 Conclusion
This paper presents a new method of incorporat-
ing BMWEs into word alignment. We first de-
tect BMWEs in a bidirectional way and then use
this information to do groupings and to enforce
already known alignment links. For the latter pro-
cess, we replace the maximum likelihood estimate
in the M-step of the EM algorithm with the MAP
estimate; this replacement allows the incorpora-
tion of the prior in the M-step of the EM algo-
rithm. We include an experimental investigation
into incorporating extracted BMWEs into a word
aligner. Although there is some work which incor-
porates BMWEs in groupings, they do not enforce
alignment links.
There are several ways in which this work can
be extended. Firstly, although we assume that our
a priori partial annotation is reliable, if we extract
such MWEs automatically, we cannot avoid erro-
neous pairs. Secondly, we assume that the rea-
son why our Prior Model II did not work was due
to the misspecification (or wrong modelling). We
would like to check this by discriminative mod-
elling. Thirdly, although here we extract BMWEs,
we can extend this to extract paraphrases and non-
literal expressions.
8 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(http://www.cngl.ie) at Dublin City Uni-
versity and Trinity College Dublin. We would also
like to thank the Irish Centre for High-End Com-
puting.
References
Bishop, Christopher M. 2006. Pattern Recognition
and Machine Learning. Springer. Cambridge, UK
Brown, Peter F., Vincent .J.D Pietra, Stephen
A.D.Pietra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Param-
eter Estimation. Computational Linguistics. 19(2),
pp. 263?311.
Callison-Burch, Chris, David Talbot and Miles Os-
borne. 2004. Statistical Machine Translation with
33
Word- and Sentence-Aligned Parallel Corpora. Pro-
ceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?04),
Main Volume. Barcelona, Spain, pp. 175?182.
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, Sayori Shimohata. 2010. Overview of the
Patent Translation Task at the NTCIR-8 Workshop.
Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pp. 293?302.
Graca, Joao de Almeida Varelas, Kuzman Ganchev,
Ben Taskar. 2007. Expectation Maximization
and Posterior Constraints. In Neural Information
Processing Systems Conference (NIPS), Vancouver,
BC, Canada, pp. 569?576.
Gale, William, and Ken Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of the 29th Annual Meeting of the Associ-
ation for Computational Linguistics. Berkeley CA,
pp. 177?184.
Koehn, Philipp, Franz Och, Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics. Edmonton,
Canada. pp. 115?124.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit. Phuket, Thailand, pp.79-86.
Koehn, Philipp, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan,
W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar,
A. Constantin, and E. Herbst, 2007. Moses: Open
source toolkit for Statistical Machine Translation.
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, Prague, Czech Republic, pp. 177?180.
Koehn, Philipp. 2010. Statistical Machine Transla-
tion. Cambridge University Press. Cambridge, UK.
Kupiec, Julian. 1993. An Algorithm for finding Noun
Phrase Correspondences in Bilingual Corpora. In
Proceedings of the 31st Annual Meeting of Associa-
tion for Computational Linguistics. Columbus. OH.
pp. 17?22.
Lambert, Patrik and Rafael Banchs. 2006. Group-
ing Multi-word Expressions According to Part-Of-
Speech in Statistical Machine Translation. In Pro-
ceedings of the EACL Workshop on Multi-Word-
Expressions in a Multilingual Context. Trento, Italy,
pp. 9?16.
McLachlan, Geoffrey J. and Thriyambakam Krishnan,
1997. The EM Algorithm and Extensions. Wiley
Series in probability and statistics. New York, NY.
Moore, Robert C.. 2003. Learning Translations of
Named-Entity Phrases from Parallel Corpora. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Budapest, Hungary. pp. 259?266.
Moore, Robert C.. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Barcelona,
Spain, pp. 333?340.
Och, Franz and Herman Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1), pp. 19?51.
Resnik, Philip and I. Dan Melamed, 1997. Semi-
Automatic Acquisition of Domain-Specific Transla-
tion Lexicons. Proceedings of the 5th Applied Nat-
ural Language Processing Conference. Washington,
DC., pp. 340?347.
Talbot, David. 2005. Constrained EM for parallel text
alignment, Natural Language Engineering, 11(3):
pp. 263?277.
Utiyama, Masao and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences, In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics. Sapporo, Japan, pp. 72?79.
Vogel, Stephan, Hermann Ney, Christoph Tillmann
1996. HMM-Based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics.
Copenhagen, Denmark, pp. 836?841.
34
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 464?471,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Dependency-Constrained Hierarchical Model with Moses
Yvette Graham??
?Department of Computing and Information Systems, The University of Melbourne
?Centre for Next Generation Localisation, Dublin City University
ygraham@unimelb.edu.au
Abstract
This paper presents a dependency-
constrained hierarchical machine transla-
tion model that uses Moses open-source
toolkit for rule extraction and decoding.
Experiments are carried out for the
German-English language pair in both di-
rections for projective and non-projective
dependencies. We examine effects on
SCFG size and automatic evaluation
results when constraints are applied with
respect to projective or non-projective
dependency structures and on the source
or target language side.
1 Introduction
A fundamental element of natural language syntax
is the dependency structure encoding the binary
asymmetric head-dependent relations captured in
dependency grammar theory. A main criteria for
determining the dependency structure of a given
sentence is the following: The linear position of
dependent, D, is specified with reference to its
head, H (Ku?bler et al, 2009). This runs in parallel
with that which hierarchical machine translation
SCFG rules encode: The linear position of a trans-
lated phrase, Xi, is specified with reference to the
lexicalised words in the rule. Figure 1 shows de-
She lives in a white house .
Ella vive en una casa blanca .
Figure 1: Projective Dependency Structures
pendency structures for She lives in a white house
and its Spanish translation, with example SCFG
(1) X ?< white house , casa blanca >
(2) X ?< white , blanca >
(3) X ?< house , casa >
(4) X ?< X0 house , casa X0 >
(5) X ?< white X0 , X0 blanca >
Figure 2: Initial rules (1), (2) and (3), with hierar-
chical rules (4) and (5)
rules shown in Figure 2. Given the existence of
initial rules (1), (2) and (3), hierarchical rules (4)
and (5) can be created. Rule (4) specifies the lin-
ear position of the translation of the English phrase
that precedes house with reference to lexicalised
casa.
For hierarchical machine translation models
(Chiang, 2005), there is no requirement for a syn-
tactic relationship to exist between the lexicalised
words of a rule and the words replaced by non-
terminals, the only requirement being that substi-
tuted words form an SMT phrase (Koehn et al,
2003). The dependency structure of either the
source or target (or indeed both) can, however, be
used to constrain rule extraction as to only allow
hierarchical rules in which the linear position of
dependents are specified with reference to the po-
sition of their lexicalised heads. For example, in
the case of the hierarchical rules in Figure 2, rule
(4) satisfies such a constraint according to both the
source and target language dependency structures
(since white is the dependent of house and blanca
is the dependent of casa, and it is both white and
blanca that are replaced by non-terminals while
the heads remain lexicalised) and results in a syn-
chronous grammar rule that positions a dependent
relative to the position of its lexicalised head. Rule
(5), on the other hand, does not satisfy such a con-
straint for either language dependency structure.
In this work, we examine a dependency-
constrained model in which hierarchical rules are
464
only permitted in which lexicalised heads spec-
ify the linear position of missing dependents, and
examine the effects of applying such constraints
across a variety of settings for German to English
and English to German translation.
2 Related Work
The increased computational complexity intro-
duced by hierarchical machine translation mod-
els (Chiang, 2005), has motivated techniques of
constraining model size as well as decoder search.
Among such include the work of Zollmann et al
(2008) and Huang and Xiang (2010), in which rule
table size is vastly reduced by means of filtering
low frequency rules, while Tomeh et al (2009),
Johnson et al (2007) and Yang and Zheng (2009)
take the approach of applying statistical signifi-
cance tests to rule filtering, with Lee et al (2012)
defining filtering methods that estimate transla-
tional effectiveness of rules.
Dependency-based constraints have also been
applied in a variety of settings to combat complex-
ity challenges. Xie et al (2011) use source side
dependency constraints for translation from Chi-
nese to English, while Shen et al (2010) apply
target-side dependency constraints for the same
language pair and direction in addition to Ara-
bic to English, Peter et al (2011) also apply de-
pendency constraints on the target side, but rather
soft constraints that can be relaxed in the case that
an ill-formed structure does in fact yield a bet-
ter translation. Gao et al (2011) similarly ap-
ply soft dependency constraints but to the source
side for Chinese to English translation, and Galley
and Manning (2009) show several advantages to
using maximum spanning tree non-projective de-
pendency parsing decoding for Chinese to English
translation. Li et al (2012), although not con-
straining with dependency structure, instead cre-
ate non-terminals with part-of-speech tag combi-
nations for Chinese words identified as heads for
translation into English.
In this paper, we apply the same dependency
constraint to SCFG rule extraction in a variety
of configurations to investigate effects of apply-
ing constraints on the source or target side, to the
language with most or least free word order, as
well as constraining with non-projective depen-
dency structures.
Non-Projective Dependencies
German 38%
English 11%
Table 1: WMT Parallel Training Data
3 Non-Projective Dependencies
A non-projectivity structure is defined as follows:
A non-projective dependency structure is a depen-
dency structure in which at least one dependency
relation exists between a head, H , and its depen-
dent, D, in which the directed path from H to
D does not include at least one word positioned
linearly in the surface form between H and D.
Figure 3 shows an example non-projective depen-
dency structure arising from English Wh-fronting.
Non-projective dependencies occur frequently
When did  the crisis begin ?
Figure 3: Non-projective Dependency Structure
for many languages, increasingly so for languages
with high levels of free words order. An exami-
nation of Chinese treebanks, for example, reports
that Chinese displays nine different kinds of non-
projective phenomena (Yuelong, 2012) with re-
ports of as many as one in four sentences in tree
banks having non-projective dependency struc-
tures (Nivre, 2007). Even for a language with
relatively rigid word order such as English non-
projectivity is still common, due to Wh-fronting,
topicalisation, scrambling and extraposition. Ta-
ble 1 shows the frequency of non-projective de-
pendency structures in WMT parallel data sets for
German and English when parsed with a state-of-
the-art non-projective dependency parser (Bohnet,
2010).
4 Constrained Model
We define the dependency constraint as follows: to
create a hierarchical rule by replacing a word or
phrase with a non-terminal, all the words of that
phrase must belong to a single complete depen-
dency tree and its head must remain lexicalised
in the rule. In this way, the hierarchical rules of
the SCFG position missing dependents relative to
the position of lexicalised heads. Before extract-
465
ing SCFG rules for the dependency-constrained
models, we transform non-projective structures
into projective ones, in order to allow the sub-
stitution of non-projective dependency trees by a
single non-terminal. Although the transformation
simplifies the dependency structure, it will intro-
duce some dis-fluency to the training data, and we
therefore include experiments to examine such ef-
fects.
Figure 4 shows a German-English translation
constrained by means of the German dependency
structure and Figure 5 shows the full set of
dependency-constrained hierarchical SCFG rules,
where dependents are specified with reference to
lexicalised heads.
5 Implementation with Moses
For rule extraction we use Moses (Williams and
Koehn, 2012) implementation of GHKM (Galley
et al, 2004; Galley et al, 2006), which although is
conventionally used to extract syntax-augmented
SCFGs from phrase-structure parses (Zollmann
and Venugopal, 2006), we apply the same rule ex-
traction tool to dependency parses. Rule extrac-
tion is implemented in such a way as not to be re-
stricted to any particular set of node labels. The
conventional input format is for example:
<tree label="NP">
<tree label="DET"> the </tree>
<tree label="NN"> cat </tree>
</tree>
The dependency-constrained ruleset can be ex-
tracted with this implementation by arranging de-
pendency structures into tree structures as fol-
lows:1
<tree label="X">
<tree label="X">
<tree label="X"> the </tree>
<tree label="X"> black </tree>
cat
</tree>
ate
<tree label="X">
<tree label="X"> the </tree>
rat
</tree>
</tree>
Since XML format requires nesting of substruc-
tures, only projective dependency structures can
be input to the tool in the way we use it, as non-
projectivity breaks nesting.
1Note that is is possible to replace X with dependency
labels.
6 Non-Projectivity Transform
We therefore transform non-projective depen-
dency structures into projective ones by relocat-
ing the dislocated dependent to a position closer
to its head so that it no longer violates projectivity.
We do this in such a way as not to break any of
the existing dependency relations between pairs of
words. Figure 6 shows an example non-projective
structure (a) before and (b) after the transforma-
tion, where the transformation results in the con-
stituent comprised of words when and begin form-
ing a continuous string, making possible the sub-
stitution of this constituent with a non-terminal.
The fact that one side of the training data from
which hierarchical rules are extracted, however, is
no longer guaranteed to be fluent, raises the ques-
tion as to what effect this disfluency might have
when the constraint is applied on the target side.
We therefore include in our evaluation for both
language directions (and for the case where the
constraints are applied to the source) the effects
of word reorder cause by the transformation. The
Figure 6: Non-Projectivity Transformation
algorithm for converting non-projective structures
is an inorder traversal of the dependency struc-
ture as follows, where words are indexed accord-
ing to their position in the original string prior to
the transformation:
Algorithm 6.1: DEP IN ORD(root)
for each d ? D and d.index < root.index
do dep in ord(d)
PRINT(root)
for each d ? D and d.index > root.index
do dep in ord(d)
466
Figure 4: German English translation with German dependency structure, words surrounded by a dashed
box form a complete dependency tree.
Rules spanning source words 0-6: ich mo?chte nur wenige anmerkungen machen .
X0 mo?chte nur wenige anmerkungen machen . X0 should like to make just a few comments .
ich mo?chte X0 . i should like to X0 .
ich mo?chte X0 machen . i should like to make X0 .
ich mo?chte X0 anmerkungen machen . i should like to make X0 comments .
ich mo?chte X0 wenige anmerkungen machen . i should like to make X0 a few comments .
ich mo?chte nur wenige anmerkungen machen X0 i should like to make just a few comments X0
non-proj X0 mo?chte X1 . X0 should like to X1 .
X0 mo?chte X1 machen . X0 should like to make X1 .
X0 mo?chte X1 anmerkungen machen . X0 should like to make X1 comments .
X0 mo?chte X1 wenige anmerkungen machen . X0 should like to make X1 a few comments .
X0 mo?chte nur wenige anmerkungen machen X1 X0 should like to make just a few comments X1
ich mo?chte X0 X1 i should like to X0 X1
ich mo?chte X0 machen X1 i should like to make X0 X1
ich mo?chte X0 anmerkungen machen X1 i should like to make X0 comments X1
ich mo?chte X0 wenige anmerkungen machen X1 i should like to make X0 a few comments X1
X0 mo?chte X1 X2 X0 should like to X1 X2
X0 mo?chte X1 machen X2 X0 should like to make X1 X2
X0 mo?chte X1 anmerkungen machen X2 X0 should like to make X1 comments X2
X0 mo?chte X1 wenige anmerkungen machen X2 X0 should like to make X1 a few comments X2
Rules spanning source words 2-5: nur wenige anmerkungen machen
X0 machen make X0
X0 anmerkungen machen make X0 comments
X0 wenige anmerkungen machen X0 a few comments
Rules spanning source words 2-4: nur wenige anmerkungen
X0 anmerkungen X0 comments
X0 wenige anmerkungen X0 a few comments
Rules spanning source words 2-3: nur wenige
X0 wenige X0 a few
Figure 5: Complete set of dependency-constrained hierarchical SCFG rules for Figure 4
467
7 Experiments
WMT training data sets were used for both paral-
lel (1.49 million German/English sentence pairs)
and monolingual training (11.51 million English
& 4.74 million German sentences). Mate non-
projective dependency parser (Bohnet, 2010) was
used for parsing both the German and English
parallel data with standard pre-trained models,
the same parser was used for projective parsing
with non-projectivity turned off.2 Parallel train-
ing data lines containing multiple sentences were
merged into a single pseudo-dependency structure
by adding an artificial root and head-dependent re-
lation between the head of the initial sentence and
any subsequent sentences. Non-projective depen-
dencies were converted into projective structures
using Algorithm 6.1.
Giza++ (Och et al, 1999) was employed for
automatic word alignment, and Moses GHKM
rule extraction (Williams and Koehn, 2012) was
used for hierarchical rule extraction for the
dependency-constrained models. Default settings
were used for rule extraction for all models with
the exception on non-fractional counting being
used, as well as Good-turing discounting. Both
the dependency-constrained and standard mod-
els use the same set of initial rules. For de-
coding, since only a single non-terminal, X , is
present for all models, Moses hierarchical decoder
(Koehn et al, 2007) was used with default set-
tings with the exception of rule span limit being re-
moved for all models. SRILM (Stolke, 2002) was
used for 5-gram language modeling and Kneser-
Ney smoothing (Kneser and Ney, 1995) for both
German-to-English and English-to-German trans-
lation. MERT (Och, 2003) was carried out on
WMT newstest2009 development set optimizing
for BLEU, and final results are reported for held-
out test sets, newstest2010 and newstest2011, with
BLEU (Papineni et al, 2001) and LR-score (Birch
and Osborne, 2010) for evaluation.
7.1 Results
Table 2 shows automatic evaluation results for
both the dependency-constrained and standard
hierarchical models for both language direc-
tions. Compared to the standard hierarchical
model (orig), the best performing dependency-
constrained models, sl npr (de-en) and tl npr (en-
2OpenNLP (Feinerer, 2012) sentence splitter is recom-
mended with the parser we and was used for preprocessing.
de), show significant decreases in mean BLEU
score, -0.44 for German to English and -0.13
for English to German. However, there is a
trade-off, as the dependency-constrained models
achieve vast reductions in model size, approx.
93% for German to English and 89% for English
to German in numbers of SCFG hierarchical rules.
This results in decreased decoding times, with the
best performing dependency-constrained models
achieving a decrease of 26% for German to En-
glish and 34% for English to German in mean de-
coding times.
The decrease in BLEU scores is not likely to be
attributed to less accurate long-distance reordering
for German to English translation, as the Kendall
Tau LR-scores for this language direction show an
increase over the standard hierarchical models of
+0.25 mean LR. Although this is not the case for
English to German, as mean LR scores show a
slight decrease (-0.11 LR).
The number of hierarchical rules (not including
glue rules) employed during decoding provides a
useful indication of to what degree each model ac-
tually uses hierarchical rules to construct transla-
tions, i.e. not simply concatenating phrases with
glue rules. For English to German translation,
while the number of hierarchical rules present in
the SCFG is vastly reduced, the number of hier-
archical rules used during decoding actually in-
creases, with double the number of hierarchical
rules used to translate test segments compared to
the standard hierarchical model, from an average
of only 0.58 hierarchical rules per segment for the
standard model to 1.19 per segment. This indi-
cates that the set of hierarchical rules is refined by
the dependency constraint.
When the more linguistically valid non-
projective dependency structure, as opposed to
the projective dependency structure, is used to
constrain rule extraction significant increases in
BLEU scores are achieved for all configurations.
The most significant gains in this respect occur
when constraints are applied on the source side,
+0.58 mean BLEU for German to English and
+0.50 mean BLEU for English to German.
In general, when constraints are applied to the
more free word order language, German, regard-
less of whether or not translation is into or out of
German, marginally higher BLEU scores result,
with an increase of +0.03 mean BLEU for German
to English translation and similarly an increase of
468
SCFG mean hier. mean segment
hier. rules newstest 2010 newstest 2011 mean rules decode time
(millions) BLEU LR-K BLEU LR-K BLEU decoder (seconds)
de-en
hpb
orig 35.25 22.30 71.86 20.47 70.55 21.39 2.51 6.76
tl re 34.77 22.31 71.43 20.49 70.27 21.40 2.63 6.39
tl are 34.77 22.41 71.16 20.36 69.89 21.39 2.68 6.14
sl are 33.87 22.40 70.78 20.27 69.78 21.34 2.71 6.02
sl re 33.87 22.06 71.38 20.15 70.25 21.11 2.41 6.17
dc
sl npr 2.49 21.57 71.87 20.09 71.04 20.95 1.15 4.99
tl npr 1.45 21.88 72.20 19.95 71.36 20.92 2.85 4.62
tl pr 1.12 21.43 71.82 19.75 70.90 20.59 1.40 3.62
sl pr 0.34 21.05 72.20 19.69 71.36 20.37 1.10 1.98
en-de
hpb
orig 36.30 16.14 70.24 15.05 69.91 15.60 0.58 7.25
tl re 35.20 16.13 69.81 14.94 69.45 15.54 1.03 5.16
tl are 35.20 16.15 69.06 14.57 68.66 15.36 1.89 4.82
sl are 35.68 15.72 69.25 14.44 69.06 15.08 1.88 5.23
sl re 35.68 15.72 70.21 14.38 69.84 15.05 1.16 5.16
dc
tl npr 4.00 16.03 70.12 14.91 69.81 15.47 1.19 4.79
sl npr 1.09 15.94 70.07 14.85 69.69 15.40 1.78 3.46
tl pr 0.92 15.88 70.46 14.78 69.90 15.33 1.23 4.05
sl pr 0.88 15.58 70.18 14.22 69.80 14.90 1.19 2.90
Table 2: Effects of dependency constraints and dependency-based reordering on translation quality for
German-to-English (de-en) and English to German (en-de), hpb=hierarchical phrase-based, orig=no re-
ordering, ?re=dependency-based word reordering where only hierarchical rules are extracted from re-
ordered training data, ?are=dependency-based word reordering where all SCFG rules extracted from re-
ordered training data, dc=dependency-constrained, ?pr=projective parse used for dependency constraint,
?npr=non-projective parse used for dependency constraint, sl?=constraints or reordering for source lan-
guage, tl?=constraints or reordering for target language, numbers of hierarchical rules reported do not
include glue rules.
+0.07 mean BLEU for English to German, with
the increase being statistically significant for Ger-
man to English for the newstest2010 test set, but
not statistically significant for newstest2011 test
set or English to German (Koehn, 2004).
Overall the best performing dependency-
constrained models are those that retain the high-
est numbers of hierarchical rules in the SCFG.
This indicates that although the dependency-
constrained models produce a refined ruleset,
they nevertheless discard some SCFG rules that
would be useful to translate the unseen test data.
One possible reason is that although the non-
projective dependency structures are significantly
better, these high-quality linguistic structures may
still not be optimal for translation. Another pos-
sibility is that a the GHKM rule extraction con-
straints combined with the dependency constraint
is causing a small set of very useful rules to be
discarded.
7.2 Dependency-based Reordering
We examine the effects of the non-projective
transformation in isolation of any dependency-
constraints by training a standard hierarchical
model on the reordered corpus with no depen-
dency constraints applied. We do this in two set-
ups. First, we extract hierarchical rules from the
reordered training corpus and initial rules from the
original unaltered corpus (? re in Table 2), as this
is the set-up for the dependency-constrained mod-
els. Simply for interest sake, we repeat this exper-
iment but extract all rules (hierarchical and initial
rules) from the reordered corpus (? are in Table 2).
Surprisingly, when non-projective reordering is
carried out on the target side no significant de-
crease in BLEU scores occurs for both language
directions. In fact, a minor increase in mean
BLEU (+0.01) is observed for German to English
translation, but this small increase is not statisti-
cally significant. For the English to German direc-
tion, a minor decrease of -0.06 mean BLEU occurs
(not statistically significant).
Similarly for German to English, when reorder-
ing is applied to the source side, only a minor
decrease (-0.05) results. Non-projective reorder-
ing causes the most significant reduction in per-
formance for English to German when the English
source is reordered, with a decrease of -0.52 mean
BLEU.
469
Conclusions
This paper examines non-projectivity and lan-
guage application for dependency-constrained
hierarchical models using Moses open-source
toolkit. Experiments show that when applied
to English to German translation, vastly reduced
model size and subsequently decreased decoding
times result with only a minor decrease in BLEU.
In addition, higher numbers of (non-glue) hierar-
chical rules are used to translate test segments. For
German to English translation, similar decreases
in model size and decoding times occur, but at the
expense of a more significant decrease in BLEU.
In general, results for the dependency-
constrained models show that applying constraints
on the source or target side does not have a major
impact on BLEU scores. Rather the use of high
quality linguistic structures is more important, as
significant improvements are made for all con-
figurations when the non-projective dependency
structure is used to constrain rule extraction.
Acknowledgments
Many thanks to the Moses developers, especially
Hieu Hoang and Alexandra Birch. Thanks also to
Tsuyoshi Okita and anonymous reviewers. This
research was supported by the Australian Re-
search Council, as well as Science Foundation Ire-
land as part of the Centre for Next Generation Lo-
calisation.
References
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
327?332, Uppsala, Sweden, July. Association for
Computational Linguistics.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97. Association for
Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270. As-
sociation for Computational Linguistics.
Ingo Feinerer. 2012. tm: Text mining package. R
package version 0.5-7.1.
Michel Galley and Christopher D Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 773?781. Association for Computational
Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 961?968. Association for Computa-
tional Linguistics.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hier-
archical phrase-based translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 857?868. Association
for Computational Linguistics.
Fei Huang and Bing Xiang. 2010. Feature-rich dis-
criminative phrase rescoring for smt. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 492?500. Association
for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Accoustics, Speech and Signal Processing, vol-
ume 1.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase based translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
470
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Sandra Ku?bler, Ryna McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Clay-
pool.
Seung-Wook Lee, Dongdong Zhang, Mu Li, Ming
Zhou, and Hae-Chang Rim. 2012. Translation
model size reduction for hierarchical phrase-based
statistical machine translation. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
291?295, Jeju Island, Korea, July. Association for
Computational Linguistics.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef
van Genabith. 2012. Using syntactic head infor-
mation in hierarchical phrase-based translation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 232?242, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Proceedings of Human
Language Technologies: The Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL-HLT), pages
396?403, Rochester, NY.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for sta-
tistical machine translation. In Proceedings of the
Joint Conference of Empirical Methods in Natu-
ral Language Processing and Very Large Corpora
(EMNLP-VLC), pages 20?28.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. Technical Re-
port RC22176(W0109-022), IBM Research Report,
September 17.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft string-to-dependency hi-
erarchical machine translation. In Marcello Fed-
erico, Mei-Yuh Hwang, Margit Ro?dder, and Sebas-
tian Stu?ker, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 246?253.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671.
Andreas Stolke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 577?585.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for
statistical machine translation. In Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII). International Association for Machine Trans-
lation.
Philip Williams and Philipp Koehn. 2012. GHKM
rule extraction and scope-3 parsing in moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 434?440, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 216?226, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Mei Yang and Jing Zheng. 2009. Toward smaller,
faster, and better hierarchical phrase-based smt. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 237?240, Suntec, Singapore,
August. Association for Computational Linguistics.
Wang Yuelong. 2012. Edge-crossing Non-projective
Phenomena in Chinese Language. Ph.D. thesis, Na-
tional University of Singapore.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Josef
Och, and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 1145?1152, Manchester, UK,
August. Coling 2008 Organizing Committee.
471
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33?41,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Continuous Measurement Scales in
Human Evaluation of Machine Translation
Yvette Graham Timothy Baldwin Alistair Moffat Justin Zobel
Department of Computing and Information Systems, The University of Melbourne
{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au
Abstract
We explore the use of continuous rat-
ing scales for human evaluation in the
context of machine translation evaluation,
comparing two assessor-intrinsic quality-
control techniques that do not rely on
agreement with expert judgments. Ex-
periments employing Amazon?s Mechan-
ical Turk service show that quality-control
techniques made possible by the use of
the continuous scale show dramatic im-
provements to intra-annotator agreement
of up to +0.101 in the kappa coefficient,
with inter-annotator agreement increasing
by up to +0.144 when additional standard-
ization of scores is applied.
1 Introduction
Human annotations of language are often required
in natural language processing (NLP) tasks for
evaluation purposes, in order to estimate how well
a given system mimics activities traditionally per-
formed by humans. In tasks such as machine
translation (MT) and natural language generation,
the system output is a fully-formed string in a tar-
get language. Annotations can take the form of
direct estimates of the quality of those outputs or
be structured as the simpler task of ranking com-
peting outputs from best-to-worst (Callison-Burch
et al, 2012).
A direct estimation method of assessment, as
opposed to ranking outputs from best-to-worst,
has the advantage that it includes in annotations
not only that one output is better than another,
but also the degree to which that output was bet-
ter than the other. In addition, direct estimation
of quality within the context of machine transla-
tion extends the usefulness of the annotated data
to other tasks such as quality-estimation (Callison-
Burch et al, 2012).
For an evaluation to be credible, the annotations
must be credible. The simplest way of establish-
ing this is to have the same data point annotated by
multiple annotators, and measure the agreement
between them. There has been a worrying trend
in recent MT shared tasks ? whether the evalu-
ation was structured as ranking translations from
best-to-worst, or by direct estimation of fluency
and adequacy ? of agreement between annotators
decreasing (Callison-Burch et al, 2008; Callison-
Burch et al, 2009; Callison-Burch et al, 2010;
Callison-Burch et al, 2011; Callison-Burch et al,
2012). Inconsistency in human evaluation of ma-
chine translation calls into question conclusions
drawn from those assessments, and is the target
of this paper: by revising the annotation process,
can we improve annotator agreement, and hence
the quality of human annotations?
Direct estimates of quality are intrinsically con-
tinuous in nature, but are often collected using an
interval-level scale with a relatively low number
of categories, perhaps to make the task cognitively
easier for human assessors. In MT evaluation,
five and seven-point interval-level scales are com-
mon (Callison-Burch et al, 2007; Denkowski and
Lavie, 2010). However, the interval-level scale
commonly used for direct estimation of translation
quality (and other NLP annotation tasks) forces
human judges to discretize their assessments into
a fixed number of categories, and this process
could be a cause of inconsistency in human judg-
ments. In particular, an assessor may be repeatedly
forced to choose between two categories, neither
of which really fits their judgment. The contin-
uous nature of translation quality assessment, as
well as the fact that many statistical methods ex-
ist that can be applied to continuous data but not
interval-level data, motivates our trial of a contin-
uous rating scale.
We use human judgments of translation fluency
as a test case and compare consistency levels when
33
the conventional 5-point interval-level scale and a
continuous visual analog scale (VAS) are used for
human evaluation. We collected data via Ama-
zon?s Mechanical Turk, where the quality of an-
notations is known to vary considerably (Callison-
Burch et al, 2010). As such, we test two quality-
control techniques based on statistical significance
? made possible by the use of the continuous rating
scale ? to intrinsically assess the quality of individ-
ual human judges. The quality-control techniques
are not restricted to fluency judgments and are rel-
evant to more general MT evaluation, as well as
other NLP annotation tasks.
2 Machine Translation Fluency
Measurement of fluency as a component of MT
evaluation has been carried out for a number of
years (LDC, 2005), but it has proven difficult
to acquire consistent judgments, even from ex-
pert assessors. Evaluation rounds such as the an-
nual Workshop on Statistical Machine Translation
(WMT) use human judgments of translation qual-
ity to produce official rankings in shared tasks, ini-
tially using an two-item assessment of fluency and
adequacy as separate attributes, and more recently
by asking judges to simply rank system outputs
against one another according to ?which transla-
tion is better?. However, the latter method also re-
ports low levels of agreement between judges. For
example, the 2007 WMT reported low levels of
consistency in fluency judgments in terms of both
intra-annotator agreement (intra-aa), with a kappa
coefficient of ? = 0.54 (moderate), and inter-
annotator agreement (inter-aa), with ? = 0.25
(slight). Adequacy judgments for the same data
received even lower scores: ? = 0.47 for intra-aa,
and ? = 0.23 for inter-aa.
While concerns over annotator agreement have
seen recent WMT evaluations move away from us-
ing fluency as an evaluation component, there can
be no question that fluency is a useful means of
evaluating translation output. In particular, it is not
biased by reference translations. The use of auto-
matic metrics is often criticized by the fact that
a system that produces a good translation which
happens not to be similar to the reference trans-
lations will be unfairly penalized. Similarly, if
human annotators are provided with one or more
reference sentences, they may inadvertently favor
translations that are similar to those references. If
fluency is judged independently of adequacy, no
reference translation is needed, and the bias is re-
moved.
In earlier work, we consider the possibility
that translation quality is a hypothetical construct
(Graham et al, 2012), and suggest applying meth-
ods of validating measurement of psychological
constructs to the validation of measurements of
translation quality. In psychology, a scale that em-
ploys more items as opposed to fewer is consid-
ered more valid. Under this criteria, a two-item
(fluency and adequacy) scale is more valid than a
single-item translation quality measure.
3 Measurement Scales
Direct estimation methods are designed to elicit
from the subject a direct quantitative estimate of
the magnitude of an attribute (Streiner and Nor-
man, 1989). We compare judgments collected
on a visual analog scale (VAS) to those using an
interval-level scale presented to the human judge
as a sequence of radio-buttons. The VAS was first
used in psychology in the 1920?s, and prior to the
digital age, scales used a line of fixed length (usu-
ally 100mm in length), with anchor labels at both
ends, and to be marked by hand with an ?X? at the
desired location (Streiner and Norman, 1989).
When an interval-scale is used in NLP evalua-
tion or other annotation tasks, it is commonly pre-
sented in the form of an adjectival scale, where
categories are labeled in increasing/decreasing
quality. For example, an MT evaluation of fluency
might specify 5 = ?Flawless English?, 4 = ?Good
English?, 3 = ?Non-native English?, 2 = ?Disfluent
English?, and 1 = ?Incomprehensible? (Callison-
Burch et al, 2007; Denkowski and Lavie, 2010).
With both a VAS and an adjectival scale, the
choice of labels can be critical. In medical re-
search, patients? ratings of their own health have
been shown to be highly dependent on the ex-
act wording of descriptors (Seymour et al, 1985).
Alexandrov (2010) provides a summary of the ex-
tensive literature on the numerous issues associ-
ated with adjectival scale labels, including bias
resulting from positively and negatively worded
items not being true opposites of one another, and
items intended to have neutral intensity in fact
proving to have unique conceptual meanings.
Likert scales avoid the problems associated with
adjectival labels, by structuring the question as
a simple statement that the respondent registers
their level of (dis)agreement with. Figure 1 shows
34
Figure 1: Amazon Mechanical Turk interface for fluency judgments with a Likert-type scale.
Figure 2: Continuous rating scale for fluency judgments with two anchors.
the Likert-type interval-level scale we use to col-
lect fluency judgments of MT output, and Fig-
ure 2 shows an equivalent VAS using the two
most extreme anchor labels, strongly disagree and
strongly agree.
4 Crowd-sourcing Judgments
The volume of judgments required for evaluation
of NLP tasks can be large, and employing experts
to undertake those judgments may not always be
feasible. Crowd-sourcing services via the Web of-
fer an attractive alternative, and have been used in
conjunction with a range of NLP evaluation and
annotation tasks. Several guides exist for instruct-
ing researchers from various backgrounds on us-
ing Amazon?s Mechanical Turk (AMT) (Gibson et
al., 2011; Callison-Burch, 2009), and allowance
for the use of AMT is increasingly being made
in research grant applications, as a cost-effective
way of gathering data. Issues remain in connec-
tion with low payment levels (Fort et al, 2011);
nevertheless, Ethics Approval Boards are typically
disinterested in projects that make use of AMT, re-
garding AMT as being a purchased service rather
than a part of the experimentation that may affect
human subjects.
The use of crowd-sourced judgments does,
however, introduce the possibility of increased in-
consistency, with service requesters typically hav-
ing no specific or verifiable knowledge about any
given worker. Hence, the possibility that a worker
is acting in good faith but not performing the task
well must be allowed for, as must the likelihood
that some workers will quite ruthlessly seek to
minimize the time spent on the task, by deliber-
ately giving low-quality or fake answers. Some
workers may even attempt to implement auto-
mated responses, so that they get paid without hav-
ing to do the work they are being paid for.
For example, if the task at hand is that of assess-
ing the fluency of text snippets, it is desirable to
employ native speakers. With AMT the requester
has the ability to restrict responses to only workers
who have a specified skill. But that facility does
not necessarily lead to confidence ? there is noth-
ing stopping a worker employing someone else
to do the test for them. Devising a test that reli-
ably evaluates whether or not someone is a native
speaker is also not at all straightforward.
Amazon allow location restrictions, based on
the registered residential address of the Turker,
which can be used to select in favor of those likely
to have at least some level of fluency (Callison-
Burch et al, 2010). We initially applied this re-
striction to both sets of judgments in experiments,
setting the task up so that only workers regis-
tered in Germany could evaluate the to-German
translations, for example. However, very low re-
35
sponse rates for languages other than to-English
were problematic, and we also received a number
of apparently-genuine requests from native speak-
ers residing outside the target countries. As a
result, we removed all location restrictions other
than for the to-English tasks.1
Crowd-sourcing judgments has the obvious risk
of being vulnerable to manipulation. On the other
hand, crowd-sourced judgments also offer the po-
tential of being more valid than those of experts,
since person-in-the-street abilities might be a more
useful yardstick for some tasks than informed aca-
demic judgment, and because a greater number of
judges may be available.
Having the ability to somehow evaluate the
quality of the work undertaken by a Turker is thus
highly desirable. We would like to be able to put
in place a mechanism that filters out non-native
speakers; native speakers with low literacy levels;
cheats; and robotic cheats. That goal is considered
in the next section.
5 Judge-Intrinsic Quality Control
One common method of quality assessment for a
new process is to identify a set of ?gold-standard?
items that have been judged by experts and whose
merits are agreed, present them to the new process
or assessor, and then assess the degree to which
the new process and the experts ?agree? on the
outcomes (Snow et al, 2008; Callison-Burch et
al., 2010). A possible concern is that even experts
can be expected to disagree (and hence have low
inter-aa levels), meaning that disagreement with
the new process will also occur, even if the new
process is a reliable one. In addition, the qual-
ity of the judgments collected is also assessed via
agreement levels, meaning that any filtering based
on a quality-control measure that uses agreement
will automatically increase consistency, even to
the extent of recalibrating non-expert workers? re-
sponses to more closely match expert judgments
(Snow et al, 2008). Moreover, if an interval-level
scale is used, standardized scores cannot be em-
ployed, so a non-expert who is more lenient than
the experts, but in a reliable and systematic man-
ner, might still have their assessments discarded.
For judgments collected on a continuous scale,
statistical tests based on difference of means (over
assessors) are possible. We structure our human
1It has also been suggested that AMT restricts Turker reg-
istration by country; official information is unclear about this.
T1 initial :
T1 repeat :
d1 
Bad Ref for T2 :
d2 
T2 initial :
Figure 3: Intrinsic quality-control distributions for
an individual judge.
intelligence tasks (HITs) on Mechanical Turk in
groups of 100 in a way that allows us to control
assignment of repeat item pairs to workers, so that
statistical tests can later be applied to an individ-
ual worker?s score distributions for repeat items.
Workers were made aware of the task structure
before accepting it ? the task preview included a
message This HIT consists of 100 fluency assess-
ments, you have 0 so far complete.
We refer to the repeat items in a HIT as
ask again translations. In addition, we inserted a
number of bad reference pairs into each HIT, with
a bad reference pair consisting of a genuine MT
system output, and a distorted sentence derived
from it, expecting that its fluency was markedly
worse than that of the corresponding system out-
put. This was done by randomly selecting two
words in the sentence and duplicating them in ran-
dom locations not adjacent to the original word
and not in the initial or sentence-final position.
Any other degradation method could also be used,
so long as it has a high probability of reducing the
fluency of the text, and provided that it is not im-
mediately obvious to the judges.
Insertion of ask again and bad reference pairs
into the HITs allowed two measurements to be
made for each worker: when presented with
an ask again pair, we expect a conscientious
judge to give similar scores (but when using
a continuous scale, certainly not identical), and
on bad reference pairings a conscientious judge
should reliably give the altered sentence a lower
score. The wide separation of the two appear-
ances of an ask again pair makes it unlikely that
a judge would remember either the sentence or
their first reaction to it, and backwards movement
through the sentences comprising each HIT was
not possible. In total, each HIT contained 100 sen-
36
Figure 4: Welch?s t-test reliability estimates plot-
ted against mean seconds per judgment.
tences, including 10 bad reference pairs, and 10
ask again pairs.
Figure 3 illustrates these two types of pairs,
presuming that over the course of one or more
HITs each worker has assessed multiple ask again
pairs generating the distribution indicated by d1,
and also multiple bad reference pairs, generating
the distribution indicated by d2. As an estimate
of the reliability of each individual judge we ap-
ply a t-test to compare ask again differences with
bad reference differences, with the expectation
that for a conscientious worker the latter should
be larger than the former. Since there is no guar-
antee that the two distributions of d1 and d2 have
the same variance, we apply Welch?s adaptation of
the Student t-test.
The null hypothesis to be tested for each AMT
worker is that the score difference for ask again
pairs is not less than the score difference for
bad reference pairs. Lower p values mean more
reliable workers; in the experiments that are re-
ported shortly, we use p < 0.05 as a threshold
of reliability. We also applied the non-parametric
Mann-Whitney test to the same data, for the pur-
pose of comparison, since there is no guarantee
that d1 and d2 will be normally distributed for a
given assessor.
The next section provides details of the experi-
mental structure, and then describes the outcomes
in terms of their effect on overall system rank-
ings. As a preliminary indication of Turker be-
havior, Figure 4 summarizes some of the data that
was obtained. Each plotted point represents one
AMT worker who took part in our experiments,
and the horizontal axis reflects their average per-
judgment time (noting that this is an imprecise
measurement, since they may have taken phone
calls or answered email while working through a
HIT, or simply left the task idle to help obscure
a lack of effort). The vertical scale is the p value
obtained for that worker when the ask again distri-
bution is compared to their bad reference distribu-
tion, with a line at p = 0.05 indicating the upper
limit of the zone for which we are confident that
they had a different overall response to ask again
pairs than they did to bad reference pairs. Note the
small number of very fast, very inaccurate work-
ers at the top left; we have no hesitation in call-
ing them unconscientious (and declining to pay
them for their completed HITs). Note also the very
small number of workers for which it was possi-
ble to reliably distinguish their ask again behavior
from their bad reference behavior.
6 Experiments
HIT Structure
A sample of 560 translations was selected at
random from the WMT 2012 published shared
task dataset for a range of language pairs, with
segments consisting of 70 translations, each as-
signed to a total of eight distinct HITs. The sen-
tences were generated as image files, as recom-
mended for judgment of translations (Callison-
Burch, 2009). Each HIT was presented to a worker
as a set of 100 sentences including a total of 30
quality control items, with only one sentence visi-
ble on-screen at any given time. Each quality con-
trol item comprised a pair of corresponding trans-
lations, widely separated within the HIT. Three
kinds of quality control pairs were used:
? ask again: system output and exact repeat;
? bad reference: system output and an altered
version of it with noticeably lower fluency;
and
? good reference: system output and the corre-
sponding human produced reference transla-
tion (as provided in the released WMT data).
Each HIT consisted of 10 groups, each containing
10 sentences: 7 ?normal? translations, plus one
of each type of quality control translation drawn
37
from one of the other groups in the HIT in such
a way that 40?60 judgments would be completed
between the elements of any quality-control pair.
Consistency of Human Judgments
Using judgments collected on the continuous rat-
ing scale, we first examine assessor consistency
based on Welch?s t-test and the non-parametric
Mann-Whitney U-test. In order to examine the de-
gree to which human assessors assign consistent
scores, we compute mean values of d1 (Figure 3)
when ask again pairs are given to the same judge,
and across pairs of judges. Three sets of results
are shown: the raw unfiltered data; data filtered
according to p < 0.05 according to the quality-
control regime described in the previous section
using the Welch?s t-test; and data filtered using
the Mann-Whitney U-test. Table 1 shows that the
t-test indicates that only 13.1% of assessors meet
quality control hurdle, while a higher proportion,
35.7%, of assessors are deemed acceptable.
The stricter filter, Welch?s t-test, yields more
consistent scores for same-judge repeat items: de-
creases of 4.5 (mean) and 4.2 (sd) are observed
when quality control is applied. In addition, re-
sults for Welch?s t-test show high levels of con-
sistency for same-judge repeat items: an average
difference of only 9.5 is observed, which is not
unreasonable, given that the scale is 100 points
in length and a 10-point difference corresponds to
just 60 pixels on the screen.
For repeat items rated by distinct judges, both
filtering methods decrease the mean difference in
scores compared to the unfiltered baseline, with
the two tests giving similar improvements.
When an interval-level scale is used to evaluate
the data, the Kappa coefficient is commonly used
to evaluate consistency levels of human judges
(Callison-Burch et al, 2007), where Pr(a) is the
relative observed agreement among raters, and
Pr(e) is the hypothetical probability of chance
agreement:
? =
Pr(a)? Pr(e)
1? Pr(e)
In order to use the Kappa coefficient to compare
agreement levels for the interval-level and contin-
uous scales, we convert continuous scale scores to
a target number of interval categories. We do this
primarily for a target number of five, as this best
provides a comparison between scores for the 5-
point interval-level scale. But we also present re-
sults for targets of four and two categories, since
the continuous scale is marked at the midway and
quarter points, providing implicit intervals. A two-
category is also interesting if the assessment pro-
cess is regarded as dichotomizing to only include
for each translation whether or not the judge con-
sidered it to be ?good? or ?bad?. Use of statisti-
cal difference of means tests on interval-level data
is not recommended; but for the purpose of illus-
tration, we also applied Welch?s t-test to quality
control workers that completed the interval-level
HITs, with the same threshold of p < 0.05.
Tables 2 and 3 show intra-annotator agreement
for the five-point interval scale and continuous
scales, with and without quality control.2 Results
for repeat items on the interval-level scale show
that quality control only alters intra-aa marginally
(Pr(a) increases by 1%), and that inter-aa levels
worsen (Pr(a) decreases by 6.2%). This confirms
that applying statistical tests to interval-level data
is not a suitable way of filtering out low quality
workers.
When comparing consistency levels of asses-
sors using the interval-level scale to those of the
continuous scale, we observe marginally lower ?
coefficients for both intra-aa (?0.009) and inter-
aa (?0.041) for the continuous scale. However,
this is likely to be in part due to the fact that the
continuous scale corresponds more intuitively to 4
categories, and agreement levels for the unfiltered
4-category continuous scale are higher than those
collected on the interval-level scale by +0.023
intra-aa and +0.014 inter-aa.
Applying quality-control on the continuous
scale results in dramatic increases in intra-aa lev-
els: +0.152 for 5-categories (5-cat), +0.100 for
4-categories (4-cat) and +0.096 for 2-categories
(2-cat). When considering inter-aa levels, quality-
control does not directly result in as dramatic an
increase, as inter-aa levels increase by +0.010
for 5-cat, +0.006 for 4-cat and +0.004 for 2-cat.
It is likely, however, that apparent disagreement
between assessors might be due to different as-
sessors judging fluency generally worse or better
than one another. The continuous scale allows for
scores to be standardized by normalizing scores
with respect to the mean and standard deviation
of all scores assigned by a given individual judge.
We therefore transform scores of each judge into
2Note that the mapping from continuous scores to cate-
gories was not applied for quality control.
38
same judge distinct judges
workers judgments mean sd mean sd
Unfiltered 100.0% 100.0% 14.0 18.4 28.9 23.5
Welch?s t-test 13.1% 23.5% 9.5 14.2 25.2 21.0
Mann-Whitney U-test 35.7% 48.8% 13.1 17.7 25.0 22.6
Table 1: Mean and standard deviation of score differences for continuous scale with ask again items
within a given judge and across two distinct judges, for no quality control (unfiltered), Welch?s t-test and
Mann-Whitney U-test with a quality-control threshold of p < 0.05.
# 5-pt. interval 5-pt. interval continuous continuous
categ- unfiltered filtered unfiltered filtered
ories Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ?
5 60.4% 0.505 61.4% 0.517 59.7% 0.496 71.8% 0.647
4 - - - - 64.6% 0.528 72.1% 0.629
2 - - - - 85.2% 0.704 90.0% 0.800
Table 2: Intra-annotator (same judge) agreement levels for 5-point interval and continuous scales for
unfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.
corresponding z-scores and use percentiles of the
combined set of all scores to map z-scores to cat-
egories where a score falling in the bottom 20 th
percentile corresponds to strongly disagree, scores
between the 20 th and 40 th percentile to disagree,
and so on. Although this method of transformation
is somewhat harsh on the continuous scale, since
scores no longer correspond to different locations
on the original scale, it nevertheless shows an in-
crease in consistency of +0.05 (5-cat), +0.086 (4-
cat) and +0.144 (2-cat). However, caution must
be taken when interpreting consistency for stan-
dardized scores, as can be seen from the increase
in agreement observed when unfiltered scores are
standardized.
Table 4 shows a breakdown by target language
of the proportion of judgments collected whose
scores met the significance threshold of p < 0.05.
Results appear at first to have shockingly low lev-
els of high quality work, especially for English and
German. When running the tasks in Mechanical
Turk, it is worth noting that we did not adopt statis-
tical tests to automatically accept/reject HITs and
we believe this would be rather harsh on workers.
Our method of quality control is a high bar to reach
and it is likely that many workers that do not meet
the significance threshold would still have been
working in good faith. In practice, we individually
examined mean scores for reference translation,
system outputs and bad reference pairs, and only
declined payment when there was no doubt the re-
English German French Spanish
10.0% 0% 57.9% 62.5%
Table 4: High quality judgments, by language.
sponse was either automatic or extremely careless.
The structure of the task and the fact that the
quality-control items were somewhat hidden may
have lulled workers into a false sense of compla-
cency, and perhaps encouraged careless responses.
However, even taking this into consideration, the
fact that none of the German speaking asses-
sors and just 10% of English speaking assessors
reached our standards serves to highlight the im-
portance of good quality-control techniques when
employing services like AMT. In addition, the risk
of getting low quality work for some languages
might be more risky than for others. The response
rate for high quality work for Spanish and French
was so much higher than German and English,
perhaps by chance, or perhaps the result of factors
that will be revealed in future experimentation.
System Rankings
As an example of the degree to which system
rankings are affected by applying quality control,
for the language direction for which we achieved
the highest number of high quality assessments,
English-to-Spanish, we include system rankings
by mean score with each measurement scale, with
and without quality control and for mean z-scores
39
# 5-pt. interval 5-pt. interval continuous continuous cont. standrdzed. cont. standrdzed.
categ- unfiltered qual.-controlled unfiltered qual.-controlled unfiltered qual.-controlled
ories Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ?
5 33.0% 0.16 26.8% 0.084 29.5% 0.119 30.3% 0.128 30.2% 0.1272 33.5% 0.169
4 - - - - 38.1% 0.174 38.5% 0.180 35.5% 0.1403 44.5% 0.260
2 - - - - 66.5% 0.331 66.8% 0.335 75.5% 0.5097 73.8% 0.475
Table 3: Inter-annotator (distinct judge) agreement levels for 5-point interval and continuous scales for
unfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.
z-scores
5-pt. 5-pt. continuous continuous continuous
unfiltered qual.-controlled unfiltered qual.-controlled qual.-controlled
Sys A 2.00 Sys A 2.00 Sys E 69.60 Sys E 74.39 Sys E 0.43
Sys B 1.98 Sys D 1.97 Sys B 61.78 Sys F 65.07 Sys B 0.16
Sys C 1.98 Sys F 1.95 Sys G 60.21 Sys G 64.51 Sys G 0.08
Sys D 1.98 Sys C 1.95 Sys F 59.38 Sys B 63.68 Sys D 0.06
Sys E 1.98 Sys E 1.95 Sys D 59.05 Sys D 63.52 Sys C 0.02
Sys F 1.97 Sys B 1.94 Sys A 57.44 Sys C 61.33 Sys F 0.01
Sys G 1.97 Sys G 1.93 Sys I 56.31 Sys A 58.43 Sys H ?0.03
Sys H 1.96 Sys H 1.90 Sys C 55.82 Sys I 57.46 Sys I ?0.07
Sys I 1.96 Sys I 1.88 Sys H 55.27 Sys H 57.04 Sys A ?0.10
Sys J 1.94 Sys J 1.81 Sys J 50.46 Sys J 50.73 Sys J ?0.23
Sys K 1.90 Sys K 1.76 Sys K 44.62 Sys K 41.25 Sys K ?0.47
Table 5: WMT system rankings based on approximately 80 randomly-selected fluency judgments per
system, with and without quality control for radio button and continuous input types, based on German-
English. The quality control method applied is annotators who score worsened system output and gen-
uine system outputs with statistically significant lower scores according to paired Student?s t-test.
when raw scores are normalized by individual as-
sessor mean and standard deviation. The results
are shown in Table 5. (Note that we do not claim
that these rankings are indicative of actual system
rankings, as only fluency of translations was as-
sessed, using an average of just 55 translations per
system.)
When comparing system rankings for unfiltered
versus quality-controlled continuous scales, firstly
the overall difference in ranking is not as dramatic
as one might expect, as many systems retain the
same rank order, with only a small number of sys-
tems changing position. This happens because
random-clickers cannot systematically favor any
system, and positive and negative random scores
tend to cancel each other out. However, even hav-
ing two systems ordered incorrectly is of concern;
careful quality control, and the use of normaliza-
tion of assessors? scores may lead to more consis-
tent outcomes. We also note that incorrect system
orderings may lead to flow-on effects for evalua-
tion of automatic metrics.
The system rankings in Table 5 also show how
the use of the continuous scale can be used to rank
systems according to z-scores, so that individual
assessor preferences over judgments can be ame-
liorated. Interestingly, the system that scores clos-
est to the mean, Sys F, corresponds to the baseline
system for the shared task with a z-score of 0.01.
7 Conclusion
We have compared human assessor consistency
levels for judgments collected on a five-point
interval-level scale to those collected on a contin-
uous scale, using machine translation fluency as
a test case. We described a method for quality-
controlling crowd-sourced annotations that results
in marked increases in intra-annotator consistency
and does not require judges to agree with experts.
In addition, the use of a continuous scale allows
scores to be standardized to eliminate individual
judge preferences, resulting in higher levels of
inter-annotator consistency.
40
Acknowledgments
This work was funded by the Australian Research
Council. Ondr?ej Bojar, Rosa Gog, Simon Gog,
Florian Hanke, Maika Vincente Navarro, Pavel
Pecina, and Djame Seddah provided translations
of task instructions, and feedback on published
HITs.
References
A. Alexandrov. 2010. Characteristics of single-item
measures in Likert scale format. The Electronic
Journal of Business Research Methods, 8:1?12.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proc. 2nd Wkshp. Statistical Machine
Translation, pages 136?158, Prague, Czech Repub-
lic.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz,
and J. Schroeder. 2008. Further meta-evaluation of
machine translation. In Proc. 3rd Wkshp. Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio.
C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Wkshp. Statistical Machine Translation,
pages 1?28, Athens, Greece.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 Joint Workshop on Statistical Machine Trans-
lation and Metrics for Machine Translation. In Proc.
5th Wkshp. Statistical Machine Translation, pages
17?53, Uppsala, Sweden.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 Workshop on Statistical
Machine Translation. In Proc. 6th Wkshp. Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada.
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. Conf. Empirical Methods in
Natural Language Processing, pages 286?295, Sin-
gapore.
M. Denkowski and A. Lavie. 2010. Choosing the right
evaluation for machine translation: An examination
of annotator and automatic metric performance on
human judgement tasks. In Proc. 9th Conf. Assoc.
Machine Translation in the Americas (AMTA), Den-
ver, Colorado.
K. Fort, G. Adda, and K. B. Cohen. 2011. Amazon
Mechanical Turk: Gold mine or coal mine? Com-
putational Linguistics, 37(2):413?420.
E. Gibson, S. Piantadosi, and K. Fedorenko. 2011. Us-
ing Mechanical Turk to obtain and analyze English
acceptability judgments. Language and Linguistics
Compass, 5/8:509?524.
Y. Graham, T. Baldwin, A. Harwood, A. Moffat, and
J. Zobel. 2012. Measurement of progress in ma-
chine translation. In Proc. Australasian Language
Technology Wkshp., pages 70?78, Dunedin, New
Zealand.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Technical report, Linguistic Data Consortium. Re-
vision 1.5.
R. A. Seymour, J. M. Simpson, J. E. Charlton, and
M. E. Phillips. 1985. An evaluation of length and
end-phrase of visiual analogue scales in dental pain.
Pain, 21:177?185.
R. Snow, B. O?Connor, D. Jursfsky, and A. Y. Ng.
2008. Cheap and fast ? but is it good? Evalu-
ating non-expert annotations for natural language
tasks. In Proc. Conf. Empirical Methods in Natu-
ral Language Processing, pages 254?263, Honolulu,
Hawaii.
D. L. Streiner and G. R. Norman. 1989. Health Mea-
surement Scales: A Practical Guide to their Devel-
opment and Use. Oxford University Press, fourth
edition.
41
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266?274,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Randomized Significance Tests in Machine Translation
Yvette Graham Nitika Mathur Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
ygraham@unimelb.edu.au, nmathur@student.unimelb.edu.au, tb@ldwin.net
Abstract
Randomized methods of significance test-
ing enable estimation of the probability
that an increase in score has occurred sim-
ply by chance. In this paper, we examine
the accuracy of three randomized meth-
ods of significance testing in the context
of machine translation: paired bootstrap
resampling, bootstrap resampling and ap-
proximate randomization. We carry out
a large-scale human evaluation of shared
task systems for two language pairs to
provide a gold standard for tests. Re-
sults show very little difference in accu-
racy across the three methods of signif-
icance testing. Notably, accuracy of all
test/metric combinations for evaluation of
English-to-Spanish are so low that there is
not enough evidence to conclude they are
any better than a random coin toss.
1 Introduction
Automatic metrics, such as BLEU (Papineni et
al., 2002), are widely used in machine translation
(MT) as a substitute for human evaluation. Such
metrics commonly take the form of an automatic
comparison of MT output text with one or more
human reference translations. Small differences
in automatic metric scores can be difficult to inter-
pret, however, and statistical significance testing
provides a way of estimating the likelihood that a
score difference has occurred simply by chance.
For several metrics, such as BLEU, standard sig-
nificance tests cannot be applied due to scores
not comprising the mean of individual sentence
scores, justifying the use of randomized methods.
Bootstrap resampling was one of the early ran-
domized methods proposed for statistical signifi-
cance testing of MT (Germann, 2003; Och, 2003;
Kumar and Byrne, 2004; Koehn, 2004), to assess
for a pair of systems how likely a difference in
BLEU scores occurred by chance. Empirical tests
detailed in Koehn (2004) show that even for test
sets as small as 300 translations, BLEU confidence
intervals can be computed as accurately as if they
had been computed on a test set 100 times as large.
Approximate randomization was subsequently
proposed as an alternate to bootstrap resam-
pling (Riezler and Maxwell, 2005). Theoretically
speaking, approximate randomization has an ad-
vantage over bootstrap resampling, in that it does
not make the assumption that samples are repre-
sentative of the populations from which they are
drawn. Both methods require some adaptation in
order to be used for the purpose of MT evalua-
tion, such as combination with an automatic met-
ric, and therefore it cannot be taken for granted
that approximate randomization will be more ac-
curate in practice. Within MT, approximate ran-
domization for the purpose of statistical testing is
also less common.
Riezler and Maxwell (2005) provide a compar-
ison of approximate randomization with bootstrap
resampling (distinct from paired bootstrap resam-
pling), and conclude that since approximate ran-
domization produces higher p-values for a set of
apparently equally-performing systems, it more
conservatively concludes statistically significant
differences, and recommend preference of approx-
imate randomization over bootstrap resampling
for MT evaluation. Conclusions drawn from ex-
periments provided in Riezler and Maxwell (2005)
are oft-cited, with experiments interpreted as ev-
idence that bootstrap resampling is overly opti-
mistic in reporting significant differences (Riezler
and Maxwell, 2006; Koehn and Monz, 2006; Gal-
ley and Manning, 2008; Green et al., 2010; Monz,
2011; Clark et al., 2011).
Our contribution in this paper is to revisit sta-
tistical significance tests in MT ? namely, boot-
strap resampling, paired bootstrap resampling and
266
approximate randomization ? and find problems
with the published formulations. We redress these
issues, and apply the tests in statistical testing of
two language pairs. Using human judgments of
translation quality, we find only very minor differ-
ences in significance levels across the three tests,
challenging claims made in the literature about rel-
ative merits of tests.
2 Revisiting Statistical Significance Tests
for MT Evaluation
First, we revisit the formulations of bootstrap
resampling and approximate randomization al-
gorithms as presented in Riezler and Maxwell
(2005). At first glance, both methods appear to
be two-tailed tests, with the null hypothesis that
the two systems perform equally well. To facili-
tate a two-tailed test, absolute values of pseudo-
statistics are computed before locating the abso-
lute value of the actual statistic (original differ-
ence in scores). Using absolute values of pseudo-
statistics is not problematic in the approximate
randomization algorithm, and results in a reason-
able two-tailed significance test. However, the
bootstrap algorithm they provide uses an addi-
tional shift-to-zero method of simulating the null
hypothesis. The way in which this shift-to-zero
and absolute values of pseudo-statistics are ap-
plied is non-standard. Combining shift-to-zero
and absolute values of pseudo-statistics results
in all pseudo-statistics that fall below the mean
pseudo-statistic to be omitted from computation of
counts later used to compute p-values. The ver-
sion of the bootstrap algorithm, as provided in the
pseudo-code, is effectively a one-tailed test, and
since this does not happen in the approximate ran-
domization algorithm, experiments appear to com-
pare p-values from a one-tailed bootstrap test di-
rectly with those of a two-tailed approximate ran-
domization test. This inconsistency is not recog-
nized, however, and p-values are compared as if
both tests are two-tailed.
A better comparison of p-values would first re-
quire doubling the values of the one-sided boot-
strap, leaving those of the two-sided approximate
randomization algorithm as-is. The results of the
two tests on this basis are extremely close, and
in fact, in two out of the five comparisons, those
of the bootstrap would have marginally higher p-
values than those of approximate randomization.
As such, it is conceivable to conclude that the ex-
periments actually show no substantial difference
in Type I error between the two tests, which is con-
sistent with results published in other fields of re-
search (Smucker et al., 2007). We also note that
the pseudo-code contains an unconventional com-
putation of mean pseudo-statistics, ?
B
, for shift-
to-zero.
Rather than speculate over whether these is-
sues with the original paper were simply presen-
tational glitches or the actual basis of the experi-
ments reported on in the paper, we present a nor-
malized version of the two-sided bootstrap algo-
rithm in Figure 1, and report on the results of our
own experiments in Section 4. We compare this
method with approximate randomization and also
paired bootstrap resampling (Koehn, 2004), which
is widely used in MT evaluation. We carry out
evaluation over a range of MT systems, not only
including pairs of systems that perform equally
well, but also pairs of systems for which one
system performs marginally better than the other.
This enables evaluation of not only Type I error,
but the overall accuracy of the tests. We carry out
a large-scale human evaluation of all WMT 2012
shared task participating systems for two language
pairs, and collect sufficient human judgments to
facilitate statistical significance tests. This hu-
man evaluation data then provides a gold-standard
against which to compare randomized tests. Since
all randomized tests only function in combina-
tion with an automatic MT evaluation metric, we
present results of each randomized test across four
different MT metrics.
3 Randomized Significance Tests
3.1 Bootstrap Resampling
Bootstrap resampling provides a way of estimat-
ing the population distribution by sampling with
replacement from a representative sample (Efron
and Tibshirani, 1993). The test statistic is taken
as the difference in scores of the two systems,
S
X
? S
Y
, which has an expected value of 0 under
the null hypothesis that the two systems perform
equally well. A bootstrap pseudo-sample consists
of the translations by the two systems (X
b
, Y
b
) of
a bootstrapped test set (Koehn, 2004), constructed
by sampling with replacement from the original
test set translations. The bootstrap distribution
S
boot
of the test statistic is estimated by calculat-
ing the value of the pseudo-statistic S
X
b
? S
Y
b
for
each pseudo-sample.
267
Set c = 0
Compute actual statistic of score differences S
X
? S
Y
on test data
Calculate sample mean ?
B
=
1
B
B?
b=1
S
X
b
? S
Y
b
over
bootstrap samples b = 1, ..., B
For bootstrap samples b = 1, ..., B
Sample with replacement from variable tuples test
sentences for systems X and Y
Compute pseudo-statistic S
X
b
? S
Y
b
on bootstrap data
If |S
X
b
? S
Y
b
? ?
B
| ? |S
X
? S
Y
|
c = c+ 1
If c/B ? ?
Reject the null hypothesis
Figure 1: Two-sided bootstrap resampling statisti-
cal significance test for automatic MT evaluation
Set c = 0
Compute actual statistic of score differences S
X
? S
Y
on test data
For random shuffles r = 1, ..., R
For sentences in test set
Shuffle variable tuples between systems X and Y
with probability 0.5
Compute pseudo-statistic S
X
r
? S
Y
r
on shuffled data
If S
X
r
? S
Y
r
? S
X
? S
Y
c = c+ 1
If c/R ? ?
Reject the null hypothesis
Figure 2: Approximate randomization statistical
significance test for automatic MT evaluation
The null hypothesis distribution S
H
0
can be es-
timated from S
boot
by applying the shift method
(Noreen, 1989), which assumes that S
H
0
has the
same shape but a different mean than S
boot
. Thus,
S
boot
is transformed into S
H
0
by subtracting the
mean bootstrap statistic from every value in S
boot
.
Once this shift-to-zero has taken place, the null
hypothesis is rejected if the probability of observ-
ing a more extreme value than the actual statistic
is lower than a predetermined p-value ?, which is
typically set to 0.05. In other words, the score dif-
ference is significant at level 1? ?.
Figure 3 provides a one-sided implementation
of bootstrap resampling, whereH
0
is that the score
of System X is less than or equal to the score of
Set c = 0
Compute actual statistic of score differences S
X
? S
Y
on test data
Calculate sample mean ?
B
=
1
B
B?
b=1
S
X
b
? S
Y
b
over
bootstrap samples b = 1, ..., B
For bootstrap samples b = 1, ..., B
Sample with replacement from variable tuples test
sentences for systems X and Y
Compute pseudo-statistic S
X
b
? S
Y
b
on bootstrap data
If S
X
b
? S
Y
b
? ?
B
? S
X
? S
Y
c = c+ 1
If c/B ? ?
Reject the null hypothesis
Figure 3: One-sided Bootstrap resampling statisti-
cal significance test for automatic MT evaluation
Set c = 0
For bootstrap samples b = 1, ..., B
If S
X
b
< S
Y
b
c = c+ 1
If c/B ? ?
Reject the null hypothesis
Figure 4: Paired bootstrap resampling randomized
significance test
System Y . Figure 5 includes a typical example of
bootstrap resampling applied to BLEU, for a pair
of systems for which differences in scores are sig-
nificant, while Figure 6 shows the same for ME-
TEOR but for a pair of systems with no significant
difference in scores.
3.2 Approximate Randomization
Unlike bootstrap, approximate randomization
does not make any assumptions about the popula-
tion distribution. To simulate a distribution for the
null hypothesis that the scores of the two systems
are the same, translations are shuffled between the
two systems so that 50% of each pseudo-sample
is drawn from each system. In the context of ma-
chine translation, this can be interpreted as each
translation being equally likely to have been pro-
duced by one system as the other (Riezler and
Maxwell, 2005).
The test statistic is taken as the difference in
scores of the two systems, S
X
? S
Y
. If there is
268
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Paired Bootstrap Res. BLEU
originc = 13
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Bootstrap Resampling BLEU
actual statisticc = 14
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Approximate Randomization  BLEU
actual statisticc = 11
Figure 5: Pseudo-statistic distributions for a typical pair of systems with close BLEU scores for each
randomized test (System F vs. System G).
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Paired Bootstrap Res. METEOR
originc = 269
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Bootstrap Resampling METEOR
actual statisticc = 275
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Approximate Randomization  METEOR
actual statisticc = 260
Figure 6: Pseudo-statistic distributions of METEOR with randomized tests (System D vs. System A).
a total of S sentences, then a total of 2
S
shuffles is
possible. If S is large, instead of generating all 2
S
possible combinations, we instead generate sam-
ples by randomly permuting translations between
the two systems with equal probability. The distri-
bution of the test statistic under the null hypoth-
esis is approximated by calculating the pseudo-
statistic, S
X
r
? S
Y
r
, for each sample. As before,
the null hypothesis is rejected if the probability of
observing a more extreme value than the actual
test statistic is lower than ?.
Figure 2 provides a one-sided implementation
of approximate randomization for MT evaluation,
where the null hypothesis is that the score of Sys-
tem X is less than or equal to the score of System
Y . Figure 5 shows a typical example of pseudo-
statistic distributions for approximate randomiza-
tion for a pair of systems with a small but signifi-
cant score difference according to BLEU, and Fig-
ure 6 shows the same for METEOR applied to a
pair of systems where no significant difference is
concluded.
3.3 Paired Bootstrap Resampling
Paired bootstrap resampling (Koehn, 2004) is
shown in Figure 4. Unlike the other two random-
ized tests, this method makes no attempt to simu-
late the null hypothesis distribution. Instead, boot-
strap samples are used to estimate confidence in-
tervals of score differences, with confidence inter-
vals not containing 0 implying a statistically sig-
nificant difference.
We compare what takes place with the two other
tests, by plotting differences in scores for boot-
strapped samples, S
X
b
? S
Y
b
, as shown in Fig-
ure 5 for BLEU and Figure 6 for METEOR. Instead
of computing counts with reference to the actual
statistic, the line through the origin provides the
cut-off for counts.
269
Adequacy Fluency Combined
p-value
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E Syst
em.F
Syst
em.G Syst
em.H Syst
em.J
Syst
em.K Syst
em.L
Syst
em.M Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E Syst
em.F
Syst
em.G Syst
em.H Syst
em.J
Syst
em.K Syst
em.L
Syst
em.M Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E Syst
em.F
Syst
em.G Syst
em.H Syst
em.J
Syst
em.K Syst
em.L
Syst
em.M
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
Figure 7: Human evaluation pairwise significance tests for Spanish-to-English systems (colored cells
denote scores for System row being significantly greater than System column .
4 Evaluation
In order to evaluate the accuracy of the three ran-
domized significance significance tests, we com-
pare conclusions reached in a human evaluation
of shared task participant systems. We carry out
a large-scale human evaluation of all participating
systems from WMT 2012 (Callison-Burch et al.,
2012) for the Spanish-to-English and English-to-
Spanish translation tasks. Large numbers of hu-
man assessments of translations were collected us-
ing Amazon?s Mechanical Turk, with strict qual-
ity control filtering (Graham et al., 2013). A to-
tal of 82,100 human adequacy assessments and
62,400 human fluency assessments were collected.
After the removal of quality control items and
filtering of judgments from low-quality workers,
this resulted in an average of 1,280 adequacy and
1,013 fluency assessments per system for Spanish-
to-English (12 systems), and 1,483 adequacy and
1,534 fluency assessments per system for English-
to-Spanish (11 systems). To remove bias with re-
spect to individual human judge preference scor-
ing severity/leniency, scores provided by each hu-
man assessor were standardized according to the
mean and standard deviation of all scores provided
by that individual.
Significance tests were carried out over the
scores for each pair of systems separately for
adequacy and fluency assessments using the
Wilcoxon rank-sum test. Figure 7 shows pairwise
significance test results for fluency, adequacy and
the combination of the two tests, for all pairs of
Spanish-to-English systems. Combined fluency
and adequacy significance test results are con-
structed as follows: if a system?s adequacy score is
significantly greater than that of another, the com-
bined conclusion is that it is significantly better,
at that significance level. Only when a tie in ad-
equacy scores occurs are fluency judgments used
to break the tie. In this case, p-values from signifi-
cance tests applied to fluency scores of that system
pair are used. For example, in Figure 7, adequacy
scores of System B are not significantly greater
than those of Systems C, D and E, while fluency
scores for System B are significantly greater than
those of the three other systems. The combined re-
sult for each pair of systems is therefore taken as
the p-value from the corresponding fluency signif-
icance test.
We use the combined human evaluation pair-
wise significant tests as a gold standard against
which to evaluate the randomized methods of sta-
tistical significance testing. We evaluate paired
bootstrap resampling (Koehn, 2004) and bootstrap
resampling as shown in Figure 3 and approxi-
mate randomization as shown in Figure 2, each
in combination with four automatic MT metrics:
BLEU (Papineni et al., 2002), NIST (NIST, 2002),
METEOR (Banerjee and Lavie, 2005) and TER
(Snover et al., 2006).
4.1 Results and Discussion
Figure 8 shows the outcome of pairwise random-
ized significance tests for each metric for Spanish-
to-English systems, and Table 1 shows numbers of
correct conclusions and accuracy of each test.
When we compare conclusions made by the
three randomized tests for Spanish-to-English sys-
tems, there is very little difference in p-values for
all pairs of systems. For both BLEU and NIST,
270
Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand.
? Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%)
0.05
BLEU 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]
NIST 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2]
METEOR 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]
TER 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]
0.01
BLEU 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
NIST 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]
TER 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
0.001
BLEU 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]
NIST 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]
METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 52 78.8 [67.0, 87.9]
TER 50 75.8 [63.6, 85.5] 51 77.3 [65.3, 86.7] 52 78.8 [67.0, 87.9]
Table 1: Accuracy of randomized significance tests for Spanish-to-English MT with four automatic
metrics, based on the WMT 2012 participant systems.
Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand.
? Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%)
0.05
BLEU 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6]
NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7]
TER 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
0.01
BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]
NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 31 56.4 [42.3, 69.7] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0]
0.001
BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]
NIST 33 60.0 [45.9, 73.0] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 31 56.4 [42.3, 69.7]
Table 2: Accuracy of randomized significance tests for English-to-Spanish MT with four automatic
metrics, based on the WMT 2012 participant systems.
all three randomized methods produce p-values
so similar that when ? thresholds are applied, all
three tests produce precisely the same set of pair-
wise conclusions for each metric. When tests are
combined with METEOR and TER, similar results
are observed: at the ? thresholds of 0.05 and 0.01,
precisely the same conclusions are drawn for both
metrics combined with each of the three tests, and
at most a difference of two conclusions at the low-
est ? level.
Table 2 shows the accuracy of each test on the
English-to-Spanish data, showing much the same
set of conclusions at all ? levels. For BLEU and
NIST, all three tests again produce precisely the
same conclusions, at p < 0.01 there is at most a
single different conclusion for METEOR, and only
at the lowest p-value level is there a single differ-
ence for TER.
271
M
E
T
E
O
R
T
E
R
N
I
S
T
B
L
E
U
Paired Bootstrap Bootstrap Approximate
Resampling Resampling Randomization
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E
Syst
em.F
Syst
em.G
Syst
em.H
Syst
em.J
Syst
em.K
Syst
em.L
Syst
em.M
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E
Syst
em.F
Syst
em.G
Syst
em.H
Syst
em.J
Syst
em.K
Syst
em.L
Syst
em.M
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E
Syst
em.F
Syst
em.G
Syst
em.H
Syst
em.J
Syst
em.K
Syst
em.L
Syst
em.M
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
Figure 8: Automatic metric pairwise randomized significance test results for Spanish-to-English systems
(colored cells denote scores for System row significantly greater than System column).
Finally, we examine which combination of met-
ric and test is most accurate for each language
pair at the conventional significance level of p <
0.05. For Spanish-to-English evaluation, NIST
combined with any of the three randomized tests
is most accurate, making 54 out of 66 (82%) cor-
rect conclusions. For English-to-Spanish, BLEU
in combination with any of the three randomized
tests, is most accurate at 62%. For both language
pairs, however, differences in accuracy for metrics
272
are not significant (Chi-square test).
For English-to-Spanish evaluation, an accuracy
as low as 62% should be a concern. This level
of accuracy for significance testing ? only making
the correct conclusion in 6 out of 10 tests ? acts
as a reminder that no matter how sophisticated the
significance test, it will never make up for flaws in
an underlying metric. When we take into account
the fact that lower confidence limits all fall below
50%, significance tests based on these metrics for
English-to-Spanish are effectively no better than a
random coin toss.
5 Conclusions
We provided a comparison of bootstrap resam-
pling and approximate randomization significance
tests for a range of automatic machine trans-
lation evaluation metrics. To provide a gold-
standard against which to evaluate randomized
tests, we carried out a large-scale human evalua-
tion of all shared task participating systems for the
Spanish-to-English and English-to-Spanish trans-
lation tasks from WMT 2012. Results showed for
many metrics and significance levels that all three
tests produce precisely the same set of conclu-
sions, and when conclusions do differ, it is com-
monly only by a single contrasting conclusion,
which is not significant. For English-to-Spanish
MT, the results of the different MT evaluation met-
ric/significance test combinations are not signifi-
cantly higher than a random baseline.
Acknowledgements
We wish to thank the anonymous reviewers for their valuable
comments. This research was supported by funding from the
Australian Research Council.
References
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for mt evaluation with improved cor-
relation with human judgements. In Proc. Wkshp.
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65?
73, Ann Arbor, MI. ACL.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada. ACL.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In Proc. of the 49th Annual Meeting of the As-
soc. Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages 176?
181, Portland, OR. ACL.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York City,
NY.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing, pages 848?856, Ed-
inburgh, Scotland. ACL.
U. Germann. 2003. Greedy decoding for statisti-
cal machine translation in almost linear time. In
Proc. of the 2003 Conference of the North American
Chapter of the Assoc. Computational Linguistics on
Human Language Technology-Volume 1, pages 1?8,
Edmonton, Canada. ACL.
Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013.
Continuous measurement scales in human evalua-
tion of machine translation. In Proc. 7th Linguis-
tic Annotation Wkshp. & Interoperability with Dis-
course, pages 33?41, Sofia, Bulgaria. ACL.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Assoc. Computational Lin-
guistics, pages 867?875, Los Angeles, CA. ACL.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between European
languages. In Proceedings of the Workshop on Sta-
tistical Machine Translation, pages 102?121, New
York City, NY. ACL.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of Empiri-
cal Methods in Natural Language Processing, pages
388?395, Barcelona, Spain. ACL.
S. Kumar and W. J. Byrne. 2004. Minimum Bayes-
risk decoding for statistical machine translation. In
HLT-NAACL, pages 169?176, Boston, MA. ACL.
C. Monz. 2011. Statistical machine translation with lo-
cal language models. In Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 869?879, Edniburgh, Scotland. ACL.
NIST. 2002. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-Occurrence Statis-
tics. Technical report.
E. W. Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley, New York City, NY.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. 41st Ann. Meet-
ing of the Assoc. Computational Linguistics, pages
160?167, Sapporo, Japan. ACL.
273
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002. A method for automatic evaluation of ma-
chine translation. In Proc. 40th Ann. Meeting of the
Assoc. Computational Linguistics, pages 311?318,
Philadelphia, PA. ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
mt. In Proc. of the ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization, pages 57?64, Ann Ar-
bor, MI. ACL.
S. Riezler and J. T. Maxwell. 2006. Grammatical
machine translation. In Proc. of the Main Confer-
ence on Human Language Technology Conference of
the North American Chapter of the Assoc. Computa-
tional Linguistics, pages 248?255, New York City,
NY. ACL.
M. Smucker, J. Allan, and B. Carterette. 2007. A com-
parison of statistical significance tests for informa-
tion retrieval evaluation. In Proc. of the Sixteenth
ACM Conference on Information and Knowledge
Management (CIKM 2007), pages 623?632, Lisbon,
Portugal. ACM.
M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and
L. Micciula. 2006. A study of translation error rate
with targeted human annotation. In Proc. 7th Bien-
nial Conf. of the Assoc. Machine Translaiton in the
Americas, pages 223?231, Boston, MA. ACL.
274
