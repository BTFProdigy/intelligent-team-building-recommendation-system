Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1021?1030,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
The Linguistic Structure of English Web-Search Queries
Cory Barr and Rosie Jones
Yahoo! Inc.
701 First Ave
Sunnyvale, CA 94089
barrc,jonesr@yahoo-inc.com
Moira Regelson
Perfect Market, Inc.
Pasadena, CA 91103
mregelson@perfectmarket.com
Abstract
Web-search queries are known to be short,
but little else is known about their structure.
In this paper we investigate the applicability
of part-of-speech tagging to typical English-
language web search-engine queries and the
potential value of these tags for improving
search results. We begin by identifying a
set of part-of-speech tags suitable for search
queries and quantifying their occurrence. We
find that proper-nouns constitute 40% of query
terms, and proper nouns and nouns together
constitute over 70% of query terms. We also
show that the majority of queries are noun-
phrases, not unstructured collections of terms.
We then use a set of queries manually la-
beled with these tags to train a Brill tag-
ger and evaluate its performance. In addi-
tion, we investigate classification of search
queries into grammatical classes based on the
syntax of part-of-speech tag sequences. We
also conduct preliminary investigative experi-
ments into the practical applicability of lever-
aging query-trained part-of-speech taggers for
information-retrieval tasks. In particular, we
show that part-of-speech information can be a
significant feature in machine-learned search-
result relevance. These experiments also in-
clude the potential use of the tagger in se-
lecting words for omission or substitution in
query reformulation, actions which can im-
prove recall. We conclude that training a part-
of-speech tagger on labeled corpora of queries
significantly outperforms taggers based on tra-
ditional corpora, and leveraging the unique
linguistic structure of web-search queries can
improve search experience.
1 Introduction
Web-search queries are widely acknowledged to be
short (2.8 words (Spink et al, 2002)) and to be fre-
quently reformulated, but little else is understood
about their grammatical structure. Since search
queries are a fundamental part of the information
retrieval task, it is essential that we interpret them
correctly. However, the variable forms queries take
complicate interpretation significantly. We hypoth-
esize that elucidating the grammatical structure of
search queries would be highly beneficial for the as-
sociated information retrieval task.
Previous work with queries (Allan and Raghavan,
2002) considered that short queries may be ambigu-
ous in their part of speech and that different docu-
ments are relevant depending on how this ambigu-
ity is resolved. For example, the word ?boat? in a
query may be intended as subject of a verb, object
of a verb, or as a verb, with each case reflecting
a distinct intent. To distinguish between the possi-
bilities, Allan and Raghavan (Allan and Raghavan,
2002) propose eliciting feedback from the user by
showing them possible contexts for the query terms.
In addition to disambiguating query terms for re-
trieval of suitable documents, part-of-speech tag-
ging can help increase recall by facilitating query
reformulation. Zukerman and Raskutti (Zukerman
and Raskutti, 2002) part-of-speech tag well-formed
questions, and use the part-of-speech tags to substi-
tute synonyms for the content words.
Several authors have leveraged part-of-speech
tagging towards improved index construction
for information retrieval through part-of-speech-
1021
based weighting schemas and stopword detection
(Crestani et al, 1998), (Chowdhury and McCabe,
2000), (Dincer and Karaoglan, 2004). Their exper-
iments show degrees of success. Recently, along
with term weighting, Lioma has been using part-
of-speech n-grams for noise and content detection
in indexes (Lioma, 2008). Our study differs from
these in that linguistic and part-of-speech focus is
almost exclusively placed on queries as opposed
to the indexed documents, reflecting our opinion
that queries exhibit their own partially predictable
and unique linguistic structure different from that
of the natural language of indexed documents.
Similarly, (Strzalkowski et al, 1998) added a layer
of natural language processing using part-of-speech
tags and syntactical parsing to the common statis-
tical information-retrieval framework, much like
experiments detailed in sections 4 and 5. Our
system differs in that our syntactic parsing system
was applied to web-search queries and uses rules
derived from the observed linguistic structure of
queries as opposed to natural-language corpora.
By focusing on the part-of-speech distribution and
syntactic structure of queries over tagged indexed
documents, with a simple bijection mapping our
query tags to other tag sets, our system offers a
complementary approach that can be used in tandem
with the techniques referenced above.
Lima and Pederson (de Lima and Pederson, 1999)
conducted related work in which part-of-speech
tagging using morphological analysis was used as
a preprocessing step for labeling tokens of web-
search queries before being parse by a probabilis-
tic context-free grammar tuned to query syntax. We
believe this technique and others relying on part-of-
speech tagging of queries could benefit from using a
query-trained tagger prior to deeper linguistic anal-
ysis.
Pasca (Pasca, 2007) showed that queries can be
used as a linguistic resource for discovering named
entities. In this paper we show that the majority
of query terms are proper nouns, and the majority
of queries are noun-phrases, which may explain the
success of this data source for named-entity discov-
ery.
In this work, we use metrics that assume a unique
correct part-of-speech tagging for each query, im-
plicitly addressing the disambiguation issue through
inter-annotator-agreement scores and tagger gener-
alization error. To identify these tags, we first ana-
lyze the different general forms of queries. In Sec-
tion 2 we determine a suitable set of part-of-speech
labels for use with search queries. We then use man-
ually labeled query data to train a tagger and eval-
uate its performance relative to one trained on the
Brown corpus in Section 3. We make observations
about the syntactic structure of web-search queries
in Section 4, showing that the majority (70%) of
queries are noun-phrases, in contrast with the com-
monly held belief that queries consist of unstruc-
tured collections of terms. Finally, we examine the
potential use of tagging in the tasks of search rele-
vance evaluation and query reformulation in Section
5.
2 Data
We sampled queries from the Yahoo! search en-
gine recorded in August 2006. Queries were sys-
tematically lower-cased and white-spaced normal-
ized. We removed any query containing a non-
ASCII character. Queries were then passed through
a high-precision proprietary query spelling correc-
tor, followed by the Penn Treebank tokenizer. No
other normalization was carried out. Despite Penn-
tokenization, queries were typical in their average
length (Jansen et al, 2000). We sampled 3,283
queries from our dataset to label, for a total of 2,508
unique queries comprised of 8,423 individual to-
kens.
2.1 Inter-rater Agreement
The sparse textual information in search queries
presents difficulties beyond standard corpora, not
only for part-of-speech tagging software but also for
human labelers. To quantify the level of these diffi-
culties we measured inter-rater agreement on a set
of 100 queries labeled by each editor. Since one
labeler annotated 84.4% of the queries, we used a
non-standard metric to determine agreement. One
hundred queries were selected at random from each
of our secondary labelers. Our primary labeler then
re-labeled these queries. Accuracy was then calcu-
lated as a weighted average, specifically the mean of
the agreement between our primary labeler and sec-
ondary labelers, weighted by the number of queries
1022
contributed by each secondary labeler. Measuring
agreement with respect to the individual part-of-
speech tag for each token, our corpus has an inter-
rater agreement of 79.3%. If we require agreement
between all tokens in a query, agreement falls to
65.4%. Using Cohen?s kappa coefficient, we have
that token-level agreement is a somewhat low 0.714
and query-level agreement is an even lower 0.641.
We attempted to accurately quantify token-level
ambiguity in queries by examining queries where
chosen labels differ. An author-labeler examined
conflicting labels and made a decision whether the
difference was due to error or genuine ambigu-
ity. Error can be a result of accidentally select-
ing the wrong label, linguistic misunderstanding
(e.g., ?chatting? labeled as a verb or gerund), or
lack of consensus between editors (e.g., model num-
bers could be nouns, proper nouns, or even num-
bers). Examples of genuinely ambiguous queries in-
clude ?download? and ?rent,? both of which could
be a noun or verb. Another major source of gen-
uine token-level ambiguity comes from strings of
proper nouns. For example, some editors consid-
ered ?stillwater chamber of commerce? one entity
and hence four proper-noun tokens while others con-
sidered only the first token a proper noun. Of the 99
conflicting token labels in our queries used to mea-
sure inter-annotator agreement, 69 were judged due
to genuine ambiguity. This left us with a metric in-
dicating query ambiguity accounts for 69.7% of la-
beling error.
2.2 Tags for Part-of-Speech Tagging Queries
In preliminary labeling experiments we found many
standard part-of-speech tags to be extremely rare in
web-search queries. Adding them to the set of possi-
ble tags made labeling more difficult without adding
any necessary resolution. In Table 1 we give the
set of tags we used for labeling. In general, part-
of-speech tags are defined according to the distribu-
tional behavior of the corresponding parts of speech.
Our tag set differs dramatically from the Brown or
Penn tag sets. Perhaps most noticeably, the sizes of
the tag sets are radically different. The Brown tag set
contains roughly 90 tags. In addition, several tags
can be appended with additional symbols to indicate
negation, genitives, etc. Our tag set contains just 19
unique classes.
Tag Example Count (%)
proper-noun texas 3384 (40.2%)
noun pictures 2601 (30.9%)
adjective big 599 (7.1%)
URI ebay.com 495 (5.9%)
preposition in 310 (3.7%)
unknown y 208 (2.5%)
verb get 198 (2.4%)
other conference06-07 174 (2.1%)
comma , 72 (0.9%)
gerund running 69 (0.8%)
number 473 67 (0.8%)
conjunction and 65 (0.8%)
determiner the 56 (0.7%)
pronoun she 53 (0.6%)
adverb quickly 28 (0.3%)
possessive ?s 19 (0.2%)
symbol ( 18 (0.2%)
sentence-ender ? 5 (0.1%)
not n?t 2 (0.0%)
Table 1: Tags used for labeling part-of-speech in web-
search queries.
Our contrasting tag sets reflect an extremely dif-
ferent use of the English language and correspond-
ing part-of-speech distribution. For example, the
Brown tag set contains unique tags for 35 types of
verbs. We use a single label to indicate all cases of
verbs. However, the corpora the Brown tag set was
designed for consists primarily of complete, natural-
language sentences. Essentially, every sentence con-
tains at least one verb. In contrast, a verb of any type
accounts for only 2.35% of our tags. Similarly, the
Brown corpus contains labels for 15 types of deter-
miners. This class makes up just 0.66% of our data.
Our most common tag is the proper noun, which
constitutes 40% of all query terms, and proper nouns
and nouns together constitute 71% of query terms.
In the Brown corpus, by contrast, the most common
tag, noun, constitutes about 13% of terms. Thus the
distribution of tag types in queries is quite different
from typical edited and published texts, and in par-
ticular, proper nouns are more common than regular
nouns.
2.3 Capitalization in Query Data
Although we have chosen to work with lowercase
data, web search queries sometimes contain capi-
1023
Use of Capitals Count % Example
Proper-nouns capitalized 48 47% list of Filipino riddles
Query-Initial-Caps 10 10% Nautical map
Init-Caps + Proper-Nouns 7 7% Condos in Yonkers
Acronym 4 4% location by IP address
Total standard capitalization 69 67%
All-caps 26 25% FAX NUMBER FOR
ALLEN CANNING CO
Each word capitalized 6 6% Direct Selling
Mixed 2 2% SONGS OF MEDONA
music feature:audio
Total non-standard capitalization 34 33%
Table 2: Ways capitalization is used in web-search
queries.
talization information. Since capitalization is fre-
quently used in other corpora to identify proper
nouns, we reviewed its use in web-search queries.
We found that the use of capitalization is inconsis-
tent. On a sample of 290,122 queries from Au-
gust 2006 only 16.8% contained some capitaliza-
tion, with 3.9% of these all-caps. To review the use
of capitalization, we hand-labeled 103 queries con-
taining capital letters (Table 2).
Neither all-lowercase (83.2%) nor all-caps (3.9%)
queries can provide us with any part-of-speech
clues. But we would like to understand the use of
capitalization in queries with varied case. In par-
ticular, how frequently does first-letter capitalization
indicate a proper noun? We manually part-of-speech
tagged 75 mixed-case queries, which contained 289
tokens, 148 of which were proper nouns. The base-
line fraction of proper nouns in this sample is thus
51% (higher than the overall background of 40.2%).
A total of 176 tokens were capitalized, 125 of them
proper nouns. Proper nouns thus made up 73.3%
of capitalized tokens, which is larger than the back-
ground occurrence of proper nouns. We can con-
clude from this that capitalization in a mixed-case
query is a fair indicator that a word is a proper noun.
However, the great majority of queries contain no
informative capitalization, so the great majority of
proper nouns in search queries must be uncapital-
ized. We cannot, therefore, rely on capitalization to
identify proper nouns.
With this knowledge of the infrequent use of capi-
tal letters in search queries in mind, we will examine
the effects of ignoring or using a query?s capitaliza-
tion for part-of-speech tagging in Section 3.4.2.
3 Tagger Accuracy on Search Queries
To investigate automation of the tagging process,
we trained taggers on our manually labeled query
set. We used 10-fold cross-validation, with 90% of
the data used for training and the remaining data
used for testing. In the sections below, we used two
datasets. The first consists of 1602 manually labeled
queries. For the experiments in Section 3.5 we la-
beled additional queries, for a total of 2503 manu-
ally labeled queries.
3.1 Part-of-Speech Tagging Software
We experimented with two freely available part-
of-speech taggers: The Brill Tagger (Brill, 1995)
and The Stanford Tagger (Toutanova and Manning,
2000; Toutanova et al, 2003).
The Brill tagger works in two stages. The initial
tagger queries a lexicon and labels each token with
its most common part-of-speech tag. If the token
is not in the lexicon, it labels the token with a de-
fault tag, which was ?proper noun? in our case. In
the second stage, the tagger applies a set of lexical
rules which examine prefixes, suffixes, and infixes.
The tagger may then exchange the default tag based
on lexical characteristics common to particular parts
of speech. After application of lexical rules, a set
of contextual rules analyze surrounding tokens and
their parts of speech, altering tags accordingly.
We chose to experiment primarily with the Brill
tagger because of its popularity, the human-readable
rules it generates, and its easily modifiable code
base. In addition, the clearly defined stages and in-
corporation of the lexicon provide an accessible way
to supply external lexicons or entity-detection rou-
tines, which could compensate for the sparse con-
textual information of search queries.
We also experimented with the Stanford Log-
Linear Part-of-Speech Tagger, which presently
holds the best published performance in the field at
96.86% on the Penn Treebank corpus. It achieves
this accuracy by expanding information sources for
tagging. In particular, it provides ?(i) more exten-
sive treatment of capitalization for unknown words;
(ii) features for the disambiguation of the tense
forms of verbs; (iii) features for disambiguating par-
ticles from prepositions and adverbs.? It uses a
maximum-entropy approach to handle information
1024
diversity without assuming predictor independence
(Toutanova and Manning, 2000).
3.2 Baseline: Most Common Tag
With proper nouns dominating the distribution, we
first considered using the accuracy of labeling all to-
kens ?proper noun? as a baseline. In this case, we
labeled 1953 of 4759 (41.0%) tokens correctly. This
is a significant improvement over the accuracy of
tagging all words as ?noun? on the Brown corpus
(approximately 13%), reflecting the frequent occur-
rence of proper nouns in search queries. However, to
examine the grammatical structure of search queries
we must demonstrate that they are not simply col-
lections of words. With this in mind, we chose in-
stead to use the most common part-of-speech tag
for a word as a baseline. We evaluated the baseline
performance on our manually labeled dataset, with
URLs removed. Each token in the set was assigned
its most common part of speech, according to the
Brill lexicon. In this case, 4845 of 7406 tokens were
tagged correctly (65.42%).
3.3 Effect of Type of Training Data
The Brill tagger software is pre-trained on the stan-
dard Wall Street Journal corpus, so the simplest pos-
sible approach is to apply it directly to the query data
set. We evaluated this ?out-of-the-box? performance
on our 1602 manually labeled queries, after mapping
tags to our reduced tag set. (Our effective training-
set size is 1440 queries, since 10% were held out
to measure accuracy through cross validation.) The
WSJ-trained tagger labeled 2293 of 4759 (48.2%)
tags correctly, a number well below the baseline
performance, demonstrating that application of the
contextual rules that Brill learns from the syntax of
natural-language corpora has a negative effect on ac-
curacy in the context of queries. When we re-trained
Brill?s tagger on a manually labeled set of queries,
we saw accuracy increase to 69.7%. The data used
to train the tagger therefore has a significant effect
on its accuracy (Table 3). The accuracy of the tag-
ger trained on query data is above the baseline, in-
dicating that search queries are somewhat more than
collections of words.
3.4 Improving Tagger Accuracy
We conducted several experiments in improving tag-
ger accuracy, summarized in Table 3 and described
in detail below.
3.4.1 Adding External Lexicon
With a training-set size of 1500 queries, compris-
ing a lexicon of roughly 4500 words, it is natural to
question if expanding the lexicon by incorporating
external sources boosts performance. To this end,
we lower-cased the lexicon of 93,696 words pro-
vided by the Brill tagger, mapped the tags to our
own tag set, and merged our lexicon from queries.
This experiment resulted in an accuracy of 71.1%, a
1.4% increase.
One explanation for the limited increase is that
this lexicon is derived from the Brown corpus and
the Penn Treebank tagging of the Wall Street Jour-
nal. These corpora are based on works published
in 1961 and 1989-1992 respectively. As shown in
Table 1, proper nouns dominate the distribution of
search-engine queries. Many of these queries will
involve recent products, celebrities, and other time-
sensitive proper nouns. We speculate that Web-
based information resources could be leveraged to
expand the lexicon of timely proper nouns, thereby
enhancing performance.
3.4.2 Experiments with Perfect Capitalization
The overall performance of the pre-trained Brill
tagger on our query set may be due to its poor per-
formance on proper nouns, our most frequent part
of speech. In the WSJ newspaper training data,
proper-nouns always start with a capital letter. As
discussed in Section 2.3, capitalization is rare in
web-search queries. To examine the effect of the
missing capitalization of proper nouns, we evaluated
a pre-trained Brill tagger on our previously men-
tioned manually labeled corpus of 1602 queries al-
tered such that only the proper nouns were capital-
ized. In this case, the tagger reached an extraordi-
nary 89.4% accuracy (Table 3). Unfortunately, the
vast majority of queries do not contain capitalization
information and those that do often contain mislead-
ing information. The pre-trained tagger achieved
only a 45.6% accuracy on non-lowercased queries,
performing even worse than on the set with no capi-
talization at all.
1025
Experiment Accuracy
Label-all-proper-noun 41.0%
WSJ-trained 48.2%
most-freq-tag-WSJ 64.4%
re-trained 69.7%
retrained + WSJ lexicon 71.1%
user capitalization 45.6%
oracle capitalization 89.4%
automatic capitalization 70.9%
Table 3: Tagging experiments on small labeled corpus.
Experiments were conducted on lower-cased queries ex-
cept where specifically indicated.
3.4.3 Automatic Capitalization
We saw in Section 2.3 that web searchers rarely
use capitalization. We have also seen that a pre-
trained Brill tagger run on queries with perfect
capitalization (?oracle? capitalization) can achieve
89.4% accuracy. We now look at how performance
might be affected if we used an imperfect algorithm
for capitalization.
In order to attempt to capitalize the proper nouns
in queries, we used a machine-learned system which
searches for the query terms and examines how of-
ten they are capitalized in the search results, weight-
ing each capitalization occurrence by various fea-
tures (Bartz et al, 2008). Though the capitalization
system provides 79.3% accuracy, using this system
we see an only a small increase of accuracy in part-
of-speech tagging at 70.9%. This system does not
improve significantly over the tagger trained on the
lower-cased corpus. One explanation is that cap-
italization information of this type could only be
obtained for 81.9% of our queries. Multiplied by
accuracy, this implies that roughly 81.9% * 79.3%
= 65.0% of our proper nouns are correctly cased.
This suggests that any technique for proper-noun de-
tection in search-engine queries must provide over
65.0% accuracy to see any performance increase.
Finally we looked at the capitalization as input by
searchers. We trained on the oracle-capitalized cor-
pus, and tested on raw queries without normaliza-
tion. We saw an accuracy of just 45.6%. Thus using
the capitalization input by web searchers is mislead-
ing and actually hurts performance.
Figure 1: Brill?s tagger trained on web-search queries.
We see that the most significant gains in performance are
with the first few hundred labeled examples, but even af-
ter 2500 examples are labeled, more labeled data contin-
ues to improve performance.
3.5 Learning Curve
It is important to understand whether tagger accu-
racy is limited by the small size of our manually la-
beled dataset. To examine the effect of dataset size,
we trained Brill?s tagger with increasing numbers of
labeled queries and evaluated accuracy with each set
size. In the interim between conducting the experi-
ments of sections 3.1 through 3.3 and those of sec-
tion 3.5, we were able to obtain 1120 new labeled
queries, allowing us to extend the learning curve.
With our complete corpus of 2722 labeled exam-
ples (for a cross-validated training-set size of 2450
labeled examples, URLs omitted), we see an accu-
racy of 78.6% on a per-token basis. We see the most
significant gains in performance with the first few
hundred labeled examples, but even after 2500 ex-
amples are labeled, more labeled data continues to
improve performance.
3.6 Comparing Taggers to Suggest
Methods for Boosting Performance
In Table 4 we see a comparison of Brill?s tagger to
the Stanford tagger trained on 2450 labeled queries.
The 0.3% performance increase is not statistically
significant. As listed in Section 3, the features
the Stanford tagger adds to achieve high accuracy
in traditional natural-language corpora are not in-
1026
Tagger Accuracy
Brill 78.6%
Stanford 78.9%
Table 4: Comparison of Brill?s tagger to the Stanford tag-
ger, on our corpus of manually annotated query logs.
formative in the domain of search-engine queries.
We believe greater performance on our data will be
achieved primarily through examination of common
sources of inter-rater disagreement (such as consis-
tent handling of ambiguity) and incorporation of ex-
ternal sources to detect proper nouns not in the lexi-
con.
To validate our intuition that expanding the lexi-
con will boost performance, we obtained a propri-
etary list of 7385 known trademarked terms used
in the sponsored-search industry. Treating these
phrases as proper nouns and adding them to the lex-
icon from the Wall Street Journal supplied with the
Brill tagger, we see our cross validated accuracy im-
prove to 80.2% (with a standard deviation of 1.85%),
the highest score achieved in our experiments. We
find it likely that incorporation of more external lex-
ical sources will result in increased performance.
Our experiments also support our hypothesis that
addressing inter-annotator agreement will boost per-
formance. We can see this by examining the results
of the experiments in section 3.3 verses section 3.5.
In section 3.3, we see the accuracy on the query-
trained Brill tagger is 69.7%. As mentioned, for
the experiment in section 3.5, we were able to ob-
tain 1120 new queries. Each of these newly labeled
queries came from the same labeler, who believes
their handling of the ambiguities inherent in search
queries became more consistent over time. With
the same training-set size of 1440 used in section
3.3, Figure 1 shows performance at 1440 queries is
roughly 6% higher. We believe this significant im-
provement is a result of more consistent handling of
query ambiguity obtained through labeling experi-
ence.
4 Query Grammar
The above-baseline performance of the Brill tagger
trained on web-search queries suggests that web-
search queries exhibit some degree of syntactical
structure. With a corpus of queries labeled with part-
of-speech information, we are in a position to ana-
lyze this structure and characterize the typical pat-
terns of part-of-speech used by web searchers. To
this end, we randomly sampled and manually la-
beled a set of 222 queries from the part-of-speech
dataset used for tagger training mentioned above.
Each query was labeled with a single meta-tag in-
dicating query type. Two author-judges simultane-
ously labeled queries and created the set of meta-
tags during much discussion, debate, and linguistic
research. A list of our meta-tags and the distribu-
tion of each are provided in Table 5. We can see
that queries consisting of a noun-phrase dominate
the distribution of query types, in contrast with the
popularly held belief that queries consist of unstruc-
tured collections of terms.
To determine how accurately a meta-tag can be
determined based on part-of-speech labels, we cre-
ated a grammar consisting of a set of rules to rewrite
part-of-speech tags into higher-level grammatical
structures. These higher-level grammatical struc-
tures are then rewritten into one of the seven classes
of meta-tags seen in Table 5. Our grammar was con-
structed by testing the output of our rewrite rules on
queries labeled with par-of-speech tags that were not
part of the 222 queries sampled for meta-tag label-
ing. Grammar rules were revised until the failure
rate on previously untested part-of-speech-labeled
queries stabilized. Failure was evaluated by two
means. In the first case, the grammar rules failed
to parse the sequence of part-of-speech tags. In the
second case, the grammar rules led to an inappro-
priate classification for a query type. As during the
labeling phase, the two author-labelers simultane-
ously reached a consensus on whether a parse failed
or succeeded, rendering an inter-annotator score in-
applicable. The resulting grammar was then tested
on the 222 queries with query-type meta-tags.
Our rules function much like production rules in
context-free grammars. As an example, the two-
tag sequence ?determiner noun? will be rewritten
as ?noun phrase.? This in turn could be re-written
into a larger structure, which will then be rewritten
into a meta-tag of query type. The primary differ-
ence between a context-free grammar or probabilis-
tic context-free grammar (such as that employed by
Lima and Pederson (de Lima and Pederson, 1999))
1027
Query Gramm. Type Example Freq (%)
noun-phrase free mp3s 155 (69.8%)
URI http:answers.yahoo.com/ 24 (10.8%)
word salad mp3s free 19 (8.1%)
other-query florida elementary reading 15 (6.8%)
conference2006-2007
unknown nama-nama calon praja ipdn 6 (2.7%)
verb-phrase download free mp3s 3 (1.4%)
question where can I download free mp3s 1 (0.45%)
Table 5: Typical grammatical forms of queries used by
web searchers, with distribution based on a sample of 222
hand-labeled queries.
and our grammar is that our rules are applied itera-
tively as opposed to recursively. As such, our gram-
mar yields a single parse for each input.
Some of our rules reflect the telegraphic nature of
web queries. For example, it is much more com-
mon to see an abbreviated noun-phrase consisting of
adjective-noun, than one consisting of determiner-
adjective-noun.
Examining the Table 5, we see that just label-
ing a query ?noun-phrase? results in an accuracy of
69.8%. Our grammar boosted this high baseline by
14% to yield an final accuracy result of 83.3% at la-
beling queries with their correct meta-type. These
meta-types could be useful in deciding how to han-
dle a query. Further enhancements to the grammar
would likely yield a performance increase. How-
ever, we feel accuracy is currently high enough to
continue with experiments towards application of
leveraging grammar-deduced query types for infor-
mation retrieval.
We can think of some of these meta-types as
elided sentences. For example, the noun-phrase
queries could be interpreted as requests of the form
?how can I obtain X? or ?where can I get informa-
tion on X?, while the verb-phrase queries are re-
quests of the form ?I would like to DO-X?.
5 Applications of Part-of-Speech Tagging
Since search queries are part of an information re-
trieval task, we would like to demonstrate that part-
of-speech tagging can assist with that task. We con-
ducted two experiments with a large-scale machine-
learned web-search ranking system. In addition, we
considered the applicability of part-of-speech tags to
the question of query reformulation.
5.1 Web Search Ranking
We worked with a proprietary experimental testbed
in which features for predicting the relevance of a
query to a document can be tested in a machine-
learning framework. Features can take a wide va-
riety of forms (boolean, real-valued, relational) and
apply to a variety of scopes (the page, the query,
or the combination). These features are evaluated
against editorial judgements and ranked according
to their significance in improving the relevance of
results. We evaluated two part-of-speech tag-based
features in this testbed.
The first experiment involved a simple query-level
feature indicating whether the query contained a
noun or a proper noun. This feature was evaluated
on thousands of queries for the test. At the conclu-
sion of the test, this feature was found to be in the
top 13% of model features, ranked in order of signif-
icance. We believe this significance represents the
importance of recognizing the presence of a noun
in a query and, of course, matching it. Within this
experimental testbed a statistically significant im-
provement of information-retrieval effectiveness is
notoriously difficult to attain. We did not see a sig-
nificant improvement in this metric. However, we
feel that our feature?s high ranking warrants report-
ing and hints at a potentially genuine boost in re-
trieval performance in a system less feature-rich.
The second experiment was more involved and re-
flected more of our intuition about the likely applica-
tion of part-of-speech tagging to the improvement of
search results. In this experiment, we part-of-speech
tagged both queries and documents. Documents
were tagged with a conventionally trained Brill tag-
ger with the resulting Penn-style tags mapped to our
tag set. Many thousands of query-document pairs
were processed in this manner. The feature was
based on the percent of times the part-of-speech tag
of a word in the query matched the part-of-speech
tag of the same word in the document. This feature
was ranked in the top 12% by significance, though
we again saw no statistically significant increase in
overall retrieval performance.
5.2 Query Reformulation
We considered the application of part-of-speech tag-
ging to the problem of query reformulation, in which
1028
Part-of-speech p(subst) subst / seen
Number 0.49 148 / 302
Adjective 0.46 2877 / 6299
Noun 0.42 15038 / 35515
Proper noun 0.39 21478 / 55331
Gerund 0.37 112 / 300
Verb 0.31 1769 / 5718
Pronoun 0.23 300 / 1319
Conjunction 0.18 85 / 464
Adverb 0.13 105 / 790
Determiner 0.10 22 / 219
Preposition 0.08 369 / 4574
Possessive 0.08 25 / 330
Not 0.03 1 / 32
Symbol 0.02 16 / 879
Other 0.02 78 / 3294
Sentence-ender 0.01 3 / 234
Comma 0.00 4 / 991
Table 6: Probability of a word being reformulated from
one query to the next, by part-of-speech tag. While
proper-nouns are the most frequent tag in our corpus, ad-
jectives are more frequently reformulated, reflecting the
fact that the proper nouns carry the core meaning of the
query.
a single word in the query is altered within the
same user session. We used a set of automatically
tagged queries to calculate change probabilities of
each word by part-of-speech tag and the results are
shown in Table 6.
The type of word most likely to be reformu-
lated is ?number.? Examples included changing a
year (?most popular baby names 2007? ? ?most
popular baby names 2008?), while others included
model, version and edition numbers (?harry potter
6? ? ?harry potter 7?) most likely indicating that
the user is looking at variants on a theme, or cor-
recting their search need. Typically a number is a
modifier of the core search meaning. The next most
commonly changed type was ?adjective,? perhaps
indicating that adjectives can be used to refine, but
not fundamentally alter, the search intent. Nouns
and proper nouns are the next most commonly mod-
ified types, perhaps reflecting user modification of
their search need, refining the types of documents
retrieved. Other parts of speech are relatively sel-
dom modified, perhaps indicating that they are not
viewed as having a large impact on the documents
retrieved.
We can see from the impact of the search engine
ranking features and from the table of query refor-
mulation likelihood that making use of the grammat-
ical structure of search queries can have an impact
on result relevance. It can also assist with tasks as-
sociated with improving recall, such as query refor-
mulation.
6 Conclusion
We have quantified, through a lexicostatistical anal-
ysis, fundamental differences between the natural
language used in standard English-language corpora
and English search-engine queries. These differ-
ences include reduced granularity in part-of-speech
classes as well as the dominance of the noun classes
in queries at the expense of classes such as verbs
frequently found in traditional corpora. In addi-
tion, we have demonstrated the poor performance of
taggers trained on traditional corpora when applied
to search-engine queries, and how this poor perfor-
mance can be overcome through query-based cor-
pora. We have suggested that greater improvement
can be achieved by proper-noun detection through
incorporation of external lexicons or entity detec-
tion. Finally, in preliminary investigations into ap-
plications of our findings, we have shown that query
part-of-speech tagging can be used to create signif-
icant features for improving the relevance of web
search results and may assist with query reformu-
lation. Improvements in accuracy can only increase
the value of POS information for these applications.
We believe that query grammar can be further ex-
ploited to increase query understanding and that this
understanding can improve the overall search expe-
rience.
References
James Allan and Hema Raghavan. 2002. Using part-
of-speech patterns to reduce query ambiguity. In Pro-
ceedings of SIGIR, pages 307?314.
Kevin Bartz, Cory Barr, and Adil Aijaz. 2008. Natu-
ral language generation in sponsored-search advertise-
ments. In Proceedings of the 9th ACM Conference on
Electronic Commerce, pages 1?9, Chicago, Illinois.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
1029
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Abdur Chowdhury and M. Catherine McCabe. 2000.
Improving information retrieval systems using part of
speech tagging.
Fabio Crestani, Mark Sanderson, and Mounia Lalmas.
1998. Short queries, natural language and spoken doc-
ument retrieval: Experiments at glasgow university.
In Proceedings of the Sixth Text Retrieval Conference
(TREC-6), pages 667?686.
Erika F. de Lima and Jan O. Pederson. 1999. Phrase
recognition and expansion for short, precision-biased
queries based on a query log. In Annual ACM Con-
ference on Research and Development in Informa-
tion Retrieval Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 145?152,
Berkeley, California.
Bekir Taner Dincer and Bahar Karaoglan. 2004. The
effect of part-of-speech tagging on ir performance for
turkish. pages 771?778.
Bernard J. Jansen, Amanda Spink, and Tefko Saracevic.
2000. Real life, real users, and real needs: a study
and analysis of user queries on the web. Information
Processing and Management, 36(2):207?227.
Christina Amalia Lioma. 2008. Part of speech N-grams
for information retrieval. Ph.D. thesis, University of
Glasgow, Glasgow, Scotland, UK.
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In CIKM,
pages 683?690.
Amanda Spink, B. J. Jansen, D. Wolfram, and T. Sarace-
vic. 2002. From e-sex to e-commerce: Web search
changes. IEEE Computer, 35(3):107?109.
Tomek Strzalkowski, Jose Perez Carballo, and Mihnea
Marinescu. 1998. Natural language information re-
trieval: Trec-3 report. In Proceedings of the Sixth Text
Retrieval Conference (TREC-6), page 39.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In COL-
ING, pages 1177?1183, Taipei, Taiwan.
1030
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 49?56,
Sydney, July 2006. c?2006 Association for Computational Linguistics
N Semantic Classes are Harder than Two
Ben Carterette?
CIIR
University of Massachusetts
Amherst, MA 01003
carteret@cs.umass.edu
Rosie Jones
Yahoo! Research
3333 Empire Ave.
Burbank, CA 91504
jonesr@yahoo-inc.com
Wiley Greiner?
Los Angeles Software Inc.
1329 Pine Street
Santa Monica, CA 90405
w.greiner@lasoft.com
Cory Barr
Yahoo! Research
3333 Empire Ave.
Burbank, CA 91504
barrc@yahoo-inc.com
Abstract
We show that we can automatically clas-
sify semantically related phrases into 10
classes. Classification robustness is im-
proved by training with multiple sources
of evidence, including within-document
cooccurrence, HTML markup, syntactic
relationships in sentences, substitutability
in query logs, and string similarity. Our
work provides a benchmark for automatic
n-way classification into WordNet?s se-
mantic classes, both on a TREC news cor-
pus and on a corpus of substitutable search
query phrases.
1 Introduction
Identifying semantically related phrases has been
demonstrated to be useful in information retrieval
(Anick, 2003; Terra and Clarke, 2004) and spon-
sored search (Jones et al, 2006). Work on seman-
tic entailment often includes lexical entailment as
a subtask (Dagan et al, 2005).
We draw a distinction between the task of iden-
tifying terms which are topically related and iden-
tifying the specific semantic class. For example,
the terms ?dog?, ?puppy?, ?canine?, ?schnauzer?,
?cat? and ?pet? are highly related terms, which
can be identified using techniques that include
distributional similarity (Lee, 1999) and within-
document cooccurrence measures such as point-
wise mutual information (Turney et al, 2003).
These techniques, however, do not allow us to dis-
tinguish the more specific relationships:
? hypernym(dog,puppy)
?This work was carried out while these authors were at
Yahoo! Research.
? hyponym(dog,canine)
? coordinate(dog,cat)
Lexical resources such as WordNet (Miller,
1995) are extremely useful, but are limited by be-
ing manually constructed. They do not contain se-
mantic class relationships for the many new terms
we encounter in text such as web documents, for
example ?mp3 player? or ?ipod?. We can use
WordNet as training data for such classification to
the extent that the training on pairs found in Word-
Net and testing on pairs found outside WordNet
provides accurate generalization.
We describe a set of features used to train n-
way supervised machine-learned classification of
semantic classes for arbitrary pairs of phrases. Re-
dundancy in the sources of our feature informa-
tion means that we are able to provide coverage
over an extremely large vocabulary of phrases. We
contrast this with techniques that require parsing
of natural language sentences (Snow et al, 2005)
which, while providing reasonable performance,
can only be applied to a restricted vocabulary of
phrases cooccuring in sentences.
Our contributions are:
? Demonstration that binary classification re-
moves the difficult cases of classification into
closely related semantic classes
? Demonstration that dependency parser paths
are inadequate for semantic classification into
7 WordNet classes on TREC news corpora
? A benchmark of 10-class semantic classifica-
tion over highly substitutable query phrases
? Demonstration that training a classifier us-
ing WordNet for labeling does not generalize
well to query pairs
? Demonstration that much of the performance
in classification can be attained using only
49
syntactic features
? A learning curve for classification of query
phrase pairs that suggests the primary bottle-
neck is manually labeled training instances:
we expect our benchmark to be surpassed.
2 Relation to Previous Work
Snow et al (2005) demonstrated binary classi-
fication of hypernyms and non-hypernyms using
WordNet (Miller, 1995) as a source of training la-
bels. Using dependency parse tree paths as fea-
tures, they were able to generalize from WordNet
labelings to human labelings.
Turney et al (2003) combined features to an-
swer multiple-choice synonym questions from the
TOEFL test and verbal analogy questions from
the SAT college entrance exam. The multiple-
choice questions typically do not consist of mul-
tiple closely related terms. A typical example is
given by Turney:
? hidden:: (a) laughable (c) ancient
(b) veiled (d) revealed
Note that only (b) and (d) are at all related to the
term, so the algorithm only needs to distinguish
antonyms from synonyms, not synonyms from say
hypernyms.
We use as input phrase pairs recorded in query
logs that web searchers substitute during search
sessions. We find much more closely related
phrases:
? hidden::
(a) secret (e) hiden
(b) hidden camera (f) voyeur
(c) hidden cam (g) hide
(d) spy
This set contains a context-dependent synonym,
topically related verbs and nouns, and a spelling
correction. All of these could cooccur on web
pages, so simple cooccurrence statistics may not
be sufficient to classify each according to the se-
mantic type.
We show that the techniques used to perform
binary semantic classification do not work as well
when extended to a full n-way semantic classifi-
cation. We show that using a variety of features
performs better than any feature alone.
3 Identifying Candidate Phrases for
Classification
In this section we introduce the two data sources
we use to extract sets of candidate related phrases
for classification: a TREC-WordNet intersection
and query logs.
3.1 Noun-Phrase Pairs Cooccuring in TREC
News Sentences
The first is a data-set derived from TREC news
corpora and WordNet used in previous work for
binary semantic class classification (Snow et al,
2005). We extract two sets of candidate-related
pairs from these corpora, one restricted and one
more complete set.
Snow et al obtained training data from the inter-
section of noun-phrases cooccuring in sentences in
a TREC news corpus and those that can be labeled
unambiguously as hypernyms or non-hypernyms
using WordNet. We use a restricted set since in-
stances selected in the previous work are a subset
of the instances one is likely to encounter in text.
The pairs are generally either related in one type
of relationship, or completely unrelated.
In general we may be able to identify related
phrases (for example with distributional similarity
(Lee, 1999)), but would like to be able to automat-
ically classify the related phrases by the type of
the relationship. For this task we identify a larger
set of candidate-related phrases.
3.2 Query Log Data
To find phrases that are similar or substitutable for
web searchers, we turn to logs of user search ses-
sions. We look at query reformulations: a pair
of successive queries issued by a single user on
a single day. We collapse repeated searches for
the same terms, as well as query pair sequences
repeated by the same user on the same day.
3.2.1 Substitutable Query Segments
Whole queries tend to consist of several con-
cepts together, for example ?new york | maps? or
?britney spears | mp3s?. We identify segments or
phrases using a measure over adjacent terms sim-
ilar to mutual information. Substitutions occur at
the level of segments. For example, a user may
initially search for ?britney spears | mp3s?, then
search for ?britney spears | music?. By aligning
query pairs with a single substituted segment, we
generate pairs of phrases which a user has substi-
tuted. In this example, the phrase ?mp3s? was sub-
stituted by the phrase ?music?.
Aggregating substitutable pairs over millions of
users and millions of search sessions, we can cal-
culate the probability of each such rewrite, then
50
test each pair for statistical significance to elim-
inate phrase rewrites which occurred in a small
number of sessions, perhaps by chance. To test
for statistical significance we use the pair inde-
pendence likelihood ratio, or log-likelihood ratio,
test. This metric tests the hypothesis that the prob-
ability of phrase ? is the same whether phrase ?
has been seen or not by calculating the likelihood
of the observed data under a binomial distribution
using probabilities derived using each hypothesis
(Dunning, 1993).
log? = logL (P (?|?) = P (?|??))L (P (?|?) 6= P (?|??))
A high negative value for ? suggests a strong
dependence between query ? and query ?.
4 Labeling Phrase Pairs for Supervised
Learning
We took a random sample of query segment sub-
stitutions from our query logs to be labeled. The
sampling was limited to pairs that were frequent
substitutions for each other to ensure a high prob-
ability of the segments having some relationship.
4.1 WordNet Labeling
WordNet is a large lexical database of English
words. In addition to defining several hun-
dred thousand words, it defines synonym sets, or
synsets, of words that represent some underly-
ing lexical concept, plus relationships between
synsets. The most frequent relationships between
noun-phrases are synonym, hyponym, hypernym,
and coordinate, defined in Table 1. We also may
use meronym and holonym, defined as the PART-OF
relationship.
We used WordNet to automatically label the
subset of our sample for which both phrases occur
in WordNet. Any sense of the first segment having
a relationship to any sense of the second would re-
sult in the pair being labeled. Since WordNet con-
tains many other relationships in addition to those
listed above, we group the rest into the other cate-
gory. If the segments had no relationship in Word-
Net, they were labeled no relationship.
4.2 Segment Pair Labels
Phrase pairs passing a statistical test are com-
mon reformulations, but can be of many seman-
tic types. Rieh and Xie (2001) categorized types
of query reformulations, defining 10 general cat-
egories: specification, generalization, synonym,
parallel movement, term variations, operator us-
age, error correction, general resource, special re-
source, and site URLs. We redefine these slightly
to apply to query segments. The summary of the
definitions is shown in Table 1, along with the dis-
tribution in the data of pairs passing the statistical
test.
4.2.1 Hand Labeling
More than 90% of phrases in query logs do not
appear in WordNet due to being spelling errors,
web site URLs, proper nouns of a temporal nature,
etc. Six annotators labeled 2, 463 segment pairs
selected randomly from our sample. Annotators
agreed on the label of 78% of pairs, with a Kappa
statistic of .74.
5 Automatic Classification
We wish to perform supervised classification of
pairs of phrases into semantic classes. To do this,
we will assign features to each pair of phrases,
which may be predictive of their semantic rela-
tionship, then use a machine-learned classifier to
assign weights to these features. In Section 7 we
will look at the learned weights and discuss which
features are most significant for identifying which
semantic classes.
5.1 Features
Features for query substitution pairs are extracted
from query logs and web pages.
5.1.1 Web Page / Document Features
We submit the two segments to a web search
engine as a conjunctive query and download the
top 50 results. Each result is converted into an
HTML Document Object Model (DOM) tree and
segmented into sentences.
Dependency Tree Paths The path from the first
segment to the second in a dependency parse
tree generated by MINIPAR (Lin, 1998)
from sentences in which both segments ap-
pear. These were previously used by Snow
et al (2005). These features were extracted
from web pages in all experiments, except
where we identify that we used TREC news
stories (the same data as used by Snow et al).
HTML Paths The paths from DOM tree nodes
the first segment appears in to nodes the sec-
ond segment appears in. The value is the
number of times the path occurs with the pair.
51
Class Description Example %
synonym one phrase can be used in place of the other without loss in meaning low cost; cheap 4.2
hypernym X is a hypernym of Y if and only if Y is a X muscle car; mustang 2.0
hyponym X is a hyponym of Y if and only if X is a Y (inverse of hypernymy) lotus; flowers 2.0
coordinate there is some Z such that X and Y are both Zs aquarius; gemini 13.9
generalization X is a generalization of Y if X contains less information about the topic lyrics; santana lyrics 4.8
specialization X is a specification of Y if X contains more information about the topic credit card; card 4.7
spelling change spelling errors, typos, punctuation changes, spacing changes peopl; people 14.9
stemmed form X and Y have the same lemmas ant; ants 3.4
URL change X and Y are related and X or Y is a URL alliance; alliance.com 29.8
other relationship X and Y are related in some other way flagpoles; flags 9.8
no relationship X and Y are not related in any obvious way crypt; tree 10.4
Table 1: Semantic relationships between phrases rewritten in query reformulation sessions, along with their prevalence in our
data.
Lexico-syntactic Patterns (Hearst, 1992) A sub-
string occurring between the two segments
extracted from text in nodes in which both
segments appear. In the example fragment
?authors such as Shakespeare?, the feature
is ?such as? and the value is the number of
times the substring appears between ?author?
and ?Shakespeare?.
5.1.2 Query Pair Features
Table 2 summarizes features that are induced
from the query strings themselves or calculated
from query log data.
5.2 Additional Training Pairs
We can double our training set by adding for each
pair u1, u2 a new pair u2, u1. The class of the new
pair is the same as the old in all cases but hyper-
nym, hyponym, specification, and generalization,
which are inverted. Features are reversed from
f(u1, u2) to f(u2, u1).
A pair and its inverse have different sets of fea-
tures, so splitting the set randomly into training
and testing sets should not result in resubstitution
error. Nonetheless, we ensure that a pair and its
inverse are not separated for training and testing.
5.3 Classifier
For each class we train a binary one-vs.-all linear-
kernel support vector machine (SVM) using the
optimization algorithm of Keerthi and DeCoste
(2005).
5.3.1 Meta-Classifier
For n-class classification, we calibrate SVM
scores to probabilities using the method described
by Platt (2000). This gives us P (class|pair) for
each pair. The final classification for a pair is
argmaxclassP (class|pair).
Source Snow (NIPS 2005) Experiment
Task binary hypernym binary hypernym
Data WordNet-TREC WordNet-TREC
Instance Count 752,311 752,311
Features minipar paths minipar paths
Feature Count 69,592 69,592
Classifier logistic Regression linear SVM
maxF 0.348 0.453
Table 3: Snow et als (2005) reported performance using lin-
ear regression, and our reproduction of the same experiment,
using a support vector machine (SVM).
5.3.2 Evaluation
Binary classifiers are evaluated by ranking in-
stances by classification score and finding the Max
F1 (the harmonic mean of precision and recall;
ranges from 0 to 1) and area under the ROC curve
(AUC; ranges from 0.5 to 1 with at least 0.8 being
?good?). The meta-classifier is evaluated by pre-
cision and recall of each class and classification
accuracy of all instances.
6 Experiments
6.1 Baseline Comparison to Snow et al?s
Previous Hypernym Classification on
WordNet-TREC data
Snow et al (2005) evaluated binary classifi-
cation of noun-phrase pairs as hypernyms or
non-hypernyms. When training and testing on
WordNet-labeled pairs from TREC sentences,
they report classifier Max F of 0.348, using de-
pendency path features and logistic regression. To
justify our choice of an SVM for classification, we
replicated their work. Snow et al provided us with
their data. With our SVM we achieved a Max F of
0.453, 30% higher than they reported.
6.2 Extending Snow et al?s WordNet-TREC
Binary Classification to N Classes
Snow et al select pairs that are ?Known Hyper-
nyms? (the first sense of the first word is a hy-
52
Feature Description
Levenshtein Distance # character insertions/deletions/substitutions to change query ? to query ? (Levenshtein, 1966).
Word Overlap Percent # words the two queries have in common, divided by num. words in the longer query.
Possible Stem 1 if the two segments stem to the same root using the Porter stemmer.
Substring Containment 1 if the first segment is a substring of the second.
Is URL 1 if either segment matches a handmade URL regexp.
Query Pair Frequency # times the pair was seen in the entire unlabeled corpus of query pairs.
Log Likelihood Ratio The Log Likelihood Ratio described in Section 3.2.1 Formula 3.2.1
Dice and Jaccard Coefficients Measures of the similarity of substitutes for and by the two phrases.
Table 2: Syntactic and statistical features over pairs of phrases.
ponym of the first sense of the second and both
have no more than one tagged sense in the Brown
corpus) and ?Known Non-Hypernyms? (no sense
of the first word is a hyponym of any sense of the
second). We wished to test whether making the
classes less cleanly separable would affect the re-
sults, and also whether we could use these features
for n-way classification.
From the same TREC corpus we extracted
known synonym, known hyponym, known coordi-
nate, known meronym, and known holonym pairs.
Each of these classes is defined analogously to the
known hypernym class; we selected these six rela-
tionships because they are the six most common.
A pair is labeled known no-relationship if no sense
of the first word has any relationship to any sense
of the second word. The class distribution was se-
lected to match as closely as possible that observed
in query logs. We labeled 50,000 pairs total.
Results are shown in Table 4(a). Although AUC
is fairly high for all classes, MaxF is low for all
but two. MaxF has degraded quite a bit for hyper-
nyms from Table 3. Removing all instances except
hypernym and no relationship brings MaxF up to
0.45, suggesting that the additional classes make it
harder to separate hypernyms.
Metaclassifier accuracy is very good, but this is
due to high recall of no relationship and coordi-
nate pairs: more than 80% of instances with some
relationship are predicted to be coordinates, and
most of the rest are predicted no relationship. It
seems that we are only distinguishing between no
vs. some relationship.
The size of the no relationship class may be bi-
asing the results. We removed those instances, but
performance of the n-class classifier did not im-
prove (Table 4(b)). MaxF of binary classifiers did
improve, even though AUC is much worse.
6.3 N-Class Classification of Query Pairs
We now use query pairs rather than TREC pairs.
6.3.1 Classification Using Only Dependency
Paths
We first limit features to dependency paths in
order to compare to the prior results. Dependency
paths cannot be obtained for all query phrase pairs,
since the two phrases must appear in the same sen-
tence together. We used only the pairs for which
we could get path features, about 32% of the total.
Table 5(a) shows results of binary classification
and metaclassification on those instances using de-
pendency path features only. We can see that de-
pendency paths do not perform very well on their
own: most instances are assigned to the ?coordi-
nate? class that comprises a plurality of instances.
A comparison of Tables 5(a) and 4(a) suggests
that classifying query substitution pairs is harder
than classifying TREC phrases.
Table 5(b) shows the results of binary clas-
sification and metaclassification on the same in-
stances using all features. Using all features im-
proves performance dramatically on each individ-
ual binary classifier as well as the metaclassifier.
6.3.2 Classification on All Query Pairs Using
All Features
We now expand to all of our hand-labeled pairs.
Table 6(a) shows results of binary and meta classi-
fication; Figure 1 shows precision-recall curves for
10 binary classifiers (excluding URLs). Our clas-
sifier does quite well on every class but hypernym
and hyponym. These two make up a very small
percentage of the data, so it is not surprising that
performance would be so poor.
The metaclassifier achieved 71% accuracy. This
is significantly better than random or majority-
class baselines, and close to our 78% interanno-
tator agreement. Thresholding the metaclassifier
to pairs with greater than .5 max class probability
(68% of instances) gives 85% accuracy.
Next we wish to see how much of the perfor-
mance can be maintained without using the com-
53
binary n-way data
class maxF AUC prec rec %
no rel .980 .986 .979 .985 80.0
synonym .028 .856 0 0 0.3
hypernym .185 .888 .512 .019 2.1
hyponym .193 .890 .462 .016 2.1
coordinate .808 .971 .714 .931 14.8
meronym .158 .905 .615 .050 0.3
holonym .120 .883 .909 .062 0.3
metaclassifier accuracy .927
(a) All seven WordNet classes. The high accuracy is
mostly due to high recall of no rel and coordinate classes.
binary n-way data
maxF AUC prec rec %
? ? ? ? 0
.086 .683 0 0 1.7
.337 .708 .563 .077 10.6
.341 .720 .527 .080 10.6
.857 .737 .757 .986 74.1
.251 .777 .500 .068 1.5
.277 .767 .522 .075 1.5
? .749
(b) Removing no relationship instances
improves MaxF and recall of all classes,
but performance is generally worse.
Table 4: Performance of 7 binary classifier and metaclassifiers on phrase-pairs cooccuring in TREC data labeled with WordNet
classes, using minipar dependency features. These features do not seem to be adequate for distinguishing classes other than
coordinate and no-relationship.
binary n-way
class maxf auc prec rec
no rel .281 .611 .067 .006
synonym .269 .656 .293 .167
hypernym .140 .626 0 0
hyponym .121 .610 0 0
coordinate .506 .760 .303 .888
spelling .288 .677 .121 .022
stemmed .571 .834 .769 .260
URL .742 .919 .767 .691
generalization .082 .547 0 0
specification .085 .528 0 0
other .393 .681 .384 .364
metaclassifier accuracy .385
(a) Dependency tree paths only.
binary n-way data
maxf auc prec rec % % full
.602 .883 .639 .497 10.6 3.5
.477 .851 .571 .278 4.5 1.5
.167 .686 .125 .017 3.7 1.2
.136 .660 0 0 3.7 1.2
.747 .935 .624 .862 21.0 6.9
.814 .970 .703 .916 11.0 3.6
.781 .972 .788 .675 4.8 1.6
1 1 1 1 16.2 5.3
.490 .883 .489 .393 3.5 1.1
.584 .854 .600 .589 3.5 1.1
.641 .895 .603 .661 17.5 5.7
? .692 ?
(b) All features.
Table 5: Binary and metaclassifier performance on the 32% of hand-labeled instances with dependency path features. Adding
all our features significantly improves performance over just using dependency paths.
putationally expensive syntactic parsing of depen-
dency paths. To estimate the marginal gain of the
other features over the dependency paths, we ex-
cluded the latter features and retrained our clas-
sifiers. Results are shown in Table 6(b). Even
though binary and meta-classifier performance de-
creases on all classes but generalizations and spec-
ifications, much of the performance is maintained.
Because URL changes are easily identifiable by
the IsURL feature, we removed those instances
and retrained the classifiers. Results are shown in
Table 6(c). Although overall accuracy is worse,
individual class performance is still high, allow-
ing us to conclude our results are not only due to
the ease of classifying URLs.
We generated a learning curve by randomly
sampling instances, training the binary classifiers
on that subset, and training the metaclassifier on
the results of the binary classifiers. The curve is
shown in Figure 2. With 10% of the instances, we
have a metaclassifier accuracy of 59%; with 100%
of the data, accuracy is 71%. Accuracy shows no
sign of falling off with more instances.
6.4 Training on WordNet-Labeled Pairs Only
Figure 2 implies that more labeled instances will
lead to greater accuracy. However, manually la-
beled instances are generally expensive to obtain.
Here we look to other sources of labeled instances
for additional training pairs.
6.4.1 Training and Testing on WordNet
We trained and tested five classifiers using 10-
fold cross validation on our set of WordNet-
labeled query segment pairs. Results for each class
are shown in Table 7. We seem to have regressed
to predicting no vs. some relationship.
Because these results are not as good as the
human-labeled results, we believe that some of our
performance must be due to peculiarities of our
data. That is not unexpected: since words that ap-
pear in WordNet are very common, features are
much noisier than features associated with query
entities that are often structured within web pages.
54
binary n-way
class maxf auc prec rec
no rel .531 .878 .616 .643
synonym .355 .820 .506 .212
hypernym .173 .821 .100 .020
hyponym .173 .797 .059 .010
coordinate .635 .921 .590 .703
spelling .778 .960 .625 .904
stemmed .703 .973 .786 .589
URL 1 1 1 1
generalization .565 .916 .575 .483
specification .661 .926 .652 .506
other .539 .898 .575 .483
metaclassifier accuracy .714
(a) All features.
binary n-way data
maxf auc prec rec %
.466 .764 .549 .482 10.4
.351 .745 .493 .178 4.2
.133 .728 0 0 2.0
.163 .733 0 0 2.0
.539 .832 .565 .732 13.9
.723 .917 .628 .902 14.9
.656 .964 .797 .583 3.4
1 1 1 1 29.8
.492 .852 .604 .604 4.8
.578 .869 .670 .644 4.7
.436 .790 .550 .444 9.8
? .714
(b) Dependency path features removed.
binary n-way
maxf auc prec rec
.512 .808 .502 .486
.350 .759 .478 .212
.156 .710 .250 .020
.187 .739 .125 .020
.634 .885 .587 .706
.774 .939 .617 .906
.717 .967 .802 .601
? ? ? ?
.581 .885 .598 .634
.665 .906 .657 .468
.529 .847 .559 .469
? .587
(c) URL class removed.
Table 6: Binary and metaclassifier performance on all classes and all hand-labeled instances. Table (a) provides a benchmark
for 10-class classification over highly substitutable query phrases. Table (b) shows that a lot of our performance can be achieved
without computationally-expensive parsing.
binary meta data
class maxf auc prec rec %
no rel .758 .719 .660 .882 57.8
synonym .431 .901 .617 .199 2.4
hypernym .284 .803 .367 .061 1.8
hyponym .212 .804 .415 .056 1.6
coordinate .588 .713 .615 .369 35.5
other .206 .739 .375 .019 0.8
metaclassifier accuracy .648
Table 7: Binary and metaclassifier performance on WordNet-
labeled instances with all features.
binary meta data
class maxf auc prec rec %
no rel .525 .671 .485 .354 31.9
synonym .381 .671 .684 .125 13.0
hypernym .211 .605 0 0 6.2
hyponym .125 .501 0 0 6.2
coordinate .623 .628 .485 .844 42.6
metaclassifier accuracy .490
Table 8: Training on WordNet-labeled pairs and testing on
hand-labeled pairs. Classifiers trained on WordNet do not
generalize well.
6.4.2 Training on WordNet, Testing on
WordNet and Hand-Labeled Pairs
We took the five classes for which human and
WordNet definitions agreed (synonyms, coordi-
nates, hypernyms, hyponyms, and no relationship)
and trained classifiers on all WordNet-labeled in-
stances. We tested the classifiers on human-
labeled instances from just those five classes. Re-
sults are shown in Table 8. Performance was
not very good, reinforcing the idea that while our
features can distinguish between query segments,
they cannot distinguish between common words.
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0  500  1000  1500  2000  2500  3000  3500  4000  4500  5000
M
et
ac
la
ss
ifi
er
 a
cc
ur
ac
y
Number of query pairs
Figure 2: Meta-classifier accuracy as a function of number of
labeled instances for training.
7 Discussion
Almost all high-weighted features are either
HTML paths or query log features; these are the
ones that are easiest to obtain. Many of the
highest-weight HTML tree features are symmet-
ric, e.g. both words appear in cells of the same ta-
ble, or as items in the same list. Here we note a
selection of the more interesting predictors.
synonym ??X or Y? expressed as a dependency
path was a high-weight feature.
hyper/hyponym ??Y and other X? as a depen-
dency path has highest weight. An interesting
feature is X in a table cell and Y appearing in
text outside but nearby the table.
sibling ?many symmetric HTML features. ?X to
the Y? as in ?80s to the 90s?. ?X and Y?, ?X,
Y, and Z? highly-weighted minipar paths.
general/specialization ?the top three features
are substring containment, word subset dif-
ference count, and prefix overlap.
spelling change ?many negative features, indi-
55
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
  F=0.531
  F=0.634
  F=0.354
F=0.172
  F=0.173
no relationship
sibling
synonym
hyponym
hypernym
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
  F=0.777
  F=0.538
  F=0.702
  F=0.565
  F=0.661
spelling change
related in some other way
stemmed form
generalization
specification
Figure 1: Precision-recall curves for 10 binary classifiers on all hand-labeled instances with all features.
cating that two words that cooccur in a web
page are not likely to be spelling differences.
other ?many symmetric HTML features. Two
words emphasized in the same way (e.g. both
bolded) may indicate some relationship.
none ?many asymmetric HTML features, e.g.
one word in a blockquote, the other bolded
in a different paragraph. Dice coefficient is a
good negative features.
8 Conclusion
We have provided the first benchmark for n-
class semantic classification of highly substi-
tutable query phrases. There is much room for im-
provement, and we expect that this baseline will
be surpassed.
Acknowledgments
Thanks to Chris Manning and Omid Madani for
helpful comments, to Omid Madani for providing
the classification code, to Rion Snow for providing
the hypernym data, and to our labelers.
This work was supported in part by the CIIR
and in part by the Defense Advanced Research
Projects Agency (DARPA) under contract number
HR001-06-C-0023. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect those of the sponsor.
References
Peter G. Anick. 2003. Using terminological feedback for
web search refinement: a log-based study. In SIGIR 2003,
pages 88?95.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
PASCAL Challenges Workshop on Recognising Textual
Entailment.
Ted E. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61?74.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of Coling 1992,
pages 539?545.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In 15th
International World Wide Web Conference (WWW-2006),
Edinburgh.
Sathiya Keerthi and Dennis DeCoste. 2005. A modified fi-
nite newton method for fast solution of large scale linear
svms. Journal of Machine Learning Research, 6:341?361,
March.
Lillian Lee. 1999. Measures of distributional similarity. In
37th Annual Meeting of the Association for Computational
Linguistics, pages 25?32.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Cybernetics
and Control Theory, 10(8):707?710. Original in Doklady
Akademii Nauk SSSR 163(4): 845?848 (1965).
Dekang Lin. 1998. Dependency-based evaluation of mini-
par. In Workshop on the Evaluation of Parsing Systems.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
J. Platt. 2000. Probabilistic outputs for support vector ma-
chines and comparison to regularized likelihood methods.
pages 61?74.
Soo Young Rieh and Hong Iris Xie. 2001. Patterns and se-
quences of multiple query reformulations in web search-
ing: A preliminary study. In Proceedings of the 64th An-
nual Meeting of the American Society for Information Sci-
ence and Technology Vol. 38, pages 246?255.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery.
In Proceedings of the Nineteenth Annual Conference on
Neural Information Processing Systems (NIPS 2005).
Egidio Terra and Charles L. A. Clarke. 2004. Scoring miss-
ing terms in information retrieval tasks. In CIKM 2004,
pages 50?58.
P.D Turney, M.L. Littman, J. Bigham, and V. Shnayder, 2003.
Recent Advances in Natural Language Processing III: Se-
lected Papers from RANLP 2003, chapter Combining in-
dependent modules in lexical multiple-choice problems,
pages 101?110. John Benjamins.
56
