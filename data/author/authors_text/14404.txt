Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038?1046,
Beijing, August 2010
Modeling Socio-Cultural Phenomena in Discourse 
Tomek Strzalkowski1,2, George Aaron Broadwell1, Jennifer Stromer-Galley1, Samira Shaikh1, Sarah Taylor3 and Nick Webb1 1ILS Institute, University at Albany, SUNY 2IPI, Polish Academy of Sciences 3Lockheed Martin Corporation tomek@albany.edu 
 
Abstract In this paper, we describe a novel ap-proach to computational modeling and understanding of social and cul-tural phenomena in multi-party dia-logues. We developed a two-tier ap-proach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred.  1. Introduction We investigate the language dynamics in small group interactions across various set-tings. Our focus in this paper is on English online chat conversations; however, the mod-els we are developing are more universal and applicable to other conversational situations: informal face-to-face interactions, formal meetings, moderated discussions, as well as interactions conducted in languages other than English, e.g., Urdu and Mandarin.  Multi-party online conversations are particu-larly interesting because they become a per-vasive form of communication within virtual communities, ubiquitous across all age groups. In particular, a great amount of communica-tion online occurs in virtual chat-rooms, typi-cally conducted using a highly informal text dialect. At the same time, the reduced-cue environment of online interaction necessitates more explicit linguistic devices to convey social and cultural nuances than is typical in face-to-face or even voice conversations.  Our objective is to develop computational models of how certain social phenomena such as leadership, power, and conflict are signaled and reflected in language through the choice of lexical, syntactic, semantic and conversa-tional forms by discourse participants. In this 
paper we report the results of an initial phase of our work during which we constructed a prototype system called DSARMD-1 (De-tecting Social Actions and Roles in Multi-party Dialogue). Given a representative seg-ment of multiparty task-oriented dialogue, DSARMD-1 automatically classifies all dis-course participants by the degree to which they deploy selected social language uses, such as topic control, task control, involve-ment, and disagreement. These are the mid-level social phenomena, which are de-ployed by discourse participants in order to achieve or assert higher-level social con-structs, including leadership. In this work we adopted a two-tier empirical approach where social language uses are modeled through observable linguistic features that can be automatically extracted from dialogue. The high-level social constructs are then inferred from a combination of language uses attrib-uted to each discourse participant; for exam-ple, a high degree of influence and a high de-gree of involvement by the same person may indicate a leadership role. In this paper we limit our discussion to the first tier only: how to effectively model and classify social lan-guage uses in multi-party dialogue.  2. Related Research Issues related to linguistic manifestation of social phenomena have not been systemati-cally researched before in computational lin-guistics; indeed, most of the effort thus far was directed towards the communicative di-mension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dia-logue focus on information content and structural components (Blaylock, 2002; Car-berry & Lambert, 1999; Stolcke, et al, 2000) in dialogue; few take into account the effects that speech acts may have upon the social 
1038
roles of discourse participants. Also relevant is research on modeling sequences of dia-logue acts ? to predict the next one (Samuel et al 1998; Ji & Bilmes, 2006 inter alia) ? or to map them onto subsequences or ?dialogue games? (Carlson 1983; Levin et al, 1998), which are attempts to formalize participants? roles in conversation (e.g., Linell, 1990; Poe-sio &?Mikheev, 1998; Field et al, 2008). There is a body of literature in anthropology, linguistics, sociology, and communication on the relationship between language and power, as well as other social phenomena, e.g., con-flict, leadership; however, existing ap-proaches typically look at language use in situations where the social relationships are known, rather than using language predic-tively. For example, conversational analysis (Sacks et al, 1974) is concerned with the structure of interaction: turn-taking, when interruptions occur, how repairs are signaled, but not what they reveal about the speakers. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and Scollon, 2001; Agar, 1994) with few systematic studies attempting to ex-plore the reverse, i.e., what the linguistic phenomena tell us about social norms and behaviors.  3. Data & Annotation Our initial focus has been on on-line chat dialogues. While chat data is plentiful on-line, its adaptation for research purposes presents a number of challenges that include users? pri-vacy issues on the one hand, and their com-plete anonymity on the other. Furthermore, most data that may be obtained from public chat-rooms is of limited value for the type of modeling tasks we are interested in due to its high-level of noise, lack of focus, and rapidly shifting, chaotic nature, which makes any longitudinal studies virtually impossible. To derive complex models of conversational be-havior, we need the interaction to be reasona-bly focused on a task and/or social objectives within a group. Few data collections exist covering multiparty dialogue, and even fewer with on-line chat. Moreover, the few collections that exist were built primarily for the purpose of training dialogue act tagging and similar linguistic phenomena; few if any of these corpora are 
suitable for deriving pragmatic models of conversation, including socio-linguistic phe-nomena. Existing resources include a multi-person meeting corpus ICSI-MRDA and the AMI Meeting Corpus (Carletta, 2007), which contains 100 hours of meetings cap-tured using synchronized recording devices. Still, all of these resources look at spoken language rather than on-line chat. There is a parallel interest in the online chat environ-ment, although the development of useful re-sources has progressed less. Some corpora exist such as the NPS Internet chat corpus (Forsyth and Martell, 2007), which has been hand-anonymized and labeled with part-of-speech tags and dialogue act labels. The StrikeCom corpus (Twitchell et al, 2007) consists of 32 multi-person chat dialogues between players of a strategic game, where in 50% of the dialogues one participant has been asked to behave ?deceptively?. It is thus more typical that those interested in the study of Internet chat compile their own corpus on an as needed basis, e.g., Wu et al (2002), Khan et al (2002), Kim et al (2007).  Driven by the need to obtain a suitable dataset we designed a series of experiments in which recruited subjects were invited to participate in a series of on-line chat sessions in a spe-cially designed secure chat-room. The ex-periments were carefully designed around topics, tasks, and games for the participants to engage in so that appropriate types of behav-ior, e.g., disagreement, power play, persuasion, etc. may emerge spontaneously. These ex-periments and the resulting corpus have been described elsewhere (Shaikh et al, 2010b), and we refer the reader to this source. Ulti-mately a corpus of 50 hours of English chat dialogue was collected comprising more than 20,000 turns and 120,000 words. In addition we also assembled a corpus of 20 hours of Urdu chat.  A subset of English language dataset has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics (which are essentially the most persistent lo-cal topics). Although full details of these an-notations are impossible to explain within the scope of this article, we briefly describe them below. Annotated datasets were used to de-velop and train automatic modules that detect and classify social uses of language in dis-course. It is important to note that the annota-
1039
tion has been developed to support the objec-tives of our project and does not necessarily conform to other similar annotation systems used in the past.  ? Communicative links. In a multi-party dia-logue an utterance may be directed towards a specific participant, a subgroup of par-ticipants or to everyone.  ? Dialogue Acts. We developed a hierarchy of 15 dialogue acts for annotating the func-tional aspect of the utterance in discussion.  The tagset we adopted is based on DAMSL (Allen & Core, 1997) and SWBD (Jurafsky et al, 1997), but compressed to 15 tags tuned significantly towards dialogue prag-matics and away from more surface char-acteristics of utterances (Shaikh et al, 2010a).  ? Local topics. Local topics are defined as nouns or noun phrases introduced into dis-course that are subsequently mentioned again via repetition, synonym, or pronoun.  ? Topic reference polarity. Some topics, which we call meso-topics, persist through a number of turns in conversation. A selec-tion of meso-topics is closely associated with the task in which the discourse par-ticipants are engaged. Meso-topics can be distinguished from the local topics because the speakers often make polarized state-ments about them.  4. Socio-linguistic Phenomena We are interested in modeling the social phe-nomena of Leadership and Power in discourse. These high-level phenomena (or Social Roles, SR) will be detected and attributed to dis-course participants based on their deployment of selected Language Uses (LU) in multi-party dialogue. Language Uses are mid-level socio-linguistic devices that link linguistic components deployed in discourse (from lexical to pragmatic) to social con-structs obtaining for and between the partici-pants. The language uses that we are currently studying are Agenda Control, Disagreement, and Involvement (Broadwell et al, 2010). Our research so far is focused on the analysis of English-language synchronous chat, and we are looking for correlations between vari-ous metrics that can be used to detect LU in multiparty dialogue. We expect that some of these correlations may be culturally specific or language-specific, as we move into the 
analysis of Urdu and Mandarin discourse in the next phase of this project. 4.1 Agenda Control in Dialogue Agenda Control is defined as efforts by a member or members of the group to advance the group?s task or goal. This is a complex LU that we will model along two dimensions: (1) Topic Control and (2) Task Control. Topic Control refers to attempts by any discourse participants to impose the topic of conversa-tion. Task Control, on the other hand, is an effort by some members of the group to de-fine the group?s project or goal and/or steer the group towards that goal. We believe that both behaviors can be detected using scalar measures per participant based on certain linguistic features of their utterances. For example, one hypothesis is that topic control is indicated by the rate of local topic introductions (LTI) per participant (Givon, 1983). Local topics may be defined quite simply as noun phrases introduced into dis-course, which are subsequently mentioned again via repetition, synonym, pronoun, or other form of co-reference. Thus, one meas-ure of topic control is the number of local topics introduced by each participant as per-centage of all local topics in a discourse.  Using an LTI index we can construct asser-tions about topic control in a discourse. For example, suppose the following information is discovered about the speaker LE in a multi-party discussion dialogue-11 where 90 local topics are identified: 1. LE introduces 23/90 (25.6%) of local top-ics in this dialogue. 2. The mean rate of local topic introductions is this dialogue is 14.29%, and standard deviation is 8.01. 3. LE is in the top quintile of participants for introducing new local topics We can now claim the following, with a de-gree of confidence (to be determined): TopicControlLTI (LE, 5, dialogue-1) We read this as follows: speaker LE exerts the highest degree of topic control in dialogue-1. Of course, LTI is just one source of evidence and we developed other metrics to comple-ment it. We mention three of them here:                                                 1 Dialogue-1 refers to an actual dataset of 90-minute chat among 7 participants, covering approximately 700 turns. The task is to select a candidate for a job given a set of resumes. 
1040
? SMT Index. This is a measure of topic con-trol suggested in (Givon, 1983) and it is based on subsequent mentions of already introduced local topics. Speakers who in-troduce topics that are discussed at length by the group tend to control the topic of the discussion. The subsequent mentions of lo-cal topics (SMT) index calculates the per-centage of second and subsequent refer-ences to the local topics, by repetition, synonym, or pronoun, relative to the speakers who introduced them.  ? Cite Score. This index measures the extent to which other participants discuss topics introduced by that speaker. The difference between SMT and CiteScore is that the lat-ter reflect to what degree a speaker?s efforts to control the topic are assented to by other participants in a conversation. ? TL Index (TL). This index stipulates that more influential speakers take longer turns than those who are less influential. The TL index is defined as the average number of words per turn for each speaker. Turn length also reflects the extent to which other participants are willing to ?yield the floor? in conversation. Like LTI, all the above indices are mapped into a degree of topic control, based on quin-tiles in normal distribution (Table 1).   
 
LTI SMT CS TL AVG LE 5 5 5 5 5.00 JR 4 4 4 3 3.75 KI 4 3 3 1 2.75 KN 3 5 4 4 4.00 KA 2 2 2 4 2.50 CS 2 2 2 2 2.00 JY 1 1 1 2 1.25 Table 1: Topic Control distribution in dialogue-1. Each row represents a speaker in the group (LE, JR, etc.). Columns show indices used, with degrees per speaker on 5-point scale based on quintiles in normal distribu-tion, and the average value. Ideally, all the above indices (and others yet to be defined) should predict the same out-come, i.e., for each dialogue participant they should assign the same degree of topic control, relative to other speakers. This is not always the case, and where the indices divert in their predictions, our level of confidence in the generated claims decreases. We are currently 
working on how these different metrics cor-relate to each other and how they should be weighted to maximize accuracy of making Topic Control claims. Nonetheless, we can already output a Topic Control map (shown in Table 1) that captures a sense of internal so-cial dynamics within the group.  The other aspect of Agenda Control phe-nomenon is Task Control. It is defined as an effort to determine the group's goal and/or steer the group towards that goal. Unlike Topic Control, which is imposed by influenc-ing the subject of conversation, Task Control is gained by directing other participants to perform certain tasks or accept certain opin-ions. Consequently, Task Control is detected by observing the usage of certain dialogue acts, including Action-Directive, Agree-Accept, Disagree-Reject, and related categories. Here again, we define several in-dices that allow us to compute a degree of Task Control in dialogue for each participant: ? Directive Index (DI). The participant who directs others is attempting to control the course of the task that the group is per-forming. We count the number of directives, i.e., utterances classified as Ac-tion-Directive, made by each participant as a percentage of all directives in discourse. ? Directed Topic Shift Index (DTSI). When a participant who controls the task offers a directive on the task, then the topic of con-versation shifts. In order to detect this con-dition, we calculate the ratio of coincidence of directive dialogue acts by each partici-pant with topic shifts following them.  ? Process Management index (PMI). Another measure of Task Control is the proportion of turns each participant has that explicitly address the problem solving process. This includes utterances that involve coordinat-ing the activities of the participants, plan-ning the order of activities, etc. These fall into the category of Task (or Process) Management in most DA tagging systems.  ? Process Management Success Index (PMSI). This index measures the degree of success by each speaker at controlling the task. A credit is given to the speaker whose suggested curse of action is supported by other speakers for each response that sup-ports the suggestion. Conversely, a credit is taken away for each response that rejects or 
1041
qualifies the suggestion. PMSI is computed as distribution of task management credits among the participants over all dialogue utterances classified as Task/Process Man-agement. 2 As an example, let?s consider the following information computed for the PMI index over dialogue-1:  1. Dialogue-1 contains 246 utterances classi-fied as Task/Process Management rather than doing the task. 2. Speaker KI makes 65 of these utterances for a PMI of 26.4%. 3. Mean PMI for participants is 14.3%; 80th percentile is >21.2%. PMI for KI is in the top quintile for all participants. Based on this evidence we may claim (with yet to be determined confidence) that: TaskControlPMI(KI, 5, dialogue-1) This may be read as follows: speaker KI ex-erts the highest degree of Task Control in dialogue-1. We note that Task Control and Topic Control do not coincide in this dis-course, at least based on the PMI index. Other index values for Task Control may be com-puted and tabulated in a way similar to LTI in Table 1. We omit these here due to space limitations. 4.2 Disagreement in Dialogue Disagreement is another language use that correlates with speaker?s power and leader-ship. There are two ways in which disagree-ment is realized: expressive disagreement and topical disagreement (Stromer-Galley, 2007; Price, 2002). Both can be detected using sca-lar measures applied to subsets of participants, typically any two participants. In addition, we can also measure for each participant the rate with which he or she generates disagreement (with any and all other speakers). Expressive Disagreement is normally understood at the level of dialogue acts, i.e., when discourse participants make explicit utterances of dis-agreement, disapproval, or rejection in re-sponse to a prior speaker?s utterance. Here is an example (KI and KA are two speakers in a multiparty dialogue in which participants                                                 2 The exact structure of the credit function is still being deter-mined experimentally. For example, more credit may be given to first supporting response and less for subsequent responses; more credit may be given for unprompted suggestions than for those that were responding to questions from others. 
discuss candidates for a youth counselor job): KA: CARLA... women are always better with kids KI: That?s not true! KI: Men can be good with kids too While such exchanges are vivid examples of expressive disagreement, we are interested in more sustained phenomenon where two speakers repeatedly disagree, thus revealing a social relationship between them. Therefore, one measure of Expressive Disagreement that we consider is the number of Disagree-Reject dialogue acts between any two speakers as a percentage of all utterances exchanged be-tween these two speakers. This becomes a basis for the Disagree-Reject Index (DRX). In dialogue-1 we have: 1. Speakers KI and KA have 47 turns between them. Among these there are 8 turns classi-fied as Disagree-Reject, for the DRX of 15.7%. 2. The mean DRX for speakers who make any Disagree-Reject utterances is 9.5%. The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates. While Expressive Disagreement is based on the use of more overt linguistic devices, Topical Disagreement is defined as a differ-ence in referential valence in utterances (statements, opinions, questions, etc.) made on a topic. Referential valence of an utterance is determined by the type of statement made about the topic in question, which can be positive (+), negative (?), or neutral (0). A positive statement is one in favor of (express advocacy) or in support of (supporting infor-mation) the topic being discussed. A negative statement is one that is against or negative on 
1042
the topic being discussed. A neutral statement is one that does not indicate the speaker?s po-sition on the topic. Here is an example of op-posing polarity statements about the same topic in discourse: Sp-1: I like that he mentions ?Volunteerism and Leadership? Sp-2: but if they?re looking for someone who is experienced then I?d cross him off Detecting topical disagreement in discourse is more complicated because its strength may vary from one topic in a conversation to the next. A reasonable approach is thus to meas-ure the degree of disagreement between two speakers on one topic first, and then extrapo-late over the entire discourse. Accordingly, our measure of topical disagreement is valua-tion differential between any two speakers as expressed in their utterances about a topic. Here, the topic (or an ?issue?) is understood more narrowly than the local topic defined in the previous section (as used in Topic Control, for example), and may be assumed to cover only the most persistent local topics, i.e., top-ics with the largest number of references in dialogue, or what we call the meso-topics. For example, in a discussion of job applicants, each of the applicants becomes a meso-topic, and there may be additional meso-topics pre-sent, such as qualifications required, etc.  The resulting Topical Disagreement Metric (TDM) captures the degree to which any two speakers advocate the opposite sides of a meso-topic. TDM is computed as an average of P-valuation differential for one speaker (advocating for a meso-topic) and (?P)-valuation differential for the other speaker (advocating against the meso-topic).  Using TDM we can construct claims related to disagreement in a given multiparty dia-logue of sufficient duration (exactly what constitutes a sufficient duration is still being researched). Below is an example based on a 90-minute chat dialogue-1 about several job candidates for a youth counselor. The discus-sion involved 7 participants, including KI and KA. Topical disagreement is measured on 5 points scale (corresponding to quintiles in normal distribution): TpDisAgreeTDM(KI,KA,?Carla?,4,dialogue-1) This may be read as follows: speakers KI and KA topically disagree to degree 4 on topic [job candidate] ?Carla? in dialogue-1. In or-
der to calculate this we compute the value of TDM index between these two speakers. We find that KA makes 30% of all positive utter-ances made by anyone about Carla (40), while KI makes 45% of all negative utterances against Carla. This places these two speakers in the top quintiles in the ?for Carla? polarity distribution and ?against Carla? distribution, respectively. Taking into account any oppos-ing polarity statements made by KA against Carla and any statements made by KI for Carla, we calculate the level of topical dis-agreement between KA and KI to be 4 on the 1-5 scale. TDM allows us to compute topical disagree-ment between any two speakers in a discourse, which may also be represented in a 2-dimensional table revealing another inter-esting aspect of internal group dynamics.  4.3 Involvement in Dialogue The third type of social language use that we discuss in this paper is Involvement. In-volvement is defined as a degree of engage-ment or participation in the discussion of a group. It is an important element of leader-ship, although its importance is expected to differ between cultures; in Western cultures, high involvement and influence (topic control) often correlates with group leadership. In order to measure Involvement we designed several indices based on turn characteristics for each speaker. Four of the indices are briefly explained below:  ? The NP index (NPI) is a measure of gross informational content contributed by each speaker in discourse. NPI counts the ratio of third-person nouns and pronouns used by a speaker to the total number of nouns and pronouns in the discourse.  ? The Turn index (TI) is a measure of inter-actional frequency; it counts the ratio of turns per participant to the total number of turns in the discourse.  ? The Topic Chain Index (TCI) counts the degree to which participants discuss of the most persistent topics. In order to calculate TCI values, we define a topic chains for all local topics. We compute frequency of mentions of these longest topics for each participant.  ? The Allotopicality Index (ATP) counts the number of mentions of local topics that were introduced by other participants. An 
1043
ATP value is the proportion of a speaker's allotopical mentions, i.e., excluding ?self-citations?, to all allotopical mentions in a discourse.  As an example, we may consider the follow-ing situation in dialogue-1: 1. Dialogue-1 contains 796 third person nouns and pronouns, excluding mentions of participants? names. 2. Speaker JR uses 180 nouns and pronouns for an NPI of 22.6%.  3. The median NPI is 14.3%; JR are in the upper quintile of participants (> 19.9%). From the above evidence we can draw the following claim: InvolvementNPI(JR, 5, dialogue-1) This may be read as: speaker JR is the most involved participant in dialogue-1. As with other language uses, multiple indices for Involvement can be combined into a 2-dimensional map capturing the group in-ternal dynamics.  5. Implementation & Evaluation We developed a prototype automated DSARMD system that comprises a series of modules that create automated annotation of the source dialogue for all the language ele-ments discussed above, including communi-cative links, dialogue acts, local/meso topics, and polarity. Automatically annotated dia-logue is then used to generate language use degree claims. In order to evaluate accuracy of the automated process we conducted a pre-liminary evaluation comparing the LU claims generated from automatically annotated data to the claims generated from manually coded dialogues. Below we briefly describe the methodology and metrics used. Each language use is asserted per a partici-pant in a discourse (or per each pair of par-ticipants, e.g., for Disagreement) on a 5-point ?strength? scale. This can be represented as an ordered sequence LUX(d1, d2, ? dn), where LU is the language use being asserted, X is the index used, di is the degree of LU attrib-uted to speaker i. This assignment is therefore a 5-way classification of all discourse par-ticipants and its correctness is measured by dividing the number of correct assignments by the total number of elements to be classi-fied, which gives the micro-averaged preci-sion. The accuracy metric is computed with 
several variants as follows: 1. Strict mapping: each complete match is counted as 1; all mismatches are counted as 0. For example, the outputs LUX (5,4,3,2,1) and LUX (4,5,3,1,1) produce two exact matches (for the third and the last speaker) for a precision of 0.4. 2. Weighted mapping: since each degree value di in LUX(d1, d2, ? dn) represents a quintile in normal distribution, we consider the po-sition of the value within the quintile. If two mismatched values are less than ? quintile apart we assign a partial credit (currently 0.5). 3. Highest ? Rest: we measure accuracy with which the highest LU degree (but not nec-essarily the same degree) is assigned to the right speaker vs. any other score. This re-sults in binary classification of scores. The sequences in (1) produce 0.6 match score. 4. High ? Low: An alternative binary classifi-cation where scores 5 and 4 are considered High, while the remaining scores are con-sidered Low. Under this metric, the se-quences in (1) match with 100% precision. The process of automatic assignment of lan-guage uses derived from automatically proc-essed dialogues was evaluated against the control set of assignments based on hu-man-annotated data. In order to obtain a reli-able ?ground truth?, each test dialogue was annotated by at least three human coders (linguistics and communication graduate stu-dents, trained). Since human annotation was done at the linguistic component level, a strict inter-annotator agreement was not required; instead, we were interested whether in each case a comparable statistical distribution of the corresponding LU index was obtained. Annotations that produced index distributions dissimilar from the majority were eliminated. Automated dialogue processing involved the following modules: ? Local topics detection identifies first men-tions by tracking occurrences of noun phrases. Subsequent mentions are identi-fied using fairly simple pronoun resolution (based mostly on lexical features), with Wordnet used to identify synonyms, etc. ? Meso-topics are identified as longest-chain local topics. Their polarity is assessed at the utterance level by noting presence of positive or negative cue words and phrases. ? Dialogue acts are tagged based on presence 
1044
of certain cue phrases derived from a train-ing corpus (Webb et al, 2008).  ? Communicative links are mapped by com-puting inter-utterance similarity based on n-gram overlap. Preliminary evaluation results are shown in Tables 3-5 with average performance over 3 chat sessions (approx 4.5 hours) involving three groups of speakers and different tasks (job candidates, political issues). Topic Con-trol and Involvement tables show average accuracy per index. For example, the LTI in-dex, computed over automatically extracted local topics, produces Topic Control assign-ments with the average precision of 80% when compared to assignments derived from human-annotated data using the strict accu-racy metric. However, automated prediction of Involvement based on NPI index is far less reliable, although we can still pick the most involved speaker with 67% accuracy. We omit the indices based on turn length (TL) and turn count (TI) because their values are trivially computed. At this time we do not combine indices into a single LU prediction. Addi-tional experiments are needed to determine how much each of these indices contributes to LU prediction. Topic  Control LTI? SMT? CS?Strict? 0.80? 0.40? 0.40?Weighted? 0.90? 0.53? 0.53?Highest?Rest? 0.90? 0.67? 0.67?High?Low? 1.00? 0.84? 0.90?Table 3: Topic Control LU assignment performance averages of selected indices over a subset of data cov-ering three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, 7 per session). 
Involvement NPI? TCI? ATP?Strict? 0.31? 0.42? 0.39?Weighted? 0.46? 0.49? 0.42?Highest?Rest? 0.67? 0.77? 0.68?High?Low? 0.58? 0.74? 0.48?Table 4: Involvement LU assignment performance av-erages for selected indices over the same subset of data as in Table 3. Topical Disagreement performance is shown in Table 5. We calculated precision and recall of assigning a correct degree of disagreement 
to each pair of speakers who are members of a group. Precision and recall averages are then computed over all meso-topics identified in the test dataset, which consists of three separate 90-minute dialogues involving 7, 5 and 7 speakers, respectively. Our calculation includes the cases where different sets of meso-topics were identified by the system and by the human coder. A strict mapping of levels of disagreement between speakers is hard to compute accurately; however, finding the speakers who disagree the most, or the least, is significantly more robust. 
Topical Disagreement Prec.? Recall?Strict? 0.33? 0.32?Weighted? 0.54? 0.54?Highest?Rest? 0.89? 0.85?High?Low? 0.77? 0.73?Table 5: Topical Disagreement LU assignment per-formance averages over 13 meso-topics discussed in three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, and 7 per session). 6. Conclusion In this paper we presented a preliminary design for modeling certain types of social phenomena in multi-party on-line dialogues. Initial, limited-scale evaluation indicates that the model can be effectively automated. Much work lies ahead, including large scale evaluation, testing index stability and resilience to NL component level error. Current performance of the system is based on only preliminary versions of linguistic modules (topic extraction, polarity assignments, etc.) which perform at only 70-80% accuracy, so these need to be improved as well. Research on Urdu and Chinese dialogues is just starting. Acknowledgements This research was funded by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the U.S. Army Research Lab. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI or the U.S. Government. 
1045
References Agar, Michael. 1994. Language Shock, Under-standing the Culture of Conversation. Quill, William Morrow, New York. Allen, J. M. Core. 1997. Draft of DAMSL: Dialog Act Markup in Several Layers. www.cs. roch-ester.edu/research/cisd/resources/damsl/  Anderson, A., et al 1991. The HCRC Map Task Corpus. Language and Speech 34(4), 351--366. Austin, J. L. 1962. How to do Things with Words. Clarendon Press, Oxford. Bird, Steven, et al 2009. Natural Language Proc-essing with Python: Analyzing Text with the Natural Language Toolkit. O'Reilly Media.  Blaylock, Nate. 2002. Managing Communicative Intentions in Dialogue Using a Collaborative Problem-Solving Model. Technical Report 774, University of Rochester, CS Dept. Broadwell, G. A et al (2010). Social Phenomena and Language Use. ILS Technical report. Carberry, Sandra and Lynn Lambert. 1999. A Process Model for Recognizing Communicative Acts and Modeling Negotiation Dialogue. Computational Linguistics, 25(1), pp. 1-53. Carletta, J. (2007). Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation Journal 41(2): 181-190 Carlson, Lauri. 1983. Dialogue Games: An Ap-proach to Discourse Analysis. D. Reidel. Eric N. Forsyth and Craig H. Martell. 2007. Lexi-cal and Discourse Analysis of Online Chat Dia-log. First IEEE International Conference on Semantic Computing (ICSC 2007), pp. 19-26. Field, D., et al 2008. Automatic Induction of Dia-logue Structure from the Companions Dialogue Corpus, 4th Int. Workshop on Human-Computer Conversation, Bellagio. Givon, Talmy. 1983. Topic continuity in discourse: A quantitative cross-language study. Amster-dam: John Benjamins.  Ivanovic, Edward. 2005. Dialogue Act Tagging for Instant Messaging Chat Sessions. In Proceed-ings of the ACL Student Research Workshop. 79?84. Ann Arbor, Michigan. Ji, Gang Jeff Bilmes. 2006. Backoff Model Train-ing using Partially Observed Data: Application to Dialog Act Tagging. HLT-NAACL Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-asca. 1997. Switchboard SWBD-DAMSL Shal-low-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/ manual.august1.html Jurafsky, D., et al 1997. Automatic detection of discourse structure for speech recognition and understanding. IEEE Workshop on Speech Recognition and Understanding, Santa Barbara. Khan, Faisal M., et al 2002. Mining Chat-room Conversations for Social and Semantic Interac-
tions. Computer Science and Engineering, Le-high University. Kim, Jihie., et al 2007. An Intelligent Discus-sion-Bot for Guiding Student Interactions in Threaded Discussions. AAAI Spring Sympo-sium on Interaction Challenges for Intelligent Assistants Levin, L., et al (1998). A discourse coding scheme for conversational Spanish. Interna-tional Conference on Speech and Language Processing. Levin, L., et al (2003). Domain specific speech acts for spoken language translation. 4th SIG-dial Workshop on Discourse and Dialogue. Linell, Per. 1990. The power of dialogue dynamics. In Ivana Markov?a and Klaus Foppa, editors, The Dynamics of Dialogue. Harvester, 147?177. Poesio, Massimo and Andrei Mikheev. 1998. The predictive power of game structure in dialogue act recognition. International Conference on Speech and Language Processing (ICSLP-98). Price, V., Capella, J. N., & Nir, L. (2002). Does disagreement contribute to more deliberative opinion? Political Communication, 19, 95-112. Sacks, H. and Schegloff, E., Jefferson, G. 1974. A simplest systematic for the organization of turn-taking for conversation. In: Language 50(4), 696-735.  Samuel, K. et al 1998. Dialogue Act Tagging with Transformation-Based Learning. 36th Annual Meeting of the ACL. Scollon, Ron and Suzanne W. Scollon. 2001. Intercultural Communication, A Discourse Ap-proach. Blackwell Publishing, Second Edition. Searle, J. R. 1969. Speech Acts. Cambridge Uni-versity Press, London-New York. Shaikh, S. et al 2010. DSARMD Annotation Guidelines, V. 2.5. ILS Technical Report.  Shaikh S. et al 2010. MPC: A Multi-Party Chat Corpus for Modeling Social Phenomena in Discourse, Proc. LREC-2010, Malta. Stolcke, Andreas et al 2000. Dialogue Act Mod-eling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguis-tics, 26(3). Stromer-Galley, J. 2007. Measuring deliberation?s content: A coding scheme. Journal of Public Deliberation, 3(1).  Tianhao Wu, et al 2002. Posting Act Tagging Us-ing Transformation-Based Learning. Founda-tions of Data Mining and Discovery, IEEE In-ternational Conference on Data Mining Twitchell, Douglas P., Jay F. Nunamaker Jr., and Judee K. Burgoon. 2004. Using Speech Act Profiling for Deception Detection. Intelligence and Security Informatics, LNCS, Vol. 3073 Webb, N., T. Liu, M. Hepple and Y. Wilks. 2008. Cross-Domain Dialogue Act Tagging. 6th In-ternational Conference on Language Resources and Evaluation (LREC-2008), Marrakech. 
1046
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 171?179,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Multi-Modal Annotation of Quest Games in Second Life  
 
Sharon Gower Small, Jennifer Stromer-Galley and Tomek Strzalkowski 
ILS Institute 
State University of New York at Albany 
Albany, NY 12222 
small@albany.edu, jstromer@albany.edu, tomek@albany.edu  
 
 
 
Abstract 
We describe an annotation tool developed to as-
sist in the creation of multimodal action-
communication corpora from on-line massively 
multi-player games, or MMGs. MMGs typically 
involve groups of players (5-30) who control 
their avatars1, perform various activities (quest-
ing, competing, fighting, etc.) and communicate 
via chat or speech using assumed screen names. 
We collected a corpus of 48 group quests in 
Second Life that jointly involved 206 players 
who generated over 30,000 messages in quasi-
synchronous chat during approximately 140 
hours of recorded action. Multiple levels of co-
ordinated annotation of this corpus (dialogue, 
movements, touch, gaze, wear, etc) are required 
in order to support development of automated 
predictors of selected real-life social and demo-
graphic characteristics of the players. The anno-
tation tool presented in this paper was developed 
to enable efficient and accurate annotation of all 
dimensions simultaneously. 
 
1 Introduction 
The aim of our project is to predict the real world 
characteristics of players of massively-multiplayer 
online games, such as Second Life (SL). We sought 
to predict actual player attributes like age or educa-
tion levels, and personality traits including leader-
ship or conformity. Our task was to do so using 
only the behaviors, communication, and interaction 
among the players produced during game play. To 
do so, we logged all players? avatar movements, 
                                                          
1 All avatar names seen in this paper have been changed to 
protect players? identities.  
?touch events? (putting on or taking off clothing 
items, for example), and their public chat messages 
(i.e., messages that can be seen by all players in the 
group). Given the complex nature of interpreting 
chat in an online game environment, we required a 
tool that would allow annotators to have a synchro-
nized view of both the event action as well as the 
chat utterances.  This would allow our annotators to 
correlate the events and the chat by marking them 
simultaneously. More importantly, being able to 
view game events enables more accurate chat anno-
tation; and conversely, viewing chat utterances 
helps to interpret the significance of certain events 
in the game, e.g., one avatar following another.  For 
example, an exclamation of: ?I can?t do it!? could 
be simply a response (rejection) to a request from 
another player; however, when the game action is 
viewed and the speaker is seen attempting to enter a 
building without success, another interpretation 
may arise (an assertion, a call for help, etc.).  
The Real World (RW) characteristics of SL 
players (and other on-line games) may be inferred 
to varying degrees from the appearance of their 
avatars, the behaviors they engage in, as well as 
from their on-line chat communications. For exam-
ple, the avatar gender generally matches the gender 
of the owner; on the other hand, vocabulary choices 
in chat are rather poor predictors of a player?s age, 
even though such correlation is generally seen in 
real life conversation.  
Second Life2 was the chosen platform because 
of the ease of creating objects, controlling the play 
environment, and collecting players? movement, 
chat, and other behaviors. We generated a corpus of 
chat and movement data from 48 quests comprised 
of 206 participants who generated over 30,000 
                                                          
2 An online Virtual World developed and launched in 2003, by 
Linden Lab, San Francisco, CA.  http://secondlife.com 
171
messages and approximately 140 hours of recorded 
action. We required an annotation tool to help us 
efficiently annotate dialogue acts and communica-
tion links in chat utterances as well as avatar 
movements from such a large corpus.  Moreover, 
we required correlation between these two dimen-
sions of chat and movement since movement and 
other actions may be both causes and effects of 
verbal communication. We developed a multi-
modal event and chat annotation tool (called RAT, 
the Relational Annotation Tool), which will simul-
taneously display a 2D rendering of all movement 
activity recorded during our Second Life studies, 
synchronized with the chat utterances. In this way 
both chat and movements can be annotated simul-
taneously: the avatar movement actions can be re-
viewed while making dialogue act annotations.  
This has the added advantage of allowing the anno-
tator to see the relationships between chat, behav-
ior, and location/movement. This paper will 
describe our annotation process and the RAT tool. 
2 Related Work 
Annotation tools have been built for a variety of 
purposes. The CSLU Toolkit (Sutton et al, 1998) is 
a suite of tools used for annotating spoken lan-
guage. Similarly, the EMU System (Cassidy and 
Harrington, 2001) is a speech database management 
system that supports multi-level annotations.  Sys-
tems have been created that allow users to readily 
build their own tools such as AGTK (Bird et al, 
2001).  The multi-modal tool DAT (Core and Al-
len, 1997) was developed to assist testing of the 
DAMSL annotation scheme.  With DAT, annota-
tors were able to listen to the actual dialogues as 
well as view the transcripts. While these tools are 
all highly effective for their respective tasks, ours is 
unique in its synchronized view of both event ac-
tion and chat utterances. 
Although researchers studying online communi-
cation use either off-the shelf qualitative data anal-
ysis programs like Atlas.ti or NVivo, a few studies 
have annotated chat using custom-built tools. One 
approach uses computer-mediated discourse analy-
sis approaches and the Dynamic Topic Analysis 
tool (Herring, 2003; Herring & Nix; 1997; Stromer-
Galley & Martison, 2009), which allows annotators 
to track a specific phenomenon of online interaction 
in chat: topic shifts during an interaction. The 
Virtual Math Teams project (Stahl, 2009) created a 
ated a tool that allowed for the simultaneous play-
back of messages posted to a quasi-synchronous 
discussion forum with whiteboard drawings that 
student math team members used to illustrate their 
ideas or visualize the math problem they were try-
ing to solve (?akir, 2009).  
A different approach to data capture of complex 
human interaction is found in the AMI Meeting 
Corpus (Carletta, 2007). It captures participants? 
head movement information from individual head-
mounted cameras, which allows for annotation of 
nodding (consent, agreement) or shaking (dis-
agreement), as well as participants? locations within 
the room; however, no complex events involving 
series of movements or participant proximity are 
considered. We are unaware of any other tools that 
facilitate the simultaneous playback of multi-modes 
of communication and behavior.  
3 Second Life Experiments 
To generate player data, we rented an island in 
Second Life and developed an approximately two 
hour quest, the Case of the Missing Moonstone.  In 
this quest, small groups of 4 to 5 players, who were 
previously unacquainted, work their way together 
through the clues and puzzles to solve a murder 
mystery. We recruited Second Life players in-game 
through advertising and setting up a shop that inter-
ested players could browse. We also used Facebook 
ads, which were remarkably effective.  
The process of the quest experience for players 
started after they arrived in a starting area of the 
island (the quest was open only to players who 
were made temporary members of our island) 
where they met other players, browsed quest-
appropriate clothing to adorn their avatars, and re-
ceived information from one of the researchers. 
Once all players arrived, the main quest began, 
progressing through five geographic areas in the 
island. Players were accompanied by a ?training 
sergeant?, a researcher using a robot avatar, that 
followed players through the quest and provided 
hints when groups became stymied along their in-
vestigation but otherwise had little interaction with 
the group.  
The quest was designed for players to encounter 
obstacles that required coordinated action, such as 
all players standing on special buttons to activate a 
door, or the sharing of information between players, 
such as solutions to a word puzzle, in order to ad-
vance to the next area of the quest (Figure 1). 
172
Slimy Roastbeef: ?who?s got the square gear?? 
Kenny Superstar: ?I do, but I?m stuck? 
Slimy Roastbeef: ?can you hand it to me?? 
Kenny Superstar: ?i don?t know how? 
Slimy Roastbeef: ?open your inventory, click 
and drag it onto me? 
 
Figure 1: Excerpt of dialogue during a coor-
dination activity 
Quest activities requiring coordination among the 
players were common and also necessary to ensure 
a sufficient degree of movement and message traf-
fic to provide enough material to test our predic-
tions, and to allow us to observe particular social 
characteristics of players. Players answered a sur-
vey before and then again after the quest, providing 
demographic and trait information and evaluating 
other members of their group on the characteristics 
of interest. 
3.1 Data Collection 
We recorded all players? avatar movements as they 
purposefully moved avatars through the virtual 
spaces of the game environment, their public chat, 
and their ?touch events?, which are the actions that 
bring objects out of player inventories, pick up ob-
jects to put in their inventories, or to put objects, 
such as hats or clothes, onto the avatars, and the 
like. We followed Yee and Bailenson?s (2008) 
technical approach for logging player behavior. To 
get a sense of the volume of data generated, 206 
players generated over 30,000 messages into the 
group?s public chat from the 48 sessions.  We com-
piled approximately 140 hours of recorded action. 
The avatar logger was implemented to record each 
avatar?s location through their (x,y,z) coordinates, 
recorded at two second intervals. This information 
was later used to render the avatar?s position on our 
2D representation of the action (section 4.1).   
4 RAT 
The Relational Annotation Tool (RAT) was built to 
assist in annotating the massive collection of data 
collected during the Second Life experiments.  A 
tool was needed that would allow annotators to see 
the textual transcripts of the chat while at the same 
time view a 2D representation of the action.  Addi-
tionally, we had a textual transcript for a select set 
of events: touch an object, stand on an object, at-
tach an object, etc., that we needed to make avail-
able to the annotator for review.  
These tool characteristics were needed for 
several reasons. First, in order to fully understand 
the communication and interaction occurring be-
tween players in the game environment and accu-
rately annotate those messages, we needed 
annotators to have as much information about the 
context as possible. The 2D map coupled with the 
events information made it easier to understand. 
For example, in the quest, players in a specific 
zone, encounter a dead, maimed body. As annota-
tors assigned codes to the chat, they would some-
times encounter exclamations, such as ?ew? or 
?gross?. Annotators would use the 2D map and the 
location of the exclaiming avatar to determine if the 
exclamation was a result of their location (in the 
zone with the dead body) or because of something 
said or done by another player. Location of avatars 
on the 2D map synchronized with chat was also 
helpful for annotators when attempting to disam-
biguate communicative links. For example, in one 
subzone, mad scribblings are written on a wall. If 
player A says ?You see that scribbling on the 
wall?? the annotator needs to use the 2D map to see 
who the player is speaking to. If player A and 
player C are both standing in that subzone, then the 
annotator can make a reasonable assumption that 
player A is directing the question to player C, and 
not player B who is located in a different subzone. 
Second, we annotated coordinated avatar move-
ment actions (such as following each other into a 
building or into a room), and the only way to read-
ily identify such complex events was through the 
2D map of avatar movements. 
The overall RAT interface, Figure 2, allows 
the annotator to simultaneously view all modes of 
representation.  There are three distinct panels in 
this interface. The left hand panel is the 2D repre-
sentation of the action (section 4.1).  The upper 
right hand panel displays the chat and event tran-
scripts (section 4.2), while the lower right hand por-
tion is reserved for the three annotator sub-panels 
(section 4.3).   
 
173
Figure 2: RAT interface 
4.1 The 2D Game Representation 
The 2D representation was the most challenging of 
the panels to implement.  We needed to find the 
proper level of abstraction for the action, while 
maintaining its usefulness for the annotator.  Too 
complex a representation would cause cognitive 
overload for the annotator, thus potentially deterio-
rating the speed and quality of the annotations.  
Conversely, an overly abstract representation would 
not be of significant value in the annotation proc-
ess.   
There were five distinct geographic areas on our 
Second Life Island: Starting Area, Mansion, Town 
Center, Factory and Apartments. An overview of 
the area in Second Life is displayed in Figure 3. We 
decided to represent each area separately as each 
group moves between the areas together, and it was 
therefore never necessary to display more than one 
area at a time.  The 2D representation of the Man-
sion Area is displayed in Figure 4 below.  Figure 5 
is an exterior view of the actual Mansion in Second 
Life. Each area?s fixed representation was rendered 
using Java Graphics, reading in the Second Life 
(x,y,z) coordinates from an XML data file. We rep-
resented the walls of the buildings as connected 
solid black lines with openings left for doorways.  
Key item locations were marked and labeled, e.g. 
Kitten, maid, the Idol, etc. Even though annotators 
visited the island to familiarize themselves with the 
layout, many mansion rooms were labeled to help 
the annotator recall the layout of the building, and 
minimize error of annotation based on flawed re-
call. Finally, the exact time of the action that is cur-
rently being represented is displayed in the lower 
left hand corner. 
 
 
 
Figure 3: Second Life overview map 
  
174
 
 
Figure 4: 2D representation of Second Life action 
inside the Mansion/Manor 
 
 
 
 
Figure 5: Second Life view of Mansion exterior 
 
Avatar location was recorded in our log files as an 
(x,y,z) coordinate at a two second interval.  Avatars 
were represented in our 2D panel as moving solid 
color circles, using the x and y coordinates. A color 
coded avatar key was displayed below the 2D rep-
resentation.  This key related the full name of every 
avatar to its colored circle representation. The z 
coordinate was used to determine if the avatar was 
on the second floor of a building.  If the z value 
indicated an avatar was on a second floor, their icon 
was modified to include the number ?2? for the du-
ration of their time on the second floor. Also logged 
was the avatar?s degree of rotation.  Using this we 
were able to represent which direction the avatar 
was looking by a small black dot on their colored 
circle. 
As the annotators stepped through the chat and 
event annotation, the action would move forward, 
in synchronized step in the 2D map.  In this way at 
any given time the annotator could see the avatar 
action corresponding to the chat and event tran-
scripts appearing in the right panels.  The annotator 
had the option to step forward or backward through 
the data at any step interval, where each step corre-
sponded to a two second increment or decrement, to 
provide maximum flexibility to the annotator in 
viewing and reviewing the actions and communica-
tions to be annotated.  Additionally, ?Play? and 
?Stop? buttons were added to the tool so the anno-
tator may simply watch the action play forward ra-
ther than manually stepping through. 
4.2 The Chat & Event Panel 
Avatar utterances along with logged Second Life 
events were displayed in the Chat and Event Panel 
(Figure 6). Utterances and events were each dis-
played in their own column.  Time was recorded for 
every utterance and event, and this was displayed in 
the first column of the Chat and Event Panel. All 
avatar names in the utterances and events were 
color coded, where the colors corresponded to the 
avatar color used in the 2D panel. This panel was 
synchronized with the 2D Representation panel and 
as the annotator stepped through the game action on 
the 2D display, the associated utterances and events 
populated the Chat and Event panel. 
 
175
 
 
Figure 6: Chat & Event Panel  
4.3 The Annotator Panels  
The Annotator Panels (Figures 7 and 10) contains 
all features needed for the annotator to quickly 
annotate the events and dialogue. Annotators could 
choose from a number of categories to label each 
dialogue utterance. Coding categories included 
communicative links, dialogue acts, and selected 
multi-avatar actions.   In the following we briefly 
outline each of these. A more detailed description 
of the chat annotation scheme is available in 
(Shaikh et al, 2010).   
4.3.1 Communicative Links 
One of the challenges in multi-party dialogue is to 
establish which user an utterance is directed to-
wards. Users do not typically add addressing in-
formation in their utterances, which leads to 
ambiguity while creating a communication link be-
tween users. With this annotation level, we asked 
the annotators to determine whether each utterance 
was addressed to some user, in which case they 
were asked to mark which specific user it was ad-
dressed to; was in response to another prior utter-
ance by a different user, which required marking 
the specific utterance responded to; or a continua-
tion of the user?s own prior utterance.  
Communicative link annotation allows for accu-
rate mapping of dialogue dynamics in the multi-
party setting, and is a critical component of tracking 
such social phenomena as disagreements and lead-
ership. 
4.3.2 Dialogue Acts 
We developed a hierarchy of 19 dialogue acts for 
annotating the functional aspect of the utterance in 
the discussion.  The tagset we adopted is loosely 
based on DAMSL (Allen & Core, 1997) and 
SWBD (Jurafsky et al, 1997), but greatly reduced 
and also tuned significantly towards dialogue 
pragmatics and away from more surface character-
istics of utterances. In particular, we ask our anno-
tators what is the pragmatic function of each 
utterance within the dialogue, a decision that often 
depends upon how earlier utterances were classi-
fied. Thus augmented, DA tags become an impor-
tant source of evidence for detecting language uses 
and such social phenomena as conformity. Exam-
ples of dialogue act tags include Assertion-Opinion, 
Acknowledge, Information-Request, and Confirma-
tion-Request. 
Using the augmented DA tagset alo presents a 
fairly challenging task to our annotators, who need 
to be trained for many hours before an acceptable 
rate of inter-annotator agreement is achieved. For 
this reason, we consider our current DA tagging as 
a work in progress. 
4.3.3 Zone coding 
Each of the five main areas had a correspond-
ing set of subzones. A subzone is a building, a 
room within a building, or any other identifiable 
area within the playable spaces of the quest, e.g. the 
Mansion has the subzones: Hall, Dining Room, 
Kitchen, Outside, Ghost Room, etc. The subzone 
was determined based on the avatar(s) (x,y,z) coor-
dinates and the known subzone boundaries. This 
additional piece of data allowed for statistical 
analysis at different levels: avatar, dialogue unit, 
and subzone. 
 
176
 
 
Figure 7: Chat Annotation Sub-Panel
4.3.4 Multi-avatar events 
As mentioned, in addition to chat we also were in-
terested in having the annotators record composite 
events involving multiple avatars over a span of 
time and space.  While the design of the RAT tool 
will support annotation of any event of interest with 
only slight modifications, for our purposes, we 
were interested in annotating two types of events 
that we considered significant for our research hy-
potheses. The first type of event was the multi-
avatar entry (or exit) into a sub-zone, including the 
order in which the avatars moved.  
Figure 8 shows an example of a ?Moves into 
Subzone? annotation as displayed in the Chat & 
Event Panel. Figure 9 shows the corresponding se-
ries of progressive moments in time portraying en-
try into the Bank subzone as represented in RAT. In 
the annotation, each avatar name is recorded in or-
der of its entry into the subzone (here, the Bank).  
Additionally, we record the subzone name and the 
time the event is completed3.  
The second type of event we annotated was the 
?follow X? event, i.e., when one or more avatars 
appeared to be following one another within a sub-
zone. These two types of events were of particular 
interest because we hypothesized that players who 
are leaders are likely to enter first into a subzone 
and be followed around once inside.  
In addition, support for annotation of other types 
of composite events can be added as needed; for 
example, group forming and splitting, or certain 
                                                          
3 We are also able to record the start time of any event but for 
our purposes we were only concerned with the end time. 
joint activities involving objects, etc. were fairly 
common in quests and may be significant for some 
analyses (although not for our hypotheses). 
For each type of event, an annotation subpanel is 
created to facilitate speedy markup while minimiz-
ing opportunities for error (Figure 10).  A ?Moves 
Into Subzone? event is annotated by recording the  
ordinal (1, 2, 3, etc.) for each avatar.  Similarly, a 
?Follows? event is coded as avatar group ?A? fol-
lows group ?B?, where each group will contain one 
or more avatars. 
 
 
 
Figure 8: The corresponding annotation for Figure 
9 event, as displayed in the Chat & Event Panel 
5 The Annotation Process 
To annotate the large volume of data generated 
from the Second Life quests, we developed an an-
notation guide that defined and described the anno-
tation categories and decision rules annotators were 
to follow in categorizing the data units (following 
previous projects (Shaikh et al, 2010). Two stu-
dents were hired and trained for approximately 60 
hours, during which time they learned how to use 
the annotation tool and the categories and rules for 
the annotation process. After establishing a satisfac-
tory level of interrater reliability (average Krippen-
dorff?s alpha of all measures was <0.8. 
Krippendorff?s alpha accounts for the probability of 
177
chance agreement and is therefore a conservative 
measure of agreement), the two students then anno-
tated the 48 groups over a four-month period. It 
took approximately 230 hours to annotate the ses-
sions, and they assigned over 39,000 dialogue act 
tags. Annotators spent roughly 7 hours marking up 
the movements and chat messages per 2.5 hour 
quest session. 
 
 
Figure 9: A series of progressive moments in time portraying avatar entry into the Bank subzone 
  
 
 
Figure 10: Event Annotation Sub-Panel, currently showing the ?Moves Into Subzone? event from 
figure 9, as well as: ?Kenny follows Elliot in Vault?
5.1 The Annotated Corpus 
The current version of the annotated corpus consists 
of thousands of tagged messages including: 4,294 
action-directives, 17,129 assertion-opinions, 4,116 
information requests, 471 confirmation requests, 
394 offer-commits, 3,075 responses to information 
requests, 1,317 agree-accepts, 215 disagree-rejects, 
and 2,502 acknowledgements, from 30,535 pre-
split utterances (31,801 post-split). We also as-
signed 4,546 following events.  
6 Conclusion 
In this paper we described the successful imple-
mentation and use of our multi-modal annotation 
tool, RAT.  Our tool was used to accurately and 
simultaneously annotate over 30,000 messages and 
approximately 140 hours of action.  For each hour 
spent annotating, our annotators were able to tag 
approximately 170 utterances as well as 36 minutes 
of action. 
The annotators reported finding the tool highly 
functional and very efficient at helping them easily 
assign categories to the relevant data units, and that 
they could assign those categories without produc-
ing too many errors, such as accidentally assigning 
the wrong category or selecting the wrong avatar. 
The function allowing for the synchronized play-
back of the chat and movement data coupled with 
the 2D map increased comprehension of utterances 
178
and behavior of the players during the quest, im-
proving validity and reliability of the results. 
 
Acknowledgements  
This research is part of an Air Force Research 
Laboratory sponsored study conducted by Colorado 
State University, Ohio University, the University at 
Albany, SUNY, and Lockheed Martin. 
 
References  
Steven Bird, Kazuaki Maeda, Xiaoyi Ma and Haejoong 
Lee. 2001. annotation tools based on the annotation 
graph API.  In Proceedings of ACL/EACL 2001 
Workshop on Sharing Tools and Resources for Re-
search and Education. 
M. P. ?akir. 2009. The organization of graphical, narra-
tive and symbolic interactions. In Studying virtual 
math teams (pp. 99-140). New York, Springer. 
J. Carletta. 2007. Unleashing the killer corpus: experi-
ences in creating the multi-everything AMI Meeting 
Corpus. Language Resources and Evaluation Journal 
41(2): 181-190. 
Mark G. Core and James F. Allen. 1997. Coding dia-
logues with the DAMSL annotation scheme.  In Pro-
ceedings of AAAI Fall 1997 Symposium. 
Steve Cassidy and Jonathan Harrington.  2001. Multi-
level annotation in the Emu speech database man-
agement system.  Speech Communication, 33:61-77. 
S. C. Herring. 2003. Dynamic topic analysis of synchro-
nous chat. Paper presented at the New Research for 
New Media: Innovative Research Symposium. Min-
neapolis, MN. 
S. C. Herring and Nix, C. G. 1997. Is ?serious chat? an 
oxymoron? Pedagogical vs. social use of internet re-
lay chat. Paper presented at the American Association 
of Applied Linguistics, Orlando, FL. 
Samira Shaikh, Strzalkowski, T., Broadwell, A., Stro-
mer-Galley, J., Taylor, S., and Webb, N. 2010. MPC: 
A Multi-party chat corpus for modeling social phe-
nomena in discourse. Proceedings of the Seventh 
Conference on International Language Resources and 
Evaluation. Valletta, Malta: European Language Re-
sources Association. 
G. Stahl. 2009. The VMT vision. In G. Stahl, (Ed.), 
Studying virtual math teams (pp. 17-29). New York, 
Springer. 
Stephen Sutton, Ronald Cole,  Jacques De 
Villiers,  Johan Schalkwyk,  Pieter Vermeulen,  Mike 
Macon,  Yonghong Yan,  Ed Kaiser,  Brian Run-
Rundle,  Khaldoun Shobaki,  Paul Hosom,  Alex 
Kain,  Johan Wouters,  Dominic Massaro,  Michael 
Cohen. 1998. Universal Speech Tools: The CSLU 
toolkit. Proceedings of the 5th ICSLP, Australia. 
 
Jennifer Stromer-Galley and Martinson, A. 2009. Coher-
ence in political computer-mediated communication: 
Comparing topics in chat. Discourse & Communica-
tion, 3, 195-216. 
 
N. Yee and Bailenson, J. N. 2008. A method for longitu-
dinal behavioral data collection in Second Life. Pres-
ence, 17, 594-596. 
 
 
179
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 41?48,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Topical Positioning: A New Method for Predicting Opinion Changes in Conversation  Ching-Sheng Lin1, Samira Shaikh1, Jennifer Stromer-Galley1,2,  Jennifer Crowley1, Tomek Strzalkowski1,3, Veena Ravishankar1   1State University of New York - University at Albany, NY 12222 USA 2Syracuse University 3Polish Academy of Sciences clin3@albany.edu, sshaikh@albany.edu, tomek@albany.edu    Abstract 
In this paper, we describe a novel approach to automatically detecting and tracking discus-sion dynamics in Internet social media by fo-cusing on attitude modeling of topics. We characterize each participant?s attitude to-wards topics as Topical Positioning, employ Topical Positioning Map to represent the posi-tions of participants with respect to each other and track attitude shifts over time. We also discuss how we used participants? attitudes towards system-detected meso-topics to re-flect their attitudes towards the overall topic of conversation. Our approach can work across different types of social media, such as Twitter discussion and online chat room. In this article, we show results on Twitter data. 
1 Introduction The popularity of social networks and the new kinds of communication they support provides never before available opportunities to examine people behaviors, ideas, and sentiments in various forms of interaction. One of the active research subjects is to automatically identify sentiment, which has been adopted in many different applica-tions such as text summarization and product re-view. In general, people express their stances and rationalize their thoughts on the topics in social media discussion platform. Moreover, some of them explicitly or implicitly establish strategies to persuade others to embrace his/her belief. For ex-ample, in the discussion of the topic ?Should the legal drinking age be lowered to 18?, the partici-pants who are against it may state their views ex-plicitly and list negative consequences of lowering 
drinking age to 18 in an attempt to change opinions of those who appear to support the change. This phenomenon actually involves two research prob-lems which have been of great interest in Natural Language Processing: opinion identification and sociolinguistic modeling of discourse. The first problem can be addressed by traditional opinion analysis that recognizes which position or stance a person is taking for the given topics (So-masundaran and Wiebe, 2009). The second part requires modeling the sociolinguistic aspects of interactions between participants to detect more subtle opinion shifts that may be revealed by changes in interpersonal conversational dynamics. In this paper, we bring these two research avenues together and describe a prototype automated sys-tem that: (1) discovers each participant?s position polarities with respect to various topics in conver-sation, (2) models how participants? positions change over the course of conversation, and (3) measures the distances between participants? rela-tive positions on all topics. We analyzed discus-sions on Twitter to construct a set of meso-topics based on the persistence of certain noun phrases and co-referential expressions used by the partici-pants. A meso-topic is any local topic in conversa-tion referred to by a noun phrase and subsequently mentioned again at least 5 times via repetition, pronoun or synonym. Meso-topics do not neces-sarily represent actual topics of conversations, but certainly are important interactive handles used by the speakers. It is our hypothesis that meso-topics can be effectively used to track and predict polarity changes in speakers? positions towards the overall topic of conversation. Once the meso-topics and their polarities for each participant are determined, we can generate a topical positioning map (or net-work) (TPN) showing relative distances between 
41
participants based on all meso-topics in discourse. Comparing different snapshots of the TPN over time, we can observe how the group?s dynamic changes, i.e., how some participants move closer to one another while others drift apart in the discus-sion. In particular, we suggest that TPN changes can track and predict participants? changes of opin-ion about the overall topic of conversation. The remainder of this paper is organized as follows. In Section 2, we review related work. In Section 3, we describe the components of the pro-posed technique and the way they are used to im-plement the system. In Section 4, we discuss initial empirical studies, including data collection and evaluation. In final section, we present conclusions and some future work. 2 Related Work While systematic research on opinion tracking and influence in dialogues is a relatively new area of computational linguistics, related research includes automatic opinion mining and sentiments extrac-tion from text (Wiebe et al, 2005; Strapparava and Mihalcea, 2008), speech (Vogt et al, 2008) and social networking sites (Martineau and Finin, 2009). Much of the recent work was focused on automatic analysis of product reviews (books, movies, etc.) and extracting customers? opinions from them (Hu and Liu, 2004; David and Pinch, 2006; Zhuang et al, 2006). A typical approach is to count the number of ?opinion? words within a text window around the product names, possibly augmented with syntactic parsing to get dependen-cies right. An opinion mining application can ex-tract either full opinion sentences (Philip et al, 2003) or may generate a more structured represen-tation (Hu and Liu, 2004). Another recent applica-tion of sentiment analysis is ECO system (Effective Communication Online) (Small et al, 2010) that constructs a model of a community-wide sentiment towards certain common issues discussed in social media, particularly forums and open blogs. This model is then used to assess whether a new post would fit into the targeted community by comparing the sentiment polarities about the concepts in the message and in the model. Potential posters are then guided in ways to shape their communication so that it minimizes the num-ber of conflicting concept sentiments, while still preserving the intended message.  
Another related research domain is about modeling the social phenomena in discourse. (Strzalkowski et al, 2010, Broadwell et al, 2012) proposed a two-tier approach that relies on extract-ing observable linguistic features of conversational text to detect mid-level social behaviors such as Topic Control, Disagreement and Involvement. These social behaviors are then used to infer high-er-level social roles such as Leader and Influencer, which may have impact on how other participants? opinions form and change. 3 System Modules  In this section, we describe a series of modules in our system, which include meso-topic extraction, topical positioning and topical positioning map, and explain how we capture opinion shifts. 3.1 Meso-Topic Extraction Participants mention many ideas and subjects in dialogue. We call these Local Topics, which are any noun phrases introduced that are subsequently mentioned via repetition, synonym, or pronoun (Strzalkowski et al, 2010) by the same participant or different participants. Some local topics persist for only a couple of turns, others for much longer; some are closely relevant to the overall discussion, while others may appear to be digressions. We identify local topics, their first mentions and sub-sequent mentions, and track participants who make these mentions. Once local topics have been intro-duced into the dialogue we track their persistence as topic chains, through repetitions of the noun phrase as well as references via pronouns and the use of synonyms. Topic chains do not have to be continuous, they may contain gaps. The lengths of these gaps are also important to measures for some behaviors. Meso-topics are the most persistent lo-cal topics, topics that are widely cited through long stretches of discourse. A selection of meso-topics is closely associated with the task in which the dis-course participants are engaged. Short ?gaps? in the chain are permitted (up to 10 turns, to accom-modate digressions, obscure references, noise, etc.). Meso-topics can be distinguished from the local topics because the participants often make polar-ized statements about them. We use the Stanford part-of-speech tagger (Klein and Manning, 2003) to automatically detect nouns and noun phrases in dialogue and select those with subsequent men-
42
tions as local topics using a fairly simple pronoun resolution method based primarily on presence of specific lexical features as well as temporal dis-tance between utterances. Princeton Wordnet (Fellbaum et al, 2006) is consulted to identify synonyms and other related words commonly used in co-references. The local topics that form suffi-ciently long co-reference chains are designated as meso-topics. 3.2 Topical Positioning Topical Positioning is defined as the attitude a speaker has towards the meso-topics of discussion. Speakers in a dialogue, when discussing issues, especially ones with some controversy, will estab-lish their attitude on each topic, classified as for, against, or neutral/undecided. In so doing, they establish their positions on the issue or topic, which shapes the agenda of the discussion and also shapes the outcomes or conclusions of the discus-sion. Characterizing topical positioning allows us to see the speakers who are for, who are against, and who are neutral/undecided on a given topic or issue. To establish topical positioning, we first identify meso-topics that are present in a discourse. For each utterance made by a speaker on a meso-topic we then establish its polarity, i.e., if this ut-terance is ?for? (positive) or ?against? (negative), or neutral on the topic. We distinguish three forms of meso-topic valuation that may be present: (a) ex-press advocacy/disadvocacy, when the valuation is applied directly to the topic (e.g., ?I?m for Carla?); (b) supporting/dissenting information, when the valuation is made indirectly by offering additional information about the topic (e.g., ?He's got experi-ence with youngsters.?); and (c) express agree-ment/disagreement with a polarized statement made by another speaker. The following measures of Topical Position-ing are defined: Topic Polarity Index, which estab-lishes the polarity of a speaker?s attitude towards the topic, and Polarity Strength Index, which measures the magnitude of this attitude.   [Topic Polarity Index (TPX)] In order to detect the polarity of Topical Positioning on meso-topic T, we count for each speaker: - All utterances on T using statements with po-larity P applied directly to T using appropriate 
adverb or adjective phrases, or when T is a di-rect object of a verb. Polarities of adjectives and adverbs are taken from the expanded ANEW lexicon (Bradley and Lang, 1999). - All utterances that offer information with po-larity P about topic T. - All responses to other speakers? statements with polarity P applied to T. In the Twitter environment (and the like), for now we in-clude a re-tweet in this category. Given these counts we can calculate TPX for each speaker as a proportion of positive, negative and neutral polarity utterances made by this speaker about topic T. A speaker whose utterances are overwhelmingly positive (80% or more) has a pro-topic position (TPX = +1); a speaker whose utter-ances are overwhelmingly negative takes an against-topic position (TPX = ?1); a speaker whose utterances are largely neutral or whose utterances vary in polarity, has a neutral/undecided position on the topic (TPX = 0).  [Polarity Strength Index (PSX)] In addition to the valence of the Topical Positioning, we also wish to calculate its strength. To do so, we calculate the proportion of utterances on the topic made by each speaker to all utterances made about this topic by all speakers in the discourse. Speakers, who make most utterances on the topic relative to other speakers, take a stronger position on this topic. PSX is measured on a 5-point scale corresponding to the quintiles in normal distribution.   Topical Positioning Measure (TPM)  In order to establish the value of Topical Position-ing for a given topic we combine the values of TPX*PSX. Topical Positioning takes values be-tween +5 (strongest pro) to 0 (neutral/undecided) to ?5 (strongest against). For example, a speaker who makes 25% of all utterances on the topic ?Carla? (group mean is 12%) and whose most statements are positive, has the strongest pro Topi-cal Positioning on Carla: +5 (for fifth quintile on the positive side). 3.3 Topical Positioning Map (TPN) Given the combined values of TPM for each par-ticipant in a group, we can calculate distances be-tween the speakers on each meso-topic as well as on all meso-topics in a conversation. For meso-
43
topics (t1, ? tN), the distance is calculated using a cosine between speakers? ?vectors? (TPMt1(A) ? TPMtN(A)) and (TPMt1(B) ? TPMtN(B)). Specifi-cally, we use (1-Cosine(V1, V2)) to represent dis-tance between node V1 and V2 in the network, where the range becomes 0 to 2.  With the aid of TPN, we can detect the opin-ion shifts and model the impact of speakers with specific social roles in the group, which in our case is the influencer. An influencer is a group partici-pant who has credibility in the group and introduc-es ideas that others pick up on or support. An influencer model is generated from mid-level soci-olinguistic behaviors, including Topic Control, Disagreement and Involvement (Shaikh et al, 2012). In order to calculate effect of the influencer on a group, we track changes in the TPN distances between speakers, and particularly between the influencer and other speakers. We want to know if the other speakers in the group moved closer to or further away from the influencer, who may be promoting a particular position on the overall sub-ject of discussion. Our hypothesis is that other par-ticipants will move closer (as a group, though not necessarily individually) to an influential speaker. We may also note that some speakers move closer while others move away, indicating a polarizing effect of an influential speaker. If there is more than one influencer in the group these effects may be still more complex. 4 Data Collection and Experiment  Our initial focus has been on Twitter discussions which enable users to create messages, i.e., ?tweets?. There are plenty of tweet messages gen-erated all the time and it is reported that Twitter has surpassed 400 million tweets per day. With the Twitter API, it is easy to collect those tweets for research, as the communications are considered public. However, most of data obtained publicly is of limited value due to its complexity, lack of fo-cus, and inability to control for many independent variables. In order to derive reliable models of conversational behavior that fulfill our interests in opinion change, we needed a controlled environ-ment with participants whose initial opinions were known and with conversation reasonably focused on a topic of interest. To do so, we recruited partic-ipants for a two-week Twitter debates on a variety of issues, one of the topics was ?Should the mini-
mum legal drinking age be lowered to 18?? We captured participants? initial positions through sur-veys before each debate, and their exit positions through surveys after the debate was completed two weeks later. The surveys were designed to col-lect both the participants? opinions about the over-all topic of conversation as well as about the roles they played in it. These data were then compared to the automatically computed TPN changes. 4.1 Data Collection To obtain a suitable dataset, we conducted two groups of controlled and secured experiments with Twitter users. The experiment was specially de-signed to ensure that participants stay on topic of discussion and that there was a minority opinion represented in the group. We assigned the same overall topic for both groups: ?lowering the drink-ing age from 21 to 18?. Before the discussion, the participants completed an 11-question survey to determine their pre-discussion attitudes toward overall topic. One participant with the minority opinion was then asked to act as an influencer in the discussion, i.e., to try to convince as many people as possible to adopt his or her position. Af-ter the discussion, the participants were asked the same 11 questions to determine if their positions have changed. All 11 questions probed various aspects of the overall topic, thus providing a relia-ble measure of participant?s opinion. All responses were on a 7-point scale from ?strongly agree? to ?strongly disagree?. The orientation of individual questions vs. the overall topic was varied to make sure that the participants did not mechanically fill their responses. Some of the questions were:  (1) Lowering the drinking age to 18 would make alcohol less of a taboo, making alcohol consump-tion a more normalized activity to be done in mod-eration.   +3 strongly agree ----- -3 strongly disagree (2) 18 year olds are more susceptible to binge drinking and other risky/irresponsible behaviors than people who are 21 and older. -3 strongly agree ----- +3 strongly disagree  (note reversed polarity) The basic statistical information about the two ex-perimental groups is given in Table 1 and the tweet distribution of each participant in Group-1 is shown in Figure 1. Participants are denoted by a 
44
two-letter abbreviation (WS, EP and so on). The current data set is only a fraction of a larger cor-pus, which is currently under development. Addi-tional datasets cover a variety of discussion topics and involve different groups of participants.  Group # participants # tweets Influencer 1 20 225 WS 2 14 222 EP Table 1: Selected details of two experimental groups.   
 Figure 1: Tweet distribution for each participant in Group-1 where participants with asterisk are against ?lowering drinking age?.  As we would like to know the participants? pre- and post-discussion attitudes about the overall top-ic, we used the responses on 11 survey questions to calculate how strongly participants feel on the overall topic of discussion. Each question is given on a seven-point scale ranging from ?+3? to ?-3?, where ?+3? implies strongly agree to keep drinking age at 21 and ?-3? means strongly disagree. Posi-tions of participants are determined by adding the scores of the 11 questions according to their re-sponses on pre- or post- discussion questionnaires. Figure 2 is an example of pre-discussion responses for two participants in Group-1. WS largely agrees that drinking age should be kept at 21 whereas EK has an opposing opinion. The pre- and post-discussion attitudes of participants in Group-1 to-wards the overall topic are shown in Figure 3.  
 Figure 2: Pre-discussion survey scores of WS and EK. 
Subsequently, we computed relative pre-discussion attitude distance between each participant and the influencer based on the pre-discussion surveys and their post-discussion attitude distance based on the post-discussion surveys. We normalized these dis-tances to a [0, 2] interval to be consistent with co-sine distance computation scale used in the TPN module. The changes from pre-discussion attitude distance to post-discussion attitude distance based on the surveys are considered the gold standard against which the system-computed TPN values are measured. As shown in Figure 4(a), the pre-discussion distance between WS and EK is 1.43 (first bar) and the post-discussion distance is 0.07 (second bar), which implies their positions on the overall topic moved significantly closer. We also note that WS?s position did not change much throughout the discussion (Figure 3). This was just as we expected since WS was our designated influ-encer, and this fact was additionally confirmed in the post survey: in response to the question ?Who was the influencer in the discussion?? the majority of participants selected WS. The post survey re-sponses from the other group also confirmed our selected influencer. In addition, we used the auto-mated DSARMD system (Strzalkowski et al, 2013) to compute the most influential participants in each group, and again the same people were identified.  
 Figure 3: Pre- and post-discussion attitudes of partici-pants in Group-1 where the left bar of the participant is their pre-discussion attitude and right bar of the partici-pant is their post-discussion attitude. 
45
  Figure 4: (a) Relative position change between speakers WS (the influencer) and EK based on surveys and au-tomatically computed TPN distance. The first bar in each par corresponds to their pre-discussion distance and second bar is post-discussion distance. We note that TPN predicts correctly that WS and EK move closer together. (b) Relative position change between partici-pants WS and BC.   
4.2 Experiment After detailed analysis of participants? opinion be-fore and after the discussion, two twitter discus-sions are run through our system to extract the required information in order to compute topical positioning as explained in section 3. In Group-1, ten meso-topics were generated by our system (in-cluding, e.g., ?drinking age?, ?teens? and ?alco-hol?). Each participant?s polarity associated with these meso-topics was computed by our system to form ten-dimensional topical positioning vectors for Group-1. In our experiment, we used the first quarter of discussion to compute initial topical po-sitioning of the group and last-three quarters to compute the final topical positioning. Once the pre- and post-topical positioning were determined, the topical positioning map between participants was calculated accordingly, i.e., pre- and post-TPN. We used the first quarter of discussion for the initial TPN because we required a sufficient amount of data to compute a stable measure; how-ever, we expected it would not fully represent par-ticipants? initial positions. Nonetheless, we should still see the change when compared with post-TPN, which was computed on the last three-quarters of the discussion. In order to detect the opinion shifts and also to measure the effect of the influencer on a group, we tracked the changes in the TPN with respect to the influencer. As shown in Figure 4(a), the pre-TPN between WS and EK is 1.33 (third bar) and post-TPN is 0.72 (fourth bar). Hence, the system determines that their opinions are moving closer which conforms to the survey results. Figure 4(b) is another example of WS and BC that system result shows the same tendency as the survey result. The pre-discussion distance between WS and BC is 1.87 (first bar) and the post-discussion distance is 1.42 (second bar), which implies their positions on the overall topic moved closer after discussion. In system detection, the pre-TPN between is 1.56 (third bar) and post-TPN is 1.02 (fourth bar), which also concludes their attitudes are closer. An-other examples showing that speaker moved away from influencer are in Figure 5(a) and 5(b). Ac-cording to the survey, the pre-discussion attitude distance between WS and CC is 0.62 (first bar) and post-discussion attitude distance is 1.35 (second bar), which implies their positions diverged after the discussion. Our system determined pre-TPN between WS and CC is 1.0 (third bar) and post-
46
TPN is 1.29 (fourth bar), which shows their diver-gence.  
Figure 5: (a) Relative position change between WS and CC based on surveys and TPN. (b) Relative position change between participants WS and RF. We note that RF moves away from WS, which is correctly predicted by TPN.   In a separate exercise we also explored differ-ent parts of the Twitter session to compute pre-TPN and post-TPN, in addition to the ? vs. ? split discussed above. In particular, we computed TPN distances between speakers at first ? vs. second ?, 
first ? vs. last ?, first ? vs. last ?, etc. Experiment results show that using the first quarter of discus-sion as initial topical positioning and last quarter as final topical positioning (? vs. ?) produces the most accurate prediction of opinion changes for all group participants: 87.5% in Group-1 and 76% in Group-2.  We should also note here that there is no specific correlation between the meso-topics and the overall topic other than the meso-topics arise spontaneously in the conversation. The set of me-so-topics in the second discussion on the same top-ic was different than the in the first discussion. In particular, meso-topics are not necessarily correlat-ed with the aspects of the overall topic that are ad-dressed in the surveys. Nonetheless, the TPN changes appear to predict the changes in surveys in both discussions. At this time the results is indica-tive only. Further experiments need to be run on additional data (currently being collected) to con-firm this finding. 5 Conclusion In this paper, we described an automated approach to detect participant?s Topical Positioning and cap-ture the opinion shifts by Topical Position Maps. This work is still in progress and we intend to pro-cess more genres of data, including Twitter and on-line chat, to confirm effects seen in the data we currently have. The future work should be able to account for the relationship between meso-topic and overall topic (i.e., supporting meso-topic means for or against overall topic). A potential so-lution could be determined by aligning with TPN of influencers who are known strongly pro- or against- overall topic. Another avenue of future work is to apply proposed model on virtual chat-room agent to guide the discussion and change par-ticipants? attitudes.  References  Bradley, M. M., & Lang, P. J. 1999. Affective norms for English words (ANEW): Instruction manual and af-fective ratings(Tech. Rep. No. C-1). Gainesville, FL: University of Florida, The Center for Research in Psychophysiology. Broadwell, George, Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Taylor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu, and Nick Webb. "Modeling Socio-Cultural Phenomena in Dis-
47
course." Journal of Natural Language Engineering (2012): 1-45. David, S., and Pinch, T. J. 2006. Six degrees of reputa-tion: The use and abuse of online review and recom-mendation systems. First Monday. Special Issue on Commercial Applications of the Internet.  Fellbaum, C., B. Haskell, H. Langone, G. Miller, R. Poddar, R. Tengi and P. Wakefield. 2006. WordNet 2.1. Hu, M., and Liu, B. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI, 755?760. Klein, D., & Manning, C. D. 2003. Accurate unlexical-ized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguis-tics-Volume 1 , 423-430. Association for Computa-tional Linguistics. Martineau, J., & Finin, T. 2009. Delta tfidf: An im-proved feature space for sentiment analysis. In Pro-ceedings of the 3rd AAAI International Conference on Weblogs and Social Media, 258-261. Philip Beineke, Trevor Hastie, Christopher Manning and Shivakumar Vaithyanathan 2003. An exploration of sentiment summarization. In Proceedings of AAAI, 12-15. Shaikh, Samira, et al2012. Modeling Influence in Online Multi-party Discourse. Cloud and Green Computing (CGC), 2012 Second International Con-ference on. IEEE. Small, S., Strzalkowski, T. and Webb, N. 2010. ECO: Effective Communication Online. Technical Report ILS-015, University at Albany, SUNY Somasundaran, S., & Wiebe, J. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP. Strapparava, C., and Mihalcea, R. 2008. Learning to Identify Emotions in Text. In Proceedings of the ACM Conference on Applied Computing ACM-SAC. Strzalkowski, T.; Broadwell, G. A.; Stromer-Galley, J.; Shaikh, S.; Taylor, S.; and Webb, N. 2010. Modeling socio-cultural phenomena in discourse. In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics, 1038-1046. Strzalkowski, T., Samira Shaikh, Ting Liu, George Aa-ron Broadwell, Jennifer Stromer-Galley, Sarah M. Taylor, Veena Ravishankar, Umit Boz, Xiaoai Ren: Influence and Power in Group Interactions. SBP 2013: 19-27 Vogt, T., Andre?, E., & Bee, N. 2008. EmoVoice?A framework for online recognition of emotions from voice. Perception in Multimodal Dialogue Systems, 188-199. Wiebe, J., Wilson, T., and Cardie, C. 2005. Annotating expressions of opinions and emotions in language. 
Journal of Language Resources and Evaluation 39(2-3):165?210 Zhuang, L., Jing, F., Zhu, X. Y., & Zhang, L. 2006. Movie review mining and summarization. In Confer-ence on Information and Knowledge Management: Proceedings of the 15 th ACM international confer-ence on Information and knowledge management, 43-50. 
48
