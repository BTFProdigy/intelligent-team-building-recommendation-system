COOPML: Towards Annotating Cooperative Discourse
Farah Benamara, Ve?ronique Moriceau, Patrick Saint-Dizier
IRIT
118 route de Narbonne
31062 Toulouse cedex France
benamara, moriceau, stdizier@irit.fr
Abstract
In this paper, we present a preliminary version of
COOPML, a language designed for annotating co-
operative discourse. We investigate the different lin-
guistic marks that identify and characterize the dif-
ferent forms of cooperativity found in written texts
from FAQs, Forums and emails.
1 What are cooperative responses and
why annotate them ?
Grice (Grice, 1975) proposed a number of maxims
that describe various ways in which speakers are en-
gaged in a cooperative conversation. Human con-
versations are governed by implicit rules, used and
understood by all conversants. The contents of a re-
sponse can be just direct w.r.t. the question literal
contents, but it can also go beyond what is normally
expected, in a relevant way, in order to meet the
questioner?s expectations. Such a response is said
to be cooperative.
Following these maxims and related works, e.g.
(Searle, 1975), in the early 1990s, a number of
forms of cooperative responses were identified.
Most of the efforts in these studies and systems fo-
cussed on the foundations and on the implementa-
tion of reasoning procedures (Gal, 1988), (Minock
et al., 1996), while little attention was paid to
question analysis and NL response generation. An
overview of these systems can be found in (Gaster-
land et al, 1994) and in (Webber et al., 2002),
based on works by (Hendrix et al., 1978), (Kaplan,
1982), (Mays et al., 1982), among others. These
systems include e.g. the identification of false pre-
suppositions and various types of misunderstand-
ings found in questions. They also include rea-
soning schemas based e.g. on constant relaxation
to provide approximate or alternative, but relevant,
answers when the direct question has no response.
Intensional reasoning schemas can also be used to
generalize over lists of basic responses or to con-
struct summaries.
The framework of Advanced Reasoning for
Question Answering (QA) systems, as described in
a recent road map, raises new challenges since an-
swers can no longer be only directly extracted from
texts (as in TREC) or databases, but requires the use
of a domain knowledge base, including a concep-
tual ontology, and dedicated inference mechanisms.
Such a perspective, obviously, reinforces and gives
a whole new insight to cooperative answering. For
example, if one asks 1:
Q4: Where is the Borme les Mimosas cinema ?
if there are no cinema in Borme les Mimosas, it can
be responded:
R4: There is none in Borme, the closests are in
Londe (8kms) and in Hyeres (20kms),
where close-by alternatives are proposed, involving
relaxing Borme, identified as a village, into close-by
villages or towns that respond to the question, eval-
uating proximity, and finally sorting the responses,
e.g. by increasing distance from Borme. This sim-
ple example shows that, if a direct response can-
not be found, several forms of knowledge, reason-
ing schemas and strategies need to be used. This is
one of the major challenges of advanced QA. An-
other challenge, not yet addressed, is the generation
of the response in natural language.
Our first aim is to study, via corpus annotations,
how humans deploy cooperative behaviours and
procedures, by what means, and what is the form of
the responses provided. Our second aim is to con-
struct a linguistically and cognitively adequate for-
mal model that integrates language, knowledge and
inference aspects involved in cooperative responses.
Our assumption is then that an automatic coopera-
tive QA system, although much more stereotyped
than any natural system, could be induced from nat-
ural productions without loosing too much of the
cooperative contents produced by humans.
From that point of view, the results presented in
this paper establish a base for investigating coop-
erativity empirically and not only in an abstract and
1Our corpora are in French, but, whenever possible we only
give here English glosses for space reasons
introspective way. Our goal is to get a kind of empir-
ical testing and then model for cooperative answer-
ing, to get clearer ideas on the structure of coopera-
tive discourse, the reasoning processes involved, the
types of knowledge involved and the NL expression
modes.
2 Related work
Discourse annotation is probably one of the most
challenging domains that involves almost all aspects
of language, from morphology to pragmatics. It is
of much importance in a number of areas, besides
QA, such as MT or dialogue. A number of discourse
annotation projects (e.g. PALinkA (Orasan, 2003),
MULI (Baumann et al., 2004), DiET (Netter et
ali. 1998), MATE (Dybkjaer et al., 2000)) mainly
deal with reference annotations (be they pronom-
inal, temporal or spatial), which is clearly a ma-
jor problem in discourse. Discourse connectives
and their related anaphoric links and discourse units
are analyzed in-depth in PDTB (Miltasakaki et al.
2004), a system now widely used in a number of
NL applications. RST discourse structures are also
identified in the Treebank corpora.
All these projects show the difficulty to annotate
discourse, the subjectivity of the criteria for both the
bracketing and the annotations. Annotation tasks
are in general labor-intensive, but results in terms of
discourse understanding are rewarding. Customisa-
tion to specific domains or forms of discourse and
the definition of test-suites are still open problems,
as outlined in PDTB and MATE.
Our contribution is more on the pragmatic side of
discourse, where there is little work done, probably
because of the complexity of the notions involved
and the difficulty to interpret them. Let us note
(Strenston, 1994) that investigates complex prag-
matic functions such as performatives and illocu-
tionary force. Our contribution is obviously inspired
by abstract and generic categorizations in pragmat-
ics, but it is more concrete in the sense that it aims
at identifying precise cooperative functions used in
everyday life in large-public applications. In a first
stage, we restrict ourselves to written QA pairs such
as FAQ, Forums and email messages, which are
quite well representative of short cooperative dis-
courses (see 3.1).
3 A typology of cooperative functions
The typology below clearly needs further testing,
stabilization and confirmation by annotators. How-
ever, it settles the main lines of cooperative dis-
course structure.
3.1 Typology of corpora
To carry out our study and subsequent evaluations,
we considered three typical sources of coopera-
tive discourses: Frequently Asked Questions (FAQ),
Forums and email question-answer pairs (EQAP),
these latter obtained by sending ourselves emails to
relevant services (e.g. for tourism: tourist offices,
airlines, hotels). The initial study was carried out on
350 question-answer pairs. Note that in the tourism
domain, FAQ are rather specific: they are not ready-
made, prototypical questions. They are rather un-
structured sets of questions produced e.g. via email
by standard users. From that point of view, they are
of much interest to us.
We have about 50% pairs coming from FAQ, 25%
from Forums and 25% from EQAP. The domains
considered are basically large-public applications:
tourism (60%, our implementations being based on
this application domain), health (22%), sport, shop-
ping and education. In all these corpora, no user
model is assumed, and there is no dialogue: QA
pairs are isolated, with no context. This is basi-
cally the type of communication encountered when
querying the Web. Our corpus is only composed of
written texts, but these are rather informal, and quite
close in style to spoken QA pairs.
FAQ, Forum and EQAP cooperative responses
share several similarities, but have also some dif-
ferences. Forums have in general longer responses
(up to half a page), whereas FAQ and EQAP are
rather short (from 2 to 12 lines, in general). FAQ
and Forums deal with quite general questions while
EQAP are more personal. EQAP provided us with
a very rich material since they allowed us to get re-
sponses to queries in which we have deliberately in-
troduced various well identified errors and miscon-
ceptions. In order to have a better analysis of how
humans react, we sent those questions to different,
closely related organizations (e.g. sending the same
ill-formed questions to several airlines). FAQ, Fo-
rums and EQAP also contain several forms of adver-
tising, and metalinguistic parameters outlining e.g.
their commercial dimensions.
From the analysis of 350 of QA pairs, taking into
account the formal pragmatics and artificial intelli-
gence perspectives, we have identified the typology
presented below, which defines the first version of
COOPML.
3.2 Cooperative discourse functions
We structure cooperative responses in terms of co-
operative functions, which are realized in responses
by means of meaningful units (MU). An MU is the
smallest unit we consider at this level; it conveys a
minimal, but comprehensive and coherent fragment
of information. In a response, MUs are connected
by means of transition units (TU), which are intro-
ductory or inserted between meaningful units. TUs
define the articulations of the cooperative discourse.
In a cooperative discourse, we distinguish three
types of MU: direct responses (DR), cooperative
know-how (CSF) and units with a marginal useful-
ness (B) such as commentaries (BC), paraphrases
(BP), advertising, useless explanations w.r.t. to the
question. These may have a metalinguistic force
(insistence, customer safety, etc) that we will not
examine in this paper. DR are not cooperative
by themselves, but they are studied here because
they introduce cooperative statements. Let us now
present a preliminary typology for DR and CSF, be-
tween parentheses are abbreviations used as XML
labels.
Direct responses (DR): are MUs corresponding
to statements whose contents can be directly elabo-
rated from texts, web pages, databases, etc., possi-
bly via deduction, but not involving any reformula-
tion of the original query. DR include the following
main categories:
? Simple responses (DS): consisting of yes/no
forms, modals, figures, propositions in either
affirmative or negative form, that directly re-
spond the question.
? Definitions, Descriptions (DD): usually text
fragments defining or describing a concept, in
response to questions e.g. of the form what is
?concept??.
? Procedures (DP): that describe how to realize
something.
? Causes, Consequences, Goals (DCC): that usu-
ally respond to questions in Why/ How?.
? Comparisons and Evaluations (DC): that re-
spond to questions asking for comparisons or
evaluations.
This classification is closely related to a typology of
questions defined in (Lehnert, 1978).
Responses involving Cooperative Know-how
(CSF) are responses that go beyond direct answers
in order to help the user when the question has no
direct solution or when the question contains a mis-
conception of some sort. These responses reflect
various forms of know-how deployed by humans.
We decompose them into two main classes: Re-
sponse Elaboration (ER) and Additional Infor-
mation (CR). The first class includes response units
that propose alternative responses to the question
whereas the latter contains a variety of complements
of information, which are useful but not absolutely
necessary. ER are in a large part inspired from spe-
cific research in Artificial Intelligence such as con-
straint relaxation and intensional calculus.
Response elaboration (ER) includes the follow-
ing MUs:
? Corrective responses (CC): that explain why a
question has no response when it contains a
misconception or a false presupposition (for-
mally, a domain integrity constraint or a factual
knowledge violation, respectively), For exam-
ple: Q5: a chalet in Corsica for 15 persons?
has no solution, a possible response is:
R5a: Chalets can accomodate a maximum of
10 persons in Corsica.
? Responses by extension (CSFR): propose al-
ternative solutions by relaxing a constraint in
the original question. There are several forms
of relaxations, reported in (Benamara et al
2004a), which are more subtle than those de-
veloped in artificial intelligence. For example,
we observed relaxation on cardinality, on sis-
ter concepts or on remote concepts with similar
prominent properties, not studied in AI, where
relaxation operates most of the time on the ba-
sis of ancestors.
Response R5a above can then be followed by
CSFRs of various forms such as: R5b: we can
offer (1) two-close-by chalets for a total of 15
persons, or
(2) another type of accomodation in Corsica:
hotel or pension for 15 persons.
Case (1) is a relaxation on cardinality (dupli-
cation of the resource) while (2) is a relaxation
that refers to sisters of the concept chalet.
? Intensional responses (CSFRI): tend to abstract
over possibly long enumerations of extensional
responses in order to provide a response at the
best level of abstraction, which is not necessar-
ily the highest. For example, Q6: How can I
get to Geneva airport ? has the following re-
sponse:
R6a: Taxis, most buses and all trains go
to Geneva airport. This level is prefered
to the more general but less informative re-
sponse R6b: Most public transportations go to
Geneva airport.
? Indirect responses (CSFI): provide responses
which are not direct w.r.t. the question (but
which may have a direct response), e.g.: is
your camping close to the highway?, can be
indirectly, but cooperatively answered:
yes, but that highway is quiet at night.. A di-
rect response would have said, e.g.: yes, we are
only 50 meters far from the highway, meaning
that the camping is of an easy access.
? Hypothetical responses (CSFH): include re-
sponses based on an hypothesis. Such re-
sponses are often related to incomplete ques-
tions, or questions which can only be partly
be answered for various reasons such as lack
of information, or vague information w.r.t the
question focus. In this case, we have a QA pair
of the form: Q7: Can I get discounts on train
tickets ? R7: You can get a discount if you are
less than 18 years old or more than 65, or if
you are travelling during week-ends.
? Clustered, case or comparative responses
(CSFC): which answer various forms of ques-
tions e.g. with vague terms (e.g. expensive, far
from the beach). For example, to Q8: is the ho-
tel Royal expensive? it is answered: R8: for its
category (3*) it is expensive, you can find 4*
hotels at the same rate.
The most frequent forms of responses are CSFR,
CSFI, CSFC, CSFRI; the two others (CC and
CSFH) are mainly found in email QA.
Additional Information units (CR) contain the
following cases:
? precisions of various forms, that deepen the re-
sponse (AF): this ?segment? or ?continuum? of
forms ranges from minor precisions and gen-
eralizations to elaborated comments, as in Q9:
Where can I buy a hiking trail map of Mount
Pilat ? which has the response R9 that starts
by an AF: R9: The parc published a 1:50 000
map with itineraries,... this map can be bought
at bookshops....
? restrictions (AR): restrict the scope of a re-
sponse, e.g. by means of conditions: Q10: Do
you refund tickets in case of a strike ? R10:
yes, a financial compensation is possible pro-
vided that the railway union agrees....
? warnings (AA): warn the questioner about
possible problems, annoyances, dangers, etc.
They may also underline the temporal versatil-
ity of the information, as it is often the case for
touristic resources (for example, hotel or flight
availability),
? justifications (AJ): justify a negative, unex-
pected or partial response: Q11: Can I be re-
funded if I loose my rail pass ?, R11: No, the
rail pass fare does not include any insurance
against loss or robbery.
? concessives (AC): introduce the possibility of
e.g. exceptions or specific treatments: Chil-
dren below 12 are not allowed to travel unac-
companied, however if a passenger is willing
to take care about him....
? suggestions - alternatives - counter-proposals
(AS): this continuum of possibilities includes
the proposition of alternatives, more or less
marked, when the query has no answer, in par-
ticular via the above ER. Q12: Can I pay the
hotel with a credit card?, R12: yes, but it is
preferable to have cash with you: you?ll get a
much better exchange rate and no commission.
The different MU have been designed with no
overlap, it is however clear that there may have
some forms of continuums between them. For ex-
ample, CSFR, although more restricted, may be
viewed as an AS, since an alternative, via relaxation,
is proposed. We then would give preference to the
CSF group over the CR, because they are more pre-
cise.
A response does not involve more, in general,
than 3 to 4 meaningful units. Most are linearly or-
ganized, but some are also embedded. At the form
level, response units of CSF (ER and CR) have
in general one or a combination of the following
forms: adverb or modal (RON), proposition (RP),
enumeration (RE), sorted response (via e.g. scalar
implicature) (RT), conditionals (RC) or case struc-
ture (RSC). These forms may have some overlap,
e.g. RE and RT.
3.3 Annotating Cooperative Discourse: a few
illustrations
Fig. 1 (next page) presents three examples anno-
tated with COOPML.
3.4 Identifying cooperative response units
The question that arises at this stage is the existence
of linguistic markers that allow for the identifica-
tion of these response units. Besides these mark-
ers, there are also constraints on the organization
of the cooperative discourse in meaningful units.
These are essentially co-occurrence, incompatibil-
ity and precedence constraints. Finally, it is possi-
ble to elaborate heuristics that give indications on
the most frequent combinations to improve MU au-
tomatic identification.
In the following subsections we first present a ty-
pology for MU delimitation, then we explain how
direct responses (DS) are identified, mainly, via the
Discourse level:
Q1: Can we buy drinking water on the Kilimandjaro ?
R1: < DS > yes < /DS >, < BP > drinking water can be bought < /BP >, < CSP >< AA > but fares
are higher than in town, up to 2USD < /AA > . < AR > It is however not allowed to bring much water from
the city with you < /AR >< /CSP >.
Q2: Is there a cinema in Borme ?
R2: < DS >No< /DS >, < CSFR > the closest cinema is at Londes (8 kms) or at Hyeres
(< AF >Cinema Olbia< /AF > at 20 kms).< /CSFR >
Q3: How can I get to the Borme castle ?
R3: < DS > You must take the GR90 from the old castle: < AF > walking distance: 30 minutes < /AF ><
/DS >. < AJ > There is no possibility to get there by car.< /AJ >
Form level:
R2: < RON > No, < /RON > < RE >< RT > The closest cinema is at Londes (8kms) or at Hyeres
(cinema Olbia at 20 kms) < /RT >< /RE >.
Figure 1: Discourse annotation
domain ontology whose structure and contents is
presented. We end the section by the linguistic
marks that identify a number of additional informa-
tion units (CR).
3.4.1 Typology of MU delimitators
Identifying meaningful response units consists in
two tasks: exploring linguistic criteria associated
with each form of cooperative response unit and
finding the boundaries of each unit. Cooperative
discourse being in general quite straightforward, it
turns out that most units are well delimited natu-
rally: about 70% of the units are single, complete
sentences, ending by a dot. The others are either
delimited by transition units TU such as connectors
(about 20%) or by specific signs (e.g. end of enu-
merations, punctuation marks). Delimiting units is
therefore in our perspective quite simple (it may not
be so in e.g. oral QA or dialogues).
3.4.2 Identification of direct responses (DS) via
the domain ontology
The identification (and the production) of a num-
ber of cooperative functions (e.g. relaxation, inten-
sional responses, direct responses) rely heavily on
ontological knowledge.
Let us present first the characteristics of the
ontology required in our approach. It is basically
a conceptual ontology where nodes are associated
with concept lexicalizations and essential proper-
ties. Each node is represented by the predicate :
onto-node(concept, lex, properties)
where concept has properties and lexicalisations
lex. Most lexicalisations are entries in the lexicon
(except for paraphrases), where morphological and
grammatical aspects are described. For example,
for hotel, we have (coded in Prolog):
onto-node(hotel,
[[hotel], [residence, hoteliere]],
[night-rate, nb-of-rooms,
facilities]) .
There are several well-designed public domain
ontologies on the net. Our ontology is a synthesis
of two existing French ontologies, that we cus-
tomized: TourinFrance (www.tourinfrance.net)
and the bilingual (French and English) the-
saurus of tourism and leisure activities
(www.iztzg.hr/indokibiblioteka/THESAUR.PDF)
which includes 2800 French terms. We manually
integrated these ontologies in WEBCOOP (Bena-
mara et al 2004a) by removing concepts that are
either too specific (i.e. too low level), like some
basic aspects of ecology or rarely considered, as e.g.
the economy of tourism. We also removed quite
surprising classifications such as sanatorium under
tourist accommodation. We finally reorganized
some concept hierarchies, so that they ?look? more
intuitive for a large public. Finally, we found that
some hierarchies are a little bit odd, for example,
we found at the same level accommodation capac-
ity and holiday accommodation whereas, in our
case, we consider that capacity is a property of the
concept tourist accommodation.
We have, at the moment, 1000 concepts in our
tourism ontology which describe accommodation
and transportation and a few other satellite elements
(geography, health, immigration). Besides the tra-
ditional ?isa? relation, we also coded the ?part-of?
relation. Synonymy is encoded via the list of lexi-
calizations.
Direct responses (DS) are essentially character-
ized by introductory markers like yes/no/this is pos-
sible and by the use of similar terms as those given
in the question (55% of the cases) or by various lex-
icalizations of the question terms, studied in depth
in (Benamara et al 2004b). An obvious situation is
when the response contains a subtype of the ques-
tion focus: opening hours of the hotel ? l?hotel
vous acceuille 24h sur 24 (approx. hotel welcomes
you round the clock). In terms of portability to other
domains than tourism, note that the various terms
used can be identified via the ontology: synonyms,
sisters, subtypes.
3.4.3 Linguistic marks
In this section, for space reasons, we explore only
three typical CR: justifications (AJ), restrictions
(AR) and warnings (AA). These MUs are charac-
terized by markers which are general terms, domain
independent for most of them. The study of these
marks for French reveals that there is little marker
overlap between units. Markers have been defined
in a first stage from corpus analysis and then gener-
alized to similar terms in order to have a larger basis
for evaluation. We also used, to a limited extend,
a bootstrapping technique to get more data (Ravin-
chandran and Hovy 2002), a method that starts by
an unambiguous set of anchors (often arguments of
a relational term) for a target sense. Searching text
fragments on the Web based on these anchors then
produces a number of ways of relating these an-
chors.
Let us now characterize linguistic markers for
each of these categories:
Restrictions (AR) are an important unit in coop-
erative discourse. There is a quite large literature in
linguistics about the expression of restrictions. In
cooperative discourse, the expression of restrictions
is realized quite straightforwardly by a small num-
ber of classes of terms:
(a) restrictive locutions: sous re?serve que, a`
l?exception de, il n?est pas autoris?e de, toutefois, etc.
(provided that),
(b) the negative form ne ... que that is typical of re-
strictions, is very frequently used
(c) restrictive modals: doit obligatoirement,
impe?rativement, ne?cessairement (must obligato-
rily),
(d) quantification with a restrictive interpretation:
seul, pas tous, au maximum (only, not all).
Justifications (AJ) is also an important mean-
ingful unit, it has however a little bit fuzzy scope.
Marks are not very clearcut. Among them, we have:
(a) marks expressing causality, mainly connectors
such as: car, parce que, en raison de,
(b) marks expressing, via other forms of negation
than in AR, the impossibility to give a positive re-
sponse, or marks ?justifying? the response: il n?y a
pas, il n?existe pas, en effet (because, there is no,
indeed).
Warnings (AA) can quite clearly be identified by
means of:
(a) verbal expressions: sachez que, veuillez a` ne
pas, mieux vaut e?viter, n?oubliez pas, attention a`,
etc. (note that, do not forget, etc.),
(b) expressions or temporal morphological marks
that indicate that data is sensitive to time and may
be true only at some point: mise a` jour, change-
ments fre?quents, etc. (frequent updates),
(c) a few other expressions such as: il n?existe pas,
mais (but) ... + comparative form.
Except for the identification of DS, which require
quite a lot of ontological resources, marks identi-
fied for the other MU studied here are quite general.
Portability of these marks to other domains and pos-
sibly to other languages should be a reasonably fea-
sible challenge.
The response elaboration part (ER) is more con-
strained in terms of marks, because of the logical
procedures that are related to. For example, the
CSFR, dealing with constraint relaxation, involves
the use of sister, daughter and sometimes parent
nodes of the focus, and often proposes at least 2
choices. It is in general associated with a negative
direct response, or an explanation why no response
can be found. It also also contains some fixed marks
that indicate a change of concept, such as another
type of. This is easily visible in the pair Q2-R2 (sec-
tion 3.3) with the mark: the closests.
3.4.4 Constraints between units
A few constraints or preferences can be formu-
lated on the organization of meaningful units, these
may be somewhat flexible, because cooperative dis-
course may have a wide range of forms:
(a) coocurrence: any DR can co-occur with an AS,
AF, AR, AA or AJ,
(b) precedence: any DR precedes any (unmarked)
AA, AR, AC, ACP, B, or any sequence DS-BP. Any
CC precedes any CSFR, CSFH or CSFRI,
(c) incompatibility: DS + DP, CSFR + CSFI,
CSFC + CSFH. Furthermore CR cannot appear
alone.
Frequent pairs are quite numerous, here are the
most typical ones: DS + P, DS + AR, CC + CSFR
or CSFH or CSFRI, DS + AJ, DS(negative) + AJ +
AS, DS + AF, DS(negative) + CSFR. These can be
considered in priority in case of ambiguities.
3.5 Evaluation by annotators
At this stage, it is necessary to have evaluated by hu-
man annotators how clear, well-delimited and easy
to use this classification is. We do not have yet pre-
cise results, but it is clear that judgments may vary
from one annotator to another. This is not only due
to the generic character of our definitions, but also
to the existence of continuums between categories,
and to the interpretation of responses that may vary
depending on context, profile and culture of annota-
tors.
An experiment carried out on three independent
subjects (annotation task followed by a discussion
of the results) reveals that there is a clear consen-
sus of 80% on the annotations we did ourselves.
The other 20% reflect interpretation variations, in
general highly contextual. These 20% are almost
the same cases for the three subjects. In particu-
lar, at the level of additional information (CR), we
observed some differences in judgement in partic-
ular between restrictions (AR) and warnings (AA),
and a few others between CSFH and CSFC whose
differences may sometimes be only superficial (pre-
sentation of the arguments of the response).
3.6 Evaluation of prototype: a first experiment
We can now evaluate the accuracy of the linguistic
marks given above. For that purpose, we designed
a programme in Prolog (for fast prototyping) that
uses: (1) the domain lexicon and ontology, to have
access e.g. to term lexicalizations and morphology,
and (2) a set of ?local? grammars that implement the
different marks. Since these marks involve lexical
and morphological variations, negation, and some
long-distance dependencies, grammars are a good
solution.
Tests were carried out on a new corpus, essen-
tially from airlines FAQ. 134 QA pairs have been
selected from this corpus containing some form of
cooperativity. The annotation of this corpus is auto-
matic, while the evaluation of the results is manual
and is carried out in parallel by both ourselves and
by an external professional evaluator. These 134
QA pairs contain a total of 237 MU, therefore an
average of 1.76 MU per response. Most responses
have 2 MU, the maximum observed being 4. Sur-
prisingly, out of the 134 pairs, only 108 contain di-
rect responses followed by various CSF, the other
16 only contain cooperative know-how responses
(CSF), without any direct response part.
Evaluation results, although carried out on a rel-
atively small set of QA pairs, give good indications
on the accuracy of the linguistic marks, and also on
the typology of the different MU. We consider here
the MU: DS, AJ, AR, AA, as characterized above:
Unit A B C Total correct annotation
DS 102 6 0 108 88%
AJ 27 6 3 36 75%
AR 36 4 2 42 86%
AA 24 0 0 24 100%
A: number of MU annotated correctly for that cate-
gory, B: MU not annotated (no decision made), C:
incorrect annotation.
MU boundaries have been correctly identified in
88% of the cases, they are mostly related to punctu-
ation marks.
There are obviously a few delicate cases where
annotation is difficult if not impossible. First, we
observed a few discontinuities: an MU can be frag-
mented. In that case, it is necessary to add an index
to the tag so that the different fragments can be un-
ambiguously related, as in:
Q: What is the deadline for an internet reservation?
R: < DR index = 1 > In the case of an electronic
ticket, you can reserve up to 24h prior to departure
< /DR > . < B > You just need to show up at the
registration desk < /B > . < DR index = 1 >
In the case of a traditional ticket ... < /DR >.
The index=1 allows to tie the two fragments of the
enumeration.
In a number of cases the direct response part
is rather indirect, making its identification via the
means presented above quite delicate:
Q: I forgot to note my reservation number, how can
I get it?
R: A confirmation email has been sent to you as
soon as the reservation has been finalized.... To
identify this portion of the response as a DR, it is
necessary to infer that the email is a potential con-
tainer for a reservation number.
4 Conclusion and Perspectives
We reported in this paper a preliminary version, for
testing, of COOPML, a language designed to an-
notate the different facets of cooperative discourse.
Our approach, still preliminary, can be viewed as a
base to investigate the different forms of coopera-
tivity on an empirical basis. This work is of much
interest to define the formal structure of a coopera-
tive discourse. It can be used in discourse parsing as
well as generation, where it needs to be paired with
other structures such as rhethorical structures. It is
so far limited to written forms. We believe the same
global structure, with minor adaptations and addi-
tional marks, is valid for dialogues and oral com-
munication, but this remains to be investigated. The
main application area where our work is of interest
is probably advanced Question-Answering systems.
Besides cooperative discourse annotation, we
have investigated the different forms lexicalization
takes between the question and the different parts
of the response, the direct response (DR), the re-
sponse elaboration (ER) and the additional infor-
mation (CR). These are subtle realizations of much
interest for natural language generation. These ele-
ments are reported in (Benamara and Saint-Dizier,
2004b).
COOPML will be extended and stabilized in the
near future along the following dimensions:
? analyze the linguistic marks associated with
the MU not investigated here, and possible cor-
relations or conflicts between MU,
? analyze its customisation to various applica-
tion domains: since quite a lot of ontological
and lexical knowledge is involved, in particu-
lar to identify DS, this needs some elaboration,
? investigate portability to other languages, in
particular investigate the cost related to lin-
guistic resources development,
? develop a robust annotator, for each of the lev-
els identified, and make it available on a stan-
dard platform,
? investigate knowledge annotation. This point
is quite innovative and of much interest be-
cause of the heavy knowledge load involved in
the production of cooperative responses.
Acknowledgements We thank all the partici-
pants of our TCAN programme project and the
CNRS for partly funding it. We also thank the 3
anonymous reviewers for their stimulating and help-
ful comments.
References
Baumann, S., Brinckmann, C., Hansen-Schirra, S.,
Kruijff, G., The MULI Project : Annotation and
Analysis of Information Structure in German and
English., LREC, 2004.
Benamara, F., Saint-Dizier, P., Dynamic Generation
of Cooperative NL responses in WEBCOOP, 9th
EWNLG, Budapest, 2003.
Benamara. F, and Saint Dizier. P, Advanced Relax-
ation for Cooperative Question Answering, in:
New Directions in Question Answering, To ap-
pear in Mark T. Maybury, (ed), AAAI/MIT Press,
2004 (a).
Benamara. F, and Saint Dizier. P, Lexicalisation
Strategies in Cooperative Question-Answering
Systems in Proc. Coling?04, Geneva, 2004 (b).
Dybkjaer, L., Bernsen, N.O., The MATE Work-
bench. A Tool in Support of Spoken Dialogue
Annotation and Information Extraction, In B.
Yuan, T. Huang, X. Tank (Eds.): Proceedings of
ICSLP?2000?, Beijing,?, 2000.
Gal, A., Cooperative Responses in Deductive
Databases, PhD Thesis, Univ. of Maryland,
1988.
Gaasterland, T., Godfrey, P., Minker, J., An
Overview of Cooperative Answering, Papers in
non-standard queries and non-standard answers,
Clarendon Press, Oxford, 1994.
Grice, H., Logic and Conversation, in Cole and
Morgan (eds), Syntax and Semantics, Academic
Press, 1975.
Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum,
J., Developing a Natural Language Interface to
Complex Data, ACM transactions on database
systems, 3(2), 1978.
Kaplan, J., Cooperative Responses from a Portable
Natural Language Query System, in M. Brady
and R. Berwick (ed), Computational Models of
Discourse, 167-208, MIT Press, 1982.
Lehnert, W., The Process of Question Answering:
a Computer Simulation of Cognition, Lawrence
Erlbaum, 1978.
Mays, E., Joshi, A., Webber, B., Taking the Ini-
tiative in Natural Language Database Interac-
tions: Monitoring as Response, EACL?82, Orsay,
France, 1982.
Miltsakaki, E., Prasad, R., Joshi, A., Webber, B.,
The Penn Discourse Treebank, LREC, 2004.
Minock M, Chu W, Yang H, Chiang K, Chow, G
and Larson, C, CoBase: A Scalable and Exten-
sible Cooperative Information System. Journal of
Intelligent Information Systems, volume 6, num-
ber 2/3,pp : 223-259, 1996.
Netter, K., Armstrong, S., Kiss, T., Klein, J., DiET -
Diagnostic and Evaluation Tools for Natural Lan-
guage Applications,, Proceedings of 1st LREC,
Granada.?, 1998.
Orasan, C., PALink: A Highly Customisable Tool
for Discourse Annotation, Paper from the SIGdial
Workshop, 2003.
Ravinchandran, D., Hovy, E., Learning Surface Text
Patterns for a Question Answering System, ACL
2002, Philadelphia.
Reiter, R., Dale, R., Building Applied Natural Lan-
guage Generation Systems, Journal of Natural
Language Engineering, volume 3, number 1,
pp:57-87, 1997.
Searle, J., Indirect Speech Acts, in Cole and Morgan
(eds), Syntax and Semantics III, Academic Press,
1975.
Strenston, J., Introduction to Spoken Dialog, Long-
man, 1994.
Webber, B., Gardent, C., Bos, J., Position State-
ment: Inference in Question-Abswering, LREC
proceedings, 2002.
Proceedings of the Fourth International Natural Language Generation Conference, pages 103?110,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating Intelligent Numerical Answers
in a Question-Answering System
Ve?ronique Moriceau
Institut de Recherche en Informatique de Toulouse
118, route de Narbonne, 31062 Toulouse, France
moriceau@irit.fr
Abstract
In this paper, we present a question-
answering system on the Web which aims
at generating intelligent answers to numer-
ical questions. These answers are gener-
ated in a cooperative way: besides a direct
answer, comments are generated to ex-
plain to the user the variation of numerical
data extracted from the Web. We present
the content determination and realisation
tasks. We also present some elements of
evaluation with respect to end-users.
1 Introduction
Search engines on the Web and most existing
question-answering (QA) systems provide the user
with a set of hyperlinks and/or Web page extracts
containing answer(s) to a question. These answers
may be incoherent to a certain degree: they may be
equivalent, complementary, contradictory, at dif-
ferent levels of precision or specificity, etc. It is
then quite difficult for the user to know which an-
swer is the correct one. Thus, an analysis of rel-
evance and coherence of candidate answers is es-
sential.
1.1 Related work
Search engines on the Web produce a set of an-
swers to a question in the form of hyperlinks or
page extracts, ranked according to content or pop-
ularity criteria (Salton, 1989; Page et al, 1998).
Some QA systems on the Web use other tech-
niques: candidate answers are ranked according
to a score which takes into account lexical re-
lations between questions and answers, semantic
categories of concepts, distance between words,
etc. (Moldovan et al, 2003), (Narayanan and
Harabagiu, 2004), (Radev and McKeown, 1998).
Recently, advanced QA systems defined rela-
tionships (equivalence, contradiction, ...) between
Web page extracts or texts containing possible an-
swers in order to combine them and to produce
a single answer (Radev and McKeown, 1998),
(Harabagiu and Lacatusu, 2004), (Webber et al,
2002).
Most systems provide the user with either a set
of potential answers (ranked or not), or the ?best?
answer according to some relevance criteria. They
do not provide answers which take into account
information from a set of candidate answers or
answer inconsistencies. As for logical approaches
used for database query, they are based on major-
ity approach or on source reliability. But, contrary
to the assumption of (Motro et al, 2004), we noted
that reliability information (information about the
author, date of Web pages, ...) is rather difficult
to obtain, so we assume that all Web pages are
equally reliable.
1.2 Motivations and goals
Our framework is advanced QA systems over open
domains. Our main goals are to model and to eval-
uate a system which, from a factoid question in
natural language (in French), selects a set of can-
didate answers on the Web and generates cooper-
ative answers in natural language. Our challenge
is (1) to generate a synthetic answer instead of a
list of potential answers (in order to avoid provid-
ing the user with too much information), and (2) to
generate relevant comments which explain the va-
riety of answers extracted from the Web (in order
to avoid misleading the user) (Grice, 1975). In a
cooperative perspective, we propose an approach
for answer generation which uses answer integra-
tion. When several possible answers are extracted
from the Web, the goal is to define a coherent core
103
from candidate answers and to generate a cooper-
ative answer, i.e. an answer with explanations.
In this paper, we focus on the integration of nu-
merical data in order to generate natural language
cooperative answers to numerical questions. We
first present some motivational problems for the
generation of numerical answers in a QA system.
Then, we present the content determination and
realization processes. Finally, we give some el-
ements of evaluation of our system outputs, with
respect to end-users.
2 On numerical data
We focus on the integration of numerical data
for the generation of natural language coopera-
tive numerical answers. We first present some re-
lated work on generation from numerical data sets.
Then we propose a model for the generation of co-
operative numerical answers.
2.1 Related work
The generation of summaries from numerical data
has been developed in some NLG systems. For ex-
ample, the system ANA (Kukich, 1983) generates
stock market reports by computing fluctuations
for a day. FoG (Goldberg et al 1994) produces
weather forecasts from forecast data. More re-
cently, StockReporter (Dale, 2003) was developed
to generate summaries describing how a stock per-
forms over a period. Yu et al (2005) propose a
system which generates summaries of sensor data
from gas turbines.
Those systems have input data analysis compo-
nents which are more or less efficient and describe
numerical time-series data. In the framework of
QA systems, there are other major problems that
the previous systems do not deal with. When a
numerical question is submitted to a QA system,
a set of numerical data is extracted from the Web.
Then, the goal is not to describe the whole data set
but to find an appropriate answer, dealing with the
user expectations (for example, contraints in the
question) or data inconsistencies. Another impor-
tant point is the analysis of numerical input data in
order to identify causes (besides time) of variation.
2.2 A typology of numerical answers
Our challenge is to develop a formal framework
for the integration of numerical data extracted
from Web pages in order to produce cooperative
numerical answers.
To define the different types of numerical
answers, we collected a set of 80 question-answer
pairs about prices, quantities, age, time, weight,
temperature, speed and distance. The goal is
to identify for each question-answer pair why
extracted numerical values are different (is this an
inconsistency? an evolution?).
A numerical question may accept several
answers when numerical values vary according
to some criteria. Let us consider the following
examples.
Example 1 :
How many inhabitants are there in France?
- Population census in France (1999): 60184186.
- 61.7: number of inhabitants in France in 2004.
Example 2 :
What is the average age of marriage of women in
2004?
- In Iran, the average age of marriage of women
was 21 years in 2004.
- In 2004, Moroccan women get married at the
age of 27.
Example 3 :
At what temperature should I serve wine?
- Red wine must be served at room temperature.
- Champagne: between 8 and 10 ? C.
- White wine: between 8 and 11 ? C.
The corpus analysis allows us to identify 3 main
variation criteria, namely time (ex.1), place (ex.2)
and restriction (ex.3: restriction on the focus, for
example: Champagne/wine). These criteria can be
combined: some numerical values vary according
to time and place, to time and restrictions, etc. (for
example, the average age of marriage vary accord-
ing to time, place and restrictions on men/women).
2.3 A model for cooperative numerical
answer generation
The system has to generate an answer from a set
of numerical data. In order to identify the different
problems, let us consider the following example :
What is the average age of marriage in France?
- In 1972, the average age of marriage was 24.5
for men and 22.4 for women. In 2005, it is 30 for
men and 28 for women.
- The average age of marriage in France increased
from 24.5 to 26.9 for women and from 26.5 to 29
for men between 1986 and 1995.
104
This set of potential answers may seem incoher-
ent but their internal coherence can be made ap-
parent once a variation criterion is identified. In a
cooperative perspective, an answer can be for ex-
ample:
In 2005, the average age of marriage in France
was 30 for men and 28 for women.
It increased by about 5.5 years between 1972 and
2005.
This answer is composed of:
1. a direct answer to the question,
2. an explanation characterizing the variation
mode of the numerical value.
To generate this kind of answer, it is necessary (1)
to integrate candidate answers in order to elabo-
rate a direct answer (for example by solving incon-
sistencies), and (2) to integrate candidate answers
characteristics in order to generate an explanation.
Figure 1 presents the general architecture of our
system which allows us to generate answers and
explanations from several different numerical an-
swers. Questions are submitted in natural lan-
guage to QRISTAL1 which analyses them and se-
lects potential answers from the Web. Then, a
grammar is applied to extract information needed
for the generation of an appropriate cooperative
answer. This information is mainly:
- the searched numerical value (val),
- the unit of measure,
- the question focus,
- the date and place of the information,
- the restriction(s) on the question focus ,
- the precision of the numerical value (for example
adverbs or prepositions such as in about 700, ...),
- linguistic clues indicating a variation of the value
(temporal adverbs, verbs of change/movement as
in the price increased to 200 euro).
For the extraction of restrictions, a set of basic
properties is defined (colors, form, material, etc.).
Ontologies are also necessary. For example, for
the question how many inhabitants are there
in France?, population of overseas regions and
metropolitan population are restrictions of France
because they are daughters of the concept France
in the ontology. On the contrary, prison popula-
tion of France is not a restriction because prison is
not a daughter of France. Several ontologies are
available2 but the lack of available knowledge for
1www.qristal.fr, Synapse De?veloppement
2http://www.daml.org/ontologies/
Figure 1: Architecture
some domains obviously influences the quality of
answers.
We define the set A = {a1, ..., aN}, with ai a
frame which gathers all this information for a nu-
merical value. Figure 2 shows an extraction result.
Figure 2: Extraction results
From the frame set, the variation criteria and
mode of the searched numerical value are iden-
tified: these components perform content deter-
mination. Finally, a natural language answer is
generated explaining those characteristics. Each
of these stages is presented in the next sections.
3 Content determination for
explanations
In order to produce explanations for data variation,
the system must have a data analysis component
105
which can infer, from extracted information, the
variation phenomena, criteria and mode.
3.1 Variation criteria
Once we have the frames representing the different
numerical values, the goal is to determine if there
is a variation and to identify the variation criteria
of the value. We assume that there is a variation if
there is at least k different numerical values with
different criteria (time, place, restriction) among
the N frames (for the moment, we arbitrarily set
k = N/4, but this has to be evaluated). Thus, a
numerical value varies according to:
1. time if T = {ai(V al), ? aj ? A,
such as ai(V al) 6= aj(V al)
? ai(Unit) = aj(Unit)
? ai(Date) 6= aj(Date) }
? card(T ) ? k
2. place if P = {ai(V al), ? aj ? A,
such as ai(V al) 6= aj(V al)
? ai(Unit) = aj(Unit)
? ai(Place) 6= aj(Place) }
? card(P ) ? k
3. restriction if Rt = {ai(V al), ? aj ? A,
such as ai(V al) 6= aj(V al)
? ai(Unit) = aj(Unit)
? ai(Restriction) 6= aj(Restriction) }
? card(Rt) ? k
4. time and place if (1) ? (2)
5. time and restriction if (1) ? (3)
6. place and restriction if (2) ? (3)
7. time, place and restriction if (1)? (2)? (3)
Numerical values can be compared only if they
have the same unit of measure. If not, they have to
be converted. More details about comparison rules
are presented in (Moriceau, 2006).
3.2 Variation mode
In the case of numerical values varying over time,
it is possible to characterize more precisely the
variation. The idea is to draw a trend (increase,
decrease, ...) of variaton over time so that a precise
explanation can be generated. For this purpose, we
draw a regression line which determines the rela-
tionship between the two extracted variables value
and date.
In particular, Pearson?s correlation coefficient (r),
related to the line slope, reflects the degree of lin-
ear relationship between two variables. It ranges
from +1 to ?1. For example, figure 3 shows that a
positive Pearson?s correlation implies a general in-
crease of values whereas a negative Pearson?s cor-
relation implies a general decrease. On the con-
trary, if r is low (?0.6 < r < 0.6), then we con-
sider that the variation is random (Fisher, 1925).
Figure 3: Variation mode
Figure 4 shows the results for the question How
many inhabitants are there in France? Differ-
ent numerical values and associated dates are ex-
tracted from Web pages. The Pearson?s correlation
is 0.694 meaning that the number of inhabitants
increases over time (between 1999 and 2005).
Figure 4: Variation mode: How many inhabitants
are there in France?
4 Answer generation
Once the searched numerical values have been ex-
tracted and characterized by their variation crite-
ria and mode, a cooperative answer is generated in
natural language. It is composed of two parts:
- a direct answer if available,
- an explanation of the value variation.
4.1 Direct answer generation
4.1.1 Question constraints
The content determination process for the di-
rect answer generation is mainly guided by con-
straints which may be explicit or implicit in the
question. For example, in the question how many
inhabitants are there in France in 2006?, there
106
are explicit constraints on time and place. On
the contrary, in how many inhabitants are there in
France?, there is no constraint on time. Let C be
the set of question constraints: C = {Ct, Cp, Cr}
with :
- Ct: constraint on time (Ct ? {exp time, ?}),
- Cp: constraint on place (Cp ? {exp place, ?}),
- Cr: constraint on restrictions (Cr ? {exp restr,
?}).
For example, in the question what is the average
age of marriage in France?: Ct = ?, Cp = France
and Cr = ?.
When there is no explicit constraint in the ques-
tion, we distinguish several cases:
- if there is no explicit constraint on time in the
question and if a numerical variation over time has
been infered from the data set, then we assume that
the user wants to have the most recent information:
Ct = max({ai(date), ai ? A}),
- if there is no explicit constraint on place in the
question and if a numerical variation according to
place has been infered from the data set, then we
assume that the user wants to have the information
for the closest place to him (the system can have
this information for example via a user model),
- if there is no explicit constraint on restrictions in
the question and if a numerical variation accord-
ing to restrictions has been infered from the data
set, then we assume that the user wants to have the
information for any restrictions.
For example, on figure 5: Ct = 2000 (the most
recent information), Cp = France and Cr = ?.
4.1.2 Candidate answers
Candidate frames for direct answers are those
which satisfy the set of constraints C . Let AC be
the set of frames which satisfy C (via subsump-
tion):
AC = {ai ? A, such as
ai(date) = (Ct ? ?) ? ai(place) = (Cp ? ?) ?
ai(restriction) =
{
Cr ? ? if Cr 6= ?
exp rest ? ? if Cr = ?
For figure 5: AC = {a1, a2, a3, a4, a5, a6}.
4.1.3 Choosing a direct answer
A direct answer has to be generated from
the set AC . We define subsets of AC which
contain frames having the same restrictions: a
direct answer will be generated for each relevant
restriction. Let A be the subsets of frames
satisfying the question constraints and having the
same restrictions: A = {AC1, ..., ACM} with:
ACi = {aj , such as ? aj , ak ? AC,
aj(restriction) = ak(restriction)
? aj(restriction) = ?},
and AC1, ..., ACM are disjoint.
For figure 5: A = {AC1, AC2} with:
AC1 = {a1, a3, a5}, subset for restriction women,
AC2 = {a2, a4, a6}, subset for restriction men.
Then, for each element in A , an answer is
generated :
? ACi ? A , answer = generate answer(ACi).
Each element of A may contain one or sev-
eral frames, i.e. one or several numerical data.
Some of these values may be aberrant (for exam-
ple, How high is the Eiffel Tower? 300m, 324m,
18cm): they are filtered out via classical statistical
methods (use of the standard deviation). Among
the remaining frames, values may be equal or not
at different degrees (rounded values, for example).
Those values have to be integrated so that a syn-
thetic answer can be generated.
There are many operators used in logical ap-
proaches for fusion: conjunction, disjunction, av-
erage, etc. But, they may produce an answer
which is not cooperative: a conjunction or disjunc-
tion of all candidates may mislead users; the aver-
age of candidates is an ?artificial? answer since it
has been computed and not extracted from Web
pages.
Our approach allows the system to choose a
value among the set of possible values, dealing
with the problem of rounded or approximative
data. Candidate values are represented by an ori-
ented graph whose arcs are weighted with the cost
between the two linked values and the weight (w)
of the departure value (its number of occurrences).
A graph G of numerical values is defined by N
the set of nodes (set of values) and A rc the set of
arcs. The cost c(x, y) of arc(x, y) is:
|x ? y|
y ? (w(x) +
n
?
i=1
w(xi)) +
n
?
i=1
c(xi, x).
with (x1, ..., xn, x) a path from x1 to x.
Finally, we define a fusion operator which
selects the value which is used for the direct
answer. This value is the one which maximizes
the difference (cost(x)) between the cost to leave
this value and the cost to arrive to this value:
107
Figure 5: Data set for What is the average age of marriage in France?
answer = y ? N , such as
cost(y) = max({ cost(n), ? n ? N ,
cost(n) = cost leave(n) ? cost arrive(n)})
with: cost leave(x) =?i c(x, xi) and,
cost arrive(x) =?i c(xi, x).
Let us consider an example. The following val-
ues are candidate for the direct answer to the ques-
tion How high is the Mont-Blanc?: 4800, 4807
(2 occurrences), 4808 (2 occurrences), 4808.75,
4810 (8 occurrences) and 4813. Figure 6 shows
the graph of values: in this example, the value
which maximizes the costs is 4810.
From the selected value, the system generates
a direct answer in natural language in the form
of Focus Verb (Precision) Value. For example,
the generated answer for How high is the Mont-
Blanc? is The Mont-Blanc is about 4810 meters
high. Here the preposition about indicates to the
user that the given value is an approximation.
For the question what is the average age of mar-
riage in France?, a direct answer has to be gen-
erated for each restriction. For the restriction men
(AC2), there are 3 candidate values: 29.8, 30 and
30.6, the value which minimizes the costs being
30. For the restriction women (AC1), there are
also 3 candidate values: 27.7, 28 and 28.5, the
value which minimizes the costs being 28. Af-
ter aggregation process, the generated direct an-
swer is: In 2000, the average age of marriage in
France was about 30 years for men and 28 years
for women.
4.2 Explanation generation
The generation of the cooperative part of the an-
swer is complex because it requires lexical knowl-
edge. This part of the answer has to explain to
the user variation phenomena of search values:
when a variation of values is identified and char-
acterised, an explanation is generated in the form
of X varies according to Criteria. In the case of
variation according to restrictions or properties of
the focus, a generalizer is generated. For exam-
ple, the average age of marriage varies for men and
women: the explanation is in the form the average
age of marriage varies according to sex. The gen-
eralizer is the mother concept in the ontology or a
property of the mother concept (Benamara, 2004).
For numerical value varying over time, if the vari-
ation mode (increase or decrease) is identified,
a more precise explanation is generated: X in-
creased/decreased between... and... instead of X
varies over time.
Here, verbs are used to express precisely numer-
ical variations. The lexicalisation process needs
deep lexical descriptions. We use for that pur-
pose a classification of French verbs (Saint-Dizier,
1999) based on the main classes defined by Word-
Net. The classes we are interested in for our
task are mainly those of verbs of state (have,
be, weight, etc.), verbs of change (increase, de-
crease, etc.) and verbs of movement (climb,
move forward/backward, etc.) used metaphori-
cally (Moriceau et al 2003). From these classes,
we selected a set of about 100 verbs which can be
applied to numerical values.
From these classes, we characterized sub-classes
of growth, decrease, etc., so that the lexicalisation
task is constrained by the type of verbs which has
to be used according to the variation mode.
A deep semantics of verbs is necessary to gen-
erate an answer which takes into account the char-
acteristics of numerical variation as well as pos-
sible: for example, the variation mode but also
the speed and range of the variation. Thus, for
each sub-class of verbs and its associated varia-
tion mode, we need a refined description of onto-
logical domains and selectional restrictions so that
108
Figure 6: Graph of candidate values for How high is the Mont-Blanc?
an appropriate verb lexicalisation can be chosen:
which verb can be applied to prices, to age, etc.?
(Moriceau et al 2003). We propose to use propor-
tional series representing verb sub-classes accord-
ing to the speed and amplitude of variation. For
example, the use of climb (resp. drop) indicates
a faster growth (resp. decrease) than go up (resp.
go down): the verb climb is prefered for the gener-
ation of Gas prices climb 20.3% in october 2005
whereas go up is prefered in Gas prices went up
7.2% in september 2005.
Verbs can possibly be associated with a preposi-
tion that refines the information (The average age
of marriage increased by about 5.5 years between
1972 and 2005).
4.3 Answer justification
Our system generates a cooperative answer com-
posed of a direct answer to the question and an ex-
planation for the possible variation of the searched
numerical value. But the answer may not be sure
because of a too high/low number of candidate
values to the direct answer. In this case, it may be
useful to add some additional information for the
user in order to justify or complete the generated
answer.
We propose to add a know-how component to
our system, which provides the user with one or
two relevant Web page extracts besides the gen-
erated answer whenever it is necessary. These ex-
tracts must contain information about the searched
numerical values, and for example some explana-
tions of the causes of numerical variation. Some
linguistic clues can be used to select page extracts:
number of numerical values concerning the ques-
tion focus, causal marks (because of, due to, ...),
etc. Figure 7 shows an output example of our sys-
tem.
Figure 7: An ouput example
5 Evaluation
In this section, we present some elements of eval-
uation of our system with respect to 15 end-users3 .
We first evaluated how users behave when they
are faced with different candidate answers to a
question. To each user, we presented 5 numeri-
cal questions and their candidate answers which
vary according to time or restrictions and ask them
to produce their own answer from candidate an-
swers. For numerical answers varying according
to restrictions, 93% of subjects produce answers
explaining the different numerical values for each
restriction. For numerical answers varying over
time, 80% of subjects produce answers giving the
most recent information (20% of subjects produce
an answer which a summary of all candidate val-
ues). This validates our hypothesis presented in
section 4.1.1.
The second point we evaluated is the answer or-
der. Our system produces answers in the form of
a direct answer, then an explanation and a justi-
fication (page extract) if necessary. We proposed
to users answers with these three parts arranged
randomly. Contrary to (Yu et al 2005) which pro-
pose first an overview and then a zoom on inter-
3Subjects are between 20 and 35 years old and are accus-
tomed to using search engines.
109
esting phenomena, 73% of subjects prefered the
order proposed by our system, perhaps because, in
QA systems, users wants to have a direct answer
to their question before having explanations.
The last point we evaluated is the quality of the
system answers. For this purpose, we asked sub-
jects to choose, for 5 questions, which answer they
prefer among: the system answer, an average, an
interval and a disjunction of all candidate answers.
91% of subjects prefered the system answer. 75%
of subjects found that the explanation produced is
useful and only 31% of subjects consulted the Web
page extract (28% of these found it useful).
6 Conclusion
We proposed a question-answering system which
generates intelligent answers to numerical ques-
tions. Candidate answers are first extracted from
the Web. Generated answers are composed of
three parts: (1) a direct answer: the content
determination process ?chooses? a direct answer
among candidates, dealing with data inconsisten-
cies and approximations, (2) an explanation: the
content determination process allows to identify,
from data sets, the possible value variations and
to infer their variation criteria (time, place or re-
strictions on the question focus), and (3) a possi-
ble Web page extract. This work has several future
directions among which we plan:
- to define precisely in which cases it is useful to
propose a Web page extract as a justification and,
- to measure the relevance of restrictions on the
question focus to avoid generating an enumeration
of values corresponding to irrelevant restrictions.
References
F. Benamara. 2004. Generating Intensional Answers
in Intelligent Question Answering Systems. LNAI
Series, volume 3123, Springer.
R. Dale. 2003. http://www.ics.mq.edu.au/ lgt-
demo/StockReporter/.
R. A. Fisher 1925. Statistical Methods for Research
Workers, originally published in London by Oliver
and Boyd.
E. Goldberg, N. Driedger, R. Kittredge. 1994. Us-
ing natural language processing to produce weather
forecasts. IEEE Expert 9(2).
H.P. Grice. 1975. Logic and conversation. In P. Cole
and J.L. Morgan, (eds.): Syntax and Semantics, Vol.
3, Speech Acts, New York, Academic Press.
S. Harabagiu and F. Lacatusu. 2004. Strategies for
Advanced Question Answering. Proceedings of the
Workshop on Pragmatics of Question Answering at
HLT-NAACL 2004.
K. Kukich. 1983. Knowledge-based report genera-
tion: a knowledge engineering approach to natural
language report generation. Ph.D. Thesis, Informa-
tion Science Department, University of Pittsburgh.
D. Moldovan, C. Clark, S. Harabagiu and S. Maiorano.
2003. COGEX: A Logic Prover for Question An-
swering. Proceedings of HLT-NAACL 2003.
V. Moriceau and P. Saint-Dizier. 2003. A Conceptual
Treatment of Metaphors for NLP. Proceedings of
ICON, Mysore, India.
V. Moriceau. 2006. Numerical Data Integration
for Question-Answering. Proceedings of EACL-
KRAQ?06, Trento, Italy.
A. Motro, P. Anokhin. 2004. Fusionplex: resolution
of data inconsistencies in the integration of hetero-
geneous information sources. Information Fusion,
Elsevier.
S. Narayanan, S. Harabagiu. 2004. Answering Ques-
tions Using Adcanced Semantics and Probabilis-
tic Inference. Proceedings of the Workshop on
Pragmatics of Question Answering, HLT-NAACL,
Boston, USA, 2004.
L. Page, S. Brin, R. Motwani, T. Winograd. 1998. The
PageRank Citation Ranking: Bringing Ordre to the
Web. Technical Report, Computer Science Depart-
ment, Stanford University.
D.R. Radev and K.R. McKeown. 1998. Generat-
ing Natural Language Summaries from Multiple On-
Line Sources. Computational Linguistics, vol. 24,
issue 3 - Natural Language Generation.
P. Saint-Dizier. 1999. Alternations and Verb Semantic
Classes for French. Predicative Forms for NL and
LKB, Kluwer Academic.
P. Saint-Dizier. 2005. PrepNet: a Framework for
Describing Prepositions: preliminary investigation
results. Proceedings of IWCS?05, Tilburg, The
Netherlands.
G. Salton. 2002. Automatic Text Processing. The
Transformation, Analysis and Retrieval of Informa-
tion by Computer, Addison-Wesley.
B. Webber, C. Gardent and J. Bos. 2002. Position
statement: Inference in Question Answering. Pro-
ceedings of LREC, Las Palmas, Spain.
J. Yu, E. Reiter, J. Hunter, C. Mellish. 2005. Choosing
the content of textual summaries of large time-series
data sets. Natural Language Engineering, 11.
110
Numerical Data Integration for Cooperative Question-Answering
Ve?ronique Moriceau
Institut de Recherche en Informatique de Toulouse
118, route de Narbonne
31062 Toulouse cedex 09, France
moriceau@irit.fr
Abstract
In this paper, we present an approach
which aims at providing numerical an-
swers in a question-answering system.
These answers are generated in a coop-
erative way: they explain the variation of
numerical values when several values, ap-
parently incoherent, are extracted from the
web as possible answers to a question.
1 Introduction
Search engines on the web and most existing
question-answering systems provide the user with
a set of hyperlinks and/or web page extracts con-
taining answer(s) to a question. These answers
may be incoherent to a certain degree: they may be
equivalent, complementary, contradictory, at dif-
ferent levels of precision or specifity, etc. It is then
quite difficult for the user to know which answer
is the correct one.
In a cooperative perspective, we propose an ap-
proach for answer generation in natural language
which uses answer integration. When several
possible answers are selected by the extraction
engine, the goal is to define a coherent core from
candidate answers and to generate a cooperative
answer, i.e. an answer with explanations. We
assume that all web pages are equally reliable
since page provenance information (defined in
(McGuinness and Pinheiro da Silva, 2004) e.g.,
source, date, author, etc.) is difficult to obtain.
To adequately deal with data integration in
question-answering, it is essential to define pre-
cisely relations existing between potential an-
swers. In this introduction, we first present related
works. Then, we define a general typology of re-
lations between candidate answers.
1.1 Related works
Most of existing systems on the web produce a
set of answers to a question in the form of hyper-
links or page extracts, ranked according to a rel-
evance score. For example, COGEX (Moldovan
et al, 2003) uses its logic prover to extract lexical
relationships between the question and its candi-
date answers. The answers are then ranked based
on their proof scores. Other systems define rela-
tionships between web page extracts or texts con-
taining possible answers: for example, (Radev and
McKeown, 1998) and (Harabagiu and Lacatusu,
2004) define agreement (when two sources report
the same information), addition (when a second
source reports additional information), contradic-
tion (when two sources report conflicting informa-
tion), etc. These relations can be classified into
the 4 relations defined by (Webber et al, 2002),
i.e. inclusion, equivalence, aggregation and al-
ternative which we present below.
Most question-answering systems provide an-
swers which take into account neither information
given by all candidate answers nor their inconsis-
tency. This is the point we focus on in the follow-
ing section.
1.2 A general typology of integration
mechanisms
To better characterize our problem, we collected
a corpus of about 100 question-answer pairs in
French that reflect different inconsistency prob-
lems (most of pairs are obtained via Google or
QRISTAL1). We first assume that all candidate an-
swers obtained via an extraction engine are poten-
tially correct, i.e. they are of the semantic type
expected by the question.
1www.qristal.fr, Synapse De?veloppement.
42 KRAQ06
For each question of our corpus, a set of possi-
ble answers is extracted from the web. The goal
of our corpus analysis is to identify relations be-
tween those answers and to define a general typol-
ogy of associated integration mechanisms. We use
for this purpose the 4 relations defined in (Web-
ber et al, 2002) and for each relation, we propose
one or several integration mechanisms in order to
generate answers which take into account charac-
teristics and particularities of candidate answers.
1.2.1 Inclusion
A candidate answer is in an inclusion relation
if it entails another answer (for example, concepts
of candidate answers linked in an ontology by the
is-a or part-of relations). For example, in Brittany
and in France are correct answers to the question
Where is Brest?, linked by an inclusion relation
since Brittany is a part of France.
1.2.2 Equivalence
Candidate answers which are linked by an
equivalence relation are consistent and entail mu-
tually. The corpus analysis allows us to identify
two main types of equivalence:
(1) Lexical equivalence: use of acronyms or
foreign language, synonymies, metonymies, para-
phrases, proportional series. For example, to the
question Who killed John Lennon?, Mark Chap-
man, the murderer of John Lennon and John
Lennon?s killer Mark Chapman are equivalent an-
swers.
(2) Equivalence with inference: in a number of
cases, some common knowledge, inferences or
computations are necessary to detect equivalence
relations. For example, The A320 is 22 and The
A320 was built in 1984 are equivalent answers to
the question How old is the Airbus A320?.
1.2.3 Aggregation
The aggregation relation defines a set of con-
sistent answers when the question accepts several
different ones. In this case, all candidate answers
are potentially correct and can be integrated in the
form of a conjunction of all these answers. For
example, an answer to the question Where is Dis-
neyland? can be in Tokyo, Paris, Hong-Kong and
Los Angeles.
1.2.4 Alternative
The alternative relation defines a set of inconsis-
tent answers. In the case of questions expecting a
unique answer, only one answer among candidates
is correct. On the contrary, all candidates can be
correct answers.
(1) A simple solution is to propose a disjunc-
tion of candidate answers. For example, if the
question When does autumn begin? has the can-
didate answers Autumn begins on September 21st
and Autumn begins on September 20th, an answer
such as Autumn begins on either September 20th
or September 21st can be proposed. (Moriceau,
2005) proposes an integration method for answers
of type date.
(2) If candidate answers have common charac-
teristics, it is possible to integrate them according
to these characteristics (?greatest common denom-
inator?). For example, the question When does the
?fe?te de la musique? take place? has the follow-
ing answers June 1st 1982, June 21st 1983, ...,
June 21st 2005. Here, the extraction engine se-
lects pages containing the dates of music festivals
over the years. Since these candidate answers have
day and month in common, an answer such as The
?fe?te de la musique? takes place every June 21st
can be proposed (Moriceau, 2005).
(3) Numerical values can be integrated in the
form of an interval, average or comparison. For
example, if the question How far is Paris from
Toulouse? has the candidate answers 713 km, 678
km and 681 km, answers such as Paris is at about
690 km from Toulouse (average) or The distance
between Paris and Toulouse is between 678 and
713 km (interval) can be proposed.
2 Motivations
In this paper, we focus on answer elaboration
from several answers of type numerical (case (3)
above). Numerical questions deal with numerical
properties such as distance, quantity, weight, age,
etc. In order to identify the different problems, let
us consider the following example :
What is the average age of marriage in France?
- In 1972, the average age of marriage was 24.5
for men and 22.4 for women. In 2005, it is 30 for
men and 28 for women.
- According to an investigation carried out by
FNAIM in 1999, the average age of marriage is
27.7 for women and 28.9 for men.
- The average age of marriage in France increased
from 24.5 to 26.9 for women and from 26.5 to 29
for men between 1986 and 1995.
This set of potential answers may seem incoher-
ent but their internal coherence can be made ap-
43 KRAQ06
parent once a variation criterion is identified. In a
cooperative perspective, an answer can be for ex-
ample:
In 2005, the average age of marriage in France is
30 for men and 28 for women.
It increased by about 5.5 years between 1972 and
2005.
This answer is composed of:
1. a direct answer to the question,
2. an explanation characterizing the variation
mode of the numerical value.
To generate this kind of answer, it is necessary (1)
to integrate candidate answers in order to elabo-
rate a direct answer (for example by solving incon-
sistencies), and (2) to integrate candidate answers
characteristics in order to generate an explanation.
In the following sections, we first define a typol-
ogy of numerical answers and then briefly present
the general architecture of the system which gen-
erates cooperative numerical answers.
2.1 A typology of numerical answers
To define the different types of numerical answers,
we collected a set of 80 question-answer pairs
about prices, quantities, age, time, weight, temper-
ature, speed and distance. The goal is to identify
for each question-answer pair:
- if the question expects one or several answers
(learnt from texts and candidate answers),
- why extracted numerical values are different (is
this an inconsistency? an evolution?).
2.1.1 The question accepts only one answer
For example, How long is the Cannes Interna-
tional Film Festival?. In this case, if there are sev-
eral candidate answers, there is an inconsistency
which has to be solved (cf. section 4.1).
2.1.2 The question accepts several answers
This is the case when numerical values vary
according to certain criteria. Let us consider the
following examples.
Example 1 :
How many inhabitants are there in France?
- Population census in France (1999): 61632485.
- 61.7: number of inhabitants in France in 2004.
In this example, the numerical value (quantity) is
a property which changes over time (1999, 2004).
Example 2 :
What is the average age of marriage of women in
2004?
- In Iran, the average age of marriage of women
went from 19 to 21 years in 2004.
- In 2004, Moroccan women get married at the
age of 27.
In this example, the numerical value (age of
marriage) varies according to place (in Iran,
Moroccan).
Example 3 :
At which temperature do I have to serve wine?
- Red wine must be served at room temperature.
- Champagne: between 8 and 10 ? C.
- White wine: between 8 and 11 ? C.
Here, the numerical value (temperature) varies
according to the question focus (wine).
The corpus analysis allows us to identify 3 main
variation criteria, namely time, place and restric-
tion (restriction on the focus, for example: Cham-
pagne/wine). These criteria can be combined:
some numerical values vary according to time and
place, to time and restrictions, etc. (for exam-
ple, the average age of marriage vary according to
time, place and restrictions on men/women). Note
that there are several levels of restrictions and that
only restrictions of the same type can be compared
(cf. section 3.2). For example, metropolitan pop-
ulation and population of overseas regions are re-
strictions of the same ontological type (geograph-
ical place) whereas prison population is a restric-
tion of a different type and is not comparable to
the previous ones.
2.2 Architecture of the system
Figure 1 presents the general architecture of our
system which allows us to generate answers and
explanations from several different numerical an-
swers.
Questions are submitted in natural language to
QRISTAL which analyses them (focus, answer ex-
pected type) and which selects potential answers
from the web: QRISTAL searches web pages con-
taining the keywords of the query and synonyms
(extraction engine). Then, an extraction gram-
mar constructs a set of frames from candidate web
pages. From the frame set, the variation crite-
ria and mode of the searched numerical value are
identified. Finally, a natural language answer is
generated explaining those characteristics. Each
of these stages is presented in the next sections.
44 KRAQ06
Figure 1: Architecture
3 Answer characterization
Answer characterization consists in 2 main stages:
- information extraction from candidate web
pages,
- characterization of variation (criteria and mode)
of numerical values if necessary.
3.1 Answer extraction
Once QRISTAL has selected candidate web
pages (those containing the question focus and
having the expected semantic type), a grammar
is applied to extract information needed for the
generation of an appropriate cooperative answer.
This information is mainly:
- the searched numerical value (val),
- the unit of measure,
- the question focus and its synonyms (focus)
(for the moment, synonyms are not considered
because it requires a lot of resources, especially in
an open domain),
- the date and place of the information,
- the restriction(s) on the question focus (essen-
tially, adjectives or relative clauses).
In addition, the corpus analysis shows that
some other information is essential for infering
the precision degree or the variation mode of
values. It is mainly linguistic clues indicating:
- the precision of the numerical value (for example
adverbs or prepositions such as in about 700, ...),
- a variation of the value (for example temporal
adverbs, verbs of change/movement as in the
price increased to 200 euro).
We define a frame ai which gathers all this in-
formation for a numerical value:
ai =
?
?
?
?
?
?
?
?
V al =
Precision =
Unit =
Focus =
Date =
P lace =
Restriction =
V ariation =
?
?
?
?
?
?
?
?
A dedicated grammar extracts this information
from candidate web pages and produces the set A
of N candidate answers: A = {a1, ..., aN}.
We use a gapping grammar (Dahl and Abramson,
1984) to gap elements which are not useful (in
the example According to an investigation car-
ried out by FNAIM in 1999, the average age of
marriage is 27.7 for women and 28.9 for men, el-
ements in bold are not useful). We give below the
main rules of the grammar, optional elements are
between brackets:
Answer ? Nominal Sentence | Verbal Sentence
Nominal Sentence ? Focus (Restriction), ..., (Date), ...,
(Place), ..., (Precision) Val (Unit)
Verbal Sentence ? Focus (Restriction), ..., (Date), ...,
(Place), ..., Verb, ..., (Precision) Val (Unit)
Verb ? VerbQuestion | Variation
VerbQuestion ? count | estimate | weigh | ...
Variation ? go up | decrease | ...
Precision ? about | on average | ...
Place ? Country | City | ...
Time ? Date | Period | ...
Restriction ? Adjective | Relative | ...
.......
Figure 2 shows an extraction result.
A syntactic analysis is also necessary to check
the relevance of extracted information. For exam-
ple, suppose that the answer population of cities
of France which have more than 2000 inhabitants
(...) is proposed to the question How many in-
habitants are there in France?. A syntactic anal-
ysis has to identify the expression cities of France
which (...) as a restriction of population and to
45 KRAQ06
Figure 2: Results of extraction
infer that 2000 inhabitants is a property of those
cities and that it is not an answer to the question.
We plan to investigate the lexical elements (prepo-
sitions, predicative terms, etc.) necessary to this
analysis.
Elements of extraction evaluation are presented
in figure 3: we submitted 30 questions to Google
and QRISTAL. Our system can select the correct
direct answer provided that QRISTAL returns the
correct answer among relevant pages (for 87% of
the questions we evaluated) and that our grammar
succeeds in extracting relevant information (this
has to be evaluated).
Figure 3: Elements of evaluation
3.2 Variation criteria
Once we have the frames representing the different
numerical values, the goal is to determine if there
is a variation and to identify the variation criteria
of the value. In fact, we assume that there is a vari-
ation if there is at least k different numerical val-
ues with different criteria (time, place, restriction)
among the N frames (k is a rate which depends
on N : the more candidate answers there are, the
greater is k). Thus, a numerical value varies ac-
cording to:
1. time if card ({ai, such as ? ai, aj ? A,
ai(V al) 6= aj(V al)
? ai(Unit) = aj(Unit)
? ai(Date) 6= aj(Date) }) ? k
2. place if card ({ai, such as ? ai, aj ? A,
ai(V al) 6= aj(V al)
? ai(Unit) = aj(Unit)
? ai(Place) 6= aj(Place) }) ? k
3. restriction if card ({ai, such as
? ai, aj ? A, ai(V al) 6= aj(V al)
? ai(Unit) = aj(Unit)
? ai(Restriction) 6= aj(Restriction)})
? k
4. time and place if (1) ? (2)
5. time and restriction if (1) ? (3)
6. place and restriction if (2) ? (3)
7. time, place and restriction if (1)? (2)? (3)
Numerical values can be compared only if they
have the same unit of measure. If not, they have to
be converted.
For each criterion (time, place or restriction),
only information of the same semantic type and of
the same ontological level can be compared. For
example, metropolitan population and prison pop-
ulation are restrictions of a different ontological
level and cannot be compared. In the same way,
place criteria can only be compared if they have
the same ontological level: for example, prices in
Paris and in Toulouse can be compared because
the ontological level of both places is city. On the
contrary, prices in Paris and in France cannot be
compared since the ontological levels are respec-
tively city and country. Several ontologies of ge-
ographical places exist, for example (Maurel and
Piton, 1999) but a deep analysis of restrictions is
necessary to identify the kind of implied knowl-
edge.
In the particular cases where no information
has been extracted for some criteria, it is also
necessary to define some comparison rules. Thus,
let crit ? {time, place, restriction} and
ai, aj ? A,
- if no information has been extracted for 2 com-
pared criteria, then we consider that those criteria
are equal (there is no information indicating that
there is a variation according to those criteria),
i.e. if ai(crit) = ? and aj(crit) = ?, then
46 KRAQ06
ai(crit) = aj(crit)
- if no information has been extracted for one of
the 2 compared criteria, then we consider that
those criteria are different (there is a variation),
i.e. if ai(crit) = ? and aj(crit) 6= ?, then
ai(crit) 6= aj(crit)
In the example of figure 2, the price varies ac-
cording to time, place and restriction. In the fol-
lowing example (figure 4), the price of gas varies
according to time (September 2005/ ?) and place
(Paris/Toulouse). For space reasons, we give only
2 frames for each example but it is obviously not
sufficient to conclude.
Figure 4: Example of variation
Variation criteria of numerical values are learnt
from texts but can also be infered (or confirmed),
for some domains, by common knowledge. For
example, the triangle inequality states that for any
three points x, y, z, we have:
distance(x, y) ? distance(x, z) + distance(z, y).
From this, we can infer that a distance between
two points vary according to restriction (itinerary).
In the following sections, we focus on numeri-
cal values which vary according to time.
3.3 Variation mode
The last step consists in identifying the variation
mode of values. The idea is to draw a trend (in-
crease, decrease, ...) of variaton in time so that an
explanation can be generated. For this purpose, we
have a set of couples (numerical value, date) repre-
senting the set of extracted answers. From this set,
we draw a regression line (a line which comes as
close to the points as possible) which determines
the relationship between the two variables value
and date.
The correlation between two variables reflects
the degree to which the variables are related. In
particular, Pearson?s correlation (r) reflects the de-
gree of linear relationship between two variables.
It ranges from +1 to ?1. A correlation of +1
means that there is a perfect positive linear rela-
tionship between variables. For example, figure
5 shows that a positive Pearson?s correlation im-
plies a general increase of values (trend) whereas
a negative Pearson?s correlation implies a general
decrease. On the contrary, if r is low (?0.6 <
r < 0.6), then the trend (increase or decrease) is
mathematically considered as random 2.
Figure 5: Variation mode
This method determines the variation mode of
numerical values (it gives a variation trend) and
determines if the values are strongly dependent on
time or not (the highest r is, the more the numeri-
cal values are dependent on time).
Figure 6 shows the results for the question How
many inhabitants are there in France? Differ-
ent numerical values and associated dates are ex-
tracted from web pages. The Pearson?s correla-
tion is 0.682 meaning that the number of inhab-
itants increases according to time (between 1999
and 2005).
4 Answer generation
Once the searched numerical values have been ex-
tracted and characterized by their variation crite-
ria and mode, a cooperative answer is generated in
natural language. It is composed of two parts:
1. a direct answer if available,
2. an explanation of the value variation.
2Statistical Methods for Research Workers, R. Fisher
(1925)
47 KRAQ06
Figure 6: Variation mode: How many inhabitants
are there in France?
In the following sections, we present some prereq-
uisites to the construction of each of these parts in
term of resources and knowledge.
4.1 Direct answer generation
There are mainly two cases: either one or several
criteria are constrained by the question (as in
How many inhabitants are there in France in
2005? where criteria of place and time are given),
or some criteria are omitted (or implicit) (as
in How many inhabitants are there in France?
where there is no information on time). In the
first case, the numerical value satisfying the
constraints is chosen (unification between the
criteria of the question and those extracted from
web pages). In the second case, we assume that
the user wants to have the most recent information.
We focus here on answers which vary accord-
ing to time. Aberrant values are first filtered out
by applying classical statistical methods. Then,
when there is only one numerical value which
satisfies the temporal constraint (given by the
question or the most recent date), then the direct
answer is generated from this value. When there
is no numerical value satisfying the temporal
constraint, only the second part of the answer
(explanation) is generated.
In the case of several numerical values satisfying
the temporal constraint, there may be approximate
values. For example, the following answers (cf
figure 6) are extracted for the question How many
inhabitants were there in France in 2004?:
(1) 61.7 millions: number of inhabitants in France
in 2004.
(2) In 2004, the French population is estimated to
61 millions.
(3) There are 62 millions of inhabitants in France
in 2004.
Each of these values is more or less approxi-
mate. The goal is then to identify which values
are approximate and to decide which numerical
value can be used for the generation task.
For that purpose, we proposed to 20 subjects a
set of question-answer pairs. For each question,
subjects were asked to choose one answer among
a set of precise and approximate values and to
explain why. For the previous question, 75%
of the subjects prefer answer (1) because it is
the most precise one, even if they consider it as
an approximation. In majority, subjects explain
that an approximate value is sufficient for great
numbers and that values must not be rounded up
(they proposed 61.7 millions or almost 62 millions
as an answer). On the contrary, subjects do not
accept approximate values in the financial domain
(price, salary, ...) but rather prefer an interval of
values.
Thus, the direct answer is generated from the
most precise numerical value if available. If all
values are approximate, then the generated answer
has to explain it: we plan to use prepositions of
approximation (about, almost, ...) or linguistics
clues which have been extracted from web pages
(precision in the frames). The choice of a partic-
ular preposition depends on the degree of preci-
sion/approximation of numerical values: PrepNet
(Saint-Dizier, 2005) provides a relatively deep de-
scription of preposition syntactic and semantic be-
haviours.
4.2 Explanation generation
Obviously, the generation of the cooperative part
of the answer is the most complex because it re-
quires complex lexical knowledge. We present
briefly some of the necessary lexical resources.
For example, verbs can be used in the answer to
express numerical variations. Lexical descriptions
are necessary and we use for that purpose a classi-
fication of French verbs (Saint-Dizier, 1999) based
on the main classes defined by WordNet. The
classes we are interested in for our task are mainly
those of verbs of change (increase, decrease, etc.:
in total, 262 verbs in French) and of verbs of
movement (climb, move forward/backward, etc.:
in total, 252 verbs in French) used metaphori-
48 KRAQ06
cally (Moriceau and Saint-Dizier, 2003). From
these classes, we have characterized sub-classes
of growth, decrease, etc., so that the lexicalisa-
tion task is constrained by the type of verbs which
has to be used according to the variation mode (if
verbs are extracted from web pages as linguistics
clues of variation, they can also be reused in the
answer).
A deep semantics of verbs (change, movement)
is necessary to generate an answer which takes
into account the characteristics of numerical vari-
ation as well as possible: for example, the vari-
ation mode but also the speed and range of the
variation. Thus, for each sub-class of verbs and
its associated variation mode, we need a refined
description of ontological domains and selectional
restrictions so that an appropriate verb lexicalisa-
tion can be chosen: which verb can be applied to
prices, to age, etc.? We plan to use proportional se-
ries representing verb sub-classes according to the
speed and amplitude of variation. For example, the
use of climb (resp. drop) indicates a faster growth
(resp. decrease) than go up (resp. go down): the
verb climb is prefered for the generation of The
increase of gas prices climb to 20.3% in october
2005 whereas go up is prefered in The increase of
gas prices go up to 7.2% in september 2005.
As for direct answer generation, verbs can possi-
bly be associated with a preposition that refines
the information (The average age of marriage in-
creased by about 5.5 years between 1972 and
2005).
5 Conclusion
In this paper, we presented an approach for the
generation of cooperative numerical answers in a
question-answering system. Our method allows us
to generate:
(1) a correct synthetic answer over a whole set
of data and,
(2) a cooperative part which explains the varia-
tion phenomenon to the user,
whenever several numerical values are extracted
as possible answers to a question. Information is
first extracted from web pages so that numerical
values can be characterized: variation criteria and
mode are then identified in order to generate ex-
planation to the user. Several future directions
are obviously considered:
? an analysis of needs for common knowledge
so that the answer characterization task is
made easier,
? an analysis of how restrictions are lexicalized
in texts (adjectives, relative clauses, etc.) in
order to extract them easily,
? an evaluation of the knowledge costs and of
what domain specific is (especially for com-
mon knowledge about restrictions),
? an evaluation of the quality of answers pro-
posed to users and of the utility of a user
model for the selection of the best answer.
References
V. Dahl and H. Abramson. 1984. On Gapping Gram-
mars. Proceedings of the Second Logic Program-
ming Conference.
S. Harabagiu and F. Lacatusu. 2004. Strategies for
Advanced Question Answering. Proceedings of the
Workshop on Pragmatics of Question Answering at
HLT-NAACL 2004.
D. Maurel and O. Piton. 1999. Un dictionnaire
de noms propres pour Intex: les noms propres
ge?ographiques. Lingvisticae Investigationes, XXII,
pp. 277-287, John Benjamins B. V., Amsterdam.
D.L. McGuinness and P. Pinheiro da Silva. 2004.
Trusting Answers on the Web. New Directions in
Question-Answering, chapter 22, Mark T. Maybury
(ed), AAAI/MIT Press.
D. Moldovan, C. Clark, S. Harabagiu and S. Maiorano.
2003. COGEX: A Logic Prover for Question An-
swering. Proceedings of HLT-NAACL 2003.
V. Moriceau and P. Saint-Dizier. 2003. A Conceptual
Treatment of Metaphors for NLP. Proceedings of
ICON.
V. Moriceau. 2005. Answer Generation with Temporal
Data Integration. Proceedings of ENLG?05.
D.R. Radev and K.R. McKeown. 1998. Generat-
ing Natural Language Summaries from Multiple On-
Line Sources. Computational Linguistics, vol. 24,
issue 3 - Natural Language Generation, pp. 469 -
500.
P. Saint-Dizier. 1999. Alternations and Verb Semantic
Classes for French. Predicative Forms for NL and
LKB, Kluwer Academic.
P. Saint-Dizier. 2005. PrepNet: a Framework for De-
scribing Prepositions: preliminary investigation re-
sults. Proccedings of IWCS?05.
B. Webber, C. Gardent and J. Bos. 2002. Position
statement: Inference in Question Answering. Pro-
ceedings of LREC.
49 KRAQ06
Answer Generation with Temporal Data Integration
Ve?ronique Moriceau
Universite? Paul Sabatier - IRIT
31062 Toulouse cedex 09, France
moriceau@irit.fr
Abstract
In this paper, we propose an approach for con-
tent determination and surface generation of an-
swers in a question-answering system on the web.
The content determination is based on a coherence
rate which takes into account coherence with other
potential answers. Answer generation is made
through the use of classical techniques and tem-
plates and is based on a certainty degree.
1 Introduction
Search engines on the web and most of existing question-
answering systems provide the user with either a set of hyper-
links or web page extracts containing answer(s) to a question.
As provenance information (defined in [McGuinness et al,
2004] e.g., source, date, author, etc.) is rather difficult to ob-
tain, we assume that all web pages are equally reliable. Then,
the problem the system has to solve is to generate an answer
to a question even if several possible answers are selected
by the extraction engine. For this purpose, we propose to
integrate, according to certain criteria, the different possible
answers in order to generate a single coherent answer which
take into account the diversity of answers (which can be re-
dundant, incomplete, inconsistent, etc.).
As our framework is WEBCOOP [Benamara, 2004], a co-
operative question-answering system on the web, our goal is
to generate answers in natural language which explain how
confident of the answer the user can be.
In this paper, we focus on aspects of content determination
and on the generation of answers in natural language. In the
following sections, we first present the main difficulties and a
general typology of integration mechanisms. Then we anal-
yse the content determination process in the case of answers
of type date. Finally, we present briefly a few elements about
generation of integrated answers and evaluation.
2 Motivations
When a user submits a question to a classical search engine
or question-answering system, he may obtain a set of poten-
tial answers which may be incoherent to some degree: we
mean by incoherent, answers that are a priori contradictory
but which can be in fact equivalent, complementary, etc. In
this case, the user may be unsastisfied because he does not
know which answer among those proposed is the correct one.
In the following sections, we present related works and a
general typology of relations between candidate answers.
2.1 Related works
Most of existing systems on the web produce a set of an-
swers to a question in the form of hyperlinks or page extracts,
ranked according to a relevance score (for example, COGEX
[Moldovan et al, 2003]). Other systems also define relation-
ships between web page extracts or texts containing possi-
ble answers ([Harabagiu et al, 2004], [Radev et al, 1998]).
For example, [Webber et al, 2002] defines 4 relationships be-
tween possible answers:
  equivalence: equivalent answers which entail mutually,
  inclusion: one-way entailment of answers,
  aggregation: answers that are mutually consistent but
not entailing, and that can be replaced by their conjunc-
tion,
  alternative: answers that are inconsistent or alternatives
and that can be replaced by their disjunction.
Most of question-answering systems generate answers
which take into account neither information given by all can-
didate answers nor their inconsistency. This is the point we
focus on in the following section.
2.2 A general typology of integration mechanisms
To better characterise our problem, we collected, via Google
or QRISTAL [QRISTAL], a corpus of around 100 question-
answer pairs in French that reflect different inconsistency
problems. We first assume that all candidate answers are po-
tentially correct. The corpus analysis enables us to define a
general typology of relations between answers. For each rela-
tion defined in [Webber et al, 2002], we identify integration
mechanisms in order to generate answers which take into ac-
count characteristics of all candidate answers.
Inclusion
The inclusion relation exists if a candidate answer entails an-
other answer (for example, between concepts of candidate an-
swers linked in an ontology by the is-a or part-of relations).
For example, in Brittany and in France are correct an-
swers to the question Where is Brest? and Brittany is a part
of France. The content determination stage consists here in
choosing which answer will be proposed to the user - the
more specific, the more generic or all answers. This can be
guided by a user model, taking into account his knowledge.
Equivalence
Candidate answers which are linked by an equivalence rela-
tion are consistent and entail mutually. The corpus analysis
allows us to identify two main types of equivalence:
(1) Lexical equivalence: synonymy, metonymy, para-
phrases, proportional series, use of acronyms or foreign
languages. For example, to the question Who killed John
Lennon?, Mark Chapman, the murderer of John Lennon and
John Lennon?s killer Mark Chapman are equivalent answers.
(2) Equivalence with inference: in a number of cases, some
common knowledge, inferences or calculation are necessary
to detect equivalence relations. For example, The A320 is 21
and The A320 has been created in 1984 are equivalent an-
swers to the question How old is the Airbus A320?.
Aggregation
The aggregation relation defines a set of consistent answers
when the question accepts several different ones. In this case,
all candidate answers are potentially correct and can be inte-
grated in the form of a conjunction of all these answers. For
example, an answer to the question Where is Disneyland? can
be in Tokyo, Paris, Hong-Kong and Los Angeles.
If answers are numerical values, the integrated answer can
be given in the form of an interval, average or comparison.
Alternative
The alternative relation defines a set of inconsistent answers.
In the case of questions expecting a unique answer, only one
answer among candidates is correct. On the contrary, all can-
didates can be correct answers.
(1) A simple solution is to propose a disjunction of
candidate answers. For example, if the question When does
autumn begin? has the candidate answers Autumn begin on
September 21st and Autumn begins on September 20th, an
answer such as Autumn begins on either September 20th or
September 21st can be proposed.
(2) If candidate answers have common characteristics, it is
possible to integrate them according to these characteristics.
For example, the question When does the French music
festival take place? has the following answers June 1st 1982,
June 21st 1983, ..., June 21st 2004. Here, the extraction en-
gine selects pages containing the dates of all music festivals.
These candidate answers have day and month in common.
Consequently, an answer such as The French music festival
takes place every June 21st can be proposed.
(3) As for the aggregation relation, numerical values
can be integrated in the form of an interval, average or
comparison. For example, if the question How far is Paris
from Toulouse? has the candidate answers 713 km, 678
km and 681 km, answers such as Paris is at about 690 km
from Toulouse (average) or The distance between Paris
and Toulouse is between 678 and 713 km (interval) can be
proposed.
In the following sections, we focus on the content deter-
mination and generation of candidate answers of type date
linked by an aggregation or alternative relation, the most
common ones.
3 Content determination
The problem we focus on in this section is the problem of
content determination when several answers to a question of
type date are selected. We consider that candidate answers
can be in the form of date or temporal interval. A date is
defined as a vector which allows the temporal localisation of
an event. Some values of vectors can be underspecified: only
relevant values for the expected information are explicit (year,
hour, etc.). Then, an interval is a couple of dates, i.e. vectors
defining a date of beginning and a date of end.
As answers selected by the extraction engine are often in
different forms (dates or intervals or both), a first step consists
in standardizing data:
  all candidate answers are in the form of an interval: this
means that a date will be in the form of an interval hav-
ing the same date of beginning and of end,
  some candidate answers may be incomplete: for ex-
ample, year or date of end is missing, etc. In some
cases, unification with other candidate answers is pos-
sible. Otherwise, incomplete answers are omitted,
  from the semantic point of view, all candidate answers
must be in the same system of temporal reference (for
example, because of possible different time zones).
Once all candidate answers have been standardized, aber-
rant answers are filtered out by applying classical statistical
methods. Then, the answer selection process can be applied.
3.1 Answer selection process
Our goal is to select, among several candidate answers, the
best answer considered as the one which is the most coherent
with other answers. For this purpose, we define a coherence
rate of answers.
Let us assume that there are N candidate answers coming
from N different web pages. We consider that each candi-
date answer is a temporal interval
  
	
where

is the
date of beginning and

the date of end of the event. Let
  



	
with Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 958?967,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Building Event Threads out of Multiple News Articles
Xavier Tannier
LIMSI-CNRS
Univ. Paris-Sud
Orsay, France
xavier.tannier@limsi.fr
Ve?ronique Moriceau
LIMSI-CNRS
Univ. Paris-Sud
Orsay, France
moriceau@limsi.fr
Abstract
We present an approach for building multi-
document event threads from a large corpus
of newswire articles. An event thread is basi-
cally a succession of events belonging to the
same story. It helps the reader to contextual-
ize the information contained in a single arti-
cle, by navigating backward or forward in the
thread from this article. A specific effort is
also made on the detection of reactions to a
particular event.
In order to build these event threads, we use a
cascade of classifiers and other modules, tak-
ing advantage of the redundancy of informa-
tion in the newswire corpus.
We also share interesting comments con-
cerning our manual annotation procedure for
building a training and testing set1.
1 Introduction
In this paper, we explore a new way of dealing
with temporal relations between events. Our task
is somewhat between multidocument summariza-
tion and classification of temporal relations between
events. We work with a large collection of En-
glish newswire articles, where each article relates
an event: the main topic of the article is a specific
event, and other older events are mentioned in order
to put it into perspective. Thus, we consider that an
event is associated with an article and that defining
temporal relations between articles is a way to define
temporal relations between events.
1This work has been partially funded by French National
Research Agency (ANR) under project Chronolines (ANR-10-
CORD-010). We would like to thank the French News Agency
(AFP) for providing us with the corpus.
The task is to build a temporal graph of arti-
cles, linked between each other by the following re-
lations:
? Same event, when two documents relate the
same event, or when a document is an update
of another one.
? Continuation, when an event is the continua-
tion or the consequence of a previous one.
We also define a subset of continuation, called
reaction, concerning a document relating the reac-
tion of someone to another event.
Some examples of these three classes will be
given in Section 3.
These relations can be represented by a directed
graph where documents are vertices and relations
are edges (as illustrated in all figures of this article).
Figure 1 shows an example of such a graph.
Press articles, and especially newswire articles,
are characterized by an important redundancy of re-
lated events. An important event2 is likely to be
treated by several successive articles, which will
give more and more details and update some num-
bers (mainly, tragedy casualty updates, as shown in
Figure 2). On the one hand, this redundancy is an
issue since a system must not show duplicate infor-
mation to the user; on the other hand, we show in
this article that it can also be of great help in the
process of extracting temporal graphs.
In what follows, we first review some of the re-
lated work in Section 2. Section 3 presents the anno-
tation procedure and the resulting annotated corpus
2Note that we do not focus intentionally on ?important?
events. However, the fact is that minor events do hardly lead
to dense temporal graphs.
958
Figure 1: Example of ?temporal graph?: around the
Pope?s death. The associated text is the title of each ar-
ticle. Relations that can be obtained by transitivity have
been hidden for clarity?s sake.
used for developing, learning and evaluating the sys-
tem. The simple modules used to predict the same
event, continuation and, possibly, reaction relations
are described in Section 4, and results are given in
Section 5.
We also propose an end-user application to this
work. When a user reads an article, the system will
then be able to provide her with a thread of events
having occurred before or after, helping her to con-
textualize the information she is reading. This appli-
cation is described in Section 6.
2 Related work
The identification of temporal relations between
events in texts has been the focus of increasing atten-
tion because of its importance in NLP applications
such as information extraction, question-answering
or summarization. The evaluation campaigns Tem-
pEval 2007 (Verhagen et al, 2007) and TempEval
2010 (Verhagen et al, 2010) focused on temporal
relation identification, mainly on temporal relations
between events and times in the same sentence or
in consecutive sentences and between events and the
creation time of documents. In this context, the goal
is to identify the type of a temporal relation which is
Figure 2: Example of ?temporal graph?: Madrid attacks,
with many updates of the initial information. Note that
articles gathered in this main pool of articles can be pos-
terior to the continuations and reactions to the described
event.
known to be present. Systems having the best results
(accuracy about 0.6) use statistical learning based
on temporal features (modality, tense, aspect, etc.)
(Mani et al, 2006; Chambers et al, 2007). More re-
cently, Mirroshandel and Ghassem-Sani (2012) pro-
posed a new method for temporal relation extraction
by using a bootstrapping method on annotated data
and have a better accuracy than state-of-the-art sys-
tems. Their method is based on the assumption that
similar event pairs in topically related documents
are likely to have the same temporal relations. For
this work, the authors had already some collections
of topically related documents and did not need to
identify them.
In the 2012 i2b2 challenge (i2b, 2012), the
problem was not only to identify the type of tempo-
ral relations, but also to decide whether a temporal
relation existed or not between two elements, either
clinical concepts or temporal expressions. But, as
in TempEval, the temporal analysis were only to be
performed within a single document.
Other works focus on event ordering. For ex-
ample, Fujiki et al (2003) and Talukdar et al (2012)
proposed methods for automatic acquisition of event
sequences from texts. They did not use tempo-
ral information present in texts and extracted se-
quences of events (e.g. arrest/escape) from sen-
tences which were already arranged in chronologi-
959
cal order. Chambers and Jurafsky (2008) proposed
a method to learn narrative chains of events related
to a protagonist in a single document. The first
step consists in detecting narrative relations between
events sharing coreferring arguments. Then, a tem-
poral classifier orders partially the connected events
with the before relation.
Concerning the identification of the reaction re-
lation, to our knowledge, there is no work on the
detection of reaction between several documents.
Pouliquen et al (2007), Krestel et al (2008) and
Balahur et al (2009) focused on the identification of
reported speech or opinions in quotations in a docu-
ment, but not on the identification of an event which
is the source of a reaction and which can possibly be
in another document.
As we can see, all these approaches, as well as
traditional information extraction approaches, lean
on information contained by a single document, and
consider an event as a word or a phrase. However,
Ahmed et al (2011) proposed a framework to group
temporally and tocipally related news articles into
same story clusters in order to reveal the temporal
evolution of stories. But in these topically related
clusters of documents, no temporal relation is de-
tected between articles or events except chronologi-
cal order. On this point of view, our task is closer
to what is done in multidocument summarization,
where a system has to detect redundant excerpts
from various texts on the same topic and present
results in a relevant chronological order. For ex-
ample, Barzilay et al (2002) propose a system for
multidocument summarization from newswire arti-
cles describing the same event. First, similar text
units from different documents are identified using
statistical techniques and shallow text analysis and
grouped into thematic clusters. Then, in each theme,
sentences which are selected as part of the summary
are ordered using the publication date of the first oc-
currence of events to order sentences.
3 Resources
We built an annotated collection of English articles,
taken from newswire texts provided by the French
news agency (AFP), spreading over the period 2004-
2012. The entire collection contains about 1.5 mil-
lion articles. Each document is an XML file contain-
ing a title, a creation time (DCT), a set of keywords
and textual content split into paragraphs.
3.1 Selection of Article Pairs
Pairs of documents were automatically selected ac-
cording to the following constraints:
? The article describes an event. Articles such
as timelines, fact files, agendas or summaries
were discarded (all these kinds of articles were
tagged by specific keywords, making the filter-
ing easy).
? The distance between the two DCTs does not
exceed 7 days.
? There are at least 2 words in common in the set
of keywords and/or 2 proper nouns in common
in the first paragraph of each article.
These last two restrictions are important, but
necessary, in order to give annotators a chance to
find some related articles. Pure random selection of
pairs over a collection of 1.5 million articles would
be impractical.
We assume that the title and the first paragraph
describe the event associated with the document.
This is a realistic hypothesis, since the basic rules
of journalism impose that the first sentence should
summarize the event by informing on the ?5 Ws?
(What, Who, When, Where, Why). However, reading
more than the first paragraph is sometimes necessary
to determine whether a relation exists between two
events.
3.2 Relation Annotation
Two annotators were asked to attribute the following
relations between each pair of articles presented by
the annotation interface system.
In a first annotation round, 7 types of relations
were annotated:
? Three relations concerning cases where the two
articles relate the same event or an update:
? number update, when a document is an
update of numerical data (see top of Fig-
ure 5),
? form update, when the second document
brings only minor corrections,
960
Figure 3: Examples of relation continuation between two
documents.
Figure 4: Examples of relation continuation-reaction be-
tween two documents.
? details, when the second document gives
more details about the events (see bottom
of Figure 5).
? development of same story, when the two docu-
ments relate two events which are included into
a third one;
? continuation, when an event is the continuation
or the consequence of a previous one. Figure 3
shows two examples of such a relation. It is
important to make clear that a continuation re-
lation is more than a simple thematic relation,
it implies a natural prolongation between two
events. For example, two sport events of the
same Olympic Games, or two different attacks
in Iraq, shall not be linked together unless a di-
rect link between both is specified in the arti-
cles.
? reaction, a subset of continuation, when a doc-
ument relates the reaction of someone to an-
other event, as illustrated by the example in
Figure 4.
Figure 5: Example of relations same-event between two
documents: update on casualties (top) or details (bottom).
? nil, when no relation can be identified between
the two documents.
The inter-annotator agreement was calculated
with Cohen?s Kappa measure (Cohen, 1960) across
150 pairs: ? ? 0.68. The agreement was low for
the first 4 types of relations mostly because the dif-
ference between relations was not clear enough. We
therefore aggregated the number update, form up-
date and details relations into a more generic and
consensual same-event relation (see Figure 5). We
also discarded the development of same story rela-
tion, leaving only same-event, continuation and re-
action.
Annotation guidelines were modified and a sec-
ond annotation round was carried out: only the
same-event, continuation, reaction and nil relations
were annotated. Inter-annotator agreement across
150 pairs was then ? ? 0.83, which is a good agree-
ment.
3.3 Relation Set Extension
This manual annotation would have led to very
sparse temporal graphs without the two following
additional processes:
? When the annotator attributed a ?non-nil? rela-
tion to a pair of documents, the annotation sys-
tem suggested other pairs to annotate around
the concerned articles.
? Same-event and continuation relations are tran-
sitive: if A same-event B and B same-event
C, then A same-event C (and respectively for
961
Pair number Learning Evaluation
Same event 762 458 304
Continuation 1134 748 386
Reaction 182 123 59
Nil 918 614 304
TOTAL 2996 1943 1053
Table 1: Characteristics of the corpus.
continuation). Then, when the annotation was
done, a transitive closure was performed on the
entire graph, in order to get more relations with
low effort (and to detect and correct some in-
consistencies in the annotations).
Finally, almost 3,000 relations were annotated.
2{3 of the annotated pairs were used for development
and learning phases, while 1{3 were kept for evalua-
tion purpose (cf. Table 1).
4 Building Temporal Graphs
As we explained in the introduction, the main pur-
pose of this paper is to show that it is possible to ex-
tract temporal graphs of events from multiple docu-
ments in a news corpus. This is achieved with the
help of redundancy of information in this corpus.
Therefore, we will use a cascade of classifiers and
other modules, each of them using the relations de-
duced by the previous one. All modules predict a
relation between two documents (i.e., two events).
We did not focus on complex algorithms or
classifiers for tuning our results, and most of our fea-
tures are very simple. The idea here is to show that
good results can be obtained in this original and use-
ful task. The process can be separated into 3 main
stages, illustrated in Figure 6:
A. Filtering out pairs that have no relation at all, i.e.
classifying between nil and non-nil relations;
B. Classifying between same-event and continua-
tion relations;
C. Extracting reactions from the set of continuation
relations.
All described classifiers use SMO (Platt, 1998),
the SVM optimization implemented into Weka (Hall
et al, 2009), with logistic models fitting (option ?-
M?). With this option, the confidence score of each
Figure 6: A 3-step classification.
prediction can be used, while SMO alone provides a
constant probability for all instances.
From now on, when considering a pair of doc-
uments, we will refer to the older document as doc-
ument 1, and to the more recent one as document 2.
The relations found between documents will be rep-
resented by a directed graph where documents are
vertices and relations are edges.
4.1 A. Nils versus non-nils
We first aim at separating nil relations (no relation
between two events) from other relations. This step
is achieved by two successive classifiers: the first
one (A.1) uses mainly similarity measures between
documents, while the second one (A.2) uses the re-
lations obtained by the first one.
4.1.1 Step A.1: Nil classifier, level 1
Features provided to the SMO classifier at this
first step are based on 3 different similarity measures
applied to pairs of titles, pairs of first sentences,
and pairs of entire documents: cosine similarity (as
implemented by Lucene search engine3), inclusion
similarity (rate of words from element 1 present in
element 2) and overlap similarity (number of words
present in both elements). This classifier is therefore
based on only 9 features.
4.1.2 Step A.2: Nil classifier, level 2
Finding relations on a document implies that the
described event is important enough to be addressed
by several articles (same-event) or to have conse-
quences (continuation). Consequently, if we find
such relations concerning a document, we are more
likely to find more of them, because this means that
3http://lucene.apache.org
962
the document has some importance. A typical exam-
ple is shown in Figure 7, where an event described
by several documents (on the left) has many contin-
uations. For this reason, we build a second classifier
A.2 using additional features related to the relations
found at step A.1:
? Number of non-nil edges, incoming to or out-
going from document 1 (2 features); the sum of
both numbers (1 extra feature);
? Number of non-nil edges, incoming to or out-
going from document 2 (2 features); the sum of
both numbers (1 extra feature);
? Number of non-nil edges found involving one
of the two documents (i.e., the sum of all edges
described above ? 1 feature).
These figures have been computed on training
set for training, and on result of step A.1 classifier
for testing. This new information will basically help
the classifier to be more optimistic toward non-nil
relations for documents having already non-nil rela-
tions.
4.2 B. Same-event versus Continuation
We are now working only with non-nil relations
(even if some relations may switch between nil and
non-nil during the transitive closure).
4.2.1 Step B.1: Relation classifier, level 1
Distinction between same-event and continua-
tion is made by the following sets of features:
? Date features:
? Difference between the two document cre-
ation times (DCTs): difference in days, in
hours, in minutes (3 features);
? Whether the creation time of doc. 1 is
mentioned in doc. 2. For this purpose,
we use the date normalization system de-
scribed in Kessler et al (2012).
? Cosine similarity between the first sen-
tence of doc. 1 and sentences of doc. 2
containing the DCT of doc. 1.
? Cosine similarity between the first sen-
tence of doc. 1 and the most similar sen-
tence of doc. 2.
Figure 7: An example of highly-connected subgraph,
corresponding to the development of an important story.
Same events are grouped by cliques (see Section 4.2.3)
and some redundant relations are not shown for clarity?s
sake.
These last three features come from the
idea that a continuation relation can be
made explicit in text by mentioning the
first event in the second document.
? Temporal features: whether words introducing
temporal relations occur in document 1 or doc-
ument 2. These manually-collected words can
be prepositions (after, before, etc.) or verbs
(follow, confirm, prove, etc.).
? Reaction features: whether verbs introducing
reactions occur in document 1 or document 2
(25 manually-collected verbs as approve, ac-
cept, welcome, vow, etc.).
? Opinion features: whether opinion words occur
in document 1 or document 2. The list of opin-
ion words comes from the MPQA subjectivity
lexicon (Wilson et al, 2005).
Only same-event relations classified with more
than 90% confidence by the classifier are kept, in
order to ensure a high precision (recall will be im-
proved at next step). This threshold has been set up
on development set.
4.2.2 Step B.2: Relation classifier, level 2
As for step A.2, a second classifier is im-
plemented, using the results of step B.1 with the
same manner as A.2 uses A.1 (collecting numbers
of same-event and continuation relations that have
been found by the previous classifier).
4.2.3 Steps B.3 and B.4: Transitive closure by
vote
As already stated, same-event and continuation
relations are transitive. Same-event is also symmet-
ric (A same-event B ? B same-event A). In the
963
graph formed by documents (vertices) and relations
(edges), it is then possible to find all cliques, i.e. sub-
sets of vertices such that every two vertices in the
subset are connected by a same-event relation, as il-
lustrated by Figure 7.
This step does not involve any learning phase.
Starting from the result of last step, we find all same-
event cliques in the graph by using the Bron and
Kerbosch (1973) algorithm. The transitive closure
process is then illustrated by Figure 8. If the classi-
fier proposed a relation between some documents of
a clique and some other documents (as D1, D2 and
D3), then a vote is necessary:
? If the document is linked to half or more of the
clique, then all missing links are created (Fig-
ure 8.a);
? Otherwise, the document is entirely discon-
nected from the clique (Figure 8.b).
This vote is done for same-event and contin-
uation relations (resp. steps B.3 and B.4). Only
cliques containing at least 3 nodes are used. A draw-
back of this voting procedure is that the final result
may not be independent of the voting order, in some
cases. However, it is assured that the result is con-
sistent, i.e. that no document will sit in two different
cliques, or that two documents from the same clique
will not have two different relations toward a third
document.
Note that this vote leads to improvements only
if the precision of the initial classifier is sufficiently
good. As we will see in Section 5.2, this is the case
in our situation, but one must keep in mind that a
vote leaning on too imprecise material would lead to
even worse results. Some experiments on the devel-
opment set show us that at least 70% precision was
necessary. Another way to ensure robustness of the
vote would be to apply the transitive closure only
on bigger cliques (e.g., containing more than 3 or
4 nodes).
4.3 C. Continuation versus Reaction
The approach for reaction extraction is different. We
first try to determine which documents describe re-
actions, regardless of which event it is a reaction to.
In the training set, all documents having at least one
incoming reaction edge are considered as reaction
?
?
Figure 8: Vote for same-event transitive closure. At the
top (a.), four nodes from the 5-node clique are linked to
document D1, which is enough to add D1 to the clique.
At the bottom (b.), only two nodes from the clique are
linked to documents D2 and D3, which is not enough to
add them into the clique. All edges from the clique to D2
and D3 are then deleted.
documents, all others are not. This distinction is
then learned with the same model and features as
for step B.1 (Section 4.2.1).
Once reaction documents have been selected,
the question is how to decide to which other doc-
ument(s) it must be linked. For example, in Fig-
ure 1, ?Queen Elizabeth expresses deep sorrow? is
a reaction to pope?s death, not to other documents in
the temporal thread (for example, not to other reac-
tions or to ?Pope in serious condition?). We did not
manage to build any classifier leading to satisfying
results at this point. We then proposed the two fol-
lowing basic heuristics, applied on all continuation
relations found after step B:
? A reaction reacts to only one event.
? A reaction reacts to an important event. Then,
among all continuation edges incoming to
the reaction document, we choose the biggest
same-event clique and create reaction edges
instead of continuations. If there is no
clique (only single nodes) or several same-size
cliques, all of them are tagged as reactions.
This module is called step C.1. Finally, a transitive
closure is performed for reactions (C.2).
964
Relation Precision Recall F1
NIL 0.754 0.821 0.786
same-event 0.832 0.812 0.822
continuation 0.736 0.696 0.715
? reaction 0.273 0.077 0.120
Table 2: Results obtained by the baseline system. Con-
tinuation scores do not consider reactions, only the last
row makes the distinction.
5 Results
5.1 Baseline
As a baseline, we propose a single classifier deter-
mining all classes at once, based on the same SMO
classifier with the exact same parameters and all
similarity-based features (on titles, first sentences
and entire documents) described in Section 4.1.1.
Table 2 shows results for this baseline. Unsur-
prisingly, same-event relations are quite well clas-
sified by this baseline, since similarity is the major
clue for this class. Continuation is much lower and
only 3 reactions are well detected.
5.2 System Evaluation
Results for all successive steps described in previous
section are shown in Figure 3. The final result of the
entire system is the last one. The first observation
is that redundancy-based steps improve performance
in a significant manner:
? Classifiers A.2 and B.2, using the number of
incoming or outgoing edges found at previous
steps, lead to very significant improvement.
? Among transitivity closure algorithms (B.3,
B.4, C.2), only same-event transitivity B.3
leads to significant improvement. Furthermore,
as we already noticed, these algorithms must be
used only when a good precision is guaranteed
at previous step. Otherwise, there is a risk of
inferring mostly bad relations. This is why we
biased classifier at step B.1 towards precision.
Finally, if this condition on precision is true,
transitivity closure is a robust way to get new
relations for free.
Results also tell that classification of relations
same-event and continuation is encouraging. Reac-
tion level gets a fair precision but a bad recall. This
Step Relation Precision Recall F1
A. NIL vs non-NIL classifier
A.1 NIL 0.764 0.815 0.788
non-NIL 0.921 0.896 0.910
A.2 NIL 0.907 0.811 0.857
??? non-NIL 0.925 0.966 0.945
B. Same-event vs continuation classifier
B.1 NIL 0.907 0.811 0.857
same-event 0.870 0.553 0.676
continuation 0.664 0.867 0.752
B.2 NIL 0.947 0.831 0.885
??? same-event 0.894 0.724 0.800
continuation 0.744 0.911 0.819
B.3 NIL 0.884 0.831 0.857
?? same-event 0.943 0.819 0.877
continuation 0.797 0.906 0.848
B.4 NIL 0.890 0.831 0.860
? same-event 0.943 0.819 0.877
continuation 0.798 0.911 0.851
C. Reaction vs continuation
C.1 NIL 0.890 0.831 0.860
C.2 same-event 0.943 0.819 0.877
continuation 0.798 0.911 0.851
? reaction 0.778 0.359 0.491
Table 3: Results obtained at each step of the classifica-
tion process. The significance of the improvement wrt
previous step (when relevant) is indicated by the Student
t-test (?: non significant; ??: p ? 0.05 (significant); ???:
p ? 0.01 (highly significant)). Steps C.1 and C.2 are
aggregated, since their results are exactly the same.
is not catastrophic since most of the missed reactions
are tagged as continuation, which is still true (only
10% of the reaction relations are mistagged as same-
event). However, there is big room for improvement
on this point.
6 Application
As we showed in previous section, results for classi-
fication of same-event and continuation relations be-
tween documents are good enough to use this system
in an application that builds ?event threads? around
an input document. The use case is the following:
? The reader reads an article (let?s say, about the
death of John Paul II, article published on Feb.
4th, 2005 (UT) ? see Figure 1).
? A link in the page suggests the user to visualize
the event thread around this article.
965
Figure 9: An example of temporal thread obtained on the death of John Paul II for user visualization (see corresponding
relation graph in Figure 1).
? All articles within a period of 7 days around
the event, sharing at least two keywords with
the current document, are collected. All pairs
are given to the system4.
? When same-event cliques are found, only the
longest article (often, the most recent one) of
each clique is presented to the user. However,
the date and time presented to the user are those
of the first article relating the event.
? This leads to a graph with only continuation
and reaction relations. Edges are ?cleaned? so
that a unique thread is visible: relations that can
be obtained by transitivity are removed, edges
between two documents are kept only if no doc-
ument can be inserted in-between.
? Nodes are presented in chronological order.
The user can visualize and navigate through
this graph (the event thread shows only titles
but full articles can be accessed by clicking on
the node).
? When found, reactions are isolated from the
main thread.
? Such a temporal thread is potentially infinite. If
the user navigates through the end of the 7-day
window, the system must be run again on the
next time span.
4In case of very important events where ?all pairs? would be
too much, the temporal window is restrained. However, there is
no real time performance issue in this system.
Figure 9 presents the result of this process on
the partial temporal graph shown in Figure 1.
7 Conclusion
This article presents a task of multidocument tem-
poral graph building. We make the assumption that
each news article (after filtering) relates an event,
and we present a system extracting relations be-
tween articles. This system uses simple features and
algorithms but takes advantage of the important re-
dundancy of information in a news corpus, by in-
corporating redundancy information in a cascade of
classifiers, and by using transitivity of relations to
infer new links.
Finally, we present an application presenting
?event threads? to the user, in order to contextual-
ize the information and recomposing the story of an
event.
Now that the task is well defined and that en-
couraging results have been obtained, we envisage to
enrich classifiers by more fine-grained temporal and
lexical information, such as narrative chains (Cham-
bers and Jurafsky, 2008) for continuation relation
or event clustering (Barzilay et al, 2002) for same-
event relation. There is no doubt that reaction de-
tection can be improved a lot, by going beyond sim-
ple lexical features and discovering specific patterns.
We also intend to adapt the described system to other
languages than English.
966
References
A. Ahmed, Q. Ho, J. Eisenstein, E.P. Xing, A.J. Smola,
and C.H. Teo. 2011. Unified Analysis of Streaming
News. In Proceedings of WWW, Hyderabad, India.
A. Balahur, R. Steinberger, E. van der Goot,
B. Pouliquen, and M. Kabadjov. 2009. Opinion
Mining on Newspaper Quotations. In Proceedings
of International Joint Conference on Web Intelli-
gence and Intelligent Agent Technologies, Milano,
Italy.
R. Barzilay, N. Elhadad, and K.R. McKeown. 2002. In-
ferring Strategies for Sentence Ordering in Multi-
document News Summarization. Journal of Artifi-
cial Intelligence Research, 17:35?55.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications
of the ACM, 16(9):575?577.
N. Chambers and D. Jurafsky. 2008. Unsupervised
Learning of Narrative Event Chains. In Proceedings
of the 46th Annual Meeting of the ACL, Columbus,
USA.
N. Chambers, S. Wang, and D. Jurafsky. 2007. Classify-
ing temporal relations between events. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, Prague,
Czech Republic, June.
J. Cohen. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Mea-
surement, 43(6):551?558.
T. Fujiki, H. Nanba, and M. Okumura. 2003. Automatic
Acquisition of Script Knowledge from a Text Col-
lection. In Proceedings of EACL, Budapest, hun-
gary.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explo-
rations, 11(1).
2012. Proceedings of i2b2/VA Shared-Tasks and Work-
shop on Challenges in Natural Language Process-
ing for Clinical Data, Chicago, USA.
R. Kessler, X. Tannier, C. Hage`ge, V. Moriceau, and
A. Bittar. 2012. Finding Salient Dates for Build-
ing Thematic Timelines. In Proceedings of the 50th
Annual Meeting of the ACL, Jeju Island, Republic of
Korea.
R. Krestel, S. Bergler, and R. Witte. 2008. Minding the
Source: Automatic Tagging of Reported Speech in
Newspaper Articles. In Proceedings of LREC, Mar-
rakech, Morocco.
I. Mani, M. Verhagen, B. Wellner, C. Lee, and J. Puste-
jovsky. 2006. Machine learning of temporal rela-
tions. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the ACL, Sydney, Australia.
S.A. Mirroshandel and G. Ghassem-Sani. 2012. Towards
Unsupervised Learning of Temporal Relations be-
tween Events. In Journal of Artificial Intelligence
Research, volume 45.
J.C. Platt, 1998. Advances in Kernel Methods - Support
Vector Learning, chapter Fast Training of Support
Vector Machines Using Sequential Minimal Opti-
mization. MIT Press.
B. Pouliquen, R. Steinberger, and C. Best. 2007. Auto-
matic Detection of Quotations in Multilingual News.
In Proceedings of RANLP, Borovets, Bulgaria.
P.P. Talukdar, D. Wijaya, and T. Mitchell. 2012. Ac-
quiring Temporal Constraints between Relations. In
Proceedings of the 21st ACM international confer-
ence on Information and knowledge management,
Hawaii.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. SemEval-2007 -
15: TempEval Temporal Relation Identification. In
Proceedings of SemEval workshop at ACL, Prague,
Czech Republic.
M. Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky.
2010. SemEval-2010 - 13: TempEval-2. In Pro-
ceedings of SemEval workshop at ACL, Uppsala,
Sweden.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In Proceedings of HLT-EMNLP.
967
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 730?739,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Finding Salient Dates for Building Thematic Timelines
Re?my Kessler
LIMSI-CNRS
Orsay, France
kessler@limsi.fr
Xavier Tannier
Univ. Paris-Sud,
LIMSI-CNRS
Orsay, France
xtannier@limsi.fr
Caroline Hage`ge
Xerox Research Center Europe
Meylan, France
hagege@xrce.xerox.com
Ve?ronique Moriceau
Univ. Paris-Sud, LIMSI-CNRS
Orsay, France
moriceau@limsi.fr
Andre? Bittar
Xerox Research Center Europe
Meylan, France
bittar@xrce.xerox.com
Abstract
We present an approach for detecting salient
(important) dates in texts in order to auto-
matically build event timelines from a search
query (e.g. the name of an event or person,
etc.). This work was carried out on a corpus
of newswire texts in English provided by the
Agence France Presse (AFP). In order to ex-
tract salient dates that warrant inclusion in an
event timeline, we first recognize and normal-
ize temporal expressions in texts and then use
a machine-learning approach to extract salient
dates that relate to a particular topic. We fo-
cused only on extracting the dates and not the
events to which they are related.
1 Introduction
Our aim here was to build thematic timelines for
a general domain topic defined by a user query.
This task, which involves the extraction of important
events, is related to the tasks of Retrospective Event
Detection (Yang et al, 1998), or New Event Detec-
tion, as defined for example in Topic Detection and
Tracking (TDT) campaigns (Allan, 2002).
The majority of systems designed to tackle this
task make use of textual information in a bag-of-
words manner. They use little temporal informa-
tion, generally only using document metadata, such
as the document creation time (DCT). The few sys-
tems that do make use of temporal information (such
as the now discontinued Google timeline), only ex-
tract absolute, full dates (that feature a day, month
and year). In our corpus, described in Section 3.1,
we found that only 7% of extracted temporal expres-
sions are absolute dates.
We distinguish our work from that of previous re-
searchers in that we have focused primarily on ex-
tracted temporal information as opposed to other
textual content. We show that using linguistic tem-
poral processing helps extract important events in
texts. Our system extracts a maximum of temporal
information and uses only this information to detect
salient dates for the construction of event timelines.
Other types of content are used for initial thematic
document retrieval. Output is a list of dates, ranked
from most important to least important with respect
to the given topic. Each date is presented with a set
of relevant sentences.
We can see this work as a new, easily evaluable
task of ?date extraction?, which is an important com-
ponent of timeline summarization.
In what follows, we first review some of the re-
lated work in Section 2. Section 3 presents the re-
sources used and gives an overview of the system.
The system used for temporal analysis is described
in Section 4, and the strategy used for indexing and
finding salient dates, as well as the results obtained,
are given in Section 51.
2 Related Work
The ISO-TimeML language (Pustejovsky et al,
2010) is a specification language for manual anno-
tation of temporal information in texts, but, to the
best of our knowledge, it has not yet actually been
used in information retrieval systems. Neverthe-
1This work has been partially funded by French National
Research Agency (ANR) under project Chronolines (ANR-10-
CORD-010). We would like to thank the French News Agency
(AFP) for providing us with the corpus.
730
less, (Alonso et al, 2007; Alonso, 2008; Kanhabua,
2009) and (Mestl et al, 2009), among others, have
highlighted that the analysis of temporal informa-
tion is often an essential component in text under-
standing and is useful in a wide range of informa-
tion retrieval applications. (Harabagiu and Bejan,
2005; Saquete et al, 2009) highlight the importance
of processing temporal expressions in Question An-
swering systems. For example, in the TREC-10 QA
evaluation campaign, more than 10% of questions
required an element of temporal processing in order
to be correctly processed (Li et al, 2005a). In multi-
document summarization, temporal processing en-
ables a system to detect redundant excerpts from
various texts on the same topic and to present re-
sults in a relevant chronological order (Barzilay and
Elhadad, 2002). Temporal processing is also useful
for aiding medical decision-making. (Kim and Choi,
2011) present work on the extraction of temporal in-
formation in clinical narrative texts. Similarly, (Jung
et al, 2011) present an end-to-end system that pro-
cesses clinical records, detects events and constructs
timelines of patients? medical histories.
The various editions of the TDT task have given
rise to the development of different systems that de-
tect novelty in news streams (Allan, 2002; Kumaran
and Allen, 2004; Fung et al, 2005). Most of these
systems are based on statistical bag-of-words mod-
els that use similarity measures to determine prox-
imity between documents (Li et al, 2005b; Brants
et al, 2003). (Smith, 2002) used spatio-temporal in-
formation from texts to detect events from a digital
library. His method used place/time collocations and
ranked events according to statistical measures.
Some efforts have been made for automatically
building textual and graphical timelines. For ex-
ample, (Allan et al, 2001) present a system that
uses measures of pertinence and novelty to con-
struct timelines that consist of one sentence per date.
(Chieu and Lee, 2004) propose a similar system that
extracts events relevant to a query from a collection
of documents. Important events are those reported
in a large number of news articles and each event is
constructed according to one single query and rep-
resented by a set of sentences. (Swan and Allen,
2000) present an approach to generating graphical
timelines that involves extracting clusters of noun
phrases and named entities. More recently, (Yan et
al., 2011b; Yan et al, 2011a) used a summarization-
based approach to automatically generate timelines,
taking into account the evolutionary characteristics
of news.
3 Resources and System Overview
3.1 AFP Corpus
For this work, we used a corpus of newswire texts
provided by the AFP French news agency. The En-
glish AFP corpus is composed of 1.3 million texts
that span the 2004-2011 period (511 documents/day
in average and 426 millions words). Each document
is an XML file containing a title, a date of creation
(DCT), set of keywords, and textual content split
into paragraphs.
3.2 AFP Chronologies
AFP ?chronologies? (textual event timelines) are a
specific type of articles written by AFP journal-
ists in order to contextualize current events. These
chronologies may concern any topic discussed in the
media, and consist in a list of dates (typically be-
tween 10 and 20) associated with a text describing
the related event(s). Figure 1 shows an example of
such a chronology. Further examples are given in
Figure 2. We selected 91 chronologies satisfying the
following constraints:
? All dates in the chronologies are between 2004
and 2011 to be sure that the related events
are described in the corpus. For example,
?Chronology of climax to Vietnam War? was
excluded because its corresponding dates do
not appear in the content of the articles.
? All dates in the chronology are anterior to the
chronology?s creation date. For example, the
chronology ?Space in 2005: A calendar?, pub-
lished in January 2005 and listing scheduled
events, was not selected (because almost no
rocket launches finally happened on the ex-
pected day).
? The temporal granularity of the chronology is
the day. For example, ?A timeline of how the
London transport attacks unfolded?, relating
the events hour by hour, is not in our focus.
731
<NewsML Version="1.2">
<NewsItem xml:lang="en">
<HeadLine>Key dates in Thai-
land?s political crisis</HeadLine>
<DateId>20100513T100519Z</DateId>
<NameLabel>Thailand-politics</NameLabel>
<DataContent>
<p>The following is a timeline of events since
the protests began, soon after Thailand?s Supreme
Court confiscated 1.4 billion dollars of Thaksin?s
wealth for abuse of power.</p>
<p>March 14: Tens of thousands of Red Shirts
demonstrate in the capital calling for Abhisit?s gov-
ernment to step down, [...]</p>
<p>March 28: The government and the Reds en-
ter into talks but hit a stalemate after two days
[...]</p>
<p>April 3: Tens of thousands of protesters move
from Bangkok?s historic district into the city?s com-
mercial heart [...]</p>
<p>April 7: Abhisit declares state of emergency
in capital after Red Shirts storm parliament.</p>
<p>April 8: Authorities announce arrest warrants
for protest leaders.</p>
. . .
</DataContent>
</NewsItem>
</NewsML>
Figure 1: Example of an AFP manual chronology.
For learning and evaluation purposes, all
chronologies were converted to a single XML
format. Each document was manually associated
with a user search query made up of the keywords
required to retrieve the chronology.
3.3 System Overview
Figure 3 shows the general architecture of the sys-
tem. First, pre-processing of the AFP corpus tags
and normalizes temporal expressions in each of the
articles (step ? in the Figure). Next, the corpus is
indexed by the Lucene search engine2 (step ?).
Given a query, a number of documents are re-
trieved by Lucene (?). These documents can be fil-
tered (?), and dates are extracted from the remain-
ing documents. These dates are then ranked in order
to show the most important ones to the user (?), to-
2http://lucene.apache.org
- Chronology of 18 months of trouble in Ivory Coast
- Chechen rebels? history of hostage-takings
- Iraqi political wrangling since March 7 election
- Athletics: Timeline of men?s 800m world record
- Major accidents in Chinese mines
- Space in 2005: A calendar
- Developments in Iranian nuclear standoff
- Chronology of climax to Vietnam War
- Timeline of ex-IMF chief?s sex attack case
- A timeline of how the London transport attacks un-
folded
Figure 2: Examples of AFP chronologies.
Figure 3: System overview.
gether with the sentences that contain them.
4 Temporal and Linguistic Processing
In this section, we describe the linguistic and tempo-
ral information extracted during the pre-processing
phase and how the extraction is carried out. We
rely on the powerful linguistic analyzer XIP (A??t-
Mokhtar et al, 2002), that we adapted for our pur-
poses.
4.1 XIP
The linguistic analyzer we use performs a deep syn-
tactic analysis of running text. It takes as input
XML files and analyzes the textual content enclosed
in the various XML tags in different ways that are
specified in an XML guide (a file providing instruc-
tions to the parser, see (Roux, 2004) for details).
XIP performs complete linguistic processing rang-
ing from tokenization to deep grammatical depen-
dency analysis. It also performs named entity recog-
732
nition (NER) of the most usual named entity cat-
egories and recognizes temporal expressions. Lin-
guistic units manipulated by the parser are either
terminal categories or chunks. Each of these units
is associated with an attribute-value matrix that con-
tains the unit?s relevant morphological, syntactic and
semantic information. Linguistic constituents are
linked by oriented and labelled n-ary relations de-
noting syntactic or semantic properties of the input
text. A Java API is provided with the parser so that
all linguistic structures and relations can be easily
manipulated by Java code.
In the following subsections, we give details of
the linguistic information that is used for the detec-
tion of salient dates.
4.2 Named Entity Recognition
Named Entity (NE) Recognition is one of the out-
puts provided by XIP. NEs are represented as unary
relations in the parser output. We used the exist-
ing NE recognition module of the English grammar
which tags the following NE types: location names,
person names and organization names. Ambigu-
ous NE types (ambiguity between type location or
organization for country names for instance) are
also considered.
4.3 Temporal Analysis
A previous module for temporal analysis was de-
veloped and integrated into the English grammar
(Hage`ge and Tannier, 2008), and evaluated during
TempEval campaign (Verhagen et al, 2007). This
module was adapted for tagging salient dates. Our
goal with temporal analysis is to be able to tag and
normalize3 a selected subset of temporal expressions
(TEs) which we consider to be relevant for our task.
This subset of expressions is described in the follow-
ing sections.
4.3.1 Absolute Dates
Absolute dates are dates that can be normalized
without external or contextual knowledge. This is
the case, for instance, of ?On January 5th 2003?.
In these expressions, all information needed for nor-
malization is contained in the linguistic expression.
3We call normalization the operation of turning a temporal
expression into a formated, fully specified representation. This
includes finding the absolute value of relative dates.
However, absolute dates are relatively infrequent in
our corpus (7%), so in order to broaden the cover-
age for the detection of salient dates, we decided to
consider relative dates, which are far more frequent.
4.3.2 DCT-relative Dates
DCT-relative temporal expressions are those
which are relative to the creation date of the docu-
ment. This class represents 40% of dates extracted
from the AFP corpus. Unlike the absolute dates, the
linguistic expression does not provide all the infor-
mation needed for normalization. External informa-
tion is required, in particular, the date which corre-
sponds to the moment of utterance. In news articles,
this is the DCT. Two sub-classes of relative TEs can
be distinguished. The first sub-class only requires
knowledge of the DCT value to perform the normal-
ization. This is the case of expressions like next Fri-
day, which correspond to the calendar date of the
first Friday following the DCT. The second sub-class
requires further contextual knowledge for normal-
ization. For example, on Friday will correspond ei-
ther to last Friday or to next Friday depending on
the context where this expression appears (e.g. He
is expected to come on Friday corresponds to next
Friday while He arrived on Friday corresponds to
last Friday). In such cases, the tense of the verb
that governs the TE is essential for normalization.
This information is provided by the linguistic analy-
sis carried out by XIP.
4.3.3 Underspecified Dates
Considering the kind of corpus we deal with
(news), we decided to consider TEs whose granu-
larity is at least equal to a day. As a result, TEs
were normalized to a numerical YYYYMMDD for-
mat (where YYYY corresponds to the year, MM to
the month and DD to the day). In case of TEs with
a granularity superior to the day or month, DD and
MM fields remain unspecified accordingly. How-
ever, these underspecified dates are not used in our
experiments.
4.4 Modality and Reported Speech
An important issue that can affect the calculation of
salient dates is the modality associated with time-
stamped events in text. For instance, the status of a
salient date candidate in a sentence like ?The meet-
733
ing takes place on Friday? has to be distinguished
from the one in ?The meeting should take place on
Friday? or ?The meeting will take place on Friday,
Mr. Hong said?. The time-stamped event meeting
takes place is factual in the first example and can
be taken as granted. In the second and third exam-
ples, however, the event does not necessarily occur.
This is expressed by the modality introduced by the
modal auxiliary should (second example), or by the
use of the future tense or reported speech (third ex-
ample). We annotate TEs with information regard-
ing the factuality of the event they modify. More
specifically, we consider the following features:
Events that are mentioned in the future: If a
time-stamped event is in the future tense, we add a
specific attribute MODALITY with value FUTURE to
the corresponding TE annotation.
Events used with a modal verb: If a time-
stamped event is introduced by a modal verb such
as should or would, then attribute MODALITY to the
corresponding TE annotation has the value MODAL.
Reported speech verbs: Reported speech verbs
(or verbs of speaking) introduce indirect or reported
speech. We dealt with time-stamped events gov-
erned by a reported speech verb, or otherwise ap-
pearing in reported speech. Once again, XIP?s lin-
guistic analysis provided the necessary information,
including the marking of reported speech verbs and
clause segmentation of complex sentences. If a rel-
evant TE modifies a reported speech verb, the anno-
tation of this TE contains a specific attribute, DE-
CLARATION=?YES?. If the relevant TE modifies
a verb that appears in a clause introduced by a re-
ported speech verb then the annotation contains the
attribute REPORTED=?YES?.
Note that the different annotations can be com-
bined (e.g. modality and reported speech can occur
for a same time-stamped event). For example, the
TE Friday in ?The meeting should take place on Fri-
day, Mr. Hong said? is annotated with both modality
and reported speech attributes.
4.5 Corpus-dependent Special Cases
While we developed the linguistic and temporal an-
notators, we took into account some specificities of
our corpus. We decided that the TEs today and
<DCT value="20050105"/>
<EC TYPE="TIMEX" value="unknown">The year
2004</EC> was the deadliest <EC TYPE="TIMEX"
value="unknown">in a decade</EC> for journalists
around the world, mainly because of the number of reporters
killed in <EC TYPE="LOCORG">Iraq</EC>, the
media rights group <EN TYPE="ORG">Reporters
Sans Frontieres</EN> (Reporters Without Bor-
ders) said <EC TYPE="DATE" SUBTYPE="REL"
REF="ST" DECLARATION="YES" value
="20050105">Wednesday</EC>.
Figure 4: Example of XIP output for a sample article.
now were not relevant for the detection of salient
dates. In the AFP news corpus, these expressions
are mostly generic expressions synomymous with
nowadays and do not really time-stamp an event
with respect to the DCT. Another specificity of the
corpus is the fact that if the DCT corresponds to a
Monday, and if an event in a past tense is described
with the associated TE on Monday or Monday, it
means that this event occurs on the DCT day itself,
and not on the Monday before. We adapted the TE
normalizer to these special cases.
4.6 Implementation and Example
As said previously, a NER module is integrated into
the XIP parser, which we used ?as is?. The TE tag-
ger and normalizer was adapted from (Hage`ge and
Tannier, 2008). We used the Java API provided with
the parser to perform the annotation and normal-
ization of TEs. The output for the linguistic and
temporal annotation consists in XML files where
only selected information is kept (structural infor-
mation distinguishing headlines from news content,
DCT), and enriched with the linguistic annotations
described before (NEs and TEs with relevant at-
tributes corresponding to the normalization and typ-
ing). Information concerning modality, future tense
and reported speech, appears as attributes on the TE
tag. Figure 4 shows an example of an analyzed ex-
cerpt of a news article.
In this news excerpt, only one TE (Wednesday) is
normalized as both The year 2004 and in a decade
are not considered to be relevant. The first one being
a generic TE and the second one being of granular-
ity superior to a year. The annotation of the relevant
TE has the attribute indicating that it time-stamps an
event realized by a reported speech verb. The nor-
734
malized value of the TE corresponds to the 5th of
January 2005, which is a Wednesday. NEs are also
annotated.
In the entire AFP corpus, 11.5 millions temporal
expressions were detected, among which 845,000
absolute dates (7%) and 4.6 millions normalized
relative dates (40%). Although we have not yet
evaluated our tagging of relative dates, the system
on which our current date normalization is based
achieved good results in the TempEval (Verhagen et
al., 2007) campaign.
5 Experiments and Results
In Section 5.1, we propose two baseline approaches
in order to give a good idea of the difficulty of the
task (Section 5.4 also discusses this point). In Sec-
tion 5.2, we present our experiments using simple
filtering and statistics on dates calculated by Lucene.
Finally, Section 5.3 gives details of our experiments
with a learning approach. In our experiments, we
used three different values to rank dates:
? occ(d) is the number of textual units (docu-
ments or sentences) containing the date d.
? Lucene provides ranked documents together
with their relevance score. luc(d) is the sum of
Lucene scores for textual units containing the
date d.
? An adaptation of classical tf.idf for dates:
tf.idf(d) = f(d).log
N
df(d)
where f(d) is the number of occurrences of
date d in the sentence (generally, f(d) = 1), N
is the number of indexed sentences and df(d)
is the number of sentences containing date d.
In all experiments (including baselines), timelines
have been built by considering only dates between
the first and the last dates of the corresponding man-
ual chronology. Processing runs were evaluated on
manually-written chronologies (see Section 3.2) ac-
cording to Mean Average Precision (MAP), which
is a widely accepted metric for ranked lists. MAP
gives a higher weight to higher ranked elements than
lower ranked elements. Significance of evaluation
results are indicated by the p-value results of the Stu-
dent?s t-test (t(90) = 1.9867).
Baselines ?only DCTs?
Model BLoccDCT BL
luc
DCT BL
tf.idf
DCT
MAP Score 0.5036 0.5521 0.5523
Baselines ?only absolute dates?
Model BLoccabs BL
luc
abs BL
tf.idf
abs
MAP Score 0.2627 0.2782 0.2778
Baselines ?absolute dates or alternatively DCTs?
Model BLoccmix BL
luc
mix BL
tf.idf
mix
MAP Score 0.4005 0.4110 0.4135
Table 1: MAP results for baseline runs.
5.1 Baseline Runs
BLDCT . Indexing and search were done at docu-
ment level (i.e. each AFP article, with its title
and keywords, is a document). Given a query,
the top 10,000 documents were retrieved. In
these runs, only the DCT for each document
was considered. Dates were ranked by one of
the three values described above (occ, luc or
tf.idf ) leading to runs BLoccDCT , BL
luc
DCT and
BLtfidfDCT .
BLabs. Indexing and search were done at sentence
level (document title and keywords are added
to sentence text). Given a query, the top 10,000
sentences were retrieved. Only absolute dates
in these sentences were considered. We thus
obtained runs BLoccabs, BL
luc
abs and BL
tfidf
abs .
Note that in this baseline, as well as in all the
subsequent runs, the information unit was the
sentence because a date was associated to a
small part of the text. The rest of the document
generally contained text that was not related to
the specific date.
BLmix. Same as BLabs, except that sentences con-
taining no absolute dates were considered and
associated to the DCT.
Table 1 shows results for these baseline runs.
Using only DCTs with Lucene scores or tf.idf(d)
already yielded interesting results, with MAP
around 0.55.
5.2 Salient Date Extraction with XIP Results
and Simple Filtering
In these experiments, we considered a Lucene index
to be built as follows: each document was taken to
735
Model MAP Score Model MAP Score
Salient date runs with all dates
SDluc 0.6962 SDtf.idf 0.6982
Salient dates runs with filtering
SDlucR 0.6975 SD
tf.idf
R 0.6996
SDlucF 0.6967 SD
tf.idf
F 0.6993
??
SDlucM 0.6978 SD
tf.idf
M 0.7005
?
SDlucD 0.7066
?? SDtf.idfD 0.7091
??
SDlucFMD 0.7086
?? SDtf.idfFMD 0.7112
??
SDlucRFMD 0.7127
?? SDtf.idfRFMD 0.7146
??
Table 2: MAP results for salient date extraction with XIP
and simple filtering. The significance of the improvement
due to filtering wrt no filtering is indicated by the Student
t-test (?: p < 0.05 (significant); ??: p < 0.01 (highly
significant)). The improvement due to using tf.idf(d) as
opposed to occ(d) is also highly significant.
be a sentence containing a normalized date. This
sentence was indexed with the title and keywords of
the AFP article containing it. Given a query, the top
10,000 documents were retrieved. Combinations be-
tween the following filtering operations were pos-
sible, by removing all dates associated with a re-
ported speech verb (R), a modal verb (M ) and/or
a future verb (F ). All these filtering operations were
intended to remove references to events that were
not certain, thereby minimizing noise in results.
These processing runs are named SD runs, with
indices representing the filtering operations. For ex-
ample, a run obtained by filtering modal and future
verbs is called SDM,F . In all combinations, dates
were ranked by the sum of Lucene scores for these
sentences (luc) or by tf.idf4.
Table 2 presents the results for this series of ex-
periments. MAP values are much higher than for
baselines. Using tf.idf(d) is only very slightly bet-
ter than luc. Filtering operations bring significant
improvement but the benefits of these different tech-
niques have to be further investigated.
5.3 Machine-Learning Runs
We used our set of manually-written chronologies
as a training corpus to perform machine learning
experiments. We used IcsiBoost5, an implementa-
4We do not present runs where dates are ranked by the num-
ber of times they appear in retrieved sentences (occ), as we did
for baselines, since results are systematically lower.
5http://code.google.com/p/icsiboost/
tion of adaptative boosting (AdaBoost (Freund and
Schapire, 1997)).
In our approach, we consider two classes: salient
dates are dates that have an entry in the manual
chronologies, while non-salient dates are all other
dates. This choice does, however, represent an im-
portant bias. The choices of journalists are indeed
very subjective, and chronologies must not exceed a
certain length, which means that relevant dates can
be thrown away. These issues will be discussed in
Section 5.4.
The classifier instances were not all sentences re-
trieved by the search engine. Using all sentences
would not yield a useful feature set. We rather ag-
gregated all sentences corresponding to the same
date before learning the classifier. Therefore, each
instance corresponded to a single date, and features
were figures concerning the set of sentences contain-
ing this date.
Features used in this series of runs are as follows:
1. Features representing the fact that the more
a date is mentioned, the more important it is
likely to be: 1) Sum of the Lucene scores for
all sentences containing the date 2) Number of
sentences containing the date 3) Ratio between
the total weights of the date and weights of all
returned dates 4) Ratio between the frequency
of the date and frequency of all returned dates;
2. Features representing the fact that an important
event is still written about, a long time after it
occurs: 1) Distance between the date and the
most recent mention of this date 2) Distance be-
tween the date and the DCT;
3. Other features: 1) Lucene?s best ranking of the
date 2) Number of times where the date is ab-
solute in the text 3) Number of times where
the date is relative (but normalized) in the text
4) Total number of keywords of the query in the
title, sentence and named entities of retrieved
documents 5) Number of times where the date
modifies a reported speech verb or is extracted
from reported speech.
We did not aim to classify dates, but rather to rank
them. Instead, we used the predicted probability
P (d) returned by the classifier, and mixed it with
the Lucene score of sentences, or with date tf.idf :
736
Model MAP Score
Machine-Learning Runs
MLlucbase 0.7033
MLluc 0.7905 ??
MLtf.idf 0.7918 ??
Table 3: MAP results for salient date extraction with
machine-learning. MLlucbase used Lucene scores and only
the first set of features described above. MLluc and
MLtf.idf used the three sets of features. They are both
highly significant under the t-test (p ? 6.10?4) wrt re-
spectively SDluc and SDtf.idf .
score(d) = P (d)? val(d)
where val(d) is either luc(d) or tf.idf(d).
Because the task is very subjective and (above
all) because of the low quantity of learning data, we
prefered not to opt for a ?learning to rank? approach.
We evaluated this approach with a classic 4-fold
cross-validation. Our 91 chronologies were ran-
domly divided into 4 sub-samples, each of them be-
ing used once as test data. The final scores, pre-
sented in Table 3, are the average of these 4 pro-
cesses. As shown in this table, the learning approach
improves MAP results by about 0.05 point.
5.4 Discussion and Final Experiment
Chronologies hand-written by journalists are a very
useful resources for evaluation of our system, as they
are completely dissociated from our research and are
an exact representation of the output we aim to ob-
tain. However, assembling such a chronology is a
very subjective task, and no clear method for evalu-
ation agreement between two journalists seems im-
mediately apparent. Only experts can build such
chronologies, and calculating this agreement would
require at least two experts from each domain, which
are hard to come by. One may then consider our sys-
tem as a useful tool for building a chronology more
objectively.
To illustrate this point, we chose four specific top-
ics6 and showed one of our runs on each topic to an
AFP expert for these subjects. We asked him to as-
sess the first 30 dates of these runs.
6Namely, ?Arab revolt timeline for Morocco?, ?Kyrgyzs-
tan unrest timeline?, ?Lebanon?s new government: a timeline?,
?Libya timeline?.
Topic APC APE
Morocco 0.5847 0.5718
Kyrgyzstan 0.6125 0.9989
Libya 0.7856 1
Lebanon 0.4673 0.7652
Table 4: Average precision results for manual evaluation
on 4 topics, against the original chronologies (APC), and
the expert assessment (APE).
Table 4 presents results for this evaluation, com-
paring average precision values obtained 1) against
the original, manual chronologies (APC), and 2)
against the expert assessment (APE). These values
show that, for 3 runs out of 4, many dates returned
by the system are considered as valid by the expert,
even if not presented in the original chronology.
Even if this experiment is not strong enough to
lead to a formal conclusion (post-hoc evaluation
with only 4 topics and a single assessor), this tends
to show that our system produces usable outputs and
that our system can be of help to journalists by pro-
viding them with chronologies that are as useful and
objective as possible.
6 Conclusion and Future Work
This article presents a task of ?date extraction? and
shows the importance of taking temporal informa-
tion into consideration and how with relatively sim-
ple temporal processing, we were able to indirectly
point to important events using the temporal infor-
mation associated with these events. Of course, as
our final goal consists in the detection of important
events, we need to take into account the textual con-
tent. In future work, we envisage providing, together
with the detection of salient dates, a semantic analy-
sis that will help determine the importance of events.
Another interesting direction in which we soon aim
to work is to consider all textual excerpts that are as-
sociated with salient dates, and use clustering tech-
niques to determine if textual excerpts correspond to
the same event or not. Finally, as our news corpus
is available both for English and French (compara-
ble corpus, not necessarily translations), we aim to
investigate cross-lingual extraction of salient dates
and salient events.
737
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond Shallowness: Incre-
mental Deep Parsing. Natural Language Engineering,
8:121?144.
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?01, pages 10?18.
James Allan, editor. 2002. Topic Detection and Tracking.
Springer.
Omar Alonso, Ricardo Baeza-Yates, and Michael Gertz.
2007. Exploratory Search Using Timelines. In
SIGCHI 2007 Workshop on Exploratory Search and
HCI Workshop.
Omar Rogelio Alonso. 2008. Temporal information re-
trieval. Ph.D. thesis, University of California at Davis,
Davis, CA, USA. Adviser-Gertz, Michael.
Regina Barzilay and Noemie Elhadad. 2002. Infer-
ring Strategies for Sentence Ordering in Multidocu-
ment News Summarization. Journal of Artificial In-
telligence Research, 17:35?55.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Proceed-
ings of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, SIGIR ?03, pages 330?337, New York, NY,
USA. ACM.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432.
Yoav Freund and Robert E. Schapire. 1997. A Decision-
Theoretic Generalization of On-Line Learning and an
Application to Boosting. Journal of Computer and
System Sciences, 55(1):119?139.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB ?05: Proceedings
of the 31st international conference on Very large data
bases, pages 181?192.
Caroline Hage`ge and Xavier Tannier. 2008. XTM: A Ro-
bust Temporal Text Processor. In Computational Lin-
guistics and Intelligent Text Processing, proceedings
of 9th International Conference CICLing 2008, pages
231?240, Haifa, Israel, February. Springer Berlin /
Heidelberg.
Sanda Harabagiu and Cosmin Adrian Bejan. 2005.
Question Answering Based on Temporal Inference. In
Proceedings of the Workshop on Inference for Textual
Question Answering, Pittsburg, Pennsylvania, USA,
July.
Hyuckchul Jung, James Allen, Nate Blaylock, Will
de Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records: ini-
tial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011 Workshop,
BioNLP ?11, pages 146?154, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Nattiya Kanhabua. 2009. Exploiting temporal infor-
mation in retrieval of archived documents. In Pro-
ceedings of the 32nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR 2009, Boston, MA, USA, July 19-
23, 2009, page 848.
Youngho Kim and Jinwook Choi. 2011. Recogniz-
ing temporal information in korean clinical narra-
tives through text normalization. Healthc Inform Res,
17(3):150?5.
Giridhar Kumaran and James Allen. 2004. Text clas-
sification and named entities for new event detection.
In SIGIR ?04: Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 297?304.
ACM.
Wei Li, Wenjie Li, Qin Lu, and Kam-Fai Wong. 2005a.
A Preliminary Work on Classifying Time Granulari-
ties of Temporal Questions. In Proceedings of Second
international joint conference in NLP (IJCNLP 2005),
Jeju Island, Korea, oct.
Zhiwei Li, Bin Wang, Mingjing Li, and Wei-Ying Ma.
2005b. A Probabilistic Model for Restrospective
News Event Detection. In Proceedings of the 28th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, Sal-
vador, Brazil. ACM Press, New York City, NY, USA.
Thomas Mestl, Olga Cerrato, Jon ?lnes, Per Myrseth,
and Inger-Mette Gustavsen. 2009. Time Challenges -
Challenging Times for Future Information Search. D-
Lib Magazine, 15(5/6).
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. Iso-timeml: An international
standard for semantic annotation. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Claude Roux. 2004. Annoter les documents XML avec
un outil d?analyse syntaxique. In 11e`me Confrence
annuelle de Traitement Automatique des Langues Na-
turelles, Fe`s, Morocco, April. ATALA.
738
Estela Saquete, Jose L. Vicedo, Patricio Mart??nez-Barco,
Rafael Mun?oz, and Hector Llorens. 2009. Enhancing
QA Systems with Complex Temporal Question Pro-
cessing Capabilities. Journal of Articifial Intelligence
Research, 35:775?811.
David A. Smith. 2002. Detecting events with date and
place information in unstructured text. In JCDL ?02:
Proceedings of the 2nd ACM/IEEE-CS joint confer-
ence on Digital libraries, pages 191?196, New York,
NY, USA. ACM.
Russell Swan and James Allen. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?00, pages 49?56, New York, NY, USA. ACM.
Marc Verhagen, Robert Gaizauskas, Franck Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 - 15: TempEval Temporal Rela-
tion Identification. In Proceedings of SemEval work-
shop at ACL 2007, Prague, Czech Republic, June. As-
sociation for Computational Linguistics, Morristown,
NJ, USA.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xi-
aoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, 27-31 July 2011, Edinburgh, UK, pages
433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceeding
of the 34th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR 2011, Beijing, China, July 25-29, 2011, pages
745?754.
Y. Yang, T. Pierce, and J. G. Carbonell. 1998. A study on
retrospective and on-line event detection. In Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, Melbourne, Australia, August. ACM Press,
New York City, NY, USA.
739
