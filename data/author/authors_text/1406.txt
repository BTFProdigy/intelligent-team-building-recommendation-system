Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1?11, Prague, June 2007. c?2007 Association for Computational Linguistics
Modelling Compression with Discourse Constraints
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Sentence compression holds promise for
many applications ranging from summarisa-
tion to subtitle generation and subtitle gen-
eration. The task is typically performed on
isolated sentences without taking the sur-
rounding context into account, even though
most applications would operate over entire
documents. In this paper we present a dis-
course informed model which is capable of
producing document compressions that are
coherent and informative. Our model is in-
spired by theories of local coherence and
formulated within the framework of Integer
Linear Programming. Experimental results
show significant improvements over a state-
of-the-art discourse agnostic approach.
1 Introduction
The computational treatment of sentence compres-
sion has recently attracted much attention in the
literature. The task can be viewed as producing a
summary of a single sentence that retains the most
important information and remains grammatically
correct (Jing 2000). Sentence compression is com-
monly expressed as a word deletion problem: given
an input sentence of words W = w1,w2, . . . ,wn, the
aim is to produce a compression by removing any
subset of these words (Knight and Marcu 2002).
Sentence compression can potentially benefit
many applications. For example, in summarisation,
a compression mechanism could improve the con-
ciseness of the generated summaries (Jing 2000;
Lin 2003). Sentence compression could be also
used to automatically generate subtitles for tele-
vision programs; the transcripts cannot usually be
used verbatim due to the rate of speech being too
high (Vandeghinste and Pan 2004). Other applica-
tions include compressing text to be displayed on
small screens (Corston-Oliver 2001) such as mobile
phones or PDAs, and producing audio scanning de-
vices for the blind (Grefenstette 1998).
Most work to date has focused on a rather sim-
ple formulation of sentence compression that does
not allow any rewriting operations, besides word re-
moval. Moreover, compression is performed on iso-
lated sentences without taking into account their sur-
rounding context. An advantage of this simple view
is that it renders sentence compression amenable to
a variety of learning paradigms ranging from in-
stantiations of the noisy-channel model (Galley and
McKeown 2007; Knight and Marcu 2002; Turner
and Charniak 2005) to Integer Linear Programming
(Clarke and Lapata 2006a) and large-margin online
learning (McDonald 2006).
In this paper we take a closer look at one of
the simplifications associated with the compression
task, namely that sentence reduction can be realised
in isolation without making use of discourse-level
information. This is clearly not true ? professional
abstracters often rely on contextual cues while creat-
ing summaries (Endres-Niggemeyer 1998). Further-
more, determining what information is important in
a sentence is influenced by a variety of contextual
factors such as the discourse topic, whether the sen-
tence introduces new entities or events that have not
been mentioned before, and the reader?s background
knowledge.
The simplification is also at odds with most appli-
cations of sentence compression which aim to cre-
ate a shorter document rather than a single sentence.
The resulting document must not only be grammat-
1
ical but also coherent if it is to function as a re-
placement for the original. However, this cannot be
guaranteed without knowing how the discourse pro-
gresses from sentence to sentence. To give a simple
example, a contextually aware compression system
could drop a word or phrase from the current sen-
tence, simply because it is not mentioned anywhere
else in the document and is therefore deemed unim-
portant. Or it could decide to retain it for the sake of
topic continuity.
We are interested in creating a compression model
that is appropriate for documents and sentences. To
this end, we assess whether discourse-level informa-
tion is helpful. Our analysis is informed by two pop-
ular models of discourse, Centering Theory (Grosz
et al 1995) and lexical chains (Morris and Hirst
1991). Both approaches model local coherence ?
the way adjacent sentences bind together to form a
larger discourse. Our compression model is an ex-
tension of the integer programming formulation pro-
posed by Clarke and Lapata (2006a). Their approach
is conceptually simple: it consists of a scoring func-
tion coupled with a small number of syntactic and
semantic constraints. Discourse-related information
can be easily incorporated in the form of additional
constraints. We employ our model to perform sen-
tence compression throughout a whole document
(by compressing sentences sequentially) and evalu-
ate whether the resulting text is understandable and
informative using a question-answering task. Our
method yields significant improvements over a dis-
course agnostic state-of-the-art compression model
(McDonald 2006).
2 Related Work
Sentence compression has been extensively stud-
ied across different modelling paradigms and has
received both generative and discriminative formu-
lations. Most generative approaches (Galley and
McKeown 2007; Knight and Marcu 2002; Turner
and Charniak 2005) are instantiations of the noisy-
channel model, whereas discriminative formulations
include decision-tree learning (Knight and Marcu
2002), maximum entropy (Riezler et al 2003),
support vector machines (Nguyen et al 2004),
and large-margin learning (McDonald 2006). These
models are trained on a parallel corpus of long
source sentences and their target compressions. Us-
ing a rich feature set derived from parse trees, the
models learn either which constituents to delete or
which words to place adjacently in the compression
output. Relatively few approaches dispense with the
parallel corpus and generate compressions in an un-
supervised manner using either a scoring function
(Clarke and Lapata 2006a; Hori and Furui 2004) or
compression rules that are approximated from a non-
parallel corpus such as the Penn Treebank (Turner
and Charniak 2005).
Our work differs from previous approaches in two
key respects. First, we present a compression model
that is contextually aware; decisions on whether to
remove or retain a word (or phrase) are informed by
its discourse properties (e.g., whether it introduces a
new topic, whether it is semantically related to the
previous sentence). Second, we apply our compres-
sion model to entire documents rather than isolated
sentences. This is more in the spirit of real-world ap-
plications where the goal is to generate a condensed
and coherent text. Previous work on summarisation
has also utilised discourse information (e.g., Barzi-
lay and Elhadad 1997; Daume? III and Marcu 2002;
Marcu 2000; Teufel and Moens 2002). However, its
application to document compression is novel to our
knowledge.
3 Discourse Representation
Obtaining an appropriate representation of discourse
is the first step towards creating a compression
model that exploits contextual information. In this
work we focus on the role of local coherence as
this is prerequisite for maintaining global coherence.
Ideally, we would like our compressed document to
maintain the discourse flow of the original. For this
reason, we automatically annotate the source docu-
ment with discourse-level information which is sub-
sequently used to inform our compression proce-
dure. We first describe our algorithms for obtaining
discourse annotations and then present our compres-
sion model.
3.1 Centering Theory
Centering Theory (Grosz et al 1995) is an entity-
orientated theory of local coherence and salience.
Although an utterance in discourse may contain sev-
eral entities, it is assumed that a single entity is
salient or ?centered?, thereby representing the cur-
rent focus. One of the main claims underlying cen-
tering is that discourse segments in which succes-
2
sive utterances contain common centers are more
coherent than segments where the center repeatedly
changes.
Each utterance Ui in a discourse segment has a
list of forward-looking centers, C f (Ui) and a unique
backward-looking center, Cb(Ui). C f (Ui) represents
a ranking of the entities invoked by Ui according
to their salience. The Cb of the current utterance
Ui, is the highest-ranked element in C f (Ui?1) that is
also in Ui. The Cb thus links Ui to the previous dis-
course, but it does so locally since Cb(Ui) is chosen
from Ui?1.
Centering Algorithm So far we have presented
centering without explicitly stating how the con-
cepts ?utterance?, ?entities? and ?ranking? are in-
stantiated. A great deal of research has been devoted
into fleshing these out and many different instantia-
tions have been developed in the literature (see Poe-
sio et al 2004 for details). Since our aim is to iden-
tify centers in discourse automatically, our param-
eter choice is driven by two considerations, robust-
ness and ease of computation.
We therefore follow previous work (e.g., Milt-
sakaki and Kukich 2000) in assuming that the unit of
an utterance is the sentence (i.e., a main clause with
accompanying subordinate and adjunct clauses).
This is in line with our compression task which also
operates over sentences. We determine which en-
tities are invoked by a sentence using two meth-
ods. First, we perform named entity identification
and coreference resolution on each document using
LingPipe1, a publicly available system. Named en-
tities and all remaining nouns are added to the C f
list. Entity matching between sentences is required
to determine the Cb of a sentence. This is done using
the named entity?s unique identifier (as provided by
LingPipe) or by the entity?s surface form in the case
of nouns not classified as named entities.
Entities are ranked according to their grammatical
roles; subjects are ranked more highly than objects,
which are in turn ranked higher than other grammat-
ical roles (Grosz et al 1995); ties are broken using
left-to-right ordering of the grammatical roles in the
sentence (Tetreault 2001). We identify grammatical
roles with RASP (Briscoe and Carroll 2002). For-
mally, our centering algorithm is as follows (where
Ui corresponds to sentence i):
1LingPipe can be downloaded from http://www.
alias-i.com/lingpipe/.
1. Extract entities from Ui.
2. Create C f (Ui) by ranking the entities in
Ui according to their grammatical role
(subjects > objects > others).
3. Find the highest ranked entity in C f (Ui?1)
which occurs in C f (Ui), set the entity to
be Cb(Ui).
The above procedure involves several automatic
steps (named entity recognition, coreference reso-
lution, identification of grammatical roles) and will
unavoidably produce some noisy annotations. So,
there is no guarantee that the right Cb will be iden-
tified or that all sentences will be marked with a Cb.
The latter situation also occurs in passages that con-
tain abrupt changes in topic. In such cases, none of
the entities realised in Ui will occur in C f (Ui?1).
Rather than accept that discourse information may
be absent in a sentence, we turn to lexical chains
as an alternative means of capturing topical content
within a document.
3.2 Lexical Chains
Lexical cohesion refers to the degree of semantic re-
latedness observed among lexical items in a docu-
ment. The term was coined by Halliday and Hasan
(1976) who observed that coherent documents tend
to have more related terms or phrases than inco-
herent ones. A number of linguistic devices can be
used to signal cohesion; these range from repeti-
tion, to synonymy, hyponymy and meronymy. Lexi-
cal chains are a representation of lexical cohesion as
sequences of semantically related words (Morris and
Hirst 1991) and provide a useful means for describ-
ing the topic flow in discourse. For instance, a docu-
ment with many different lexical chains will prob-
ably contain several topics. And main topics will
tend to be represented by dense and long chains.
Words participating in such chains are important for
our compression task ? they reveal what the docu-
ment is about ? and in all likelihood should not be
deleted.
Lexical Chains Algorithm Barzilay and Elhadad
(1997) describe a technique for text summarisation
based on lexical chains. Their algorithm uses Word-
Net to build chains of nouns (and noun compounds).
These are ranked heuristically by a score based on
their length and homogeneity. A summary is then
produced by extracting sentences corresponding to
3
strong chains, i.e., chains whose score is two stan-
dard deviations above the average score.
Like Barzilay and Elhadad (1997), we wish to
determine which lexical chains indicate the most
prevalent discourse topics. Our assumption is that
terms belonging to these chains are indicative of the
document?s main focus and should therefore be re-
tained in the compressed output. Barzilay and El-
hadad?s scoring function aims to identify sentences
(for inclusion in a summary) that have a high con-
centration of chain members. In contrast, we are in-
terested in chains that span several sentences. We
thus score chains according to the number of sen-
tences their terms occur in. For example, the chain
{house3, home3, loft3, house5} (where wordi de-
notes word occurring in sentence i) would be given
a score of two as the terms only occur in two sen-
tences. We assume that a chain signals a prevalent
discourse topic if it occurs throughout more sen-
tences than the average chain. The scoring algorithm
is outlined more formally below:
1. Compute the lexical chains for the document.
2. Score(Chain) = Sentences(Chain).
3. Discard chains if Score(Chain) < Avg(Score).
4. Mark terms from the remaining chains as being
the focus of the document.
We use the method of Galley and McKeown (2003)
to compute lexical chains for each document.2 This
is an improved version of Barzilay and Elhadad?s
(1997) original algorithm.
Before compression takes place, all documents
are pre-processed using the centering and lexical
chain algorithms described above. In each sentence
we mark the center Cb(Ui) if one exists. Words (or
phrases) that are present in the current sentence and
function as the center in the next sentence Cb(Ui+1)
are also flagged. Finally, words are marked if they
are part of a prevalent chain. An example of our dis-
course annotation is given in Figure 1.
4 The Compression Model
Our model is an extension of the approach put for-
ward in Clarke and Lapata (2006a). Their work tack-
les sentence compression as an optimisation prob-
lem. Given a long sentence, a compression is formed
by retaining the words that maximise a scoring func-
2The software is available from http://www1.cs.
columbia.edu/?galley/.
Bad



weather dashed hopes of attempts to halt
the




flow1 during what was seen as a lull in
the lava?s momentum. Experts say that even
if the eruption stopped




today2 , the pressure of
lava piled up behind for six




miles3 would
bring debris cascading down on to the


 
town
anyway. Some estimate the volcano is pouring out
one million tons of debris a




day2 , at a




rate1
of 15




ft3 per




second2 , from a fissure that opened
in mid-December.
The Italian Army




yesterday2 detonated 400lb of
dynamite 3,500 feet up Mount Etna?s slopes.
Figure 1: Excerpt of document from our test set with
discourse annotations. Centers are in double boxes;
terms occurring in lexical chains are in oval boxes.
Words with the same subscript are members of the
same chain (e.g., today, day, second, yesterday)
tion. The latter is essentially a language model cou-
pled with a few constraints ensuring that the re-
sulting output is grammatical. The language model
and the constraints are encoded as linear inequal-
ities whose solution is found using Integer Linear
Programming (ILP, Vanderbei 2001; Winston and
Venkataramanan 2003).
We selected this model for several reasons. First
it does not require a parallel corpus and thus can be
ported across domains and text genres, whilst de-
livering state-of-the-art results (see Clarke and La-
pata 2006a for details). Second, discourse-level in-
formation can be easily incorporated by augment-
ing the constraint set. This is not the case for other
approaches (e.g., those based on the noisy channel
model) where compression is modelled by gram-
mar rules indicating which constituents to delete in a
syntactic context. Third, the ILP framework delivers
a globally optimal solution by searching over the en-
tire compression space3 without employing heuris-
tics or approximations during decoding.
We begin by recapping the formulation of Clarke
and Lapata (2006a). Let W = w1,w2, . . . ,wn denote
a sentence for which we wish to generate a com-
pression. A set of binary decision variables repre-
sent whether each word wi should be included in the
3For a sentence of length n, there are 2n compressions.
4
compression or not. Let:
yi =
{
1 if wi is in the compression
0 otherwise ?i ? [1 . . .n]
A trigram language model forms the backbone of
the compression model. The language model is for-
mulated as an integer program with the introduction
of extra decision variables indicating which word
sequences should be retained or dropped from the
compression. Let:
pi =
{
1 if wi starts the compression
0 otherwise ?i ? [1 . . .n]
qi j =
?
?
?
1 if sequence wi,w j ends
the compression ?i ? [1 . . .n?1]
0 otherwise ? j ? [i+ 1 . . .n]
xi jk =
?
?
?
1 if sequence wi,w j,wk ?i ? [1 . . .n?2]
is in the compression ? j ? [i+ 1 . . .n?1]
0 otherwise ?k ? [ j + 1 . . .n]
The objective function is expressed in Equa-
tion (1). It is the sum of all possible trigrams mul-
tiplied by the appropriate decision variable. The ob-
jective function also includes a significance score for
each word multiplied by the decision variable for
that word (see the last summation term in (1)). This
score highlights important content words in a sen-
tence and is defined in Section 4.1.
maxz =
n
?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j)
+
n
?
i=1
yi ? I(wi) (1)
subject to:
yi, pi,qi j,xi jk = 0 or 1 (2)
A set of sequential constraints4 are added to the
problem to only allow results which combine valid
trigrams.
4We have omitted sequential constraints due to space limi-
tations. The full details are given in Clarke and Lapata (2006a).
4.1 Significance Score
The significance score is an attempt at capturing the
gist of a sentence. It gives more weight to content
words that appear in the deepest level of embed-
ding in the syntactic tree representing the source
sentence:
I(wi) =
l
N
? fi log FaFi (3)
The score is computed over a large corpus where wi
is a content word (i.e., a noun or verb), fi and Fi are
the frequencies of wi in the document and corpus
respectively, and Fa is the sum of all content words
in the corpus. l is the number of clause constituents
above wi, and N is the deepest level of embedding.
4.2 Sentential Constraints
The model also contains a small number of
sentence-level constraints. Their aim is to preserve
the meaning and structure of the original sentence
as much as possible. The majority of constraints
revolve around modification and argument struc-
ture and are defined over parse trees or gram-
matical relations. For example, the following con-
straint template disallows the inclusion of modifiers
(e.g., nouns, adjectives) without their head words:
yi ? y j ? 0 (4)
?i, j : w j modifies wi
Other constraints force the presence of modifiers
when the head is retained in the compression. This
way, it is ensured that negation will be preserved in
the compressed output:
yi ? y j = 0 (5)
?i, j : w j modifies wi ? w j = not
Argument structure constraints make sure that
the resulting compression has a canonical argument
structure. For instance a constraint ensures that if a
verb is present in the compression then so are its ar-
guments:
yi ? y j = 0 (6)
?i, j : w j ? subject/object of verb wi
Finally, Clarke and Lapata (2006a) propose one
discourse constraint which forces the system to pre-
serve personal pronouns in the compressed output:
yi = 1 (7)
?i : wi ? personal pronouns
5
4.3 Discourse Constraints
In addition to the constraints described above, our
model includes constraints relating to the centering
and lexical chains representations discussed in Sec-
tion 3. Recall that after some pre-processing, each
sentence is marked with: its own center Cb(Ui), the
center Cb(Ui+1) of the sentence following it and
words that are members of high scoring chains cor-
responding to the document?s focus. We introduce
two new types of constraints based on these addi-
tional knowledge sources.
The first constraint is the centering constraint
which operates over adjacent sentences. It ensures
that the Cb identified in the source sentence is re-
tained in the target compression. If present, the en-
tity realised as the Cb in the following sentence is
also retained:
yi = 1 (8)
?i : wi ? {Cb(Ui),Cb(Ui+1)}
Consider for example the discourse in Figure 1. The
constraints generated from Equation (8) will require
the compression to retain lava in the first two sen-
tences and debris in sentences two and three.
We also add a lexical chain constraint. This ap-
plies only to nouns which are members of prevalent
chains:
yi = 1 (9)
?i : wi ? document focus lexical chain
This constraint is complementary to the centering
constraint; the sentences it applies to do not have to
be adjacent and the entities under consideration are
not restricted to a specific syntactic role (e.g., sub-
ject or object). See for instance the words flow and
rate in Figure 1 which are members of the same
chain (marked with subscript one). According to
constraint (9) both words must be included in the
compressed document.
The constraints just described ensure that the
compressed document will retain the discourse flow
of the original and will preserve terms indicative
of important topics. We argue that these constraints
will additionally benefit sentence-level compres-
sion, as words which are not signalled as discourse
relevant can be dropped.
4.4 Applying the Constraints
Our compression system is given a (sentence sepa-
rated) document as input. The ILP model just pre-
sented is then applied sequentially to all sentences
to generate a compressed version of the original. We
thus create and solve an ILP for every sentence.5 In
the formulation of Clarke and Lapata (2006a) a sig-
nificance score (see Section 4.1) highlights which
nouns and verbs to include in the compression. As
far as nouns are concerned, our discourse constraints
perform a similar task. Thus, when a sentence con-
tains discourse annotations, we are inclined to trust
them more and only calculate the significance score
for verbs.
During development it was observed that apply-
ing all discourse constraints simultaneously (see
Equations (7)?(9)) results in relatively long com-
pressions. To counter this, we employ these con-
straints using a back-off strategy that relies on pro-
gressively less reliable information. Our back-off
model works as follows: if centering information is
present, we apply the appropriate constraints (Equa-
tion (8)). If no centers are present, we back-off to the
lexical chain information using Equation (9), and in
the absence of the latter we back-off to the pronoun
constraint (Equation (7)). Finally, if discourse infor-
mation is entirely absent from the sentence, we de-
fault to the significance score. Sentential constraints
(see Section 4.2) are applied throughout irrespec-
tively of discourse constraints. In our test data (see
Section 5 for details), the centering constraint was
used in 68.6% of the sentences. The model backed
off to lexical chains for 13.7% of the test sentences,
whereas the pronoun constraint was applied in 8.5%.
Finally, the noun and verb significance score was
used on the remaining 9.2%. An example of our sys-
tem?s output for the text in Figure 1 is given in Fig-
ure 2.
5 Experimental Set-up
In this section we present our experimental set-up.
We briefly introduce the model used for compar-
ison with our approach and give details regarding
our compression corpus and parameter estimation.
Finally, we describe our evaluation methodology.
5We use the publicly available lp solve solver (http://
www.geocities.com/lpsolve/).
6
Bad weather dashed hopes to halt the flow during
what was seen as lull in lava?s momentum. Ex-
perts say that even if eruption stopped, the pres-
sure of lava piled would bring debris cascading.
Some estimate volcano is pouring million tons of
debris from fissure opened in mid-December. The
Army yesterday detonated 400lb of dynamite.
Figure 2: System output on excerpt from Figure 1.
Comparison with state-of-the-art An obvious
evaluation experiment would involve comparing
the ILP model without any discourse constraints
against the discourse informed model presented in
this work. Unfortunately, the two models obtain
markedly different compression rates6 which ren-
ders the comparison of their outputs problematic. To
put the comparison on an equal footing, we evalu-
ated our approach against a state-of-the-art model
that achieves a compression rate similar to ours
without taking discourse-level information into ac-
count. McDonald (2006) formalises sentence com-
pression in a discriminative large-margin learning
framework as a classification task: pairs of words
from the source sentence are classified as being ad-
jacent or not in the target compression. A large
number of features are defined over words, parts
of speech, phrase structure trees and dependen-
cies. These are gathered over adjacent words in the
compression and the words in-between which were
dropped.
It is important to note that McDonald (2006) is not
a straw-man system. It achieves highly competitive
performance compared with Knight and Marcu?s
(2002) noisy channel and decision tree models. Due
to its discriminative nature, the model is able to use
a large feature set and to optimise compression ac-
curacy directly. In other words, McDonald?s model
has a head start against our own model which does
not utilise a parallel corpus and has only a few con-
straints. The comparison of the two systems allows
us to investigate whether discourse information is re-
dundant when using a powerful sentence compres-
sion model.
Corpus Previous work on sentence compres-
sion has used almost exclusively the Ziff-Davis,
6The discourse agnostic ILP model has a compression rate
of 81.2%; when discourse constraints are include the rate drops
to 65.4%.
a compression corpus derived automatically from
document-abstract pairs (Knight and Marcu 2002).
Unfortunately, this corpus is not suitable for our
purposes since it consists of isolated sentences. We
thus created a document-based compression corpus
manually. Following Clarke and Lapata (2006a), we
asked annotators to produce compressions for 82
stories (1,629 sentences) from the BNC and the LA
Times Washington Post.7 48 documents (962 sen-
tences) were used for training, 3 for development (63
sentences), and 31 for testing (604 sentences).
Parameter Estimation Our parameters for the
ILP model followed closely Clarke and Lapata
(2006a). We used a language model trained on
25 million tokens from the North American News
corpus. The significance score was based on 25
million tokens from the same corpus. Our re-
implementation of McDonald (2006) used an identi-
cal feature set, and a slightly modified loss function
to encourage compression on our data set.8
Evaluation Previous studies evaluate how well-
formed the automatically derived compressions are
out of context. The target sentences are typi-
cally rated by naive subjects on two dimensions,
grammaticality and importance (Knight and Marcu
2002). Automatic evaluation measures have also
been proposed. Riezler et al (2003) compare the
grammatical relations found in the system output
against those found in a gold standard using F-score
which Clarke and Lapata (2006b) show correlates
reliably with human judgements.
Following previous work, sentence-based com-
pressions were evaluated automatically using F-
score computed over grammatical relations which
we obtained by RASP (Briscoe and Carroll 2002).
Besides individual sentences, our goal was to evalu-
ate the compressed document as whole. Our evalu-
ation methodology was motivated by two questions:
(1) are the documents readable? and (2) how much
key information is preserved between the source
document and its target compression? We assume
here that the compressed document is to function as
a replacement for the original. We can thus measure
the extent to which the compressed version can be
7The corpus is available from http://homepages.inf.
ed.ac.uk/s0460084/data/.
8McDonald?s (2006) results are reported on the Ziff-Davis
corpus.
7
What is posing a threat to the town? (lava)
What hindered attempts to stop the lava flow?
(bad weather)
What did the Army do first to stop the lava flow?
(detonate explosives)
Figure 3: Example questions with answer key.
used to find answers for questions which are derived
from the original and represent its core content.
We therefore employed a question-answering
evaluation paradigm which has been previously used
for summarisation evaluation and text comprehen-
sion (Mani et al 2002; Morris et al 1992). The
overall objective of our Q&A task is to determine
how accurate each document (generated by differ-
ent compression systems) is at answering questions.
For this we require a methodology for constructing
Q&A pairs and for scoring each document.
Two annotators were independently instructed
to create Q&A pairs for the original documents
in the test set. Each annotator read the document
and then drafted no more than ten questions and
answers related to its content. Annotators were
asked to create factual-based questions which re-
quired an unambiguous answer; these were typically
who/what/where/when/how style questions. Anno-
tators then compared and revised their question-
answer pairs to create a common agreed upon set.
Revisions typically involved merging questions, re-
wording and simplifying questions, and in some
cases splitting a question into multiple questions.
Documents for which too few questions were cre-
ated or for which questions or answers were too am-
biguous were removed. This left an evaluation set
of six documents with between five to eight con-
cise questions per document. Some example ques-
tions corresponding to the document from Figure 1
are given in Figure 3; correct answers are shown in
parentheses.
Compressed documents and their accompanying
questions were presented to human subjects who
were asked to provide answers as best they could.
We elicited answers for six documents in three com-
pression conditions: gold standard, using the ILP
discourse model, and McDonald?s (2006) model.
Each participant was also asked to rate the readabil-
ity of the compressed document on a seven point
scale. A Latin Square design prevented participants
Model CompR F-Score
McDonald 60.1% 36.0%?
Discourse ILP 65.4% 39.6%
Gold Standard 70.3% ??
Table 1: Compression results: compression rate and
relation-based F-score; ? sig. diff. from Discourse
ILP (p < 0.05 using the Student t test).
Model Readability Q&A
McDonald 2.6? 53.7%??
Discourse ILP 3.0? 68.3%
Gold Standard 5.5? 80.7%
Table 2: Human Evaluation Results: average read-
ability ratings and average percentage of questions
answered correctly. ?: sig. diff. from Gold Standard;
?: sig. diff. from Discourse ILP.
from seeing two different compressions of the same
document.
The study was conducted remotely over the In-
ternet. Participants were presented with a set of in-
structions that explained the Q&A task and provided
examples. Subjects were first asked to read the com-
pressed document and rate its readability. Questions
were then presented one at a time and participants
were allowed to consult the document for the an-
swer. Once a participant had provided an answer
they were not allowed to modify it. Thirty unpaid
volunteers took part in our Q&A study. All were self
reported native English speakers.
The answers provided by the participants were
scored against the answer key. Answers were con-
sidered correct if they were identical to the answer
key or subsumed by it. For instance, Mount Etna
was considered a right answer to the first question
from Figure 3. A compressed document receives a
full score if subjects have answered all questions re-
lating to it correctly.
6 Results
As a sanity check, we first assessed the compres-
sions produced by our model and McDonald (2006)
on a sentence-by-sentence basis without taking the
documents into account. There is no hope for gener-
ating shorter documents if the compressed sentences
are either too wordy or too ungrammatical. Table 1
shows the compression rates (CompR) for the two
8
systems and evaluates the quality of their output us-
ing F-score based on grammatical relations. As can
be seen, the Discourse ILP compressions are slightly
longer than McDonald (65.4% vs. 60.1%) but closer
to the human gold standard (70.3%). This is not sur-
prising, the Discourse ILP model takes the entire
document into account, and compression decisions
will be slightly more conservative. The Discourse
ILP?s output is significantly better than McDonald in
terms of F-score, indicating that discourse-level in-
formation is generally helpful. Both systems could
use further improvement as inter-annotator agree-
ment on this data yields an F-score of 65.8%.
Let us now consider the results of our document-
based evaluation. Table 2 shows the mean readabil-
ity ratings obtained for each system and the per-
centage of questions answered correctly. We used
an Analysis of Variance (ANOVA) to examine the ef-
fect of compression type (McDonald, Discourse ILP,
Gold Standard). The ANOVA revealed a reliable ef-
fect on both readability and Q&A. Post-hoc Tukey
tests showed that McDonald and the Discourse ILP
model do not differ significantly in terms of read-
ability. However, they are significantly less read-
able than the gold standard (? < 0.05). For the Q&A
task we observe that our system is significantly bet-
ter than McDonald (? < 0.05) and not significantly
worse than the gold standard.
These results indicate that the automatic systems
lag behind the human gold standard in terms of
readability. When reading entire documents, sub-
jects are less tolerant of ungrammatical construc-
tions. We also find out that despite relatively low
readability, the documents are overall understand-
able. The discourse informed model generates more
informative documents ? the number of questions
answered correctly increases by 15% in comparison
to McDonald. This is an encouraging result suggest-
ing that there may be advantages in developing com-
pression models that exploit contextual information.
7 Conclusions and Future Work
In this paper we proposed a novel method for au-
tomatic sentence compression. Central in our ap-
proach is the use of discourse-level information
which we argue is an important prerequisite for doc-
ument (as opposed to sentence) compression. Our
model uses integer programming for inferring glob-
ally optimal compressions in the presence of lin-
guistically motivated constraints. Our discourse con-
straints aim to capture local coherence and are in-
spired by centering theory and lexical chains. We
showed that our model can be successfully em-
ployed to produce compressed documents that pre-
serve most of the original?s core content.
Our approach to document compression differs
from most summarisation work in that our sum-
maries are fairly long. However, we believe this is
the first step into understanding how compression
can help summarisation. In the future, we will in-
terface our compression model with sentence ex-
traction. The discourse annotations can help guide
the extraction method into selecting topically re-
lated sentences which can consequently be com-
pressed together. The compression rate can be tai-
lored through additional constraints which act on
the output length to ensure precise word limits are
obeyed.
We also plan to study the effect of global dis-
course structure (Daume? III and Marcu 2002) on the
compression task. In general, we will assess the im-
pact of discourse information more systematically
by incorporating it into generative and discrimina-
tive modelling paradigms.
Acknowledgements We are grateful to Ryan Mc-
Donald for his help with the re-implementation of
his system and our annotators Vasilis Karaiskos
and Sarah Luger. Thanks to Simone Teufel, Alex
Lascarides, Sebastian Riedel, and Bonnie Web-
ber for insightful comments and suggestions. La-
pata acknowledges the support of EPSRC (grant
GR/T04540/01).
References
Barzilay, R. and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proceedings of
the Intelligent Scalable Text Summarization Work-
shop (ISTS), ACL-97.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC?2002).
Las Palmas, Gran Canaria, pages 1499?1504.
Clarke, James and Mirella Lapata. 2006a.
Constraint-based sentence compression: An
integer programming approach. In Proceedings
of the COLING/ACL 2006 Main Conference
9
Poster Sessions. Sydney, Australia, pages
144?151.
Clarke, James and Mirella Lapata. 2006b. Models
for sentence compression: A comparison across
domains, training requirements and evaluation
measures. In Proceedings of the 21st Inter-
national Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association
for Computational Linguistics. Sydney, Australia,
pages 377?384.
Corston-Oliver, Simon. 2001. Text Compaction for
Display on Very Small Screens. In Proceedings of
the NAACL Workshop on Automatic Summariza-
tion. Pittsburgh, PA, pages 89?98.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-
channel model for document compression. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL
2002). Philadelphia, PA, pages 449?456.
Endres-Niggemeyer, Brigitte. 1998. Summarising
Information. Springer, Berlin.
Galley, Michel and Kathleen McKeown. 2003.
Improving word sense disambiguation in lexi-
cal chaining. In Proceedings of 18th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?03). pages 1486?1488.
Galley, Michel and Kathleen McKeown. 2007. Lex-
icalized markov grammars for sentence compres-
sion. In In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL-HLT?2007). Rochester, NY.
Grefenstette, Gregory. 1998. Producing Intelligent
Telegraphic Text Reduction to Provide an Audio
Scanning Service for the Blind. In Proceedings of
the AAAI Symposium on Intelligent Text Summa-
rization. Stanford, CA, pages 111?117.
Grosz, Barbara J., Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for modeling
the local coherence of discourse. Computational
Linguistics 21(2):203?225.
Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman, London.
Hori, Chiori and Sadaoki Furui. 2004. Speech sum-
marization: an approach through word extraction
and a method for evaluation. IEICE Transactions
on Information and Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
6th conference on Applied Natural Language Pro-
cessing (ANLP?2000). Seattle, WA, pages 310?
315.
Knight, Kevin and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artificial
Intelligence 139(1):91?107.
Lin, Chin-Yew. 2003. Improving summarization
performance by sentence compression ? a pilot
study. In Proceedings of the 6th International
Workshop on Information Retrieval with Asian
Languages. Sapporo, Japan, pages 1?8.
Mani, Inderjeet, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
2002. SUMMAC: A text summarization evalua-
tion. Natural Language Engineering 8(1):43?68.
Marcu, Daniel. 2000. The Theory and Practice of
Discourse Parsing and Summarization. The MIT
Press, Cambridge, MA.
McDonald, Ryan. 2006. Discriminative sentence
compression with soft syntactic constraints. In
Proceedings of the 11th EACL. Trento, Italy.
Miltsakaki, Eleni and Karen Kukich. 2000. The
role of centering theory?s rough-shift in the teach-
ing and evaluation of writing skills. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?2000).
pages 408?415.
Morris, A., G. Kasper, and D. Adams. 1992. The
effects and limitations of automated text condens-
ing on reading comprehension performance. In-
formation Systems Research 3(1):17?35.
Morris, Jane and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indi-
cator of the structure of text. Computational Lin-
guistics 17(1):21?48.
Nguyen, Minh Le, Akira Shimazu, Susumu
Horiguchi, Tu Bao Ho, and Masaru Fukushi.
2004. Probabilistic sentence reduction using
support vector machines. In Proceedings of
the 20th COLING. Geneva, Switzerland, pages
743?749.
Poesio, Massimo, Rosemary Stevenson, Barbara Di
Eugenio, and Janet Hitzeman. 2004. Centering: a
10
parametric theory and its instantiations. Compu-
tational Linguistics 30(3):309?363.
Riezler, Stefan, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence con-
densation using ambiguity packing and stochas-
tic disambiguation methods for lexical-functional
grammar. In Proceedings of the HLT/NAACL. Ed-
monton, Canada, pages 118?125.
Tetreault, Joel R. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics 27(4):507?520.
Teufel, Simone and Marc Moens. 2002. Summa-
rizing scientific articles ? experiments with rele-
vance and rhetorical status. Computational Lin-
guistics 28(4):409?446.
Turner, Jenine and Eugene Charniak. 2005. Su-
pervised and unsupervised learning for sentence
compression. In Proceedings of the 43rd ACL.
Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence
compression for automated subtitling: A hybrid
approach. In Proceedings of the ACL Workshop
on Text Summarization. Barcelona, Spain, pages
89?95.
Vanderbei, Robert J. 2001. Linear Programming:
Foundations and Extensions. Kluwer Academic
Publishers, Boston, 2nd edition.
Winston, Wayne L. and Munirpallam Venkatara-
manan. 2003. Introduction to Mathematical Pro-
gramming. Brooks/Cole.
11
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 377?384,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Models for Sentence Compression: A Comparison across Domains,
Training Requirements and Evaluation Measures
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Sentence compression is the task of pro-
ducing a summary at the sentence level.
This paper focuses on three aspects of
this task which have not received de-
tailed treatment in the literature: train-
ing requirements, scalability, and auto-
matic evaluation. We provide a novel com-
parison between a supervised constituent-
based and an weakly supervised word-
based compression algorithm and exam-
ine how these models port to different do-
mains (written vs. spoken text). To achieve
this, a human-authored compression cor-
pus has been created and our study high-
lights potential problems with the auto-
matically gathered compression corpora
currently used. Finally, we assess whether
automatic evaluation measures can be
used to determine compression quality.
1 Introduction
Automatic sentence compression has recently at-
tracted much attention, in part because of its affin-
ity with summarisation. The task can be viewed
as producing a summary of a single sentence that
retains the most important information while re-
maining grammatically correct. An ideal compres-
sion algorithm will involve complex text rewriting
operations such as word reordering, paraphrasing,
substitution, deletion, and insertion. In default of
a more sophisticated compression algorithm, cur-
rent approaches have simplified the problem to a
single rewriting operation, namely word deletion.
More formally, given an input sentence of words
W = w1,w2, . . . ,wn, a compression is formed bydropping any subset of these words. Viewing the
task as word removal reduces the number of pos-
sible compressions to 2n; naturally, many of these
compressions will not be reasonable or grammati-
cal (Knight and Marcu 2002).
Sentence compression could be usefully em-
ployed in wide range of applications. For exam-
ple, to automatically generate subtitles for televi-
sion programs; the transcripts cannot usually be
used verbatim due to the rate of speech being too
high (Vandeghinste and Pan 2004). Other applica-
tions include compressing text to be displayed on
small screens (Corston-Oliver 2001) such as mo-
bile phones or PDAs, and producing audio scan-
ning devices for the blind (Grefenstette 1998).
Algorithms for sentence compression fall into
two broad classes depending on their training re-
quirements. Many algorithms exploit parallel cor-
pora (Jing 2000; Knight and Marcu 2002; Riezler
et al 2003; Nguyen et al 2004a; Turner and Char-
niak 2005; McDonald 2006) to learn the corre-
spondences between long and short sentences in
a supervised manner, typically using a rich feature
space induced from parse trees. The learnt rules
effectively describe which constituents should be
deleted in a given context. Approaches that do
not employ parallel corpora require minimal or
no supervision. They operationalise compression
in terms of word deletion without learning spe-
cific rules and can therefore rely on little linguistic
knowledge such as part-of-speech tags or merely
the lexical items alone (Hori and Furui 2004). Al-
ternatively, the rules of compression are approxi-
mated from a non-parallel corpus (e.g., the Penn
Treebank) by considering context-free grammar
derivations with matching expansions (Turner and
Charniak 2005).
Previous approaches have been developed and
tested almost exclusively on written text, a no-
table exception being Hori and Furui (2004) who
focus on spoken language. While parallel cor-
pora of original-compressed sentences are not nat-
urally available in the way multilingual corpora
are, researchers have obtained such corpora auto-
matically by exploiting documents accompanied
by abstracts. Automatic corpus creation affords
the opportunity to study compression mechanisms
377
cheaply, yet these mechanisms may not be repre-
sentative of human performance. It is unlikely that
authors routinely carry out sentence compression
while creating abstracts for their articles. Collect-
ing human judgements is the method of choice for
evaluating sentence compression models. How-
ever, human evaluations tend to be expensive and
cannot be repeated frequently; furthermore, com-
parisons across different studies can be difficult,
particularly if subjects employ different scales, or
are given different instructions.
In this paper we examine some aspects of the
sentence compression task that have received lit-
tle attention in the literature. First, we provide a
novel comparison of supervised and weakly su-
pervised approaches. Specifically, we study how
constituent-based and word-based methods port to
different domains and show that the latter tend to
be more robust. Second, we create a corpus of
human-authored compressions, and discuss some
potential problems with currently used compres-
sion corpora. Finally, we present automatic evalu-
ation measures for sentence compression and ex-
amine whether they correlate reliably with be-
havioural data.
2 Algorithms for Sentence Compression
In this section we give a brief overview of the algo-
rithms we employed in our comparative study. We
focus on two representative methods, Knight and
Marcu?s (2002) decision-based model and Hori
and Furui?s (2004) word-based model.
The decision-tree model operates over parallel
corpora and offers an intuitive formulation of sen-
tence compression in terms of tree rewriting. It
has inspired many discriminative approaches to
the compression task (Riezler et al 2003; Nguyen
et al 2004b; McDonald 2006) and has been
extended to languages other than English (see
Nguyen et al 2004a). We opted for the decision-
tree model instead of the also well-known noisy-
channel model (Knight and Marcu 2002; Turner
and Charniak 2005). Although both models yield
comparable performance, Turner and Charniak
(2005) show that the latter is not an appropriate
compression model since it favours uncompressed
sentences over compressed ones.1
Hori and Furui?s (2004) model was originally
developed for Japanese with spoken text in mind,
1The noisy-channel model uses a source model trainedon uncompressed sentences. This means that the most likelycompressed sentence will be identical to the original sen-tence as the likelihood of a constituent deletion is typicallyfar lower than that of leaving it in.
SHIFT transfers the first word from the input list ontothe stack.
REDUCE pops the syntactic trees located at the topof the stack, combines them into a new tree and thenpushes the new tree onto the top of the stack.
DROP deletes from the input list subsequences of wordsthat correspond to a syntactic constituent.
ASSIGNTYPE changes the label of the trees at the topof the stack (i.e., the POS tag of words).
Table 1: Stack rewriting operations
it requires minimal supervision, and little linguis-
tic knowledge. It therefor holds promise for lan-
guages and domains for which text processing
tools (e.g., taggers, parsers) are not readily avail-
able. Furthermore, to our knowledge, its perfor-
mance on written text has not been assessed.
2.1 Decision-based Sentence Compression
In the decision-based model, sentence compres-
sion is treated as a deterministic rewriting process
of converting a long parse tree, l, into a shorter
parse tree s. The rewriting process is decomposed
into a sequence of shift-reduce-drop actions that
follow an extended shift-reduce parsing paradigm.
The compression process starts with an empty
stack and an input list that is built from the orig-
inal sentence?s parse tree. Words in the input list
are labelled with the name of all the syntactic con-
stituents in the original sentence that start with it.
Each stage of the rewriting process is an operation
that aims to reconstruct the compressed tree. There
are four types of operations that can be performed
on the stack, they are illustrated in Table 1.
Learning cases are automatically generated
from a parallel corpus. Each learning case is ex-
pressed by a set of features and represents one of
the four possible operations for a given stack and
input list. Using the C4.5 program (Quinlan 1993)
a decision-tree model is automatically learnt. The
model is applied to a parsed original sentence in
a deterministic fashion. Features for the current
state of the input list and stack are extracted and
the classifier is queried for the next operation to
perform. This is repeated until the input list is
empty and the stack contains only one item (this
corresponds to the parse for the compressed tree).
The compressed sentence is recovered by travers-
ing the leaves of the tree in order.
2.2 Word-based Sentence Compression
The decision-based method relies exclusively on
parallel corpora; the caveat here is that appropri-
ate training data may be scarce when porting this
model to different text domains (where abstracts
378
are not available for automatic corpus creation) or
languages. To alleviate the problems inherent with
using a parallel corpus, we have modified a weakly
supervised algorithm originally proposed by Hori
and Furui (2004). Their method is based on word
deletion; given a prespecified compression length,
a compression is formed by preserving the words
which maximise a scoring function.
To make Hori and Furui?s (2004) algorithm
more comparable to the decision-based model, we
have eliminated the compression length parameter.
Instead, we search over all lengths to find the com-
pression that gives the maximum score. This pro-
cess yields more natural compressions with vary-
ing lengths. The original score measures the sig-
nificance of each word (I) in the compression and
the linguistic likelihood (L) of the resulting word
combinations.2 We add some linguistic knowledge
to this formulation through a function (SOV ) that
captures information about subjects, objects and
verbs. The compression score is given in Equa-
tion (1). The lambdas (?I , ?SOV , ?L) weight thecontribution of the individual scores:
S(V ) =
M
?
i=1
?II(vi)+?sovSOV (vi)
+?LL(vi|vi?1,vi?2) (1)
The sentence V = v1,v2, . . . ,vm (of M words)that maximises the score S(V ) is the best com-
pression for an original sentence consisting of N
words (M < N). The best compression can be
found using dynamic programming. The ??s in
Equation (1) can be either optimised using a small
amount of training data or set manually (e.g., if
short compressions are preferred to longer ones,
then the language model should be given a higher
weight). Alternatively, weighting could be dis-
pensed with by including a normalising factor in
the language model. Here, we follow Hori and Fu-
rui?s (2004) original formulation and leave the nor-
malisation to future work. We next introduce each
measure individually.
Word significance score The word signifi-
cance score I measures the relative importance of
a word in a document. It is similar to tf-idf, a term
weighting score commonly used in information re-
trieval:
I(wi) = fi log FAFi (2)
2Hori and Furui (2004) also have a confidence score basedupon how reliable the output of an automatic speech recog-nition system is. However, we need not consider this scorewhen working with written text and manual transcripts.
Where wi is the topic word of interest (topic wordsare either nouns or verbs), fi is the frequency of wiin the document, Fi is the corpus frequency of wiand FA is the sum of all topic word occurrences inthe corpus (?i Fi).
Linguistic score The linguistic score?s
L(vi|vi?1,vi?2) responsibility is to select somefunction words, thus ensuring that compressions
remain grammatical. It also controls which topic
words can be placed together. The score mea-
sures the n-gram probability of the compressed
sentence.
SOV Score The SOV score is based on the in-
tuition that subjects, objects and verbs should not
be dropped while words in other syntactic roles
can be considered for removal. This score is based
solely on the contents of the sentence considered
for compression without taking into account the
distribution of subjects, objects or verbs, across
documents. It is defined in (3) where fi is the doc-ument frequency of a verb, or word bearing the
subject/object role and ?default is a constant weightassigned to all other words.
SOV (wi) =
?
?
?
fi if wi in subject, objector verb role
?default otherwise (3)
The SOV score is only applied to the head word of
subjects and objects.
3 Corpora
Our intent was to assess the performance of the
two models just described on written and spo-
ken text. The appeal of written text is understand-
able since most summarisation work today fo-
cuses on this domain. Speech data not only pro-
vides a natural test-bed for compression applica-
tions (e.g., subtitle generation) but also poses ad-
ditional challenges. Spoken utterances can be un-
grammatical, incomplete, and often contain arte-
facts such as false starts, interjections, hesitations,
and disfluencies. Rather than focusing on sponta-
neous speech which is abundant in these artefacts,
we conduct our study on the less ambitious do-
main of broadcast news transcripts. This lies in-
between the extremes of written text and sponta-
neous speech as it has been scripted beforehand
and is usually read off an autocue.
One stumbling block to performing a compara-
tive study between written data and speech data
is that there are no naturally occurring parallel
379
speech corpora for studying compression. Auto-
matic corpus creation is not a viable option ei-
ther, speakers do not normally create summaries
of their own utterances. We thus gathered our own
corpus by asking humans to generate compres-
sions for speech transcripts.
In what follows we describe how the manual
compressions were performed. We also briefly
present the written corpus we used for our exper-
iments. The latter was automatically constructed
and offers an interesting point of comparison with
our manually created corpus.
Broadcast News Corpus Three annotators
were asked to compress 50 broadcast news sto-
ries (1,370 sentences) taken from the HUB-4
1996 English Broadcast News corpus provided by
the LDC. The HUB-4 corpus contains broadcast
news from a variety of networks (CNN, ABC,
CSPAN and NPR) which have been manually tran-
scribed and split at the story and sentence level.
Each document contains 27 sentences on average
and the whole corpus consists of 26,151 tokens.3
The Robust Accurate Statistical Parsing (RASP)
toolkit (Briscoe and Carroll 2002) was used to au-
tomatically tokenise the corpus.
Each annotator was asked to perform sentence
compression by removing tokens from the original
transcript. Annotators were asked to remove words
while: (a) preserving the most important infor-
mation in the original sentence, and (b) ensuring
the compressed sentence remained grammatical. If
they wished they could leave a sentence uncom-
pressed by marking it as inappropriate for com-
pression. They were not allowed to delete whole
sentences even if they believed they contained no
information content with respect to the story as
this would blur the task with abstracting.
Ziff-Davis Corpus Most previous work (Jing
2000; Knight and Marcu 2002; Riezler et al 2003;
Nguyen et al 2004a; Turner and Charniak 2005;
McDonald 2006) has relied on automatically con-
structed parallel corpora for training and evalua-
tion purposes. The most popular compression cor-
pus originates from the Ziff-Davis corpus ? a col-
lection of news articles on computer products. The
corpus was created by matching sentences that oc-
cur in an article with sentences that occur in an
abstract (Knight and Marcu 2002). The abstract
sentences had to contain a subset of the original
sentence?s words and the word order had to remain
the same.
3The compression corpus is available at http://
homepages.inf.ed.ac.uk/s0460084/data/.
A1 A2 A3 Av. Ziff-Davis
Comp% 88.0 79.0 87.0 84.4 97.0
CompR 73.1 79.0 70.0 73.0 47.0
Table 2: Compression Rates (Comp% measures
the percentage of sentences compressed; CompR
is the mean compression rate of all sentences)
1 2 3 4 5 6 7 8 9 10Length of word span dropped
0
0.1
0.2
0.3
0.4
0.5
Re
lati
ve 
num
ber
 of 
dro
ps
Annotator 1Annotator 2Annotator 3Ziff-Davis
+
Figure 1: Distribution of span of words dropped
Comparisons Following the classification
scheme adopted in the British National Corpus
(Burnard 2000), we assume throughout this paper
that Broadcast News and Ziff-Davis belong to dif-
ferent domains (spoken vs. written text) whereas
they represent the same genre (i.e., news). Table 2
shows the percentage of sentences which were
compressed (Comp%) and the mean compression
rate (CompR) for the two corpora. The annota-
tors compress the Broadcast News corpus to a
similar degree. In contrast, the Ziff-Davis corpus
is compressed much more aggressively with a
compression rate of 47%, compared to 73% for
Broadcast News. This suggests that the Ziff-Davis
corpus may not be a true reflection of human
compression performance and that humans tend
to compress sentences more conservatively than
the compressions found in abstracts.
We also examined whether the two corpora dif-
fer with regard to the length of word spans be-
ing removed. Figure 1 shows how frequently word
spans of varying lengths are being dropped. As can
be seen, a higher percentage of long spans (five
or more words) are dropped in the Ziff-Davis cor-
pus. This suggests that the annotators are remov-
ing words rather than syntactic constituents, which
provides support for a model that can act on the
word level. There is no statistically significant dif-
ference between the length of spans dropped be-
tween the annotators, whereas there is a signif-
icant difference (p < 0.01) between the annota-
tors? spans and the Ziff-Davis? spans (using the
380
Wilcoxon Test).
The compressions produced for the Broadcast
News corpus may differ slightly to the Ziff-Davis
corpus. Our annotators were asked to perform
sentence compression explicitly as an isolated
task rather than indirectly (and possibly subcon-
sciously) as part of the broader task of abstracting,
which we can assume is the case with the Ziff-
Davis corpus.
4 Automatic Evaluation Measures
Previous studies relied almost exclusively on
human judgements for assessing the well-
formedness of automatically derived com-
pressions. Although human evaluations of
compression systems are not as large-scale as in
other fields (e.g., machine translation), they are
typically performed once, at the end of the de-
velopment cycle. Automatic evaluation measures
would allow more extensive parameter tuning
and crucially experimentation with larger data
sets. Most human studies to date are conducted
on a small compression sample, the test portion
of the Ziff-Davis corpus (32 sentences). Larger
sample sizes would expectedly render human
evaluations time consuming and generally more
difficult to conduct frequently. Here, we review
two automatic evaluation measures that hold
promise for the compression task.
Simple String Accuracy (SSA, Bangalore et al
2000) has been proposed as a baseline evaluation
metric for natural language generation. It is based
on the string edit distance between the generated
output and a gold standard. It is a measure of the
number of insertion (I), deletion (D) and substi-
tution (S) errors between two strings. It is defined
in (4) where R is the length of the gold standard
string.
Simple String Accuracy = (1? I +D+S
R
) (4)
The SSA score will assess whether appropriate
words have been included in the compression.
Another stricter automatic evaluation method
is to compare the grammatical relations found in
the system compressions against those found in a
gold standard. This allows us ?to measure the se-
mantic aspects of summarisation quality in terms
of grammatical-functional information? (Riezler
et al 2003). The standard metrics of precision,
recall and F-score can then be used to measure
the quality of a system against a gold standard.
Our implementation of the F-score measure used
the grammatical relations annotations provided by
RASP (Briscoe and Carroll 2002). This parser is
particularly appropriate for the compression task
since it provides parses for both full sentences
and sentence fragments and is generally robust
enough to analyse semi-grammatical compres-
sions. We calculated F-score over all the relations
provided by RASP (e.g., subject, direct/indirect
object, modifier; 15 in total).
Correlation with human judgements is an im-
portant prerequisite for the wider use of automatic
evaluation measures. In the following section we
describe an evaluation study examining whether
the measures just presented indeed correlate with
human ratings of compression quality.
5 Experimental Set-up
In this section we present our experimental set-
up for assessing the performance of the two al-
gorithms discussed above. We explain how differ-
ent model parameters were estimated. We also de-
scribe a judgement elicitation study on automatic
and human-authored compressions.
Parameter Estimation We created two vari-
ants of the decision-tree model, one trained on
the Ziff-Davis corpus and one on the Broadcast
News corpus. We used 1,035 sentences from the
Ziff-Davis corpus for training; the same sentences
were previously used in related work (Knight and
Marcu 2002). The second variant was trained on
1,237 sentences from the Broadcast News corpus.
The training data for both models was parsed us-
ing Charniak?s (2000) parser. Learning cases were
automatically generated using a set of 90 features
similar to Knight and Marcu (2002).
For the word-based method, we randomly
selected 50 sentences from each training set
to optimise the lambda weighting parame-
ters4. Optimisation was performed using Pow-
ell?s method (Press et al 1992). Recall from Sec-
tion 2.2 that the compression score has three
main parameters: the significance, linguistic, and
SOV scores. The significance score was calcu-
lated using 25 million tokens from the Broadcast
News corpus (spoken variant) and 25 million to-
kens from the North American News Text Cor-
pus (written variant). The linguistic score was es-
timated using a trigram language model. The lan-
guage model was trained on the North Ameri-
4To treat both models on an equal footing, we attemptedto train the decision-tree model solely on 50 sentences. How-ever, it was unable to produce any reasonable compressions,presumably due to insufficient learning instances.
381
can corpus (25 million tokens) using the CMU-
Cambridge Language Modeling Toolkit (Clarkson
and Rosenfeld 1997) with a vocabulary size of
50,000 tokens and Good-Turing discounting. Sub-
jects, objects, and verbs for the SOV score were
obtained from RASP (Briscoe and Carroll 2002).
All our experiments were conducted on sen-
tences for which we obtained syntactic analyses.
RASP failed on 17 sentences from the Broadcast
news corpus and 33 from the Ziff-Davis corpus;
Charniak?s (2000) parser successfully parsed the
Broadcast News corpus but failed on three sen-
tences from the Ziff-Davis corpus.
Evaluation Data We randomly selected
40 sentences for evaluation purposes, 20 from
the testing portion of the Ziff-Davis corpus (32
sentences) and 20 sentences from the Broadcast
News corpus (133 sentences were set aside for
testing). This is comparable to previous studies
which have used the 32 test sentences from the
Ziff-Davis corpus. None of the 20 Broadcast
News sentences were used for optimisation. We
ran the decision-tree system and the word-based
system on these 40 sentences. One annotator was
randomly selected to act as the gold standard for
the Broadcast News corpus; the gold standard
for the Ziff-Davis corpus was the sentence that
occurred in the abstract. For each original sen-
tence we had three compressions; two generated
automatically by our systems and a human au-
thored gold standard. Thus, the total number of
compressions was 120 (3x40).
Human Evaluation The 120 compressions
were rated by human subjects. Their judgements
were also used to examine whether the automatic
evaluation measures discussed in Section 4 corre-
late reliably with behavioural data. Sixty unpaid
volunteers participated in our elicitation study, all
were self reported native English speakers. The
study was conducted remotely over the Internet.
Participants were presented with a set of instruc-
tions that explained the task and defined sentence
compression with the aid of examples. They first
read the original sentence with the compression
hidden. Then the compression was revealed by
pressing a button. Each participant saw 40 com-
pressions. A Latin square design prevented sub-
jects from seeing two different compressions of
the same sentence. The order of the sentences was
randomised. Participants were asked to rate each
compression they saw on a five point scale taking
into account the information retained by the com-
pression and its grammaticality. They were told all
o: Apparently Fergie very much wants to have a career intelevision.d: A career in television.w: Fergie wants to have a career in television.g: Fergie wants a career in television.
o: Many debugging features, including user-defined breakpoints and variable-watching and message-watchingwindows, have been added.d: Many debugging features.w: Debugging features, and windows, have been added.g: Many debugging features have been added.
o: As you said, the president has just left for a busy threedays of speeches and fundraising in Nevada, Californiaand New Mexico.d: As you said, the president has just left for a busy threedays.w: You said, the president has left for three days ofspeeches and fundraising in Nevada, California andNew Mexico.g: The president left for three days of speeches andfundraising in Nevada, California and New Mexico.
Table 3: Compression examples (o: original sen-
tence, d: decision-tree compression, w: word-
based compression, g: gold standard)
compressions were automatically generated. Ex-
amples of the compressions our participants saw
are given in Table 3.
6 Results
Our experiments were designed to answer three
questions: (1) Is there a significant difference
between the compressions produced by super-
vised (constituent-based) and weakly unsuper-
vised (word-based) approaches? (2) How well
do the two models port across domains (written
vs. spoken text) and corpora types (human vs. au-
tomatically created)? (3) Do automatic evaluation
measures correlate with human judgements?
One of our first findings is that the the decision-
tree model is rather sensitive to the style of training
data. The model cannot capture and generalise sin-
gle word drops as effectively as constituent drops.
When the decision-tree is trained on the Broadcast
News corpus, it is unable to create suitable com-
pressions. On the evaluation data set, 75% of the
compressions produced are the original sentence
or the original sentence with one word removed.
It is possible that the Broadcast News compres-
sion corpus contains more varied compressions
than those of the Ziff-Davis and therefore a larger
amount of training data would be required to learn
a reliable decision-tree model. We thus used the
Ziff-Davis trained decision-tree model to obtain
compressions for both corpora.
Our results are summarised in Tables 4 and 5.
Table 4 lists the average compression rates for
382
Broadcast News CompR SSA F-score
Decision-tree 0.55 0.34 0.40
Word-based 0.72 0.51 0.54
gold standard 0.71 ? ?
Ziff-Davis CompR SSA F-score
Decision-tree 0.58 0.20 0.34
Word-based 0.60 0.19 0.39
gold standard 0.54 ? ?
Table 4: Results using automatic evaluation mea-
sures
Compression Broadcast News Ziff-Davis
Decision-tree 2.04 2.34
Word-based 2.78 2.43
gold standard 3.87 3.53
Table 5: Mean ratings from human evaluation
each model as well as the models? performance ac-
cording to the two automatic evaluation measures
discussed in Section 4. The row ?gold standard?
displays human-produced compression rates. Ta-
ble 5 shows the results of our judgement elicitation
study.
The compression rates (CompR, Table 4) indi-
cate that the decision-tree model compresses more
aggressively than the word-based model. This is
due to the fact that it mostly removes entire con-
stituents rather than individual words. The word-
based model is closer to the human compres-
sion rate. According to our automatic evaluation
measures, the decision-tree model is significantly
worse than the word-based model (using the Stu-
dent t test, SSA p < 0.05, F-score p < 0.05) on
the Broadcast News corpus. Both models are sig-
nificantly worse than humans (SSA p < 0.05, F-
score p < 0.01). There is no significant difference
between the two systems using the Ziff-Davis cor-
pus on both simple string accuracy and relation
F-score, whereas humans significantly outperform
the two systems.
We have performed an Analysis of Variance
(ANOVA) to examine whether similar results are
obtained when using human judgements. Statisti-
cal tests were done using the mean of the ratings
(see Table 5). The ANOVA revealed a reliable ef-
fect of compression type by subjects and by items
(p < 0.01). Post-hoc Tukey tests confirmed that
the word-based model outperforms the decision-
tree model (? < 0.05) on the Broadcast news cor-
pus; however, the two models are not significantly
Measure Ziff-Davis Broadcast News
SSA 0.171 0.348*
F-score 0.575** 0.532**
*p < 0.05 **p < 0.01
Table 6: Correlation (Pearson?s r) between evalu-
ation measures and human ratings. Stars indicate
level of statistical significance.
different when using the Ziff-Davis corpus. Both
systems perform significantly worse than the gold
standard (? < 0.05).
We next examine the degree to which the auto-
matic evaluation measures correlate with human
ratings. Table 6 shows the results of correlating
the simple string accuracy (SSA) and relation F-
score against compression judgements. The SSA
does not correlate on both corpora with human
judgements; it thus seems to be an unreliable mea-
sure of compression performance. However, the F-
score correlates significantly with human ratings,
yielding a correlation coefficient of r = 0.575 on
the Ziff-Davis corpus and r = 0.532 on the Broad-
cast news. To get a feeling for the difficulty of
the task, we assessed how well our participants
agreed in their ratings using leave-one-out resam-
pling (Weiss and Kulikowski 1991). The technique
correlates the ratings of each participant with the
mean ratings of all the other participants. The aver-
age agreement is r = 0.679 on the Ziff-Davis cor-
pus and r = 0.746 on the Broadcast News corpus.
This result indicates that F-score?s agreement with
the human data is not far from the human upper
bound.
7 Conclusions and Future Work
In this paper we have provided a comparison be-
tween a supervised (constituent-based) and a min-
imally supervised (word-based) approach to sen-
tence compression. Our results demonstrate that
the word-based model performs equally well on
spoken and written text. Since it does not rely
heavily on training data, it can be easily extended
to languages or domains for which parallel com-
pression corpora are scarce. When no parallel cor-
pora are available the parameters can be manu-
ally tuned to produce compressions. In contrast,
the supervised decision-tree model is not partic-
ularly robust on spoken text, it is sensitive to the
nature of the training data, and did not produce ad-
equate compressions when trained on the human-
authored Broadcast News corpus. A comparison
of the automatically gathered Ziff-Davis corpus
383
with the Broadcast News corpus revealed impor-
tant differences between the two corpora and thus
suggests that automatically created corpora may
not reflect human compression performance.
We have also assessed whether automatic eval-
uation measures can be used for the compression
task. Our results show that grammatical relations-
based F-score (Riezler et al 2003) correlates re-
liably with human judgements and could thus be
used to measure compression performance auto-
matically. For example, it could be used to assess
progress during system development or for com-
paring across different systems and system config-
urations with much larger test sets than currently
employed.
In its current formulation, the only function
driving compression in the word-based model
is the language model. The word significance
and SOV scores are designed to single out im-
portant words that the model should not drop. We
have not yet considered any functions that encour-
age compression. Ideally these functions should be
inspired from the underlying compression process.
Finding such a mechanism is an avenue of future
work. We would also like to enhance the word-
based model with more linguistic knowledge; we
plan to experiment with syntax-based language
models and more richly annotated corpora.
Another important future direction lies in apply-
ing the unsupervised model presented here to lan-
guages with more flexible word order and richer
morphology than English (e.g., German, Czech).
We suspect that these languages will prove chal-
lenging for creating grammatically acceptable
compressions. Finally, our automatic evaluation
experiments motivate the use of relations-based F-
score as a means of directly optimising compres-
sion quality, much in the same way MT systems
optimise model parameters using BLEU as a mea-
sure of translation quality.
Acknowledgements
We are grateful to our annotators Vasilis Karaiskos, Beata
Kouchnir, and Sarah Luger. Thanks to Jean Carletta, Frank
Keller, Steve Renals, and Sebastian Riedel for helpful com-
ments and suggestions. Lapata acknowledges the support of
EPSRC (grant GR/T04540/01).
References
Bangalore, Srinivas, Owen Rambow, and Steve Whittaker.2000. Evaluation metrics for generation. In Proceedings
of the 1st INLG. Mitzpe Ramon, Israel, pages 1?8.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Spain, pages 1499?1504.
Burnard, Lou. 2000. The Users Reference Guide for the
British National Corpus (World Edition). British NationalCorpus Consortium, Oxford University Computing Ser-vice.
Charniak, Eugene. 2000. A maximum-entropy-inspiredparser. In Proceedings of the 1st NAACL. San Francisco,CA, pages 132?139.
Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical lan-guage modeling using the CMU?cambridge toolkit. In
Proceedings of Eurospeech. Rhodes, Greece, pages 2707?2710.
Corston-Oliver, Simon. 2001. Text Compaction for Displayon Very Small Screens. In Proceedings of the NAACL
Workshop on Automatic Summarization. Pittsburgh, PA,pages 89?98.
Grefenstette, Gregory. 1998. Producing Intelligent Tele-graphic Text Reduction to Provide an Audio Scanning Ser-vice for the Blind. In Proceedings of the AAAI Symposium
on Intelligent Text Summarization. Stanford, CA, pages111?117.
Hori, Chiori and Sadaoki Furui. 2004. Speech summariza-tion: an approach through word extraction and a methodfor evaluation. IEICE Transactions on Information and
Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence Reduction for Automatic TextSummarization. In Proceedings of the 6th ANLP. Seat-tle,WA, pages 310?315.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression. Artificial Intelligence 139(1):91?107.
McDonald, Ryan. 2006. Discriminative sentence compres-sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy, pages 297?304.
Nguyen, Minh Le, Susumu Horiguchi, Akira Shimazu, andBao Tu Ho. 2004a. Example-based sentence reduction us-ing the hidden Markov model. ACM TALIP 3(2):146?158.
Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,Tu Bao Ho, and Masaru Fukushi. 2004b. Probabilisticsentence reduction using support vector machines. In Pro-
ceedings of the 20th COLING. Geneva, Switzerland, pages743?749.
Press, William H., Saul A. Teukolsky, William T. Vetterling,and Brian P. Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press,New York, NY, USA.
Quinlan, J. R. 1993. C4.5 ? Programs for Machine Learn-
ing. The Morgan Kaufmann series in machine learning.Morgan Kaufman Publishers.
Riezler, Stefan, Tracy H. King, Richard Crouch, and AnnieZaenen. 2003. Statistical sentence condensation usingambiguity packing and stochastic disambiguation meth-ods for lexical-functional grammar. In Proceedings of the
HLT/NAACL. Edmonton, Canada, pages 118?125.
Turner, Jenine and Eugene Charniak. 2005. Supervised andunsupervised learning for sentence compression. In Pro-
ceedings of the 43rd ACL. Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence compres-sion for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summarization.Barcelona, Spain, pages 89?95.
Weiss, Sholom M. and Casimir A. Kulikowski. 1991. Com-
puter systems that learn: classification and prediction
methods from statistics, neural nets, machine learning,
and expert systems. Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.
384
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 144?151,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Constraint-based Sentence Compression
An Integer Programming Approach
James Clarke and Mirella Lapata
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
jclarke@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The ability to compress sentences while
preserving their grammaticality and most
of their meaning has recently received
much attention. Our work views sentence
compression as an optimisation problem.
We develop an integer programming for-
mulation and infer globally optimal com-
pressions in the face of linguistically moti-
vated constraints. We show that such a for-
mulation allows for relatively simple and
knowledge-lean compression models that
do not require parallel corpora or large-
scale resources. The proposed approach
yields results comparable and in some
cases superior to state-of-the-art.
1 Introduction
A mechanism for automatically compressing sen-
tences while preserving their grammaticality and
most important information would greatly bene-
fit a wide range of applications. Examples include
text summarisation (Jing 2000), subtitle genera-
tion from spoken transcripts (Vandeghinste and
Pan 2004) and information retrieval (Olivers and
Dolan 1999). Sentence compression is a complex
paraphrasing task with information loss involv-
ing substitution, deletion, insertion, and reordering
operations. Recent years have witnessed increased
interest on a simpler instantiation of the compres-
sion problem, namely word deletion (Knight and
Marcu 2002; Riezler et al 2003; Turner and Char-
niak 2005). More formally, given an input sen-
tence of words W = w1,w2, . . . ,wn, a compressionis formed by removing any subset of these words.
Sentence compression has received both gener-
ative and discriminative formulations in the liter-
ature. Generative approaches (Knight and Marcu
2002; Turner and Charniak 2005) are instantia-
tions of the noisy-channel model: given a long sen-
tence l, the aim is to find the corresponding short
sentence s which maximises the conditional prob-
ability P(s|l). In a discriminative setting (Knight
and Marcu 2002; Riezler et al 2003; McDonald
2006), sentences are represented by a rich fea-
ture space (typically induced from parse trees) and
the goal is to learn rewrite rules indicating which
words should be deleted in a given context. Both
modelling paradigms assume access to a training
corpus consisting of original sentences and their
compressions.
Unsupervised approaches to the compression
problem are few and far between (see Hori and Fu-
rui 2004 and Turner and Charniak 2005 for excep-
tions). This is surprising considering that parallel
corpora of original-compressed sentences are not
naturally available in the way multilingual corpora
are. The scarcity of such data is demonstrated by
the fact that most work to date has focused on a
single parallel corpus, namely the Ziff-Davis cor-
pus (Knight and Marcu 2002). And some effort
into developing appropriate training data would be
necessary when porting existing algorithms to new
languages or domains.
In this paper we present an unsupervised model
of sentence compression that does not rely on a
parallel corpus ? all that is required is a corpus
of uncompressed sentences and a parser. Given a
long sentence, our task is to form a compression
by preserving the words that maximise a scoring
function. In our case, the scoring function is an
n-gram language model, ?with a few strings at-
tached?. While straightforward to estimate, a lan-
guage model is a fairly primitive scoring function:
it has no notion of the overall sentence structure,
grammaticality or underlying meaning. We thus
couple our language model with a small number
of structural and semantic constraints capturing
global properties of the compression process.
We encode the language model and linguistic
constraints as linear inequalities and use Integer
Programming (IP) to infer compressions that are
consistent with both. The IP formulation allows us
to capture global sentence properties and can be
easily manipulated to provide compressions tai-
lored for specific applications. For example, we
144
could prevent overly long or overly short compres-
sions or generally avoid compressions that lack
a main verb or consist of repetitions of the same
word.
In the following section we provide an overview
of previous approaches to sentence compression.
In Section 3 we motivate the treatment of sentence
compression as an optimisation problem and for-
mulate our language model and constraints in the
IP framework. Section 4 discusses our experimen-
tal set-up and Section 5 presents our results. Dis-
cussion of future work concludes the paper.
2 Previous Work
Jing (2000) was perhaps the first to tackle the sen-
tence compression problem. Her approach uses
multiple knowledge sources to determine which
phrases in a sentence to remove. Central to her
system is a grammar checking module that spec-
ifies which sentential constituents are grammati-
cally obligatory and should therefore be present
in the compression. This is achieved using sim-
ple rules and a large-scale lexicon. Other knowl-
edge sources include WordNet and corpus evi-
dence gathered from a parallel corpus of original-
compressed sentence pairs. A phrase is removed
only if it is not grammatically obligatory, not the
focus of the local context and has a reasonable
deletion probability (estimated from the parallel
corpus).
In contrast to Jing (2000), the bulk of the re-
search on sentence compression relies exclusively
on corpus data for modelling the compression
process without recourse to extensive knowledge
sources (e.g., WordNet). Approaches based on the
noisy-channel model (Knight and Marcu 2002;
Turner and Charniak 2005) consist of a source
model P(s) (whose role is to guarantee that the
generated compression is grammatical), a chan-
nel model P(l|s) (capturing the probability that
the long sentence l is an expansion of the com-
pressed sentence s), and a decoder (which searches
for the compression s that maximises P(s)P(l|s)).
The channel model is typically estimated using
a parallel corpus, although Turner and Charniak
(2005) also present semi-supervised and unsu-
pervised variants of the channel model that esti-
mate P(l|s) without parallel data.
Discriminative formulations of the compres-
sion task include decision-tree learning (Knight
and Marcu 2002), maximum entropy (Riezler
et al 2003), support vector machines (Nguyen
et al 2004), and large-margin learning (McDonald
2006). We describe here the decision-tree model
in more detail since we will use it as a basis for
comparison when evaluating our own models (see
Section 4). According to this model, compression
is performed through a tree rewriting process in-
spired by the shift-reduce parsing paradigm. A se-
quence of shift-reduce-drop actions are performed
on a long parse tree, l, to create a smaller tree, s.
The compression process begins with an input
list generated from the leaves of the original sen-
tence?s parse tree and an empty stack. ?Shift? oper-
ations move leaves from the input list to the stack
while ?drop? operations delete from the input list.
Reduce operations are used to build trees from the
leaves on the stack. A decision-tree is trained on a
set of automatically generated learning cases from
a parallel corpus. Each learning case has a target
action associated with it and is decomposed into a
set of indicative features. The decision-tree learns
which action to perform given this set of features.
The final model is applied in a deterministic fash-
ion in which the features for the current state are
extracted and the decision-tree is queried. This is
repeated until the input list is empty and the final
compression is recovered by traversing the leaves
of resulting tree on the stack.
While most compression models operate over
constituents, Hori and Furui (2004) propose a
model which generates compressions through
word deletion. The model does not utilise parallel
data or syntactic information in any form. Given a
prespecified compression rate, it searches for the
compression with the highest score according to a
function measuring the importance of each word
and the linguistic likelihood of the resulting com-
pressions (language model probability). The score
is maximised through a dynamic programming al-
gorithm.
Although sentence compression has not been
explicitly formulated as an optimisation problem,
previous approaches have treated it in these terms.
The decoding process in the noisy-channel model
searches for the best compression given the source
and channel models. However, the compression
found is usually sub-optimal as heuristics are used
to reduce the search space or is only locally op-
timal due to the search method employed. The
decoding process used in Turner and Charniak?s
(2005) model first searches for the best combina-
tion of rules to apply. As they traverse their list
of compression rules they remove sentences out-
side the 100 best compressions (according to their
channel model). This list is eventually truncated
to 25 compressions.
In other models (Hori and Furui 2004; McDon-
ald 2006) the compression score is maximised
145
using dynamic programming. The latter guaran-
tees we will find the global optimum provided the
principle of optimality holds. This principle states
that given the current state, the optimal decision
for each of the remaining stages does not depend
on previously reached stages or previously made
decisions (Winston and Venkataramanan 2003).
However, we know this to be false in the case of
sentence compression. For example, if we have
included modifiers to the left of a head noun in
the compression then it makes sense that we must
include the head also. With a dynamic program-
ming approach we cannot easily guarantee such
constraints hold.
3 Problem Formulation
Our work models sentence compression explicitly
as an optimisation problem. There are 2n possible
compressions for each sentence and while many
of these will be unreasonable (Knight and Marcu
2002), it is unlikely that only one compression
will be satisfactory. Ideally, we require a func-
tion that captures the operations (or rules) that can
be performed on a sentence to create a compres-
sion while at the same time factoring how desir-
able each operation makes the resulting compres-
sion. We can then perform a search over all possi-
ble compressions and select the best one, as deter-
mined by how desirable it is.
Our formulation consists of two basic compo-
nents: a language model (scoring function) and a
small number of constraints ensuring that the re-
sulting compressions are structurally and semanti-
cally valid. Our task is to find a globally optimal
compression in the presence of these constraints.
We solve this inference problem using Integer Pro-
gramming without resorting to heuristics or ap-
proximations during the decoding process. Integer
programming has been recently applied to several
classification tasks, including relation extraction
(Roth and Yih 2004), semantic role labelling (Pun-
yakanok et al 2004), and the generation of route
directions (Marciniak and Strube 2005).
Before describing our model in detail, we in-
troduce some of the concepts and terms used in
Linear Programming and Integer Programming
(see Winston and Venkataramanan 2003 for an in-
troduction). Linear Programming (LP) is a tool
for solving optimisation problems in which the
aim is to maximise (or minimise) a given function
with respect to a set of constraints. The function
to be maximised (or minimised) is referred to as
the objective function. Both the objective function
and constraints must be linear. A number of deci-
sion variables are under our control which exert
influence on the objective function. Specifically,
they have to be optimised in order to maximise
(or minimise) the objective function. Finally, a set
of constraints restrict the values that the decision
variables can take. Integer Programming is an ex-
tension of linear programming where all decision
variables must take integer values.
3.1 Language Model
Assume we have a sentence W = w1,w2, . . . ,wnfor which we wish to generate a compression.
We introduce a decision variable for each word
in the original sentence and constrain it to be bi-
nary; a value of 0 represents a word being dropped,
whereas a value of 1 includes the word in the com-
pression. Let:
yi =
{ 1 if wi is in the compression0 otherwise ?i? [1 . . .n]
If we were using a unigram language model,
our objective function would maximise the overall
sum of the decision variables (i.e., words) multi-
plied by their unigram probabilities (all probabili-
ties throughout this paper are log-transformed):
maxz = n?
i=1
yi ?P(wi)
Thus if a word is selected, its corresponding yi isgiven a value of 1, and its probability P(wi) ac-cording to the language model will be counted in
our total score, z.
A unigram language model will probably gener-
ate many ungrammatical compressions. We there-
fore use a more context-aware model in our objec-
tive function, namely a trigram model. Formulat-
ing a trigram model in terms of an integer program
becomes a more involved task since we now must
make decisions based on word sequences rather
than isolated words. We first create some extra de-
cision variables:
pi =
{1 if wi starts the compression0 otherwise ?i ? [1 . . .n]
qi j =
?
?
?
1 if sequence wi,w j endsthe compression ?i ? [1 . . .n?1]
0 otherwise ? j ? [i+1 . . .n]
xi jk =
?
?
?
1 if sequence wi,w j,wk ?i ? [1 . . .n?2]is in the compression ? j ? [i+1 . . .n?1]
0 otherwise ?k ? [ j +1 . . .n]
Our objective function is given in Equation (1).
This is the sum of all possible trigrams that can
occur in all compressions of the original sentence
where w0 represents the ?start? token and wi is the
ith word in sentence W . Equation (2) constrains
146
the decision variables to be binary.
maxz = n?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j) (1)
subject to:
yi, pi,qi j,xi jk = 0 or 1 (2)
The objective function in (1) allows any combi-
nation of trigrams to be selected. This means that
invalid trigram sequences (e.g., two or more tri-
grams containing the symbol ?end?) could appear
in the output compression. We avoid this situation
by introducing sequential constraints (on the de-
cision variables yi,xi jk, pi, and qi j) that restrict theset of allowable trigram combinations.
Constraint 1 Exactly one word can begin a
sentence.
n
?
i=1
pi = 1 (3)
Constraint 2 If a word is included in the sen-
tence it must either start the sentence or be pre-
ceded by two other words or one other word and
the ?start? token w0.
yk ? pk ?
k?2
?
i=0
k?1
?
j=1
xi jk = 0 (4)
?k : k ? [1 . . .n]
Constraint 3 If a word is included in the sen-
tence it must either be preceded by one word and
followed by another or it must be preceded by one
word and end the sentence.
y j ?
j?1
?
i=0
n
?
k= j+1
xi jk ?
j?1
?
i=0
qi j = 0 (5)
? j : j ? [1 . . .n]
Constraint 4 If a word is in the sentence it
must be followed by two words or followed by one
word and then the end of the sentence or it must be
preceded by one word and end the sentence.
yi ?
n?1
?
j=i+1
n
?
k= j+1
xi jk ?
n
?
j=i+1
qi j ?
i?1
?
h=0
qhi = 0 (6)
?i : i ? [1 . . .n]
Constraint 5 Exactly one word pair can end
the sentence.
n?1
?
i=0
n
?
j=i+1
qi j = 1 (7)
Example compressions using the trigram model
just described are given in Table 1. The model in
O: He became a power player in Greek Politics in1974, when he founded the socialist Pasok Party.LM: He became a player in the Pasok.Mod: He became a player in the Pasok Party.Sen: He became a player in politics.Sig: He became a player in politics when he foundedthe Pasok Party.O: Finally, AppleShare Printer Server, formerly aseparate package, is now bundled with Apple-Share File Server.LM: Finally, AppleShare, a separate, AppleShare.Mod: Finally, AppleShare Server, is bundled.Sen: Finally, AppleShare Server, is bundled withServer.Sig: AppleShare Printer Server package is now bun-dled with AppleShare File Server.
Table 1: Compression examples (O: original sen-
tence, LM: compression with the trigram model,
Mod: compression with LM and modifier con-
straints, Sen: compression with LM, Mod and
sentential constraints, Sig: compression with LM,
Mod, Sen, and significance score)
its current state does a reasonable job of modelling
local word dependencies, but is unable to capture
syntactic dependencies that could potentially al-
low more meaningful compressions. For example,
it does not know that Pasok Party is the object
of founded or that Appleshare modifies Printer
Server.
3.2 Linguistic Constraints
In this section we propose a set of global con-
straints that extend the basic language model pre-
sented in Equations (1)?(7). Our aim is to bring
some syntactic knowledge into the compression
model and to preserve the meaning of the original
sentence as much as possible. Our constraints are
linguistically and semantically motivated in a sim-
ilar fashion to the grammar checking component
of Jing (2000). Importantly, we do not require any
additional knowledge sources (such as a lexicon)
beyond the parse and grammatical relations of the
original sentence. This is provided in our experi-
ments by the Robust Accurate Statistical Parsing
(RASP) toolkit (Briscoe and Carroll 2002). How-
ever, there is nothing inherent in our formulation
that restricts us to RASP; any other parser with
similar output could serve our purposes.
Modifier Constraints Modifier constraints
ensure that relationships between head words and
their modifiers remain grammatical in the com-
pression:
yi ? y j ? 0 (8)
?i, j : w j ? wi?s ncmods
yi ? y j ? 0 (9)
?i, j : w j ? wi?s detmods
147
Equation (8) guarantees that if we include a non-
clausal modifier (ncmod) in the compression then
the head of the modifier must also be included; this
is repeated for determiners (detmod) in (9).
We also want to ensure that the meaning of the
original sentence is preserved in the compression,
particularly in the face of negation. Equation (10)
implements this by forcing not in the compression
when the head is included. A similar constraint
is added for possessive modifiers (e.g., his, our),
as shown in Equation (11). Genitives (e.g., John?s
gift) are treated separately, mainly because they
are encoded as different relations in the parser (see
Equation (12)).
yi ? y j = 0 (10)
?i, j : w j ? wi?s ncmods?w j = not
yi ? y j = 0 (11)
?i, j : w j ? wi?s possessive detmods
yi ? y j = 0 (12)
?i, j : wi ? possessive ncmods
?w j = possessive
Compression examples with the addition of the
modifier constraints are shown in Table 1. Al-
though the compressions are grammatical (see the
inclusion of Party due to the modifier Pasok and
Server due to AppleShare), they are not entirely
meaning preserving.
Sentential Constraints We also define a few
intuitive constraints that take the overall sentence
structure into account. The first constraint (Equa-
tion (13)) ensures that if a verb is present in the
compression then so are its arguments, and if any
of the arguments are included in the compression
then the verb must also be included. We thus force
the program to make the same decision on the
verb, its subject, and object.
yi ? y j = 0 (13)
?i, j : w j ? subject/object of verb wi
Our second constraint forces the compression to
contain at least one verb provided the original sen-
tence contains one as well:
?
i?verbs
yi ? 1 (14)
Other sentential constraints include Equa-
tions (15) and (16) which apply to prepositional
phrases, wh-phrases and complements. These con-
straints force the introducing term (i.e., the prepo-
sition, complement or wh-word) to be included in
the compression if any word from within the syn-
tactic constituent is also included. The reverse is
also true, i.e., if the introducing term is included at
least one other word from the syntactic constituent
should also be included.
yi ? y j ? 0 (15)
?i, j : w j ? PP/COMP/WH-P
?wi starts PP/COMP/WH-P
?
i?PP/COMP/WH-P
yi ? y j ? 0 (16)
? j : w j starts PP/COMP/WH-P
We also wish to handle coordination. If two head
words are conjoined in the original sentence, then
if they are included in the compression the coordi-
nating conjunction must also be included:
(1? yi)+ y j ? 1 (17)
(1? yi)+ yk ? 1 (18)
yi +(1? y j)+(1? yk) ? 1 (19)
?i, j,k : w j ?wk conjoined by wi
Table 1 illustrates the compression output when
sentential constraints are added to the model. We
see that politics is forced into the compression due
to the presence of in; furthermore, since bundled
is in the compression, its object with Server is in-
cluded too.
Compression-related Constraints Finally,
we impose some hard constraints on the com-
pression output. First, Equation (20) disallows
anything within brackets in the original sentence
from being included in the compression. This
is a somewhat superficial attempt at excluding
parenthetical and potentially unimportant material
from the compression. Second, Equation (21)
forces personal pronouns to be included in the
compression. The constraint is important for
generating coherent document as opposed to
sentence compressions.
yi = 0 (20)
?i : wi ? brackets
yi = 1 (21)
?i : wi ? personal pronouns
It is also possible to influence the length of the
compressed sentence. For example, Equation (22)
forces the compression to contain at least b tokens.
Alternatively, we could force the compression to
be exactly b tokens (by substituting ? with =
in (22)) or to be less than b tokens (by replacing ?
with ?).1
n
?
i=1
yi ? b (22)
3.3 Significance Score
While the constraint-based language model pro-
duces more grammatical output than a regular lan-
1Compression rate can be also limited to a range by in-cluding two inequality constraints.
148
guage model, the sentences are typically not great
compressions. The language model has no notion
of which content words to include in the compres-
sion and thus prefers words it has seen before. But
words or constituents will be of different relative
importance in different documents or even sen-
tences.
Inspired by Hori and Furui (2004), we add to
our objective function (see Equation (1)) a signif-
icance score designed to highlight important con-
tent words. Specifically, we modify Hori and Fu-
rui?s significance score to give more weight to con-
tent words that appear in the deepest level of em-
bedding in the syntactic tree. The latter usually
contains the gist of the original sentence:
I(wi) =
l
N
? fi log FaFi (23)The significance score above is computed using a
large corpus where wi is a topic word (i.e., a nounor verb), fi and Fi are the frequency of wi in thedocument and corpus respectively, and Fa is thesum of all topic words in the corpus. l is the num-
ber of clause constituents above wi, and N is thedeepest level of embedding. The modified objec-
tive function is given below:
maxz = n?
i=1
yi ? I(wi)+
n
?
i=1
pi ?P(wi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k= j+1
xi jk ?P(wk|wi,w j)
+
n?1
?
i=0
n
?
j=i+1
qi j ?P(end|wi,w j) (24)
A weighting factor could be also added to the ob-
jective function, to counterbalance the importance
of the language model and the significance score.
4 Evaluation Set-up
We evaluated the approach presented in the pre-
vious sections against Knight and Marcu?s (2002)
decision-tree model. This model is a good basis for
comparison as it operates on parse trees and there-
fore is aware of syntactic structure (as our models
are) but requires a large parallel corpus for training
whereas our models do not; and it yields compara-
ble performance to the noisy-channel model.2 The
decision-tree model was compared against two
variants of our IP model. Both variants employed
the constraints described in Section 3.2 but dif-
fered in that one variant included the significance
2Turner and Charniak (2005) argue that the noisy-channelmodel is not an appropriate compression model since it usesa source model trained on uncompressed sentences and as aresult tends to consider compressed sentences less likely thanuncompressed ones.
score in its objective function (see (24)), whereas
the other one did not (see (1)). In both cases the
sequential constraints from Section 3.1 were ap-
plied to ensure that the language model was well-
formed. We give details below on the corpora we
used and explain how the different model parame-
ters were estimated. We also discuss how evalua-
tion was carried out using human judgements.
Corpora We evaluate our systems on two dif-
ferent corpora. The first is the compression corpus
of Knight and Marcu (2002) derived automatically
from document-abstract pairs of the Ziff-Davis
corpus. This corpus has been used in most pre-
vious compression work. We also created a com-
pression corpus from the HUB-4 1996 English
Broadcast News corpus (provided by the LDC).
We asked annotators to produce compressions for
50 broadcast news stories (1,370 sentences).3
The Ziff-Davis corpus is partitioned into train-
ing (1,035 sentences) and test set (32 sentences).
We held out 50 sentences from the training for de-
velopment purposes. We also split the Broadcast
News corpus into a training and test set (1,237/133
sentences). Forty sentences were randomly se-
lected for evaluation purposes, 20 from the test
portion of the Ziff-Davis corpus and 20 from the
Broadcast News corpus test set.
Parameter Estimation The decision-tree
model was trained, using the same feature set
as Knight and Marcu (2002) on the Ziff-Davis
corpus and used to obtain compressions for both
test corpora.4 For our IP models, we used a
language model trained on 25 million tokens from
the North American News corpus using the CMU-
Cambridge Language Modeling Toolkit (Clarkson
and Rosenfeld 1997) with a vocabulary size of
50,000 tokens and Good-Turing discounting.
The significance score used in our second model
was calculated using 25 million tokens from the
Broadcast News Corpus (for the spoken data) and
25 million tokens from the American News Text
Corpus (for the written data). Finally, the model
that includes the significance score was optimised
against a loss function similar to McDonald
(2006) to bring the language model and the score
into harmony. We used Powell?s method (Press
et al 1992) and 50 sentences (randomly selected
from the training set).
3The corpus is available from http://homepages.inf.
ed.ac.uk/s0460084/data/.4We found that the decision-tree was unable to producemeaningful compressions when trained on the BroadcastNews corpus (in most cases it recreated the original sen-tence). Thus we used the decision model trained on Ziff-Davis to generate Broadcast News compressions.
149
We also set a minimum compression length (us-
ing the constraint in Equation (22)) in both our
models to avoid overly short compressions. The
length was set at 40% of the original sentence
length or five tokens, whichever was larger. Sen-
tences under five tokens were not compressed.
In our modeling framework, we generate and
solve an IP for every sentence we wish to com-
press. We employed lp solve for this purpose, an
efficient Mixed Integer Programming solver.5 Sen-
tences typically take less than a few seconds to
compress on a 2 GHz Pentium IV machine.
Human Evaluation As mentioned earlier, the
output of our models is evaluated on 40 exam-
ples. Although the size of our test set is compa-
rable to previous studies (which are typically as-
sessed on 32 sentences from the Ziff-Davis cor-
pus), the sample is too small to conduct signif-
icance testing. To counteract this, human judge-
ments are often collected on compression out-
put; however the evaluations are limited to small
subject pools (often four judges; Knight and
Marcu 2002; Turner and Charniak 2005; McDon-
ald 2006) which makes difficult to apply inferen-
tial statistics on the data. We overcome this prob-
lem by conducting our evaluation using a larger
sample of subjects.
Specifically, we elicited human judgements
from 56 unpaid volunteers, all self reported na-
tive English speakers. The elicitation study was
conducted over the Internet. Participants were pre-
sented with a set of instructions that explained the
sentence compression task with examples. They
were asked to judge 160 compressions in to-
tal. These included the output of the three au-
tomatic systems on the 40 test sentences paired
with their gold standard compressions. Partici-
pants were asked to read the original sentence and
then reveal its compression by pressing a button.
They were told that all compressions were gen-
erated automatically. A Latin square design en-
sured that subjects did not see two different com-
pressions of the same sentence. The order of the
sentences was randomised. Participants rated each
compression on a five point scale based on the in-
formation retained and its grammaticality. Exam-
ples of our experimental items are given in Table 2.
5 Results
Our results are summarised in Table 3 which de-
tails the compression rates6 and average human
5The software is available from http://www.
geocities.com/lpsolve/.6We follow previous work (see references) in using theterm ?compression rate? to refer to the percentage of words
O: Apparently Fergie very much wants to have a ca-reer in television.G: Fergie wants a career in television.D: A career in television.LM: Fergie wants to have a career.Sig: Fergie wants to have a career in television.O: The SCAMP module, designed and built byUnisys and based on an Intel process, contains theentire 48-bit A-series processor.G: The SCAMP module contains the entire 48-bit A-series processor.D: The SCAMP module designed Unisys and basedon an Intel process.LM: The SCAMP module, contains the 48-bit A-seriesprocessor.Sig: The SCAMP module, designed and built byUnisys and based on process, contains the A-series processor.
Table 2: Compression examples (O: original sen-
tence, G: Gold standard, D: Decision-tree, LM: IP
language model, Sig: IP language model with sig-
nificance score)
Model CompR Rating
Decision-tree 56.1% 2.22??
LangModel 49.0% 2.23??
LangModel+Significance 73.6% 2.83?
Gold Standard 62.3% 3.68?
Table 3: Compression results; compression rate
(CompR) and average human judgements (Rat-
ing); ?: sig. diff. from gold standard; ?: sig. diff.
from LangModel+Significance
ratings (Rating) for the three systems and the gold
standard. As can be seen, the IP language model
(LangModel) is most aggressive in terms of com-
pression rate as it reduces the original sentences
on average by half (49%). Recall that we enforce a
minimum compression rate of 40% (see (22)). The
fact that the resulting compressions are longer, in-
dicates that our constraints instill some linguistic
knowledge into the language model, thus enabling
it to prefer longer sentences over extremely short
ones. The decision-tree model compresses slightly
less than our IP language model at 56.1% but still
below the gold standard rate. We see a large com-
pression rate increase from 49% to 73.6% when
we introduce the significance score into the objec-
tive function. This is around 10% higher than the
gold standard compression rate.
We now turn to the results of our elicitation
study. We performed an Analysis of Variance
(ANOVA) to examine the effect of different system
compressions. Statistical tests were carried out on
the mean of the ratings shown in Table 3. We ob-
serve a reliable effect of compression type by sub-
retained in the compression.
150
jects (F1(3,165) = 132.74, p < 0.01) and items(F2(3,117) = 18.94, p < 0.01). Post-hoc Tukeytests revealed that gold standard compressions are
perceived as significantly better than those gener-
ated by all automatic systems (? < 0.05). There is
no significant difference between the IP language
model and decision-tree systems. However, the IP
model with the significance score delivers a sig-
nificant increase in performance over the language
model and the decision tree (? < 0.05).
These results indicate that reasonable compres-
sions can be obtained with very little supervision.
Our constraint-based language model does not
make use of a parallel corpus, whereas our second
variant uses only 50 parallel sentences for tuning
the weights of the objective function. The models
described in this paper could be easily adapted to
other domains or languages provided that syntac-
tic analysis tools are to some extent available.
6 Conclusions and Future Work
In this paper we have presented a novel method
for automatic sentence compression. A key aspect
of our approach is the use of integer program-
ming for inferring globally optimal compressions
in the presence of linguistically motivated con-
straints. We have shown that such a formulation
allows for a relatively simple and knowledge-lean
compression model that does not require parallel
corpora or access to large-scale knowledge bases.
Our results demonstrate that the IP model yields
performance comparable to state-of-the-art with-
out any supervision. We also observe significant
performance gains when a small amount of train-
ing data is employed (50 parallel sentences). Be-
yond the systems discussed in this paper, the ap-
proach holds promise for other models using de-
coding algorithms for searching the space of pos-
sible compressions. The search process could be
framed as an integer program in a similar fashion
to our work here.
We obtain our best results using a model whose
objective function includes a significance score.
The significance score relies mainly on syntactic
and lexical information for determining whether
a word is important or not. An appealing future
direction is the incorporation of discourse-based
constraints into our models. The latter would high-
light topical words at the document-level instead
of considering each sentence in isolation. An-
other important issue concerns the portability of
the models presented here to other languages and
domains. We plan to apply our method to lan-
guages with more flexible word order than English
(e.g., German) and more challenging spoken do-
mains (e.g., meeting data) where parsing technol-
ogy may be less reliable.
Acknowledgements
Thanks to Jean Carletta, Amit Dubey, Frank Keller, Steve
Renals, and Sebastian Riedel for helpful comments and sug-
gestions. Lapata acknowledges the support of EPSRC (grant
GR/T04540/01).
References
Briscoe, E. J. and J. Carroll. 2002. Robust accurate statisti-cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical lan-guage modeling using the CMU?cambridge toolkit. In
Proceedings of Eurospeech. Rhodes, Greece, pages 2707?2710.
Hori, Chiori and Sadaoki Furui. 2004. Speech summariza-tion: an approach through word extraction and a methodfor evaluation. IEICE Transactions on Information and
Systems E87-D(1):15?25.
Jing, Hongyan. 2000. Sentence reduction for automatic textsummarization. In Proceedings of the 6th ANLP. Seattle,WA, pages 310?315.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression. Artificial Intelligence 139(1):91?107.
Marciniak, Tomasz and Michael Strube. 2005. Beyond thepipeline: Discrete optimization in NLP. In Proceedings of
the 9th CoNLL. Ann Arbor, MI, pages 136?143.
McDonald, Ryan. 2006. Discriminative sentence compres-sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy, pages 297?304.
Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,Tu Bao Ho, and Masaru Fukushi. 2004. Probabilistic sen-tence reduction using support vector machines. In Pro-
ceedings of the 20th COLING. Geneva, Switzerland, pages743?749.
Olivers, S. H. and W. B. Dolan. 1999. Less is more; eliminat-ing index terms from subordinate clauses. In Proceedings
of the 37th ACL. College Park, MD, pages 349?356.
Press, William H., Saul A. Teukolsky, William T. Vetterling,and Brian P. Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak.2004. Semantic role labeling via integer linear program-ming inference. In Proceedings of the 20th COLING.Geneva, Switzerland, pages 1346?1352.
Riezler, Stefan, Tracy H. King, Richard Crouch, and AnnieZaenen. 2003. Statistical sentence condensation usingambiguity packing and stochastic disambiguation meth-ods for lexical-functional grammar. In Proceedings of
the HLT/NAACL. Edmonton, Canada, pages 118?125.
Roth, Dan and Wen-tau Yih. 2004. A linear programmingformulation for global inference in natural language tasks.In Proceedings of the 8th CoNLL. Boston, MA, pages 1?8.
Turner, Jenine and Eugene Charniak. 2005. Supervised andunsupervised learning for sentence compression. In Pro-
ceedings of the 43rd ACL. Ann Arbor, MI, pages 290?297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence compres-sion for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summarization.Barcelona, Spain, pages 89?95.
Winston, Wayne L. and Munirpallam Venkataramanan.2003. Introduction to Mathematical Programming.Brooks/Cole.
151
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 129?137,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incremental Integer Linear Programming for Non-projective Dependency
Parsing
Sebastian Riedel and James Clarke
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
s.r.riedel@sms.ed.ac.uk, jclarke@ed.ac.uk
Abstract
Integer Linear Programming has recently
been used for decoding in a number of
probabilistic models in order to enforce
global constraints. However, in certain ap-
plications, such as non-projective depen-
dency parsing and machine translation,
the complete formulation of the decod-
ing problem as an integer linear program
renders solving intractable. We present an
approach which solves the problem in-
crementally, thus we avoid creating in-
tractable integer linear programs. This ap-
proach is applied to Dutch dependency
parsing and we show how the addition
of linguistically motivated constraints can
yield a significant improvement over state-
of-the-art.
1 Introduction
Many inference algorithms require models to
make strong assumptions of conditional indepen-
dence between variables. For example, the Viterbi
algorithm used for decoding in conditional ran-
dom fields requires the model to be Markovian.
Strong assumptions are also made in the case of
McDonald et al?s (2005b) non-projective depen-
dency parsing model. Here attachment decisions
are made independently of one another1. However,
often such assumptions can not be justified. For
example in dependency parsing, if a subject has
already been identified for a given verb, then the
probability of attaching a second subject to the
verb is zero. Similarly, if we find that one coor-
dination argument is a noun, then the other argu-
1If we ignore the constraint that dependency trees must be
cycle-free (see sections 2 and 3 for details).
ment cannot be a verb. Thus decisions are often
co-dependent.
Integer Linear Programming (ILP) has recently
been applied to inference in sequential condi-
tional random fields (Roth and Yih, 2004), this
has allowed the use of truly global constraints
during inference. However, it is not possible to
use this approach directly for a complex task like
non-projective dependency parsing due to the ex-
ponential number of constraints required to pre-
vent cycles occurring in the dependency graph.
To model all these constraints explicitly would re-
sult in an ILP formulation too large to solve effi-
ciently (Williams, 2002). A similar problem also
occurs in an ILP formulation for machine transla-
tion which treats decoding as the Travelling Sales-
man Problem (Germann et al, 2001).
In this paper we present a method which extends
the applicability of ILP to a more complex set of
problems. Instead of adding all the constraints we
wish to capture to the formulation, we first solve
the program with a fraction of the constraints. The
solution is then examined and, if required, addi-
tional constraints are added. This procedure is re-
peated until all constraints are satisfied. We apply
this dependency parsing approach to Dutch due
to the language?s non-projective nature, and take
the parser of McDonald et al (2005b) as a starting
point for our model.
In the following section we introduce depen-
dency parsing and review previous work. In Sec-
tion 3 we present our model and formulate it as
an ILP problem with a set of linguistically mo-
tivated constraints. We include details of an in-
cremental algorithm used to solve this formula-
tion. Our experimental set-up is provided in Sec-
tion 4 and is followed by results in Section 5 along
with runtime experiments. We finally discuss fu-
129
Figure 1: A Dutch dependency tree for ?I?ll come
at twelve and then you?ll get what you deserve?
ture research and potential improvements to our
approach.
2 Dependency Parsing
Dependency parsing is the task of attaching words
to their arguments. Figure 1 shows a dependency
graph for the Dutch sentence ?I?ll come at twelve
and then you?ll get what you deserve? (taken from
the Alpino Corpus (van der Beek et al, 2002)). In
this dependency graph the verb ?kom? is attached
to its subject ?ik?. ?kom? is referred to as the head
of the dependency and ?ik? as its child. In labelled
dependency parsing edges between words are la-
belled with the relation captured. In the case of
the dependency between ?kom? and ?ik? the label
would be ?subject?.
In a dependency tree every token must be the
child of exactly one other node, either another to-
ken or the dummy root node. By definition, a de-
pendency tree is free of cycles. For example, it
must not contain dependency chains such as ?en?
? ?kom?? ?ik?? ?en?. For a more formal def-
inition see previous work (Nivre et al, 2004).
An important distinction between dependency
trees is whether they are projective or non-
projective. Figure 1 is an example of a projec-
tive dependency tree, in such trees dependencies
do not cross. In Dutch and other flexible word or-
der languages such as German and Czech we also
encounter non-projective trees, in these cases the
trees contain crossing dependencies.
Dependency parsing is useful for applications
such as relation extraction (Culotta and Sorensen,
2004) and machine translation (Ding and Palmer,
2005). Although less informative than lexicalised
phrase structures, dependency structures still cap-
ture most of the predicate-argument information
needed for applications. It has the advantage of be-
ing more efficient to learn and parse.
McDonald et al (2005a) introduce a depen-
dency parsing framework which treats the task as
searching for the projective tree that maximises
the sum of local dependency scores. This frame-
Figure 2: An incorrect partial dependency tree.
The verb ?krijg? is incorrectly coordinated with
the preposition ?om?.
work is efficient and has also been extended to
non-projective trees (McDonald et al, 2005b). It
provides a discriminative online learning algo-
rithm which when combined with a rich feature set
reaches state-of-the-art performance across multi-
ple languages.
However, within this framework one can only
define features over single attachment decisions.
This leads to cases where basic linguistic con-
straints are not satisfied (e.g. verbs with two sub-
jects or incompatible coordination arguments). An
example of this for Dutch is illustrated in Figure 2
which was produced by the parser of McDonald
et al (2005b). Here the parse contains a coordi-
nation of incompatible word classes (a preposition
and a verb).
Our approach is able to include additional con-
straints which forbid configurations such as those
in Figure 2. While McDonald and Pereira (2006)
address the issue of local attachment decisions by
defining scores over attachment pairs, our solution
is more general. Furthermore, it is complementary
in the sense that we could formulate their model
using ILP and then add constraints.
The method we present is not the only one that
can take global constraints into account. Deter-
ministic dependency parsing (Nivre et al, 2004;
Yamada and Matsumoto, 2003) can apply global
constraints by conditioning attachment decisions
on the intermediate parse built. However, for effi-
ciency a greedy search is used which may produce
sub-optimal solutions. This is not the case when
using ILP.
3 Model
Our underlying model is a modified labelled ver-
sion2 of McDonald et al (2005b):
s(x,y) =
?
(i,j,l)?y
s(i, j, l)
=
?
(i,j,l)?y
w ? f(i, j, l)
2Note that this is not described in the McDonald papers
but implemented in his software.
130
where x is a sentence, y is a set of labelled de-
pendencies, f(i, j, l) is a multidimensional fea-
ture vector representation of the edge from token
i to token j with label l and w the correspond-
ing weight vector. For example, a feature f101 in fcould be:
f101(i, j, l) =
?
?
?
?
?
1 if t(i) = ?en? ? p(j) = V
?l = ?coordination?
0 otherwise
where t(i) is the word at token i and p(j) the part-
of-speech tag at token j.
Decoding in this model amounts to finding the
y for a given x that maximises s(x,y):
y? = arg max
y
s(x,y)
while fulfilling the following constraints:
T1 For every non-root token in x there exists ex-
actly one head; the root token has no head.
T2 There are no cycles.
Thus far, the formulation follows McDonald
et al (2005b) and corresponds to the Maximum
Spanning Tree (MST) problem. In addition to T1
and T2, we include a set of linguistically moti-
vated constraints:
A1 Heads are not allowed to have more than one
outgoing edge labelled l for all l in a set of
labels U .
C1 In a symmetric coordination there is exactly
one argument to the right of the conjunction
and at least one argument to the left.
C2 In an asymmetric coordination there are no ar-
guments to the left of the conjunction and at
least two arguments to the right.
C3 There must be at least one comma between
subsequent arguments to the left of a sym-
metric coordination.
C4 Arguments of a coordination must have com-
patible word classes.
P1 Two dependencies must not cross if one of
their labels is in a set of labels P .
A1 covers constraints such as ?there can only
be one subject? if U contains ?subject? (see Sec-
tion 4.4 for more details of U ). C1 applies to
configurations which contain conjunctions such as
?en?,?of? or ?maar? (?and?, ?or? and ?but?). C2
will rule-out settings where a conjunction such as
?zowel? (translates as ?both?) having arguments
to its left. C3 forces coordination arguments to
the left of a conjunction to have commas in be-
tween. C4 avoids parses in which incompatible
word classes are coordinated, such as nouns and
verbs. Finally, P1 allows selective projective pars-
ing: we can, for instance, forbid the crossing of
?Noun-Determiner? dependencies if we add the
corresponding label type to P (see Section 4.4 for
more details of P ) . If we extend P to contain all
labels we forbid any type of crossing dependen-
cies. This corresponds to projective parsing.
3.1 Decoding
McDonald et al (2005b) use the Chu-Liu-
Edmonds (CLE) algorithm to solve the maxi-
mum spanning tree problem. However, global con-
straints cannot be incorporated into the CLE algo-
rithm (McDonald et al, 2005b). We alleviate this
problem by presenting an equivalent Integer Lin-
ear Programming formulation which allows us to
incorporate global constraints naturally.
Before giving full details of our formulation
we first introduce some of the concepts of lin-
ear and integer linear programming (for a more
thorough introduction see Winston and Venkatara-
manan (2003)).
Linear Programming (LP) is a tool for solving
optimisation problems in which the aim is to max-
imise (or minimise) a given linear function with
respect to a set of linear constraints. The func-
tion to be maximised (or minimised) is referred
to as the objective function. A number of decision
variables are under our control which exert influ-
ence on the objective function. Specifically, they
have to be optimised in order to maximise (or min-
imise) the objective function. Finally, a set of con-
straints restrict the values that the decision vari-
ables can take. Integer Linear Programming is an
extension of linear programming where all deci-
sion variables must take integer values.
There are several explicit formulations of the
MST problem as an integer linear program in the
literature (Williams, 2002). They are based on
the concept of eliminating subtours (cycles), cuts
(disconnections) or requiring intervertex flows
(paths). However, in practice these formulations
cause long solve times ? as the first two meth-
131
Algorithm 1 Incremental Integer Linear Program-
ming
C ? Bx
repeat
y? solve(C, Ox, Vx)
W ? violated(y, Ix)
C ? C ?W
until V = ?
return y
ods yield an exponential number of constraints.
Although the latter scales cubically, it produces
non-fractional solutions in its relaxed version; this
causes long runtimes for the branch and bound al-
gorithm (Williams, 2002) commonly used in inte-
ger linear programming. We found out experimen-
tally that dependency parsing models of this form
do not converge on a solution after multiple hours
of solving, even for small sentences.
As a workaround for this problem we follow an
incremental approach akin to the work of Warme
(1998). Instead of adding constraints which forbid
all possible cycles in advance (this would result
in an exponential number of constraints) we first
solve the problem without any cycle constraints.
The solution is then examined for cycles, and if
cycles are found we add constraints to forbid these
cycles; the solver is then run again. This process
is repeated until no more violated constraints are
found. The same procedure is used for other types
of constraints which are too expensive to add in
advance (e.g. the constraints of P1).
Algorithm 1 outlines our approach. For a sen-
tence x, Bx is the set of constraints that we addin advance and Ix are the constraints we add in-crementally. Ox is the objective function and Vxis a set of variables including integer declarations.
solve(C, O, V ) maximises the objective function
O with respect to the set of constraints C and vari-
ables V . violated(y, I) inspects the proposed so-
lution (y) and returns all constraints in I which are
violated.
The number of iterations required in this ap-
proach is at most polynomial with respect to the
number of variables (Gro?tschel et al, 1981). In
practice, this technique converges quickly (less
than 20 iterations in 99% of approximately 12,000
sentences), yielding average solve times of less
than 0.5 seconds.
Our approach converges quickly due to the
quality of the scoring function. Its weights have
been trained on cycle free data, thus it is more
likely to guide the search to a cycle free solution.
In the following section we present the objec-
tive function Ox, variables Vx and linear con-straints Bx and Ix needed for parsing x using Al-gorithm 1.
3.1.1 Variables
Vx contains a set of binary variables to representlabelled edges:
ei,j,l ?i ? 0..n, j ? 1..n,
l ? bestk(i, j)
where n is the number of tokens and the index 0
represents the root token. bestk(i, j) is the set of klabels with highest s(i, j, l). ei,j,l equals 1 if thereis a edge (i.e., a dependency) with the label l be-
tween token i (head) and j (child), 0 otherwise. k
depends on the type of constraints we want to add.
For the plain MST problem it is sufficient to set
k = 1 and only take the best scoring label for each
token pair. However, if we want a constraint which
forbids duplicate subjects we need to provide ad-
ditional labels to fall back on.
Vx also contains a set of binary auxiliary vari-ables:
di,j ?i ? 0..n, j ? 1..n
which represent the existence of a dependency be-
tween tokens i and j. We connect these to the ei,j,lvariables by the constraint:
di,j =
?
l?bestk(i,j)
ei,j,l
3.1.2 Objective Function
Given the above variables our objective function
Ox can be represented as (using a suitable k):
?
i,j
?
l?bestk(i,j)
s(i, j, l) ? ei,j,l
3.1.3 Base Constraints
We first introduce a set of base constraints Bxwhich we add in advance.
Only One Head (T1) Every token has exactly
one head:
?
i
di,j = 1
for non-root tokens j > 0 in x. An exception is
made for the artificial root node:
?
i
di,0 = 0
132
Label Uniqueness (A1) To enforce uniqueness
of children with labels in U we augment our model
with the constraint:
?
j
ei,j,l ? 1
for every token i in x and label l in U .
Symmetric Coordination (C1) For each con-
junction token i which forms a symmetric coor-
dination we add:
?
j<i
di,j ? 1
and
?
j>i
di,j = 1
Asymmetric Coordination (C2) For each con-
junction token i which forms an asymmetric coor-
dination we add:
?
j<i
di,j = 0
and
?
j>i
di,j ? 2
3.1.4 Incremental Constraints
Now we present the set of constraints Ix we addincrementally. The constraints are chosen based on
the two criteria: (1) adding them to the base con-
straints (those added in advance) would result in
an extremely large program, and (2) it must be ef-
ficient to detect whether the constraint is violated
in y.
No Cycles (T2) For every possible cycle c for
the sentence x we have a constraint which forbids
the case where all edges in c are active simultane-
ously:
?
(i,j)?c
di,j ? |c| ? 1
Comma Coordination (C3) For each symmet-
ric conjunction token i which forms a symmetric
coordination and each set of tokens A in x to the
left of i with no comma between each pair of suc-
cessive tokens we add:
?
a?A
di,a ? |A| ? 1
which forbids configurations where i has the argu-
ment tokens A.
Compatible Coordination Arguments (C4)
For each conjunction token i and each set of to-
kens A in x with incompatible POS tags, we add a
constraint to forbid configurations where i has the
argument tokens A.
?
a?A
di,a ? |A| ? 1
Selective Projective Parsing (P1) For each pair
of triplets (i, j, l1) and (m, n, l2) we add the con-straint:
ei,j,l1 + em,n,l2 ? 1
if l1 or l2 is in P .
3.2 Training
For training we use single-best MIRA (McDon-
ald et al, 2005a). This is an online algorithm that
learns by parsing each sentence and comparing
the result with a gold standard. Training is com-
plete after multiple passes through the whole cor-
pus. Thus we decode using the Chu-Liu-Edmonds
(CLE) algorithm due to its speed advantage over
ILP (see Section 5.2 for a detailed comparison of
runtimes).
The fact that we decode differently during train-
ing (using CLE) and testing (using ILP) may de-
grade performance. In the presence of additional
constraints weights may be able to capture other
aspects of the data.
4 Experimental Set-up
Our experiments were designed to answer the fol-
lowing questions:
1. How much do our additional constraints help
improve accuracy?
2. How fast is our generic inference method in
comparison with the Chu-Liu-Edmonds algo-
rithm?
3. Can approximations be used to increase the
speed of our method while remaining accu-
rate?
Before we try to answer these questions we briefly
describe our data, features used, settings for U and
P in our parametric constraints, our working envi-
ronment and our implementation.
133
4.1 Data
We use the Alpino treebank (van der Beek et al,
2002), taken from the CoNLL shared task of mul-
tilingual dependency parsing3. The CoNLL data
differs slightly from the original Alpino treebank
as the corpus has been part-of-speech tagged using
a Memory-Based-Tagger (Daelemans et al, 1996).
It consists of 13,300 sentences with an average
length of 14.6 tokens. The data is non-projective,
more specifically 5.4% of all dependencies are
crossed by at least one other dependency. It con-
tains approximately 6000 sentences more than the
Alpino corpus used by Malouf and van Noord
(2004).
The training set was divided into a 10% devel-
opment set (dev) while the remaining 90% is used
as a training and cross-validation set (cross). Fea-
ture sets, constraints and training parameters were
selected through training on cross and optimising
against dev.
Our final accuracy scores and runtime eval-
uations were acquired using a nine-fold cross-
validation on cross
4.2 Environment and Implementation
All our experiments were conducted on a Intel
Xeon with 3.8 Ghz and 4Gb of RAM. We used
the open source Mixed Integer Programming li-
brary lp solve4 to solve the Integer Linear Pro-
grams. Our code ran in Java and called the JNI-
wrapper around the lp solve library.
4.3 Feature Sets
Our feature set was determined through experi-
mentation with the development set. The features
are based upon the data provided within the Alpino
treebank. Along with POS tags the corpus contains
several additional attributes such as gender, num-
ber and case.
Our best results on the development set were
achieved using the feature set of McDonald et al
(2005a) and a set of features based on the addi-
tional attributes. These features combine the at-
tributes of the head with those of the child. For
example, if token i has the attributes a1 and a2,and token j has the attribute a3 then we createdthe features (a1 ? a3) and (a2 ? a3).
3For details see http://nextens.uvt.nl/
?conll.
4The software is available from http://www.
geocities.com/lpsolve.
4.4 Constraints
All the constraints presented in Section 3 were
used in our model. The set U of unique labels
constraints contained su, obj1, obj2, sup, ld, vc,
predc, predm, pc, pobj1, obcomp and body. Here
su stands for subject and obj1 for direct object (for
full details see Moortgat et al (2000)).
The set of projective labels P contained cnj,
for coordination dependencies; and det, for de-
terminer dependencies. One exception was added
for the coordination constraint: dependencies can
cross when coordinated arguments are verbs.
One drawback of hard deterministic constraints
is the undesirable effect noisy data can cause. We
see this most prominently with coordination argu-
ment compatibility. Words ending in ?en? are typ-
ically wrongly tagged and cause our coordination
argument constraint to discard correct coordina-
tions. As a workaround we assigned words ending
in ?en? a wildcard POS tag which is compatible
with all POS tags.
5 Results
In this section we report our results. We not only
present our accuracy but also provide an empiri-
cal evaluation of the runtime behaviour of this ap-
proach and show how parsing can be accelerated
using a simple approximation.
5.1 Accuracy
An important question to answer when using
global constraints is: How much of a performance
boost is gained when using global constraints?
We ran the system without any linguistic con-
straints as a baseline (bl) and compared it to a
system with the additional constraints (cnstr). To
evaluate our systems we use the accuracy over la-
belled attachment decisions:
LAC = NlNt
where Nl is the number of tokens with correcthead and label and Nt is the total number of to-kens. For completeness we also report the unla-
belled accuracy:
UAC = NuNt
where Nu is the number of tokens with correcthead.
134
LAC UAC LC UC
bl 84.6% 88.9% 27.7% 42.2%
cnstr 85.1% 89.4% 29.7% 43.8%
Table 1: Labelled (LAC) and unlabelled (UAC) ac-
curacy using nine-fold cross-validation on cross
for baseline (bl) and constraint-based (constr) sys-
tem. LC and UC are the percentages of sentences
with 100% labelled and unlabelled accuracy, re-
spectively.
Table 1 shows our results using nine-fold cross-
validation on the cross set. The baseline system
(no additional constraints) gives an unlabelled ac-
curacy of 84.6% and labelled accuracy of 88.9%.
When we add our linguistic constraints the per-
formance increases by 0.5% for both labelled and
unlabelled accuracy. This increase is significant
(p < 0.001) according to Dan Bikel?s parse com-
parison script and using the Sign test (p < 0.001).
Now we give a little insight into how our results
compare with the rest of the community. The re-
ported state-of-the-art parser of Malouf and van
Noord (2004) achieves 84.4% labelled accuracy
which is very close numerically to our baseline.
However, they use a subset of the CoNLL Alpino
treebank with a higher average number of tokens
per sentences and also evaluate control relations,
thus results are not directly comparable. We have
also run our parser on the relatively small (approx-
imately 400 sentences) CoNNL test data. The best
performing system (McDonald et al 2006; note:
this system is different to our baseline) achieves
79.2% labelled accuracy while our baseline sys-
tem achieves 78.6% and our constrained version
79.8%. However, a significant difference is only
observed between our baseline and our constraint-
based system.
Examining the errors produced using the dev
set highlight two key reasons why we do not see
a greater improvement using our constraint-based
system. Firstly, we cannot improve on coordina-
tions that include words ending with ?en? based on
the workaround present in Section 4.4. This prob-
lem can only be solved by improving POS taggers
for Dutch or by performing POS tagging within
the dependency parsing framework.
Secondly, our system suffers from poor next
best solutions. That is, if the best solution violates
some constraints, then we find the next best solu-
tion is typically worse than the best solution with
violated constraints. This appears to be a conse-
quence of inaccurate local score distributions (as
opposed to inaccurate best local scores). For ex-
ample, suppose we attach two subjects, t1 and t2,to a verb, where t1 is the actual subject while t2is meant to be labelled as object. If we forbid this
configuration (two subjects) and if the score of la-
belling t1 object is higher than that for t2 beinglabelled subject, then the next best solution will
label t1 incorrectly as object and t2 incorrectly assubject. This is often the case, and thus results in a
drop of accuracy.
5.2 Runtime Evaluation
We now concentrate on the runtime of our method.
While we expect a longer runtime than using the
Chu-Liu-Edmonds as in previous work (McDon-
ald et al, 2005b), we are interested in how large
the increase is.
Table 2 shows the average solve time (ST) for
sentences with respect to the number of tokens in
each sentence for our system with constraints (cn-
str) and the Chu-Liu-Edmonds (CLE) algorithm.
All solve times do not include feature extraction
as this is identical for all systems. For cnstr we
also show the number of sentences that could not
be parsed after two minutes, the average number
of iterations and the average duration of the first
iteration.
The results show that parsing using our generic
approach is still reasonably fast although signifi-
cantly slower than using the Chu-Liu-Edmonds al-
gorithm. Also, only a small number of sentences
take longer than two minutes to parse. Thus, in
practice we would not see a significant degrada-
tion in performance if we were to fall back on the
CLE algorithm after two minutes of solving.
When we examine the average duration of the
first iteration it appears that the majority of the
solve time is spent within this iteration. This could
be used to justify using the CLE algorithm to find
a initial solution as starting point for the ILP solver
(see Section 6).
5.3 Approximation
Despite the fact that our parser can parse all sen-
tences in a reasonable amount of time, it is still sig-
nificantly slower than the CLE algorithm. While
this is not crucial during decoding, it does make
discriminative online training difficult as training
requires several iterations of parsing the whole
corpus.
135
Tokens 1-10 11-20 21-30 31-40 41-50 >50
Count 5242 4037 1835 650 191 60
Avg. ST (CLE) 0.27ms 0.98ms 3.2ms 7.5ms 14ms 23ms
Avg. ST (cnstr) 5.6ms 52ms 460ms 1.5s 7.2s 33s
ST > 120s (cnstr) 0 0 0 0 3 3
Avg. # iter. (cnstr) 2.08 2.87 4.48 5.82 8.40 15.17
Avg. ST 1st iter. (cnstr) 4.2ms 37ms 180ms 540ms 1.3s 2.6s
Table 2: Runtime evaluation for different sentence lengths. Average solve time (ST) for our system
with constraints (constr), the Chu-Liu-Edmonds algorithm (CLE), number of sentences with solve times
greater than 120 seconds, average number of iterations and first iteration solve time.
q=5 q=10 all CLE
LAC 84.90% 85.11% 85.14% 85.14%
ST 351s 760s 3640s 20s
Table 3: Labelled accuracy (LAC) and total solve
time (ST) for the cross dataset using varying q val-
ues and the Chu-Liu-Edmonds algorithm (CLE)
Thus we investigate if it is possible to speed up
our inference using a simple approximation. For
each token we now only consider the q variables
in Vx with the highest scoring edges. For exam-ple, if we set q = 2 the set of variables for a to-
ken j will contain two variables, either both for
the same head i but with different labels (variables
ei,j,l1 and ei,j,l2) or two distinct heads i1 and i2(variables ei1,j,l1 and ei2,j,l2) where labels l1 and
l2 may be identical.
Table 3 shows the effect of different q values
on solve time for the full corpus cross (roughly
12,000 sentences) and overall accuracy. We see
that solve time can be reduced by 80% while only
losing a marginal amount of accuracy when we set
q to 10. However, we are unable to reach the 20
seconds solve time of the CLE algorithm. Despite
this, when we add the time for preprocessing and
feature extraction, the CLE system parses a cor-
pus in around 15 minutes whereas our system with
q = 10 takes approximately 25 minutes5. When
we view the total runtime of each system we see
our system is more competitive.
6 Discussion
While we have presented significant improve-
ments using additional constraints, one may won-
5Even when caching feature extraction during training
McDonald et al (2005a) still takes approximately 10 minutes
to train.
der whether the improvements are large enough
to justify further research in this direction; espe-
cially since McDonald and Pereira (2006) present
an approximate algorithm which also makes more
global decisions. However, we believe that our ap-
proach is complementary to their model. We can
model higher order features by using an extended
set of variables and a modified objective function.
Although this is likely to increase runtime, it may
still be fast enough for real world applications. In
addition, it will allow exact inference, even in the
case of non-projective parsing. Also, we argue that
this approach has potential for interesting exten-
sions and applications.
For example, during our runtime evaluations we
find that a large fraction of solve time is spent in
the first iteration of our incremental algorithm. Af-
ter the first iteration the solver uses its last state to
efficiently search for solutions in the presence of
new constraints. Some solvers allow the specifica-
tion of an initial solution as a starting point, thus it
is expected that significant improvements in terms
of speed can be made by using the CLE algorithm
to provide an initial solution.
Our approach uses a generic algorithm to solve
a complex task. Thus other applications may ben-
efit from it. For instance, Germann et al (2001)
present an ILP formulation of the Machine Trans-
lation (MT) decoding task in order to conduct ex-
act inference. However, their model suffers from
the same type of exponential blow-up we observe
when we add all our cycle constraints in advance.
In fact, the constraints which cause the exponential
explosion in their graphically formulation are of
the same nature as our cycle constraints. We hope
that the incremental approach will allow exact MT
decoding for longer sentences.
136
7 Conclusion
In this paper we have presented a novel ap-
proach for inference using ILP. While previous ap-
proaches which use ILP for decoding have solved
each integer linear program in one run, we incre-
mentally add constraints and solve the resulting
program until no more constraints are violated.
This allows us to efficiently use ILP for depen-
dency parsing and add constraints which provide
a significant improvement over the current state-
of-the-art parser (McDonald et al, 2005b) on the
Dutch Alpino corpus (see bl row in Table 1).
Although slower than the baseline approach,
our method can still parse large sentences (more
than 50 tokens) in a reasonable amount of time
(less than a minute). We have shown that pars-
ing time can be significantly reduced using a
simple approximation which only marginally de-
grades performance. Furthermore, we believe that
the method has potential for further extensions and
applications.
Acknowledgements
Thanks to Ivan Meza-Ruiz, Ruken C?ak?c?, Beata
Kouchnir and Abhishek Arun for their contribu-
tion during the CoNLL shared task and to Mirella
Lapata for helpful comments and suggestions.
References
Culotta, Aron and Jeffery Sorensen. 2004. Dependency tree
kernels for relation extraction. In 42nd Annual Meeting of
the Association for Computational Linguistics. Barcelona,
Spain, pages 423?429.
Daelemans, W., J. Zavrel, and S. Berck. 1996. MBT: A
memory-based part of speech tagger-generator. In Pro-
ceedings of the Fourth Workshop on Very Large Corpora.
pages 14?27.
Ding, Yuan and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency insertion
grammars. In The 43rd Annual Meeting of the Association
of Computational Linguistics. Ann Arbor, MI, USA, pages
541?548.
Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal de-
coding for machine translation. In Meeting of the Asso-
ciation for Computational Linguistics. Toulouse, France,
pages 228?235.
Gro?tschel, M., L. Lova?sz, and A. Schrijver. 1981. The ellip-
soid method and its consequences in combina- torial opti-
mization. Combinatorica I:169? 197.
Malouf, Robert and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
Proc. of IJCNLP-04 Workshop ?Beyond Shallow Analy-
ses?. Sanya City, Hainan Island, China.
McDonald, R., K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In 43rd An-
nual Meeting of the Association for Computational Lin-
guistics. Ann Arbor, MI, USA, pages 91?98.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In 11th Con-
ference of the European Chapter of the Association for
Computational Linguistics. Trento, Italy, pages 81?88.
McDonald, R., F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree
algorithms. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Methods in
Natural Language Processing. Association for Computa-
tional Linguistics, Vancouver, British Columbia, Canada,
pages 523?530.
McDonald, Ryan, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-stage
discriminative parser. In Proceedings of CoNLL-2006.
New York, USA.
Moortgat, M., I. Schuurman, and T. van der Wouden.
2000. Cgn syntactische annotatie. Internal report Corpus
Gesproken Nederlands.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proceedings of CoNLL-2004. Boston,
MA, USA, pages 49?56.
Roth, D. and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of CoNLL-2004,. Boston, MA, USA, pages
1?8.
van der Beek, L., G. Bouma, R. Malouf, G. van Noord,
Leonoor van der Beek, Gosse Bouma, Robert Malouf, and
Gertjan van Noord. 2002. The Alpino dependency tree-
bank. In Computational Linguistics in the Netherlands
(CLIN). Rodopi.
Warme, David Michael. 1998. Spanning Trees in Hyper-
graphs with Application to Steiner Trees. Ph.D. thesis,
University of Virginia.
Williams, Justin C. 2002. A linear-size zero - one program-
ming model for the minimum spanning tree problem in
planar graphs. Networks 39:53?60.
Winston, Wayne L. and Munirpallam Venkataramanan.
2003. Introduction to Mathematical Programming.
Brooks/Cole.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings of IWPT . pages 195?206.
137
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958?967,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Reading to Learn: Constructing Features from Semantic Abstracts
Jacob Eisenstein
?
James Clarke
?
Dan Goldwasser
?
Dan Roth
??
?
Beckman Institute for Advanced Science and Technology,
?
Department of Computer Science
University of Illinois
Urbana, IL 61801
{jacobe,clarkeje,goldwas1,danr}@illinois.edu
Abstract
Machine learning offers a range of tools
for training systems from data, but these
methods are only as good as the underly-
ing representation. This paper proposes to
acquire representations for machine learn-
ing by reading text written to accommo-
date human learning. We propose a novel
form of semantic analysis called read-
ing to learn, where the goal is to obtain
a high-level semantic abstract of multi-
ple documents in a representation that fa-
cilitates learning. We obtain this abstract
through a generative model that requires
no labeled data, instead leveraging repe-
tition across multiple documents. The se-
mantic abstract is converted into a trans-
formed feature space for learning, result-
ing in improved generalization on a rela-
tional learning task.
1 Introduction
Machine learning offers a range of powerful tools
for training systems to act in complex environ-
ments, but these methods depend on a well-chosen
representation for features. For learning to suc-
ceed the representation often must be crafted with
knowledge about the application domain. This
poses a bottleneck, requiring expertise in both ma-
chine learning and the application domain. How-
ever, domain experts often express their knowl-
edge through text; one direct expression is through
text designed to aid human learning. In this paper
we exploit text written by domain experts in or-
der to build a more expressive representation for
learning. We term this approach reading to learn.
The following scenario demonstrates the moti-
vation for reading to learn. Imagine an agent given
a task within its world/environment. The agent has
no prior knowledge of the task but can perceive the
world through low-level sensors. Learning directly
from the sensors may be difficult, as interesting
tasks typically require a complex combination of
sensors. Our goal is to acquire domain knowledge
through the semantic analysis of text, so as to pro-
duce higher-level relations through combinations
of sensors.
As a concrete example consider the problem of
learning how to make legal moves in Freecell soli-
taire. Relevant sensors may indicate if an object
is a card or a freecell, whether a card is a certain
value, and whether two values are in sequence.
Although it is possible to express the rules with
a combination of sensors, learning this combina-
tion is difficult. Text can facilitate learning by pro-
viding relations at the appropriate level of gen-
eralization. For example, the sentence: ?You can
place a card on an empty freecell,? suggests not
only which sensors are useful together but also
how these sensors should be linked. Assuming the
sensors are represented as predicates, one possi-
ble relation this sentence suggests is: r(x, y) =
card(x) ? freecell(y) ? empty(y). Armed
with this new relation the agent?s learning task
may be simpler. Throughout the paper we refer to
low-level sensory input as sensor or predicate, and
to a higher level concept as a logical formula or re-
lation.
Our approach to semantic analysis does not re-
quire a complete semantic representation of the
text. We merely wish to acquire a semantic ab-
stract of a document or document collection, and
use the discovered relations to facilitate data-
driven learning. This will allow us to directly eval-
uate the contribution of the extracted relations for
learning.
We develop an approach to recover semantic ab-
stracts that uses minimal supervision: we assume
only a very small set of lexical glosses, which map
from words to sensors. This marks a substantial
departure from previous work on semantic pars-
ing, which requires either annotations of the mean-
ings of each individual sentence (Zettlemoyer and
Collins, 2005; Liang et al, 2009), or alignments
of sentences to grounded representations of the
958
world (Chen and Mooney, 2008). For the purpose
of learning, this approach may be inapplicable, as
such text is often written at a high level of abstrac-
tion that permits no grounded representation.
There are two properties of our setting that
make unsupervised learning feasible. First, it is
not necessary to extract a semantic representation
of each individual sentence, but rather a summary
of the semantics of the document collection. Er-
rors in the semantic abstract are not fatal, as long
it guides the learning component towards a more
useful representation. Second, we can exploit rep-
etition across documents, which should generally
express the same underlying meaning. Logical for-
mulae that are well-supported by multiple docu-
ments are especially likely to be useful.
The rest of this paper describes our approach
for recovering semantic abstracts and outlines how
we apply and evaluate this approach on the Free-
cell domain. The paper contributes the following
key ideas: (1) Interpreting abstract ?instructional?
text, written at a level that does not correspond
to concrete sensory inputs in the world, so that
no grounded representation is possible, (2) read-
ing to learn, a new setting in which extracted se-
mantic representations are evaluated by whether
they facilitate learning; (3) abstractive semantic
summarization, aimed at capturing broad seman-
tic properties of a multi-document dataset, rather
than a semantic parse of individual sentences; (4) a
novel, minimally-supervised generative model for
semantic analysis which leverages both lexical and
syntactic properties of text.
2 Approach Overview
We describe our approach to text analysis as mul-
tidocument semantic abstraction, with the goal of
discovering a compact set of logical formulae to
explain the text in a document collection. To this
end, we develop a novel generative model in which
natural language sentences (e.g., ?You can always
place cards in empty freecells?) are stochastically
generated from logical formulae (e.g., card(x)?
freecell(y) ? empty(y)). We formally define
a generative process that reflects our intuitions
about the relationship between formulae and sen-
tences (Section 3), and perform sampling-based
inference to recover the formulae most likely to
have generated the observed data (Section 4). The
top N such formulae can then be added as addi-
tional predicates for relational learning.
Our semantic representation consists of con-
junctions of literals, each of which includes a sin-
gle predicate (e.g., empty) and one or more vari-
ables (e.g., x). Predicates describe atomic seman-
tic concepts, while variables construct networks
of relationships between them. While the impor-
tance of the predicates is obvious, the variable
assignments also exert a crucial influence on the
semantics of the conjunction: modifying a sin-
gle variable in the formula above from empty(y)
to empty(x) yields a formula that is trivially
false for all groundings (since cards can never be
empty).
Thus, our generative model must account for the
influence of both predicates and variables on the
sentences in the documents. A natural choice is to
use the predicates to influence the lexical items,
while letting the variables determine the syntac-
tic structure. For example, the formula card(x)?
freecell(y) ? empty(y) contains three pred-
icates and two variables. The predicates influence
the lexical items in a direct way: we expect that
sentences generated from this formula will include
a member of the gloss set for each predicate ?
the sentence ?Put the cards on the empty free-
cells? should be more likely than ?Columns are
constructed by playing cards in alternating colors.?
The impact of the variables on the generative
process is more subtle. The sharing of the variable
y suggests a relationship between the predicates
freecell and empty. This should be realized
in the syntactic structure of the sentence. Model-
ing syntax using a dependency tree, we expect that
the glosses for predicates that share terms will ap-
pear in compact sub-trees, while predicates that do
not share terms should be more distant. One pos-
sible surface realization of this logical formula is
the sentence, ?Put the card on the empty freecell,?
whose dependency parse is shown in the left tree
of Figure 1. The glosses empty and freecell are im-
mediately adjacent, while card is more remote.
We develop two metrics that quantify the com-
pactness of a set of variable assignments with
respect to a dependency tree: excess terms, and
shared terms. The number of excess terms in a
subtree is the number of unique terms assigned
to words in the subtree, minus the maximum arity
of any predicate in the subtree. Shared terms arise
whenever a node has multiple subtrees which each
contain the same variable. We will use the alterna-
tive alignments in Figure 1 to provide a more de-
tailed explanation. In each tree, the variables are
written in the nodes belonging to the associated
lexical items; variables are written over arrows to
indicate membership in some node in the subtree.
Excess Terms Alignment A of Fig-
ure 1, corresponding to the formula
959
Put
card on
freecell
the empty
the
X
X
X
XXX
X XX
XX
X
X
X
Y
XXY
X XY
XY
Y
X
Y
Y
XYY
X YY
YY
Y
X
Y
Z
XYZ
X YZ
YZ
Z
Dependency tree Alignment A Alignment B Alignment C Alignment D
Figure 1: A dependency parse and four different variable assignments. Each literal is aligned to a word (a
node in the graph), and the associated variables are written in the box. Variables belonging to descendant
nodes are written over the arrows.
card(x)?freecell(x)?empty(x), has zero
excess terms in every subtree; there is a total of one
variable, and all the predicates are unary. In Align-
ment B, card(x) ? freecell(x) ? empty(y),
there are excess terms at the root, and in the top
two subtrees on the right-hand side. Alignment C
contains an excess term at only the root node.
Even though it contains the same number of
unique variables as Alignment B, it is not penal-
ized as harshly because the alignment of variables
better corresponds to the syntactic structure.
Alignment D contains the greatest number of
excess terms: two at the root of the tree, and one
in each of the top two subtrees on the right side.
Shared Terms According to the excess term
metric, the best choice is simply to introduce as
few variables as possible. For this reason, we also
penalize shared terms which occur when a node
has subtree children that share a variable. In Fig-
ure 1, Alignments A and B each contain a shared
term at the top node; Alignments C and D contain
no shared terms.
Overall, we note that Alignment B is penalized
on both metrics, as it contains both excess terms
and shared terms; the syntactic structure of the
sentence makes such a variable assignment rela-
tively improbable.
card(x) & freecell(y) & empty(y)
f(y)e(y)c(x)
f(y)e(y)c(x)
Put the card on the empty freecell
(a)
(b)
(c)
(d)
(e)
Figure 2: A graphical depiction of the generative
process by which sentences are produced from for-
mulae
3 Generative Model
These intuitions are formalized in a generative
account of how sentences are stochastically pro-
duced from a set of logical formulae. This gener-
ative story guides an inference procedure for re-
covering logical formulae that are likely to have
generated any observed set of texts, which is de-
scribed in Section 4.
The outline of the generative process is depicted
in Figure 2. For each sentence, we begin in step (a)
by drawing a formula f from a Dirichlet pro-
cess (Ferguson, 1973). The Dirichlet process de-
960
fines a non-parametric mixture model, and has the
effect of adaptively selecting the appropriate num-
ber of formulae to explain the observed sentences
in the corpus.
1
We then draw the sentence length
from some distribution over positive integers; as
the sentence length is always observed, we need
not define the distribution (step (b)). In step (c), a
dependency tree is drawn from a uniform distribu-
tion over spanning trees with a number of nodes
equal to the length of the sentence. In step (d) we
draw an alignment of the literals in f to nodes in
the dependency tree, written a
t
(f). The distribu-
tion over alignments is described in Section 3.1.
Finally, the aligned literals are used to generate the
words at each slot in the dependency tree. A more
formal definition of this process is as follows:
? Draw ?, the expected number of literals per
formula, from a Gamma distribution G(u, v).
? Draw an infinite set of formulae f . For each
formula f
i
,
? Draw the formula length #|f
i
| from a
Poisson distribution, n
i
? Poisson(?).
? Draw n
i
literals from a uniform distri-
bution.
? Draw pi, an infinite multinomial distribution
over formulae: pi ? GEM(pi
0
), where GEM
refers to the stick-breaking prior (Sethura-
man, 1994) and pi
0
= 1 is the concentra-
tion parameter. By attaching the multinomial
pi to the infinite set of formulae f , we cre-
ate a Dirichlet process. This is conventionally
writtenDP (pi
0
, G
0
), where the base distribu-
tionG
0
encodes only the distribution over the
number of literals, Poisson(?).
? For each of D documents, draw the number
of sentences T ? Poisson. For each of the T
sentences in the document,
? Draw a formula f ? DP (pi
0
, G
0
) from
the Dirichlet Process described above.
? Draw a sentence length #|s| ? Poisson.
? Draw a dependency graph t (a spanning
tree of size #|s|) from a uniform distri-
bution.
? Draw an alignment a
t
(f), an injective
mapping from literals in f to nodes in
the dependency structure t. The distribu-
tion over alignments is described in Sec-
tion 3.1.
1
There are many recent applications of Dirichlet pro-
cesses in natural language processing, e.g. Goldwater et al
(2006).
? Draw the sentence s from the formula
f and the alignment a(f). For each
word token w
i
? s is drawn from
p(w
i
|a
t
(f, i)), where a
t
(f, i) indicates
the (possibly empty) literal assigned
to slot i in the alignment a
t
(f) (Sec-
tion 3.2).
3.1 Distribution over Alignments
The distribution over alignments reflects our intu-
ition that when literals share variables, they will
be aligned to word slots that are nearby in the de-
pendency structure; literals that do not share vari-
ables should be more distant. This is formalized by
applying the concepts of excess terms and shared
terms defined in Section 2. After computing the
number of excess and shared terms in each sub-
tree t
i
, we can compute a local score (LS ) for that
subtree:
LS (a
t
(f); t
i
) = ? ?NShared(a
t
(f), t
i
)
+ ? ?NExcess(a
t
(f), t
i
) ? height(t
i
).
This scoring function can be applied recursively to
each subtree in t; the overall score of the tree is the
recursive sum,
score(a
t
(f); t) = LS (a
t
(f); t)+
n
?
i
score(a
t
(f); t
i
),
(1)
where t
i
indicates the i
th
subtree of t. We hypoth-
esize a generative process that produces all possi-
ble alignments, scores them using score(a
t
(f); t),
and selects an alignment with probability,
p(a
t
(f)) ? exp{?score(a
t
(f); t)}. (2)
In our experiments, we define the parameters ? =
1, ? = 1.
3.2 Generation of Lexical Items
Once the logical formula is aligned to the parse
structure, the generation of the lexical items in
the sentence is straightforward. For word slots to
which no literals are aligned, the lexical item is
drawn from a language model ?, estimated from
the entire document collection. For slots to which
at least one literal is aligned, we construct a lan-
guage model ? in which the probability mass is
divided equally among all glosses of aligned pred-
icates. The language model ? is used as a backoff,
so that there is a strong bias in favor of generating
glosses, but some probability mass is reserved for
the other lexical items.
961
4 Inference
This section describes a sampling-based inference
procedure for obtaining a set of formulae f that
explain the observed text s and dependency struc-
tures t. We perform Gibbs sampling over the
formulae assigned to each sentence. Using the
Chinese Restaurant Process interpretation of the
Dirichlet Process (Aldous, 1985), we marginalize
pi, the infinite multinomial over all possible for-
mulae: at each sampling step we select either an
existing formula, or stochastically generate a new
formula. After each full round of Gibbs sampling,
a set of Metropolis-Hastings moves are applied to
explore modifications of the formulae. This proce-
dure converges on a stationary Markov chain cen-
tered on a set of formulae that cohere well with the
lexical and syntactic properties of the text.
4.1 Assigning Sentences to Formulae
For each sentence s
i
and dependency tree t
i
, a hid-
den variable y
i
indicates the index of the formula
that generates the text. We can resample y
i
using
Gibbs sampling. In the non-parametric setting, y
i
ranges over all non-negative integers; the Chinese
Restaurant Process formulation marginalizes the
infinite-dimensional parameter pi, yielding a prior
based on the counts for each ?active? formula (to
which at least one other sentence is assigned), and
a pseudo-count representing all non-active formu-
lae. Given K formulae, the prior on selecting for-
mula j is:
p(y
i
= j|y
?i
, pi
0
) ?
{
n
?i
(j) j < K
pi
0
j = K,
(3)
where y
?i
refers to the assignments of all y other
than y
i
and n
?i
refers to the counts over these as-
signments. Each j < K identifies an existing for-
mula in f , to which at least one other sentence is
assigned. When j = K, this means a new formula
f
?
must be generated.
To perform Gibbs sampling, we draw from the
posterior distribution over y
i
,
p(y
i
|s
i
, t
i
f , f
?
,y
?i
, pi
0
) ?
p(y
i
|y
?i
, pi
0
)p(s
i
, t
i
|y
i
, f , f
?
),
where the first term is the prior defined in Equa-
tion 3 and the latter term is the likelihood of gener-
ating the parsed sentence ?s
i
, t
i
? from the formula
indexed by y
i
.
To compute the probability of a parsed sentence
given a formula, we sum over alignments,
p(s, t|f) =
?
a
t
(f)
p(s, t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(t,a
t
(f)|f)
=
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(4)
applying the chain rule and independence assump-
tions from the generative model. The result is a
product of three terms: the likelihood of the lexi-
cal items given the aligned predicates (defined in
Section 3.2; the likelihood of the alignment given
the dependency tree and formula (defined in equa-
tion 2), and the probability of the dependency tree
given the formula, which is uniform.
Equation 4 takes a sum across alignments, but
most of the probability mass of p(s|a
t
(f)) will
be concentrated on alignments in which predicates
cover words that gloss them. Thus, we can apply
an approximation,
p(s, t|f) ?
N
?
a
t
(f)
p(s|a
t
(f))p(a
t
(f)|t, f)p(t|f),
(5)
in which we draw N samples in which predicates
are aligned to their glosses whenever possible.
Similarly, Equation 2 quantifies the likelihood
of an alignment only to a constant of proportional-
ity; again, a sum over possible alignments is nec-
essary. We do not expect the prior on alignments
to be strongly peaked like the sentence likelihood,
so we approximate the normalization term by sam-
pling M alignments at random and extrapolating:
p(a
t
(f)|t, f) ? q(a
t
(f); t)
=
q(a
t
(f); t)
?
a
?
t
(f)
q(a
?
t
(f); t)
?
#|a
?
t
(f)|
M
q(a
t
(f); t)
?
M
a
?
t
(f)
q(a
?
t
(f); t)
,
where q(a
t
(f); t) = exp{?score(a
t
(f); t)}, de-
fined in Equation 2. In our experiments, we set N
to at most 10, and M = 20. Drawing larger num-
bers of samples had no discernible effect on sys-
tem output.
962
4.1.1 Generating new formulae
Chinese Restaurant Process sampling requires the
generation of new candidate formulae at each re-
sampling stage. To generate a new formula, we
first sample the number of literals. As described
in the generative story (Section 3), the number
of literals is drawn from a Poisson distribution
with parameter ?. We treat ? as unknown and
marginalize, using the Gamma hyperprior G(u, v).
Due to Poisson-Gamma conjugacy, this marginal-
ization can be performed analytically, yielding
a Negative-Binomial distribution with parameters
?u+
?
i
#|f
i
|, (1+K+v)
?1
?, where
?
i
#|f
i
| is
the sum of the number of literals in each formula,
and K is the number of formulae which generate
at least one sentence. In this sense, the hyperpriors
u and v act as pseudo counts. We set u = 3, v = 1,
reflecting a weak prior expectation of three literals
per predicate.
After drawing the size of the formula, the predi-
cates are selected from a uniform random distribu-
tion. Finally, the terms are assigned: at each slot,
we reuse a previous term with probability 0.5, un-
less none is available; otherwise a new term is gen-
erated.
4.2 Proposing changes to formulae
The assignment resampling procedure has the
ability to generate new formulae, thus exploring
the space of relational features. However, to ex-
plore this space more rapidly, we introduce four
Metropolis-Hastings moves that modify existing
formulae (Gilks, 1995): adding a literal, deleting
a literal, substituting a literal, and rearranging the
terms of the formula. For each proposed move, we
recompute the joint likelihood of the formula and
all aligned sentences. The move is stochastically
accepted based on the ratio of the joint likelihoods
of the new and old configurations, multiplied by a
Hastings correction.
The joint likelihood with respect to formula f
is computed as p(s, t, f) = p(f)
?
i
p(s
i
, t
i
|f).
The prior on f considers only the number of liter-
als, using a Negative-Binomial distribution as de-
scribed in section 4.1.1. The likelihood p(s
i
, t
i
|f)
is given in equation 4. The Hastings correction is
p?(f
?
? f)/p?(f ? f
?
), with p?(f ? f
?
) indicat-
ing the probability of proposing a move from f
to f
?
,and p?(f
?
? f) indicating the probability of
proposing the reverse move. The Hastings correc-
tions depend on the arity of the predicates being
added and removed; the derivation is straightfor-
ward but tedious. We plan to release a technical
report with complete details.
4.3 Summary of inference
The final inference procedure iterates between
Gibbs sampling of assignments of formulae to
sentences, and manipulating the formulae through
Metropolis-Hastings moves. A full iteration com-
prises proposing a move to each formula, and then
using Gibbs sampling to reconsider all assign-
ments. If a formula no longer has any sentences
assigned to it, then it is dropped from the active
set, and can no longer be selected in Gibbs sam-
pling ? this is standard in the Chinese Restaurant
Process.
Five separate Markov chains are maintained in
parallel. To allow the sampling procedure to con-
verge to a stationary distribution, each chain be-
gins with 100 iterations of ?burn-in? sampling,
without storing the output. At this point, we per-
form another 100 iterations, storing the state at the
end of each iteration.
2
All formulae are ranked ac-
cording to the cumulative number of sentences to
which they are assigned (across all five Markov
chains), aggregating the counts for multiple in-
stances of identical formulae. This yields a ranked
list of formulae which will be used in our frame-
work as features for relational learning.
5 Evaluation
Our experimental setup is designed to evaluate the
quality of the semantic abstraction performed by
our model. The logical formulae obtained by our
system are applied as features for relational learn-
ing of the rules of the game of Freecell solitaire.
We investigate whether these features enable bet-
ter generalization given varying number of train-
ing examples of Freecell game states. We also
quantify the specific role of syntax, lexical choice,
and feature expressivity in learning performance.
This section describes the details of this evalua-
tion.
5.1 Relational Learning
We perform relational learning using Inductive
Logic Programming (ILP), which constructs gen-
eralized rules by assembling smaller logical for-
mulae to explain observed propositional exam-
ples (Muggleton, 1995). The lowest level formu-
lae consist of basic sensors that describe the en-
vironment. ILP?s expressivity enables it to build
complex conjunctions of these building blocks,
but at the cost of tractability. Our evaluation asks
whether the logical formulae abstracted from text
2
Sampling for more iterations was not found to affect per-
formance on development data, and the model likelihood ap-
peared stationary after 100 iterations.
963
Predicate Glosses
card(x) card
tableau(x) column, tableau
freecell(x) freecell, cell
homecell(x) foundation, cell, homecell
value(x,y) ace, king, rank, 8, 3, 7, lowest,
highest
successor(x,y) higher, sequence, sequential
color(x,y) black, red, color
suit(x,y) suit, club, diamond, spade,
heart
on(x,y) onto
top(x,y) bottom, available, top
empty(x) empty
Table 1: Predicates in the Freecell world model,
with natural language glosses obtained from the
development set text.
can transform the representation to facilitate learn-
ing. We compare against both the sensor-level rep-
resentation as well as richer representations that do
not benefit from the full power of our model?s se-
mantic analysis.
The ALEPH
3
ILP system, which is primarily
based on PROGOL (Muggleton, 1995), was used
to induce the rules of game. The search parame-
ters remained constant for all experiments.
5.2 Resources
There are four types of resources required to work
in the reading-to-learn setting: a world model, in-
structional text, a small set of glosses that map
from text to elements of the world model, and la-
beled examples of correct and incorrect actions
in the world. In our experiments, we consider
the domain of Freecell solitaire, a popular card
game (Morehead and Mott-Smith, 1983) in which
cards are moved between various types of loca-
tions, depending on their suit and rank. We now
describe the resources for the Freecell domain in
more detail.
WorldModel Freecell solitaire can be described
formally using first order logic; we consider a
slightly modified version of the representation
from the Planning Domain Definition Language
(PDDL), which is used in automatic game-playing
competitions. Specifically, there are 87 constants:
52 cards, 16 locations, 13 values, four suits, and
two colors. These constants are combined with a
fixed set of 11 predicates, listed in Table 1.
Instructional Text Our approach relies on text
that describes how to operate in the Freecell soli-
taire domain. A total of five instruction sets were
3
Freely available from http://www.comlab.ox.
ac.uk/activities/machinelearning/Aleph/
obtained from the Internet. Due to the popular-
ity of the Microsoft implementation of Freecell,
instructions often contain information specific to
playing Freecell on a computer. We manually re-
moved sentences which did not focus on the card
aspects of Freecell (e.g., how to set up the board
and information regarding where to click to move
cards). In order to use our semantic abstraction
model, the instructions were part-of-speech tagged
with the Stanford POS Tagger (Toutanova and
Manning, 2000) and dependency parses were ob-
tained using Malt (Nivre, 2006).
Glosses Our reading to learn setting requires a
small set of glosses, which are surface forms com-
monly used to represent predicates from the world
model. We envision an application scenario in
which a designer manually specifies a few glosses
for each predicate. However, for the purposes of
evaluation, it would be unprincipled for the exper-
imenters to handcraft the ideal set of glosses. In-
stead, we gathered a development set of text and
annotated the lexical mentions of the world model
predicates in text. This annotation is used to ob-
tain glosses to apply to the evaluation text. This
approximates a scenario in which the designer has
a reasonable idea of how the domain will be de-
scribed in text, but no prior knowledge of the spe-
cific details of the text instructions. Our exper-
iments used glosses that occurred two or more
times in the instructions: this yields a total of 32
glosses for 11 predicates, as shown in Table 1.
Evaluation game data Ultimately, the seman-
tic abstraction obtained from the text is applied
to learning on labeled examples of correct and
incorrect actions in the world model. For evalu-
ation, we automatically generated a set of move
scenarios: game states with one positive example
(a legal move) and one negative example (an ille-
gal move). To avoid bias in the data we generate
an equal number of move scenarios from each of
three types: moves to the freecells, homecells, and
tableaux. For our experiments we vary the number
of move scenarios in the training set; the develop-
ment and test sets consist of 900 and 1500 move
scenarios respectively.
5.3 Evaluation Settings
We compare four different feature sets, which
will be provided to the ALEPH ILP learner. All
feature sets include the sensor-level predicates
shown in Table 1. The FULL-MODEL feature
set alo includes the top logical formulae ob-
tained in our model?s semantic abstract (see Sec-
964
tion 4.3). The NO-SYNTAX feature set is obtained
from a variant of our model in which the in-
fluence of syntax is removed by setting parame-
ters ?, ? = 0. The SENSORS-ONLY feature set
uses only the sensor-level predicates. Finally, the
RELATIONAL-RANDOM feature set is constructed
by replacing each feature in the FULL-MODEL set
with a randomly generated relational feature of
identical expressivity (each predicate is replaced
by a randomly chosen alternative with identical
arity; terms are also assigned randomly). This en-
sures that any performance gains obtained by our
model were not due merely to the greater expres-
sivity of its relational features. The number of fea-
tures included in each scenario is tuned on a de-
velopment set of test examples.
The performance metric assesses the ability
of the ILP learner to classify proposed Freecell
moves as legal or illegal. As the evaluation set
contains an equal number of positive and negative
examples, accuracy is the appropriate metric. The
training scenarios are randomly generated; we re-
peat each run 50 times and average our results. For
the RELATIONAL-RANDOM feature set ? in which
predicates and terms are chosen randomly ? we
also regenerate the formulae per run.
6 Results
Table 2 shows a comparison of the results
using the setup described above. Our FULL-
MODEL achieves the best performance at ev-
ery training set size, consistently outperforming
the SENSORS-ONLY representation by an abso-
lute difference of three to four percent. This
demonstrates the semantic abstract obtained by
our model does indeed facilitate machine learning
in this domain.
RELATIONAL-RANDOM provides a baseline of
relational features with equal expressivity to those
chosen by our model, but with the predicates and
terms selected randomly. We consistently outper-
form this baseline, demonstrate that the improve-
ment obtained over the sensors only representation
is not due merely to the added expressivity of our
features.
The third row compares against NO-SYNTAX,
a crippled version of our model that incorpo-
rates lexical features but not the syntactic struc-
ture. The results are stronger than the SENSORS-
ONLY and RELATIONAL-RANDOM baselines, but
still weaker than our full system. This demon-
strates the syntactic features incorporated by our
model result in better semantic representations of
the underlying text.
Features Number of training scenarios
15 30 60 120
SENSORS-ONLY 79.12 88.07 92.77 93.73
RELATIONAL-RANDOM 82.72 89.14 93.08 94.17
NO-SYNTAX 80.98 89.79 94.11 97.04
FULL-MODEL 82.89 91.00 95.23 97.45
Table 2: Results as number of training examples
varied. Each value represents the accuracy of the
induced rules obtained with the given feature set.
card(x
1
) ? tableau(x
2
)
card(x
1
) ? freecell(x
2
)
homecell(x
1
) ? value(x
2
,x
3
)
empty(x
1
) ? freecell(x
1
)
card(x
1
) ? top(x
1
,x
2
)
card(x
1
) ? homecell(x
2
)
freecell(x
1
) ? homecell(x
2
)
card(x
1
) ? tableau(x
1
)
card(x
1
) ? top(x
2
,x
1
)
homecell(x
1
)
card(x
1
) ? homecell(x
1
)
color(x
1
,x
2
) ? value(x
3
,x
4
)
suit(x
1
,x
2
) ? value(x
3
,x
4
)
value(x
1
,x
2
) ? value(x
3
,x
4
)
homecell(x
1
) ? successor(x
2
,x
3
)
Figure 3: The top 15 features recovered by the se-
mantic abstraction of our full model.
Figure 3 shows the top 15 formulae recovered
by the full model running on the evaluation text.
Features such as empty(x
1
) ? freecell(x
1
)
are useful because they reuse variables to ensure
that objects have key properties ? in this case, en-
suring that a freecell is empty. Other features, such
as homecell(x
1
) ? value(x
2
, x
3
), help to fo-
cus the search on useful conjunctions of predicates
(in Freecell, the legality of playing a card on a
homecell depends on the value of the card). Note
that three of these 15 formulae are trivially use-
less, in that they are always false: e.g., card(x
1
)
? tableau(x
1
). This illustrates the importance
of term assignment in obtaining useful features
for learning. In the NO-SYNTAX system, which
ignores the relationship between term assignment
and syntactic structure, eight of the top 15 formu-
lae were trivially useless due to term incompatibil-
ity.
7 Related Work
This paper draws on recent literature on extract-
ing logical forms from surface text (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005; Downey
et al, 2005; Liang et al, 2009), interpreting lan-
guage in the context of a domain (Chen and
Mooney, 2008), and using an actionable domain
to guide text interpretation (Branavan et al, 2009).
We differentiate our research in several dimen-
sions:
965
Language Interpretation Instructional text de-
scribes generalized statements about entities in
the domain and the way they interact, thus the
text does not correspond directly to concrete sen-
sory inputs in the world (i.e., a specific world
state). Our interpretation captures these general-
izations as first-order logic statements that can be
evaluated given a specific state. This contrasts to
previous work which interprets ?directions? and
thus assumes a direct correspondence between text
and world state (Branavan et al, 2009; Chen and
Mooney, 2008).
Supervision Our work avoids supervision in the
form of labeled examples, using only a minimal
set of natural language glosses per predicate. Pre-
vious work also considered the supervision signal
obtained by interpreting natural language in the
context of a formal domain. Branavan et al (2009)
use feedback from a world model as a supervi-
sion signal. Chen and Mooney (2008) use tempo-
ral alignment of text and grounded descriptions of
the world state. In these approaches, concrete do-
main entities are grounded in language interpreta-
tion, and therefore require only a propositional se-
mantic representation. Previous approaches for in-
terpreting generalized natural language statements
are trained from labeled examples (Zettlemoyer
and Collins, 2005; Lu et al, 2008).
Level of analysis We aim for an abstractive
semantic summary across multiple documents,
whereas other approaches attempt to produce log-
ical forms for individual sentences (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005). We
avoid the requirement that each sentence have a
meaningful interpretation within the domain, al-
lowing us to handle relatively unstructured text.
Evaluation We do not evaluate the representa-
tions obtained by our model; rather we assess
whether these representations improve learning
performance. This is similar to work on Geo-
Query (Wong and Mooney, 2007; Ge and Mooney,
2005), and also to recent work on following step-
by-step directions (Branavan et al, 2009). While
these evaluations are performed on the basis of in-
dividual sentences, actions, or system responses,
we evaluate the holistic semantic analysis obtained
by our system.
Model We treat surface text as generated from a
latent semantic description. Lu et al (2008) ap-
ply a generative model, but require a complete
derivation from semantics to the lexical represen-
tation, while we favor a more flexible semantic
analysis that can be learned without annotation
and applied to noisy text. More similar is the work
of Liang et al (2009), which models the gener-
ation of semantically-relevant fields using lexical
and discourse features. Our approach differs by
accounting for syntax, which enables a more ex-
pressive semantic representation that includes un-
grounded variables.
Relational learning The output of our semantic
analysis is applied to learning in a structured rela-
tional space, using ILP. A key difficulty with ILP
is that the increased expressivity dramatically ex-
pands the hypothesis space, and it is widely agreed
that some learning bias is required for ILP to be
tractable (N?edellec et al, 1996; Cumby and Roth,
2003). Our work can be viewed as a new method
for acquiring such bias from text; moreover, our
approach is not specialized for ILP and may be
used to transform the feature space in other forms
of relational learning as well (Roth and Yih, 2001;
Cumby and Roth, 2003; Richardson and Domin-
gos, 2006).
8 Conclusion
This paper demonstrates a new setting for seman-
tic analysis, which we term reading to learn. We
handle text which describes the world in gen-
eral terms rather than refereing to concrete enti-
ties in the domain. We obtain a semantic abstract
of multiple documents, using a novel, minimally-
supervised generative model that accounts for both
syntax and lexical choice. The semantic abstract
is represented as a set of predicate logic formu-
lae, which are applied as higher-order features for
learning. We demonstrate that these features im-
prove learning performance, and that both the lex-
ical and syntactic aspects of our model yield sub-
stantial contributions.
In the current setup, we produce an ?overgener-
ated? semantic representation comprised of useful
features for learning but also some false positives.
Learning in our system can be seen as the process
of pruning this representation by selecting useful
formulae based on interaction with the training
data. In the future we hope to explore ways to in-
terleave semantic analysis with exploration of the
learning domain, by using the environment as a
supervision signal for linguistic analysis.
Acknowledgments We thank Gerald DeJong,
Julia Hockenmaier, Alex Klementiev and the
anonymous reviewers for their helpful feedback.
This work is supported by DARPA funding under
the Bootstrap Learning Program and the Beckman
Institute Postdoctoral Fellowship.
966
References
Aldous, David J. 1985. Exchangeability and re-
lated topics. Lecture Notes in Math 1117:1?198.
Branavan, S. R. K., Harr Chen, Luke Zettle-
moyer, and Regina Barzilay. 2009. Reinforce-
ment learning for mapping instructions to ac-
tions. In Proceedings of the Joint Conference
of the Association for Computational Linguis-
tics and International Joint Conference on Nat-
ural Language Processing Processing (ACL-
IJCNLP 2009). Singapore.
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: A test of grounded lan-
guage acquisition. In Proceedings of 25th In-
ternational Conference on Machine Learning
(ICML 2008). Helsinki, Finland, pages 128?
135.
Cumby, Chad and Dan Roth. 2003. On kernel
methods for relational learning. In Proceed-
ings of the Twentieth International Conference
(ICML 2003). Washington, DC, pages 107?114.
Downey, Doug, Oren Etzioni, and Stephen Soder-
land. 2005. A probabilistic model of redun-
dancy in information extraction. In Proceedings
of the International Joint Conference on Arti-
ficial Intelligence (IJCAI 2005). pages 1034?
1041.
Ferguson, Thomas S. 1973. A bayesian analysis
of some nonparametric problems. The Annals
of Statistics 1(2):209?230.
Ge, Ruifang and Raymond J. Mooney. 2005. A
statistical semantic parser that integrates syn-
tax and semantics. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL-2005). Ann Arbor,
MI, pages 128?135.
Gilks, Walter R. 1995. Markov Chain Monte
Carlo in Practice. Chapman & Hall/CRC.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics
(COLING-ACL 2006). Sydney, Australia, pages
673?680.
Liang, Percy, Michael Jordan, and Dan Klein.
2009. Learning semantic correspondences with
less supervision. In Proceedings of the Joint
Conference of the Association for Computa-
tional Linguistics and International Joint Con-
ference on Natural Language Processing Pro-
cessing (ACL-IJCNLP 2009). Singapore.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning representa-
tions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2008).
Honolulu, Hawaii, pages 783?792.
Morehead, Albert H. and Geoffrey Mott-Smith.
1983. The Complete Book of Solitaire and Pa-
tience Games. Bantam.
Muggleton, Stephen. 1995. Inverse entailment and
progol. New Generation Computing Journal
13:245?286.
N?edellec, C., C. Rouveirol, H. Ad?e, F. Bergadano,
and B. Tausend. 1996. Declarative bias in ILP.
In L. De Raedt, editor, Advances in Inductive
Logic Programming, IOS Press, pages 82?103.
Nivre, Joakim. 2006. Inductive dependency pars-
ing. Springer.
Richardson, Matthew and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107?136.
Roth, Dan and Wen-tau Yih. 2001. Relational
learning via propositional algorithms: An infor-
mation extraction case study. In Proceedings of
the International Joint Conference on Artificial
Intelligence (IJCAI 2001). pages 1257?1263.
Sethuraman, Jayaram. 1994. A constructive def-
inition of dirichlet priors. Statistica Sinica
4(2):639?650.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used
in a maximum entropy part-of-speech tagger.
In Proceedings of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora
(EMNLP/VLC-2000). pages 63?70.
Wong, Yuk Wah and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic
parsing with lambda calculus. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 2007). Prague,
Czech Republic, pages 128?135.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proceedings of the 21st
Conference on Uncertainty in Artificial Intelli-
gence (UAI 2005). pages 658?666.
967
Proceedings of NAACL HLT 2009: Short Papers, pages 5?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Revisiting Optimal Decoding for Machine Translation IBM Model 4
Sebastian Riedel?? James Clarke?
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801
?sebastian.riedel@gmail.com ?clarkeje@gmail.com
Abstract
This paper revisits optimal decoding for statis-
tical machine translation using IBM Model 4.
We show that exact/optimal inference using
Integer Linear Programming is more practical
than previously suggested when used in con-
junction with the Cutting-Plane Algorithm. In
our experiments we see that exact inference
can provide a gain of up to one BLEU point
for sentences of length up to 30 tokens.
1 Introduction
Statistical machine translation (MT) systems typ-
ically contain three essential components: (1) a
model, specifying how the process of translation oc-
curs; (2) learning regime, dictating the estimation of
model?s parameters; (3) decoding algorithm which
provides the most likely translation of an input sen-
tence given a model and its parameters.
The search space in statistical machine transla-
tion is vast which can make it computationally pro-
hibitively to perform exact/optimal decoding (also
known as search and MAP inference) especially
since dynamic programming methods (such as the
Viterbi algorithm) are typically not applicable. Thus
greedy or heuristic beam-based methods have been
prominent (Koehn et al, 2007) due to their effi-
ciency. However, the efficiency of such methods
have two drawbacks: (1) they are approximate and
give no bounds as to how far their solution is
away from the true optimum; (2) it can be difficult
to incorporate additional generic global constraints
into the search. The first point may be especially
problematic from a research perspective as without
bounds on the solutions it is difficult to determine
whether the model or the search algorithm requires
improvement for better translations.
Similar problems exist more widely throughout
natural language processing where greedy based
methods and heuristic beam search have been used
in lieu of exact methods. However, recently there has
been an increasing interest in using Integer Linear
Programming (ILP) as a means to find MAP solu-
tions. ILP overcomes the two drawbacks mentioned
above as it is guaranteed to be exact, and has the
ability to easily enforce global constraints through
additional linear constraints. However, efficiency is
usually sacrificed for these benefits.
Integer Linear Programming has previously been
used to perform exact decoding for MT using IBM
Model 4 and a bigram language model. Germann
et al (2004) view the translation process akin to the
travelling salesman problem; however, from their re-
ported results it is clear that using ILP naively for de-
coding does not scale up beyond short sentences (of
eight tokens). This is due to the exponential num-
ber of constraints required to represent the decod-
ing problem as an ILP program. However, work in
dependency parsing (Riedel and Clarke, 2006) has
demonstrated that it is possible to use ILP to perform
efficient inference for very large programs when
used in an incremental manner. This raises the ques-
tion as to whether incremental (or Cutting-Plane)
ILP can also be used to decode IBM Model 4 on
real world sentences.
In this work we show that it is possible. Decod-
ing IBM Model 4 (in combination with a bigram
language model) using Cutting-Plane ILP scales to
much longer sentences. This affords us the oppor-
tunity to finally analyse the performance of IBM
Model 4 and the performance of its state-of-the-
5
art ReWrite decoder. We show that using exact in-
ference provides an increase of up to one BLEU
point on two language pairs (French-English and
German-English) in comparison to decoding using
the ReWrite decoder. Thus the ReWrite decoder per-
forms respectably but can be improved slightly, al-
beit at the cost of efficiency.
Although the community has generally moved
away from word-based models, we believe that dis-
playing optimal decoding in IBM Model 4 lays the
foundations of future work. It is the first step in pro-
viding a method for researchers to gain greater in-
sight into their translation models by mapping the
decoding problem of other models into an ILP rep-
resentation. ILP decoding will also allow the incor-
poration of global linguistic constraints in a manner
similar to work in other areas of natural language
processing.
The remainder of this paper is organised as fol-
lows: Sections 2 and 3 briefly recap IBM Model 4
and its ILP formulation. Section 4 reviews the
Cutting-Plane Algorithm. Section 5 outlines our ex-
periments and we end the paper with conclusions
and a discussion of open questions for the commu-
nity.
2 IBM Model 4
In this paper we focus on the translation model de-
fined by IBM Model 4 (Brown et al, 1993). Transla-
tion using IBM Model 4 is performed by treating the
translation process a noisy-channel model where the
probability of the English sentence given a French
sentence is, P (e|f) = P (f |e) ? P (e), where P (e) is
a language model of English. IBM Model 4 defines
P (f |e) and models the translation process as a gen-
erative process of how a sequence of target words
(in our case French or German) is generated from a
sequence of source words (English).
The generative story is as follows. Imagine we
have an English sentence, e = e1, . . . , el and along
with a NULL word (eo) and French sentence, f =
f1, . . . , fm. First a fertility is drawn for each English
word (including the NULL symbol). Then, for each
ei we then independently draws a number of French
words equal to ei?s fertility. Finally we process the
English source tokens in sequence to determine the
positions of their generated French target words. We
refer the reader to Brown et al (1993) for full details.
3 Integer Linear Programming
Formulation
Given a trained IBM Model 4 and a French sentence
f we need to find the English sentence e and align-
ment a with maximal p (a, e|f) w p (e) ? p (a, f |e).1
Germann et al (2004) present an ILP formula-
tion of this problem. In this section we will give a
very high-level description of the formulation.2 For
brevity we refer the reader to the original work for
more details.
In the formulation of Germann et al (2004) an
English translation is represented as the journey of
a travelling salesman that visits one English token
(hotel) per French token (city). Here the English to-
ken serves as the translation of the French one. A
set of binary variables denote whether or not cer-
tain English token pairs are directly connected in
this journey. A set of constraints guarantee that for
each French token exactly one English token is vis-
ited. The formulation also contains an exponential
number of constraints which forbid the possible cy-
cles the variables can represent. It is this set of con-
straints that renders MT decoding with ILP difficult.
4 Cutting Plane Algorithm
The ILP program above has an exponential number
of (cycle) constraints. Hence, simply passing the ILP
to an off-the-shelf ILP solver is not practical for all
but the smallest sentences. For this reason Germann
et al (2004) only consider sentences of up to eight
tokens. However, recent work (Riedel and Clarke,
2006) has shown that even exponentially large de-
coding problems may be solved efficiently using ILP
solvers if a Cutting-Plane Algorithm (Dantzig et al,
1954) is used.3
A Cutting-Plane Algorithm starts with a subset of
the complete set of constraints. In our case this sub-
set contains all but the (exponentially many) cycle
constraints. The corresponding ILP is solved by a
1Note that in theory we should be maximizing p (e|f). How-
ever, this requires summation over all possible alignments and
hence the problem is usually simplified as described here.
2Note that our actual formulation differs slightly from the
original work because we use a first order modelling language
that imposed certain restrictions on the type of constraints al-
lowed.
3It is worth mentioning that Cutting Plane Algorithms have
been successfully applied for solving very large instances of the
Travelling Salesman Problem, a problem essentially equivalent
to the decoding in IBM Model 4.
6
standard ILP solver, and the solution is inspected
for cycles. If it contains no cycles, we have found
the true optimum: the solution with highest score
that does not violate any constraints. If the solution
does contain cycles, the corresponding constraints
are added to the ILP which is in turn solved again.
This process is continued until no more cycles can
be found.
5 Evaluation
In this section we describe our experimental setup
and results.
5.1 Experimental setup
Our experimental setup is designed to answer sev-
eral questions: (1) Is exact inference in IBM Model 4
possible for sentences of moderate length? (2) How
fast is exact inference using Cutting-Plane ILP?
(3) How well does the ReWrite Decoder4 perform
in terms of finding the optimal solution? (4) Does
optimal decoding produce better translations?
In order to answer these questions we obtain
a trained IBM Model 4 for French-English and
German-English on Europarl v3 using GIZA++. A
bigram language model with Witten-Bell smooth-
ing was estimated from the corpus using the CMU-
Cambridge Language Modeling Toolkit.
For exact decoding we use the two models to gen-
erate ILP programs for sentences of length up to
(and including) 30 tokens for French and 25 tokens
for German.5 We filter translation candidates follow-
ing Germann et al (2004) by using only the top ten
translations for each word6 and a list of zero fertil-
ity words.7 This resulted in 1101 French and 1062
German sentences for testing purposes. The ILP pro-
grams were then solved using the method described
in Section 3. This was repeated using the ReWrite
Decoder using the same models.
5.2 Results
The Cutting-Plane ILP decoder (which we will refer
to as ILP decoder) produced output for 986 French
sentences and 954 German sentences. From this we
can conclude that it is possible to solve 90% of our
4Available at http://www.isi.edu/
licensed-sw/rewrite-decoder/
5These limits were imposed to ensure the Python script gen-
erating the ILP programs did not run out of memory.
6Based on t(e|f).
7Extracted using the rules in the filter script
rewrite.mkZeroFert.perl
sentences exactly using ILP. For the remaining 115
and 108 sentences we did not produce a solution due
to: (1) the solver not completing within 30 minutes,
or (2) the solver running out of memory.8
Table 1 shows a comparison of the results, bro-
ken down by input sentence length, obtained on the
986 French and 954 German sentences using the ILP
and ReWrite decoders. First we turn our attention to
the solve times obtained using ILP (for the sentences
for which the solution was found within 30 min-
utes). The table shows that the average solve time
is under one minute per sentence. As we increase
the sentence length we see the solve time increases,
however, we never see an order of magnitude in-
crease between brackets as witnessed by Germann
et al (2004) thus optimal decoding is more practi-
cal than previously suggested. The average number
of Cutting-Plane iterations required was 4.0 and 5.6
iterations for French and German respectively with
longer sentences requiring more on average.
We next examine the performance of the two de-
coders. Following Germann et al (2004) we define
the ReWrite decoder as finding the optimal solution
if the English sentence is the same as that produced
by the ILP decoder. Table 1 shows that the ReWrite
decoder finds the optimal solution 40.1% of the time
for French and 29.1% for German. We also see the
ReWrite decoder is less likely to find the optimal so-
lution of longer sentences. We now look at the model
scores more closely. The average log model error
per token shows that the ReWrite decoder?s error is
proportional to sentence length and on average the
ReWrite decoder is 2.2% away from the optimal so-
lution in log space and 60.6% in probability space9
for French, and 4.7% and 60.9% for German.
Performing exact decoding increases the BLEU
score by 0.97 points on the French-English data set
and 0.61 points on the German-English data set with
similar performance increases observed for all sen-
tence lengths.
6 Discussion and Conclusions
In this paper we have demonstrated that optimal de-
coding of IBM Model 4 is more practical than previ-
ously suggested. Our results and analysis show that
8All experiments were run on 3.0GHz Intel Core 2 Duo with
4GB RAM using a single core.
9These high error rates are an artefact of the extremely small
probabilities involved.
7
Len # Solve Stats BLEU%Eq Err Time ReW ILP Diff
1?5 21 85.7 15.0 0.7 56.5 56.2 -0.32
6?10 121 64.5 7.8 1.4 26.1 28.0 1.90
11?15 118 47.9 5.9 2.7 22.9 23.7 0.85
16?20 238 37.4 6.3 13.9 20.4 20.8 0.41
21?25 266 30.5 6.6 70.1 20.9 22.5 1.62
26?30 152 25.7 5.3 162.6 20.9 22.3 1.38
1?30 986 40.1 6.5 48.1 21.7 22.6 0.97
(a) French-English
Len # Solve Stats BLEU%Eq Err Time ReW ILP Diff
1?5 31 83.9 27.4 0.8 40.7 41.1 0.44
6?10 175 51.4 19.7 1.7 19.2 20.9 1.72
11?15 242 30.6 17.4 5.5 16.0 16.7 0.72
16?20 257 19.1 14.4 23.9 15.8 15.9 0.16
21?25 249 15.7 14.0 173.4 15.3 15.9 0.61
1?25 954 29.1 16.4 53.5 16.1 16.7 0.61
(b) German-English
Table 1: Results on the two corpora. Len: range of sentence lengths; #: number of sentences in this range; %Eq: percentage of
times ILP decoder returned same English sentence; Err: average difference between decoder scores per token (?10?2) in log space;
Time: the average solve time per sentence of ILP decoder in seconds; BLEU ReW, BLEU ILP, BLEU Diff: the BLEU scores of the
output and difference between BLEU scores.
exact decoding has a practical purpose. It has al-
lowed us to investigate and validate the performance
of the ReWrite decoder through comparison of the
outputs and model scores from the two decoders.
Exact inference also provides an improvement in
translation quality as measured by BLEU score.
During the course of this research we have en-
countered numerous challenges that were not appar-
ent at the start. These challenges raise some interest-
ing research questions and practical issues one must
consider when embarking on exact inference using
ILP. The first issue is that the generation of the ILP
programs can take a long time. This leads us to won-
der if there may be a way to provide tighter integra-
tion of program generation and solving. Such an in-
tegration would avoid the need to query the models
in advance for all possible model components the
solver may require.
Related to this issue is how to tackle the incor-
poration of higher order language models. Currently
we use our bigram language model in a brute-force
manner: in order to generate the ILP we evaluate
the probability of all possible bigrams of English
candidate tokens in advance. It seems clear that
with higher order models this process will become
prohibitively expensive. Moreover, even if the ILP
could be generated efficiently, they will obviously be
larger and harder to solve than our current ILPs. One
possible solution may be the use of so-called de-
layed column generation strategies which incremen-
tally add parts of the objective function (and hence
the language model), but only when required by the
ILP solver.10
10Note that delayed column generation is dual to performing
cutting planes.
The use of ILP in other NLP tasks has provided
a principled and declarative manner to incorporate
global linguistic constraints on the system output.
This work lays the foundations for incorporating
similar global constraints for translation. We are cur-
rently investigating linguistic constraints for IBM
Model 4 and other word-based models in general. A
further extension is to reformulate higher-level MT
models (phrase- and syntax-based) within the ILP
framework. These representations could be more de-
sirable from a linguistic constraint perspective as the
formulation of constraints may be more intuitive.
Acknowledgements
We would like to thank Ulrich Germann and Daniel
Marcu for their help with the ISI ReWrite Decoder.
References
Brown, Peter F., Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics of sta-
tistical machine translation: parameter estimation. Compu-
tational Linguistics 19(2):263?311.
Dantzig, George B., Ray Fulkerson, and Selmer M. Johnson.
1954. Solution of a large-scale traveling salesman problem.
Operations Research 2:393?410.
Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2004. Fast and optimal decoding for ma-
chine translation. Artificial Intelligence 154(1-2):127?143.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In ACL 2009 Demos. Prague, Czech Republic, pages
177?180.
Riedel, Sebastian and James Clarke. 2006. Incremental integer
linear programming for non-projective dependency parsing.
In EMNLP 2006. pages 129?137.
8
Discourse Constraints for
Document Compression
James Clarke?
University of Illinois at
Urbana-Champaign
Mirella Lapata??
University of Edinburgh
Sentence compression holds promise for many applications ranging from summarization to
subtitle generation. The task is typically performed on isolated sentences without taking the
surrounding context into account, even though most applications would operate over entire
documents. In this article we present a discourse-informed model which is capable of producing
document compressions that are coherent and informative. Our model is inspired by theories of
local coherence and formulated within the framework of integer linear programming. Experimen-
tal results show significant improvements over a state-of-the-art discourse agnostic approach.
1. Introduction
Recent years have witnessed increasing interest in sentence compression. The task
encompasses automatic methods for shortening sentences with minimal information
loss while preserving their grammaticality. The popularity of sentence compression is
largely due to its relevance for applications. Summarization is a case in point here. Most
summarizers to date aim to produce informative summaries at a given compression
rate. If we can have a compression component that reduces sentences to a minimal
length and still retains the most important content, then we should be able to pack more
information content into a fixed size summary. In other words, sentence compression
would allow summarizers to increase the overall amount of information extracted
without increasing the summary length (Lin 2003; Zajic et al 2007). It could also be
used as a post-processing step in order to render summaries more coherent and less
repetitive (Mani, Gates, and Bloedorn 1999).
Beyond summarization, a sentence compression module could be used to display
text on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aid
for the blind (Grefenstette 1998). Sentence compression could also benefit information
retrieval by eliminating extraneous information from the documents indexed by the
? Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N Goodwin Ave,
Urbana, IL 61801, USA. E-mail: clarkeje@illinois.edu.
?? School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK.
E-mail: mlap@inf.ed.ac.uk.
Submission received: 10 September 2008; revised submission received: 27 October 2009; accepted for
publication: 6 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
retrieval engine. This way it would be possible to store less information in the index
without dramatically affecting retrieval performance (Olivers and Dolan 1999).
In theory, sentence compression may involve several rewrite operations such as
deletion, substitution, insertion, and word reordering. In practice, however, the task
is commonly defined as a word deletion problem: Given an input sentence of words
x = x1, x2, . . . , xn, the aim is to produce a compression by removing any subset of these
words (Knight and Marcu 2002). Many sentence compression models aim to learn dele-
tion rules from a parsed parallel corpus of source sentences and their target compres-
sions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007;
Cohn and Lapata 2009). For example, Knight and Marcu (2002) learn a synchronous
context-free grammar (Aho and Ullman 1969) from such a corpus. The grammar rules
have weights (essentially probabilities estimated using maximum likelihood) and are
used to find the best compression from the set of all possible compressions for a given
sentence. Other approaches exploit syntactic information without making explicit use
of a parallel grammar?for example, by learning which words or constituents to delete
from a parse tree (Riezler et al 2003; Nguyen et al 2004; McDonald 2006; Clarke and
Lapata 2008).
Despite differences in formulation and training requirements (some approaches
require a parallel corpus, whereas others do not), existing models are similar in that
they compress sentences in isolation without taking their surrounding context into
account. This is in marked contrast with common practice in summarization. Pro-
fessional abstractors often rely on contextual cues while creating summaries (Endres-
Niggemeyer 1998). This is true of automatic summarization systems too, which consider
the position of a sentence in a document and how it relates to its surrounding sentences
(Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and
Moens 2002). Determining which information is important in a sentence is not merely
a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is
less likely). A variety of contextual factors can play a role, such as the discourse topic,
whether the sentence introduces new entities or events that have not been mentioned
before, or the reader?s background knowledge.
A sentence-centric view of compression is also at odds with most relevant appli-
cations which aim to create a shorter document rather than a single sentence. The
resulting document must not only be grammatical but also coherent if it is to function as
a replacement for the original. However, this cannot be guaranteed without knowledge
of how the discourse progresses from sentence to sentence. To give a simple example, a
contextually aware compression system could drop a word or phrase from the current
sentence, simply because it is not mentioned anywhere else in the document and is
therefore deemed unimportant. Or it could decide to retain it for the sake of topic
continuity.
In this article we are interested in creating a compression model that is appropriate
for both documents and sentences. Luckily, a variety of discourse theories have been
developed over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi
1995; Halliday and Hasan 1976) and have found application in summarization (Barzilay
and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation
applications (Scott and de Souza 1990; Kibble and Power 2004). In creating a context-
sensitive compression model we are faced with three important questions: (1) Which
type of discourse information is useful for compression? (2) Is it amenable to automatic
processing (there is little hope for interfacing our compression model with applications
if discourse-level cues cannot be identified robustly)? and (3) How are sentence- and
document-based information best integrated in a unified modeling framework?
412
Clarke and Lapata Discourse Constraints for Document Compression
In building our compression model we borrow insights from two popular models
of discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains
(Morris and Hirst 1991). Both approaches capture local coherence?the way adjacent
sentences bind together to form a larger discourse. They also both share the view that
discourse coherence revolves around discourse entities and the way they are intro-
duced and discussed. We first automatically augment our documents with annotations
pertaining to centering and lexical chains, which we subsequently use to inform our
compression model. The latter is an extension of the integer linear programming for-
mulation proposed by Clarke and Lapata (2008). In a nutshell, sentence compression is
modeled as an optimization problem. Given a long sentence, a compression is formed
by retaining the words that maximize a scoring function coupled with a small number
of constraints ensuring that the resulting output is grammatical. The constraints are en-
coded as linear inequalities whose solution is found using integer linear programming
(ILP; Winston and Venkataramanan 2003; Vanderbei 2001). Discourse-level information
can be straightforwardly incorporated by slightly changing the compression objective?
we now wish to compress entire documents rather than isolated sentences?and aug-
menting the constraint set with discourse-specific constraints. We use our model to
compress whole documents (rather than sentences sequentially) and evaluate whether
the resulting text is understandable and informative using a question-answering task.
We show that our method yields significant improvements over discourse agnostic
state-of-the-art compression models (McDonald 2006; Clarke and Lapata 2008).
The remainder of this article is organized as follows. Section 2 provides an overview
of related work. In Section 3 we present the ILP framework and compression model we
employ in our experiments. We introduce our discourse-related extensions in Sections 4
and 5. Section 6 discusses our experimental set-up and evaluation methodology. Our
results are presented in Section 7. Discussion of future work concludes the paper.
2. Related Work
Sentence compression has been extensively studied across different modeling para-
digms and has received both generative and discriminative formulations. Most gen-
erative approaches (Knight and Marcu 2002; Turner and Charniak 2005; Galley and
McKeown 2007) are instantiations of the noisy-channel model, whereas discriminative
formulations include decision-tree learning (Knight and Marcu 2002), maximum en-
tropy (Riezler et al 2003), support vector machines (Nguyen et al 2004), and large-
margin learning (McDonald 2006; Cohn and Lapata 2009). These models are trained
on a parallel corpus and learn either which constituents to delete or which words to
place adjacently in the compression output. Relatively few approaches dispense with
the parallel corpus and generate compressions in an unsupervised manner using either
a scoring function (Hori and Furui 2004; Clarke and Lapata 2008) or compression rules
that are approximated from a non-parallel corpus such as the Penn Treebank (Turner
and Charniak 2005).
The majority of sentence compression approaches only look at sentences in isolation
without taking into account any discourse information. However, there are two notable
exceptions. Jing (2000) uses information from the local context as evidence for and
against the removal of phrases during sentence compression. The idea here is that
words or phrases which have more links to the surrounding context are more indicative
of its topic, and thus should not be dropped. The topic is not explicitly identified;
instead the importance of each phrase is determined by the number of lexical links
within the local context. A link is created between two words if they are repetitions,
413
Computational Linguistics Volume 36, Number 3
morphologically related, or associated in WordNet (Fellbaum 1998) through a lexical
relation (e.g., hyponymy, synonymy). Links have weights?for example, repetition is
considered more important than hypernymy. Each word is assigned a context weight
based on the number of links to the local context and the importance of each relation
type. Phrases are scored by the sum of their children?s context scores. The decision to
drop a phrase is influenced by several factors, besides the local context, such as the
phrase?s grammatical role and previous evidence from a parallel corpus.
Daume? III and Marcu (2002) generalize sentence compression to document com-
pression. Given a document D = w1, w2, . . . , wn the goal is to produce a summary, S, by
dropping any subset of words from D. Their system uses the discourse structure of a
document and the syntactic structure of each of its sentences in order to decide which
words to drop. Specifically, they extend Knight and Marcu?s (2002) noisy-channel model
so that it can be applied to entire documents. In its simpler sentence compression instan-
tiation, the noisy-channel model has two components, a language model and a channel
model, both of which act on probabilistic context-free grammar (PCFG) representations.
Daume? III and Marcu define a noisy-channel model over syntax and discourse trees.
Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they represent
documents by trees whose leaves correspond to elementary discourse units (edus) and
whose nodes specify how these and larger units (e.g., multi-sentence segments) are
linked to each other by rhetorical relations (e.g., Contrast, Elaboration). Discourse units
are further characterized in terms of their text importance: nuclei denote central seg-
ments, whereas satellites denote peripheral ones. Their model therefore learns not only
which syntactic constituents to drop but also which discourse units are unimportant.
While Daume? III and Marcu (2002) present a hybrid summarizer that can simulta-
neously delete words and sentences from a document, the majority of summarization
systems to date simply select and present to the user the most important sentences in
a text (see Mani [2001] for a comprehensive overview of the methods used to achieve
this). Discourse-level information plays a prominent role here as the overall document
organization can indicate whether a sentence should be included in the summary. A
variety of approaches have focused on cohesion (Halliday and Hasan 1976) and the
way it is expressed in discourse. The term broadly describes a variety of linguistic
devices responsible for making the elements of a text appear unified or connected.
Examples include word repetition, anaphora, ellipsis, and the use of synonyms or
superordinates. The underlying assumption is that sentences connected to many other
sentences are likely to carry salient information and should therefore be included
in the summary (Sjorochod?ko 1972). In exploiting cohesion for summarization, it is
necessary to somehow represent cohesive ties. For instance, Boguraev and Kennedy
(1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad
(1997) operationalize cohesion via lexical chains?sequences of related words spanning
a topical unit (Morris and Hirst 1991). Besides repetition, they also examine semantic
relations based on synonymy, antonymy, hypernymy, and holonymy (we discuss their
approach in more detail in Section 4.1).
Other approaches characterize the document in terms of discourse structure
and rhetorical relations. Documents are commonly represented as trees (Mann and
Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al 2001)
and the position of a sentence in a tree is indicative of its importance. To give an ex-
ample, Marcu (2000) proposes a summarization algorithm based on RST. Assuming
that nuclei are more salient than satellites, the importance of sentential or clausal units
can be determined based on tree depth. Alternatively, discourse structure can be repre-
sented as a graph (Wolf and Gibson 2004) and sentence importance is determined in
414
Clarke and Lapata Discourse Constraints for Document Compression
graph-theoretic terms, by using graph connectivity measures such as in-degree or
PageRank (Brin and Page 1998). Although a great deal of research in summarization
has focused on global properties of discourse structure, there is evidence that local
coherence may also be useful without the added complexity of computing discourse
representations. (Unfortunately, discourse parsers have yet to achieve levels of perfor-
mance comparable to syntactic parsers.) Teufel and Moens (2002) identify discourse
relations on a sentence-by-sentence basis without presupposing an explicit discourse
structure. Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995)?a theory
of local discourse structure that models the interaction of referential continuity and
salience of discourse entities?Ora?san (2003) proposes a summarization algorithm that
extracts sentences with at least one entity in common. The idea here is that summaries
containing sentences referring to the same entity will be more coherent. Other work
has relied on centering not so much to create summaries but to assess whether they are
readable (Barzilay and Lapata 2008).
Our approach differs from previous sentence compression approaches in three
key respects. First, we present a compression model that is contextually aware; decisions
on whether to remove or retain a word (or phrase) are informed by its discourse prop-
erties (e.g., whether it introduces a new topic, or whether it is semantically related to the
previous sentence). Unlike Jing (2000) we explicitly identify topically important words
and assume specific representations of discourse structure. Secondly, in contrast to
Daume? III and Marcu (2002) and other summarization work, we adopt a less global
and more shallow representation of discourse based on Centering Theory and lexical
chains. One of our aims is to exploit discourse features that can be computed efficiently
and relatively cheaply. Thirdly, our compression model can be applied to isolated
sentences as well as to entire documents. We claim the latter is more in the spirit of real-
world applications where the goal is to generate a condensed and coherent text. Unlike
Daume? III and Marcu (2002) our model can delete words but not sentences, although it
could be used to compress documents of any type, even summaries.
3. The Compression Model
Our model is an extension of the approach put forward in Clarke and Lapata (2008)
where they formulate sentence compression as an optimization problem. Given a long
sentence, a compression is created by retaining the words that maximize a scoring func-
tion. The latter is essentially a language model coupled with a few constraints ensuring
that the resulting output is grammatical. The language model and the constraints are
encoded as linear inequalities whose solution is found using ILP.1
Their model is a good point of departure for studying document-based compres-
sion. As it does not require a parallel corpus, it can be ported across domains and
text genres, while delivering state-of-the-art results (see Clarke and Lapata [2008] for
details). Importantly, discourse-level information can be easily incorporated in two
ways: Firstly, by applying the compression objective to entire documents rather than
individual sentences; and secondly, by augmenting the constraint set with discourse-
related information. This is not the case for other approaches (e.g., those based on
the noisy channel model) where compression is modeled by grammar rules indicating
which constituents to delete in a syntactic context. Moreover, ILP delivers a globally
1 It is outside the scope of this article to provide an introduction to ILP. We refer the interested reader to
Winston and Venkataramanan (2003) and Vanderbei (2001) for comprehensive overviews.
415
Computational Linguistics Volume 36, Number 3
optimal solution by searching over the entire compression space2 without employing
heuristics or approximations during decoding (see Turner and Charniak [2005] and
McDonald [2006] for examples).
Besides sentence compression, the ILP modeling framework has been applied to
a wide range of natural language processing tasks demonstrating improvements over
more traditional methods. Examples include reluctant paraphrasing (Dras 1997), rela-
tion extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al 2004),
concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006),
dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and
coreference resolution (Denis and Baldridge 2007).
In the following we describe Clarke and Lapata?s (2008) model in more detail.
Sections 4?5 present our extensions and modifications.
3.1 Language Model
Let x = x0, x1, x2, . . . , xn denote a source sentence for which we wish to generate a target
compression. We use x0 to denote the ?start? token. We introduce a decision variable
for each word in the source and constrain it to be binary; a value of 0 represents a word
being dropped, whereas a value of 1 includes the word in the target compression. Let:
?i =
{
1 if xi is in the compression
0 otherwise
?i ? [1 . . . n]
A trigram language model forms the backbone of the compression model. The language
model is formulated as an integer linear program with the introduction of extra decision
variables indicating which word sequences should be retained or dropped from the
compression. Let:
?i =
{
1 if xi starts the compression
0 otherwise
?i ? [1 . . . n]
?ij =
?
?
?
1 if sequence xi, xj ends
the compression ?i ? [0 . . . n ? 1]
0 otherwise ?j ? [i + 1 . . . n]
?ijk =
?
?
?
1 if sequence xi, xj, xk ?i ? [0 . . . n ? 2]
is in the compression ?j ? [i + 1 . . . n ? 1]
0 otherwise ?k ? [j + 1 . . . n]
The objective function is expressed in Equation (1). It is the sum of all possible trigrams
multiplied by the appropriate decision variable where n is the length of the sentence
(note all probabilities throughout this paper are log-transformed). The objective func-
tion also includes a significance score I(xi) for each word xi multiplied by the decision
2 For a sentence of length n, there are 2n compressions.
416
Clarke and Lapata Discourse Constraints for Document Compression
variable for that word (see the first summation term in Equation (1)). This score high-
lights important content words in a sentence and is defined in Section 3.2.
max z =
n
?
i=1
?i ? ?I(xi) +
n
?
i=1
?i ? P(xi|start)
+
n?2
?
i=1
n?1
?
j=i+1
n
?
k=j+1
?ijk ? P(xk|xi, xj)
+
n?1
?
i=0
n
?
j=i+1
?ij ? P(end|xi, xj)
??min ? ? ? ?max ? ? (1)
Note that we add a weighting factor, ?, to the objective, in order to counterbalance the
importance of the language model and the significance score.
The final component of our objective function, ? ? ?, relates to the compression rate.
As we explain shortly (Equations (7) and (8)) the compressions our model generates
are subject to a prespecified compression rate. For instance we may wish to create com-
pressions at a minimum rate of 40% and maximum rate of 70%. The compression rate
constraint can be violated with a penalty, ?, which applies to each word. ?min counts the
number of words under the compression rate and ?max the number of words over the
compression rate. Thus, the more the output violates the compression rate, the larger
the penalty will be. In other words, the term ?min ? ? ? ?max ? ? acts as a soft constraint
providing a means to guide the compression towards the desired rate. The violation
penalty ? is tuned experimentally and may vary depending on the desired compression
rate or application.
The objective function in Equation (1) allows any combination of trigrams to be
selected. As a result, invalid trigram sequences (e.g., two or more trigrams containing
the ?end? token) could appear in the target compression. We avoid this situation by
introducing sequential constraints (on the decision variables ?i, ?ijk, ?i, and ?ij) that
restrict the set of allowable trigram combinations.
Constraint 1. Exactly one word can begin a sentence.
n
?
i=1
?i = 1 (2)
Constraint 2. If a word is included in the sentence it must either start the sentence or be
preceded by two other words or one other word and the ?start? token x0.
?k ? ?k ?
k?2
?
i=0
k?1
?
j=i+1
?ijk = 0 (3)
?k : k ? [1 . . . n]
417
Computational Linguistics Volume 36, Number 3
Constraint 3. If a word is included in the sentence it must either be preceded by one word
and followed by another or it must be preceded by one word and end the sentence.
?j ?
j?1
?
i=0
n
?
k=j+1
?ijk ?
j?1
?
i=0
?ij = 0 (4)
?j : j ? [1 . . . n]
Constraint 4. If a word is in the sentence it must be followed by two words or followed
by one word and then the end of the sentence or it must be preceded by one word and
end the sentence.
?i ?
n?1
?
j=i+1
n
?
k=j+1
?ijk ?
n
?
j=i+1
?ij ?
i?1
?
h=0
?hi = 0 (5)
?i : i ? [1 . . . n]
Constraint 5. Exactly one word pair can end the sentence.
n?1
?
i=0
n
?
j=i+1
?ij = 1 (6)
Note that Equations (2)?(6) are merely well-formedness constraints and differ from the
compression-specific constraints which we discuss subsequently. Any language model
formulated as an ILP would require similar constraints.
Compression rate constraints. Depending on the application or the task at hand, we
may require that the compressions fall within a specific compression rate. We assume
here that our model is given a compression rate range, cmin% ? cmax%, and create two
constraints that penalize compressions which do not fall within this range:
n
?
i=0
?i + ?min ? cmin ? n (7)
n
?
i=0
?i ? ?max ? cmax ? n (8)
Here, ?i is still a decision variable for each word, n is the number of words in the
sentence, ? is the number of words over or under the compression rate, and cmin and
cmax are the limits of the range.
3.2 Significance Score
The significance score is an attempt at capturing the gist of a sentence. The score has
two components which correspond to document and sentence importance, respectively.
Given a sentence and its syntactic parse, we define I(xi) as:
I(xi) = fi log
Fa
Fi
? lN (9)
418
Clarke and Lapata Discourse Constraints for Document Compression
where xi is a topic word, fi is xi?s document frequency, Fi its corpus frequency, and Fa the
sum of all topic words in the corpus; l is the number of clause constituents above xi, and
N is the deepest level of clause embedding in the parse.
The first term in Equation (9) is similar to tf ? idf ; it highlights words that are
important in the document and should therefore not be dropped. The score is not
applied indiscriminately to all words in a sentence but solely to topic-related words,
which are approximated by nouns and verbs. This is offset by the importance of these
words in the specific sentence being compressed. Intuitively, in a sentence with multiply
nested clauses, more deeply embedded clauses tend to carry more semantic content.
This is illustrated in Figure 1, which depicts the clause embedding for the sentence Mr
Field has said he will resign if he is not reselected, a move which could divide the party nationally.
Here, the most important information is conveyed by clauses S3 (he will resign) and S4
(if he is not reselected), which are embedded. Accordingly, we should give more weight
to words found in these clauses than in the main clause (S1 in Figure 1). A simple way
to enforce this is to give clauses weight proportional to the level of embedding (see the
second term in Equation (9)). Therefore in Figure 1, the term lN is 1.0 (4/4) for clause S4,
0.75 (3/4) for clause S3, and so on. Individual words inherit their weight from their
clauses. We obtain syntactic information in our experiments from RASP (Briscoe and
Carroll 2002), a domain-independent, robust parsing system for English. However, any
other parser with broadly similar output (e.g., Lin 2001) could also serve our purposes.
Note that the significance score in Equation (9) does not weight differentially the
contribution of tf ? idf versus level of embedding. Although we found in our exper-
iments that the latter term was as important as tf ? idf in producing meaningful com-
pressions, there may be applications or data sets where the contribution of the two terms
varies. This could be easily remedied by introducing a weighting factor.
3.3 Sentential Constraints
In its original formulation, the model also contains a small number of sentence-level
constraints. Their aim is to preserve the meaning and structure of the original sentence
as much as possible. The majority of constraints revolve around modification and
Figure 1
The clause embedding of the sentence Mr Field has said he will resign if he is not reselected, a move
which could divide the party nationally; nested boxes correspond to nested clauses.
419
Computational Linguistics Volume 36, Number 3
argument structure and are defined over parse trees or grammatical relations which
as mentioned earlier we extract from RASP.
Modifier Constraints. Modifier constraints ensure that relationships between head words
and their modifiers remain grammatical in the compression:
?i ? ?j ? 0 (10)
?i, j : xj ? xi?s ncmods
?i ? ?j ? 0 (11)
?i, j : xj ? xi?s detmods
Equation (10) guarantees that if we include a non-clausal modifier3 (ncmod) in the
compression (such as an adjective or a noun) then the head of the modifier must also be
included; this is repeated for determiners (detmod) in Equation (11).
Other modifier constraints ensure the meaning of the source sentence is preserved
in the compression. For example, Equation (12) enforces not in the compression when
the head is included. A similar constraint is added for possessive modifiers (e.g., his,
our), including genitives (e.g., John?s gift), as shown in Equation (13).
?i ? ?j = 0 (12)
?i, j : xj ? xi?s ncmods ? xj = not
?i ? ?j = 0 (13)
?i, j : xj ? xi?s possessive mods
Argument Structure Constraints. Argument structure constraints make sure that the re-
sulting compression has a canonical argument structure. The first constraint (Equa-
tion (14)) ensures that if a verb is present in the compression then so are its arguments,
and if any of the arguments are included in the compression then the verb must also be
included.
?i ? ?j = 0 (14)
?i, j : xj ? subject/object of verb xi
Another constraint forces the compression to contain at least one verb provided the
source sentence contains one as well:
?
i:xi?verbs
?i ? 1 (15)
3 Clausal modifiers (cmod) are adjuncts modifying entire clauses. In the example he ate the cake because he
was hungry, the because-clause is a modifier of the sentence he ate the cake.
420
Clarke and Lapata Discourse Constraints for Document Compression
Other constraints apply to prepositional phrases and subordinate clauses and force the
introducing term (i.e., the preposition, or subordinator) to be included in the compres-
sion if any word from within the syntactic constituent is also included:
?i ? ?j ? 0 (16)
?i, j : xj ? PP/SUB ? xi starts PP/SUB
By subordinator (SUB) we mean wh-words (e.g., who, which, how, where), the word that,
and subordinating conjunctions (e.g., after, although, because). The reverse is also true?
that is, if the introducing term is included, at least one other word from the syntactic
constituent should also be included.
?
i:xi?PP/SUB
?i ? ?j ? 0 (17)
?j : xj starts PP/SUB
All the constraints described thus far are mostly syntactic. They operate over
parse trees or dependency graphs. In the following sections we present our discourse-
specific constraints. But first we discuss how we represent and automatically detect
discourse-related information.
4. Discourse Representation
Obtaining an appropriate representation of discourse is the first step toward creating a
compression model that exploits document-level information. Our goal is to annotate
documents automatically with discourse-level information which will subsequently be
used to inform our compression procedure. As mentioned in Section 2 previous summa-
rization work has mainly focused on cohesion (Sjorochod?ko 1972; Barzilay and Elhadad
1997) or global discourse structure (Marcu 2000; Daume? III and Marcu 2002). We also
opt for a cohesion-based representation of discourse operationalized by lexical chains
(Morris and Hirst 1991). Computing global discourse structure robustly and accurately
is far from trivial. For example, Daume? III and Marcu (2002) employ an RST parser4
but find that it produces noisy output for documents containing longer sentences.
We therefore focus on the less ambitious task of characterizing local coherence?the
way adjacent sentences bind together to form a larger discourse. Although it does
not explicitly capture long distance relationships between sentences, local coherence is
still an important prerequisite for maintaining global coherence. Specifically, we turn
to Centering Theory (Grosz, Weinstein, and Joshi 1995) and adopt an entity-based
representation of discourse.
In the following sections we briefly introduce lexical chains and centering and
describe our algorithms for obtaining discourse annotations.
4 This is the decision-based parser described in Marcu (2000); it achieves an F1 of 38.2 for the identification
of elementary discourse units, 50.0 for hierarchical spans, 39.9 for nuclearity, and 23.4 for relation
assignment.
421
Computational Linguistics Volume 36, Number 3
4.1 Lexical Chains
Lexical cohesion refers to the degree of semantic relatedness observed among lexical
items in a document. The term was coined by Halliday and Hasan (1976), who observed
that coherent documents tend to have more related terms or phrases than incoherent
ones. A number of linguistic devices can be used to signal cohesion; these range from
repetition, to synonymy, hyponymy, and meronymy. Lexical chains are a representation
of lexical cohesion as sequences of semantically related words (Morris and Hirst 1991).
There is a close relationship between discourse structure and cohesion. Related words
tend to co-occur within the same discourse. Thus, cohesion is a surface indicator of
discourse structure and can be identified through lexical chains.
Lexical chains provide a useful means for describing the topic flow in discourse.
For example, a document containing the chain {house, home, loft, house} will proba-
bly describe a situation involving a house. Documents often have multiple topics (or
themes) and consequently will contain many different lexical chains. Some of these
topics will be peripheral and thus represented by short chains whereas main topics
will correspond to dense longer chains. Words participating in the latter chains are
important for our compression task?they reveal what the document is about?and in
all likelihood should not be deleted.
Barzilay and Elhadad (1997) describe a technique for building lexical chains for
extractive text summarization. In their approach chains of semantically related expres-
sions are used to select sentences for inclusion in a summary. Their algorithm uses
WordNet (Fellbaum 1998) to build chains of nouns (and noun compounds). Nouns
are considered related if they are repetitions or linked in WordNet via synonymy,
antonymy, hypernymy, and holonymy. Computing lexical chains would be relatively
straightforward if each word was always represented by a single sense. However, due
to the high level of polysemy inherent in WordNet, algorithms developed for computing
lexical chains must adopt some strategy for disambiguating word senses. For example,
Hirst and St-Onge (1998) greedily disambiguate a word as soon as it is encountered by
selecting the sense most strongly related to existing chain members, whereas Barzilay
and Elhadad (1997) consider all possible alternatives of word senses and then choose
the best one among them.
Once created, lexical chains can serve to highlight which document sentences are
more topical, and should therefore be included in a summary. Barzilay and Elhadad
(1997) rank their chains heuristically by a score based on their length and homogeneity.
They generate summaries by extracting sentences corresponding to strong chains, that
is, chains whose score is two standard deviations above the average score. Analogously,
we also wish to determine which lexical chains indicate the most prevalent discourse
topics. Our assumption is that terms belonging to these chains are indicative of the
document?s main focus and should therefore be retained in the compressed output.
Barzilay and Elhadad?s (1997) scoring function aims to identify sentences (for inclusion
in a summary) that have a high concentration of chain members. In contrast, we are
interested in chains that span several sentences. We thus score chains according to the
number of sentences their terms occur in. For example, the hypothetical chain {house3,
home3, loft3, house5} (where wordi denotes word occurring in sentence i) would be given a
score of two as the terms occur only in two sentences. We assume that a chain signals a
prevalent discourse topic if it occurs throughout more sentences than the average chain.
The scoring algorithm is outlined more formally as:
1. Compute the lexical chains for the document.
422
Clarke and Lapata Discourse Constraints for Document Compression
2. Score(Chain) = Sentences(Chain).
3. Discard chains for which Score(Chain) < Average(Score).
4. Mark terms from the remaining chains as being the focus of the document.
We use the method of Galley and McKeown (2003) to compute lexical chains for
each document.5 It improves on Barzilay and Elhadad?s (1997) original algorithm by
providing better word sense disambiguation and linear runtime. The algorithm pro-
ceeds in three steps. Initially, a graph is built representing all possible interpretations
of the document under consideration. The text is processed sequentially, comparing
each word against all words previously read. If a relation exists between the senses of
the current word and any possible sense of a previous word, a connection is formed
between the appropriate words and senses. The strength of the connection is a function
of the type of relationship and of the distance between the words in the text (in terms
of words, sentences, and paragraphs). Words are represented as nodes in the graph and
semantic relations as weighted edges. The relations considered by Galley and McKeown
(2003) are all first-order WordNet relations, with the addition of siblings?two words
are considered siblings if they are both hyponyms of the same hypernym. Next, all
occurrences of a given word are collected together. For each sense of a target word,
the strength of all connections involving that sense are summed, giving that sense a
unified score. The sense with the highest unified score is chosen as the correct sense
for the target word. Lastly, the lexical chains are constructed by collecting same sense
words into the same chain.
Figure 2 illustrates the lexical chains created by our algorithm for three documents
(taken from our test set). Chains are shown in oval boxes; members of the same chain
have the same index. The algorithm identifies three chains in the first document: {flow,
rate}, {today, day, yesterday}, and {miles, ft}. In the second document the chains are {body}
and {month, night}, and in the third {policeman, police}, {woman, woman, boyfriend, man}.
As can be seen, members of a chain represent a shared concept (e.g., ?time?, ?linear
unit?, or ?person?). In some cases important topics are missed. For instance, in the first
document no chains were created with the words lava or debris. The second document
is about Mrs Allan and contains many references to her. However, because Mrs Allan is
not listed in WordNet it is not possible to create any chains for this word or any of its
coreferents (e.g., she, her). A similar problem is observed in the third document where
Anderson is not included in any chain even though he is one of the main protagonists
throughout the text. We next turn to Centering Theory as a means of identifying which
entities are prominent in a document.
4.2 Centering Theory
Centering Theory (Grosz, Weinstein, and Joshi 1995) is an entity-orientated theory of
local coherence and salience. One of the main ideas underlying centering is that certain
entities mentioned in an utterance are more central than others. This in turn imposes
constraints on the use of referring expressions and in particular on the use of pronouns.
The theory begins by assuming that a discourse is broken into ?utterances.? These
can be phrases, clauses, sentences, or even paragraphs. At any point in discourse,
some entities are considered more salient than others, and are expected to exhibit
5 The software is available from http://www1.cs.columbia.edu/nlp/tools.cgi.
423
Computational Linguistics Volume 36, Number 3
Figure 2
Excerpts of documents from our test set with discourse annotations. Centers are in double boxes;
terms occurring in lexical chains are in oval boxes. Words with the same subscript are members
of the same chain (e.g., police, policeman).
different properties. Specifically, although each utterance may contain several entities, it
is assumed that a single entity is ?centered,? thereby representing the current discourse
focus. One of the main claims underlying centering is that discourse segments in which
successive utterances contain common centers are more coherent than segments where
the center repeatedly changes.
Each utterance Uj in a discourse has a list of forward-looking centers, Cf (Uj), and
a unique backward-looking center, Cb(Uj). Cf (Uj) represents a ranking of the entities
invoked by Uj according to their salience. Thus, some entities in the discourse are
deemed more important than others. The Cb of the current utterance Uj is the highest-
ranked element in Cf (Uj?1) that is also in Uj. (Centering hypothesizes that the Cb is
likely to be realized as a pronoun.) Entities are commonly ranked in terms of their
grammatical function, namely, subjects are ranked more highly than objects, which are
more highly ranked than the rest (Grosz, Weinstein, and Joshi 1995). The Cb links Uj to
the previous discourse, but it does so locally since Cb(Uj) is chosen from Uj?1.
Centering formalizes fluctuations in topic continuity in terms of transitions be-
tween adjacent utterances. Grosz, Weinstein, and Joshi (1995) distinguish between three
424
Clarke and Lapata Discourse Constraints for Document Compression
types of transitions. In CONTINUE transitions, Cb(Uj) = Cb(Uj?1) and Cb(Uj) is the
most highly ranked element entity in Uj. In RETAIN transitions Cb(Uj) = Cb(Uj?1) but
Cb(Uj) is not the most highly ranked element entity in Uj. And in SHIFT transitions
Cb(Uj) = Cb(Uj?1). These transitions are ordered: CONTINUEs are preferred over RE-
TAINs, which are preferred over SHIFTs. And discourses with many CONTINUE transi-
tions are considered more coherent than those which repeatedly SHIFT from one center
to the other.
We demonstrate these concepts in passages (1a)?(1c) taken from Walker, Joshi, and
Prince (1998).
(1) a. Jeff helped Dick wash the car.
CF(Jeff, Dick, car)
b. He washed the windows as Dick waxed the car.
CF(Jeff, Dick, car)
CB=Jeff
c. He soaped a pane.
CF(Jeff, pane)
CB=Jeff
Here, the first utterance does not have a backward-looking center but has three forward-
looking centers Jeff, Dick, and car. To determine the backward-looking center of (1b) we
find the highest ranked entity among the forward-looking centers in (1a) which also
occurs in (1b). This is Jeff as it is the subject (and thus most salient entity) in (1a) and
present (as a pronoun) in (1b). The same procedure is applied for utterance (1c). Also
note that (1a) and (1b) are linked via a CONTINUE transition. The same is true for (1b)
and (1c).
For the purposes of our document compression application, we are not so much
interested in characterizing our texts in terms of entity transitions. Because they are all
written by humans, we can assume they are more or less coherent. Nonetheless, identi-
fying the centers in discourse seems important. These will indicate what the document
is about, who the main protagonists are, and how the discourse focus progresses. We
would probably not want to delete entities functioning as backward-looking centers.
As Centering is primarily a linguistic theory rather than a computational one,
it is not explicitly stated how the concepts of ?utterance,? ?entities,? and ?ranking?
are instantiated. A great deal of research has been devoted to fleshing these out and
many different instantiations have been developed in the literature (see Poesio et al
[2004] for details). In our case, the instantiation will have a bearing on the reliability
of the algorithm to detect centers. If the parameters are too specific then it may not be
possible to accurately determine the center for a given utterance. Because our aim is
to identify centers in discourse automatically, our parameter choice is driven by two
considerations: robustness and ease of computation.
We therefore follow previous work (e.g., Miltsakaki and Kukich 2000) in assuming
that the unit of an utterance is the sentence (i.e., a main clause with accompanying
subordinate and adjunct clauses). This is a simplistic view of an utterance; however it
is in line with our compression task, which also operates over sentences. We determine
which entities are invoked by a sentence using two methods. First, we perform named
entity identification and coreference resolution on each document using LingPipe,6 a
6 LingPipe can be downloaded from http://alias-i.com/lingpipe/.
425
Computational Linguistics Volume 36, Number 3
publicly available system. Named entities are not the only type of entity to occur in our
data, thus to ensure a high entity recall we add named entities and all remaining nouns7
to the Cf list. Entity matching between sentences is required to determine the Cb of a sen-
tence. This is done using the named entity?s unique identifier (as provided by LingPipe)
or by the entity?s surface form in the case of nouns not classified as named entities.
We follow Grosz, Weinstein, and Joshi (1995) in ranking entities according to their
grammatical roles; subjects are ranked more highly than objects, which are in turn
ranked higher than other grammatical roles; ties are broken using left-to-right ordering
of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles
using RASP (Briscoe and Carroll 2002). Formally, our centering algorithm is as follows
(where Uj corresponds to sentence j):
1. Extract entities from Uj.
2. Create Cf (Uj) by ranking the entities in Uj according to their grammatical
role (subjects > objects > others, ties broken using left-to-right word order
of Uj).
3. Find the highest ranked entity in Cf (Uj?1) which occurs in Cf (Uj); set the
entity to be Cb(Uj).
This procedure involves several automatic steps (named entity recognition, coreference
resolution, and identification of grammatical roles) and will unavoidably produce some
noisy annotations. There is no guarantee, therefore, that the right Cb will be identified or
that all sentences will be marked with a Cb. The latter situation also occurs in passages
that contain abrupt changes in topic. In such cases, none of the entities realized in Uj will
occur in Cf (Uj?1). Hopefully, lexical chains will come to the rescue here as an alternative
means of capturing local content within a document.
Figure 2 shows the centers (in double boxes) identified by our algorithm. In the first
document lava and debris are marked as centers, in the second document Mrs Allan (and
its coreferents), and in the third one Peter Anderson and allotment. When comparing the
annotations produced by centering and the lexical chains, we observe that they tend
to be complementary. Proper nouns that lexical chains miss out on are often identi-
fied by centering. When the latter fails, due to errors in coreference resolution or the
identification of grammatical relations, lexical chains can be more robust because only
WordNet is required for their computation. As an example consider the third document
in Figure 2. Here, lexical chains provide a better insight into the text. Were we to rely
solely on centering, we would obtain annotations only for two entities, namely, Peter
Anderson and allotment.
5. The Discourse-Inspired Compression Model
We now turn our attention to incorporating discourse information into our compression
model. Before compression takes place, all documents are processed using the center-
ing and lexical chain algorithms described earlier. In each sentence we annotate the
center Cb(Uj) if one exists. Words (or phrases) that are present in the current sentence
and function as the center in the next sentence Cb(Uj+1) are also flagged. Finally, words
7 As determined by the word?s part-of-speech tag.
426
Clarke and Lapata Discourse Constraints for Document Compression
are marked if they are part of a prevalent (high scoring) chain. Provided with this
additional knowledge our model takes a (sentence-separated) source document as input
and generates a compressed version by applying sentence-level and discourse-level
constraints to the entire document rather than to each sentence sequentially. In our
earlier formulation of the compression task (Clarke and Lapata 2008), we create and
solve an ILP for every sentence, whereas now an ILP is solved for each document.
This makes sense from a discourse perspective as compression decisions are not made
independently of each other. Also note that this latter formulation brings compression
closer to summarization as we can manipulate the document compression rate directly,
for example, by adding a constraint that forces the target document to be less than b to-
kens. This allows the model to choose how much to compress each individual sentence
without requiring that they all have the same compression rate. Accordingly, we modify
our objective function by introducing a sum over all sentences (assuming l sentences are
present in the document) and adding an additional index g to each decision variable to
track the sentence it came from:
max z =
l
?
g=1
[ ng
?
i=1
?g,i ? ?I(xg,i) +
ng
?
i=1
?g,i ? P(xg,i|start)
+
ng?2
?
i=1
ng?1
?
j=i+1
ng
?
k=j+1
?g,ijk ? P(xg,k|xg,i, xg,j)
+
ng?1
?
i=0
ng
?
j=i+1
?g,ij ? P(end|xg,i, xg,j)
?
?
??min ? ? ? ?max ? ? (18)
We also modify the compression rate soft constraint to act over the whole document
rather than sentences. This allows some sentences to violate the compression rate with-
out incurring a penalty, provided the compression rate of the document falls within the
specified range.
Document Compression Rate Constraints. We wish to penalize compressions which do not
fall within a desired compression rate range (cmin% ? cmax%).
l
?
g=1
ng
?
i=0
?g,i + ?min ? cmin ?
l
?
g=1
ng (19)
l
?
g=1
ng
?
i=0
ng
?
i=0
?g,i ? ?max ? cmax ?
l
?
g=1
ng (20)
Besides the new objective function and compression rate constraints, the model
makes use of all the sentence-level constraints introduced in Section 3.3, but is crucially
enhanced with three discourse constraints explained in the following.
427
Computational Linguistics Volume 36, Number 3
5.1 Discourse Constraints
Our first goal to is preserve the focus of each sentence. If the center, Cb, is identified in
the source sentence it must be retained in the target compression. If present, the entity
realized as the Cb in the following sentence should also be retained to ensure the focus
is preserved from one sentence to the next. Such a condition is easily captured with the
following ILP constraint:
?i = 1 (21)
?i : xi ? {Cb(Uj), Cb(Uj+1)}
As an example, consider the first discourse in Figure 2. The constraints generated from
Equation (21) will require the compression to retain lava in the first two sentences and
debris in the second and third sentences.
As mentioned in the previous section, the centering algorithm relies on NLP tech-
nology that is not 100% accurate (named entity detection, parsing, and coreference
resolution). Therefore, the algorithm can only approximate the center for each sen-
tence and in some cases fails to identify any centers at all. Lexical chains provide a
complementary annotation of the topic or theme of the document using information
which is not restricted to adjacent sentences. Recall that once chains are created, they
are scored, and chains with scores less than the average are discarded. We consider all
remaining lexical chains as topical and require that words in these be retained in the
compression.
?i = 1 (22)
?i : xi ? document topical lexical chain
Consider again the first text in Figure 2. Here, flow and rate are members of the same
chain (marked with subscript 1). According to constraint (22) both words must be
included in the compressed document. In the third document the words relating to
?police? (police, policeman) and ?people? (woman, boyfriend, man) also would be retained
in the compression.
Our final discourse constraint concerns pronouns. Specifically, we force per-
sonal pronouns (whose antecedent may not always be identified) to be included in the
compression.
?i = 1 (23)
?i : xi ? personal pronouns
The constraints just described ensure that the compressed document will retain
the discourse flow of the source document and will preserve terms indicative of
important topics. Document compression aside, the discourse constraints will also
benefit sentence-level compression. They provide our model, which so far relied on
syntactic evidence and surface level document characteristics (i.e., word frequencies),
additional evidence for retaining (discourse) relevant words.
428
Clarke and Lapata Discourse Constraints for Document Compression
5.2 Applying the Constraints
As explained earlier we apply the model and the constraints to each document. In our
earlier sentence-based formulation, a significance score (see Section 3.2) was used to
highlight which nouns and verbs should be included in the compression. As far as
nouns are concerned, our discourse constraints perform a similar task. Thus, when a
sentence contains discourse annotations, we are inclined to trust them more and only
calculate the significance score for verbs.
During development it was observed that applying all discourse constraints si-
multaneously (see Equations (21)?(23)) results in relatively long compressions. To
counteract this, we employ these constraints using a back-off strategy that relies on
progressively less reliable information. Our back-off model works as follows: If center-
ing information is present, we apply the appropriate constraints (Equation (21)). If no
centers are present, we back off to the lexical chain information using Equation (22), and
in the absence of the latter we back off to the pronoun constraint (Equation (23)). Finally,
if discourse information is entirely absent from the sentence, we default to the sig-
nificance score. Sentential constraints are applied throughout irrespective of discourse
constraints. We determined this ordering (i.e., centering first, then lexical chains, and
then pronouns) on the development set. Centering tends to be more precise, whereas
lexical chains have high recall but lower precision in terms of identifying which entities
are in focus and should therefore not be dropped. In our test data (see Section 6 for
details), the centering constraint was used in 68.6% of the sentences. The model backed
off to lexical chains for 13.7% of the test sentences, whereas the pronoun constraint
was applied in 8.5%. Finally, the noun and verb significance score was used on the
remaining 9.2%. Examples of our system?s output for the texts in Figure 2 are given in
Figure 3.
6. Experimental Set-up
In this section we present our experimental set-up for assessing the performance of
the compression model. We describe the compression corpus used in our study, briefly
introduce the model used for comparison with our approach, and explain how system
output was evaluated.
6.1 Compression Corpus
Previous work on sentence compression has used almost exclusively the Ziff-Davis cor-
pus, a compression corpus derived automatically from document?abstract pairs (Knight
and Marcu 2002). Unfortunately, this corpus is not suitable for our purposes because it
consists of isolated sentences taken from several different documents. We thus created
a document-based compression corpus manually. Specifically, annotators were pre-
sented with one document at a time and asked to compress sentences sequentially
by removing tokens. They were free to remove any words they deemed superfluous,
provided their deletions (a) preserved the most important information in the source sen-
tence, and (b) ensured the compressed sentence remained grammatical. If they wished,
they could leave a sentence uncompressed. They were not allowed to delete whole
sentences even if they believed they contained no information content with respect to
the story, as this would blur the task with summarization. Following these guidelines,
429
Computational Linguistics Volume 36, Number 3
Figure 3
Compression output on excerpts from Figure 2 using the discourse model. Words that are
dropped are striken out.
the annotators created compressions for 82 stories (1,629 sentences) from the BNC and
the LA Times and Washington Post.8 Forty-eight (48) documents (962 sentences) were
used for training, 3 for development (63 sentences), and 31 for testing (604 sentences).
6.2 Comparison with State-of-the-Art
The discourse-based compression model was evaluated against our earlier sentence-
based ILP model (without the discourse constraints). In addition, we compared our ap-
proach against a state-of-the-art model which does not take discourse-level information
into account, does not use ILP, and is sentence-based. We give a brief description in the
following, and refer the interested reader to McDonald (2006) for details.
McDonald (2006) formalizes sentence compression as a classification task in a dis-
criminative large-margin learning framework: Pairs of words from the source sentence
are classified as being adjacent or not in the target compression. Let x = x1, . . . , xn
denote a source sentence with a target compression y = y1, . . . , ym where each yj oc-
curs in x. The function L(yi) ? {1 . . . n} maps word yi in the target to the index of
the word in the source, x (subject to the constraint that L(yi) < L(yi+1)). McDonald
defines the score of a compression y for a sentence x as the dot product between
8 The corpus is available from http://homepages.inf.ed.ac.uk/s0460084/data/.
430
Clarke and Lapata Discourse Constraints for Document Compression
a high-dimensional feature representation over bigrams and a corresponding weight
vector:
s(x, y) =
|y|
?
j=2
w ? f(x, L(yj?1), L(yj)) (24)
Decoding in this framework amounts to finding the combination of bigrams that maxi-
mize the scoring function in Equation (24). The maximization is solved using dynamic
programming (see McDonald [2006] for details).
The model parameters are estimated using the Margin Infused Relaxed Algorithm
(MIRA; Crammer and Singer 2003), a discriminative large-margin online learning tech-
nique. This algorithm learns by compressing each sentence and comparing the result
with the gold standard. The weights are updated so that the score of the correct com-
pression (the gold standard) is greater than the score of all other compressions by a
margin proportional to their loss. The loss function is the number of words falsely re-
tained or dropped in the incorrect compression relative to the gold standard. McDonald
employs a rich feature set defined over words, parts of speech, phrase structure trees,
and dependencies. These are gathered over adjacent words in the compression and the
words in between which were dropped.
It is important to note that McDonald (2006) is not a straw-man system. It achieves
highly competitive performance compared with Knight and Marcu?s (2002) noisy-
channel and decision-tree models. Due to its discriminative nature, the model is able
to use a large feature set and to optimize compression accuracy directly. In other words,
McDonald?s model has a head start against our own model which does not utilize a
large parallel corpus and has only a few constraints. The comparison of the two systems
allows us to establish that we have a competitive state-of-the-art system, even without
discourse constraints.
We trained McDonald?s (2006) model on the full training set (48 documents, 962
sentences). Our implementation used an identical feature set, the only difference being
that our phrase structure and dependency features were extracted from the output
of Roark?s (2001) parser. McDonald uses Charniak?s (2000) parser, which performs
comparably. We also employed a slightly modified loss function to encourage compres-
sion on our data set. McDonald?s results were reported on the Ziff-Davis corpus. The
language model required for the ILP system was trained on 80 million tokens from the
English GigaWord corpus (LDC2007T07) using the SRI Language Modeling Toolkit with
Kneser-Ney discounting. The significance score was calculated on 80 million tokens
from the same corpus. The ILP model presented in Equation (1) implements a weighted
combination of the significance score with a language model. The weight was tuned
on the development set which consisted of three source documents and their target
compressions. Our optimization procedure used Powell?s method (Press et al 1992) and
a loss function based on the grammatical relations F1 between the gold standard and
system output. The optimal weight was approximately 9.0. Note that the development
set was the only source of parallel data our model had access to.
In order to compare all three models (sentence-based ILP, discourse-based ILP, and
McDonald [2006]) on an equal footing, we ensured that their compression rates were
similar. To do this, we first run McDonald?s model on our data and then set the com-
pression rate for our ILP models so that it is comparable to his output. This can be done
relatively straightforwardly by adjusting the compression rate range soft constraint. In
our experiments we set the minimum compression rate to 57%, the upper rate to 62%,
431
Computational Linguistics Volume 36, Number 3
and the violation penalty (?) to ?99. In practice, the soft constraint controlling the
compression rate can be removed or specifically tuned to suit the application.
6.3 Evaluation
Previous studies evaluate the well-formedness of automatically generated compres-
sions out of context. The target sentences are typically rated by naive subjects on two
dimensions, grammaticality and importance (Knight and Marcu 2002). Automatic eval-
uation measures have also been proposed. Riezler et al (2003) compare the grammatical
relations found in the system output against those found in a gold standard using F1.
Although F1 conflates grammaticality and importance into a single score, it neverthe-
less has been shown to correlate reliably with human judgments (Clarke and Lapata
2006).
The aims of our evaluation study were twofold. Firstly, we wanted to examine
whether our discourse constraints improve the compressions for individual sentences.
There is no hope for generating shorter documents if the compressed sentences are
either too wordy or too ungrammatical. Secondly and more importantly, our goal was
to evaluate the compressed documents as a whole by examining whether they are
readable and the degree to which they retain key information when compared to the
originals. We evaluated sentence-based compressions automatically using F1 and the
grammatical relations annotations provided by RASP (Briscoe and Carroll 2002). This
parser is suited to the compression task as it provides parses for both full sentences
and sentence fragments and is generally robust enough to analyze semi-grammatical
sentences. We computed F1 over all the relations provided by RASP (e.g., subject,
direct/indirect object, modifier; 17 in total). We compared the output of our discourse
system on the test set (31 documents, 604 sentences) against the sentence-based ILP
model and McDonald (2006).
Our document-level evaluation was motivated by two questions: (1) Are the com-
pressed documents readable? and (2) How much key information is preserved between
the source document and its target compression? The readability of a document is
fairly straightforward to measure by asking participants to provide a rating (e.g., on a
seven-point scale). Measuring how much information is preserved in the compressed
document is more involved. Under the assumption that the target document is to
function as a replacement for the source, we can measure the extent to which the
compressed version can be used to find answers for questions which have been derived
from the source and are representative of its core content. We thus created questions
from the source and then determined whether it was possible to find their answers by
reading the compressed target. The more questions a hypothetical compression system
can answer, the better it is at compressing the document as a whole.
A question-answering (Q&A) paradigm has been used previously to evaluate
summaries and text compression. Morris, Kasper, and Adams (1992) performed one
of the first Q&A evaluations to investigate the degree to which documents could be
summarized before reading comprehension diminished. Their corpus consisted of four
passages randomly selected from a set of sample Graduate Management Aptitude Test
(GMAT) reading comprehension tests. The texts covered a range of topics including
medieval literature, 18th-century Japan, minority-operated businesses, and Florentine
art. Accompanying each text were eight multiple-choice questions, each containing
five possible answers. The questions were provided by the Educational Testing Service
and were designed to measure the subjects? reading comprehension. Subjects were
432
Clarke and Lapata Discourse Constraints for Document Compression
given various textual treatments: the full text, a human-authored abstract, three system-
generated extracts, and a final treatment where merely the questions were presented
without any text. The questions-only treatment was used as a control to investigate if
subjects could answer questions without any source material. Subjects were instructed
to read the passage (if provided) and answer the multiple choice questions.
The advantage of using standardized tests, such as the GMAT reading compre-
hension test, is that Q&A pairs are provided along with a method for scoring answers
(the correct answer is one among five possible choices). However, our corpora do not
contain ready prepared Q&A pairs; thus we require a methodology for constructing
questions and their answers and scoring documents against the answers. One such
methodology is presented in the TIPSTER Text Summarization Evaluation (SUMMAC;
Mani et al 2002). SUMMAC was concerned with producing summaries tailored to
specific topics. The Q&A task involved an evaluation where a topic-related summary
for a document was evaluated in terms of its ?informativeness,? namely, the degree
to which it contained answers found in the source document to a set of topic-related
questions. For each topic (three in total), 30 relevant documents were chosen to generate
a single summary. One annotator per topic came up with no more than five questions
relating to the obligatory aspects of the topic. An obligatory aspect of a topic was
defined as information that must be present in the document for the document to be
relevant to the topic. The annotators then created an answer key for their topic by
annotating the passages and phrases from the documents which provided the answers
to the questions. In the SUMMAC evaluation, the annotator for each topic was tasked
with scoring the system summaries. Scoring involved comparing the summaries against
the answer key (annotated passages from the source documents) while judging whether
the summary provided a Correct, Partially Correct, or Missing answer. If a summary con-
tained an answer key and sufficient context the summary was deemed correct; however,
summaries would be considered partially correct if the answer key was present but with
insufficient context. If context was completely missing, misleading, or the answer key
was absent then the summary was judged missing.
Our methodology for constructing Q&A pairs and for scoring documents is in-
spired by the SUMMAC evaluation exercise (Mani et al 2002). Rather than creating
questions for document sets (or topics) our questions were derived from individual
documents. Two annotators were independently instructed to read the documents from
our (test) corpus and create Q&A pairs. Each annotator drafted no more than ten
questions and answers per document, related to its content. Annotators were asked
to create fact-based questions which required an unambiguous answer; these were
typically who, what, where, when, and how?style questions. The purpose of using two
annotators per document was to allow annotators to compare and revise their Q&A
pairs; this process was repeated until a common agreed-upon set of questions was
reached. Revisions typically involved merging and simplifying questions to make them
clearer, and in some cases splitting a question into multiple questions. Documents for
which too few questions were agreed upon and for which the questions and answers
were too ambiguous were removed. This left an evaluation set of six documents with
between five to eight concise questions per document. Figure 4 shows a document from
our test set and the questions and answers our annotators created for it.
For scoring our documents we adopt a more objective method than SUMMAC.
Instead of asking the annotator who constructed the questions to check the document
compressions for the answers, we ask naive participants to read the compressed doc-
uments and answer the questions as best as they can. During evaluation, the source
document is not shown to our subjects; thus, if the compression is difficult to read, the
433
Computational Linguistics Volume 36, Number 3
Figure 4
Example document from our test set and questions with answer key created for this document.
participants have no point of reference to help them understand the compression. This
is a departure from previous evaluations within text generation tasks, where the source
text is available at judgment time; in our case only the system output is available.
The document-based evaluation was conducted remotely over the Internet using
a custom-built Web interface. Upon loading the Web interface, participants were pre-
sented with a set of instructions that explained the Q&A task and provided examples.
434
Clarke and Lapata Discourse Constraints for Document Compression
Table 1
Compression results: compression rate and relation-based F1.
Model CompR Precision Recall F1
McDonald 60.1% 43.9% 36.5%? 37.9%?
Sentence ILP 62.1% 40.7%? 39.4%? 39.0%?
Discourse ILP 61.0% 46.2% 44.2% 42.2%
Gold Standard 70.3% ?? ?? ??
? Significantly different from Discourse ILP (p < 0.01 using the Wilcoxon test).
Subjects were first asked to read the compressed document and then rate its readability
on a seven-point scale where 7 = excellent, and 1 = terrible. Next, questions were
presented one at a time (the order being is defined by the annotators) and participants
were encouraged to consult the document for the answer. Answers were written directly
into a text field on the Web interface which allowed free-form text to be submitted. Once
a participant provided an answer and confirmed the answer, the interface locked the
answer to ensure it was not modified later. This was necessary because later questions
could reveal information which would help answer previous questions.
We elicited answers for six documents in four compression conditions: gold stan-
dard, using the ILP sentence-based model, the ILP discourse model, and McDonald?s
(2006) model. A Latin square design was used to prevent participants from seeing
multiple treatments (compressions) of the same document thus removing any learning
effect. A total of 116 unpaid volunteers completed the experiment. They were recruited
through student mailing lists and the Language Experiments Web site.9 The answers
provided by our subjects were scored against an answer key. A correct answer was
marked with a score of one, and zero otherwise. In cases where two answers were
required, a score of 0.5 was awarded to each correct answer. The score for a compressed
document is the average of its question scores. All subsequent tests and comparisons
are performed on the document score.
7. Results
We first assessed the compressions produced by the two ILP models (Discourse and
Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the
compression rates (CompR) for the three systems and evaluates the quality of their
output using grammatical relations F1. As can be seen, all three systems produce
comparable compression rates. The Discourse ILP compressions are slightly longer than
McDonald?s (2006) (61.0% vs. 60.1%) and slightly shorter than the Sentence ILP model
(61.0% vs. 62.1%). The Discourse ILP model is significantly better than McDonald (2006)
and Sentence ILP in terms of F1, indicating that discourse-level information is generally
helpful. All three systems could use further improvement, as inter-annotator agreement
on this data yields an F1 of 65.8% (Clarke 2008).
Let us now consider the results of our document-based evaluation. Table 2 shows
the mean readability ratings obtained for each system and the percentage of questions
answered correctly. We used an analysis of variance (ANOVA) to examine the effect
9 Available at http://www.language-experiments.org.
435
Computational Linguistics Volume 36, Number 3
Table 2
Human evaluation results: average readability ratings and average percentage of questions
answered correctly.
Model Readability Q&A (%)
McDonald 2.52? 51.42??
Sentence ILP 2.76? 52.35??
Discourse ILP 3.10? 71.38?
Gold Standard 5.41? 85.48?
? Significantly different from Gold Standard.
? Significantly different from Discourse ILP.
of compression type (McDonald, Sentence ILP, Discourse ILP, Gold Standard). The
ANOVA revealed a reliable effect on both readability and Q&A. Post hoc Tukey tests
showed that McDonald and the two ILP models do not differ significantly in terms
of readability. However, they are all significantly less readable than the gold standard
(? < 0.01). For the Q&A task, we observe that our system is significantly better than
McDonald (? < 0.01) and Sentence ILP (? < 0.01), but significantly worse than the gold
standard (? < 0.05). McDonald and Sentence ILP yield comparable performance (their
difference is not statistically significant).
These results indicate that the automatic systems lag behind the human gold stan-
dard in terms of readability. When reading entire documents, subjects are less tolerant
of ungrammatical constructions. We also find out that, despite relatively low readability,
the documents are overall understandable. The discourse-based model generates more
informative documents?the number of questions answered correctly increases by 19%
in comparison to McDonald and Sentence ILP. This is an encouraging result suggesting
that there are advantages in developing compression models that exploit discourse-
level information information.
Figure 5 shows the output of the ILP systems (Discourse and Sentence) on two
test documents. Words that are dropped have been stricken out. As can be seen, the
two systems produce different compressions, and the discourse-based output is more
coherent. This is corroborated by the readability results where the discourse ILP model
received the highest rating. Also note that some of the compressions produced by the
sentence-based model distort the meaning of the original text, presumably leading the
reader to make wrong inferences. For example, in the second document (Sentence ILP
version) one infers that the victim was urged to report the incident. Moreover, important
information is often omitted, for example, that the victim was indeed raped or that the
strike would be damaging not only to the company but also to its staff (see the Sentence
ILP version in the first document).
8. Conclusions and Future Work
In this article we proposed a novel method for automatic sentence compression. Central
in our approach is the use of discourse-level information, which we argue is an impor-
tant prerequisite for document (as opposed to sentence) compression. Our model uses
integer linear programming for inferring globally optimal compressions in the presence
of linguistically motivated constraints. Our discourse constraints aim to capture local
coherence and are inspired by Centering Theory and lexical chains. We showed that our
436
Clarke and Lapata Discourse Constraints for Document Compression
Improvements in certain allowances were made, described as divisive by
the unions, but the company has refused to compromise on a reduction in
the shorter working week. Ford dismissed an immediate meeting with the
unions but did not rule out talks after Christmas. It said that a strike would
be damaging to the company and to its staff. Production closed down at Ford
last night for the Christmas period. Plants will open again on January 2.D
is
co
u
rs
e
IL
P
Improvements in certain allowances were made, described as divisive by
the unions, but the company has refused to compromise on a reduction in
the shorter working week. Ford dismissed an immediate meeting with the
unions but did not rule out talks after Christmas. It said that a strike would
be damaging to the company and to its staff. Production closed down at Ford
last night for the Christmas period. Plants will open again on January 2.S
en
te
n
ce
IL
P
He threatened her by forcing his truncheon under her chin and then raped
her. She said he only refrained from inserting his truncheon into her, after she
begged him not to. Afterwards he told her not to report the incident because
he could have her ?nicked? for soliciting. She did not report it because she
did not think she would be believed. Police investigated after an anonymous
report.D
is
co
u
rs
e
IL
P
He threatened her by forcing his truncheon under her chin and then raped
her. She said he only refrained from inserting his truncheon into her, after she
begged him not to. Afterwards he told her not to report the incident because
he could have her ?nicked? for soliciting . She did not report it because she
did not think she would be believed. Police investigated after an anonymous
report.S
en
te
n
ce
IL
P
Figure 5
Output of Discourse and Sentence ILP systems on two test documents. Words that are stricken
out have been dropped.
model can be successfully employed to produce compressed documents that preserve
most of the original core content.
Our results confirm the conventional wisdom that discourse-level information is
helpful in summarization. We also show that this type of information can be identified
robustly in free text. Our experiments focused primarily on local discourse structure us-
ing two complementary representations. Centering tends to produce more annotations
since it tries to identify a center in every sentence. Lexical chains tend to provide more
general information, such as the major topics in a document. Due to their approximate
nature, there is no one representation that is uniquely suited to the compression task.
Rather, it is the synergy between lexical chains and centering that brings improvements.
The discourse annotations proposed here are not specific to our model. They could
be easily translated into features and incorporated into discriminative modeling par-
adigms (e.g., Nguyen et al 2004; McDonald 2006; Cohn and Lapata 2009). The same
is true for the Q&A evaluation paradigm employed in our experiments. It could be
straightforwardly adapted to assess the information content of shorter summaries and
potentially used to perform large-scale comparisons within and across systems.
Our approach differs from most summarization work in that our summaries are
fairly long. However, we believe this is the first step to understanding how com-
pression can help summarization. An obvious extension would be to interface our
437
Computational Linguistics Volume 36, Number 3
compression model with sentence extraction (see Martins and Smith [2009] for an ILP
formulation of a model that jointly performs sentence extraction and compression,
without, however, taking discourse level information into account). The discourse
annotations can help guide the extraction method into selecting topically related sen-
tences which can consequently be compressed together. More generally, formulating the
summarization process in the ILP framework outlined here would allow the integration
of varied and sometimes conflicting constraints during summary generation. Examples
include the summary length, and whether it is coherent, grammatical, or repetitive. Ad-
ditional flexibility can be introduced by changing some of the constraints from hard to
soft (as we did with the compression rate constraints), although determining the penalty
for constraint violation manually using prior knowledge is a non-trivial task (Chang,
Ratinov, and Roth 2007) and automatically learning the constraint penalty results in a
harder learning problem. Importantly, under the ILP formulation such constraints can
be explicitly encoded and applied during inference while finding a globally optimal
solution.
Acknowledgments
We are grateful to Ryan McDonald for his
help with the re-implementation of his
system, and our annotators Vasilis Karaiskos
and Sarah Luger. Thanks to Alex Lascarides,
Sebastian Riedel, and Bonnie Webber for
insightful comments and suggestions, and to
the anonymous referees whose feedback
helped to substantially improve the present
article. Lapata acknowledges the support of
EPSRC (grant GR/T04540/01).
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Computer and System
Sciences, 3:37?56.
Barzilay, R. and M. Elhadad. 1997. Using
lexical chains for text summarization. In
Proceedings of the ACL-97 Intelligent Scalable
Text Summarization Workshop, pages 10?17,
Madrid.
Barzilay, Regina and Mirella Lapata. 2006.
Aggregation via set partitioning for
natural language generation. In Proceedings
of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 359?366, New York, NY.
Barzilay, Regina and Mirella Lapata. 2008.
Modeling local coherence: An entity-based
approach. Computational Linguistics,
34(1):1?34.
Boguraev, Branimir and Chris Kennedy.
1997. Salience-based content
characterization of text documents. In
Proceedings of the ACL?97/EACL?97
Workshop on Intelligent Scalable Text
Summarization, pages 2?9, Madrid.
Brin, Sergey and Michael Page. 1998.
Anatomy of a large-scale hypertextual
Web search engine. In Proceedings of the
7th Conference on World Wide Web,
pages 107?117, Brisbane.
Briscoe, E. J. and J. Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC?2002), pages 1499?1504,
Las Palmas.
Carlson, Lynn, John M. Conroy, Daniel
Marcu, Dianne P. O?Leary, Mary E.
Okurowski, and Anthony Taylor. 2001. An
empirical study on the relation between
abstracts, extracts, and the discourse
structure of texts. In Proceedings of the
DUC-2001 Workshop on Text Summarization,
New Orleans, LA.
Chang, Ming-Wei, Lev Ratinov, and Dan
Roth. 2007. Guiding semi-supervision with
constraint-driven learning. In Proceedings
of the 22nd International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 280?287, Prague.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st North American Annual Meeting of the
Association for Computational Linguistics,
pages 132?139, Seattle, WA.
Clarke, James. 2008. Global Inference for
Sentence Compression: An Integer Linear
Programming Approach. Ph.D. thesis,
University of Edinburgh.
Clarke, James and Mirella Lapata. 2006.
Models for sentence compression: A
comparison across domains, training
requirements and evaluation measures.
In Proceedings of the 21st International
438
Clarke and Lapata Discourse Constraints for Document Compression
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 377?384,
Sydney.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:399?429.
Cohn, Trevor and Mirella Lapata. 2009.
Sentence compression as tree transduction.
Journal of Artificial Intelligence Research,
34:637?674.
Corston-Oliver, Simon. 2001. Text
compaction for display on very small
screens. In Proceedings of the NAACL
Workshop on Automatic Summarization,
pages 89?98, Pittsburgh, PA.
Corston-Oliver, Simon H. 1998. Computing
representations of the structure of written
discourse. Technical Report MSR-TR-98-15,
Microsoft Research, Redmond, WA.
Crammer, Koby and Yoram Singer. 2003.
Ultraconservative online algorithms for
multiclass problems. Journal of Machine
Learning Research, 3:951?991.
Daume? III, Hal and Daniel Marcu. 2002.
A noisy-channel model for document
compression. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 449?456,
Philadelphia, PA.
Denis, Pascal and Jason Baldridge. 2007.
Joint determination of anaphoricity and
coreference resolution using integer
programming. In Proceedings of Human
Language Technologies 2007: The Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 236?243, Rochester, NY.
Dras, Mark. 1997. Reluctant paraphrase:
Textual restructuring under an
optimisation model. In Proceedings of the
Fifth Biannual Meeting of the Pacific
Association for Computational Linguistics,
pages 98?104, Ohme.
Endres-Niggemeyer, Brigitte. 1998.
Summarising Information. Springer, Berlin.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Database. MIT Press,
Cambridge, MA.
Galley, Michel and Kathleen McKeown.
2003. Improving word sense disambiguation
in lexical chaining. In Proceedings of 18th
International Joint Conference on Artificial
Intelligence (IJCAI?03), pages 1486?1488,
Acapulco, Mexico.
Galley, Michel and Kathleen McKeown.
2007. Lexicalized Markov grammars for
sentence compression. In Proceedings of
Human Language Technologies 2007: The
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 180?187, Rochester, NY.
Grefenstette, Gregory. 1998. Producing
Intelligent Telegraphic Text Reduction to
Provide an Audio Scanning Service for the
Blind. In Proceedings of the AAAI Symposium
on Intelligent Text Summarization,
pages 111?117, Stanford, CA.
Grosz, Barbara J., Scott Weinstein, and
Aravind K. Joshi. 1995. Centering: a
framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Halliday, M. A. K. and Ruqaiya Hasan.
1976. Cohesion in English. Longman,
London.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of
context for the detection and correction
of malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Database.
MIT Press, Cambridge, MA,
pages 305?332.
Hori, Chiori and Sadaoki Furui. 2004.
Speech summarization: An approach
through word extraction and a method
for evaluation. IEICE Transactions on
Information and Systems, E87-D(1):15?25, 1.
Jing, Hongyan. 2000. Sentence reduction
for automatic text summarization. In
Proceedings of the 6th conference on
Applied Natural Language Processing,
pages 310?315, Seattle, WA.
Kibble, Rodger and Richard Power. 2004.
Optimising referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Knight, Kevin and Daniel Marcu. 2002.
Summarization beyond sentence
extraction: a probabilistic approach to
sentence compression. Artificial Intelligence,
139(1):91?107.
Kupiec, Julian, Jan O. Pedersen, and Francine
Chen. 1995. A trainable document
summarizer. In Proceedings of SIGIR-95,
pages 68?73, Seattle, WA.
Lin, Chin-Yew. 2003. Improving
summarization performance by sentence
compression?A pilot study. In Proceedings
of the 6th International Workshop on
Information Retrieval with Asian Languages,
pages 1?8, Sapporo.
Lin, Dekang. 2001. LaTaT: Language and text
analysis tools. In Proceedings of the first
Human Language Technology Conference,
pages 222?227, San Francisco, CA.
439
Computational Linguistics Volume 36, Number 3
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins,
Amsterdam.
Mani, Inderjeet, The?re`se Firmin, David
House, Gary Klein, Beth Sundheim, and
Lynette Hirschman. 2002. The TIPSTER
SUMMAC Text Summarization
Evaluation. Natural Language Engineering,
8:43?68.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 558?565,
College Park, MD.
Mann, William C. and Sandra A. Thompson.
1988. Rhetorical structure theory: Toward a
functional theory of text organization. Text,
8(3):243?281.
Marciniak, Tomasz and Michael Strube.
2005. Beyond the pipeline: Discrete
optimization in NLP. In Proceedings of
the Ninth Conference on Computational
Natural Language Learning (CoNLL?2005),
pages 136?143, Ann Arbor, MI.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization. The
MIT Press, Cambridge, MA.
Martins, Andre? and Noah A. Smith. 2009.
Summarization with a joint model for
sentence extraction and compression. In
Proceedings of the Workshop on Integer Linear
Programming for Natural Language
Processing, pages 1?9, Boulder, CO.
Martins, Andre?, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 342?350, Suntec.
McDonald, Ryan. 2006. Discriminative
sentence compression with soft syntactic
constraints. In Proceedings of the
11th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 297?304, Trento.
Miltsakaki, Eleni and Karen Kukich. 2000.
The role of centering theory?s rough-shift
in the teaching and evaluation of writing
skills. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 408?415, Hong Kong.
Morris, A., G. Kasper, and D. Adams. 1992.
The effects and limitations of automated
text condensing on reading
comprehension performance. Information
Systems Research, 3(1):17?35.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17(1):21?48.
Nguyen, Minh Le, Akira Shimazu, Susumu
Horiguchi, Tu Bao Ho, and Masaru
Fukushi. 2004. Probabilistic sentence
reduction using support vector machines.
In Proceedings of the 20th International
Conference on Computational Linguistics,
pages 743?749, Geneva.
Olivers, S. H. and W. B. Dolan. 1999. Less is
more; eliminating index terms from
subordinate clauses. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 349?356,
College Park, MD.
Ono, Kenji, Kazuo Sumita, and Seiji Miike.
1994. Abstract generation based on
rhetorical structure extraction. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 344?348, Kyoto.
Ora?san, Constantin. 2003. An evolutionary
approach for improving the quality of
automatic summaries. In ACL Workshop on
Multilingual Summarization and Question
Answering, pages 37?45, Sapporo, Japan.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: a parametric theory and
its instantiations. Computational Linguistics,
30(3):309?363.
Press, William H., Saul A. Teukolsky,
William T. Vetterling, and Brian P.
Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge
University Press, Cambridge, UK.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of the
20th International Conference on
Computational Linguistics, pages 1346?1352,
Geneva.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Riezler, Stefan, Tracy H. King, Richard
Crouch, and Annie Zaenen. 2003.
Statistical sentence condensation using
ambiguity packing and stochastic
disambiguation methods for
lexical-functional grammar. In Proceedings
of the 2003 Human Language Technology
Conference of the North American Chapter of
440
Clarke and Lapata Discourse Constraints for Document Compression
the Association for Computational Linguistics,
pages 118?125, Edmonton.
Roark, Brian. 2001. Probabilistic top?down
parsing and language modeling.
Computational Linguistics, 27(2):249?276.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press,
New York, pages 47?73.
Sjorochod?ko, E. F. 1972. Adaptive method
for automatic abstracting and indexing. In
Information Processing 71: Proceedings of the
IFIP Congress 71, pages 1179?1182,
Amsterdam.
Tetreault, Joel R. 2001. A corpus-based
evaluation of centering and pronoun
resolution. Computational Linguistics,
27(4):507?520.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles?
Experiments with relevance and rhetorical
status. Computational Linguistics,
28(4):409?446.
Turner, Jenine and Eugene Charniak.
2005. Supervised and unsupervised
learning for sentence compression. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics, pages 290?297, Ann
Arbor, MI.
Vanderbei, Robert J. 2001. Linear
Programming: Foundations and Extensions.
Kluwer Academic Publishers, Boston,
2nd edition.
Walker, Marilyn, Aravind Joshi, and
Ellen Prince. 1998. Centering in
naturally occurring discourse: An
overview. In Centering Theory in
Discourse. Oxford University Press,
Oxford, pages 1?28.
Winston, Wayne L. and Munirpallam
Venkataramanan. 2003. Introduction to
Mathematical Programming. Brooks/Cole,
Independence, KY.
Wolf, Florian and Edward Gibson. 2004.
Paragraph-, word-, and coherence-based
approaches to sentence ranking: A
comparison of algorithm and human
performance. In Proceedings of the
42nd Meeting of the Association for
Computational Linguistics, pages 383?390,
Barcelona.
Zajic, David, Bonnie J. Dorr, Jimmy J. Lin,
and Richard M. Schwartz. 2007.
Multi-candidate reduction: Sentence
compression as a tool for document
summarization tasks. Information
Processing and Management,
43(6):1549?1570.
441

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1486?1495,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Confidence Driven Unsupervised Semantic Parsing
Dan Goldwasser ? Roi Reichart ? James Clarke ? Dan Roth ?
?Department of Computer Science, University of Illinois at Urbana-Champaign
{goldwas1,clarkeje,danr}@illinois.edu
?Computer Science and Artificial Intelligence Laboratory, MIT
roiri@csail.mit.edu
Abstract
Current approaches for semantic parsing take
a supervised approach requiring a consider-
able amount of training data which is expen-
sive and difficult to obtain. This supervision
bottleneck is one of the major difficulties in
scaling up semantic parsing.
We argue that a semantic parser can be trained
effectively without annotated data, and in-
troduce an unsupervised learning algorithm.
The algorithm takes a self training approach
driven by confidence estimation. Evaluated
over Geoquery, a standard dataset for this
task, our system achieved 66% accuracy, com-
pared to 80% of its fully supervised counter-
part, demonstrating the promise of unsuper-
vised approaches for this task.
1 Introduction
Semantic parsing, the ability to transform Natural
Language (NL) input into a formal Meaning Repre-
sentation (MR), is one of the longest standing goals
of natural language processing. The importance of
the problem stems from both theoretical and practi-
cal reasons, as the ability to convert NL into a formal
MR has countless applications.
The term semantic parsing has been used ambigu-
ously to refer to several semantic tasks (e.g., se-
mantic role labeling). We follow the most common
definition of this task: finding a mapping between
NL input and its interpretation expressed in a well-
defined formal MR language. Unlike shallow se-
mantic analysis tasks, the output of a semantic parser
is complete and unambiguous to the extent it can be
understood or even executed by a computer system.
Current approaches for this task take a data driven
approach (Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007), in which the learning algorithm is
given a set of NL sentences as input and their cor-
responding MR, and learns a statistical semantic
parser ? a set of parameterized rules mapping lex-
ical items and syntactic patterns to their MR. Given
a sentence, these rules are applied recursively to de-
rive the most probable interpretation.
Since semantic interpretation is limited to the syn-
tactic patterns observed in the training data, in or-
der to work well these approaches require consider-
able amounts of annotated data. Unfortunately an-
notating sentences with their MR is a time consum-
ing task which requires specialized domain knowl-
edge and therefore minimizing the supervision ef-
fort is one of the key challenges in scaling semantic
parsers.
In this work we present the first unsupervised
approach for this task. Our model compensates
for the lack of training data by employing a self
training protocol based on identifying high confi-
dence self labeled examples and using them to re-
train the model. We base our approach on a sim-
ple observation: semantic parsing is a difficult struc-
tured prediction task, which requires learning a com-
plex model, however identifying good predictions
can be done with a far simpler model capturing re-
peating patterns in the predicted data. We present
several simple, yet highly effective confidence mea-
sures capturing such patterns, and show how to use
them to train a semantic parser without manually an-
notated sentences.
Our basic premise, that predictions with high con-
fidence score are of high quality, is further used to
improve the performance of the unsupervised train-
1486
ing procedure. Our learning algorithm takes an EM-
like iterative approach, in which the predictions of
the previous stage are used to bias the model. While
this basic scheme was successfully applied to many
unsupervised tasks, it is known to converge to a
sub optimal point. We show that by using confi-
dence estimation as a proxy for the model?s pre-
diction quality, the learning algorithm can identify
a better model compared to the default convergence
criterion.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), consist-
ing of natural language questions and their prolog
interpretations used to query a database consisting
of U.S. geographical information. Our experimental
results show that using our approach we are able to
train a good semantic parser without annotated data,
and that using a confidence score to identify good
models results in a significant performance improve-
ment.
2 Semantic Parsing
We formulate semantic parsing as a structured pre-
diction problem, mapping a NL input sentence (de-
noted x), to its highest ranking MR (denoted z). In
order to correctly parametrize and weight the pos-
sible outputs, the decision relies on an intermediate
representation: an alignment between textual frag-
ments and their meaning representation (denoted y).
Fig. 1 describes a concrete example of this termi-
nology. In our experiments the input sentences x
are natural language queries about U.S. geography
taken from the Geoquery dataset. The meaning rep-
resentation z is a formal language database query,
this output representation language is described in
Sec. 2.1.
The prediction function, mapping a sentence to its
corresponding MR, is formalized as follows:
z? = Fw(x) = arg max
y?Y,z?Z
wT?(x,y, z) (1)
Where ? is a feature function defined over an input
sentence x, alignment y and output z. The weight
vector w contains the model?s parameters, whose
values are determined by the learning process.
We refer to the arg max above as the inference
problem. Given an input sentence, solving this in-
How many states does the Colorado river run through? 
count( state( traverse( river( const(colorado))))
x 
z 
y 
Figure 1: Example of an input sentence (x), meaning rep-
resentation (z) and the alignment between the two (y) for
the Geoquery domain
ference problem based on ? and w is what com-
promises our semantic parser. In practice the pars-
ing decision is decomposed into smaller decisions
(Sec. 2.2). Sec. 4 provides more details about the
feature representation and inference procedure used.
Current approaches obtain w using annotated
data, typically consisting of (x, z) pairs. In Sec. 3 we
describe our unsupervised learning procedure, that is
how to obtain w without annotated data.
2.1 Target Meaning Representation
The output of the semantic parser is a logical for-
mula, grounding the semantics of the input sen-
tence in the domain language (i.e., the Geoquery
domain). We use a subset of first order logic con-
sisting of typed constants (corresponding to specific
states, etc.) and functions, which capture relations
between domains entities and properties of entities
(e.g., population : E ? N ). The seman-
tics of the input sentence is constructed via func-
tional composition, done by the substitution oper-
ator. For example, given the function next to(x)
and the expression const(texas), substitution
replaces the occurrence of the free variable x
with the expression, resulting in a new formula:
next to(const(texas)). For further details
we refer the reader to (Zelle and Mooney, 1996).
2.2 Semantic Parsing Decisions
The inference problem described in Eq. 1 selects the
top ranking output formula. In practice this decision
is decomposed into smaller decisions, capturing lo-
cal mapping of input tokens to logical fragments and
their composition into larger fragments. These deci-
sions are further decomposed into a feature repre-
sentation, described in Sec. 4.
The first type of decisions are encoded directly by
the alignment (y) between the input tokens and their
corresponding predicates. We refer to these as first
1487
order decisions. The pairs connected by the align-
ment (y) in Fig. 1 are examples of such decisions.
The final output structure z is constructed by
composing individual predicates into a complete
formula. For example, consider the formula pre-
sented in Fig. 1: river( const(colorado))
is a composition of two predicates river and
const(colorado). We refer to the composition
of two predicates, associated with their respective
input tokens, as second order decisions.
In order to formulate these decisions, we intro-
duce the following notation. c is a constituent in the
input sentence x and D is the set of all function and
constant symbols in the domain. The alignment y is
a set of mappings between constituents and symbols
in the domain y = {(c, s)} where s ? D.
We denote by si the i-th output predicate compo-
sition in z, by si?1(si) the composition of the (i?1)-
th predicate on the i-th predicate and by y(si) the in-
put word corresponding to that predicate according
to the alignment y.
3 Unsupervised Semantic Parsing
Our learning framework takes a self training ap-
proach in which the learner is iteratively trained over
its own predictions. Successful application of this
approach depends heavily on two important factors
- how to select high quality examples to train the
model on, and how to define the learning objective
so that learning can halt once a good model is found.
Both of these questions are trivially answered
when working in a supervised setting: by using the
labeled data for training the model, and defining the
learning objective with respect to the annotated data
(for example, loss-minimization in the supervised
version of our system).
In this work we suggest to address both of the
above concerns by approximating the quality of
the model?s predictions using a confidence measure
computed over the statistics of the self generated
predictions. Output structures which fall close to the
center of mass of these statistics will receive a high
confidence score.
The first issue is addressed by using examples as-
signed a high confidence score to train the model,
acting as labeled examples.
We also note that since the confidence score pro-
vides a good indication for the model?s prediction
performance, it can be used to approximate the over-
all model performance, by observing the model?s to-
tal confidence score over all its predictions. This
allows us to set a performance driven goal for our
learning process - return the model maximizing the
confidence score over all predictions. We describe
the details of integrating the confidence score into
the learning framework in Sec. 3.1.
Although using the model?s prediction score (i.e.,
wT?(x,y, z)) as an indication of correctness is a
natural choice, we argue and show empirically, that
unsupervised learning driven by confidence estima-
tion results in a better performing model. This
empirical behavior also has theoretical justification:
training the model using examples selected accord-
ing to the model?s parameters (i.e., the top rank-
ing structures) may not generalize much further be-
yond the existing model, as the training examples
will simply reinforce the existing model. The statis-
tics used for confidence estimation are different than
those used by the model to create the output struc-
tures, and can therefore capture additional informa-
tion unobserved by the prediction model. This as-
sumption is based on the well established idea of
multi-view learning, applied successfully to many
NL applications (Blum and Mitchell, 1998; Collins
and Singer, 1999). According to this idea if two
models use different views of the data, each of them
can enhance the learning process of the other.
The success of our learning procedure hinges
on finding good confidence measures, whose confi-
dence prediction correlates well with the true quality
of the prediction. The ability of unsupervised confi-
dence estimation to provide high quality confidence
predictions can be explained by the observation that
prominent prediction patterns are more likely to be
correct. If a non-random model produces a predic-
tion pattern multiple times it is likely to be an in-
dication of an underlying phenomenon in the data,
and therefore more likely to be correct. Our specific
choice of confidence measures is guided by the intu-
ition that unlike structure prediction (i.e., solving the
inference problem) which requires taking statistics
over complex and intricate patterns, identifying high
quality predictions can be done using much simpler
patterns that are significantly easier to capture.
In the reminder of this section we describe our
1488
Algorithm 1 Unsupervised Confidence driven
Learning
Input: Sentences {xl}Nl=1,
initial weight vector w
1: define Confidence : X ? Y ? Z ? R,
i = 0, Si = ?
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = arg maxy,zw
T?(xl,y, z)
5: Si = Si ? {xl, y?, z?}
6: end for
7: Confidence = compute confidence statistics
8: Sconfi = select from Si using Confidence
9: wi ? Learn(?iS
conf
i )
10: i = i+ 1
11: until Sconfi has no new unique examples
12: best = arg maxi(
?
s?Si
Confidence(s))/|S|
13: return wbest
learning approach. We begin by introducing the
overall learning framework (Sec. 3.1), we then ex-
plain the rational behind confidence estimation over
self-generated data and introduce the confidence
measures used in our experiments (Sec. 3.2). We
conclude with a description of the specific learning
algorithms used for updating the model (Sec. 3.3).
3.1 Unsupervised Confidence-Driven Learning
Our learning framework works in an EM-like
manner, iterating between two stages: making pre-
dictions based on its current set of parameters and
then retraining the model using a subset of the pre-
dictions, assigned high confidence. The learning
process ?discovers? new high confidence training
examples to add to its training set over multiple it-
erations, and converges when the model no longer
adds new training examples.
While this is a natural convergence criterion, it
provides no performance guarantees, and in practice
it is very likely that the quality of the model (i.e., its
performance) fluctuates during the learning process.
We follow the observation that confidence estima-
tion can be used to approximate the performance of
the entire model and return the model with the high-
est overall prediction confidence.
We describe this algorithmic framework in detail
in Alg. 1. Our algorithm takes as input a set of
natural language sentences and a set of parameters
used for making the initial predictions1. The algo-
rithm then iterates between the two stages - predict-
ing the output structure for each sentence (line 4),
and updating the set of parameters (line 9). The
specific learning algorithms used are discussed in
Sec. 3.3. The training examples required for learn-
ing are obtained by selecting high confidence exam-
ples - the algorithm first takes statistics over the cur-
rent predicted set of output structures (line 7), and
then based on these statistics computes a confidence
score for each structure, selecting the top ranked
ones as positive training examples, and if needed,
the bottom ones as negative examples (line 8). The
set of top confidence examples (for either correct or
incorrect prediction), at iteration i of the algorithm,
is denoted Sconfi . The exact nature of the confidence
computation is discussed in Sec. 3.2.
The algorithm iterates between these two stages,
at each iteration it adds more self-annotated exam-
ples to its training set, learning therefore converges
when no new examples are added (line 11). The al-
gorithm keeps track of the models it trained at each
stage throughout this process, and returns the one
with the highest averaged overall confidence score
(lines 12-13). At each stage, the overall confidence
score is computed by averaging over all the confi-
dence scores of the predictions made at that stage.
3.2 Unsupervised Confidence Estimation
Confidence estimation is calculated over a batch of
input (x) - output (z) pairs. Each pair decomposes
into smaller first order and second order decisions
(defined Sec. 2.2). Confidence estimation is done by
computing the statistics of these decisions, over the
entire set of predicted structures. In the rest of this
section we introduce the confidence measures used
by our system.
Translation Model The first approach essentially
constructs a simplified translation model, capturing
word-to-predicate mapping patterns. This can be
considered as an abstraction of the prediction model:
we collapse the intricate feature representation into
1Since we commit to the max-score output prediction, rather
than summing over all possibilities, we require a reasonable ini-
tialization point. We initialized the weight vector using simple,
straight-forward heuristics described in Sec. 5.
1489
high level decisions and take statistics over these de-
cisions. Since it takes statistics over considerably
less variables than the actual prediction model, we
expect this model to make reliable confidence pre-
dictions. We consider two variations of this ap-
proach, the first constructs a unigram model over the
first order decisions and the second a bigram model
over the second order decisions. Formally, given a
set of predicted structures we define the following
confidence scores:
Unigram Score:
p(z|x) =
|z|?
i=1
p(si|y(si))
Bigram Score:
p(z|x) =
|z|?
i=1
p(si?1(si)|y(si?1), y(si))
Structural Proportion Unlike the first approach
which decomposes the predicted structure into in-
dividual decisions, this approach approximates the
model?s performance by observing global properties
of the structure. We take statistics over the propor-
tion between the number of predicates in z and the
number of words in x.
Given a set of structure predictions S, we com-
pute this proportion for each structure (denoted as
Prop(x, z)) and calculate the average proportion
over the entire set (denoted as AvProp(S)). The
confidence score assigned to a given structure (x,y)
is simply the difference between its proportion and
the averaged proportion, or formally
PropScore(S, (x, z)) = AvProp(S)?Prop(x, z)
This measure captures the global complexity of the
predicted structure and penalizes structures which
are too complex (high negative values) or too sim-
plistic (high positive values).
Combined The two approaches defined above
capture different views of the data, a natural question
is then - can these two measures be combined to pro-
vide a more powerful estimation? We suggest a third
approach which combines the first two approaches.
It first uses the score produced by the latter approach
to filter out unlikely candidates, and then ranks the
remaining ones with the former approach and selects
those with the highest rank.
3.3 Learning Algorithms
Given a set of self generated structures, the param-
eter vector can be updated (line 9 in Alg. 1). We
consider two learning algorithm for this purpose.
The first is a binary learning algorithm, which
considers learning as a classification problem, that
is finding a set of weights w that can best sepa-
rate correct from incorrect structures. The algo-
rithm decomposes each predicted formula and its
corresponding input sentence into a feature vector
?(x,y, z) normalized by the size of the input sen-
tence |x|, and assigns a binary label to this vector2.
The learning process is defined over both positive
and negative training examples. To accommodate
that we modify line 8 in Alg. 1, and use the con-
fidence score to select the top ranking examples as
positive examples, and the bottom ranking examples
as negative examples. We use a linear kernel SVM
with squared-hinge loss as the underlying learning
algorithm.
The second is a structured learning algorithm
which considers learning as a ranking problem, i.e.,
finding a set of weights w such that the ?gold struc-
ture? will be ranked on top, preferably by a large
margin to allow generalization.The structured learn-
ing algorithm can directly use the top ranking pre-
dictions of the model (line 8 in Alg. 1) as training
data. In this case the underlying algorithm is a struc-
tural SVM with squared-hinge loss, using hamming
distance as the distance function. We use the cutting-
plane method to efficiently optimize the learning
process? objective function.
4 Model
Semantic parsing as formulated in Eq. 1 is an in-
ference procedure selecting the top ranked output
logical formula. We follow the inference approach
in (Roth and Yih, 2007; Clarke et al, 2010) and
formalize this process as an Integer Linear Program
(ILP). Due to space consideration we provide a brief
description, and refer the reader to that paper for
more details.
2Without normalization longer sentences would have more
influence on binary learning problem. Normalization is there-
fore required to ensure that each sentence contributes equally to
the binary learning problem regardless of its length.
1490
4.1 Inference
The inference decision (Eq. 1) is decomposed into
smaller decisions, capturing mapping of input to-
kens to logical fragments (first order) and their com-
position into larger fragments (second order). We
encode a first-order decision as ?cs, a binary vari-
able indicating that constituent c is aligned with the
logical symbol s. A second-order decision ?cs,dt, is
encoded as a binary variable indicating that the sym-
bol t (associated with constituent d) is an argument
of a function s (associated with constituent c). We
frame the inference problem over these decisions:
Fw(x) = arg max
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?w
T?2(x, c, s, d, t) (2)
We restrict the possible assignments to the deci-
sion variables, forcing the resulting output formula
to be syntactically legal, for example by restricting
active ?-variables to be type consistent, and force
the resulting functional composition to be acyclic.
We take advantage of the flexible ILP framework,
and encode these restrictions as global constraints
over Eq. 2. We refer the reader to (Clarke et al,
2010) for a full description of the constraints used.
4.2 Features
The inference problem defined in Eq. (2) uses two
feature functions: ?1 and ?2.
First-order decision features ?1 Determining if
a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.3 Existing ap-
proaches rely on annotated data to extend the lexi-
con. Instead we rely on external knowledge (Miller
et al, 1990) and add features which measure the lex-
ical similarity between a constituent and a logical
symbol?s surface forms (as defined by the lexicon).
3The lexicon contains on average 1.42 words per function
and 1.07 words per constant.
Model Description
INITIAL MODEL Manually set weights (Sec. 5.1)
PRED. SCORE normalized prediction (Sec. 5.1)
ALL EXAMPLES All top structures (Sec. 5.1)
UNIGRAM Unigram score (Sec. 3.2)
BIGRAM Bigram score (Sec. 3.2)
PROPORTION Words-predicate prop (Sec. 3.2)
COMBINED Combined estimators (Sec. 3.2)
RESPONSE BASED Supervised (binary) (Sec. 5.1)
SUPERVISED Fully Supervised (Sec. 5.1)
Table 1: Compared systems and naming conventions.
Second-order decision features ?2 Second order
decisions rely on syntactic information. We use
the dependency tree of the input sentence. Given
a second-order decision ?cs,dt, the dependency fea-
ture takes the normalized distance between the head
words in the constituents c and d. In addition, a set
of features indicate which logical symbols are usu-
ally composed together, without considering their
alignment to the text.
5 Experiments
In this section we describe our experimental evalua-
tion. We compare several confidence measures and
analyze their properties. Tab. 1 defines the naming
conventions used throughout this section to refer to
the different models we evaluated. We begin by de-
scribing our experimental setup and then proceed to
describe the experiments and their results. For the
sake of clarity we focus on the best performing mod-
els (COMBINED using BIGRAM and PROPORTION)
first and discuss other models later in the section.
5.1 Experimental Settings
In all our experiments we used the Geoquery
dataset (Zelle and Mooney, 1996), consisting of U.S.
geography NL questions and their corresponding
Prolog logical MR. We used the data split described
in (Clarke et al, 2010), consisting of 250 queries for
evaluation purposes. We compared our system to
several supervised models, which were trained us-
ing a disjoint set of queries. Our learning system
had access only to the NL questions, and the log-
ical forms were only used to evaluate the system?s
performance. We report the proportion of correct
structures (accuracy). Note that this evaluation cor-
1491
responds to the 0/1 loss over the predicted structures.
Initialization Our learning framework requires an
initial weight vector as input. We use a straight for-
ward heuristic and provide uniform positive weights
to three features. This approach is similar in spirit
to previous works (Clarke et al, 2010; Zettlemoyer
and Collins, 2007). We refer to this system as INI-
TIAL MODEL throughout this section.
Competing Systems We compared our system to
several other systems:
(1) PRED. SCORE: An unsupervised frame-
work using the model?s internal prediction score
(wT?(x,y, z)) for confidence estimation.
(2) ALL EXAMPLES: Treating all predicted struc-
tures as correct, i.e., at each iteration the model is
trained over all the predictions it made. The re-
ported score was obtained by selecting the model at
the training iteration with the highest overall confi-
dence score (see line 12 in Alg. 1).
(3) RESPONSE BASED: A natural upper bound to
our framework is the approach used in (Clarke et al,
2010). While our approach is based on assessing
the correctness os the model?s predictions according
to unsupervised confidence estimation, their frame-
work is provided with external supervision for these
decisions, indicating if the predicted structures are
correct.
(4) SUPERVISED: A fully supervised framework
trained over 250 (x, z) pairs using structured SVM.
5.2 Results
Our experiments aim to clarify three key points:
(1) Can a semantic parser indeed be trained with-
out any form of external supervision? this is our
key question, as this is the first attempt to approach
this task with an unsupervised learning protocol.4 In
order to answer it, we report the overall performance
of our system in Tab. 2.
The manually constructed model INITIALMODEL
achieves a performance of 0.22. We can expect
learning to improve on this baseline. We com-
pare three self-trained systems, ALL EXAMPLES,
PREDICTIONSCORE and COMBINED, which differ
4While unsupervised learning for various semantic tasks has
been widely discussed, this is the first attempt to tackle this task.
We refer the reader to Sec. 6 for further discussion of this point.
in their sample selection strategy, but all use con-
fidence estimation for selecting the final seman-
tic parsing model. The ALL EXAMPLES approach
achieves an accuracy score of 0.656. PREDICTION-
SCORE only achieves a performance of 0.164 us-
ing the binary learning algorithm and 0.348 us-
ing the structured learning algorithm. Finally, our
confidence-driven technique COMBINED achieved a
score of 0.536 for the binary case and 0.664 for the
structured case, the best performing models in both
cases. As expected, the supervised systems RE-
SPONSE BASED and SUPERVISED achieve the best
performance.
These results show that training the model with
training examples selected carefully will improve
learning - as the best performance is achieved with
perfect knowledge of the predictions correctness
(RESPONSE BASED). Interestingly the difference
between the structured version of our system and
that of RESPONSE BASED is only 0.07, suggesting
that we can recover the binary feedback signal with
high precision. The low performance of the PRE-
DICTIONSCORE model is also not surprising, and it
demonstrates one of the key principles in confidence
estimation - the score should be comparable across
predictions done over different inputs, and not the
same input, as done in PREDICTIONSCORE model.
(2) How does confidence driven sample selection
contribute to the learning process? Comparing
the systems driven by confidence sample-selection
to the ALL EXAMPLES approach uncovers an inter-
esting tradeoff between training with more (noisy)
data and selectively training the system with higher
quality examples. We argue that carefully select-
ing high quality training examples will result in bet-
ter performance. The empirical results indeed sup-
port our argument, as the best performing model
(RESPONSE BASED) is achieved by sample selec-
tion with perfect knowledge of prediction correct-
ness. The confidence-based sample selection system
(COMBINED) is the best performing system out of
all the self-trained systems. Nonetheless, the ALL
EXAMPLES strategy performs well when compared
to COMBINED, justifying a closer look at that aspect
of our system.
We argue that different confidence measures cap-
ture different properties of the data, and hypothe-
1492
size that combining their scores will improve the re-
sulting model. In Tab. 3 we compare the results of
the COMBINED measure to the results of its individ-
ual components - PROPORTION and BIGRAM. We
compare these results both when using the binary
and structured learning algorithms. Results show
that using the COMBINED measure leads to an im-
proved performance, better than any of the individ-
ual measures, suggesting that it can effectively ex-
ploit the properties of each confidence measure. Fur-
thermore, COMBINED is the only sample selection
strategy that outperforms ALL EXAMPLES.
(3) Can confidence measures serve as a good
proxy for the model?s performance? In the unsu-
pervised settings we study the learning process may
not converge to an optimal model. We argue that
by selecting the model that maximizes the averaged
confidence score, a better model can be found. We
validate this claim empirically in Tab. 4. We com-
pare the performance of the model selected using
the confidence score to the performance of the fi-
nal model considered by the learning algorithm (see
Sec. 3.1 for details). We also compare it to the best
model achieved in any of the learning iterations.
Since these experiments required running the
learning algorithm many times, we focused on the
binary learning algorithm as it converges consider-
ably faster. In order to focus the evaluation on the
effects of learning, we ignore the initial model gen-
erated manually (INITIAL MODEL) in these exper-
iments. In order to compare models performance
across the different iterations fairly, a uniform scale,
such as UNIGRAM and BIGRAM, is required. In the
case of the COMBINED measure we used the BI-
GRAM measure for performance estimation, since it
is one of its underlying components. In the PRED.
SCORE and PROPORTION models we used both their
confidence prediction, and the simple UNIGRAM
confidence score to evaluate model performance (the
latter appear in parentheses in Tab. 4).
Results show that the over overall confidence
score serves as a reliable proxy for the model perfor-
mance - using UNIGRAM and BIGRAM the frame-
work can select the best performing model, far better
than the performance of the default model to which
the system converged.
Algorithm Supervision Acc.
INITIAL MODEL ? 0.222
SELF-TRAIN: (Structured)
PRED. SCORE ? 0.348
ALL EXAMPLES ? 0.656
COMBINED ? 0.664
SELF-TRAIN: (Binary)
PRED. SCORE ? 0.164
COMBINED ? 0.536
RESPONSE BASED
BINARY 250 (binary) 0.692
STRUCTURED 250 (binary) 0.732
SUPERVISED
STRUCTURED 250 (struct.) 0.804
Table 2: Comparing our Self-trained systems with
Response-based and supervised models. Results show
that our COMBINED approach outperforms all other un-
supervised models.
Algorithm Accuracy
SELF-TRAIN: (Structured)
PROPORTION 0.6
BIGRAM 0.644
COMBINED 0.664
SELF-TRAIN: (Binary)
BIGRAM 0.532
PROPORTION 0.504
COMBINED 0.536
Table 3: Comparing COMBINED to its components BI-
GRAM and PROPORTION. COMBINED results in a better
score than any of its components, suggesting that it can
exploit the properties of each measure effectively.
Algorithm Best Conf. estim. Default
PRED. SCORE 0.164 0.128 (0.164) 0.134
UNIGRAM 0.52 0.52 0.4
BIGRAM 0.532 0.532 0.472
PROPORTION 0.504 0.27 (0.504) 0.44
COMBINED 0.536 0.536 0.328
Table 4: Using confidence to approximate model perfor-
mance. We compare the best result obtained in any of the
learning algorithm iterations (Best), the result obtained
by approximating the best result using the averaged pre-
diction confidence (Conf. estim.) and the result of us-
ing the default convergence criterion (Default). Results
in parentheses are the result of using the UNIGRAM con-
fidence to approximate the model?s performance.
1493
6 Related Work
Semantic parsing has attracted considerable interest
in recent years. Current approaches employ various
machine learning techniques for this task, such as In-
ductive Logic Programming in earlier systems (Zelle
and Mooney, 1996; Tang and Mooney, 2000) and
statistical learning methods in modern ones (Ge and
Mooney, 2005; Nguyen et al, 2006; Wong and
Mooney, 2006; Kate and Mooney, 2006; Zettle-
moyer and Collins, 2005; Zettlemoyer and Collins,
2007; Zettlemoyer and Collins, 2009).
The difficulty of providing the required supervi-
sion motivated learning approaches using weaker
forms of supervision. (Chen and Mooney, 2008;
Liang et al, 2009; Branavan et al, 2009; Titov and
Kozhevnikov, 2010) ground NL in an external world
state directly referenced by the text. The NL input in
our setting is not restricted to such grounded settings
and therefore we cannot exploit this form of supervi-
sion. Recent work (Clarke et al, 2010; Liang et al,
2011) suggest using response-based learning proto-
cols, which alleviate some of the supervision effort.
This work takes an additional step in this direction
and suggest an unsupervised protocol.
Other approaches to unsupervised semantic anal-
ysis (Poon and Domingos, 2009; Titov and Kle-
mentiev, 2011) take a different approach to seman-
tic representation, by clustering semantically equiv-
alent dependency tree fragments, and identifying
their predicate-argument structure. While these ap-
proaches have been applied successfully to semantic
tasks such as question answering, they do not ground
the input in a well defined output language, an essen-
tial component in our task.
Our unsupervised approach follows a self training
protocol (Yarowsky, 1995; McClosky et al, 2006;
Reichart and Rappoport, 2007b) enhanced with con-
straints restricting the output space (Chang et al,
2007; Chang et al, 2009). A Self training proto-
col uses its own predictions for training. We esti-
mate the quality of the predictions and use only high
confidence examples for training. This selection cri-
terion provides an additional view, different than the
one used by the prediction model. Multi-view learn-
ing is a well established idea, implemented in meth-
ods such as co-training (Blum and Mitchell, 1998).
Quality assessment of a learned model output was
explored by many previous works (see (Caruana and
Niculescu-Mizil, 2006) for a survey), and applied
to several NL processing tasks such as syntactic
parsing (Reichart and Rappoport, 2007a; Yates et
al., 2006), machine translation (Ueffing and Ney,
2007), speech (Koo et al, 2001), relation extrac-
tion (Rosenfeld and Feldman, 2007), IE (Culotta and
McCallum, 2004), QA (Chu-Carroll et al, 2003)
and dialog systems (Lin and Weng, 2008).
In addition to sample selection we use confidence
estimation as a way to approximate the overall qual-
ity of the model and use it for model selection. This
use of confidence estimation was explored in (Re-
ichart et al, 2010), to select between models trained
with different random starting points. In this work
we integrate this estimation deeper into the learning
process, thus allowing our training procedure to re-
turn the best performing model.
7 Conclusions
We introduced an unsupervised learning algorithm
for semantic parsing, the first for this task to the best
of our knowledge. To compensate for the lack of
training data we use a self-training protocol, driven
by unsupervised confidence estimation. We demon-
strate empirically that our approach results in a high
preforming semantic parser and show that confi-
dence estimation plays a vital role in this success,
both by identifying good training examples as well
as identifying good over all performance, used to
improve the final model selection.
In future work we hope to further improve un-
supervised semantic parsing performance. Particu-
larly, we intend to explore new approaches for confi-
dence estimation and their usage in the unsupervised
and semi-supervised versions of the task.
Acknowledgments We thank the anonymous re-
viewers for their helpful feedback. This material
is based upon work supported by DARPA under
the Bootstrap Learning Program and Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, AFRL, or the US government.
1494
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In ACL.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised l earning algorithms. In
ICML.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the ACL.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
D. Chen and R. Mooney. 2008. Learning to sportscast: a
test of grounded language acquisition. In ICML.
J. Chu-Carroll, J. Prager K. Czuba, and A. Ittycheriah.
2003. In question answering, two heads are better than
on. In HLT-NAACL.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL, 7.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP?VLC.
A. Culotta and A. McCallum. 2004. Confidence estima-
tion for information extraction. In HLT-NAACL.
R. Ge and R. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In CoNLL.
R. Kate and R. Mooney. 2006. Using string-kernels for
learning semantic parsers. In ACL.
Y. Koo, C. Lee, and B. Juang. 2001. Speech recogni-
tion and utterance verification based on a generalized
confidence score. IEEE Transactions on Speech and
Audio Processing, 9(8):821?832.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
ACL.
P. Liang, M.I. Jordan, and D. Klein. 2011. Deep compo-
sitional semantics from shallow supervision. In ACL.
F. Lin and F. Weng. 2008. Computing confidence scores
for all sub parse trees. In ACL.
D. McClosky, E. Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In HLT-NAACL.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J.
Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured svm ensemble classification
models. In ACL.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In EMNLP.
R. Reichart and A. Rappoport. 2007a. An ensemble
method for selection of high quality parses. In ACL.
R. Reichart and A. Rappoport. 2007b. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
R. Reichart, R. Fattal, and A. Rappoport. 2010. Im-
proved unsupervised pos induction using intrinsic
clustering quality and a zipfian constraint. In CoNLL.
B. Rosenfeld and R. Feldman. 2007. Using corpus statis-
tics on entities to improve semi?supervised relation
extraction from the web. In ACL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construction
of database interfaces: integrating statistical and rela-
tional learning for semantic parsing. In EMNLP.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In ECML.
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In ACL.
I. Titov and M. Kozhevnikov. 2010. Bootstrapping
semantic analyzers from non-contradictory texts. In
ACL.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9?40.
Y.W. Wong and R. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
NAACL.
Y.W. Wong and R. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised method. In ACL.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In EMNLP.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
AAAI.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI.
L. Zettlemoyer and M. Collins. 2007. Online learning of
relaxed CCG grammars for parsing to logical form. In
CoNLL.
L. Zettlemoyer and M. Collins. 2009. Learning context-
dependent mappings from sentences to logical form.
In ACL.
1495
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18?27,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Driving Semantic Parsing from the World?s Response
James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61820
{clarkeje,goldwas1,mchang21,danr}@illinois.edu
Abstract
Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.
1 Introduction
Semantic Parsing, the process of converting text
into a formal meaning representation (MR), is one
of the key challenges in natural language process-
ing. Unlike shallow approaches for semantic in-
terpretation (e.g., semantic role labeling and in-
formation extraction) which often result in an in-
complete or ambiguous interpretation of the natu-
ral language (NL) input, the output of a semantic
parser is a complete meaning representation that
can be executed directly by a computer program.
Semantic parsing has mainly been studied in the
context of providing natural language interfaces
to computer systems. In these settings the target
meaning representation is defined by the seman-
tics of the underlying task. For example, provid-
ing access to databases: a question posed in nat-
ural language is converted into a formal database
query that can be executed to retrieve information.
Example 1 shows a NL input query and its corre-
sponding meaning representation.
Example 1 Geoquery input text and output MR
?What is the largest state that borders Texas??
largest(state(next to(const(texas))))
Previous works (Zelle and Mooney, 1996; Tang
and Mooney, 2001; Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) employ
machine learning techniques to construct a seman-
tic parser. The learning algorithm is given a set of
input sentences and their corresponding meaning
representations, and learns a statistical semantic
parser ? a set of rules mapping lexical items and
syntactic patterns to their meaning representation
and a score associated with each rule. Given a sen-
tence, these rules are applied recursively to derive
the most probable meaning representation. Since
semantic interpretation is limited to syntactic pat-
terns identified in the training data, the learning
algorithm requires considerable amounts of anno-
tated data to account for the syntactic variations
associated with the meaning representation. An-
notating sentences with their MR is a difficult,
time consuming task; minimizing the supervision
effort required for learning is a major challenge in
scaling semantic parsers.
This paper proposes a new model and learning
paradigm for semantic parsing aimed to alleviate
the supervision bottleneck. Following the obser-
vation that the target meaning representation is to
be executed by a computer program which in turn
provides a response or outcome; we propose a re-
sponse driven learning framework capable of ex-
ploiting feedback based on the response. The feed-
back can be viewed as a teacher judging whether
the execution of the meaning representation pro-
duced the desired response for the input sentence.
18
This type of supervision is very natural in many
situations and requires no expertise, thus can be
supplied by any user.
Continuing with Example 1, the response gen-
erated by executing a database query would be
used to provide feedback. The feedback would be
whether the generated response is the correct an-
swer for the input question or not, in this case New
Mexico is the desired response.
In response driven semantic parsing, the learner
is provided with a set of natural language sen-
tences and a feedback function that encapsulates
the teacher. The feedback function informs the
learner whether its interpretation of the input sen-
tence produces the desired response. We consider
scenarios where the feedback is provided as a bi-
nary signal, correct +1 or incorrect ?1.
This weaker form of supervision poses a chal-
lenge to conventional learning methods: semantic
parsing is in essence a structured prediction prob-
lem requiring supervision for a set of interdepen-
dent decisions, while the provided supervision is
binary, indicating the correctness of a generated
meaning representation. To bridge this difference
we propose two novel learning algorithms suited
to the response driven setting.
Furthermore, to account for the many syntac-
tic variations associated with the MR, we propose
a new model for semantic parsing that allows us
to learn effectively and generalize better. Cur-
rent semantic parsing approaches extract parsing
rules mapping NL to their MR, restricting pos-
sible interpretations to previously seen syntactic
patterns. We replace the rigid inference process
induced by the learned parsing rules with a flex-
ible framework. We model semantic interpreta-
tion as a sequence of interdependent decisions,
mapping text spans to predicates and use syntac-
tic information to determine how the meaning of
these logical fragments should be composed. We
frame this process as an Integer Linear Program-
ming (ILP) problem, a powerful and flexible in-
ference framework that allows us to inject rele-
vant domain knowledge into the inference process,
such as specific domain semantics that restrict the
space of possible interpretations.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), a
database consisting of U.S. geographical informa-
tion, and natural language questions. Our experi-
mental results show that our model with response
driven learning can outperform existing models
trained with annotated logical forms.
The key contributions of this paper are:
Response driven learning for semantic parsing
We propose a new learning paradigm for learn-
ing semantic parsers without any annotated mean-
ing representations. The supervision for learning
comes from a binary feedback signal based a re-
sponse generated by executing a meaning repre-
sentation. This type of supervision signal is nat-
ural to produce and can be acquired from non-
expert users.
Novel training algorithms Two novel train-
ing algorithms are developed within the response
driven learning paradigm. The training algorithms
are applicable beyond semantic parsing and can be
used in situations where it is possible to obtain bi-
nary feedback for a structured learning problem.
Flexible semantic interpretation process We
propose a novel flexible semantic parsing model
that can handle previously unseen syntactic varia-
tions of the meaning representation.
2 Semantic Parsing
The goal of semantic parsing is to produce a func-
tion F : X ? Z that maps from the space natural
language input sentences, X , to the space of mean-
ing representations, Z . This type of task is usu-
ally cast as a structured output prediction problem,
where the goal is to obtain a model that assigns the
highest score to the correct meaning representa-
tion given an input sentence. However, in the task
of semantic parsing, this decision relies on identi-
fying a hidden intermediate representation (or an
alignment) that captures the way in which frag-
ments of the text correspond to the meaning repre-
sentation. Therefore, we formulate the prediction
function as follows:
z? = Fw(x) = argmax
y?Y ,z?Z
wT?(x,y, z) (1)
Where ? is a feature function that describes the
relationships between an input sentence x, align-
ment y and meaning representation z. w is the
weight vector which contains the parameters of the
model. We refer to the argmax above as the in-
ference problem. The feature function combined
with the nature of the inference problem defines
the semantic parsing model. The key to producing
19
What is the largest Texas?
largest( const(texas))))
New Mexico
x:
y:
z:
r:
that bordersstate
state( next_to(
Figure 1: Example input sentence, meaning repre-
sentation, alignment and answer for the Geoquery
domain
a semantic parser involves defining a model and a
learning algorithm to obtain w.
In order to exemplify these concepts we con-
sider the Geoquery domain. Geoquery contains a
query language for a database of U.S. geograph-
ical facts. Figure 1 illustrates concrete examples
of the terminology introduce. The input sentences
x are natural language queries about U.S. geog-
raphy. The meaning representations z are logical
forms which can be executed on the database to
obtain a response which we denote with r. The
alignment y captures the associations between x
and z.
Building a semantic parser involves defining the
model (feature function ? and inference problem)
and a learning strategy to obtain weights (w) as-
sociated with the model. We defer discussion of
our model until Section 4 and first focus on our
learning strategy.
3 Structured Learning with Binary
Feedback
Previous approaches to semantic parsing have
assumed a fully supervised setting where
a training set is available consisting of ei-
ther: input sentences and logical forms
{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,
2005)) or input sentences, logical forms
and a mapping between their constituents
{(xl,yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).
Given such training examples a weight vector w
can be learned using structured learning methods.
Obtaining, through annotation or other means, this
form of training data is an expensive and difficult
process which presents a major bottleneck for
semantic parsing.
To reduce the burden of annotation we focus
on a new learning paradigm which uses feedback
from a teacher. The feedback signal is binary
(+1,?1) and informs the learner whether a pre-
dicted logical form z? when executed on the target
Algorithm 1 Direct Approach (Binary Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Bl ? {} for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: add (?(xl, y?, z?)/|xl|, f) to Bl
7: end for
8: w? BinaryLearn(B) where B = ?lBl
9: until no Bl has new unique examples
10: return w
domain produces the desired response or outcome.
This is a very natural method for providing super-
vision in many situations and requires no exper-
tise. For example, a user can observe the response
and provide a judgement. The general form of
the teacher?s feedback is provided by a function
Feedback : X ? Z ? {+1,?1}.
For the Geoquery domain this amounts to
whether the logical form produces the correct re-
sponse r for the input sentence. Geoquery has the
added benefit that the teacher can be automated
if we have a dataset consisting of input sentences
and response pairs {(xl, rl)}Nl=1. Feedback eval-
uates whether a logical form produces a response
matching r:
Feedback (xl, z) =
{
+1 if execute(z) = rl
?1 otherwise
We are now ready to present our learning
with feedback algorithms that operate in situations
where input sentences, {xl}Nl=1, and a teacher
feedback mechanism, Feedback , are available. We
do not assume the availability of logical forms.
3.1 Direct Approach (Binary Learning)
In general, a weight vector can be considered
good if when used in the inference problem (Equa-
tion (1)) it scores the correct logical form and
alignment (which may be hidden) higher than all
other logical forms and alignments for a given in-
put sentence. The intuition behind the direct ap-
proach is that the feedback function can be used to
subsample the space of possible structures (align-
ments and logical forms (Y ? Z)) for a given in-
put x. The feedback mechanism indicates whether
the structure is good (+1) or bad (?1). Using this
20
intuition we can cast the problem of learning a
weight vector for Equation (1) as a binary classifi-
cation problem where we directly consider struc-
tures the feedback assigns +1 as positive examples
and those assigned ?1 as negative.
We represent the input to the binary classifier
as the feature vector ?(x,y, z) normalized by the
size of the input sentence1 |x|, and the label as the
result from Feedback (x, z).
Algorithm 1 outlines the approach in detail. The
first stage of the algorithm iterates over all the
training input sentences and computes the best
logical form z? and alignment y? by solving the in-
ference problem (line 4). The feedback function
is queried (line 5) and a training example for the
binary predictor created using the normalized fea-
ture vector from the triple containing the sentence,
alignment and logical form as input and the feed-
back as the label. This training example is added
to the working set of training examples for this in-
put sentence (line 6). All the feedback training ex-
amples are used to train a binary classifier whose
weight vector is used in the next iteration (line 8).
The algorithm repeats until no new unique training
examples are added to any of the working sets for
any input sentence. Although the number of possi-
ble training examples is very large, in practice the
algorithm is efficient and converges quickly. Note
that this approach is capable of using a wide va-
riety of linear classifiers as the base learner (line
8).
A policy is required to specify the nature of
the working set of training examples (Bl) used for
training the base classifier. This is pertinent in line
6 of the algorithm. Possible policies include: al-
lowing duplicates in the working set (i.e., Bl is
a multiset), disallowing duplicates (Bl is a set),
or only allowing one example per input sentence
(?Bl? = 1). We adopt the first approach in this
paper.2
3.2 Aggressive Approach (Structured
Learning)
There is important implicit information which
the direct approach ignores. It is implicit that
when the teacher indicates an input paired with
an alignment and logical form is good (+1 feed-
1Normalization is required to ensure that each sentence
contributes equally to the binary learning problem regardless
of the sentence?s length.
2The working set Bl for each sentence may contain multi-
ple positive examples with the same and differing alignments.
Algorithm 2 Aggressive Approach (Structured
Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Sl ? ? for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: if f is +1 then
7: Sl ? {(xl, y?, z?)}
8: end if
9: end for
10: w? StructLearn(S,?) where S = ?lSl
11: until no Sl has changed
12: return w
back) that in order to repeat this behavior all other
competing structures should be made suboptimal
(or bad). To leverage this implicit information
we adopt a structured learning strategy in which
we consider the prediction as the optimal structure
and all others as suboptimal. This is in contrast to
the direct approach where only structures that have
explicitly received negative feedback are consid-
ered subopitmal.
When a structure is found with positive feed-
back it is added to the training pool for a struc-
tured learner. We consider this approach aggres-
sive as the structured learner implicitly considers
all other structures as being suboptimal. Negative
feedback indicates that the structure should not be
added to the training pool as it will introduce noise
into the learning process.
Algorithm 2 outlines the learning in more detail.
As before, y? and z? are predicted using the cur-
rent weight vector and feedback received (lines 4
and 5). When positive feedback is received a new
training instance for a structured learner is created
from the input sentence and prediction (line 7) this
training instance replaces any previous instance
for the input sentence. When negative feedback
is received the training pool Sl is not updated. A
weight vector is learned using a structured learner
where the training data S contains at most one ex-
ample per input sentence. In the first iteration of
the outer loop the training data S will contain very
few examples. In each subsequent iteration the
newly learned weight vector allows the algorithm
to acquire new examples. This is repeated until no
21
new examples are added or changed in S.
Like the direct approach, this learning frame-
work is makes very few assumptions about the
type of structured learner used as a base learner
(line 10).3
4 Model
Semantic parsing is the process of converting a
natural language input into a formal logic repre-
sentation. This process is performed by associat-
ing lexical items and syntactic patterns with logi-
cal fragments and composing them into a complete
formula. Existing approaches rely on extracting
a set of parsing rules, mapping text constituents
to a logical representation, from annotated train-
ing data and applying them recursively to obtain
the meaning representation. Adapting to new data
is a major limitation of these approaches as they
cannot handle inputs containing syntactic patterns
which were not observed in the training data. For
example, assume the training data produced the
following set of parsing rules:
Example 2 Typical parsing rules
(1) NP [?x.capital(x)]? capital
(2) PP [ const(texas)]? of Texas
(3) NNP [ const(texas)]? Texas
(4) NP [capital(const(texas))]?
NP[?x.capital(x)] PP [ const(texas)]
At test time the parser is given the sentences in
Example 3. Despite the lexical similarity in these
examples, the semantic parser will correctly parse
the first sentence but fail to parse the second be-
cause the lexical items belong to different a syn-
tactic category (i.e., the word Texas is not part of a
preposition phrase in the second sentence).
Example 3 Syntactic variations of the same MR
Target logical form: capital(const(texas))
Sentence 1: ?What is the capital of Texas??
Sentence 2: ?What is Texas? capital??
The ability to adapt to unseen inputs is one
of the key challenges in semantic parsing. Sev-
eral works (Zettlemoyer and Collins, 2007; Kate,
2008) have addressed this issue explicitly by man-
ually defining syntactic transformation rules that
can help the learned parser generalize better. Un-
fortunately these are only partial solutions as a
3Mistake driven algorithms that do not enforce margin
constraints may not be able to generalize using this proto-
col since they will repeat the same prediction at training time
and therefore will not update the model.
manually constructed rule set cannot cover the
many syntactic variations.
Given the previous example, we observe
that it is enough to identify that the function
capital(?) and the constant const(texas)
appear in the target MR, since there is only a single
way to compose these entities into a single formula
? capital(const(texas)).
Motivated by this observation we define our
meaning derivation process over the rules of the
MR language and use syntactic information as a
way to bias the MR construction process. That
is, our inference process considers the entire space
of meaning representations irrespective of the pat-
terns observed in the training data. This is possi-
ble as the MRs are defined by a formal language
and formal grammar.4 The syntactic information
present in the natural language is used as soft ev-
idence (features) which guides the inference pro-
cess to good meaning representations.
This formulation is a major shift from existing
approaches that rely on extracting parsing rules
from the training data. In existing approaches
the space of possible meaning representations is
constrained by the patterns in the training data
and syntactic structure of the natural language in-
put. Our formulation considers the entire space of
meaning representations and allows the model to
adapt to previously unseen data and always pro-
duce a semantic interpretation by using the pat-
terns observed in the input.
We frame our semantic interpretation process
as a constrained optimization process, maximiz-
ing the objective function defined by Equation 1
which relies on extracting lexical and syntactic
features instead of parsing rules. In the remain-
der of this section we explain the components of
our inference model.
4.1 Target Meaning Representation
Following previous work, we capture the se-
mantics of the Geoquery domain using a sub-
set of first-order logic consisting of typed con-
stants and functions. There are two types: en-
tities E in the domain and numeric values N .
Functions describe a functional relationship over
types (e.g., population : E ? N ). A com-
plete logical form is constructed through func-
tional composition; in our formalism this is per-
4This is true for all meaning representations designed to
be executed by a computer system.
22
formed by the substitution operator. For ex-
ample, given the function next to(x) and
the expression const(texas), substitution re-
places the occurrence of the free variable x, with
the expression, resulting in a new logical form:
next to(const(texas)). Due to space lim-
itations we refer the reader to (Zelle and Mooney,
1996) for a detailed description of the Geoquery
domain.
4.2 Semantic Parsing as Constrained
Optimization
Recall that the goal of semantic parsing is to pro-
duce the following function (Equation (1)):
Fw(x) = argmax
y,z
wT?(x,y, z)
However, given that y and z are complex struc-
tures it is necessary to decompose the structure
into a set of smaller decisions to facilitate efficient
inference.
In order to define our decomposition we intro-
duce additional notation: c is a constituent (or
word span) in the input sentence x and D is the
set of all function and constant symbols in the do-
main. The alignment y is defined as a set of map-
pings between constituents and symbols in the do-
main y = {(c, s)} where s ? D.
We decompose the construction of an alignment
and logical form into two types of decisions:
First-order decisions. A mapping between con-
stituents and logical symbols (functions and con-
stants).
Second-order decisions. Expressing how logi-
cal symbols are composed into a complete logical
interpretation. For example, whether next to
and state forms next to(state(?)) or
state(next to(?)).
Note that for all possible logical forms and
alignments there exists a one-to-one mapping to
these decisions.
We frame the inference problem as an Integer
Linear Programming (ILP) problem (Equation (2))
in which the first-order decisions are governed by
?cs, a binary decision variable indicating that con-
stituent c is aligned with logical symbol s. And
?cs,dt capture the second-order decisions indicat-
ing the symbol t (associated with constituent d)
is an argument to function s (associated with con-
stituent c).
Fw(x) = argmax
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?wT?2(x, c, s, d, t) (2)
It is clear that there are dependencies between
the ?-variables and ?-variables. For example,
given that ?cs,dt is active, the corresponding ?-
variables ?cs and ?dt must also be active. In order
to ensure a consistent solution we introduce a set
of constraints on Equation (2). In addition we add
constraints which leverage the typing information
inherent in the domain to eliminate logical forms
that are invalid in the Geoquery domain. For ex-
ample, the function length only accepts river
types as input. The set of constraints are:
? A given constituent can be associated with
exactly one logical symbol.
? ?cs,dt is active if and only if ?cs and ?dt are
active.
? If ?cs,dt is active, s must be a function and
the types of s and t should be consistent.
? Functional composition is directional and
acyclic.
The flexibility of ILP has previously been advan-
tageous in natural language processing tasks (Roth
and Yih, 2007) as it allows us to easily incorporate
such constraints.
4.3 Features
The inference problem defined in Equation (2)
uses two feature functions: ?1 and ?2.
First-order decision features ?1 Determining
if a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.5 This lexicon is
small and only used as a starting point. Existing
approaches rely on annotated logical forms to ex-
tend the lexicon. However, in our setting we do
not have access to annotated logical forms, instead
we rely on external knowledge to supply further
5The lexicon contains on average 1.42 words per func-
tion and 1.07 words per constant. For example the function
next to has the lexical entries: borders, next, adjacent and
the constant illinois the lexical item illinois.
23
information. We add features which measure the
lexical similarity between a constituent and a logi-
cal symbol?s surface forms (as defined by the lexi-
con). Two metrics are used: stemmed word match
and a similarity metric based on WordNet (Miller
et al, 1990) which allows our model to account
for words not in the lexicon. The WordNet met-
ric measures similarity based on synonymy, hy-
ponymy and meronymy (Do et al, 2010). In the
case where the constituent is a preposition, which
are notorious for being ambiguous, we add a fea-
ture that considers the current lexical context (one
word to the left and right) in addition to word sim-
ilarity.
Second-order decision features ?2 Determin-
ing how to compose two logical symbols relies on
syntactic information, in our model we use the de-
pendency tree (Klein and Manning, 2003) of the
input sentence. Given a second-order decision
?cs,dt, the dependency feature takes the normal-
ized distance between the head words in the con-
stituents c and d. A set of features also indicate
which logical symbols are usually composed to-
gether, without considering their alignment to text.
5 Experiments
In this section we describe our experimental setup,
which includes the details of the domain, re-
sources and parameters.
5.1 Domain and Corpus
We evaluate our system on the Geoquery domain
as described previously. The domain consists of
a database and Prolog query language for U.S.
geographical facts. The corpus contains of 880
natural language queries paired with Prolog log-
ical form queries ((x, z) pairs). We follow previ-
ous approaches and transform these queries into a
functional representation. We randomly select 250
sentences for training and 250 sentences for test-
ing.6 We refer to the training set as Response 250
(R250) indicating that each example x in this data
set has a corresponding desired database response
r. We refer the testing set as Query 250 (Q250)
where the examples only contain the natural lan-
guage queries.
6Our inference problem is less constrained than previous
approaches thus we limit the training data to 250 examples
due to scalability issues. We also prune the search space by
limiting the number of logical symbol candidates per word
(on average 13 logical symbols per word).
Precision and recall are typically used as eval-
uation metrics in semantic parsing. However, as
our model inherently has the ability to map any
input sentence into the space of meaning repre-
sentations the trade off between precision and re-
call does not exist. Thus, we report accuracy: the
percentage of meaning representations which pro-
duce the correct response. This is equivalent to
recall in previous work (Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007).
5.2 Resources and Parameters
Feedback Recall that our learning framework
does not require meaning representation annota-
tions. However, we do require a Feedback func-
tion that informs the learner whether a predicted
meaning representation when executed produces
the desired response for a given input sentence.
We automatically generate a set of natural lan-
guage queries and response pairs {(x, r)} by exe-
cuting the annotated logical forms on the database.
Using this data we construct an automatic feed-
back function as described in Section 3.
Domain knowledge Our learning approaches
require an initial weight vector as input. In or-
der to provide an initial starting point, we initialize
the weight vector using a similar procedure to the
one used in (Zettlemoyer and Collins, 2007) to set
weights for three features and a bias term. The
weights were developed on the training set using
the feedback function to guide our choices.
Underlying Learning Algorithms In the direct
approach the base linear classifier we use is a lin-
ear kernel Support Vector Machine with squared-
hinge loss. In the aggressive approach we de-
fine our base structured learner to be a structural
Support Vector Machine with squared-hinge loss
and use hamming distance as the distance func-
tion. We use a custom implementation to op-
timize the objective function using the Cutting-
Plane method, this allows us to parrallelize the
learning process by solving the inference problem
for multiple training examples simultaneously.
6 Results
Our experiments are designed to answer three
questions:
1. Is it possible to learn a semantic parser with-
out annotated logical forms?
24
Algorithm R250 Q250
NOLEARN 22.2 ?
DIRECT 75.2 69.2
AGGRESSIVE 82.4 73.2
SUPERVISED 87.6 80.4
Table 1: Accuracy of learned models on R250 data and
Q250 (testing) data. NOLEARN: using initialized weight
vector, DIRECT: using feedback with the direct approach,
AGGRESSIVE: using feedback with the aggressive approach,
SUPERVISED: using gold 250 logical forms for training.
Note that none of the approaches use any annotated logical
forms besides the SUPERVISED approach.
Algorithm # LF Accuracy
AGGRESSIVE ? 73.2
SUPERVISED 250 80.4
W&M 2006 ? 310 ? 60.0
W&M 2007 ? 310 ? 75.0
Z&C 2005 600 79.29
Z&C 2007 600 86.07
W&M 2007 800 86.59
Table 2: Comparison against previously published results.
Results show that with a similar number of logical forms
(# LF) for training our SUPERVISED approach outperforms
existing systems, while the AGGRESSIVE approach remains
competitive without using any logical forms.
2. How much performance do we sacrifice by
not restricting our model to parsing rules?
3. What, if any, are the differences in behaviour
between the two learning with feedback ap-
proaches?
We first compare how well our model performs
under four different learning regimes. NOLEARN
uses a manually initialized weight vector. DIRECT
and AGGRESSIVE use the two response driven
learning approaches, where a feedback function
but no logical forms are provided. As an up-
per bound we train the model using a fully SU-
PERVISED approach where the input sentences are
paired with hand annotated logical forms.
Table 1 shows the accuracy of each setup. The
model without learning (NOLEARN) gives a start-
ing point with an accuracy of 22.2%. The re-
sponse driven learning methods perform substan-
tially better than the starting point. The DIRECT
approach which uses a binary learner reaches an
accuracy of 75.2% on the R250 data and 69.2% on
the Q250 (testing) data. While the AGGRESSIVE
approach which uses a structured learner sees a
bigger improvement, reaching 82.4% and 73.2%
respectively. This is only 7% below the fully SU-
PERVISED upper bound of the model.
To answer the second question, we compare a
supervised version of our model to existing se-
mantic parsers. The results are in Table 2. Al-
though the numbers are not directly comparable
due to different splits in the data7, we can see that
with a similar number of logical forms for train-
ing our SUPERVISED approach outperforms ex-
isting systems (Wong and Mooney, 2006; Wong
and Mooney, 2007), while the AGGRESSIVE ap-
proach remains competitive without using any log-
ical forms. Our SUPERVISED model is still very
competitive with other approaches (Zettlemoyer
and Collins, 2007; Wong and Mooney, 2007),
which used considerably more annotated logical
forms in the training phase.
In order to answer the third question, we turn
our attention to the differences between the two
response driven learning approaches. The DIRECT
and AGGRESSIVE approaches use binary feedback
to learn, however they utilize the signal differently.
DIRECT uses the signal directly to learn a bi-
nary classifier capable of replicating the feedback,
whereas AGGRESSIVE learns a structured predic-
tor that can repeatedly obtain the logical forms
for which positive feedback was received. Thus,
although the AGGRESSIVE outperforms the DI-
RECT approach the concepts each approach learns
may be different. Analysis over the training data
shows that in 66.8% examples both approaches
predict a logical form that gives the correct an-
swer. While AGGRESSIVE correctly answers an
additional 16% which DIRECT gets incorrect. In
the opposite direction, DIRECT correctly answers
8.8% that AGGRESSIVE does not. Leaving only
8.4% of the examples that both approaches pre-
dict incorrect logical forms. This suggests that an
approach which combines DIRECT and AGGRES-
SIVE may be able to improve even further.
Figure 2 shows the accuracy on the entire train-
ing data (R250) at each iteration of learning. We
see that the AGGRESSIVE approach learns to cover
more of the training data and at a faster rate than
DIRECT. Note that the performance of the DI-
RECT approach drops at the first iteration. We hy-
pothesize this is due to imbalances in the binary
feedback dataset (too many negative examples) in
the first iteration.
7It is relatively difficult to compare different approaches
in the Geoquery domain given that many existing papers do
not use the same data split.
25
70 1 2 3 4 5 6
90
0
10
20
30
40
50
60
70
80
Learning Iterations
A
cc
ur
ac
y 
on
 R
es
po
ns
e 
25
0
Direct Approach
Aggressive Approach
Initialization
Figure 2: Accuracy on training set as number of learning
iterations increases.
7 Related Work
Learning to map sentences to a meaning repre-
sentation has been studied extensively in the NLP
community. Early works (Zelle and Mooney,
1996; Tang and Mooney, 2000) employed induc-
tive logic programming approaches to learn a se-
mantic parser. More recent works apply statisti-
cal learning methods to the problem. In (Ge and
Mooney, 2005; Nguyen et al, 2006), the input to
the learner consists of complete syntactic deriva-
tions for the input sentences annotated with logi-
cal expressions. Other works (Wong and Mooney,
2006; Kate and Mooney, 2006; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009) try to alleviate the
annotation effort by only taking sentence and log-
ical form pairs to train the models. Learning is
then defined over hidden patterns in the training
data that associate logical symbols with lexical
and syntactic elements.
In this work we take an additional step to-
wards alleviating the difficulty of training seman-
tic parsers and present a world response based
training protocol. Several recent works (Chen and
Mooney, 2008; Liang et al, 2009; Branavan et
al., 2009) explore using an external world context
as a supervision signal for semantic interpretation.
These works operate in settings different to ours as
they rely on an external world state that is directly
referenced by the input text. Although our frame-
work can also be applied in these settings we do
not assume that the text can be grounded in a world
state. In our experiments the input text consists of
generalized statements which describe some infor-
mation need that does not correspond directly to a
grounded world state.
Our learning framework closely follows recent
work on learning from indirect supervision. The
direct approach resembles learning a binary clas-
sifier over a latent structure (Chang et al, 2010a);
while the aggressive approach has similarities with
work that uses labeled structures and a binary
signal indicating the existence of good structures
to improve structured prediction (Chang et al,
2010b).
8 Conclusions
In this paper we tackle one of the key bottlenecks
in semantic parsing ? providing sufficient super-
vision to train a semantic parser. Our solution is
two fold, first we present a new training paradigm
for semantic parsing that relies on natural, hu-
man level supervision. Second, we suggest a new
model for semantic interpretation that does not
rely on NL syntactic parsing rules, but rather uses
the syntactic information to bias the interpretation
process. This approach allows the model to gener-
alize better and reduce the required amount of su-
pervision. We demonstrate the effectiveness of our
training paradigm and interpretation model over
the Geoquery domain, and show that our model
can outperform fully supervised systems.
Acknowledgements We are grateful to Rohit Kate and
Raymond Mooney for their help with the Geoquery dataset.
Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembu
and the three anonymous reviewers for their insightful com-
ments. This material is based upon work supported by the
Air Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181 and by DARPA under the Bootstrap
Learning Program. Any opinions, findings, and conclusion or
recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the AFRL
or DARPA.
References
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning for map-
ping instructions to actions. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained la-
tent representations. In Proc. of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
26
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010b. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In Proc. of
the International Conference on Machine Learning
(ICML).
Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydis-
waran. 2010. Robust, Light-weight Approaches to
compute Lexical Similarity. Computer Science Re-
search and Technical Reports, University of Illinois.
http://hdl.handle.net/2142/15462.
R. Ge and R. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
R. Kate and R. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
R. Kate. 2008. Transforming meaning representation
grammars to improve semantic parsing. In Proc. of
the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Proc. of the Conference on Advances in
Neural Information Processing Systems (NIPS).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic
parsing with structured svm ensemble classification
models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construc-
tion of database interfaces: integrating statistical and
relational learning for semantic parsing. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming
for semantic parsing. In Proc. of the European Con-
ference on Machine Learning (ECML).
Y.-W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proc. of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).
Y.-W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proc. of the National Conference on Artificial In-
telligence (AAAI).
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. of
the Annual Conference in Uncertainty in Artificial
Intelligence (UAI).
L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and on Computational Natural Language
Learning (EMNLP-CoNLL).
L. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
27
