Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 193?200, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Predicting Sentences using N-Gram Language Models
Steffen Bickel, Peter Haider, and Tobias Scheffer
Humboldt-Universita?t zu Berlin
Department of Computer Science
Unter den Linden 6, 10099 Berlin, Germany
{bickel, haider, scheffer}@informatik.hu-berlin.de
Abstract
We explore the benefit that users in sev-
eral application areas can experience from
a ?tab-complete? editing assistance func-
tion. We develop an evaluation metric
and adapt N -gram language models to
the problem of predicting the subsequent
words, given an initial text fragment. Us-
ing an instance-based method as base-
line, we empirically study the predictabil-
ity of call-center emails, personal emails,
weather reports, and cooking recipes.
1 Introduction
Prediction of user behavior is a basis for the con-
struction of assistance systems; it has therefore been
investigated in diverse application areas. Previous
studies have shed light on the predictability of the
next unix command that a user will enter (Motoda
and Yoshida, 1997; Davison and Hirsch, 1998), the
next keystrokes on a small input device such as a
PDA (Darragh and Witten, 1992), and of the trans-
lation that a human translator will choose for a given
foreign sentence (Nepveu et al, 2004).
We address the problem of predicting the subse-
quent words, given an initial fragment of text. This
problem is motivated by the perspective of assis-
tance systems for repetitive tasks such as answer-
ing emails in call centers or letters in an adminis-
trative environment. Both instance-based learning
and N -gram models can conjecture completions of
sentences. The use of N -gram models requires the
application of the Viterbi principle to this particular
decoding problem.
Quantifying the benefit of editing assistance to a
user is challenging because it depends not only on
an observed distribution over documents, but also
on the reading and writing speed, personal prefer-
ence, and training status of the user. We develop
an evaluation metric and protocol that is practical,
intuitive, and independent of the user-specific trade-
off between keystroke savings and time lost due to
distractions. We experiment on corpora of service-
center emails, personal emails of an Enron execu-
tive, weather reports, and cooking recipes.
The rest of this paper is organized as follows.
We review related work in Section 2. In Section 3,
we discuss the problem setting and derive appropri-
ate performance metrics. We develop the N -gram-
based completion method in Section 4. In Section 5,
we discuss empirical results. Section 6 concludes.
2 Related Work
Shannon (1951) analyzed the predictability of se-
quences of letters. He found that written English
has a high degree of redundancy. Based on this find-
ing, it is natural to ask whether users can be sup-
ported in the process of writing text by systems that
predict the intended next keystrokes, words, or sen-
tences. Darragh and Witten (1992) have developed
an interactive keyboard that uses the sequence of
past keystrokes to predict the most likely succeed-
ing keystrokes. Clearly, in an unconstrained applica-
tion context, keystrokes can only be predicted with
limited accuracy. In the specific context of entering
URLs, completion predictions are commonly pro-
193
vided by web browsers (Debevc et al, 1997).
Motoda and Yoshida (1997) and Davison and
Hirsch (1998) developed a Unix shell which pre-
dicts the command stubs that a user is most likely
to enter, given the current history of entered com-
mands. Korvemaker and Greiner (2000) have de-
veloped this idea into a system which predicts en-
tire command lines. The Unix command predic-
tion problem has also been addressed by Jacobs and
Blockeel (2001) who infer macros from frequent
command sequences and predict the next command
using variable memory Markov models (Jacobs and
Blockeel, 2003).
In the context of natural language, several typ-
ing assistance tools for apraxic (Garay-Vitoria and
Abascal, 2004; Zagler and Beck, 2002) and dyslexic
(Magnuson and Hunnicutt, 2002) persons have been
developed. These tools provide the user with a list of
possible word completions to select from. For these
users, scanning and selecting from lists of proposed
words is usually more efficient than typing. By con-
trast, scanning and selecting from many displayed
options can slow down skilled writers (Langlais et
al., 2002; Magnuson and Hunnicutt, 2002).
Assistance tools have furthermore been developed
for translators. Computer aided translation systems
combine a translation and a language model in order
to provide a (human) translator with a list of sug-
gestions (Langlais et al, 2000; Langlais et al, 2004;
Nepveu et al, 2004). Foster et al (2002) introduce
a model that adapts to a user?s typing speed in or-
der to achieve a better trade-off between distractions
and keystroke savings. Grabski and Scheffer (2004)
have previously developed an indexing method that
efficiently retrieves the sentence from a collection
that is most similar to a given initial fragment.
3 Problem Setting and Evaluation
Given an initial text fragment, a predictor that solves
the sentence completion problem has to conjecture
as much of the sentence that the user currently in-
tends to write, as is possible with high confidence?
preferably, but not necessarily, the entire remainder.
The perceived benefit of an assistance system is
highly subjective, because it depends on the expen-
diture of time for scanning and deciding on sug-
gestions, and on the time saved due to helpful as-
sistance. The user-specific benefit is influenced by
quantitative factors that we can measure. We con-
struct a system of two conflicting performance indi-
cators: our definition of precision quantifies the in-
verse risk of unnecessary distractions, our definition
of recall quantifies the rate of keystroke savings.
For a given sentence fragment, a completion
method may ? but need not ? cast a completion con-
jecture. Whether the method suggests a completion,
and how many words are suggested, will typically
be controlled by a confidence threshold. We con-
sider the entire conjecture to be falsely positive if at
least one word is wrong. This harsh view reflects
previous results which indicate that selecting, and
then editing, a suggested sentence often takes longer
than writing that sentence from scratch (Langlais et
al., 2000). In a conjecture that is entirely accepted
by the user, the entire string is a true positive. A
conjecture may contain only a part of the remaining
sentence and therefore the recall, which refers to the
length of the missing part of the current sentence,
may be smaller than 1.
For a given test collection, precision and recall
are defined in Equations 1 and 2. Recall equals
the fraction of saved keystrokes (disregarding the
interface-dependent single keystroke that is most
likely required to accept a suggestion); precision is
the ratio of characters that the users have to scan
for each character they accept. Varying the confi-
dence threshold of a sentence completion method re-
sults in a precision recall curve that characterizes the
system-specific trade-off between keystroke savings
and unnecessary distractions.
Precision =
?
accepted completions string length?
suggested completions string length
(1)
Recall =
?
accepted completions string length?
all queries length of missing part
(2)
4 Algorithms for Sentence Completion
In this section, we derive our solution to the sen-
tence completion problem based on linear interpola-
tion of N -gram models. We derive a k best Viterbi
decoding algorithm with a confidence-based stop-
ping criterion which conjectures the words that most
likely succeed an initial fragment. Additionally, we
194
briefly discuss an instance-based method that pro-
vides an alternative approach and baseline for our
experiments.
In order to solve the sentence completion problem
with an N -gram model, we need to find the most
likely word sequence wt+1, . . . , wt+T given a word
N -gram model and an initial sequence w1, . . . , wt
(Equation 3). Equation 4 factorizes the joint proba-
bility of the missing words; the N -th order Markov
assumption that underlies the N -gram model simpli-
fies this expression in Equation 5.
argmax
wt+1,...,wt+T
P (wt+1, . . . , wt+T |w1, . . . , wt) (3)
= argmax
wt+1,...,wt+T
T?
j=1
P (wt+j |w1, . . . , wt+j?1) (4)
= argmax
T?
j=1
P (wt+j |wt+j?N+1, . . . , wt+j?1) (5)
The individual factors of Equation 5 are provided by
the model. The Markov order N has to balance suffi-
cient context information and sparsity of the training
data. A standard solution is to use a weighted linear
mixture of N -gram models, 1 ? n ? N , (Brown et
al., 1992). We use an EM algorithm to select mixing
weights that maximize the generation probability of
a tuning set of sentences that have not been used for
training.
We are left with the following questions: (a)
how can we decode the most likely completion effi-
ciently; and (b) how many words should we predict?
4.1 Efficient Prediction
We have to address the problem of finding the
most likely completion, argmaxwt+1,...,wt+T
P (wt+1, . . . , wt+T |w1, . . . , wt) efficiently, even
though the size of the search space grows exponen-
tially in the number of predicted words.
We will now identify the recursive structure in
Equation 3; this will lead us to a Viterbi al-
gorithm that retrieves the most likely word se-
quence. We first define an auxiliary variable
?t,s(w?1, . . . , w?N |wt?N+2, . . . , wt) in Equation 6; it
quantifies the greatest possible probability over all
arbitrary word sequences wt+1, . . . , wt+s, followed
by the word sequence wt+s+1 = w?1, . . . , wt+s+N =
w?N , conditioned on the initial word sequence
wt?N+2, . . . , wt.
In Equation 7, we factorize the last transition and
utilize the N -th order Markov assumption. In Equa-
tion 8, we split the maximization and introduce a
new random variable w?0 for wt+s. We can now refer
to the definition of ? and see the recursion in Equa-
tion 9: ?t,s depends only on ?t,s?1 and the N -gram
model probability P (w?N |w?1, . . . , w?N?1).
?t,s(w?1, . . . , w?N |wt?N+2, . . . , wt) (6)
= max
wt+1,...,wt+s
P (wt+1, . . . , wt+s, wt+s+1 = w?1,
. . . , wt+s+N = w?N |wt?N+2, . . . , wt)
= max
wt+1,...,wt+s
P (w?N |w?1, . . . , w?N?1) (7)
P (wt+1, . . . , wt+s, wt+s+1 = w?1,
. . . , wt+s+N?1 = w?N?1|wt?N+2, . . . , wt)
= max
w?0
max
wt+1,...,wt+s?1
P (w?N |w?1, . . . , w?N?1) (8)
P (wt+1, . . . , wt+s?1, wt+s = w?0,
. . . , wt+s+N?1 = w?N?1|wt?N+2, . . . , wt)
= max
w?0
P (w?N |w?1, . . . , w?N?1)
?t,s?1(w?0, . . . , w?N?1|wt+N?2, . . . , wt)
(9)
Exploiting the N -th order Markov assumption,
we can now express our target probability (Equation
3) in terms of ? in Equation 10.
max
wt+1,...,wt+T
P (wt+1, . . . , wt+T |wt?N+2, . . . , wt) (10)
= max
w?1,...,w?N
?t,T?N (w?1, . . . , w?N |wt?N+2, . . . , wt)
The last N words in the most likely sequence
are simply the argmaxw?1,...,w?N ?t,T?N (w
?1, . . . , w?N |
wt?N+2, . . . , wt). In order to collect the preceding
most likely words, we define an auxiliary variable ?
in Equation 11 that can be determined in Equation
12. We have now found a Viterbi algorithm that is
linear in T , the completion length.
?t,s(w?1, . . . , w?N |wt?N+2, . . . , wt) (11)
= argmax
wt+s
max
wt+1,...,wt+s?1
P (wt+1, ..., wt+s, wt+s+1 = w?1, ...,
wt+s+N = w?N |wt?N+2, ..., wt)
= argmax
w?0
?t,s?1(w?0, . . . , w?N?1|wt?N+2, . . . , wt)
P (w?N |w?1, . . . , w?N?1) (12)
The Viterbi algorithm starts with the most recently
entered word wt and moves iteratively into the fu-
ture. When the N -th token in the highest scored ? is
a period, then we can stop as our goal is only to pre-
dict (parts of) the current sentence. However, since
195
there is no guarantee that a period will eventually
become the most likely token, we use an absolute
confidence threshold as additional criterion: when
the highest ? score is below a threshold ?, we stop
the Viterbi search and fix T .
In each step, Viterbi stores and updates
|vocabulary size|N many ? values?unfeasibly
many except for very small N . Therefore, in Table
1 we develop a Viterbi beam search algorithm
which is linear in T and in the beam width. Beam
search cannot be guaranteed to always find the
most likely word sequence: When the globally
most likely sequence w?t+1, . . . , w?t+T has an initial
subsequence w?t+1, . . . , w?t+s which is not among
the k most likely sequences of length s, then that
optimal sequence is not found.
Table 1: Sentence completion with Viterbi beam
search algorithm.
Input: N -gram language model, initial sentence fragment
w1, . . . , wt, beam width k, confidence threshold ?.
1. Viterbi initialization:
Let ?t,?N (wt?N+1, . . . , wt|wt?N+1, . . . , wt) = 1;
let s = ?N + 1;
beam(s ? 1) = {?t,?N (wt?N+1, . . . , wt|wt?N+1,
. . . , wt)}.
2. Do Viterbi recursion until break:
(a) For all ?t,s?1(w?0, . . . , w?N?1| . . .) in
beam(s ? 1), for all wN in vocabulary, store
?t,s(w?1, . . . , w?N | . . .) (Equation 9) in beam(s)
and calculate ?t,s(w?1, . . . , w?N | . . .) (Equation
12).
(b) If argmaxwN maxw?1,...,w?N?1
?t,s(w?1, . . . , w?N | . . .) = period then break.
(c) If max ?t,s(w?1, . . . , w?N |wt?N+1, . . . , wt) < ?
then decrement s; break.
(d) Prune all but the best k elements in beam(s).
(e) Increment s.
3. Let T = s+N . Collect words by path backtracking:
(w?t+T?N+1, . . . , w?t+T )
= argmax ?t,T?N (w?1, . . . , w?N |...).
For s = T ?N . . . 1:
w?t+s = ?t,s(w?t+s+1, . . . , w?t+s+N |
wt?N+1, . . . , wt).
Return w?t+1, . . . , w?t+T .
4.2 Instance-based Sentence Completion
An alternative approach to sentence completion
based on N-gram models is to retrieve, from the
training collection, the sentence that starts most sim-
ilarly, and use its remainder as a completion hypoth-
esis. The cosine similarity of the TFIDF representa-
tion of the initial fragment to be completed, and an
equally long fragment of each sentence in the train-
ing collection gives both a selection criterion for the
nearest neighbor and a confidence measure that can
be compared against a threshold in order to achieve
a desired precision recall balance.
A straightforward implementation of this near-
est neighbor approach becomes infeasible when the
training collection is large because too many train-
ing sentences have to be processed. Grabski and
Scheffer (2004) have developed an indexing struc-
ture that retrieves the most similar (using cosine sim-
ilarity) sentence fragment in sub-linear time. We use
their implementation of the instance-based method
in our experimentation.
5 Empirical Studies
we investigate the following questions. (a) How
does sentence completion with N -gram models
compare to the instance-based method, both in terms
of precision/recall and computing time? (b) How
well can N -gram models complete sentences from
collections with diverse properties?
Table 2 gives an overview of the four document
collections that we use for experimentation. The
first collection has been provided by a large online
store and contains emails sent by the service center
in reply to customer requests (Grabski and Scheffer,
2004). The second collection is an excerpt of the
recently disclosed email correspondence of Enron?s
management staff (Klimt and Yang, 2004). We use
3189 personal emails sent by Enron executive Jeff
Dasovich; he is the individual who sent the largest
number of messages within the recording period.
The third collection contains textual daily weather
reports for five years from a weather report provider
on the Internet. Each report comprises about 20
sentences. The last collection contains about 4000
cooking recipes; this corpus serves as an example of
a set of thematically related documents that might be
found on a personal computer.
We reserve 1000 sentences of each data set for
testing. As described in Section 4, we split the
remaining sentences in training (75%) and tuning
196
Table 2: Evaluation data collections.
Name Language #Sentences Entropy
service center German 7094 1.41
Enron emails English 16363 7.17
weather reports German 30053 4.67
cooking recipes German 76377 4.14
(25%) sets. We mix N -gram models up to an order
of five and estimate the interpolation weights (Sec-
tion 4). The resulting weights are displayed in Fig-
ure 1. In Table 2, we also display the entropy of the
collections based on the interpolated 5-gram model.
This corresponds to the average number of bits that
are needed to code each word given the preceding
four words. This is a measure of the intrinsic redun-
dancy of the collection and thus of the predictability.
1
1
1
1
2
2
2
2
3
3
3
3
4
4
5
5
5
4
0% 20% 40% 60% 80% 100%
cooking recipes
weather reports
Enron emails
service center
Figure 1: N -gram interpolation weights.
Our evaluation protocol is as follows. The beam
width parameter k is set to 20. We randomly draw
1000 sentences and, within each sentence, a posi-
tion at which we split it into initial fragment and
remainder to be predicted. A human evaluator is
presented both, the actual sentence from the collec-
tion and the initial fragment plus current comple-
tion conjecture. For each initial fragment, we first
cast the most likely single word prediction and ask
the human evaluator to judge whether they would
accept this prediction (without any changes), given
that they intend to write the actual sentence. We in-
crease the length of the prediction string by one ad-
ditional word and recur, until we reach a period or
exceed the prediction length of 20 words.
For each judged prediction length, we record the
confidence measure that would lead to that predic-
tion. With this information we can determine the
results for all possible threshold values of ?. To save
evaluation time, we consider all predictions that are
identical to the actual sentence as correct and skip
those predictions in the manual evaluation.
We will now study how the N -gram method com-
pares to the instance-based method. Figure 2 com-
pares the precision recall curves of the two meth-
ods. Note that the maximum possible recall is typi-
cally much smaller than 1: recall is a measure of the
keystroke savings, a value of 1 indicates that the user
saves all keystrokes. Even for a confidence thresh-
old of 0, a recall of 1 is usually not achievable.
Some of the precision recall curves have a con-
cave shape. Decreasing the threshold value in-
creases the number of predicted words, but it also
increases the risk of at least one word being wrong.
In this case, the entire sentence counts as an incor-
rect prediction, causing a decrease in both, precision
and recall. Therefore ? unlike in the standard in-
formation retrieval setting ? recall does not increase
monotonically when the threshold is reduced.
For three out of four data collections, the instance-
based learning method achieves the highest max-
imum recall (whenever this method casts a con-
jecture, the entire remainder of the sentence is
predicted?at a low precision), but for nearly all
recall levels the N -gram model achieves a much
higher precision. For practical applications, a high
precision is needed in order to avoid distracting,
wrong predictions. Varying the threshold, the N -
gram model can be tuned to a wide range of different
precision recall trade-offs (in three cases, precision
can even reach 1), whereas the confidence threshold
of the instance-based method has little influence on
precision and recall.
We determine the standard error of the precision
for the point of maximum F1-measure. For all data
collections and both methods the standard error is
below 0.016. Correct and incorrect prediction ex-
amples are provided in Table 3 for the service center
data set, translated from German into English. The
confidence threshold is adjusted to the value of max-
imum F1-measure. In two of these cases, the predic-
tion nicely stops at fairly specific terms.
How do precision and recall depend on the string
length of the initial fragment and the string length
of the completion cast by the systems? Figure 3
shows the relationship between the length of the ini-
tial fragment and precision and recall. The perfor-
mance of the instance-based method depends cru-
cially on a long initial fragment. By contrast, when
197
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6
Pre
cis
ion
Recall
service center
N-graminstance-based
 0
 0.2
 0.4
 0.6
 0.8
 0  0.01  0.02  0.03  0.04  0.05
Pre
cis
ion
Recall
Enron emails
N-graminstance-based
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.02  0.04  0.06
Pre
cis
ion
Recall
weather reports
N-graminstance-based
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15
Pre
cis
ion
Recall
cooking recipes
N-graminstance-based
Figure 2: Precision recall curves for N -gram and instance-based methods of sentence completion.
Table 3: Prediction examples for service center data.
Initial fragment (bold face) and intended, missing part Prediction
Please complete your address. your address.
Kindly excuse the incomplete shipment. excuse the
Our supplier notified us that the pants are undeliverable. notified us that the
The mentioned order is not in our system. not in our system.
We recommend that you write down your login name and password. that you write down your login name and password.
The value will be accounted for in your invoice. be accounted for in your invoice.
Please excuse the delay. delay.
Please excuse our mistake. the delay.
If this is not the case give us a short notice. us your address and customer id.
the fragment length exceeds four with the N-gram
model, then this length and the accuracy are nearly
independent; the model considers no more than the
last four words in the fragment.
Figure 4 details the relation between string length
of the prediction and precision/recall. We see that
we can reach a constantly high precision over the en-
tire range of prediction lengths for the service center
data with the N-gram model. For the other collec-
tions, the maximum prediction length is 3 or 5 words
in comparison to much longer predictions cast by the
nearest neighbor method. But in these cases, longer
predictions result in lower precision.
How do instance-based learning and N -gram
completion compare in terms of computation time?
The Viterbi beam search decoder is linear in the pre-
diction length. The index-based retrieval algorithm
is constant in the prediction length (except for the fi-
nal step of displaying the string which is linear but
can be neglected). This is reflected in Figure 5 (left)
which also shows that the absolute decoding time
of both methods is on the order of few milliseconds
on a PC. Figure 5 (right) shows how prediction time
grows with the training set size.
We experiment on four text collections with di-
verse properties. The N -gram model performs re-
markably on the service center email collection.
Users can save 60% of their keystrokes with 85%
of all suggestions being accepted by the users, or
save 40% keystrokes at a precision of over 95%. For
cooking recipes, users can save 8% keystrokes at
60% precision or 5% at 80% precision. For weather
reports, keystroke savings are 2% at 70% correct
suggestions or 0.8% at 80%. Finally, Jeff Dasovich
of Enron can enjoy only a marginal benefit: below
1% of keystrokes are saved at 60% entirely accept-
able suggestions, or 0.2% at 80% precision.
How do these performance results correlate with
properties of the model and text collections? In Fig-
ure 1, we see that the mixture weights of the higher
order N -gram models are greatest for the service
center mails, smaller for the recipes, even smaller
for the weather reports and smallest for Enron. With
50% of the mixture weights allocated to the 1-gram
model, for the Enron collection the N -gram comple-
tion method can often only guess words with high
prior probability. From Table 2, we can further-
more see that the entropy of the text collection is
inversely proportional to the model?s ability to solve
the sentence completion problem. With an entropy
198
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12  14  16  18  20
Pre
cis
ion
Query length
service center
N-graminstance-based  0
 0.2
 0.4
 0.6
 0.8
 1
 2  4  6  8  10
Pre
cis
ion
Query length
Enron emails
N-graminstance-based
 0
 0.2
 0.4
 0.6
 0  2  4  6  8  10  12  14  16  18  20
Pre
cis
ion
Query length
weather report
N-graminstance-based  0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12  14  16  18  20
Pre
cis
ion
Query length
cooking recipes
N-graminstance-based
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  2  4  6  8  10  12  14  16  18  20
Re
cal
l
Query length
service center
N-graminstance-based  0
 0.05
 0.1
 2  4  6  8  10
Re
cal
l
Query length
Enron emails
N-graminstance-based
 0
 0.05
 0.1
 0.15
 0  2  4  6  8  10  12  14  16  18  20
Re
cal
l
Query length
weather report
N-graminstance-based
 0.05
 0.1
 0.15
 0.2
 0.25
 0  2  4  6  8  10  12  14  16  18  20
Re
cal
l
Query length
cooking recipes
N-graminstance-based
Figure 3: Precision and recall dependent on string length of initial fragment (words).
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  2  4  6  8  10  12  14  16  18  20
Pre
cis
ion
Prediction length
service center
N-graminstance-based  0
 0.2
 0.4
 0.6
 0.8
 2  4  6  8  10
Pre
cis
ion
Prediction length
Enron emails
N-graminstance-based
 0
 0.1
 0.2
 0.3
 0.4
 0  2  4  6  8  10  12  14  16  18  20
Pre
cis
ion
Prediction length
weather report
N-graminstance-based
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12  14  16  18  20
Pre
cis
ion
Prediction length
cooking recipes
N-graminstance-based
 0.5
 0.6
 0.7
 0.8
 0.9
 0  2  4  6  8  10  12  14  16  18  20
Re
cal
l
Prediction length
service center
N-graminstance-based
 0
 0.2
 0.4
 0.6
 2  4  6  8  10
Re
cal
l
Prediction length
Enron emails
N-graminstance-based
 0.05
 0.1
 0.15
 0  2  4  6  8  10  12  14  16  18  20
Re
cal
l
Prediction length
weather report
N-graminstance-based
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12  14  16  18  20
Re
cal
l
Prediction length
cooking recipes
N-graminstance-based
Figure 4: Precision and recall dependent on prediction string length (words).
of only 1.41, service center emails are excellently
predictable; by contrast, Jeff Dasovich?s personal
emails have an entropy of 7.17 and are almost as
unpredictable as Enron?s share price.
6 Conclusion
We discussed the problem of predicting how a user
will complete a sentence. We find precision (the
number of suggested characters that the user has to
read for every character that is accepted) and recall
(the rate of keystroke savings) to be appropriate per-
formance metrics. We developed a sentence com-
pletion method based on N -gram language models.
We derived a k best Viterbi beam search decoder.
Our experiments lead to the following conclusions:
(a) The N -gram based completion method has a
better precision recall profile than index-based re-
trieval of the most similar sentence. It can be tuned
to a wide range of trade-offs, a high precision can
be obtained. The execution time of the Viterbi beam
search decoder is in the order of few milliseconds.
(b) Whether sentence completion is helpful
strongly depends on the diversity of the document
collection as, for instance, measured by the entropy.
For service center emails, a keystroke saving of 60%
can be achieved at 85% acceptable suggestions; by
contrast, only a marginal keystroke saving of 0.2%
can be achieved for Jeff Dasovich?s personal emails
at 80% acceptable suggestions. A modest but signif-
icant benefit can be observed for thematically related
documents: weather reports and cooking recipes.
199
 0
 2
 4
 6
 8
 10
 1  2  3  4  5  6  7  8  9  10  11
Pre
dic
tion
 tim
e -
 m
s
Prediction length
service center
n-graminstance-based
 10
 20
 30
 40
 50
 1  2  3
Pre
dic
tion
 tim
e -
 m
s
Prediction length
weather reports
n-graminstance-based  0
 0.5
 1
 1.5
 10  20  30  40  50  60  70  80  90  100
Pre
dic
tion
 tim
e -
 m
s
Training set size in %
service center
n-graminstance-based
 0
 10
 20
 30
 40
 10  20  30  40  50  60  70  80  90  100
Pre
dic
tion
 tim
e -
 m
s
Training set size in %
weather report
n-graminstance-based
Figure 5: Prediction time dependent on prediction length in words (left) and prediction time dependent on
training set size (right) for service center and weather report collections.
Acknowledgment
This work has been supported by the German Sci-
ence Foundation DFG under grant SCHE540/10.
References
P. Brown, S. Della Pietra, V. Della Pietra, J. Lai, and
R. Mercer. 1992. An estimate of an upper bound
for the entropy of english. Computational Linguistics,
18(2):31?40.
J. Darragh and I. Witten. 1992. The Reactive Keyboard.
Cambridge University Press.
B. Davison and H. Hirsch. 1998. Predicting sequences of
user actions. In Proceedings of the AAAI/ICML Work-
shop on Predicting the Future: AI Approaches to Time
Series Analysis.
M. Debevc, B. Meyer, and R. Svecko. 1997. An adap-
tive short list for documents on the world wide web. In
Proceedings of the International Conference on Intel-
ligent User Interfaces.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
N. Garay-Vitoria and J. Abascal. 2004. A comparison of
prediction techniques to enhance the communication
of people with disabilities. In Proceedings of the 8th
ERCIM Workshop User Interfaces For All.
K. Grabski and T. Scheffer. 2004. Sentence completion.
In Proceedings of the ACM SIGIR Conference on In-
formation Retrieval.
N. Jacobs and H. Blockeel. 2001. The learning shell:
automated macro induction. In Proceedings of the In-
ternational Conference on User Modelling.
N. Jacobs and H. Blockeel. 2003. Sequence predic-
tion with mixed order Markov chains. In Proceedings
of the Belgian/Dutch Conference on Artificial Intelli-
gence.
B. Klimt and Y. Yang. 2004. The Enron corpus: A new
dataset for email classification research. In Proceed-
ings of the European Conference on Machine Learn-
ing.
B. Korvemaker and R. Greiner. 2000. Predicting Unix
command lines: adjusting to user patterns. In Pro-
ceedings of the National Conference on Artificial In-
telligence.
P. Langlais, G. Foster, and G. Lapalme. 2000. Unit com-
pletion for a computer-aided translation typing system.
Machine Translation, 15:267?294.
P. Langlais, M. Loranger, and G. Lapalme. 2002. Trans-
lators at work with transtype: Resource and evalua-
tion. In Proceedings of the International Conference
on Language Resources and Evaluation.
P. Langlais, G. Lapalme, and M. Loranger. 2004.
Transtype: Development-evaluation cycles to boost
translator?s productivity. Machine Translation (Spe-
cial Issue on Embedded Machine Translation Systems,
17(17):77?98.
T. Magnuson and S. Hunnicutt. 2002. Measuring the ef-
fectiveness of word prediction: The advantage of long-
term use. Technical Report TMH-QPSR Volume 43,
Speech, Music and Hearing, KTH, Stockholm, Swe-
den.
H. Motoda and K. Yoshida. 1997. Machine learning
techniques to make computers easier to use. In Pro-
ceedings of the Fifteenth International Joint Confer-
ence on Artificial Intelligence.
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004.
Adaptive language and translation models for interac-
tive machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
C. Shannon. 1951. Prediction and entropy of printed
english. In Bell Systems Technical Journal, 30, 50-64.
W. Zagler and C. Beck. 2002. FASTY - faster typing
for disabled persons. In Proceedings of the European
Conference on Medical and Biological Engineering.
200
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1810?1815,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Model of Individual Differences in Gaze Control During Reading
Niels Landwehr
1
and Sebastian Arzt
1
and Tobias Scheffer
1
and Reinhold Kliegl
2
1
Department of Computer Science, Universit?at Potsdam
August-Bebel-Stra?e 89, 14482 Potsdam, Germany
{landwehr, sarzt, scheffer}@cs.uni-potsdam.de
2
Department of Psychology, Universit?at Potsdam
Karl-Liebknecht-Stra?e 24/25, 14476 Potsdam OT/Golm
kliegl@uni-potsdam.de
Abstract
We develop a statistical model of saccadic
eye movements during reading of isolated
sentences. The model is focused on rep-
resenting individual differences between
readers and supports the inference of the
most likely reader for a novel set of eye
movement patterns. We empirically study
the model for biometric reader identifica-
tion using eye-tracking data collected from
20 individuals and observe that the model
distinguishes between 20 readers with an
accuracy of up to 98%.
1 Introduction
During skilled reading, the eyes of a reader do
not move smoothly over a text. Instead, read-
ing proceeds by alternating between brief fixations
on individual words and short ballistic movements
called saccades that move the point of fixation to a
new location. Evidence in psychological research
indicates that patterns of fixations and saccades are
driven partly by low-level visual cues (e.g., word
length), and partly by linguistic and cognitive pro-
cessing of the text (Kliegl et al., 2006; Rayner,
1998).
Eye-movement patterns are frequently studied
in cognitive psychology as they provide a rich
and detailed record of the visual, oculomotor, and
linguistic processes involved in reading. Com-
putational models of eye-movement control de-
veloped in psychology, such as SWIFT (Engbert
et al., 2005; Schad and Engbert, 2012) or E-Z
Reader (Reichle et al., 1998; Reichle et al., 2012),
simulate the generation of eye movements based
on physiological and psychological constraints re-
lated to attention, visual perception, and the ocu-
lomotor system. Recently, the problem of mod-
eling eye movements has also been approached
from a machine-learning perspective. Matties and
S?gaard (2013) and Hara et al. (2012) study con-
ditional random field models to predict which
words in a text are fixated by a reader. Nilsson
and Nivre (2009) use a transition-based log-linear
model to predict a sequence of fixations for a text.
A central observation made by these studies, as
well as by earlier psychological work (Erdmann
and Dodge, 1898; Huey, 1908), is that eye move-
ment patterns vary significantly between individ-
uals. As one example of the strength of indi-
vidual differences in reading eye movements, we
cite Dixon (1951) who compared the reading pro-
cesses of university professors and graduate stu-
dents of physics, education, and history on read-
ing material in their own and the two other fields.
He did not find strong effects of his experimen-
tal variables (i.e., field of research, expertise in re-
search) but ?if there is one thing that this study
has shown, it is that individual differences in read-
ing skill existed among the subjects of all depart-
ments. Fast and slow readers were found in every
department, and the overlapping of distributions
from passage to passage was enormous? (p. 173).
Even though it is possible to predict across a large
base of readers with some accuracy whether spe-
cific words will be fixated (Matties and S?gaard,
2013), a strong variation between readers in at-
tributes such as the fraction of skipped words and
total number of saccades has been observed (Hara
et al., 2012).
Some recent work has studied eye movement
patterns as a biometric feature. Most studies are
based on an artificial visual stimulus, such as a
moving (Kasprowski and Ober, 2004; Komogort-
sev et al., 2010; Rigas et al., 2012b; Zhang and
Juhola, 2012) or fixed (Bednarik et al., 2005) dot
on a computer screen, or a specific image stimu-
lus (Rigas et al., 2012a). In the most common use
1810
case of biometric user identification, a decision on
whether access should be granted has to be made
after performing some test that requires the user?s
attention and therefore cannot take a long time. By
contrast, our work is motivated by a less intrusive
scenario in which the user is monitored continu-
ously during access to, for instance, a device or
document. When the accumulated evidence sup-
ports the conclusion that the user is not authorized,
access can be terminated or additional credentials
requested. In this use case, identification has to
be based on saccadic eye movements that occur
while a user is reading an arbitrary text?as op-
posed to movements that occur in response to a
fixed, controlled visual stimulus. Holland and Ko-
mogortsev (2012) study reader recognition based
on a set of aggregate features derived from eye
movements, irrespective of the text being read;
their work will serve as reference in our empiri-
cal study.
The paper is organized as follows. Section 2 de-
tails the problem setting. Section 3 introduces the
statistical model and discusses parameter estima-
tion and inference. Section 5 presents empirical
results, Section 6 concludes.
2 Problem Setting and Notation
Let R denote a set of readers, and
X = {X
1
, ...,X
n
} a set of texts. Each r ? R
generates a set of eye movement patterns
S
(r)
= {S
(r)
1
, . . . ,S
(r)
n
} on the set of texts X , by
S
(r)
i
? p(S|X
i
, r)
where p(S|X, r) is a reader-specific distribution
over eye movement patterns given a text X. A pat-
tern is a sequence S = ((s
1
, d
1
), . . . , (s
T
, d
T
)) of
fixations, consisting of a fixation position s
t
(posi-
tion in text that was fixated) and duration d
t
? R
(length of fixation in milliseconds). In our experi-
ments, individual sentences are presented in a sin-
gle line on a screen, thus we only model a hori-
zontal gaze position s
t
? R.
At test time, we observe novel eye move-
ment patterns
?
S = {
?
S
1
, . . . ,
?
S
m
} on a novel set
of texts
?
X = {
?
X
1
, ...,
?
X
m
} generated by an un-
known reader r ? R. The goal is to infer
r
?
= arg max
r?R
p(r|
?
S,
?
X ). (1)
?20 ?10 0 10 2000.1
0.20.3
0.4
Amplitude
Density
Refixation
 
 Empirical DistributionGamma Fit
?20 ?10 0 10 200
0.1
0.2
Amplitude
Density
Next Word Move
 
 Empirical DistributionGamma Fit
?20 ?10 0 10 2000.05
0.10.15
Amplitude
Density
Forward Skip
 
 Empirical DistributionGamma Fit
?20 ?10 0 10 2000.05
0.10.15
0.2
Amplitude
Density
Regression
 
 Empirical DistributionGamma Fit
Figure 1: Empirical distributions over amplitudes
and Gamma fits for different saccade types.
3 Statistical Model of Eye Movements
We solve Problem 1 by estimating reader-specific
models p(S|X;?
r
) for r ? R, and solving for
p(r|
?
S,
?
X ; ?) ?
(
m
?
i=1
p(
?
S
i
|
?
X
i
;?
r
)
)
p(r) (2)
where all ?
r
are aggregated into a global model ?.
Assuming a uniform prior p(r) over readers, this
reduces to predicting the reader r that maximizes
the likelihood p(
?
S|
?
X ;?
r
) =
?
m
i=1
p(
?
S
i
|
?
X
i
;?
r
).
We formulate a model p(S|X;?) of a sequence
S of fixations given a text X. The model defines
a dynamic probabilistic process that successively
generates the fixation positions s
t
and durations d
t
in S, reflecting how a reader generates a sequence
of saccades in response to a text stimulus X. The
joint distribution over all fixation positions and du-
rations is assumed to factorize as
p(s
1
, . . . , s
T
, d
1
, . . . , d
T
|X;?)
= p(s
1
, d
1
|X;?)
T?1
?
t=1
p(s
t+1
, d
t+1
|s
t
,X;?).
The conditional p(s
t+1
, d
t+1
|s
t
,X;?) models the
generation of the next fixation position and du-
ration given the current fixation position s
t
. In
the psychological literature, four different sac-
cadic types are distinguished: a reader can refix-
ate the current word (refixation), fixate the next
word in the text (next word movement), move the
1811
fixation to a word after the next word (forward
skip), or regress to fixate a word occurring ear-
lier than the currently fixated word in the text (re-
gression) (Heister et al., 2012). We observe em-
pirically, that modeling the amplitude as a mixture
of four Gamma distributions matches the empiri-
cal distribution of amplitudes in our data well?
see Figure 1. Modeling the amplitudes as a sin-
gle distribution, instead of a mixture of four dis-
tributions, results in a substantially lower out-
of-sampling likelihood of the model. Therefore,
at each time t, the model first draws a saccadic
type u
t+1
? {1, 2, 3, 4} from a multinomial dis-
tribution and then generates a saccade amplitude
a
t+1
and fixation duration d
t+1
from type-specific
Gamma distributions. Formally, the generative
process is given by
u
t+1
? p(u|pi) = Mult(u|pi) (3)
a
t+1
? p(a|u
t+1
, s
t
,X;?) (4)
d
t+1
? p(d|u
t+1
;?). (5)
Afterwards the model updates the fixation position
according to s
t+1
= s
t
+ a
t+1
. The joint param-
eter vector ? concatenates parameters of the indi-
vidual distributions in Equations 3 to 5. Figure 2
shows a slice in the dynamical model.
Given the current fixation position s
t
, the text
X, and the chosen saccadic type u
t+1
, the am-
plitude is constrained to fall within a specific
interval?for instance, within the characters of the
currently fixated word for refixations. Therefore,
we model the distribution over the saccade ampli-
tude given the saccadic type (Equation 4) as trun-
cated Gamma distributions, given by
1
G(x|[l, r];?, ?) =
{
G(x|?,?)?
r
l
G(x?|?,?)dx?
if x ? [l, r]
0 otherwise
where G(x|?, ?) =
1
?
?
?(?)
x
??1
e
?x
?
is the Gamma distribution with shape parameter
? and scale parameter ?, and ? is the Gamma
function. For x ? G(x|?, ?) it holds that
G(x|[l, r];?, ?) is the conditional distribution of
x given that x ? [l, r]. The distribution over a sac-
1
The definition is straightforwardly generalized to open
truncation intervals.
tu 1tu ?ta 1ta ?td 1td ?t 1t ?Xts 1ts ?
Figure 2: Graphical model notation of a slice in
the dynamic model. Parameters are omitted to
avoid notational clutter.
cade amplitude given the saccade type is given by
p(a|u
t+1
= 1, s
t
,X;?) =
{
?G(a|[0, r];?
1
, ?
1
) if a > 0
(1? ?)G(?a|[0, l]; ??
1
,
?
?
1
) otherwise
(6)
where the parameter ? reflects the probability for
a forward saccade within a refixation, and
p(a|u
t+1
= 2, s
t
,X;?) = G(a|[l
+
, r
+
];?
2
, ?
2
)
p(a|u
t+1
= 3, s
t
,X;?) = G(a|(r
+
,?);?
3
, ?
3
)
p(a|u
t+1
= 4, s
t
,X;?) = G(?a|(?l,?);?
4
, ?
4
).
(7)
Here, the truncation intervals reflect the con-
straints on the amplitude a
t+1
given u
t+1
, s
t
and
X. Let w
l
(w
r
) denote the position of the left-
most (right-most) character of the currently fix-
ated word, and let w
+
l
, w
+
r
denote these positions
for the word following the currently fixated word.
Then l = w
l
? s
t
, r = w
r
? s
t
, l
+
= w
+
l
? s
t
,
and r
+
= w
+
r
? s
t
. The parameter vector ?
contains the parameters ?, ??
1
,
?
?
1
and ?
i
, ?
i
for
i ? {2, 3, 4}.
The distribution over fixation durations given
saccade type is modeled by a Gamma distribution
p(d|u
t+1
;?) = G(d|?
u
t+1
, ?
u
t+1
)
with type-specific parameters ?
u
, ?
u
for
u ? {1, 2, 3, 4} that are concatenated into a
parameter vector ?.
It remains to specify the distribution over initial
fixation positions and durations p(s
1
, d
1
|X;?),
which is given by additional Gamma distributions
s
1
? G(d|?
0
, ?
0
) d
1
? G(d|?
0
, ?
0
)
1812
0 10 20 30 40 50 60 700
0.2
0.4
0.6
0.8
1
number of training sentences n
ident
ificat
ion a
ccura
cy
Accuracy Over Training Sentences n (m=72)
 
 
full modelsaccade type + amplitudesaccade type onlyHolland & K. (weighted)Holland & K. (unweighted)random guessing
0 10 20 30 40 50 60 700
0.2
0.4
0.6
0.8
1
number of test sentences m
ident
ificat
ion a
ccura
cy
Accuracy Over Test Sentences m (n=72)
 
 
full modelsaccade type + amplitudesaccade type onlyHolland & K. (weighted)Holland & K. (unweighted)random guessing
Figure 3: Reader identification accuracy as a function of the number of training sentences (left) and test
sentences (right) read for different model variants. Error bars indicate the standard error.
where the parameters ?
0
, ?
0
, ?
0
, ?
0
are aggregated
into the joint parameter vector ?.
4 Parameter Estimation and Inference
Given a set S
(r)
of eye movement observations for
reader r ? R on texts X , the MAP estimate of the
parameters is
?
r
= arg max
?
p(?|S
(r)
,X )
= arg max
?
(
n
?
i=1
p(S
(r)
i
|X
i
;?)
)
p(?). (8)
A Dirichlet distribution (add-one smoothing) is a
natural, conjugate prior for the multinomial distri-
bution; we use uninformative priors for all other
distributions. The structure of the model implies
that the posterior can be maximized by fitting the
parameters pi to the observed saccadic types un-
der the Dirichlet prior, and independently fitting
the distributions p(a
t
|u
t
,X, s
t
;?) and p(d
t
|u
t
;?)
by maximum likelihood to the saccade amplitudes
and durations observed for each saccade type.
The resulting maximum likelihood problems are
slightly non-standard in that we have to fit Gamma
distributions that are truncated differently for each
data point, depending on the textual content at
the position where the saccade occurs (see Equa-
tions 6 and 7). We solve the resulting optimization
problems using a Quasi-Newton method. To avoid
overfitting, we use a backoff-smoothing technique
for p(a
t
|u
t
,X, s
t
;?) and p(d
t
|u
t
;?): we replace
reader-specific parameter estimates by estimates
obtained from the corresponding data of all read-
ers if the number of data points from which the dis-
tributions are estimated falls below a cutoff value.
The cutoff value is tuned by cross-validation on
the training data.
At test time, we have to infer likelihoods
p(S
i
|X;?
r
) (Equation 2). This is done by evalu-
ating the multinomial and (truncated) Gamma dis-
tributions in the model for the corresponding ob-
servations and model parameters.
5 Empirical Study
We empirically study the proposed model and sev-
eral baselines using eye-movement records of 20
individuals (Heister et al., 2012). For each indi-
vidual, eye movements are recorded while read-
ing each of the 144 sentences in the Potsdam Sen-
tence Corpus (Kliegl et al., 2006). The data set
contains fixation positions and durations that have
been obtained from raw eye movement data by
appropriate preprocessing. Eye movements were
recorded with an EyeLink II system with a 500-
Hz sampling rate (SR Research, Osgoode, On-
tario, Canada). All recordings and calibration
were binocular. We randomly sample disjoint
sets of n training sentences and m test sentences
from the set of 144 sentences. Models are esti-
mated on the eye movement records of individu-
als on the training sentences (Equation 8). The
eye-movement records of one individual on all test
sentences constitute a test example; the model in-
fers the most likely individual to have generated
these test observations (Equation 2). Identifica-
tion accuracy is the fraction of times an individ-
ual is correctly inferred; random guessing yields
an accuracy of 0.05. Results are averaged over 20
training and test sets.
We study the model introduced in Section 3
1813
train
ing s
enten
ces n
test sentences m
Accuracy Over Training and Test Sentences
 
 
0 20 40 60
0
10
20
30
40
50
60
70
accuracy0
0.2
0.4
0.6
0.8
1
0 5 10 15 200.5
0.6
0.7
0.8
0.9
1
number of individuals
ident
ificat
ion a
ccura
cy
Accuracy Over Number of Individuals
 
 
full modelsaccade type + amplitudesaccade type onlyHolland & K. (weighted)Holland & K. (unweighted)
Figure 4: Identification accuracy as a function of the number of training and test sentences read for
full model (left). Identification accuracy as a function of the number of individuals that have to be
distinguished for different model variants (right). Error bars indicate the standard error.
(full model), a model variant in which the variable
d
t+1
and corresponding distribution is removed
(saccade type + amplitude), and a simple model
that only fits a multinomial distribution to saccade
types (saccade type only). Additionally, we com-
pare against the feature-based reader identification
approach by Holland & Komogortsev (2012). Six
of the 14 features used by Holland & Komogort-
sev depend on saccade velocities and vertical fix-
ation positions. As this information was not avail-
able in the preprocessed data set that we used, we
implemented the remaining features. There is ex-
tensive empirical evidence that saccade velocity
scales with saccade amplitude. Specifically, the
relationship between logarithmic peak saccade ve-
locity and logarithmic saccade amplitude is lin-
ear over a wide range of amplitudes and veloci-
ties; this is known as the main sequence relation-
ship (Bahill et al., 1975). Therefore, we do not ex-
pect that adding saccade velocities would dramat-
ically affect performance of this baseline. Holland
& Komogortsev employ a weighted combination
of features; we report results for the method with
and without feature weighting.
Figure 3 shows identification accuracy as a
function of the number n of training sentences
used to estimate model parameters (left) and as
a function of the number m of test sentences on
which inference of the most likely reader is based
(right, cf. Equation 2). The full model achieves
up to 98.25% accuracy, significantly outperform-
ing the Holland & Komogortsev (2012) baseline
(91.25%, without feature weighting) and simpler
model variants. All methods perform much better
than random guessing. Figure 4 (left) shows iden-
tification accuracy as a function of both training
size n and test size m for the full model.
We finally study how identification accuracy
changes with the number of individuals that have
to be distinguished. To this end, we perform the
same study as above, but with randomly sampled
subsets of the overall set of 20 individuals. In these
experiments, we average over 50 random train-test
splits. Figure 4 (right) shows identification ac-
curacy as a function of the number of individu-
als. We observe that identification accuracy drops
with the number of individuals for all methods;
our model consistently outperforms the baselines.
6 Conclusions
We have developed a model of individual differ-
ences in eye movements during reading, and stud-
ied its application in a biometric task. At test time,
individuals are identified based on eye movements
on novel text. Our approach thus provides poten-
tially unobtrusive biometric identification without
requiring users to react to a specific stimulus. Em-
pirical results show clear advantages over an exist-
ing approach for reader identification.
Acknowledgments
We would like to thank Christoph Sawade for in-
sightful discussions and help with the eye move-
ment data. We gratefully acknowledge support
from the German Research Foundation (DFG),
grant LA 3270/1-1.
1814
References
A. Terry Bahill, Michael R. Clark, and Lawrence Stark.
1975. The main sequence: a tool for studying hu-
man eye movements. Mathematical Biosciences,
24:191?204.
Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, and
Pasi Fr?anti. 2005. Eye-movements as a biometric.
In Proceedings of the 14th Scandinavian Conference
on Image Analysis.
W. Robert Dixon. 1951. Studies in the psychology of
reading. In W. S. Morse, P. A. Ballantine, and W. R.
Dixon, editors, Univ. of Michigan Monographs in
Education No. 4. Univ. of Michigan Press.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: A dynamical
model of saccade generation during reading. Psy-
chological Review, 112(4):777?813.
Bruno Erdmann and Raymond Dodge. 1898. Psy-
chologische Untersuchungen ?uber das Lesen. Halle:
Max Niemeyer.
Tadayoshi Hara, Daichi Mochihashi, Yoshino Kano,
and Akiko Aizawa. 2012. Predicting word fixations
in text with a CRF model for capturing general read-
ing strategies among readers. In Proceedings of the
First Workshop on Eye-Tracking and Natural Lan-
guage Processing.
Julian Heister, Kay-Michael W?urzner, and Reinhold
Kliegl. 2012. Analysing large datasets of eye move-
ments during reading. In James S. Adelman, editor,
Visual word recognition. Vol. 2: Meaning and con-
text, individuals and development, pages 102?130.
Corey Holland and Oleg V. Komogortsev. 2012. Bio-
metric identification via eye movement scanpaths in
reading. In Proceedings of the 2011 International
Joint Conference on Biometrics.
Edmund B. Huey. 1908. The psychology and peda-
gogy of reading. Cambridge, Mass.: MIT Press.
Pawel Kasprowski and Jozef Ober. 2004. Eye move-
ments in biometrics. In Proceedings of the 2004 In-
ternational Biometric Authentication Workshop.
Reinhold Kliegl, Antje Nuthmann, and Ralf Engbert.
2006. Tracking the mind during reading: The in-
fluence of past, present, and future words on fix-
ation durations. Journal of Experimental Psychol-
ogy: General, 135(1):12?35.
Oleg V. Komogortsev, Sampath Jayarathna, Cecilia R.
Aragon, and Mechehoul Mahmoud. 2010. Biomet-
ric identification via an oculomotor plant mathemat-
ical model. In Proceedings of the 2010 Symposium
on Eye-Tracking Research & Applications.
Franz Matties and Anders S?gaard. 2013. With blink-
ers on: robust prediction of eye movements across
readers. In Proceedings of the 2013 Conference on
Empirical Natural Language Processing.
Mattias Nilsson and Joakim Nivre. 2009. Learning
where to look: Modeling eye movements in reading.
In Proceedings of the 13th Conference on Computa-
tional Natural Language Learning.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
Erik D. Reichle, Tessa Warren, and Kerry McConnell.
2012. Using e-z reader to model the effects of
higher-level language processing on eye movements
during reading. Psychonomic Bulletin & Review,
16(1):1?21.
Ioannis Rigas, George Economou, and Spiros Fotopou-
los. 2012a. Biometric identification based on
the eye movements and graph matching techniques.
Pattern Recognition Letters, 33(6).
Ioannis Rigas, George Economou, and Spiros Fotopou-
los. 2012b. Human eye movements as a trait for bio-
metrical identification. In Proceedings of the IEEE
5th International Conference on Biometrics: The-
ory, Applications and Systems.
Daniel Schad and Ralf Engbert. 2012. The zoom lens
of attention: Simulating shuffled versus normal text
reading using the swift model. Visual Cognition,
20(4-5):391?421.
Youming Zhang and Martti Juhola. 2012. On biomet-
ric verification of a user by means of eye movement
data mining. In Proceedings of the 2nd Interna-
tional Conference on Advances in Information Min-
ing and Management.
1815
