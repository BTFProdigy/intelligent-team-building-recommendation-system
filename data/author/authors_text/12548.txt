Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 587?595, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Combining Multiple Forms of Evidence While Filtering
Yi Zhang ?
Information System and Technology Management
School of Engineering
University of California, Santa Cruz
Santa Cruz, CA 95064, USA
yiz@soe.ucsc.edu
Jamie Callan
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
callan@cs.cmu.edu
Abstract
This paper studies how to go beyond relevance
and enable a filtering system to learn more in-
teresting and detailed data driven user models
from multiple forms of evidence. We carry out
a user study using a real time web based per-
sonal news filtering system, and collect exten-
sive multiple forms of evidence, including ex-
plicit and implicit user feedback. We explore
the graphical modeling approach to combine
these forms of evidence. To test whether the ap-
proach can help us understand the domain bet-
ter, we use graph structure learning algorithm
to derive the causal relationships between dif-
ferent forms of evidence. To test whether the
approach can help the system improve the per-
formance, we use the graphical inference algo-
rithms to predict whether a user likes a docu-
ment based on multiple forms of evidence. The
results show that combining multiple forms
of evidence using graphical models can help
us better understand the filtering problem, im-
prove filtering system performance, and handle
various data missing situations naturally.
1 Introduction
An adaptive personal information filtering system is an
autonomous agent that delivers information to the user in
a dynamic environment over a period of time. A com-
mon filtering approach is adapting existing text classi-
fication/retrieval algorithms to classify incoming docu-
ments as either relevant or non relevant using user pro-
files learned from explicit user feedback on documents
the user has seen. However, there are other important
criteria for the user besides relevance, such as readabil-
ity (Collins-Thompson and Callan, 2004), novelty (Har-
man, 2003), and authority (Kleinberg, 1998). Besides,
much information about the user and the document can
be collected by a filtering system. These suggest a way to
improve the current filtering system: going beyond rele-
vance and using multiple forms of evidence.
?This research was done while at the Language Technolo-
gies Institute, Carnegie Mellon University.
Unfortunately, there is no standard evaluation data set
for this research, and there is not much work on finding
a good theory to combine various forms of evidence. To
solve the first problem, we designed a user study and col-
lect thousands of cases with multiple forms of evidence,
including the content of a document, explicit and im-
plicit user feedback, such as a user?s mouse usage, key
board usage, document length, novelty, relevance, read-
ability, authority, user profile characteristics, news source
information, and whether a user likes a document or not.
Solving the second problem is very challenging. A good
model should have the representation power to combine
multiple forms of evidence; it should be able to help us
understand the relationships between various forms of ev-
idence; it should use the evidence to improve filtering
system performance; and it should handle various prob-
lems like missing data in an operational environment ro-
bustly.
On the other hand, researchers have identified three
major advantages of graphical modeling approach: 1) it
provides inference tools to naturally handle situations of
missing data entry because of the conditional dependen-
cies encoded in the graph structure; 2) it can learn causal
relationships in the domain, thus help us to understand
the problem and to predict the consequences of interven-
tion; and 3) it can easily combine prior knowledge (such
as partial information about the causal relationship) with
data in this framework. This approach has been applied
to model computer software users (Horvitz et al, 1998),
car drivers (Pynadath and Wellman, 1995), and students
(Conati et al, 1997). Motivated by the prior work, we
choose to use graphical models as our solution. To under-
stand relationships between various forms of evidence,
we use the causal graph structure learning algorithms (ad-
vantage 2), together with some prior knowledge of the
domain (advantage 3), to derive the causal relationships
between different user feedback, actions and user con-
text. To improve the existing filtering system, especially
in the situation of missing data, we use statistical infer-
ence tools to predict how a user will like a document,
using information available in different missing evidence
situations (advantage 1). We also try linear regression as
an alternative approach.
The following sections describe our efforts towards
587
Figure 1: The user study system structure. The structured
information, such as user feedback and crawler statistics,
are kept in the database. The content of each web page
crawled is saved in the news repository.
collecting data and customizing the graphical modeling
approach to combine multiple forms of evidence for fil-
tering. We begin with a description of the user study in
Section 2, followed by some preliminary data analysis
on the data collected in Section 3. Section 4 explores
causal structure learning algorithm to understand the re-
lationships between various forms of evidence from the
data and Section 5 explores how to improve the system
performance using multiple forms of evidence. Section
6 discusses related work and how this work differs from
existing work, and Section 7 concludes.
2 User Study
No existing filtering database contains the level of detail
that we needed for our study, so we developed a web
based news story filtering system to collect an evalua-
tion data set (Figure 1). This system constantly gathers
and recommends information to the users. The system
includes a crawler with 8000 candidate RSS news feeds
(Pilgrim, 2002) to crawl every day. The Lemur indexer
indexes the crawled document stream incrementally, and
an adaptive filtering system recommends documents to
the users using a modified logistic regression algorithm
(Zhang, 2004). Users read and evaluate what the system
has delivered to them. An example of the web interface
after user login is in Figure 2.
More than 20 paid subjects from 19 different programs
at Carnegie Mellon University, who are otherwise not af-
filiated with our research, participated in the study for 4
weeks. We expected to collect enough data for evalua-
tion over this period of time. The subjects were required
to read the news for about 1 hour per day and provide
explicit feedback for each page they visited. 1 28 users
1In the last week of the study, some subjects read 2 hours
per day. They are encouraged but not required to do so.
Figure 2: Web interface after a user logged in.
Figure 3: Evaluation user interface. The interface for user
to give their explicit feedback of the current news story.
tried this system. However, only 21 users are official paid
subjects, among which one worked only for 2 weeks and
20 worked for about 4 weeks.
2.1 Data collected
We have collected 7881 feedback entries from all 28
users, among which 7839 were from the 21 official par-
ticipants. Each entry contains several different forms of
evidence for a news story a user clicked.2 Our intention
to collect the evidence is not to be exhaustive, but repre-
sentative. The evidence can be roughly classified into the
following five categories listed in Tables 1 to 5.3
Explicit user feedback After finishing reading a news
story, a user clicks a button on the toolbar of the
browser to bring up an evaluation interface shown in
Figure 3. Through this interface, the user provided
the explicit feedback to tell the hidden properties
about current story, including the topics the news
belongs to (classes), how the user likes this news
2Each entry is for a <document, user class, time> tuple.
3The forms of evidence are listed in the first column and we
will get the the other columns later in Section 3.
588
(user likes), how relevant the news is related to the
class(es) (relevant), how novel the news is (novel),
whether the news matches the readability level of
the user (readable), and whether the news is au-
thoritative (authoritative). user likes, relevant
and novel are recorded as integers ranging from 1
(least) to 5 (most). readable and authoritative are
recorded as 0 or 1. A user has the option to provide
partial instead of all explicit feedback. A user can
create new classes, and choose multiple classes for
one documents.
User actions The browser adapted from (Claypool et al,
2001) recorded some user actions, such as mouse ac-
tivities, scroll bar activities, and keyboard activities
(Table 2). TimeOnPage is the number of seconds the
user spent on a page, and EventOnScroll is the num-
ber of clicks on the scroll bars. When the mouse
is out of the browser window or when the browser
window is not focused, the browser does not capture
any activities. More details about the actions are in
(Le and Waseda, 2000).
Topic information Each participant filled out an exit
questionnaire and answered several topic/class4 spe-
cific questions for each of his/her most popular 10
topics and other topics with more than 20 evalu-
ated documents each (Table 3). The questions in-
clude how familiar the user is with the topic be-
fore the study (topic familiar before), how the user
likes this topic (topic like), and how confident the
user is with respect to the answers he/she provided
(topic confidence). We include this information
as evidence, because they may be collected when
a topic is created and used by filtering systems.
Whether collecting them in exit questionnaire af-
fects the answers needs further investigation.
News Source Information For each news source (RSS
feed), we collected the number of web pages that
link to it (RSS link), the number of pages that link to
the server that provided it (host link), and the speed
of the server that hosts it.
Content based evidence Three pieces of evidence are
collected to represent the content of each document:
the relevance score, the readability score and the
number of words in the document (doc len) (Table
5). To estimate the relevance score of a document,
the system processes all the documents a user put
into a class ordered by the feedback time and adap-
tively learns a topic specific relevance model using
the relevance feedback the user provided. The rel-
evance score of a documents is estimated using a
4
?topic? and ?class? are used interchangeably in the paper.
Table 1: Basic descriptive statistics about explicit feed-
backs.
Variable Mean variance corr miss
user likes 3.5 1.2 1 0.05
relevant 3.5 1.3 0.73 0.005
novel 3.6 1.33 0.70 0.008
authoritative 0.88 0.32 0.50 0.065
readable 0.90 0.30 0.54 0.012
modified logistic regression model learned from all
feedback before it (Zhang, 2004). To estimate the
readability score of document, the system processes
all the documents in all users? classes ordered by the
feedback time and adaptively learns a user indepen-
dent readability model using a logistic regression al-
gorithm.
3 Preliminary data analysis
The means and variances of all variables are in Tables 1 to
5. These basic descriptive statistics are very diverse. The
values of some evidence may be missing; only the user
actions and news source information were always col-
lected. Out of the 7991 entries, only 4522 (57%) entries
contain no missing value. The missing rate of each form
of evidence is also reported in the tables. There are sev-
eral reasons for missing data. For example, the explicit
feedback is missing because users didn?t always follow
instructions, the relevance score is missing for the first
story in a class, and the topic familiar before values
for many topics are missing because we only collected
the topic specific answers for larger topics. We expect
missing data to be common in operational environments.
The correlation coefficient between each evidence and
the explicit feedback user likes is also listed (corr).
The high correlation coefficients between user likes and
other forms of explicit feedback are not very interest-
ing because we can only get explicit feedback after a
user reads the document. The correlation coefficient be-
tween relevance score and user likes is 0.37, the highest
among all forms of evidence that the system can get be-
fore delivering a document. This is not surprising since
most filtering systems only consider relevance and use
relevance score to make decisions.
The correlation coefficients between user likes and
the topic information (Table 3) are relatively high.
This suggests collecting topic familiar before or
topic like in a real filtering system, since they are in-
formative and collecting them requires less user effort (a
user only needs to provide information on the class level
instead of document level). Section 5 will show how to
use it with other forms of evidence in a filtering system.
The correlation coefficients between the news source in-
589
Table 2: Basic descriptive statistics about user actions.
The unit for time is second.
Variable Mean variance corr
TimeOnPage 7.2? 104 1.3? 105 0.14
EventOnScroll 1 3.6 0.1
ClickOnWindow 0.93 2.5 0.05
TimeOnMouse 2? 103 5.8? 103 0.02
MSecForDownArrow 211 882 0.08
NumOfDownArrow 1.1 4.7 0.09
MSecForUpArrow 29 240 0.03
NumOfUpArrow 0.10 0.8 0.04
NumOfPageUp 0.12 0.9 ' 0
NumOfPageDown 0.14 1 ' 0
MSecForPageUp 22 202 ' 0
MSecForPageDown 28 251 ' 0
Table 3: Basic descriptive statistics about topics. Each
variable ranges from 1 to 7.
variable Mean variance corr miss
topic familiar before 3.6 1.9 0.30 0.27
topic like 4.9 2.0 0.30 0.27
topic confidence 4.7 2.0 0.34 0.27
Table 4: Basic descriptive statistics about news sources.
variable Mean variance corr
RSS link 90.35 4.89 0.14
host link 4.41? 104 7.5? 107 0.08
RSS SPEED 3.92? 105 3.7? 109 -0.08
Table 5: Basic descriptive statistics about documents.
The length of the document does not include HTML tags.
variable mean variance corr miss
doc length 837 1.2? 103 0.04 0.05
relevant score 0.49 0.42 0.37 0.18
readability score 0.52 0.16 0.25 0.11
formation and user likes are weaker (Table 4). The cor-
relation coefficient between user likes and each user ac-
tion (Table 2) is even lower (Table 1). Some actions, such
as TimeOnPage, are more correlated with user likes
than other refined actions, such as NumOfPageDown.
This finding agrees with (Claypool et al, 2001).
4 Understanding the domain using causal
structure learning
Correlation analysis in Section 3 has helped us to get
some initial idea about the data collected. However, in
order to better understand the underlying truth of the do-
main, we need to go beyond correlation and uncover the
causal relationships between different variables.
To do that, we first specify N nodes, one for each form
of evidence to be included in the model. Then PC algo-
rithm is used (Spirtes et al, 2000) to search the causal
relationships between multiple forms of evidence from
the data collected. To make the search space smaller,
some prior domain knowledge, such as forbidden edges,
required edges or temporal tiers, can be introduced be-
fore searching. In our experiments, we manually spec-
ified some prior knowledge based on the first authors?
experience and intuition as the following 5-tier tempo-
ral tier: 5 1) Topic info = (familiar topic before),
RSS info =(RSS link, host link), document length
(doc len); 2) hidden criteria, such as relevant, novel,
authoritative, and readable; 3) system generated
scores, such as relevance score and readability score; 4)
user likes; 5) user actions, such as seconds spent on a
page (TimeOnPage) or the number of clicks on the ? key
(NumOfDownArrow). This informs the learning algo-
rithm that? from a higher level to lower level is prohib-
ited.
It is very encouraging to see that the structure learned
automatically looks reasonable (Figure 4). Accord-
ing to the graph, novel, relevant, authoritative,
readabilty of a document and whether a user is
familiar with the topic before using the system
(familar topic before) are direct causes of the user?s
preference for a document (user likes) . How fa-
miliar with this topic a user is before participating
the study (topic familiar before) and the number of
web links to the news source (RSS link) directly af-
fect the user?s relevant and authoritative feedback
and readability score. Relevant, authoritative,
familiar topic before and host link influence a user?s
actions, such as the EventOnScroll.
Comparing Tables 2 to 5 with Figure 4, one may ask
why some variables are correlated with user likes al-
though there is no direct links between them and user
likes. For example, why the correlation between rele-
vance score and user likes is 0.39, while there is no di-
rect link between them. Does Figure 4 contradict Ta-
ble 5? The answer is ?no?. In fact the indirect causal
relationship between them tells us why relevance score
and user likes are correlated: relevance score and user
likes have a common cause relevant. Most of the re-
fined actions, such as the number of pressing page up key
(NumOfPageUp), are far away from user likes. This
implies that these refined actions are not very informative
if we want to use the learned model to predict whether
a user likes a document or not. This finding agree with
(Claypool et al, 2001) and Table 2.
The node authoritative is directly linked to
readability score and host link. The link between
host link and authoritative confirms the existing ap-
proaches that use the web link structure to estimate the
5Other priors are also possible.
590
Figure 4: User independent causal graphical structure
learned using PC algorithm. X ? Y means X is a di-
rect cause of Y. X ?Y means the algorithm cannot tell if
X causes Y or if Y causes X. X ?? Y means the algo-
rithm found some problem, which may happen due to a
latent common cause of X and Y, a chance pattern in the
sample, or other violations of assumptions.
Figure 5: Structure of GM complete.
Figure 6: Structure of GM causal.
authority of a page (Kleinberg, 1998). The links between
readability score, readable and authoritative are very
interesting. They suggest the difficulty to understand a
page may make the user feel it is not authoritative. Fur-
ther investigation shows that although the percentage of
un-authoritative news is less than 15% in general, among
the 187 news stories some users identified as ?difficult?
using class labels, 73% were also rated as not authorita-
tive. Besides some successful web page authority algo-
rithms that only use hyper links, the estimation of author-
ity may be further improved using the content of a page.
There are links among relevant, novel, readable and
authoritative. Although the algorithm failed to tell the
causal direction between some pairs of variables, it sug-
gests that the four variables influence each other. This
may be an inherent property of the document; or because
a user is likely to rate one aspect of the document higher
than he/she should if the other aspects are good.
One may ask why the structure in Figure 4 contains
no link between readable and readability score, since
intuitively it should exist. To answer this question, one
needs to understand that the causal relationships learned
automatically are what the algorithm ?believes? based on
the evidence of the data, the assumptions it makes, and
the prior constraints we engineered. They may have er-
rors, because the data is noisy, or the assumptions and
the prior constraints may be wrong. For example, the PC
algorithm do statistical test about the independence re-
lationships among variables using the data and the final
results are subject to the error of the statistical test. The
PC algorithm assumes no hidden variables, however be-
sides relevant, novel, authoritative, and readable, other
hidden variables, such as whether a document is up-to-
date, interesting, misleading, etc. (Schamber and Bate-
man, 1996), may exist and influence a user?s preference
for a document. Thus it is not surprising that some of the
causal relationships, such as the link between readable
and readability score, are missed in the final graph be-
cause of the limitation of the learning algorithms. The
model learned only sheds some light on the relationships
between the variables instead of uncovering the whole
truth. It only serves as a starting point for us. To further
understand the domain, we may want to break down some
variables in the current graph further and relate them to
either the user or document properties. In general, causal
discovery is inherently difficult and far from solved.
5 Improving system performance using
inference algorithms
A primary task of a filtering system is to predict user
preference (user likes) for a document so that the sys-
tem can decide whether to deliver it to the user. To
tell whether combining multiple forms of evidence using
591
graphical models can improve system performance, we
evaluate the proposed solution on the task of predicting
user likes while filtering.
To predict user likes, the system needs to learn a
graphical model: the combination of a graph structure
and a set of local conditional probability functions or po-
tential functions. Doing inference over the causal struc-
ture learned in the previous section is difficult because of
the circles and a mixture of directed and undirected links
on the graph. So, we tried the following directed acyclic
graphical models.
GM complete, an almost complete Bayesian network:
In this graph, we order the nodes from top to bot-
tom, and the parents of a node are all the nodes
above it, such as in Figure 5. For this structure, the
order of the nodes is not very important when using
Gaussian distributions.
GM causal, a graphical model inspired by causal models:
We manually modify the causal structure in Figure
4 to make it a directed acyclic graph as in Figure 6.
In the graphs, RSS info=(RSS link, host link) and Topic
info=topic familiar before, topic like) are 2 dimensional
vectors representing the information about the news
source and the topic in Table 4 and Table 3. actions =
(TimeOnPage, ...) is a 12 dimensional vector repre-
senting the user actions in Table 1. user likes is the
target variable the system wants to predict.
Before learning the parameters of the model, we need
to choose a specific conditional form for the probability
function associated with each node. We chose Gaussian
distributions. If the parents of node X are Y, P (X|Y ) =
N(m + W ? Y,?), where N(?,?) is a gaussian distri-
bution with mean ? and covariance ?. This is a com-
monly used distribution for continuous valued nodes. It
assumes the joint distribution of these variables is mul-
tivariate Gaussian, which may be wrong. Nevertheless,
because of the mathematical convenience, the existence
of efficient learning and inference algorithms for Gaus-
sian networks, and the availability of modeling tools, we
chose this distribution. Using the BNT Toolbox (Mur-
phy, 2001), the maximum likelihood estimations of the
parameters (m,W,?) were learned using EM algorithm
and junction tree inference engine(Cowell et al, 1999)
over the graphical models, with whatever information
was available on the first 2/3 of the data.
An alternative approach to combine multiple forms of
evidence is linear regression. We tried two special meth-
ods to solve the missing evidence problem while using
linear regression: 1) building a model that does not use
the evidence that is missing for each missing situation
(LR different); or 2)mean substitution: replacing each
missing value for an evidence with the average of the
Figure 7: Comparison of the prediction power of differ-
ent models using 7952 cases for evaluation. The vertical
axis is the correlation coefficient between the predicted
value of user likes using the model and the true explicit
feedback provided by the users. The order of different
forms of evidence is set manually, based on how easy it
is to collect each evidence.
observed evidence (LR mean). For K different forms
of evidence, the system may need to handle 2K differ-
ent evidence missing situations. A large number of linear
regression models need to be learned if we use the first
approach, considering K is higher than 15 in some of our
experiments. Building 215 models is almost impossible
for us, so a heuristic approach, which is discussed later,
was used to make the experiments possible.
Not all 7991 cases collected in the user study were
used in the experiments. We conducted two sets of ex-
periments. For the first set of experiments, we use 7952
cases for which user likes is not missing. For the other
set of runs, we use only cases without missing value. In
this task, the value of each variable is continuous and nor-
malized to variance one. Each model is learned using all
information available on the first 2/3 of the cases, and
tested on the remaining 1/3 of the cases. The correlation
coefficient between the predicted value of user likes and
the true explicit user likes feedback provided by the
users is used as the evaluation measure. Our baseline is
using relevance score alone, which has a correlation co-
efficient of 0.367 with 95% confidence interval 0.33-0.40
on the last 1/3 of the 7952 cases.
5.1 Experimental results and discussions
Figure 7 shows the effectiveness of different models
at different testing conditions as indicated by the hor-
izontal axis. From left to right, additional sources
of evidence are given when testing. At the very left
of the figure (x=RSS info), a model predicts the
592
value of user likes only given the value of RSS info
at testing time. ?+explicit? means the explicit feed-
back (except user likes) about the current document is
given besides the value of actions, relevance score,
readability score, RSS info, and TopicInfo. The
graphical models and LR mean model were trained with
all evidence/features, and the learned models are inde-
pendent of the testing condition. LR different models
were only trained with features that are also provided at
testing time, so there is one model per testing condition.
6
The results show that GM complete performs sim-
ilarly to LR different. This is not surprising. Theo-
retically, if there is no missing entries in training data,
GM complete?s estimation of the conditional distribu-
tion of P (user likes|available evidence) would be the
same as that of LR different on a testing case with miss-
ing evidence.
Comparing the correlation coefficients under dif-
ferent testing conditions when using LR different or
GM complete, we can see that as more forms of ev-
idence are available, the performance improves. If
only the news source information of a document
(RSS info) is given, all models perform poorly. The
readability score improves the system performance sig-
nificantly. This is nice and interesting, because the evi-
dence is user independent and can be estimated efficiently
for each document. The performance keeps improving
as topic info and relevance score were added. To
collect them, we needs user feedback on previous doc-
uments. The performance improvement is not very ob-
vious with actions added. This means that given other
evidence (RSS info, topic info, relevance score and
readability score), the system won?t improve its predic-
tion of the document much by observing these actions.
However, this is only true when we use a model learned
for all users and other forms of evidence are available. It
does not mean the actions are useless if we learn user
specific model, or if other forms of evidence (such as
relevance score) are not available. All models perform
very good with explicit feedback added. However, this is
a ?cheating? condition of less interest to us.
The performances of LR mean and GM causal
do not increase monotonically as more forms of ev-
idence are added. They perform much worse than
LR different and GM complete. Why does a structure
that looks more causally reasonable not perform well
6However, for a specific testing condition, the training data
and testing data contain cases where some evidence that is sup-
posed to be available is missing. These cases in training data
were ignored and not used to learn a LR different model.
However, ignoring such kind of cases in testing data makes
comparison of different runs difficult. So we used mean sub-
stitution approach to fill the required missing features in testing
data while using LR different.
Model Cond. corr RLow RUp
LR mean +R 0.2783 0.2426 0.3132
LR different +R 0.4372 0.4058 0.4677
GM complete +R 0.4247 0.3928 0.4555
GM causal +R 0.3078 0.2728 0.342
LR mean +A 0.2646 0.2286 0.2998
LR different +A 0.4375 0.406 0.4679
GM complete +A 0.4315 0.3999 0.4622
GM causal +A 0.3086 0.2736 0.3428
Table 6: A comparison of different models on all data un-
der the +relevance score (+R) and +action (+A) con-
ditions. Corr is the correlation coefficient between the
predicted value of user likes using the model and the
true explicit feedback provided by the users. RLO and
RUP are the lower and upper bounds for a 95% confi-
dence interval for each coefficient.
as the simple GM complete? We may answer this
question better by comparing the underlying assumptions
of these algorithms. GM complete only assumes the
joint distribution of all variables is multivariate Gaus-
sian. GM causal makes much stronger independence
assumptions by removing some links between variables.
As mentioned before, the causal relationships learned au-
tomatically are not perfect, which may cause the poor
performance of GM causal. LR mean also suffers
from the strong conditional independent assumptions.
Table 6 reports the performance together with the
confidence intervals of all the models under the
+relevance score and +actions conditions. Under
both conditions, GM complete and LR different are
statistically significantly better than the baseline 0.367.
LR mean and GM causal are significantly worse. It
means using multiple forms of evidence may hurt some
models and benefit others. Further analysis about the
+actions runs shows that LR mean gave explicit feed-
back too much weight and overlooked other less strong
evidence. At testing time, it did not handle the problem of
missing explicit feedback well and thus performed poorly.
Although GM complete also gave very high weights to
explicit feedback, it could infer the missing values based
on other available evidence at testing time, thus per-
formed better than LR mean. LR different didn?t con-
sider explicit feedback for training, thus it didn?t over-
look other forms of evidence and suffer from the problem
less. LR mean may work reasonably if explicit vari-
ables are not included, however the large difference on
how informative each evidence is will still hurt the per-
formance of LR mean to some extent when some strong
evidence is missing. For GM complete approach, a sin-
gle model is needed to handle various evidence missing
situations. If we use LR different approach, several mod-
els are needed. As we mentioned before, there are 2K
593
Model Cond. Corr RLow RUp
LR mean +R 0.13 0.08 0.18
LR different +R 0.41 0.37 0.45
GM complete +R 0.41 0.37 0.45
GM causal +R 0.41 0.375 0.45
LR mean +A 0.11 0.061 0.16
LR different +A 0.42 0.38 0.46
GM complete +A 0.42 0.38 0.46
GM causal +A 0.38 0.33 0.42
Table 7: The performance on 4522 no missing value cases
under the +relevance score (+R) and +action (+A)
conditions.
different evidence missing combinations, and 2K linear
regression models are needed in order to handle all these
situations using LR different approach. LR different may
be preferred if K is small, while graphical modeling us-
ing GM complete may be a better approach to handle
different data missing situations if K is big.
So far, all results are based on 7952 cases where
some evidence may be missing. We also compared
the models under different testing conditions using the
4522 cases that do not have any missing value (Table
7). GM causal performs significantly better than be-
fore. We need to be very careful with the structures while
using the graphical modeling approach, since a structure
that looks more reasonable may work poorly on the in-
ference task. However, we couldn?t not draw any con-
clusion on whether GM complete is better in general,
because the answer may be different with different con-
ditional probability distributions, different data sets, or a
better structure learning algorithm.
6 Related Work
There has been some research on news filtering using
time-coded implicit feedback (Lang, 1995; Morita and
Shinoda, 1994). We noticed that an independent work
uses a different graphical modeling approach, depen-
dency network, to understand the relationships between
implicit measures and explicit satisfaction while user
were conducting their web searches and viewing results,
and then uses decision tree to predict user satisfaction
with results (Fox et al, 2005). Our work differs from
the previous work in the goal of the task, the range of ev-
idence considered, the modeling approach we took, and
the findings reached.
There has been a lot of related research on using im-
plicit feedback (Kelly and Teevan, 2003). The user
actions we collected are based on (Claypool et al,
2001). There is much work about how to handle miss-
ing data. (Schafer and Graham, 2002) discussed several
approaches such as case deletion, mean substitution, and
recommended maximum likelihood (ML) and Bayesian
multiple imputation (MI). LR mean uses mean substitu-
tion, LR different uses case deletion, and graphical mod-
els follow the ML approach.
There has been some research on criteria beyond topic
relevance (Carbonell and Goldstein, 1998) (Zhang et al,
2002) (Collins-Thompson and Callan, 2004) (Kleinberg,
1998). (Schamber and Bateman, 1996) identified crite-
ria underlying users? relevance judgements and explored
how users employed the criteria in making evaluations
by asking users to interpret and sort criteria independent
of document manually. In the literature, the word ?rele-
vant? is used ambiguously, either as a narrow definition
of ?related to the matter at hand (aboutness)? or a broader
definition of ?having the ability to satisfy the needs of the
user?. When it is used by the second definition, such as
in (Schamber and Bateman, 1996), researchers are usu-
ally studying what we refer to as user likes. In this paper,
we use ?relevant? as is defined in the first definition and
use the phrase ?user likes? for the second definition. De-
spite the vocabulary difference, our work is motivated by
the early research. The major contributions of our work
in this area are: 1) we model the user likes and other cri-
teria as hidden variables; 2) we quantify the importance
of various criteria based on probabilistic reasoning; and
3) we have explored the new methodology for combining
these criteria with implicit and explicit user feedback.
7 CONCLUSION
We have explored how to combine multiple forms of evi-
dence using the graphical modeling approach. This work
is significant because it addresses some long-standing is-
sues in the adaptive information filtering community: the
integration of a wider range of user-specific and user-
independent evidence, and handling situations like miss-
ing data that occur in operational environments.
We have analyzed the user study data using graphical
models, as well as linear regression algorithms. The ex-
perimental results show that the graphical modeling ap-
proach can help us to understand the causal relationships
between multiple forms of evidence in the domain and
explain the real world scenario better. It can also help
the filtering system to predict user preference more accu-
rately with multiple forms of evidence compared to using
a relevance model only.
As more forms of evidence are added, missing data is a
common problem because of system glitches or because
users will not behave as desired. A real system needs to
handle missing data by either ignoring it or by estimat-
ing it based on what is known. The graphical modeling
approach addresses this problem naturally. LR different
handles the problem by building many different models to
be used at different data missing conditions. LR different
and GM complete perform similarly. When the types
594
of evidence is few, LR different probably is preferable
because of the simplicity. However, as more forms of
evidence are added, a more powerful model, such as
GM complete, may be preferred because of the com-
putation and space efficiency.
We only collected data for documents users clicked.
Further investigation is needed to look at data not clicked,
which is a critical step to see whether the improvement on
prediction accuracy of user preference will help the sys-
tem serve the user better in a real system. This is the first
step towards using graphical models to combine multiple
forms of evidence while filtering. The proposed solution,
especially the data analyzing methodology used in this
paper, can also be used in other IR tasks besides filtering,
such as context-based retrieval.
8 Acknowledgments
We thank Jaime Carbonell, Tom Minka, Stephen Robert-
son, Yiming Yang, Wei Xu, Peter Spirtes, Diane Kelley,
Paul Ogilvie, Kevyn Collins-Thompson, Luo Si, Joemon
Jose for valuable discussions about the work described in
this paper.
This research was funded in part by a fellowship from
IBM and a grant from National Science Foundation. Any
opinions, findings, conclusions or recommendations ex-
pressed in this paper are the authors?, and do not neces-
sarily reflect those of the sponsors.
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference.
Mark Claypool, Phong Le, Makoto Wased, and David
Brown. 2001. Implicit interest indicators. In Intel-
ligent User Interfaces.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of the HLT/NAACL 2004 Conference.
C. Conati, A. S. Gertner, K. VanLehn, and M. J.
Druzdzel. 1997. On-line student modeling for
coached problem solving using Bayesian networks. In
Proceedings of the Sixth International Conference on
User Modeling, pages 231?242.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen,
and David J. Spiegelhalter. 1999. Probabilistic Net-
works and Expert Systems. Springer.
Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan
Dumais, and Thomas White. 2005. Evaluating im-
plicit measures to improve web search. In ACM Trans.
Information Systems, volume 23.
Donna Harman. 2003. Overview of the TREC 2002 nov-
elty track. In The Eleventh Text REtrieval Conference
(TREC-11). NIST 500-251.
E. Horvitz, J. Breese, D. Heckerman, D. Hovel, and
K. Rommelse. 1998. The Lumiere project: Bayesian
user modeling for inferring the goals and needs of soft-
ware users. In Proceedings of the Fourteenth Confer-
ence on Uncertainty in Artificial Intelligence, July.
Diane Kelly and Jaime Teevan. 2003. Implicit feedback
for inferring user preference: a bibliography. SIGIR
Forum, 37(2):18?28.
J. Kleinberg. 1998. Authoritative sources in a hyper-
linked environment. In Proc. 9th ACM-SIAM Sympo-
sium on Discrete Algorithms.
Ken Lang. 1995. Newsweeder: Learning to filter news.
In Proceedings of the Twelfth International Conference
on Machine Learning.
Phong Le and Makoto Waseda. 2000. A curious browser:
Implicit ratings. http://www.cs.wpi.edu/ clay-
pool/mqp/iii/.
Masahiro Morita and Yoichi Shinoda. 1994. Informa-
tion filtering based on user behavior analysis and best
match text retrieval. In Proceedings of the 17th ACM
SIGIR conference.
Kevyn Murphy. 2001. The Bayes net toolbox for matlab.
In Computing Science and Statistics.
Mark Pilgrim. 2002. What is RSS.
http://www.xml.com/pub/a/2002/12/18/dive-into-
xml.html.
D.V. Pynadath and W.P. Wellman. 1995. Accounting for
context in plan recognition, with application to traffic
monitoring. In Proceedings of the Eleventh Confer-
ence on Uncertainty in Artificial Intelligence.
Joseph L. Schafer and John W. Graham. 2002. Missing
data: Our view of the state of art. In Psychological
Methods, volume 7, No 2.
Linda Schamber and Judy Bateman. 1996. User crite-
ria in relevance evaluation: Toward development of a
measurement scale. In ASIS 1996 Annual Conference
Proceedings, October.
Perter Spirtes, Clark Glymour, and Richard Scheines.
2000. Causation, Prediction, and Search. The MIT
Press.
Yi Zhang, Jamie Callan, and Tom Minka. 2002. Novelty
and redundancy detection in adaptive filtering. In Pro-
ceedings of the 25th Annual International ACM SIGIR
Conference.
Yi Zhang. 2004. Using Bayesian priors to combine clas-
sifiers for adaptive filtering. In Proceedings of the 27th
Annual International ACM SIGIR Conference.
595
Abstract
We demonstrate a new research approach to the
problem of predicting the reading difficulty of a
text passage, by recasting readability in terms of
statistical language modeling.  We derive a measure
based on an extension of multinomial na?ve Bayes
classification that combines multiple language
models to estimate the most likely grade level for a
given passage.  The resulting classifier is not spe-
cific to any particular subject and can be trained
with relatively little labeled data.  We perform pre-
dictions for individual Web pages in English and
compare our performance to widely-used semantic
variables from traditional readability measures.  We
show that with minimal changes, the classifier may
be retrained for use with French Web documents.
For both English and French, the classifier main-
tains consistently good correlation with labeled
grade level (0.63 to 0.79) across all test sets.  Some
traditional semantic variables such as type-token
ratio gave the best performance on commercial cal-
ibrated test passages, while our language modeling
approach gave better accuracy for Web documents
and very short passages (less than 10 words).
1 Introduction
In the course of constructing a search engine for stu-
dents, we wanted a method for retrieving Web pages
that were not only relevant to a student's query, but also
well-matched to their reading ability.  Widely-used tra-
ditional readability formulas such as Flesch-Kincaid
usually perform poorly in this scenario.  Such formulas
make certain assumptions about the text: for example,
that the sample has at least 100 words and uses well-
defined sentences.  Neither of these assumptions need
be true for Web pages or other non-traditional docu-
ments.  We seek a more robust technique for predicting
reading difficulty that works well on a wide variety of
document types.
To do this, we turn to simple techniques from statis-
tical language modeling.  Advances in this field in the
past 20 years, along with greater access to training data,
make the application of such techniques to readability
quite timely.  While traditional formulas are based on
linear regression with two or three variables, statistical
language models can capture more detailed patterns of
individual word usage.  As we show in our evaluation,
this generally results in better accuracy for Web docu-
ments and very short passages (less than 10 words).
Another benefit of a language modeling approach is that
we obtain a probability distribution across all grade
models, not just a single grade prediction.  
Statistical models of text rely on training data, so in
Section 2 we describe our Web training corpus and note
some trends that are evident in word usage.  Section 3
summarizes related work on readability, focusing on
existing vocabulary-based measures that can be thought
of as simplified language model techniques.  Section 4
defines the modified multinomial na?ve Bayes model.
Section 5 describes our smoothing and feature selection
techniques.  Section 6 evaluates our model's generaliza-
tion performance, accuracy on short passages, and sen-
sitivity to the amount of training data.  Sections 7 and 8
discuss the evaluation results and give our observations
and conclusions.
2 Description of Web Corpus
First, we define the following standard terms when
referring to word frequencies in a corpus.  A token is de-
fined as any word occurrence in the collection.  A type
refers to a specific word-string, and is counted only once
no matter how many times the word token of that type
occurs in the collection.
For training our model, we were aware of no signifi-
cant collection of Web pages labeled by reading diffi-
culty level, so we assembled our own corpus.  There are
numerous commercial reading comprehension tests
available that have graded passages, but this would have
reduced the emphasis we wanted on Web documents.
Also, some commercial packages themselves use read-
A Language Modeling Approach to Predicting Reading Difficulty
Kevyn Collins-Thompson       Jamie Callan 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
4502 Newell Simon Hall 
Pittsburgh, PA 15213-8213 
{kct, callan}@cs.cmu.edu 
 
ability measures when authoring the graded passages,
making the data somewhat artificial and biased toward
traditional semantic variables.
We gathered 550 English documents across 12
American grade levels, containing a total of 448,715
tokens and 17,928 types.  The pages were drawn from a
wide variety of subject areas: fiction, non-fiction, his-
tory, science, etc.  We were interested in the accuracy
available at individual grade levels, so we selected
pages which had been assigned a specific grade level by
the Web site author.  For example, in some cases the
assigned grade level was that of the classroom page
where the document was acquired.
Before defining a classification model, we examined
the corpus for trends in word frequency.  One obvious
pattern was that more difficult words were introduced at
later grade levels.  Earlier researchers (e.g. Chall, 1983,
p. 63) have also observed that concrete words like ?red?
become less likely in higher grades.  Similarly, higher
grade levels use more abstract words with increased fre-
quency.  We observed both types of behavior in our Web
corpus.  Figure 1 shows four words drawn from our cor-
pus.  Data from each of the 12 grades in the corpus are
shown, ordered by ascending grade level.  The solid line
is a smoothed version of the word frequency data.  The
word ?red? does indeed show a steady decline in usage
with grade level, while the probability of the word
?determine? increases.  Other words like ?perimeter?
attain maximum probability in a specific grade range,
perhaps corresponding to the period in which these con-
cepts are emphasized in the curriculum.  The word ?the?
is very common and varies less in frequency across
grade levels.
Our main hypothesis in this work is that there are
enough distinctive changes in word usage patterns be-
tween grade levels to give accurate predictions with
simple language models, even when the subject domain
of the documents is unrestricted.
3 Related Work
There is a significant body of work on readability that
spans the last 70 years.  A comprehensive summary of
early readability work may be found in Chall (1958) and
Probability of the word "perimeter"
0
0.00005
0.0001
0.00015
0.0002
0.00025
0.0003
0.00035
0.0004
0.00045
0.0005
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|gr
ad
e)
Probability of the word "red"
0
0.0002
0.0004
0.0006
0.0008
0.001
0.0012
0.0014
0.0016
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|gr
ad
e)
Probability of the word "determine"
0
0.0002
0.0004
0.0006
0.0008
0.001
0.0012
0.0014
0.0016
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|g
ra
de
)
Probability of the word "the"
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0 1 2 3 4 5 6 7 8 9 10 11 12
Grade Class
P(
wo
rd
|gr
ad
e)
Figure 1.  Examples of four different word usage trends across grades 1-12, as sampled from our 400K-token
corpus of Web documents.  Curves showing word frequency data smoothed across grades using kernel regression
for the words (clockwise from top left): ?red?, ?determine?, ?the?, and ?perimeter?.
Klare (1963).  In 1985 a study by Mitchell (1985)
reviewed 97 different reading comprehension tests,
although few of these have gained wide use.
?Traditional? readability measures are those that rely
on two main factors: the familiarity of semantic units
(words or phrases) and the complexity of syntax.  Mea-
sures that estimate semantic difficulty using a word list
(as opposed to, say, number of syllables in a word) are
termed ?vocabulary-based measures?. 
Most similar to our work are the vocabulary-based
measures, such as the Lexile measure (Stenner et al,
1988), the Revised Dale-Chall formula (Chall and Dale,
1995) and the Fry Short Passage measure (Fry, 1990).
All of these use some type of word list to estimate
semantic difficulty: Lexile (version 1.0) uses the Car-
roll-Davies-Richman corpus of 86,741 types (Carroll et
al., 1971); Dale-Chall uses the Dale 3000 word list; and
Fry's Short Passage Measure uses Dale & O'Rourke's
?The Living Word Vocabulary? of 43,000 types (Dale
and O'Rourke, 1981).  Each of these word lists may be
thought of as a simplified language model.  The model
we present below may be thought of as a generalization
of the vocabulary-based approach, in which we build
multiple language models - in this study, one for each
grade - that capture more fine-grained information about
vocabulary  usage.
To our knowledge, the only previous work which
has considered a language modeling approach to read-
ability is a preliminary study by Si and Callan (2001).
Their work was limited to a single subject domain - sci-
ence - and three broad ranges of difficulty.  In contrast,
our model is not specific to any subject and uses 12 indi-
vidual grade models trained on a greatly expanded train-
ing set.  While our model is also initially based on na?ve
Bayes, we do not treat each class as independent.
Instead, we use a mixture of grade models, which
greatly improves accuracy.  We also do not include sen-
tence length as a syntactic component.  Si and Callan
did not perform any analysis of feature selection meth-
ods so it is unclear whether their classifier was conflat-
ing topic prediction with difficulty prediction.  In this
paper we examine feature selection as well as our
model's ability to generalize.
4 The Smoothed Unigram Model
Our statistical model is based on a variation of the mult-
inomial na?ve Bayes classifier, which we call the
?Smoothed Unigram? model.  In text classification
terms, each class is described by a language model cor-
responding to a predefined level of difficulty.  For
English Web pages, we trained 12 language models cor-
responding to the 12 American grade levels.
The language models we use are simple: they are
based on unigrams and assume that the probability of a
token is independent of the surrounding tokens, given
the grade language model.  A unigram language model
is defined by a list of types (words) and their individual
probabilities.  Although this is a weak model, it can be
trained from less data than more complex models, and
turns out to give good accuracy for our problem.
4.1 Prediction with Multinomial Na?ve Bayes
We define a generative model for a text passage T in
which we assume T was created by a hypothetical
author using the following algorithm:
1. Choose a grade language model Gi  from some
complete set of unigram models G according to a prior
distribution P(Gi).  Each Gi has a multinomial distribu-
tion over a vocabulary V.
2. Choose a passage length L in tokens according to
the distribution  P(L | Gi).
3. Assuming a ?bag of words? model for the passage,
sample L tokens from Gi ?s multinomial distribution
based on the ?na?ve? assumption that each token is inde-
pendent of all other tokens in the passage, given the lan-
guage model Gi.  
The probability of T given model Gi is therefore: 
where C(w) is the count of the type w in T.
Our goal is to find the most likely grade language
model given the text T, or equivalently, the model Gi that
maximizes .  We derive L(Gi | T)
from (1) via Bayes? Rule, which is:
However, we first make two further assumptions:
1. All grades are equally likely a priori, and there-
fore  where NG  is the number of grades.
2. The passage length probability P(L|Gi) is indepen-
dent of grade level.
Substituting (1) into (2), simplifying, and taking log-
arithms, we obtain:
where log Z represents combined factors involving pas-
sage length and the uniform prior P(Gi) which, accord-
ing to our assumptions, do not influence the prediction
outcome and may be ignored.  The sum in (3) is easily
computed: for each token in T, we simply look up its log
probability in the language model of Gi and sum over all
P T Gi( ) P L Gi( ) L !
P w Gi( )
C w( )
C w( )!-------------------------------
w T?
??=
L Gi T( ) P Gi T( )log=
P Gi T( )
P Gi( )P T Gi( )
P T( )----------------------------------=
P Gi( ) 1 NG?=
L Gi T( ) C w( ) P w Gi( )log
w T?

Zlog+=
(1)
(2)
(3)
tokens to obtain the total likelihood of the passage given
the grade.  We do this for all language models, and
select the one with maximum likelihood.  An example
of the set of log-likelihoods calculated across all 12
grade models, with a maximum point clearly evident, is
shown in Figure 2.
5 Implementation
Given the above theoretical model, we describe two fur-
ther aspects of our classification method: smoothing and
feature selection.
5.1 Smoothing
We will likely see some types in test documents that are
missing or rarely seen in our training documents.  This
is a well-known issue in language model applications,
and it is standard to compensate for this sparseness by
smoothing the frequencies in the trained models.  To do
this, we adjust our type probability estimates by shifting
part of the model?s probability mass from observed
types to unseen and rare types.
We first apply smoothing to each grade?s language
model individually.  We use a technique called Simple
Good-Turing smoothing, which is a popular method for
natural language applications.  We omit the details here,
which are available in Gale and Sampson (1995).
Next, we apply smoothing across grade language
models.  This is a departure from standard text classifi-
cation methods, which treat the classes as independent.
For reading difficulty, however, we hypothesize that
nearby grade models are in fact highly related, so that
even if a type is unobserved in one grade?s training data,
we can estimate its probability in the model by interpo-
lating estimates from nearby grade models.
For example, suppose we wish to estimate P(w|G)
for a type w in a grade model G.  If the type w occurs in
at least one grade language model, we can perform
regression with a Gaussian kernel (Hastie et al, 2001, p.
165) across all grade models to obtain a smoothed value
for P(w|G).  With training, we found the optimal kernel
width to be 2.5 grade levels.  If w does not occur in any
grade model (an ?out-of-vocabulary? type) we can back
off to a traditional semantic variable.  In this study, we
used an estimate which is a function of type length:
                         
where w is a type, i is a grade index between 1 and 12,
|w| is w?s length in characters, and C = -13, D = 10 based
on statistics from the Web corpus. 
5.2 Feature Selection
Feature selection is an important step in text classifica-
tion: it can lessen the computational burden by reducing
the number of features and increase accuracy by remov-
ing ?noise? words having low predictive power.
The first feature selection step for many text classifi-
ers is to remove the most frequent types (?stopwords?).
This must be considered carefully for our problem: at
lower grade levels, stopwords make up the majority of
token occurrences and removing them may introduce
bias.  We therefore do not remove stopwords.
Another common step is to remove low-frequency
types ? typically those that occur less than 2 to 5 times
in a model?s training data.  Because we smooth across
grade models, we perform a modified version of this
step, removing from all models any types occurring less
than 3 times in the entire corpus.
Unlike the usual text classification scenario, we also
wish to avoid some types that are highly grade-specific.
For example, a type that is very frequent in the grade 3
model but that never occurs in any other model seems
more likely to be site-specific noise than a genuine
vocabulary item.  We therefore remove any types occur-
ring in less than 3 grade models, no matter how high
their frequency.  Further study is needed to explore ways
to avoid over-fitting the classifier while reducing the
expense of removing possibly useful features.
We investigated scoring each remaining type based
on its estimated ability to predict (positively or nega-
tively) a particular grade.  We used a form of Log-Odds
Ratio, which has been shown to give superior perfor-
mance for multinomial na?ve Bayes classifiers (Mlad-
enic and Grobelnik, 1998).  Our modified Log-Odds
measure computes the largest absolute change in log-
likelihood between a given grade and all other grades.
P w Gi( )log C
w
D
------+ i w?( )??
Figure 2.  The log-likelihood of a typical 100-word Grade
5 passage relative to the language models for grades 1 to
12.  The maximum log-likelihood in this example is
achieved for the Grade 6 language model.  Note the nega-
tive scale.
L o g-L ike lih o o d  o f S a m p le G rad e 5 P as sa ge  
R elative  to  L an gu a ge  M o d els  fo r Gra d es 1  - 1 2
-810
-800
-790
-780
-770
-760
-750
1 2 3 4 5 6 7 8 9 10 11 12
G ra d e  Le ve l
Lo
g-
lik
eli
ho
od
We tried various thresholds for our Log-Odds measure
and found that the highest accuracy was achieved by
using all remaining features. 
5.3 Implementation Specifics
We found that we could reduce prediction variance with
two changes to the model.  First, rather than choosing
the single most likely grade language model, we calcu-
late the average grade level of the top N results,
weighted by the relative differences in likelihood
(essentially the expected class).  The tradeoff is a small
bias toward the middle grades.  All results reported here
use this averaging method, with N=2.
Second, to account for vocabulary variation within
longer documents, we partition the document text into
passages of 100 tokens each.  We then obtain a grade
level prediction for each passage.  This creates a distri-
bution of grade levels across the document.  Previous
work (Stenner, 1996, also citing Squires et al, 1983 and
Crawford et al, 1975) suggests that a comprehension
rate of 75% for a text is a desirable target.  We therefore
choose the grade level that lies at the 75th-percentile of
the distribution, interpolating if necessary, to obtain our
final prediction.
6 Evaluation
State-of-the-art performance for this classification task
is hard to estimate.  The results from the most closely
related previous work (Si and Callan, 2001) are not
directly comparable to ours; among other factors, their
task used a dataset trained on science curriculum
descriptions, not text written at different levels of diffi-
culty.  There also appear to be few reliable studies of
human-human interlabeler agreement.  A very limited
study by Gartin et al (1994) gave a mean interlabeler
standard deviation of 1.67 grade levels, but this study
was limited to just 3 samples across 10 judges.  Never-
theless, we believe that an objective element to readabil-
ity assessment exists, and we state our main results in
terms of correlation with difficulty level, so that at least
a broad comparison with existing measures is possible.
Our evaluation looked at four aspects of the model.
First, we measured how well the model trained on our
Web corpus generalized to other, previously unseen, test
data.  Second, we looked at the effect of passage length
on accuracy.  Third, we estimated the effect of addi-
tional training data on the accuracy of the model.
Finally, we looked at how well the model could be
extended to a language other than English ? in this
study, we give results for French.
6.1 Overall Accuracy and Generalization Ability
We used two methods for assessing how well our classi-
fier generalizes beyond the Web training data.  First, we
applied 10-fold cross-validation on the Web corpus
(Kohavi 1995).  This chooses ten random partitions for
each grade?s training data such that 90% is used for
training and 10% held back as a test set.  Second, we
used two previously unseen test sets: a set of 228 lev-
eled documents from Reading A-Z.com, spanning grade
1 through grade 6; and 17 stories from Diagnostic Read-
ing Scales (DRS) spanning grades 1.4 through 5.5.  The
Reading A-Z files were converted from PDF files using
optical character recognition; spelling errors were cor-
rected but sentence boundary errors were left intact to
simulate the kinds of problems encountered with Web
documents.  The DRS files were noise-free.
Because the Smoothed Unigram classifier only mod-
els semantic and not syntactic difficulty, we compared
its accuracy to predictions based on three widely-used
semantic difficulty variables as shown below.  All pre-
diction methods used a 100-token window size.
1. UNK:  The fraction of ?unknown? tokens in the
text, relative to the Dale 3000 word list.  This is the
semantic variable of the Revised Dale-Chall measure.
2. TYPES:  The number of types (unique words) in
a 100-token passage.
3. MLF: The mean log frequency of the passage rel-
ative to a large English corpus.  This is approximately
the semantic variable of the unnormalized Lexile (ver-
sion 1.0) score.  Because the Carroll-Davies-Richman
corpus was not available to us, we used the written sub-
set of the British National Corpus (Burnard, 1995)
which has 921,074 types. (We converted these to the
American equivalents.)
We also included a fourth predictor: the Flesch-
Kincaid score (Kincaid et al 1975), which is a linear
combination of the text?s average sentence length (in
tokens), and the average number of syllables per token.
This was included for illustration purposes only, to ver-
ify the effect of syntactic noise. The results of the evalu-
ation are summarized in Table 1.
On the DRS test collection, the TYPES and Flesch-
Kincaid predictors had the best correlation with labeled
grade level (0.93). TYPES also obtained the best corre-
lation (0.86) for the Reading A-Z documents. However,
Reading A-Z documents were written to pre-established
criteria which includes objective factors such as type/
token ratio (Reading A-Z.com, 2003), so it is not sur-
prising that the correlation is high.  The Smoothed Uni-
gram measure achieved consistently good correlation
(0.63 ? 0.67) on both DRS and Reading A-Z test sets.
Flesch-Kincaid performs much more poorly for the
Reading A-Z data, probably because of the noisy sen-
tence structure. In general, mean log frequency (MLF)
performed worse than expected ? the reasons for this
require further study but may be due to the fact the BNC
corpus may not be representative enough of vocabulary
found at earlier grades.
For Web data, we examined two subsets of the cor-
pus: grades 1? 6 and grades 1? 12. The correlation of all
variables with difficulty dropped substantially for Web
grades 1?6, except for Smoothed Unigram, which
stayed at roughly the same level (0.64) and was the best
performer.  The next best variable was UNK (0.38). For
the entire Web grades 1? 12 data set, the Smoothed Uni-
gram measure again achieved the best correlation
(0.79).  The next best predictor was again UNK (0.63).
On the Web corpus, the largest portions of Smoothed
Unigram?s accuracy gains were achieved in grades 4? 8.
Without cross-grade smoothing, correlation for Web
document predictions fell significantly, to 0.46 and 0.68
for the grade 1-6 and 1-12 subsets respectively.
We measured the type coverage of the language
models created from our Web training corpus, using the
Web (via cross-validation) and Reading A-Z test sets.
Type coverage tells us how often on average a type from
a test passage is found in our statistical model. On the
Reading A-Z test set (Grades 1 ? 6), we observed a
mean type coverage of 89.1%, with a standard deviation
of 6.65%. The mean type coverage for the Web corpus
was 91.69%, with a standard deviation of 5.86%. These
figures suggest that the 17,928 types in the training set
are sufficient to give enough coverage of the test data
that we only need to back off outside the language
model-based estimates for an average of 8-10 tokens in
any 100-token passage.
6.2 Effect of Passage Length on Accuracy
Most readability formulas become unreliable for pas-
sages of less than 100 tokens (Fry 1990).  With Web
applications, it is not uncommon for samples to contain
as few as 10 tokens or less.  For example, educational
Web sites often segment a story or lesson into a series of
image pages, with the only relevant page content being a
caption.  Short passages also arise for tasks such as esti-
mating the reading difficulty of page titles, user queries,
or questionnaire items.  Our hypothesis was that the
Smoothed Unigram model, having more fine-grained
models of word usage, would be less sensitive to pas-
sage length and give superior accuracy for very short
passages, compared to traditional semantic statistics.
In the extreme case, consider two single-word ?pas-
sages?: ?bunny? and ?bulkheads?.  Both words have two
syllables and both occur 5 times in the Carroll-Davies-
Richman corpus.  A variable such as mean log fre-
quency would assign identical difficulty to both of these
passages, while our model would clearly distinguish
them according to each word?s grade usage.
To test this hypothesis, we formed passages of
length L by sampling L consecutive tokens from near
the center of each Reading A-Z test document.  We
compared the RMS error of the Smoothed Unigram pre-
diction on these passages to that obtained from the UNK
semantic variable.  We computed different predictions
for both methods by varying the passage length L from 3
tokens to 100 tokens.
The results are shown in Figure 3.  Accuracy for the
two methods was comparable for passages longer than
about 50 tokens, but Smoothed Unigram obtained statis-
tically significant improvements at the 0.05 level for 4,
5, 6, 7, and 8-word passages.  In those cases, the predic-
tion is accurate enough that very short passages may be
reliably classified into low, medium, and high levels of
difficulty.
6.3 Effect of Training Set Size on Accuracy
We derived the learning curve of our classifier as a func-
tion of the mean model training set size in tokens.  The
lowest mean RMS error of 1.92 was achieved at the
maximum training set size threshold of 32,000 tokens
per grade model.  We fit a monotonically decreasing
power-law function to the data points (Duda et al 2001,
p. 492).  This gave extrapolated estimates for mean
RMS error of about 1.79 at 64,000 tokens per model,
1.71 at 128,000 tokens per model, and 1.50 at 1,000,000
tokens per model.
While doubling the current mean training set size to
64,000 tokens per model would give a useful reduction
in RMS error (about 6.7%), each further reduction of
Files Grade Range
Smoothed
Unigram UNK TYPES MLF FK
DRS 17 1.4 - 5.5 0.67 0.72 0.93 0.50 0.93
Reading A-Z 228 1.0 - 6.0 0.63 0.78 0.86 0.49 0.30
Web (Gr. 1-6) 250 1.0 - 6.0 0.64 0.38 0.26 0.36 0.25
Web (Gr. 1-12) 550 1.0 - 12 0.79 0.63 0.38 0.47 0.47
Table 1. Correlations between predictors and grade level, for the English collections used in our study.
All predictors were trained on the Web corpus, with the Web tests using 10-fold cross-validation.
that magnitude would require a corresponding doubling
of the training set size.  This is the trade-off that must be
considered between overall RMS accuracy and the cost
of gathering labeled data.
6.4 Application to French Web Pages
To test the flexibility of our language model approach,
we did a preliminary study for French reading difficulty
prediction.  We created a corpus of 189 French Web
pages labeled at 5 levels of difficulty, containing a total
of  394,410 tokens and 27,632 types (unstemmed).
The classification algorithm was identical to that
used for English except for a minor change in the fea-
ture selection step.  We found that, because of the
inflected nature of French and the relatively small train-
ing set, we obtained better accuracy by normalizing
types into ?type families? by using a simplified stem-
ming algorithm that removed plurals, masculine/femi-
nine endings, and basic verb endings.
A chart of the actual versus predicted difficulty label
is shown in Figure 4.  The classifier consistently under-
predicts difficulty for the highest level, while somewhat
over-predicting for the lowest level.  This may be partly
due to the bias toward central grades caused by averag-
ing the top 2 predictions.  More work on language-spe-
cific smoothing may also be needed.  With 10-fold
cross-validation, the French model obtained a mean cor-
relation of 0.64 with labeled difficulty.  For comparison,
using the type/token ratio gave a mean correlation of
0.48.  While further work and better training data are
needed, the results seem promising given that only a few
hours of effort were required to gather the French data
and adjust the classifier?s feature selection.
7 Discussion
While word difficulty is well-known to be an excellent
predictor of reading difficulty (Chall & Edgar, 1995), it
was not at all clear how effective our language model
approach would be for predicting Web page reading dif-
ficulty.  It was also unknown how much training data
would be required to get good vocabulary coverage on
Web data.  Although retraining for other applications or
domains may be desirable, two factors appear responsi-
ble for the fact that our classifier, trained on Web data,
generalizes reasonably well to unseen test data from
other sources.
First, smoothing across classes greatly reduces the
training data required for individual grade models. By
?borrowing? word frequency data from nearby grades,
the effective number of types for each grade model is
multiplied by a factor of five or more.  This helps
explain the type coverage of about 90% on our test data.
Second, because we are interested in the relative
likelihoods of grade levels, accurate relative type proba-
bilities are more important than absolute probabilities.
Indeed, trying to learn absolute type probabilities would
be undesirable since it would fit the model too closely to
whatever specific topics were in the training set.  The
important functions of relative likelihood appear to be
general indicators such as the grade when a word is first
introduced into usage, whether it generally increases or
decreases with grade level, and whether it is most fre-
quent in a particular grade range.
Further study is required to explore just how much
this model of vocabulary usage can be generalized to
other languages.  Our results with French suggest that
once we have normalized incoming types to accommo-
Passage Length vs. Prediction Accuracy
(Grade 4: ReadingA-Z)
0
1
2
3
4
5
6
7
1 10 100
Passage Length (Words)
Me
an
 R
MS
 E
rro
r
%-UNK
Smoothed
Unigram
Figure 3.  The effect of passage size on RMS predic-
tion error for Grade 4 documents, comparing
Smoothed Unigram to the UNK semantic variable.
Error bars show 95% confidence interval.  The grey
vertical lines mark logarithmic length.
Actual v s. P re dicte d D ifficulty Le v e l
Fre nch Le v e ls 1 - 5
0
0 .5
1
1 .5
2
2 .5
3
3 .5
4
4 .5
5
0 1 2 3 4 5
L ab e le d  L e ve l
Pr
ed
ict
ed
 L
ev
el
Figure 4.  Actual vs. predicted difficulty label for docu-
ments from the French Web corpus.  The data have
been ?jittered? to show clusters more clearly.  The diag-
onal line represents perfect prediction.
date the morphology of a language, the same core classi-
fier approach may still be applicable, at least for some
family of languages.
8 Conclusions
We have shown that reading difficulty can be estimated
with a simple language modeling approach using a mod-
ified na?ve Bayes classifier.  The classifier's effective-
ness is improved by explicitly modeling class
relationships and smoothing frequency data across
classes as well as within each class.
Our evaluation suggests that reasonably effective
models can be trained with small amounts of easily-
acquired data.  While this data is less-rigorously graded,
such material also greatly reduces the cost of creating a
readability measure, making it easy to modify for spe-
cific tasks or populations. 
As an example of retraining, we showed that the
classifier obtained good correlation with difficulty for at
least two languages, English and French, with the only
algorithm difference being a change in the morphology
handling during feature processing. 
We also showed that the Smoothed Unigram method
is robust for short passages and Web documents.  Some
traditional variables like type/token ratio gave excellent
correlation with difficulty on commercial leveled pas-
sages, but the same statistics performed inconsistently
on Web-based test sets.  In contrast, the Smoothed Uni-
gram method had good accuracy across all test sets.
The problem of reading difficulty prediction lies in
an interesting region between classification and regres-
sion, with close connections to ordinal regression (Mac-
Cullagh, 1980) and discriminative ranking models
(Crammer and Singer, 2001).  While simple methods
like modified na?ve Bayes give reasonably good results,
more sophisticated techniques may give more accurate
predictions, especially at lower grades, where vocabu-
lary progress is measured in months, not years.
Acknowledgements
This work was supported by NSF grant IIS-0096139
and Dept. of Education grant R305G03123.  Any opin-
ions, findings, conclusions, or recommendations
expressed in this material are the authors?, and do not
necessarily reflect those of the sponsors.  We thank the
anonymous reviewers for their comments and Luo Si for
helpful discussions.
References
Burnard, L. (ed.) 1995.  The Users Reference Guide for the
British National Corpus. Oxford: Oxford University
Computing Services.
Carroll, J. B., Davies, P., Richman, B.  1971.  Word Frequency
Book.  Boston: Houghton Mifflin.
Chall, J.S.  1958.  Readability: An appraisal of research and
application.  Bureau of Educational Research Mono-
graphs, No. 34.  Columbus, OH: Ohio State Univ. Press. 
Chall, J.S.  1983.  Stages of Reading Development. McGraw-
Hill.  
Chall, J.S. and Dale, E.  1995.  Readability Revisited: The New
Dale-Chall Readability Formula.  Cambridge, MA:
Brookline Books.
Crammer, K. and Singer, Y.  2001.  Pranking with ranking.
Proceedings of NIPS 2001. 641-647.
Dale, E. and O'Rourke, J.  1981.  The Living Word Vocabulary.
Chicago, IL: World Book/Childcraft International.
Duda, R. O., Hart, P. E., and Stork, D. G.  2001.  Pattern Clas-
sification (Second Edition), Wiley, New York.
Fry, E.  1990.  A readability formula for short passages. J. of
Reading, May 1990, 594-597.
Gale, W., Sampson, G.  1995.  Good-Turing frequency estima-
tion without tears, J. of Quant. Linguistics, v. 2, 217-237.
Gartin, S., et al 1994. W. Virginia Agriculture Teachers? Esti-
mates of Magazine Article Readability. J. Agr. Ed. 35(1).
Hastie, T., Tibshirani, R., Friedman, J.  2001.  The Elements of
Statistical Learning.  Springer-Verlag, New York.
Kincaid, J., Fishburne, R., Rodgers, R. and Chissom, B.  1975.
Derivation of new readability formulas for navy enlisted
personnel.  Branch Report 8-75.  Millington, TN: Chief
of Naval Training.
Klare, G. R.  1963.  The Measurement of Readability.  Ames,
IA.  Iowa State University Press.
Kohavi, R.  1995.  A study of cross-validation and bootstrap
for accuracy estimation and model selection.  Proc. of the
14th Int. Joint Conf. on Artificial Intelligence (IJCAI
1995).  Montreal, Canada.  1137 - 1145.
MacCullagh, P.  1980.  Regression models for ordinal data. J.
of the Royal Statistical Society B, vol.42, 109-142.
Mitchell, J.V.  1985.  The Ninth Mental Measurements Year-
book. Lincoln, Nebraska: Univ. of Nebraska Press.
Mladenic D., and Grobelnik, M.  1998.  Feature selection for
classification based on text hierarchy. Working Notes of
Learning from Text and the Web, CONALD-98. Carnegie
Mellon Univ., Pittsburgh, PA.
Reading A-Z.com  2003.  Reading A-Z Leveling and Correla-
tion Chart (HTML page). http://www.readinga-z.com/
newfiles/correlate.html
Si, L. and Callan, J.  2001.  A statistical model for scientific
readability.   Proc. of CIKM 2001.  Atlanta, GA, 574-576.
Stenner, A. J., Horabin, I., Smith, D.R., and Smith, M. 1988.
The Lexile Framework.  Durham, NC: Metametrics.
Proceedings of NAACL HLT 2007, pages 460?467,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Lexical and Grammatical Features to Improve Readability 
Measures for First and Second Language Texts 
Michael J. Heilman 
 
Kevyn Collins-
Thompson 
Jamie Callan 
 
Maxine Eskenazi 
 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
4502 Newell Simon Hall 
Pittsburgh, PA 15213-8213 
 {mheilman,kct,callan,max}@cs.cmu.edu 
 
 
Abstract 
This work evaluates a system that uses in-
terpolated predictions of reading difficulty 
that are based on both vocabulary and 
grammatical features.  The combined ap-
proach is compared to individual gram-
mar- and language modeling-based 
approaches.  While the vocabulary-based 
language modeling approach outper-
formed the grammar-based approach, 
grammar-based predictions can be com-
bined using confidence scores with the 
vocabulary-based predictions to produce 
more accurate predictions of reading dif-
ficulty for both first and second language 
texts.  The results also indicate that gram-
matical features may play a more impor-
tant role in second language readability 
than in first language readability. 
1 Introduction 
The REAP tutoring system (Heilman, et al 2006), 
aims to provide authentic reading materials of the 
appropriate difficulty level, in terms of both vo-
cabulary and grammar, for English as a Second 
Language students.  An automatic measure of read-
ability that incorporated both lexical and gram-
matical features was thus needed. 
For first language (L1) learners (i.e., children 
learning their native tongue), reading level has 
been predicted using a variety of techniques, based 
on models of a student?s lexicon, grammatical sur-
face features such as sentence length (Flesch, 
1948), or combinations of such features (Schwarm 
and Ostendorf, 2005).  It was shown by Collins-
Thompson and Callan (2004) that a vocabulary-
based language modeling approach was effective at 
predicting the readability of grades 1 to 12 of Web 
documents of varying length, even with high levels 
of noise.   
Prior work on first language readability by 
Schwarm and Ostendorf (2005) incorporated 
grammatical surface features such as parse tree 
depth and average number of verb phrases.  This 
work combining grammatical and lexical features 
was promising, but it was not clear to what extent 
the grammatical features improved predictions.   
Also, discussions with L2 instructors suggest 
that a more detailed grammatical analysis of texts 
that examines features such as passive voice and 
various verb tenses can provide better features with 
which to predict reading difficulty.  One goal of 
this work is to show that the use of pedagogically 
motivated grammatical features (e.g., passive 
voice, rather than the number of words per sen-
tence) can improve readability measures based on 
lexical features alone. 
One of the differences between L1 and L2 read-
ability is the timeline and processes by which first 
and second languages are acquired.  First language 
acquisition begins at infancy, and the primary 
grammatical structures of the target language are 
acquired by age four in typically developing chil-
460
dren (Bates, 2003).  That is, most grammar is ac-
quired prior to the beginning of a child?s formal 
education.  Therefore, most grammatical features 
seen at high reading levels such as high school are 
present with similar frequencies at low reading 
levels such as grades 1-3 that correspond to ele-
mentary school-age children.  It should be noted 
that sentence length is one grammar-related differ-
ence that can be observed as L1 reading level in-
creases.  Sentences are kept short in texts for low 
L1 reading levels in order to reduce the cognitive 
load on child readers.  The average sentence length 
of texts increases with the age and reading level of 
the intended audience.  This phenomenon has been 
utilized in early readability measures (Flesch, 
1948).  Vocabulary change, however, continues 
even into adulthood, and has been shown to be a 
more effective predictor of L1 readability than 
simpler measures such as sentence length (Collins-
Thompson and Callan, 2005). 
Second language learners, unlike their L1 coun-
terparts, are still very much in the process of ac-
quiring the grammar of their target language.  In 
fact, even intermediate and advanced students of 
second languages, who correspond to higher L2 
reading levels, often struggle with the grammatical 
structures of their target language.  This phenome-
non suggests that grammatical features may play a 
more important role in predicting and measuring 
L2 readability.  That is not to say, however, that 
vocabulary cannot be used to predict L2 reading 
levels.  Second language learners are learning both 
vocabulary and grammar concurrently, and reading 
materials for this population are chosen or au-
thored according to both lexical and grammatical 
complexity.  Therefore, the authors predict that a 
readability measure for texts intended for second 
language learners that incorporates both grammati-
cal and lexical features could clearly outperform a 
measure based on only one of these two types of 
features. 
This paper begins with descriptions of the lan-
guage modeling and grammar-based prediction 
systems.  A description of the experiments follows 
that covers both the evaluation metrics and corpora 
used.  Experimental results are presented, followed 
by a discussion of these results, and a summary of 
the conclusions of this work.  
2 Language Model Readability Prediction 
for First Language Texts 
Statistical language modeling exploits patterns of 
use in language.  To build a statistical model of 
text, training examples are used to collect statistics 
such as word frequency and order.  Each training 
example has a label that tells the model the ?true? 
category of the example.  In this approach, one 
statistical model is built for each grade level to be 
predicted. 
The statistical language modeling approach has 
several advantages over traditional readability 
formulas, which are usually based on linear regres-
sion with two or three variables.  First, a language 
modeling approach generally gives much better 
accuracy for Web documents and short passages 
(Collins-Thompson and Callan, 2004).  Second, 
language modeling provides a probability distribu-
tion across all grade models, not just a single pre-
diction.  Third, language modeling provides more 
data on the relative difficulty of each word in the 
document.  This might allow an application, for 
example, to provide more accurate vocabulary as-
sistance. 
The statistical model used for this study is 
based on a variation of the multinomial Na?ve 
Bayes classifier.  For a given text passage T, the 
semantic difficulty of T relative to a specific grade 
level Gi is predicted by calculating the likelihood 
that the words of T were generated from a repre-
sentative language model of Gi.  This likelihood is 
calculated for each of a number of language mod-
els, corresponding to reading difficulty levels.  The 
reading difficulty of the passage is then estimated 
as the grade level of the language model most 
likely to have generated the passage T. 
The language models employed in this work are 
simple: they are based on unigrams and assume 
that the probability of a token is independent of the 
surrounding tokens.  A unigram language model is 
simply defined by a list of types (words) and their 
individual probabilities.  Although this is a weak 
model, it can be effectively trained from less la-
beled data than more complex models, such as bi-
gram or trigram models.  Additionally, higher 
order n-gram models might capture grammatical as 
well as lexical differences.  The relative contribu-
tions of grammatical and lexical features were thus 
better distinguished by using unigram language 
461
models that more exclusively focus on lexical dif-
ferences. 
In this language modeling approach, a genera-
tive model is assumed for a passage T, in which a 
hypothetical author generates the tokens of T by: 
1. Choosing a grade language model, Gi, 
from the set G = {Gi} of 12 unigram language 
models, according to a prior probability distri-
bution P(Gi). 
2. Choosing a passage length |T| in tokens ac-
cording to a probability distribution P(|T|). 
3. Sampling |T| tokens from Gi?s multinomial 
word distribution according to the ?na?ve? as-
sumption that each token is independent of all 
other tokens in the passage, given the language 
model Gi. 
These assumptions lead to the following expres-
sion for the probability of T being generated by 
language model Gi according to a multinomial dis-
tribution: 
 
?
?
=
Vw
wC
i
i
wC
GwPTTPGTP )!(
)|(|!||)(|)|(
)(
 
 
Next, according to Bayes? Theorem: 
  
)(
)|()()|(
TP
GTPGPTGP iii = . 
 
Substituting (1) into (2), taking logarithms, and 
simplifying produces: 
 
SRwC
GwPwCTGP
Vw
i
Vw
i
loglog)!(log
)|(log)()|(log
++?
=


?
?
, 
 
where V is the list of all types in the passage T, w is 
a type in V, and C(w) is the number of tokens with 
type w in T.  For simplicity, the factor R represents 
the contribution of the prior P(Gi), and S represents 
the contribution of the passage length |T|, given the 
grade level.   
Two further assumptions are made to simplify 
the illustration: 
1. That all grades are equally likely a priori.   
That is, 
G
i N
GP 1)( =  where NG is the number 
of grade levels.  For example, if there are 12 
grade levels, then NG = 12.  This allows log R to 
be ignored. 
2. That all passage lengths (up to a maximum 
length M) are equally likely.  This allows log S 
to be ignored. 
These may be poor assumptions in a real appli-
cation, but they can be easily included or excluded 
in the model as desired.  The log C(w)! term can 
also be ignored because it is constant across levels.  
Under these conditions, an extremely simple form 
for the grade likelihood remains.  In order to find 
which model Gi maximizes Equation (3), the 
model which Gi that maximizes the following 
equation must be found: 
 
)|(log)()|( i
Vw
i GwPwCGTL 
?
=  
 
This is straightforward to compute: for each token 
in the passage T, the log probability of the token 
according to the language model of Gi is calcu-
lated.  Summing the log probabilities of all tokens 
produces the overall likelihood of the passage, 
given the grade.  The grade level with the maxi-
mum likelihood is then chosen as the final read-
ability level prediction. 
This study employs a slightly more sophisti-
cated extension of this model, in which a sliding 
window is moved across the text, with a grade pre-
diction being made for each window.  This results 
in a distribution of grade predictions.  The grade 
level corresponding to a given percentile of this 
distribution is chosen as the prediction for the en-
tire document.  The values used in these experi-
ments for the percentile thresholds for L1 and L2 
were chosen by accuracy on held-out data. 
3 Grammatical Construction Readability 
Prediction for Second Language Texts 
The following sections describe the approach to 
predicting readability based on grammatical fea-
tures.  As with any classifier, two components are 
required to classify texts by their reading level: 
first, a definition for and method of identifying 
features; second, an algorithm for using these fea-
tures to classify a given text.  A third component, 
training data, is also necessary in this classification 
462
task.  The corpus of materials used for training and 
testing is discussed in a subsequent section. 
3.1 Features for Grammar-based Prediction 
L2 learners usually learn grammatical patterns ex-
plicitly from grammar explanations in L2 text-
books, unlike their L1 counterparts who learn them 
implicitly through natural interactions.  Grammati-
cal features would therefore seem to be an essential 
component of an automatic readability measure for 
L2 learners, who must actively acquire both the 
lexicon and grammar of their target language. 
The grammar-based readability measure relies 
on being able to automatically identify grammati-
cal constructions in text.  Doing so is a multi-step 
process that begins by syntactically parsing the 
document.  The Stanford Parser (Klein and Man-
ning, 2002) was used to produce constituent struc-
ture trees.  The choice of parser is not essential to 
the approach, although the accuracy of parsing 
does play a role in successful identification of cer-
tain grammatical patterns. PCFG scores from the 
parser were also used to filter out some of the ill-
formed text present in the test corpora.  The default 
training set of Penn Treebank (Marcus et al 1993) 
was used for the parser because the domain and 
style of those texts actually matches fairly well 
with the domain and style of the texts on which a 
reading level predictor for second language learn-
ers might be used. 
Once a document is parsed, the predictor uses 
Tgrep2 (Rohde, 2005), a tree structure searching 
tool, to identify instances of the target patterns.  A 
Tgrep2 pattern defines dominance, sisterhood, 
precedence, and other relationships between nodes 
in the parse tree for a sentence.  A pattern can also 
place constraints on the terminal symbols (e.g., 
words and punctuation), such that a pattern might 
require a form of the copula ?be? to exist in a cer-
tain position in the construction.  An example of a 
TGrep2 search pattern for the progressive verb 
tense is the following: 
 
?VP < /^VB/ < (VP < VBG)? 
 
Searching for this pattern returns sentences in 
which a verb phrase (VP) dominates an auxiliary 
verb (whose symbol begins with VB) as well as 
another verb phrase, which in turn dominates a 
verb in gerund form (VBG).  An example of a 
matching sentence is, ?The student was reading a 
book,? shown in Figure 2. 
 
Figure 2: The parse tree for an example sentence 
that matches a pattern for progressive verb tense. 
 
A set of 22 relevant grammatical constructions 
were identified from grammar textbooks for three 
different ESL levels (Fuchs et al, 2005).  These 
grammar textbooks had different authors and pub-
lishers than the ones used in the evaluation corpora 
in order to minimize the chance of experimental 
results not generalizing beyond the specific materi-
als employed in this study.  The ESL levels corre-
spond to the low-intermediate (hereafter, level 3), 
high-intermediate (level 4), and advanced (level 5) 
courses at the University of Pittsburgh?s English 
Language Institute.  The constructions identified in 
these grammar textbooks were then implemented 
in the form of Tgrep2 patterns.   
 
Feature  Lowest Level Highest Level 
Passive Voice 0.11 0.71 
Past Participle 0.28 1.63 
Perfect Tense 0.01 0.33 
Relative Clause 0.54 0.60 
Continuous 
Tense 
0.19 0.27 
Modal 0.80 1.44 
Table 1: The rates of occurrence per 100 words of 
a few of the features used by the grammar-based 
predictor.  Rates are shown for the lowest (2) and 
highest (5) levels in the L2 corpus. 
 
The rate of occurrence of constructions was 
calculated on a per word basis.  A per-word rather 
a book 
The student 
S 
VP 
VBD VP 
VBG 
NP 
was 
reading 
NP 
463
than a per-sentence measure was chosen because a 
per-sentence measure would depend too greatly on 
sentence length, which also varies by level.  It was 
also desirable to avoid having sentence length con-
founded with other features.  Table 1 shows that 
the rates of occurrence of certain constructions be-
come more frequent as level increases.  This sys-
tematic variation across levels is the basis for the 
grammar-based readability predictions. 
A second feature set was defined that consisted 
of 12 grammatical features that could easily be 
identified without computationally intensive syn-
tactic parsing.  These features included sentence 
length, the various verb forms in English, includ-
ing the present, progressive, past, perfect, continu-
ous tenses, as well as part of speech labels for 
words.  The goal of using a second feature set was 
to examine how dependent prediction quality was 
on a specific set of features, as well as to test the 
extent to which the output of syntactic parsing 
might improve prediction accuracy. 
3.2 Algorithm for Grammatical Feature-
based Classification 
A k-Nearest Neighbor (kNN) algorithm is used for 
classification based on the grammatical features 
described above.  The kNN algorithm is an in-
stance-based learning technique originally devel-
oped by Cover and Hart (1967) by which a test 
instance is classified according to the classifica-
tions of a given number (k) of training instances 
closest to it.  Distance is defined in this work as the 
Euclidean distance of feature vectors.  Mitchell 
(1997) provides more details on the kNN algo-
rithm.  This algorithm was chosen because it has 
been shown to be effective in text classification 
tasks when compared to other popular methods 
(Yang 1999).  A k value of 12 was chosen because 
it provided the best performance on held-out data. 
Additionally, it is straightforward to calculate 
a confidence measure with which kNN predictions 
can be combined with predictions from other clas-
sifiers?in this case with predictions from the uni-
gram language modeling-based approach described 
above.  A confidence measure was important in 
this task because it provided a means with which to 
combine the grammar-based predictions with the 
predictions from the language modeling-based 
predictor while maintaining separate models for 
each type of feature.  These separate models were 
maintained to better determine the relative contri-
butions of grammatical and lexical features. 
A static linear interpolation of predictions us-
ing the two approaches led to only minimal reduc-
tions of prediction error, likely because predictions 
from the poorer performing grammar-based classi-
fier were always given the same weight.  However, 
with the confidence measures, predictions from the 
grammar-based classifier could be given more 
weight when the confidence measure was high, and 
less weight when the measure was low and the 
predictions were likely to be inaccurate.  The case-
dependent interpolation of prediction values al-
lowed for the effective combination of language 
modeling- and grammar-based predictions.  
The confidence measure employed is the pro-
portion of the k most similar training examples, or 
nearest neighbors, that agree with the final label 
chosen for a given test document.  For example, if 
seven of ten neighbors have the same label, then 
the confidence score will be 0.6.  The interpolated 
readability prediction value is calculated as fol-
lows: 
 
LI = LLM + CkNN * LGR, 
 
where LLM is the language model-based prediction, 
LGR is the grammar-based prediction from the kNN 
algorithm, and CkNN is the confidence value for the 
kNN prediction.  The language modeling approach 
is treated as a black box, but it would likely be 
beneficial to have confidence measures for it as 
well. 
4 Descriptions of Experiments 
This section describes the experiments used to test 
the hypothesis that grammar-based features can 
improve readability measures for English, espe-
cially for second language texts.  The measures 
and cross-validation setup are described.  A de-
scription of the evaluation corpora of labeled first 
and second language texts follows. 
4.1 Experimental Setup 
Two measurements were used in evaluating the 
effectiveness of the reading level predictions.  
First, the correlation coefficient evaluated whether 
the trends of prediction values matched the trends 
for human-labeled texts.  Second, the mean 
squared error of prediction values provided a 
464
measure of how correct each of the predictors was 
on average,  penalizing more severe errors more 
heavily.  Mean square error was used rather than 
simple accuracy (i.e., number correct divided by 
sample size) because the task of readability predic-
tion is more akin to regression than classification.  
Evaluation measures such as accuracy, precision, 
and recall are thus less meaningful for readability 
prediction tasks because they do not capture the 
fact that an error of 4 levels is more costly than an 
error of a single level. 
A nine-fold cross-validation was employed.  
The data was first split into ten sets.  One set was 
used as held-out data for selecting the parameter k 
for the kNN algorithm and the percentile value for 
the language modeling predictor, and then the re-
maining nine were used to evaluate the quality of 
predictions.  Each of these nine was in turn se-
lected as the test set, and the other eight were used 
as training data. 
4.2 Corpora of Labeled Texts 
Two corpora of labeled texts were used in the 
evaluation.  The first corpus was from a set of texts 
gathered from the Web for a prior evaluation of the 
language modeling approach.  The 362 texts had 
been assigned L1 levels (1-12) by grade school 
teachers, and consisted of approximately 250,000 
words.  For more details on the L1 corpus, see 
(Collins-Thompson and Callan, 2005). 
The second corpora consisted of textbook mate-
rials (Adelson-Goldstein and Howard, 2004, for 
level 2; Ediger and Pavlik, 2000, for levels 3 and 4; 
Silberstein, 2002, for level 5) from a series of Eng-
lish as a Second Language reading courses at the 
English Language Institute at the University of 
Pittsburgh.  The four reading practice textbooks 
that constitute this corpus were from separate au-
thors and publishers than the grammar textbooks 
used to select and define grammatical features.  
The reading textbooks in the corpus are used in 
courses intended for beginning (level 2) through 
advanced (level 5) students.  The textbooks were 
scanned into electronic format, and divided into 
fifty roughly equally sized files.  This second lan-
guage corpus consisted of approximately 200,000 
words. 
Although the sources and formats of the two 
corpora were different, they share a number of 
characteristics.  Their size was roughly equal. The 
documents in both were also fairly but not per-
fectly evenly distributed across the levels.  Both 
corpora also contained a significant amount of 
noise which made accurate prediction of reading 
level more challenging.  The L1 corpus was from 
the Web, and therefore contained navigation 
menus, links, and the like.  The texts in the L2 cor-
pus also contained significant levels of noise due to 
the inclusion of directions preceding readings, ex-
ercises and questions following readings, as well as 
labels on figures and charts.  The scanned files 
were not hand-corrected in this study, in part to test 
that the measures are robust to noise, which is pre-
sent in the Web documents for which the readabil-
ity measures are employed in the REAP tutoring 
system.  
The grammar-based prediction seems to be 
more significantly negatively affected by the noise 
in the two corpora because the features rely more 
on dependencies between different words in the 
text.  For example, if a word happened to be part of 
an image caption rather than a well-formed sen-
tence, the unigram language modeling approach 
would only be affected for that word, but the 
grammar-based approach might be affected for 
features spanning an entire clause or sentence. 
5 Results of Experiments 
The results show that for both the first and sec-
ond language corpora, the language modeling 
(LM) approach alone produced more accurate pre-
dictions than the grammar-based approach alone.  
The mean squared error values (Table 2) were 
lower, and the correlation coefficients (Table 3) 
were higher for the LM predictor than the gram-
mar-based predictor.   
The results also indicate that while grammar-
based predictions are not as accurate as the vo-
cabulary-based scores, they can be combined with 
vocabulary-based scores to produce more accurate 
interpolated scores.  The interpolated predictions 
combined by using the kNN confidence measure 
were slightly and in most tests significantly more 
accurate in terms of mean squared error than the 
predictions from either single measure.   Interpola-
tion using the first set of grammatical features led 
to 7% and 22% reductions in mean squared error 
on the L1 and L2 corpora, respectively.  These re-
sults were verified using a one-tailed paired t-test 
465
of the squared error values of the predictions, and 
significance levels are indicated in Table 2. 
 
Mean Squared Error Values 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 5.02 0.51 
Grammar 10.27 1.08 
Interpolation 4.65* 0.40** 
Grammar2 (feature set #2) 12.77 1.26 
Interp2. (feature set #2) 4.73 0.43* 
Table 2.  Comparison of Mean Squared Error of 
predictions compared to human labels for different 
methods.  Interpolated values are significantly bet-
ter compared to language modeling predictions 
where indicated (* = p<0.05, ** = p<0.01). 
 
Correlation Coefficients 
Test Set (Num. Levels) L1(12) L2(4) 
Language Modeling 0.71 0.80 
Grammar 0.46 0.55 
Interpolation 0.72 0.83 
Grammar2 (feature set #2) 0.34 0.48 
Interp2. (feature set #2) 0.72 0.81 
Table 3.  Comparison of Correlation Coefficients 
of prediction values to human labels for different 
prediction methods. 
 
The trends were similar for both sets of gram-
matical features.  However, the first set of features 
that included complex syntactic constructs led to 
better performance than the second set, which in-
cluded only verb tenses, part of speech labels, and 
sentence length.  Therefore, when syntactic parsing 
is not feasible because of corpora size, it seems 
that grammatical features requiring only part-of-
speech tagging and word counts may still improve 
readability predictions.  This is practically impor-
tant because parsing can be too computationally 
intensive for large corpora. 
All prediction methods performed better, in 
terms of correlations, on the L2 corpus than on the 
L1 corpus.  The L2 corpus is somewhat smaller in 
size and should, if only on the basis of training ma-
terial available to the prediction algorithms, actu-
ally be more difficult to predict than the L1 corpus.  
To ensure that the range of levels was not causing 
the four-level L2 corpus to have higher predictions 
than the twelve-level L1 corpus, the L1 corpus was 
also divided into four bins (grades 1-3, 4-6, 7-9, 
10-12).  The accuracy of predictions for the binned 
version of the L1 corpus was not substantially dif-
ferent than for the 12-level version. 
6 Discussion 
In the experimental tests, the LM approach was 
more effective for measuring both L1 and L2 read-
ability.  There are several potential causes of this 
effect.  First, the language modeling approach can 
utilize all the words as they appear in the text as 
features, while the grammatical features were cho-
sen and defined manually.  As a result, the LM 
approach can make measurements on a text for as 
many features as there are words in its lexicon.  
Additionally, the noise present in the corpora likely 
affected the grammar-based approach dispropor-
tionately more because that method relies on accu-
rate parsing of relationships between words. 
Additionally, English is a morphologically im-
poverished language compared to most languages.  
Text classification, information retrieval, and many 
other human language technology tasks can be ac-
complished for English without accounting for 
grammatical features such as morphological inflec-
tions.  For example, an information retrieval sys-
tem can perform reasonably well in English 
without performing stemming, which does not 
greatly increase performance except when queries 
and documents are short (Krovetz, 1993). 
However, most languages have a rich morphol-
ogy by which a single root form may have thou-
sands or perhaps millions of inflected or derived 
forms.  Language technologies must account for 
morphological features in such languages or the 
vocabulary grows so large that it becomes unman-
ageable.  Lee (2004), for example, showed that 
morphological analysis can improve the quality of 
statistical machine translation for Arabic.  Thus it 
seems that grammatical features could contribute 
even more to measures of readability for texts in 
other languages. 
That said, the use of grammatical features ap-
pears to play a more important role in readability 
measures for L2 than for L1.  When interpolated 
with grammar-based scores, the reduction of mean 
squared error over the language modeling approach 
for L1 was only 7%, while for L2 the reduction or 
squared error was 22%.  An evaluation on corpora 
with less noise would likely bring out these differ-
466
ences further and show grammar to be an even 
more important factor in second language readabil-
ity.  This result is consistent with the fact that sec-
ond language learners are still in the process of 
acquiring the basic grammatical constructs of their 
target language. 
7 Conclusion 
The results of this work suggest that grammatical 
features can play a role in predicting reading diffi-
culty levels for both first and second language texts 
in English.  Although a vocabulary-based language 
modeling approach outperformed the grammar-
based predictor, an interpolated measure using 
confidence scores for the grammar-based predic-
tions showed improvement over both individual 
measures.  Also, grammar appears to play a more 
important role in second language readability than 
in first language readability.  Ongoing work aims 
to improve grammar-based readability by reducing 
noise in training data, automatically creating larger 
grammar feature sets, and applying more sophisti-
cated modeling techniques. 
8 Acknowledgements 
We would like to acknowledge Lori Levin for use-
ful advice regarding grammatical constructions, as 
well as the anonymous reviewers for their sugges-
tions.  
This material is based on work supported by 
NSF grant IIS-0096139 and Dept. of Education 
grant R305G03123. Any opinions, findings, con-
clusions or recommendations expressed in this ma-
terial are the authors', and do not necessarily reflect 
those of the sponsors. 
References 
J. Adelson-Goldstein and L. Howard. 2004.  Read and 
Reflect 1.  Oxford University Press, USA. 
E. Bates. 2003. On the nature and nurture of language. 
In R. Levi-Montalcini, D. Baltimore, R. Dulbecco, F. 
Jacob, E. Bizzi, P. Calissano, & V. Volterra (Eds.), 
Frontiers of biology: The brain of Homo sapiens (pp. 
241?265). Rome: Istituto della Enciclopedia Italiana 
fondata da Giovanni Trecanni. 
M. Fuchs, M. Bonner, M. Westheimer. 2005.  Focus on 
Grammar, 3rd Edition.  Pearson ESL. 
K. Collins-Thompson and J. Callan. 2004.  A language 
modeling approach to predicting reading difficulty.  
Proceedings of the HLT/NAACL Annual Conference. 
T. Cover and P. Hart. 1967.  Nearest neighbor pattern 
classification.  IEEE Transactions on Information 
Theory, 13, 21-27. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
Intermediate.  Oxford University Press, USA. 
A. Ediger and C. Pavlik.  2000.  Reading Connections 
High Intermediate.  Oxford University Press, USA. 
M. Heilman, K. Collins-Thompson, J. Callan & M. Es-
kenazi. 2006. Classroom success of an Intelligent Tu-
toring System for lexical practice and reading 
comprehension. Proceedings of the Ninth Interna-
tional Conference on Spoken Language Processing. 
D. Klein and C. D. Manning. 2002. Fast Exact Inference 
with a Factored Model for Natural Language Parsing. 
Advances in Neural Information Processing Systems 
15 (NIPS 2002), December 2002. 
R. Krovetz. 1993. Viewing morphology as an inference 
process. SIGIR-93, 191?202. 
Y. Lee. 2004.  Morphological Analysis for Statistical 
Machine Translation.  Proceedings of the 
HLT/NAACL Annual Conference. 
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. 
"Building a large annotated corpus of English: the 
Penn Treebank." Computational Linguistics, 19(2). 
T. Mitchell. 1997. Machine Learning.  The McGraw-
Hill Companies, Inc.  pp. 231-236. 
D. Rohde. 2005. Tgrep2 User Manual. 
http://tedlab.mit.edu/~dr/Tgrep2/tgrep2.pdf. 
S. Schwarm, and M. Ostendorf. 2005.  Reading Level 
Assessment Using Support Vector Machines and Sta-
tistical Language Models.  Proceedings of the Annual 
Meeting of the Association for Computational Lin-
guistics. 
S. Silberstein, B. K. Dobson, and M. A. Clarke. 2002. 
Reader's Choice, 4th edition.  University of Michigan 
Press/ESL. 
Y. Yang. 1999.  A re-examination of text categorization 
methods.  Proceedings of ACM SIGIR Conference on 
Research and Development in Information Retrieval 
(SIGIR'99, pp 42--49). 
467
Proceedings of NAACL HLT 2007, pages 476?483,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic and human scoring of word definition responses
Kevyn Collins-Thompson and Jamie Callan
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, U.S.A. 15213-8213
{kct|callan}@cs.cmu.edu
Abstract
Assessing learning progress is a critical
step in language learning applications and
experiments. In word learning, for exam-
ple, one important type of assessment is
a definition production test, in which sub-
jects are asked to produce a short defini-
tion of the word being learned. In current
practice, each free response is manually
scored according to how well its mean-
ing matches the target definition. Manual
scoring is not only time-consuming, but
also limited in its flexibility and ability to
detect partial learning effects.
This study describes an effective auto-
matic method for scoring free responses
to definition production tests. The algo-
rithm compares the text of the free re-
sponse to the text of a reference definition
using a statistical model of text semantic
similarity that uses Markov chains on a
graph of individual word relations. The
model can take advantage of both corpus-
and knowledge-based resources. Evalu-
ated on a new corpus of human-judged
free responses, our method achieved sig-
nificant improvements over random and
cosine baselines in both rank correlation
and label error.
1 Introduction
Human language technologies are playing an in-
creasingly important role in the science and prac-
tice of language learning. For example, intelligent
Computer Assisted Language Learning (CALL) sys-
tems are being developed that can automatically tai-
lor lessons and questions to the needs of individual
students (Heilman et al, 2006). One critical task
that language tutors, word learning experiments, and
related applications have in common is assessing the
learning progress of the student or experiment sub-
ject during the course of the session.
When the task is learning new vocabulary, a vari-
ety of tests have been developed to measure word
learning progress. Some tests, such as multiple-
choice selection of a correct synonym or cloze com-
pletion, are relatively passive. In production tests,
on the other hand, students are asked to write or say
a short phrase or sentence that uses the word being
learned, called the target word, in a specified way.
In one important type of production test, called a
definition production test, the subject is asked to de-
scribe the meaning of the target word, as they under-
stand it at that point in the session. The use of such
tests has typically required a teacher or researcher
to manually score each response by judging its sim-
ilarity in meaning to the reference definition of the
target word. The resulting scores can then be used
to analyze how a person?s learning of the word re-
sponded to different stimuli, such as seeing the word
used in context. A sample target word and its ref-
erence definition, along with examples of human-
judged responses, are given in Sections 3.3 and 4.1.
However, manual scoring of the definition re-
sponses has several drawbacks. First, it is time-
consuming and must be done by trained experts.
Moreover, if the researcher wanted to test a new hy-
476
pothesis by examining the responses with respect to
a different but related definition, the entire set of re-
sponses would have to be manually re-scored against
the new target. Second, manual scoring can often
be limited in its ability to detect when partial learn-
ing has taken place. This is due to the basic trade-
off between the sophistication of the graded scoring
scale, and the ease and consistency with which hu-
man judges can use the scale. For example, it may
be that the subject did not learn the complete mean-
ing of a particular target word, but did learn that this
target word had negative connotations. The usual
binary or ternary score would provide no or little
indication of such effects. Finally, because manual
scoring almost always must be done off-line after the
end of the session, it presents an obstacle to our goal
of creating learning systems that can adapt quickly,
within a single learning session.
This study describes an effective automated
method for assessing word learning by scoring free
responses to definition production tests. The method
is flexible: it can be used to analyze a response with
respect to whatever reference target(s) the teacher or
researcher chooses. Such a test represents a pow-
erful new tool for language learning research. It is
also a compelling application of human language
technologies research on semantic similarity, and
we review related work for that area in Section 2.
Our probabilistic model for computing text seman-
tic similarity, described in Section 3, can use both
corpus-based and knowledge-based resources. In
Section 4 we describe a new dataset of human def-
inition judgments and use it to measure the effec-
tiveness of the model against other measures of text
similarity. Finally, in Section 5 we discuss further
directions and applications of our work.
2 Related Work
The problem of judging a subject response against a
target definition is a type of text similarity problem.
Moreover, it is a text semantic similarity task, since
we require more than measuring direct word overlap
between the two text fragments. For example, if the
definition of the target word ameliorate is to improve
something and the subject response is make it better,
the response clearly indicates that the subject knows
the meaning of the word, and thus should receive a
high score, even though the response and the target
definition have no words in common.
Because most responses are short (1 ? 10 words)
our task falls somewhere between word-word simi-
larity and passage similarity. There is a broad field
of existing work in estimating the semantic similar-
ity of individual words. This field may be roughly
divided into two groups. First, there are corpus-
based measures, which use statistics or models de-
rived from a large training collection. These require
little or no human effort to construct, but are limited
in the richness of the features they can reliably repre-
sent. Second, there are knowledge-based measures,
which rely on specialized resources such as dictio-
naries, thesauri, experimental data, WordNet, and so
on. Knowledge-based measures tend to be comple-
mentary to a corpus-based approach and emphasize
precision in favor of recall. This is discussed further,
along with a good general summary of text semantic
similarity work, by (Mihalcea et al, 2006).
Because of the fundamental nature of the se-
mantic similarity problem, there are close connec-
tions with other areas of human language tech-
nologies such as information retrieval (Salton and
Lesk, 1971), text alignment in machine transla-
tion (Jayaraman and Lavie, 2005), text summariza-
tion (Mani and Maybury, 1999), and textual co-
herence (Foltz et al, 1998). Educational applica-
tions include automated scoring of essays, surveyed
in (Valenti et al, 2003), and assessment of short-
answer free-response items (Burstein et al, 1999).
As we describe in Section 3, we use a graph to
model relations between words to perform a kind
of semantic smoothing on the language models of
the subject response and target definition before
comparing them. Several types of relation, such
as synonymy and co-occurrence, may be combined
to model the interactions between terms. (Cao
et al, 2005) also formulated a term dependency
model combining multiple term relations in a lan-
guage modeling framework, applied to information
retrieval. Our graph-based approach may be viewed
as a probabilistic variation on the spreading activa-
tion concept, originally proposed for word-word se-
mantic similarity by (Quillian, 1967).
Finally, (Mihalcea et al, 2006) describe a text se-
mantic similarity measure that combines word-word
similarities between the passages being compared.
477
Due to limitations in the knowledge-based similarity
measures used, semantic similarity is only estimated
between words with the same part-of-speech. Our
graph-based approach can relate words of different
types and does not have this limitation. (Mihalcea
et al, 2006) also evaluate their method in terms of
paraphrase recognition using binary judgments. We
view our task as somewhat different than paraphrase
recognition. First, our task is not symmetric: we do
not expect the target definition to be a paraphrase
of the subject?s free response. Second, because we
seek sensitive measures of learning, we want to dis-
tinguish a range of semantic differences beyond a
binary yes/no decision.
3 Statistical Text Similarity Model
We start by describing relations between pairs of
terms using a general probability distribution. These
pairs can then combine into a graph, which we can
apply to define a semantic distance between terms.
3.1 Relations between individual words
One way to model word-to-word relationships is us-
ing a mixture of links, where each link defines a par-
ticular type of relationship. In a graph, this may be
represented by a pair of nodes being joined by mul-
tiple weighted edges, with each edge correspond-
ing to a different link type. Our link-based model
is partially based on one defined by (Toutanova et
al., 2004) for prepositional attachment. We allow
directed edges because some relationships such as
hypernyms may be asymmetric. The following are
examples of different types of links.
1. Stemming: Two words are based on common
morphology. Example: stem and stemming.
We used Porter stemming (Porter, 1980).
2. Synonyms and near-synonyms: Two words
share practically all aspects of meaning.
Example: quaff and drink. Our synonyms
came from WordNet (Miller, 1995).
3. Co-occurrence. Both words tend to appear to-
gether in the same contexts.
Example: politics and election.
4. Hyper- and hyponyms: Relations such as ?X
is a kind of Y ?, as obtained from Wordnet or
other thesaurus-like resources.
Example: airplane and transportation.
5. Free association: A relation defined by the fact
that a person is likely to give one word as a free-
association response to the other.
Example: disaster and fear. Our data was ob-
tained from the Univ. of South Florida associa-
tion database (Nelson et al, 1998).
We denote link functions using ?1, . . . , ?m to
summarize different types of interactions between
words. Each ?m(wi, wj) represents a specific type
of lexical or semantic relation or constraint between
wi and wj . For each link ?m, we also define a
weight ?m that gives the strength of the relationship
between wi and wj for that link.
Our goal is to predict the likelihood of a target
definition D given a test response R consisting of
terms {w0 . . . wk} drawn from a common vocabu-
lary V . We are thus interested in the conditional dis-
tribution p(D | R). We start by defining a simple
model that can combine the link functions in a gen-
eral purpose way to produce the conditional distribu-
tion p(wi|wj) given arbitrary terms wi and wj . We
use a log-linear model of the general form
p(wi|wj) =
1
Z
exp
L?
m=0
?m(i)?m(wi, wj) (1)
In the next sections we show how to combine the
estimate of individual pairs p(wi|wj) into a larger
graph of term relations, which will enable us to cal-
culate the desired p(D | R).
3.2 Combining term relations using graphs
Graphs provide one rich model for representing mul-
tiple word relationships. They can be directed or
undirected, and typically use nodes of words, with
word labels at the vertices, and edges denoting word
relationships. In this model, the dependency be-
tween two words represents a single inference step
in which the label of the destination word is inferred
from the source word. Multiple inference steps may
then be chained together to perform longer-range in-
ference about word relations. In this way, we can in-
fer the similarity of two terms without requiring di-
rect evidence for the relations between that specific
pair. Using the link functions defined in Section 3.1,
478
we imagine a generative process where an author A
creates a short text of N words as follows.
Step 0: Choose an initial word w0 with probabil-
ity P (w0|A). (If we have already generated N
words, stop.)
Step i: Given we have chosen wi?1, then with prob-
ability 1?? output the word wi?1 and reset the
process to step 0. Otherwise, with probability
?, sample a new word wi according to the dis-
tribution:
P (wi|wi?1) =
1
Z
exp
L?
m=0
?m(i)?m(wi, wi?1)
(2)
where Z is the normalization quantity.
This conditional probability may be interpreted
as a mixture model in which a particular link type
?m(.) is chosen with probability ?m(i) at timestep
i. Note that the mixture is allowed to change at each
timestep. For simplicity, we limit the number of
such changes by grouping the timesteps of the walk
into three stages: early, middle and final. The func-
tion ?(i) defines how timestep i maps to stage s,
where s ? {0, 1, 2}, and we now refer to ?m(s) in-
stead of ?m(i).
Suppose we have a definition D consisting of
terms {di}. For each link type ?m(.) we define a
transition matrix C(D,m) based on the definition
D. The reason D influences the transition matrix
is that some link types, such as proximity and co-
occurrence, are context-specific. Each stage s has
an overall transition matrix C(D, s) as the mixture
of the individual C(D,m), as follows.
C(D, s) =
M?
m=1
?m(s)C(D,m) (3)
Combining the stages over k steps into a single
transition matrix, which we denote Ck, we have
Ck =
k?
i=0
C(D,?(i)) (4)
We denote the (i, j) entry of a matrix Ak by Aki,j .
Then for a particular term di, the probability that a
chain reaches di after k steps, starting at word w is
Pk(di|w) = (1 ? ?)?
kCkw,di (5)
where we identify w and di with their corresponding
indices into the vocabulary V . The overall probabil-
ity p(di|w) of generating a definition term di given
a word w is therefore
P (di|w) =
??
k=0
Pk(di|w) = (1??)(
??
k=0
?kCk)w,di
(6)
The walk continuation probability ? can be
viewed as a penalty for long chains of inference. In
practice, to perform the random walk steps we re-
place the infinite sum of Eq. 6 with a small number
of steps (up to 5) on a sparse representation of the
adjacency graph. We obtained effective link weights
?m(i) empirically using held-out data. For simplic-
ity we assume that the same ? is used across all link
types, but further improvement may be possible by
extending the model to use link-specific decays ?m.
Fine-tuning these parameter estimation methods is a
subject of future work.
3.3 Using the model for definition scoring
In our study the reference definition for the target
word consisted of the target word, a rare synonym,
a more frequent synonym, and a short glossary-like
definition phrase. For example, the reference defini-
tion for abscond was
abscond; absquatulate; escape; to leave quickly
and secretly and hide oneself, often to avoid arrest
or prosecution.
In general, we define the score of a response R
with respect to a definition D as the probability
that the definition is generated by the response, or
p(D|R). Equivalently, we can score by log p(D|R)
since the log function is monotonic. So making the
simplifying assumption that the terms di ? D are
exchangable (the bag-of-words assumption), and
taking logarithms, we have:
log p(D|R) = log
?
di?D
p(di|R)
=
?
di?D
log[(1 ? ?)(
m?
k=0
?kCk)R,di ]
(7)
Suppose that the response to be scored is run from
the cops. In practical terms, Eq. 7 means that for our
479
example, we ?light up? the nodes in the graph cor-
responding to run, from, the and cops by assigning
some initial probability, and the graph is then ?run?
using the transition matrix C according to Eq. 7. In
this study, the initial node probabilities are set to val-
ues proportional to the idf values of the correspond-
ing term, so that P (di) = idf(di)P idf(di) . After m steps,
the probabilities at the nodes for each term in the
reference definition R are read off, and their log-
arithms summed. Similar to an AND calculation,
we calculate a product of sums over the graph, so
that responses reflecting multiple aspects of the tar-
get definition are rewarded more highly than a very
strong prediction for only a single definition term.
4 Evaluation
We first describe our corpus of gold standard human
judgments. We then explain the different text sim-
ilarity methods and baselines we computed on the
corpus responses. Finally, we give an analysis and
discussion of the results.
4.1 Corpus
We obtained a set of 734 responses to definition pro-
duction tests from a word learning experiment at the
University of Pittsburgh (Bolger et al, 2006). In
total, 72 target words, selected by the same group,
were used in the experiment. In this experiment,
subjects were asked to learn the meaning of target
words after seeing them used in a series of context
sentences. We set aside 70 responses for training,
leaving 664 responses in the final test dataset.
Each response instance was coded using the scale
shown in Table 1, and a sample set of subject re-
sponses and scores is shown in Table 2. The target
word was treated as having several key aspects of
meaning. The coders were instructed to judge a re-
sponse according to how well it covered the various
aspects of the target definition. If the response cov-
ered all aspects of the target definition, but also in-
cluded extra irrelevant information, this was treated
as a partial match at the discretion of the coders.
We obtained three codings of the final dataset.
The first two codings were obtained using an in-
dependent group, the QDAP Center at the Univer-
sity of Pittsburgh. Initially, five human coders, with
varying degrees of general coding experience, were
Score Meaning
0 Completely wrong
1 Some partial aspect is correct
2 One major aspect, or more than one
minor aspect, is correct
3 Covers all aspects correctly
Table 1: Scale for human definition judgements.
Response Human
Score
depart secretly 3
quietly make away, escape 3
to flee, run away 2
flee 2
to get away with 1
to steal or take 0
Table 2: Examples of human scores of responses for
the target word abscond.
trained by the authors using one set of 10 example
instances and two training sessions of 30 instances
each. Between the two training sessions, one of the
authors met with the coders to discuss the ratings
and refine the rating guidelines. After training, the
authors selected the two coders who had the best
inter-coder agreement on the 60 training instances.
These two coders then labeled the final test set of
664 instances. Our third coding was obtained from
an initial coding created by an expert in the Univer-
sity of Pittsburgh Psychology department and then
adjusted by one of the authors to resolve a small
number of internal inconsistencies, such as when the
same response to the same target had been given a
different score.
Inter-coder agreement was measured using lin-
ear weighted kappa, a standard technique for or-
dinal scales. Weighted kappa scores for all three
coder pairs are shown in Table 3. Overall, agree-
ment ranged from moderate (0.64) to good (0.72).
4.2 Baseline Methods
We computed three baselines as reference points for
lower and upper performance bounds.
Random. The response items were assigned la-
bels randomly.
480
Coder pair Weighted
Kappa
1, 2 0.68
2, 3 0.64
1, 3 0.72
Table 3: Weighted kappa inter-rater reliability for
three human coders on our definition response
dataset (664 items).
Method Spearman Rank
Correlation
Random 0.3661
Cosine 0.4731
LSA 0.4868
Markov 0.6111
LSA + Markov 0.6365
Human 0.8744
Table 4: Ability of methods to match human ranking
of responses, as measured by Spearman rank corre-
lation (corrected for ties).
Human choice of label. We include a method
that, given an item and a human label from one of the
coders, simply returns a label of the same item from
a different coder, with results repeated and averaged
over all coders. This gives an indication of an upper
bound based on human performance.
Cosine similarity using tf.idf weighting. Cosine
similarity is a widely-used text similarity method
for tasks where the passages being compared of-
ten have significant direct word overlap. We repre-
sent response items and reference definitions as vec-
tors of terms using tf.idf weighting, a standard tech-
nique from information retrieval (Salton and Buck-
ley, 1997) that combines term frequency (tf) with
term specificity (idf). A good summary of arguments
for using idf can be found in (Robertson, 2004). To
compute idf, we used frequencies from a standard
100-million-word corpus of written and spoken En-
glish 1. We included a minimal semantic similar-
ity component by applying Porter stemming (Porter,
1980) on terms.
1The British National Corpus (Burnage and Dunlop, 1992),
using American spelling conversion.
4.3 Methods
In addition to the baseline methods, we also ran the
following three algorithms over the responses.
Markov chains (?Markov?). This is the method
described in Section 3. A maximum of 5 random
walk steps were used, with a walk continuation
probability of 0.8. Each walk step used a mixture of
synonym, stem, co-occurrence, and free-association
links. The link weights were trained on a small set
of held-out data.
Latent Semantic Analysis (LSA). LSA (Lan-
dauer et al, 1998) is a corpus-based unsupervised
technique that uses dimensionality reduction to clus-
ter terms according to multi-order co-occurrence re-
lations. In these experiments, we obtained LSA-
based similarity scores between responses and target
definitions using the software running on the Univer-
sity of Colorado LSA Web site (LSA site, 2006). We
used the pairwise text passage comparison facility,
using the maximum 300 latent factors and a general
English corpus (Grade 1 ? first-year college).
Although LSA and the Markov chain approach
are based on different principles, we chose to ap-
ply LSA to this new response-scoring task and cor-
pus because LSA has been widely used as a text se-
mantic similarity measure for other tasks and shown
good performance (Foltz et al, 1998).
LSA+Markov. To test the effectiveness of com-
bining two different ? and possibly complemen-
tary ? approaches to response scoring, we created
a normalized, weighted linear combination of the
LSA and Markov scores, with the model combina-
tion weight being derived from cross-validation on a
held-out dataset.
4.4 Results
We measured the effectiveness of each scoring
method from two perspectives: ranking quality, and
label accuracy.
First, we measured how well each scoring method
was able to rank response items by similarity to the
target definition. To do this, we calculated the Spear-
man Rank Correlation (corrected for ties) between
the ranking based on the scoring method and the
ranking based on the human-assigned scores, aver-
aged over all sets of target word responses.
Table 4 summarizes the ranking results. For
481
Method Label error (RMS)
Top 1 Top 3
Random 1.4954 1.6643
Cosine 0.8194 1.0540
LSA 0.8009 0.9965
Markov 0.7222 0.7968
LSA + Markov 1.1111 1.0650
Human 0.1944 0.4167
Table 5: Root mean squared error (RMSE) of la-
bel(s) for top-ranked item, and top-three items for
all 77 words in the dataset.
overall quality of ranking, the Markov method had
significantly better performance than the other au-
tomated methods (p < 2.38e?5). LSA gave a
small, but not significant, improvement in overall
rank quality over the cosine baseline. 2 The sim-
ple combination of LSA and Markov resulted in a
slightly higher but statistically insignificant differ-
ence (p < 0.253).
Second, we examined the ability of each method
to find the most accurate responses ? that is, the re-
sponses with the highest human label on average ?
for a given target word. To do this, we calculated the
Root Mean Squared Error (RMSE) of the label as-
signed to the top item, and the top three items. The
results are shown in Table 5. For top-item detec-
tion, our Markov model had the lowest RMS error
(0.7222) of the automated methods, but the differ-
ences from Cosine and LSA were not statistically
significant, while differences for all three from Ran-
dom and Human baselines were significant. For
the top three items, the difference between Markov
(0.7968) and LSA (0.9965) was significant at the
p < 0.03 level.
Comparing the overall rank accuracy with top-
item accuracy, the combined LSA + Markov method
was significantly worse at finding the three best-
quality responses (RMSE of 1.0650) than Markov
(0.7968) or LSA (0.9965) alone. The reasons for
this require further study.
2All statistical significance results reported here used the
Wilcoxon Signed-Ranks test.
5 Discussion
Even though definition scoring may seem more
straightforward than other automated learning as-
sessment problems, human performance was still
significantly above the best automated methods in
our study, for both ranking and label accuracy. There
are certain additions to our model which seem likely
to result in further improvement.
One of the most important is the ability to identify
phrases or colloquial expressions. Given the short
length of a response, these seem critical to handle
properly. For example, to get away with something
is commonly understood to mean secretly guilty, not
a physical action. Yet the near-identical phrase to
get away from something means something very dif-
ferent when phrases and idioms are considered.
Despite the gap between human and automated
performance, the current level of accuracy of the
Markov chain approach has already led to some
promising early results in word learning research.
For example, in a separate study of incremental
word learning (Frishkoff et al, 2006), we used our
measure to track increments in word knowledge
across multiple trials. Each trial consisted of a sin-
gle passage that was either supportive ? containing
clues to the meaning of unfamiliar words ? or not
supportive. In this separate study, broad learning ef-
fects identified by our measure were consistent with
effects found using manually-scored pre- and post-
tests. Our automated method also revealed a pre-
viously unknown interaction between trial spacing,
the proportion of supportive contexts per word, and
reader skill.
In future applications, we envision using our auto-
mated measure to allow a form of feedback for intel-
ligent language tutors, so that the system can auto-
matically adapt its behavior based on the student?s
test responses. With some adjustments, the same
scoring model described in this study may also be
applied to the problem of finding supportive contexts
for students.
6 Conclusions
We presented results for both automated and hu-
man performance of an important task for language
learning applications: scoring definition responses.
We described a probabilistic model of text seman-
482
tic similarity that uses Markov chains on a graph of
term relations to perform a kind of semantic smooth-
ing. This model incorporated both corpus-based and
knowledge-based resources to compute text seman-
tic similarity. We measured the effectiveness of both
our method and LSA compared to cosine and ran-
dom baselines, using a new corpus of human judg-
ments on definition responses from a language learn-
ing experiment. Our method outperformed the tf.idf
cosine similarity baseline in ranking quality and in
ability to find high-scoring definitions. Because
LSA and our Markov chain method are based on
different approaches and resources, it is difficult to
draw definitive conclusions about performance dif-
ferences between the two methods.
Looking beyond definition scoring, we believe au-
tomated methods for assessing word learning have
great potential as a new scientific tool for language
learning researchers, and as a key component of in-
telligent tutoring systems that can adapt to students.
Acknowledgements
We thank D.J. Bolger and C. Perfetti for the use
of their definition response data, Stuart Shulman
for his guidance of the human coding effort, and
the anonymous reviewers for their comments. This
work supported by U.S. Dept. of Education grant
R305G03123. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are the authors? and do not necessarily reflect those
of the sponsors.
References
D.J. Bolger, M. Balass, E. Landen and C.A. Perfetti.
2006. Contextual Variation and Definitions in Learn-
ing the Meanings of Words. (In press.)
G. Burnage and D. Dunlop. 1992. Encoding the British
National Corpus. English Language Corpora: De-
sign, Analysis and Exploitation. The 13th Intl. Conf.
on Engl. Lang. Res. in Computerized Corpora. Ni-
jmegen. J. Aarts, P. de Haan, N. Oostdijk, Eds.
J. Burstein, S. Wolff, and L. Chi. 1999. Using Lexi-
cal Semantic Techniques to Classify Free-Responses.
Breadth and Depth of Semantic Lexicons. Kluwer
Acad. Press, p. 1?18.
G. Cao, J-Y. Nie, and J. Bai. Integrating Word Relation-
ships into Language Models. SIGIR 2005, 298?305.
P.W. Foltz, W. Kintsch, and T. Landauer. 1998. An Intro-
duction to Latent Semantic Analysis. Discourse Pro-
cesses, 25(2):285?307.
G. Frishkoff, K. Collins-Thompson, J. Callan, and C.
Perfetti. 2007. The Nature of Incremental Word
Learning: Context Quality, Spacing Effects, and Skill
Differences in Meaning Acquisition Across Multiple
Contexts. (In preparation.)
M. Heilman, K. Collins-Thompson, J. Callan and M. Es-
kanazi. 2006. Classroom Success of an Intelligent Tu-
toring System for Lexical Practice and Reading Com-
prehension. ICSLP 2006.
S. Jayaraman and A. Lavie. Multi-Engine Ma-
chine Translation Guided by Explicit Word Matching.
EAMT 2005.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
Introduction to Latent Semantic Analysis. Discourse
Processes, 25:259?284.
LSA Web Site. http://lsa.colorado.edu
I. Mani and M.T. Maybury (Eds.) 1999. Advances in
Automatic Text Summarization. MIT Press.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures of Text
Semantic Similarity. AAAI 2006
G. Miller. 1998. WordNet: A Lexical Database for En-
glish. Communications of the ACM, 38(11) 39?41.
D.L. Nelson, C.L. McEvoy, and T.A. Schreiber.
1998. The University of South Florida word
association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/
M. Porter. 1980. An Algorithm for Suffix-
stripping. Program, 14(3) 130?137.
http://www.tartarus.org/martin/PorterStemmer
M. Quillian. 1967. Word Concepts: A Theory and Sim-
ulation of some Basic Semantic Capabilities. Behav.
Sci., 12: 410?430.
S. Robertson. 2004. Understanding Inverse Document
Frequency: on Theoretical Arguments for IDF. J. of
Documentation, 60:503?520.
G. Salton and C. Buckley. 1997. Term Weighting Ap-
proaches in Automatic Text Retrieval. Reading in In-
formation Retrieval. Morgan Kaufmann.
G. Salton and M. Lesk. 1971. Computer Evaluation
of Indexing and Text Processing. Prentice-Hall. 143
? 180.
K. Toutanova, C.D. Manning, and A.Y. Ng. 2004. Learn-
ing Random Walk Models for Inducing Word Depen-
dency Distributions. ICML 2004.
S. Valenti, F. Neri, and A. Cucchiarelli. 2003. An
Overview of Current Research on Automated Essay
Grading. J. of Info. Tech. Ed., Vol. 2.
483
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 85?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Dictionary Definitions based Homograph Identification using a  
Generative Hierarchical Model 
 
 
Anagha Kulkarni Jamie Callan 
Language Technologies Institute 
School of Computer Science, Carnegie Mellon University 
5000 Forbes Ave, Pittsburgh, PA 15213, USA 
{anaghak, callan}@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
A solution to the problem of homograph 
(words with multiple distinct meanings) iden-
tification is proposed and evaluated in this pa-
per. It is demonstrated that a mixture model 
based framework is better suited for this task 
than the standard classification algorithms ? 
relative improvement of 7% in F1 measure 
and 14% in Cohen?s kappa score is observed.  
1 Introduction 
Lexical ambiguity resolution is an important re-
search problem for the fields of information re-
trieval and machine translation (Sanderson, 2000; 
Chan et al, 2007). However, making fine-grained 
sense distinctions for words with multiple closely-
related meanings is a subjective task (Jorgenson, 
1990; Palmer et al, 2005), which makes it difficult 
and error-prone.  Fine-grained sense distinctions 
aren?t necessary for many tasks, thus a possibly-
simpler alternative is lexical disambiguation at the 
level of homographs (Ide and Wilks, 2006).  
Homographs are a special case of semantically 
ambiguous words:  Words that can convey multi-
ple distinct meanings. For example, the word bark 
can imply two very different concepts ? ?outer 
layer of a tree trunk?, or, ?the sound made by a 
dog? and thus is a homograph. Ironically, the defi-
nition of the word ?homograph? is itself ambiguous 
and much debated; however, in this paper we con-
sistently use the above definition.  
If the goal is to do word-sense disambiguation 
of homographs in a very large corpus, a manually-
generated homograph inventory may be impracti-
cal. In this case, the first step is to determine which 
words in a lexicon are homographs.  This problem 
is the subject of this paper. 
2 Finding the Homographs in a Lexicon 
Our goal is to identify the homographs in a large 
lexicon.  We assume that manual labor is a scarce 
resource, but that online dictionaries are plentiful 
(as is the case on the web).  Given a word from the 
lexicon, definitions are obtained from eight dic-
tionaries: Cambridge Advanced Learners Diction-
ary (CALD), Compact Oxford English Dictionary, 
MSN Encarta, Longman Dictionary of Contempo-
rary English (LDOCE), The Online Plain Text 
English Dictionary, Wiktionary, WordNet and 
Wordsmyth. Using multiple dictionaries provides 
more evidence for the inferences to be made and 
also minimizes the risk of missing meanings be-
cause a particular dictionary did not include one or 
more meanings of a word (a surprisingly common 
situation). We can now rephrase the problem defi-
nition as that of determining which words in the 
lexicon are homographs given a set of dictionary 
definitions for each of the words.  
2.1 Features 
We use nine meta-features in our algorithm. In-
stead of directly using common lexical features 
such as n-grams we use meta-features which are 
functions defined on the lexical features. This ab-
85
straction is essential in this setup for the generality 
of the approach. For each word w to be classified 
each of the following meta-features are computed. 
 
1. Cohesiveness Score: Mean of the cosine simi-
larities between each pair of definitions of w. 
2. Average Number of Definitions: The average 
number of definitions per dictionary. 
3. Average Definition Length: The average 
length (in words) of definitions of w. 
4. Average Number of Null Similarities: The 
number of definition pairs that have zero co-
sine similarity score (no word overlap). 
5. Number of Tokens: The sum of the lengths 
(in words) of the definitions of w. 
6. Number of Types: The size of the vocabulary 
used by the set of definitions of w. 
7. Number of Definition Pairs with n Word 
Overlaps: The number of definition pairs that 
have more than n=2 words in common. 
8. Number of Definition Pairs with m Word 
Overlaps: The number of definition pairs that 
have more than m=4 words in common. 
9. Post Pruning Maximum Similarity: (below) 
 
The last feature sorts the pair-wise cosine similar-
ity scores in ascending order, prunes the top n% of 
the scores, and uses the maximum remaining score 
as the feature value.  This feature is less ad-hoc 
than it may seem.  The set of definitions is formed 
from eight dictionaries, so almost identical defini-
tions are a frequent phenomenon, which makes the 
maximum cosine similarity a useless feature. A 
pruned maximum turns out to be useful informa-
tion. In this work n=15 was found to be most in-
formative using a tuning dataset.  
Each of the above features provides some 
amount of discriminative power to the algorithm. 
For example, we hypothesized that on average the 
cohesiveness score will be lower for homographs 
than for non-homographs. Figure 1 provides an 
illustration. If empirical support was observed for 
such a hypothesis about a candidate feature then 
the feature was selected. This empirical evidence 
was derived from only the training portion of the 
data (Section 3.1).  
The above features are computed on definitions 
stemmed with the Porter Stemmer. Closed class 
words, such as articles and prepositions, and dic-
tionary-specific stopwords, such as ?transitive?, 
?intransitive?, and ?countable?, were also removed. 
Figure 1. Histogram of Cohesiveness scores for Homo-
graphs and Non-homographs. 
2.2 Models 
We formulate the homograph detection process as 
a generative hierarchical model. Figure 2 provides 
the plate notation of the graphical model. The la-
tent (unobserved) variable Z models the class in-
formation: homograph or non-homograph. Node X 
is the conditioned random vector (Z is the condi-
tioning variable) that models the feature vector. 
 
Figure 2.  Plate notation for the proposed model. 
 
This setup results in a mixture model with two 
components, one for each class. The Z is assumed 
to be Bernoulli distributed and thus parameterized 
by a single parameter p. We experiment with two 
continuous multivariate distributions, Dirichlet and 
Multivariate Normal (MVN), for the conditional 
distribution of X|Z. 
Z ~ Bernoulli (p) 
X|Z ~ Dirichlet (az)   
OR 
X|Z ~ MVN (muz, covz) 
We will refer to the parameters of the condi-
tional distribution as ?z. For the Dirichlet distribu-
tion, ?z is a ten-dimensional vector az = (az1, .., 
az10). For the MVN, ?z represents a nine-
dimensional mean vector muz = (muz1, .., muz9) 
N 
p Z 
X ? 
 
86
and a nine-by-nine-dimensional covariance matrix 
covz. We use maximum likelihood estimators 
(MLE) for estimating the parameters (p, ?z). The 
MLEs for Bernoulli and MVN parameters have 
analytical solutions. Dirichlet parameters were es-
timated using an estimation method proposed and 
implemented by Tom Minka1. 
We experiment with three model setups: Super-
vised, semi-supervised, and unsupervised. In the 
supervised setup we use the training data described 
in Section 3.1 for parameter estimation and then 
use thus fitted models to classify the tuning and 
test dataset. We refer to this as the Model I. In 
Model II, the semi-supervised setup, the training 
data is used to initialize the Expectation-
Maximization (EM) algorithm (Dempster et al, 
1977) and the unlabeled data, described in Section 
3.1, updates the initial estimates. The Viterbi 
(hard) EM algorithm was used in these experi-
ments. The E-step was modified to include only 
those unlabeled data-points for which the posterior 
probability was above certain threshold. As a re-
sult, the M-step operates only on these high poste-
rior data-points. The optimal threshold value was 
selected using a tuning set (Section 3.1). The unsu-
pervised setup, Model III, is similar to the semi-
supervised setup except that the EM algorithm is 
initialized using an informed guess by the authors. 
3 Data 
In this study, we concentrate on recognizing 
homographic nouns, because homographic ambi-
guity is much more common in nouns than in 
verbs, adverbs or adjectives. 
3.1 Gold Standard Data 
A set of potentially-homographic nouns was identi-
fied by selecting all words with at least two noun 
definitions in both CALD and LDOCE.  This set 
contained 3,348 words. 
225 words were selected for manual annotation 
as homograph or non-homograph by random sam-
pling of words that were on the above list and used 
in prior psycholinguistic studies of homographs 
(Twilley et al, 1994; Azuma, 1996) or on the Aca-
demic Word List (Coxhead, 2000). 
Four annotators at, the Qualitative Data Analysis 
Program at the University of Pittsburgh, were 
                                                          
1
 http://research.microsoft.com/~minka/software/fastfit/ 
trained to identify homographs using sets of dic-
tionary definitions.  After training, each of the 225 
words was annotated by each annotator. On aver-
age, annotators categorized each word in just 19 
seconds.  The inter-annotator agreement was 0.68, 
measured by Fleiss? Kappa. 
23 words on which annotators disagreed (2/2 
vote) were discarded, leaving a set of 202 words 
(the ?gold standard?) on which at least 3 of the 4 
annotators agreed. The best agreement between the 
gold standard and a human annotator was 0.87 
kappa, and the worst was 0.78. The class distribu-
tion (homographs and non-homographs) was 0.63, 
0.37. The set of 3,123 words that were not anno-
tated was the unlabeled data for the EM algorithm. 
4 Experiments and Results 
A stratified division of the gold standard data in 
the proportion of 0.75 and 0.25 was done in the 
first step. The smaller portion of this division was 
held out as the testing dataset. The bigger portion 
was further divided into two portions of 0.75 and 
0.25 for the training set and the tuning set, respec-
tively. The best and the worst kappa between a 
human annotator and the test set are 0.92 and 0.78. 
Each of the three models described in Section 
2.2 were experimented with both Dirichlet and 
MVN as the conditional. An additional experiment 
using two standard classification algorithms ? Ker-
nel Based Na?ve Bayes (NB) and Support Vector 
Machines (SVM) was performed. We refer to this 
as the baseline experiment. The Na?ve Bayes clas-
sifier outperformed SVM on the tuning as well as 
the test set and thus we report NB results only. A 
four-fold cross-validation was employed for the all 
the experiments on the tuning set. The results are 
summarized in Table 1. The reported precision, 
recall and F1 values are for the homograph class.  
The na?ve assumption of class conditional fea-
ture independence is common to simple Na?ve 
Bayes classifier, a kernel based NB classifier; 
however, unlike simple NB it is capable of model-
ing non-Gaussian distributions. Note that in spite 
of this advantage the kernel based NB is outper-
formed by the MVN based hierarchical model. Our 
nine features are by definition correlated and thus 
it was our hypothesis that a multivariate distribu-
tion such as MVN which can capture the covari-
ance amongst the features will be a better fit. The 
above finding confirms this hypothesis. 
87
 Table 1. Results for the six models and the baseline on the tuning and test set.
One of the known situations when mixture mod-
els out-perform standard classification algorithms 
is when the data comes from highly overlapping 
distributions. In such cases the classification algo-
rithms that try to place the decision boundary in a 
sparse area are prone to higher error-rates than 
mixture model based approach. We believe that 
this is explanations of the observed results. On the 
test set a relative improvement of 7% in F1 and 
14% in kappa statistic is obtained using the MVN 
mixture model. 
The results for the semi-supervised models are 
non-conclusive. Our post-experimental analysis 
reveals that the parameter updation process using 
the unlabeled data has an effect of overly separat-
ing the two overlapping distributions. This is trig-
gered by our threshold based EM methodology 
which includes only those data-points for which 
the model is highly confident; however such data-
points are invariable from the non-overlapping re-
gions of the distribution, which gives a false view 
to the learner that the distributions are less over-
lapping. We believe that the unsupervised models 
also suffer from the above problem in addition to 
the possibility of poor initializations. 
5 Conclusions 
We have demonstrated in this paper that the prob-
lem of homograph identification can be ap-
proached using dictionary definitions as the source 
of information about the word. Further more, using 
multiple dictionaries provides more evidence for 
the inferences to be made and also minimizes the 
risk of missing few meanings of the word.  
We can conclude that by modeling the underly-
ing data generation process as a mixture model, the 
problem of homograph identification can be per-
formed with reasonable accuracy.  
The capability of identifying homographs from 
non-homographs enables us to take on the next 
steps of sense-inventory generation and lexical 
ambiguity resolution. 
Acknowledgments 
We thank Shay Cohen and Dr. Matthew Harrison for the 
helpful discussions. This work was supported in part by 
the Pittsburgh Science of Learning Center which is 
funded by the National Science Foundation, award 
number SBE-0354420.  
References  
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum 
likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series 
B, 39(1):1?38. 
A. Coxhead. 2000. A New Academic Word List. 
TESOL, Quarterly, 34(2): 213-238. 
J. Jorgenson. 1990. The psychological reality of word 
senses. Journal of Psycholinguistic Research 19:167-
190. 
L. Twilley, P. Dixon, D. Taylor, and K. Clark. 1994. 
University of Alberta norms of relative meaning fre-
quency for 566 homographs. Memory and Cognition. 
22(1): 111-126. 
M. Sanderson. 2000. Retrieving with good sense. In-
formation Retrieval, 2(1): 49-69. 
M. Palmer, H. Dang, C. Fellbaum, 2005. Making fine-
grained and coarse-grained sense distinctions. Jour-
nal of Natural Language Engineering. 13: 137-163.  
N. Ide and Y. Wilks. 2006. Word Sense Disambigua-
tion, Algorithms and Applications. Springer, 
Dordrecht, The Netherlands. 
T. Azuma. 1996. Familiarity and Relatedness of Word 
Meanings: Ratings for 110 Homographs. Behavior 
Research Methods, Instruments and Computers. 
28(1): 109-124. 
Y. Chan, H. Ng, and D. Chiang. 2007. Proceeding of 
Association for Computational Linguistics, Prague, 
Czech Republic. 
 Tuning Set Test Set 
 
Preci-
sion Recall F1 Kappa 
Preci-
sion Recall F1 Kappa 
Model I ? Dirichlet 0.84 0.74 0.78 0.47 0.81 0.62 0.70 0.34 
Model II ? Dirichlet 0.85 0.71 0.77 0.45 0.81 0.60 0.68 0.33 
Model III ? Dirichlet 0.78 0.74 0.76 0.37 0.82 0.56 0.67 0.32 
Model I ? MVN 0.70 0.75 0.78 0.32 0.80 0.73 0.76 0.41 
Model II ? MVN 0.74 0.82 0.78 0.34 0.71 0.79 0.74 0.25 
Model III ? MVN 0.69 0.89 0.77 0.22 0.64 0.84 0.72 0.22 
Baseline ? NB 0.82 0.73 0.77 0.43 0.82 0.63 0.71 0.36 
88
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 271?279,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Metric-based Framework for Automatic Taxonomy Induction 
 
 
Hui Yang 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
huiyang@cs.cmu.edu 
Jamie Callan 
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
callan@cs.cmu.edu 
  
 
Abstract 
This paper presents a novel metric-based 
framework for the task of automatic taxonomy 
induction. The framework incrementally clus-
ters terms based on ontology metric, a score 
indicating semantic distance; and transforms 
the task into a multi-criteria optimization 
based on minimization of taxonomy structures 
and modeling of term abstractness. It com-
bines the strengths of both lexico-syntactic 
patterns and clustering through incorporating 
heterogeneous features. The flexible design of 
the framework allows a further study on which 
features are the best for the task under various 
conditions. The experiments not only show 
that our system achieves higher F1-measure 
than other state-of-the-art systems, but also re-
veal the interaction between features and vari-
ous types of relations, as well as the interac-
tion between features and term abstractness.  
1 Introduction 
Automatic taxonomy induction is an important 
task in the fields of Natural Language 
Processing, Knowledge Management, and Se-
mantic Web. It has been receiving increasing 
attention because semantic taxonomies, such as 
WordNet (Fellbaum, 1998), play an important 
role in solving knowledge-rich problems, includ-
ing question answering (Harabagiu et al, 2003) 
and textual entailment (Geffet and Dagan, 2005). 
Nevertheless, most existing taxonomies are ma-
nually created at great cost. These taxonomies 
are rarely complete; it is difficult to include new 
terms in them from emerging or rapidly changing 
domains. Moreover, manual taxonomy construc-
tion is time-consuming, which may make it un-
feasible for specialized domains and personalized 
tasks. Automatic taxonomy induction is a solu-
tion to augment existing resources and to pro-
duce new taxonomies for such domains and 
tasks. 
Automatic taxonomy induction can be decom-
posed into two subtasks: term extraction and re-
lation formation. Since term extraction is rela-
tively easy, relation formation becomes the focus 
of most research on automatic taxonomy induc-
tion. In this paper, we also assume that terms in a 
taxonomy are given and concentrate on the sub-
task of relation formation. 
Existing work on automatic taxonomy induc-
tion has been conducted under a variety of 
names, such as ontology learning, semantic class 
learning, semantic relation classification, and 
relation extraction. The approaches fall into two 
main categories: pattern-based and clustering-
based. Pattern-based approaches define lexical-
syntactic patterns for relations, and use these pat-
terns to discover instances of relations. Cluster-
ing-based approaches hierarchically cluster terms 
based on similarities of their meanings usually 
represented by a vector of quantifiable features. 
Pattern-based approaches are known for their 
high accuracy in recognizing instances of rela-
tions if the patterns are carefully chosen, either 
manually (Berland and Charniak, 1999; Kozare-
va et al, 2008) or via automatic bootstrapping 
(Hearst, 1992; Widdows and Dorow, 2002; Girju 
et al, 2003). The approaches, however, suffer 
from sparse coverage of patterns in a given cor-
pus. Recent studies (Etzioni et al, 2005; Kozare-
va et al, 2008) show that if the size of a corpus, 
such as the Web, is nearly unlimited, a pattern 
has a higher chance to explicitly appear in the 
corpus. However, corpus size is often not that 
large; hence the problem still exists. Moreover, 
since patterns usually extract instances in pairs, 
the approaches suffer from the problem of incon-
sistent concept chains after connecting pairs of 
instances to form taxonomy hierarchies.  
Clustering-based approaches have a main ad-
vantage that they are able to discover relations 
271
which do not explicitly appear in text. They also 
avoid the problem of inconsistent chains by ad-
dressing the structure of a taxonomy globally 
from the outset. Nevertheless, it is generally be-
lieved that clustering-based approaches cannot 
generate relations as accurate as pattern-based 
approaches. Moreover, their performance is 
largely influenced by the types of features used. 
The common types of features include contextual 
(Lin, 1998), co-occurrence (Yang and Callan, 
2008), and syntactic dependency (Pantel and Lin, 
2002; Pantel and Ravichandran, 2004). So far 
there is no systematic study on which features 
are the best for automatic taxonomy induction 
under various conditions. 
This paper presents a metric-based taxonomy 
induction framework. It combines the strengths 
of both pattern-based and clustering-based ap-
proaches by incorporating lexico-syntactic pat-
terns as one type of features in a clustering 
framework. The framework integrates contex-
tual, co-occurrence, syntactic dependency, lexi-
cal-syntactic patterns, and other features to learn 
an ontology metric, a score indicating semantic 
distance, for each pair of terms in a taxonomy; it 
then incrementally clusters terms based on their 
ontology metric scores. The incremental cluster-
ing is transformed into an optimization problem 
based on two assumptions: minimum evolution 
and abstractness. The flexible design of the 
framework allows a further study of the interac-
tion between features and relations, as well as 
that between features and term abstractness. 
2 Related Work 
There has been a substantial amount of research 
on automatic taxonomy induction. As we men-
tioned earlier, two main approaches are pattern-
based and clustering-based.  
Pattern-based approaches are the main trend 
for automatic taxonomy induction. Though suf-
fering from the problems of sparse coverage and 
inconsistent chains, they are still popular due to 
their simplicity and high accuracy. They have 
been applied to extract various types of lexical 
and semantic relations, including is-a, part-of, 
sibling, synonym, causal, and many others.  
Pattern-based approaches started from and still 
pay a great deal of attention to the most common  
is-a relations. Hearst (1992) pioneered using a 
hand crafted list of hyponym patterns as seeds 
and employing bootstrapping to discover is-a 
relations. Since then, many approaches (Mann, 
2002; Etzioni et al, 2005; Snow et al, 2005) 
have used Hearst-style patterns in their work on 
is-a relations. For instance, Mann (2002) ex-
tracted is-a relations for proper nouns by Hearst-
style patterns. Pantel et al (2004) extended is-a 
relation acquisition towards terascale, and auto-
matically identified hypernym patterns by mi-
nimal edit distance. 
Another common relation is sibling, which de-
scribes the relation of sharing similar meanings 
and being members of the same class. Terms in 
sibling relations are also known as class mem-
bers or similar terms. Inspired by the conjunction 
and appositive structures, Riloff and Shepherd 
(1997), Roark and Charniak (1998) used co-
occurrence statistics in local context to discover 
sibling relations. The KnowItAll system (Etzioni 
et al, 2005) extended the work in (Hearst, 1992) 
and bootstrapped patterns on the Web to discover 
siblings; it also ranked and selected the patterns 
by statistical measures. Widdows and Dorow 
(2002) combined symmetric patterns and graph 
link analysis to discover sibling relations. Davi-
dov and Rappoport (2006) also used symmetric 
patterns for this task. Recently, Kozareva et al 
(2008) combined a double-anchored hyponym 
pattern with graph structure to extract siblings. 
The third common relation is part-of. Berland 
and Charniak (1999) used two meronym patterns 
to discover part-of relations, and also used statis-
tical measures to rank and select the matching 
instances. Girju et al (2003) took a similar ap-
proach to Hearst (1992) for part-of relations. 
Other types of relations that have been studied 
by pattern-based approaches include question-
answer relations (such as birthdates and inven-
tor) (Ravichandran and Hovy, 2002), synonyms 
and antonyms (Lin et al, 2003), general purpose 
analogy (Turney et al, 2003), verb relations (in-
cluding similarity, strength, antonym, enable-
ment and temporal) (Chklovski and Pantel, 
2004), entailment (Szpektor et al, 2004), and 
more specific relations, such as purpose, creation 
(Cimiano and Wenderoth, 2007), LivesIn, and 
EmployedBy (Bunescu and Mooney , 2007).  
 The most commonly used technique in pat-
tern-based approaches is bootstrapping (Hearst, 
1992; Etzioni et al, 2005; Girju et al, 2003; Ra-
vichandran and Hovy, 2002; Pantel and Pennac-
chiotti, 2006). It utilizes a few man-crafted seed 
patterns to extract instances from corpora, then 
extracts new patterns using these instances, and 
continues the cycle to find new instances and 
new patterns. It is effective and scalable to large 
datasets; however, uncontrolled bootstrapping 
272
soon generates undesired instances once a noisy 
pattern brought into the cycle. 
 To aid bootstrapping, methods of pattern 
quality control are widely applied. Statistical 
measures, such as point-wise mutual information 
(Etzioni et al, 2005; Pantel and Pennacchiotti, 
2006) and conditional probability (Cimiano and 
Wenderoth, 2007),   have been shown to be ef-
fective to rank and select patterns and instances. 
Pattern quality control is also investigated by 
using WordNet (Girju et al, 2006), graph struc-
tures built among terms (Widdows and Dorow, 
2002; Kozareva et al, 2008), and pattern clusters 
(Davidov and Rappoport, 2008). 
Clustering-based approaches usually represent 
word contexts as vectors and cluster words based 
on similarities of the vectors (Brown et al, 1992; 
Lin, 1998). Besides contextual features, the vec-
tors can also be represented by verb-noun rela-
tions (Pereira et al, 1993), syntactic dependency 
(Pantel and Ravichandran, 2004; Snow et al, 
2005), co-occurrence (Yang and Callan, 2008), 
conjunction and appositive features (Caraballo, 
1999). More work is described in (Buitelaar et 
al., 2005; Cimiano and Volker, 2005). Cluster-
ing-based approaches allow discovery of rela-
tions which do not explicitly appear in text. Pan-
tel and Pennacchiotti (2006), however, pointed 
out that clustering-based approaches generally 
fail to produce coherent cluster for small corpora. 
In addition, clustering-based approaches had on-
ly applied to solve is-a and sibling relations. 
Many clustering-based approaches face the 
challenge of appropriately labeling non-leaf clus-
ters. The labeling amplifies the difficulty in crea-
tion and evaluation of taxonomies. Agglomera-
tive clustering (Brown et al, 1992; Caraballo, 
1999; Rosenfeld and Feldman, 2007; Yang and 
Callan, 2008) iteratively merges the most similar 
clusters into bigger clusters, which need to be 
labeled. Divisive clustering, such as CBC (Clus-
tering By Committee) which constructs cluster 
centroids by averaging the feature vectors of a 
subset of carefully chosen cluster members (Pan-
tel and Lin, 2002; Pantel and Ravichandran, 
2004), also need to label the parents of split clus-
ters. In this paper, we take an incremental clus-
tering approach, in which terms and relations are 
added into a taxonomy one at a time, and their 
parents are from the existing taxonomy. The ad-
vantage of the incremental approach is that it 
eliminates the trouble of inventing cluster labels 
and concentrates on placing terms in the correct 
positions in a taxonomy hierarchy.  
The work by Snow et al (2006) is the most 
similar to ours because they also took an incre-
mental approach to construct taxonomies. In their 
work, a taxonomy grows based on maximization 
of conditional probability of relations given evi-
dence; while in our work based on optimization 
of taxonomy structures and modeling of term 
abstractness. Moreover, our approach employs 
heterogeneous features from a wide range; while 
their approach only used syntactic dependency. 
We compare system performance between (Snow 
et al, 2006) and our framework in Section 5.  
3 The Features 
The features used in this work are indicators of 
semantic relations between terms. Given two in-
put terms yx cc , , a feature is defined as a func-
tion generating a single numeric score 
?),( yx cch ? or a vector of numeric scores 
?),( yx cch ?n. The features include contextual, 
co-occurrence, syntactic dependency, lexical-
syntactic patterns, and miscellaneous.  
The first set of features captures contextual in-
formation of terms. According to Distributional 
Hypothesis (Harris, 1954), words appearing in 
similar contexts tend to be similar. Therefore, 
word meanings can be inferred from and 
represented by contexts. Based on the hypothe-
sis, we develop the following features: (1) Glob-
al Context KL-Divergence: The global context of 
each input term is the search results collected 
through querying search engines against several 
corpora (Details in Section 5.1). It is built into a 
unigram language model without smoothing for 
each term. This feature function measures the 
Kullback-Leibler divergence (KL divergence) 
between the language models associated with the 
two inputs. (2) Local Context KL-Divergence: 
The local context is the collection of all the left 
two and the right two words surrounding an input 
term. Similarly, the local context is built into a 
unigram language model without smoothing for 
each term; the feature function outputs KL diver-
gence between the models. 
The second set of features is co-occurrence. In 
our work, co-occurrence is measured by point-
wise mutual information between two terms:  
)()(
),(
log),(
yx
yx
yx
cCountcCount
ccCount
ccpmi =  
where Count(.) is defined as the number of doc-
uments or sentences containing the term(s); or n 
as in ?Results 1-10 of about n for term? appear-
ing on the first page of Google search results for 
a term or the concatenation of a term pair. Based 
273
on different definitions of Count(.), we have (3) 
Document PMI, (4) Sentence PMI, and (5) 
Google PMI as the co-occurrence features. 
The third set of features employs syntactic de-
pendency analysis. We have (6) Minipar Syntac-
tic Distance to measure the average length of the 
shortest syntactic paths (in the first syntactic 
parse tree returned by Minipar1) between two 
terms in sentences containing them, (7) Modifier 
Overlap, (8) Object Overlap, (9) Subject Over-
lap, and (10) Verb Overlap to measure the num-
ber of overlaps between modifiers, objects, sub-
jects, and verbs, respectively, for the two terms 
in sentences containing them. We use Assert2 to 
label the semantic roles. 
The fourth set of features is lexical-syntactic 
patterns. We have (11) Hypernym Patterns based 
on patterns proposed by (Hearst, 1992) and 
(Snow et al, 2005), (12) Sibling Patterns which 
are basically conjunctions, and (13) Part-of Pat-
terns based on patterns proposed by (Girju et al, 
2003) and (Cimiano and Wenderoth, 2007). Ta-
ble 1 lists all patterns. Each feature function re-
turns a vector of scores for two input terms, one 
score per pattern. A score is 1 if two terms match 
a pattern in text, 0 otherwise. 
The last set of features is miscellaneous. We 
have (14) Word Length Difference to measure the 
length difference between two terms, and (15) 
Definition Overlap to measure the number of 
word overlaps between the term definitions ob-
tained by querying Google with ?define:term?. 
These heterogeneous features vary from sim-
ple statistics to complicated syntactic dependen-
cy features, basic word length to comprehensive 
Web-based contextual features. The flexible de-
sign of our learning framework allows us to use 
all of them, and even allows us to use different 
sets of them under different conditions, for in-
stance, different types of relations and different 
abstraction levels. We study the interaction be-
                                                 
1
 http://www.cs.ualberta.ca/lindek/minipar.htm. 
2
 http://cemantix.org/assert. 
tween features and relations and that between 
features and abstractness in Section 5. 
4 The Metric-based Framework 
This section presents the metric-based frame-
work which incrementally clusters terms to form 
taxonomies. By minimizing the changes of tax-
onomy structures and modeling term abstractness 
at each step, it finds the optimal position for each 
term in a taxonomy. We first introduce defini-
tions, terminologies and assumptions about tax-
onomies; then, we formulate automatic taxono-
my induction as a multi-criterion optimization 
and solve it by a greedy algorithm; lastly, we 
show how to estimate ontology metrics.      
4.1 Taxonomies, Ontology Metric, Assump-
tions, and Information Functions 
We define a taxonomy T as a data model that 
represents a set of terms C and a set of relations 
R between these terms. T can be written as 
T(C,R). Note that for the subtask of relation for-
mation, we assume that the term set C is given. A 
full taxonomy is a tree containing all the terms in 
C. A partial taxonomy is a tree containing only a 
subset of terms in C.  
In our framework, automatic taxonomy induc-
tion is the process to construct a full taxonomy T?
given a set of terms C and an initial partial tax-
onomy ),( 000 RST , where CS ?0 . Note that T0 is 
possibly empty. The process starts from the ini-
tial partial taxonomy T0 and randomly adds terms 
from C to T0 one by one, until a full taxonomy is 
formed, i.e., all terms in C are added. 
Ontology Metric 
We define an ontology metric as a distance 
measure between two terms (cx,cy) in a taxonomy 
T(C,R). Formally, it is a function ?? CCd : ?+, 
where C is the set of terms in T.  An ontology 
metric d on a taxonomy T with edge weights w 
for any term pair (cx,cy)?C is the sum of all edge 
weights along the shortest path between the pair: 
?
?
=
),(
,),(
,
)(),(
yxPe
yxyxwT
yx
ewccd
 
Hypernym Patterns Sibling Patterns 
NPx (,)?and/or other NPy NPx and/or NPy 
such NPy as NPx Part-of Patterns 
NPy (,)? such as NPx NPx of NPy 
NPy (,)? including NPx NPy?s NPx 
NPy (,)? especially NPx NPy has/had/have NPx 
NPy like NPx NPy is made (up)? of NPx 
NPy called NPx NPy comprises NPx 
NPx is a/an NPy NPy consists of NPx 
NPx , a/an NPy  
Table 1. Lexico-Syntactic Patterns. 
 
Figure 1. Illustration of Ontology Metric. 
274
where ),( yxP  is the set of edges defining the 
shortest path from term cx to cy . Figure 1 illu-
strates ontology metrics for a 5-node taxonomy. 
Section 4.3 presents the details of learning ontol-
ogy metrics. 
Information Functions 
The amount of information in a taxonomy T is 
measured and represented by an information 
function Info(T). An information function is de-
fined as the sum of the ontology metrics among a 
set of term pairs. The function can be defined 
over a taxonomy, or on a single level of a tax-
onomy. For a taxonomy T(C,R), we define its 
information function as: 
?
?<
=
Cycxcyx
yx ccdTInfo
,,
),()(   (1) 
Similarly, we define the information function 
for an abstraction level Li as:  
?
?<
=
iLycxcyx
yxii ccdLInfo
,,
),()(   (2) 
where Li is the subset of terms lying at the ith lev-
el of a taxonomy T. For example, in Figure 1, 
node 1 is at level L1, node 2 and node 5 level L2. 
Assumptions 
Given the above definitions about taxonomies, 
we make the following assumptions: 
Minimum Evolution Assumption. Inspired by 
the minimum evolution tree selection criterion 
widely used in phylogeny (Hendy and Penny, 
1985), we assume that a good taxonomy not only 
minimizes the overall semantic distance among 
the terms but also avoid dramatic changes. Con-
struction of a full taxonomy is proceeded by add-
ing terms one at a time, which yields a series of 
partial taxonomies. After adding each term, the 
current taxonomy Tn+1 from the previous tax-
onomy Tn is one that introduces the least changes 
between the information in the two taxonomies: 
),(minarg '
'
1 TTInfoT n
T
n ?=+  
where the information change function is 
|)()(| ),( baba TInfoTInfoTTInfo ?=? .  
Abstractness Assumption. In a taxonomy, con-
crete concepts usually lay at the bottom of the 
hierarchy while abstract concepts often occupy 
the intermediate and top levels. Concrete con-
cepts often represent physical entities, such as 
?basketball? and ?mercury pollution?. While ab-
stract concepts, such as ?science? and ?econo-
my?, do not have a physical form thus we must 
imagine their existence. This obvious difference 
suggests that there is a need to treat them diffe-
rently in taxonomy induction. Hence we assume 
that terms at the same abstraction level have 
common characteristics and share the same Info(.) 
function. We also assume that terms at different 
abstraction levels have different characteristics; 
hence they do not necessarily share the same  
Info(.) function. That is to say, ,concept  Tc ??
, leveln abstractio TLi ?  (.). uses ii InfocLc ??  
4.2 Problem Formulation 
The Minimum Evolution Objective 
Based on the minimum evolution assumption, we 
define the goal of taxonomy induction is to find 
the optimal full taxonomy T?  such that the infor-
mation changes are the least since the initial par-
tial taxonomy T0, i.e., to find:  
),(minarg? '0
'
TTInfoT
T
?=   (3) 
where 'T  is a full taxonomy, i.e., the set of terms 
in 'T  equals C. 
To find the optimal solution for Equation (3),  
T? , we need to find the optimal term set C? and 
the optimal relation set R? . Since the optimal term 
set for a full taxonomy is always C, the only un-
known part left is R? . Thus, Equation (3) can be 
transformed equivalently into:
 
)),(),,((minarg? 000''
'
RSTRCTInfoR
R
?=  
Note that in the framework, terms are added 
incrementally into a taxonomy. Each term inser-
tion yields a new partial taxonomy T. By the 
minimum evolution assumption, the optimal next 
partial taxonomy is one gives the least informa-
tion change. Therefore, the updating function for 
the set of relations 1+nR after a new term z is in-
serted can be calculated as: 
)),(),},{((minarg? '
'
nnn
R
RSTRzSTInfoR ??=
 
By plugging in the definition of the information 
change function (.,.)Info? in Section 4.1 and Equ-
ation (1), the updating function becomes: 
|),(),(|minarg?
,}{,'
??
???
?=
nSycxc
yx
znSycxc
yx
R
ccdccdR
 
The above updating function can be transformed 
into a minimization problem: 
yx
ccdccdu
ccdccdu
u
znSycxc
yx
nSycxc
yx
nSycxc
yx
znSycxc
yx
<
??
??
??
??
???
???
}{,,
,}{,
),(),(                 
),(),(    subject to
 
min
 
The minimization follows the minimum evolu-
tion assumption; hence we call it the minimum 
evolution objective. 
 
275
The Abstractness Objective 
The abstractness assumption suggests that term 
abstractness should be modeled explicitly by 
learning separate information functions for terms 
at different abstraction levels. We approximate 
an information function by a linear interpolation 
of some underlying feature functions. Each ab-
straction level Li is characterized by its own in-
formation function Infoi(.). The least square fit of 
Infoi(.) is: .|)(|min 2iTiii HWLInfo ?  
By plugging Equation (2) and minimizing over 
every abstraction level, we have: 
2
,,
,
)),(),((min yxji
j
ji
i iLycxc
yx cchwccd ?? ? ?
?
where jih , (.,.) is the jth underlying feature func-
tion for term pairs at level Li, jiw , is the weight 
for jih , (.,.). This minimization follows the ab-
stractness assumption; hence we call it the ab-
stractness objective. 
The Multi-Criterion Optimization Algorithm 
We propose that both minimum evolution and 
abstractness objectives need to be satisfied. To 
optimize multiple criteria, the Pareto optimality 
needs to be satisfied (Boyd and Vandenberghe, 
2004). We handle this by introducing   0,1 to 
control the contribution of each objective. The 
multi-criterion optimization function is: 
yx
cchwccdv
ccdccdu
ccdccdu
vu
yxji
j
ji
i Lcc
yx
zScc
yx
Scc
yx
Scc
yx
zScc
yx
iyx
n
yx
n
yx
n
yx
n
yx
<
?=
??
??
?+
?? ?
??
??
?
???
???
2)),(),((                              
),(),(                   
),(),(      subject to
)1(min
,,
,
}{,,
,}{,
??
The above optimization can be solved by a gree-
dy optimization algorithm. At each term insertion 
step, it produces a new partial taxonomy by add-
ing to the existing partial taxonomy a new term z, 
and a new set of relations R(z,.). z is attached to 
every nodes in the existing partial taxonomy; and 
the algorithm selects the optimal position indi-
cated by R(z,.), which minimizes the multi-
criterion objective function. The algorithm is: 
);,(
)};)1((min{arg
;
\
RST
vuRR
{z}SS
SCz
(z,.)R
Output 
            
            
 foreach
?? ?+??
??
?
   
The above algorithm presents a general incre-
mental clustering procedure to construct taxono-
mies. By minimizing the taxonomy structure 
changes and modeling term abstractness at each 
step, it finds the optimal position of each term in 
the taxonomy hierarchy. 
4.3 Estimating Ontology Metric 
Learning a good ontology metric is important for 
the multi-criterion optimization algorithm. In this 
work, the estimation and prediction of ontology 
metric are achieved by ridge regression (Hastie et 
al., 2001). In the training data, an ontology me-
tric d(cx,cy) for a term pair (cx,cy) is generated by 
assuming every edge weight as 1 and summing 
up all the edge weights along the shortest path 
from cx to cy. We assume that there are some un-
derlying feature functions which measure the 
semantic distance from term cx to cy. A weighted 
combination of these functions approximates the 
ontology metric for (cx,cy): 
?= ),(),( yxjjj cchwyxd  
where jw  is the jth weight for ),( yxj cch , the jth 
feature function. The feature functions are gener-
ated as mentioned in Section 3.  
5 Experiments  
5.1 Data 
The gold standards used in the evaluation are 
hypernym taxonomies extracted from WordNet 
and ODP (Open Directory Project), and me-
ronym taxonomies extracted from WordNet. In 
WordNet taxonomy extraction, we only use the 
word senses within a particular taxonomy to en-
sure no ambiguity. In ODP taxonomy extraction, 
we parse the topic lines, such as ?Topic 
r:id=`Top/Arts/Movies??, in the XML databases 
to obtain relations, such as is_a(movies, arts). In 
total, there are 100 hypernym taxonomies, 50 
each extracted from WordNet3 and ODP4, and 50 
meronym taxonomies from WordNet5. Table 2  
                                                 
3
 WordNet hypernym taxonomies are from 12 topics: ga-
thering, professional, people, building, place, milk, meal, 
water, beverage, alcohol, dish, and herb. 
4
 ODP hypernym taxonomies are from 16 topics: computers, 
robotics, intranet, mobile computing, database, operating 
system, linux, tex, software, computer science, data commu-
nication, algorithms, data formats, security multimedia, and 
artificial intelligence. 
5
 WordNet meronym taxonomies are from 15 topics: bed, 
car, building, lamp, earth, television, body, drama, theatre, 
water, airplane, piano, book, computer, and watch. 
Statistics WN/is-a ODP/is-a WN/part-of 
#taxonomies 50 50 50 
#terms 1,964 2,210 1,812 
Avg #terms 39 44 37 
Avg depth 6 6 5 
Table 2. Data Statistics. 
 
276
summarizes the data statistics. 
We also use two Web-based auxiliary datasets 
to generate features mentioned in Section 3: 
? Wikipedia corpus. The entire Wikipedia corpus 
is downloaded and indexed by Indri6. The top 
100 documents returned by Indri are the global 
context of a term when querying with the term.  
? Google corpus. A collection of the top 1000 
documents by querying Google using each 
term, and each term pair. Each top 1000 docu-
ments are the global context of a query term. 
Both corpora are split into sentences and are used 
to generate contextual, co-occurrence, syntactic 
dependency and lexico-syntactic pattern features.  
5.2 Methodology 
We evaluate the quality of automatic generated 
taxonomies by comparing them with the gold 
standards in terms of precision, recall and F1-
measure. F1-measure is calculated as 2*P*R/ 
(P+R), where P is precision, the percentage of 
correctly returned relations out of the total re-
turned relations, R is recall, the percentage of 
correctly returned relations out of the total rela-
tions in the gold standard. 
Leave-one-out cross validation is used to aver-
age the system performance across different 
training and test datasets. For each 50 datasets 
from WordNet hypernyms, WordNet meronyms 
or ODP hypernyms, we randomly pick 49 of 
them to generate training data, and test on the 
remaining dataset. We repeat the process for 50 
times, with different training and test sets at each 
                                                 
6
 http://www.lemurproject.org/indri/. 
time, and report the averaged precision, recall 
and F1-measure across all 50 runs. 
We also group the fifteen features in Section 3 
into six sets: contextual, co-concurrence, pat-
terns, syntactic dependency, word length differ-
ence and definition. Each set is turned on one by 
one for experiments in Section 5.4 and 5.5. 
5.3 Performance of Taxonomy Induction 
In this section, we compare the following auto-
matic taxonomy induction systems: HE, the sys-
tem by Hearst (1992) with 6 hypernym patterns; 
GI, the system by Girju et al (2003) with 3 me-
ronym patterns; PR, the probabilistic framework 
by Snow et al (2006); and ME, the metric-based 
framework proposed in this paper. To have a fair 
comparison, for PR, we estimate the conditional 
probability of a relation given the evidence 
P(Rij|Eij), as in (Snow et al 2006), by using the 
same set of features as in ME. 
Table 3 shows precision, recall, and F1-
measure of each system for WordNet hypernyms 
(is-a), WordNet meronyms (part-of) and ODP 
hypernyms (is-a). Bold font indicates the best 
performance in a column. Note that HE is not 
applicable to part-of, so is GI to is-a. 
Table 3 shows that systems using heterogene-
ous features (PR and ME) achieve higher F1-
measure than systems only using patterns (HE 
and GI) with a significant absolute gain of >30%. 
Generally speaking, pattern-based systems show 
higher precision and lower recall, while systems 
using heterogeneous features show lower preci-
sion and higher recall. However, when consider-
ing both precision and recall, using heterogene-
ous features is more effective than just using pat-
terns. The proposed system ME consistently pro-
duces the best F1-measure for all three tasks.  
The performance of the systems for ODP/is-a 
is worse than that for WordNet/is-a. This may be 
because there is more noise in ODP than in 
WordNet/is-a 
System Precision Recall F1-measure 
HE 0.85 0.32 0.46 
GI n/a n/a n/a 
PR 0.75 0.73 0.74 
ME 0.82 0.79 0.82 
ODP/is-a 
System Precision Recall F1-measure 
HE 0.31 0.29 0.30 
GI n/a n/a n/a 
PR 0.60 0.72 0.65 
ME 0.64 0.70 0.67 
WordNet/part-of 
System Precision Recall F1-measure 
HE n/a n/a n/a 
GI 0.75 0.25 0.38 
PR 0.68 0.52 0.59 
ME 0.69 0.55 0.61 
Table 3. System Performance. 
Feature  is-a sibling part-
of 
Benefited 
Relations  
Contextual 0.21 0.42 0.12 sibling 
Co-occur. 0.48 0.41 0.28 All 
Patterns 0.46 0.41 0.30 All 
Syntactic 0.22 0.36 0.12 sibling 
Word Leng. 0.16 0.16 0.15 All but 
limited 
Definition 0.12 0.18 0.10 Sibling but 
limited 
Best Features Co-
occur., 
patterns  
Contextual, 
co-occur., 
patterns 
Co-
occur., 
patterns 
 
Table 4. F1-measure for Features vs. Relations: WordNet. 
277
WordNet. For example, under artificial intelli-
gence, ODP has neural networks, natural lan-
guage and academic departments. Clearly, aca-
demic departments is not a hyponym of artificial 
intelligence. The noise in ODP interferes with 
the learning process, thus hurts the performance. 
5.4 Features vs. Relations 
This section studies the impact of different sets 
of features on different types of relations. Table 4 
shows F1-measure of using each set of features 
alone on taxonomy induction for WordNet is-a, 
sibling, and part-of relations. Bold font means a 
feature set gives a major contribution to the task 
of automatic taxonomy induction for a particular 
type of relation. 
Table 4 shows that different relations favor 
different sets of features.  Both co-occurrence 
and lexico-syntactic patterns work well for all 
three types of relations. It is interesting to see 
that simple co-occurrence statistics work as good 
as lexico-syntactic patterns. Contextual features 
work well for sibling relations, but not for is-a 
and part-of. Syntactic features also work well for 
sibling, but not for is-a and part-of. The similar 
behavior of contextual and syntactic features 
may be because that four out of five syntactic 
features (Modifier, Subject, Object, and Verb 
overlaps) are just surrounding context for a term. 
Comparing the is-a and part-of columns in 
Table 4 and the ME rows in Table 3, we notice a 
significant difference in F1-measure. It indicates 
that combination of heterogeneous features gives 
more rise to the system performance than a sin-
gle set of features does. 
5.5 Features vs. Abstractness 
This section studies the impact of different sets 
of features on terms at different abstraction le-
vels. In the experiments, F1-measure is evaluated 
for terms at each level of a taxonomy, not the 
whole taxonomy. Table 5 and 6 demonstrate F1-
measure of using each set of features alone on 
each abstraction levels. Columns 2-6 are indices 
of the levels in a taxonomy. The larger the indic-
es are, the lower the levels. Higher levels contain 
abstract terms, while lower levels contain con-
crete terms. L1 is ignored here since it only con-
tains a single term, the root. Bold font indicates 
good performance in a column. 
Both tables show that abstract terms and con-
crete terms favor different sets of features. In 
particular, contextual, co-occurrence, pattern, 
and syntactic features work well for terms at L4-
L6, i.e., concrete terms; co-occurrence works well 
for terms at L2-L3, i.e., abstract terms. This differ-
ence indicates that terms at different abstraction 
levels have different characteristics; it confirms 
our abstractness assumption in Section 4.1.  
We also observe that for abstract terms in 
WordNet, patterns work better than contextual 
features; while for abstract terms in ODP, the 
conclusion is the opposite. This may be because 
that WordNet has a richer vocabulary and a more 
rigid definition of hypernyms, and hence is-a 
relations in WordNet are recognized more effec-
tively by using lexico-syntactic patterns; while 
ODP contains more noise, and hence it favors 
features requiring less rigidity, such as the con-
textual features generated from the Web. 
6 Conclusions  
This paper presents a novel metric-based tax-
onomy induction framework combining the 
strengths of lexico-syntactic patterns and cluster-
ing. The framework incrementally clusters terms 
and transforms automatic taxonomy induction 
into a multi-criteria optimization based on mini-
mization of taxonomy structures and modeling of 
term abstractness. The experiments show that our 
framework is effective; it achieves higher F1-
measure than three state-of-the-art systems. The 
paper also studies which features are the best for 
different types of relations and for terms at dif-
ferent abstraction levels.  
Most prior work uses a single rule or feature 
function for automatic taxonomy induction at all 
levels of abstraction. Our work is a more general 
framework which allows a wider range of fea-
tures and different metric functions at different 
abstraction levels.  This more general framework 
has the potential to learn more complex taxono-
mies than previous approaches. 
Acknowledgements 
This research was supported by NSF grant IIS-
0704210. Any opinions, findings, conclusions, or 
recommendations expressed in this paper are of 
the authors, and do not necessarily reflect those 
of the sponsor. 
Feature  L2 L3 L4 L5 L6 
Contextual 0.29 0.31 0.35 0.36 0.36 
Co-occurrence 0.47 0.56 0.45 0.41 0.41 
Patterns 0.47 0.44 0.42 0.39 0.40 
Syntactic 0.31 0.28 0.36 0.38 0.39 
Word Length 0.16 0.16 0.16 0.16 0.16 
Definition 0.12 0.12 0.12 0.12 0.12 
Table 5. F1-measure for Features vs. Abstractness: 
WordNet/is-a. 
Feature  L2 L3 L4 L5 L6 
Contextual 0.30 0.30 0.33 0.29 0.29 
Co-occurrence 0.34 0.36 0.34 0.31 0.31 
Patterns 0.23 0.25 0.30 0.28 0.28 
Syntactic 0.18 0.18 0.23 0.27 0.27 
Word Length 0.15 0.15 0.15 0.14 0.14 
Definition 0.13 0.13 0.13 0.12 0.12 
Table 6. F1-measure for Features vs. Abstractness: 
ODP/is-a. 
278
References 
M. Berland and E. Charniak. 1999. Finding parts in very 
large corpora. ACL?99. 
S. Boyd and L. Vandenberghe. 2004. Convex optimization. 
In Cambridge University Press, 2004.  
P. Brown, V. D. Pietra, P. deSouza, J. Lai, and R. Mercer. 
1992. Class-based ngram models for natural language. 
Computational Linguistics, 18(4):468?479. 
P. Buitelaar, P. Cimiano, and B. Magnini. 2005. Ontology 
Learning from Text: Methods, Evaluation and Applica-
tions. Volume 123 Frontiers in Artificial Intelligence and 
Applications. 
R. Bunescu and R. Mooney. 2007.  Learning to Extract 
Relations from the Web using Minimal Supervision. 
ACL?07. 
S. Caraballo. 1999.  Automatic construction of a hypernym-
labeled noun hierarchy from text. ACL?99. 
T. Chklovski and P. Pantel. 2004. VerbOcean: mining the 
web for fine-grained semantic verb relations. EMNLP 
?04. 
P. Cimiano and J. Volker. 2005. Towards large-scale, open-
domain and ontology-based named entity classification. 
RANLP?07. 
P. Cimiano and J. Wenderoth. 2007. Automatic Acquisition 
of Ranked Qualia Structures from the Web. ACL?07. 
D. Davidov and A. Rappoport. 2006. Efficient Unsuper-
vised Discovery of Word Categories Using Symmetric 
Patterns and High Frequency Words. ACL?06. 
D. Davidov and A. Rappoport. 2008. Classification of Se-
mantic Relationships between Nominals Using Pattern  
Clusters. ACL?08. 
D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabil-
istic model of redundancy in information extraction. IJ-
CAI?05.  
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: an ex-
perimental study. Artificial Intelligence, 165(1):91?134. 
C. Fellbuam. 1998. WordNet: An Electronic Lexical Data-
base. MIT Press. 1998. 
M. Geffet and I. Dagan. 2005. The Distributional Inclusion 
Hypotheses and Lexical Entailment. ACL?05. 
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning 
Semantic Constraints for the Automatic Discovery of 
Part-Whole Relations. HLT?03. 
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic 
Discovery of Part-Whole Relations. Computational Lin-
guistics, 32(1): 83-135. 
Z. Harris. 1985. Distributional structure. In Word, 10(23): 
146-162s, 1954.  
T. Hastie, R. Tibshirani and J. Friedman. 2001. The Ele-
ments of Statistical Learning: Data Mining, Inference, 
and Prediction. Springer-Verlag, 2001. 
M. Hearst. 1992. Automatic acquisition of hyponyms from 
large text corpora. COLING?92. 
M. D. Hendy and D. Penny. 1982. Branch and bound algo-
rithms to determine minimal evolutionary trees. Mathe-
matical Biosciences 59: 277-290. 
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class 
Learning from the Web with Hyponym Pattern Linkage 
Graphs. ACL?08. 
D. Lin, 1998. Automatic retrieval and clustering of similar 
words. COLING?98. 
D. Lin, S. Zhao, L. Qin, and M. Zhou. 2003. Identifying 
Synonyms among Distributionally Similar Words. IJ-
CAI?03. 
G. S. Mann. 2002. Fine-Grained Proper Noun Ontologies 
for Question Answering. In Proceedings of SemaNet? 02: 
Building and Using Semantic Networks, Taipei. 
P. Pantel and D Lin. 2002. Discovering word senses from 
text. SIGKDD?02. 
P. Pantel and D. Ravichandran. 2004. Automatically labe-
ling semantic classes. HLT/NAACL?04.  
P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards 
terascale knowledge acquisition. COLING?04. 
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging 
Generic Patterns for Automatically Harvesting Semantic 
Relations. ACL?06. 
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of English words. ACL?93. 
D. Ravichandran and E. Hovy. 2002. Learning surface text 
patterns for a question answering system. ACL?02. 
E. Riloff and J. Shepherd. 1997. A corpus-based approach 
for building semantic lexicons. EMNLP?97. 
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lexicon 
construction. ACL/COLING?98. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntac-
tic patterns for automatic hypernym discovery. NIPS?05. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogeneous Evidence. 
ACL?06. 
B. Rosenfeld and R. Feldman. 2007. Clustering for unsu-
pervised relation identification. CIKM?07. 
P. Turney, M. Littman, J. Bigham, and V. Shnayder. 2003. 
Combining independent modules to solve multiple-
choice synonym and analogy problems. RANLP?03.  
S. M. Harabagiu, S. J. Maiorano and M. A. Pasca. 2003. 
Open-Domain Textual Question Answering Techniques. 
Natural Language Engineering 9 (3): 1-38, 2003. 
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. 
Scaling web-based acquisition of entailment relations. 
EMNLP?04.  
D. Widdows and B. Dorow. 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ?02. 
H. Yang and J. Callan. 2008. Learning the Distance Metric 
in a Personal Ontology. Workshop on Ontologies and In-
formation Systems for the Semantic Web of CIKM?08. 
279
