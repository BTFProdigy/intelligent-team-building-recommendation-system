Coling 2010: Poster Volume, pages 294?302,
Beijing, August 2010
A Novel Method for Bilingual Web Page Acquisition from 
Search Engine Web Records 
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin Yao, Qiaoming Zhu 
School of Computer Science & Technology, Soochow University 
{20094227002, hongy, 20074227065071, jyao, qmzhu}@suda.edu.cn 
Abstract
A new approach has been developed 
for acquiring bilingual web pages from 
the result pages of search engines, 
which is composed of two challenging 
tasks. The first task is to detect web 
records embedded in the result pages 
automatically via a clustering method 
of a sample page. Identifying these 
useful records through the clustering 
method allows the generation of highly 
effective features for the next task 
which is high-quality bilingual web 
page acquisition. The task of 
high-quality bilingual web page 
acquisition is a classification problem. 
One advantage of our approach is that it 
is search engine and domain 
independent. The test is based on 2516 
records extracted from six search 
engines automatically and annotated 
manually, which gets a high precision 
of 81.3% and a recall of 94.93%. The 
experimental results indicate that our 
approach is very effective. 
1 Introduction 
There have been extensive studies on parallel 
resource extraction from parallel monolingual 
web pages of some bilingual web sites (Chen 
and Nie, 2000; Resnik and Smith, 2003; Zhang 
et al, 2006; Shi et al, 2006). Candidate parallel 
web pages are acquired by making use of URL 
strings or HTML tags, then the translation 
equivalence of the candidate pairs are verified 
via content-based features.  
  However, we observe that bilingual 
resources may exist not only in two parallel 
monolingual web pages, but also in single 
bilingual web pages. For example, many news 
web pages and English learning pages are 
bilingual. Based on this observation, 
researchers have proposed methods to improve 
parallel sentences extraction within a bilingual 
web page. Jiang (2009) uses an adaptive 
pattern-based method to mine interesting 
bilingual data based on the observation that 
bilingual data usually appears collectively 
following similar patterns. Because the World 
Wide Web is composed of billions of pages, it 
is a challenging task to locate valuable 
bilingual pages. 
  To acquire bilingual web pages 
automatically, a novel and effective method is 
proposed in this paper by making use of search 
engines, such as Baidu (http://www.baidu.com). 
By submitting parallel sentence pairs to the 
given search engine, lots of result pages with 
web records are returned, most of which are 
linked to bilingual web pages. We first identify 
and extract all result records automatically by 
selecting and analyzing a sample page with a 
clustering method, and then select high-quality 
bilingual web pages from candidates with 
classification algorithms. 
Our method has the following advantages: 
  1. Former researchers extract parallel corpus 
from specific bilingual web sites. Since search 
engines index amounts of web pages, and we 
aim to acquire bilingual pages based on them, 
our method expands the corpus source greatly. 
  2. For one search engine, only one sample 
result page is used to generate the record 
wrapper. Then the wrapper is used to identify 
web records from other result pages of the same 
search engine. Compared with existing data 
record extraction technologies, such as MDR 
(Liu et al, 2003; Zhai and Liu, 2006), our 
method is more effective and efficient. 
294
  3. We model the issue of verification 
bilingual pages as a binary-class classification 
problem. The records acquired automatically 
and annotated manually are utilized to train and 
test the classifier. This work is domain and 
search engine independent. That is to say, the 
records acquired from any search engine in any 
domain are used indiscriminately as training 
and testing dataset. 
  The rest of the paper is organized as follows. 
Related works are introduced in section 2. 
Section 3 provides an overview of our solution. 
The work about bilingual page acquisition and 
verification is introduced in section 4 and 5. 
Section 6 presents the experiments and results. 
Finally section 7 concludes the paper. 
2 Related Work 
As far as we know, there is no publication 
available on acquiring bilingual web pages. 
Most existing studies, such as Nie (1999), 
Resnik and Smith (2003) and Shi (2006), mine 
parallel web documents within bilingual web 
sites first and then extract bilingual sentences 
from mined parallel documents using sentence 
alignment method. 
  In this paper, the candidate bilingual web 
pages are acquired by analyzing web records 
embedded in the search engines? result pages. 
Therefore, record extraction from result pages 
is a critical technique in our method. Many 
researches, such as Laender (2002), have been 
developed various solutions in web information 
extraction from kinds of perspectives. 
  Earlier web information extraction systems 
(Baumgartner et al, 2001; Liu et al, 2000; Zhai 
and Liu, 2005) require users to provide labeled 
data so that the extraction rules could be learned. 
Yet such semi-automatic methods are not 
scalable enough to the whole Web which 
changes at any time. That?s why more and more 
researchers focus on fully or nearly fully 
automatic solutions.  
  Structured data objects are normally database 
records retrieved from underlying web 
databases and displayed on the web pages with 
some fixed templates, so automatic extraction 
methods try to find such patterns and use them 
to extract more data. Several approaches have 
succeeded to address the problem automatically 
without human assistance. IEPAD (Chang and 
Lui, 2001) identifies sub-strings that appear 
many times in a document. By traversing the 
DOM tree of the Web page, MDR extracts the 
data-rich sub-tree indirectly by detecting the 
existence of multiple similar generalized-nodes. 
The key limitation is its greedy manner of 
identifying a data region. DEPTA (Zhai and Liu, 
2005) uses visual information (locations on the 
screen at which the tags are rendered) to infer 
the structural relationship among tags and to 
construct a tag tree. NET (Liu and Zhai, 2005) 
extracts flat or nested data records by 
post-order or pre-order traversal of the tag tree. 
ViNTs (Zhao et al, 2005) considers the web 
page as a tag tree, and utilizes both visual 
content features as well as tag tree structures. It 
assumes that data records are located in a 
minimum data-rich sub-tree and separated by 
separators of tag forests. Zhao (2006) explicitly 
aims at extracting all dynamic sections from 
web pages, and extracting records in each 
section, whereas ViNTs focuses on record 
extraction from a single section. Miao (2009) 
figures out how tag paths format the whole page. 
Compared with the previous method, it 
compares pairs of tag path occurrence patterns 
to estimate how likely these tag paths represent 
the same list of objects instead of comparing 
one pair of individual sub-trees in the record. It 
brings some noise. We follow this method and 
make appropriate improvement for our task. 
3 Basic Concepts and Overview 
3.1 Basic Concepts 
Some basic concepts are introduced below. 
Figure 1. An example of search engine return
295
Tag Path: The path of a tag consists of all 
nodes from the tree root <html> to itself. We 
use tag path to specify the location of the tag. 
The tag paths are classified into two types: text 
tag paths and non-text tag paths. 
  Data Record: When a page is considered as 
strings of tokens, data records are enwrapped 
by one or more tag paths, which compose the 
visually repeating pattern in a page. This paper 
aims to extract such structured data records that 
are produced by computer programs following 
some fixed templates, while whose contents are 
usually retrieved from backend databases. For 
example, there are four records in Figure 1. 
3.2 Method Overview 
We can get much more bilingual web pages by 
submitting parallel sentence pairs to the search 
engine than submitting monolingual queries. 
Based on this observation, our work is as 
shown in Figure 2. The algorithm consists of 
two steps: 1) Record wrapper generation. By 
submitting parallel sentence pairs to search 
engines, result pages containing lots of web 
records are returned. In order to generate record 
wrappers, we select and analyze a sample page 
and then apply clustering method to tag paths 
with similar patterns. We apply these wrappers 
to extract more records, which are linked to 
candidate bilingual web pages. 2) High-quality 
bilingual page acquisition. In order to acquire 
high-quality bilingual pages from candidates, a 
binary classifier is constructed to decide 
whether the candidate pages are bilingual or not. 
In order to improve the classifier, some useful 
resources are used, such as a dictionary and 
translation equivalents. 
However, a result page often contains some 
information irrelevant to the query, such as 
information related to the hosting site of the 
search engine, which increases the difficulty of 
record extraction. Besides, there are also many 
irrelevant records irrelevant to the query. So 
our focus is to acquire plenty of features to 
filter out the irrelevant pages from the 
candidates.
In this paper, the first result page is chosen as 
the sample page and Affinity Propagation (AP) 
clustering is used. The reason lies in Frey and 
Dueck (2007), which proves that to produce the 
groups of tag paths; the AP algorithm does not 
require the three restrictions: 1) the samples 
must be of a specific kind, 2) the similarity 
values must be in a specific range, and 3) the 
similarity matrix must be symmetric. In order 
to decide the type of a page, the Support 
Vector Machines (SVM) (Cortes and Vapnik, 
1995) classifier on Fuzzy C-means is 
constructed combining with word-overlap, 
length and frequency measures. SVM is 
well-fitted to treat such classification problems 
that involve interrelated features likes ours, 
while most probabilistic classifiers, such as 
Na?ve Bayes classifier, strongly assume feature 
independence (DuVerle and Prendinger, 2009). 
Figure 2. Overview of the method 
4 Bilingual Page Acquisition 
4.1 Result Page Extraction 
The result pages of a search engine consist of a 
ranked list of document summaries linked to the 
actual documents or web pages. A web 
document summary typically contains the title 
and URL of the web page, links to live and 
cached versions of the page and, most 
importantly, a short text summary, or a snippet, 
to convey the contents of the page. Such 
snippets embedded in result pages of search 
engines are query-dependent summaries. White 
(2001) finds the result pages are sensitive to the 
content and language of the query. If the query 
is monolingual, the returned search results are 
mostly monolingual, while the result pages are 
bilingual if the query is bilingual. In order to 
acquire more bilingual web pages, we submit 
parallel translation pairs. Figure 1 gives an 
example result page from Baidu, in which the 
snapshot consists of four records related to the 
query, which consists of ?I see.? and its 
translation ???????. The results have 
296
more effective advantages than submitting the 
query ?I see.? or ??????? respectively. 
4.2 Clustering With Path Similarity 
Given a web page, we get the occurrence 
positions of each tag path the same as the 
sequence in the preorder traversal of the page?s 
DOM tree. Certainly, there are many tag paths 
which appear several times in the whole page. 
So an inverted mapping from HTML tag paths 
to their positions is built easily. For example, 
there are 599 tag paths formatting the sample 
page in Figure 1, and after the inverted mapping, 
we acquire 86 unique tag paths in all. Only tick 
off one part of the results as shown in Table 1, 
where Pi represents the ith unique tag path, and 
the vector Si is defined to store the occurrence 
positions of Pi in the third column.  
  As introduced above, detecting visually 
repeating tag paths is a clustering problem. 
Above all, a factor in determining the clustering 
performance is the choice of similarity 
functions, which captures the likelihood that 
two data samples belong to the same cluster. In 
our case, the similarity scores between two tag 
paths aim to capture how their positions are 
close to each other and how they interleave 
each other. 
  With the purpose of characterizing how close 
two tag paths appear, we only acquire the 
distance between paths? average positions, 
which is easy to obtain by the acquired 
occurrence vectors. For example, the average 
position of P11 and P15 in Table 1 is 227 and 
215, so the distance between them is 12. 
L UniqueTag  Path (Pi)
Occurrences (Si) of Pi
1 \html 1 
3 \html\head\#text 3,4,7,8,9 
9 \html\body\table 
84,93,115,146,180, 
217,258,292,335,372,
406,437 
11 \html\body\table\tr
15,85,94,116,147,181,
218,259,293,336,373,
407,438 
14
\html\body\table\tr
\td\#text 
18,21,24,27,55,79,87,
91,97,111,113 
15
\html\body\table\tr
\td\a 
19,88,118,149,183, 
220,261,295,338,375,
409,440 
Table 1. Unique tag paths of the sample page 
  However, the most difficult problem is how 
to capture the interleaving characteristic 
between two tag paths. Before doing that, 
another vector Oi is produced. Oi(k) indicates 
whether the tag path Pi occurs in the position k 
or not by its value. In addition, the value is 
binary that 0 or 1, and 0 shows Pi doesn?t occur 
in the position k, while 1 shows the opposite. Of 
particular note, the length of each Oi is equal to 
the total number of HTML tags that formatting 
the whole web page. Take the tag path P3
(?\html\head\#text?) in Table 1 as an example, 
whose position vector O3 is (0, 0, 1, 1, 0, 0, 1, 1, 
1, 0? 0), and the vector?s length is 599, 
because there are totally 599 tag paths 
formatting the sample page in Figure 1. 
  Based on the position vectors, we capture 
how tag path Pi and Pj interleave each other by 
a segment 
ji
OOD /  of Oi divided by Oj. We 
aim to find such tag paths that divide each other 
in average. In other words if the variance of 
counts in the segment 
ji
OOD /  is stable, they 
are likely to be grouped in the same cluster. So, 
we define the interleaving measureP in terms 
of the variances of 
ji
OOD /  and ij OOD /  as: 
)}( ),(   max{),( // ijji OOOOji DVarDVarOO  P (1)
where
ji
OOD /  is acquired by Oj as follows: if 
value of Oj(k) is 1, Oi(k) is a separator to
segment itself into several regions. The value of 
every element in the segment is the count of Pi
that occurs in every region, which is the 
number of 1 in the region. 
Figure 3. An Example of tag paths 
  In addition, there may be many consecutive 
separators in Oi, and we integrate them into one. 
Besides, the segment is a non-empty set. So if 
there is no occurrence of Pi in one region, we 
297
will ignore this special region. Figure 3 shows 
three tag paths. P1 and P2 are likely to belong 
to the same cluster because of their regular 
occurrences, whereas the occurrences of P3 are 
comparatively irregular. By our method, 
31 / OO
D  = {1, 1, 1} and 
13 /OO
D = {1, 2, 1}. We 
integrate separators once and ignore an empty 
region in the process of getting
31 / OO
D .
  Both the score of the closeness measure and 
the interleaving measure for any two tag paths 
are non-negative real numbers. And a smaller 
value of either measure indicates a high 
frequency that the two tag paths appear 
regularly. The measure ),( ji PPV  defined 
below is inversely proportional of these two 
measures. 
HP
HV
u
 
),(),(
),(
jiji
ji OOSSc
PP (2)
where H  is a non-negative term that avoids 
dividing by 0 and normalizes the similarity 
value so that it falls into the range (0, 1]. In our 
experiment, we choose H = 10. By Equation 2, 
we calculate the similarity value of any pair of 
tag paths. As expected, the pairwise similarity 
matrix is fed into the AP clustering algorithm 
directly, and each cluster acquired from AP 
clustering contains n tag paths, which indicates 
that those n paths appear repeatedly together 
with high frequency, and the tag paths that have 
no remarkable relation are spilt into different 
clusters. For the given sample page in Figure 1, 
the number of identified clusters is 16.  
  We observe that HTML code of most data 
records contain more than three HTML tags, so 
we only examine the clusters containing four or 
more visual signals. In the clustering result of 
sample page in Figure 1, there are three 
clusters? sizes less than four. Meanwhile, we 
also note that: 
 1. The feature page of a common search 
engine usually contains 10 or more web records 
with similar layout pattern. So we define a 
threshold T=3. If an ancestor tag path doesn?t 
occur more than T times, we believe these tag 
path dose not lead a record.  
 2. Usually the content of the result pages 
returned by search engines is completely related 
to the queries, which means the data records 
that we are interested in are distributed in the 
whole page as main component. So the 
occurrence position of valuable tag paths must 
be global optimization. In this paper, the scope 
between beginning and ending occurrence must 
be wider than three quarters of the length of the 
web page. 
  Thus, we get essential clusters fit with above 
observations, which is denoted by C= {C1,
C2?CM}. Once we have the essential clusters, 
we apply them in new web page of the same 
search engine to identify data records. 
4.3 Data Record Extraction  
Based on the essential clusters, we extract the 
exact data records from the real content of text 
tag path that follow the ancestor tag path.  
  In order to describe the extraction process in 
details, we firstly define DaI as the child tag 
paths of an ancestor tag path Pa, and suppose 
that (Pos1? Posi? Posm) is the occurrence 
vector of Pa, which means at each position Posi
the tag path Pi occurs. Da(i) is such a tag path set 
that the position Pos of every path in it is Posi
<Pos<Posi+1. In the meantime, such path strings 
must begin with the same prefix of Pa. Such as 
in Table 2, Da(i) contains tag paths from Posi to 
Posi+1-1, and we obtain the ith records 
embedded in the result pages by acquiring the 
real content of all text tag paths in Da(i).
Occurrence 
of Pa
DaI of 
Pa
Child tag path 
Pos1 Pa:\html\body\table\tr 
Pos1+1 Pt:\html\body\table\tr\?
?? ?? 
Pos2-1 
Da(1)
Pk: ?? 
? ? ?
Posi Pa:\html\body\table\tr 
Posi+1 Pt:\html\body\table\tr\?
  ?? ?? 
Posi+1-1 
Da(i)
Pn: ?? 
? ? ?
Posm Pa:\html\body\table\tr 
  ?? 
Da(m)
 ?? 
Table 2. Collection of child tag paths for 
ancestor tag path
298
5 Bilingual Web Page Verification 
Based on the previous work, we capture a list of 
records based on a holistic analysis of a result 
page, and each record contains snippets and 
URLs related to the query. In this section, we 
aim to decide whether the candidate pages that 
returned records are linked to are bilingual or 
not by putting some statistical features 
(collected from snippets) into an effective SVM 
classification. 
  To the acquired snippets, some necessary 
preprocessing is made before we acquire 
useful features. We remove most of the noise 
that affect the precision and robustness of the 
entire system by such methods as recovery of 
abbreviation words, deletion of noisy words, 
amendment for half or full punctuations and 
simplified or traditional characters, and so on. 
  The snippet is described with more regular 
contents after preprocessing. We cut the 
snippet into several segments by its language. 
Each segment of the snippet is just represented 
in one language, which is either English or 
Chinese in this paper and different from its 
adjacent segments. So the source snippets are 
transferred into such language strings that 
consist of C and E, where C stands for Chinese 
and E stands for English. It is unlikely that 
continuous C or E exists in the same language 
string. We store the real text Tc (Te) that each C 
(E) stands for. We take the snippet ?I see. ??
???I quit! ?????? as example, its 
language string is ?ECEC? and real text string 
is TeTcTeTc, where the two Te stand for ?I see? 
and ?I quit?, the two Tc stand for ??????
and ??????.
  Note that different feature functions for the 
classifier will lead to different results, it is 
important to choose feature functions that will 
help to discriminate different classes. In this 
paper, the SVM classifier involves 
word-overlap, length and frequency features. 
We define these three features based on the 
snippet itself as follows: 
(1) Word-Overlap measure  
  Word overlap judges the similarity of 
Chinese term and English term. In this paper, 
we acquire the word-overlap score between any 
two adjacent language segments. The similarity 
Score(c_res,e_res) of Chinese term and English 
term is based on word-overlap as following: 
1 1
( ( , ))
( _ , _ )
p
i j
i j q
Max Sim c e
Score c res e res
I
 d d 
?
 (3) 
where the denominator is normalization factor, 
and in our experiment we select p+q as its value, 
where p stands for the length of Chinese term 
and q stands for the length of English term. In 
addition, ci stands for the ith word of Chinese 
term and ej stands for the jth word of English 
term. Sim(ci,ej) in Liu (2003) and Deng (2004) 
stands for the similarity of Chinese word ci and
English word ej.
  In our experiment, the Chinese and English 
sub-snippets are equivalent to Chinese and 
English sentences of the bilingual pages. In the 
segmented snippet, with regard to each 
sub-snippet T, which is at even position in the 
language string, we separately evaluate the 
intermediate score for snippet T with its left 
and right neighbors by Equation 3. Especially 
when T doesn?t have right or left neighbor, the 
score for T with its null neighbor is 0. So for 
every sub-snippet that needs to be scored the 
word-overlap score, there are two candidate 
scores with its adjacent neighbors. Then we 
choose the higher value as one item of an 
intermediate result vector. Either the length of 
the language string is 2 u n or 2 u n+1, the 
length of intermediate vector is n, and the final 
score is computed as follows: 
mn
InV
sScore
n
k
k
u
 
?
 1)( (4)
where Score(s) stands for the final score of 
snippet s on the word-overlap measure, and 
vector InV is the intermediate result vector as 
mentioned before. The length of the vector InV
is n, and m is the number of its items that is not 
equal to zero. m/n is used as a useful measure 
of length, because it indicates how many 
parallel pairs are there in the same snippet. 
(2) Length-Based measure  
  We acquire three scores about length 
measure. Take the language string ?ECECEC? 
as example, we use ?E1C1E2C2E3C3? to replace 
it for simple description. We acquire one score 
of the length measure as follows: 
)(
))()((
)( 1
sLen
eLencLen
sScore
m
i
?
 

 (5)
299
where s and m stand for the same as in Equation 
4. In addition, c and e stands for such 
sub-snippet that Score(c,e) contributes to 
?
 
n
k
kInV
1
. The function Len(s) is to compute the 
number of words in the sentence. 
We acquire the length of language string. If 
the length is too long or too short, the 
associated web page is unlikely to be a 
bilingual page. At the same time, we are not 
interested in some language strings although 
the lengths of them are appropriate. So we also 
store the variances of lengths about each 
sub-snippet. 
(3) Frequency-Based measure 
  According to the result pages, queries often 
occur in the title, snippet, or advertisements. 
They are highlighted to make them easier to 
identify. Hence we aim to acquire the 
frequency of the query in one whole snippet as 
a feature. 
  Based on the three measures above, a 
number of records (containing snippets and 
URLs) for training and testing can be converted 
them into a 6-dimensional feature space. In our 
experiments, nonlinear SVM with Gaussian 
Radial Basis Function (RBF) kernel is used. 
The performance of the SVM classifier 
indicates that it is a reliable way to verify 
whether the page is bilingual or not by the 
content of snippet. 
6 Experiments and Results 
6.1 The Data Set 
To acquire enough experimental data, we 
collect from Google, Baidu, Yahoo, Youdao, 
Bing and Tecent Soso, and the effectiveness of 
our algorithm is evaluated based on the data set 
from these six search engines. 
  Result records of search engines are 
collected by program and by human beings 
with submitting different queries respectively. 
They are used for checking the performance of 
record extraction. When evaluating the method 
of verification bilingual web pages, 2300 
records (60% are positive instances) are 
chosen for training the SVM classifier, and 
other 230 are selected randomly as test records 
from the whole record set. 
  The training data is annotated by human in 
two methods. The first method is motivated by 
the content of each source snippet. The 
annotators assign the type of web pages by 
scanning the text of every snippet. If the snippet 
contains many parallel term pairs, we annotate 
the page as bilingual or monolingual if not 
parallel. We also use another annotation 
method, which is to reach the URL by the 
Internet Explorer. By checking the content of 
the real web page, annotators decide the type of 
the candidate pages. And the biggest difference 
between the two public hand-classified dataset 
appears when some snippets of candidate 
pages have no clues in their content to predict 
classifications. 
6.2 Evaluation On Bilingual Page 
Acquisition
The entire system is evaluated by measuring 
the performance of the binary SVM classifier. 
And how the classifier performance changes 
with three features is shown in Table 3, where 
W, L and F separately stand for the 
word-overlap, length and frequency measures. 
  In order to improve the performance of 
word-overlap measure, we use not only the 
bilingual dictionary but also translation 
equivalents, which are extracted from parallel 
corpora. Because the bilingual dictionary 
doesn?t contain all necessary entries, the 
classifier with only word-overlap measure 
accepts many wrong pairs.  
Feature W W +L W +L+F
Precision 70.2% 81.02% 85.10% 
Table 3. SVM Classifier Performance changes 
with more features added to the classifier 
Table 3 shows that the length feature and the 
frequency feature have a significant effect on 
bilingual web page verification because of the 
natural relationship among queries, snippets 
and true web pages.  
#1 #2 
N
P(%) R(%) P(%) R(%)
1 85.1 92.3 75% 84.8 
2 80.7 95.1 72.8 85.7 
3 78.1 97.4 71.0 93.0 
aver 81.3 94.93 72.93 87.83
Table 4. Performance versus training data types 
300
  Three experiments of verification bilingual 
web pages based on two different training 
datasets are conducted whose results are 
shown in Table 4. #1 stands for the data set 
annotated by snippets, and #2 stands for the 
training data annotated by URLs. Precision and 
recall are used to evaluate our method. The 
average precision based on training dataset #2 
is 73%, which is lower than the precision of 
81.3% resulting from the dataset #1, because in 
many cases, some snippets are weakly related 
with real text in the real pages introduced by 
search engine summarization algorithm. From 
the table, we also see that the recalls in dataset 
#1 and #2 are both relatively high, which 
means our classifier can select high-quality 
bilingual pages with high accuracy. 
6.3 Evaluation On Web Record 
Extraction
Record extraction has significant effect on 
bilingual web page collection. A useful 
intermediate evaluation of the whole scheme is 
conducted by measuring the performance of 
record extraction.  
  We built a prototype system to test the 
algorithm of record extraction based on the 
clustering of similar records. On a laptop with a 
Pentium M 1.7G processor, the process of 
constructing records wrapper for a given search 
engine is done in 10 to 30 seconds. Once the 
wrapper is built, the record extraction from a 
new result page is done in a small fraction of a 
second.
  In order to test the robustness of the 
generated wrapper, we compare the records 
extracted by our method with the test records 
acquired manually. The precision and recall 
measures are used to evaluate the result. 98% 
of all the records are extracted by program, 
with a precision of 99%. The precision 
indicates that the generated wrappers in our 
experiment are quite robust to acquire records. 
The recall is lower than the precision, which 
indicates that it sometimes misses a few records. 
The reason for this is that in the extraction step, 
the records different from more common ones 
are eliminated.  
  We compare our performance with the work 
in Zhao (2006), which addresses the issue of 
differentiating dynamic sections and records 
based on the sample result pages. It generates 
section wrappers by identifying section 
boundary markers in nine steps. It is more 
complicated in computation than ours because 
it renders each result page and extracts its 
content lines by a traversal of the DOM tree, 
while we use tag structure of a page. The 
accordance is making full use of the sample 
pages for given search engines. The method 
also gets a high precision of 98.8% and a recall 
of 98.7%.  
7 Conclusion
The paper presents a novel method to acquire 
bilingual web pages automatically via search 
engines. In order to improve the efficiency and 
effectiveness, the snippets of search engines 
rather than the contents of the massive pages 
are analyzed to locate bilingual pages. 
Bilingual web page verification is modeled as 
a classification problem with word-overlap, 
length and frequency measures. Based on the 
similarity of HTML structures, AP clustering 
is used to extract web records from result 
pages of search engines. Experiments show 
that our algorithm has good performance in 
precision and recall. 
  As a valuable resource for up-to-date 
bilingual terms and sentences, bilingual web 
pages are counterpart to parallel monolingual 
web pages. Our method brings an efficient and 
effective solution to bilingual language 
engineering. 
References 
Adelberg B., NoDoSE. 1998. A tool for semi- 
  Automatically extracting structured and sem- 
  istructured data from text documents. In:
  Proc.ACM SIGMOD Conference on  man- 
  agement of Data, Seattle, WA (1998).
Baumgartner R., S. Flesca and G. Gottlob.2001. 
  Visual Web Information Extraction with 
  Lixto. Proceedings of the 27th International  
  Conference on Very Large Data Bases,
   pp.119-128, September 11-14, 2001  
Chang C., S. Lui. 2001. Information Extraction  
  based on Pattern Discovery. In Proceedings
  of the 10th international conference on 
  World Wide Web. pp.681-688, May 01-05, 
  2001, Hong Kong. 
Chen Jiang and Jian-Yun Nie. 2000. Web 
301
  Parallel text mining for Chinese-English 
  cross-language information retrieval. Proce-
  edings of RIAO2000 Content-Based Multi- 
  media Information Access, CID, Paris
Cortes, C. and V. Vapnik. 1995. Support-vector 
  network. Machine Learning 20, pp.273-297. 
Deng Dan. 2004. Research on Chinese-English 
  word alignment. Institute of Computing 
  Technology Chinese Academy of Sciences,
  Master Thesis. (in Chinese). 
DuVerle David, Helmut Prendinger. 2009. A  
  Novel Discourse Parser Based on Support 
  Vector Machine Classification. The 47th  
  Annual Meeting of the Association for 
  Computational Linguistics. pp. 665-673 
Frey B. J. and D. Dueck. 2007. Clustering by 
  passing messages between data points. 
Science, 315(5814):972-976. 
Laender A, B. Ribeiro-Neto, A. da Silva, J.  
  Teixeira. 2002. A Brief Survey of Web Data 
  Extraction Tools. ACM SIGMOD Record.
Volume 31, Number 2.
Liu B. and Y. Zhai. 2005. System for extracting 
  Web data from flat and nested data records.  
  In Proceedings of the Conference on Web  
  Information Systems Engineering,
  pp.487-495. 
Liu B., R. Grossman and Y. Zhai. 2003. Mining 
  Data Records in Web Pages. In Proceedings
  of the ninth ACM SIGKDD international  
  conference on Knowledge Discovery and  
  Data mining, Washington, D.C, pp.601-606. 
Liu Feifan, Jun Zhao, Bo Xu. 2003. Building 
  Large-Scale Domain Independent Chinese-  
  English Bilingual Corpus and the Researches 
  on Sentence Alignment. Joint Symposium on 
  Computational Linguistics.
Liu L., C. Pu and W. Han. 2000. An XML- 
  Enabled Wrapper Construction System for 
  Web Information Sources. Proceedings of  
  the 16th International Conference on Data  
  Engineering, pp.611. 
Long Jiang, Shiquan Yang, Ming Zhou, Xiao- 
  hua Liu and Qingsheng Zhou. 2009. Mining 
  Bilingual Data from the Web with Adaptive- 
  ly Learnt Patterns. The 47th Annual Meeting 
  of the Association for Computational Lingui- 
  stics. pp. 870-878 (2009) 
Miao Gengxin, Junichi Tatemura, Wang-Pin 
  Hsiung, Arsany Sawires, Louise E. Moser.  
  2009. Extracting data records from the web  
  using tag path clustering. In Proceedings of  
  the 18th International Conference on World 
  Wide Web, Spain, Madrid. 
Nie Jian-Yun, Michel Simard, Pierre Isabelle, 
Richard Durand 1999. Cross-Language 
Information Retrieval based on Parallel 
Texts and Automatic Mining of Parallel 
Texts in the Web. SIGIR-1999; 74-81. 
Resnik Philip and Noah A. Smith. 2003. The  
  web as a Parallel Corpus. Computational  
  Linguistics.
Shi Lei, Cheng Niu, Ming Zhou, and Jianfeng 
  Gao. 2006. A DOM Tree Alignment Model  
  for Mining Parallel Data from the Web. In  
Joint Proceedings of the Association for  
  Computational Linguistics and the Internati- 
  onal Conference on Computational Linguist- 
  ics, Sydney, Australia. 
White, R., Jose, J. & Ruthven, R. 2001.Query- 
  biased web page summarisation: a task- 
  oriented evaluation. In Proceedings of the 
  24th ACM SIGIR Conference on Research  
  and Development of Information Retrieval.
  New Orleans, Louisiana, United States, pp.  
  412-413. 
Zhai Y., B. Liu. 2005. Extracting Web Data  
  Using Instance-Based Learning. Web Infor- 
  mation Systems Engineering.
Zhai Y., B. Liu. 2005. Web Data Extraction 
  Based on Partial Tree Alignment. In 
Proceedings of the 14th international  
  conference on World Wide Web. May 10-14, 
  2005, Chiba, Japan.
Zhang Ying, Ke Wu, Jianfeng Gao, Phil Vines. 
  2006. Automatic Acquisition of Chinese- 
  English Parallel Corpus from the web. In 
Proceedings of 28th European Conference  
  on Information Retrieval.
Zhao H., W. Meng, Z. Wu, V. Raghavan, C.  
  Yu. 2006. Automatic Extraction of Dynamic 
  Record Sections From Search Engine Result 
  Pages. In Proceedings of the 32nd Internatio- 
nal conference on Very large databases.
302
Coling 2010: Poster Volume, pages 436?444,
Beijing, August 2010
 
ABSTRACT 
Re-ranking for Information Retrieval 
aims to elevate relevant feedbacks and 
depress negative ones in initial retrieval 
result list. Compared to relevance feed-
back-based re-ranking method widely 
adopted in the literature, this paper pro-
poses a new method to well use three 
features in known negative feedbacks to 
identify and depress unknown negative 
feedbacks. The features include: 1) the 
minor (lower-weighted) terms in negative 
feedbacks; 2) hierarchical distance (HD) 
among feedbacks in a hierarchical clus-
tering tree; 3) obstinateness strength of 
negative feedbacks. We evaluate the 
method on the TDT4 corpus, which is 
made up of news topics and their relevant 
stories. And experimental results show 
that our new scheme substantially out-
performs its counterparts. 
1. INTRODUCTION 
When we start out an information retrieval jour-
ney on a search engine, the first step is to enter a 
query in the search box. The query seems to be 
the most direct reflection of our information 
needs. However, it is short and often out of stan-
dardized syntax and terminology, resulting in a 
large number of negative feedbacks. Some re-
searches focus on exploring long-term query logs 
to acquire query intent. This may be helpful for 
obtaining information relevant to specific inter-
ests but not to daily real-time query intents. Es-
pecially it is extremely difficult to determine 
whether the interests and which of them should 
be involved into certain queries. Therefore, given 
a query, it is important to ?locally? ascertain its 
intent by using the real-time feedbacks. 
Intuitively it is feasible to expand the query 
using the most relevant feedbacks (Chum et al, 
2007). Unfortunately search engines just offer 
?farraginous? feedbacks (viz. pseudo-feedback) 
which may involve a great number of negative 
feedbacks. And these negative feedbacks never 
honestly lag behind relevant ones in the retrieval 
results, sometimes far ahead because of their 
great literal similarity to query. These noisy 
feedbacks often mislead the process of learning 
query intent.  
For so long, there had no effective approaches 
to confirm the relevance of feedbacks until the 
usage of the web click-through data (Joachims et 
al., 2003). Although the data are sometimes in-
credible due to different backgrounds and habits 
of searchers, they are still the most effective way 
to specify relevant feedbacks. This arouses re-
cent researches about learning to rank based on 
supervised or semi-supervised machine learning 
methods, where the click-through data, as the 
direct reflection of query intent, offer reliable 
training data to learning the ranking functions. 
Although the learning methods achieve sub-
stantial improvements in ranking, it can be found 
that lots of ?obstinate? negative feedbacks still 
permeate retrieval results. Thus an interesting 
question is why the relevant feedbacks are able 
to describe what we really need, but weakly repel 
what we do not need. This may attribute to the 
inherent characteristics of pseudo-feedback, i.e. 
their high literal similarity to queries. Thus no 
matter whether query expansion or learning to 
rank, they may fall in the predicament that ?fa-
voring? relevant feedbacks may result in ?favor-
ing? negative ones, and that ?hurting? negative 
feedbacks may result in ?hurting? relevant ones. 
However, there are indeed some subtle differ-
ences between relevant and negative feedbacks, 
e.g. the minor terms (viz. low-weighted terms in 
texts). Although these terms are often ignored in 
Negative Feedback: The Forsaken Nature Available for Re-ranking
Yu Hong, Qing-qing Cai, Song Hua, Jian-min Yao, Qiao-ming Zhu 
School of Computer Science and Technology, Soochow University 
jyao@suda.edu.cn 
436
relevance measurement because their little effect 
on mining relevant feedbacks that have the same 
topic or kernel, they are useful in distinguishing 
relevant feedbacks from negative ones. As a re-
sult, these minor terms provides an opportunity 
to differentiate the true query intent from its 
counterpart intents (called ?opposite intents? 
thereafter in this paper). And the ?opposite in-
tents? are adopted to depress negative feedbacks 
without ?hurting? the ranks of relevant feedbacks. 
In addition, hierarchical clustering tree is helpful 
to establish the natural similarity correlation 
among information. So this paper adopts the hi-
erarchical distance among feedbacks in the tree 
to enhance the ?opposite intents? based division 
of relevant and negative feedbacks. Finally, an 
obstinateness factor is also computed to deal 
with some obstinate negative feedbacks in the 
top list of retrieval result list. In fact, Teevan 
(Teevan et al, 2008) observed that most search-
ers tend to browse only a few feedbacks in the 
first one or two result pages. So our method fo-
cuses on improving the precision of highly 
ranked retrieval results.  
The rest of the paper is organized as follows. 
Section 2 reviews the related work. Section 3 
describes our new irrelevance feedback-based 
re-ranking scheme and the HD measure. Section 
4 introduces the experimental settings while Sec-
tion 5 reports experimental results. Finally, Sec-
tion 6 draws the conclusion and indicates future 
work. 
2. RELATED WORK 
Our work is motivated by information search 
behaviors, such as eye-tracking and click through 
(Joachims, 2003). Thereinto, the click-through 
behavior is most widely used for acquiring query 
intent. Up to  present, several interesting fea-
tures, such as click frequency and hit time on 
click graph (Craswell et al, 2007), have been 
extracted from click-through data to improve 
search results. However, although effective on 
query learning, they fail to avoid the thorny 
problem that even when the typed query and the 
click-through data are the same, their intents may 
not be the same for different searchers.  
A considerable number of studies have ex-
plored pseudo-feedback to learn query intent, 
thus refining page ranking. However, most of 
them focus on the relevant feedbacks. It is until 
recently that negative ones begin to receive some 
attention. Zhang (Zhang et al, 2009) utilize the 
irrelevance distribution to estimate the true rele-
vance model. Their work gives the evidence that 
negative feedbacks are useful in the ranking 
process. However, their work focuses on gener-
ating a better description of query intent to attract 
relevant information, but ignoring that negative 
feedbacks have the independent effect on repel-
ling their own kind. That is, if we have a king, 
we will not refuse a queen. In contrast, Wang 
(Wang et al, 2008) benefit from the independent 
effect from the negative feedbacks. Their method 
represents the opposite of query intent by using 
negative feedbacks and adopts that to discount 
the relevance of each pseudo-feedback to a query. 
However, their work just gives a hybrid repre-
sentation of opposite intent which may overlap 
much with the relevance model. Although an-
other work (Wang et al, 2007) of them filters 
query terms from the opposite intent, such filter-
ing makes little effect because of the sparsity of 
the query terms in pseudo-feedback. 
Other related work includes query expansion, 
term extraction and text clustering. In fact, query 
expansion techniques are often the chief benefi-
ciary of click-through data (Chum et al, 2007). 
However, the query expansion techniques via 
clicked feedbacks fail to effectively repel nega-
tive ones. This impels us to focus on un-clicked 
feedbacks. Cao (Cao et al, 2008) report the ef-
fectiveness of selecting good expansion terms for 
pseudo-feedback. Their work gives us a hint 
about the shortcomings of the one-sided usage of 
high-weighted terms. Lee (Lee et al, 2008) adopt 
a cluster-based re-sampling method to emphasize 
the core topic of a query. Their repeatedly feed-
ing process reveals the hierarchical relevance of 
pseudo-feedback. 
3. RE-RANKING SCHEME 
3.1 Re-ranking Scheme 
The re-ranking scheme, as shown in Figure 1, 
consists of three components: acquiring negative 
feedbacks, measuring irrelevance feedbacks and 
re-ranking pseudo-feedback. 
Given a query and its search engine results, we 
start off the re-ranking process after a trigger 
point. The point may occur at the time when 
searchers click on ?next page? or any hyperlink. 
437
All feedbacks before the point are assumed to 
have been seen by searchers. Thus the un-clicked 
feedbacks before the point will be treated as the 
known negative feedbacks because they attract 
no attention of searchers. This may be questioned 
because searchers often skip some hyperlinks 
that have the same contents as before, even if the 
links are relevant to their interests. However, 
such skip normally reflects the true searching 
intent because novel relevant feedbacks always 
have more attractions after all. 
 
Figure 1. Re-ranking scheme 
Another crucial step after the trigger point is to 
generate the opposite intent by using the known 
negative feedbacks. But now we temporarily 
leave the issue to Section 3.2 and assume that we 
have obtained a good representation of the oppo-
site intent, and meanwhile that of query intent 
has been composed of the highly weighted terms 
in the known relevant feedbacks and query terms. 
Thus, given an unseen pseudo-feedback, we can 
calculate its overall ranking score predisposed to 
the opposite intent as follows: 
          scoreIscoreOscoreR ___ ??= ?        (1) 
where the O_score is the relevance score to the 
opposite intent, I_score is that to the query intent 
and ?  is a weighting factor. On the basis, we 
re-rank the unseen feedbacks in ascending order. 
That is, the feedback with the largest score ap-
pears at the bottom of the ranked list. 
It is worthwhile to emphasize that although the 
overall ranking score, i.e. R_score, looks similar 
to Wang (Wang et al, 2008) who adopts the in-
versely discounted value (i.e. the relevance score 
is calculated as -scoreI _ scoreO _?? ) to re-rank 
feedbacks in descending order, they are actually 
quite different because our overall ranking score 
as shown in Equation (1) is designed to depress 
negative feedbacks, thereby achieving the similar 
effect to filtering. 
3.2 Representing Opposite Intent 
It is necessary for the representation of opposite 
intent to obey two basic rules: 1) the opposite 
intent should be much different from the query 
intent; and 2) it should reflect the independent 
effect of negative feedbacks. 
Given a query, it seems easy to represent its 
opposite intent by using a vector of 
high-weighted terms of negative feedbacks. 
However, the vector is actually a ?close relative? 
of query intent because the terms often have 
much overlap with that of relevant feedbacks. 
And the overlapping terms are exactly the source 
of the highly ranked negative feedbacks. Thus 
we should throw off the overlapping terms and 
focus on the rest instead.  
In this paper, we propose two simple facilities 
in representing opposite intent. One is a vector of 
the weighted terms (except query terms) occur-
ring in the known negative feedbacks, named as 
)( qO ? , while another further filters out the 
high-weighted terms occurring in the known 
relevant feedbacks, named as . Although )( rqO ??
)( qO ?  filters out query terms, the terms are so 
sparse that they contribute little to opposite intent 
learning. Thus, we will not explore  fur-
ther in this paper (Our preliminary experiments 
confirm our reasoning). In contrast,  not 
only differs from the representation of query in-
tent due to its exclusion of query terms but also 
emphasize the low-weighted terms occurring in 
negative feedbacks due to exclusion of 
high-weighted terms occurring in the known 
relevant feedbacks. 
)( qO ?
)( rqO ??
3.3 Employing Opposite Intent 
Another key issue in our re-ranking scheme is 
how to measure the relevance of all the feed-
backs to the opposite intent, i.e. O_score, thereby 
the ranking score R_score. For simplicity, we 
only consider Boolean measures in employing 
opposite intent to calculate the ranking score 
R_score. 
Assume that given a query, there are  
known relevant feedbacks and 
N
N  known nega-
tive ones. First, we adopt query expansion to ac-
quire the representation of query intent. This is 
done by pouring all terms of the  relevant 
feedbacks and query terms into a bag of words, 
where all the occurring weights of each term are 
N
438
accumulated, and extracting n top-weighted 
terms to represent the query intent as )( rqI ++ . 
Then, we use the N  negative feedbacks to rep-
resent the n-dimensional opposite intents 
. For any unseen pseudo-feedback u, we 
also represent it using an n-dimensional vector 
 which contains its n top-weighted terms. In 
all the representation processes, the TFIDF 
weighting is adopted. 
)( rqO ??
)(uV
Thus, for an unseen pseudo-feedback u, the 
relevance scores to the query intent and the op-
posite intent can be measured as: 
                   (2) 
}  )(  ),(  {)(_
}  )(  ),(  {)(_
rqOuVBuscoreO
rqIuVBuscoreI
??=
++=
where  indicates Boolean calculation: },{ ??B
                       (3) 
??
?
?
?=
?=?
Yxif
Yxif
Yxb
XxYxbYXB
i
i
i
ii
        ,0
         ,1
},{
  },,{},{
In particular, we simply set the factor ? , as 
mentioned in Equation (1), to 1 so as to balance 
the effect of query intent and its opposite intent 
on the overall ranking score. The intuition is that 
if an unseen pseudo-feedback has more overlap-
ping terms with )( rqO ??  than , it will 
has higher probability of being depressed as an 
negative feedback. 
)( rqI ++
Two alternatives to the above Boolean meas-
ure are to employ the widely-adopted VSM co-
sine measure and Kullback-Liebler (KL) diver-
gence (Thollard et al, 2000). However, such 
term-weighting alternatives will seriously elimi-
nate the effect of low-weighted terms, which is 
core of our negative feedback-based re-ranking 
scheme.  
3.4 Hierarchical Distance (HD) Measure  
The proposed method in Section 3.3 ignores 
two key issues. First, given a query, although 
search engine has thrown away most opposite 
intents, it is unavoidable that the 
pseudo-feedback still involves more than one 
opposite intent. However, the representation 
 has the difficulty in highlighting all the 
opposite intents because the feature fusion of the 
representation smoothes the independent charac-
teristics of each opposite intent. Second, given 
several opposite intents, they have different lev-
els of effects on the negative score . 
And the effects cannot be measured by the uni-
lateral score.  
)( rqO ??
)(_ uscoreO
 
Figure 2. Weighted distance calculation 
To solve the issues, we propose a hierarchical 
distance based negative measure, abbr. HD, 
which measures the distances among feedbacks 
in a hierarchical clustering tree, and involves 
them into hierarchical division of relevance score. 
Given two random leaves u and v in the tree, 
their HD score is calculated as: 
             
),(
),(
),(_
vuW
vurel
vuscoreHD =           (4) 
where ),( ??rel  indicates textual similarity, ),( ??W  
indicates the weighted distance in the tree, which 
is calculated as: 
                ?
?
=
mi
i vuwvuW ),(),(              (5) 
where m is the total number of the edges between 
two leaves,  indicates the weight of the 
i-th edge. In this paper, we adopt CLUTO to 
generate the hierarchical binary tree, and simply 
let each  equal 1. Thus the 
),( ??iw
),( ??iw ),( ??W  be-
comes to be the number of edges m, for example, 
the  equals 5 in Figure 2. ),( kjW
On the basis, given an unseen feedback u, we 
can acquire its modified re-ranking score 
scoreR _ ?  by following steps. First, we regard 
each known negative feedback as an opposite 
intent, following the two generative rules (men-
tioned in section 3.2) to generate its 
n-dimensional representation . Addition-
ally we represent both the known relevant feed-
backs and the unseen feedback u as 
n-dimensional term vectors. Second, we cluster 
these feedbacks to generate a hierarchical binary 
tree and calculate the HD score for each pair of 
)( rqO ??
),( ?u , where ?  denotes a leaf in the tree except u. 
Thus the modified ranking score is calculated as: 
? ?
? ?
?=?
Ni Nj
ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ (6) 
where iv  indicates the i-th known negative 
feedback in the leaves, N  is the total number of 
439
v , j  indicates the j-th known relevant feed-
back,  is the total number of 
v
N v . Besides, we 
still adopt Boolean value to measure the textual 
similarity  in both clustering process and 
ranking score calculation, thus the HD score in 
the formula (6) can be calculated as follows: 
),( ??rel
     
),(
)(_
),(_                   
),(
}  )(  ),(  {
),(_                
vuW
uscoreO
vuscoreHD
vuW
vVuVB
vuscoreHD
=
=
       (7) 
3.5 Obstinateness Factor 
Additionally we involve an interesting feature, 
i.e. the obstinate degree, into our re-ranking 
scheme. The degree is represented by the rank of 
negative feedbacks in the original retrieval re-
sults. That is, the more ?topping the list? an 
negative feedback is, the more obstinate it is.  
Therefore we propose a hypothesis that if a 
feedback is close to the obstinate feedback, it 
should be obstinate too. Thus given an unseen 
feedback u, its relevance to an opposite intent in 
HD can be modified as: 
          )(_)1()(_ uscoreO
rnk
uscoreO ?+=? ?        (8) 
where  indicates the rank of the opposite 
intent in original retrieval results (Note: in HD, 
every known negative feedback is an opposite 
intent), 
rnk
?  is a smoothing factor. Because as-
cending order is used in our re-ranking process, 
by the weighting coefficient, i.e. )/1( rnk?+ , the 
feedback close to the obstinate opposite intents 
will be further depressed. But the coefficient is 
not commonly used. In HD, we firstly ascertain 
the feedback closest to u, and if the feedback is 
known to be negative, set to maxv , we will use 
the Equation (8) to punish the pair of (u, maxv ) 
alone, otherwise without any punishment. 
4. EXPERIMENTAL SETTING 
4.1 Data Set 
We evaluate our methods with two TDT collec-
tions: TDT 2002 and TDT 2003. There are 3,085 
stories in the TDT 2002 collection are manually 
labeled as relevant to 40 news topics, 30,736 
ones irrelevant to any of the topics. And 3,083 
news stories in the TDT 2003 collection are la-
beled as relevant to another 40 news topics, 
15833 ones irrelevant to them. In our evaluation, 
we adopt TDT 2002 as training set, and TDT 
2003 as test set. Besides, only English stories are 
used, both Mandarin and Arabic ones are re-
placed by their machine-translated versions (i.e. 
mttkn2 released by LDC). 
Corpus good fair poor 
TDT 2002 26 7 7 
TDT 2003 22 10 8 
Table 1. Number of queries referring to different 
types of feedbacks (Search engine: Lucene 2.3.2) 
In our experiments, we realize a simple search 
engine based on Lucene 2.3.2 which applies 
document length to relevance measure on the 
basis of traditional literal term matching. To 
emulate the real retrieval process, we extract the 
title from the interpretation of news topic and 
regard it as a query, and then we run the search 
engine on the TDT sets and acquire the first 1000 
pseudo-feedback for each query. All feedbacks 
will be used as the input of our re-ranking proc-
ess, where the hand-crafted relevant stories de-
fault to the clicked feedbacks. By the search en-
gine, we mainly obtain three types of 
pseudo-feedback: ?good?, ?fair? and ?poor?, 
where ?good? denotes that more than 5 clicked 
(viz. relevant) feedbacks are in the top 10, ?fair? 
denotes more than 2 but less than 5, ?poor? de-
notes less than 2. Table 1 shows the number of 
queries referring to different types of feedbacks. 
4.2 Evaluation Measure 
We use three evaluation measures in experiments, 
P@n, NDCG@n and MAP. Thereinto, P@n de-
notes the precision of top n feedbacks. On the 
basis, NDCG takes into account the influence of 
position to precision. NDCG at position n is cal-
culated as: 
      
n
n
i
ur
n Z
iNDCG
Z
nNDCG
i?= +?=?= 1
)(
)1log(
12
@
1
@    (9) 
where i is the position in the result list, Zn is a 
normalizing factor and chosen so that for the 
perfect list DCG at each position equals one, and 
r(ui) equals 1 when ui is relevant feedback, else 0. 
While MAP additionally takes into account recall, 
calculated as:  
        ? ?= = ?= mi kj ijii jpurRmMAP 1 1 ))@()((11    (10) 
where m is the total number of queries, so MAP 
gives the average measure of precision and recall 
440
for multiple queries, Ri is the total number of 
feedbacks relevant to query i, and k is the num-
ber of pseudo-feedback to the query. Here k is 
indicated to be 1000, thus Map can give the av-
erage measure for all positions of result list. 
4.3 Systems 
We conduct experiments using four main sys-
tems, in which the search engine based on Lu-
cene 2.3.2, regarded as the basic retrieval system, 
provides the pseudo-feedback for the following 
three re-ranking systems. 
Exp-sys: Query is expanded by the first N known 
relevant feedbacks and represented by an 
n-dimensional vector which consists of n distinct 
terms. The standard TFIDF-weighted cosine 
metric is used to measure the relevance of the 
unseen pseudo-feedback to query. And the rele-
vance-based descending order is in use. 
Wng-sys: A system realizes the work of Wang 
(Wang et al, 2008), where the known relevant 
feedbacks are used to represent query intent, and 
the negative feedbacks are used to generate op-
posite intent. Thus, the relevance score of a feed-
back is calculated as I_scorewng- O_score?w? wng, 
and the relevance-based descending order is used 
in re-ranking. 
Our-sys: A system is approximately similar to 
Wng-sys except that the relevance is measured by 
O_scoreour- ?? I_scoreour and the pseudo-feedback 
is re-ranked in ascending order.  
Additionally both Wng-sys and Our-sys have 
three versions. We show them in Table 2, where 
?I? corresponds to the generation rule of query 
intent, ?O? to that of opposite intent, Rel. means 
relevance measure, u is an unseen feedback, v is 
a known relevant feedback, v  is a known nega-
tive feedback. 
5. RESULTS 
5.1 Main Training Result 
We evaluate the systems mainly in two circum-
stances: when both  and N N  equal 1 and 
when they equal 5. In the first case, we assume 
that retrieval capability is measured under given 
few known feedbacks; in the second, we emulate 
the first page turning after several feedbacks 
have been clicked by searchers. Besides, the ap-
proximately optimal value of n for the Exp-sys, 
which is trained to be 50, is adopted as the global 
value for all other systems. The training results 
are shown in Figure 3, where the Exp-sys never 
gains much performance improvement when n is 
greater than 50. In fairness to effects of ?I? and 
?O? on relevance measure, we also make n  
equal 50. In addition, all the discount factors 
(viz.? , ? w2 and ? w3) initially equal 1, and the 
smoothing factor ?  is trained to be 0.5. 
Table 2. All versions of both Wngs and Ours 
 
Figure 3. Parameter training of Exp-sys 
For each query we re-rank all the 
pseudo-feedback, including that defined as 
known, so P@20 and NDCG@20 are in use to 
avoid over-fitting (such as P@10 and 
NDCG@10 given both  and N N  equal 5 ). 
We show the main training results in Table 3, 
where our methods achieve much better per-
formances than the re-ranking methods based on 
relevant feedback learning when N= N =5. 
Thereinto, our basic system, i.e. Our-sys1, at 
least achieves approximate 5% improvement on 
P@20, 3% on NDCG@20 and 1% on MAP than 
the optimal wng-sys (viz. wng-sys1). And obvi-
?I? n-dimensional vector for each v, Number of v in use is N
?O? None 
Wng-sys1
Rel. NvuscoreR
N
i
w /)),cos((_
1
1 ?==  
?I?
Number of v in use is N, all v combine into a n-dimensional 
bag of words bw2
?O?
Number of v  in use is N , all v combine into a 
n-dimensional words bag 2wb  
Wng-sys2
Rel. ),cos(),cos(_ 2222 wwww bubuscoreR ??= ?  
?I?
?O?
Similar generation rules to Wng-sys2 except that query 
terms are removed from bag of words  and 3wb 3wb  Wng-sys3
Rel. ),cos(),cos(_ 3333 wwww bubuscoreR ??= ?  
?I? )( rqI ++  in section 3.3 
?O? )( rqO ??  in section 3.2 Our-sys1
Rel. scoreIscoreOscoreR ___ ??= ?  
?I?
?O?
The same generation rules to Our-sys1 
Our-sys2
Rel.
HD algorithm: ? ?
? ?
?=?
Ni Nj
ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ 
?I?
?O?
The same generation rules to Our-sys1 
Our-sys3
Rel.
HD algorithm + obstinateness factor: 
)(_)1()(_ uscoreO
rnk
uscoreO ?+=? ?  
441
ously the most substantial improvements are 
contributed by the HD measure which even in-
creases the P@20 of Our-sys1 by 8.5%, 
NDCG@20 by 13% and MAP by 9%. But it is 
slightly disappointing that the obstinateness fac-
tor only has little effectiveness on performance 
improvement, although Our-sys3 nearly wins 
the best retrieval results. This may stem from 
?soft? punishment on obstinateness, that is, for 
an unseen feedback, only the obstinate com-
panion closest to the feedback is punished in 
relevance measure. 
Table 3. Main training results 
It is undeniable that all the re-ranking systems 
work worse than the basic search engine when 
the known feedbacks are rare, such as =N N =1. 
This motivates an additional test on the higher 
values of both  and N N ( =N N =9), as shown 
in Table 4. Thus it can be found that most of the 
re-ranking systems achieve much better per-
formance than the basic search engine. An im-
portant reason for this is that more key terms can 
be involved into representations of both query 
intent and its opposite intent. So it seems that 
more manual intervention is always reliable. 
However in practice, seldom searchers are will-
ing to use an unresponsive search engine that can 
only offer relatively satisfactory feedbacks after 
lots of click-through and page turning. And in 
fact at least two pages (if one page includes 10 
pseudo-feedback) need to be turned in the train-
ing corpus when both  and N N  equal 9. So 
we just regard the improvements benefiting from 
high click-through rate as an ideal status, and 
still adopt the practical numerical value of  
and 
N
N , i.e. =N N =5, to run following test. 
5.2 Constraint from Query 
A surprising result is that Exp-sys always 
achieves the worst MAP value, even worse than 
the basic search engine even if high value of N is 
in use, such as the performance when N equal 9 
in Table 4. It seems to be difficult to question the 
reasonability of the system because it always 
selects the most key terms to represent query in-
tent by query expansion. But an obvious differ-
ence between Exp-sys and other re-ranking sys-
tems could explain the result. That is the query 
terms consistently involved in query representa-
tion by Exp-sys. 
Table 4. Effects of  and N N  on re-ranking 
performance (when =N N =9, n= n =50) 
In fact, Wng-sys1 never overly favor the query 
terms because they are not always the main body 
of an independent feedback, and our systems 
even remove the query terms from the opposite 
intent directly. Conversely Exp-sys continuously 
enhances the weights of query terms which result 
in over-fitting and bias. The visible evidence for 
this is shown in Figure 4, where Exp-sys 
achieves better Precision and NDCG than the 
basic search engine at the top of result list but 
worse at the subsequent parts. The results illus-
trate that too much emphasis placed on query 
terms in query expansion is only of benefit to 
elevating the originally high-ranked relevant 
feedback but powerless to pull the straggler out 
of the bottom of result list.  
 
Figure 4. MAP comparison (basic vs Exp) 
5.3 Positive Discount Loss 
Obviously Wang (Wang et al, 2008) has noticed 
the negative effects of query terms on re-ranking. 
Therefore his work (reproduced by Wng-sys1, 2, 
3 in this paper) avoids arbitrarily enhancing the 
terms in query representation, even removes 
them as Wng-sys3. This indeed contributes to the 
- Our-sys1 Our-sys2 Exp-sys Wng-sys1 Basic 
P@20 0.6603 0.8141 0.63125 0.7051 0.6588
NDCG@20 0.7614 0.8587 0.8080 0.7797 0.6944
MAP 0.6583 0.7928 0.5955 0.7010 0.6440
systems N = N P@20 NDCG@20 MAP Factor 
Basic - 0.6588 0.6944 0.6440 - 
1 0.4388 0.4887 0.3683 - Exp-sys 
5 0.5613 0.6365 0.5259 - 
1 0.5653 0.6184 0.5253 - Wng-sys1
5 0.6564 0.7361 0.6506 - 
1 0.5436 0.6473 0.4970 2w? =1Wng-sys2
5 0.5910 0.7214 0.5642 2w? =1
1 0.5436 0.6162 0.4970 3w? =1Wng-sys3
5 0.5910 0.6720 0.5642 3w? =1
1 0.5628 0.6358 0.4812 ? =1 Our-sys1
5 0.7031 0.7640 0.6603 ? =1 
1 0.6474 0.6761 0.5967 ? =1 Our-sys2
5 0.7885 0.8381 0.7499 ? =1 
1 0.6026 0.6749 0.5272 ? =0.5Our-sys3
5 0.7897 0.8388 0.7464 ? =0.5
442
improvement of the re-ranking system, such as 
the better performances of Wng-sys1, 2, 3 shown 
in Table 3, although Wng-sys3 has no further 
improvement than Wng-sys2 because of the spar-
sity of query terms. On the basis, the work re-
gards the terms in negative feedbacks as noises 
and reduces their effects on relevance measure as 
much as possible. This should be a reasonable 
scheme, but interestingly it does not work well in 
our experiments. For example, although 
Wng-sys2 and Wng-sys3 eliminate the relevance 
score calculated by using the terms in negative 
feedbacks, they perform worse than Wng-sys1 
which never make any discount. 
systems ?? =0.5 ?? =1 ?? =2 
Our-sys1 0.4751 0.6603 0.6901 
Wng-sys2 0.6030 0.5642 0.4739 
Wng-sys3 0.6084 0.5642 0.4739 
Table 5. Effects on MAP  
 Additionally when we increase the discount 
factor 2w?  and 3w? , as shown in Table 5, the 
performances (MAP) of Wng-sys2 and Wng-sys3 
further decrease. This illustrates that the 
high-weighted terms of high-ranked negative 
feedbacks are actually not noises. Otherwise why 
do the feedbacks have high textual similarity to 
query and even to their neighbor relevant feed-
backs? Thus it actually hurts real relevance to 
discount the effect of the terms. 
Conversely Our-sys1 can achieve further im-
provement when the discount factor ?  in-
creases, as shown in Table 5. It is because the 
discount contributes to highlighting minor terms 
of negative feedbacks, and these terms always 
have little overlap with the kernel of relevant 
feedbacks. Additionally the minor terms are used 
to generate the main body of opposite intent in 
our systems, thus the discount can effectively 
separate opposite intent from positive query rep-
resentation. Thereby we can use relatively pure 
representation of opposite intent to detect and 
repel subsequent negative feedbacks. 
5.4 Availability of Minor Terms 
Intuitively we can involve more terms into query 
representation to alleviate the positive discount 
loss. But it does not work in practice. For exam-
ple, Wng-sys2 shown in Figure 5 has no obvious 
improvement no matter how many terms are in-
cluded in query representation. Conversely 
Our-sys1 can achieve much more improvement 
when it involves more terms into the opposite 
intent. For example, when the number of terms 
increases to 150, Our-sys1 has approximately 5% 
better MAP than Wng-sys2, shown in Figure 5. 
 
Figure 5. Effects on MAP in modifying the di-
mensionality n (when N= N =5, ? =1) 
 This result illustrates that minor terms are 
available for repelling negative feedbacks, but 
too weak to recall relevant feedbacks. In fact, the 
minor terms are just the low-weighted terms in 
text. Current text representation techniques often 
ignore them because of their marginality. How-
ever minor terms can reflect fine distinctions 
among feedbacks, even if they have the same 
topic. And the distinctions are of great impor-
tance when we determine why searchers say 
?Yes? to some feedbacks but ?No? to others. 
Table 6. Main test results 
5.5 Test Result 
We run all systems on test corpus, i.e. TDT2003, 
but only report four main systems: Wng-sys1, 
Our-sys1, Our-sys2 and Our-sys3. Other systems 
are omitted because of their poor performances. 
The test results are shown in Table 6 which in-
cludes not only global performances for all test 
queries but also local ones on three distinct types 
of queries, i.e. ?good?, ?fair? and ?poor?. There-
into, Our-sys2 achieves the best performance 
around all types of queries. So it is believable 
systems metric good fair poor global Factor
P@20 0.7682 0.5450 0.2643 0.6205
NDCG@20 0.8260 0.6437 0.4073 0.7041Wng-sys1
MAP 0.6634 0.4541 0.9549 0.6620
- 
P@20 0.8273 0.5700 0.2643 0.6603
NDCG@20 0.8679 0.6620 0.4017 0.7314Our-sys1
MAP 0.6740 0.4573 0.9184 0.6623
? =2,
? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8199 0.4180 0.7894Our-sys2
MAP 0.7148 0.6313 0.9897 0.7427
? =2,
? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8200 0.4180 0.7894Our-sys3
MAP 0.7145 0.6292 0.9897 0.7420
? =2,
? =0.5
443
that hierarchical distance of clustering tree al-
ways plays an active role in distinguishing nega-
tive feedbacks from relevant ones. But it is sur-
prising that Our-sys3 achieves little worse per-
formance than Our-sys2. This illustrates poor 
robustness of obstinateness factor. 
Interestingly, the four systems all achieve very 
high MAP scores but low P@20 and NDCG@20 
for ?poor? queries. This is because the queries 
have inherently sparse relevant feedbacks: less 
than 6? averagely. Thus the highest p@20 is 
only approximate 0.3, i.e. 6/20. And the low 
NDCG@20 is in the same way. Besides, all 
MAP scores for ?fair? queries are the worst. We 
find that this type of query involves more mac-
roscopic features which results in more kernels 
of negative feedbacks. Although we can solve 
the issue by increasing the dimensionality of op-
posite intent, it undoubtedly impairs the effi-
ciency of re-ranking.  
6. CONCLUSION 
This paper proposes a new re-ranking scheme 
to well explore the opposite intent. In particular, 
a hierarchical distance-based (HD) measure is 
proposed to differentiate the opposite intent from 
the true query intent so as to repel negative 
feedbacks. Experiments show substantial out-
performance of our methods. 
Although our scheme has been proven effec-
tive in most cases, it fails on macroscopic queries. 
In fact, the key difficulty of this issue lies in how 
to ascertain the focal query intent given various 
kernels in pseudo-feedback. Fortunately, 
click-through data provide some useful informa-
tion for learning real query intent. Although it 
seems feasible to generate focal intent represen-
tation by using overlapping terms in clicked 
feedbacks, such representation is just a reproduc-
tion of macroscopic query since the overlapping 
terms can only reflect common topic instead of 
focal intent. Therefore, it is important to segment 
clicked feedbacks into different blocks, and as-
certain the block of greatest interest to searchers.  
References 
Allan, J., Lavrenko, V., and Nallapati, R. 2002. 
UMass at TDT 2002, Topic Detection and 
Tracking: Workshop. 
Craswell, N., and Szummer, M. Random walks on 
the click graph. 2007. In Proceedings of the 
Conference on Research and Development in 
Information Retrieval. SIGIR '30. ACM Press, 
New York, NY, 239-246. 
Cao, G. H., Nie, J. Y., and Gao, J. F. 2008. Stephen 
Robertson. Selecting Good Expansion Terms for 
Pseudo-Relevance Feedback. In Proceedings of 
the Conference on Research and Development in 
Information Retrieval. SIGIR '31. ACM Press, 
New York, NY, 243-250. 
Chum, O., Philbin, J., Sivic, J., and Zisserman, A. 
2007. Automatic query expansion with a genera-
tive feature model for object retrieval. In Pro-
ceedings of the 11th International Conference on 
Computer Vision, Rio de Janeiro, Brazil, 1?8. 
Joachims, T., Granka, L., and Pan, B. 2003. Accu-
rately Interpreting Clickthrough Data as Implicit 
Feedback. In Proceedings of the Conference on 
Research and Development in Information Re-
trieval. SIGIR '28. New York, NY, 154-161. 
Lee, K. S., Croft, W. B., and Allan, J. 2008 A Clus-
ter-Based Resampling Method for 
Pseudo-Relevance Feedback. In Proceedings of 
the Conference on Research and Development in 
Information Retrieval. SIGIR '31. ACM Press, 
New York, NY, 235-242. 
Thollard, F., Dupont, P., and Higuera, L.2000. 
Probabilistic DFA Inference Using Kull-
back-Leibler Divergence and Minimality. In 
Proceedings of the 17th Int'l Conf on Machine 
Learning. San Francisco: Morgan Kaufmann, 
975-982. 
Teevan, J. T., Dumais, S. T., and Liebling, D. J. 
2008. To Personalize or Not to Personalize: 
Modeling Queries with Variation in User Intent. 
In Proceedings of the Conference on Research 
and Development in Information Retrieval. 
SIGIR '31. New York, NY, 163-170. 
Wang, X. H., Fang, H., and Zhai, C. X. 2008. A 
Study of Methods for Negative Relevance Feed-
back. In Proceedings of the Conference on Re-
search and Development in Information Re-
trieval. SIGIR '31. ACM Press, New York, NY, 
219-226. 
Wang, X. H., Fang, H., and Zhai, C. X. 2007. Im-
prove retrieval accuracy for difficult queries us-
ing negative feedback. In Proceedings of the 
sixteenth ACM conference on Conference on 
information and knowledge management. ACM 
press, New York, NY, USA, 991-994. 
Zhang, P., Hou, Y. X., and Song, D. 2009. Ap-
proximating True Relevance Distribution from a 
Mixture Model based on Irrelevance Data. In 
Proceedings of the Conference on Research and 
Development in Information Retrieval. SIGIR 
'31. ACM Press, New York, NY, 107-114. 
444
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
An Iterative Link-based Method for Parallel Web Page Mining 
Le Liu1, Yu Hong1, Jun Lu2, Jun Lang2, Heng Ji3, Jianmin Yao1 
1School of Computer Science & Technology, Soochow University, Suzhou, 215006, China 
2Institute for Infocomm Research, Singapore, 138632 
3Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA 
giden@sina.cn,{tianxianer,lujun59,billlangjun}@gmail.com 
jih@rpi.edu,jyao@suda.edu.cn 
 
Abstracts 
Identifying parallel web pages from bi-
lingual web sites is a crucial step of bi-
lingual resource construction for cross-
lingual information processing. In this 
paper, we propose a link-based approach 
to distinguish parallel web pages from bi-
lingual web sites. Compared with the ex-
isting methods, which only employ the 
internal translation similarity (such as 
content-based similarity and page struc-
tural similarity), we hypothesize that the 
external translation similarity is an effec-
tive feature to identify parallel web pages. 
Within a bilingual web site, web pages 
are interconnected by hyperlinks. The 
basic idea of our method is that the trans-
lation similarity of two pages can be in-
ferred from their neighbor pages, which 
can be adopted as an important source of 
external similarity. Thus, the translation 
similarity of page pairs will influence 
each other. An iterative algorithm is de-
veloped to estimate the external transla-
tion similarity and the final translation 
similarity. Both internal and external 
similarity measures are combined in the 
iterative algorithm. Experiments on six 
bilingual websites demonstrate that our 
method is effective and obtains signifi-
cant improvement (6.2% F-Score) over 
the baseline which only utilizes internal 
translation similarity. 
1 Introduction 
Parallel corpora have played an important role in 
multilingual Natural Language Processing, espe-
cially in Machine Translation (MT) and Cross-
lingual Information Retrieval(CLIR). However, 
it?s time-consuming to build parallel corpora 
manually. Some existing parallel corpora are 
subject to subscription or license fee and thus not 
freely available, while others are domain-specific. 
Therefore, a lot of previous research has focused 
on automatically mining parallel corpora from 
the web. 
In the past decade, there have been extensive 
studies on parallel resource extraction from the 
web (e.g., Chen and Nie, 2000; Resnik 2003; 
Jiang et al., 2009) and many effective Web min-
ing systems have been developed such as 
STRAND, PTMiner, BITS and WPDE. For most 
of these mining systems, there is a typical paral-
lel resource mining strategy which involves three 
steps: (1) locate the bilingual websites (2) identi-
fy  parallel web pages from these bilingual web-
sites and (3) extract bilingual resources from the 
parallel web pages.  
In this paper, we focus on the step (2) which is 
regarded as the core of the mining system 
(Chunyu, 2007). Estimating the translation simi-
larity of two pages is the most basic and key 
problem in this step. Previous approaches have 
tried to tackle this problem by using the infor-
mation within the pages. For example, in the 
STRAND and PTMiner system, a structural fil-
tering process that relies on the analysis of the 
underlying HTML structure of pages is used to 
determine a set of pair-specific structural values, 
and then the values are used to decide whether 
the pages are translations of one another. The 
BITS system filters out bad pairs by using a large 
bilingual dictionary to compute a content-based 
similarity score and comparing the score with a 
threshold. The WPDE system combines URL 
similarity, structure similarity with content-based 
similarity to discover and verify candidate paral-
lel page pairs. Some other features or rules such 
as page size ratio, predefined hypertexts which 
link to different language versions of a web page 
are also used in most of these systems. Here, all 
of the mining systems are simply using the in-
formation within the page in the process of find-
1216
ing parallel web pages. In this paper, we attempt 
to explore other information to identify parallel 
web pages. 
On the Internet, most web pages are linked by 
hyperlinks. We argue that the translation similar-
ity of two pages depends on not only their inter-
nal information but also their neighbors. The 
neighbors of a web page are a set of pages, 
which link to the page. We find that the similari-
ty of neighbors can provide more reliable evi-
dence in estimating the translation similarity of 
two pages.  
The main issues are discussed in this paper as 
follows:  
? Can the neighbors of candidate page pairs 
really contribute to estimating the translation 
similarity?  
? How to estimate the translation similarity of 
candidate page pairs by using their neighbors? 
Our method has the following advantages: 
High performance 
The external and internal information is com-
bined to verify parallel page pairs in our method, 
while in previous mining systems, only internal 
information was used. Experimental results show 
that compared with existing parallel page pair 
identification technologies, our method obtains 
both higher precision and recall (6.2% and 6.3% 
improvement than the baseline, respectively). In 
addition, the external information used in our 
method is a more effective feature than internal 
features alone such as structural similarity and 
content-based similarity. 
Language independent 
In principle, our method is language inde-
pendent and can be easily ported to new lan-
guage pairs, except for the language-specific bi-
lingual lexicons. Our method takes full ad-
vantage of the link information that is language-
independent. For the bilingual lexicons in our 
experiments, compared to previous methods, our 
method does not need a big bilingual lexicon, 
which is good news to less-resource language 
pairs. 
Unsupervised and fewer parameters 
In previous work, some parameters need to be 
optimized. Due to the diversity of web page 
styles, it is not trivial to obtain the best parame-
ters. Some previous researches(Resnik, 2003; 
Zhang et al., 2006) attempt to optimize parame-
ters by employing machine learning method. In 
contrast, in our method, only two parameters 
need to be estimated. One parameter remains 
stable for different style websites. Another pa-
rameter can be easily adjusted to achieve the best 
performance. Therefore, our method can be used 
in other websites with different styles, without 
much effort to optimize these parameters.  
2 Related Work 
A large amount of literature has been published 
on parallel resource mining from the web. Ac-
cording to the existing form of the parallel re-
source on the Internet, related work can be cate-
gorized as follows: 
Mining from bilingual websites 
Most existing web mining systems aimed at 
mining bilingual resource from the bilingual 
websites, such as PTMiner (Nie et al., 1999), 
STRAND (Resnik and Smith, 2003), BITS (Ma 
and Liberman, 1999), PTI (Chen et al., 2004). 
PTMiner uses search engines to pinpoint the 
candidate sites that are likely to contain parallel 
pages, and then uses the collected URLs as seeds 
to further crawl each web site for more URLs. 
Web page pairs are extracted based on manually 
defined URL pattern matching, and further fil-
tered according to several criteria. STRAND us-
es a search engine to search for multilingual 
websites and generated candidate page pairs 
based on manually created substitution rules. 
Then, it filters some candidate pairs by analyzing 
the HTML pages. PTI crawls the web to fetch 
(potentially parallel) candidate multilingual web 
documents by using a web spider. To determine 
the parallelism between potential document pairs, 
a filename comparison module is used to check 
filename resemblance, and a content analysis 
module is used to measure the semantic similari-
ty. BITS was the first to obtain bilingual web-
sites by employing a language identification 
module, and then for each bilingual website, it 
extracts parallel pages based on their content.  
Mining from bilingual web pages 
Parallel/bilingual resources may exist not only 
in two parallel monolingual web pages, but also 
in single bilingual web pages. Jiang et al. (2009) 
used an adaptive pattern-based method to mine 
interesting bilingual data based on the observa-
tion that bilingual data usually appears collec-
tively following similar patterns. They found that 
bilingual web pages are a promising source of 
up-to-date bilingual terms/sentences which cover 
many domains and application scenarios. In ad-
dition, Feng et al. (2010) proposed a new method 
1217
to automatically acquire bilingual web pages 
from the result pages of a search engine.  
Mining from comparable corpus 
Several attempts have been made to extract 
parallel resources from comparable corpora. 
Zhao et al. (2002) proposed a robust, adaptive 
approach for mining parallel sentences from a 
bilingual comparable news collection. In their 
method, sentence length models and lexicon-
based models were combined under a maximum 
likelihood criterion. Smith et al. (2010) found 
that Wikipedia contains a lot of comparable doc-
uments, and adopted a ranking model to select 
parallel sentence pairs from comparable docu-
ments. Bharadwaj et al. (2011) used a SVM clas-
sifier with some new features to identify parallel 
sentences from Wikipedia.  
3 Iterative Link-based Parallel Web 
Pages Mining 
As mentioned, the basic idea of our method is 
that the similarity of two pages can be inferred 
from their neighbors. This idea is illustrated in 
Figure 1.  
A D
E
C
B
A?
D?
E?
C?
B?
?
 
Figure 1 Illustration of the link-based method 
In Figure 1, A, B, C, D and E are some pages 
in the same language; while A?, B?, C?, D? and E? 
are some pages in another language. The solid 
black arrows indicate the links between these 
pages. For example, page A points to C, page B? 
points to C? and so on. Then the page set {A, B, 
D, E} is called the neighbors of page C. Similar-
ly, the page set {A?, B?, D?, E?} contains the 
neighbors of page C?. If the page pairs : <A, A?>, 
<B, B?>, <D, D?> and <E, E?> have high transla-
tion similarities, then it can be inferred that page 
C and C? have a high probability to be a pair of 
parallel pages. Every page has its own neighbors. 
For each web page, our method views link-in and 
link-out hyperlinks as the same. Thus, the linked 
pages will influence each other in estimating the 
translation similarity. For example, the similari-
ties of two pairs <A, A?> and <C, C?> will influ-
ence each other. It is an iterative process. We 
will elaborate the process in the following sec-
tions.  
Since our goal is to find parallel pages in a 
specific website, the key task is to evaluate the 
translation similarity of two pages (which are in 
different languages) as accurately as possible. 
The final similarity of two pages should depend 
both on their internal similarity and external sim-
ilarity. The internal similarity means the similari-
ty estimated by using the information in the page 
itself, such as the structure similarity and the 
content-based similarity of the two pages. On the 
other hand, the external similarity of two pages is 
the similarity depending on their neighbors. The 
final translation similarity is called the En-
hanced Translation Similarity (ETS). The ETS 
of two pages can be calculated as follows:  
   (   )        (   )  (   )  
                                      (   )   [   ]              (1) 
Where,    (   ) is the internal translation simi-
larity of two pages: e and c;     (   ) represents 
the external translation similarity of pages e and 
c.    (   ) indicates the final similarity of two 
pages, which combines the internal with external 
translation similarity. 
In this paper, we conduct the experiments on 
English-Chinese parallel page pair mining. How-
ever, our method is language-independent. Thus, 
it can be applied to other language pairs by only 
replacing a bilingual lexicon. The symbol e and c 
always indicate an English page and a Chinese 
page respectively in this paper. In the following 
sections, we will describe how to calculate the 
   (   ) and     (   ) step by step. 
3.1 Preprocessing 
The input of our method is a bilingual website. 
This paper aims to find English/Chinese parallel 
pages. So a 3-gram language model is used to 
identify (or classify) the language of a certain 
document. The performance of the language 
identification module achieves 99.5% accuracy 
through in-house testing. As a result, a set of 
English pages and a set of Chinese pages are ob-
tained. In order to get the neighbors of a page, 
for each bilingual website, two networks are con-
structed based on the hyperlinks, one for English 
pages and another for Chinese pages. 
3.2 The Internal Translation Similarity 
Following Resnik and Smith (2003), three fea-
tures are used to evaluate the internal translation 
similarity of two pages: 
1218
The size ratio of two pages 
The length ratio of two documents is the sim-
plest criterion for determining whether two doc-
uments are parallel or not. Parallel documents 
tend to be similar in length. And it is reasonable 
to assume that for text E in one language and text 
F in another language, length(E) ? C?length(F), 
where C is a constant that depends on the lan-
guage pair. Here, the content length of a web 
page is regarded as its length. 
The structure similarity of two pages 
The HTML tags describe and control a web 
page?s structure. Therefore, the structure similar-
ity of two pages can be calculated by their 
HTML tags. Here, the HTML tags of each page 
are extracted (except the visual tags such as ?B?, 
?FONT?.) as a linear sequence. Then the struc-
ture similarity of two pages is computed by com-
paring their linearized sequences. In this paper, 
the LCS algorithm (Dan, 1997) is adopted to find 
the longest common sequences of the two HTML 
tag sequences. The ratio of LCS length and the 
average length of two HTML tag sequences are 
used as the structure similarity of the two pages.  
The content-based translation similarity of 
two pages 
The basic idea is that if two documents are 
parallel, they will contain word pairs that are mu-
tual translations (Ma, 1999). So the percentage of 
translation word pairs in the two pages can be 
considered as the content-based similarity. The 
translation words of two documents can be ex-
tracted by using a bilingual lexicon. Here, for 
each word in English document, we will try to 
find a corresponding word in Chinese document.  
Finally, the internal translation similarity of 
two pages is calculated as follows: 
   (   )       (   )  (   )  
                                          (   )   [   ]        (2) 
Where,     (   )  and        (   )  are the con-
tent-based and structural similarity of page   and 
  respectively. In addition, the size ratio of two 
pages is used to filter invalid page pairs.  
3.3 The External and Enhanced Transla-
tion Similarity 
As described above, the external translation 
similarity of two pages depends on their neigh-
bors:  
    (   )     (  ( )   ( )) (3) 
Where, PG(x), a set of pages, is the neighbors of 
page x. Obviously, the similarity of two sets re-
lies on the similarity of the elements in the two 
sets. Here, the elements are namely web pages. 
So,     (   ) equals to    (  ( )   ( )), and 
   (  ( )   ( ))  depends on    (     ) 
(       belongs to    ( )   ( ) , respectively) 
and    (   ) . According to Equation (1), 
   (   )  depends on    (   )  and     (   ) . 
Therefore, it is a process of iteration.    (   ) 
will converge after a certain number of iterations. 
Thus,     (   )  is defined as the enhanced 
similarity of page   and   after the i-th iteration, 
and the same is for     
 (   ) and     (  ( ) 
  ( )) .     (  ( )   ( ))  is computed by 
the following algorithm: 
Algorithm 1: Estimating the external transla-
tion similarity 
Input:      ( )   ( ) 
Output:     
 (   ) 
Procedure:  
   ? 0 
     ?   ( ) 
      ?   ( ) 
While        and       are both not empty: 
             
?                          (   
   (   ))  
    ?     +        (   ) 
Remove   from        
Remove   from       
    
 (   )       ( ( )  ( )) 
                             (   ( )     ( ) ) 
Algorithm 2 Estimating the enhanced transla-
tion similarity 
Input:      , (the English and Chinese page set) 
Output:    (   )           
Initialization: Set ETS(e, c) random value or 
small value 
Procedure:  
LOOP: 
For each   in    : 
For each   in   : 
     (   )         
 (   ) 
                                          (   )     (   ) 
Parameters normalization 
UNTIL    (   ) is stable  
Algorithm 1 tries to find the real parallel pairs 
from   ( ) and   ( ). The similarity of   ( ) 
and   ( ) is calculated based on the similarity 
1219
values of these pairs. Finally,    (   ) is calcu-
lated by the following algorithm 2. 
In Algorithm 2, the input    and    are English 
and Chinese page sets in a certain bilingual web-
site. We use algorithm 2 to estimate the en-
hanced translation similarity. 
3.4 Find the Parallel Page Pairs 
At last, the enhanced translation similarity of 
every pair is obtained, and the parallel page pairs 
can be extracted in terms of these similarities: 
Algorithm 3 Finding parallel page pairs 
Input:       
    (   )              
       (or       ) 
Output:  Parallel Page Pairs List :     
Procedure:  
LOOP: 
                       (   (   )) 
Add       to     
Remove   from     
Remove   from     
UNTIL size of     >       (or    (   )  < 
       ) 
This algorithm is similar to Algorithm 1 in 
each bilingual website. The input      is an 
integer threshold which means that only top 
      page pairs will be extracted in a certain 
website. It needs to be noted that      is al-
ways less than      and     . While the input 
        is another kind of threshold that is 
used for extracting page pairs with high transla-
tion similarity.  
4 Experiments and Analysis 
4.1 Experimental setup 
Our experiments focus on six bilingual websites. 
Most of them are selected from HK government 
websites. All the web pages were retrieved by 
using a web site download tool: HTTrack1. We 
notice that a small amount of pages doesn?t al-
ways contain valuable contents. So, we put a 
threshold (100 bytes in our experiment) on the 
web pages' content to filter meaningless pages. In 
order to evaluate our method, the bilingual page 
pairs of each website are annotated by a human 
annotator. Finally, we got 23109 pages and 
11684 bilingual page pairs in total for testing. 
                                                 
1 http://www.httrack.com/ 
The basic information of these websites is listed 
in Table 1. 
It?s time-consuming to annotate whether two 
pages is parallel or not. Note that if a website 
contains N English pages and M Chinese pages, 
an annotator has to label N*M page pairs. To the 
best of our knowledge, there is no large scale and 
public parallel page pair dataset with human an-
notation. So we try to build a reliable and large-
scale dataset. 
In our experiments, URL similarity is used to 
reduce the workload for annotation. For a certain 
website, firstly, we obtain its URL pattern be-
tween English and Chinese pages manually. For 
example, in the website ?www.gov.hk?, the URL 
pairs like: 
http://www.gov.hk/en/about/govdirectory/   (English) 
http://www.gov.hk/sc/about/govdirectory/   (Chinese) 
The URL pairs always point to a pair of paral-
lel pages. So <?/en/?,?/sc/?> is considered as a 
URL pattern that was used to find parallel pages. 
For the other URLs that can?t match the pattern, 
we have to label them by hand. The column ?No 
pattern pairs? in Table 1 shows that the number 
of parallel page pairs which mismatch any pat-
terns. 
Table 1 Number of pages and bilingual page pairs of 
each websites 
Site ID En/Ch pages 
Total 
pairs 
No pat-
tern pairs 
URL 
S1 1101/1098 1092 20 www.gov.hk 
S2 501/497 487 7 www.customs.gov.hk 
S3 995/775 768 12 www.sbc.edu.sg 
S4 4085/3838 3648 4 www.swd.gov.hk 
S5 660/637 637 0 www.landsd.gov.hk 
S6 4733/4626 4615 8 www.td.gov.hk 
 total 12075/11471 11684 51  
Each website listed in Table 1 has a URL pat-
tern for most parallel web pages. Some previous 
researches used the URL similarity or patterns to 
find parallel page pairs. However, due to the di-
versity of web page styles and website mainte-
nance mechanisms, bilingual websites adopt var-
ied naming schemes for parallel documents (Shi, 
et al, 2006). The effect of URL pattern-based 
mining always depends on the style of website. 
In order to build a large dataset, the URL pattern 
is not used in our method. Our method is able to 
handle bilingual websites without URL pattern 
rules. 
In addition, an English-Chinese dictionary 
with 64K words pairs is used in our experiments. 
Algorithm 3 needs a threshold       or 
1220
       . It is very hard to tune the        
because it varies a lot in different websites and 
language pairs. However, Table 1 shows that the 
number of parallel pages is smaller than that of 
English and Chinese pages. Here, for each web-
site, the      is set to the number of Chinese 
pages (which is always smaller than that of Eng-
lish pages). In this way, the precision will never 
reach 100%, but it is more practical in a real ap-
plication. As a result, in some experiments, we 
only report the F-score, and the precision and 
recall can be calculated as follows:  
          
       (            )
      
                 (4) 
       
       (            )
        
                      (5) 
Where,        for each website is listed in the 
?Total  pairs? column of Table 1. 
4.2 Results and Analysis 
Performance of the Baseline 
Let?s start by presenting the performance of a 
baseline method as follows. The baseline only 
employs the internal translation similarity for 
parallel web pages mining. Algorithm 3 is also 
used to get the page pairs in baseline system. 
Here, the input    (   )  is replaced by 
   (   ) . The parameter   in Equation 2 is a 
discount factor. For different   values, the per-
formance of baseline system on six websites is 
shown in Figure 2. In the Figure 2, it shows that 
when   is set to 0.6, the baseline system achieves 
the best performance. The precision, recall and 
F-score are 85.84%, 87.55% and 86.69% respec-
tively. So in the following experiments, we al-
ways set ? to 0.6.  
 
Figure 2 Performances of baseline system with differ-
ent   value 
Performance of Our Method 
As described in Section 3, our method com-
bines the internal with external translation simi-
larity in estimating the final translation similarity 
(i.e., ETS) of two pages. So, the discount factor 
  in Equation (1) is important in our method. 
Besides, as shown in Algorithm 2, the iterative 
algorithm is used to calculate the similarity. Then, 
one question is that how many iterations are re-
quired in our algorithm. Figure 3 shows the per-
formance of our method on each website. Its hor-
izontal axis represents the number of iterations 
and the vertical axis represents the F-score. And 
for each website, the F-scores with different   
(range from 0.2 to 0.8) are also reported in this 
figure. From Figure 3, it is very easy to find that 
the best iteration number is 3. For almost all the 
websites, the performance of our method 
achieves the maximal values and converges after 
the third iteration. In addition, Figure 3 also indi-
cates that our method is robust for different web-
sites. In the following experiments, the iteration 
number is set to 3. 
Next, let?s turn to the discount factor  . Figure 
4 reports the experimental results on the whole 
dataset. Here, the horizontal axis represents the 
discount factor   and the vertical axis represents 
the F-score.     means that only the internal 
similarity is used in the algorithm, so the F-score 
equals to that in Figure 2 when      . On the 
contrary,     means that only the external 
similarity is used in the method, and the F-score 
is 80.20%. The performance is lower than the 
baseline system when only the external link in-
formation is used, but it is much better than the 
performance of the content-based method and 
structure-based method whose F-scores are 64.82% 
and 64.0% respectively. Besides, it is shown 
from Figure 4, the performance is improved sig-
nificantly when the internal and external similari-
ty measures are combined together. Furthermore, 
it is somewhat surprising that the discount factor 
  is not important as we previously expected. In 
fact, if we discard the cases that   equals to 0 or 
1, the difference between the maximum and min-
imum F-score will be 0.76% which is very small. 
This finding indicates that the internal and exter-
nal similarity can easily be combined and we 
don?t need to make many efforts to tune this pa-
rameter when our method is applied to other 
websites. The reason of this phenomenon is that, 
no matter how much weight (i.e., 1-  ) was as-
signed  to the internal similarity, the internal sim-
ilarity always provides a relatively good initial 
60
65
70
75
80
85
90
95
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
P
er
fo
rm
a
n
ce
(%
) 
? 
F-score Precision Recall
1221
 
Figure 3 Experiment results of our method on each website
iterative direction. In the following experiments, 
the parameter ? is set to 0.6. 
 
Figure 4 The F-scores of our method with different 
the value of ? 
The weight of pages 
The weight of the neighbor pages should also 
be considered. For example, in the most websites, 
it is very common that most of the web pages 
contain a hyperlink which points to the homep-
age of the website. While in most of the Eng-
lish/Chinese websites, almost every English page 
will link to the English homepage and each Chi-
nese page will point to Chinese homepage. The 
English and Chinese homepages are probably 
parallel, but they will be helpless to find parallel 
web pages, because they are neighbors of almost 
every page in the site. On the contrary, some-
times the parallel homepages have negative ef-
fects on finding parallel pages They will increase 
the translation similarity of two pages which are 
not indeed mutual translations. So it is necessary 
to amend the Algorithm 1.  
The weight of each page is calculated accord-
ing to its popularity: 
 ( )     
    
    ( )   
  (6) 
where ( ) indicates the weight of page  ,   is 
the number of all pages,     ( ) is the number 
of pages pointing to page   and   is a constant 
for smoothing.  
In this paper, the weights of pages are used in 
two ways: 
Weight 1: The 9th line of Algorithm 1 is 
amended by the page weight as follows: 
     ?           (   )  ( ( )   ( ))    
Weight 2: The pages with low weight are re-
moved from the input of Algorithm 1. 
The experiment results are shown in Table 2.  
Table 2 The effect of page weight 
Type No Weight Weight 1 Weight 2 
F-score (%) 92.91 92.78 92.75 
Surprisingly, no big differences are found after 
the introduction of the page weight. The side ef-
fect of popular pages is not so large in our meth-
od. In the neighbor pages of a certain page, the 
popular pages are the minority. Besides, the iter-
ative process makes our method more stable and 
robust. 
The impact of the size of bilingual lexicon 
The baseline system mainly combines the con-
tent-based similarity with structure similarity. 
86.69  
92.15  
92.42  
92.67  
92.78  
92.83  
92.91  
92.83  
92.61  
92.40  
80.20  
79
81
83
85
87
89
91
93
95
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F
-s
co
re
(%
) 
? 
1222
And two kinds of similarity measures are also 
used in our method. As Ma and Liberman (1999) 
pointed out, not all translators create translated 
pages that look like the original page which 
means that the structure similarity does not al-
ways work well. Compared to the structure simi-
larity, the content-based is more reliable and has 
wider applicability. Furthermore, the bilingual 
lexicon is the only information that relates to the 
language pairs, and other features (such as struc-
ture and link information) are all language inde-
pendent. So, it?s important to investigate the ef-
fect of lexicon size in our method. We test the 
performance of our method with different size of 
the bilingual dictionary. The experiment results 
are shown in Figure 5. In this figure, the horizon-
tal axis represents the bilingual lexicon size and 
the vertical axis represents the F-score. With the 
decline of the lexicon size, the performances of 
both the baseline method and our method are 
decreased. However, we can find that the descent 
rate of our method is smaller than that of the 
baseline. It indicates that our method does not 
need a big bilingual lexicon which is good news 
for the low-resource language pairs. 
 
Figure 5 The impact of the size of bilingual lexicon 
Error analysis  
Errors occur when the two pages are similar in 
terms of structure, content and their neighbors. 
For example, Figure 6 illustrates a typical web 
page structure. There are 5 parts in the web page: 
 ,  ,  ,   and  . Part   always contains the 
main content of this page. While part  ,  ,   and 
  always contain some hyperlinks such as ?home? 
in part   and ?About us? in part  . Links in   
and   sometimes relate to the content of the page. 
For such a kind of non-parallel page pairs, let?s 
assume that the two pages have the same struc-
ture (as shown in Figure 6). In addition, their 
content part   is very short and contains the 
same or related topics. As a result, the links in 
other 4 parts are likely to be similar. In this case, 
our method is likely to regard the two pages as 
parallel.  
M
U
B
L R
 
Figure 6 A typical web page structure 
There are about 920 errors when our system 
obtains its best performance. By carefully inves-
tigating the error page pairs, we find that more 
than 90% errors fall into the category discussed 
above. The websites used in our experiments 
mainly come from Hong Kong government web-
sites. Some government departments regularly 
publish quarterly or monthly work reports on one 
issue through their websites. These reports look 
very similar except the publish date and some 
data in them. The other 10% errors happen be-
cause of the particularity of the web pages, e.g. 
very short pages, broken pages and so on. 
5 Conclusions and Future Work 
Parallel corpora are valuable resources for a lot 
of NLP research problems and applications, such 
as MT and CLIR. This paper introduces an effi-
cient and effective solution to bilingual language 
processing. We first explore how to extract paral-
lel page pairs in bilingual websites with link in-
formation between web pages. Firstly, we hy-
pothesize that the translation similarity of pages 
should be based on both internal and external 
translation similarity. Secondly, a novel iterative 
method is proposed to verify parallel page pairs. 
Experimental results show that our method is 
much more effective than the baseline system 
with 6.2% improvement on F-Score. Further-
more, our method has some significant contribu-
tions. For example, compared to previous work, 
our method does not depend on bilingual lexi-
cons, and the parameters in our method have lit-
tle effect on the final performance. These fea-
tures improve the applicability of our method. 
In the future work, we will study some method 
on extracting parallel resource from existing par-
allel page pairs, which are challenging tasks due 
to the diversity of page structures and styles. Be-
sides, we will evaluate the effectiveness of our 
mined data on MT or other applications. 
78
80
82
84
86
88
90
92
94
64K 32K 16K 8K 4K 2K 1K
F
-s
co
re
 (
%
) 
Lexicon Size 
Baseline Our Method
1223
Acknowledgments 
This research work has been sponsored by Na-
tional Natural Science Foundation of China 
(Grants No.61373097 and No.61272259), one 
National Natural Science Foundation of Jiangsu 
Province (Grants No.BK2011282), one Major 
Project of College Natural Science Foundation of 
Jiangsu Province (Grants No.11KJA520003) and 
one National Science Foundation of Suzhou City 
(Grants No.SH201212).  
The corresponding author of this paper, ac-
cording to the meaning given to this role by 
School of computer science and technology at 
Soochow University, is Yu Hong 
Reference 
Chen, Jiang and Jianyun Nie. 2000. Automatic con-
struction of parallel English-Chinese corpus for 
cross-language information retrieval. Proceedings 
of the sixth conference on Applied Natural Lan-
guage Processing, 21?28. 
Resnik, Philip and Noah A. Smith. 2003. The Web as 
a Parallel Corpus. Meeting of the Association for 
Computational Linguistics 29(3). 349?380. 
Kit, Chunyu and Jessica Yee Ha Ng. 2007. An Intelli-
gent Web Agent to Mine Bilingual Parallel Pages 
via Automatic Discovery of URL Pairing Patterns. 
Web Intelligence and Intelligent Agent Technology 
Workshops, 526?529. 
Zhang, Ying, Ke Wu, Jianfeng Gao and Phil Vines. 
2006. Automatic Acquisition of Chinese-English 
Parallel Corpus from the Web. Joint Proceedings of 
the Association for Computational Linguistics and 
the International Conference on Computational 
Linguistics, 420?431. 
Nie, Jianyun, Michel Simard, Pierre Isabelle and 
Richard Durand. 1999. Cross-language information 
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. Proceedings of 
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information 
retrieval, 74?81. 
Ma, Xiaoyi and Mark Y. Liberman. 1999. BITS: A 
Method for Bilingual Text Search over the Web. 
Machine Translation Summit VII. 
Chen, Jisong, Rowena Chau and Chung-Hsing Yeh. 
2004. Discovering Parallel Text from the World 
Wide Web. The Australasian Workshop on Data 
Mining and Web Intelligence, vol. 32, 157?161. 
Dunedin, New Zealand. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu 
and Qingsheng Zhu. 2009. Mining Bilingual Data 
from the Web with Adaptively Learnt Patterns. 
Proceedings of the Joint Conference of the 47th 
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, vol. 2, 870?878. 
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin  
Yao and Qiaoming Zhu. 2010. A novel method for 
bilingual web page acquisition from search engine 
web records. Proceedings of the 23rd International 
Conference on Computational Linguistics: Posters, 
294?302.  
Zhao, Bing and Stephan Vogel. 2002. Adaptive Paral-
lel Sentences Mining from Web Bilingual News 
Collection. IEEE International Conference on Data 
Mining, 745?748. 
Smith, Jason R., Chris Quirk and Kristina Toutanova. 
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. Hu-
man Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 403?
411. 
Bharadwaj, Rohit G. and Vasudeva Varma. 2011. 
Language independent identification of parallel 
sentences using wikipedia. Proceedings of the 20th 
International Conference Companion on World 
Wide Web, 11?12. Hyderabad, India. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences: Computerss Science and Computa-
tional Biology. Cambridge University Press  
Shi, Lei, Cheng Niu, Ming Zhou and Jianfeng Gao. 
2006. A DOM Tree Alignment Model for Mining 
Parallel Data from the Web. Proceedings of the 
21st International Conference on Computational 
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, 489?496. 
 
 
1224
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846?1851,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Constructing Information Networks Using One Single Model
Qi Li
?
Heng Ji
?
Yu Hong
??
Sujian Li
?
?
Computer Science Department, Rensselaer Polytechnic Institute, USA
?
School of Computer Science and Technology, Soochow University, China
?
Key Laboratory of Computational Linguistics, Peking University, MOE, China
?
{liq7,hongy2,jih}@rpi.edu,
?
lisujian@pku.edu.cn
Abstract
In this paper, we propose a new frame-
work that unifies the output of three infor-
mation extraction (IE) tasks - entity men-
tions, relations and events as an informa-
tion network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other. For example, many rela-
tions can now be considered as the re-
sultant states of events. Our approach
achieves substantial improvements over
traditional pipelined approaches, and sig-
nificantly advances state-of-the-art end-to-
end event argument extraction.
1 Introduction
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely inter-
dependent: entity mentions are core components
of relations and events, and the extraction of rela-
tions and events can help to accurately recognize
entity mentions. In addition, the theory of eventu-
alities (D?olling, 2011) suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to ex-
tract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks (e.g., (Roth and Yih, 2007;
Riedel and McCallum, 2011; Yang and Cardie,
2013; Riedel et al., 2009; Singh et al., 2013; Li et
al., 2013; Li and Ji, 2014)). For example, Roth and
Yih (2007) conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments (Li et al.,
2013), and entity mentions and relations (Li and
Ji, 2014). However, a single model that can ex-
tract all of them has never been studied so far.
Asif Mohammed Hanif detonated explosives in Tel Aviv
AttackPerson Weapon Geopolitical Entity
Place
InstrumentAttacker
Agent-Artifact
Physical
x1 x2 x3 x4 x5 x6 x7 x8x:
y:
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Ar-
rows represent information arcs.
For the first time, we uniformly represent the IE
output from each sentence as an information net-
work, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to con-
struct the information networks (Collins, 2002; Li
et al., 2013; Li and Ji, 2014). In addition to the per-
ceptron update, we also apply k-best MIRA (Mc-
Donald et al., 2005), which refines the perceptron
update in three aspects: it is flexible in using var-
ious loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in (Li et
al., 2013; Li and Ji, 2014), we are able to cap-
ture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention ?Asif Mohammed Hanif ? is an Attacker
of the Attack event triggered by ?detonated?, and
the Weapon mention ?explosives? is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them. Similarly we can infer
the Physical relation between ?Asif Mohammed
Hanif ? and ?Tel Aviv?.
However, in practice many useful interactions
are missing during testing because of the data spar-
1846
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE?05
1
training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet (Baker and
Sato, 2003) to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the de-
tailed implementation of our new framework.
2 Approach
We uniformly represent the IE output from each
sentence as an information network y = (V,E).
Each node v
i
? V is represented as a triple
?u
i
, v
i
, t
i
? of start index u
i
, end index v
i
, and node
type t
i
. A node can be an entity mention or an
event trigger. A particular type of node is ? (nei-
ther entity mention nor event trigger), whose max-
imal length is always 1. Similarly, each infor-
mation arc e
j
? E is represented as ?u
j
, v
j
, r
j
?,
where u
j
and v
j
are the end offsets of the nodes,
and r
j
is the arc type. For instance, in Fig-
ure 1, the event trigger ?detonated? is represented
as ?4, 4, Attack?, the entity mention ?Asif Mo-
hammed Hanif ? is represented as ?1, 3, Person?,
and their argument arc is ?4, 3, Attacker?. Our
goal is to extract the whole information network y
for a given sentence x.
2.1 Decoding Algorithm
Our joint decoding algorithm is based on ex-
tending the segment-based algorithm described in
our previous work (Li and Ji, 2014). Let x =
(x
1
, ..., x
m
) be the input sentence. The decoder
performs two types of actions at each token x
i
from left to right:
? NODEACTION(i, j): appends a new node
?j, i, t? ending at the i-th token, where i? d
t
<
j ? i, and d
t
is the maximal length of type-t
nodes in training data.
? ARCACTION(i, j): for each j < i, incremen-
tally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: ?i, j, r?.
After each action, the top-k hypotheses are se-
lected according to their features f(x, y
?
) and
1
http://www.itl.nist.gov/iad/mig//tests/ace
weights w:
best
k
y
?
?buffer
f(x, y
?
) ?w
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type con-
straints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the final
prediction. The upper-bound time complexity of
the decoding algorithm is O(d ? b ? m
2
), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
2.2 Parameter Estimation
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
z = argmax
y
?
?Y(x)
f(x, y
?
) ?w
and then updates the feature weights by using:
w
new
= w + f(x, y)? f(x, z)
We relax the exact inference problem by the afore-
mentioned beam-search procedure. The stan-
dard perceptron will cause invalid updates be-
cause of inexact search. Therefore we apply early-
update (Collins and Roark, 2004), an instance of
violation-fixing methods (Huang et al., 2012). In
the rest of this paper, we override y and z to denote
prefixes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA (McDonald et al., 2005),
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formu-
lated as a quadratic programming problem:
min ?w
new
?w?
s.t. w
new
f(x, y)?w
new
f(x, z) ? L(y, z)
?z ? best
k
(x,w)
We employ the following three loss functions
for comparison:
1847
Freq. Relation Type Event Type Arg-1 Arg-2 Example
159 Physical Transport Artifact Destination He
(arg-1)
was escorted
(trigger)
into Iraq
(arg-2)
.
46 Physical Attack Target Place Many people
(arg-1)
were in the cafe
(arg-2)
during the blast
(trigger)
.
42 Agent-Artifact Attack Attacker Instrument Terrorists
(arg-1)
might use
(trigger)
the devices
(arg-2)
as weapons.
41 Physical Transport Artifact Origin The truck
(arg-1)
was carrying
(trigger)
Syrians fleeing the war in Iraq
(arg-2)
.
33 Physical Meet Entity Place They
(arg-1)
have reunited
(trigger)
with their friends in Norfolk
(arg-2)
.
32 Physical Die Victim Place Two Marines
(arg-1)
were killed
(trigger)
in the fighting in Kut
(arg-2)
.
28 Physical Attack Attacker Place Protesters
(arg-1)
have been clashing
(trigger)
with police in Tehran
(arg-2)
.
26 ORG-Affiliation End-Position Person Entity NBC
(arg-2)
is terminating
(trigger)
freelance reporter Peter Arnett
(arg-1)
.
Table 1: Frequent overlapping relation and event types in the training set.
? The first one is F
1
loss:
L
1
(y, z) = 1?
2 ? |y ? z|
|y|+ |z|
When counting the numbers, we treat each node
and arc as a single unit. For example, in Fig-
ure 1, |y| = 6.
? The second one is 0-1 loss:
L
2
(y, z) =
{
1 y 6= z
0 y = z
It does not discriminate the extent to which z
deviates from y.
? The third loss function counts the difference be-
tween y and z:
L
3
(y, z) = |y|+ |z| ? 2 ? |y ? z|
Similar to F
1
loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
2.3 Joint Relation-Event Features
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized fea-
tures in pipelined approaches. In addition to the
features as described in (Li et al., 2013; Li and
Ji, 2014), we can make use of joint features be-
tween relations and events, given the fact that
relations are often ending or starting states of
events (D?olling, 2011). Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y
?
dur-
ing the search, if both arguments of a relation par-
ticipate in an event, we compose the correspond-
ing argument roles and relation type as a joint fea-
ture for y
?
. For example, for the structure in Fig-
ure 1, we obtain the following joint relation-event
features:
Attacker Instrument
Agent-Artifact
Attacker Place
Physical
Split Sentences Mentions Relations Triggers Arguments
Train 7.2k 25.7k 4.8k 2.8k 4.5k
Dev 1.7k 6.3k 1.2k 0.7k 1.1k
Test 1.5k 5.3k 1.1k 0.6k 1.0k
Table 2: Data set
0 20 40 60 80 100Number of instances0
2
4
6
8
10
12
14
Freq
uenc
y
Trigger WordsFrame IDs
Figure 2: Distribution of triggers and their frames.
2.4 Semantic Frame Features
One major challenge of constructing information
networks is the data sparsity problem in extract-
ing event triggers. For instance, in the sen-
tence: ?Others were mutilated beyond recogni-
tion.? The Injure trigger ?mutilated? does not oc-
cur in our training data. But there are some sim-
ilar words such as ?stab? and ?smash?. We uti-
lize FrameNet (Baker and Sato, 2003) to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a ba-
sic type of semantic concept, and contains a num-
ber of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame ?Cause harm? is closely
related with Injure event and contains 68 lexical
units such as ?stab?, ?smash? and ?mutilate?.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
1848
Methods
Entity Mention (%)
Relation (%)
Event Trigger (%)
Event Argument (%)
P R F
1
P R F
1
P R F
1
P R F
1
Pipelined Baseline
83.6 75.7 79.5
68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7
Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1
Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A
Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6
Joint w/ MIRA w/ F
1
Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4
Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9
Joint w/ MIRA w/ L
3
Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8
Table 3: Overall performance on test set.
once in the training data from 100 to 60 and al-
leviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, ?Cause harm? will be a feature for ?mu-
tilated?. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
3 Experiments
3.1 Data and Evaluation
We use ACE?05 corpus to evaluate our method
with the same data split as in (Li and Ji, 2014). Ta-
ble 2 summarizes the statistics of the data set. We
report the performance of extracting entity men-
tions, relations, event triggers and arguments sep-
arately using the standard F
1
measures as defined
in (Ji and Grishman, 2008; Chan and Roth, 2011):
? An entity mention is correct if its entity type (7
in total) and head offsets are correct.
? A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
? An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
? An argument link is correct if its event subtype,
offsets and role match those of any of the refer-
ence argument mentions.
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by hand-
crafted patterns (Chang and Manning, 2012).
3.2 Results
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall perfor-
mance of our approaches and baseline methods.
Our joint model with perceptron update out-
performs the state-of-the-art pipelined approach
in (Li et al., 2013; Li and Ji, 2014), and further
improves the joint event extraction system in (Li
et al., 2013) (p < 0.05 for entity mention extrac-
tion, and p < 0.01 for other subtasks, accord-
ing to Wilcoxon Signed RankTest). For the k-
best MIRA update, the L
3
loss function achieved
better performance than F
1
loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also significantly outperforms perceptron update
on relation extraction and event argument extrac-
tion (p < 0.01). It is particularly encouraging to
see the end output of an IE system (event argu-
ments) has made significant progress (12.2% ab-
solute gain over traditional pipelined approach).
3.3 Discussions
3.3.1 Feature Study
Rank Feature Weight
1 Frame=Killing Die 0.80
2 Frame=Travel Transport 0.61
3 Physical(Artifact, Destination) 0.60
4 w
1
=?home? Transport 0.59
5 Frame=Arriving Transport 0.54
6 ORG-AFF(Person, Entity) 0.48
7 Lemma=charge Charge-Indict 0.45
8 Lemma=birth Be-Born 0.44
9 Physical(Artifact,Origin) 0.44
10 Frame=Cause harm Injure 0.43
Table 4: Top Features about Event Triggers.
Table 4 lists the weights of the most significant
features about event triggers. The 3
rd
, 6
th
, and
9
th
rows are joint relation-event features. For in-
stance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
1849
and FrameNet based features are of vital impor-
tance to event trigger labeling. We tested the im-
pact of each type of features by excluding them in
the experiments of ?MIRA w/ L
3
loss?. We found
that FrameNet based features provided 0.8% and
2.2% F
1
gains for event trigger and argument la-
beling respectively. Joint relation-event features
also provided 0.6% F
1
gain for relation extraction.
3.3.2 Remaining Challenges
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remain-
ing errors suggest to incorporate external world
knowledge. For example, some words act as trig-
gers for some certain types of events only when
they appear together with some particular argu-
ments:
? ?Williams picked up the child again and this
time, threw
Attack
her out the window.?
The word ?threw? is used as an Attack event
trigger because the Victim argument is a ?child?.
? ?Ellison to spend $10.3 billion to get
Merge Org
his company.? The common word ?get? is
tagged as a trigger of Merge Org, because its
object is ?company?.
? ?We believe that the likelihood of them
using
Attack
those weapons goes up.?
The word ?using? is used as an Attack event
trigger because the Instrument argument is
?weapons?.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
? ?we are paying great attention to their ability to
defend
Attack
on the ground.?,
our system fails to extract ?defend? as an Attack
trigger. In the training data, ?defend? appears mul-
tiple times, but none of them is tagged as Attack.
For instance, in the sentence:
? ?North Korea could do everything to defend it-
self. ?
?defend? is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to ac-
tual events. Event coreference is necessary to rec-
ognize them correctly. For example, in the follow-
ing two sentences from the same document:
? ?It?s important that people all over the world
know that we don?t believe in the war
Attack
.?,
? ?Nobody questions whether this
Attack
is right
or not.?
?this? refers to ?war? in its preceding contexts.
Without event coreference resolution, it is difficult
to tag it as an Attack event trigger.
4 Conclusions
We presented the first joint model that effectively
extracts entity mentions, relations and events
based on a unified representation: information
networks. Experiment results on ACE?05 cor-
pus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance significantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger la-
beling.
Although our primary focus in this paper is in-
formation extraction in the ACE paradigm, we be-
lieve that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.
References
Collin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proc. ACL, pages 161?164.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551?560.
1850
Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proc. LREC, pages 3735?3740.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
Johannes D?olling. 2011. Aspectual coercion and even-
tuality structure. pages 189?226.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73?82.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. ACL, pages 91?98.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proc. EMNLP.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic ap-
proach to bio-molecular event extraction. In Proc.
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing: Shared Task.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640?1649.
1851
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Cross-Entity Inference to Improve Event Extraction 
Yu Hong     Jianfeng Zhang     Bin Ma     Jianmin Yao     Guodong Zhou     Qiaoming Zhu 
School of Computer Science and Technology, Soochow University, Suzhou City, China 
{hongy, jfzhang, bma, jyao, gdzhou, qmzhu}@suda.edu.cn 
 
 
Abstract 
Event extraction is the task of detecting certain 
specified types of events that are mentioned in 
the source language data. The state-of-the-art 
research on the task is transductive inference 
(e.g. cross-event inference). In this paper, we 
propose a new method of event extraction by 
well using cross-entity inference. In contrast to 
previous inference methods, we regard entity-
type consistency as key feature to predict event 
mentions. We adopt this inference method to 
improve the traditional sentence-level event ex-
traction system. Experiments show that we can 
get 8.6% gain in trigger (event) identification, 
and more than 11.8% gain for argument (role) 
classification in ACE event extraction. 
1 Introduction 
The event extraction task in ACE (Automatic Con-
tent Extraction) evaluation involves three challeng-
ing issues: distinguishing events of different types, 
finding the participants of an event and determin-
ing the roles of the participants. 
The recent researches on the task show the 
availability of transductive inference, such as that 
of the following methods: cross-document, cross-
sentence and cross-event inferences. Transductive 
inference is a process to use the known instances to 
predict the attributes of unknown instances. As an 
example, given a target event, the cross-event in-
ference can predict its type by well using the re-
lated events co-occurred with it within the same 
document. From the sentence: 
(1)He left the company. 
it is hard to tell whether it is a Transport event in 
ACE, which means that he left the place; or an 
End-Position event, which means that he retired 
from the company. But cross-event inference can 
use a related event ?Then he went shopping? within 
the same document to identify it as a Transport 
event correctly. 
As the above example might suggest, the avail-
ability of transductive inference for event extrac-
tion relies heavily on the known evidences of an 
event occurrence in specific condition. However, 
the evidence supporting the inference is normally 
unclear or absent. For instance, the relation among 
events is the key clue for cross-event inference to 
predict a target event type, as shown in the infer-
ence process of the sentence (1). But event relation 
extraction itself is a hard task in Information Ex-
traction. So cross-event inference often suffers 
from some false evidence (viz., misleading by un-
related events) or lack of valid evidence (viz., un-
successfully extracting related events). 
In this paper, we propose a new method of 
transductive inference, named cross-entity infer-
ence, for event extraction by well using the rela-
tions among entities. This method is firstly 
motivated by the inherent ability of entity types in 
revealing event types. From the sentences: 
(2)He left the bathroom. 
(3)He left Microsoft. 
it is easy to identify the sentence (2) as a Transport 
event in ACE, which means that he left the place, 
because nobody would retire (End-Position type) 
from a bathroom. And compared to the entities in 
sentence (1) and (2), the entity ?Microsoft? in (3) 
would give us more confidence to tag the ?left? 
event as an End-Position type, because people are 
used to giving the full name of the place where 
they retired. 
The cross-entity inference is also motivated by 
the phenomenon that the entities of the same type 
often attend similar events. That gives us a way to 
predict event type based on entity-type consistency. 
From the sentence: 
(4)Obama beats McCain. 
it is hard to identify it as an Elect event in ACE, 
which means Obama wins the Presidential Election, 
1127
or an Attack event, which means Obama roughs 
somebody up. But if we have the priori knowledge 
that the sentence ?Bush beats McCain? is an Elect 
event, and ?Obama? was a presidential contender 
just like ?Bush? (strict type consistency), we have 
ample evidence to predict that the sentence (4) is 
also an Elect event. 
Indeed above cross-entity inference for event-
type identification is not the only use of entity-type 
consistency. As we shall describe below, we can 
make use of it at all issues of event extraction: 
y For event type: the entities of the same type 
are most likely to attend similar events. And the 
events often use consistent or synonymous trigger. 
y For event argument (participant): the enti-
ties of the same type normally co-occur with simi-
lar participants in the events of the same type. 
y For argument role: the arguments of the 
same type, for the most part, play the same roles in 
similar events. 
With the help of above characteristics of entity, 
we can perform a step-by-step inference in this 
order:  
y Step 1: predicting event type and labeling 
trigger given the entities of the same type. 
y Step 2: identifying arguments in certain event 
given priori entity type, event type and trigger that 
obtained by step 1. 
y Step 3: determining argument roles in certain 
event given entity type, event type, trigger and ar-
guments that obtained by step 1 and step 2. 
On the basis, we give a blind cross-entity infer-
ence method for event extraction in this paper. In 
the method, we first regard entities as queries to 
retrieve their related documents from large-scale 
language resources, and use the global evidences 
of the documents to generate entity-type descrip-
tions. Second we determine the type consistency of 
entities by measuring the similarity of the type de-
scriptions. Finally, given the priori attributes of 
events in the training data, with the help of the en-
tities of the same type, we perform the step-by-step 
cross-entity inference on the attributes of test 
events (candidate sentences). 
In contrast to other transductive inference meth-
ods on event extraction, the cross-entity inference 
makes every effort to strengthen effects of entities 
in predicting event occurrences. Thus the inferen-
tial process can benefit from following aspects: 1) 
less false evidence, viz. less false entity-type con-
sistency (the key clue of cross-entity inference), 
because the consistency can be more precisely de-
termined with the help of fully entity-type descrip-
tion that obtained based on the related information 
from Web; 2) more valid evidence, viz. more enti-
ties of the same type (the key references for the 
inference), because any entity never lack its con-
geners. 
2 Task Description 
The event extraction task we addressing is that of 
the Automatic Content Extraction (ACE) evalua-
tions, where an event is defined as a specific occur-
rence involving participants. And event extraction 
task requires that certain specified types of events 
that are mentioned in the source language data be 
detected. We first introduce some ACE terminol-
ogy to understand this task more easily: 
y Entity: an object or a set of objects in one of 
the semantic categories of interest, referred to in 
the document by one or more (co-referential) entity 
mentions. 
y Entity mention: a reference to an entity (typi-
cally, a noun phrase). 
y Event trigger: the main word that most clear-
ly expresses an event occurrence (An ACE event 
trigger is generally a verb or a noun). 
y Event arguments: the entity mentions that 
are involved in an event (viz., participants). 
y Argument roles: the relation of arguments to 
the event where they participate. 
y Event mention: a phrase or sentence within 
which an event is described, including trigger and 
arguments. 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 separate event types 
and do not consider the hierarchical structure 
among them. Besides, the ACE evaluation plan 
defines the following standards to determine the 
correctness of an event extraction: 
y A trigger is correctly labeled if its event type 
and offset (viz., the position of the trigger word in 
text) match a reference trigger. 
y An argument is correctly identified if its event 
type and offsets match any of the reference argu-
ment mentions, in other word, correctly recogniz-
ing participants in an event. 
y An argument is correctly classified if its role 
matches any of the reference argument mentions. 
Consider the sentence: 
1128
(5) It has refused in the last five years to revoke 
the license of a single doctor for committing medi-
cal errors.1
The event extractor should detect an End-
Position event mention, along with the trigger 
word ?revoke?, the position ?doctor?, the person 
whose license should be revoked, and the time dur-
ing which the event happened: 
 Event type End-Position 
Trigger revoke 
a single doctor Role=Person 
doctor Role=Position Arguments 
the last five years Role=Time-within 
Table 1: Event extraction example 
It is noteworthy that event extraction depends on 
previous phases like name identification, entity 
mention co-reference and classification. Thereinto, 
the name identification is another hard task in ACE 
evaluation and not the focus in this paper. So we 
skip the phase and instead directly use the entity 
labels provided by ACE. 
3 Related Work 
Almost all the current ACE event extraction sys-
tems focus on processing one sentence at a time 
(Grishman et al, 2005; Ahn, 2006; Hardyet al 
2006). However, there have been several studies 
using high-level information from a wider scope:  
Maslennikov and Chua (2007) use discourse 
trees and local syntactic dependencies in a pattern-
based framework to incorporate wider context to 
refine the performance of relation extraction. They 
claimed that discourse information could filter noi-
sy dependency paths as well as increasing the reli-
ability of dependency path extraction. 
Finkel et al (2005) used Gibbs sampling, a sim-
ple Monte Carlo method used to perform approxi-
mate inference in factored probabilistic models. By 
using simulated annealing in place of Viterbi de-
coding in sequence models such as HMMs, CMMs, 
and CRFs, it is possible to incorporate non-local 
structure while preserving tractable inference. 
They used this technique to augment an informa-
tion extraction system with long-distance depend-
ency models, enforcing label consistency and 
extraction template consistency constraints. 
Ji and Grishman (2008) were inspired from the 
hypothesis of ?One Sense Per Discourse? (Ya-
                                                          
1 Selected from the file ?CNN_CF_20030304.1900.02? in 
ACE-2005 corpus. 
rowsky, 1995); they extended the scope from a 
single document to a cluster of topic-related docu-
ments and employed a rule-based approach to 
propagate consistent trigger classification and 
event arguments across sentences and documents. 
Combining global evidence from related docu-
ments with local decisions, they obtained an appre-
ciable improvement in both event and event 
argument identification. 
Patwardhan and Riloff (2009) proposed an event 
extraction model which consists of two compo-
nents: a model for sentential event recognition, 
which offers a probabilistic assessment of whether 
a sentence is discussing a domain-relevant event; 
and a model for recognizing plausible role fillers, 
which identifies phrases as role fillers based upon 
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic 
model allows the two components to jointly make 
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?. 
Gupta and Ji (2009) used cross-event informa-
tion within ACE extraction, but only for recovering 
implicit time information for events. 
Liao and Grishman (2010) propose document 
level cross-event inference to improve event ex-
traction. In contrast to Gupta?s work, Liao do not 
limit themselves to time information for events, but 
rather use related events and event-type consis-
tency to make predictions or resolve ambiguities 
regarding a given event. 
4 Motivation 
In event extraction, current transductive inference 
methods focus on the issue that many events are 
missing or spuriously tagged because the local in-
formation is not sufficient to make a confident de-
cision. The solution is to mine credible evidences 
of event occurrences from global information and 
regard that as priori knowledge to predict unknown 
event attributes, such as that of cross-document 
and cross-event inference methods.  
However, by analyzing the sentence-level base-
line event extraction, we found that the entities 
within a sentence, as the most important local in-
formation, actually contain sufficient clues for 
event detection. It is only based on the premise that 
we know the backgrounds of the entities before-
hand. For instance, if we knew the entity ?vesu-
vius? is an active volcano, we could easily identify 
1129
the word ?erupt?, which co-occurred with the en-
tity, as the trigger of a ?volcanic eruption? event 
but not that of a ?spotty rash?. 
In spite of that, it is actually difficult to use an 
entity to directly infer an event occurrence because 
we normally don?t know the inevitable connection 
between the background of the entity and the event 
attributes. But we can well use the entities of the 
same background to perform the inference. In de-
tail, if we first know entity(a) has the same back-
ground with entity(b), and we also know that 
entity(a), as a certain role, participates in a specific 
event, then we can predict that entity(b) might par-
ticiptes in a similar event as the same role. 
Consider the two sentences2 from ACE corpus: 
(5) American case for war against Saddam. 
(6) Bush should torture the al Qaeda chief op-
erations officer. 
The sentences are two event mentions which 
have the same attributes: 
Event type Attack 
Trigger war 
American Role=Attacker 
(5) 
Arguments 
Saddam Role=Target 
Event type Attack 
Trigger torture 
Bush Role=Attacker 
(6) 
Arguments 
...Qaeda chief ... Role=Target 
Table 2: Cross-entity inference example 
From the sentences, we can find that the entities 
?Saddam? and ?Qaeda chief? have the same back-
ground (viz., terrorist leader), and they are both the 
arguments of Attack events as the role of Target. 
So if we previously know any of the event men-
tions, we can infer another one with the help of the 
entities of the same background. 
In a word, the cross-entity inference, we pro-
posed for event extraction, bases on the hypothesis: 
Entities of the consistent type normally partici-
pate in similar events as the same role. 
As we will introduce below, some statistical da-
ta from ACE training corpus can support the hy-
pothesis, which show the consistency of event type 
and role in event mentions where entities of the 
same type occur. 
4.1 Entity Consistency and Distribution 
Within the ACE corpus, there is a strong entity 
consistency: if one entity mention appears in a type 
                                                          
2 They are extracted from the files ?CNN_CF_20030305.1900. 
00-1? and ?CNN_CF_20030303.1900.06-1? respectively. 
of event, other entity mentions of the same type 
will appear in similar events, and even use the 
same word to trigger the events. To see this we 
calculated the conditional probability (in the ACE 
corpus) of a certain entity type appearing in the 33 
ACE event subtypes. 
0
50
100
150
200
250
Be?Born
M
arry
D
ivorce
Injure
D
ie
Transport
Transfer?
Transfer?
Start?O
rg
M
erge?
D
eclare?
End?O
rg
A
ttack
D
em
onstr
M
eet
Phone?
Start?
End?
N
om
inate
Elect
A
rrest?Jail
Release?
Trial?
Charge?
Sue
Convict
Sentence
Fine
Execute
Extradite
A
cquit
A
ppeal
Pardon
Event typeF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 1. Conditional probability of a certain entity 
type appearing in the 33 ACE event subtypes (Here 
only the probabilities of Population-Center, Ex-
ploding and Air entities as examples) 
0
50
100
150
200
250
Person
Place
Buyer
Seller
Beneficiary
Price
A
rtifact
O
rigin
D
estination
G
iver
Recipient
M
oney
O
rg
A
gent
Victim
Instrum
ent
Entity
A
ttacker
Target
D
efendant
A
djudicator
Prosecutor
Plaintiff
Crim
e
Position
Sentence
Vehicle
Tim
e?A
fter
Tim
e?Before
Tim
e?A
t?
Tim
e?A
t?End
Tim
e?
Tim
e?
Tim
e?H
olds
Tim
e?
RoleF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 2. Conditional probability of an entity type 
appearing as the 34 ACE role types (Here only the 
probabilities of Population-Center, Exploding and 
Air entities as examples) 
As there are 33 event subtypes and 43 entity 
types, there are potentially 33*43=1419 entity-
event combinations. However, only a few of these 
appear with substantial frequency. For example, 
the Population-Center entities only occur in 4 
types of event mentions with the conditional prob-
ability more than 0.05. From Table 3, we can find 
that only Attack and Transport events co-occur 
frequently with Population-Center entities (see 
Figure 1 and Table 3). 
Event Cond.Prob. Freq. 
Transport 0.368 197 
Attack 0.295 158 
Meet 0.073 39 
Die 0.069 37 
Table 3: Events co-occurring with Population-
Center with the conditional probability > 0.05 
Actually we find that most entity types appear in 
more restricted event mentions than Population-
Center entity. For example, Air entity only co-
occurs with 5 event types (Attack, Transport, Die, 
Transfer-Ownership and Injure), and Exploding 
1130
entity co-occurs with 4 event types (see Figure 1). 
Especially, they only co-occur with one or two 
event types with the conditional probability more 
than 0.05. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 24 7 12 
Freq. >10 37 4 2 
Freq. >50 41 1 1 
Table 4: Distribution of entity-event combination 
corresponding to different co-occurrence frequency 
Table 4 gives the distributions of whole ACE 
entity types co-occurring with event types. We can 
find that there are 37 types of entities (out of 43 in 
total) appearing in less than 5 types of event men-
tions when entity-event co-occurrence frequency is 
larger than 10, and only 2 (e.g. Individual) appear-
ing in more than 10 event types. And when the fre-
quency is larger than 50, there are 41 (95%) entity 
types co-occurring with less than 5 event types. 
These distributions show the fact that most in-
stances of a certain entity type normally participate 
in events of the same type. And the distributions 
might be good predictors for event type detection 
and trigger determination. 
Air (Entity type) 
Attack 
event 
Fighter plane (subtype 1): 
?MiGs? ?enemy planes? ?warplanes? ?allied 
aircraft? ?U.S. jets? ?a-10 tank killer? ?b-1 
bomber? ?a-10 warthog? ?f-14 aircraft? 
?apache helicopter? 
Spacecraft (subtype 2): 
?russian soyuz capsule? ?soyuz? 
Civil aviation (subtype 3): 
?airliners? ?the airport? ?Hooters Air execu-
tive? 
Transport 
event 
Private plane (subtype 4): 
?Marine One? ?commercial flight? ?private 
plane? 
Table 5: Event types co-occurred with Air entities 
Besides, an ACE entity type actually can be di-
vided into more cohesive subtypes according to 
similarity of background of entity, and such a sub-
type nearly always co-occur with unique event 
type. For example, the Air entities can be roughly 
divided into 4 subtypes: Fighter plane, Spacecraft, 
Civil aviation and Private plane, within which the 
Fighter plane entities all appear in Attack event 
mentions, and other three subtypes all co-occur 
with Transport events (see Table 5). This consis-
tency of entities in a subtype is helpful to improve 
the precision of the event type predictor. 
4.2 Role Consistency and Distribution 
The same thing happens for entity-role combina-
tions: entities of the same type normally play the 
same role, especially in the event mentions of the 
same type. For example, the Population-Center 
entities occur in ACE corpus as only 4 role types: 
Place, Destination, Origin and Entity respectively 
with conditional probability 0.615, 0.289, 0.093, 
0.002 (see Figure 2). And They mainly appear in 
Transport event mentions as Place, and in Attack 
as Destination. Particularly the Exploding entities 
only occur as Instrument and Artifact respectively 
with the probability 0.986 and 0.014. They almost 
entirely appear in Attack events as Instrument. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 32 5 6 
Freq. >10 38 3 2 
Freq. >50 42 1 0 
Table 6: Distribution of entity-role combination 
corresponding to different co-occurrence frequency 
Table 6 gives the distributions of whole entity-
role combinations in ACE corpus. We can find that 
there are 38 entity types (out of 43 in total) occur 
as less than 5 role types when the entity-role co-
occurrence frequency is larger than 10. There are 
42 (98%) when the frequency is larger than 50, and 
only 2 (e.g. Individual) when larger than 10. The 
distributions show that the instances of an entity 
type normally occur as consistent role, which is 
helpful for cross-entity inference to predict roles. 
5 Cross-entity Approach  
In this section we present our approach to using 
blind cross-entity inference to improve sentence-
level ACE event extraction. 
Our event extraction system extracts events in-
dependently for each sentence, because the defini-
tion of event mention constrains them to appear in 
the same sentence. Every sentence that at least in-
volves one entity mention will be regarded as a 
candidate event mention, and a randomly selected 
entity mention from the candidate will be the star-
ing of the whole extraction process. For the entity 
mention, information retrieval is used to mine its 
background knowledge from Web, and its type is 
determined by comparing the knowledge with 
those in training corpus. Based on the entity type, 
the extraction system performs our step-by-step 
cross-entity inference to predict the attributes of 
1131
the candidate event mention: trigger, event type, 
arguments, roles and whether or not being an event 
mention. The main frame of our event extraction 
system is shown in Figure 3, which includes both 
training and testing processes. 
 
Figure 3. The frame of cross-entity inference for event extraction (including training and testing processes) 
In the training process, for every entity type in 
the ACE training corpus, a clustering technique 
(CLUTO toolkit)3 is used to divide it into different 
cohesive subtypes, each of which only contains the 
entities of the same background. For instance, the 
Air entities will be divided into Fighter plane, 
Spacecraft, Civil aviation, Private plane, etc (see 
Table 5). And for each subtype, we mine event 
mentions where this type of entities appear from 
ACE training corpus, and extract all the words 
which trigger the events to establish corresponding 
trigger list. Besides, a set of support vector ma-
chine (SVM) based classifiers are also trained: 
y Argument Classifier: to distinguish arguments 
of a potential trigger from non-arguments4; 
y Role Classifier: to classify arguments by ar-
gument role; 
y Reportable-Event Classifier (Trigger Classi-
fier): Given entity types, a potential trigger, an 
event type, and a set of arguments, to determine 
whether there is a reportable event mention. 
                                                          
3http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=h
tml&identifier=ADA439508 
4 It is noteworthy that a sentence may include more than one 
event (more than one trigger). So it is necessary to distinguish 
arguments of a potential trigger from that of others. 
In the test process, for each candidate event 
mention, our event extraction system firstly pre-
dicts its triggers and event types: given an ran-
domly selected entity mention from the candidate, 
the system determines the entity subtype it belong-
ing to and the corresponding trigger list, and then 
all non-entity words in the candidate are scanned 
for a instance of triggers from the list. When an 
instance is found, the system tags the candidate as 
the event type that the most frequently co-occurs 
with the entity subtype in the events that triggered 
by the instance. Secondly the argument classifier is 
applied to the remaining mentions in the candidate; 
for any argument passing that classifier, the role 
classifier is used to assign a role to it. Finally, once 
all arguments have been assigned, the reportable-
event classifier is applied to the candidate; if the 
result is successful, this event mention is reported. 
5.1 Further Division of Entity Type  
One of the most important pretreatments before 
our blind cross-entity inference is to divide the 
ACE entity type into more cohesive subtype. The 
greater consistency among backgrounds of entities 
in such a subtype might be good to improve the 
precision of cross-entity inference.  
1132
For each ACE entity type, we collect all entity 
mentions of the type from training corpus, and re-
gard each such mention as a query to retrieve the 
50 most relevant documents from Web. Then we 
select 50 key words that the most weighted by 
TFIDF in the documents to roughly describe back-
ground of entity. After establishing the vector 
space model (VSM) for each entity mention of the 
type, we adopt a clustering toolkit (CLUTO) to 
further divide the mentions into different subtypes. 
Finally, for each subtype, we describe its centroid 
by using 100 key words which the most frequently 
occurred in relevant documents of entities of the 
subtype. 
In the test process, for an entity mention in a 
candidate event mention, we determine its type by 
comparing its background against all centroids of 
subtypes in training corpus, and the subtype whose 
centroid has the most Cosine similarity with the 
background will be assigned to the entity. It is 
noteworthy that global information from the Web 
is only used to measure the entity-background con-
sistency and not directly in the inference process. 
Thus our event extraction system actually still per-
forms a sentence-level inference based on local 
information. 
5.2 Cross-Entity Inference 
Our event extraction system adopts a step-by-
step cross-entity inference to predict event. As dis-
cussed above, the first step is to determine the trig-
ger in a candidate event mention and tag its event 
type based on consistency of entity type. Given the 
domain of event mention that restrained by the 
known trigger, event type and entity subtype, the 
second step is to distinguish the most probable ar-
guments that co-occurring in the domain from the 
non-arguments. Then for each of the arguments, 
the third step can use the co-occurring arguments 
in the domain as important contexts to predict its 
role. Finally, the inference process determines 
whether the candidate is a reportable event men-
tion according to a confidence coefficient. In the 
following sections, we focus on introducing the 
three classifiers: argument classifier, role classifier 
and reportable-event classifier. 
5.2.1   Cross-Entity Argument Classifier 
For a candidate event mention, the first step 
gives its event type, which roughly restrains the 
domain of event mentions where the arguments of 
the candidate might co-occur. On the basis, given 
an entity mention in the candidate and its type (see 
the pretreatment process in section 5.1), the argu-
ment classifier could predict whether other entity 
mentions co-occur with it in such a domain, if yes, 
all the mentions will be the arguments of the can-
didate. In other words, if we know an entity of a 
certain type participates in some event, we will 
think of what entities also should participate in the 
event. For instance, when we know a defendant 
goes on trial, we can conclude that the judge, law-
yer and witness should appear in court. 
Argument Classifier 
Feature 1: an event type (an event-mention domain) 
Feature 2: an entity subtype 
Feature 3: entity-subtype co-occurrence in domain 
Feature 4: distance to trigger 
Feature 5: distances to other arguments 
Feature 6: co-occurrence with trigger in clause 
Role Classifier 
Feature 1 and Feature 2 
Feature 7: entity-subtypes of arguments 
Reportable-Event Classifier 
Feature 1 
Feature 8: confidence coefficient of trigger in domain 
Feature 9: confidence coefficient of role in domain 
Table 7: Features selected for SVM-based cross-
entity classifiers 
A SVM-based argument classifier is used to de-
termine arguments of candidate event mention. 
Each feature of this classifier is the conjunction of: 
y The subtype of an entity 
y The event type we are trying to assign an ar-
gument to 
y A binary indicator of whether this entity sub-
type co-occurs with other subtypes in such an 
event type (There are 266 entity subtypes, and so 
266 features for each instance) 
Some minor features, such as another binary indi-
cator of whether arguments co-occur with trigger 
in the same clause (see Table 7). 
5.2.2 Cross-Entity Role Classifier 
For a candidate event mention, the arguments 
that given by the second step (argument classifier) 
provide important contextual information for pre-
dicting what role the local entity (also one of the 
arguments) takes on. For instance, when citizens 
(Arg1) co-occur with terrorist (Arg2), most likely 
the role of Arg1 is Victim. On the basis, with the 
help of event type, the prediction might be more 
1133
precise. For instance, if the Arg1 and Arg2 co-
occur in an Attack event mention, we will have 
more confidence in the Victim role of Arg1. 
Besides, as discussed in section 4, entities of the 
same type normally take on the same role in simi-
lar events, especially when they co-occur with sim-
ilar arguments in the events (see Table 2). 
Therefore, all instances of co-occurrence model 
{entity subtype, event type, arguments} in training 
corpus could provide effective evidences for pre-
dicting the role of argument in the candidate event 
mention. Based on this, we trained a SVM-based 
role classifier which uses following features: 
y Feature 1 and Feature 2 (see Table 7) 
y Given the event domain that restrained by the 
entity and event types, an indicator of what sub-
types of arguments appear in the domain. (266 en-
tity subtypes make 266 features for each instance) 
5.2.3 Reportable-Event Classifier 
At this point, there are still two issues need to be 
resolved. First, some triggers are common words 
which often mislead the extraction of candidate 
event mention, such as ?it?, ?this?, ?what?, etc. 
These words only appear in a few event mentions 
as trigger, but when they once appear in trigger list, 
a large quantity of noisy sentences will be regarded 
as candidates because of their commonness in sen-
tences. Second, some arguments might be tagged 
as more than one role in specific event mentions, 
but as ACE event guideline, one argument only 
takes on one role in a sentence. So we need to re-
move those with low confidence. 
A confidence coefficient is used to distinguish 
the correct triggers and roles from wrong ones. The 
coefficient calculate the frequency of a trigger (or a 
role) appearing in specific domain of event men-
tions and that in whole training corpus, then com-
bines them to represent its confidence degree, just 
like TFIDF algorithm. Thus, the more typical trig-
gers (or roles) will be given high confidence. 
Based on the coefficient, we use a SVM-based 
classifier to determine the reportable events. Each 
feature of this classifier is the conjunction of: 
y An event type (domain of event mentions) 
y Confidence coefficients of triggers in domain 
y Confidence coefficients of roles in the domain. 
6 Experiments 
We followed Liao (2010)?s evaluation and ran-
domly select 10 newswire texts from the ACE 
2005 training corpus as our development set, 
which is used for parameter tuning, and then con-
duct a blind test on a separate set of 40 ACE 2005 
newswire texts. We use the rest of the ACE train-
ing corpus (549 documents) as training data for our 
event extraction system.  
To compare with the reported work on cross-
event inference (Liao, 2010) and its sentence-level 
baseline system, we cross-validate our method on 
10 separate sets of 40 ACE texts, and report the 
optimum, worst and mean performances (see Table 
8) on the data by using Precision (P), Recall (R) 
and F-measure (F). In addition, we also report the 
performance of two human annotators on 40 ACE 
newswire texts (a random blind test set): one 
knows the rules of event extraction; the other 
knows nothing about it. 
6.1 Main Results  
From the results presented in Table 8, we can 
see that using the cross-entity inference, we can 
improve the F score of sentence-level event extrac-
tion for trigger classification by 8.59%, argument 
classification by 11.86%, and role classification by 
11.9% (mean performance). Compared to the 
cross-event inference, we gains 2.87% improve-
ment for argument classification, and 3.81% for 
role classification (mean performance). Especially, 
our worst results also have better performances 
than cross-event inference. 
Nonetheless, the cross-entity inference has 
worse F score for trigger determination. As we can 
see, the low Recall score weaken its F score (see 
Table 8). Actually, we select the sentence which at 
least includes one entity mention as candidate 
event mention, but lots of event mentions in ACE 
never include any entity mention. Thus we have 
missed some mentions at the starting of inference 
process. 
In addition, the annotator who knows the rules 
of event extraction has a similar performance trend 
with systems: high for trigger classification, mid-
dle for argument classification, and low for role 
classification (see Table 8). But the annotator who 
never works in this field obtains a different trend: 
higher performance for argument classification. 
This phenomenon might prove that the step-by-
step inference is not the only way to predicate 
event mention because human can determine ar-
guments without considering triggers and event 
types. 
1134
                            Performance 
System/Human Trigger (%) Argument (%) Role (%) 
 P R F P R F P R F 
Sentence-level baseline 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46
Cross-event inference 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55
Cross-entity inference (optimum) 73.4 66.2 69.61 56.96 55.1 56 49.3 46.59 47.9 
Cross-entity inference (worst) 71.3 64.17 66.1 51.28 50.3 50.78 46.3 44.3 45.28
Cross-entity inference (mean) 72.9 64.3 68.33 53.4 52.9 53.15 51.6 45.5 48.36
Human annotation 1 (blind) 58.9 59.1 59.0 62.6 65.9 64.2 50.3 57.69 53.74
Human annotation 2 (know rules) 74.3 76.2 75.24 68.5 75.8 71.97 61.3 68.8 64.86
Table 8: Overall performance on blind test data
6.2 Influence of Clustering on Inference  
A main part of our blind inference system is the 
entity-type consistency detection, which relies 
heavily on the correctness of entity clustering and 
similarity measurement. In training, we used 
CLUTO clustering toolkit to automatically gener-
ate different types of entities based on their back-
ground-similarities. In testing, we use K-nearest 
neighbor algorithm to determine entity type. 
Fighter plane (subtype 1 in Air entities): 
?warplanes? ?allied aircraft? ?U.S. jets? ?a-10 tank killer? 
?b-1 bomber? ?a-10 warthog? ?f-14 aircraft? ?apache heli-
copter? ?terrorist? ?Saddam? ?Saddam Hussein? ?Bagh-
dad??
Table 9: Noises in subtype 1 of ?Air? entities (The 
blod fonts are noises) 
We obtained 129 entity subtypes from training 
set. By randomly inspecting 10 subtypes, we found 
nearly every subtype involves no less than 19.2% 
noises. For example, the subtype 1 of ?Air? in Ta-
ble 5 lost the entities of ?MiGs? and ?enemy 
planes?, but involved ?terrorist?, ?Saddam?, etc 
(See Table 9). Therefore, we manually clustered 
the subtypes and retry the step-by-step cross-entity 
inference. The results (denoted as ?Visible 1?) are 
shown in Table 10, within which, we additionally 
show the performance of the inference on the 
rough entity types provided by ACE (denoted as 
?Visible 2?), such as the type of ?Air?, ?Popula-
tion-Center?, ?Exploding?, etc., which normally 
can be divided into different more cohesive sub-
types. And the ?Blind? in Table 10 denotes the 
performances on our subtypes obtained by CLUTO. 
It is surprised that the performances (see Table 
10, F-score) on ?Visible 1? entity subtypes are just 
a little better than ?Blind? inference. So it seems 
that the noises in our blind entity types (CLUTO 
clusters) don?t hurt the inference much. But by re-
inspecting the ?Visible 1? subtypes, we found that 
their granularities are not enough small: the 89 
manual entity clusters actually can be divided into 
more cohesive subtypes. So the improvements of 
inference on noise-free ?Visible 1? subtypes are 
partly offset by loss on weakly consistent entities 
in the subtypes. It can be proved by the poor per-
formances on ?Visible 2? subtypes which are much 
more general than ?Visible 1?. Therefore, a rea-
sonable clustering method is important in our in-
ference process. 
F-score Trigger  Argument Role 
Blind 68.33 53.15 48.36 
Visible 1 69.15 53.65 48.83 
Visible 2 51.34 43.40 39.95 
Table 10: Performances on visible VS blind  
7 Conclusions and Future Work  
We propose a blind cross-entity inference method 
for event extraction, which well uses the consis-
tency of entity mention to achieve sentence-level 
trigger and argument (role) classification. Experi-
ments show that the method has better perform-
ance than cross-document and cross-event 
inferences in ACE event extraction. 
The inference presented here only considers the 
helpfulness of entity types of arguments to role 
classification. But as a superior feature, contextual 
roles can provide more effective assistance to role 
determination of local argument. For instance, 
when an Attack argument appears in a sentence, a 
Target might be there. So if we firstly identify 
simple roles, such as the condition that an argu-
ment has only a single role, and then use the roles 
as priori knowledge to classify hard ones, may be 
able to further improve performance.
Acknowledgments 
We thank Ruifang He. And we acknowledge the 
support of the National Natural Science Founda-
tion of China under Grant Nos. 61003152, 
60970057, 90920004. 
1135
References  
David Ahn. 2006. The stages of event extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events.Sydney, Aus-
tralia. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling. In Proc. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370, 
Ann Arbor, MI, June. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
In Proc. ACE 2005 Evaluation Workshop, Gaithers-
burg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Using 
Surface Text Features. In Proc. AAAI06 Workshop on 
Event Extraction and Synthesis. Boston, MA. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254?262, Columbus, OH, 
June. 
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event 
Extraction. In Proc. ACL-2010, pages 789-797, Upp-
sala, Sweden, July. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi resolution Framework for Information Extrac-
tion from Free Text. In Proc. 45th Annual Meeting of 
the Association of Computational Linguistics, pages 
592?599, Prague, Czech Republic, June. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. In Proc. Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 2007, pages 717?727, Prague, Czech Re-
public, June. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. Conference on Empirical 
Methods in Natural Language Processing 2009, 
(EMNLP-09). 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc. 
ACL 1995. Cambridge, MA. 
1136
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 569?573,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Effective Selection of Translation Model Training Data 
Le Liu  Yu Hong*  Hao Liu  Xing Wang  Jianmin Yao 
School of Computer Science & Technology, Soochow University, China 
{20124227052, hongy, 20134227035, 20114227047, jyao}@suda.edu.cn 
 
Abstract 
Data selection has been demonstrated to 
be an effective approach to addressing 
the lack of high-quality bitext for statisti-
cal machine translation in the domain of 
interest. Most current data selection 
methods solely use language models 
trained on a small scale in-domain data to 
select domain-relevant sentence pairs 
from general-domain parallel corpus. By 
contrast, we argue that the relevance be-
tween a sentence pair and target domain 
can be better evaluated by the combina-
tion of language model and translation 
model. In this paper, we study and exper-
iment with novel methods that apply 
translation models into domain-relevant 
data selection. The results show that our 
methods outperform previous methods. 
When the selected sentence pairs are 
evaluated on an end-to-end MT task, our 
methods can increase the translation per-
formance by 3 BLEU points.* 
1 Introduction 
Statistical machine translation depends heavily 
on large scale parallel corpora. The corpora are 
necessary priori knowledge for training effective 
translation model. However, domain-specific 
machine translation has few parallel corpora for 
translation model training in the domain of inter-
est. For this, an effective approach is to automat-
ically select and expand domain-specific sen-
tence pairs from large scale general-domain par-
allel corpus. The approach is named Data Selec-
tion. Current data selection methods mostly use 
language models trained on small scale in-
domain data to measure domain relevance and 
select domain-relevant parallel sentence pairs to 
expand training corpora. Related work in litera-
ture has proven that the expanded corpora can 
substantially improve the performance of ma-
                                                 
* Corresponding author 
chine translation (Duh et al, 2010; Haddow and 
Koehn, 2012). 
However, the methods are still far from satis-
factory for real application for the following rea-
sons: 
? There isn?t ready-made domain-specific 
parallel bitext. So it?s necessary for data se-
lection to have significant capability in min-
ing parallel bitext in those assorted free texts. 
But the existing methods seldom ensure 
parallelism in the target domain while se-
lecting domain-relevant bitext. 
? Available domain-relevant bitext needs keep 
high domain-relevance at both the sides of 
source and target language. But it?s difficult 
for current method to maintain two-sided 
domain-relevance when we aim at enhanc-
ing parallelism of bitext.   
In a word, current data selection methods can?t 
well maintain both parallelism and domain-
relevance of bitext. To overcome the problem, 
we first propose the method combining transla-
tion model with language model in data selection. 
The language model measures the domain-
specific generation probability of sentences, be-
ing used to select domain-relevant sentences at 
both sides of source and target language. Mean-
while, the translation model measures the trans-
lation probability of sentence pair, being used to 
verify the parallelism of the selected domain-
relevant bitext. 
2 Related Work 
The existing data selection methods are mostly 
based on language model. Yasuda et al (2008) 
and Foster et al (2010) ranked the sentence pairs 
in the general-domain corpus according to the 
perplexity scores of sentences, which are com-
puted with respect to in-domain language models. 
Axelrod et al (2011) improved the perplexity-
based approach and proposed bilingual cross-
entropy difference as a ranking function with in- 
and general- domain language models. Duh et al 
(2013) employed the method of (Axelrod et al, 
569
2011) and further explored neural language mod-
el for data selection rather than the conventional 
n-gram language model. Although previous 
works in data selection (Duh et al, 2013; Koehn 
and Haddow, 2012; Axelrod et al, 2011; Foster 
et al, 2010; Yasuda et al, 2008) have gained 
good performance, the methods which only 
adopt language models to score the sentence 
pairs are sub-optimal. The reason is that a sen-
tence pair contains a source language sentence 
and a target language sentence, while the existing 
methods are incapable of evaluating the mutual 
translation probability of sentence pair in the tar-
get domain. Thus, we propose novel methods 
which are based on translation model and lan-
guage model for data selection. 
3 Training Data Selection Methods 
We present three data selection methods for 
ranking and selecting domain-relevant sentence 
pairs from general-domain corpus, with an eye 
towards improving domain-specific translation 
model performance. These methods are based on 
language model and translation model, which are 
trained on small in-domain parallel data.  
3.1 Data Selection with Translation Model 
Translation model is a key component in statisti-
cal machine translation. It is commonly used to 
translate the source language sentence into the 
target language sentence. However, in this paper, 
we adopt the translation model to evaluate the 
translation probability of sentence pair and de-
velop a simple but effective variant of translation 
model to rank the sentence pairs in the general-
domain corpus. The formulations are detailed as 
below: 
 (   )  
 
(    )
  
? ?  (     )
  
   
  
       (1) 
  ? (   )
  
       (2) 
Where  (   ) is the translation model, which is 
IBM Model 1 in this paper, it represents the 
translation probability of target language sen-
tence   conditioned on source language sentence 
 .    and    are the number of words in sentence 
  and  respectively.  (     )  is the translation 
probability of word    conditioned on word   and 
is estimated from the small in-domain parallel 
data. The parameter   is a constant and is as-
signed with the value of 1.0.   is the length-
normalized IBM Model 1, which is used to score 
general-domain sentence pairs. The sentence pair 
with higher score is more likely to be generated 
by in-domain translation model, thus, it is more 
relevant to the in-domain corpus and will be re-
mained to expand the training data.  
3.2 Data Selection by Combining Transla-
tion and Language model  
As described in section 1, the existing data selec-
tion methods which only adopt language model 
to score sentence pairs are unable to measure the 
mutual translation probability of sentence pairs. 
To solve the problem, we develop the second 
data selection method, which is based on the 
combination of translation model and language 
model. Our method and ranking function are 
formulated as follows: 
   (   )   (   )   ( )        (3) 
    ? (   )
  
 ? ( )
  
             (4) 
Where  (   ) is a joint probability of sentence   
and   according to the translation model  (   ) 
and language model  ( ), whose parameters are 
estimated from the small in-domain text.   is the 
improved ranking function and used to score the 
sentence pairs with the length-normalized trans-
lation model  (   )and language model  ( ). 
The sentence pair with higher score is more simi-
lar to in-domain corpus, and will be picked out.  
3.3 Data Selection by Bidirectionally   
Combining Translation and Language 
Models  
As presented in subsection 3.2, the method com-
bines translation model and language model to 
rank the sentence pairs in the general-domain 
corpus. However, it does not evaluate the inverse 
translation probability of sentence pair and the 
probability of target language sentence. Thus, we 
take bidirectional scores into account and simply 
sum the scores in both directions.  
  ? (   )
  
 ? ( )
  
 ? (   )
  
 ? ( )
  
 
 (5) 
Again, the sentence pairs with higher scores are 
presumed to be better and will be selected to in-
corporate into the domain-specific training data. 
This approach makes full use of two translation 
models and two language models for sentence 
pairs ranking. 
570
4 Experiments 
4.1 Corpora 
We conduct our experiments on the Spoken Lan-
guage Translation English-to-Chinese task. Two 
corpora are needed for the data selection. The in-
domain data is collected from CWMT09, which 
consists of spoken dialogues in a travel setting, 
containing approximately 50,000 parallel sen-
tence pairs in English and Chinese. Our general-
domain corpus mined from the Internet contains 
16 million sentence pairs. Both the in- and gen-
eral- domain corpora are identically tokenized (in 
English) and segmented (in Chinese)1. The de-
tails of corpora are listed in Table 1. Additionally, 
we evaluate our work on the 2004 test set of 
?863? Spoken Language Translation task (?863? 
SLT), which consists of 400 English sentences 
with 4 Chinese reference translations for each. 
Meanwhile, the 2005 test set of ?863? SLT task, 
which contains 456 English sentences with 4 ref-
erences each, is used as the development set to 
tune our systems.  
Bilingual Cor-
pus 
#sentence #token 
Eng Chn Eng Chn 
In-domain 50K 50K 360K 310K 
General-domain 16M 16M 3933M 3602M 
Table 1. Data statics 
4.2 System settings 
We use the NiuTrans 2  toolkit which adopts 
GIZA++ (Och and Ney, 2003) and MERT (Och, 
2003) to train and tune the machine translation 
system. As NiuTrans integrates the mainstream 
translation engine, we select hierarchical phrase-
based engine (Chiang, 2007) to extract the trans-
lation rules and carry out our experiments. 
Moreover, in the decoding process, we use the 
NiuTrans decoder to produce the best outputs, 
and score them with the widely used NIST mt-
eval131a3  tool. This tool scores the outputs in 
several criterions, while the case-insensitive 
BLEU-4 (Papineni et al, 2002) is used as the 
evaluation for the machine translation system. 
4.3 Translation and Language models 
Our work relies on the use of in-domain lan-
guage models and translation models to rank the 
sentence pairs from the general-domain bilingual 
training set. Here, we employ ngram language 
                                                 
1http://www.nlplab.com/NiuPlan/NiuTrans.YourData.ch.html 
2http://www.nlplab.com/NiuPlan/NiuTrans.ch.html#download 
3 http://ww.itl.nist.gov/iad/mig/tools 
model and IBM Model 1 for data selection. Thus, 
we use the SRI Language Modeling Toolkit 
(Stolcke, 2002) to train the in-domain 4-gram 
language model with interpolated modified 
Kneser-Ney discounting (Chen and Goodman, 
1998). The language model is only used to score 
the general-domain sentences. Meanwhile, we 
use the language model training scripts integrat-
ed in the NiuTrans toolkit to train another 4-gram 
language model, which is used in MT tuning and 
decoding. Additionally, we adopt GIZA++ to get 
the word alignment of in-domain parallel data 
and form the word translation probability table. 
This table will be used to compute the translation 
probability of general-domain sentence pairs.  
4.4 Baseline Systems 
As described above, by using the NiuTrans 
toolkit, we have built two baseline systems to 
fulfill ?863? SLT task in our experiments. The 
In-domain baseline trained on spoken language 
corpus has 1.05 million rules in its hierarchical-
phrase table. While, the General-domain baseline 
trained on 16 million sentence pairs has a hierar-
chical phrase table containing 1.7 billion transla-
tion rules. These two baseline systems are 
equipped with the same language model which is 
trained on large-scale monolingual target lan-
guage corpus. The BLEU scores of the In-
domain and General-domain baseline system are 
listed in Table 2.  
Corpus 
Hierarchical 
phrase 
Dev Test 
In-domain 1.05M 15.01 21.99 
General-domain 1747M 27.72 34.62 
Table 2. Translation performances of In-domain and 
General-domain baseline systems 
The results show that General-domain system 
trained on a larger amount of bilingual resources 
outperforms the system trained on the in-domain 
corpus by over 12 BLEU points. The reason is 
that large scale parallel corpus maintains more 
bilingual knowledge and language phenomenon, 
while small in-domain corpus encounters data 
sparse problem, which degrades the translation 
performance. However, the performance of Gen-
eral-domain baseline can be improved further. 
We use our three methods to refine the general-
domain corpus and improve the translation per-
formance in the domain of interest. Thus, we 
build several contrasting systems trained on re-
fined training data selected by the following dif-
ferent methods.  
571
? Ngram: Data selection by 4-gram LMs with 
Kneser-Ney smoothing. (Axelrod et al, 
2011) 
? Neural net: Data selection by Recurrent 
Neural LM, with the RNNLM Tookit. (Duh 
et al, 2013) 
? Translation Model (TM): Data selection 
with translation model: IBM Model 1. 
? Translation model and Language Model 
(TM+LM): Data selection by combining 4-
gram LMs with Kneser-Ney smoothing and 
IBM model 1(equal weight).  
? Bidirectional TM+LM: Data selection by 
bidirectionally combining translation and 
language models (equal weight).  
4.5 Results of Training Data Selection 
We adopt five methods for extracting domain-
relevant parallel data from general-domain cor-
pus. Using the scoring methods, we rank the sen-
tence pairs of the general-domain corpus and 
select only the top N = {50k, 100k, 200k, 400k, 
600k, 800k, 1000k} sentence pairs as refined 
training data. New MT systems are then trained 
on these small refined training data. Figure 1 
shows the performances of systems trained on 
selected corpora from the general-domain corpus. 
The horizontal coordinate represents the number 
of selected sentence pairs and vertical coordinate 
is the BLEU scores of MT systems.  
 
Figure 1. Results of the systems trained on only a sub-
set of the general-domain parallel corpus. 
From Figure 1, we conclude that these five da-
ta selection methods are effective for domain-
specific translation. When top 600k sentence 
pairs are picked out from general-domain corpus 
to train machine translation systems, the systems 
perform higher than the General-domain baseline 
trained on 16 million parallel data. The results 
indicate that more training data for translation 
model is not always better. When the domain-
specific bilingual resources are deficient, the 
domain-relevant sentence pairs will play an im-
portant role in improving the translation perfor-
mance.  
Additionally, it turns out that our methods 
(TM, TM+LM and Bidirectional TM+LM) are 
indeed more effective in selecting domain-
relevant sentence pairs. In the end-to-end SMT 
evaluation, TM selects top 600k sentence pairs 
of general-domain corpus, but increases the 
translation performance by 2.7 BLEU points. 
Meanwhile, the TM+LM and Bidirectional 
TM+LM have gained 3.66 and 3.56 BLEU point 
improvements compared against the general-
domain baseline system. Compared with the 
mainstream methods (Ngram and Neural net), 
our methods increase translation performance by 
nearly 3 BLEU points, when the top 600k sen-
tence pairs are picked out. Although, in the fig-
ure 1, our three methods are not performing bet-
ter than the existing methods in all cases, their 
overall performances are relatively higher. We 
therefore believe that combining in-domain 
translation model and language model to score 
the sentence pairs is well-suited for domain-
relevant sentence pair selection. Furthermore, we 
observe that the overall performance of our 
methods is gradually improved. This is because 
our methods are combining more statistical char-
acteristics of in-domain data in ranking and se-
lecting sentence pairs. The results have proven 
the effectiveness of our methods again. 
5 Conclusion 
We present three novel methods for translation 
model training data selection, which are based on 
the translation model and language model. Com-
pared with the methods which only employ lan-
guage model for data selection, we observe that 
our methods are able to select high-quality do-
main-relevant sentence pairs and improve the 
translation performance by nearly 3 BLEU points. 
In addition, our methods make full use of the 
limited in-domain data and are easily implement-
ed. In the future, we are interested in applying 
20.00
22.00
24.00
26.00
28.00
30.00
32.00
34.00
36.00
38.00
40.00
0 200 400 600 800 1000
Axelord et al(2011) Duh et al(2013)
TM TM+LM
Bidirectional TM+LM
572
our methods into domain adaptation task of sta-
tistical machine translation in model level. 
Acknowledgments 
This research work has been sponsored by two 
NSFC grants, No.61373097 and No.61272259, 
and one National Science Foundation of Suzhou 
(Grants No. SH201212).  
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages 355?362, Edinburgh, Scotland, 
UK, July. Association for Computational Linguis-
tics. 
Peter F.Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: Pa-
rameter estimation. Computational linguistics, 
1993, 19(2): 263-311. 
Stanley Chen and Joshua Goodman. 1998. An Empir-
ical Study of Smoothing Techniques for Language 
Modeling. Technical Report 10-98, Computer Sci-
ence Group, Harvard University.  
Moore Robert C, Lewis William. 2010. Intelligent 
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Pa-
pers. Association for Computational Linguistics, 
2010: 220-224. 
Chiang David. A hierarchical phrase-based model for 
statistical machine translation. 2005. In Proceed-
ings of the 43rd Annual Meeting on Association for 
Computational Linguistics, pages: 263-270. Asso-
ciation for Computational Linguistics. 
Kevin Duh, Graham Neubig, Katsuhito Sudoh and  
Hajime Tsukada. Adaptation Data Selection using 
Neural Language Models: Experiments in Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational Lin-
guistics, pages 678-683, Sofia, Bulgaria, August 4-
9 2013.  
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 
2010.Analysis of translation model adaptation for 
statistical machine translation. In Proceedings of 
the International Workshop on Spoken Language 
Translation (IWSLT) - Technical Papers Track.  
George Foster, Cyril Goutte, and Roland Kuhn. 2010. 
Discriminative Instance Weighting for Domain 
Adaptation in Statistical Machine Translation. Em-
pirical Methods in Natural Language Processing. 
Barry Haddow and Philipp Koehn. 2012. Analysing 
the effect of out-of-domain data on smt systems. In 
Proceedings of the Seventh Workshop on Statistical 
Machine Translation, pages 422?432, Montreal, 
Canada, June. Association for Computational Lin-
guistics. 
Och, Franz Josef, and Hermann Ney. A systematic 
comparison of various statistical alignment models. 
Computational linguistics 29.1 (2003): 19-51.  
Och, Franz Josef. Minimum error rate training in sta-
tistical machine translation. Proceedings of the 41st 
Annual Meeting on Association for Computational 
Linguistics-Volume 1. Association for Computa-
tional Linguistics, 2003.  
Philipp Koehn and Barry Haddow. 2012. Towards 
effective use of training data in statistical machine 
translation. In WMT. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A method for auto-
matic evaluation of machine translation. In ACL.  
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. Spoken Language Pro-
cessing. 
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 
NiuTrans: an open source toolkit for phrase-based 
and syntax-based machine translation. In Proceed-
ings of the ACL 2012 System Demonstrations. As-
sociation for Computational Linguistics, 2012: 19-
24. 
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, 
and Eiichiro Sumita. 2008. Method of selecting 
training data to build a compact and efficient trans-
lation model. International Joint Conference on 
Natural Language Processing.  
573
Jumping Distance based Chinese Person Name Disambiguation1
Yu Hong  Fei Pei  Yue-hui Yang  Jian-min Yao  Qiao-ming Zhu 
School of Computer Science and Technology, Soochow University 
No.1 Shizi street, Suzhou City, Jiansu Province, China 
{hongy, 20094527004, 0727401137, jyao, qmzhu}@suda.edu.cn
Abstract
In this paper, we describe a Chinese person 
name disambiguation system for news articles 
and report the results obtained on the data set of 
the CLP 2010 Bakeoff-31. The main task of the 
Bakeoff is to identify different persons from the 
news stories that contain the same person-name 
string. Compared to the traditional methods, 
two additional features are used in our system: 
1) n-grams co-occurred with target name string; 
2) Jumping distance among the n-grams. On the 
basis, we propose a two-stage clustering algo-
rithm to improve the low recall.
1   Our Novel Try
For this task, we propose a Jumping-Distance 
based n-gram model (abbr. DJ n-gram) to de-
scribe the semantics of the closest contexts of 
the target person-name strings. 
The generation of the DJ n-gram model 
mainly involves two steps. First, we mine the 
Jumping tree for the target string; second, we 
give the statistical description of the tree. 
z Jumping Tree 
Given a target string, we firstly extract the 
sentence where it locates as its closest context. 
Then we segment the sentence into n-
grams(Chen et al ,2009) (only Bi-gram and Tri-
gram are used in this paper). For each n-gram, 
we regard it as the beginning of a jumping jour-
ney. And the places where we jump are the sen-
tences which involve the n-gram. By the same 
way, we segment the sentences into n-grams 
which will be regarded as the new beginnings to 
open further jumping. The procedure will run 
iteratively until there are no sentences in the 
document (viz. the document which involves 
the target string) can be used to jump. Actually, 
we find there are only 3 jumps in average in our 
previous test and simultaneously 11 sentences 
in a document can be involved into the jumping 
journey. Thus, we can obtain a Jumping Tree 
where each jumping route from the initially n-
gram (viz. the gram in the closes context) refer 
to a branch. And for each intermediate node, its 
child-nodes are the n-grams co-occurred with it 
in the same sentences. 
The motivation to generate the Jumping Tree 
is to imitate the thinking model of human rec-
ognizing the word senses and semantics. In de-
tail, for each intermediate node of the tree, its 
child-nodes all come from its closest contexts, 
especially the nodes co-occur with it in the 
same sentences which involve the real grammar 
and semantic relations. Thus the child-nodes 
normally provide the natural inference for its 
word sense. For example, given the string 
?SARS?, we can deduce its sense from its child 
nodes ?Severe?, ?Acute?, ?Respiratory? and 
?Syndromes? even if we see the string for the 
first time. On the basis, the procedure of infer-
ence run iteratively, that is, the tree always use 
the child nodes deduce the meaning of their fa-
ther nodes then further ancestor nodes until the 
root. Thus the tree acts as a hierarchical under-
standing procedure. Additionally, the distances 
among nodes in the tree give the degree of se-
mantic relation.  
In the task of person-name disambiguation, 
we use the Jumping Tree to deduce the identi-
ties and backgrounds of a person. Each branch 
of the tree refers to a property of the person. 
z Jumping-Distance based n-gram model 
In this paper, we give a simple statistical 
model to describe the Jumping Tree. Given a 
node in the tree (viz. an n-gram), we record the Supported by the National Natural Science Foundation 
of China under Grant No. 60970057, No.60873105.
steps jumping from the root to it, viz. the depth 
of the node in the tree. Then based on the priori-
trained TFIDF value, we calculate the genera-
tion probability of the node as follows: 
depth
TFP D? 
where the D  denotes the smoothing factor.
In fact, we create more comprehensive mod-
els to describe the semantic correlations among 
the nodes in the Jumping Tree. The models well 
use the distances among the nodes in local 
Jumping Tree (viz. the tree generated based on 
the test document) and that normalized on the 
large-scale training data to calculate the prob-
ability of n-grams correboratively generate a 
semantics. They try to imitate the thinking 
model of human combine differents features to 
understand panoramic knowledge. In the task of 
name disambiguation, we can use the models to 
improve the distinguishment of different per-
sons who have the same name. And we have 
illustrate the well effectiveness on the topic de-
scription and relevance measurement in other 
tasks, such as Link Detection. But we actually 
didn?t use the models to perform the task of 
name diaambiguation this time with the aim to 
purely evaluate the usefulness of the Jumping 
Tree.
2    Systems
For the task of Chinese person name disam-
biguation, we submitted two systems as follows: 
z System1 
The system involves two main components: 
DJ-based name Identification error detection 
and DJ-based person name disambiguation. 
The first component, viz. DJ-based name 
segmentation error detection, aims to distin-
guish the target string referring to person name 
from that referring to something else. Such as, 
the string ???? can be a person name ?Hai 
Huang? but also a name of sea ?the Yellow 
Sea?. And the detection component focuses on 
obtaining the pure person name ?Hai Huang?. 
The detection component firstly establish two 
classes of features which respectively describe 
the nature of human and that of things. Such as, 
the features ?professor?, ?research?, ?honest? et 
al., can roughly be determined as the nature of 
human, and conversely the features ?solid?, 
?collapse?, ?deep? et al, can be that of things. 
For obtaining the features, we extract 10,000 
documents that discuss person, eg. ?Albert Ein-
stein? and 6000 documents that discuss tech-
nology, science, geography, et.al., from 
Wikipedia2. For each document, we generate its 
Jumping Tree, and regard the nodes in the tree 
as the features. After that, we combine the 
weights of the same features and normalized the 
value by dividing that by the average weight in 
the specific class of features. 
Based on the two classes of features, given a 
target string and the document where it occurs, 
the detection component firstly generate the 
Jumping Tree of the document, and then deter-
mines whether the string is person name or 
things by measuring the similarity of  the tree to 
the classes of features. Here, we simply use the 
VSM and Cosine metric ?Bagga and Baldwin, 
1998? to obtain the similarity. 
The second component, viz. DJ-based person 
name disambiguation, firstly generates the 
Jumping trees for all documents that involve 
specific person name. And a two-stage cluster-
ing algorithm is adopted to divide the docu-
ments and refer each cluster to a person. The 
first stage of the algorithm runs a strict division 
which focuses on obtaining high precision. The 
second stage performs a soft division which is 
used to improve recall. The two-stage clustering 
algorithm(Ikeda et al,2009) initially obtains the 
optimal parameters that respectively refer to the 
maximum precision and recall based on training 
data, and then regards a statistical tradeoff as 
the final value of the parameters. Here, the Af-
finity Propagation clustering tools (Frey BJ and 
Dueck D, 2007) is in use. 
z System2 
The system is similar to the system1 except 
that it additionally involve Named Entity Identi-
fication (Artiles et.al,2009B; Popescu,O. and 
Magnini, B.,2007)before the two-stage cluster-
ing in the component of person name disam-
biguation. In detail, given a person name and 
the documents that it occurs in, the disambigua-
tion component of System2 firstly adopt NER 
CRF++ toolkit3  provided by MSRA to identify 
Named Entities(Chen et al, 2006) that involve 
the given name string, such as the entity ???
?? (viz. Gao-ming Li in English) when given 
the target name string ????(viz. Ming Gao in 
English). Thus the documents can be roughly 
divided into different clusters of Named Entities 
without name segmentation errors. After that, 
we additionally adopt the two-stage clustering 
algorithm to further divide each cluster. Thus 
we can deal with the issue of disambiguation 
without the interruption of name segmentation 
errors.
3   Data sets 
z Training dataset: They contain about 30 
Chinese personal names, and a document set of 
about 100-300 news articles from collection of 
Xinhua news documents in a time span of four-
teen years are provided for each personal name. 
z External dataset: Chinese Wikipedia2 per-
sonal attribution (Cucerzan, 2007; Nguyen and 
Cao,2008).
z Test dataset: There are about 26 Chinese 
personal names, which are similar to train data 
sets.
4     Experiments 
The systems that run on test dataset are evalu-
ated by both B-Cubed (Bagga and  Baldwin, 
1998; Artiles et al,2009A) and P-IP (Artiles  et 
al., 2007 ;Artiles et al,2009A). And the systems 
that run on training dataset were only evaluated 
by B-Cubed. 
In experiments, we firstly evaluate the per-
formance of name segmentation error detection 
on the training dataset. For comparison, we ad-
ditionally perform another detection method 
which only using Name Entity Identifcation 
(NER CRF++ tools) to distinguish name-strings 
from the discarded ones. The results are shown 
in table 1. We can find that our error detection 
method can achieve more recall than NER, but 
lower precision. 
Besides, we evaluate the performance of the 
two-stage clustering in the component of name 
disambiguation step by step. Four steps are in 
use to evaluate the first-stage clustering method 
as follows: 
z DJ2
This step look like to run the system1 men-
tionedin in section 3 which don?t involve the 
prior-division of documents by using NER be-
fore the first-stage clustering in the component 
of name disambiguation. Especially it don?t 
perform the second-stage clustering to improve 
the recall probability. 
z DJ2+NER
This step is similar to the step of DJ2 men-
tioned above except that it perform the prior-
divison of documents by using NER. 
z NER+DJ 
This step is also similar to the step of DJ2 ex-
cept that its name segmentation error detection 
performs by using the NER. 
z NER2+DJ
This step is similar to the step of NER+DJ 
except that it involve the treatment of prior-
divison as that in DJ2+NER.
The performances of the four steps are shown 
in table 2. We can find that all steps achieve 
poor recall. And the step of DJ2 achieve the best 
F-score although it don?t involve the prior-
division. That is because NER is helpful to im-
prove precision but not recall, as shown in table 
1. Conversely, DJ2 can avoid the bias caused by 
the procedure of greatly maximizing the preci-
sion.
P recall F-score
DJ-based 0.62 0.81 0.70
NER-based 0.91 0.77 0.71
Table 1: Performance of name segmentation 
error detection 
P IP F-score
DJ2 80.49 53.85 60.12 
DJ2+NER 88.56 51.30 59.02 
NER+DJ 93.27 46.78 57.44 
NER2+DJ 97.79 42.13 55.47 
Table 2: Performances of the-stage clustering 
Additionally, another two steps are used to 
evluate the both two stages of clustering in 
name disambiguation. The steps are as follows: 
z DJ2+NER_2
This step is similar to the step of DJ2+NER 
except that it additionally run the second-stage 
clustering to improve recall. 
z NER2+DJ_2
This step also run the second-stage clustering 
on the basis of NER2+DJ. 
The performances of the two step are shown 
in table 3. We can find that the F-scores both 
have been improved substantially. And the two 
steps still maintain the original distribution be-
tween precision and recall. That is, the 
DJ2+NER_2, which has outperformance on re-
call in the name segmentation error detection, 
still maintain the higher recall at the second-
stage clustering. And NER2+DJ_2 also main-
tains higher precision. This illustrates that the 
clustering has no ability to remedy the short-
comings of NER in the prior-division. 
P IP F-score 
DJ2+NER_2 82.65 63.40    66.59 
NER2+DJ_2 87.71 60.45 66.23 
Table 3: Performances of two-stage clustering 
The test results of the two systems mentioned in 
section 3 are shown in the table 4. We also 
show the performances of each stage clustering 
as that on training dataset. We can find that the 
poor performance mainly come from the low 
recall, which illustrates that the DJ-based n-
gram disambiguation is not robust. 
B-Cubed
precision recall F-Score
System1(one 
t )
85.26 28.43 37.74
System1(both 
t )
84.51 44.17 51.42
P-IP
P IP F-Score
System2(one 
t )
88.4 39.47 50.52
System2(both 
t )
88.36 55.23 63.89
Table 4 :Test results 
5.Conclusions
In this paper, we report a hybrid Chinese per-
sonal disambiguation system and a novel algo-
rithm for extract useful global n-gram features 
from the context .Experiment showed that our 
algorithm performed high precision and poor 
recall. Furthermore, two-stage clustering can 
handl a change in the one-stage clustering algo-
rithm, especially for recall score. In the future, 
we will investigate global new types of features 
to improve the recall score and local new types 
of features to improve the precision score. For 
instance, the location and organization besides 
the person in the named-entities. And we try to 
use Hierarchical Agglomerative Clustering al-
gorithm to help raise the recall score.
References 
Artiles J, J Gonzalo and S Sekine. 2007. The 
SemEval-2007 WePS Evaluation: ?Establish-
ing a benchmark for the Web People Search 
Task.?, The SemEval-2007, 64-69, Associa-
tion for Computational Linguistics.
Artiles Javier, Julio Gonzalo and Satoshi Se-
kine.2009A. ?WePS 2 Evaluation Campaign: 
overview of the Web People Search Cluster-
ing Task,? In 2nd Web People Search 
Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Artiles J, E Amig?o and J Gonzalo. 2009B.The 
Role of Named Entities in Web People 
Search. Proceedings of the 2009 Conference 
on Empirical Methods Natural Language 
Processing, 534?542,Singapore, August 2009.  
Bagga A and Baldwin B. 1998. Entity-based 
cross-document coreferenceing using the 
Vector Space Model.Proceedings of the 17th
international conference on computational 
linguistics. Volume 1, 79-85. 
Chen,Ying., Sophia Yat., Mei Lee and Chu-Ren 
Huang. 2009. PolyUHK:A Roubust Informa-
tion Extraction System for Web Personal 
Names In 2nd Web People Search Evaluation 
Workshop (WePS 2009), 18th WWW Con-
ference.
Chen Wen-liang, Zhang Yu-jie. 2006. Chinese 
Named Entity Recognition with Conditional 
Random Fields. Proceedings of the Fifth 
SIGHAN Workshop on Chinese Language 
Processing.
Cucerzan, Silviu. 2007. Large scale named en-
tity Disambiguation based on Wikipedia data. 
In The EMNLP-CoNLL-2007. 
Frey BJ and Dueck D. 2007. Clustering by 
Passing Messages Between Data 
Points .science, 2007 - sciencemag.org. 
Ikeda MS, Ono I, Sato MY and Nakagawa H. 
2009. Person Name disambiguation on the 
Web by Two-Stage Clustering. In 2nd Web 
People Search Evaluation Workshop(WePS 
2009),18th WWW Conference.
Popescu,O and Magnini, B. 2007. IRST-
BP:Web People Search Using Name Enti-
ties.Proceeding s of the 4th International 
Workshop on Semantic Evaluations (SemE-
val-2007), 195-198, Prague June 2007. Asso-
ciation for Computational Linguistics. 
