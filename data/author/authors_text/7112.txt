A Word-Order Database for Testing  
Computational Models of Language Acquisition 
  William Gregory Sakas 
Department of Computer Science 
PhD Programs in Linguistics and Computer Science 
Hunter College and The Graduate Center 
City University of New York 
sakas@hunter.cuny.edu 
 
 
 
 
Abstract 
An investment of effort over the last two 
years has begun to produce a wealth of 
data concerning computational psycholin-
guistic models of syntax acquisition. The 
data is generated by running simulations 
on a recently completed database of word 
order patterns from over 3,000 abstract 
languages. This article presents the design 
of the database which contains sentence 
patterns, grammars and derivations that 
can be used to test acquisition models from 
widely divergent paradigms. The domain 
is generated from grammars that are lin-
guistically motivated by current syntactic 
theory and the sentence patterns have been 
validated as psychologically/developmen-
tally plausible by checking their frequency 
of occurrence in corpora of child-directed 
speech. A small case-study simulation is 
also presented. 
1 Introduction 
The exact process by which a child acquires the 
grammar of his or her native language is one of the 
most beguiling open problems of cognitive 
science. There has been recent interest in computer 
simulation of the acquisition process and the 
interrelationship between such models and linguis-
tic and psycholinguistic theory. The hope is that 
through computational study, certain bounds can 
be established which may be brought to bear on 
pivotal issues in developmental psycholinguistics.   
Simulation research is a significant departure 
from standard learnability models that provide 
results through formal proof (e.g., Bertolo, 2001; 
Gold, 1967; Jain et al, 1999; Niyogi, 1998; Niyogi 
& Berwick, 1996; Pinker, 1979; Wexler & Culi-
cover, 1980, among many others). Although 
research in learnability theory is valuable and 
ongoing, there are several disadvantages to formal 
modeling of language acquisition:  
? Certain proofs may involve impractically many 
steps for large language domains (e.g. those 
involving Markov methods). 
? Certain paradigms are too complex to readily 
lend themselves to deductive study (e.g. con-
nectionist models).1 
? Simulations provide data on intermediate stages 
whereas formal proofs typically prove whether 
a domain is (or more often is not) learnable a 
priori to specific trials. 
? Proofs generally require simplifying assump-
tions which are often distant from natural lan-
guage.  
However, simulation studies are not without 
disadvantages and limitations. Most notable 
perhaps, is that out of practicality, simulations are 
typically carried out on small, severely circum-
scribed domains ? usually just large enough to 
allow the researcher to hone in on how a particular 
model (e.g. a connectionist network or a principles 
& parameters learner) handles a few grammatical 
features (e.g. long-distance agreement and/or 
topicalization) often, though not always, in a single 
language. So although there have been many 
successful studies that demonstrate how one 
algorithm or another is able to acquire some aspect 
of grammatical structure, there is little doubt that 
the question of what mechanism children actually 
employ during the acquisition process is still open. 
This paper reports the development of a large, 
multilingual database of sentence patterns, gram-
                                                          
1 Although see Niyogi, 1998 for some insight. 
mars and derivations that may be used to test 
computational models of syntax acquisition from 
widely divergent paradigms. The domain is 
generated from grammars that are linguistically 
motivated by current syntactic theory and the 
sentence patterns have been validated as psycho-
logically/developmentally plausible by checking 
their frequency of occurrence in corpora of child-
directed speech. We report here the structure of the 
domain, its interface and a case-study that demon-
strates how the domain has been used to test the 
feasibility of several different acquisition strate-
gies.  
The domain is currently publicly available on 
the web via http://146.95.2.133 and it is our hope 
that it will prove to be a valuable resource for 
investigators interested in computational models of 
natural language acquisition. 
2 The Language Domain Database 
The focus of the language domain database, 
(hereafter LDD), is to make readily available the 
different word order patterns that children are 
typically exposed to, together with all possible 
syntactic derivations of each pattern. The patterns 
and their derivations are generated from a large 
battery of grammars that incorporate many features 
from the domain of natural language. 
At this point the multilingual language domain 
contains sentence patterns and their derivations 
generated from 3,072 abstract grammars. The 
patterns encode sentences in terms of tokens 
denoting the grammatical roles of words and 
complex phrases, e.g., subject (S), direct object 
(O1), indirect object (O2), main verb (V), auxiliary 
verb (Aux), adverb (Adv), preposition (P), etc. An 
example pattern is S Aux V O1 which corresponds 
to the English sentence: The little girl can make a 
paper airplane. There are also tokens for topic and 
question markers for use when a grammar specifies 
overt topicalization or question marking. 
Declarative sentences, imperative sentences, 
negations and questions are represented within the 
LDD, as is prepositional movement/stranding 
(pied-piping), null subjects, null topics, topicaliza-
tion and several types of movement. 
Although more work needs to be done, a first 
round study of actual child-directed sentences from 
the CHILDES corpus (MacWhinney, 1995) 
indicates that our patterns capture many sentential 
word orders that children typically encounter in the 
period from 1-1/2 to 2-1/2 years; the period 
generally accepted by psycholinguists to be when 
children establish the correct word order of their 
native language. For example, although the LDD is 
currently limited to degree-0 (i.e. no embedding) 
and does not contain DP-internal structure, after 
examining by hand, several thousand sentences 
from corpora in the CHILDES database in five 
languages (English, German, Italian, Japanese and 
Russian), we found that approximately 85% are 
degree-0 and an approximate 10 out of 11 have no 
internal DP structure. 
Adopting the principles and parameters (P&P) 
hypothesis (Chomsky, 1981) as the underlying 
framework, we implemented an application that 
generated patterns and derivations given the 
following points of variation between languages: 
 
1. Affix Hopping  2. Comp Initial/Final 
3. I to C Movement  4. Null Subject 
5. Null Topic  6. Obligatory Topic 
7. Object Final/Initial  8. Pied Piping 
9. Question Inversion   10. Subject Initial/Final 
11. Topic Marking   12. V to I Movement 
13. Obligatory Wh movement 
 
The patterns have fully specified X-bar struc-
ture, and movement is implemented as HPSG local 
dependencies. Pattern production is generated top-
down via rules applied at each subtree level. 
Subtree levels include: CP, C', IP, I', NegP, Neg', 
VP, V' and PP. After the rules are applied, the 
subtrees are fully specified in terms of node 
categories, syntactic feature values and constituent 
order. The subtrees are then combined by a simple 
unification process and syntactic features are 
percolated down. In particular, movement chains 
are represented as traditional ?slash? features 
which are passed (locally) from parent to daughter; 
when unification is complete, there is a trace at the 
bottom of each slash-feature path. Other features 
include +/-NULL for non-audible tokens (e.g. 
S[+NULL] represents a null subject pro), +TOPIC 
to represent a topicalized token, +WH to represent 
?who?, ?what?, etc. (or ?qui?, ?que? if one pre-
fers), +/-FIN to mark if a verb is tensed or not and 
the illocutionary (ILLOC) features Q, DEC, IMP 
for questions, declaratives and imperatives respec-
tively.  
Although further detail is beyond the scope of 
this paper, those interested may refer to Fodor et 
al. (2003) which resides on the LDD website. 
It is important to note that the domain is suit-
able for many paradigms beyond the P&P frame-
work. For example the context-free rules (with 
local dependencies) could be easily extracted and 
used to test probabilistic CFG learning in a 
multilingual domain. Likewise the patterns, 
without their derivations, could be used as input to 
statistical/connectionist models which eschew 
traditional (generative) structure altogether and 
search for regularity in the left-to-right strings of 
tokens that makeup the learner's input stream. Or, 
the patterns could help bootstrap the creation of a 
domain that might be used to test particular types 
of lexical learning by using the patterns as tem-
plates where tokens may be instantiated with actual 
words from a lexicon of interest to the investigator. 
The point is that although a particular grammar 
formalism was used to generate the patterns, the 
patterns are valid independently of the formalism 
that was in play during generation.2  
To be sure, similar domains have been con-
structed. The relationship between the LDD and 
other artificial domains is summarized in Table 1.   
In designing the LDD, we chose to include 
syntactic phenomena which: 
i) occur in a relatively high proportion of the 
known natural languages; 
                                                          
2 If this is the case, one might ask: Why bother with a 
grammar formalism at all; why not use actual child-directed 
speech as input instead of artificially generated patterns? 
Although this approach has proved workable for several types 
of non-generative acquisition models, a generative (or hybrid) 
learner is faced with the task of selecting the rules or 
parameter values that generate the linguistic environment 
being encountered by the learner. In order to simulate this, 
there must be some grammatical structure incorporated into 
the experimental design that serves as the target the learner 
must acquire. Constructing a viable grammar and a parser with 
coverage over a multilingual domain of real child-directed 
speech is a daunting proposition. Even building a parser to 
parse a single language of child-directed speech turns out to be 
extremely difficult. See, for example, Sagae, Lavie, & 
MacWhinney (2001), which discusses an impressive number 
of practical difficulties encountered while attempting to build 
a parser that could cope with the EVE corpus; one the cleanest 
transcriptions in the CHILDES database. By abstracting away 
from actual child-directed speech, we were able to build a 
pattern generator and include the pattern derivations in the 
database for retrieval during simulation runs, effectively 
sidestepping the need to build an online multilingual parser.  
ii) are frequently exemplified in speech di-
rected to 2-year-olds; 
iii) pose potential learning problems (e.g. cross-
language ambiguity) for which theoretical 
solutions are needed; 
iv) have been a focus of linguistic and/or psy-
cholinguistic research; 
v) have a syntactic analysis that is broadly 
agreed on. 
As a result the following have been included:  
? By criteria (i) and (ii): negation, non-
declarative sentences (questions, impera-
tives).  
? By criterion (iv): null subject parameter 
(Hyams 1986 and since).  
? By criterion (iv): affix-hopping (though not 
widespread in natural languages). 
? By criterion (v): no scrambling yet. 
There are several phenomena that the LDD 
does not yet include: 
? No verb subcategorization. 
? No interface with LF (cf. Briscoe 2000; 
Villavicencio 2000).  
? No discourse contexts to license sentence 
fragments (e.g., DP or PP fragments). 
? No XP-internal structure yet (except PP = P 
+ O3, with piping or stranding).  
? No Linear Correspondence Axiom (Kayne 
1994). 
? No feature checking as implementation of 
movement parameters (Chomsky 1995).  
 
Table 1: A history of abstract domains for word-
order acquisition modeling. 
 
# 
parame
ters 
# 
lan-
guages 
Tree 
struc-
ture? 
Language 
properties 
Gibson & 
Wexler 
(1994) 
3 8 Not fully specified Word order, V2 
Bertolo et. 
al (1997b) 7 
64 
distinct Yes 
G&W + V-raising to 
Agr, T; deg-2 
Kohl (1999) 
based on 
Bertolo 
12 2,304 Partial 
Bertolo et al 
(1997b) + 
scrambling 
Sakas & 
Nishimoto 
(2002) 
4 16 Yes G&W + null subject/topic 
LDD 13 3,072 Yes 
S&N + wh-movt + 
imperatives +aux 
inversion,  etc. 
The LDD on the web: The two primary purposes 
of the web-interface are to allow the user to 
interactively peruse the patterns and the derivations 
that the LDD contains and to download raw data 
for the user to work with locally. 
Users are asked to register before using the 
LDD online. The user ID is typically an email 
address, although no validity checking is carried 
out. The benefit of entering a valid email address is 
simply to have the ability to recover a forgotten 
password, otherwise a user can have full access 
anonymously.  
The interface has three primary areas: Gram-
mar Selection, Sentence Selection and Data 
Download. First a user has to specify, on the 
Grammar Selection page, which settings of the 13 
parameters are of interest and save those settings as 
an available grammar. A user may specify multiple 
grammars. Then in the sentence selection page a 
user may peruse sentences and their derivations. 
On this page a user may annotate the patterns and 
derivations however he or she wishes. All grammar 
settings and annotations are saved and available 
the next time the user logs on. Finally on the Data 
Download page, users may download data so that 
they can use the patterns and derivations offline.  
The derivations are stored as bracketed strings 
representing tree structure. These are practically 
indecipherable by human users. E.g.: 
 
(CP[ILLOC Q][+FIN][+WH] "Adv[+TOPIC]" (Cbar[ILLOC 
Q] [+FIN][+WH][SLASH Adv](C[ILLOC Q][+FIN] "KA" ) 
(IP[ILLOC Q][+FIN][+WH][SLASH Adv]"S" (Ibar[ILLOC 
Q][+FIN][+WH][SLASH Adv](I[ILLOC 
Q][+FIN]"Aux[+FIN]")(NegP[+WH] [SLASH 
Adv](NegBar[+WH][SLASH Adv](Neg "NOT") 
(VP[+WH][SLASH Adv](Vbar[+WH][SLASH 
Adv](V"Verb")"O1" "O2" (PP[+WH] "P" "O3[+WH]" 
)"Adv[+NULL][SLASH Adv]")))))))) 
 
To be readable, the derivations are displayed 
graphically as tree structures. Towards this end we 
have utilized a set of publicly available LaTex 
macros: QTree (Siskind & Dimitriadis, [online]). A 
server-side script parses the bracketed structures 
into the proper QTree/LaTex format from which a 
pdf file is generated and subsequently sent to the 
user's client application.  
Even with the graphical display, a simple sen-
tence-by-sentence presentation is untenable given 
the large amount of linguistic data contained in the 
database. The Sentence Selection area allows users 
to access the data filtered by sentence type and/or 
by grammar features (e.g. all sentences that have 
obligatory-wh movement and contain a preposi-
tional phrase), as well as by the user?s defined 
grammar(s) (all sentences that are "Italian-like").  
On the Data Download page, users may filter 
sentences as on the Sentence Selection page and 
download sentences in a tab-delimited format. The 
entire LDD may also be downloaded ? approxi-
mately 17 MB compressed, 600 MB as a raw ascii 
file. 
3 A Case Study: Evaluating the efficiency 
of parameter-setting acquisition models. 
We have recently run experiments of seven 
parameter-setting (P&P) models of acquisition on 
the domain. What follows is a brief discussion of 
the algorithms and the results of the experiments. 
We note in particular where results stemming from 
work with the LDD lead to conclusions that differ 
from those previously reported. We stress that this 
is not intended as a comprehensive study of 
parameter-setting algorithms or acquisition 
algorithms in general. There is a large number of 
models that are omitted; some of which are targets 
of current investigation. Rather, we present the 
study as an example of how the LDD could be 
effectively utilized.  
In the discussion that follows we will use the 
terms ?pattern?, ?sentence? and ?input? inter-
changeably to mean a left-to-right string of tokens 
drawn from the LDD without its derivation. 
3.1 A Measure of Feasibility 
As a simple example of a learning strategy and 
of our simulation approach, consider a domain of 4 
binary parameters and a memoryless learner 3 
which blindly guesses how all 4 parameters should 
be set upon encountering an input sentence. Since 
there are 4 parameters, there are 16 possible 
combinations of parameter settings. i.e., 16 
different grammars. Assuming that each of the 16 
grammars is equally likely to be guessed, the 
learner will consume, on average, 16 sentences 
before achieving the target grammar. This is one 
measure of a model?s efficiency or feasibility. 
                                                          
3 By ?memoryless? we mean that the learner processes inputs 
one at a time without keeping a history of encountered inputs 
or past learning events. 
However, when modeling natural language 
acquisition, since practically all human learners 
attain the target grammar, the average number of 
expected inputs is a less informative statistic than 
the expected number of inputs required for, say, 
99% of all simulation trials to succeed. For our 
blind-guess learner, this number is 72.4 We will 
use this 99-percentile feasibility measure for most 
discussion that follows, but also include the 
average number of inputs for completeness. 
3.2 The Simulations  
In all experiments: 
? The learners are memoryless. 
? The language input sample presented to the 
learner consists of only grammatical sentences 
generated by the target grammar. 
? For each learner, 1000 trials were run for each 
of the 3,072 target languages in the LDD.  
? At any point during the acquisition process, 
each sentence of the target grammar is equally 
likely to be presented to the learner. 
Subset Avoidance and Other Local Maxima: 
Depending on the algorithm, it may be the case 
that a learner will never be motivated to change its 
current hypothesis (Gcurr), and hence be unable to 
ultimately achieve the target grammar (Gtarg). For 
example, most error-driven learners will be trapped 
if Gcurr generates a language that is a superset of 
the language generated by Gtarg. There is a wealth 
of learnability literature that addresses local 
maxima and their ramifications.5 However, since 
our study?s focus is on feasibility (rather than on 
whether a domain is learnable given a particular 
algorithm), we posit a built-in avoidance mecha-
nism, such as the subset principle and/or default 
values that preclude local maxima; hence, we set 
aside trials where a local maximum ensues. 
                                                          
4 The average and 99-percentile figures (16 and 72) in this 
section are easily derived from the fact that input consumption 
follows a hypergeometric distribution. 
5 Discussion of the problem of subset relationships among 
languages starts with Gold?s (1967) seminal paper and is 
discussed in Berwick (1985) and Wexler & Manzini (1987). 
Detailed accounts of the types of local maxima that the learner 
might encounter in a domain similar to the one we employ are 
given in Frank & Kapur (1996), Gibson & Wexler (1994), and 
Niyogi & Berwick (1996). 
3.3 The Learners' strategies 
In all cases the learner is error-driven: if Gcurr can 
parse the current input pattern, retain it.6 
The following refers to what the learner does 
when Gcurr fails on the current input. 
 
? Error-driven, blind-guess (EDBG): adopt any 
grammar from the domain chosen at random ? 
not psychologically plausible, it serves as our 
baseline. 
? TLA (Gibson & Wexler, 1994): change any one 
parameter value of those that make up Gcurr. 
Call this new grammar Gnew. If Gnew can parse 
the current input, adopt it. Otherwise, retain 
Gcurr. 
? Non-Greedy TLA (Niyogi & Berwick, 1996): 
change any one parameter value of those that 
make up Gcurr. Adopt it. (I.e. there is no testing 
of the new grammar against the current input). 
? Non-SVC TLA (Niyogi & Berwick, 1996): try 
any grammar in the domain. Adopt it only in the 
event that it can parse the current input. 
? Guessing STL (Fodor, 1998a): Perform a 
structural parse of the current input. If a choice 
point is encountered, chose an alternative based 
on one of the following and then set parameter 
values based on the final parse tree: 
? STL Random Choice (RC) ? randomly pick a 
parsing alternative. 
? Minimal Chain (MC) ? pick the choice that 
obeys the Minimal Chain Principle (De Vin-
cenzi, 1991), i.e., avoid positing movement 
transformations if possible. 
? Local Attachment/Late Closure (LAC) ?pick 
the choice that attaches the new word to the 
current constituent (Frazier, 1978). 
 
The EDBG learner is our first learner of inter-
est. It is easy to show that the average and 99% 
scores increase exponentially in the number of 
parameters and syntactic research has proposed 
more than 100 (e.g. Cinque, 1999). Clearly, human 
learners do not employ a strategy that performs as 
poorly as this. Results will serve as a baseline to 
compare against other models. 
                                                          
6 We intend for a ?can-parse/can?t-parse outcome? to be 
equivalent to the result from a language membership test. If 
the current input sentence is one of the set of sentences 
generated by Gcurr, can-parse is engendered; if not, can?t-
parse. 
 
 99% Average 
EDBG 16,663 3,589 
Table 2: EDBG, # of sentences consumed 
 
The TLA: The TLA incorporates two search 
heuristics: the Single Value Constraint (SVC) and 
Greediness. In the event that Gcurr cannot parse the 
current input sentence s, the TLA attempts a 
second parse with a randomly chosen new gram-
mar, Gnew, that differs from Gcurr by exactly one 
parameter value (SVC). If Gnew can parse s, Gnew 
becomes the new Gcurr otherwise Gnew is rejected as 
a hypothesis (Greediness). Following Berwick and 
Niyogi (1996), we also ran simulations on two 
variants of the TLA ? one with the Greediness 
heuristic but without the SVC (TLA minus SVC, 
TLA?SVC) and one with the SVC but without 
Greediness (TLA minus Greediness, TLA?Greed). 
The TLA has become a seminal model and has 
been extensively studied (cf. Bertolo, 2001 and 
references therein; Berwick & Niyogi, 1996; Frank 
& Kapur, 1996; Sakas, 2000; among others).  The 
results from the TLA variants operating in the 
LDD are presented in Table 3. 
 
 99% Average 
TLA-SVC 67,896 11,273 
TLA-Greed 19,181 4,110 
TLA 16,990 961 
Table 3: TLA variants, # of sentences consumed 
 
Particularly interesting is that contrary to results 
reported by Niyogi & Berwick (1996) and Sakas & 
Nishimoto (2002), the SVC and Greediness 
constraints do help the learner achieve the target in 
the LDD. The previous research was based on 
simulations run on much smaller 9 and 16 lan-
guage domains (see Table 1). It would seem that 
the local hill-climbing search strategies employed 
by the TLA do improve learning efficiency in the 
LDD. However, even at best, the TLA performs 
less well than the blind guess learner. We conjec-
ture that this fact probably rules out the TLA as a 
viable model of human language acquisition. 
The STL: Fodor?s Structural Triggers Learner 
(STL) makes greater use of the parser than the 
TLA. A key feature of the model is that parameter 
values are not simply the standardly presumed 0 or 
1, but rather bits of tree structure or treelets. Thus, 
a grammar, in the STL sense, is a collection of 
treelets rather than a collection of 1's and 0's. The 
STL is error-driven. If Gcurr cannot license s, new 
treelets will be utilized to achieve a successful 
parse.7 Treelets are applied in the same way as any 
?normal? grammar rule, so no unusual parsing 
activity is necessary. The STL hypothesizes 
grammars by adding parameter value treelets to 
Gcurr when they contribute to a successful parse. 
The basic algorithm for all STL variants is: 
1. If Gcurr can parse the current input sentence, 
retain the treelets that make up Gcurr. 
2. Otherwise, parse the sentence making use of 
any or all parametric treelets available and 
adopt those treelets that contribute to a suc-
cessful parse. We call this parametric de-
coding. 
Because the STL can decode inputs into their 
parametric signatures, it stands apart from other 
acquisition models in that it can detect when an 
input sentence is parametrically ambiguous. 
During a parse of s, if more than one treelet could 
be used by the parser (i.e., a choice point is 
encountered), then s is parametrically ambiguous. 
The TLA variants do not have this capacity 
because they rely only on a can-parse/can?t-parse 
outcome and do not have access to the on-line 
operations of the parser. Originally, the ability to 
detect ambiguity was employed in two variations 
of the STL: the strong STL (SSTL) and the weak 
STL. 
The SSTL executes a full parallel parse of each 
input sentence and adopts only those treelets 
(parameter values) that are present in all the 
generated parse trees. This would seem to make 
the SSTL an extremely powerful, albeit psycho-
logically implausible, learner.8 However, this is not 
necessarily the case. The SSTL needs some 
unambiguity to be present in the structures derived 
from the sentences of the target language. For 
example, there may not be a single input generated 
by Gtarg that when parsed yields an unambiguous 
treelet for a particular parameter. 
                                                          
7 In addition to the treelets, UG principles are also available 
for parsing, as they are in the other models discussed above.  
8 It is important to note that Fodor (1998a) does not put forth 
the strong STL as a psychologically plausible model. Rather, it 
is intended to demonstrate the potential effectiveness of 
parametric decoding.  
Unlike the SSTL, the weak STL executes a 
psychologically plausible left-to-right serial 
(deterministic) parse. One variant of the weak 
STL, the waiting STL (WSTL), deals with ambigu-
ous inputs abiding by the heuristic: Don?t learn 
from sentences that contain a choice point. These 
sentences are simply discarded for the purposes of 
learning. This is not to imply that children do not 
parse ambiguous sentences they hear, but only that 
they set no parameters if the current evidence is 
ambiguous. 
As with the TLA, these STL variants have been 
studied from a mathematical perspective (Bertolo 
et al, 1997a; Sakas, 2000). Mathematical analyses 
point to the fact that the strong and weak STL are 
extremely efficient learners in conducive domains 
with some unambiguous inputs but may become 
paralyzed in domains with high degrees of ambigu-
ity. These mathematical analyses among other 
considerations spurred a new class of weak STL 
variants which we informally call the guessing STL 
family. 
The basic idea behind the guessing STL models 
is that there is some information available even in 
sentences that are ambiguous, and some strategy 
that can exploit that information. We incorporate 
three different heuristics into the original STL 
paradigm, the RC, MC and LAC heuristics 
described above. 
Although the MC and LAC heuristics are not 
stochastic, we regard them as ?guessing? heuristics 
because, unlike the WSTL, a learner cannot be 
certain that the parametric treelets obtained from a 
parse guided by MC and LAC are correct for the 
target. These heuristics are based on well-
established human parsing strategies. Interestingly, 
the difference in performance between the three 
variants is slight. Although we have just begun to 
look at this data in detail, one reason may be that 
the typical types of problems these parsing 
strategies address are not included in the LDD (e.g. 
relative clause attachment ambiguity). Still, the 
STL variants perform the most efficiently of the 
strategies presented in this small study (approxi-
mately a 100-fold improvement over the TLA). 
Certainly this is due to the STL's ability to perform 
parametric decoding. See Fodor (1998b) and Sakas 
& Fodor (2001) for detailed discussion about the 
power of decoding when applied to the acquisition 
process. 
 
Guessing 
STL 99% Average 
RC 1,486 166 
MC 1,412 160 
LAC 1,923 197 
Table 4: guessing STL family, # of sen-
tences consumed 
4 Conclusion and future work 
The thrust of our current research is directed at 
collecting data for a comprehensive, comparative 
study of psycho-computational models of syntax 
acquisition. To support this endeavor, we have 
developed the Language Domain Database ? a 
publicly available test-bed for studying acquisition 
models from diverse paradigms. 
Mathematical analysis has shown that learners 
are extremely sensitive to various distributions in 
the input stream (Niyogi & Berwick, 1996; Sakas, 
2000, 2003). Approaches that thrive in one domain 
may dramatically flounder in others. So, whether a 
particular computational model is successful as a 
model of natural language acquisition is ultimately 
an empirical issue and depends on the exact 
conditions under which the model performs well 
and the extent to which those favorable conditions 
are in line with the facts of human language. The 
LDD is a useful tool that can be used within such 
an empirical research program.  
Future work: Though the LDD has been vali-
dated against CHILDES data in certain respects, 
we intend to extend this work by adding distribu-
tions to the LDD that correspond to actual distribu-
tions of child-directed speech. For example, what 
percentage of utterances, in child-directed Japa-
nese, contain pro-drop? object-drop? How often in 
English does the pattern: S[+WH] aux Verb O1 
occur and at what periods of a child's develop-
ment? We believe that these distributions will shed 
some light on many of the complex subtleties 
involved in ambiguity disambiguation and the role 
of nondeterminism and statistics in the language 
acquisition process. This is proving to be a 
formidable, yet surmountable task; one that we are 
just beginning to tackle. 
Acknowledgements 
This paper reports work done in part with other 
members of CUNY-CoLAG (CUNY's Computa-
tional Language Acquisition Group) including 
Janet Dean Fodor, Virginia Teller, Eiji Nishimoto, 
Aaron Harnley, Yana Melnikova, Erika Troseth, 
Carrie Crowther, Atsu Inoue, Yukiko Koizumi, 
Lisa Resig-Ferrazzano, and Tanya Viger. Also 
thanks to Charles Yang for much useful discussion, 
and valuable comments from the anonymous 
reviewers. This research was funded by PSC-
CUNY Grant #63387-00-32 and CUNY Collabora-
tive Grant #92902-00-07. 
References 
Bertolo, S. (Ed.) (2001). Language Acquisition and 
Learnability. Cambridge, UK: Cambridge University 
Press. 
Bertolo, S., Broihier, K., Gibson, E., & Wexler, K. 
(1997a). Characterizing learnability conditions for 
cue-based learners in parametric language systems. 
Proceedings of the Fifth Meeting on Mathematics of 
Language. 
Bertolo, S., Broihier, K., Gibson, E., and Wexler, K. 
(1997b) Cue-based learners in parametric language 
systems: Application of general results to a recently 
proposed learning algorithm based on unambiguous 
'superparsing'. In M. G. Shafto and P. Langley (eds.) 
the Cognitive Science Society, Mahwah NJ: Law-
rence Erlbaum Associates. 
Berwick, R. C., & Niyogi, P. (1996). Learning from 
triggers. Linguistic Inquiry, 27 (4), 605-622. 
Briscoe, T. (2000). Grammatical acquisition: Inductive 
bias and coevolution of language and the language 
acquisition device. Language, 76 (2), 245-296. 
Chomsky, N. (1981) Lectures on Government and 
Binding, Dordrecht: Foris Publications. 
Chomsky, N. (1995) The Minimalist Program. Cam-
bridge MA: MIT Press. 
Cinque, G. (1999) Adverbs and Functional Heads. 
Oxford Oxford, UK:University Press, Oxford, UK. 
Fodor, J. D. (1998a)  Unambiguous triggers, Linguistic 
Inquiry 29.1, 1-36. 
Fodor, J. D. (1998b) Parsing to learn. Journal of 
Psycholinguistic Research 27.3, 339-374. 
Fodor, J.D., Melnikova, Y. & Troseth, E. (2002) A 
structurally defined language domain for testing 
syntax acquisition models.  Technical Report. CUNY 
Graduate Center. 
Gibson, E. and Wexler, K. (1994) Triggers. Linguistic 
Inquiry 25, 407-454. 
Gold, E. M. (1967) Language identification in the limit. 
Information and Control 10, 447-474. 
Hyams, N. (1986) Language Acquisition and the Theory 
of Parameters. Dordrecht: Reidel. 
Jain, S., E. Martin, D. Osherson, J. Royer, and A. 
Sharma. (1991) Systems That Learn. 2nd ed. Cam-
bridge, MA: MIT Press.  
Kayne, R. S. (1994) The Antisymmetry of Syntax. 
Cambridge MA: MIT Press. 
Kohl, K.T. (1999)  An Analysis of Finite Parameter 
Learning in Linguistic Spaces. Master?s Thesis, MIT. 
MacWhinney, B. (1995) The CHILDES Project: Tools 
for Analyzing Talk. (2nd ed.) Hillsdale, NJ: Lawrence 
Erlbaum Associates.  
Niyogi, P (1998) The Informational Complexity of 
Learning: Perspectives on Neural Networks and 
Generative Grammar Dordrecht: Kluwer Academic. 
Pinker, S. (1979) Formal models of language learning, 
Cognition 7, 217-283. 
Sagae, K., Lavie, A., MacWhinney, B. (2001) Parsing 
the CHILDES database: Methodology and lessons 
learned. In Proceedings of the Seventh International 
Workshop in Parsing Technologies. Beijing, China. 
Sakas, W.G. (in prep) Grammar/Language smoothness 
and the need (or not) of syntactic parameters. Hunter 
College and The Graduate Center, City University of 
New York.  
Sakas, W.G. (2000) Ambiguity and the Computational 
Feasibility of Syntax Acquisition, Doctoral Disserta-
tion, City University of New York. 
Sakas, W.G. and Fodor, J.D. (2001). The Structural 
Triggers Learner. In S. Bertolo (ed.) Language Ac-
quisition and Learnability. Cambridge, UK: Cam-
bridge University Press. 
Sakas, W.G. and Nishimoto, E. (2002) Search, Structure 
or Statistics? A Comparative Study of Memoryless 
Heuristics for Syntax Acquisition, Proceedings of the 
24th Annual  Conference  of the Cognitive Science 
Society.  Hillsdale NJ: Lawrence  Erlbaum Associ-
ates,  
Siskind, J.M & Dimitriadis, A., [Online 5/20/2003] 
Documentation for qtree, a LaTex tree package 
http://www.ling.upenn.edu/advice/latex/qtree/  
Villavicencio, A. (2000) The use of default unification 
in a system of lexical types. Paper presented at the 
Workshop on Linguistic Theory and Grammar Im-
plementation, Birmingham,UK. 
Wexler, K. and Culicover, P. (1980) Formal Principles 
of Language Acquisition. Cambridge MA: MIT 
Press. 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 61-66, Lisbon, Portugal, 2000. 
Modeling the Effect of Cross-Language Ambiguity on 
Human Syntax Acquisition 
Will iam Gregory  Sakas  
Department  of Computer  Science 
Hunter College and The Graduate Center 
City University of New York 
New York, NY 10021 
sakasOhunter ,  cuny. edu 
Abst rac t  
A computational framework is presented which 
is used to model the process by which human 
language learners acquire the syntactic ompo- 
nent of their native language. The focus is fea- 
sibility - -  is acquisition possible within a rea- 
sonable amount of time and/or with a reason- 
able amount of work? The approach abstracts 
away from specific linguistic descriptions in or- 
der to make a 'broad-stroke' prediction of an 
acquisition model's behavior by formalizing fac- 
tors that contribute to cross-linguistic ambigu- 
ity. Discussion centers around an application 
to Fodor's Structural Trigger's Learner (STL) 
(1998) 1 and concludes with the proposal that 
successful computational modeling requires a 
parallel psycholinguistic investigation ofthe dis- 
tribution of ambiguity across the domain of hu- 
man languages. 
1 Principles and Parameters  
Chomsky (1981) (and elsewhere) has proposed 
that all natural languages hare the same in- 
nate universal principles (Universal Grammar 
- -  UG) and differ only with respect o the set- 
tings of a finite number of parameters. The syn- 
tactic component of a grammar in the principles 
and parameters (henceforth P&P) framework, 
is simply a collection of parameter values - -  one 
value per parameter. (Standardly, two values 
are available per parameter.) The set of human 
1The STL is an acquisition model in the principles 
and parameters paradigm. The results presented here 
are not intended to forward an argument for or against 
the model, or for that matter, for or against the prin- 
ciples and parameters paradigm. Rather, the results 
axe presented to point out (the possibly not-so- earth- 
shattering observation) that the acquisition mechanism 
can  be extremely sensitive to ambiguity. 
grammars i the set of all possible combinations 
of parameter values (and lexicon). 
The P&P framework was motivated to a large 
degree by psycholinguistic data demonstrating 
the extreme fficiency of human language acqui- 
sition. Children acquire the grammar of their 
native language at an early age - -  generally ac- 
cepted to be in the neighborhood of five years 
old. In the P&P framework, even if the lin- 
guistic theory delineates over a billion possible 
grammars, a learner need only determine the 
correct 30 values that correspond to the gram- 
mar that generates the sentences of the tar- 
get language. 2 given that a successful syntac- 
tic theory must provide for an efficient acquisi- 
tion mechanism, and since, prima facie, param- 
eter values seem transparently learnable, it is 
not surprising that parameters have been incor- 
porated into current generative syntactic the- 
ories. However, the exact process of parame- 
ter setting has been studied only recently (e.g. 
Clark (1992), Gibson and Wexler (1994), Yang 
(1999), Briscoe (2000), among others) and al- 
though it has proved linguistically fruitful to 
construct parametric analyses, it turns out to 
be surprisingly difficult to construct a workable 
model of parameter-value acquisition. 
1.1 Parametr i c  Ambigu i ty  
A sentence is parametrically ambiguous if it is 
licensed by two or more distinct combinations 
of parameter values. Ambiguity is a natural en- 
emy of efficient language acquisition. The prob- 
lem is that, due to ambiguity, there does not 
exist a one-to-one correspondence b tween the 
linear 'word-order' surface strings of the input 
sample and the correct parameter values that 
generate the target language. Clearly, if ev- 
230 binary parameters entails approximately a billion 
grammars (230 = 1,073,741,824.) 
61 
ery sentence of the target language triggers one 
and only one set of parameter values (i.e. ev- 
ery sentence is completely unambiguous) and 
the learner, upon encountering an input, can 
determine what those values are, the parameter 
setting process is truly transparent. Unfortu- 
nately, not all natural languages entences are 
so distinctively earmarked by their parametric 
signatures. However, if there exists some degree 
of parametric unambiguity in a learner's input 
sample, a learner can set parameters by: 1) de- 
coding the parametric signature of an input sen- 
tence, 2) determining if ambiguity exists, and 3) 
using the input to guide parameter setting only 
in the case that the sentence is parametrically 
unambiguous. The motto of such a learner is: 
Don't learn .from ambiguous input and learning 
efficiency can be measured by the number of 
sentences the learner has to wait for usable, un- 
ambiguous inputs to occur in the input stream. 3
2 The  St ructura l  T r iggers  Learner  
One recent model of human syntax acquisi- 
tion, The Structural Triggers ?earner 
(ST?) (Fodor, 1998), employs the human 
parsing mechanism to determine if an input is 
parametrically ambiguous. Parameter values 
are viewed as bits of tree structure (treelets). 
When the learner's current grammar is insuffi- 
cient to parse the current input sentence, the 
treelets may be utilized during the parsing pro- 
cess in the same way as any natural language 
grammar would be applied; no unusual parsing 
activity is necessary. The treelets are adopted 
as part of the learner's current grammar hy- 
pothesis when: 1) they are required for a suc- 
cessful parse of the current input sentence and 
2) the sentence is unambiguous. The STL thus 
learns only from fully unambiguous sentences. 4 
3Of course, the extent o which such unambiguous 
sentences exist in the domain of human languages i an 
empirical issue. This is an important open research ques- 
tion which is the focus of a recent research endeavor here 
at CUNY. Our approach involves tagging a large, cross- 
linguistic set of child-directed sentences, drawn from the 
CHILDES database, with each sentence's parametric sig- 
nature. By cross-tabulating the shared parameter values 
against different languages, the study should shed some 
light as to the shape of ambiguity in input samples typ- 
ically encountered by children. 
4This is actually the strategy employed by just one 
of several different STL variants, some of which are de- 
signed to manage domains in which unambiguous sen- 
AA 
,xA Parameter value 
treelets 
? .-.....~,,~.....,,,m~,~. Sentence 
Sente~cce 
Current grammar 
Figure 1: An example of how the STL acquires 
new parameter values. 
See Figure 1. 
3 The  Feas ib i l i ty  o f  the  STL  
The number of input sentences consumed by the 
STL before convergence on the target grammar 
can be derived from a relatively straightforward 
Markov analysis. Importantly, the formulation 
most useful to analyze performance does not re- 
quire states which represent he grammars of 
the parameter space (contra Niyogi and Berwick 
(1996)). Instead, each state of the system de- 
picts the number of parameters that have been 
set, t, and  the state transitions represent the 
probability that the STL  will adopt some num-  
ber of new parameter values, w, on the basis of 
the current state and whatever usable paramet- 
ric information is revealed by the current input 
sentence. See Figure 2. 
The  following factors (described in detail be- 
low) determine the transition probabilities: 
? the number  of parameters that have been 
set (t) 
? the number of relevant parameters (r) 
? the expression rate (e) 
? the effective expression rate (e I) 
Not all parameters are relevant parameters. 
Irrelevant parameters control properties of phe- 
nomena not present in the target language, such 
as clitic order in a language without clitics. For 
tences are rare or nonexistent. 
62 
Figffre 2: A transition diagram for the STL per- 
forming in a parameter space of three parame- 
ters. Nodes represent he current number of 
parameters that have been correctly set. Arcs 
indicate a change in the number that are cor- 
rectly set. In this diagram, after each input is 
consumed, 0, 1 or 2 new parameters may be set. 
Once the learner enters tate 3, it has converged 
on the target. 
our purposes, the number of relevant parame- 
ters, r, is the total number of parameters that 
need to be set in order to license all and only 
the sentences of the target language. 
Of the parameters relevant o the target lan- 
guage as a whole, only some will be relevant o  
any given sentence. A sentence xpresses those 
parameters for which a specific value is required 
in order to build a parse tree, i.e. those pa- 
rameters which are essential to the sentence's 
structural description. For instance, if a sen- 
tence does not have a relative clause, it will not 
express parameters that concern only relative 
clauses; if it is a declarative sentence, it won't 
express the properties peculiar to questions; and 
so on. The expression rate, e, for a language, 
is the average number of parameters expressed 
by its input sentences. Suppose that each sen- 
tence, on average, is ambiguous with respect o 
a of the parameters it expresses. The effective 
expression rate, e ~, is the mean proportion of 
expressed parameters that are expressed unam- 
biguously (i.e. e' = (e - a)/e). It will also be 
useful to consider a' = (1 - e~). 
3.1 Der ivat ion  of  a Transition 
Probability Function 
To present he derivation of the probability that 
the system will change from an arbitrary state 
St to state St+w, (0 < w < e) it is useful to set 
ambiguity aside for a moment. In order to set al 
r parameters, the STL has to encounter enough 
batches of e parameter values, possibly overlap- 
ping with each other, to make up the full set of 
r parameter values that have to be established. 
Let H(wlt, r,e) be the probability that an ar- 
bitrary input sentence xpresses w new (i.e. as 
yet unset) parameters, out of the e parameters 
expressed, given that the learner has already set 
t parameters (correctly), for a domain in which 
there are r total parameters that need to be set. 
This is a specification of the hypergeometric 
distribution and is given in Equation 1. 
H(wlt,r,e)= (~t)(~-t~?) (i) (:) 
Now, to deal with ambiguity, the effective 
rate of expression, e t, is brought into play. Re- 
call that e ~ is the proportion of expressed pa- 
rameters that are expressed unambiguously. It 
follows that the probability that any single pa- 
rameter is expressed unambiguously is also e ~ 
and the probability that all of the expressed, 
but as yet unset parameters are expressed un- 
ambiguously is e ~w. That is, the probability that 
an input is effectively unambiguous and hence 
usable for learning is equal to e ~w. 
f H(w\[t,r,e)  w, O<w<e 
t H(O\[t'r'e)+ i_~l H(ilt'r'e)(1-e'i)'w=O 
(2) 
Equation (2) can be used to calculate the prob- 
ability of any possible transition of the Markov 
system that models STL performance. One 
method to determine the number of sentences 
expected to be consumed by the STL is to sum 
the number of sentences consumed in each state. 
Let E(Si) represent the expected number of sen- 
tences that will be consumed in state Si. E is 
given by the following recurrence relation: 5
E(So) = 1/e'? 
E(Zn) = II(1-P(Sn----~Sn)) ~ P(Si-'-~Sn)(Si) 
i~r t~e 
(5) 
The expected total is simply: 
r - -1  
Etot=E(So)+ ~ E(Si) (4) 
i=e 
which is equal to the expected number to be 
consumed before any parameters have been set 
5The functional E is derived from basic properties 
of Markov Chains. See Taylor and Karlin (1994) for a 
general derivation. 
63 
e a~(%)  r =15 r =20 r =25 r =30 
1 20 62 90 119 150 
40 83 120 159 200 
60 124 180 238 300 
80 249 360 477 599 
5 20 15 22 29 36 
40 34 46 59 73 
60 144 176 210 245 
80 3 ,300  3 ,466  3 ,666 3 ,891 
10 20 14 18 23 28 
40 174 187 203 221 
60 9 ,560  9 ,621 9 ,727  9 ,878  
80 9 ,765 ,731  9 ,766 ,375  9 ,768 ,376  9 ,772 ,740  
15 20 28 32 37 41 
40 2 ,127  2 ,136  2 ,153 2 ,180 
60 931 ,323  931 ,352  931 ,479  931 ,822  
80 . . .over  l0  b i l l i on  . . .  
20 20 87 91 95 
40 27,351 27 ,361 27 ,383  
60 90 ,949 ,470  90 ,949 ,504  90 ,949 ,728  
80 . . . in  the  t r i l l i ons  . . .  
Table 1: Average number of inputs consumed 
by the waiting-STL before convergence. Fixed 
rate of expression. 
(= E(So)) plus the number expected to be con- 
sumed after the first successful learning event 
(at which point the learner will be in state Se) 
summed with the number of sentences expected 
to be consumed in every other state up to the 
state just before the target is attained (St- l ) .  
Etot can be tractably calculated using dynamic 
programming. 
3.2 Some Results 
Table 1 presents numerical results derived by 
fixing different values of r, e, and e ~. In order 
to make assessments of performance across dif- 
ferent situations in terms of increasing rates of 
ambiguity, a percentage measure of ambiguity, 
a ~, is employed which is directly derived from 
er: a ~ = 1 - e ~, and is presented in Table 1 as a 
percent (the proportion is multiplied by 100). 
Notice that the number of parameters to be 
set (r) has relatively little effect on convergence 
time. What dominates learning speed is am- 
biguity and expression rates. When a ~ and e 
are both high, the STL is consuming an unrea- 
sonable number of input sentences. However, 
the problem is not intrinsic to the STL model 
of acquisition. Rather, the problem is due to 
a too rigid restriction present in the current 
formulation of the input sample. By relaxing 
the restriction, the expected performance of the 
STL improves dramatically. But first, it is in- 
formative to discuss why the framework, as pre- 
sented so far, leads to the prediction that the 
STL will consume an extremely large number 
of sentences at rates of ambiguity and expres- 
sion approaching natural anguage. 
By far the greatest amount of damage in- 
flicted by ambiguity occurs at the very earliest 
stages of learning. This is because before any 
learning takes place, the STL must wait for the 
occurrence of a sentence that is fully unambigu- 
ous. Such sentences are bound to be extremely 
rare if the expression rate and the degree of am- 
biguity is high. For instance, a sentence with 20 
out of 20 parameters unambiguous will virtually 
never occur if parameters are ambiguous on av- 
erage 99% of the time (the probability would be 
(1/100)2?). 
After learning gets underway, STL perfor- 
mance improves tremendously; the generally 
damaging effect of ambiguity is mitigated. Ev- 
ery successful learning event decreases the num- 
ber of parameters till to be set. Hence, the 
expression rate of unset parameters decreases 
as learning proceeds. And to be usable by the 
STL, the only parameters that need to be ex- 
pressed unambiguously are those that have not 
yet been set. For example, if 19 parameters 
have already been set and e = r = 20 as in the 
example above, the probability of encountering 
a usable sentence in the case that parameters 
are ambiguous on average 99% of the time and 
the input sample consists of sentences express- 
ing 20 parameters, is only (1/100) 1 = 1/100. 
This can be derived by plugging into Equation 
(2): w = 1, t = 19, e ---- 20, and r = 20 which is 
equal to: H(1119, 20, 20)(1/100) 1 = (1)(1/100). 
Clearly~ the probability of seeing usable in- 
puts increases rapidly as the number of param- 
eters that are set increases. All that is needed, 
therefore, is to get parameter setting started, so 
that the learner can be quickly be pulled down 
into more comfortable regions of parametric ex- 
pression. Once parameter setting is underway, 
the STL is extremely efficient. 
3.3 Distr ibuted Expression Rate 
So far e has been conveniently taken to be fixed 
across all sentences of the target language. In 
which case, when e = 10, the learner will have 
to wait for a sentence with exactly 10 unam- 
biguously expressed parameters in order to get 
started on learning, and as discussed above, it 
can be expected that this will be a very long 
wait. However, if one takes the value of e to be 
uniformly distributed (rather than fixed) then 
the learner will encounter some sentences which 
64 
express fewer than 10 parameters, and which 
are correspondingly more likely to be fully un- 
ambiguous and hence usable for learning. 
In fact, any distribution of e can be incorpo- 
rated into the framework presented so far. Let 
Di(x) denote the probability distribution of ex- 
pression of the input sample. That is, the prob- 
ability that an arbitrarily chosen sentence from 
the input sample I expresses x parameters. For 
example, if Di imposes a uniform distribution, 
then DI(x) = 1/emax where every sentence x- 
presses at least 1 parameter and emax is the 
maximum number of parameters expressed by 
any sentence. Given Di, a new transition prob- 
ability P'(St '+ St+w) = P'(wlt, r, emax, e') can 
be formulated as: 
?maz  
P'(w\[t,r,e . . . .  et)-- - ~ Di(i)P(wlt,r,i,e' ) (5) 
where P is defined in (2) above and emaz repre- 
sents the maximum number of parameters that 
a sentence may express instead of a fixed num- 
ber for all sentences. 
To see why Equation (5) is valid, consider 
that to set w new parameters at least w must 
be expressed in the current input sentence. Also 
usable, are sentences that express more  param-  
eters (w + I, w + 2, w + 3,..., emax). Thus,  the 
probability of setting w new parameters  is sim- 
ply the sum of the probabilities that a sentence 
expressing a number  of parameters,  i, f rom w 
to emax, is encountered by the STL (= Di(i)), 
times the probability that the STL can set w ad- 
ditional parameters given that i are expressed. 
By replacing P with P' in Equation 3 and mod- 
ifying the derivation of the base case, 6 the to- 
tal expected number of sentences that will be 
consumed by the STL given a distribution of 
expression Di(x) can be calculated. 
Table 2 presents numerical results derived by 
fixing r and a' and allowing e to vary uniformly 
from 0 to emax. As in Table 1, a percentage 
measure of ambiguity, a', is employed. 
The results displayed in the table indicate 
a striking decrease in the number of sentences 
that the that the STL can be expected to con- 
sume compared to those obtained with a fixed 
expression rate in place. As a rough compari- 
son, with the ambiguity rate (a') at 80%: when 
6E(So) = 1/(1 - -  P'(So --+ So)) 
emaz  a ' (%)  r = 15 r = 20 r = 25 r = 30 
1 20 124 180 238 300 
40 166 240 318 399 
60 249 360 477 599 
80 498 720 954 1198 
20 28 40 53 67 
40 46 65 86 107 
60 89 124 161 199 
80 235 324 417 511 
10 20 17 24 32 40 
40 40 55 70 86 
60 102 137 173 209 
80 323 430 538 648 
15 20 
40 
60 
80 
20 20 
40 
60 
80 
15 21 27 33 
46 62 77 93 
134 176 219 262 
447 586 726 868 
20 26 32 
74 91 109 
223 275 327 
755 931 1108 
Table 2: Average number of inputs consumed 
by the STL before convergence. Uniformly dis- 
tributed rate of expression. 
e varies uniformly from 0 to 10, the STL re- 
quires 430 sentences (from Table 2); when e is 
fixed at 5, the number of sentences required is 
3,466 (from Table 1). 
4 D iscuss ion  
Although presented as a feasibility analysis of 
parameter-setting - -  specifically of STL perfor- 
mance, it should be clear that the relevant fac- 
tors e', e, r, etc. can be applied to shape an 
abstract input domain for almost any learning 
strategy. This is important because questions of 
a model's feasibility have proved difficult to an- 
swer in spaces of a linguistically plausible size. 
Recent attempts necessarily rely on severely 
small, highly circumscribed language domains 
(e.g. Gibson and Wexler (1994), among others). 
These studies frequently involve the construc- 
tion of an idealized language sample which is 
(at best) an accurate subset of sentences that 
a child might hear. A simulated learner is let 
loose on the input space and results consist of 
either the structure of the grammar(s) acquired 
or the specific circumstances under which the 
learner succeeds or fails to attain the target. 
Without question, this research agenda is valu- 
able and can bring to light interesting charac- 
teristics of the acquisition process. (cf. Gibson 
and Wexler's (1994) argument for certain de- 
fault parameter values based on the potential 
success or failure of verb-second acquisition in 
a three-parameter domain. And, for a different 
perspective, Elman et al's (1996) discussions of 
65 
English part-of-speech and past-tense morphol- 
ogy acquisition in a connectionist framework.) 
I stress that my point here is not to give a 
full accounting of STL performance. Substan- 
tim work has been completed towards this end 
(Sakas (2000), Sakas and Fodor (In press.)), 
as well as development of a similar frame- 
work to other models (See Sakas and Demner- 
Fushman (In prep.) for an application to Gib- 
son and Wexler's Triggering Learning Algo- 
rithm). Rather, I intend to put forth the conjec- 
ture that syntax acquisition is extremely sensi- 
rive to the distribution of ambiguity, and, given 
this extreme sensitivity, suggest that simulation 
studies need to be conducted in conjunction 
with a broader analysis which abstracts away 
from whatever linguistic particulars are neces- 
sary to bring about the sentences required to 
build the input sample that feeds the simulated 
learner. 
Ultimately, whether a particular acquisition 
model is successful is an empirical issue and de- 
pends on the exact conditions under which the 
model performs well and the extent to which 
those favorable conditions are in line with the 
facts of human language. Thus, I believe a 
three-fold approach to validate a computational 
model of acquisition is warranted. First, an 
abstract analysis (similar to the one presented 
here) should be constructed that can be used 
to uncover a model's sweet spots - -  where the 
shape of ambiguity is favorable to learning per- 
formance. Second, a computational psycholin- 
guistic study should be undertaken to see if the 
model's weet spots are in line with the distri- 
bution of ambiguity in natural anguage. And 
finally, a simulation should be carried out. 
Obviously, this a huge proposal requiring 
years of person-hours and coordinated planning 
among researchers with diverse skills. But if 
computational modeling is going to eventually 
lay claim to a model which accurately mir- 
rors the human process of language acquisition, 
years of fine grinding are necessary. 
Acknowledgments .  This work was supported 
in part by PSC-CUNY-30 Research Grant 61595- 
00-30. The three-fold approach is at the root of a 
project we have begun at The City University of New 
York. Much thanks to my collaborators Janet Dean 
Fodor and Virginia Teller for many useful discussions 
and input, as well as to two anonymous reviewers for 
their helpful comments. 
Re ferences  
E.J. Briscoe. 2000. Grammatical Acquisition: 
Inductive Bias and Coevolution of Language 
and the Language Acquisition Device. Lan- 
guage, 76(2). 
N. Chomsky. 1981. Lectures on Government 
and Binding. Foris. Dordrecht 
R. Clark. 1992. The selection of syntactic 
knowledge. Language Acquisition, 2(2):83- 
149. 
J.L. Elman, E. Bates, M.A. Johnson, 
A. Karmiloff-Smith, D. Parisi, and K. Plun- 
kett. Rethinking Innateness: A Connection- 
ist Perspective on Development. MIT Press, 
Cambridge, MA. 
J.D. Fodor. 1998. Unambiguous triggers. Lin- 
guistic Inquiry, 29(1):1-36. 
E. Gibson and K. Wexler. 1994. Triggers. Lin- 
guistic Inquiry, 25(3):407-454. 
P. Niyogi and R.C. Berwick. 1996. A language 
learning model for finite parameter spaces. 
Cognition, 61:161-193. 
W.G. Sakas. 2000. Ambiguity and the Compu- 
tational Feasibility of Syntax Acquisition. Un- 
published Ph.D. dissertation, City University 
of New York. 
W.G. Sakas and D. Demner-Fushman. I  Prep. 
Simulating Parameter Setting Performance in
Domains with a Large Number of Parameters: 
A Hybrid Approach. 
W.G. Sakas and J.D. Fodor. In Press. The 
Structural Triggers Learner. In Stefano 
Bertolo, editor, Parametric Linguistics and 
Learnability: A Self-contained Tutorial for 
Linguists. Cambridge University Press, Cam- 
bridge,UK. 
H.M. Taylor and S. Karlin. 1994. An Introduc- 
tion to Stochastic Modeling. Academic Press, 
San Diego, CA. 
C.D. Yang. 1999. A selectionist theory of lan- 
guage acquisition. In Proceedings of the 37th 
Annual Meeting of the A CL. Association for 
Computational Linguistics. 
66 
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 69?71,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Statistics vs. UG in language acquisition: 
Does a bigram analysis predict auxiliary inversion? 
 
 
Xu?n-Nga Cao Kam Iglika Stoyneshka Lidiya Tornyova 
PhD Program in Linguistics PhD Program in Linguistics PhD Program in Linguistics 
The Graduate Center, 
City University of New York 
The Graduate Center, 
City University of New York 
The Graduate Center, 
City University of New York 
xkam@gc.cuny.edu idst_r@yahoo.com ltornyova@gc.cuny.edu 
 
William Gregory Sakas Janet Dean Fodor 
PhD Programs in Computer Science and Linguistics 
The Graduate Center 
PhD Program in Linguistics  
The Graduate Center 
Department of Computer Science, 
Hunter College, 
City University of New York 
sakas@hunter.cuny.edu 
City University of New York 
jfodor@gc.cuny.edu 
 
 
 
 
Extended Abstract 
Reali & Christiansen (2003, 2004) have challenged 
Chomsky?s most famous "poverty of stimulus" 
claim (Chomsky, 1980) by showing that a 
statistical learner which tracks transitional 
probabilities between adjacent words (bigrams) 
can correctly differentiate grammatical and 
ungrammatical auxiliary inversion in questions like 
(1) and (2): 
 
(1) Is the little boy who is crying hurt? 
(2) *Is the little boy who crying is hurt? 
 
No examples like (1) occurred in the corpus that 
R&C employed, yet the grammatical form was 
chosen by the bigram model in 92% of the test 
sentence pairs. R&C conclude that no innate 
knowledge is necessary to guide child learners in 
making this discrimination, because the input 
evidently contains enough indirect statistical 
information (from other sentence types) to lead 
learners to the correct generalization.  
 
R&C's data are impressive, but there is reason to 
doubt that they extend to other natural languages or 
even to other constructions in English. While 
replicating R&C's Experiment 1 (see Data [A]), we 
discovered that its success rests on 'accidental' 
English facts. 
 
Six bigrams differ between the grammatical and 
ungrammatical versions of a sentence. (The 6 
relevant bigrams for the test sentence pair (1)/(2) 
are shown in Table 1.) However, 86% of the 
correctly predicted test sentences were definitively 
selected by the single bigram "who is" (or "that 
is"), because it occurred in the corpus and none of 
the remaining 5 bigrams did. 
 
Distinctive 
bigrams in (1) who is is crying crying hurt 
Distinctive 
bigrams in (2) who crying crying is is hurt 
 
Table 1. Six bigrams that differentiate Is the little 
boy who is crying hurt? from Is the little boy who 
crying is hurt? The first sentence is selected (as 
grammatical) solely due to the high probability of 
who is. 
 
It can be anticipated that when there is no 
bigram ?who/that is? in the grammatical test 
69
sentence (e.g., in relative clauses with object-gaps, 
auxiliaries such as was, can, or do-support), the 
learning will be less successful. Our results 
confirm this prediction: object relatives like (4) 
and (5), where "who/that is? is not present, were 
poorly discriminated (see Data [B]). 
 
(4) Is the wagon your sister is pushing red? 
(5) *Is the wagon your sister pushing is red? 
  
Results for sentences with only main verbs, 
requiring do-support in question-formation, like (6) 
and (7), were also very weak (see Data [C]). 
 
(6) Does the boy who plays the drum want a 
cookie? 
(7) *Does the boy who play the drum wants a 
cookie? 
  
Furthermore, the powerful effect of "who/that 
is" in R&C?s experiment reflects no knowledge of 
relative clauses. It rests on the homophony of 
English relative pronouns with interrogative "who" 
and deictic "that". In R&C's training-set, the 
phonological/orthographic form "who" occurred as 
relative pronoun only 3 times, but as interrogative 
pronoun 44 times. R&C's analysis didn't 
differentiate these. (Similarly for "that": 14 relative 
versus 778 deictic or complementizer.) 
 
In some languages relative pronouns are 
homophonous with other parts of speech (e.g., with 
determiners in German). We explored the possible 
effects of this by replacing the relative pronouns in 
the English corpus with ?the?. Discrimination 
between grammatical and ungrammatical aux-
inversion was poor (see Data [D]). 
 
Many human languages lack any such 
superficial overlaps with relative pronouns. So 
unless there are other cues instead, learning can be 
expected to be unsuccessful in those languages too. 
We tested this hypothesis in two ways:  
 
(i) We distinguished relative pronouns from their 
non-relative homophones in English by coding 
the former as ?who-rel? and ?that-rel? in both 
the corpus and the test sentences. We found a 
greatly reduced ability to select the grammatical 
aux-inversion construction (see Data [E]).  
 
(ii) We tested verb fronting in Dutch questions, 
using a Dutch corpus comparable to the English 
corpus used by R&C (the Groningen Dutch 
corpus from CHILDES; approximately 21,000 
utterances of child-directed speech, age 1;8 to 
1;11). Due largely to verb-final word order in 
relative clauses, there was no one distinctive 
bigram that could be relied on to predict the 
correct choice. Performance on a set of 20 items 
tested so far was no better than chance (see 
Data [F]). Clearly, the Dutch examples 
provided no alternative cues for selecting the 
grammatical version.   
 
Thus, the success rate in R&C?s experiment has 
very limited applicability. In general, bigram 
probability (or sentence cross-entropy, as 
computed in these experiments) is a poor predictor 
of grammaticality; e.g., the measure that prefers (1) 
over (2) mis-prefers (8) over (9):  
 
(8) *Scared you want to the doggie. 
(9) She can hear what we?re saying. 
 
We conclude that the bigram evidence against 
the poverty of the stimulus for language 
acquisition has not been substantiated to date. It 
remains to be seen whether richer statistics-based 
inductive models will offer more robust cross-
language learnability. 
References 
Chomsky, N. (1980). in M. Piattelli-Palmarini, 
(1980) Language and Learning: The Debate 
Between Jean Piaget and Noam Chomsky. 
Cambridge: Harvard University Press.  
 
Reali, F. & Christiansen, M. H. (2003). 
Reappraising Poverty of Stimulus Argument: A 
Corpus Analysis Approach. BUCLD 28 
Proceedings Supplement. 
 
Reali, F. & Christiansen, M. H. (2004). Structure 
Dependence in Language Acquisition: 
Uncovering the Statistical Richness of the 
Stimulus. Proceedings of the 26th Annual 
Meeting of the Cognitive Science Society. 
 
 
70
 Data 
 
  
% correct 
 
% incorrect 
 
% can?t 
choose 
# of sentence 
pairs tested 
to date 
 
Experiment 
A 87 13 0 100 Replication of R&C 
B 33 15 52 100 Object-gap 
C 50 50 0 50 Do-support 
D 17 41 42 100 ?The? replacement 
E 17 39 44 100 Who-rel/That-rel 
F 45 50 5 20 Dutch 
 
71
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 120?128,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Psychocomputational Linguistics: 
A Gateway to the Computational Linguistics Curriculum 
 
William Gregory Sakas 
Department of Computer Science, Hunter College 
Ph.D. Programs in Linguistics and Computer Science, The Graduate Center 
City University of New York (CUNY) 
695 Park Avenue, North 1008 
New York, NY, USA, 10065 
sakas@hunter.cuny.edu 
 
 
 
Abstract 
Computational modeling of human language 
processes is a small but growing subfield of 
computational linguistics. This paper 
describes a course that makes use of recent 
research in psychocomputational modeling as 
a framework to introduce a number of 
mainstream computational linguistics 
concepts to an audience of linguistics, 
cognitive science and computer science 
doctoral students. The emphasis on what I 
take to be the largely interdisciplinary nature 
of computational linguistics is particularly 
germane for the computer science students. 
Since 2002 the course has been taught three 
times under the auspices of the MA/PhD 
program in Linguistics at The City University 
of New York?s Graduate Center. A brief 
description of some of the students? 
experiences after having taken the course is 
also provided. 
1 Introduction 
A relatively small (but growing) subfield of 
computational linguistics, psychocomputational 
modeling affords a strong foundation from which 
to introduce graduate students in linguistics to 
various computational techniques, and students in 
computer science1 (CS) to a variety of topics in 
                                                          
1 For rhetorical reasons I will often crudely partition the 
student makeup of the course into linguistics students and CS 
students. This preempts lengthy illocutions such as ? ? the 
students with a strong computational background as compared 
to students with a strong linguistics background.? In fact there 
have been students from other academic disciplines in 
attendance bringing with them a range of technical facility in 
both CS and linguistics; linguistics students with an 
psycholinguistics, though it has rarely been 
incorporated into the computational linguistics 
curriculum.  
Psychocomputational modeling involves the 
construction of computer models that embody one 
or more psycholinguistic theories of natural 
(human) language processing and use. Over the 
past decade or so, there's been renewed interest 
within the computational linguistics community 
related to the possibility of incorporating human 
language strategies into computational language 
technologies. This is evidenced by the occassional 
special session at computational linguistics 
meetings (e.g., ACL-1999 Thematic Session on 
Computational Psycholinguistics), several 
workshops (e.g., COLING-2004, ACL-2005 
Psychocomputational Models of Human Language 
Acquisition, ACL-2004 Incremental Parsing: 
Bringing Engineering and Cognition Together), 
recent conference themes (e.g., CoNLL-2008 "... 
models that explain natural phenomena relating to 
human language") and regular invitations to 
psycholinguists to deliver plenary addresses at 
recent ACL and COLING meetings.  
Unfortunately, it is too often the case that 
computational linguistics programs (primarily 
those housed in computer science departments) 
delay the introduction of cross-disciplinary 
psycholinguistic / computational linguistics 
approaches until either late in a student's course of 
study (usually as an elective) or not at all. At the 
City University of New York (CUNY)?s Graduate 
Center (the primary Ph.D.-granting school of the 
university) I have created a course that presents 
                                                                                           
undergraduate degree in CS; and CS students with a strong 
undergraduate background in theoretical linguistics. 
120
research in this cross-disciplinary area relatively 
early in the graduate curriculum. I have also run an 
undergraduate version of the course at Hunter 
College, CUNY in the interdisciplinary Thomas 
Hunter Honors Program. 
I contend that a course developed within the 
m?lange of psycholinguistics and computational 
linguistics is not only a valuable asset in a student's 
repertoire of graduate experience, but can 
effectively be used as a springboard to introduce a 
variety of techniques and topics central to the 
broader field of CL/NLP. 
2 Background 
The CUNY Graduate Center (hereafter, GC) has 
doctoral programs in both computer science and 
linguistics. The Linguistics Program also contains 
two master's tracks. Closely linked to both 
programs, but administratively independent of 
either, there exists a Cognitive Science 
Concentration.2 In spring of 2000, I was asked by 
the Cognitive Science Coordinator to create an 
interdisciplinary course in "computing and 
language" that would be attractive to linguistics, 
speech and hearing, computer science, philosophy, 
mathematics, psychology and anthropology 
students. One might imagine how a newly-minted 
Ph.D. might react to this request. Well this one, not 
yet realizing the potential for abuse of junior 
faculty (a slightly more sage me later wondered if 
there was a last minute sabbatical-related 
cancellation of some course that needed to be 
replaced ASAP) dove in and developed 
Computational Mechanisms of Syntax Acquisition.  
The course was designed to cover generative 
and non-generative linguistics and debates 
surrounding (child) first language acquisition 
principally focused on the question of Chomsky?s 
Universal Grammar (UG), or not? Four 
computational models drawn from diverse 
paradigms ? connectionist learning, statistical 
formal language induction, principles-and-
parameters acquisition and acquisition in an 
optimality theory framework3 ? were presented and 
                                                          
2 This is not a state-registered program, but rather an "in-
house" means of allowing students to receive recognition of 
interdisciplinary studiy in cognitive science.  
3 Although the semester ended as we were only just getting to 
cover optimality theoretic acquisition. 
discussion of the UG-or-not debate was framed in 
the context of these models.  
Over the past eight years, I've taught three 
variations of this course gradually molding the 
course away from a seminar format into a 
seminar/lecture format, dropping a large chunk of 
the UG-or-not debate, and targeting the course 
primarily for students who are in search of a "taste" 
of computational linguistics who might very well 
go on to take other CL-related course work.4 What 
follows is a description of the design 
considerations, student makeup, and course content 
focusing on its last instantiation in Spring 2007. 
3 The Course: Computational Natural 
Language Learning 
Most readers will recognize the most recent title of 
the course which was shamelessly borrowed from 
the ACL?s Special Interest Group on Natural 
Language Learning?s annual meeting. Although 
largely an NLP-oriented meeting, the title and 
indeed many of the themes of the meeting?s CFPs 
over the years accurately portray the material 
covered in the course.  
The course is currently housed in the GC?s 
Linguistics Program and is primarily designed to 
serve linguistics doctoral and masters students who 
want some exposure to computational linguistics 
but with a decidedly linguistics emphasis. 
Importantly though, the course often needs to serve 
a high percentage of students from other graduate 
programs. 
The GC Linguistics and Computer Science 
Programs also offer other computational linguistics 
(CL) courses: a Natural Language Processing 
(NLP) applications survey course, a corpus 
analysis course, a statistical NLP course and a CL 
methods sequence (in addition to a small variety of 
electives). Although (at least until recently, see 
Section 7) these courses are not taught within a 
structured CL curriculum, they effectively serve as 
the ?meat-and-potatoes? CL courses which require 
projects and assignments involving programming, 
a considerable math component and extensive 
experimentation with existing NLP/CL 
                                                          
4 Though the details of the undergraduate version of the course 
are beyond the scope of this paper, it is worth noting that it did 
not undergo this gradual revision; it was structured much as 
the original Cognitive Science version of the course, actually 
with an increased UG-or-not discussion. 
121
applications. The students taking these classes 
have already reached the point where they intend 
to include a substantial amount of computational 
linguistics in their dissertation or master?s thesis.  
Computational Natural Language Learning is 
somewhat removed from these other courses and 
the design considerations were purposefully 
directed at providing an ?appetizer? that would 
both entice interested students into taking other 
courses, and prepare them with some experience in 
computational linguistics techniques. Over time the 
course has evolved to incorporate the following set 
of prerequisites and goals.  
 
? No programming prerequisite, no introduction to 
programming Many of the students who take the 
course are first or second year linguistics students 
who have had little or no programming 
background. Students are aware that 
"Programming for Linguists" is part of the CL 
methods sequence. They come to this course 
looking for either an overview of CL, or for how 
CL concepts might be relevant to psycholinguistic 
or theoretical linguistics research.  
Often there are students who have had a 
substantial programming background ? including 
graduate students in computer science. This hasn?t 
proved to be problematic since the assignments 
and  projects are designed not to involve 
programming. 
 
? Slight math prerequisite, exposure to 
probabilities, and information theory Students are 
expected to be comfortable with basic algebra. I 
dissuade students from taking the course who are 
intimidated by a one-line formula with a few Greek 
letters in it. Students are not expected to know 
what a conditional probability is, but will leave the 
course with a decent grasp of basic 
(undergraduate-level) concepts in probability and 
information theory. 
This lightweight math prerequisite actually does 
split the class for a brief time during the semester 
as the probability/information theory lecture and 
assignment is a cinch for the CS students, and 
typically quite difficult for the linguistics students. 
But this is balanced by the implementation of the 
design consideration expressed in the next bullet. 
 
? No linguistics prerequisite, exposure to syntactic 
theory Students need to know what a syntax tree is 
(at least in the formal language sense) but do not 
need to know a particular theory of human 
language syntax (e.g., X-bar theory or even S ? 
NP VP). By the end of the semester students will 
be comfortable with elementary syntax beyond the 
level covered by most undergraduate ?Ling 101? 
courses.  
 
? Preparation for follow-up CL courses Students 
leaving this course should be comfortably prepared 
to enter the other GC computational linguistics 
offerings.5  
 
? Appreciation of the interdisciplinary nature of 
CL Not all students move on to other 
computational linguistics courses. Perhaps the 
most important goal of the course is to expose CS 
and linguistics students (and others) to the role that 
computational linguistics can play in areas of 
theoretical linguistics and cognitive science 
research, and conversely to the role that cognitive 
science and linguistics can play in the field of 
computational linguistics.  
 
3.1 Topical units 
In this section I present the syllabus of the course 
framed in topical units. They have varied over the 
years; what follows is the content of the course 
mostly as it was taught in Spring 2007.  
Janet Dean Fodor and I lead an active 
psychocomputational modeling research group at 
the City University of New York: CUNY CoLAG ? 
CUNY Computational Language Acquisition 
Group which is primarily dedicated to the design 
and evaluation of computational models of first  
language acquisition. Most, though not all, of the 
topical units purposefully contain material that 
intersects with CUNY CoLAG?s ongoing research 
efforts. 
The depth of coverage of the units is designed to 
give the students some fluency in computational 
issues (e.g., use and ramifications of Markov 
assumptions), and a basic understanding beyond 
exposure to the computational mechanisms of CL 
(e.g., derivation of the standard MLE bigram 
                                                          
5 The one exception in the Linguistics Program is Corpus 
Linguistics which has a programming prerequisite, and the 
occasional CL elective course in Computer Science targeted 
primarily for their more advanced students. 
122
probability formula), but not designed to allow 
students to bypass a more computationally rigorous 
NLP survey course. The same is true of the breadth 
of coverage; a comprehensive survey is not the 
goal. For example, in the ngram unit, typically no 
more than two or at most three smoothing 
techniques are covered.  
Note that the citations in this section are mostly 
required reading, but some articles are optional. It 
has been my experience however, that the students 
by and large read most of the material since the 
readings were highly directed (i.e., which sections 
and pages are most relevant to the course.) 
Supplemental materials that present introductory 
mathematics and tutorial presentations are not 
exhaustively listed, but included Jurafsky and 
Martin (2000), Goldsmith (2007, previously 
online) and a host of  (other) online resources.  
 
History of linguistics and computation [1 
lecture] The history is framed around the question 
?Is computational linguistics, well uh, linguistics?? 
We conclude with ?It was, then it wasn?t, now 
maybe it is, or at least in part, should be.? The 
material is tightly tied to Lee (2004); with 
additional discussion along the lines of Sakas 
(2004). 
 
Syntax [1 lecture] This is a crash course in syntax 
using a context-free grammar with 
transformational movement. The more difficult 
topics include topicalization (including null-topic), 
Wh-movement and verb-second phenomena. We 
make effective use of an in-house database of 
abstract though linguistically viable cross-
linguistic sentence patterns and tree structures ? 
the CUNY CoLAG Domain (Sakas, 2003). The 
point of this lecture is to introduce non-linguistics 
students to the intricacies of a linguistically viable 
grammatical theory.  
 
Language Acquisition [1 lecture] We discuss 
some of the current debates in L1 (a child?s first) 
language acquisition surrounding ?no negative 
evidence? (Marcus, 1993), Poverty of the Stimulus 
(Pullum, 1996), and Chomsky?s conceptualization 
of Universal Grammar. This is the least 
computational lecture of the semester, although it 
often generates some of the most energized 
discussion. The language acquisition unit is the 
central arena in which we stage most of the rest of 
the topics in the course. 
 
Gold and the Subset Principle [2 lectures] 
During the presentation of Gold?s (1967) and 
Angluin's (1980) proofs and discussion of how 
they might be used to argue (often incorrectly) for 
a Universal Grammar (Johnson, 2004) some core 
CL topics are introduced including formal 
language classes (the Chomsky Hierarchy) and the 
notions of hypothesis space and search space. The 
first (toy) probabilistic analyses are also presented 
(e.g., given a finite enumeration and a probability p 
that an arbitrary non-target grammar licenses a 
sentence in the input sample, what is the ?worst 
case? number of sentences required to converge on 
the target grammar?) 
Next, the Subset Principle and linguistic 
overgeneralization (Fodor and Sakas, 2005) is 
introduced. An important focus is on how keeping 
(statistical) track of what?s not encountered might 
supply a ?retreat? mechanism to pull back from an 
over-general hypothesis. Although the 
mathematical details of how the statistics might 
work are omitted, this topic leads neatly into a unit 
on Bayesian learning later in the semester. 
This is an important two lectures. It's the first 
unit where students are exposed to the use of 
computational techniques applied to theoretical 
issues in psycholinguistics. By this point, students 
often are intellectually engaged in the debates 
surrounding L1 acquisition. To understand the 
arguments presented in this unit students need to 
flex their computational muscles for the first time. 
 
Connectionism [3 lectures] This unit covers the 
basics of Simple Recurrent Network (SRN) 
learning (Elman, 1990, 1993). More or less, Elman 
argues that language acquisition is not necessarily 
the acquisition of rules operating over atomic 
linguistic units (e.g., phrase markers) but rather the 
process of capturing the ?dynamics? of word 
patterns in the input stream. He demonstrates how 
this can be simulated in an SRN paradigm. 
The mechanics of how an SRN operates and can 
be used to model language acquisition phenomena 
is covered but more importantly core concepts 
common to most all supervised machine learning 
paradigms are emphasized. Topics include how 
training and testing corpora are developed and 
used, cross validation, hill-climbing, learning bias, 
123
linear and non-linear separation of the hypothesis 
space, etc. A critique of SRN learning is also 
covered (Marcus, 1998) which presents the 
important distinction between generalization 
performance and learning within the training 
space in a way that is approachable by non-CS 
students, but also interesting to CS-students. 
 
Information retrieval [1 lecture] Elman (1990) 
uses hierarchal clustering to analyze some of his 
results. I use Elman's application of clustering to 
take a brief digression from the psycholinguistics 
theme of the course and present an introduction to 
vector space models and document clustering.  
This is the most challenging technical lecture of 
the semester and is included only when there are a 
relatively high proportion of CS students in 
attendance. Most of the linguistics students get a 
decent feel for the material, but most require a 
second exposure it in another course to fully 
understand the math. That said, the linguistics 
students do understand how weight heuristics are 
used to effectively represent documents in vectors 
(though most linguistics students have a hard time 
swallowing the bag-of-words paradigm at face 
value), and how vectors can be nearer or farther 
from each other in a hypothesis space. 
 
Ngram language models [3 lecture] In this unit 
we return to psycholinguistics. Reali and 
Christiansen, (2005) present a simple ngram 
language model of child-directed speech to argue 
against the need for innate UG-provided 
knowledge of hierarchal syntactic structure. Basic 
probability and information theory is introduced ? 
conditional probabilities and Markov assumptions, 
the chain rule, Bayes Rule, maximum likelihood 
estimates, entropy, etc. Although relatively easy 
for the CS students (they had their hands full with 
the syntax unit), introduction of this material is 
invaluable to the linguistics students who need to 
be somewhat fluent in it before entering our other 
CL offerings. 
We continue with a presentation of the sparse data 
problem, Zipf?s law, corpus cross-entropy and a 
handful of smoothing techniques (Reali & 
Christiansen use a particularly impoverished 
version of deleted interpolation). We continue with 
a discussion of the pros and cons of employing 
Markov assumptions in computational linguistics 
generally, the relationship of Markov assumptions 
to incremental learning and psycholinguistic 
modeling, and the use of cross-entropy as an 
evaluation metric, and end with a brief discussion 
of the descriptional necessity (or not) of traditional 
generative grammars (Pereira, 2000).  
. 
"Ideal" learning, Bayesian learning and 
computational resources [1 lecture] Regier and 
Gahl (2004) in response to Lidz et al (2003) 
present a Bayesian learning model that learns the 
correct structural referent for anaphoric "one" in 
English from a corpus of child-directed speech. 
Similarly to Reali & Christiansen (op. cit.), they 
argue against the need for innate knowledge of 
hierarchal structure since their Bayesian model 
starts tabula rasa and learns from linear word 
strings with no readily observable structure.  
The fundamental mechanics of Bayesian 
inference is presented. Since most Bayesian 
models are able to retreat from overgeneral 
hypotheses in the absence of positive evidence, the 
course returns to overgeneralization errors, the 
Subset Principle and the alternative of using 
statistics as a possible retreat mechanism. 
Computationally heavy ("ideal") batch processing, 
and incremental (psychologically plausible) 
processing are contrasted here as is the use of 
heuristics (psycholinguistically-based or not) to 
mitigate the potentially huge computational cost of 
searching a large domain.  
 
Principle and parameters [2 lectures] As the 
academic scheduling has worked out, the course is 
usually offered during years when the Linguistics 
Program does not offer a linguistics-based 
learnability course. As a result, there is a unit on 
acquisition within a principles-and-parameters 
(P&P) framework (Chomsky, 1981). Roughly, in 
the P&P framework cross-linguistic commonalities 
are considered principles, and language variation is 
standardly specified by the settings of a bank of 
binary parameters (i.e., UG = principles + 
parameters; a specific language = principles + 
parameters + parameter settings). 
Although this unit is the furthest away from 
mainstream CL, it has served as a useful means to 
introduce deterministic learning (Sakas and Fodor, 
2001), versus non-deterministic learning (Yang, 
2002), the effectiveness of hill-climbing in 
124
linguistically smooth and non-smooth domains,6 as 
well as the notion of computational complexity and 
combinatorial explosion (n binary parameters 
yields a search space of 2n possible grammars). 
Finally, and perhaps most importantly there is 
extensive discussion of the difficulty of building 
computational systems that can efficiently and 
correctly learn to navigate through domains with 
an enormous amount of ambiguity. 
In the P&P framework ambiguity stems from 
competition of cross-linguistic structural analyses 
of surface word order patterns. For example, given 
a (tensed) S V O sentence pattern, is the V situated 
under the phrase maker I (English), or under the 
phrase marker C (German)? Although this is a 
somewhat different form of ambiguity than the 
within-language structural ambiguity that is all too 
familiar to those of use working in CL, it serves as 
useful background material for the next unit. 
 
Part of speech tagging and statistical parsing [3 
lectures] In this unit we begin by putting aside the 
psycholinguistics umbrella of the course and cover 
introductory CL in a more traditional manner. 
Using Charniak (1997) as the primary reading, we 
cover rudimentary HMM's, and probabilistic 
CFG's. We use supplemental materials to introduce 
lexicalized statistical parsing (e.g., Jurafsky and 
Martin, 2000 and online materials). We then turn 
back to psycholinguistics and after a (somewhat 
condensed overview) of human sentence 
processing, discuss the viability of probabilistic 
parsing as a model of human sentence processing 
(Keller, 2005). This unit, more than some others, is 
lightweight on detailed computational mechanics; 
the material is presented throughout at a level 
similar to that of Charniak?s article. For example 
the specifics of EM algorithms are not covered 
although what they do, and why they are necessary 
are. 
The Linguistics Program at CUNY is very active 
in human sentence processing research and this 
unit is of interest to many of the linguistics 
students. In particular we contrast computational 
approaches that employ nondeterminism and 
parallelism to mainstream psycholinguistics 
models which are primarily deterministic, serial 
and employ a reanalysis strategy when evaluating a 
                                                          
6 By "smooth", I mean a correlation between the similarity of 
grammars, and the similarity of languages they generate. 
parse ?online? (though of course there is a 
significant amount of research that falls outside of 
this mainstream). We then focus on issues of 
computational resources that each paradigm 
requires.  
In some ways the last lectures of this unit best 
embody the goal of exposing the students to the 
potential of interdisciplinary research in 
computational linguistics. The CS students leave 
with an appreciation of psycholinguistic 
approaches to human sentence processing, and the 
linguistics students with a firm grasp of the 
effectiveness of computational approaches. 
4 Assignments and Projects 
Across the three most recent incarnations of the 
course the number and difficulty of the 
assignments and projects has varied quite a bit. In 
the last version, there were three assignments (five 
to ten hours of student effort each) and one project 
(twenty to thirty hours effort). 
Due to the typically small size of the course, 
assignments and projects (beyond weekly 
readings) were often individually tailored and 
assessed. The goal of the assignments was to 
concretize the CL aspects of the primarily 
psycholinguistic readings with either hands-on use 
of the computer, mathematically-oriented problem 
sets, or a critical evaluation of the CL 
methodologies employed. A handful of examples 
follow.  
 
? Gold and the Subset Principle (Assignments) 
All students are asked to formulate a Markov chain 
(though at this point in the course, not by that 
name) of a Gold-style enumeration learner 
operating over a small finite domain (e.g., 4 
grammars, 12 sentences and a sentence to grammar 
mapping). The more mathematically inclined are 
additionally asked to calculate the expected value 
of the number of input sentences consumed by a 
learner operating over an enumeration of n 
grammars and given a generalized mapping of 
sentences to grammars, or to formally prove the 
learnability of any finite domain of languages 
given text (positive) presentation of input.  
 
? Connectionism (Assignments) All students 
were asked to pick a language from the CUNY 
CoLAG domain, develop a training and test set 
125
from that language using existing software and run 
a predict-the-next-word SRN simulation on either a 
MatLab or TLearn neural network platform. 
Linguistics and CS students were paired on this 
assignment. When the assignment is given, a 
relatively detailed after-class lab tutorial on how to 
run the software is presented. 
 
? Ngram language models (Projects) One CS 
student implemented a version of Reali and 
Christiansen?s experiment and was asked to 
evaluate the effectiveness of different smoothing 
techniques on child-directed speech and to design a 
study of how to evaluate differences between 
child-directed speech and adult-to-adult speech in 
terms of language modeling. A linguistics student 
was asked to write a paper explaining how one 
could develop a computational evaluation of how a 
bigram learner might be evaluated longitudinally. 
(I.e., to answer the question, how can one measure 
the effectiveness of a language model after each 
input sentence?). Another linguistics student (with 
strong CS skills) created an annotation tool that 
semi-automatically mapped child-directed speech 
in French onto the CoLAG Domain tag set.  
 
5 Students: Past and Current  
As mentioned earlier, the Linguistics Doctoral 
Program at CUNY has just recently begun to 
structure their computational linguistics offerings 
into a cohesive course of study (described briefly 
in Section 7). During the past several years 
Computational Natural Learning has been offered 
on an ad hoc basis primarily in response to student 
demand and demographics of students? 
computational skills. Since the course was not 
originally intended to serve any specific function 
as part of a larger curriculum, and was not 
integrated into a reoccurring schedule there has 
been little need to carry out a systematic evaluation 
of the impact of the course on students? academic 
careers. Still a few anecdotal accounts will help 
give a picture of the course?s effectiveness. 
After the first Cognitive Science offering of the 
course in 2000, approximately 30 graduate 
students have taken one of the three subsequent 
incarnations. Two of the earliest linguistics 
students went on to take undergraduate CS courses 
in programming and statistics, and subsequently 
came back to take graduate level CL courses.7 
They have obtained their doctorates and are 
currently working in industry as computational 
linguists. One is a lead software engineer for an 
information retrieval startup company in New 
York that does email data mining. And though I?ve 
lost track of the other student, she was at one point 
working for a large software company on the west 
coast.  
I am currently the advisor of one CS student, 
and two linguistics students who have taken the 
course. One linguistics student is in the throws of 
writing a dissertation on the plausibility of 
exploiting statistical regularities of various 
syntactic structures (contra regularities of word 
strings) in child-directed speech during L1 
acquisition. The other is relatively early in her 
academic career, but is interested in working on 
computational semantics and discourse analysis 
within a categorial grammar framework. Her 
thoughts currently revolve around establishing 
(and formalizing) relationships between traditional 
linguistics-oriented semantics and a computational 
semantics paradigm. She hopes to make 
contributions to both linguistics and CL. The CS 
student, also early in his career, is interested in 
semi-productive multi-word expressions and how 
young children can come to acquire them. His idea 
is to employ a learning component in a machine 
translation system that can be trained to translate 
productive metaphors between a variety of 
languages. 
These five students have chosen to pursue 
specific areas of study and research directly as a 
result of having taken Computational Natural 
Language Learning early in their careers.  
I am also sitting on two CS students? second 
qualifying exam committees. One is working on 
machine translation of Hebrew and the other 
working on (relatively) mainstream word-sense 
disambiguation. Both of their qualifying exam 
papers show a sensitivity to psycholinguistics that 
I?m frankly quite happy to see, and am sure 
wouldn?t have been present without their having 
taken the course.  
The parsing unit was just added this past spring 
semester and I?ve had two recent discussions with 
                                                          
7 The CL methods sequence was established only 3 years ago, 
previously students were encouraged to develop their basic 
computational skills at one of CUNY?s undergraduate schools. 
126
a second year linguistics student about 
incorporating a statistical component into a current 
psycholinguistic model of human sentence 
processing. Another second year student has 
expressed interested in doing a comprehensive 
study of neural network models of human sentence 
parsing for his first qualifying paper. It?s not clear 
that they will ultimately pursue these directions, 
but I?m certain they wouldn?t have thought of the 
possibilities if they hadn?t taken the Computational 
Natural Language Learning. 
Finally, most all of the students who have taken 
the course have also taken the NLP-survey course 
(no programming required), slightly less than a 
third have moved on to the CL methods sequence 
(includes an introduction to programming), or if 
they have some CS background move directly to 
Corpus Analysis (programming experience 
required as a prerequisite). We hope that 
eventually, especially in light of the GC?s new 
computational linguistics program, the course will 
serve as the gateway for many more students to 
begin to pursue studies that will lead to research 
areas in both psychocomputational modeling and 
more mainstream CL. 
6 Brief Discussion 
It is my view that computational linguistics is by 
nature a cross-disciplinary endeavor. Indeed, one 
could argue that only after the incorporation of 
techniques and strategies gleaned from theoretical 
advances in psychocomputational modeling of 
language, can we achieve truly transparent (to the 
user) human-computer language applications.  
That argument notwithstanding, a course such as 
the one described in this paper can effectively 
serve as an introduction to an assortment of 
concepts in computational linguistics that can 
broaden the intellectual horizons of both CS and 
linguistics students, as well providing a foundation 
that students can build on in the pursuit of more 
advanced studies in the field. 
7 Postscript: The Future of the Course  
The CUNY Graduate Center has recently created a 
structured computational linguistics program 
housed in the GC?s Linguistics Program. The 
program consists of a Computational Linguistics 
Concentration in the Linguistics Master?s 
subprogram, and particularly relevant to the 
discussion in this article, a Computational 
Linguistics Certificate8 (both fall under the 
acronym CLC). Any City University of New York 
doctoral student can enroll in CLC concurrently 
with enrollment in their primary doctoral program 
(as one might imagine, we expect a substantial 
number of Linguistics and CS doctoral candidates 
to enroll in the CLC program). 
Due to my newly-acquired duties as director of 
the CLC program and to scheduling constraints on 
CLC faculty teaching assignments, the course 
cannot be offered again until Fall 2009 or Spring 
2010. At that time Computational Natural 
Language Learning will need to morph into a more 
technically advanced elective course in applied 
machine learning techniques in computational 
linguistics (or some such) since the CLC course of 
study currently posits the NLP survey course and 
the CL Methods sequence as the first year 
introductory requirements.  
However, I expect that a course similar to the 
one described here will supplement the NLP 
survey course as a first year requirement in Fall 
2010. The course will be billed as having broad 
appeal and made available to both CLC students 
and linguistics, CS and other students who might 
not want or require the ?meat-and-potatoes? that 
CLC offers, but who only desire a CL ?appetizer?. 
Though if the appetizer is tasty enough, students 
may well hunger for the main course. 
Acknowledgments 
I would like to thank the three anonymous 
reviewers for helpful comments, and the many 
intellectually diverse and engaging students I?ve 
had the pleasure to introduce to the field of 
computational linguistics. 
References  
Angluin, D. (1980). Inductive inference of formal 
languages from positive data. Information and 
Control 45:117-135. 
Charniak, E. (1997). Statistical Techniques for Natural 
language Parsing. AI Magazine 18:33-44. 
Chomsky, N. (1981). Lectures on government and 
binding: Studies in generative grammar. Dordrecht: 
Foris. 
                                                          
8 Pending state Department of Education approval, hopefully 
to be received in Spring 2009. 
127
Elman, J. L. (1990). Finding structure in time. Cognitive 
Science 14:179-211. 
Elman, J. L. (1993). Learning and development in 
neural networks: The importance of starting small. 
Cognition 48:71-99. 
Fodor, J. D., and Sakas, W. G. (2005). The Subset 
Principle in syntax: Costs of compliance. Journal of 
Linguistics 41:513-569. 
Gold, E. M. (1967). Language identification in the limit. 
Information and Control 10:447-474. 
Goldsmith, J. (2007). Probability for Linguists. 
Mathematics and Social Sciences 180:5-40. 
Johnson, K. (2004). Gold?s Theorem and Cognitive 
Science. Philosophy of Science 71:571-592. 
Jurafsky, D., and Martin, J. H. (2000). Speech and 
Language Processing: An Introduction to Natural 
Language Processing, Computational Linguistics, 
and Speech Recognition   
Keller, F. (2005). Probabilistic Models of Human 
Sentence Processing. Presented at Probabilistic 
Models of Cognition: The Mathematics of Mind, 
IPAM workshop, Los Angeles. 
Lee, L. (2004). "I'm sorry Dave, I'm afraid I can't do 
that" : Linguistics, statistics, and natural language 
processing circa 2001. In Computer Science: 
Reflections on the Field, 111-118. 
Washington:National Academies Press. 
Lidz, J., Waxman, S., and Freedman, J. (2003). What 
infants know about syntax but couldn?t have learned: 
Experimental evidence for syntactic structure at 18 
months. Cognition 89:65-73. 
Marcus, G. F. (1993). Negative evidence in language 
acquisition. Cognition 46:53-85. 
Marcus, G. F. (1998). Can connectionism save 
constructivism?  Cognition 66:153-182. 
Pereira, F. (2000) Formal grammar and information 
theory: Together again. Philosophical Transactions 
of the Royal Society A358:1239-1253. 
Pullum, G. K. (1996). Learnability, hyperlearning, and 
the poverty of the stimulus. Proceedings of the 22nd 
Annual Meeting of the Berkley Linguistics Society: 
General Session and Parasession on the Role of 
Learnability in Grammatical Theory, Berkeley: 498-
513. 
Reali, F., and Christiansen, M. H. (2005). Uncovering 
the richness of the stimulus: Structural dependence 
and indirect statistical evidence. Cognitive Science 
29:1007-10018. 
Regier, T., and Gahl, S. (2004). Learning the 
unlearnable: The role of missing evidence. Cognition 
93:147-155. 
Sakas, W. G., and Fodor, J. D. (2001). The Structural 
Triggers Learner. In Language Acquisition and 
Learnability, ed. S. Bertolo, 172-233. Cambridge: 
Cambridge University Press. 
Sakas, W. G. (2003) A Word-Order Database for 
Testing Computational Models of Language 
Acquisition, Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics, 
ACL-2003: 415-422. 
Sakas, W. G. (2004) Introduction. Proceedings of the 
First Workshop on Psycho-computational Models of 
Human Language Acquisition, COLING-2004. 
Geneva: iv-vi. 
Yang, C. D. (2002). Knowledge and learning in natural 
language. New York: Oxford University Press. 
 
128
