Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371?380,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Accuracy-Based Scoring for DOT: Towards Direct Error Minimization for
Data-Oriented Translation
Daniel Galron
CIMS
New York University
galron@cs.nyu.edu
Sergio Penkale, Andy Way
CNGL
Dublin City University
{spenkale,away}
@computing.dcu.ie
I. Dan Melamed
AT&T Shannon Laboratory
{lastname}
@research.att.com
Abstract
In this work we present a novel technique
to rescore fragments in the Data-Oriented
Translation model based on their contri-
bution to translation accuracy. We de-
scribe three new rescoring methods, and
present the initial results of a pilot experi-
ment on a small subset of the Europarl cor-
pus. This work is a proof-of-concept, and
is the first step in directly optimizing trans-
lation decisions solely on the hypothesized
accuracy of potential translations resulting
from those decisions.
1 Introduction
The Data-Oriented Translation (DOT) (Poutsma,
2000) model is a tree-structured translation model,
in which linked subtree fragments extracted from
a parsed bitext are composed to cover a source-
language sentence to be translated. Each linked
fragment pair consists of a source-language side
and a target-language side, similar to (Wu, 1997).
Translating a new sentence involves composing
the linked fragments into derivations so that a
new source-language sentence is covered by the
source tree fragments of the linked pairs, where
the yields of the target-side derivations are the can-
didate translations. Derivations are scored accord-
ing to their likelihood, and the translation is se-
lected from the derivation pair with the highest
score. However, we have no reason to believe that
maximizing likelihood is the best way to maxi-
mize translation accuracy ? likelihood and accu-
racy do not necessarily correlate well.
We can frame the problem as a search problem,
where we are searching a space of derivations for
the one that yields the highest scoring translation.
By putting weights on the derivations in the search
space, we wish to point the decoder in the direc-
tion of the optimal translation. Since we want
the decoder to find the translation with the high-
est evaluation score, we would want to score the
derivations with weights that correlate well with
the particular evaluation measure in mind.
Much of the work in the MT literature has
focused on the scoring of translation decisions
made. (Yamada and Knight, 2001) follow (Brown
et al, 1993) in using the noisy channel model,
by decomposing the translation decisions mod-
eled by the translation model into different types,
and inducing probability distributions via max-
imum likelihood estimation over each decision
type. This model is then decoded as described
in (Yamada and Knight, 2002). This type of ap-
proach is also followed in (Galley et al, 2006).
There has been some previous work on
accuracy-driven training techniques for SMT, such
as MERT (Och, 2003) and the Simplex Armijo
Downhill method (Zhao and Chen, 2009), which
tune the parameters in a linear combination of var-
ious phrase scores according to a held-out tun-
ing set. While this does tune the relative weights
of the scores to maximize the accuracy of candi-
dates in the tuning set, the scores themselves in the
linear combination are not necessarily correlated
with the accuracy of the translation. Tillmann and
Zhang (2006) present a procedure to directly opti-
mize the global scoring function used by a phrase-
based decoder on the accuracy of the translations.
Similarly to MERT, Tillmann and Zhang estimate
the parameters of a weight vector on a linear com-
bination of (binary) features using a global objec-
tive function correlated with BLEU (Papineni et
al., 2002).
In this work, we prototype some methods for
moving directly towards incorporating a measure
of the translation quality of each fragment used,
bringing DOT more into the mainstream of cur-
rent SMT research. In Section 2 we describe
probability-based DOT fragment scoring. In Sec-
tion 3 we describe our rescoring setup and the
371
(a)
S
NP VP
V
likes
NP
S
NP VP
V
pla??t
PP
P
a`
NP
(b)
NP
John
NP
John
(c)
S
NP
John
VP
V
likes
NP
S
NP VP
V
pla??t
PP
P
a`
NP
John
(d)
NP
Mary
NP
Mary
Figure 1: Example DOT Fragments.
three rescoring methods. In Section 4, we describe
our experiments. In Section 5 we compare the
results of rescoring the fragments with the three
methods. In Section 6 we discuss some of the
decisions that are affected by our rescoring meth-
ods. Finally, we discuss the next steps in training
the DOT system by optimizing over a translation
accuracy-based objective function in Section 7.
2 DOT Scoring
As described in previous work (Poutsma, 2000;
Hearne and Way, 2003), DOT scores translations
according to the probabilities of the derivations,
which are in turn computed from the relative fre-
quencies of linked tree fragments in a parallel tree-
bank. Linked fragment pairs are conditionally in-
dependent, so the score of a derivation is the prod-
uct of the probabilities of all the linked fragments
used. To find the probability of a translation,
DOT marginalizes over the scores of all deriva-
tions yielding the translation.
From a parallel treebank aligned at the sub-
sentential level, we extract all possible linked frag-
ment pairs by first selecting all linked pairs of
nodes in the treebank to be the roots of a new sub-
tree pair, and then selecting a (possibly empty) set
of linked node pairs that are descendants of the
newly selected fragment roots and deleting all sub-
tree pairs dominated by these nodes. Leaves of
fragments can either be terminals, or non-terminal
frontier nodes where we can compose other frag-
ments (c.f. (Eisner, 2003)). We give example DOT
fragment pairs in Figure 1.
Given two subtree pairs ?s
1
, t
1
? and ?s
2
, t
2
?,
we can compose them using the DOT composi-
tion operator ? if the leftmost non-terminal fron-
tier node of s
1
is equal to the root node of s
2
,
and the leftmost non-terminal frontier node of s
1
?s
linked counterpart in t
1
is equal to the root node
of t
2
. The resulting tree pair consists of a copy
of s
1
where s
2
has been inserted at the leftmost
frontier node, and a copy of t
1
where t
2
has been
inserted at the node linked to s
1
?s leftmost frontier
node (Hearne and Way, 2003).
In Figure 1, fragment pair (a) is a fragment with
two open substitution sites. If we compose this
fragment pair with fragment pair (b), the source
side composition must take place on the leftmost
non-terminal frontier node (the leftmost NP). On
the target side we compose on the frontier linked
to the leftmost source side non-terminal frontier.
The result is fragment pair (c). If we now com-
pose the resulting fragment pair with fragment pair
(d), we obtain a fragment pair with no open sub-
stitution sites whose source-side yield is John likes
Mary and whose target-side yield is Mary pla??t a`
John. Note that there are two different derivations
using the fragment pairs in Figure 1 that result in
the same fragment pair, namely (a) ? (b) ? (d), and
(c) ? (d).
For a given linked fragment pair ?d
s
, d
t
?, the
probability assigned to it is
P (?d
s
, d
t
?) =
|?d
s
, d
t
?|
?
r(u
s
)=r(d
s
)?r(u
t
)=r(d
t
)
|?u
s
, u
t
?|
(1)
where |?d
s
, d
t
?| is the number of times the frag-
ment pair ?d
s
, d
t
? is found in the bitext, and r(d)
is the root nonterminal of d. Essentially, the prob-
ability assigned to the fragment pair is the relative
frequency of the fragment pair to the pair of non-
terminals that root the fragments.
Then, with the assumption that DOT fragments
are conditionally independent, the probability of a
derivation is
P (d) = P (?d
s
, d
t
?
1
? . . . ? ?d
s
, d
t
?
N
)
=
?
i
P (?d
s
, d
t
?
i
) (2)
In the original DOT formulation, DOT disam-
biguated translations according to their probabil-
ities. Since a translation can have many possible
derivations, to obtain the probability of a transla-
tion it is necessary to marginalize over the distinct
derivations yielding a translation. The probabil-
ity of a translation w
t
of a source sentence w
s
, is
372
given by (3):
P (w
s
, w
t
) =
?
d?D
P (d
?w
s
,w
t
?
) (3)
and the translation is chosen so as to maximize (4):
w?
t
= argmax
w
t
P (w
s
, w
t
) (4)
Hearne and Way (2006) examined alternative dis-
ambiguation strategies. They found that rather
than disambiguating on the translation probability,
the translation quality would improve by disam-
biguating on the derivation probability, as in (5):
w?
t
= argmax
d
P (d) (5)
Our analysis suggest that this is because many
derivations with very low probabilities generate
the same, poor translation. When applying Equa-
tion (3) to marginalize over those derivations, the
resulting score is higher for the poor translation
than a better translation with fewer derivations but
where the derivations had higher likelihood.
Using the DOT model directly is difficult ?
the number of fragments extracted from a paral-
lel treebank is exponential in the size of the tree-
bank. Therefore we use the Goodman reduction
of DOT (Hearne, 2005) to create an isomorphic
PCFG representation of the DOT model that is lin-
ear in the size of the treebank. The idea behind the
Goodman reduction is that rather than storing frag-
ments in the grammar and translating via compo-
sition, we simultaneously build up the fragments
using the PCFG reduction and compose them to-
gether. To perform the reduction, we first relabel
the two linked nodes (X, Y) with the new label
X=Y. We then label each node in the parallel tree-
bank with a unique Goodman index. Each binary-
branching node and its two children can be inter-
nal or root/frontier. We add rules to the grammar
reflecting the role that each node can take, keeping
unaligned nodes as fragment-internal nodes. So in
the case where a node and both of its children are
aligned, we commit 8 rules into the grammar, as
follows:
LHS ? RHS1 RHS2 LHS+a ? RHS1 RHS2
LHS ? RHS1+b RHS2 LHS+a ? RHS1+b RHS2
LHS ? RHS1 RHS2+c LHS+a ? RHS1 RHS2+c
LHS ? RHS1+b RHS+c LHS+a ? RHS1+b RHS2+c
A category label which ends in a ?+? symbol fol-
lowed by a Goodman index is fragment-internal
and all other nodes are either fragment roots or
S=S
1
N=N
3
John
VP
2
V
4
likes
N=N
5
Mary
S=S
1
N=N
4
Mary
VP
2
V
5
pla??t
PP
3
P
6
a`
N=N
7
John
Source PCFG Target PCFG
S=S? N=N VP+2 0.5 S=S? N=N VP+2 0.5
S=S? N=N+3 VP+2 0.5 S=S? N=N+4 VP+2 0.5
S=S+1? N=N VP+2 0.5 S=S+1? N=N VP+2 0.5
S=S+1? N=N+3 VP+2 0.5 S=S+1? N=N+4 VP+2 0.5
N=N? John 0.5 N=N?Mary 0.5
N=N+3? John 1 N=N+4?Mary 1
VP+2? V+4 N=N 0.5 VP+2? V+5 PP+3 1
VP+2? V+4 N=N+5 0.5 V+5? pla??t 1
V+4? likes 1 PP+3? P+6 N=N 0.5
N=N?Mary 0.5 PP+3? P+6 N=N+7 0.5
N=N+5?Mary 1 P+6? a` 1
N=N? John 0.5
N=N+7? John 1
Figure 2: A parallel tree and its corresponding Goodman re-
duction.
frontier nodes. A fragment pair, then, is a pair of
subtrees in which the root does not have an index,
all internal nodes have indices, and all the leaves
are either terminals or un-indexed nodes. We give
an example Goodman reduction in Figure 2.
While we store the source grammar and the tar-
get grammar separately, we also keep track of the
correspondence between source and target Good-
man indices and can easily identify the alignments
according to the Goodman indices. Probabilities
for the PCFG rules are computed monolingually
as in the standard Goodman reduction for DOP
(Goodman, 1996). In decoding with the Goodman
reduction, we first find the n-best parses on the
source side, and for each source fragment, we con-
struct the k-best fragments on the target side. We
finally compute the bilingual derivation probabil-
ities by multiplying the source and target deriva-
tion probabilities by the target fragment relative
frequencies conditioned on the source fragment.
There are a few problems with a likelihood-
based scoring scheme. First, it is not clear that
if a fragment is more likely to be seen in training
data then it is more likely to be used in a correct
translation of an unseen sentence. In our analysis
of the candidate translations of the DOT system,
we observed that frequently, the highest-likelihood
candidate translation output by the system was not
the highest-accuracy candidate inferred. An addi-
tional problem is that, as described in (Johnson,
2002), the relative frequency estimator for DOP
373
(and by extension, DOT) is known to be biased
and inconsistent.
3 Accuracy-Based Fragment Scoring
In our work, we wish to incorporate a measure
of fragment accuracy into the scoring. To do so,
we reformulate the scoring of DOT as log-linear
rather than probabilistic, in order to incorporate
non-likelihood features into the derivation scores.
For all tree fragment pairs ?d
s
, d
t
?, let
l(?d
s
, d
t
?) = log(p(?d
s
, d
t
?)) (6)
The general form of a rescored tree fragment will
be
s(?d
s
, d
t
?) = ?
0
l(?d
s
, d
t
?) +
k
?
i=1
?
i
f
i
(?d
s
, d
t
?)
(7)
where each ?
i
is the weight of that term in the fi-
nal score, and each f
i
(d) is a feature. In this work,
we only consider f
1
(d), an accuracy-based score,
although in future work we will consider a wide
variety of features in the scoring function, includ-
ing combinations of the different scoring schemes
described below, binary lexical features, binary
source-side syntactic features, and local target side
features. The score of a derivation is now given by
(8):
s(d) = s(?d
s
, d
t
?
1
? . . . ? ?d
s
, d
t
?
N
)
=
?
i
s(?d
s
, d
t
?
i
) (8)
In order to disambiguate between candidate
translations, we follow (Hearne and Way, 2006)
by using Equation (5).
3.1 Structured Fragment Rescoring
In all our approaches, we rescore fragments ac-
cording to their contribution to the accuracy of
a translation. We would like to give fragments
that contribute to good translations relatively high
scores, and give fragments that contribute to bad
translations relatively low scores, so that during
decoding fragments that are known to contribute to
good translations would be chosen over those that
are known to contribute to bad translations. Fur-
thermore, we would like to score each fragment in
a derivation independently, since bad translations
may contain good fragments, and vice-versa.
In practice, it is infeasible to rescore only those
fragments seen during the rescoring process, due
to the Goodman reduction for DOT. If we were to
properly rescore each fragment, a new rule would
need to be added to the grammar for each rule ap-
pearing in the fragment. Since the number of frag-
ments is exponential, this would lead to a substan-
tial increase in grammar size. Instead, we rescore
the individual rules in the fragments, by evenly di-
viding the total amount of scoring mass among the
rules of the particular fragment, and then assigning
them the average of the rule scores over all frag-
ments in which they appear. That is for each rule
r in a fragment f consisting of c
f
(r) rules with
score ?(f), the score of the rule is given as:
s(r) =
?
f :r?f
?(f)/c
f
(r)
|f |
(11)
This has the further advantage that we are al-
lowing fragments that were unseen during tuning
to be rescored according to previously seen frag-
ment substructures.
To implement this scheme, we select a set of or-
acle translations for each sentence in the tuning
data by evaluating all the candidate translations
against the gold standard translation using the F-
score (Turian et al, 2003), and selecting those
with the highest F
1
-measure, with exponent 1. We
use GTM, rather than BLEU, because BLEU is
not known to work well on a per-sentence level
(Lavie et al, 2004) as needed for oracle selection.
We then compare all the target-side fragments in-
ferred in the translation process for each candidate
translation against the fragments that yielded the
oracles. There are two relevant parts of the frag-
ments ? the internal yields (i.e. the terminal leaves
of the fragment) and the substitution sites (i.e. the
frontiers where other fragments attach). We score
the fragments rooted at the substitution sites sepa-
rately from the parent fragment. We can uniquely
identify the set of fragments that can be rooted at
substitution sites by determining the span of the
linked source-side derivation.
To compare two fragments, we define an edit
distance between them. For a given fragment d,
let r(d) be the root of the fragment, let r(d) ?
rhs1 be the left subtree of r(d), and let r(d) ?
rhs2 be the right subtree. The difference between
a candidate fragment d
c
and an oracle fragment
d
gs
is given by the equations in Table 1.
These equations define a minimum edit dis-
tance between two fragment trees, allowing sub-
fragment order inversion, insertion, and deletion
374
?(d
c
, d
gs
) =
(
0 if d
c
= d
gs
1 if d
c
6= d
gs
Base case: d
c
and d
gs
are unary subtrees or substitution sites (9)
?(d
c
, d
gs
) = min
8
>
>
>
>
<
>
>
>
>
:
?(d
c
? rhs1, d
gs
? rhs1) + ?(d
c
? rhs2, d
gs
? rhs2),
?(d
c
? rhs2, d
gs
? rhs1) + ?(d
c
? rhs1, d
gs
? rhs2) + 1,
?(d
c
, d
gs
? rhs1) + |y(d
gs
? rhs2)|,
?(d
c
, d
gs
? rhs2) + |y(d
gs
? rhs1)|,
?(d
c
? rhs1, d
gs
) + |y(d
c
? rhs2)|,
?(d
c
? rhs2, d
gs
) + |y(d
c
? rhs1)|
(10)
Table 1: The recursive relation defining the fragment difference between two fragments.
(a) A
B
b
C
c
(b) A
C
c
B
b
(c) D
A
B
b
F
f
E
e
Figure 3: Comparing trees (a) and (b) with our distance met-
ric yields a value of 1. The difference between trees (a) and
(c) is 2, and for trees (b) and (c) the distance is 3.
as edit operations. For example, the only dif-
ference between trees (a) and (b) in Figure 3 is
that their children have been inverted. To com-
pare these trees using our distance metric, we first
compute the first argument of the min function in
Equation (10), directly comparing the structure of
each immediate subtree. We then compute the sec-
ond argument, obtaining the cost of performing an
inversion, and finally compute the remaining argu-
ments, assessing the cost of allowing each tree to
be a direct subtree of the other. The result of this
computation is 1, representing the inversion oper-
ation required to transform tree (a) into tree (b).
If we compare trees (a) and (c) in Figure 3, we
obtain a value of 2, given that the minimum opera-
tions required to transform tree (a) into tree (c) are
inserting an additional subtree at the top level and
then substituting the subtree rooted by C for the
subtree rooted by F. If we compare tree (b) with
tree (c) then the distance is 3, since we are now
required to also replace the subtree rooted by C by
the one rooted by B.
Since it is not efficient to compute the differ-
ences directly, we utilize common substructures
and derive a dynamic programming implementa-
tion of the recursion. We compare each fragment
against the set of oracle fragments for the same
source span, and select the lowest cost as the score,
assigning the candidate the negative difference be-
tween it and the oracle fragment it is most similar
to, as in (12):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
??(d
t
, d
o
t
) (12)
In practice, given the Goodman reduction for
DOT, we divide the fragment score by the number
of rules in the fragment, and assign the average of
those scores for each rule instance across all frag-
ments rescored.
3.2 Normalized Structured Fragment
Rescoring
In the structured fragment rescoring scheme, the
scores that the fragments are assigned are the un-
normalized edit distances between the two frag-
ments. It may be better to normalize the fragment
scores, rather than using the minimum number of
tree transformations to convert one fragment into
the other. We would expect that when compar-
ing larger fragments, on average there would be
more transformations needed to change one into
the other than when comparing small fragments.
However in the previous scheme, small fragments
would have higher scores than large fragments,
since fewer differences would be observed. The
normalized score is given in (13):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
log(1 ? ?(d
t
, d
o
t
)/
max(|d
t
|, |d
o
t
|))
(13)
Essentially, we are normalizing the edit distance
by the maximum edit distance possible, namely
the size of the largest fragment of the two being
compared.
3.3 Fragment Surface Rescoring
The disadvantage of the minimum tree fragment
edit approach is that it explicitly takes the internal
375
syntactic structure of the fragment into account.
In comparing two fragments, they may have the
same (or very similar) surface yields, but differ-
ent internal structures. The previous approach
would penalize the candidate fragment, even if its
yield is quite close to the oracle. In this rescor-
ing method, we extract the leaves of the candi-
date and oracle fragments, representing the substi-
tution sites by the source span which their frag-
ments cover. We then compare them using the
Damerau-Levenshtein distance ?
dl
(d
c
, d
gs
) (Dam-
erau, 1964) between the two fragment yields, and
score them as in (14):
f(?d
s
, d
t
?) = max
?d
o
s
,d
o
t
??D
o
:d
o
s
=d
s
??
dl
(d
t
, d
o
t
) (14)
In Equation (14) we are selecting the maximal
score for ?d
s
, d
t
? from its comparison to all the
possible corresponding oracle fragments. In this
way, we are choosing to score ?d
s
, d
t
? against the
oracle fragment it is closest to.
4 Experiments
For our pilot experiments, we tested all the rescor-
ing methods in the previous section on Spanish-to-
English translation against the relative-frequency
baseline. We randomly selected 10,000 sentences
from the Europarl corpus (Koehn, 2005), and
parsed and aligned the bitext as described in (Tins-
ley et al, 2009). From the parallel treebank, we
extracted a Goodman reduction DOT grammar, as
described in (Hearne, 2005), although on an order
of magnitude greater amount of training data. Un-
like (Bod, 2007), we did not use the unsupervised
version of DOT, and did not attempt to scale up
our amount of training data to his levels, although
in ongoing work we are optimizing our system to
be able to handle that amount of training data. To
perform the rescoring, we randomly chose an ad-
ditional 30K sentence pairs from the Spanish-to-
English bitext. We rescored the grammar by trans-
lating the source side of the 10K training sentence
pairs and 10K of the additional sentences, and us-
ing the methods in Section 3 to score the frag-
ments derived in the translation process. We then
performed the same experiment translating the full
40K-sentence set. Rules in the grammar that were
not used during tuning were rescored using a de-
fault score defined to be the median of all scores
observed.
Our system performs translation by first obtain-
ing the n-best parses for the source sentences and
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.30 10.31 10.32 10.27 10.08
NSFR 8.31 9.37 9.53 9.66 9.90
FSR 10.19 10.25 10.18 10.19 9.93
NIST SFR 3.792 3.805 3.808 3.800 3.781
NSFR 3.431 3.638 3.661 3.693 3.722
FSR 3.784 3.799 3.792 3.795 3.764
F-SCORE SFR 40.92 40.82 40.86 40.84 40.78
NSFR 37.53 39.50 39.93 40.38 40.78
FSR 40.83 40.85 40.87 40.91 40.67
Table 2: Results on test set. Rescoring on 20K sentences.
SFR stands for Structured Fragment Rescoring, NSFR for
Normalized SFR and FSR for Fragment Surface Rescoring.
system-i-j represents the corresponding system with ?
0
= i
and ?
1
= j. Underlined results are statistically significantly
better than the baseline at p = 0.01.
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.59 10.58 10.41 10.38 10.08
NSFR 8.61 9.71 9.90 9.96 9.93
FSR 10.49 10.48 10.35 10.38 10.06
NIST SFR 3.841 3.835 3.810 3.807 3.785
NSFR 3.515 3.694 3.713 3.734 3.727
FSR 3.834 3.833 3.820 3.816 3.784
F-SCORE SFR 41.12 40.99 40.86 40.88 40.75
NSFR 38.16 40.39 40.69 40.90 40.75
FSR 41.03 41.02 41.01 40.98 40.72
Table 3: Results on test set. Rescoring on 40K sentences. Un-
derlined are statistically significantly better than the baseline
at p = 0.01.
then computing the k-best bilingual derivations for
each source parse. In our experiments we used
beams of n = 10, 000 and k = 5. We also ex-
perimented with different values of ?
0
and ?
1
in
Equation (7). We set these parameters manually,
although in future work we will automatically tune
them, perhaps using a MERT-like algorithm.
We tested our rescored grammars on a set of
2,000 randomly chosen Europarl sentences, and
used a set of 200 randomly chosen sentences as
a development test set. 1
5 Results
Translation quality results can be found in Tables
2 and 3. In these tables, columns labeled i-j in-
dicate that the corresponding system was trained
using parameters ?
0
= i and ?
1
= j in Equa-
tion 7. Statistical significance tests for NIST and
BLEU were performed using Bootstrap Resam-
pling (Koehn, 2004).
1All sentences, including the ones used for training, were
limited to a length of at most 20 words.
376
BLEU NIST F-SCORE
Baseline 10.82 3.493 42.31
2-8 4-6 5-5 6-4 8-2
BLEU SFR 11.34 12.12 11.94 11.97 11.78
NSFR 9.68 10.99 11.38 11.63 11.30
FSR 11.40 11.49 11.72 11.91 11.72
NIST SFR 3.653 3.727 3.723 3.708 3.694
NSFR 3.376 3.530 3.554 3.616 3.572
FSR 3.655 3.675 3.698 3.701 3.675
F-SCORE SFR 44.84 45.47 45.36 45.33 45.08
NSFR 41.44 43.38 44.18 44.79 44.26
FSR 44.68 44.91 45.15 45.19 44.82
Table 4: Results on development test set. Rescoring on 40K
sentences.
As Table 2 indicates, all three rescoring meth-
ods significantly outperform the relative frequency
baseline. The unnormalized structured fragment
rescoring method performed the best, with the
largest improvement of 1.5 BLEU points, a 17.5%
relative improvement. We note that the BLEU
scores for both the baseline and the experiments
are low. This is to be expected, because the gram-
mar is extracted from a very small bitext espe-
cially when the heterogeneity of the Europarl cor-
pus is considered. In our analysis, only 32.5 per-
cent of the test sentences had a complete source-
side parse, meaning that a lot of structural infor-
mation is lost contributing to arbitrary target-side
ordering. In these experiments we did not use an
additional language model. DOT (and many other
syntax-based SMT systems) essentially have the
target language model encoded within the trans-
lation model, since the inferences derived dur-
ing translations link source structures to target
structures, so in principle, no additional language
model should be necessary. Furthermore, we only
evaluate against a single reference, which also
contributes to the lowering of absolute scores. To
provide a sanity check against a state-of-the-art
system, we trained the Moses phrase-based MT
system (Koehn et al, 2007) using our training
corpus, using no language model and using uni-
form feature weights, to provide a fair comparison
against our baseline. We used this system to de-
code our development test set, and as a result we
obtained a BLEU score of 10.72, which is compa-
rable to the score obtained by our baseline on the
same set.
When we scale up to tuning on 40,000 sen-
tences we see an improvement in BLEU scores as
well, as shown in Table 3. When tuning on 40K
sentences, we observe an increase of 1.81 BLEU
points on the best-performing system, which is a
20.6% improvement over the baseline. We note
that rescoring on 20K sentences rescores approxi-
mately 275,000 rules out of 655,000 in the gram-
mar, whereas rescoring on 40K sentences rescores
approximately 280,000.
To analyze the benefits of the rescored gram-
mar, we set aside a separate development set that
we decoded with the grammar trained on 40K sen-
tences. The results are presented in Table 4. The
analysis is presented in Section 6.
Interestingly, there is a large difference between
the normalized and unnormalized versions of the
SFR scoring scheme. Our analysis suggests that
the differences are mostly due to numerical issues,
namely the difference in magnitude between the
NSFR scores and the likelihood scores in the linear
combination, and the default value assigned when
the NSFR score was zero. In ongoing work, we
are working to address these issues.
For most configurations the difference between
SFR and FSR was not statistically significant at
p = 0.05. Our analysis indicated that surface dif-
ferences tended to co-occur with structural differ-
ences. We hypothesize that as we scale up to larger
and more ambiguous grammars, the system will
infer more derivations with the same yields, ren-
dering a larger difference between the quality of
the two scoring mechanisms.
6 Discussion
To analyze the advantages and disadvantages of
our approach over the baseline, we closely ex-
amined and compared the derivations made on
the devset translation by the SFR-scored gram-
mar and the likelihood-scored grammar. Although
the BLEU scores are rather low, there were sev-
eral sentences in which the SFR-scored grammar
showed a marked improvement over the baseline.
We observed two types of improvements.
The first is where the rescored grammar gave
us translations that, while still generally bad, were
closer to the gold standard than the baseline trans-
lation. For example, the Spanish sentence ?Y en
tercer lugar , esta? el problema de la aplicacio?n uni-
forme del Derecho comunitario .? translates into
the gold standard ?Thirdly , we have the problem
of the uniform application of Community law .?
The baseline grammar translates the sentence as
?on third place , Transport and Tourism . I are
the problems of the implementation standardised
is the EU law .? with a GTM F-Score of 0.378,
377
sn=NP+67600 ?1.97/?5.66
NP+67608
the rapporteur
sp=PP+67601
s=IN ?0.48/?0.37
in
sn=SBAR+165198 ?1.39/?1.90
nc=TO+165203
to
dn=VP 0/?0.49
make
sn=NP+36950 ?5.89/?5.09
NP+36952
the rapporteur
sp=PP+36951 ?4.28/?3.81
s=IN ?0.48/?0.37
in
sn=NP+36953
dn=DT 0/?0.58
both
nc=NNS ?1.03/?0.81
questions
Figure 4: Target side of the highest-scoring translations for a sentence, according to the baseline system (left) and the SFR
system (right). Boxed nodes are substitution sites. Scores in superscripts denote the score of the sub-derivation according to
the baseline and to the SFR system.
and the rescored grammar outputs the translation
?to there in the third place , I are the problem of
the implementation standardised is the Commu-
nity law .?, with an F-Score of 0.5. While many of
the fragments in the derivations that yielded these
two translations differ, the ones we would like to
focus on are the fragments that yield the transla-
tion of ?comunitario?. The grammar contains sev-
eral competing unary fragment pairs for ?comuni-
taro?. In the baseline grammar, the pair (aq=NNP
? comunitario, aq=NNP ? EU) has a score
of ?0.693147, whereas the pair (aq=NNP ?
comunitario, aq=NNP? Community) has a
score of ?1.38629. In the rescored grammar how-
ever, (aq=NNP ? comunitario, aq=NNP ?
EU) has a score of -0.762973, whereas (aq=NNP
? comunitario, aq=NNP ? Community)
has a score of -0.74399. In effect, the rescoring
scheme rescored the word alignment itself. This
suggests that in future work, it may be possible
to integrate a word aligner or fragment aligner di-
rectly into the MT training method.
The other improvement was where the baseline
and the SFR-scored grammar output translations
of roughly the same quality according to the eval-
uation measure, yet in terms of human evaluation,
the SFR translation was much better than the base-
line translation. For instance, our devset contained
the Spanish sentence ?Estoy de acuerdo con el po-
nente en dos cuestiones .? The baseline transla-
tion given is ?I agree with the rapporteur in to
make .?, and the SFR-scored translation given is
?I agree with the rapporteur in both questions .?.
While both translations have the same GTM score
against the gold standard ?I agree with the rap-
porteur on two issues .?, clearly, the second one
is of far higher quality than the first. As we can
see in Figure 4, the derivation over the substring
?in both questions? gets a higher score than ?in
to make? when translated with the rescored gram-
mar. In the baseline, ?en dos cuestiones? is not
translated as a whole unit ? rather, the derivation of
?el ponente en dos cuestiones? is decomposed into
four subderivations, yielding ?el? ?ponente? ?en?
?dos cuestiones?, where each of those is translated
separately, into ??? ?the rapporteur? ?in? and ?to
make?. The SFR-scored grammar, however, out-
puts a different bilingual derivation. The source
is decomposed into five sub-derivations, one for
each word, and each word is translated separately.
Then, the rescored target fragments set the proper
target-side word order and select the target-side
words that maximize the score of the subderiva-
tion covering the source span. We note that in this
example, the score of translating ?dos? to ?make?
was higher than the score of translating ?dos? to
?both?. However, the higher level target frag-
ment that composed the translation of ?dos? to-
gether with the translation of ?cuestiones? yielded
a higher score when composing ?both questions?
rather than ?to make?.
7 Conclusions and Future Work
The results presented above indicate that aug-
menting the scoring mechanism with an accuracy-
based measure is a promising direction for transla-
tion quality improvement. It gives us a statistically
significant improvement over the baseline, and our
analysis has indicated that the system is indeed
making better decisions, moving us a step closer
towards the goal of making translation decisions
based on the hypothesis of the resulting transla-
378
tion?s accuracy.
Now that we have demonstrated that translation
quality can be improved by incorporating a mea-
sure of fragment quality into the scoring scheme,
our immediate next step is to optimize our sys-
tem so that we can scale up to significantly larger
training and tuning sets, and determine whether
the improvements we have noted carry over when
the likelihood is computed from more data. Af-
terwards, we will implement a training scheme
to maximize an accuracy-based objective func-
tion, for instance, by minimizing the difference
between the scores of the highest-scoring deriva-
tion and the oracle derivations, in effect maximiz-
ing the score of the highest-scoring translation.
The rescoring method presented in this paper
need not be limited to DOT. Fragments can be
thought of as analogous to phrases in Phrase-
Based SMT systems ? we could implement a sim-
ilar rescoring system for phrase-based systems,
where we generate several candidate translations
for source sentences in a tuning set, and score each
phrase used against the phrases used in a set of or-
acles. More broadly, we could potentially take any
statistical MT system, and compare the features
of all candidates generated against those of oracle
translations, and score those that are closer to the
oracle higher than those further away.
Finally, by explicitly framing the translation
problem as a search problem, where we are di-
vorcing the inferences in the search space (i.e.
the model) from the path we take to find the op-
timal inference according to some criterion (i.e.
the scoring scheme), we can remove some of the
variability when comparing two models or scoring
mechanisms (Lopez, 2009).
Acknowledgements
This work is supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142). We would like to
thank the anonymous reviewers for their helpful
comments and suggestions.
References
R. Bod. 2007. Unsupervised syntax-based ma-
chine translation: The contribution of discontiguous
phrases. In Proceedings of the 11th Machine Trans-
lation Summit, pages 51?57, Copenhagen, Den-
mark.
P. F. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
F. J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7(3):171?176.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Companion Volume,
pages 205?208, Sapporo.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. 2006. Scalable in-
ference and training of context-rich syntactic trans-
lation models. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 961?968, Sydney, Aus-
tralia.
J. Goodman. 1996. Efficient algorithms for parsing the
DOP model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 143?152, Philadelphia, PA.
M. Hearne and A. Way. 2003. Seeing the wood for the
trees: Data-oriented translation. In Proceedings of
the Ninth Machine Translation Summit, pages 165?
172, New Orleans, LA.
M. Hearne and A. Way. 2006. Disambiguation strate-
gies for data-oriented translation. In Proceedings of
the 11th Conference of the European Association for
Machine Translation, pages 59?68, Oslo, Norway.
M. Hearne. 2005. Data-Oriented Models of Parsing
and Translation. Ph.D. thesis, Dublin City Univer-
sity, Dublin, Ireland.
M. Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):71?76, March.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics, demonstation session, pages
177?180, Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
379
the Conference on Empirical Methods in Natural
Language Processing, pages 388?395, Barcelona,
Spain.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
A. Lavie, K. Sagae, and S. Jayaraman. 2004. The sig-
nificance of recall in automatic metrics for MT eval-
uation. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Ameri-
cas, pages 134?143, Washington, DC.
A. Lopez. 2009. Translation as weighted deduction. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532?540,
Athens, Greece.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 311?318, Philadelphia, PA.
A. Poutsma. 2000. Data-oriented translation. In The
18th International Conference on Computational
Linguistics, pages 635?641, Saarbru?cken, Germany.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical MT. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 721?728, Sydney, Australia.
J. Tinsley, M. Hearne, and A. Way. 2009. Parallel tree-
banks in phrase-based statistical machine transla-
tion. In Proceedings of the Tenth International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing), pages 318?331, Mex-
ico City, Mexico.
J. Turian, L. Shen, and I. D. Melamed. 2003. Eval-
uation of machine translation and its evaluation. In
Proceedings of the Ninth Machine Translation Sum-
mit, pages 386?393, New Orleans, LA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France.
K. Yamada and K. Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 303?310, Philadelphia, PA.
B. Zhao and S. Chen. 2009. A simplex armijo
downhill algorithm for optimizing statistical ma-
chine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 21?
24, Boulder, Colorado.
380
  	
ff 	
	ffStatistical Machine Translation by Parsing
I. Dan Melamed
Computer Science Department
New York University
New York, NY, U.S.A.
10003-6806
 
lastname  @cs.nyu.edu
Abstract
In an ordinary syntactic parser, the input is a string,
and the grammar ranges over strings. This paper
explores generalizations of ordinary parsing algo-
rithms that allow the input to consist of string tu-
ples and/or the grammar to range over string tu-
ples. Such algorithms can infer the synchronous
structures hidden in parallel texts. It turns out that
these generalized parsers can do most of the work
required to train and apply a syntax-aware statisti-
cal machine translation system.
1 Introduction
A parser is an algorithm for inferring the structure
of its input, guided by a grammar that dictates what
structures are possible or probable. In an ordinary
parser, the input is a string, and the grammar ranges
over strings. This paper explores generalizations of
ordinary parsing algorithms that allow the input to
consist of string tuples and/or the grammar to range
over string tuples. Such inference algorithms can
perform various kinds of analysis on parallel texts,
also known as multitexts.
Figure 1 shows some of the ways in which ordi-
nary parsing can be generalized. A synchronous
parser is an algorithm that can infer the syntactic
structure of each component text in a multitext and
simultaneously infer the correspondence relation
between these structures.1 When a parser?s input
can have fewer dimensions than the parser?s gram-
mar, we call it a translator. When a parser?s gram-
mar can have fewer dimensions than the parser?s
input, we call it a synchronizer. The corre-
sponding processes are called translation and syn-
chronization. To our knowledge, synchronization
has never been explored as a class of algorithms.
Neither has the relationship between parsing and
word alignment. The relationship between trans-
lation and ordinary parsing was noted a long time
1A suitable set of ordinary parsers can also infer the syntac-
tic structure of each component, but cannot infer the correspon-
dence relation between these structures.
tra
ns
lat
ion
syn
ch
ron
iza
tio
n
syn
ch
ron
ou
s p
ars
ing
1
parsing
32
2
3
1
.
.
.
...
ordinary
I = dimensionality of input
D
 =
 d
im
en
sio
na
lity
 o
f g
ra
m
m
ar
synchronization
(I >= D)
parsing
synchronous
(D=I)
word
alignment
translation
(D >= I)
ordinary
parsing
(D=I=1)
generalized parsing
(any D; any I)
Figure 1: Generalizations of ordinary parsing.
ago (Aho & Ullman, 1969), but here we articu-
late it in more detail: ordinary parsing is a spe-
cial case of synchronous parsing, which is a special
case of translation. This paper offers an informal
guided tour of the generalized parsing algorithms in
Figure 1. It culminates with a recipe for using these
algorithms to train and apply a syntax-aware statis-
tical machine translation (SMT) system.
2 Multitext Grammars and Multitrees
The algorithms in this paper can be adapted for any
synchronous grammar formalism. The vehicle for
the present guided tour shall be multitext grammar
(MTG), which is a generalization of context-free
grammar to the synchronous case (Melamed, 2003).
We shall limit our attention to MTGs in Generalized
Chomsky Normal Form (GCNF) (Melamed et al,
2004). This normal form allows simpler algorithm
descriptions than the normal forms used by Wu
(1997) and Melamed (2003).
In GCNF, every production is either a terminal
production or a nonterminal production. A nonter-
minal production might look like this:
 

	

	

	



A
D(2)
B
Generalized Multitext Grammars
I. Dan Melamed
Computer Science Department
New York University
715 Broadway, 7th Floor
New York, NY, 10003, USA
 
lastname  @cs.nyu.edu
Giorgio Satta
Dept. of Information Eng?g
University of Padua
via Gradenigo 6/A
I-35131 Padova, Italy
 
lastname  @dei.unipd.it
Benjamin Wellington
Computer Science Department
New York University
715 Broadway, 7th Floor
New York, NY, 10003, USA
 
lastname  @cs.nyu.edu
Abstract
Generalized Multitext Grammar (GMTG) is a syn-
chronous grammar formalism that is weakly equiv-
alent to Linear Context-Free Rewriting Systems
(LCFRS), but retains much of the notational and in-
tuitive simplicity of Context-Free Grammar (CFG).
GMTG allows both synchronous and independent
rewriting. Such flexibility facilitates more perspic-
uous modeling of parallel text than what is possible
with other synchronous formalisms. This paper in-
vestigates the generative capacity of GMTG, proves
that each component grammar of a GMTG retains
its generative power, and proposes a generalization
of Chomsky Normal Form, which is necessary for
synchronous CKY-style parsing.
1 Introduction
Synchronous grammars have been proposed for
the formal description of parallel texts representing
translations of the same document. As shown by
Melamed (2003), a plausible model of parallel text
must be able to express discontinuous constituents.
Since linguistic expressions can vanish in transla-
tion, a good model must be able to express inde-
pendent (in addition to synchronous) rewriting. In-
version Transduction Grammar (ITG) (Wu, 1997)
and Syntax-Directed Translation Schema (SDTS)
(Aho and Ullman, 1969) lack both of these prop-
erties. Synchronous Tree Adjoining Grammar
(STAG) (Shieber, 1994) lacks the latter and allows
only limited discontinuities in each tree.
Generalized Multitext Grammar (GMTG) offers
a way to synchronize Mildly Context-Sensitive
Grammar (MCSG), while satisfying both of the
above criteria. The move to MCSG is motivated
by our desire to more perspicuously account for
certain syntactic phenomena that cannot be easily
captured by context-free grammars, such as clitic
climbing, extraposition, and other types of long-
distance movement (Becker et al, 1991). On the
other hand, MCSG still observes some restrictions
that make the set of languages it generates less ex-
pensive to analyze than the languages generated by
(properly) context-sensitive formalisms.
More technically, our proposal starts from Mul-
titext Grammar (MTG), a formalism for synchro-
nizing context-free grammars recently proposed by
Melamed (2003). In MTG, synchronous rewriting
is implemented by means of an indexing relation
that is maintained over occurrences of nonterminals
in a sentential form, using essentially the same ma-
chinery as SDTS. Unlike SDTS, MTG can extend
the dimensionality of the translation relation be-
yond two, and it can implement independent rewrit-
ing by means of partial deletion of syntactic struc-
tures. Our proposal generalizes MTG by moving
from component grammars that generate context-
free languages to component grammars whose gen-
erative power is equivalent to Linear Context-Free
Rewriting Systems (LCFRS), a formalism for de-
scribing a class of MCSGs. The generalization is
achieved by allowing context-free productions to
rewrite tuples of strings, rather than single strings.
Thus, we retain the intuitive top-down definition of
synchronous derivation original in SDTS and MTG
but not found in LCFRS, while extending the gen-
erative power to linear context-free rewriting lan-
guages. In this respect, GMTG has also been in-
spired by the class of Local Unordered Scattered
Context Grammars (Rambow and Satta, 1999). A
syntactically very different synchronous formalism
involving LCFRS has been presented by Bertsch
and Nederhof (2001).
This paper begins with an informal description of
GMTG. It continues with an investigation of this
formalism?s generative capacity. Next, we prove
that in GMTG each component grammar retains its
generative power, a requirement for synchronous
formalisms that Rambow and Satta (1996) called
the ?weak language preservation property.? Lastly,
we propose a synchronous generalization of Chom-
sky Normal Form, which lays the groundwork for
synchronous parsing under GMTG using a CKY-
style algorithm (Younger, 1967; Melamed, 2004).
2 Informal Description and Comparisons
GMTG is a generalization of MTG, which is itself
a generalization of CFG to the synchronous case.
Here we present MTG in a new notation that shows
the relation to CFG more clearly. For example, the
following MTG productions can generate the multi-
text [(I fed the cat), (ya kota kormil)]:1
  (S)  (S)    PN  VP 
	  PN  VP 	 (1)
 
PN 	

PN 	
 
I 	

ya 	 (2)
 
VP 	

VP 	
 
V  NP  	

NP  V  	 (3)
 
V 	

V 	
 
fed 	

kormil 	 (4)
 
NP 	

NP 	
 
D  N  	

N  	 (5)
 
D 	

	
 
the 	

	 (6)
 
N 	

N 	
 
cat 	

kota 	 (7)
Each production in this example has two com-
ponents, the first modeling English and the sec-
ond (transliterated) Russian. Nonterminals with the
same index must be rewritten together (synchronous
rewriting). One strength of MTG, and thus also
GMTG, is shown in Productions (5) and (6). There
is a determiner in English, but not in Russian, so
Production (5) does not have the nonterminal D in
the Russian component and (6) applies only to the
English component (independent rewriting). For-
malisms that do not allow independent rewriting re-
quire a corresponding  to appear in the second
component on the right-hand side (RHS) of Produc-
tion (5), and this  would eventually generate the
empty string. This approach has the disadvantage
that it introduces spurious ambiguity about the po-
sition of the ?empty? nonterminal with respect to
the other nonterminals in its component. Spurious
ambiguity leads to wasted effort during parsing.
GMTG?s implementation of independent rewrit-
ing through the empty tuple () serves a very differ-
ent function from the empty string. Consider the
following GMTG:
 
	

	
 
	

	 (8)
 
	

	
 

	


	 (9)
 
	

	
 ffAn Automatic Filter for Non-Parallel Texts
Chris Pike
Computer Science Department
New York University
715 Broadway, 7th Floor
New York, NY 10003 USA
 
lastname  @cs.nyu.edu
I. Dan Melamed
Computer Science Department
New York University
715 Broadway, 7th Floor
New York, NY 10013 USA
 
lastname  @cs.nyu.edu
Abstract
Numerous cross-lingual applications, including
state-of-the-art machine translation systems, require
parallel texts aligned at the sentence level. However,
collections of such texts are often polluted by pairs
of texts that are comparable but not parallel. Bitext
maps can help to discriminate between parallel and
comparable texts. Bitext mapping algorithms use a
larger set of document features than competing ap-
proaches to this task, resulting in higher accuracy.
In addition, good bitext mapping algorithms are not
limited to documents with structural mark-up such
as web pages. The task of filtering non-parallel text
pairs represents a new application of bitext mapping
algorithms.
1 Introduction
In June 2003, the U.S. government organized a
?Surprise Language Exercise? for the NLP commu-
nity. The goal was to build the best possible lan-
guage technologies for a ?surprise? language in just
one month (Oard, 2003). One of the main technolo-
gies pursued was machine translation (MT). Statis-
tical MT (SMT) systems were the most successful
in this scenario, because their construction typically
requires less time than other approaches. On the
other hand, SMT systems require large quantities of
parallel text as training data. A significant collec-
tion of parallel text was obtained for this purpose
from multiple sources. SMT systems were built and
tested; results were reported.
Much later we were surprised to discover that a
significant portion of the training data was not par-
allel text! Some of the document pairs were on the
same topic but not translations of each other. For
today?s sentence-based SMT systems, this kind of
data is noise. How much better would the results
have been if the noisy training data were automati-
cally filtered out? This question is becoming more
important as SMT systems increase their reliance on
automatically collected parallel texts.
There is abundant literature on aligning parallel
texts at the sentence level. To the best of our knowl-
edge, all published methods happily misalign non-
parallel inputs, without so much as a warning. There
is also some recent work on distinguishing paral-
lel texts from pairs of unrelated texts (Resnik and
Smith, 2003). In this paper, we propose a solution to
the more difficult problem of distinguishing parallel
texts from texts that are comparable but not parallel.
Definitions of ?comparable texts? vary in the lit-
erature. Here we adopt a definition that is most
suitable for filtering SMT training data: Two texts
are ?comparable? if they are not alignable at ap-
proximately the sentence level. This definition is
also suitable for other applications of parallel texts,
such as machine-assisted translation and computer-
assisted foreign language learning.
Resnik and Smith (2003) suggested three ap-
proaches to filtering non-parallel texts: STRAND,
tsim, and a combination of the two. STRAND relies
on mark-up within a document to reveal the docu-
ment?s structure. STRAND then predicts that doc-
uments with the same structure are parallel. Tsim
uses a machine-readable bilingual dictionary to find
word-to-word matches between two halves of a bi-
text. It then computes a similarity score based on the
maximum cardinality bipartite matching between
the two halves. We chose to compare our method
with tsim because we were interested in an approach
that works with both marked up and plain text doc-
uments.
2 A Modification to SIMR
Our work is based on a modification of the SIMR
bitext mapping algorithm (Melamed, 1999). The
SIMR algorithm attempts to construct a piecewise
linear approximation to the True Bitext Map (TBM)
of a bitext by greedily searching for small chains
of points of correspondence. Each chain forms one
section of the approximation. SIMR uses a two-
phase approach to generating chains. First, it gen-
erates a set of potential points of correspondence
within a search rectangle. Next, it searches the
Figure 1: On the left is part of a bitext map gener-
ated by SIMR for non-parallel texts. On the right is
part of a bitext map for parallel texts.
points of correspondence for chains whose points
meet requirements for linearity, injectivity, and
maximum angle deviation. If no such chain is
found, the search rectangle is expanded and the
search repeats.
Our method of detecting translations is based on
the premise that SIMR will find fewer points of cor-
respondence in comparable texts than it will in par-
allel texts. This is because points of correspondence
are more likely to occur in closely corresponding
locations in the two halves of a bitext than in two
documents that are merely comparable. Therefore,
the bitext map of parallel texts will usually be much
denser than the bitext map of comparable texts.
Figure 1 above contrasts the bitext maps output by
SIMR for non-parallel and parallel texts.
To maximize the percentage of correctly classi-
fied document pairs, we need to maximize the dif-
ference between the map densities of parallel and
comparable texts. SIMR?s built in restrictions on
the chains it will accept severely limit the number of
points of correspondence SIMR accepts from most
non-parallel texts. Despite this SIMR still gener-
ated bitext maps for some non-parallel documents
that had densities very close to the densities of par-
allel documents. Chains of spurious points tended to
form over a longer section of the bitext than correct
chains. Therefore we introduced an additional pa-
rameter that limited the length of chains that SIMR
would accept. This modification of SIMR is called
SIMR-cl.
Chains are not perfectly linear. Therefore we can-
not calculate chain length by simply taking the dis-
tance between the first and last points in the chain.
Instead we find the smallest possible rectangle for
which all points in the chain are interior points. We
then calculate the length of the chain as the distance
from the lower left corner to the upper right hand
corner of the rectangle.
When SIMR finds an acceptable chain the search
rectangle is moved so that the point on the lower
left is no longer included in the search. As a result,
when SIMR is finding a large number of chains, the
length of those chains will remain relatively short.
Therefore, in parallel texts SIMR will find many
chains and limiting the chain length will have a
minimal effect on the number of chains SIMR will
find. On a non-parallel text, however, SIMR will
find fewer sets of points of correspondence meet-
ing the criteria for a chain. The result is longer
chains, which can be filtered by our new parame-
ter. E.g., the non-parallel bitext map in Figure 1,
which was created without the chain length param-
eter, has on average 630 characters between points.
In contrast, running SIMR on the same pair of non-
parallel documents with a maximum chain length
of 700 yielded only 22 points of correspondence, or
3032 characters between points on average.
3 Training
Training SIMR-cl, much like SIMR, requires a state
space search algorithm, and an objective function to
evaluate the current state. We chose to use simulated
annealing to perform our state space search. The
first step in training is to generate a set of parameter
values that make up the current state. SIMR-cl uses
the standard SIMR parameters plus the additional
chain length parameter discussed above. Once the
current state is set SIMR-cl generates a bitext map
and calculates the density of the map. The bitext
map density is defined as the number of points in
the bitext map divided by the length of the main di-
agonal of the bitext space. We call this the SIMR-cl
score.
Our objective function seeks to drive the param-
eters to a state where we can select a single thresh-
old value that will classify all candidate bitexts in
the development set correctly. That is, all paral-
lel texts should have a SIMR-cl score greater than
the threshold, and all non-parallel texts should have
a SIMR-cl score less than the threshold. We can-
not achieve this by simply measuring the percentage
of correctly classified candidate text pairs, because
any given change to the parameters is not likely to
change the classification of any candidate bitexts.
In order to measure the amount of error we bor-
rowed the concept of margin slack from the support
vector machines literature. For simplicity we used a
margin of zero, which reduces the margin slack of a
SIMR-cl score to the difference between the thresh-
old density, and the density of a misclassified candi-
date pair. Any correctly classified candidate pair is
defined to have a margin slack of zero. From there
we defined our objective as minimizing the sum of
the margin slack of all candidate pairs. All that is
left at this point is to select an optimal threshold. We
performed a line search for the best possible thresh-
old for each parameter set.
4 Experiments
In our first two experiments we limited the points of
correspondence to orthographic cognates. We used
the Longest Common Subsequence Ratio (LCSR)
to measure similarity (Melamed, 1995). The LCSR
ratio is the length of the longest common subse-
quence of two tokens, divided by the length of the
longer token. In our English-Hindi experiments we
used an English-Hindi dictionary because the lan-
guages are written in different character sets, limit-
ing the effectiveness of orthographic cognates.
4.1 STRAND data
Before evaluating our approach on the more difficult
task of discriminating parallel texts from compara-
ble texts, we compared it to previous approaches
on the easier task of discriminating parallel texts
from unrelated texts. For this purpose, we used
the STRAND corpus, which consists of 326 can-
didate bitexts in French and English1 (Resnik and
Smith, 2003). As a precursor to generating a bitext
map of a candidate pair we tokenized the STRAND
documents and generated the axis files required by
SIMR-cl. We attempted several schemes on training
data and found that generating one token per HTML
tag gave us the best results.
While the end performance of the two approaches
was comparable, we did find that tsim had an advan-
tage over SIMR-cl in training. Resnik and Smith
(2003) trained tsim using 32 of the 326 available
STRAND candidate pairs to achieve their published
result. We repeated their experiments using 1/4
of the available candidate pairs for training and
found no improvement, indicating that tsim can be
optimally trained using a small development set.
By contrast, using 32 training instances, SIMR-
cl achieved only 86% agreement with the human
judges, compared to tsim?s 96%. When trained with
1/4 of the candidate pairs, SIMR-cl achieved 96%
accuracy.
4.2 Filtering of Comparable Texts
We were unable to find a suitable corpus contain-
ing both parallel and comparable texts. Expert opin-
ion suggests that no such corpora are publicly avail-
able2. Therefore we proceeded by simulation. We
constructed 3 sets of two corpora from the Ro-
manian/English Multext-East 1984 corpus (Tufis,
1We removed all document pairs which were not in
French/English.
2Doug Oard, personal cummunication, 2004.
text length 164 820 1640
tsim 66% 66% 66%
SIMR-cl 90% 96.5% 98.5%
Table 1: Percentage of documents correctly classi-
fied by tsim and SIMR-cl on parallel and compara-
ble corpora with texts of varying lengths, by average
number of words in the English text.
1999). We constructed parallel texts by breaking the
corpus into aligned chunks of 10, 50, and 100 seg-
ments. We then simulated comparable texts by pair-
ing non-aligned, consecutive chunks of the same
length. We chose to use consecutive chunks because
there is a better chance for overlap between words in
adjacent segments than in segments far apart. After
breaking the corpus into chunks, 1/3 of the chunks
were used as a training set and the remaining 2/3
were used as a test set. We had 63 training and 130
test pairs of size 100, 126 training and 259 test pairs
of size 50, and 642 training and 1285 test pairs of
size 10. On average each English segment was 16
words in length.
Since a Romanian/English bilingual dictionary
was not readily available, we created a dictionary
for tsim by searching all aligned segments for cog-
nates. We then performed the same optimization
process for tsim and SIMR-cl using documents con-
taining 10, 50, and 100 segments. After performing
our optimizations, we found that the LCSR param-
eters optimized for tsim generated a dictionary con-
taining 3380 pairs.
Using this parameter set, tsim correctly classified
66% of the documents in the 1984 corpus. The ac-
curacy was the same for all bitext lengths. Much
like tsim, we found that for SIMR-cl the optimal
parameter set was independent of the length of the
bitexts being compared. SIMR-cl did however per-
form better on longer texts. Regardless, SIMR-cl
outperformed tsim on all text lengths, as shown in
table 1.
4.3 The Surprise Language Data
Encouraged by our success on French/English and
on Romanian/English, we applied our method to
the Hindi/English data used during the surprise lan-
guage exercise. We did not have Hindi/English
bitexts that were reliably classified as parallel or
not, so we could not optimize SIMR-cl?s parame-
ters specifically for this language pair. However, we
were interested in determining how sensitive the pa-
rameters were to changes in the input language pair
and text genre. So we simply reused the param-
eters that were found to be optimal on the Roma-
nian/English 1984 corpus.
With these parameters, we ran SIMR-cl on just
over half of the Hindi/English collection, the part
that was collected from Indian government web
pages. Our method classified 6 of the document
pairs as non-parallel. Some of these 6 document
pairs were relatively long, together they accounted
for 7% of the English word count in this part of the
collection.
We asked a Hindi speaker to compare the Hindi
and English text in each of these 6 document pairs.
For each text pair, we asked our informant:
1. Do the texts express the same ideas?
2. If yes, was one of the texts probably written as
a translation of the other?
3. If yes, was the translation done roughly at the
sentence level?
The informant decided that in all 6 cases, the pair
of texts expressed the same ideas. However in 4 of
the pairs, the two texts were probably written in-
dependently, rather than one as a translation of the
other. In the remaining two texts, the informant
found large omissions on the English side, larger
than what typical alignment algorithms can handle.
In these latter two documents, our Hindi infor-
mant also discovered an interesting phenomenon
that we were not expecting ? the sections that were
translated were summarized to some degree. I.e.,
even in sections where the order of ideas was largely
the same in the two languages, the English word-
ing was much more terse (the informant said ?com-
pressed?), and omitted many details.
In summary, our method achieved 100% pre-
cision in filtering out document pairs that were
comparable but not parallel. We then asked our
informant to examine 3 document pairs that our
method accepted as parallel. After a cursory in-
spection, the informant answered yes to all 3 ques-
tions above for each of these pairs. Unfortunately, it
would have been very time-consuming to evaluate
recall rigourously, because it would entail exhaus-
tive reading of pairs of documents in parallel, to en-
sure that there were no non-parallel segments.
5 Conclusions
We have shown that SIMR-cl, a modified version
of the SIMR bitext mapping algorithm, can reli-
ably discriminate between parallel and comparable
texts. We have demonstrated that SIMR-cl is effec-
tive on three language pairs, including two where no
bilingual dictionary was available. In addition, we
have presented tentative evidence that the parame-
ters of SIMR-cl are not very sensitive to particular
language pairs or text genres on this task.
Our results suggest several new avenues for fu-
ture research. First, it would be useful to com-
bine our method for filtering out non-parallel texts
with methods for detecting omissions in translations
(Melamed, 1996). Some of the translations found
on the web today might be made more literal by
deleting the untranslated parts. Second, we seem to
have discovered the existence of training data for a
machine learning approach to translation with sum-
marization. Third, our results suggest that the den-
sity of a bitext map is highly correlated with its ac-
curacy, and that this correlation is largely invariant
across language pairs and text genres. If this is true,
then it should be possible to train bitext mapping al-
gorithms without any hand-aligned training data, by
using map density as the objective function instead
of RMS error.
Acknowledgements
Thanks to Philip Resnik and Noah Smith for shar-
ing STRAND data, human judgements, and tsim
scores. Thanks also to Noah Smith for providing a
tsim implementation. This research was sponsored
by the DARPA TIDES program, by an NSF CA-
REER award, and by an equipment gift from Sun
Microsystems.
References
I. Dan Melamed. 1995. Automatic evaluation and
uniform filter cascades for inducing n-best trans-
lation lexicons. In Proceedings of the 3rd ACL
Workshop on Very Large Corpora (WVLC), Cam-
bridge, Massachusetts.
I. Dan Melamed. 1996. Automatic detection of
omissions in translations. In Proceedings of
the International Conference on Computational
Linguistics (COLING) 1996, pages 764?769,
Copenhagen, Denmark, August.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguis-
tics, 25(1):107?139, March.
D. Oard. 2003. The surprise language excercises.
In ACM Transactions on Asian Language Infor-
mation Processing (TALIP), pages 79?84, New
York, NY, June.
P. Resnik and N. A. Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, pages
349?380, September.
D. Tufis. 1999. Multext-east 1984 corpus.
http://nl.ijs.si/ME/.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 873?880,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Advances in Discriminative Parsing
Joseph Turian and I. Dan Melamed
{lastname}@cs.nyu.edu
Computer Science Department
New York University
New York, New York 10003
Abstract
The present work advances the accu-
racy and training speed of discrimina-
tive parsing. Our discriminative parsing
method has no generative component, yet
surpasses a generative baseline on con-
stituent parsing, and does so with mini-
mal linguistic cleverness. Our model can
incorporate arbitrary features of the in-
put and parse state, and performs fea-
ture selection incrementally over an ex-
ponential feature space during training.
We demonstrate the flexibility of our ap-
proach by testing it with several pars-
ing strategies and various feature sets.
Our implementation is freely available at:
http://nlp.cs.nyu.edu/parser/.
1 Introduction
Discriminative machine learning methods have
improved accuracy on many NLP tasks, including
POS-tagging, shallow parsing, relation extraction,
and machine translation. Some advances have also
been made on full syntactic constituent parsing.
Successful discriminative parsers have relied on
generative models to reduce training time and
raise accuracy above generative baselines (Collins
& Roark, 2004; Henderson, 2004; Taskar et al,
2004). However, relying on information from a
generative model might prevent these approaches
from realizing the accuracy gains achieved by dis-
criminative methods on other NLP tasks. Another
problem is training speed: Discriminative parsers
are notoriously slow to train.
In the present work, we make progress towards
overcoming these obstacles. We propose a flexi-
ble, end-to-end discriminative method for training
parsers, demonstrating techniques that might also
be useful for other structured prediction problems.
The proposed method does model selection with-
out ad-hoc smoothing or frequency-based feature
cutoffs. It requires no heuristics or human effort
to optimize the single important hyper-parameter.
The training regime can use all available informa-
tion from the entire parse history. The learning al-
gorithm projects the hand-provided features into a
compound feature space and performs incremen-
tal feature selection over this large feature space.
The resulting parser achieves higher accuracy than
a generative baseline, despite not using a genera-
tive model as a feature.
Section 2 describes the parsing algorithm. Sec-
tion 3 presents the learning method. Section 4
presents experiments with discriminative parsers
built using these methods. Section 5 compares our
approach to related work.
2 Parsing Algorithm
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put. Spans cross if they overlap but neither con-
tains the other. An item is a (span, label) pair. A
state is a partial parse, i.e. a set of items, none
of whose spans may cross. A parse inference is
a (state, item) pair, i.e. a state and an item to be
added to it. The frontier of a state consists of the
items with no parents yet. The children of a candi-
date inference are the frontier items below the item
to be inferred, and the head of a candidate infer-
ence is the child item chosen by English head rules
(Collins, 1999, pp. 238?240). A parse path is a
sequence of parse inferences. For some input sen-
tence and training parse tree, a state is correct if
the parser can infer zero or more additional items
to obtain the training parse tree, and an inference
873
is correct if it leads to a correct state.
Given input sentence s, the parser searches for
parse p? out of the possible parses P(s):
p? = arg min
p?P(s)
C?(p) (1)
where C?(p) is the cost of parse p under model ?:
C?(p) =
?
i?p
c?(i) (2)
Section 3.1 describes how to compute c?(i). Be-
cause c?(i) ? R+, the cost of a partial parse mono-
tonically increases as we add items to it.
The parsing algorithm considers a succession
of states. The initial state contains terminal items,
whose labels are the POS tags given by the tagger
of Ratnaparkhi (1996). Each time we pop a state
from the agenda, c? computes the costs for the
candidate bottom-up inferences generated from
that state. Each candidate inference results in a
successor state to be placed on the agenda.
The cost function c? can consider arbitrary
properties of the input and parse state. We are not
aware of any tractable solution to Equation 1, such
as dynamic programming. Therefore, the parser
finds p? using a variant of uniform-cost search.
The parser implements the search using an agenda
that stores entire states instead of single items.
Each time a state is popped from the agenda, the
parser uses depth-first search starting from the
state that was popped until it (greedily) finds a
complete parse. In preliminary experiments, this
search strategy was faster than standard uniform-
cost search (Russell & Norvig, 1995).
3 Training Method
3.1 General Setting
Our training set I consists of candidate inferences
from the parse trees in the training data. From
each training inference i ? I we generate the tuple
?X(i), y(i), b(i)?. X(i) is a feature vector describing
i, with each element in {0, 1}. We will use X f (i) to
refer to the element of X(i) that pertains to feature
f . y(i) = +1 if i is correct, and y(i) = ?1 if not.
Some training examples might be more important
than others, so each is given a bias b(i) ? R+, as
detailed in Section 3.3.
The goal during training is to induce a hypothe-
sis h?(i), which is a real-valued inference scoring
function. In the present work, h? is a linear model
parameterized by a real vector ?, which has one
entry for each feature f :
h?(i) = ? ? X(i) =
?
f
? f ? X f (i) (3)
The sign of h?(i) predicts the y-value of i and the
magnitude gives the confidence in this prediction.
The training procedure optimizes ? to minimize
the expected risk R? over training set I. R? is the
objective function, a combination of loss function
L? and regularization term ??:
R?(I) = L?(I) + ?? (4)
The loss of the inference set decomposes into the
loss of individual inferences:
L?(I) =
?
i?I
l?(i) (5)
In principle, l? can be any loss function, but in the
present work we use the log-loss (Collins et al,
2002):
l?(i) = b(i) ? ln(1 + exp(???(i))) (6)
and ??(i) is the margin of inference i:
??(i) = y(i) ? h?(i) (7)
Inference cost c?(i) in Equation 2 is l?(i) com-
puted using y(i) = +1 and b(i) = 1, i.e.:
c?(i) = ln(1 + exp(?h?(i))) (8)
?? in Equation 4 is a regularizer, which penal-
izes complex models to reduce overfitting and gen-
eralization error. We use the `1 penalty:
?? =
?
f
? ? |? f | (9)
where ? is a parameter that controls the strength
of the regularizer. This choice of objective R? is
motivated by Ng (2004), who suggests that, given
a learning setting where the number of irrelevant
features is exponential in the number of train-
ing examples, we can nonetheless learn effectively
by building decision trees to minimize the `1-
regularized log-loss. On the other hand, Ng (2004)
suggests that most of the learning algorithms com-
monly used by discriminative parsers will overfit
when exponentially many irrelevant features are
present.1
Learning over an exponential feature space is
the very setting we have in mind. A priori, we de-
fine only a set A of simple atomic features (given
1including the following learning algorithms:
? unregularized logistic regression
? logistic regression with an `2 penalty (i.e. a Gaussian prior)
? SVMs using most kernels
? multilayer neural nets trained by backpropagation
? the perceptron algorithm
874
in Section 4). The learner then induces compound
features, each of which is a conjunction of possi-
bly negated atomic features. Each atomic feature
can have one of three values (yes/no/don?t care),
so the size of the compound feature space is 3|A|,
exponential in the number of atomic features. It
was also exponential in the number of training ex-
amples in our experiments (|A| ? |I|).
3.2 Boosting `1-Regularized Decision Trees
We use an ensemble of confidence-rated decision
trees (Schapire & Singer, 1999) to represent h?.2
The path from the root to each node n in a decision
tree corresponds to some compound feature f , and
we write ?(n) = f . To score an inference i using
a decision tree, we percolate the inference?s fea-
tures X(i) down to a leaf n and return confidence
??(n). An inference i percolates down to node n iff
X?(n) = 1. Each leaf node n keeps track of the pa-
rameter value ??(n).3 The score h?(i) given to an
inference i by the whole ensemble is the sum of the
confidences returned by the trees in the ensemble.
Listing 1 Outline of training algorithm.
1: procedure T????(I)
2: ensemble? ?
3: ?? ?
4: while dev set accuracy is increasing do
5: t ? tree with one (root) node
6: while the root node cannot be split do
7: decay `1 parameter ?
8: while some leaf in t can be split do
9: split the leaf to maximize gain
10: percolate every i ? I to a leaf node
11: for each leaf n in t do
12: update ??(n) to minimize R?
13: append t to ensemble
Listing 1 presents our training algorithm. At
the beginning of training, the ensemble is empty,
? = 0, and the `1 parameter ? is set to? (Steps 1.2
and 1.3). We train until the objective cannot be fur-
ther reduced for the current choice of ?. We then
determine the accuracy of the parser on a held-out
development set using the previous ? value (be-
fore it was decreased), and stop training when this
2Turian and Melamed (2005) reported that decision trees
applied to parsing have higher accuracy and training speed
than decision stumps, so we build full decision trees rather
than stumps.
3Any given compound feature can appear in more than one
tree, but each leaf node has a distinct confidence value. For
simplicity, we ignore this possibility in our discussion.
accuracy reaches a plateau (Step 1.4). Otherwise,
we relax the regularization penalty by decreasing
? (Steps 1.6 and 1.7) and continue training. In this
way, instead of choosing the best ? heuristically,
we can optimize it during a single training run
(Turian & Melamed, 2005).
Each training iteration (Steps 1.5?1.13) has sev-
eral steps. First, we choose some compound fea-
tures that have high magnitude gradient with re-
spect to the objective function. We do this by
building a new decision tree, whose leaves rep-
resent the chosen compound features (Steps 1.5?
1.9). Second, we confidence-rate each leaf to min-
imize the objective over the examples that per-
colate down to that leaf (Steps 1.10?1.12). Fi-
nally, we append the decision tree to the ensem-
ble and update parameter vector ? accordingly
(Step 1.13). In this manner, compound feature se-
lection is performed incrementally during train-
ing, as opposed to a priori.
Our strategy minimizing the objective R?(I)
(Equation 4) is a variant of steepest descent
(Perkins et al, 2003). To compute the gradient of
the unpenalized loss L? with respect to the param-
eter ? f of feature f , we have:
?L?(I)
?? f
=
?
i?I
?l?(i)
???(i) ?
???(i)
?? f
(10)
where:
???(i)
?? f
= y(i) ? X f (i) (11)
Using Equation 6, we define the weight of an ex-
ample i under the current model as the rate at
which loss decreases as the margin of i increases:
w?(i) = ? ?l?(i)
???(i) = b(i) ?
1
1 + exp(??(i)) (12)
Recall that X f (i) is either 0 or 1. Combining Equa-
tions 10?12 gives:
?L?(I)
?? f
= ?
?
i?I
X f (i)=1
y(i) ? w?(i) (13)
We define the gain of feature f as:
G?(I; f ) = max
(
0,
??????
?L?(I)
?? f
??????
? ?
)
(14)
Equation 14 has this form because the gradient of
the penalty term is undefined at ? f = 0. This dis-
continuity is why `1 regularization tends to pro-
duce sparse models. If G?(I; f ) = 0, then the ob-
jective R?(I) is at its minimum with respect to pa-
rameter ? f . Otherwise, G?(I; f ) is the magnitude
875
of the gradient of the objective as we adjust ? f in
the appropriate direction.
To build each decision tree, we begin with a root
node. The root node corresponds to a dummy ?al-
ways true? feature. We recursively split nodes by
choosing a splitting feature that will allow us to in-
crease the gain. Node n with corresponding com-
pound feature ?(n) = f can be split by atomic fea-
ture a if:
G?(I; f ? a) + G?(I; f ? ?a) > G?(I; f ) (15)
If no atomic feature satisfies the splitting crite-
rion in Equation 15, then n becomes a leaf node
of the decision tree and ??(n) becomes one of the
values to be optimized during the parameter up-
date step. Otherwise, we choose atomic feature a?
to split node n:
a? = arg max
a?A
(G?(I; f ? a) + G?(I; f ? ?a))
(16)
This split creates child nodes n1 and n2, with
?(n1) = f ? a? and ?(n2) = f ? ?a?.
Parameter update is done sequentially on only
the most recently added compound features, which
correspond to the leaves of the new decision tree.
After the entire tree is built, we percolate exam-
ples down to their appropriate leaf nodes. We then
choose for each leaf node n the parameter ??(n)
that minimizes the objective over the examples in
that leaf. A convenient property of decision trees
is that the leaves? compound features are mutually
exclusive. Their parameters can be directly opti-
mized independently of each other using a line
search over the objective.
3.3 The Training Set
We choose a single correct path from each training
parse tree, and the training examples correspond to
all candidate inferences considered in every state
along this path.4 In the deterministic setting there
is only one correct path, so example generation
is identical to that of Sagae and Lavie (2005). If
parsing proceeds non-deterministically then there
might be multiple paths that lead to the same final
parse, so we choose one randomly. This method
of generating training examples does not require a
working parser and can be run prior to any train-
ing. The disadvantage of this approach is that it
minimizes the error of the parser at correct states
only. It does not account for compounded error or
4Nearly all of the examples generated are negative (y = ?1).
teach the parser to recover from mistakes grace-
fully.
Turian and Melamed (2005) observed that uni-
form example biases b(i) produced lower accuracy
as training progressed, because the induced clas-
sifiers minimized the error per example. To min-
imize the error per state, we assign every train-
ing state equal value and share half the value uni-
formly among the negative examples for the ex-
amples generated from that state and the other half
uniformly among the positive examples.
We parallelize training by inducing 26 label
classifiers (one for each non-terminal label in the
Penn Treebank). Parallelization might not uni-
formly reduce training time because different la-
bel classifiers train at different rates. However, par-
allelization uniformly reduces memory usage be-
cause each label classifier trains only on inferences
whose consequent item has that label.
4 Experiments
Discriminative parsers are notoriously slow to
train. For example, Taskar et al (2004) took sev-
eral months to train on the ? 15 word sentences
in the English Penn Treebank (Dan Klein, p.c.).
The present work makes progress towards faster
discriminative parser training: our slowest classi-
fier took fewer than 5 days to train. Even so, it
would have taken much longer to train on the en-
tire treebank. We follow Taskar et al (2004) in
training and testing on ? 15 word sentences in
the English Penn Treebank (Taylor et al, 2003).
We used sections 02?21 for training, section 22
for development, and section 23 for testing, pre-
processed as per Table 1. We evaluated our parser
using the standard PARSEVAL measures (Black et
al., 1991): labelled precision, labelled recall, and
labelled F-measure (Prec., Rec., and F1, respec-
tively), which are based on the number of non-
terminal items in the parser?s output that match
those in the gold-standard parse.5
As mentioned in Section 2, items are inferred
bottom-up and the parser cannot infer any item
that crosses an item already in the state. Although
there are O(n2) possible (span, label) pairs over a
frontier containing n items, we reduce this to the
? 5 ? n inferences that have at most five children.6
5The correctness of a stratified shuffling test has been called
into question (Michael Collins, p.c.), so we are not aware of
any valid significance tests for observed differences in PAR-
SEVAL scores.
6Only 0.57% of non-terminals in the preprocessed develop-
876
Table 1 Steps for preprocessing the data. Starred steps are performed only when parse trees are available
in the data (e.g. not on test data).
1. * Strip functional tags and trace indices, and remove traces.
2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).)
3. Remove quotation marks (i.e. terminal items tagged ?? or ??). (Bikel, 2004)
4. * Raise punctuation. (Bikel, 2004)
5. Remove outermost punctuation.a
6. * Remove unary projections to self (i.e. duplicate items with the same span and label).
7. POS tag the text using the tagger of Ratnaparkhi (1996).
8. Lowercase headwords.
aAs pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful infor-
mation. Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding
punctuation features, one of which encoded the sentence-final punctuation.
To ensure the parser does not enter an infinite loop,
no two items in a state can have both the same
span and the same label. Given these restrictions
on candidate inferences, there were roughly 40
million training examples generated in the train-
ing set. These were partitioned among the 26 con-
stituent label classifiers. Building a decision tree
(Steps 1.5?1.9 in Listing 1) using the entire ex-
ample set I can be very expensive. We estimate
loss gradients (Equation 13) using a sample of the
inference set, which gives a 100-fold increase in
training speed (Turian & Melamed, 2006).
Our atomic feature set A contains 300K fea-
tures, each of the form ?is there an item in group
J whose label/headword/headtag/headtagclass is
?X???.7 Possible values of ?X? for each predicate
are collected from the training data. For 1 ? n ? 3,
possible values for J are:
? the first/last n child items
? the first n left/right context items
? the n children items left/right of the head
? the head item.
The left and right context items are the frontier
items to the left and right of the children of the
candidate inference, respectively.
4.1 Different Parsing Strategies
To demonstrate the flexibility of our learn-
ing procedure, we trained three different
parsers: left-to-right (l2r), right-to-left (r2l),
ment set have more than five children.
7The predicate headtagclass is a supertype of the headtag.
Given our compound features, these are not strictly necessary,
but they accelerate training. An example is ?proper noun,?
which contains the POS tags given to singular and plural
proper nouns. Space constraints prevent enumeration of the
headtagclasses, which are instead provided at the URL given
in the abstract.
Table 2 Results on the development set, training
and testing using only ? 15 word sentences.
active
? features % Rec. % Prec. F1
l2r 0.040 11.9K 89.86 89.63 89.74
b.u. 0.020 13.7K 89.92 89.84 89.88
r2l 0.014 14.0K 90.66 89.81 90.23
and non-deterministic bottom-up (b.u.). The
non-deterministic parser was allowed to choose
any bottom-up inference. The other two parsers
were deterministic: bottom-up inferences had
to be performed strictly left-to-right or right-
to-left, respectively. We stopped training when
each parser had 15K active features. Figure 1
shows the accuracy of the different runs over the
development set as training progressed. Table 2
gives the PARSEVAL scores of these parsers at
their optimal `1 penalty setting. We found that
the perplexity of the r2l model was low so that,
in 85% of the sentences, its greedy parse was the
optimal one. The l2r parser does poorly because
its decisions were more difficult than those of the
other parsers. If it inferred far-right items, it was
more likely to prevent correct subsequent infer-
ences that were to the left. But if it inferred far-left
items, then it went against the right-branching
tendency of English sentences. The left-to-right
parser would likely improve if we were to use a
left-corner transform (Collins & Roark, 2004).
Parsers in the literature typically choose some
local threshold on the amount of search, such as
a maximum beam width. With an accurate scor-
ing function, restricting the search space using
a fixed beam width might be unnecessary. In-
stead, we imposed a global threshold on explo-
ration of the search space. Specifically, if the
877
Figure 1 F1 scores on the development set of the
Penn Treebank, using only ? 15 word sentences.
The x-axis shows the number of non-zero param-
eters in each parser, summed over all classifiers.
85%
86%
87%
88%
89%
90%
15K10K5K2.5K1.5K
De
vel
. F-
me
asu
re
total number of non-zero parameters
right-to-leftleft-to-rightbottom up
parser has found some complete parse and has
explored at least 100K states (i.e. scored at least
100K inferences), search stopped prematurely and
the parser would return the (possibly sub-optimal)
current best complete parse. The l2r and r2l
parsers never exceeded this threshold, and al-
ways found the optimal complete parse. However,
the non-deterministic bottom-up parser?s search
was cut-short in 28% of the sentences. The non-
deterministic parser can reach each parse state
through many different paths, so it searches a
larger space than a deterministic parser, with more
redundancy.
To gain a better understanding of the weak-
nesses of our parser, we examined a sample of
50 development sentences that the r2l parser did
not get entirely correct. Roughly half the errors
were due to noise and genuine ambiguity. The re-
maining errors fell into three types, occurring with
roughly the same frequency:
? ADVPs and ADJPs The r2l parser had F1 =
81.1% on ADVPs, and F1 = 71.3% on ADJPs. An-
notation of ADJP and ADVP in the PTB is inconsis-
tent, particularly for unary projections.
? POS Tagging Errors Many of the parser?s er-
rors were due to incorrect POS tags. In future work
we will integrate POS-tagging as inferences of the
parser, allowing it to entertain competing hypothe-
ses about the correct tagging.
? Bilexical dependencies Although compound
features exist to detect affinities between words,
the parser had difficulties with bilexical depen-
dency decisions that were unobserved in the train-
ing data. The classifier would need more training
data to learn these affinities.
Figure 2 F1 scores of right-to-left parsers with dif-
ferent atomic feature sets on the development set
of the Penn Treebank, using only ? 15 word sen-
tences.
85%
86%
87%
88%
89%
90%
91%
30K20K10K5K2.5K1.5K
De
vel
. F-
me
asu
re
total number of non-zero parameters
kitchen sinkbaseline
4.2 More Atomic Features
We compared our right-to-left parser with the
baseline set of atomic features to one with a far
richer atomic feature set, including unbounded
context features, length features, and features of
the terminal items. This ?kitchen sink? parser
merely has access to many more item groups J, de-
scribed in Table 3. All features are all of the form
given earlier, except for length features (Eisner &
Smith, 2005). Length features compute the size of
one of the groups of items in the indented list in
Table 3. The feature determines if this length is
equal to/greater than to n, 0 ? n ? 15. The kitchen
sink parser had 1.1 million atomic features, 3.7
times the number available in the baseline. In fu-
ture work, we plan to try linguistically more so-
phisticated features (Charniak & Johnson, 2005)
as well as sub-tree features (Bod, 2003; Kudo et
al., 2005).
Figure 2 shows the accuracy of the right-to-
left parsers with different atomic feature sets over
the development set as training progressed. Even
though the baseline training made progress more
quickly than the kitchen sink, the kitchen sink?s F1
surpassed the baseline?s F1 early in training, and at
6.3K active parameters it achieved a development
set F1 of 90.55%.
4.3 Test Set Results
To situate our results in the literature, we compare
our results to those reported by Taskar et al (2004)
and Turian and Melamed (2005) for their dis-
criminative parsers, which were also trained and
tested on ? 15 word sentences. We also compare
our parser to a representative non-discriminative
878
Table 3 Item groups available in the kitchen sink run.
? the first/last n child items, 1 ? n ? 4
? the first n left/right context items, 1 ? n ? 4
? the n children items left/right of the head, 1 ? n ? 4
? the nth frontier item left/right of the leftmost/head/rightmost child item, 1 ? n ? 3
? the nth terminal item left/right of the leftmost/head/rightmost terminal item dominated by the item
being inferred, 1 ? n ? 3
? the leftmost/head/rightmost child item of the leftmost/head/rightmost child item
? the following groups of frontier items:
? all items
? left/right context items
? non-leftmost/non-head/non-rightmost child items
? child items left/right of the head item, inclusive/exclusive
? the terminal items dominated by one of the item groups in the indented list above
Table 4 Results of parsers on the test set, training
and testing using only ? 15 word sentences.
% Rec. % Prec. F1
Turian and Melamed (2005) 86.47 87.80 87.13
Bikel (2004) 87.85 88.75 88.30
Taskar et al (2004) 89.10 89.14 89.12
kitchen sink 89.26 89.55 89.40
parser (Bikel, 2004)8, the only one that we were
able to train and test under exactly the same ex-
perimental conditions (including the use of POS
tags from the tagger of Ratnaparkhi (1996)). Ta-
ble 4 shows the PARSEVAL results of these four
parsers on the test set.
5 Comparison with Related Work
Our parsing approach is based upon a single end-
to-end discriminative learning machine. Collins
and Roark (2004) and Taskar et al (2004) beat
the generative baseline only after using the stan-
dard trick of using the output from a generative
model as a feature. Henderson (2004) finds that
discriminative training was too slow, and reports
accuracy higher than generative models by dis-
criminatively reranking the output of his genera-
tive model. Unlike these state-of-the-art discrimi-
native parsers, our method does not (yet) use any
information from a generative model to improve
training speed or accuracy. As far as we know, we
present the first discriminative parser that does not
use information from a generative model to beat a
8Bikel (2004) is a ?clean room? reimplementation of the
Collins (1999) model with comparable accuracy.
generative baseline (the Collins model).
The main limitation of our work is that we can
do training reasonably quickly only on short sen-
tences because a sentence with n words gener-
ates O(n2) training inferences in total. Although
generating training examples in advance with-
out a working parser (Turian & Melamed, 2005)
is much faster than using inference (Collins &
Roark, 2004; Henderson, 2004; Taskar et al,
2004), our training time can probably be de-
creased further by choosing a parsing strategy with
a lower branching factor. Like our work, Ratna-
parkhi (1999) and Sagae and Lavie (2005) gener-
ate examples off-line, but their parsing strategies
are essentially shift-reduce so each sentence gen-
erates only O(n) training examples.
An advantage of our approach is its flexibility.
As our experiments showed, it is quite simple to
substitute in different parsing strategies. Although
we used very little linguistic information (the head
rules and the POS tag classes), our model could
also start with more sophisticated task-specific
features in its atomic feature set. Atomic features
that access arbitrary information are represented
directly without the need for an induced interme-
diate representation (cf. Henderson, 2004).
Other papers (Clark & Curran, 2004; Kaplan
et al, 2004, e.g.) have applied log-linear mod-
els to parsing. These works are based upon con-
ditional models, which include a normalization
term. However, our loss function forgoes normal-
ization, which means that it is easily decomposed
into the loss of individual inferences (Equation 5).
879
Decomposition of the loss allows the objective to
be optimized in parallel. This might be an ad-
vantage for larger structured prediction problems
where there are more opportunities for paralleliza-
tion, for example machine translation.
The only important hyper-parameter in our
method is the `1 penalty factor. We optimize it
as part of the training process, choosing the value
that maximizes accuracy on a held-out develop-
ment set. This technique stands in contrast to more
ad-hoc methods for choosing hyper-parameters,
which may require prior knowledge or additional
experimentation.
6 Conclusion
Our work has made advances in both accuracy
and training speed of discriminative parsing. As
far as we know, we present the first discriminative
parser that surpasses a generative baseline on con-
stituent parsing without using a generative compo-
nent, and it does so with minimal linguistic clev-
erness. Our approach performs feature selection
incrementally over an exponential feature space
during training. Our experiments suggest that the
learning algorithm is overfitting-resistant, as hy-
pothesized by Ng (2004). If this is the case, it
would reduce the effort required for feature engi-
neering. An engineer can merely design a set of
atomic features whose powerset contains the req-
uisite information. Then, the learning algorithm
can perform feature selection over the compound
feature space, avoiding irrelevant compound fea-
tures.
In future work, we shall make some standard
improvements. Our parser should infer its own
POS tags to improve accuracy. A shift-reduce
parsing strategy will generate fewer training in-
ferences, and might lead to shorter training times.
Lastly, we plan to give the model linguistically
more sophisticated features. We also hope to ap-
ply the model to other structured prediction tasks,
such as syntax-driven machine translation.
Acknowledgments
The authors would like to thank Chris Pike,
Cynthia Rudin, and Ben Wellington, as well as
the anonymous reviewers, for their helpful com-
ments and constructive criticism. This research
was sponsored by NSF grants #0238406 and
#0415933.
References
Bikel, D. M. (2004). Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman,
R., Harrison, P., et al (1991). A procedure for quantitatively
comparing the syntactic coverage of English grammars. In
Speech and Natural Language.
Bod, R. (2003). An efficient implementation of a new DOP
model. In EACL.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In ACL.
Clark, S., & Curran, J. R. (2004). Parsing the WSJ using
CCG and log-linear models. In ACL.
Collins, M. (1999). Head-driven statistical models for natu-
ral language parsing. Doctoral dissertation.
Collins, M. (2003). Head-driven statistical models for natural
language parsing. Computational Linguistics, 29(4).
Collins, M., & Roark, B. (2004). Incremental parsing with
the perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., & Singer, Y. (2002). Logis-
tic regression, AdaBoost and Bregman distances. Machine
Learning, 48(1-3).
Eisner, J., & Smith, N. A. (2005). Parsing with soft and hard
constraints on dependency length. In IWPT.
Henderson, J. (2004). Discriminative training of a neural
network statistical parser. In ACL.
Kaplan, R. M., Riezler, S., King, T. H., Maxwell, III, J. T.,
Vasserman, A., & Crouch, R. (2004). Speed and accuracy
in shallow and deep stochastic parsing. In HLT/NAACL.
Kudo, T., Suzuki, J., & Isozaki, H. (2005). Boosting-based
parse reranking with subtree features. In ACL.
Magerman, D. M. (1995). Statistical decision-tree models
for parsing. In ACL.
Ng, A. Y. (2004). Feature selection, `1 vs. `2 regularization,
and rotational invariance. In ICML.
Perkins, S., Lacker, K., & Theiler, J. (2003). Grafting: Fast,
incremental feature selection by gradient descent in func-
tion space. Journal of Machine Learning Research, 3.
Ratnaparkhi, A. (1996). A maximum entropy part-of-speech
tagger. In EMNLP.
Ratnaparkhi, A. (1999). Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-
3).
Russell, S., & Norvig, P. (1995). Artificial intelligence: A
modern approach.
Sagae, K., & Lavie, A. (2005). A classifier-based parser with
linear run-time complexity. In IWPT.
Schapire, R. E., & Singer, Y. (1999). Improved boosting us-
ing confidence-rated predictions. Machine Learning, 37(3).
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C.
(2004). Max-margin parsing. In EMNLP.
Taylor, A., Marcus, M., & Santorini, B. (2003). The Penn
Treebank: an overview. In A. Abeille? (Ed.), Treebanks:
Building and using parsed corpora (chap. 1).
Turian, J., & Melamed, I. D. (2005). Constituent parsing by
classification. In IWPT.
Turian, J., & Melamed, I. D. (2006). Computational chal-
lenges in parsing by classification. In HLT-NAACL work-
shop on computationally hard problems and joint inference
in speech and language processing.
880
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 977?984,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Empirical Lower Bounds on the Complexity of Translational Equivalence ?
Benjamin Wellington
Computer Science Dept.
New York University
New York, NY 10003
{lastname}@cs.nyu.edu
Sonjia Waxmonsky
Computer Science Dept.
University of Chicago?
Chicago, IL, 60637
wax@cs.uchicago.edu
I. Dan Melamed
Computer Science Dept.
New York University
New York, NY, 10003
{lastname}@cs.nyu.edu
Abstract
This paper describes a study of the pat-
terns of translational equivalence exhib-
ited by a variety of bitexts. The study
found that the complexity of these pat-
terns in every bitext was higher than sug-
gested in the literature. These findings
shed new light on why ?syntactic? con-
straints have not helped to improve statis-
tical translation models, including finite-
state phrase-based models, tree-to-string
models, and tree-to-tree models. The
paper also presents evidence that inver-
sion transduction grammars cannot gen-
erate some translational equivalence rela-
tions, even in relatively simple real bi-
texts in syntactically similar languages
with rigid word order. Instructions
for replicating our experiments are at
http://nlp.cs.nyu.edu/GenPar/ACL06
1 Introduction
Translational equivalence is a mathematical rela-
tion that holds between linguistic expressions with
the same meaning. The most common explicit rep-
resentations of this relation are word alignments
between sentences that are translations of each
other. The complexity of a given word alignment
can be measured by the difficulty of decomposing
it into its atomic units under certain constraints de-
tailed in Section 2. This paper describes a study
of the distribution of alignment complexity in a
variety of bitexts. The study considered word
alignments both in isolation and in combination
with independently generated parse trees for one
or both sentences in each pair. Thus, the study
? Thanks to David Chiang, Liang Huang, the anonymous
reviewers, and members of the NYU Proteus Project for help-
ful feedback. This research was supported by NSF grant #?s
0238406 and 0415933.
? SW made most of her contribution while at NYU.
is relevant to finite-state phrase-based models that
use no parse trees (Koehn et al, 2003), tree-to-
string models that rely on one parse tree (Yamada
and Knight, 2001), and tree-to-tree models that
rely on two parse trees (Groves et al, 2004, e.g.).
The word alignments that are the least complex
on our measure coincide with those that can be
generated by an inversion transduction grammar
(ITG). Following Wu (1997), the prevailing opin-
ion in the research community has been that more
complex patterns of word alignment in real bitexts
are mostly attributable to alignment errors. How-
ever, the experiments in Section 3 show that more
complex patterns occur surprisingly often even in
highly reliable alignments in relatively simple bi-
texts. As discussed in Section 4, these findings
shed new light on why ?syntactic? constraints have
not yet helped to improve the accuracy of statisti-
cal machine translation.
Our study used two kinds of data, each con-
trolling a different confounding variable. First,
we wanted to study alignments that contained as
few errors as possible. So unlike some other stud-
ies (Zens and Ney, 2003; Zhang et al, 2006), we
used manually annotated alignments instead of au-
tomatically generated ones. The results of our ex-
periments on these data will remain relevant re-
gardless of improvements in technology for auto-
matic word alignment.
Second, we wanted to measure how much of
the complexity is not attributable to systematic
translation divergences, both in the languages as
a whole (SVO vs. SOV), and in specific construc-
tions (English not vs. French ne. . . pas). To elim-
inate this source of complexity of translational
equivalence, we used English/English bitexts. We
are not aware of any previous studies of word
alignments in monolingual bitexts.
Even manually annotated word alignments vary
in their reliability. For example, annotators some-
times link many words in one sentence to many
977
(a) that
,
I
believe
we
all
find
unacceptable
,
regardless
of
political
party
,
je
pense
que
,
independamment
de
notre
parti
,
nous
trouvons
tous
cela
inacceptable
(b)
(Y / Y,Y) ??> (D C / D,C)
*
(S / S) ??> (X A  / X A X) (X / X,X) ??> (Y B / B Y,Y)
X A Y B A D C B A
B
D
A
CY
A
Y
B
X
A
X
S
S
believe
party
pense
unacc
that
celaparti inacc
Figure 1: (a) Part of a word alignment. (b) Derivation of this word alignment using only binary and nullary productions
requires one gap per nonterminal, indicated by commas in the production rules.
words in the other, instead of making the effort to
tease apart more fine-grained distinctions. A study
of such word alignments might say more about
the annotation process than about the translational
equivalence relation in the data. The inevitable
noise in the data motivated us to focus on lower
bounds, complementary to Fox (2002), who wrote
that her results ?should be looked on as more of an
upper bound.? (p. 307) As explained in Section 3,
we modified all unreliable alignments so that they
cannot increase the complexity measure. Thus, we
arrived at complexity measurements that were un-
derestimates, but reliably so. It is almost certain
that the true complexity of translational equiva-
lence is higher than what we report.
2 A Measure of Alignment Complexity
Any translation model can memorize a training
sentence pair as a unit. For example, given a sen-
tence pair like (he left slowly / slowly he left) with
the correct word alignment, a phrase-based trans-
lation model can add a single 3-word biphrase to
its phrase table. However, this biphrase would not
help the model predict translations of the individ-
ual words in it. That?s why phrase-based models
typically decompose such training examples into
their sub-biphrases and remember them too. De-
composing the translational equivalence relations
in the training data into smaller units of knowledge
can improve a model?s ability to generalize (Zhang
et al, 2006). In the limit, to maximize the chances
of covering arbitrary new data, a model should de-
compose the training data into the smallest pos-
sible units, and learn from them.1 For phrase-
based models, this stipulation implies phrases of
length one. If the model is a synchronous rewrit-
ing system, then it should be able to generate ev-
ery training sentence pair as the yield of a binary-
1Many popular models learn from larger units at the same
time, but the size of the smallest learnable unit is what?s im-
portant for our purposes.
branching synchronous derivation tree, where ev-
ery word-to-word link is generated by a different
derivation step. For example, a model that uses
production rules could generate the previous ex-
ample using the synchronous productions
(S, S) ? (X Y / Y X); (X, X) ? (U V / U V);
(Y, Y) ? (slowly, slowly); (U, U) ? (he, he);
and (V, V) ? (left, left).
A problem arises when this kind of decomposi-
tion is attempted for the alignment in Figure 1(a).
If each link is represented by its own nonterminal,
and production rules must be binary-branching,
then some of the nonterminals involved in gener-
ating this alignment need discontinuities, or gaps.
Figure 1(b) illustrates how to generate the sen-
tence pair and its word alignment in this manner.
The nonterminals X and Y have one discontinuity
each.
More generally, for any positive integer k, it is
possible to construct a word alignment that cannot
be generated using binary production rules whose
nonterminals all have fewer than k gaps (Satta and
Peserico, 2005). Our study measured the com-
plexity of a word alignment as the minimum num-
ber of gaps needed to generate it under the follow-
ing constraints:
1. Each step of the derivation generates no more
than two different nonterminals.
2. Each word-to-word link is generated from a
separate nonterminal.2
Our measure of alignment complexity is analo-
gous to what Melamed et al (2004) call ?fan-
out.?3 The least complex alignments on this mea-
sure ? those that can be generated with zero gaps
? are precisely those that can be generated by an
2If we imagine that each word is generated from a sep-
arate nonterminal as in GCNF (Melamed et al, 2004), then
constraint 2 becomes a special case of constraint 1.
3For grammars that generate bitexts, fan-out is equal to
the maximum number of allowed gaps plus two.
978
bitext # SPs min median max 95% C.I.
Chinese/English 491 4 24 52 .02
Romanian/English 200 2 19 76 .03
Hindi/English 90 1 10 40 .04
Spanish/English 199 4 23 49 .03
French/English 447 2 15 29 .01
Eng/Eng MTEval 5253 2 26 92 .01
Eng/Eng fiction 6263 2 15 97 .01
Table 1: Number of sentence pairs and mini-
mum/median/maximum sentence lengths in each bitext.
All failure rates reported later have a 95% confidence
interval that is no wider than the value shown for each bitext.
ITG. For the rest of the paper, we restrict our atten-
tion to binary derivations, except where explicitly
noted otherwise.
To measure the number of gaps needed to gener-
ate a given word alignment, we used a bottom-up
hierarchical alignment algorithm to infer a binary
synchronous parse tree that was consistent with
the alignment, using as few gaps as possible. A
hierarchical alignment algorithm is a type of syn-
chronous parser where, instead of constraining in-
ferences by the production rules of a grammar, the
constraints come from word alignments and possi-
bly other sources (Wu, 1997; Melamed and Wang,
2005). A bottom-up hierarchical aligner begins
with word-to-word links as constituents, where
some of the links might be to nothing (?NULL?). It
then repeatedly composes constituents with other
constituents to make larger ones, trying to find a
constituent that covers the entire input.
One of the important design choices in this kind
of study is how to treat multiple links attached to
the same word token. Word aligners, both hu-
man and automatic, are often inconsistent about
whether they intend such sets of links to be dis-
junctive or conjunctive. In accordance with its
focus on lower bounds, the present study treated
them as disjunctive, to give the hierarchical align-
ment algorithm more opportunities to use fewer
gaps. This design decision is one of the main dif-
ferences between our study and that of Fox (2002),
who treated links to the same word conjunctively.
By treating many-to-one links disjunctively, our
measure of complexity ignored a large class of dis-
continuities. Many types of discontinuous con-
stituents exist in text independently of any trans-
lation. Simard et al (2005) give examples such
as English verb-particle constructions, and the
French negation ne. . . pas. The disparate elements
of such constituents would usually be aligned to
the same word in a translation. However, when
PP NP
b)
V
S leftGeorgeFriday
George left on Friday
VP
S
NP
V PPleftGeorgeFriday
George left on Friday
on
ona)
Figure 2: a) With a parse tree constraining the top sentence,
a hierarchical alignment is possible without gaps. b) With a
parse tree constraining the bottom sentence, no such align-
ment exists.
our hierarchical aligner saw two words linked to
one word, it ignored one of the two links. Our
lower bounds would be higher if they accounted
for this kind of discontinuity.
3 Experiments
3.1 Data
We used two monolingual bitexts and five
bilingual bitexts. The Romanian/English and
Hindi/English data came from Martin et al (2005).
For Chinese/English and Spanish/English, we
used the data from Ayan et al (2005). The
French/English data were those used by Mihalcea
and Pedersen (2003). The monolingual bitext la-
beled ?MTEval? in the tables consists of multiple
independent translations from Chinese to English
(LDC, 2002). The other monolingual bitext, la-
beled ?fiction,? consists of two independent trans-
lations from French to English of Jules Verne?s
novel 20,000 Leagues Under the Sea, sentence-
aligned by Barzilay and McKeown (2001).
From the monolingual bitexts, we removed all
sentence pairs where either sentence was longer
than 100 words. Table 1 gives descriptive statis-
tics for the remaining data. The table also shows
the upper bound of the 95% confidence intervals
for the coverage rates reported later. The results
of experiments on different bitexts are not directly
comparable, due to the varying genres and sen-
tence lengths.
3.2 Constraining Parse Trees
One of the main independent variables in our ex-
periments was the number of monolingual parse
trees used to constrain the hierarchical alignments.
To induce models of translational equivalence,
some researchers have tried to use such trees to
constrain bilingual constituents: The span of ev-
ery node in the constraining parse tree must coin-
cide with the relevant monolingual span of some
979
crew astronautsincluded
S
NP VP
NP
VP
VP
S
NP
PP
theinare crewincludedastronauts
the
Figure 3: A word alignment that cannot be generated with-
out gaps in a manner consistent with both parse trees.
node in the bilingual derivation tree. These ad-
ditional constraints can thwart attempts at hierar-
chical alignment that might have succeeded oth-
erwise. Figure 2a shows a word alignment and a
parse tree that can be hierarchically aligned with-
out gaps. George and left can be composed in both
sentences into a constituent without crossing any
phrase boundaries in the tree, as can on and Fri-
day. These two constituents can then be composed
to cover the entire sentence pair. On the other
hand, if a constraining tree is applied to the other
sentence as shown in Figure 2b, then the word
alignment and tree constraint conflict. The projec-
tion of the VP is discontinuous in the top sentence,
so the links that it covers cannot be composed into
a constituent without gaps. On the other hand, if a
gap is allowed, then the VP can compose as on Fri-
day . . . left in the top sentence, where the ellipsis
represents a gap. This VP can then compose with
the NP complete a synchronous parse tree. Some
authors have applied constraining parse trees to
both sides of the bitext. The example in Figure 3
can be hierarchically aligned using either one of
the two constraining trees, but gaps are necessary
to align it with both trees.
3.3 Methods
We parsed the English side of each bilingual bitext
and both sides of each English/English bitext us-
ing an off-the-shelf syntactic parser (Bikel, 2004),
which was trained on sections 02-21 of the Penn
English Treebank (Marcus et al, 1993).
Our bilingual bitexts came with manually anno-
tated word alignments. For the monolingual bi-
texts, we used an automatic word aligner based
on a cognate heuristic and a list of 282 function
words compiled by hand. The aligner linked two
words to each other only if neither of them was on
the function word list and their longest common
subsequence ratio (Melamed, 1995) was at least
0.75. Words that were not linked to another word
in this manner were linked to NULL. For the pur-
poses of this study, a word aligned to NULL is
a non-constraint, because it can always be com-
posed without a gap with some constituent that is
adjacent to it on just one side of the bitext. The
number of automatically induced non-NULL links
was lower than what would be drawn by hand.
We modified the word alignments in all bi-
texts to minimize the chances that alignment errors
would lead to an over-estimate of alignment com-
plexity. All of the modifications involved adding
links to NULL. Due to our disjunctive treatment
of conflicting links, the addition of a link to NULL
can decrease but cannot increase the complexity of
an alignment. For example, if we added the links
(cela, NULL) and (NULL, that) to the alignment
in Figure 1, the hierarchical alignment algorithm
could use them instead of the link between cela
and that. It could thus generate the modified align-
ment without using a gap. We added NULL links
in two situations. First, if a subset of the links
in an alignment formed a many-to-many mapping
but did not form a bipartite clique (i.e. every word
on one side linked to every word on the other side),
then we added links from each of these words to
NULL. Second, if n words on one side of the bi-
text aligned to m words on the other side with
m > n then we added NULL links for each of
the words on the side with m words.
After modifying the alignments and obtaining
monolingual parse trees, we measured the align-
ment complexity of each bitext using a hierarchi-
cal alignment algorithm, as described in Section 2.
Separate measurements were taken with zero, one,
and two constraining parse trees. The synchronous
parser in the GenPar toolkit4 can be configured for
all of these cases (Burbank et al, 2005).
Unlike Fox (2002) and Galley et al (2004), we
measured failure rates per corpus rather than per
sentence pair or per node in a constraining tree.
This design was motivated by the observation that
if a translation model cannot correctly model a cer-
tain word alignment, then it is liable to make incor-
rect inferences about arbitrary parts of that align-
ment, not just the particular word links involved in
a complex pattern. The failure rates we report rep-
resent lower bounds on the fraction of training data
4http://nlp.cs.nyu.edu/GenPar
980
# of gaps allowed ? 0/0 0/1 or 1/0
Chinese/English 26 = 5% 0 = 0%
Romanian/English 1 = 0% 0 = 0%
Hindi/English 2 = 2% 0 = 0%
Spanish/English 3 = 2% 0 = 0%
French/English 3 = 1% 0 = 0%
Table 2: Failure rates for hierarchical alignment of bilingual
bitexts under word alignment constraints only.
# of gaps allowed on
non-English side ? 0 1 2
Chinese/English 298 = 61% 28 = 6% 0 = 0%
Romanian/English 82 = 41% 6 = 3% 1 = 0%
Hindi/English 33 = 37% 1 = 1% 0 = 0%
Spanish/English 75 = 38% 4 = 2% 0 = 0%
French/English 67 = 15% 2 = 0% 0 = 0%
Table 3: Failure rates for hierarchical alignment of bilin-
gual bitexts under the constraints of a word alignment and a
monolingual parse tree on the English side.
that is susceptible to misinterpretation by overcon-
strained translation models.
3.4 Summary Results
Table 2 shows the lower bound on alignment fail-
ure rates with and without gaps for five languages
paired with English. This table represents the
case where the only constraints are from word
alignments. Wu (1997) has ?been unable to find
real examples? of cases where hierarchical align-
ment would fail under these conditions, at least
in ?fixed-word-order languages that are lightly in-
flected, such as English and Chinese.? (p. 385).
In contrast, we found examples in all bitexts that
could not be hierarchically aligned without gaps,
including at least 5% of the Chinese/English sen-
tence pairs. Allowing constituents with a single
gap on one side of the bitext decreased the ob-
served failure rate to zero for all five bitexts.
Table 3 shows what happened when we used
monolingual parse trees to restrict the composi-
tions on the English side. The failure rates were
above 35% for four of the five language pairs, and
61% for Chinese/English! Again, the failure rate
fell dramatically when one gap was allowed on the
unconstrained (non-English) side of the bitext. Al-
lowing two gaps on the non-English side led to al-
most complete coverage of these word alignments.
Table 3 does not specify the number of gaps al-
lowed on the English side, because varying this pa-
rameter never changed the outcome. The only way
that a gap on that side could increase coverage is if
there was a node in the constraining parse tree that
# of gaps ? 0/0 0/1 0/2
0 CTs 171 = 3% 0 = 0% 0 = 0%
1 CTs 1792 = 34% 143 = 3% 7 = 0%
2 CTs 3227 = 61% 3227 = 61% 3227 = 61%
Table 4: Failure rates for hierarchical alignment of the
MTEval bitext, over varying numbers of gaps and constrain-
ing trees (CTs).
# of gaps ? 0/0 0/1 0/2
0 CTs 23 = 0% 0 = 0% 0 = 0%
1 CTs 655 = 10% 22 = 0% 1 = 0%
2 CTs 1559 = 25% 1559 = 25% 1559 = 25%
Table 5: Failure rates for hierarchical alignment of the fic-
tion bitext, over varying numbers of gaps and constraining
trees (CTs).
had at least four children whose translations were
in one of the complex permutations. The absence
of such cases in the data implies that the failure
rates under the constraints of one parse tree would
be identical even if we allowed production rules of
rank higher than two.
Table 4 shows the alignment failure rates for the
MTEval bitext. With word alignment constraints
only, 3% of the sentence pairs could not be hierar-
chically aligned without gaps. Allowing a single
gap on one side decreased this failure rate to zero.
With a parse tree constraining constituents on one
side of the bitext and with no gaps, alignment fail-
ure rates rose from 3% to 34%, but allowing a
single gap on the side of the bitext that was not
constrained by a parse tree brought the failure rate
back down to 3%. With two constraining trees the
failure rate was 61%, and allowing gaps did not
lower it, for the same reasons that allowing gaps
on the tree-constrained side made no difference in
Table 3.
The trends in the fiction bitext (Table 5) were
similar to those in the MTEval bitext, but the cov-
erage was always higher, for two reasons. First,
the median sentence size was lower in the fiction
bitext. Second, the MTEval translators were in-
structed to translate as literally as possible, but the
fiction translators paraphrased to make the fiction
more interesting. This freedom in word choice re-
duced the frequency of cognates and thus imposed
fewer constraints on the hierarchical alignment,
which resulted in looser estimates of the lower
bounds. We would expect the opposite effect with
hand-aligned data (Galley et al, 2004).
To study how sentence length correlates with
the complexity of translational equivalence, we
took subsets of each bitext while varying the max-
981
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 10  20  30  40  50  60  70  80  90  100
fa
ilu
re
 ra
te
maximum length of shortest sentence
0 constraining trees
Chinese/Eng
MTeval
fiction
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 10  20  30  40  50  60  70  80  90  100
fa
ilu
re
 ra
te
maximum length of shorter sentence
1 constraining tree
Chinese/Eng
Romanian/Eng
Hindi/Eng
Spanish/Eng
MTeval
French/Eng
fiction
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 10  20  30  40  50  60  70  80  90  100
fa
ilu
re
 ra
te
maximum length of shorter sentence
2 constraining trees
MTeval
fiction
Figure 4: Failure rates for hierarchical alignment without gaps vs. maximum length of shorter sentence.
category ? 1 2 3
valid reordering 12 10 5
parser error n/a 16 25
same word used differently 15 4 0
erroneous cognates 3 0 0
total sample size 30 30 30
initial failure rate (%) 3.25 31.9 38.4
% false negatives 60?7 66?7 84?3
adjusted failure rate (%) 1.3?.22 11?2.2 6?1.1
Table 6: Detailed analysis of hierarchical alignment failures
in MTEval bitext.
imum length of the shorter sentence in each pair.5
Figure 4 plots the resulting alignment failure rates
with and without constraining parse trees. The
lines in these graphs are not comparable to each
other because of the variety of genres involved.
3.5 Detailed Failure Analysis
We examined by hand 30 random sentence pairs
from the MTEval bitext in each of three different
categories: (1) the set of sentence pairs that could
not be hierarchically aligned without gaps, even
without constraining parse trees; (2) the set of sen-
tence pairs that could not be hierarchically aligned
without gaps with one constraining parse tree, but
that did not fall into category 1; and (3) the set
of sentence pairs that could not be hierarchically
aligned without gaps with two constraining parse
trees, but that did not fall into category 1 or 2. Ta-
ble 6 shows the results of this analysis.
In category 1, 60% of the word alignments that
could not be hierarchically aligned without gaps
were caused by word alignment errors. E.g.:
1a GlaxoSmithKline?s second-best selling drug may have
to face competition.
1b Drug maker GlaxoSmithKline may have to face com-
petition on its second best selling product.
The word drug appears in both sentences, but for
different purposes, so drug and drug should not
5The length of the shorter sentence is the upper bound on
the number of non-NULL word alignments.
have been linked.6 Three errors were caused by
words like targeted and started, which our word
alignment algorithm deemed cognates. 12 of the
hierarchical alignment failures in this category
were true failures. For example:
2a Cheney denied yesterday that the mission of his trip
was to organize an assault on Iraq, while in Manama.
2b Yesterday in Manama, Cheney denied that the mis-
sion of his trip was to organize an assault on Iraq.
The alignment pattern of the words in bold is
the familiar (3,1,4,2) permutation, as in Figure 1.
Most of the 12 true failures were due to movement
of prepositional phrases. The freedom of move-
ment for such modifiers would be greater in bitexts
that involve languages with less rigid word order
than English.
Of the 30 sentence pairs in category 2, 16 could
not be hierarchically aligned due to parser errors
and 4 due to faulty word alignments. 10 were due
to valid word reordering. In the following exam-
ple, a co-referring pronoun causes the word align-
ment to fail with a constraining tree on the second
sentence:
3a But Chretien appears to have changed his stance after
meeting with Bush in Washington last Thursday.
3b But after Chretien talked to Bush last Thursday in
Washington, he seemed to change his original stance.
25 of the 30 sentence pairs in category 3 failed
to align due to parser error. 5 examples failed be-
cause of valid word reordering. 1 of the 5 reorder-
ings was due to a difference between active voice
and passive voice, as in Figure 3.
The last row of Table 6 takes the various rea-
sons for alignment failure into account. It esti-
mates what the failure rates would be if the mono-
lingual parses and word alignments were perfect,
with 95% confidence intervals. These revised rates
emphasize the importance of reliable word align-
ments for this kind of study.
6This sort of error is likely to happen with other word
alignment algorithms too, because words and their common
translations are likely to be linked even if they?re not transla-
tionally equivalent in the given sentence.
982
4 Discussion
Figure 1 came from a real bilingual bitext,
and Example 2 in Section 3.5 came from a
real monolingual bitext.7 Neither of these ex-
amples can be hierarchically aligned correctly
without gaps, even without constraining parse
trees. The received wisdom in the literature
led us to expect no such examples in bilin-
gual bitexts, let alne in monolingual bitexts.
See http://nlp.cs.nyu.edu/GenPar/ACL06 for
more examples. The English/English lower
bounds are very loose, because the automatic word
aligner would not link words that were not cog-
nates. Alignment failure rates on a hand aligned
bitext would be higher. We conclude that the ITG
formalism cannot account for the ?natural? com-
plexity of translational equivalence, even when
translation divergences are factored out.
Perhaps our most surprising results were those
involving one constraining parse tree. These re-
sults explain why constraints from independently
generated monolingual parse trees have not im-
proved statistical translation models. For exam-
ple, Koehn et al (2003) reported that ?requiring
constituents to be syntactically motivated does not
lead to better constituent pairs, but only fewer con-
stituent pairs, with loss of a good amount of valu-
able knowledge.? This statement is consistent with
our findings. However, most of the knowledge
loss could be prevented by allowing a gap. With
a parse tree constraining constituents on the En-
glish side, the coverage failure rate was 61% for
the Chinese/English bitext (top row of Table 3),
but allowing a gap decreased it to 6%. Zhang and
Gildea (2004) found that their alignment method,
which did not use external syntactic constraints,
outperformed the model of Yamada and Knight
(2001). However, Yamada and Knight?s model
could explain only the data that would pass the no-
gap test in our experiments with one constraining
tree (first column of Table 3). Zhang and Gildea?s
conclusions might have been different if Yamada
and Knight?s model were allowed to use discon-
tinuous constituents. The second row of Ta-
ble 4 suggests that when constraining parse trees
are used without gaps, at least 34% of training sen-
tence pairs are likely to introduce noise into the
model, even if systematic syntactic differences be-
tween languages are factored out. We should not
7The examples were shortened for the sake of space and
clarity.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70
cu
m
ul
at
ive
 %
ag
e 
of
 s
en
te
nc
es
span length
Figure 5: Lengths of spans covering words in (3,1,4,2) per-
mutations.
be surprised when such constraints do more harm
than good.
To increase the chances that a translation model
can explain complex word alignments, some au-
thors have proposed various ways of extending
a model?s domain of locality. For example,
Callison-Burch et al (2005) have advocated for
longer phrases in finite-state phrase-based transla-
tion models. We computed the phrase length that
would be necessary to cover the words involved
in each (3,1,4,2) permutation in the MTEval bi-
text. Figure 5 shows the cumulative percentage of
these cases that would be covered by phrases up to
a certain length. Only 9 of the 171 cases (5.2%)
could be covered by phrases of length 10 or less.
Analogous techniques for tree-structured transla-
tion models involve either allowing each nonter-
minal to generate both terminals and other non-
terminals (Groves et al, 2004; Chiang, 2005), or,
given a constraining parse tree, to ?flatten? it (Fox,
2002; Zens and Ney, 2003; Galley et al, 2004).
Both of these approaches can increase coverage of
the training data, but, as explained in Section 2,
they risk losing generalization ability.
Our study suggests that there might be some
benefits to an alternative approach using discontin-
uous constituents, as proposed, e.g., by Melamed
et al (2004) and Simard et al (2005). The large
differences in failure rates between the first and
second columns of Table 3 are largely indepen-
dent of the tightness of our lower bounds. Syn-
chronous parsing with discontinuities is computa-
tionally expensive in the worst case, but recently
invented data structures make it feasible for typi-
cal inputs, as long as the number of gaps allowed
per constituent is fixed at a small maximum (Wax-
monsky and Melamed, 2006). More research is
needed to investigate the trade-off between these
costs and benefits.
983
5 Conclusions
This paper presented evidence of phenomena that
can lead to complex patterns of translational
equivalence in bitexts of any language pair. There
were surprisingly many examples of such patterns
that could not be analyzed using binary-branching
structures without discontinuities. Regardless of
the languages involved, the translational equiva-
lence relations in most real bitexts of non-trivial
size cannot be generated by an inversion trans-
duction grammar. The low coverage rates without
gaps under the constraints of independently gen-
erated monolingual parse trees might be the main
reason why ?syntactic? constraints have not yet in-
creased the accuracy of SMT systems. Allowing a
single gap in bilingual phrases or other types of
constituent can improve coverage dramatically.
References
Necip Ayan, Bonnie J. Dorr, and Christof Monz. 2005.
Alignment link projection using transformation-
based learning. In EMNLP.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL.
Andrea Burbank, Marine Carpuat, Stephen Clark,
Markus Dreyer and Pamela Fox, Declan Groves,
Keith Hall, Mary Hearne, I. Dan Melamed,
Yihai Shen, Andy Way, Ben Wellington, and
Dekai Wu. 2005. Final Report on Statistical
Machine Translation by Parsing. JHU CLSP.
http://www.clsp.jhu.edu/ws2005
/groups/statistical/report.html
Dan Bikel. 2004. A distributional analysis of a lexical-
ized statistical parsing model. In EMNLP.
Chris Callison-Burch, Colin Bannard, and Josh
Scroeder. 2005. Scaling phrase-based statistical
machine translation to larger corpora and longer
phrases. In ACL.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL.
Bonnie Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Com-
putational Linguistics 20(4):597?633.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Declan Groves, Mary Hearne, and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In COLING.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In NAACL.
Mitchell Marcus, Beatrice Santorini, and Mary-Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignments for languages with scarce re-
sources. In ACL Workshop on Building and Using
Parallel Texts.
I. Dan Melamed. 1995. Automatic evaluation and uni-
form filter cascades for inducing N -best translation
lexicons. In ACL Workshop on Very Large Corpora.
I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In
ACL.
I. Dan Melamed and Wei Wang. 2005. Gen-
eralized Parsers for Machine Translation.
NYU Proteus Project Technical Report 05-001
http://nlp.cs.nyu.edu/pubs/.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT-NAACL
Workshop on Building and Using Parallel Texts.
LDC. 2002. NIST MT evaluation data, Linguistic
Data Consortium catalogue # LDC2002E53.
http://projects.ldc.upenn.edu
/TIDES/mt2003.html.
Giorgio Satta and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In EMNLP.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Guassier, Cyril Goutte, and
Kenji Yamada. 2005. Translating with non-
contiguous phrases. In EMNLP.
Sonjia Waxmonsky and I. Dan Melamed. 2006. A dy-
namic data structure for parsing with discontinuous
constituents. NYU Proteus Project Technical Report
06-001 http://nlp.cs.nyu.edu/pubs/.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In COL-
ING.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In HLT-NAACL.
984
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 141?151,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Constituent Parsing by Classification
Joseph Turian and I. Dan Melamed
{lastname}@cs.nyu.edu
Computer Science Department
New York University
New York, New York 10003
Abstract
Ordinary classification techniques can
drive a conceptually simple constituent
parser that achieves near state-of-the-art
accuracy on standard test sets. Here we
present such a parser, which avoids some
of the limitations of other discriminative
parsers. In particular, it does not place
any restrictions upon which types of fea-
tures are allowed. We also present sev-
eral innovations for faster training of dis-
criminative parsers: we show how train-
ing can be parallelized, how examples
can be generated prior to training with-
out a working parser, and how indepen-
dently trained sub-classifiers that have
never done any parsing can be effectively
combined into a working parser. Finally,
we propose a new figure-of-merit for best-
first parsing with confidence-rated infer-
ences. Our implementation is freely avail-
able at: http://cs.nyu.edu/?turian/
software/parser/
1 Introduction
Discriminative machine learning methods have im-
proved accuracy on many NLP tasks, such as POS-
tagging (Toutanova et al, 2003), machine translation
(Och & Ney, 2002), and relation extraction (Zhao &
Grishman, 2005). There are strong reasons to believe
the same would be true of parsing. However, only
limited advances have been made thus far, perhaps
due to various limitations of extant discriminative
parsers. In this paper, we present some innovations
aimed at reducing or eliminating some of these lim-
itations, specifically for the task of constituent pars-
ing:
? We show how constituent parsing can be per-
formed using standard classification techniques.
? Classifiers for different non-terminal labels can be
induced independently and hence training can be
parallelized.
? The parser can use arbitrary information to evalu-
ate candidate constituency inferences.
? Arbitrary confidence scores can be aggregated in
a principled manner, which allows beam search.
In Section 2 we describe our approach to parsing. In
Section 3 we present experimental results.
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put sentence. Spans cross if they overlap but nei-
ther contains the other. An item (or constituent) is
a (span, label) pair. A state is a set of parse items,
none of which may cross. A parse inference is a pair
(S , i), given by the current state S and an item i to be
added to it. A parse path (or history) is a sequence
of parse inferences over some input sentence (Klein
& Manning, 2001). An item ordering (ordering, for
short) constrains the order in which items may be in-
ferred. In particular, if we prescribe a complete item
ordering, the parser is deterministic (Marcus, 1980)
and each state corresponds to a unique parse path.
For some input sentence and gold-standard parse, a
state is correct if the parser can infer zero or more
additional items to obtain the gold-standard parse. A
parse path is correct if it leads to a correct state. An
141
inference is correct if adding its item to its state is
correct.
2 Parsing by Classification
Recall that with typical probabilistic parsers, our
goal is to output the parse ?P with the highest like-
lihood for the given input sentence x:
?P = arg max
P?P(x)
Pr(P) (1)
= arg max
P?P(x)
?
I?P
Pr(I) (2)
or, equivalently,
= arg max
P?P(x)
?
I?P
log(Pr(I)) (3)
where each I is a constituency inference in the parse
path P.
In this work, we explore a generalization in which
each inference I is assigned a real-valued confidence
score Q(I) and individual confidences are aggre-
gated using some function A, which need not be a
sum or product:
?P = arg max
P?P(x)
A
I?P
Q(I) (4)
In Section 2.1 we describe how we induce scoring
function Q(I). In Section 2.2 we discuss the aggre-
gation function A. In Section 2.3 we describe the
method used to restrict the size of the search space
over P(x).
2.1 Learning the Scoring Function Q(I)
During training, our goal is to induce the scoring
function Q, which assigns a real-valued confidence
score Q(I) to each candidate inference I (Equa-
tion 4). We treat this as a classification task: If infer-
ence I is correct, we would like Q(I) to be a positive
value, and if inference I is incorrect, we would like
Q(I) to be a negative value.
Training discriminative parsers can be computa-
tionally very expensive. Instead of having a single
classifier score every inference, we parallelize train-
ing by inducing 26 sub-classifiers, one for each con-
stituent label ? in the Penn Treebank (Taylor, Mar-
cus, & Santorini, 2003): Q(I?) = Q?(I?), where
Q? is the ?-classifier and I? is an inference that in-
fers a constituent with label ?. For example, the VP-
classifier QVP would score the VP-inference in Fig-
ure 1, preferably assigning it a positive confidence.
Figure 1 A candidate VP-inference, with head-
children annotated using the rules given in (Collins,
1999).
VP (was)
NP (timing) VBD / was ADJP (perfect)
DT / The NN / timing JJ / perfect
Each ?-classifier is independently trained on training
set E?, where each example e? ? E? is a tuple (I?, y),
I? is a candidate ?-inference, and y ? {?1}. y = +1 if
I? is a correct inference and ?1 otherwise. This ap-
proach differs from that of Yamada and Matsumoto
(2003) and Sagae and Lavie (2005), who parallelize
according to the POS tag of one of the child items.
2.1.1 Generating Training Examples
Our method of generating training examples does
not require a working parser, and can be run prior to
any training. It is similar to the method used in the
literature by deterministic parsers (Yamada & Mat-
sumoto, 2003; Sagae & Lavie, 2005) with one ex-
ception: Depending upon the order constituents are
inferred, there may be multiple bottom-up paths that
lead to the same final parse, so to generate training
examples we choose a single random path that leads
to the gold-standard parse tree.1 The training ex-
amples correspond to all candidate inferences con-
sidered in every state along this path, nearly all of
which are incorrect inferences (with y = ?1). For
instance, only 4.4% of candidate NP-inferences are
correct.
2.1.2 Training Algorithm
During training, for each label ? we induce scor-
ing function Q? to minimize the loss over training
examples E?:
Q? = arg min
Q??
?
(I?,y)?E?
L(y ? Q??(I?)) (5)
1 The particular training tree paths used in our experiments are
included in the aforementioned implementation so that our
results can be replicated under the same experimental condi-
tions.
142
where y ? Q?(I?) is the margin of example (I?, y).
Hence, the learning task is to maximize the margins
of the training examples, i.e. induce scoring function
Q? such that it classifies correct inferences with pos-
itive confidence and incorrect inferences with nega-
tive confidence. In our work, we minimized the lo-
gistic loss:
L(z) = log(1 + exp(?z)) (6)
i.e. the negative log-likelihood of the training sam-
ple.
Our classifiers are ensembles of decisions trees,
which we boost (Schapire & Singer, 1999) to min-
imize the above loss using the update equations
given in Collins, Schapire, and Singer (2002). More
specifically, classifier QT? is an ensemble comprising
decision trees q1?, . . . , q
T
? , where:
QT? (I?) =
T
?
t=1
qt?(I?) (7)
At iteration t, decision tree qt? is grown, its leaves
are confidence-rated, and it is added to the ensemble.
The classifier for each constituent label is trained in-
dependently, so we henceforth omit ? subscripts.
An example (I, y) is assigned weight wt(I, y):2
wt(I, y) = 1
1 + exp(y ? Qt?1(I)) (8)
The total weight of y-value examples that fall in leaf
f is W tf ,y:
W tf ,y =
?
(I,y?)?E
y?=y, I? f
wt(I, y) (9)
and this leaf has loss Ztf :
Ztf = 2 ?
?
W tf ,+ ?W
t
f ,? (10)
Growing the decision tree: The loss of the entire
decision tree qt is
Z(qt) =
?
leaf f?qt
Ztf (11)
2 If we were to replace this equation with wt(I, y) =
exp(y?Qt?1(I))?1, but leave the remainder of the algorithm un-
changed, this algorithm would be confidence-rated AdaBoost
(Schapire & Singer, 1999), minimizing the exponential loss
L(z) = exp(?z). In preliminary experiments, however, we
found that the logistic loss provided superior generalization
accuracy.
We will use Zt as a shorthand for Z(qt). When grow-
ing the decision tree, we greedily choose node splits
to minimize this Z (Kearns & Mansour, 1999). In
particular, the loss reduction of splitting leaf f us-
ing feature ? into two children, f ? ? and f ? ??, is
?Ztf (?):
?Ztf (?) = Ztf ? (Ztf?? + Ztf???) (12)
To split node f , we choose the ?? that reduces loss
the most:
?? = arg max
???
?Ztf (?) (13)
Confidence-rating the leaves: Each leaf f is
confidence-rated as ?tf :
?tf =
1
2
? log
W tf ,+ + 
W tf ,? + 
(14)
Equation 14 is smoothed by the  term (Schapire
& Singer, 1999) to prevent numerical instability in
the case that either W tf ,+ or W
t
f ,? is 0. In our ex-
periments, we used  = 10?8. Although our exam-
ple weights are unnormalized, so far we?ve found
no benefit from scaling  as Collins and Koo (2005)
suggest. All inferences that fall in a particular leaf
node are assigned the same confidence: if inference
I falls in leaf node f in the tth decision tree, then
qt(I) = ?tf .
2.1.3 Calibrating the Sub-Classifiers
An important concern is when to stop growing the
decision tree. We propose the minimum reduction
in loss (MRL) stopping criterion: During training,
there is a value ?t at iteration t which serves as a
threshold on the minimum reduction in loss for leaf
splits. If there is no splitting feature for leaf f that
reduces loss by at least ?t then f is not split. For-
mally, leaf f will not be bisected during iteration t if
max??? ?Ztf (?) < ?t. The MRL stopping criterion
is essentially `0 regularization:?t corresponds to the
`0 penalty parameter and each feature with non-zero
confidence incurs a penalty of ?t, so to outweigh the
penalty each split must reduce loss by at least ?t.
?t decreases monotonically during training at
the slowest rate possible that still allows train-
ing to proceed. We start by initializing ?1 to ?,
and at the beginning of iteration t we decrease ?t
only if the root node ? of the decision tree can-
not be split. Otherwise, ?t is set to ?t?1. Formally,
143
?t = min(?t?1,max??? ?Zt?(?)). In this manner, the
decision trees are induced in order of decreasing ?t.
During training, the constituent classifiers Q?
never do any parsing per se, and they train at dif-
ferent rates: If ? , ??, then ?t? isn?t necessarily
equal to ?t?? . We calibrate the different classifiers by
picking some meta-parameter ?? and insisting that
the sub-classifiers comprised by a particular parser
have all reached some fixed ? in training. Given ??,
the constituent classifier for label ? is Qt?, where
?t? ? ?? > ?
t+1
? . To obtain the final parser, we
cross-validate ??, picking the value whose set of con-
stituent classifiers maximizes accuracy on a devel-
opment set.
2.1.4 Types of Features used by the Scoring
Function
Our parser operates bottom-up. Let the frontier of
a state be the top-most items (i.e. the items with no
parents). The children of a candidate inference are
those frontier items below the item to be inferred, the
left context items are those frontier items to the left
of the children, and the right context items are those
frontier items to the right of the children. For exam-
ple, in the candidate VP-inference shown in Figure 1,
the frontier comprises the NP, VBD, and ADJP items,
the VBD and ADJP items are the children of the VP-
inference (the VBD is its head child), the NP is the left
context item, and there are no right context items.
The design of some parsers in the literature re-
stricts the kinds of features that can be usefully and
efficiently evaluated. Our scoring function and pars-
ing algorithm have no such limitations. Q can, in
principle, use arbitrary information from the history
to evaluate constituent inferences. Although some of
our feature types are based on prior work (Collins,
1999; Klein & Manning, 2003; Bikel, 2004), we
note that our scoring function uses more history in-
formation than typical parsers.
All features check whether an item has some
property; specifically, whether the item?s la-
bel/headtag/headword is a certain value. These fea-
tures perform binary tests on the state directly, un-
like Henderson (2003) which works with an inter-
mediate representation of the history. In our baseline
setup, feature set ? contained five different feature
types, described in Table 1.
Table 2 Feature item groups.
? all children
? all non-head children
? all non-leftmost children
? all non-rightmost children
? all children left of the head
? all children right of the head
? head-child and all children left of the head
? head-child and all children right of the head
2.2 Aggregating Confidences
To get the cumulative score of a parse path P, we ap-
ply aggregatorA over the confidences Q(I) in Equa-
tion 4. Initially, we definedA in the customary fash-
ion as summing the loss of each inference?s confi-
dence:
?P = arg max
P?P(x)
?
?
?
?
?
?
?
?
?
I?P
L (Q(I))
?
?
?
?
?
?
?
(15)
with the logistic loss L as defined in Equation 6. (We
negate the final sum because we want to minimize
the loss.) This definition of A is motivated by view-
ing L as a negative log-likelihood given by a logistic
function (Collins et al, 2002), and then using Equa-
tion 3. It is also inspired by the multiclass loss-based
decoding method of Schapire and Singer (1999).
With this additive aggregator, loss monotonically in-
creases as inferences are added, as in a PCFG-based
parser in which all productions decrease the cumu-
lative probability of the parse tree.
In preliminary experiments, this aggregator gave
disappointing results: precision increased slightly,
but recall dropped sharply. Exploratory data analy-
sis revealed that, because each inference incurs some
positive loss, the aggregator very cautiously builds
the smallest trees possible, thus harming recall. We
had more success by defining A to maximize the
minimum confidence. Essentially,
?P = arg max
P?P(x)
min
I?P
Q(I) (16)
Ties are broken according to the second lowest con-
fidence, then the third lowest, and so on.
2.3 Search
Given input sentence x, we choose the parse path P
in P(x) with the maximum aggregated score (Equa-
tion 4). Since it is computationally intractable to
144
Table 1 Types of features.
? Child item features test if a particular child item has some property. E.g. does the item one right of the
head have headword ?perfect?? (True in Figure 1)
? Context item features test if a particular context item has some property. E.g. does the first item of left
context have headtag NN? (True)
? Grandchild item features test if a particular grandchild item has some property. E.g. does the leftmost
child of the rightmost child item have label JJ? (True)
? Exists features test if a particular group of items contains an item with some property. E.g. does some
non-head child item have label ADJP? (True) Exists features select one of the groups of items specified in
Table 2. Alternately, they can select the terminals dominated by that group. E.g. is there some terminal
item dominated by non-rightmost children items that has headword ?quux?? (False)
consider every possible sequence of inferences, we
use beam search to restrict the size of P(x). As
an additional guard against excessive computation,
search stopped if more than a fixed maximum num-
ber of states were popped from the agenda. As usual,
search also ended if the highest-priority state in the
agenda could not have a better aggregated score than
the best final parse found thus far.
3 Experiments
Following Taskar, Klein, Collins, Koller, and Man-
ning (2004), we trained and tested on ? 15 word sen-
tences in the English Penn Treebank (Taylor et al,
2003), 10% of the entire treebank by word count.3
We used sections 02?21 (9753 sentences) for train-
ing, section 24 (321 sentences) for development,
and section 23 (603 sentences) for testing, prepro-
cessed as per Table 3. We evaluated our parser us-
ing the standard PARSEVAL measures (Black et
al., 1991): labelled precision, recall, and F-measure
(LPRC, LRCL, and LFMS, respectively), which are
computed based on the number of constituents in the
parser?s output that match those in the gold-standard
parse. We tested whether the observed differences in
PARSEVAL measures are significant at p = 0.05 us-
ing a stratified shuffling test (Cohen, 1995, Section
5.3.2) with one million trials.4
As mentioned in Section 1, the parser cannot in-
fer any item that crosses an item already in the state.
3 There was insufficient time before deadline to train on all
sentences.
4 The shuffling test we used was originally implemented
by Dan Bikel (http://www.cis.upenn.edu/?dbikel/
software.html) and subsequently modified to compute p-
values for LFMS differences.
We placed three additional candidacy restrictions
on inferences: (a) Items must be inferred under the
bottom-up item ordering; (b) To ensure the parser
does not enter an infinite loop, no two items in a state
can have both the same span and the same label;
(c) An item can have no more than K = 5 children.
(Only 0.24% of non-terminals in the preprocessed
development set have more than five children.) The
number of candidate inferences at each state, as well
as the number of training examples generated by the
algorithm in Section 2.1.1, is proportional to K. In
our experiment, there were roughly |E?| ? 1.7 mil-
lion training examples for each classifier.
3.1 Baseline
In the baseline setting, context item features (Sec-
tion 2.1.4) could refer to the two nearest items of
context in each direction. The parser used a beam
width of 1000, and was terminated in the rare event
that more than 10,000 states were popped from the
agenda. Figure 2 shows the accuracy of the base-
line on the development set as training progresses.
Cross-validating the choice of ?? against the LFMS
(Section 2.1.3) suggested an optimum of ?? = 1.42.
At this ??, there were a total of 9297 decision tree
splits in the parser (summed over all constituent
classifiers), LFMS = 87.16, LRCL = 86.32, and
LPRC = 88.02.
3.2 Beam Width
To determine the effect of the beam width on the
accuracy, we evaluated the baseline on the devel-
opment set using a beam width of 1, i.e. parsing
entirely greedily (Wong & Wu, 1999; Kalt, 2004;
Sagae & Lavie, 2005). Table 4 compares the base-
145
Table 3 Steps for preprocessing the data. Starred steps are performed only on input with tree structure.
1. * Strip functional tags and trace indices, and remove traces.
2. * Convert PRT to ADVP. (This convention was established by Magerman (1995).)
3. Remove quotation marks (i.e. terminal items tagged ?? or ??). (Bikel, 2004)
4. * Raise punctuation. (Bikel, 2004)
5. Remove outermost punctuation.a
6. * Remove unary projections to self (i.e. duplicate items with the same span and label).
7. POS tag the text using Ratnaparkhi (1996).
8. Lowercase headwords.
9. Replace any word observed fewer than 5 times in the (lower-cased) training sentences with UNK.
a As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation may discard useful information.
It?s also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser
after adding punctuation features, one of which encoded the sentence-final punctuation.
Figure 2 PARSEVAL scores of the baseline on the ? 15 words development set of the Penn Treebank. The
top x-axis shows accuracy as the minimum reduction in loss ?? decreases. The bottom shows the correspond-
ing number of decision tree splits in the parser, summed over all classifiers.
74%
76%
78%
80%
82%
84%
86%
88%
90%
 20000 10000 5000 2500 1000 250
74%
76%
78%
80%
82%
84%
86%
88%
90%
0.341.02.75.0102540120
PA
R
SE
VA
L 
sc
or
e
Total # of splits
Minimum reduction in loss
Labelled precision
Labelled F-measure
Labelled recall
line results on the development set with a beam
width of 1 and a beam width of 1000.5 The wider
beam seems to improve the PARSEVAL scores of
the parser, although we were unable to detect a sta-
tistically significant improvement in LFMS on our
relatively small development set.
5 Using a beam width of 100,000 yielded output identical to
using a beam width of 1000.
3.3 Context Size
Table 5 compares the baseline to parsers that could
not examine as many context items. A significant
portion of the baseline?s accuracy is due to contex-
tual clues, as evidenced by the poor accuracy of the
no context run. However, we did not detect a signif-
icant difference between using one context item or
two.
146
Table 4 PARSEVAL results on the ? 15 words
development set of the baseline, varying the beam
width. Also, the MRL that achieved this LFMS and
the total number of decision tree splits at this MRL.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
Beam=1 86.36 86.20 86.53 2.03 7068
Baseline 87.16 86.32 88.02 1.42 9297
Table 5 PARSEVAL results on the ? 15 words de-
velopment set, given the amount of context avail-
able. is statistically significant. The score differences
between ?context 0? and ?context 1? are significant,
whereas the differences between ?context 1? and the
baseline are not.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
Context 0 75.15 75.28 75.03 3.38 3815
Context 1 86.93 85.78 88.12 2.45 5588
Baseline 87.16 86.32 88.02 1.42 9297
Table 6 PARSEVAL results of decision stumps on
the ? 15 words development set, through 8200
splits. The differences between the stumps run and
the baseline are statistically significant.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
Stumps 85.72 84.65 86.82 2.39 5217
Baseline 87.07 86.05 88.12 1.92 7283
3.4 Decision Stumps
Our features are of relatively fine granularity. To test
if a less powerful machine could provide accuracy
comparable to the baseline, we trained a parser in
which we boosted decisions stumps, i.e. decision
trees of depth 1. Stumps are equivalent to learning
a linear discriminant over the atomic features. Since
the stumps run trained quite slowly, it only reached
8200 splits total. To ensure a fair comparison, in Ta-
ble 6 we chose the best baseline parser with at most
8200 splits. The LFMS of the stumps run on the de-
velopment set was 85.72%, significantly less accu-
rate than the baseline.
For example, Figure 3 shows a case where NP
classification better served by the informative con-
junction ?1 ? ?2 found by the decision trees. Given
Figure 3 An example of a decision (a) stump and
(b) tree for scoring NP-inferences. Each leaf?s value
is the confidence assigned to all inferences that fall
in this leaf. ?1 asks ?does the first child have a de-
terminer headtag??. ?2 asks ?does the last child have
a noun label??. NP classification is better served by
the informative conjunction ?1??2 found by the de-
cision trees.
(a)
?1
true f alse
+0.5 0
(b)
?1
true f alse
?2
true f alse
0
+1.0 -0.2
Table 7 PARSEVAL results of deterministic parsers
on the ? 15 words development set through 8700
splits. A shaded cell means that the difference be-
tween this value and that of the baseline is statisti-
cally significant. All differences between l2r and r2l
are significant.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ?? total
l2r 83.61 82.71 84.54 3.37 2157
r2l 85.76 85.37 86.15 3.39 1881
Baseline 87.07 86.05 88.12 1.92 7283
the sentence ?The man left?, at the initial state there
are six candidate NP-inferences, one for each span,
and ?(NP The man)? is the only candidate inference
that is correct. ?1 is true for the correct inference and
two of the incorrect inferences (?(NP The)? and ?(NP
The man left)?). ?1 ? ?2, on the other hand, is true
only for the correct inference, and so it is better at
discriminating NPs over this sample.
3.5 Deterministic Parsing
Our baseline parser simulates a non-deterministic
machine, as at any state there may be several correct
decisions. We trained deterministic variations of the
parser, for which we imposed strict left-to-right (l2r)
and right-to-left (r2l) item orderings. For these vari-
ations we generated training examples using the cor-
responding unique path to each gold-standard train-
ing tree. The r2l run reached only 8700 splits to-
tal, so in Table 7 we chose the best baseline and l2r
147
Table 8 PARSEVAL results of the full vocabulary
parser on the ? 15 words development set. The dif-
ferences between the full vocabulary run and the
baseline are not statistically significant.
Dev Dev Dev MRL #splits
LFMS LRCL LPRC ? total
Baseline 87.16 86.32 88.02 1.42 9297
Full vocab 87.50 86.85 88.15 1.27 10711
parser with at most 8700 splits.
r2l parsing is significantly more accurate than l2r.
The reason is that the deterministic runs (l2r and r2l)
must avoid prematurely inferring items that come
later in the item ordering. This puts the l2r parser
in a tough spot. If it makes far-right decisions, it?s
more likely to prevent correct subsequent decisions
that are earlier in the l2r ordering, i.e. to the left.
But if it makes far-left decisions, then it goes against
the right-branching tendency of English sentences.
In contrast, the r2l parser is more likely to be correct
when it infers far-right constituents.
We also observed that the accuracy of the de-
terministic parsers dropped sharply as training pro-
gressed (See Figure 4). This behavior was unex-
pected, as the accuracy curve levelled off in every
other experiment. In fact, the accuracy of the deter-
ministic parsers fell even when parsing the training
data. To explain this behavior, we examined the mar-
gin distributions of the r2l NP-classifier (Figure 5).
As training progressed, the NP-classifier was able to
reduce loss by driving up the margins of the incor-
rect training examples, at the expense of incorrectly
classifying a slightly increased number of correct
training examples. However, this is detrimental to
parsing accuracy. The more correct inferences with
negative confidence, the less likely it is at some state
that the highest confidence inference is correct. This
effect is particularly pronounced in the deterministic
setting, where there is only one correct inference per
state.
3.6 Full Vocabulary
As in traditional parsers, the baseline was smoothed
by replacing any word that occurs fewer than five
times in the training data with the special token UNK
(Table 3.9). Table 8 compares the baseline to a full
vocabulary run, in which the vocabulary contained
all words observed in the training data. As evidenced
by the results therein, controlling for lexical sparsity
did not significantly improve accuracy in our setting.
In fact, the full vocabulary run is slightly more ac-
curate than the baseline on the development set, al-
though this difference was not statistically signifi-
cant. This was a late-breaking result, and we used
the full vocabulary condition as our final parser for
parsing the test set.
3.7 Test Set Results
Table 9 shows the results of our best parser on the
? 15 words test set, as well as the accuracy reported
for a recent discriminative parser (Taskar et al,
2004) and scores we obtained by training and test-
ing the parsers of Charniak (2000) and Bikel (2004)
on the same data. Bikel (2004) is a ?clean room?
reimplementation of the Collins parser (Collins,
1999) with comparable accuracy. Both Charniak
(2000) and Bikel (2004) were trained using the gold-
standard tags, as this produced higher accuracy on
the development set than using Ratnaparkhi (1996)?s
tags.
3.8 Exploratory Data Analysis
To gain a better understanding of the weaknesses of
our parser, we examined a sample of 50 develop-
ment sentences that the full vocabulary parser did
not get entirely correct. Besides noise and cases of
genuine ambiguity, the following list outlines all er-
ror types that occurred in more than five sentences,
in roughly decreasing order of frequency. (Note that
there is some overlap between these groups.)
? ADVPs and ADJPs A disproportionate amount of
the parser?s error was due to ADJPs and ADVPs.
Out of the 12.5% total error of the parser on the
development set, an absolute 1.0% was due to
ADVPs, and 0.9% due to ADJPs. The parser had
LFMS = 78.9%,LPRC = 82.5%,LRCL = 75.6%
on ADVPs, and LFMS = 68.0%,LPRC =
71.2%,LRCL = 65.0% on ADJPs.
These constructions can sometimes involve tricky
attachment decisions. For example, in the frag-
ment ?to get fat in times of crisis?, the parser?s
output was ?(VP to (VP get (ADJP fat (PP in (NP
(NP times) (PP of (NP crisis)))))))? instead of the
correct construction ?(VP to (VP get (ADJP fat) (PP
in (NP (NP times) (PP of (NP crisis))))))?.
148
Figure 4 LFMS of the baseline and the deterministic runs on the ? 15 words development set of the Penn
Treebank. The x-axis shows the LFMS as training progresses and the number of decision tree splits in-
creases.
 74
 76
 78
 80
 82
 84
 86
 88
 8700 5000 2500 1000 250
 74
 76
 78
 80
 82
 84
 86
 88
Pa
rs
ev
al
 F
M
S
Total # of splits
Baseline
Right-to-left
Left-to-right
Figure 5 The margin distributions of the r2l NP-classifier, early in training and late in training, (a) over the
incorrect training examples and (b) over the correct training examples.
(a)
-20
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1
M
ar
gi
n
Percentile
Late in training
Early in training
(b)
-40
-30
-20
-10
 0
 10
 20
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  1
M
ar
gi
n
Percentile
Late in training
Early in training
The amount of noise present in ADJP and ADVP
annotations in the PTB is unusually high. Annota-
tion of ADJP and ADVP unary projections is partic-
ularly inconsistent. For example, the development
set contains the sentence ?The dollar was trading
sharply lower in Tokyo .?, with ?sharply lower?
bracketed as ?(ADVP (ADVP sharply) lower)?.
?sharply lower? appears 16 times in the complete
training section, every time bracketed as ?(ADVP
sharply lower)?, and ?sharply higher? 10 times,
always as ?(ADVP sharply higher)?. Because of the
high number of negative examples, the classifiers?
149
Table 9 PARSEVAL results of on the ? 15 words test set of various parsers in the literature. The differ-
ences between the full vocabulary run and Bikel or Charniak are significant. Taskar et al (2004)?s output
was unavailable for significance testing, but presumably its differences from the full vocab parser are also
significant.
Test Test Test Dev Dev Dev
LFMS LRCL LPRC LFMS LRCL LPRC
Full vocab 87.13 86.47 87.80 87.50 86.85 88.15
Bikel (2004) 88.85 88.31 89.39 86.82 86.43 87.22
Taskar et al (2004) 89.12 89.10 89.14 89.98 90.22 89.74
Charniak (2000) 90.09 90.01 90.17 89.50 89.69 89.32
bias is to cope with the noise by favoring negative
confidences predictions for ambiguous ADJP and
ADVP decisions, hence their abysmal labelled re-
call. One potential solution is the weight-sharing
strategy described in Section 3.5.
? Tagging Errors Many of the parser?s errors
were due to poor tagging. Preprocessing sentence
?Would service be voluntary or compulsory ??
gives ?would/MD service/VB be/VB voluntary/JJ
or/CC UNK/JJ? and, as a result, the parser brack-
ets ?service . . . compulsory? as a VP instead of
correctly bracketing ?service? as an NP. We also
found that the tagger we used has difficulties with
completely capitalized words, and tends to tag
them NNP. By giving the parser access to the same
features used by taggers, especially rich lexical
features (Toutanova et al, 2003), the parser might
learn to compensate for tagging errors.
? Attachment decisions The parser does not de-
tect affinities between certain word pairs, so it has
difficulties with bilexical dependency decisions.
In principle, bilexical dependencies can be rep-
resented as conjunctions of feature given in Sec-
tion 2.1.4. Given more training data, the parser
might learn these affinities.
4 Conclusions
In this work, we presented a near state-of-the-
art approach to constituency parsing which over-
comes some of the limitations of other discrimina-
tive parsers. Like Yamada and Matsumoto (2003)
and Sagae and Lavie (2005), our parser is driven by
classifiers. Even though these classifiers themselves
never do any parsing during training, they can be
combined into an effective parser. We also presented
a beam search method under the objective function
of maximizing the minimum confidence.
To ensure efficiency, some discriminative parsers
place stringent requirements on which types of fea-
tures are permitted. Our approach requires no such
restrictions and our scoring function can, in prin-
ciple, use arbitrary information from the history to
evaluate constituent inferences. Even though our
features may be of too fine granularity to dis-
criminate through linear combination, discrimina-
tively trained decisions trees determine useful fea-
ture combinations automatically, so adding new fea-
tures requires minimal human effort.
Training discriminative parsers is notoriously
slow, especially if it requires generating examples by
repeatedly parsing the treebank (Collins & Roark,
2004; Taskar et al, 2004). Although training time
is still a concern in our setup, the situation is ame-
liorated by generating training examples in advance
and inducing one-vs-all classifiers in parallel, a tech-
nique similar in spirit to the POS-tag parallelization
in Yamada and Matsumoto (2003) and Sagae and
Lavie (2005).
This parser serves as a proof-of-concept, in that
we have not fully exploited the possibilities of en-
gineering intricate features or trying more complex
search methods. Its flexibility offers many oppor-
tunities for improvement, which we leave to future
work.
Acknowledgments
The authors would like to thank Dan Bikel, Mike
Collins, Ralph Grishman, Adam Meyers, Mehryar
Mohri, Satoshi Sekine, and Wei Wang, as well as the
anonymous reviewers, for their helpful comments
150
and constructive criticism. This research was spon-
sored by an NSF CAREER award, and by an equip-
ment gift from Sun Microsystems.
References
Bikel, D. M. (2004). Intricacies of Collins? pars-
ing model. Computational Linguistics, 30(4),
479?511.
Black, E., Abney, S., Flickenger, D., Gdaniec, C.,
Grishman, R., Harrison, P., et al (1991).
A procedure for quantitatively comparing the
syntactic coverage of English grammars. In
Speech and Natural Language (pp. 306?311).
Charniak, E. (2000). A maximum-entropy-inspired
parser. In NAACL (pp. 132?139).
Cohen, P. R. (1995). Empirical methods for artificial
intelligence. MIT Press.
Collins, M. (1999). Head-driven statistical models
for natural language parsing. Unpublished
doctoral dissertation, UPenn.
Collins, M. (2003). Head-driven statistical models
for natural language parsing. Computational
Linguistics, 29(4), 589?637.
Collins, M., & Koo, T. (2005). Discriminative
reranking for natural language parsing. Com-
putational Linguistics, 31(1), 25?69.
Collins, M., & Roark, B. (2004). Incremental pars-
ing with the perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., & Singer, Y. (2002).
Logistic regression, AdaBoost and Bregman
distances. Machine Learning, 48(1-3), 253?
285.
Henderson, J. (2003). Inducing history representa-
tions for broad coverage statistical parsing. In
HLT/NAACL.
Kalt, T. (2004). Induction of greedy controllers
for deterministic treebank parsers. In EMNLP
(pp. 17?24).
Kearns, M. J., & Mansour, Y. (1999). On the boost-
ing ability of top-down decision tree learning
algorithms. Journal of Computer and Systems
Sciences, 58(1), 109?128.
Klein, D., & Manning, C. D. (2001). Parsing and
hypergraphs. In IWPT (pp. 123?134).
Klein, D., & Manning, C. D. (2003). Accurate un-
lexicalized parsing. In ACL (pp. 423?430).
Magerman, D. M. (1995). Statistical decision-tree
models for parsing. In ACL (pp. 276?283).
Marcus, M. P. (1980). Theory of syntactic recogni-
tion for natural languages. MIT Press.
Och, F. J., & Ney, H. (2002). Discriminative training
and maximum entropy models for statistical
machine translation. In ACL.
Ratnaparkhi, A. (1996). A maximum entropy part-
of-speech tagger. In EMNLP (pp. 133?142).
Sagae, K., & Lavie, A. (2005). A classifier-based
parser with linear run-time complexity. In
IWPT.
Schapire, R. E., & Singer, Y. (1999). Improved
boosting using confidence-rated predictions.
Machine Learning, 37(3), 297?336.
Taskar, B., Klein, D., Collins, M., Koller, D., &
Manning, C. (2004). Max-margin parsing.
In EMNLP (pp. 1?8).
Taylor, A., Marcus, M., & Santorini, B. (2003). The
Penn Treebank: an overview. In A. Abeille?
(Ed.), Treebanks: Building and using parsed
corpora (pp. 5?22).
Toutanova, K., Klein, D., Manning, C. D., & Singer,
Y. (2003). Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In
HLT/NAACL (pp. 252?259).
Wong, A., & Wu, D. (1999). Learning a
lightweight robust deterministic parser. In
EUROSPEECH.
Yamada, H., & Matsumoto, Y. (2003). Statistical
dependency analysis with support vector ma-
chines. In IWPT.
Zhao, S., & Grishman, R. (2005). Extracting rela-
tions with integrated information using kernel
methods. In ACL.
151
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 17?24,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Computational Challenges in Parsing by Classification
Joseph Turian and I. Dan Melamed
{lastname}@cs.nyu.edu
Computer Science Department
New York University
New York, New York 10003
Abstract
This paper presents a discriminative
parser that does not use a generative
model in any way, yet whose accu-
racy still surpasses a generative base-
line. The parser performs feature selec-
tion incrementally during training, as op-
posed to a priori, which enables it to
work well with minimal linguistic clever-
ness. The main challenge in building this
parser was fitting the training data into
memory. We introduce gradient sampling,
which increased training speed 100-fold.
Our implementation is freely available at
http://nlp.cs.nyu.edu/parser/.
1 Introduction
Discriminative machine learning methods have im-
proved accuracy on many NLP tasks, including
POS-tagging, shallow parsing, relation extraction,
and machine translation. However, only limited ad-
vances have been made on full syntactic constituent
parsing. Successful discriminative parsers have used
generative models to reduce training time and raise
accuracy above generative baselines (Collins &
Roark, 2004; Henderson, 2004; Taskar et al, 2004).
However, relying upon information from a gener-
ative model might limit the potential of these ap-
proaches to realize the accuracy gains achieved by
discriminative methods on other NLP tasks. Another
difficulty is that discriminative parsing approaches
can be very task-specific and require quite a bit of
trial and error with different hyper-parameter values
and types of features.
In the present work, we make progress towards
overcoming these obstacles. We propose a flexible,
well-integrated method for training discriminative
parsers, demonstrating techniques that might also
be useful for other structured learning problems.
The learning algorithm projects the hand-provided
atomic features into a compound feature space and
performs incremental feature selection from this
large feature space. We achieve higher accuracy than
a generative baseline, despite not using the standard
trick of including an underlying generative model.
Our training regime does model selection without
ad-hoc smoothing or frequency-based feature cut-
offs, and requires no heuristics to optimize the single
hyper-parameter.
We discuss the computational challenges we over-
came to build this parser. The main difficulty is that
the training data fit in memory only using an indirect
representation,1 so the most costly operation during
training is accessing the features of a particular ex-
ample. We show how to train a parser effectively un-
der these conditions. We also show how to speed up
training by using a principled sampling method to
estimate the loss gradients used in feature selection.
?2 describes the parsing algorithm. ?3 presents
the learning method and techniques used to reduce
training time. ?4 presents experiments with discrim-
inative parsers built using these methods. ?5 dis-
1Similar memory limitations exist in other large-scale NLP
tasks. Syntax-driven SMT systems are typically trained on
an order of magnitude more sentences than English parsers,
and unsupervised estimation methods can generate an arbitrary
number of negative examples (Smith & Eisner, 2005).
17
cusses possible issues in scaling to larger example
sets.
2 Parsing Algorithm
The following terms will help to explain our work.
A span is a range over contiguous words in the in-
put. Spans cross if they overlap but neither contains
the other. An item is a (span, label) pair. A state is a
partial parse, i.e. a set of items, none of whose spans
cross. A parse inference is a (state, item) pair, i.e. a
state and a (consequent) item to be added to it. The
frontier of a state consists of the items with no par-
ents yet. The children of an inference are the frontier
items below the item to be inferred, and the head of
an inference is the child item chosen by head rules
(Collins, 1999, pp. 238?240). A parse path is a se-
quence of parse inferences. For some input sentence
and training parse tree, a state is correct if the parser
can infer zero or more additional items to obtain the
training parse tree and an inference is correct if it
leads to a correct state.
Now, given input sentence s we compute:
p? = arg min
p?P(s)
?
????????
?
i?p
l(i)
?
????????
(1)
where P(s) are possible parses of the sentence, and
the loss (or cost) l of parse p is summed over the
inferences i that lead to the parse. To find p?, the
parsing algorithm considers a sequence of states.
The initial state contains terminal items, whose la-
bels are the POS tags given by Ratnaparkhi (1996).
The parser considers a set of (bottom-up) inferences
at each state. Each inference results in a successor
state to be placed on the agenda. The loss function
l can consider arbitrary properties of the input and
parse state,2 which precludes a tractable dynamic
programming solution to Equation 1. Therefore, we
do standard agenda-based parsing, but instead of
items our agenda stores entire states, as per more
general best-first search over parsing hypergraphs
(Klein & Manning, 2001). Each time we pop a state
from the agenda, l computes a loss for the bottom-
up inferences generated from that state. If the loss
of the popped state exceeds that of the current best
complete parse, search is done and we have found
the optimal parse.
2I.e. we make no context-free assumptions.
3 Training Method
3.1 General Setting
From each training inference i ? I we generate the
tuple ?X(i), y(i), b(i)?. X(i) is a feature vector de-
scribing i, with each element in {0, 1}. The observed
y-value y(i) ? {?1,+1} is determined by whether i
is a correct inference or not. Some training exam-
ples might be more important than others, so each is
given an initial bias b(i) ? R+.
Our goal during training is to induce a real-valued
inference scoring function (hypothesis) h(i;?),
which is a linear model parameterized by a vector
? of reals:
h(i;?) = ? ? X(i) =
?
f
? f ? X f (i) (2)
Each f is a feature. The sign of h(i;?) predicts the
y-value of i and the magnitude gives the confidence
in this prediction.
The training procedure optimizes ? to minimize
the expected risk R:
R(I;?) = L(I;?) + ?(?) (3)
In principle, L can be any loss function, but in the
present work we use the log-loss (Collins et al,
2002):
L(I;?) =
?
i?I
l(i;?) =
?
i?I
b(i) ? ?(?(i;?)) (4)
where:
?(?) = ln(1 + exp(??)) (5)
and the margin of inference i under the current
model ? is:
?(i;?) = y(i) ? h(i;?) (6)
For a particular choice of ?, l(i) in Equation 1 is
computed according to Equation 4 using y(i) = +1
and b(i) = 1.
?(?) in Equation 3 is a regularizer, which penal-
izes overly complex models to reduce overfitting and
generalization error. We use the `1 penalty:
?(?) =
?
f
? ? |? f | (7)
where ? is the `1 parameter that controls the strength
of the regularizer. This choice of objective R is mo-
tivated by Ng (2004), who suggests that, given a
18
learning setting where the number of irrelevant fea-
tures is exponential in the number of training exam-
ples, we can nonetheless learn effectively by build-
ing decision trees to minimize the `1-regularized
log-loss. Conversely, Ng (2004) suggests that most
of the learning algorithms commonly used by dis-
criminative parsers will overfit when exponentially
many irrelevant features are present.3
Learning over an exponential feature space is the
very setting we have in mind. A priori, we define
only a set A of simple atomic features (see ?4).
However, the learner induces compound features,
each of which is a conjunction of possibly negated
atomic features. Each atomic feature can have three
values (yes/no/don?t care), so the size of the com-
pound feature space is 3|A|, exponential in the num-
ber of atomic features. It was also exponential in
the number of training examples in our experiments
(|A| ? |I|).
We use an ensemble of confidence-rated deci-
sion trees (Schapire & Singer, 1999) to represent h.4
Each node in a decision tree corresponds to a com-
pound feature, and the leaves of the decision trees
keep track of the parameter values of the compound
features they represent. To score an inference using
a decision tree, we percolate the inference down to
a leaf and return that leaf?s confidence. The overall
score given to an inference by the whole ensemble
is the sum of the confidences returned by the trees in
the ensemble.
3.2 Boosting `1-Regularized Decision Trees
Listing 1 presents our training algorithm. (Sampling
will be explained in ?3.3. Until then, assume that
the sample S is the entire training set I.) At the be-
ginning of training, the ensemble is empty, ? = 0,
and the `1 parameter ? is set to?. We train until the
objective cannot be further reduced for the current
choice of ?. We then relax the regularization penalty
by decreasing ? and continuing training. We also de-
3including the following learning algorithms:
? unregularized logistic regression
? logistic regression with an `2 penalty (i.e. a Gaussian prior)
? SVMs using most kernels
? multilayer neural nets trained by backpropagation
? the perceptron algorithm
4Turian and Melamed (2005) show that that decision trees ap-
plied to parsing have higher accuracy and training speed than
decision stumps.
Listing 1 Training algorithm.
1: procedure T????(I)
2: ensemble? ?
3: h(i)? 0 for all i ? I
4: for T = 1 . . .? do
5: S ? priority sample I
6: extract X(i) for all i ? S
7: build decision tree t using S
8: percolate every i ? I to a leaf node in t
9: for each leaf f in t do
10: choose ? f to minimize R
11: add ? f to h(i) for all i in this leaf
termine the accuracy of the parser on a held-out de-
velopment set using the previous ? value (before it
was decreased), and can stop training when this ac-
curacy plateaus. In this way, instead of choosing the
best ? heuristically, we can optimize it during a sin-
gle training run (Turian & Melamed, 2005).
Our strategy for optimizing ? to minimize the ob-
jective R (Equation 3) is a variant of steepest descent
(Perkins et al, 2003). Each training iteration has
several steps. First, we choose some new compound
features that have high magnitude gradient with re-
spect to the objective function. We do this by build-
ing a new decision tree, whose leaves represent the
new compound features.5 Second, we confidence-
rate each leaf to minimize the objective over the ex-
amples that percolate down to that leaf. Finally, we
append the decision tree to the ensemble and up-
date parameter vector ? accordingly. In this manner,
compound feature selection is performed incremen-
tally during training, as opposed to a priori.
To build each decision tree, we begin with a root
node, and we recursively split nodes by choosing a
splitting feature that will allow us to decrease the
objective. We have:
?L(I;?)
?? f
=
?
i?I
?l(i;?)
??(i;?) ?
??(i;?)
?? f
(8)
where:
??(i;?)
?? f
= y(i) ? X f (i) (9)
We define the weight of an example under the cur-
rent model as:
w(i;?) = ? ?l(i;?)
??(i;?) = b(i) ?
1
1 + exp(?(i;?)) . (10)
5Any given compound feature can appear in more than one
tree.
19
and:
W y?f (I;?) =
?
i?I
X f (i)=1,y(i)=y?
w(i;?) (11)
Combining Equations 8?11 gives:6
?L
?? f
= W?1f ?W
+1
f (12)
We define the gain G f of feature f as:
G f = max
(
0,
??????
?L
?? f
??????
? ?
)
(13)
Equation 13 has this form because the gradient of the
penalty term is undefined at ? f = 0. This discontinu-
ity is why `1 regularization tends to produce sparse
models. If G f = 0, then the objective R is at its min-
imum with respect to parameter ? f . Otherwise, G f
is the magnitude of the gradient of the objective as
we adjust ? f in the appropriate direction.
The gain of splitting node f using some atomic
feature a is defined as
?G f (a) = G f?a + G f??a (14)
We allow node f to be split only by atomic features
a that increase the gain, i.e. ?G f (a) > G f . If no such
feature exists, then f becomes a leaf node of the de-
cision tree and ? f becomes one of the values to be
optimized during the parameter update step. Other-
wise, we choose atomic feature a? to split node f :
a? = arg max
a?A
?G f (a) (15)
This split creates child nodes f ? a? and f ??a?. If no
root node split has positive gain, then training has
converged for the current choice of `1 parameter ?.
Parameter update is done sequentially on only the
most recently added compound features, which cor-
respond to the leaves of the new decision tree. After
the entire tree is built, we percolate examples down
to their appropriate leaf nodes. We then choose for
each leaf node f the parameter ? f that minimizes the
objective R over the examples in that leaf. Decision
trees ensure that these compound features are mu-
tually exclusive, so they can be directly optimized
independently of each other using a line search over
the objective R.
6Since ? is fixed during a particular training iteration and I is
fixed throughout training, we omit parameters (I;?) henceforth.
3.3 Sampling for Faster Feature Selection
Building a decision tree using the entire example set
I can be very expensive, which we will demonstrate
in ?4.2. However, feature selection can be effective
even if we don?t examine every example. Since the
weight of high-margin examples can be several or-
ders of magnitude lower than that of low-margin ex-
amples (Equation 10), the contribution of the high-
margin examples to feature weights (Equation 11)
will be insignificant. Therefore, we can ignore most
examples during feature selection as long as we have
good estimates of feature weights, which in turn give
good estimates of the loss gradients (Equation 12).
As shown in Step 1.5 of Listing 1, before building
each decision tree we use priority sampling (Duffield
et al, 2005) to choose a small subset of the ex-
amples according to the example weights given by
the current classifier, and the tree is built using only
this subset. We make the sample small enough that
its entire atomic feature matrix will fit in memory.
To optimize decision tree building, we compute and
cache the sample?s atomic feature matrix in advance
(Step 1.6).
Even if the sample is missing important informa-
tion in one iteration, the training procedure is capa-
ble of recovering it from samples used in subsequent
iterations. Moreover, even if a sample?s gain esti-
mates are inaccurate and the feature selection step
chooses irrelevant compound features, confidence
updates are based upon the entire training set and
the regularization penalty will prevent irrelevant fea-
tures from having their parameters move away from
zero.
3.4 The Training Set
Our training set I contains all inferences considered
in every state along the correct path for each gold-
standard parse tree (Sagae & Lavie, 2005).7 This
method of generating training examples does not re-
quire a working parser and can be run prior to any
training. The downside of this approach is that it
minimizes the error of the parser at correct states
only. It does not account for compounded error or
teach the parser to recover from mistakes gracefully.
7Since parsing is done deterministically right-to-left, there can
be no more than one correct inference at each state.
20
Turian and Melamed (2005) observed that uni-
form example biases b(i) produced lower accuracy
as training progressed, because the induced classi-
fiers minimized the example-wise error. Since we
aim to minimize the state-wise error, we express this
bias by assigning every training state equal value,
and?for the examples generated from that state?
sharing half the value uniformly among the nega-
tive examples and the other half uniformly among
the positive examples.
Although there are O(n2) possible spans over a
frontier containing n items, we reduce this to the
O(n) inferences that cannot have more than 5 chil-
dren. With no restriction on the number of children,
there would be O(n2) bottom-up inferences at each
state. However, only 0.57% of non-terminals in the
preprocessed development set have more than five
children.
Like Turian and Melamed (2005), we parallelize
training by inducing 26 label classifiers (one for
each non-terminal label in the Penn Treebank). Par-
allelization might not uniformly reduce training time
because different label classifiers train at different
rates. However, parallelization uniformly reduces
memory usage because each label classifier trains
only on inferences whose consequent item has that
label. Even after parallelization, the atomic feature
matrix cannot be cached in memory. We can store
the training inferences in memory using only an in-
direct representation. More specifically, for each in-
ference i in the training set, we cache in memory
several values: a pointer i to a tree cut, its y-value
y(i), its bias b(i), and its confidence h(i) under the
current model. We cache h(i) throughout training be-
cause it is needed both in computing the gradient of
the objective during decision tree building (Step 1.7)
as well as subsequent minimization of the objective
over the decision tree leaves (Step 1.10). We update
the confidences at the end of each training iteration
using the newly added tree (Step 1.11).
The most costly operation during training is to ac-
cess the feature values in X(i). An atomic feature
test determines the value Xa(i) for a single atomic
feature a by examining the tree cut pointed to by in-
ference i. Alternately, we can perform atomic fea-
ture extraction, i.e. determine all non-zero atomic
features over i.8 Extraction is 100?1000 times more
expensive than a single test, but is necessary during
decision tree building (Step 1.7) because we need
the entire vector X(i) to accumulate inferences in
children nodes. Essentially, for each inference i that
falls in some node f , we accumulate w(i) in Wy(i)f?a
for all a with Xa(i) = 1. After all the inferences in a
node have been accumulated, we try to split the node
(Equation 15). The negative child weights are each
determined as Wyf??a = W
y
f ?W
y
f?a.
4 Experiments
We follow Taskar et al (2004) and Turian and
Melamed (2005) in training and testing on ? 15
word sentences in the English Penn Treebank (Tay-
lor et al, 2003). We used sections 02?21 for train-
ing, section 22 for development, and section 23,
for testing. We use the same preprocessing steps as
Turian and Melamed (2005): during both training
and testing, the parser is given text POS-tagged by
the tagger of Ratnaparkhi (1996), with capitalization
stripped and outermost punctuation removed.
For reasons given in Turian and Melamed (2006),
items are inferred bottom-up right-to-left. As men-
tioned in ?2, the parser cannot infer any item that
crosses an item already in the state. To ensure the
parser does not enter an infinite loop, no two items
in a state can have both the same span and the same
label. Given these restrictions, there were roughly 40
million training examples. These were partitioned
among the constituent label classifiers.
Our atomic feature set A contains features of
the form ?is there an item in group J whose la-
bel/headword/headtag/headtagclass9 is ?X???. Pos-
sible values of ?X? for each predicate are collected
from the training data. Some examples of possible
values for J include the last n child items, the first n
left context items, all right context items, and the ter-
minal items dominated by the non-head child items.
Space constraints prevent enumeration of the head-
tagclasses and atomic feature templates, which are
8Extraction need not take the na??ve approach of performing |A|
different tests, and can be optimized by using knowledge about
the nature of the atomic feature templates.
9The predicate headtagclass is a supertype of the headtag.
Given our compound features, these are not strictly neces-
sary, but they accelerate training. An example is ?proper noun,?
which contains the POS tags given to singular and plural proper
nouns.
21
Figure 1 F1 score of our parser on the development
set of the Penn Treebank, using only ? 15 word sen-
tences. The dashed line indicates the percent of NP
example weight lost due to sampling. The bottom
x-axis shows the number of non-zero parameters in
each parser, summed over all label classifiers.
7.5K5K2.5K1.5K1K 84%
85%
86%
87%
88%
89%
90%
91%5.42.51.00.5
De
vel
. F-
me
asu
re
total number of non-zero parameters
training time (days)
0%
5%
10%
15%
20%
25%
30%
35%
we
igh
t lo
st d
ue 
to s
am
plin
g
instead provided at the URL given in the abstract.
These templates gave 1.1 million different atomic
features. We experimented with smaller feature sets,
but found that accuracy was lower. Charniak and
Johnson (2005) use linguistically more sophisticated
features, and Bod (2003) and Kudo et al (2005) use
sub-tree features, all of which we plan to try in fu-
ture work.
We evaluated our parser using the standard PAR-
SEVAL measures (Black et al, 1991): labelled
precision, labelled recall, and labelled F-measure
(Prec., Rec., and F1, respectively), which are based
on the number of non-terminal items in the parser?s
output that match those in the gold-standard parse.
The solid curve Figure 1 shows the accuracy of
the parser over the development set as training pro-
gressed. The parser exceeded 89% F-measure af-
ter 2.5 days of training. The peak F-measure was
90.55%, achieved at 5.4 days using 6.3K active
parameters. We omit details given by Turian and
Melamed (2006) in favor of a longer discussion in
?4.2.
4.1 Test Set Results
To situate our results in the literature, we compare
our results to those reported by Taskar et al (2004)
and Turian and Melamed (2005) for their discrimi-
native parsers, which were also trained and tested on
? 15 word sentences. We also compare our parser
to a representative non-discriminative parser (Bikel,
Table 1 PARSEVAL results of parsers on the test
set, using only ? 15 word sentences.
F1 % Rec. % Prec. %
Turian and Melamed (2005) 87.13 86.47 87.80
Bikel (2004) 88.30 87.85 88.75
Taskar et al (2004) 89.12 89.10 89.14
our parser 89.40 89.26 89.55
Table 2 Profile of an NP training iteration, given
in seconds, using an AMD Opteron 242 (64-bit,
1.6Ghz). Steps refer to Listing 1.
Step Description mean stddev %
1.5 Sample 1.5s 0.07s 0.7%
1.6 Extraction 38.2s 0.13s 18.6%
1.7 Build tree 127.6s 27.60s 62.3%
1.8 Percolation 31.4s 4.91s 15.3%
1.9?11 Leaf updates 6.2s 1.75s 3.0%
1.5?11 Total 204.9s 32.6s 100.0%
2004),10 the only one that we were able to train and
test under exactly the same experimental conditions
(including the use of POS tags from Ratnaparkhi
(1996)). Table 1 shows the PARSEVAL results of
these four parsers on the test set.
4.2 Efficiency
40% of non-terminals in the Penn Treebank are
NPs. Consequently, the bottleneck in training is
induction of the NP classifier. It was trained on
1.65 million examples. Each example had an aver-
age of 440 non-zero atomic features (stddev 123),
so the direct representation of each example re-
quires a minimum 440 ? sizeof(int) = 1760 bytes,
and the entire atomic feature matrix would re-
quire 1760 bytes ? 1.65 million = 2.8 GB. Con-
versely, an indirectly represent inference requires
no more 32 bytes: two floats (the cached confi-
dence h(i) and the bias term b(i)), a pointer to a
tree cut (i), and a bool (the y-value y(i)). Indi-
rectly storing the entire example set requires only
32 bytes ? 1.65 million = 53 MB plus the treebank
and tree cuts, a total of 400 MB in our implementa-
tion.
We used a sample size of |S | = 100, 000 examples
to build each decision tree, 16.5 times fewer than
the entire example set. The dashed curve in Figure 1
10Bikel (2004) is a ?clean room? reimplementation of the
Collins (1999) model with comparable accuracy.
22
shows the percent of NP example weight lost due
to sampling. As training progresses, fewer examples
are informative to the model. Even though we ignore
94% of examples during feature selection, sampling
loses less than 1% of the example weight after a day
of training.
The NP classifier used in our final parser was
an ensemble containing 2316 trees, which took
five days to build. Overall, there were 96871 de-
cision tree leaves, only 2339 of which were non-
zero. There were an average of 40.4 (7.4 std-
dev) decision tree splits between the root of a
tree and a non-zero leaf, and nearly all non-
zero leaves were conjunctions of atomic fea-
ture negations (e.g. ?(some child item is a verb) ?
?(some child item is a preposition)). The non-zero
leaf confidences were quite small in magnitude
(0.107 mean, 0.069 stddev) but the training exam-
ple margins over the entire ensemble were nonethe-
less quite high: 11.7 mean (2.92 stddev) for correct
inferences, 30.6 mean (11.2 stddev) for incorrect in-
ferences.
Table 2 profiles an NP training iteration, in which
one decision tree is created and added to the
NP ensemble. Feature selection in our algorithm
(Steps 1.5?1.7) takes 1.5+38.2+127.6 = 167.3s, far
faster than in na??ve approaches. If we didn?t do sam-
pling but had 2.8GB to spare, we could eliminate the
extraction step (Step 1.6) and instead cache the en-
tire atomic feature matrix before the loop. However,
tree building (Step 1.7) scales linearly in the number
of examples, and would take 16.5 ?127.6s = 2105.4s
using the entire example set. If we didn?t do sam-
pling and couldn?t cache the atomic feature matrix,
tree building would also require repeatedly perform-
ing extraction. The number of individual feature ex-
tractions needed to build a single decision tree is the
sum over the internal nodes of the number of exam-
ples that percolate down to that node. There are an
average of 40.8 (7.8 stddev) internal nodes in each
tree and most of the examples fall in nearly all of
them. This property is caused by the lopsided trees
induced under `1 regularization. A conservative es-
timate is that each decision tree requires 25 extrac-
tions times the number of examples. So extraction
would add at least 25 ? 16.5 ? 38.2s = 15757.5s on
top of 2105.40s, and hence building each decision
tree would take at least (15757.5+2105.40)/167.3 ?
100 times as long as it does currently.
Our decision tree ensembles contain over two or-
ders of magnitude more compound features than
those in Turian and Melamed (2005). Our overall
training time was roughly equivalent to theirs. This
ratio corroborates the above estimate.
5 Discussion
The NP classifier was trained only on the 1.65 mil-
lion NP examples in the 9753 training sentences with
? 15 words (168.8 examples/sentence). The number
of examples generated is quadratic in the sentence
length, so there are 41.7 million NP examples in all
39832 training sentences of the whole Penn Tree-
bank (1050 examples/sentence), 25 times as many
as we are currently using.
The time complexity of each step in the train-
ing loop (Steps 1.5?11) is linear over the number
of examples used by that step. When we scale up
to the full treebank, feature selection will not re-
quire a sample 25 times larger, so it will no longer
be the bottleneck in training. Instead, each itera-
tion will be dominated by choosing leaf confidences
and then updating the cached example confidences,
which would require 25 ? (31.4s + 6.2s) = 940s per
iteration. These steps are crucial to the current train-
ing algorithm, because it is important to have exam-
ple confidences that are current with respect to the
model. Otherwise, we cannot determine the exam-
ples most poorly classified by the current model, and
will have no basis for choosing an informative sam-
ple.
We might try to save training time by building
many decision trees over a single sample and then
updating the confidences of the entire example set
using all the new trees. But, if this confidence up-
date is done using feature tests, then we have merely
deferred the cost of the confidence update over the
entire example set. The amount of training done on
a particular sample is proportional to the time sub-
sequently spent updating confidences over the entire
example set. To spend less time doing confidence
updates, we must use a training regime that is sub-
linear with respect to the training time. For exam-
ple, Riezler (2004) reports that the `1 regularization
term drives many of the model?s parameters to zero
during conjugate gradient optimization, which are
23
then pruned before subsequent optimization steps to
avoid numerical instability. Instead of building de-
cision tree(s) at each iteration, we could perform n-
best feature selection followed by parallel optimiza-
tion of the objective over the sample.
The main limitation of our work so far is that
we can do training reasonably quickly only on short
sentences, because a sentence with n words gen-
erates O(n2) training inferences in total. Although
generating training examples in advance without a
working parser (Sagae & Lavie, 2005) is much faster
than using inference (Collins & Roark, 2004; Hen-
derson, 2004; Taskar et al, 2004), our training time
can probably be decreased further by choosing a
parsing strategy with a lower branching factor. Like
our work, Ratnaparkhi (1999) and Sagae and Lavie
(2005) generate examples off-line, but their parsing
strategies are essentially shift-reduce so each sen-
tence generates only O(n) training examples.
6 Conclusion
Our work has made advances in both accuracy and
training speed of discriminative parsing. As far as
we know, we present the first discriminative parser
that surpasses a generative baseline on constituent
parsing without using a generative component, and
it does so with minimal linguistic cleverness.
The main bottleneck in our setting was memory.
We could store the examples in memory only using
an indirect representation. The most costly opera-
tion during training was accessing the features of a
particular example from this indirect representation.
We showed how to train a parser effectively under
these conditions. In particular, we used principled
sampling to estimate loss gradients and reduce the
number of feature extractions. This approximation
increased the speed of feature selection 100-fold.
We are exploring methods for scaling training
up to larger example sets. We are also investigat-
ing the relationship between sample size, training
time, classifier complexity, and accuracy. In addi-
tion, we shall make some standard improvements
to our parser. Our parser should infer its own POS
tags. A shift-reduce parsing strategy will generate
fewer examples, and might lead to shorter training
time. Lastly, we plan to give the model linguistically
more sophisticated features. We also hope to apply
the model to other structured learning tasks, such as
syntax-driven SMT.
References
Bikel, D. M. (2004). Intricacies of Collins? parsing model.
Computational Linguistics.
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman,
R., Harrison, P., et al (1991). A procedure for quantitatively
comparing the syntactic coverage of English grammars. In
Speech and Natural Language.
Bod, R. (2003). An efficient implementation of a new DOP
model. In EACL.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In ACL.
Collins, M. (1999). Head-driven statistical models for natural
language parsing. Doctoral dissertation.
Collins, M., & Roark, B. (2004). Incremental parsing with the
perceptron algorithm. In ACL.
Collins, M., Schapire, R. E., & Singer, Y. (2002). Logistic re-
gression, AdaBoost and Bregman distances. Machine Learn-
ing, 48(1-3).
Duffield, N., Lund, C., & Thorup, M. (2005). Prior-
ity sampling estimating arbitrary subset sums. (http:
//arxiv.org/abs/cs.DS/0509026)
Henderson, J. (2004). Discriminative training of a neural net-
work statistical parser. In ACL.
Klein, D., & Manning, C. D. (2001). Parsing and hypergraphs.
In IWPT.
Kudo, T., Suzuki, J., & Isozaki, H. (2005). Boosting-based
parse reranking with subtree features. In ACL.
Ng, A. Y. (2004). Feature selection, `1 vs. `2 regularization, and
rotational invariance. In ICML.
Perkins, S., Lacker, K., & Theiler, J. (2003). Grafting: Fast,
incremental feature selection by gradient descent in function
space. Journal of Machine Learning Research, 3.
Ratnaparkhi, A. (1996). A maximum entropy part-of-speech
tagger. In EMNLP.
Ratnaparkhi, A. (1999). Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-3).
Riezler, S. (2004). Incremental feature selection of `1 regular-
ization for relaxed maximum-entropy modeling. In EMNLP.
Sagae, K., & Lavie, A. (2005). A classifier-based parser with
linear run-time complexity. In IWPT.
Schapire, R. E., & Singer, Y. (1999). Improved boosting using
confidence-rated predictions. Machine Learning, 37(3).
Smith, N. A., & Eisner, J. (2005). Contrastive estimation: Train-
ing log-linear models on unlabeled data. In ACL.
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C.
(2004). Max-margin parsing. In EMNLP.
Taylor, A., Marcus, M., & Santorini, B. (2003). The Penn Tree-
bank: an overview. In A. Abeille? (Ed.), Treebanks: Building
and using parsed corpora (chap. 1).
Turian, J., & Melamed, I. D. (2005). Constituent parsing by
classification. In IWPT.
Turian, J., & Melamed, I. D. (2006). Advances in discriminative
parsing. In ACL.
24
Models of Translational Equivalence 
among Words 
I. Dan  Me lamed*  
West Group 
Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. 
First, most words translate to only one other word. Second, bitext correspondence is typically 
only partial--many words in each text have no clear equivalent in the other text. This article 
presents methods for biasing statistical translation models to reflect hese properties. Evalua- 
tion with respect to independent human judgments has confirmed that translation models biased 
in this fashion are significantly more accurate than a baseline knowledge-free model. This arti- 
cle also shows how a statistical translation model can take advantage of preexisting knowledge 
that might be available about particular language pairs. Even the simplest kinds of language- 
specific knowledge, such as the distinction between content words and function words, are 
shown to reliably boost ranslation model performance on some tasks. Statistical models that 
reflect knowledge about he model domain combine the best of both the rationalist and empiricist 
paradigms. 
1. Introduction 
The idea of a computer system for translating from one language to another is almost 
as old as the idea of computer systems. Warren Weaver wrote about mechanical trans- 
lation as early as 1949. More recently, Brown et al (1988) suggested that it may be 
possible to construct machine translation systems automatically. Instead of codifying 
the human translation process from introspection, Brown and his colleagues proposed 
machine learning techniques to induce models of the process from examples of its in- 
put and output. The proposal generated much excitement, because it held the promise 
of automating a task that forty years of research ave proven very labor-intensive and 
error-prone. Yet very few other researchers have taken up the cause, partly because 
Brown et al's (1988) approach was quite a departure from the paradigm in vogue at 
the time. 
Formally, Brown et al (1988) built statistical models of translational equivalence 
(or translation models 1, for short). In the context of computational linguistics, trans- 
lational equivalence is a relation that holds between two expressions with the same 
meaning, where the two expressions are in different languages. Empirical estimation 
of statistical translation models is typically based on parallel texts or bitexts--pairs 
of texts that are translations of each other. As with all statistical models, the best 
translation models are those whose parameters correspond best with the sources of 
variance in the data. Probabilistic translation models whose parameters reflect univer- 
sal properties of translational equivalence and/or  existing knowledge about particular 
* D1-66F, 610 Opperman Drive, Eagan, MN 55123. E-marl: dan.melamed@westgroup.com 
1 The term translation model, which is standard in the literature, refers to a mathematical re ationship 
between two data sets. In this context, he term implies nothing about he process of translation 
between atural languages, automated orotherwise. 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 2 
languages and language pairs benefit from the best of both the empiricist and ratio- 
nalist traditions. 
This article presents three such models, along with methods for efficiently esti- 
mating their parameters. Each new method is designed to account for an additional 
universal property of translational equivalence in bitexts: 
. 
. 
. 
Most word tokens translate to only one word token. I approximate this 
tendency with a one-to-one assumption. 
Most text segments are not translated word-for-word. I build an explicit 
noise model. 
Different linguistic objects have statistically different behavior in 
translation. I show a way to condition translation models on different 
word classes to help account for the variety. 
Quantitative valuation with respect to independent human judgments has shown 
that each of these three estimation biases significantly improves translation model ac- 
curacy over a baseline knowledge-free model. However, these biases will not produce 
the best possible translation models by themselves. Anyone attempting to build an op- 
timal translation model should infuse it with all available knowledge sources, includ- 
ing syntactic, dictionary, and cognate information. My goal here is only to demonstrate 
the value of some previously unused kinds of information that are always available for 
translation modeling, and to show how these information sources can be integrated 
with others. 
A review of some previously published translation models follows an introduction 
to translation model taxonomy. The core of the article is a presentation of the model 
estimation biases described above. The last section reports the results of experiments 
designed to evaluate these innovations. 
Throughout this article, I shall use CA??/~GT4A/~C letters to denote entire text 
corpora and other sets of sets, CAPITAL letters to denote collections, including se- 
quences and bags, and italics for scalar variables. I shall also distinguish between 
types and tokens by using bold font for the former and plain font for the latter. 
2. Translation Model Decomposition 
There are two kinds of applications of translation models: those where word order 
plays a crucial role and those where it doesn't. Empirically estimated models of trans- 
lational equivalence among word types can play a central role in both kinds of appli- 
cations. 
Applications where word order is not essential include 
? cross-language information retrieval (e.g., McCarley 1999), 
? multilingual document filtering (e.g., Oard 1997), 
? computer-assisted language learning (e.g., Nerbonne t al. 1997), 
? certain machine-assisted translation tools (e.g., Macklovitch 1994; 
Melamed 1996a), 
? concordancing for bilingual exicography (e.g., Catizone, Russell, and 
Warwick 1989; Gale and Church 1991), 
222 
Melamed Models of Translational Equivalence 
? corpus linguistics (e.g., Svartvik 1992), 
? "crummy" machine translation (e.g., Church and Hovy 1992; Resnik 
1997). 
For these applications, empirically estimated models have a number of advantages 
over handcrafted models such as on-line versions of bilingual dictionaries. Two of 
the advantages are the possibility of better coverage and the possibility of frequent 
updates by nonexpert users to keep up with rapidly evolving vocabularies. 
A third advantage is that statistical models can provide more accurate information 
about he relative importance of different translations. Such information is crucial for 
applications such as cross-language information retrieval (CLIR). In the vector space 
approach to CLIR, the query vector Q' is in a different language (a different vector 
space) from the document vectors D. A word-to-word translation model T can map QI 
into a vector Q in the vector space of D. In order for the mapping to be accurate, T must 
be able to encode many levels of relative importance among the possible translations 
of each element of QI. A typical bilingual dictionary says only what the possible 
translations are, which is equivalent to positing a uniform translational distribution. 
The performance of cross-language information retrieval with a uniform T is likely to 
be limited in the same way as the performance of conventional information retrieval 
without erm-frequency information, i.e., where the system knows which terms occur 
in which documents, but not how often (Buckley 1993). 
Applications where word order is crucial include speech transcription for trans- 
lation (Brousseau et al 1995), bootstrapping of OCR systems for new languages (Philip 
Resnik and Tapas Kanungo, personal communication), interactive translation 
(Foster, Isabelle, and Plamondon 1996), and fully automatic high-quality machine 
translation (e.g., A1-Onaizan et al 1999). In such applications, a word-to-word trans- 
lation model can serve as an independent module in a more complex sequence-to- 
sequence translation model. 2The independence of such a module is desirable for two 
reasons, one practical and one philosophical. The practical reason is illustrated in 
this article: Order-independent translation models can be accurately estimated more 
efficiently in isolation. The philosophical reason is that words are an important epis- 
temological category in our naive mental representations of language. We have many 
intuitions (and even some testable theories) about what words are and how they be- 
have. We can bring these intuitions to bear on our translation models without being 
distracted by other facets of language, such as phrase structure. For example, the 
translation models presented in the last two chapters of Melamed (to appear) cap- 
ture the intuitions that words can have multiple senses and that spaces in text do not 
necessarily delimit words. 
The independence of a word-to-word translation module in a sequence-to-sequence 
translation model can be effected by a two-stage decomposition. The first stage is based 
on the observation that every sequence L is just an ordered bag, and that the bag B 
can be modeled independently of its order O. For example, the sequence (abc I consists 
of the bag {c,a, b} and the ordering relation {(b,2), (a, 1), (c,3)}. If we represent each 
sequence L as a pair (B, O), then 
Pr(L) - Pr(B,O) (1) 
-- Pr(B)-Pr(OIB ). (2) 
2 "Sentence-to-sentence" might be a more transparent term than "sequence-to-sequence," but all the 
models that I'm aware of apply equally well to sequences of words that are not sentences. 
223 
Computational Linguistics Volume 26, Number 2 
Now, let L1 and L2 be two sequences and let A be a one-to-one mapping between 
the elements of L1 and the elements of L2. Borrowing a term from the operations 
research literature, I shall refer to such mappings as assignments. 3 Let .4 be the set of 
all possible assignments between L1 and L2. Using assignments, we can decompose 
conditional and joint probabilities over sequences: 
Pr(LIIL2) = ~ Pr(L1,A\[L2) (3) 
AG.4 
Pr(L,,L2) = ~ Pr(L1, A, L2) (4) 
ACA 
where 
Pr(L,,A\]L2) - Pr(B1,01,AIL2) (5) 
= Pr(B1,AIL2) ? Pr(OI\[B1, A, L2) (6) 
Pr(L1,A, L2) ~ Pr(B,, O1, A,  B2, 02) (7) 
= Pr(B1, A, B2). Pr(O1, O2IB1,A, B2) (8) 
Summing bag pair probabilities over all possible assignments, we obtain a bag-to-bag 
t rans la t ion  mode l :  
Pr(B1, B2) = ~ Pr(B,, A, B2) (9) 
AEA 
The second stage of decomposition takes us from bags of words to the words 
that they contain. The following bag pair generation process illustrates how a word- 
to-word translation model can be embedded in a bag-to-bag translation model for 
languages ?1 and ?2: 
. 
2. 
3. 
Generate a bag size /.4 1 is also the assignment size. 
Generate l language-independent concepts C1,..., C1. 
From each concept Ci, 1 < i < I, generate a pair of word sequences (ffi, rTi) 
from ?~ x ?~, according to the distribution trans(G ~), to lexicalize the 
concept in the two languages. 5 Some concepts are not lexicalized in some 
languages, so one of ffi and rTi may be empty. 
A pair of bags containing m and n nonempty word sequences can be generated by a 
process where l is anywhere between 1 and m + n. 
For notational convenience, the elements of the two bags can be labeled so that 
B1 - {u~,...,t~} and B 2 ~ {V~ . . . . .  ~} ,  where some of the 1/'s and "?'s may be 
empty. The elements of an assignment, hen, are pairs of bag element labels: A -- 
{(h,jl) . . . . .  (h, jl)}, where each i ranges over  {IJ 1 . . . . .  11l}, eachj ranges over {v~ . . . . .  x~}, 
3 Assignments are different from Brown, Della Pietra, Della Pietra, and Mercer's (1993) alignments in 
that assignments can range over pairs of arbitrary labels, not necessarily sequence position indexes. 
Also, unlike alignments, assignments must be one-to-one. 
4 The exact nature of the bag size distribution is immaterial for the present purposes. 
5 Since they are put into bags, ffi and r7 i could just as well be bags instead of sequences. I make them 
sequences only to be consistent with more sophisticated models that account for noncompositional 
compounds (e.g. Melamed, to appear, Chapter 8). 
224 
Melamed Models of Translational Equivalence 
each i is distinct, and each j is distinct. The label pairs in a given assignment can be 
generated in any order, so there are I! ways to generate an assignment of size I. 6 It 
follows that the probability of generating a pair of bags (B1, B2) with a particular 
assignment A of size l is 
Pr(B1,A, B2\]I,C, trans) : Pr(1). I! n E Pr(C)trans('fi'vilC)" 
(i,j) ff A CCC 
(lO) 
The above equation holds regardless of how we represent concepts. There are 
many plausible representations, such as pairs of trees from synchronous tree adjoining 
grammars (Abeill6 et al 1990; Shieber 1994; Candito 1998), lexical conceptual struc- 
tures (Dorr 1992) and WordNet synsets (Fellbaum 1998; Vossen 1998). Of course, for a 
representation to be used, a method must exist for estimating its distribution in data. 
A useful representation will reduce the entropy of the trans distribution, which is con- 
ditioned on the concept distribution as shown in Equation 10. This topic is beyond the 
scope of this article, however. I mention it only to show how the models presented 
here may be used as building blocks for models that are more psycholinguistically 
sophisticated. 
To make the translation model estimation methods presented here as general as 
possible, I shall assume a totally uninformative concept representation--the rans dis- 
tribution itself. In other words, I shall assume that each different pair of word sequence 
types is deterministically generated from a different concept, so that trans(.1i,~i\]C) is
zero for all concepts except one. Now, a bag-to-bag translation model can be fully 
specified by the distributions of l and trans. 
Pr(B1,A, B2\]I, trans) = Pr(l). I! H trans(~,~j) 
(i,j) CA 
(11) 
The probability distribution trans (.1, ~) is a word-to-word translation model. Unlike 
the models proposed by Brown et al (1993b), this model is symmetric, because both 
word bags are generated together from a joint probability distribution. Brown and his 
colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the 
other hal l  so they are represented by conditional probability distributions. A sequence- 
to-sequence translation model can be obtained from a word-to-word translation model 
by combining Equation 11 with order information as in Equation 8. 
3. The One-to-One Assumption 
The most general word-to-word translation model trans(.1, ~), where ,i and ?? range 
over sequences in ?1 and ?2, has an infinite number of parameters. This model can 
be constrained in various ways to make it more practical. The models presented in 
this article are based on the one-to-one assumption: Each word is translated to at 
most one other word. In these models, .1 and ?? may consist of at most one word each. 
As before, one of the two sequences (but not both) may be empty. I shall describe 
empty sequences as consisting of a special NULL word, so that each word sequence 
will contain exactly one word and can be treated as a scalar. Henceforth, I shall write u 
and v instead of 11 and ~?. Under the one-to-one assumption, a pair of bags containing m 
6 The number of permutations is smaller when either bag contains two or more identical elements, but 
this detail will not affect he estimation algorithms presented here. 
225 
Computational Linguistics Volume 26, Number 2 
and n nonempty words can be generated by a process where the bag size I is anywhere 
between max(m, n) and m + n. 
The one-to-one assumption is not as restrictive as it may appear: The explanatory 
power of a model based on this assumption may be raised to an arbitrary level by 
extending Western notions of what words are to include words that contain spaces 
(e.g., in English) or several characters (e.g., in Chinese). For example, I have shown 
elsewhere how to estimate word-to-word translation models where a word can be a 
noncompositional compound consisting of several space-delimited tokens (Melamed, 
to appear). For the purposes of this article, however, words are the tokens generated 
by my tokenizers and stemmers for the languages in question. Therefore, the models 
in this article are only a first approximation to the vast complexities of translational 
equivalence between atural languages. They are intended mainly as stepping stones 
towards better models. 
4. Previous Work 
4.1 Models of Co-occurrence 
Most methods for estimating translation models from bitexts tart with the following 
intuition: Words that are translations of each other are more likely to appear in cor- 
responding bitext regions than other pairs of words. Following this intuition, most 
authors begin by counting the number of times that word types in one half of the 
bitext co-occur with word types in the other half. Different co-occurrence counting 
methods tem from different models of co-occurrence. 
A model of co-occurrence is a Boolean predicate, which indicates whether a given 
pair of word tokens co-occur in corresponding regions of the bitext space. Different 
models of co-occurrence are possible, depending on the kind of bitext map that is avail- 
able, the language-specific information that is available, and the assumptions made 
about he nature of translational equivalence. All the translation models reviewed and 
introduced in this article can be based on any of the co-occurrence models described 
by Melamed (1998a). For expository purposes, however, I shall assume a boundary- 
based model of co-occurrence throughout this article. A boundary-based model of 
co-occurrence assumes that both halves of the bitext have been segmented into s seg- 
ments, so that segment Ui in one half of the bitext and segment Vi in the other half 
are mutual translations, 1 < i < s. 
Under the boundary-based model of co-occurrence, there are several ways to com- 
pute co-occurrence counts cooc(u, v) between word types u and v. In the models of 
Brown, Della Pietra, Della Pietra, and Mercer (1993), reviewed in Section 4.3, 
s 
COOC(R, V) = ~ ei(u) .j~(V), (12) 
i=1 
where ei and j5 are the unigram frequencies of u and v, respectively, in each aligned 
text segment i. For most translation models, this method produces uboptimal results, 
however, when ei(u) > 1 and )~(v) > 1. I argue elsewhere (Melamed 1998a) that 
cooc(u, v) = ~ min\[ei(u),j~(v)\] (13) 
i=1 
is preferable, and this is the method used for the models introduced in Section 5. 
226 
Melamed Models of Translational Equivalence 
He 
II 
Figure 1 
nods his head 
I " 
hoche la tete 
nods and hoche often co-occur, as do nods and head. The direct association between ods and 
hoche, and the direct association between ods and head give rise to an indirect association 
between hoche and head. 
4.2 Nonprobabilistic Translation Lexicons 
Many researchers have proposed greedy algorithms for estimating nonprobabilistic 
word-to-word translation models, also known as translation lexicons (e.g., Catizone, 
Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 
1994; Melamed 1995; Wu and Xia 1994). Most of these algorithms can be summarized 
as follows: 
1. Choose a similarity function S between word types in ?1 and word types 
in ?2. 
2. Compute association scores S(u,v) for a set of word type pairs 
(U, V) C (?1 X ?2) that occur in training data. 
3. Sort the word pairs in descending order of their association scores. 
4. Discard all word pairs for which S(u, v) is less than a chosen threshold. 
The remaining word pairs become the entries in the translation lexicon. 
The various proposals differ mainly in their choice of similarity function. Almost all 
the similarity functions in the literature are based on a model of co-occurrence with 
some linguistically motivated filtering (see Fung \[1995\] for a notable xception). 
Given a reasonable similarity function, the greedy algorithm works remarkably 
well, considering how simple it is. However, the association scores in Step 2 are typ- 
ically computed independently of each other. The problem with this independence 
assumption is illustrated in Figure 1. The two word sequences represent correspond- 
ing regions of an English/French bitext. If nods and hoche co-occur much more often 
than expected by chance, then any reasonable similarity metric will deem them likely 
to be mutual translations. Nods and hoche are indeed mutual translations, o their ten- 
dency to co-occur is called a direct association. Now, suppose that nods and head often 
co-occur in English. Then hoche and head will also co-occur more often than expected 
by chance. The dashed arrow between hoche and head in Figure i represents an indirect 
association, since the association between hoche and head arises only by virtue of the 
association between each of them and nods. Models of translational equivalence that 
are ignorant of indirect associations have "a tendency.. ,  tobe confused by collocates" 
(Dagan, Church, and Gale 1993,5). 
Paradoxically, the irregularities (noise) in text and in translation mitigate the prob- 
lem. If noise in the data reduces the strength of a direct association, then the same 
noise will reduce the strengths of any indirect associations that are based on this direct 
227 
Computational Linguistics Volume 26, Number 2 
Table 1 
Variables used to describe translation models. 
(U, V) = the two halves of the bitext 
(U, V) = a pair of aligned text segments in (/d, V) 
e(u) = the unigram frequency of u in U 
f(v) = the unigram frequency of v in V 
cooc(u, v) = the number of times that u and v co-occur 
trans(vlu ) = the probability that a token of u will be translated as a token of v 
association. On the other hand, noise can reduce the strength of an indirect associa- 
tion without affecting any direct associations. Therefore, direct associations are usually 
stronger than indirect associations. If all the entries in a translation lexicon are sorted 
by their association scores, the direct associations will be very dense near the top of 
the list, and sparser towards the bottom. 
Gale and Church (1991) have shown that entries at the very top of the list can 
be over 98% correct. Their algorithm gleaned lexicon entries for about 61% of the 
word tokens in a sample of 800 English sentences. To obtain 98% precision, their 
algorithm selected only entries for which it had high confidence that the association 
score was high. These would be the word pairs that co-occur most frequently. A 
random sample of 800 sentences from the same corpus showed that 61% of the word 
tokens, where the tokens are of the most frequent types, represent 4.5% of all the word 
types. 
A similar strategy was employed by Wu and Xia (1994) and by Fung (1995). 
Fung skimmed off the top 23.8% of the noun-noun entries in her lexicon to achieve a 
precision of 71.6%. Wu and Xia have reported automatic acquisition of 6,517 lexicon 
entries from a 3.3-million-word corpus, with a precision of 86%. The first 3.3 million 
word tokens in an English corpus from a similar genre contained 33,490 different word 
types, suggesting a recall of roughly 19%. Note, however, that Wu and Xia chose to 
weight their precision estimates by the probabilities attached to each entry: 
For example, if the translation set for English word detect has the 
two correct Chinese candidates with 0.533 probability and with 0.277 
probability, and the incorrect ranslation with 0.190 probability, then 
we count this as 0.810 correct ranslations and 0.190 incorrect ransla- 
tions. (Wu and Xia 1994, 211) 
This is a reasonable valuation method, but it is not comparable to methods that 
simply count each lexicon entry as either right or wrong (e.g., Daille, Gaussier, and 
Lang6 1994; Melamed 1996b). A weighted precision estimate pays more attention to 
entries that are more frequent and hence easier to estimate. Therefore, weighted pre- 
cision estimates are generally higher than unweighted ones. 
4.3 Reestimated Sequence-to-Sequence Translation Models 
Most probabilistic translation model reestimation algorithms published to date are 
variations on the theme proposed by Brown et al (1993b). These models involve con- 
ditional probabilities, but they can be compared to symmetric models if the latter are 
normalized by the appropriate marginal distribution. I shall review these models using 
the notation in Table 1. 
228 
Melamed Models of Translational Equivalence 
4.3.1 Models Using Only Co-occurrence Information. Brown and his colleagues em- 
ploy the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) 
to estimate the parameters of their Model 1. On iteration i, the EM algorithm reesti- 
mates the model parameters transi(v\]u) based on their estimates from iteration i -  1. 
In Model 1, the relationship between the new parameter estimates and the old ones is 
transi_l(VlU ) ? e(u) -f(v) transi(vlu) = z ~_, (u,v)e(u,v) ~u,eutransi-l(VlU') 
(14) 
where z is a normalizing factor.  7 
It is instructive to consider the form of Equation 14 when all the translation prob- 
abilities trans(v\[u) for a particular u are initialized to the same constant p, as Brown 
et al (1993b, 273) actually do: 
transl(v\]u) : z E p.e(u) . f(v) (15) 
(u,v)c(u,v) p. \]U\[ 
: z E e(u) . f(v) (16) 
(u,v)e(u,v) pU\] 
The initial translation probability transl(v\]u) is set proportional to the co-occurrence 
count of u and v and inversely proportional to the length of each segment U in which 
u occurs. The intuition behind the numerator is central to most bitext-based translation 
models: The more often two words co-occur, the more likely they are to be mutual 
translations. The intuition behind the denominator is that the co-occurrence count of 
u and v should be discounted to the degree that v also co-occurs with other words in 
the same segment pair. 
Now consider how Equation 16 would behave if all the text segments on each 
side were of the same length, s so that each token of v co-occurs with exactly c words 
(where c is constant): 
transl(vlu ) : z E e(u) . f (v)  (17) 
c (u,v) c (u,v) 
z ~ e(u) . f(v) (18) 
c 
(u,v) e(u,v) 
The normalizing coefficient z is constant over all words. The only difference between 
Equations 16 and 18 is that the former discounts co-occurrences proportionally to the 
segment lengths. When information about segment lengths is not available, the only 
information available to initialize Model 1 is the co-occurrence counts. This property 
makes Model 1 an appropriate baseline for comparison to more sophisticated models 
that use other information sources, both in the work of Brown and his colleagues and 
in the work described here. 
7 This expression is obtained by substituting Brown, Della Pietra, Della Pietra, and Mercer's (1993) 
Equation 17 into their Equation 14. 
8 Or, equivalently, if the notion of segments were dispensed with altogether, as under the distance-based 
model of co-occurrence (Melarned 1998a). 
229 
Computational Linguistics Volume 26, Number 2 
4.3.2 Word Order Correlation Biases. In any bitext, the positions of words relative to 
the true bitext map correlate with the positions of their translations. The correlation is
stronger for language pairs with more similar word order. Brown et al (1988) intro- 
duced the idea that this correlation can be encoded in translation model parameters. 
Dagan, Church, and Gale (1993) expanded on this idea by replacing Brown et al's 
(1988) word alignment parameters, which were based on absolute word positions in 
aligned segments, with a much smaller set of relative offset parameters. The much 
smaller number of parameters allowed Dagan, Church, and Gale's model to be effec- 
tively trained on much smaller bitexts. Vogel, Ney, and Tillmann (1996) have shown 
how some additional assumptions can turn this model into a hidden Markov model, 
enabling even more efficient parameter estimation. 
It cannot be overemphasized that the word order correlation bias is just knowledge 
about the problem domain, which can be used to guide the search for the optimum 
model parameters. Translational equivalence can be empirically modeled for any pair 
of languages, but some models and model biases work better for some language pairs 
than for others. The word order correlation bias is most useful when it has high 
predictive power, i.e., when the distribution of alignments or offsets has low entropy. 
The entropy of this distribution is indeed relatively low for the language pair that both 
Brown and his colleagues and Dagan, Church, and Gale were working with--French 
and English have very similar word order. A word order correlation bias, as well as 
the phrase structure biases in Brown et al's (1993b) Models 4 and 5, would be less 
beneficial with noisier training bitexts or for language pairs with less similar word 
order. Nevertheless, one should use all available information sources, if one wants to 
build the best possible translation model. Section 5.3 suggests a way to add the word 
order correlation bias to the models presented in this article. 
4.4 Reestimated Bag-to-Bag Translation Models 
At about the same time that I developed the models in this article, Hiemstra (1996) 
independently developed his own bag-to-bag model of translational equivalence. His 
model is also based on a one-to-one assumption, but it differs from my models in that 
it allows empty words in only one of the two bags, the one representing the shorter 
sentence. Thus, Hiemstra's model is similar to the first model in Section 5, but it has 
a little less explanatory power. Hiemstra's approach also differs from mine in his use 
of the Iterative Proportional Fitting Procedure (IPFP) (Deming and Stephan 1940) for 
parameter estimation. 
The IPFP is quite sensitive to initial conditions, so Hiemstra investigated a num- 
ber of initialization options. Choosing the most advantageous, Hiemstra has published 
parts of the translational distributions ofcertain words, induced using both his method 
and Brown et al's (1993b) Model 1 from the same training bitext. Subjective compar- 
ison of these examples uggests that Hiemstra's method is more accurate. Hiemstra 
(1998) has also evaluated the recall and precision of his method and of Model 1 on a 
small hand-constructed setof link tokens in a particular bitext. Model 1 fared worse, 
on average. 
5. Parameter Estimation 
This section describes my methods for estimating the parameters ofa symmetric word- 
to-word translation model from a bitext. For most applications, we are interested in 
estimating the probability trans(u,v) of jointly generating the pair of words (u,v). 
Unfortunately, these parameters cannot be directly inferred from a training bitext, 
because we don't know which words in one half of the bitext were generated together 
230 
Melamed Models of Translational Equivalence 
with which words in the other half. The observable features of the bitext are only the 
co-occurrence counts cooc(u, v) (see Section 4.1). 
Methods for estimating translation parameters from co-occurrence ounts typically 
involve l ink counts links(u, v), which represent hypotheses about the number of times 
that u and v were generated together, for each u and v in the bitext. A l ink token is 
an ordered pair of word tokens, one from each half of the bitext. A l ink type is an 
ordered pair of word types. The link counts links(u, v) range over link types. We can 
always estimate trans(u, v) by normalizing link counts so that Y\]~u,v trans(u, v) = 1: 
trans(u, v) = links(u, v) 
Y~-u,,v, links(u', v') (19) 
For estimation purposes, it is convenient to also employ a separate set of non- 
probabilistic parameters score(u, v), which represent the chances that u and v can ever 
be mutual translations, i.e., that there exists some context where tokens u and v are 
generated from the same concept. The relationship between score(u, v) and trans(u, v) 
can be more or less direct, depending on the model and its estimation method. Each 
of the models presented below uses a different score formulation. 
All my methods for estimating the translation parameters trans(u,v) share the 
following general outline: 
. 
. 
. 
. 
. 
Initialize the score parameters to a first approximation, based only on the 
co-occurrence counts. 
Approximate the expected link counts links(u, v), as a function of the 
score parameters and the co-occurrence counts. 
Estimate trans(u, v), by normalizing the link counts as in Equation 19. If 
less than .0001 of the trans(u, v) distribution changed from the previous 
iteration, then stop. 
Reestimate the parameters score(u, v), as a function of the link counts 
and the co-occurrence counts. 
Repeat from Step 2. 
Under certain conditions, a parameter estimation process of this sort is an instance of 
the expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977). As 
explained below, meeting these conditions is computationally too expensive for my 
models. 9 Therefore, I employ some approximations, which lack the EM algorithm's 
convergence guarantee. 
The maximum likelihood approach to estimating the unknown parameters i to 
find the set of parameters ~) that maximize the probability of the training bitext (U, V). 
~) = arg rn~x Pr(U, VIO ) (20) 
The probability of the bitext is a sum over the distribution ~4 of possible assignments: 
Pr(U, Vie) = ~ Pr(U,A, Vie). (21) 
AE.,4 
9 For example, the expectation i  Step 2 would need to be computed exactly, rather than merely 
approximated. 
231 
Computational Linguistics Volume 26, Number 2 
The munber of possible assignments grows exponentially with the size of aligned 
text segments in the bitext. Due to the parameter interdependencies introduced by 
the one-to-one assumption, we are unlikely to find a method for decomposing the 
assignments into parameters that can be estimated independently of each other as in 
Brown et al \[1993b, Equation 26\]). Barring such a decomposition method, the MLE 
approach is infeasible. This is why we must make do with approximations to the EM 
algorithm. 
In this situation, Brown et al (1993b, 293) recommend "evaluating the expectations 
using only a single, probable alignment." The single most probable assignment Ama~ 
is the maximum a posteriori (MAP) assignment: 
Amax = ar~maxPr(U,A, VIO ) (22) -- AE~4 
= ar~maxPr(l) ? l! I I  trans(ui, vj) (23) 
-- AE,,4 (i,j) cA 
= argmaxl?g \[ Pr(1)'l! I I -  AG,4 (i,j)EAtrans(ui'vJ)\] (24) 
= argmax {log\[Pr(l) ? 1!\] + v  AC~4 (i,j) ~EA logtrans(ui, vj)} (25) 
To simplify things further, let us assume that Pr(l) ? I! is constant, so that 
Amax = argmax ~ logtrans(ui, vj). (26) 
AE~4 (i,j) cA 
If we represent the bitext as a bipartite graph and weight the edges by log trans(u, v), 
then the right-hand side of Equation 26 is an instance of the weighted maximum 
matching problem and Ama~ is its solution. For a bipartite graph G = (V1 U V2, E), 
with v = IV1 U V21 and e = IEI, the lowest currently known upper bound on the 
computational complexity of this problem is O(ve + v 2 log v) (Ahuja, Magnati, and 
Orlin 1993, 500). Although this upper bound is polynomial, it is still too expensive 
for typical bitexts. 1? Subsection 5.1.2 describes a greedy approximation to the MAP 
approximation. 
5.1 Method A: The Competitive Linking Algorithm 
5.1.1 Step 1: Initialization. Almost every translation model estimation algorithm ex- 
ploits the well-known correlation between translation probabilities and co-occurrence 
counts. Many algorithms also normalize the co-occurrence ounts cooc(u,v) by the 
marginal frequencies of u and v. However, these quantities account for only the three 
shaded cells in Table 2. The statistical interdependence between two word types can 
be estimated more robustly by considering the whole table. For example, Gale and 
Church (1991, 154) suggest hat "~b 2, a X2-1ike statistic, seems to be a particularly 
good choice because it makes good use of the off-diagonal cells" in the contingency 
table. 
10 At  least  for my cur rent  very  ineff ic ient imp lementat ion .  
232 
Melamed Models of Translational Equivalence 
Table 2 
A co-occurrence ontingency table. 
u -~u Total 
v cooc( u,v) 
~v cooc(u,~v) cooc(-~u,-~v) cooc(.,~v) 
Total cooc(-,u,.) II cooc(.,.) 
In informal experiments described elsewhere (Melamed 1995), I found that the 
G 2 statistic suggested by Dunning (1993) slightly outperforms ?2. Let the cells of the 
contingency table be named as follows: 
Now, 
Ilul ul 
v a b 
~v c d 
B(a\[a + b, pl)B(c\[c + d, p2) (27) 
G2(u,v) = -2log B(al a + b,p)B(c\[c + d,p) 
where B(kln, p) = (nk)pk(1--p)n--k are binomial probabilities. The statistic uses maximum 
likelihood estimates for the probability parameters: Pl = ~'a p2 = 74-d'c P -  a+b+c+a'a+c 
G 2 is easy to compute because the binomial coefficients in the numerator and in the 
denominator cancel each other out. All my methods initialize the parameters score(u, v) 
to G2(u,v), except hat any pairing with NULL is initialized to an infinitesimal value. 
I have also found it useful to smooth the co-occurrence ounts, e.g., using the Simple 
Good-Turing smoothing method (Gale and Sampson 1995), before computing G2. 
5.1.2 Step 2: Estimation of Link Counts. To further reduce the complexity of esti- 
mating link counts, I employ the competitive linking algorithm, which is a greedy 
approximation to the MAP approximation: 
1. Sort all the score(u, v) from highest o lowest. 
2. For each score(u, v), in order: 
(a) 
(b) 
If u (resp., v) is NULL, consider all tokens of v (resp., u) in the 
bitext linked to NULL. Otherwise, link all co-occurring token 
pairs (u, v) in the bitext. 
The one-to-one assumption implies that linked words cannot be 
linked again. Therefore, remove all linked word tokens from 
their respective halves of the bitext. 
The competitive linking algorithm can be viewed as a heuristic search for the most 
likely assignment in the space of all possible assignments. The heuristic is that the 
most likely assignments contain links that are individually the most likely. The search 
proceeds by a process of elimination. In the first search iteration, all the assignments 
that do not contain the most likely link are discarded. In the second iteration, all 
the assignments hat do not contain the second most likely link are discarded, and 
233 
Computational Linguistics Volume 26, Number 2 
so on until only one assignment remains, u The algorithm greedily selects the most 
likely links first, and then selects less likely links only if they don't conflict with 
previous elections. The probability of a link being rejected increases with the number 
of links that are selected before it, and thus decreases with the link's score. In this 
problem domain, the competitive linking algorithm usually finds one of the most 
likely assignments, asI will show in Section 6. Under an appropriate hashing scheme, 
the expected running time of the competitive linking algorithm is linear in the size of 
the input bitext. 
The competitive linking algorithm and its one-to-one assumption are potent weap- 
ons against he ever-present sparse data problem. They enable accurate stimation 
of translational distributions even for words that occur only once, as long as the 
surrounding words are more frequent. In most translation models, link scores are 
correlated with co-occurrence frequency. So, links between tokens u and v for which 
score(u, v) is highest are the ones for which there is the most evidence, and thus also the 
ones that are easiest to predict correctly. Winner-take-all link assignment methods, uch 
as the competitive linking algorithm, can prevent links based on indirect associations 
(see Section 4.2), thereby leveraging their accuracy on the more confident links to 
raise the accuracy of the less confident links. For example, suppose that ul and u2 
co-occur with vl and v2 in the training data, and the model estimates score(u1, vl) -- 
.05, score (ul, v2) = .02, and score(u2, v2) = .01. According to the one-to-one assumption, 
(Ul, v2) is an indirect association and the correct ranslation of v2 is u2. To the extent hat 
the one-to-one assumption is valid, it reduces the probability of spurious links for the 
rarer words. The more incorrect candidate translations can be eliminated for a given 
rare word, the more likely the correct ranslation is to be found. So, the probability of 
a correct match for a rare word is proportional to the fraction of words around it that 
can be linked with higher confidence. This fraction is largely determined by two bitext 
properties: the distribution of word frequencies, and the distribution of co-occurrence 
counts. Melamed (to appear) explores these properties in greater depth. 
5.1.3 Step 3: Reestimation of the Model Parameters. Method A reestimates the score 
parameters a the logarithm of the trans parameters. The competitive linking algorithm 
only cares about he relative magnitudes ofthe various core(u, v). However, Equation 26 
is a sum rather than a product, so I scale the trans parameters logarithmically, to be 
consistent with its probabilistic interpretation: 
scoreA(u, v) = log trans(u, v) (28) 
5.2 Method B: Improved Estimation Using an Explicit Noise Model 
Yarowsky (1993, 271) has shown that "for several definitions of sense and collocation, 
an ambiguous word has only one sense in a given collocation with a probability of 
90-99%." In other words, a single contextual c ue can be a highly reliable indicator of 
a word's sense. One of the definitions of "sense" studied by Yarowsky was a word 
token's translation i the other half of a bitext. For example, the English word sentence 
may be considered to have two senses, corresponding to its French translations peine 
(judicial sentence) and phrase (grammatical sentence). If a token of sentence occurs in 
the vicinity of a word like jury or prison, then it is far more likely to be translated 
as peine than as phrase. "In the vicinity of" is one kind of collocation. Co-occurrence 
11 The competitive linking algorithm can be generalized to stop searching before the number of possible 
assignments i  reduced to one, at which point the link counts can be computed as probabilistically 
weighted averages over the remaining assignments. I use this method to resolve ties. 
234 
Melamed Models of Translational Equivalence 
700 I I J P I I I I I 
600 
500 GI. 
---~ 400 
O 
~300 
E 
C 
200 
100 
cooc(u ,v )  = 8 
cooc(u,v) = 9 / 
cooc(u,v) = 12 / 
cooc(u,v) = 16 
P 
I I I I I I I I I 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
links(u,v) / cooc(u, v) 
Figure 2 
The ratio links(u, v)/cooc(u, v), for several values of cooc(u, v). 
in bitext space is another kind of collocation. If each word's translation is treated as 
a sense tag (Resnik and Yarowsky 1997), then "translational" collocations have the 
unique property that the collocate and the word sense are one and the same! 
Method B exploits this property under the hypothesis that "one sense per collo- 
cation" holds for translational collocations. This hypothesis implies that if u and v 
are possible mutual translations, and a token u co-occurs with a token v in the bitext, 
then with very high probability the pair (u, v) was generated from the same concept 
and should be linked. To test this hypothesis, I ran one iteration of Method A on 
300,000 aligned sentence pairs from the Canadian Hansards bitext. I then plotted the 
links(u,v) ratio ~ for several values of cooc(u, v) in Figure 2. The curves show that the ratio 
links(u,v) cooc(u,v) tends to be either very high or very low. This bimodality is not an artifact 
of the competitive linking process, because in the first iteration, linking decisions are 
based only on the initial similarity metric. 
Information about how often words co-occur without being linked can be used to 
links(u,v) bias the estimation of translation model parameters. The smaller the ratio cooc(u,v), the 
more likely it is that u and v are not mutual translations, and that links posited between 
tokens of u and v are noise. The bias can be implemented via auxiliary parameters 
that model the curve illustrated in Figure 2. The competitive linking algorithm creates 
all the links of a given type independently of each other. 12 So, the distribution of 
the number links(u, v) of links connecting word types u and v can be modeled by a 
binomial distribution with parameters cooc(u, v) and p(u, v). p(u, v) is the probability 
12 Except for the case when mult iple tokens of the same word type occur near each other, which I hereby 
sweep under  the carpet. 
235 
Computational Linguistics Volume 26, Number 2 
Table 3 
Variables used to describe Method B. 
links (u, v) 
B(kIn, p) 
.~+ 
T 
K 
N 
= the number of times that u and v are hypothesized to
co-occur as mutual translations 
= probability of k being generated from a binomial distribution 
with parameters n and p 
= probability of a link given mutual translations 
= probability of a link given not mutual translations 
= probability of a link 
= probability of mutual translations 
= total number of links in the bitext 
= total number of co-occurrences in the bitext 
that u and v will be linked when they co-occur. There is never enough data to robustly 
estimate ach p parameter  separately. Instead, I shall model  all the p's with just two 
parameters. For u and v that are mutual  translations, p(u, v) will average to a relatively 
high probability, which I will call ~+. for u and v that are not mutual  translations, 
p(u, v) will average to a relatively low probability, which I will call ),-. ~+ and ,k- 
links(u,v) correspond to the two peaks of the distribution cooc(u,v), which is il lustrated in Figure 2. 
The two parameters can also be interpreted as the rates of true and false positives. If 
the translation in the bitext is consistent and the translation model  is accurate, then 
~+ will be close to one and ,~- will be close to zero. 
To find the most likely values of the auxil iary parameters ,k+ and )~-, I adopt the 
standard method of max imum likelihood estimation, and find the values that maxi- 
mize the probabil ity of the link frequency distributions, under the usual independence 
assumptions: 
Pr(linkslm?del) = H Pr(links(u, v)Icooc(u, v), ~+, k-) (29) 
tlIV 
Table 3 summarizes the variables involved in this auxil iary estimation process. 
The factors on the right-hand side of Equation 29 can be written explicitly with 
the help of a mixture coefficient. Let ~- be the probabil ity that an arbitrary co-occurring 
pair of word types are mutual  translations. Let B(kln, p) denote the probabil ity that k 
links are observed out of n co-occurrences, where k has a binomial distribution with 
parameters n and p. Then the probabil ity that word types u and v will be linked 
links(u, v) times out of cooc(u, v) co-occurrences is a mixture of two binomials: 
Pr(links(u, v)Icooc(u, v), ,k +, )~-) = TB(links(u, v)Icooc(u, v), )~+) 
+ (1 - ~-)B(links(u,v)lcooc(u,v ),A-). (30) 
One more variable allows us to express -r in terms of A + and ~-:  Let )~ be the 
probabil ity that an arbitrary co-occuring pair of word tokens will be linked, regardless 
of whether they are mutual  translations. Since ~- is constant over all word types, it 
also represents the probabil ity that an arbitrary co-occurring pair of word tokens are 
mutual  translations. Therefore, 
= ~-~+ + (1 - T))~-. (31) 
), can also be estimated empirically. Let K be the total number  of links in the bitext 
236 
Melamed Models of Translational Equivalence 
._ 
.~ -1.2 - 
E 
._= 
-1.4 
~D "G 
? 
E -1.6 
c~ 
-1.8 0 e% 
oo JGoo ooo2 _o 
. _  
0.75 0.8 0.85 0.9 0.95 0.01 0.008 ~-- 
Figure 3 
Pr(linkslmodel), asgiven in Equation 29, has only one global maximum in the region of 
interest, where 1 > ),+ > ,~ > ,~- > 0. 
and let N be the total number of word token pair co-occurrences: 
K = Z links(u, v), (32) 
u,v  
N = ~ cooc(u, v). (33) 
U,V 
By definition, 
A = K/N. (34) 
Equating the right-hand sides of Equations 31 and 34 and rearranging the terms, we 
get: 
K/N-  A- (35) 
Since r is now a function of )~+ and )~-, only the latter two variables represent degrees 
of freedom in the model. 
In the preceding equations, either u or v can be NULL. However, the number 
of times that a word co-occurs with NULL is not an observable feature of bitexts. 
To make sense of co-occurrences with NULL ,  we can view co-occurrences a potential 
links and cooc(u, v) as the maximum number of times that tokens of u and v might 
be linked. From this point of view, cooc(u, NULL) should be set to the unigram fre- 
quency of u, since each token of u represents one potential link to NULL. Similarly for 
cooc( NULL, V). These co-occurrence counts should be summed together with all the 
others in Equation 33. 
The probability function expressed by Equations 29 and 30 may have many local 
maxima. In practice, these local maxima are like pebbles on a mountain, invisible at 
low resolution. I computed Equation 29 over various combinations of A + and A- after 
one iteration of Method A over 300,000 aligned sentence pairs from the Canadian 
Hansard bitext. Figure 3 illustrates that the region of interest in the parameter space, 
where 1 > A + > )~ > )~- > 0, has only one dominant global maximum. This global 
maximum can be found by standard hill-climbing methods, as long as the step size is 
large enough to avoid getting stuck on the pebbles. 
237 
Computational Linguistics Volume 26, Number 2 
Given estimates for A + and A-, we can compute B(links(u,v)\[cooc(u,v), A +) and 
B(links(u, v)\[cooc(u, v), A-) for each occurring combination of links and cooc values. 
These are the probabilities that links (u, v) links were generated out of cooc(u, v) possible 
links by a process that generates correct links and by a process that generates incorrect 
links, respectively. The ratio of these probabilities i the likelihood ratio in favor of the 
types u and v being possible mutual translations, for all u and v: 
B(links(u, v)\[cooc(u, v), A +) 
scoreB(u, v) = log B(links(u, v)Icooc(u, v), A-)" (36) 
Method B differs from Method A only in its redefinition of the score function in 
Equation 36. The auxiliary parameters A + and A- and the noise model that they 
represent can be employed the same way in translation models that are not based on 
the one-to-one assumption. 
5.3 Method C: Improved Estimation Using Preexisting Word Classes 
In Method B, the estimation of the auxiliary parameters A + and A- depends only on 
the overall distribution of co-occurrence ounts and link frequencies. All word pairs 
that co-occur the same number of times and are linked the same number of times are 
assigned the same score. More accurate models can be induced by taking into account 
various features of the linked tokens. For example, frequent words are translated less 
consistently than rare words (Catizone, Russell, and Warwick 1989). To account for 
these differences, we can estimate separate values of A + and A- for different ranges of 
cooc(u, v). Similarly, the auxiliary parameters can be conditioned on the linked parts 
of speech. A kind of word order correlation bias can be effected by conditioning the 
auxiliary parameters on the relative positions of linked word tokens in their respective 
texts. Just as easily, we can model ink types that coincide with entries in an on-line 
bilingual dictionary separately from those that do not (cf. Brown et al 1993). When 
the auxiliary parameters are conditioned on different link classes, their optimization 
is carried out separately for each class: 
B (links (u, v)\[cooc(u, v), A +) 
scorec(u, vlZ = class(u, v)) = log B(links(u, v)\[cooc(u, v), A z)" (37) 
Section 6.1.1 describes the link classes used in the experiments below. 
6. Evaluation 
6.1 Evaluation at the Token Level 
This section compares translation model estimation methods A, B, and C to each other 
and to Brown et al's (1993b) Model 1. To reiterate, Model 1 is based on co-occurrence 
information only; Method A is based on the one-to-one assumption; Method B adds the 
"one sense per collocation" hypothesis to Method A; Method C conditions the auxiliary 
parameters of Method B on various word classes. Whereas Methods A and B and 
Model 1 were fully specified in Section 4.3.1 and Section 5, the latter section described 
a variety of features on which Method C might classify links. For the purposes of 
the experiments described in this article, Method C employed the simple classification 
in Table 4 for both languages in the bitext. All classification was performed by table 
lookup; no context-aware part-of-speech tagger was used. In particular, words that 
were ambiguous between open classes and closed classes were always deemed to be in 
the closed class. The only language-specific knowledge involved in this classification 
238 
Melamed Models of Translational Equivalence 
Table 4 
Word classes used by Method C for the experiments described in this article. 
Link classes were constructed by taking the cross-product of the word classes. 
Class Code Description 
EOS 
EOP 
SCM 
SYM 
NU 
C 
F 
End-Of-Sentence punctuation 
End-Of-Phrase punctuation, such as commas and colons 
Subordinate Clause Markers, such as " and ( 
Symbols, such as ~ and * 
the NULL word, in a class by itself 
Content words: nouns, adjectives, adverbs, non-auxiliary verbs 
all other words, i.e., function words 
method is the list of function words in class F. Certainly, more sophisticated word 
classification methods could produce better models, but even the simple classification 
in Table 4 should suffice to demonstrate the method's potential. 
6.1.1 Experiment 1. Until now, translation models have been evaluated either sub- 
jectively (e.g. White and O'Connell 1993) or using relative metrics, such as perplex- 
ity with respect o other models (Brown et al 1993b). Objective and more accurate 
tests can be carried out using a "gold standard." I hired bilingual annotators to link 
roughly 16,000 corresponding words between on-line versions of the Bible in French 
and English. This bitext was selected to facilitate widespread use and standardiza- 
tion (see Melamed \[1998c\] for details). The entire Bible bitext comprised 29,614 verse 
pairs, of which 250 verse pairs were hand-linked using a specially developed anno- 
tation tool. The annotation style guide (Melamed 1998b) was based on the intuitions 
of the annotators, o it was not biased towards any particular translation model. The 
annotation was replicated five times by seven different annotators. 
Each of the four methods was used to estimate a word-to-word translation model 
from the 29,614 verse pairs in the Bible bitext. All methods were deemed to have 
converged when less than .0001 of the translational probability distribution changed 
from one iteration to the next. The links assigned by each of methods A, B, and C in the 
last iteration were normalized into joint probability distributions using Equation 19. I 
shall refer to these joint distributions as Model A, Model B, and Model C, respectively. 
Each of the joint probability distributions was further normalized into two conditional 
probability distributions, one in each direction. Since Model 1 is inherently directional, 
its conditional probability distributions were estimated separately in each direction, 
instead of being derived from a joint distribution. 
The four models' predictions were compared to the gold standard annotations. 
Each model guessed one translation (either stochastically or deterministically, depend- 
ing on the task) for each word on one side of the gold standard bitext. Therefore, 
precision = recall here, and I shall refer to the results simply as "percent correct." The 
accuracy of each model was averaged over the two directions of translation: English to 
French and French to English. The five-fold replication of annotations in the test data 
enabled computation of the statistical significance of the differences in model accuracy. 
The statistical significance of all results in this section was measured at the c~ -- .05 
level, using the Wilcoxon signed ranks test. Although the models were evaluated on 
part of the same bitext on which they were trained, the evaluations were with respect 
to the translational equivalence relation hidden in this bitext, not with respect o any 
of the bitext's visible features. Such testing on training data is standard practice for 
239 
Computational Linguistics Volume 26, Number 2 
unsupervised learning algorithms, where the objective is to compare several methods. 
Of course, performance would degrade on previously unseen data. 
In addition to the different translation models, there were two other independent 
variables in the experiment: method of translation and whether function words were 
included. Some applications, uch as query translation for CLIR, don't care about func- 
tion words. To get a sense of the relative ffectiveness of the different translation model 
estimation methods when function words are taken out of the equation, I removed 
from the gold standard all link tokens where one or both of the linked words were 
closed-class words. Then, I removed all closed-class words (including nonalphabetic 
symbols) from the models and renormalized the conditional probabilities. 
The method of translation was either single-best or whole distribution. Single- 
best translation is the kind that somebody might use to get the gist of a foreign- 
language document. The input to the task was one side of the gold standard bitext. 
The output was the model's ingle best guess about the translation of each word in 
the input, together with the input word. In other words, each model produced link 
tokens consisting of input words and their translations. For some applications, it is 
insufficient to guess only the single most likely translation of each word in the input. 
The model is expected to output the whole distribution of possible translations for 
each input word. This distribution is then combined with other distributions that are 
relevant to the application. For example, for cross-language information retrieval, the 
translational distribution can be combined with the distribution of term frequencies. 
For statistical machine translation, the translational distribution can be decoded with 
a source language model (Brown et al 1988; A1-Onaizan et al 1999). To predict how 
the different models might perform in such applications, the whole distribution task 
was to generate a whole set of links from each input word, weighted according to 
the probability assigned by the model to each of the input word's translations. Each 
model was tested on this task with and without function words. 
The mean results are plotted in Figures 4 and 5 with 95% confidence intervals. 
All four graphs in these figures are on the same scale to facilitate comparison. On 
both tasks involving the entire vocabulary, each of the biases presented in this article 
improves the efficiency of modeling the available training data. When closed-class 
words were ignored, Model 1 performed better than Method A, because open-class 
words are more likely to violate the one-to-one assumption. However, the explicit noise 
model in Methods B and C boosted their scores ignificantly higher than Model 1 and 
Method A. Method B was better than Method C at choosing the single best open-class 
links, and the situation was reversed for the whole distribution of open-class links. 
However, the differences in performance between these two methods were tiny on 
the open-class tasks, because they left only two classes for Method C to distinguish: 
content words and NULLS. Most of the scores on the whole distribution task were lower 
than their counterparts on the single-best translation task, because it is more difficult 
for any statistical method to correctly model the less common translations. The "best" 
translations are usually the most common. 
6.1.2 Experiment 2. To study how the benefits of the various biases vary with training 
corpus size, I evaluated Models A, B, C, and 1 on the whole distribution translation 
task, after training them on three different-size subsets of the Bible bitext. The first 
subset consisted of only the 250 verse pairs in the gold standard. The second subset 
included these 250 plus another andom sample of 2,250 for a total of 2,500, an order 
of magnitude larger than the first subset. The third subset contained all 29,614 verse 
pairs in the Bible bitext, roughly an order of magnitude larger than the second subset. 
All models were compared to the five gold standard annotations, and the scores were 
240 
Melamed Models of Translational Equivalence 
(a) 
0.5 
O 
o 
?- 
o 
(1) 
Q. 
0.45 
0.4 
0.35 
0.3 
0.25 
0.2 
0.15 
................................................. t .. ................................................... 
} I t I 
Model 1 Model A Model B Model C 
(b) 
0.5 
0.45 
0.4 
'-" 0.35 O ?..) 
E 
? 0.3 
(3. 
0.25 
0.2 
i 
i t 
0.15 ' ' ' ' 
Model 1 Model A Model B Model C 
Figure 4 
Comparison of model performance on single-best translation task. (a) All links; (b) open-class 
links only. 
241 
Computational Linguistics Volume 26, Number 2 
(a) 
0.5 
(b) 
0.45 
0.4 
"6 
'- 0.35 O 
o 
E 
~ 0.3 
0.25 
0.2 
0.15 
0.5 
.......................................... i ................... 
t 
I I I I 
Model 1 Model A Model B Model C 
0.45 
0.4 
*5 
"- 0.35 O 
o 
E 
0.3 o 
0.25 
0.2 
0.15 ' ' ' ' 
Model 1 Model A Model B Model C 
Figure 5 
Comparison of model performance on whole distribution task. (a) All links; (b) open-class 
links only. 
242 
Melamed Models of Translational Equivalence 
o 
O 
o 
c- 
o 
o) 
f3.. 
0.4 
0.35 
0.3 
0.25 
0.2 
0 .15 
0.1 
0.05 
0 
................ / / / /  
Mode l  C , 
Mode l  B . . . . . . . . .  
Mode l  A ..... ? ..... 
? Mode l  1 ......... ~ ......... 
250 2500 
number  of t ra in ing verse  pairs  (on log scale)  
Figure 6 
Effects of training set size on model accuracy on the whole distribution task. 
29614 
averaged over the two directions of translation, as before. Again, because the total 
probability assigned to all translations for each source word was one, precision = 
recall = percent correct on this task. The mean scores over the five gold standard 
annotations are graphed in Figure 6, where the right edge of the figure corresponds to
the means of Figure 5(a). The figure supports the hypothesis n Melamed (to appear, 
Chapter 7) that the biases presented in this article are even more valuable when the 
training data are more sparse. The one-to-one assumption is useful, even though it 
forces us to use a greedy approximation to maximum likelihood. In relative terms, 
the advantage of the one-to-one assumption is much more pronounced on smaller 
training sets. For example, Model A is 102% more accurate than Model I when trained 
on only 250 verse pairs. The explicit noise model buys a considerable gain in accuracy 
across all sizes of training data, as do the link classes of Model C. In concert, when 
trained and tested only on the gold standard test set, the three biases outperformed 
Model 1 by up to 125%. This difference is even more significant given the absolute 
performance ceiling of 82% established by the interannotator agreement rates on the 
gold standard. 
6.2 Evaluation at the Type Level 
An important application of statistical translation models is to help lexicographers 
compile bilingual dictionaries. Dictionaries are written to answer the question, "What 
are the possible translations of X?" This is a question about link types, rather than 
about link tokens. 
Evaluation by link type is a thorny issue. Human judges often disagree about the 
degree to which context should play a role in judgments of translational equivalence. 
For example, the Harper-Collins French Dictionary (Cousin et al 1990) gives the following 
French translations for English appoint: nommer, engager, fixer, d~signer. Likewise, most 
243 
Computational Linguistics Volume 26, Number 2 
100000 
10000 
Q) 
o 1000 
C~ 
o 
C o 100 
o 
o 
~ 10 
0 
~ 3 / 3  
Figure 7 
2/2 
1/1 
.1 I I 
0 30000 60000 
entry number 
C_ 
90000 
Distribution of link type scores. The long plateaus correspond to the most common 
li,k~(u,v). 1/1,2/2, and 3/3. combinations of cooc ) ? 
lay judges would not consider instituer a correct French translation of appoint. In actual 
translations, however, when the object of the verb is commission, task force, panel, etc., 
English appoint is usually translated into French as instituer. To account for this kind of 
context-dependent translational equivalence, link types must be evaluated with respect 
to the bitext whence they were induced. 
I performed a post hoc evaluation of the link types produced by an earlier version 
of Method B (Melamed 1996b). The bitext used for this evaluation was the same aligned 
Hansards bitext used by Gale and Church (1991), except hat I used only 300,000 
aligned segment pairs to save time. The bitext was automatically pretokenized to 
delimit punctuation, English possessive pronouns, and French elisions. Morphological 
variants in both halves of the bitext were stemmed to a canonical form. 
The link types assigned by the converged model were sorted by the scores in 
Equation 36. Figure 7 shows the distribution of these scores on a log scale. The log 
scale helps to illustrate the plateaus in the curve. The longest plateau represents he 
set of word pairs that were linked once out of one co-occurrence (1/1) in the bitext. 
All these word pairs were equally likely to be correct. The second-longest plateau 
resulted from word pairs that were linked twice out of two co-occurrences (2/2) and 
the third longest plateau is from word pairs that were linked three times out of three 
co-occurrences (3/3). As usual, the entries with higher scores were more likely to be 
correct. By discarding entries with lower scores, coverage could be traded for accuracy. 
This trade-off was measured at three points, representing cutoffs at the end of each of 
the three longest plateaus. 
The traditional method of measuring coverage requires knowledge of the correct 
link types, which is impossible to determine without a gold standard. An approximate 
coverage measure can be based on the number of different words in the corpus. For 
244 
Melamed Models of Translational Equivalence 
Table 5 
Lexicon coverage at three different minimum score thresholds. The bitext 
contained 41,028 different English words and 36,314 different French 
words, for a total of 77,342. 
Total English French 
Cutoff Minimum Lexicon Words Words 
Plateau Score Entries Represented % Represented % 
3/3 28 32,274 14,299 35 13,409 37 
2/2 18 43,075 18,533 45 1Z133 47 
1/1 9 88,633 36,371 89 33,017 91 
lexicons extracted from corpora, perfect coverage implies at least one entry containing 
each word in the corpus. One-sided variants, which consider only source words, have 
also been used (Gale and Church 1991). Table 5 shows both the marginal (one-sided) 
and the combined coverage at each of the three cutoff points. It also shows the absolute 
number of (non-NULL) entries in each of the three lexicons. Of course, the size of 
automatically induced lexicons depends on the size of the training bitext. Table 5 
shows that, given a sufficiently large bitext, the method can automatically construct 
translation lexicons with as many entries as published bilingual dictionaries. 
The next task was to measure accuracy. It would have taken too long to evaluate 
every lexicon entry manually. Instead, I took five random samples (with replacement) 
of 100 entries each from each of the three lexicons. Each of the samples was first com- 
pared to a translation lexicon extracted from a machine-readable ilingual dictionary 
(Cousin et al 1991). All the entries in the sample that appeared in the dictionary were 
assumed to be correct. I checked the remaining entries in all the samples by hand. To 
account for context-dependent translational equivalence, I evaluated the accuracy of 
the translation lexicons in the context of the bitext whence they were extracted, using 
a simple bilingual concordancer. A lexicon entry (u,v) was considered correct if u and 
v ever appeared as direct translations of each other in an aligned segment pair. That 
is, a link type was considered correct if any of its tokens were correct. 
Direct translations come in different flavors. Most entries that I checked by hand 
were of the plain vanilla variety that you might find in a bilingual dictionary (entry 
type V). However, a significant munber of words translated into a different part of 
speech (entry type P). For instance, in the entry (protection, prot6g6), the English word 
is a noun but the French word is an adjective. This entry appeared because to have 
protection is often translated as ~tre prot~g~ ('to be protected') in the bitext. The entry 
will never occur in a bilingual dictionary, but users of translation lexicons, be they 
human or machine, will want to know that translations often happen this way. 
The evaluation of translation models at the word type level is complicated by the 
possibility of phrasal translations, such as imm~diatement ~-~ right away. All the methods 
being evaluated here produce models of translational equivalence between individual 
words only. How can we decide whether asingle-word translation "matches" a phrasal 
translation? The answer lies in the observation that corpus-based lexicography usually 
involves alexicographer. Bilingual lexicographers can work with bilingual concordanc- 
ing software that can point them to instances of any link type induced from a bitext 
and display these instances sorted by their contexts (e.g. Simard, Foster, and Perrault 
1993). Given an incomplete link type, the lexicographer can usually reconstruct the 
complete link type from the contexts in the concordance. For example, if the model 
proposes an equivalence between immddiatement and right, a bilingual concordance 
245 
Computational Linguistics Volume 26, Number 2 
Table 6 
Distribution of different ypes of correct lexicon entries at varying levels of 
coverage (mean + standard eviation). 
Cutoff Coverage % Type V % Type P % Type I Total % Accuracy 
3/3 36% 89 4- 2.2 3.4 :E 0.5 7.6 + 3.2 99,2 4- 0.8 
2/2 46% 81 4- 3.0 8.0 ::E 2.1 9.8 + 1.8 99.0 4- 1.4 
1/1 90% 82 + 2.5 4.4 + 0.5 6.0 + 1.9 92.8 + 1.1 
can show the lexicographer that the model  was really trying to capture the equiva- 
lence between imm#diatement and right away or between imm#diatement and right now. 
I counted incomplete ntries in a third category (entry type I). Whether links in this 
category should be considered correct depends on the application. 
Table 6 shows the distribution of correct lexicon entries among the types V, P and I. 
Figure 8 graphs the accuracy of the method against coverage, with 95% confidence 
intervals. The upper  curve represents accuracy when incomplete links are considered 
correct, and the lower when they are considered incorrect. On the former metric, the 
method can generate translation lexicons with accuracy and coverage both exceeding 
90%, as well as dictionary-size translation lexicons that are over 99% correct. 
7. Conclusion 
There are many ways to model  translational equivalence and many ways to estimate 
translation models. "The mathematics of statistical machine translation" proposed by 
Brown et al (1993b) are just one kind of mathematics for one kind of statistical trans- 
100 
98 
96 
>, 
o 94 
o 92 
90 
88 
86 
84 
(99.2%) 0%) 
rrlct 
~ ( 9 2 . 8 % )  
(91.6%) t"-.. 
incomplete : in-correct ........... t(86.8%) 
- -  I I I 
36 46 90 
% coverage 
Figure 8 
Translation lexicon accuracy with 95% confidence intervals at varying levels of coverage. 
246 
Melamed Models of Translational Equivalence 
lation. In this article, I have proposed and evaluated new kinds of translation model 
biases, alternative parameter estimation strategies, and techniques for exploiting pre- 
existing knowledge that may be available about particular languages and language 
pairs. On a variety of evaluation metrics, each infusion of knowledge about the prob- 
lem domain resulted in better translation models. 
Each innovation presented here opens the way for more research. Model biases can 
be mixed and matched with each other, with previously published biases like the word 
order correlation bias, and with other biases yet to be invented. The competitive linking 
algorithm can be generalized in various ways. New kinds of preexisting knowledge 
can be exploited to improve accuracy for particular language pairs or even just for 
particular bitexts. It is difficult to say where the greatest advances will come from. Yet, 
one thing is clear from our current vantage point: Research on empirical methods for 
modeling translational equivalence has not run out of steam, as some have claimed, 
but has only just begun. 
Acknowledgments 
Much of this research was performed at the 
Department of Computer and Information 
Science at the University of Pennsylvania, 
where it was supported by an equipment 
grant from Sun MicroSystems Laboratories 
and by ARPA Contract 
#N66001-94C-6043. Many thanks to my 
former colleagues at UPenn and to the 
anonymous reviewers for their insightful 
suggestions for improvement. 
References 
AbeillG Anne, Yves Schabes, and Aravind 
K. Joshi. 1990. Using lexicalized tree 
adjoining rammars for machine 
translation. In Proceedings ofthe 13th 
International Conference on Computational 
Linguistics. Helsinki, Finland. 
Ahuja, Ravindra K., Thomas L. Magnati, 
and James B. Orlin. 1993. Network Flows: 
Theory, Algorithms, and Applications. 
Prentice Hall, Englewood Cliffs, NJ. 
A1-Onaizan, Yaser, Jan Curin, Michael Jahr, 
Kevin Knight, John Lafferty, I. Dan 
Melamed, Franz J. Och, David Purdy, 
Noah A. Smith, and David Yarowsky. 
1999. Statistical machine translation. CLSP 
Technical Report. Baltimore, MD. 
Available at www.clsp.jhu.edu/ws99/ 
projects/mt/final_report/mt-final- 
report.ps 
Brousseau, Julie, Caroline Drouin, George 
Foster, Pierre Isabelle, Roland Kuhn, Yves 
Normandin, and Pierre Plamondon. 1995. 
French speech recognition i  an automatic 
dictation system for translators: The 
TransTalk project. In Proceedings of
EuroSpeech'95, pages 193-196, Madrid, 
Spain. 
Brown, Peter F., John Cocke, Stephen A. 
Della Pietra, Vincent J. Della Pietra, 
Fredrick Jelinek, Robert L. Mercer, and 
Paul Roossin. 1988. A statistical approach 
to language translation. In Proceedings of
the 12th International Conference on 
Computational Linguistics, pages 71-76, 
Budapest, Hungary. 
Brown, Peter F., Stephen A. Della Pietra, 
Vincent J. Della Pietra, Meredith J.
Goldsmith, Jan Hajic, Robert L. Mercer 
and Surya Mohanty. 1993a. But 
dictionaries are data too. In Proceedings of
the ARPA HLT Workshop, ages 202-205, 
Princeton, NJ. 
Brown, Peter F., Stephen A. Della Pietra, 
Vincent J. Della Pietra, and Robert L. 
Mercer. 1993b. The mathematics of
statistical machine translation: Parameter 
estimation. Computational Linguistics 
19(2):263-311. 
Buckley, Chris. 1993. The importance of 
proper weighting methods. In Proceedings 
of the DARPA Workshop on Human Language 
Technology, pages 349-352, Princeton, NJ. 
Candito, Marie?H~l~ne. 1998. Building 
parallel LTAG for French and Italian. In 
COLING-ACL "98:36 Annual Meeting of the 
Association for Computational Linguistics and 
17th International Conference on 
Computational Linguistics, pages 211-217, 
Montreal, Canada. 
Catizone, Roberta, Graham Russell, and 
Susan Warwick. 1989. Deriving 
translation data from bilingual texts. In 
Proceedings ofthe First International Lexical 
Acquisition Workshop. Detroit, MI. 
Church, Kenneth W., and Eduard H. Hovy. 
1993. Good applications for crummy 
machine translation. Machine Translation 8. 
Cousin, Pierre-Henri, Lorna Sinclair, 
Jean-Francois Allain, and Catherine E. 
Love. 1990. The Harper Collins French 
Dictionary. Harper Collins Publishers, 
247 
Computational Linguistics Volume 26, Number 2 
New York, NY. 
Cousin, Pierre-Henri, Lorna Sinclair, 
Jean-Francois Allain, and Catherine E. 
Love. 1991. The Collins Paperback French 
Dictionary. Harper Collins Publishers, 
Glasgow. 
Dagan, Ido, Kenneth W. Church, and 
William A. Gale. 1993. Robust word 
alignment for machine aided translation. 
In Proceedings ofthe Workshop on Very Large 
Corpora: Academic and Industrial 
Perspectives, pages 1-8, Columbus, OH. 
Daille, B~atrice, l~ric Gaussier, and 
Jean-Marc Lang4. 1994. Towards 
automatic extraction of monolingual and 
bilingual terminology. Proceedings ofthe 
15th International Conference on 
Computational Linguistics, pages 515-521, 
Kyoto, Japan. 
Deming, W. Edwards, and Frederick F. 
Stephan. 1940. On a least squares 
adjustment of a sampled frequency table 
when the expected marginal totals are 
known. The Annals of Mathematical 
Statistics, 11:42~444. 
Dempster, Arthur P., N. M. Laird, and 
Donald B. Rubin. 1977. Maximum 
likelihood from incomplete data via the 
EM algorithm. Journal of the Royal 
Statistical Society, 39(B):1-38. 
Dorr, Bonnie J. 1992. The use of lexical 
semantics in interlingual machine 
translation. Machine Translation, 
7(3):135-193. 
Dunning, Ted. 1993. Accurate methods for 
the statistics of surprise and coincidence. 
Computational Linguistics 19(1):61-74. 
Fellbaum, Christiane, editor. 1998. WordNet: 
An Electronic Lexical Database. MIT Press. 
Foster, George, Pierre Isabelle, and Pierre 
Plamondon. 1996. Word completion: A
first step toward target-text mediated 
IMT. In Proceedings ofthe 16th International 
Conference on Computational Linguistics, 
pages 394-399, Copenhagen, Denmark. 
Fung, Pascale. 1995. A pattern matching 
method for finding noun and proper 
noun translations from noisy parallel 
corpora. In Proceedings ofthe 33rd Annual 
Meeting, pages 236-243, Boston, MA. 
Association for Computational 
Linguistics. 
Gale, William A., and Kenneth W. Church. 
1991. Identifying word correspondences 
in parallel texts. Proceedings ofthe DARPA 
SNL Workshop, pages 152-157, Asilomar, 
CA. 
Gale, William A., and Geoff Sampson. 1995. 
Good-Turing frequency estimation 
without tears. Journal of Quantitative 
Linguistics, 2:217-237. Swets & Zeitlinger 
Publishers, Sassenheim, The Netherlands. 
Hiemstra, Djoerd. 1996. Using Statistical 
Methods to Create a Bilingual Dictionary. 
Masters thesis, University of Twente, The 
Netherlands. 
Hiemstra, Djoerd. 1998. Multilingual 
domain modeling in twenty-one: 
Automatic reation of a bi-directional 
translation lexicon from a parallel corpus. 
In Proceedings ofthe Eighth meeting of 
Computational Linguistics in the Netherlands 
(CLIN), pages 41-58. 
Kumano, Akira, and Hideki Hirakawa. 1994. 
Building an MT dictionary from parallel 
texts based on linguistic and statistical 
information. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics, pages 76-81, Kyoto, Japan. 
McCarley, J. Scott. 1999. Should we translate 
the documents or the queries in 
cross-language information retrieval? In 
Proceedings ofthe 37th Annual Meeting, 
pages 208-214, College Park, MD. 
Association for Computational 
Linguistics. 
Macklovitch, Elliott. 1994. Using bi-textual 
alignment for translation validation: The 
TransCheck system. In Proceedings ofthe 
1st Conference ofthe Association for Machine 
Translation in the Americas, pages 157-168. 
Columbia, MD. 
Melamed, I. Dan. 1995. Automatic 
evaluation and uniform filter cascades for 
inducing N-best translation lexicons. In 
Proceedings ofthe Third Workshop on Very 
Large Corpora, pages 184-198, Cambridge, 
MA. 
Melamed, I. Dan. 1996a. Automatic 
detection of omissions in translations. In 
Proceedings ofthe 16th International 
Conference on Computational Linguistics, 
pages 764-769, Copenhagen, Denmark. 
Melamed, I. Dan. 1996b. Automatic 
construction of clean broad-coverage 
translation lexicons. In Proceedings ofthe 
2nd Conference ofthe Association for Machine 
Translation in the Americas, pages 125-134, 
Montreal, Canada. 
Melamed, I. Dan. 1998a. Models of 
co-occurrence. Institute for Research in 
Cognitive Science Technical Report 
#98-05. University of Pennsylvania, 
Philadelphia, PA. 
Melamed, I. Dan. 1998b. Annotation style 
guide for the blinker project. Institute for 
Research in Cognitive Science Technical 
Report #98-06. University of 
Pennsylvania, Philadelphia, PA. 
Melamed, I. Dan. 1998c. Manual annotation 
of translational equivalence: The blinker 
project. Institute for Research in Cognitive 
248 
Melamed Models of Translational Equivalence 
Science Technical Report #98-07. 
University of Pennsylvania, Philadelphia, 
PA. 
Melamed, I. Dan. To appear. Empirical 
Methods for Exploiting Parallel Texts, MIT 
Press. 
Nerbonne, John, Lauri Karttunen, Elena 
Paskaleva, Gabor Proszeky, and Tiit 
Roosmaa. 1997. Reading more into 
foreign languages. In Proceedings ofthe 5th 
ACL Conference on Applied Natural Language 
Processing, pages 135-138, Washington, 
DC. 
Oard, Douglas W. 1997. Adaptive filtering 
of multilingual document streams. In 
Proceedings ofthe 5th RIAO Conference on 
Computer-Assisted Information Retrieval, 
pages 233-253, Montreal, Canada. 
Resnik, Philip. 1997. Evaluating multilingual 
gisting of Web pages. In Proceedings ofthe 
AAAI Symposium on Natural Language 
Processing for the World Wide Web. Stanford, 
CA. 
Resnik, Philip, and David Yarowsky. 1997. 
A perspective on word sense 
disambiguation methods and their 
evaluation. In Proceedings ofthe SIGLEX 
Workshop on Tagging Text with Lexical 
Semantics, pages 79-86, Washington, DC. 
Shieber, Stuart. 1994. Restricting the 
weak-generative capacity of synchronous 
tree-adjoining grammars. Computational 
Intelligence, 10(4):371-385. 
Simard, Michel, George F. Foster, and 
Francois Perrault. 1993. TransSearch: A 
bilingual concordance tool. Centre 
d'innovation en technologies de 
l'information, Laval, Canada. 
Svartvik, Jan. 1992. Directions in Corpus 
Linguistics. Mouton de Gruyter, Berlin. 
Vogel, Stephan, Hermann Ney, and 
Christoph Tillmann. 1996. HMM-based 
word alignment in statistical translation. 
In Proceedings ofthe 16th International 
Conference on Computational Linguistics. 
Copenhagen, Denmark. 
Piek, Vossen, editor. 1998. Eurowordnet: A 
Multilingual Database with Lexical Semantic 
Networks. Kluwer Academic Publishers. 
White, John S., and Theresa A. O'Connell. 
1993. Evaluation of machine translation. 
In Proceedings ofthe ARPA HLT Workshop, 
pages 206-210, Princeton, NJ. 
Wu, Dekai, and Xuanyin Xia. 1994. Learning 
an English-Chinese l xicon from a 
parallel corpus. In Proceedings ofthe First 
Conference ofthe Association for Machine 
Translation i  the Americas, pages 206-213, 
Columbia, MD. 
Yarowsky, David. 1993. One sense per 
collocation. In Proceedings ofthe DARPA 
Workshop on Human Language Technology, 
pages 266-271, Princeton, NJ. 
249 

