Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 36?41,
Prague, June 2007. c?2007 Association for Computational Linguistics
Recognizing Textual Entailment Using Sentence Similarity based on 
Dependency Tree Skeletons 
 Rui Wang and G?nter Neumann 
LT-lab, DFKI 
Stuhlsatzenhausweg 3, 66123 Saarbr?cken, Germany 
{wang.rui,Neumann}@dfki.de 
 
 
Abstract 
We present a novel approach to RTE that 
exploits a structure-oriented sentence rep-
resentation followed by a similarity func-
tion. The structural features are automati-
cally acquired from tree skeletons that are 
extracted and generalized from dependency 
trees. Our method makes use of a limited 
size of training data without any external 
knowledge bases (e.g. WordNet) or hand-
crafted inference rules. We have achieved 
an accuracy of 71.1% on the RTE-3 devel-
opment set performing a 10-fold cross 
validation and 66.9% on the RTE-3 test 
data. 
1 Introduction 
Textual entailment has been introduced as a rela-
tion between text expressions, capturing the fact 
that the meaning of one expression can be inferred 
from the other (Dagan and Glickman, 2004). More 
precisely, textual entailment is defined as ?? a 
relationship between a coherent text T and a lan-
guage expression, which is considered as a hy-
pothesis, H. We say that T entails H (H is a conse-
quent of T), denoted by T ? H, if the meaning of 
H, as interpreted in the context of T, can be in-
ferred from the meaning of T.?  
Table 1 displays several examples from the 
RTE-3 development set. For the third pair (id=410) 
the key knowledge needed to decide whether the 
entailment relation holds is that ?[PN1]?s wife, 
[PN2]? entails ?The name of [PN1]?s wife is 
[PN2]?, although T contains much more (irrelevant) 
information. On the other hand, the first pair (id=1) 
requires an understanding of concepts with oppo-
site meanings (i.e. ?buy? and ?sell?), which is a 
case of semantic entailment. 
The different sources of possible entailments 
motivated us to consider the development of spe-
cialized entailment strategies for different NLP 
tasks. In particular, we want to find out the poten-
tial connections between entailment relations be-
longing to different linguistic layers for different 
applications. 
In this paper, we propose a novel approach to-
wards structure-oriented entailment based on our 
empirical discoveries from the RTE corpora: 1) H 
is usually textually shorter than T; 2) not all infor-
mation in T is relevant to make decisions for the 
entailment; 3) the dissimilarity of relations among 
the same topics between T and H are of great im-
portance.  
Based on the observations, our primary method 
starts from H to T (i.e. in the opposite direction of 
the entailment relation) so as to exclude irrelevant 
information from T. Then corresponding key top-
ics and predicates of both elements are extracted. 
We then represent the structural differences be-
tween T and H by means of a set of Closed-Class 
Symbols. Finally, these acquired representations 
(named Entailment Patterns - EPs) are classified by 
means of a subsequence kernel. 
The Structure Similarity Function is combined 
with two robust backup strategies, which are re-
sponsible for cases that are not handled by the EPs. 
One is a Triple Similarity Function applied on top 
of the local dependency relations of T and H; the 
other is a simple Bag-of-Words (BoW) approach 
that calculates the overlapping ratio of H and T. 
Together, these three methods deal with different 
entailment cases in practice. 
36
2 Related Work 
Conventional methods for RTE define measures for 
the similarity between T and H either by assuming 
an independence between words (Corley and Mi-
halcea, 2005) in a BoW fashion or by exploiting 
syntactic interpretations. (Kouylekov and Magnini, 
2006) explore a syntactic tree editing distance to 
detect entailment relations. Since they calculate the 
similarity between the two dependency trees of T 
and H directly, the noisy information may decrease 
accuracy. This observation actually motivated us to 
start from H towards the most relevant information 
in T. 
Logic rules (as proposed by (Bos and Markert, 
2005)) or sequences of allowed rewrite rules (as in 
(de Salvo Braz et al, 2005)) are another fashion of 
tackling RTE. One the best two teams in RTE-2 
(Tatu et al, 2006) proposed a knowledge represen-
tation model which achieved about 10% better per-
formance than the third (Zanzotto and Moschitti, 
2006) based on their logic prover. The other best 
team in RTE-2 (Hickl et al, 2006) automatically 
acquired extra training data, enabling them to 
achieve about 10% better accuracy than the third as 
well. Consequently, obtaining more training data 
and embedding deeper knowledge were expected 
to be the two main directions pointed out for future 
research in the RTE-2 summary statement. How-
ever, except for the positive cases of SUM, T-H 
pairs are normally not very easy to collect auto-
matically. Multi-annotator agreement is difficult to 
reach on most of the cases as well. The knowledge-
based approach also has its caveats since logical 
rules are usually implemented manually and there-
fore require a high amount of specialized human 
expertise in different NLP areas. 
 Another group (Zanzotto and Moschitti, 2006) 
utilized a tree kernel method for cross-pair similar-
ity, which showed an improvement, and this has 
motivated us to investigate kernel-based methods. 
The main difference in our method is that we apply 
subsequence kernels on patterns extracted from the 
dependency trees of T and H, instead of applying 
tree kernels on complete parsing trees. On the one 
hand, this allows us to discover essential parts in-
dicating an entailment relationship, and on the 
other hand, computational complexity is reduced. 
3 An Overview of RTE 
Figure 1 shows the different processing techniques 
and depths applied to the RTE task. Our work fo-
cuses on constructing a similarity function operat-
ing between sentences. In detail, it consists of sev-
eral similarity scores with different domains of 
locality on top of the dependency structure. Figure 
2 gives out the workflow of our system. The main 
part of the sentence similarity function is the Struc-
ture Similarity Function; two other similarity 
scores are calculated by our backup strategies. The 
first backup strategy is a straightforward BoW 
method that we will not present in this paper (see 
more details in (Corley and Mihalcea, 2005)); 
Id Ta s k Te x t H y po th e s i s En ta i l s ?
1 IE
Th e sa le wa s m a d e to  p a y Yu ko s' U S $  2 7 .5  b illio n  ta x b ill, Yu g a n skn efteg a z 
wa s o rig in a lly so ld  fo r U S $  9 .4  b illio n  to  a  little  kn o wn  co m p a n y 
B a ika lfin a n sg ro u p  wh ich  wa s la ter b o ught  b y th e R u ssia n  sta te-o wn ed  o il 
B a ika lfin a n sg ro u p  
wa s so ld  to  
R o sn eft.
Y E S
3 9 0 IR
T yp ho o n  X a n g sa n e la shed  th e P hilip p ine  ca p ita l o n  Th u rsd a y, 
g ro u n d in g  flig h ts, h a ltin g  vessels a n d  clo sin g  sch o o ls a n d  m a rkets a fter 
trig g erin g  fa ta l fla sh  flo o d s in  th e cen tre o f th e  co u n try.
A  typ ho o n  b a tters 
th e  P hilip p ines . Y E S
4 1 0 Q A
(S en ten ce 1  ...) . A lo n g  with  th e first la d y's m o th er, Jen n a  Welch , th e 
weeken d  g a th erin g  in clu d es th e p resid en t's p a ren ts, fo rm er P resid en t 
G eo rge H .W . Bush a nd  his wife, Ba rb a ra ;  h is sister D o ro  Ko ch  a n d  h er 
h u sb a n d , B o b b y;  a n d  h is b ro th er, M a rvin , a n d  h is wife , M a rg a ret.
Th e n a m e o f 
G eo rg e H .W. 
B u sh 's wife is 
B a rb a ra .
Y E S
7 3 9 SU M
Th e FD A  wo u ld  n o t sa y in  wh ich  sta tes  th e p ills  h a d  b een  so ld , b u t 
in stea d  reco m m en d ed  th a t cu sto m ers d eterm in e wh eth er p ro d u cts th ey 
b o u g h t a re b ein g  reca lled  b y ch eckin g  th e sto re list  o n  th e  FD A  Web  site , 
a n d  th e b a tch  list .  Th e b a tch  n u m b ers a p p ea r o n  th e co n ta in er's la b el.
Th e FD A 
p ro vid ed  a  list  o f 
sta tes  in  wh ich  th e 
p ills  h a ve b een  
N O
Table 1 Examples from RTE-3 
Figure 1 Overview of RTE 
37
while the second one is based on a triple set repre-
sentation of sentences that expresses the local de-
pendency relations found by a parser1. 
A dependency structure consists of a set of triple 
relations (TRs). A TR is of the form <node1, rela-
tion, node2>, where node1 represents the head, 
node2 the modifier and relation the dependency 
relation. Chief requirements for the backup system 
are robustness and simplicity. Accordingly, we 
construct a similarity function, the Triple Similar-
ity Function (TSF), which operates on two triple 
sets and determines how many triples of H2 are 
contained in T. The core assumption here is that 
the higher the number of matching triple elements, 
the more similar both sets are, and the more likely 
it is that T entails H. 
TSF uses an approximate matching function. 
Different cases (i.e. ignoring either the parent node 
or the child node, or the relation between nodes) 
might provide different indications for the similar-
ity of T and H. In all cases, a successful match be-
tween two nodes means that they have the same 
lemma and POS. We then sum them up using dif-
ferent weights and divide the result by the cardinal-
ity of H for normalization. The different weights 
learned from the corpus indicate that the ?amount 
of missing linguistic information? affect entailment 
decisions differently. 
4 Workflow of the Main Approach 
Our Structure Similarity Function is based on the 
hypothesis that some particular differences be-
tween T and H will block or change the entailment 
relationship. Initially we assume when judging the 
entailment relation that it holds for each T-H pair 
                                                 
1
 We are using Minipar (Lin, 1998) and Stanford Parser (Klein 
and Manning, 2003) as preprocessors, see also sec. 5.2. 
2
 Note that henceforth T and H will represent either the origi-
nal texts or the dependency structures. 
(using the default value ?YES?). The major steps 
are as follows (see also Figure 2): 
4.1 Tree Skeleton Extractor 
Since we assume that H indicates how to extract 
relevant parts in T for the entailment relation, we 
start from the Tree Skeleton of H (TSH). First, we 
construct a set of keyword pairs using all the nouns 
that appear in both T and H. In order to increase 
the hits of keyword pairs, we have applied a partial 
search using stemming and some word variation 
techniques on the substring level. For instance, the 
pair (id=390) in Table 1 has the following list of 
keyword pairs, 
<Typhoon_Xangsane ## typhoon, 
Philippine ## Philippines> 
Then we mark the keywords in the dependency 
trees of T and H and extract the sub-trees by ignor-
ing the inner yields. Usually, the Root Node of H 
(RNH) is the main verb; all the keywords are con-
tained in the two spines of TSH (see Figure 3). 
Note that in the Tree Skeleton of T (TST), 1) the 
Root Node (RNT) can either be a verb, a noun or 
even a dependency relation, and 2) if the two Foot 
Nodes (FNs) belong to two sentences, a dummy 
node is created that connects the two spines. 
Thus, the prerequisite for this algorithm is that 
TSH has two spines containing all keywords in H, 
and T satisfies this as well. For the RTE-3 devel-
opment set, we successfully extracted tree skele-
Figure 3 Example of a Tree Skeleton 
Figure 2 Workflow of the System 
38
?
=
?
=
+?
=
?
=
=
><><
||
1
|'|
1'
)
'
,(
||
1
|'|
1'
)
'
,(
)',',,(
H
j
H
j j
CCSjCCSCCSK
T
i
T
i i
CCSiCCSCCSK
HTHT
esubsequencK
tons from 254 pairs, i.e., 32% of the data is cov-
ered by this step, see also sec. 5.2. 
Next, we collapse some of the dependency rela-
tion names from the parsers to more generalized 
tag names, e.g., collapsing <OBJ2> and <DESC> 
to <OBJ>. We group together all nodes that have 
relation labels like <CONJ> or <NN>, since they 
are assumed to refer to the same entity or belong to 
one class of entities sharing some common charac-
teristics. Lemmas are removed except for the key-
words. Finally, we add all the tags to the CCS set. 
Since a tree skeleton TS consists of spines con-
nected via the same root node, TS can be trans-
formed into a sequence. Figure 4 displays an ex-
ample corresponding to the second pair (id=390) of 
Table 1. Thus, the general form of a sequential rep-
resentation of a tree skeleton is: 
LSP #RN# RSP 
where LSP represents the Left Spine, RSP repre-
sents the Right Spine, and RN is the Root Node. 
On basis of this representation, a comparison of the 
two tree skeletons is straightforward: 1) merge the 
two LSPs by excluding the longest common prefix, 
and 2) merge the two RSPs by excluding the long-
est common suffix. Then the Spine Difference (SD) 
is defined as the remaining infixes, which consists 
of two parts, SDT and SDH. Each part can be either 
empty (i.e. ?) or a CCS sequence. For instance, the 
two SDs of the example in Figure 4 (id=390) are 
(LSD ? Left SD; RSD ? Right SD; ## is a separa-
tor sign): 
LSDT(N) ## LSDH(?) 
RSDT(?) ## RSDH(?) 
We have observed that two neighboring depend-
ency relations of the root node of a tree skeleton 
(<SUBJ> or <OBJ>) can play important roles in 
predicting the entailment relation as well. There-
fore, we assign them two extra features named 
Verb Consistence (VC) and Verb Relation Con-
sistence (VRC). The former indicates whether two 
root nodes have a similar meaning, and the latter 
indicates whether the relations are contradictive 
(e.g. <SUBJ> and <OBJ> are contradictive). 
We represent the differences between TST and 
TSH by means of an Entailment Pattern (EP), 
which is a quadruple <LSD, RSD, VC, VRC>. VC 
is either true or false, meaning that the two RNs 
are either consistent or not. VRC has ternary value, 
whereby 1 means that both relations are consistent, 
-1 means at least one pair of corresponding rela-
tions is inconsistent, and 0 means RNT is not a 
verb.3 The set of EPs defines the feature space for 
the subsequence kernels in our Structure Similarity 
Function. 
4.2 Structure Similarity Function 
We define the function by constructing two basic 
kernels to process the LSD and RSD part of an EP, 
and two trivial kernels for VC and VRC. The four 
kernels are combined linearly by a composite ker-
nel that performs binary classification on them. 
Since all spine differences SDs are either empty 
or CCS sequences, we can utilize subsequence 
kernel methods to represent features implicitly, cf. 
(Bunescu and Mooney, 2006). Our subsequence 
kernel function is: 
whereby T and H refers to all spine differences 
SDs from T and H, and |T| and |H| represent the 
cardinalities of SDs. The function KCCS(CCS,CCS?) 
checks whether its arguments are equal. 
Since the RTE task checks the relationship be-
tween T and H, we need to consider collocations 
of some CCS subsequences between T and H as 
well. Essentially, this kernel evaluates the similar-
ity of T and H by means of those CCS subse-
quences appearing in both elements. The kernel 
function is as follows: 
On top of the two simple kernels, KVC, and KVRC, 
we use a composite kernel to combine them line-
arly with different weights: 
VRCVCncollocatioesubsequenccomposite KKKKK ???? +++= , 
                                                 
3
 Note that RNH is guaranteed to be a verb, because otherwise 
the pair would have been delegated to the backup strategies. 
????
= = = =
?=
><><
||
1
'||
1'
||
1
'||
1'
''
),(),(
)',',,(
T
i
T
i
H
j
H
j
jjCCSiiCCS
ncollocatio
CCSCCSKCCSCCSK
HTHTK
Figure 4 Spine Merging 
39
where ? and ? are learned from the training corpus; 
?=?=1. 
5 Evaluation 
We have evaluated four methods: the two backup 
systems as baselines (BoW and TSM, the Triple 
Set Matcher) and the kernel method combined with 
the backup strategies using different parsers, Mini-
par (Mi+SK+BS) and the Stanford Parser 
(SP+SK+BS). The experiments are based on RTE-
3 Data 4 . For the kernel-based classification, we 
used the classifier SMO from the WEKA toolkit 
(Witten and Frank, 1999).  
5.1 Experiment Results 
RTE-3 data include the Dev Data (800 T-H pairs, 
each task has 200 pairs) and the Test Data (same 
size). Experiment A performs a 10-fold cross-
validation on Dev Data; Experiment B uses Dev 
Data for training and Test Data for testing cf. Table 
2 (the numbers denote accuracies): 
Systems\Tasks IE IR QA SUM All 
Exp A: 10-fold Cross Validation on RTE-3 Dev Data 
BoW 54.5 70 76.5 68.5 67.4 
TSM 53.5 60 68 62.5 61.0 
Mi+SK+BS 63 74 79 68.5 71.1 
SP+SK+BS 60.5 70 81.5 68.5 70.1 
Exp B: Train: Dev Data; Test: Test Data 
BoW 54.5 66.5 76.5 56 63.4 
TSM 54.5 62.5 66 54.5 59.4 
Mi+SP+SK+BS 58.5 70.5 79.5 59 66.9 
Table 2 Results on RTE-3 Data 
For the IE task, Mi+SK+BS obtained the highest 
improvement over the baseline systems, suggesting 
that the kernel method seems to be more appropri-
ate if the underlying task conveys a more ?rela-
tional nature.? Improvements in the other tasks are 
less convincing as compared to the baselines. Nev-
ertheless, the overall result obtained in experiment 
B would have been among the top 3 of the RTE-2 
challenge. We utilize the system description table 
of (Bar-Haim et al, 2006) to compare our system 
with the best two systems of RTE-2 in Table 35: 
                                                 
4
 See (Wang and Neumann, 2007) for details concerning the 
experiments of our method on RTE-2 data. 
5
 Following the notation in  (Bar-Haim et al, 2006): Lx: Lexi-
cal Relation DB; Ng: N-Gram / Subsequence overlap; Sy: 
Syntactic Matching / Alignment; Se: Semantic Role Labeling; 
LI: Logical Inference; C: Corpus/Web; M: ML Classification; 
B: Paraphrase Technology / Background Knowledge; L: Ac-
quisition of Entailment Corpora. 
Systems Lx Ng Sy Se LI C M B L 
Hickl et al X X X X  X X  X 
Tatu et al X    X   X  
Ours  X X    X   
Table 3 Comparison with the top 2 systems in 
RTE-2. 
Note that the best system (Hickl et al, 2006) ap-
plies both shallow and deep techniques, especially 
in acquiring extra entailment corpora. The second 
best system (Tatu et al, 2006) contains many 
manually designed logical inference rules and 
background knowledge. On the contrary, we ex-
ploit no additional knowledge sources besides the 
dependency trees computed by the parsers, nor any 
extra training corpora. 
5.2 Discussions 
Table 4 shows how our method performs for the 
task-specific pairs matched by our patterns: 
Tasks IE IR QA SUM ALL 
ExpA:Matched 53 19 23.5 31.5 31.8 
ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8 
ExpB:Matched 58.5 16 27.5 42 36 
ExpB:Accuracy 57.2 81.5 90.9 65.5 68.8 
Table 4 Performances of our method 
For IE pairs, we find good coverage, whereas 
for IR and QA pairs the coverage is low, though it 
achieves good accuracy. According to the experi-
ments, BoW has already achieved the best per-
formance for SUM pairs cf. Table 2. 
As a whole, developing task specific entailment 
operators is a promising direction. As we men-
tioned in the first section, the RTE task is neither a 
one-level nor a one-case task. The experimental 
results uncovered differences among pairs of dif-
ferent tasks with respect to accuracy and coverage. 
On the one hand, our method works successfully 
on structure-oriented T-H pairs, most of which are 
from IE. If both TST and TSH can be transformed 
into CCS sequences, the comparison performs well, 
as in the case of the last example (id=410) in Table 
1. Here, the relation between ?wife?, ?name?, and 
?Barbara? is conveyed by the punctuation ?,?, the 
verb ?is?, and the preposition ?of?. Other cases like 
the ?work for? relation of a person and a company 
or the ?is located in? relation between two location 
names are normally conveyed by the preposition 
?of?. Based on these findings, taking into account 
more carefully the lexical semantics based on in-
ference rules of functional words might be helpful 
in improving RTE. 
40
On the other hand, accuracy varies with T-H 
pairs from different tasks. Since our method is 
mainly structure-oriented, differences in modifiers 
may change the results and would not be caught 
under the current version of our tree skeleton. For 
instance, ?a commercial company? will not entail 
?a military company?, even though they are struc-
turally equivalent. 
Most IE pairs are constructed from a binary rela-
tion, and so meet the prerequisite of our algorithm 
(see sec. 4.1). However, our method still has rather 
low coverage. T-H pairs from other tasks, for ex-
ample like IR and SUM, usually contain more in-
formation, i.e. more nouns, the dependency trees of 
which are more complex. For instance, the pair 
(id=739) in Table 1 contains four keyword pairs 
which we cannot handle by our current method. 
This is one reason why we have constructed extra 
T-H pairs from MUC, TREC, and news articles 
following the methods of (Bar-Haim et al, 2006). 
Still, the overall performance does not improve. 
All extra training data only serves to improve the 
matched pairs (about 32% of the data set) for 
which we already have high accuracy (see Table 4). 
Thus, extending coverage by machine learning 
methods for lexical semantics will be the main fo-
cus of our future work. 
6 Conclusions and Future Work 
Applying different RTE strategies for different 
NLP tasks is a reasonable solution. We have util-
ized a structure similarity function to deal with the 
structure-oriented pairs, and applied backup strate-
gies for the rest. The results show the advantage of 
our method and direct our future work as well. In 
particular, we will extend the tree skeleton extrac-
tion by integrating lexical semantics based on in-
ference rules for functional words in order to get 
larger domains of locality. 
Acknowledgements 
The work presented here was partially supported 
by a research grant from BMBF to the DFKI pro-
ject HyLaP (FKZ: 01 IW F02) and the EC-funded 
project QALL-ME. 
References 
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B. and Szpektor, I. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proc. of the PASCAL RTE-2 Challenge. 
Bos, J. and Markert, K. 2005. Combining Shallow and 
Deep NLP Methods for Recognizing Textual Entail-
ment. In Proc. of the PASCAL RTE Challenge. 
Bunescu, R. and Mooney, R. 2006. Subsequence Ker-
nels for Relation Extraction. In Advances in Neural 
Information Processing Systems 18. MIT Press. 
Corley, C. and Mihalcea, R. 2005. Measuring the Se-
mantic Similarity of Texts. In Proc. of the ACL 
Workshop on Empirical Modeling of Semantic 
Equivalence and Entailment. 
Dagan, R., Glickman, O. 2004. Probabilistic textual 
entailment: Generic applied modelling of language 
variability. In PASCAL Workshop on Text Under-
standing and Mining. 
de Salvo Braz, R., Girju, R., Punyaka-nok, V., Roth, D., 
and Sammons, M. 2005. An Inference Model for Se-
mantic Entailment in Natural Language. In Proc. of 
the PASCAL RTE Challenge. 
Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink, 
B. and Shi, Y. 2006. Recognizing Textual Entailment 
with LCC?s GROUNDHOG System. In Proc. of the 
PASCAL RTE-2 Challenge. 
Klein, D. and Manning, C. 2003. Accurate Unlexical-
ized Parsing. In Proc. of ACL 2003. 
Kouylekov, M. and Magnini, B. 2006. Tree Edit Dis-
tance for Recognizing Textual Entailment: Estimat-
ing the Cost of Insertion. In Proc. of the PASCAL 
RTE-2 Challenge. 
Lin, D. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems. 
Tatu, M., Iles, B., Slavik, J., Novischi, A. and Moldo-
van, D. 2006. COGEX at the Second Recognizing 
Textual Entailment Challenge. In Proc. of the PAS-
CAL RTE-2 Challenge. 
Wang, R. and Neumann, G. 2007. Recognizing Textual 
Entailment Using a Subsequence Kernel Method. In 
Proc. of AAAI 2007. 
Witten, I. H. and Frank, E. Weka: Practical Machine 
Learning Tools and Techniques with Java Implemen-
tations. Morgan Kaufmann, 1999. 
Zanzotto, F.M. and Moschitti, A. 2006. Automatic 
Learning of Textual Entailments with Cross-pair 
Similarities. In Proc. of ACL 2006. 
41
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 784?792,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Recognizing Textual Relatedness with Predicate-Argument Structures
Rui Wang
Dept of Computational Linguistics
Saarland University
66123 Saarbr?ucken, Germany
rwang@coli.uni-sb.de
Yi Zhang
Dept of Computational Linguistics
Saarland University
LT-Lab, DFKI GmbH
D-66123 Saarbr?ucken, Germany
yzhang@coli.uni-sb.de
Abstract
In this paper, we first compare several
strategies to handle the newly proposed
three-way Recognizing Textual Entailment
(RTE) task. Then we define a new mea-
surement for a pair of texts, called Textual
Relatedness, which is a weaker concept
than semantic similarity or paraphrase. We
show that an alignment model based on the
predicate-argument structures using this
measurement can help an RTE system to
recognize the Unknown cases at the first
stage, and contribute to the improvement
of the overall performance in the RTE task.
In addition, several heterogeneous lexical
resources are tested, and different contri-
butions from them are observed.
1 Introduction
Recognizing Textual Entailment (RTE) (Dagan et
al., 2006) is a task to detect whether one Hypoth-
esis (H) can be inferred (or entailed) by a Text
(T). Being a challenging task, it has been shown
that it is helpful to applications like question an-
swering (Harabagiu and Hickl, 2006). The recent
research on RTE extends the two-way annotation
into three-way
1 2
, making it even more difficult,
but more linguistic-motivated.
The straightforward strategy is to treat it as a
three-way classification task, but the performance
suffers a significant drop even when using the
same classifier and the same feature model. In
fact, it can also be dealt with as an extension to the
traditional two-way classification, e.g., by identi-
1
http://nlp.stanford.edu/RTE3-pilot/
2
http://www.nist.gov/tac/tracks/2008/
rte/rte.08.guidelines.html
fying the Entailment (E) cases first and then fur-
ther label the Contradiction (C) and Unknown (U)
T-H pairs. Some other researchers also work on
detecting negative cases, i.e. contradiction, in-
stead of entailment (de Marneffe et al, 2008).
However, according to our best knowledge, the
detailed comparison between these strategies has
not been fully explored, let alne the impact of the
linguistic motivation behind the strategy selection.
This paper will address this issue.
Take the following example from the RTE-4 test
set (Giampiccolo et al, 2009) as an example,
T: At least five people have been killed in
a head-on train collision in north-eastern
France, while others are still trapped in the
wreckage. All the victims are adults.
H: A French train crash killed children.
This is a pair of two contradicting texts, the
mentioning of events (i.e. train crash) in both T
and H are assumed to refer the same event
3
. In
fact, the only contradicting part lies in the sec-
ond sentence of T against H, that is, whether
there are children among the victims. Therefore,
this pair could also be classified as a Known (K)
pair (=E?C) against Unknown (U) pairs, instead
of being classified as a Non-entailment (N) case
(=C?U) against E case in the traditional two-way
annotation.
Furthermore, many state-of-the-art RTE ap-
proaches which are based on overlapping informa-
tion or similarity functions between T and H, in
fact over-cover the E cases, and sometimes, cover
the C cases as well. Therefore, in this paper, we
3
See more details about the annotation guideline at
http://www.nist.gov/tac/tracks/2008/rte/
rte.08.guidelines.html
784
would like to test whether applying this style of
approaches to capture the K cases instead of E
cases is more effective. While in lexical seman-
tics, semantic relatedness is a weaker concept than
semantic similarity, there is no counterpart at the
sentence or text level. Therefore, in this paper, we
propose a Recognizing Textual Relatedness (RTR)
task as a subtask or the first step of RTE. By doing
so, we choose predicate-argument structure (PAS)
as the feature representation, which has already
been shown quite useful in the previous RTE chal-
lenges (Wang and Neumann, 2007).
In order to obtain the PAS, we utilize a Semantic
Role Labeling (SRL) system developed by Zhang
et al (2008). Although SRL has been shown to be
effective for many tasks, e.g. information extrac-
tion, question answering, etc., it has not been suc-
cessfully used for RTE, mainly due to the low cov-
erage of the verb frame or semantic role resources
or the low performance of the automatic SRL sys-
tems. The recent CoNLL shared tasks (Surdeanu
et al, 2008; Haji?c et al, 2009) have been focus-
ing on semantic dependency parsing along with
the traditional syntactic dependency parsing. The
PAS from the system output is almost ready for
use to build applications based on it. Therefore,
another focus of this paper will be to apply SRL to
the RTE task. In particular, it can improve the first
stage binary classification (K vs. U), and the final
result improves as well.
The rest of the paper will be organized as fol-
lows: Section 2 will give a brief literature review
on both RTE and SRL; Section 3 describes the se-
mantic parsing system, which includes a syntactic
dependency parser and an SRL system; Section 4
presents an algorithm to align two PASs to recog-
nize textual relatedness between T and H, using
several lexical resources; The experiments will be
described in Section 5, followed by discussions;
and the final section will conclude the paper and
point out directions to work on in the future.
2 Related Work
Although the term of Textual Relatedness has not
been widely used by the community (as far as
we know), many researchers have already incor-
porated modules to tackle it, which are usually
implemented as an alignment module before the
inference/learning module is applied. For exam-
ple, Pado et al (2009) mentioned two alignment
modules, one is a phrase-based alignment system
called MANLI (MacCartney et al, 2008), and the
other is a stochastic aligner based on dependency
graphs. Siblini and Kosseim (2009) performed the
alignment on top of two ontologies. In this paper,
we would like to follow this line of research but on
another level of representation, i.e. the predicate-
argument structures (PAS), together with different
lexical semantic resources.
As for the whole RTE task, many people di-
rectly do the three-way classification with selec-
tive features (e.g. Agichtein et al (2009)) or dif-
ferent inference rules to identify entailment and
contradiction simultaneously (e.g. Clark and Har-
rison (2009)); while other researchers also extend
their two-way classification system into three-way
by performing a second-stage classification after-
wards. An interesting task proposed by de Marn-
effe et al (2008) suggested an alternative way to
deal with the three-way classification, that is, to
split out the contradiction cases first. However,
it has been shown to be more difficult than the
entailment recognition. Based on these previous
works and our experimental observations, we pro-
pose an alternative two-stage binary classification
approach, i.e. to identify the unknown cases from
the known cases (entailment and contradiction)
first. And the results show that due to the nature
of these approaches based on overlapping infor-
mation or similarity between T and H, this way of
splitting is more reasonable.
However, RTE systems using semantic role la-
belers has not shown very promising results, al-
though SRL has been successfully used in many
other NLP tasks, e.g. information extraction,
question answering, etc. According to our anal-
ysis of the data, there are mainly three reasons: a)
the limited coverage of the verb frames or predi-
cates; b) the undetermined relationships between
two frames or predicates; and c) the unsatisfy-
ing performance of an automatic SRL system.
For instance, Burchardt et al (2007) attempted to
use FrameNet (Baker et al, 1998) for the RTE-3
challenge, but did not show substantial improve-
ment. With the recent CoNLL challenges, more
and more robust and accurate SRL systems are
ready for use, especially for the PAS identifica-
tion. For the lexical semantics, we also discover
that, if we relax the matching criteria (from simi-
larity to relatedness), heterougeous resources can
contribute to the coverage differently and then the
effectiveness of PAS will be shown as well.
785
3 Semantic Parsing
In order to obtain the predicate-argument struc-
tures for the textual entailment corpus, we use the
semantic role labeler described in (Zhang et al,
2008). The SRL system is trained on the Wall
Street Journal sections of the Penn Treebank us-
ing PropBank and NomBank annotation of ver-
bal and nominal predicates, and relations to their
arguments, and produces as outputs the semantic
dependencies. The head words of the arguments
(including modifiers) are annotated as a direct de-
pendent of the corresponding predicate words, la-
beled with the type of the semantic relation (Arg0,
Arg1 . . . , and various ArgMs). Note that for the
application of SRL in RTE task, the PropBank and
NomBank notation appears to be more accessible
and robust than the the FrameNet notation (with
much more detailed roles or frame elements bond
to specific verb frames).
As input, the SRL system requires syntactic
dependency analysis. We use the open source
MST Parser (McDonald et al, 2005), trained also
on the Wall Street Journal Sections of the Penn
Treebank, using a projective decoder with second-
order features. Then the SRL system goes through
a pipeline of 4-stage processing: predicate identifi-
cation (PI) identifies words that evokes a semantic
predicate; argument identification (AI) identifies
the arguments of the predicates; argument classifi-
cation (AC) labels the argument with the semantic
relations (roles); and predicate classification (PC)
further differentiate different use of the predicate
word. All components are built as maximal en-
tropy based classifiers, with their parameters es-
timated by the open source TADM system
4
, fea-
ture sets selected on the development set. Evalu-
ation results from previous years? CoNLL shared
tasks show that the system achieves state-of-the-
art performance, especially for its out-domain ap-
plications.
4 Textual Relatedness
As we mentioned in the introduction, we break
down the three-way classification into a two-stage
binary classification. Furthermore, we treat the
first stage as a subtask of the main task, which
determines whether H is relevant to T. Similar to
the probabilistic entailment score, we use a relat-
edness score to measure such relationship. Due
4
http://tadm.sourceforge.net/
to the nature of the entailment recognition that
H should be fully entailed by T, we also make
this relatedness relationship asymmetric. Roughly
speaking, this Relatedness function R(T,H) can
be described as whether or how relevant H is to
some part of T. The relevance can be realized as
string similarity, semantic similarity, or being co-
occurred in similar contexts.
Before we define the relatedness function for-
mally, let us look at the representation again. After
semantic parsing described in the previous section,
we obtain a PAS for each sentence. On top of it,
we define a predicate-argument graph (PAG), the
nodes of which are predicates, arguments or some-
times both, and the edges of which are labeled se-
mantic relations. Notice that each predicate can
dominate zero, one, or more arguments, and each
argument have one or more predicates which dom-
inate it. Furthermore, the graph is not necessar-
ily fully connected. Thus, the R(T,H) function
can be defined on the dependency representation
as follows: if the PAG of H is semantically rel-
evant to part of the PAG of T, H is semantically
relevant to T.
In order to compare the two graphs, we further
reduce the alignment complexity by breaking the
graphs into sets of trees. Two types of decomposed
trees are considered: one is to take each predicate
as the root of a tree and arguments as child nodes,
and the other is on the contrary, to take each ar-
gument as root and their governing predicates as
child nodes. We name them as Predicate Trees (P-
Trees) and Argument Trees (A-Trees) respectively.
To obtain the P-Trees, we enumerate each predi-
cate, find all the arguments which it directly dom-
inates, and then construct a P-Tree. The algorithm
to obtain A-Trees works in the similar way. Fi-
nally, we will have a set of P-Trees and a set of A-
Trees for each PAG, both of which are simple trees
with depth of one. Figure 1 shows an example of
such procedures. Notice that we do not consider
cross-sentential inference, instead, we simply take
the union of tree sets from all the sentences. Figure
2 illustrates the PAG for both T and H after seman-
tic parsing, and the resulting P-Trees and A-Trees
after applying the decomposition algorithm.
Formally, we define the relatedness function for
a T-H pair as the maximum value of the related-
ness scores of all pairs of trees in T and H (P-trees
and A-trees).
786
ad e f
b c e
ge
a
d e
a b a
f
e
g
e f
b ca
d
g
A?Tree(s)
P?Tree(s)
Figure 1: Decomposition of predicate-argument graphs (left) into P-Trees (right top) and A-Trees (right
bottom)
R(T,H) = max
1?i?r,1?j?s
{
R(Tree
T
i
, T ree
H
j
)
}
In order to compare two P-Trees or A-Trees,
we further define each predicate-argument pair
contained in a tree as a semantic dependency
triple. Each semantic dependency triple con-
tains a predicate, an argument, and the seman-
tic dependency label in between, in the form
of ?Predicate,Dependency,Argument?. Then
we define the relatedness function between two
trees as the minimum value of the relatedness
scores of all the triple pairs from the two trees.
R(Tree
T
, T ree
H
) = min
1?i?n,1?j?m
{
R(?P
T
, D
T
i
, A
T
i
?, ?P
H
, D
H
j
, A
H
j
?)
}
For the relatedness function between two se-
mantic dependency triples, we define the follow-
ing two settings: the FULL match and the NOT-
FULL match. Either match requires that the pred-
icates are related at the first place. The former
means both the dependencies and the arguments
are related; while the latter only requires the de-
pendencies to be related.
R(?P
T
, D
T
, A
T
?, ?P
H
, D
H
, A
H
?) =
?
?
?
Full R(P
T
,P
H
)=R(D
T
,D
H
)=R(A
T
,A
H
)=1
NotFull R(P
T
,P
H
)=R(D
T
,D
H
)=1
Other Otherwise
Now, the only missing components in our defi-
nition is the relatedness functions between pred-
icates, arguments, and semantic dependencies.
Fortunately, many people have done research on
semantic relatedness in lexical semantics that we
could use. Therefore, these functions can be
realized by different string matching algorithms
and/or lexical resources. Since the meaning of rel-
evance is rather wide, apart from the string match-
ing of the lemmas, we also incorporate various
resources, from distributionally collected ones to
hand-crafted ontologies. We choose VerbOcean
(Chklovski and Pantel, 2004) to obtain the relat-
edness between predicates (after using WordNet
(Fellbaum, 1998) to change all the nominal pred-
icates into verbs) and use WordNet for the argu-
ment alignment. For the verb relations in Ver-
bOcean, we consider all of them as related; and
for WordNet, we not only use the synonyms, hy-
ponyms, and hypernyms, but antonyms as well.
Consequently, we simplify these basic relatedness
functions into a binary decision. If the correspond-
ing strings are matched or the relations mentioned
above exist, the two predicates, arguments, or de-
pendencies are related; otherwise, not.
In addition, the Normalized Google Distance
(NGD) (Cilibrasi and Vitanyi, 2007) is applied to
both cases
5
. As for the comparison between de-
pendencies, we simply apply the string matching,
except for modifier labels, which we treat them as
the same
6
. In all, the main idea here is to incorpo-
rate both distributional semantics and ontological
semantics in order to see whether their contribu-
tions are overlapping or complementary. In prac-
tice, we use empirical value 0.5 as the threshold.
Below the threshold means they are related, oth-
5
You may find the NGD values of all the con-
tent word pairs in RTE-3 and RTE-4 datasets at
http://www.coli.uni-sb.de/
?
rwang/
resources/RTE3_RTE4_NGD.txt.
6
This is mainly because it is more difficult for the SRL
system to differentiate modifier labels than the complements.
787
crash
killed
train
children
A0 A1
A1whilepeople
A1
killed
... ... train
collision
still
A1
trapped
... ...
AM?ADV
AM?ADV
A1
crash
train
killed
children
A0 A1
crash
P?Trees
A?Trees
whilepeople
A1
killed
... ... train
collision
still
A1
trapped
... ...
AM?ADV
AM?ADV
killed killed killed collision trapped trapped
people while ... ... train still ... ...
A1 AM?ADV A1 AM?ADV
killed killed crash
traincrash children
A1A0 A1
T H
PAG
Figure 2: Predicate-argument graphs and corresponding P-Trees and A-trees of the T-H pair
erwise not. In order to achieve a better coverage,
we use the OR operator to connect all the related-
ness functions above, which means, if any of them
holds, the two items are related.
Notice that, although we define only the relat-
edness between T and H, in principle, the graph
representation can also be used for the entailment
relationship. However, since it needs more fine-
grained analysis and resources, we will leave it as
the future work.
5 Experiments
In order to evaluate our method, we setup several
experiments. The baseline system here is a simple
Naive Bayes classifier with a feature set contain-
ing the Bag-of-Words (BoW) overlapping ratio be-
tween T and H, and also the syntactic dependency
overlapping ratio. The feature model combines
two baseline systems proposed by previous work,
which gives out quite competitive performance.
Since the main goal of this paper is to show the
impact of the PAS-based alignment module, we
will not compare our results with other RTE sys-
tems (In fact, the baseline system already outper-
forms the average accuracy score of the RTE-4
challenge).
The main data set used for testing here is the
RTE-4 data set with three-way annotations (500
entailment T-H pairs (E), 150 contradiction pairs
(C), and 350 unknown pairs (U)). The results on
RTE-3 data set (combination of the development
set and test set, in all, 822 E pairs, 161 C pairs,
and 617 U pairs) is also shown, although the origi-
nal annotation is two-way and the three-way anno-
tation was done by different researchers after the
challenge
7
.
We will first show the performance of the base-
line systems, followed by the results of our PAS-
based alignment module and its impact on the
whole task. After that, we will also give more de-
tailed analysis of our alignment module, according
to different lexical relatedness measurements.
7
The annotation of the development set was done by stu-
dents at Stanford, and the annotation of the test set was done
as double annotation by NIST assessors, followed by adjudi-
cation of disagreements. Answers were kept consistent with
the two-way decisions in the main task gold answer file.
788
5.1 Baselines
The baseline systems used here are based on over-
lapping ratio of words and syntactic dependencies
between T and H. For the word overlapping ratio,
we calculate the number of overlapping tokens be-
tween T and H and normalize it by dividing it by
the number of tokens in H. The syntactic depen-
dency overlapping ratio works similarly: we cal-
culate the number of overlapping syntactic depen-
dencies and divide it by the number of syntactic
dependencies in H, i.e. the same as the number
of tokens. Enlightened by the relatedness func-
tion, we also allow either FULL match (meaning
both the dependencies and the parent tokens are
matched), and NOTFULL match (meaning only the
dependencies are matched). Here we only use
string match between lemmas and syntactic de-
pendencies. Table 1 presents the performance of
the baseline system.
The results show that, even with the same clas-
sifier and the same feature model, with a proper
two-stage strategy, it can already achieve better
results than the three-way classification. Note
that, the first strategy is not so successful, and
that is the traditional two-way annotation of the
RTE task. Our explanation here is that the BoW
method (even with syntactic dependency features)
is based on overlapping information shared by T
and H, which essentially means the more informa-
tion they share, the more relevant they are, instead
of being more similar or the same. Therefore, for
the ?ECU ? E/CU? setting, methods based on
overlapping information are not the best choice,
while for ?ECU ? U/EC?, they are more ap-
propriate.
In addition, the upper bound numbers show the
accuracy when the first-stage classification is per-
fect, which give us an indication of how far we
could go. The lower upper bound for the second
strategy is mainly due to the low proportion of the
C cases (15%) in the data set; while the other two
both show large space for improvement.
5.2 The PAS-based Alignment Module
In this subsection, we present a separate evalua-
tion of our PAS-based alignment module. As we
mentioned before (cf. Section 4), there are sev-
eral parameters to be tuned in our alignment algo-
rithm: a) whether the relatedness function between
P-Trees asks for the FULL match; b) whether the
function for A-Trees asks for the FULL match; and
c) whether both P-Trees and A-Trees being related
are required or either of them holds is enough.
Since they are all binary values, we use the 3-digit
code to represent each setting, e.g. [FFO]
8
means
either P-Trees are FULL matched or A-Trees are
FULL matched. The performances of different set-
tings of the module are shown in the following
Precision-Recall figure 3,
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  
68
 
70
 
72
 
74
 
76
 
78
 
80
 
82
 
84
 
86
Recall (%)
Pre
cis
ion
 (%)
[FFA
][N
FA]
[FN
A]
[FFO
]
[NN
A]
[NF
O]
[FN
O]
[NN
O]
Figure 3: Precision and recall of different align-
ment settings
Since we will combine this module with the
baseline system and it will be integrated as the
first-stage classification, the F1 scores are not in-
dicative for selecting the best setting. Intuitively,
we may prefer higher precision than recall.
One limitation of our method we need to point
out here is that, if some important predicates or ar-
guments in H are not (correctly) identified by the
SRL system, fewer P-Trees and A-Trees are re-
quired to be related to some part of T, thus, the
relatedness of the whole pair could easily be satis-
fied, leading to false positive cases.
5.3 Impact on the Final Results
The best settings for RTE-3 data set is [NNA] and
for RTE-4 data set is [NFO], which are both in the
middle of the setting range shown in the previous
figure 3.
As for the integration of the PAS-based align-
ment model with our BoW-based baseline, we
only consider the third two-stage classification
strategy in Table 1. Other strategies would also be
interesting to try, however, the proposed alignment
algorithm exploits relatedness between T and H,
which might not be fine-grained enough to detect
8
F stands for FULL, and O stands for OR. Other letters
are, N stands for NOTFULL, and A stands for AND.
789
Strategies Three-Way Two-Stage
E/C/U E/CU ? E/C/U C/EU ? C/E/U U/EC ? U/E/C
Accuray 53.20% 50.00% 53.50% 54.20%
Upper Bound / 82.80% 68.70% 84.90%
Table 1: Performances of the Baselines
entailment or contradiction. New alignment algo-
rithm has to be designed to explore other strate-
gies. Thus, in this work, we believe that the align-
ment algorithm based on PAS (and other methods
based on overlapping information between T and
H) is suitable for the U/EC ? U/E/C classifi-
cation strategy.
Table 2 shows the final results.
The first observation is that the improvement of
accuracy on the first stage of the classification can
be preserved to the final results. And our PAS-
based alignment module can help, though there
is still large space for improvement. Compared
with the significantly improved results on RTE-4,
the improvement on RTE-3 is less obvious, mainly
due to the relatively lower precision (70.33% vs.
79.67%) of the alignment module itself.
Also, we have to say that the improvement is not
as big as we expected. There are several reasons
for this. Besides the limitation of our approach
mentioned in the previous section, the predicates
and arguments themselves might be too sparse to
convey all the information we need for the en-
tailment detection. In addition, in some sense,
the baseline is quite strong for this comparison,
since the PAS-based alignment module relies on
the overlapping words at the first place, there are
quite a few pairs solved by both the main approach
and the baseline. Then, it would be interesting
to take a closer look at the lexical resources used
in the main system, which is another additional
knowledge it has, comparing with the baseline.
5.4 Impact of the Lexical Resources
We did an ablation test of the lexical resources
used in our alignment module. Recall that we
have applied three lexical resources, VerbOcean
for the predicate relatedness function, WordNet
for the argument relatedness function, and Nor-
malized Google Distance for both. Table 3 shows
the performances of the system without each of the
resources,
The results clearly show that each lexical re-
source does contribute some improvement to the
final performance of the system and it confirms
the idea of combining lexical resources being ac-
quired in different ways. For instance, at the
beginning, we expected that the relationship be-
tween ?people? and ?children? could be captured
by WordNet, but in fact not. Fortunately, the NGD
has a quite low value of this pair of words (0.21),
which suggests that they occur together quite of-
ten, or in other words, they are relevant.
One interesting future work on this aspect is to
substitute the OR connector between these lexical
resources with an AND operator. Thus, instead of
using them to achieve a higher coverage, whether
they could be filters for each other to increase the
precision will also be interesting to know.
6 Conclusion and Future Work
In this paper, we address the motivation and issues
of casting the three-way RTE problem into a two-
stage binary classification task. We apply an SRL
system to derive the predicate-argument structure
of the input sentences, and propose ways of cal-
culating semantic relatedness between the shallow
semantic structures of T and H. The experiments
show improvements in the first-stage classifica-
tion, which accordingly contribute to the final re-
sults of the RTE task.
For future work, we would like to see whether
the PAS can help the second-stage classification
as well, e.g. the semantic dependency of negation
(AM-NEG) could be helpful for the contraction
recognition. Furthermore, since the PAS is usu-
ally a bag of unconnected graphs, we could find
a way to joint them together, in order to consider
both inter- and intra- sentential inferences based
on it.
In addition, this approach has the potential to
be integrated with other RTE modules. For in-
stance, for the predicate alignment, we may con-
sider to use DIRT rules (Lin and Pantel, 2001)
or other paraphrase resources (Callison-Burch,
2008), and for the argument alignment, exter-
nal named-entity recognizer and anaphora resolver
would be very helpful. Even more, we also plan to
compare/combine it with other methods which are
not based on overlapping information between T
and H.
790
Systems Baseline1 Baseline2 SRL+Baseline2 The First Stage
Data Sets Three-Way Two-Stage Two-Stage Baseline2 SRL+Baseline2 SRL
RTE-3 [NNA] 52.19% 52.50% 53.69%(2.87%?) 59.50% 60.56%(1.78%?) 70.33%
RTE-4 [NFO] 53.20% 54.20% 56.60%(6.39%?) 67.10% 70.20%(4.62%?) 79.67%
Table 2: Results on the Whole Datasets
Data Sets SRL+Baseline SRL+Baseline - VO SRL+Baseline - NGD SRL+Baseline - WN
RTE-3 [NNA] 53.69% 53.19%(0.93%?) 53.50%(0.35%?) 52.88%(1.51%?)
RTE-4 [NFO] 56.60% 56.00%(1.06%?) 56.10%(0.88%?) 55.70%(1.59%?)
Table 3: Impact of the Lexical Resources
Acknowledgments
The first author is funded by the PIRE PhD
scholarship program sponsored by the German
Research Foundation (DFG). The second author
thanks the German Excellence Cluster of Multi-
modal Computing and Interaction for the support
of the work.
References
Eugene Agichtein, Walt Askew, and Yandong Liu.
2009. Combining Lexical, Syntactic, and Semantic
Evidence for Textual Entailment Classification. In
Proceedings of the First Text Analysis Conference
(TAC 2008).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
Volume 1, pages 86?90, Montreal, Canada.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, Prague, Czech
Republic.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04), Barcelona, Spain.
Rudi Cilibrasi and Paul M. B. Vitanyi. 2007. The
Google Similarity Distance. IEEE/ACM Trans-
actions on Knowledge and Data Engineering,
19(3):370?383.
Peter Clark and Phil Harrison. 2009. Recognizing
Textual Entailment with Logical Inference. In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Machine Learning Challenges,
volume 3944 of Lecture Notes in Computer Science,
pages 177?190. Springer.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2009. The Fourth PASCAL Recognizing Textual
Entailment Challenge. In Proceedings of the First
Text Analysis Conference (TAC 2008).
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning, Boulder, CO, USA.
Sanda Harabagiu and Andrew Hickl. 2006. Meth-
ods for Using Textual Entailment in Open-Domain
Question Answering. In Proceedings of COLING-
ACL 2006, pages 905?912, Sydney, Australia.
Dekang Lin and Patrick Pantel. 2001. DIRT - Dis-
covery of Inference Rules from Text. In In Proceed-
ings of the ACM SIGKDD Conference on Knowledge
Discovery and Data Mining.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP 2008.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of hlt-emnlp 2005, pages 523?530, Vancouver,
Canada.
Sebastian Pado, Marie-Catherine de Marneffe, Bill
MacCartney, Anna N. Rafferty, Eric Yeh, and
791
Christopher D. Manning. 2009. Deciding en-
tailment and contradiction with stochastic and edit
distance-based alignment. In Proceedings of the
First Text Analysis Conference (TAC 2008).
Reda Siblini and Leila Kosseim. 2009. Using Ontol-
ogy Alignment for the TAC RTE Challenge. In Pro-
ceedings of the First Text Analysis Conference (TAC
2008).
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th conference on computational natural lan-
guage learning (CoNLL-2008), Manchester, UK.
Rui Wang and G?unter Neumann. 2007. Recog-
nizing textual entailment using a subsequence ker-
nel method. In Proceedings of the Twenty-Second
AAAI Conference on Artificial Intelligence (AAAI-
07), pages 937?942, Vancouver, Canada.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning (CoNLL 2008), pages 198?202,
Manchester, UK.
792
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 211?219,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Inference Rules and their Application to Recognizing Textual Entailment
Georgiana Dinu
Saarland University
Campus, D-66123 Saarbru?cken
dinu@coli.uni-sb.de
Rui Wang
Saarland University
Campus, D-66123 Saarbru?cken
rwang@coli.uni-sb.de
Abstract
In this paper, we explore ways of improv-
ing an inference rule collection and its ap-
plication to the task of recognizing textual
entailment. For this purpose, we start with
an automatically acquired collection and
we propose methods to refine it and ob-
tain more rules using a hand-crafted lex-
ical resource. Following this, we derive
a dependency-based structure representa-
tion from texts, which aims to provide a
proper base for the inference rule appli-
cation. The evaluation of our approach
on the recognizing textual entailment data
shows promising results on precision and
the error analysis suggests possible im-
provements.
1 Introduction
Textual inference plays an important role in many
natural language processing (NLP) tasks. In recent
years, the recognizing textual entailment (RTE)
(Dagan et al, 2006) challenge, which focuses on
detecting semantic inference, has attracted a lot of
attention. Given a text T (several sentences) and a
hypothesis H (one sentence), the goal is to detect
if H can be inferred from T.
Studies such as (Clark et al, 2007) attest that
lexical substitution (e.g. synonyms, antonyms) or
simple syntactic variation account for the entail-
ment only in a small number of pairs. Thus, one
essential issue is to identify more complex expres-
sions which, in appropriate contexts, convey the
same (or similar) meaning. However, more gener-
ally, we are also interested in pairs of expressions
in which only a uni-directional inference relation
holds1.
1We will use the term inference rule to stand for such con-
cept; the two expressions can be actual paraphrases if the re-
lation is bi-directional
A typical example is the following RTE pair in
which accelerate to in H is used as an alternative
formulation for reach speed of in T.
T: The high-speed train, scheduled for a trial run on Tues-
day, is able to reach a maximum speed of up to 430 kilome-
ters per hour, or 119 meters per second.
H: The train accelerates to 430 kilometers per hour.
One way to deal with textual inference is
through rule representation, for example X wrote
Y ? X is author of Y. However, manually building
collections of inference rules is time-consuming
and it is unlikely that humans can exhaustively
enumerate all the rules encoding the knowledge
needed in reasoning with natural language. In-
stead, an alternative is to acquire these rules au-
tomatically from large corpora. Given such a rule
collection, the next step to focus on is how to suc-
cessfully use it in NLP applications. This paper
tackles both aspects, acquiring inference rules and
using them for the task of recognizing textual en-
tailment.
For the first aspect, we extend and refine an ex-
isting collection of inference rules acquired based
on the Distributional Hypothesis (DH). One of the
main advantages of using the DH is that the only
input needed is a large corpus of (parsed) text2.
For the extension and refinement, a hand-crafted
lexical resource is used for augmenting the origi-
nal inference rule collection and exclude some of
the incorrect rules.
For the second aspect, we focus on applying
these rules to the RTE task. In particular, we use
a structure representation derived from the depen-
dency parse trees of T and H, which aims to cap-
ture the essential information they convey.
The rest of the paper is organized as follows:
Section 2 introduces the inference rule collection
2Another line of work on acquiring paraphrases uses com-
parable corpora, for instance (Barzilay and McKeown, 2001),
(Pang et al, 2003)
211
we use, based on the Discovery of Inference Rules
from Text (henceforth DIRT) algorithm and dis-
cusses previous work on applying it to the RTE
task. Section 3 focuses on the rule collection it-
self and on the methods in which we use an exter-
nal lexical resource to extend and refine it. Sec-
tion 4 discusses the application of the rules for the
RTE data, describing the structure representation
we use to identify the appropriate context for the
rule application. The experimental results will be
presented in Section 5, followed by an error analy-
sis and discussions in Section 6. Finally Section 7
will conclude the paper and point out future work
directions.
2 Background
A number of automatically acquired inference
rule/paraphrase collections are available, such as
(Szpektor et al, 2004), (Sekine, 2005). In our
work we use the DIRT collection because it is the
largest one available and it has a relatively good
accuracy (in the 50% range for top generated para-
phrases, (Szpektor et al, 2007)). In this section,
we describe the DIRT algorithm for acquiring in-
ference rules. Following that, we will overview
the RTE systems which take DIRT as an external
knowledge resource.
2.1 Discovery of Inference Rules from Text
The DIRT algorithm has been introduced by (Lin
and Pantel, 2001) and it is based on what is called
the Extended Distributional Hypothesis. The orig-
inal DH states that words occurring in similar
contexts have similar meaning, whereas the ex-
tended version hypothesizes that phrases occur-
ring in similar contexts are similar.
An inference rule in DIRT is a pair of binary
relations ? pattern1(X,Y ), pattern2(X,Y ) ?
which stand in an inference relation. pattern1 and
pattern2 are chains in dependency trees3 while X
and Y are placeholders for nouns at the end of this
chain. The two patterns will constitute a candi-
date paraphrase if the sets of X and Y values ex-
hibit relevant overlap. In the following example,
the two patterns are prevent and provide protection
against.
X
subj
???? prevent
obj
??? Y
X
subj
???? provide
obj
??? protection
mod
???? against
pcomp
????? Y
3obtained with the Minipar parser (Lin, 1998)
X put emphasis on Y
? X pay attention to Y
? X attach importance to Y
? X increase spending on Y
? X place emphasis on Y
? Y priority of X
? X focus on Y
Table 1: Example of DIRT algorithm output. Most
confident paraphrases of X put emphasis on Y
Such rules can be informally defined (Szpek-
tor et al, 2007) as directional relations between
two text patterns with variables. The left-hand-
side pattern is assumed to entail the right-hand-
side pattern in certain contexts, under the same
variable instantiation. The definition relaxes the
intuition of inference, as we only require the en-
tailment to hold in some and not all contexts, mo-
tivated by the fact that such inferences occur often
in natural text.
The algorithm does not extract directional in-
ference rules, it can only identify candidate para-
phrases; many of the rules are however uni-
directional. Besides syntactic rewriting or lexi-
cal rules, rules in which the patterns are rather
complex phrases are also extracted. Some of the
rules encode lexical relations which can also be
found in resources such as WordNet while oth-
ers are lexical-syntactic variations that are unlikely
to occur in hand-crafted resources (Lin and Pan-
tel, 2001). Table 1 gives a few examples of rules
present in DIRT4.
Current work on inference rules focuses on
making such resources more precise. (Basili et
al., 2007) and (Szpektor et al, 2008) propose at-
taching selectional preferences to inference rules.
These are semantic classes which correspond to
the anchor values of an inference rule and have
the role of making precise the context in which the
rule can be applied 5. This aspect is very impor-
tant and we plan to address it in our future work.
However in this paper we investigate the first and
more basic issue: how to successfully use rules in
their current form.
4For simplification, in the rest of the paper we will omit
giving the dependency relations in a pattern.
5For example X won Y entails X played Y only when Y
refers to some sort of competition, but not if Y refers to a
musical instrument.
212
2.2 Related Work
Intuitively such inference rules should be effective
for recognizing textual entailment. However, only
a small number of systems have used DIRT as a re-
source in the RTE-3 challenge, and the experimen-
tal results have not fully shown it has an important
contribution.
In (Clark et al, 2007)?s approach, semantic
parsing to clause representation is performed and
true entailment is decided only if every clause
in the semantic representation of T semantically
matches some clause in H. The only variation al-
lowed consists of rewritings derived from Word-
Net and DIRT. Given the preliminary stage of this
system, the overall results show very low improve-
ment over a random classification baseline.
(Bar-Haim et al, 2007) implement a proof
system using rules for generic linguistic struc-
tures, lexical-based rules, and lexical-syntactic
rules (these obtained with a DIRT-like algorithm
on the first CD of the Reuters RCV1 corpus). The
entailment considers not only the strict notion of
proof but also an approximate one. Given premise
p and hypothesis h, the lexical-syntactic compo-
nent marks all lexical noun alignments. For ev-
ery pair of alignment, the paths between the two
nouns are extracted, and the DIRT algorithm is
applied to obtain a similarity score. If the score
is above a threshold the rule is applied. However
these lexical-syntactic rules are only used in about
3% of the attempted proofs and in most cases there
is no lexical variation.
(Iftene and Balahur-Dobrescu, 2007) use DIRT
in a more relaxed manner. A DIRT rule is em-
ployed in the system if at least one of the anchors
match in T and H, i.e. they use them as unary
rules. However, the detailed analysis of the sys-
tem that they provide shows that the DIRT com-
ponent is the least relevant one (adding 0.4% of
precision).
In (Marsi et al, 2007), the focus is on the use-
fulness of DIRT. In their system a paraphrase sub-
stitution step is added on top of a system based on
a tree alignment algorithm. The basic paraphrase
substitution method follows three steps. Initially,
the two patterns of a rule are matched in T and
H (instantiations of the anchors X , Y do not have
to match). The text tree is transformed by apply-
ing the paraphrase substitution. Following this,
the transformed text tree and hypothesis trees are
aligned. The coverage (proportion of aligned con-
X write Y ?X author Y
X, founded in Y ?X, opened in Y
X launch Y ? X produce Y
X represent Z ? X work for Y
death relieved X? X died
X faces menace from Y ? X endangered by Y
X, peace agreement for Y
? X is formulated to end war in Y
Table 2: Example of inference rules needed in
RTE
tent words) is computed and if above some thresh-
old, entailment is true. The paraphrase compo-
nent adds 1.0% to development set results and only
0.5% to test sets, but a more detailed analysis on
the results of the interaction with the other system
components is not given.
3 Extending and refining DIRT
Based on observations of using the inference rule
collection on the real data, we discover that 1)
some of the needed rules still lack even in a very
large collection such as DIRT and 2) some system-
atic errors in the collection can be excluded. On
both aspects, we use WordNet as additional lexi-
cal resource.
Missing Rules
A closer look into the RTE data reveals that
DIRT lacks many of the rules that entailment pairs
require.
Table 2 lists a selection of such rules. The
first rows contain rules which are structurally very
simple. These, however, are missing from DIRT
and most of them also from other hand-crafted re-
sources such as WordNet (i.e. there is no short
path connecting the two verbs). This is to be ex-
pected as they are rules which hold in specific con-
texts, but difficult to be captured by a sense dis-
tinction of the lexical items involved.
The more complex rules are even more difficult
to capture with a DIRT-like algorithm. Some of
these do not occur frequently enough even in large
amounts of text to permit acquiring them via the
DH.
Combining WordNet and DIRT
In order to address the issue of missing rules,
we investigate the effects of combining DIRT with
an exact hand-coded lexical resource in order to
create new rules.
For this we extended the DIRT rules by adding
213
X face threat of Y
? X at risk of Y
face
? confront, front, look, face up
threat
? menace, terror, scourge
risk
? danger, hazard, jeopardy,
endangerment, peril
Table 3: Lexical variations creating new rules
based on DIRT rule X face threat of Y ? X at risk
of Y
rules in which any of the lexical items involved
in the patterns can be replaced by WordNet syn-
onyms. In the example above, we consider the
DIRT rule X face threat of Y ? X, at risk of Y
(Table 3).
Of course at this moment due to the lack of
sense disambiguation, our method introduces lots
of rules that are not correct. As one can see, ex-
pressions such as front scourge do not make any
sense, therefore any rules containing this will be
incorrect. However some of the new rules created
in this example, such as X face threat of Y ? X,
at danger of Y are reasonable ones and the rules
which are incorrect often contain patterns that are
very unlikely to occur in natural text.
The idea behind this is that a combination of
various lexical resources is needed in order to
cover the vast variety of phrases which humans
can judge to be in an inference relation.
The method just described allows us to identify
the first four rules listed in Table 2. We also ac-
quire the rule X face menace of Y ? X endangered
by Y (via X face threat of Y ? X threatened by Y,
menace ? threat, threaten ? endanger).
Our extension is application-oriented therefore
it is not intended to be evaluated as an independent
rule collection, but in an application scenario such
as RTE (Section 6).
In our experiments we also made a step towards
removing the most systematic errors present in
DIRT. DH algorithms have the main disadvantage
that not only phrases with the same meaning are
extracted but also phrases with opposite meaning.
In order to overcome this problem and since
such errors are relatively easy to detect, we ap-
plied a filter to the DIRT rules. This eliminates
inference rules which contain WordNet antonyms.
For such a rule to be eliminated the two patterns
have to be identical (with respect to edge labels
and content words) except from the antonymous
words; an example of a rule eliminated this way is
X have confidence in Y ? X lack confidence in Y.
As pointed out by (Szpektor et al, 2007) a thor-
ough evaluation of a rule collection is not a trivial
task; however due to our methodology we can as-
sume that the percentage of rules eliminated this
way that are indeed contradictions gets close to
100%.
4 Applying DIRT on RTE
In this section we point out two issues that are en-
countered when applying inference rules for tex-
tual entailment. The first issue is concerned with
correctly identifying the pairs in which the knowl-
edge encoded in these rules is needed. Follow-
ing this, another non-trivial task is to determine
the way this knowledge interacts with the rest of
information conveyed in an entailment pair. In or-
der to further investigate these issues, we apply the
rule collection on a dependency-based representa-
tion of text and hypothesis, namely Tree Skeleton.
4.1 Observations
A straightforward experiment can reveal the num-
ber of pairs in the RTE data which contain rules
present in DIRT. For all the experiments in this pa-
per, we use the DIRT collection provided by (Lin
and Pantel, 2001), derived from the DIRT algo-
rithm applied on 1GB of news text. The results
we report here use only the most confident rules
amounting to more than 4 million rules (top 40 fol-
lowing (Lin and Pantel, 2001)).6
Following the definition of an entail-
ment rule, we identify RTE pairs in which
pattern1(w1, w2) and pattern2(w1, w2) are
matched one in T and the other one in H and
?pattern1(X,Y ), pattern2(X,Y )? is an infer-
ence rule. The pair bellow is an example of this.
T: The sale was made to pay Yukos US$ 27.5 billion tax
bill, Yuganskneftegaz was originally sold for US$ 9.4 bil-
lion to a little known company Baikalfinansgroup which was
later bought by the Russian state-owned oil company Ros-
neft.
H: Baikalfinansgroup was sold to Rosneft.
6Another set of experiments showed that for this particu-
lar task, using the entire collection instead of a subset gave
similar results.
214
On average, only 2% of the pairs in the RTE
data is subject to the application of such inference
rules. Out of these, approximately 50% are lexical
rules (one verb entailing the other). Out of these
lexical rules, around 50% are present in WordNet
in a synonym, hypernym or sister relation. At a
manual analysis, close to 80% of these are correct
rules; this is higher than the estimated accuracy of
DIRT, probably due to the bias of the data which
consists of pairs which are entailment candidates.
However, given the small number of inference
rules identified this way, we performed another
analysis. This aims at determining an upper
bound of the number of pairs featuring entailment
phrases present in a collection. Given DIRT and
the RTE data, we compute in how many pairs
the two patterns of a paraphrase can be matched
irrespective of their anchor values. An example is
the following pair,
T: Libya?s case against Britain and the US concerns the
dispute over their demand for extradition of Libyans charged
with blowing up a Pan Am jet over Lockerbie in 1988.
H: One case involved the extradition of Libyan suspects
in the Pan Am Lockerbie bombing.
This is a case in which the rule is correct and
the entailment is positive. In order to determine
this, a system will have to know that Libya?s case
against Britain and the US in T entails one case
in H. Similarly, in this context, the dispute over
their demand for extradition of Libyans charged
with blowing up a Pan Am jet over Lockerbie in
1988 in T can be replaced with the extradition of
Libyan suspects in the Pan Am Lockerbie bombing
preserving the meaning.
Altogether in around 20% of the pairs, patterns
of a rule can be found this way, many times with
more than one rule found in a pair. However, in
many of these pairs, finding the patterns of an in-
ference rule does not imply that the rule is truly
present in that pair.
Considering a system is capable of correctly
identifying the cases in which an inference rule
is needed, subsequent issues arise from the way
these fragments of text interact with the surround-
ing context. Assuming we have a correct rule
present in an entailment pair, the cases in which
the pair is still not a positive case of entailment
can be summarized as follows:
? The entailment rule is present in parts of the
text which are not relevant to the entailment
value of the pair.
? The rule is relevant, however the sentences
in which the patterns are embedded block the
entailment (e.g. through negative markers,
modifiers, embedding verbs not preserving
entailment)7
? The rule is correct in a limited number of con-
texts, but the current context is not the correct
one.
To sum up, making use of the knowledge en-
coded with such rules is not a trivial task. If rules
are used strictly in concordance with their defini-
tion, their utility is limited to a very small number
of entailment pairs. For this reason, 1) instead of
forcing the anchor values to be identical as most
previous work, we allow more flexible rule match-
ing (similar to (Marsi et al, 2007)) and 2) fur-
thermore, we control the rule application process
using a text representation based on dependency
structure.
4.2 Tree Skeleton
The Tree Skeleton (TS) structure was proposed by
(Wang and Neumann, 2007), and can be viewed
as an extended version of the predicate-argument
structure. Since it contains not only the predi-
cate and its arguments, but also the dependency
paths in-between, it captures the essential part of
the sentence.
Following their algorithm, we first preprocess
the data using a dependency parser8 and then
select overlapping topic words (i.e. nouns) in T
and H. By doing so, we use fuzzy match at the
substring level instead of full match. Starting
with these nouns, we traverse the dependency
tree to identify the lowest common ancestor node
(named as root node). This sub-tree without the
inner yield is defined as a Tree Skeleton. Figure
1 shows the TS of T of the following positive
example,
T For their discovery of ulcer-causing bacteria, Aus-
tralian doctors Robin Warren and Barry Marshall have re-
ceived the 2005 Nobel Prize in Physiology or Medicine.
H Robin Warren was awarded a Nobel Prize.
Notice that, in order to match the inference rules
with two anchors, the number of the dependency
7See (Nairn et al, 2006) for a detailed analysis of these
aspects.
8Here we also use Minipar for the reason of consistence
215
Figure 1: Dependency structure of text. Tree
skeleton in bold
paths contained in a TS should also be two. In
practice, among all the 800 T-H pairs of the RTE-
2 test set, we successfully extracted tree skeletons
in 296 text pairs, i.e., 37% of the test data is cov-
ered by this step and results on other data sets are
similar.
Applying DIRT on a TS
Dependency representations like the tree skele-
ton have been explored by many researchers, e.g.
(Zanzotto and Moschitti, 2006) have utilized a tree
kernel method to calculate the similarity between
T and H, and (Wang and Neumann, 2007) chose
subsequence kernel to reduce the computational
complexity. However, the focus of this paper is to
evaluate the application of inference rules on RTE,
instead of exploring methods of tackling the task
itself. Therefore, we performed a straightforward
matching algorithm to apply the inference rules
on top of the tree skeleton structure. Given tree
skeletons of T and H, we check if the two left de-
pendency paths, the two right ones or the two root
nodes contain the patterns of a rule.
In the example above, the rule X obj???
receive
subj
???? Y ? X
obj2
???? award
obj1
???? Y satisfies
this criterion, as it is matched at the root nodes.
Notice that the rule is correct only in restricted
contexts, in which the object of receive is some-
thing which is conferred on the basis of merit.
However in this pair, the context is indeed the cor-
rect one.
5 Experiments
Our experiments consist in predicting positive en-
tailment in a very straightforward rule-based man-
ner (Table 4 summarizes the results using three
different rule collections). For each collection we
select the RTE pairs in which we find a tree skele-
ton and match an inference rule. The first number
in our table entries represents how many of such
pairs we have identified, out the 1600 of devel-
opment and test pairs. For these pairs we simply
predict positive entailment and the second entry
represents what percentage of these pairs are in-
deed positive entailment. Our work does not fo-
cus on building a complete RTE system; however,
we also combine our method with a bag of words
baseline to see the effects on the whole data set.
5.1 Results on a subset of the data
In the first two columns (DirtTS and Dirt+WNTS)
we consider DIRT in its original state and DIRT
with rules generated with WordNet as described
in Section 3; all precisions are higher than 67%9.
After adding WordNet, approximately in twice as
many pairs, tree skeletons and rules are matched,
while the precision is not harmed. This may in-
dicate that our method of adding rules does not
decrease precision of an RTE system.
In the third column we report the results of us-
ing a set of rules containing only the trivial iden-
tity ones (IdTS). For our current system, this can
be seen as a precision upper bound for all the
other collections, in concordance with the fact that
identical rules are nothing but inference rules of
highest possible confidence. The fourth column
(Dirt+Id+WNTS) contains what can be consid-
ered our best setting. In this setting considerably
more pairs are covered using a collection contain-
ing DIRT and identity rules with WordNet exten-
sion.
Although the precision results with this setting
are encouraging (65% for RTE2 data and 72% for
RTE3 data), the coverage is still low, 8% for RTE2
and 6% for RTE3. This aspect together with an er-
ror analysis we performed are the focus of Section
7.
The last column (Dirt+Id+WN) gives the preci-
sion we obtain if we simply decide a pair is true
entailment if we have an inference rule matched in
it (irrespective of the values of the anchors or of
the existence of tree skeletons). As expected, only
identifying the patterns of a rule in a pair irrespec-
tive of tree skeletons does not give any indication
of the entailment value of the pair.
9The RTE task is considered to be difficult. The aver-
age accuracy of the systems in the RTE-3 challenge is around
61% (Giampiccolo et al, 2007)
216
RTE Set DirtTS Dirt + WNTS IdTS Dirt + Id + WNTS Dirt + Id + WN
RTE2 49/69.38 94/67.02 45/66.66 130/65.38 673/50.07
RTE3 42/69.04 70/70.00 29/79.31 93/72.05 661/55.06
Table 4: Coverage/precision with various rule collections
RTE Set BoW Main
RTE2 (85 pairs) 51.76% 60.00%
RTE3 (64 pairs) 54.68% 62.50%
Table 5: Precision on the covered RTE data
RTE Set (800 pairs) BoW Main & BoW
RTE2 56.87% 57.75%
RTE3 61.12% 61.75%
Table 6: Precision on full RTE data
5.2 Results on the entire data
At last, we also integrate our method with a bag
of words baseline, which calculates the ratio of
overlapping words in T and H. For the pairs that
our method covers, we overrule the baseline?s de-
cision. The results are shown in Table 6 (Main
stands for the Dirt + Id + WNTS configuration).
On the full data set, the improvement is still small
due to the low coverage of our method, however
on the pairs that are covered by our method (Ta-
ble 5), there is a significant improvement over the
overlap baseline.
6 Discussion
In this section we take a closer look at the data in
order to better understand how does our method
of combining tree skeletons and inference rules
work. We will first perform error analysis on what
we have considered our best setting so far. Fol-
lowing this, we analyze data to identify the main
reasons which cause the low coverage.
For error analysis we consider the pairs incor-
rectly classified in the RTE3 test data set, consist-
ing of a total of 25 pairs. We classify the errors
into three main categories: rule application errors,
inference rule errors, and other errors (Table 7).
In the first category, the tree skeleton fails to
match the corresponding anchors of the inference
rules. For instance, if someone founded the Insti-
tute of Mathematics (Instituto di Matematica) at
the University of Milan, it does not follow that they
founded The University of Milan. The Institute of
Mathematics should be aligned with the Univer-
sity of Milan, which should avoid applying the in-
ference rule for this pair.
A rather small portion of the errors (16%) are
caused by incorrect inference rules. Out of these,
two are correct in some contexts but not in the en-
tailment pairs in which they are found. For exam-
ple, the following rule X generate Y ? X earn Y is
used incorrectly, however in the restricted context
of money or income, the two verbs have similar
meaning. An example of an incorrect rule is X is-
sue Y ? X hit Y since it is difficult to find a context
in which this holds.
The last category contains all the other errors.
In all these cases, the additional information con-
veyed by the text or the hypothesis which cannot
be captured by our current approach, affects the
entailment. For example an imitation diamond is
not a diamond, and more than 1,000 members
of the Russian and foreign media does not entail
more than 1,000 members from Russia; these are
not trivial, since lexical semantics and fine-grained
analysis of the restrictors are needed.
For the second part of our analysis we discuss
the coverage issue, based on an analysis of uncov-
ered pairs. A main factor in failing to detect pairs
in which entailment rules should be applied is the
fact that the tree skeleton does not find the corre-
sponding lexical items of two rule patterns.
Issues will occur even if the tree skeleton struc-
ture is modified to align all the corresponding frag-
ments together. Consider cases such as threaten to
boycott and boycott or similar constructions with
other embedding verbs such as manage, forget, at-
tempt. Our method can detect if the two embedded
verbs convey a similar meaning, however not how
the embedding verbs affect the implication.
Independent of the shortcomings of our tree
skeleton structure, a second factor in failing to de-
tect true entailment still lies in lack of rules. For
instance, the last two examples in Table 2 are en-
tailment pair fragments which can be formulated
as inference rules, but it is not straightforward to
acquire them via the DH.
217
Source of error % pairs
Incorrect rule application 32%
Incorrect inference rules 16%
Other errors 52%
Table 7: Error analysis
7 Conclusion
Throughout the paper we have identified impor-
tant issues encountered in using inference rules for
textual entailment and proposed methods to solve
them. We explored the possibility of combin-
ing a collection obtained in a statistical, unsuper-
vised manner, DIRT, with a hand-crafted lexical
resource in order to make inference rules have a
larger contribution to applications. We also inves-
tigated ways of effectively applying these rules.
The experiment results show that although cover-
age is still not satisfying, the precision is promis-
ing. Therefore our method has the potential to be
successfully integrated in a larger entailment de-
tection framework.
The error analysis points out several possible
future directions. The tree skeleton representation
we used needs to be enhanced in order to capture
more accurately the relevant fragments of the text.
A different issue remains the fact that a lot of rules
we could use for textual entailment detection are
still lacking. A proper study of the limitations of
the DH as well as a classification of the knowledge
we want to encode as inference rules would be a
step forward towards solving this problem.
Furthermore, although all the inference rules we
used aim at recognizing positive entailment cases,
it is natural to use them for detecting negative
cases of entailment as well. In general, we can
identify pairs in which the patterns of an inference
rule are present but the anchors are mismatched, or
they are not the correct hypernym/hyponym rela-
tion. This can be the base of a principled method
for detecting structural contradictions (de Marn-
effe et al, 2008).
8 Acknowledgments
We thank Dekang Lin and Patrick Pantel for
providing the DIRT collection and to Grzegorz
Chrupa?a, Alexander Koller, Manfred Pinkal and
Stefan Thater for very useful discussions. Geor-
giana Dinu and Rui Wang are funded by the IRTG
and PIRE PhD scholarship programs.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpek-
tor, and Moshe Friedman. 2007. Semantic inference
at the lexical-syntactic level for textual entailment
recognition. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 131?136, Prague, June. Association for Com-
putational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of 39th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 50?57,
Toulouse, France, July. Association for Computa-
tional Linguistics.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
In Proceedings of RANLP, Borovets, Bulgaria.
Peter Clark, Phil Harrison, John Thompson, William
Murray, Jerry Hobbs, and Christiane Fellbaum.
2007. On the role of lexical and world knowledge
in rte3. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
54?59, Prague, June. Association for Computational
Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
Vol. 3944, Springer, pages 177?190. Quionero-
Candela, J.; Dagan, I.; Magnini, B.; d?Alch-Buc, F.
Machine Learning Challenges.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047, Columbus, Ohio, June. Association for
Computational Linguistics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 1?9, Prague, June. Association
for Computational Linguistics.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, pages 125?130,
Prague, June. Association for Computational Lin-
guistics.
Dekang Lin and Patrick Pantel. 2001. Dirt. discov-
ery of inference rules from text. In KDD ?01: Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 323?328, New York, NY, USA. ACM.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of
Parsing Systems, Granada.
218
Erwin Marsi, Emiel Krahmer, and Wauter Bosma.
2007. Dependency-based paraphrasing for recog-
nizing textual entailment. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 83?88, Prague, June. Associa-
tion for Computational Linguistics.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5 (Inference in Com-
putational Semantics, Buxton, UK.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In HLT-NAACL, pages 102?109.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs.
In Proceedings of International Workshop on Para-
phrase, pages 80?87, Jeju Island, Korea.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In In Proceedings of
EMNLP, pages 41?48.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT, pages 683?691, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Rui Wang and Gu?nter Neumann. 2007. Recognizing
textual entailment using sentence similarity based on
dependency tree skeletons. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 36?41, Prague, June. Associa-
tion for Computational Linguistics.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 401?408, Morristown, NJ, USA. Association
for Computational Linguistics.
219
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 378?386,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Cross-Domain Dependency Parsing Using a Deep Linguistic Grammar
Yi Zhang
LT-Lab, DFKI GmbH and
Dept of Computational Linguistics
Saarland University
D-66123 Saarbru?cken, Germany
yzhang@coli.uni-sb.de
Rui Wang
Dept of Computational Linguistics
Saarland University
66123 Saarbru?cken, Germany
rwang@coli.uni-sb.de
Abstract
Pure statistical parsing systems achieves
high in-domain accuracy but performs
poorly out-domain. In this paper, we
propose two different approaches to pro-
duce syntactic dependency structures us-
ing a large-scale hand-crafted HPSG gram-
mar. The dependency backbone of an
HPSG analysis is used to provide general
linguistic insights which, when combined
with state-of-the-art statistical dependency
parsing models, achieves performance im-
provements on out-domain tests.?
1 Introduction
Syntactic dependency parsing is attracting more
and more research focus in recent years, par-
tially due to its theory-neutral representation, but
also thanks to its wide deployment in various
NLP tasks (machine translation, textual entailment
recognition, question answering, information ex-
traction, etc.). In combination with machine learn-
ing methods, several statistical dependency pars-
ing models have reached comparable high parsing
accuracy (McDonald et al, 2005b; Nivre et al,
2007b). In the meantime, successful continuation
of CoNLL Shared Tasks since 2006 (Buchholz and
Marsi, 2006; Nivre et al, 2007a; Surdeanu et al,
2008) have witnessed how easy it has become to
train a statistical syntactic dependency parser pro-
vided that there is annotated treebank.
While the dissemination continues towards var-
ious languages, several issues arise with such
purely data-driven approaches. One common
observation is that statistical parser performance
drops significantly when tested on a dataset differ-
ent from the training set. For instance, when using
?The first author thanks the German Excellence Cluster
of Multimodal Computing and Interaction for the support of
the work. The second author is funded by the PIRE PhD
scholarship program.
the Wall Street Journal (WSJ) sections of the Penn
Treebank (Marcus et al, 1993) as training set, tests
on BROWN Sections typically result in a 6-8%
drop in labeled attachment scores, although the av-
erage sentence length is much shorter in BROWN
than that in WSJ. The common interpretation is
that the test set is heterogeneous to the training set,
hence in a different ?domain? (in a loose sense).
The typical cause of this is that the model overfits
the training domain. The concerns over random
choice of training corpus leading to linguistically
inadequate parsing systems increase over time.
While the statistical revolution in the field
of computational linguistics gaining high pub-
licity, the conventional symbolic grammar-based
parsing approaches have undergone a quiet pe-
riod of development during the past decade, and
reemerged very recently with several large scale
grammar-driven parsing systems, benefiting from
the combination of well-established linguistic the-
ories and data-driven stochastic models. The ob-
vious advantage of such systems over pure statis-
tical parsers is their usage of hand-coded linguis-
tic knowledge irrespective of the training data. A
common problem with grammar-based parser is
the lack of robustness. Also it is difficult to de-
rive grammar compatible annotations to train the
statistical components.
2 Parser Domain Adaptation
In recent years, two statistical dependency parsing
systems, MaltParser (Nivre et al, 2007b) and
MSTParser (McDonald et al, 2005b), repre-
senting different threads of research in data-driven
machine learning approaches have obtained high
publicity, for their state-of-the-art performances in
open competitions such as CoNLL Shared Tasks.
MaltParser follows the transition-based ap-
proach, where parsing is done through a series
of actions deterministically predicted by an oracle
model. MSTParser, on the other hand, follows
378
the graph-based approach where the best parse
tree is acquired by searching for a spanning tree
which maximizes the score on either a partially
or a fully connected graph with all words in the
sentence as nodes (Eisner, 1996; McDonald et al,
2005b).
As reported in various evaluation competitions,
the two systems achieved comparable perfor-
mances. More recently, approaches of combining
these two parsers achieved even better dependency
accuracy (Nivre and McDonald, 2008). Granted
for the differences between their approaches, both
systems heavily rely on machine learning methods
to estimate the parsing model from an annotated
corpus as training set. Due to the heavy cost of
developing high quality large scale syntactically
annotated corpora, even for a resource-rich lan-
guage like English, only very few of them meets
the criteria for training a general purpose statisti-
cal parsing model. For instance, the text style of
WSJ is newswire, and most of the sentences are
statements. Being lack of non-statements in the
training data could cause problems, when the test-
ing data contain many interrogative or imperative
sentences as in the BROWN corpus. Therefore, the
unbalanced distribution of linguistic phenomena
in the training data leads to inadequate parser out-
put structures. Also, the financial domain specific
terminology seen in WSJ can skew the interpreta-
tion of daily life sentences seen in BROWN.
There has been a substantial amount of work on
parser adaptation, especially from WSJ to BROWN.
Gildea (2001) compared results from different
combinations of the training and testing data to
demonstrate that the size of the feature model
can be reduced via excluding ?domain-dependent?
features, while the performance could still be pre-
served. Furthermore, he also pointed out that if the
additional training data is heterogeneous from the
original one, the parser will not obtain a substan-
tially better performance. Bacchiani et al (2006)
generalized the previous approaches using a maxi-
mum a posteriori (MAP) framework and proposed
both supervised and unsupervised adaptation of
statistical parsers. McClosky et al (2006) and Mc-
Closky et al (2008) have shown that out-domain
parser performance can be improved with self-
training on a large amount of unlabeled data. Most
of these approaches focused on the machine learn-
ing perspective instead of the linguistic knowledge
embraced in the parsers. Little study has been re-
ported on approaches of incorporating linguistic
features to make the parser less dependent on the
nature of training and testing dataset, without re-
sorting to huge amount of unlabeled out-domain
data. In addition, most of the previous work have
been focusing on constituent-based parsing, while
the domain adaptation of the dependency parsing
has not been fully explored.
Taking a different approach towards parsing,
grammar-based parsers appear to have much
linguistic knowledge encoded within the gram-
mars. In recent years, several of these linguisti-
cally motivated grammar-driven parsing systems
achieved high accuracy which are comparable to
the treebank-based statistical parsers. Notably are
the constraint-based linguistic frameworks with
mathematical rigor, and provide grammatical anal-
yses for a large variety of phenomena. For in-
stance, the Head-Driven Phrase Structure Gram-
mar (Pollard and Sag, 1994) has been success-
fully applied in several parsing systems for more
than a dozen of languages. Some of these gram-
mars, such as the English Resource Grammar
(ERG; Flickinger (2002)), have undergone over
decades of continuous development, and provide
precise linguistic analyses for a broad range of
phenomena. These linguistic knowledge are en-
coded in highly generalized form according to lin-
guists? reflection for the target languages, and tend
to be largely independent from any specific do-
main.
The main issue of parsing with precision gram-
mars is that broad coverage and high precision on
linguistic phenomena do not directly guarantee ro-
bustness of the parser with noisy real world texts.
Also, the detailed linguistic analysis is not always
of the highest interest to all NLP applications. It
is not always straightforward to scale down the
detailed analyses embraced by deep grammars to
a shallower representation which is more acces-
sible for specific NLP tasks. On the other hand,
since the dependency representation is relatively
theory-neutral, it is possible to convert from other
frameworks into its backbone representation in de-
pendencies. For HPSG, this is further assisted by
the clear marking of head daughters in headed
phrases. Although the statistical components of
the grammar-driven parser might be still biased
by the training domain, the hand-coded grammar
rules guarantee the basic linguistic constraints to
be met. This not to say that domain adaptation is
379
HPSG DBExtraction
HPSG DBFeature Models
MSTParserFeature Model
MaltParserFeature Model
Section 3.1
Section 3.3
McDonald
et al, 2005
Nivre
et al, 2007
Nivre andMcDonald,2008
Section 4.2
Section 4.3
Figure 1: Different dependency parsing models
and their combinations. DB stands for dependency
backbone.
not an issue for grammar-based parsing systems,
but the built-in linguistic knowledge can be ex-
plored to reduce the performance drop in pure sta-
tistical approaches.
3 Dependency Parsing with HPSG
In this section, we explore two possible applica-
tions of the HPSG parsing onto the syntactic de-
pendency parsing task. One is to extract depen-
dency backbone from the HPSG analyses of the
sentences and directly convert them into the tar-
get representation; the other way is to encode the
HPSG outputs as additional features into the ex-
isting statistical dependency parsing models. In
the previous work, Nivre and McDonald (2008)
have integrated MSTParser and MaltParser
by feeding one parser?s output as features into the
other. The relationships between our work and
their work are roughly shown in Figure 1.
3.1 Extracting Dependency Backbone from
HPSG Derivation Tree
Given a sentence, each parse produced by the
parser is represented by a typed feature structure,
which recursively embeds smaller feature struc-
tures for lower level phrases or words. For the
purpose of dependency backbone extraction, we
only look at the derivation tree which corresponds
to the constituent tree of an HPSG analysis, with
all non-terminal nodes labeled by the names of the
grammar rules applied. Figure 2 shows an exam-
ple. Note that all grammar rules in ERG are ei-
ther unary or binary, giving us relatively deep trees
when compared with annotations such as Penn
Treebank. Conceptually, this conversion is sim-
ilar to the conversions from deeper structures to
GR reprsentations reported by Clark and Curran
(2007) and Miyao et al (2007).
np_title_cmpnd
ms_n2 proper_np
subjh
generic_proper_ne
Haag
play_v1
hcomp
proper_np
generic_proper_ne
Elianti.
playsMs.
Figure 2: An example of an HPSG derivation tree
with ERG
Ms. Haag plays Elianti.
hcompnp_title_cmpnd subjh
Figure 3: An HPSG dependency backbone struc-
ture
The dependency backbone extraction works by
first identifying the head daughter for each bi-
nary grammar rule, and then propagating the head
word of the head daughter upwards to their par-
ents, and finally creating a dependency relation, la-
beled with the HPSG rule name of the parent node,
from the head word of the parent to the head word
of the non-head daughter. See Figure 3 for an ex-
ample of such an extracted backbone.
For the experiments in this paper, we used July-
08 version of the ERG, which contains in total
185 grammar rules (morphological rules are not
counted). Among them, 61 are unary rules, and
124 are binary. Many of the binary rules are
clearly marked as headed phrases. The gram-
mar also indicates whether the head is on the left
(head-initial) or on the right (head-final). How-
ever, there are still quite a few binary rules which
are not marked as headed-phrases (according to
the linguistic theory), e.g. rules to handle coor-
dinations, appositions, compound nouns, etc. For
these rules, we refer to the conversion of the Penn
Treebank into dependency structures used in the
CoNLL 2008 Shared Task, and mark the heads of
these rules in a way that will arrive at a compat-
ible dependency backbone. For instance, the left
most daughters of coordination rules are marked
as heads. In combination with the right-branching
analysis of coordination in ERG, this leads to the
same dependency attachment in the CoNLL syn-
tax. Eventually, 37 binary rules are marked with
a head daughter on the left, and 86 with a head
daughter on the right.
Although the extracted dependency is similar to
380
the CoNLL shared task dependency structures, mi-
nor systematic differences still exist for some phe-
nomena. For example, the possessive ??s? is an-
notated to be governed by its preceding word in
CoNLL dependency; while in HPSG, it is treated as
the head of a ?specifier-head? construction, hence
governing the preceding word in the dependency
backbone. With several simple tree rewriting
rules, we are able to fix the most frequent inconsis-
tencies. With the rule-based backbone extraction
and repair, we can finally turn our HPSG parser
outputs into dependency structures1. The unla-
beled attachment agreement between the HPSG
backbone and CoNLL dependency annotation will
be shown in Section 4.2.
3.2 Robust Parsing with HPSG
As mentioned in Section 2, one pitfall of using a
precision-oriented grammar in parsing is its lack
of robustness. Even with a large scale broad cover-
age grammar like ERG, using our settings we only
achieved 75% of sentential coverage2. Given that
the grammar has never been fine-tuned for the fi-
nancial domain, such coverage is very encourag-
ing. But still, the remaining unparsed sentences
comprise a big coverage gap.
Different strategies can be taken here. One
can either keep the high precision by only look-
ing at full parses from the HPSG parser, of which
the analyses are completely admitted by gram-
mar constraints. Or one can trade precision for
extra robustness by looking at the most proba-
ble incomplete analysis. Several partial parsing
strategies have been proposed (Kasper et al, 1999;
Zhang and Kordoni, 2008) as the robust fallbacks
for the parser when no available analysis can be
derived. In our experiment, we select the se-
quence of most likely fragment analyses accord-
ing to their local disambiguation scores as the par-
tial parse. When combined with the dependency
backbone extraction, partial parses generate dis-
joint tree fragments. We simply attach all frag-
ments onto the virtual root node.
1It is also possible map from HPSG rule names (together
with the part-of-speech of head and dependent) to CoNLL
dependency labels. This remains to be explored in the future.
2More recent study shows that with carefully designed
retokenization and preprocessing rules, over 80% sentential
coverage can be achieved on the WSJ sections of the Penn
Treebank data using the same version of ERG. The numbers
reported in this paper are based on a simpler preprocessor,
using rather strict time/memory limits for the parser. Hence
the coverage number reported here should not be taken as an
absolute measure of grammar performance.
3.3 Using Feature-Based Models
Besides directly using the dependency backbone
of the HPSG output, we could also use it for build-
ing feature-based models of statistical dependency
parsers. Since we focus on the domain adapta-
tion issue, we incorporate a less domain dependent
language resource (i.e. the HPSG parsing outputs
using ERG) into the features models of statistical
parsers. As mordern grammar-based parsers has
achieved high runtime efficency (with our HPSG
parser parsing at an average speed of?3 sentences
per second), this adds up to an acceptable over-
head.
3.3.1 Feature Model with MSTParser
As mentioned before, MSTParser is a graph-
based statistical dependency parser, whose learn-
ing procedure can be viewed as the assignment
of different weights to all kinds of dependency
arcs. Therefore, the feature model focuses on each
kind of head-child pair in the dependency tree, and
mainly contains four categories of features (Mc-
donald et al, 2005a): basic uni-gram features, ba-
sic bi-gram features, in-between POS features, and
surrounding POS features. It is emphasized by the
authors that the last two categories contribute a
large improvement to the performance and bring
the parser to the state-of-the-art accuracy.
Therefore, we extend this feature set by adding
four more feature categories, which are similar to
the original ones, but the dependency relation was
replaced by the dependency backbone of the HPSG
outputs. The extended feature set is shown in Ta-
ble 1.
3.3.2 Feature Model with MaltParser
MaltParser is another trend of dependency
parser, which is based on transitions. The learning
procedure is to train a statistical model, which can
help the parser to decide which operation to take at
each parsing status. The basic data structures are a
stack, where the constructed dependency graph is
stored, and an input queue, where the unprocessed
data are put. Therefore, the feature model focuses
on the tokens close to the top of the stack and also
the head of the queue.
Provided with the original features used in
MaltParser, we add extra ones about the top
token in the stack and the head token of the queue
derived from the HPSG dependency backbone.
The extended feature set is shown in Table 2 (the
new features are listed separately).
381
Uni-gram Features: h-w,h-p; h-w; h-p; c-w,c-p; c-w; c-p
Bi-gram Features: h-w,h-p,c-w,c-p; h-p,c-w,c-p; h-w,c-w,c-p; h-w,h-p,c-p; h-w,h-p,c-w; h-w,c-w; h-p,c-p
POS Features of words in between: h-p,b-p,c-p
POS Features of words surround: h-p,h-p+1,c-p-1,c-p; h-p-1,h-p,c-p-1,c-p; h-p,h-p+1,c-p,c-p+1; h-p-1,h-p,c-p,c-p+1
Table 1: The Extra Feature Set for MSTParser. h: the HPSG head of the current token; c: the current
token; b: each token in between; -1/+1: the previous/next token; w: word form; p: POS
POS Features: s[0]-p; s[1]-p; i[0]-p; i[1]-p; i[2]-p; i[3]-p
Word Form Features: s[0]-h-w; s[0]-w; i[0]-w; i[1]-w
Dependency Features: s[0]-lmc-d; s[0]-d; s[0]-rmc-d; i[0]-lmc-d
New Features: s[0]-hh-w; s[0]-hh-p; s[0]-hr; i[0]-hh-w; i[0]-hh-p; i[0]-hr
Table 2: The Extended Feature Set for MaltParser. s[0]/s[1]: the first and second token on the top of
the stack; i[0]/i[1]/i[2]/i[3]: front tokens in the input queue; h: head of the token; hh: HPSG DB head of
the token; w: word form; p: POS; d: dependency relation; hr: HPSG rule; lmc/rmc: left-/right-most child
With the extra features, we hope that the train-
ing of the statistical model will not overfit the in-
domain data, but be able to deal with domain in-
dependent linguistic phenomena as well.
4 Experiment Results & Error Analyses
To evaluate the performance of our different
dependency parsing models, we tested our ap-
proaches on several dependency treebanks for En-
glish in a similar spirit to the CoNLL 2006-2008
Shared Tasks. In this section, we will first de-
scribe the datasets, then present the results. An
error analysis is also carried out to show both pros
and cons of different models.
4.1 Datasets
In previous years of CoNLL Shared Tasks, sev-
eral datasets have been created for the purpose
of dependency parser evaluation. Most of them
are converted automatically from existing tree-
banks in various forms. Our experiments adhere
to the CoNLL 2008 dependency syntax (Yamada
et al 2003, Johansson et al 2007) which was
used to convert Penn-Treebank constituent trees
into single-head, single-root, traceless and non-
projective dependencies.
WSJ This dataset comprises of three portions.
The larger part is converted from the Penn Tree-
bank Wall Street Journal Sections #2?#21, and
is used for training statistical dependency parsing
models; the smaller part, which covers sentences
from Section #23, is used for testing.
Brown This dataset contains a subset of con-
verted sentences from BROWN sections of the
Penn Treebank. It is used for the out-domain test.
PChemtb This dataset was extracted from the
PennBioIE CYP corpus, containing 195 sentences
from biomedical domain. The same dataset has
been used for the domain adaptation track of the
CoNLL 2007 Shared Task. Although the original
annotation scheme is similar to the Penn Treebank,
the dependency extraction setting is slightly dif-
ferent to the CoNLLWSJ dependencies (e.g. the
coordinations).
Childes This is another out-domain test set from
the children language component of the TalkBank,
containing dialogs between parents and children.
This is the other datasets used in the domain adap-
tation track of the CoNLL 2007 Shared Task. The
dataset is annotated with unlabeled dependencies.
As have been reported by others, several system-
atic differences in the original CHILDES annota-
tion scheme has led to the poor system perfor-
mances on this track of the Shared Task in 2007.
Two main differences concern a) root attach-
ments, and b) coordinations. With several sim-
ple heuristics, we change the annotation scheme of
the original dataset to match the Penn Treebank-
based datasets. The new dataset is referred to as
CHILDES*.
4.2 HPSG Backbone as Dependency Parser
First we test the agreement between HPSG depen-
dency backbone and CoNLL dependency. While
approximating a target dependency structure with
rule-based conversion is not the main focus of this
work, the agreement between two representations
gives indication on how similar and consistent the
two representations are, and a rough impression of
whether the feature-based models can benefit from
the HPSG backbone.
382
# sentence ? w/s DB(F)% DB(P)%
WSJ 2399 24.04 50.68 63.85
BROWN 425 16.96 66.36 76.25
PCHEMTB 195 25.65 50.27 61.60
CHILDES* 666 7.51 67.37 70.66
WSJ-P 1796 (75%) 22.25 71.33 ?
BROWN-P 375 (88%) 15.74 80.04 ?
PCHEMTB-P 147 (75%) 23.99 69.27 ?
CHILDES*-P 595 (89%) 7.49 73.91 ?
Table 3: Agreement between HPSG dependency
backbone and CoNLL 2008 dependency in unla-
beled attachment score. DB(F): full parsing mode;
DB(P): partial parsing mode; Punctuations are ex-
cluded from the evaluation.
The PET parser, an efficient parser HPSG parser
is used in combination with ERG to parse the
test sets. Note that the training set is not used.
The grammar is not adapted for any of these spe-
cific domain. To pick the most probable read-
ing from HPSG parsing outputs, we used a dis-
criminative parse selection model as described
in (Toutanova et al, 2002) trained on the LOGON
Treebank (Oepen et al, 2004), which is signifi-
cantly different from any of the test domain. The
treebank contains about 9K sentences for which
HPSG analyses are manually disambiguated. The
difference in annotation make it difficult to sim-
ply merge this HPSG treebank into the training set
of the dependency parser. Also, as Gildea (2001)
suggests, adding such heterogeneous data to the
training set will not automatically lead to perfor-
mance improvement. It should be noted that do-
main adaptation also presents a challenge to the
disambiguation model of the HPSG parser. All
datasets we use in our should be considered out-
domain to the HPSG disambiguation model.
Table 3 shows the agreement between the HPSG
backbone and CoNLL dependency in unlabeled at-
tachment score (UAS). The parser is set in either
full parsing or partial parsing mode. Partial pars-
ing is used as a fallback when full parse is not
available. UAS are reported on all complete test
sets, as well as fully parsed subsets (suffixed with
?-p?).
It is not surprising to see that, without a de-
cent fallback strategy, the full parse HPSG back-
bone suffers from insufficient coverage. Since the
grammar coverage is statistically correlated to the
average sentence length, the worst performance is
observed for the PCHEMTB. Although sentences
in CHILDES* are significantly shorter than those
in BROWN, there is a fairly large amount of less
well-formed sentences (either as a nature of child
language, or due to the transcription from spoken
dialogs). This leads to the close performance be-
tween these two datasets. PCHEMTB appears to be
the most difficult one for the HPSG parser. The
partial parsing fallback sets up a good safe net for
sentences that fail to parse. Without resorting to
any external resource, the performance was sig-
nificantly improved on all complete test sets.
When we set the coverage of the HPSG gram-
mar aside and only compare performance on the
subsets of these datasets which are fully parsed
by the HPSG grammar, the unlabeled attachment
score jumps up significantly. Most notable is
that the dependency backbone achieved over 80%
UAS on BROWN, which is close to the perfor-
mance of state-of-the-art statistical dependency
parsing systems trained on WSJ (see Table 5 and
Table 4). The performance difference across data
sets correlates to varying levels of difficulties in
linguists? view. Our error analysis does confirm
that frequent errors occur in WSJ test with finan-
cial terminology missing from the grammar lexi-
con. The relative performance difference between
the WSJ and BROWN test is contrary to the results
observed for statistical parsers trained on WSJ.
To further investigate the effect of HPSG parse
disambiguation model on the dependency back-
bone accuracy, we used a set of 222 sentences
from section of WSJ which have been parsed with
ERG and manually disambiguated. Comparing
to the WSJ-P result in Table 3, we improved the
agreement with CoNLL dependency by another
8% (an upper-bound in case of a perfect disam-
biguation model).
4.3 Statistical Dependency Parsing with
HPSG Features
Similar evaluations were carried out for the statis-
tical parsers using extra HPSG dependency back-
bone as features. It should be noted that the per-
formance comparison between MSTParser and
MaltParser is not the aim of this experiment,
and the difference might be introduced by the spe-
cific settings we use for each parser. Instead, per-
formance variance using different feature models
is the main subject. Also, performance drop on
out-domain tests shows how domain dependent
the feature models are.
For MaltParser, we use Arc-Eager algo-
383
rithm, and polynomial kernel with d = 2. For
MSTParser, we use 1st order features and a pro-
jective decoder (Eisner, 1996).
When incorporating HPSG features, two set-
tings are used. The PARTIAL model is derived by
robust-parsing the entire training data set and ex-
tract features from every sentence to train a uni-
fied model. When testing, the PARTIAL model is
used alone to determine the dependency structures
of the input sentences. The FULL model, on the
other hand is only trained on the full parsed subset
of sentences, and only used to predict dependency
structures for sentences that the grammar parses.
For the unparsed sentences, the original models
without HPSG features are used.
Parser performances are measured using
both labeled and unlabeled attachment scores
(LAS/UAS). For unlabeled CHILDES* data, only
UAS numbers are reported. Table 4 and 5 summa-
rize results for MSTParser and MaltParser,
respectively.
With both parsers, we see slight performance
drops with both HPSG feature models on in-
domain tests (WSJ), compared with the original
models. However, on out-domain tests, full-parse
HPSG feature models consistently outperform the
original models for both parsers. The difference is
even larger when only the HPSG fully parsed sub-
sets of the test sets are concerned. When we look
at the performance difference between in-domain
and out-domain tests for each feature model, we
observe that the drop is significantly smaller for
the extended models with HPSG features.
We should note that we have not done any
feature selection for our HPSG feature models.
Nor have we used the best known configurations
of the existing parsers (e.g. second order fea-
tures in MSTParser). Admittedly the results on
PCHEMTB are lower than the best reported results
in CoNLL 2007 Shared Task, we shall note that we
are not using any in-domain unlabeled data. Also,
the poor performance of the HPSG parser on this
dataset indicates that the parser performance drop
is more related to domain-specific phenomena and
not general linguistic knowledge. Nevertheless,
the drops when compared to in-domain tests are
constantly decreased with the help of HPSG analy-
ses features. With the results on BROWN, the per-
formance of our HPSG feature models will rank
2nd on the out-domain test for the CoNLL 2008
Shared Task.
Unlike the observations in Section 4.2, the par-
tial parsing mode does not work well as a fall-
back in the feature models. In most cases, its
performances are between the original models and
the full-parse HPSG feature models. The partial
parsing features obscure the linguistic certainty of
grammatical structures produced in the full model.
When used as features, such uncertainty leads
to further confusion. Practically, falling back to
the original models works better when HPSG full
parse is not available.
4.4 Error Analyses
Qualitative error analysis is also performed. Since
our work focuses on the domain adaptation, we
manually compare the outputs of the original sta-
tistical models, the dependency backbone, and the
feature-based models on the out-domain data, i.e.
the BROWN data set (both labeled and unlabeled
results) and the CHILDES* data set (only unlabeled
results).
For the dependency attachment (i.e. unlabeled
dependency relation), fine-grained HPSG features
do help the parser to deal with colloquial sen-
tences, such as ?What?s wrong with you??. The
original parser wrongly takes ?what? as the root of
the dependency tree and ??s? is attached to ?what?.
The dependency backbone correctly finds out the
root, and thus guide the extended model to make
the right prediction. A correct structure of ?...,
were now neither active nor really relaxed.? is also
predicted by our model, while the original model
wrongly attaches ?really? to ?nor? and ?relaxed?
to ?were?. The rich linguistic knowledge from
the HPSG outputs also shows its usefulness. For
example, in a sentence from the CHILDES* data,
?Did you put dolly?s shoes on??, the verb phrase
?put on? can be captured by the HPSG backbone,
while the original model attaches ?on? to the adja-
cent token ?shoes?.
For the dependency labels, the most diffi-
culty comes from the prepositions. For example,
?Scotty drove home alone in the Plymouth?, all
the systems get the head of ?in? correct, which
is ?drove?. However, none of the dependency la-
bels is correct. The original model predicts the
?DIR? relation, the extended feature-based model
says ?TMP?, but the gold standard annotation is
?LOC?. This is because the HPSG dependency
backbone knows that ?in the Plymouth? is an ad-
junct of ?drove?, but whether it is a temporal or
384
Original PARTIAL FULL
LAS% UAS% LAS% UAS% LAS% UAS%
WSJ 87.38 90.35 87.06 90.03 86.87 89.91
BROWN 80.46 (-6.92) 86.26 (-4.09) 80.55 (-6.51) 86.17 (-3.86) 80.92 (-5.95) 86.58 (-3.33)
PCHEMTB 53.37 (-33.8) 62.11 (-28.24) 54.69 (-32.37) 64.09 (-25.94) 56.45 (-30.42) 65.77 (-24.14)
CHILDES* ? 72.17 (-18.18) ? 74.91 (-15.12) ? 75.64 (-14.27)
WSJ-P 87.86 90.88 87.78 90.85 87.12 90.25
BROWN-P 81.58 (-6.28) 87.41 (-3.47) 81.92 (-5.86) 87.51 (-3.34) 82.14 (-4.98) 87.80 (-2.45)
PCHEMTB-P 56.32 (-31.54) 65.26 (-25.63) 59.36 (-28.42) 69.20 (-21.65) 60.69 (-26.43) 70.45 (-19.80)
CHILDES*-P ? 72.88 (-18.00) ? 76.02 (-14.83) ? 76.76 (-13.49)
Table 4: Performance of the MSTParser with different feature models. Numbers in parentheses are
performance drops in out-domain tests, comparing to in-domain results. The upper part represents the
results on the complete data sets, and the lower part is on the fully parsed subsets, indicated by ?-P?.
Original PARTIAL FULL
LAS% UAS% LAS% UAS% LAS% UAS%
WSJ 86.47 88.97 85.39 88.10 85.66 88.40
BROWN 79.41 (-7.06) 84.75 (-4.22) 79.10 (-6.29) 84.58 (-3.52) 79.56 (-6.10) 85.24 (-3.16)
PCHEMTB 61.05 (-25.42) 71.32 (-17.65) 61.01 (-24.38) 70.99 (-17.11) 60.93 (-24.73) 70.89 (-17.51)
CHILDES* ? 74.97 (-14.00) ? 75.64 (-12.46) ? 76.18 (-12.22)
WSJ-P 86.99 89.58 86.09 88.83 85.82 88.76
BROWN-P 80.43 (-6.56) 85.78 (-3.80) 80.46 (-5.63) 85.94 (-2.89) 80.62 (-5.20) 86.38 (-2.38)
PCHEMTB-P 63.33 (-23.66) 73.54 (-16.04) 63.27 (-22.82) 73.31 (-15.52) 63.16 (-22.66) 73.06 (-15.70)
CHILDES*-P ? 75.95 (-13.63) ? 77.05 (-11.78) ? 77.30 (-11.46)
Table 5: Performance of the MaltParser with different feature models.
locative expression cannot be easily predicted at
the pure syntactic level. This also suggests a joint
learning of syntactic and semantic dependencies,
as proposed in the CoNLL 2008 Shared Task.
Instances of wrong HPSG analyses have also
been observed as one source of errors. For most of
the cases, a correct reading exists, but not picked
by our parse selection model. This happens more
often with the WSJ test set, partially contributing
to the low performance.
5 Conclusion & Future Work
Similar to our work, Sagae et al (2007) also con-
sidered the combination of dependency parsing
with an HPSG parser, although their work was to
use statistical dependency parser outputs as soft
constraints to improve the HPSG parsing. Nev-
ertheless, a similar backbone extraction algorithm
was used to map between different representa-
tions. Similar work also exists in the constituent-
based approaches, where CFG backbones were
used to improve the efficiency and robustness of
HPSG parsers (Matsuzaki et al, 2007; Zhang and
Kordoni, 2008).
In this paper, we restricted our investigation on
the syntactic evaluation using labeled/unlabeled
attachment scores. Recent discussions in the
parsing community about meaningful cross-
framework evaluation metrics have suggested to
use measures that are semantically informed. In
this spirit, Zhang et al (2008) showed that the se-
mantic outputs of the same HPSG parser helps in
the semantic role labeling task. Consistent with
the results reported in this paper, more improve-
ment was achieved on the out-domain tests in their
work as well.
Although the experiments presented in this pa-
per were carried out on a HPSG grammar for En-
glish, the method can be easily adapted to work
with other grammar frameworks (e.g. LFG, CCG,
TAG, etc.), as well as on langugages other than
English. We chose to use a hand-crafted grammar,
so that the effect of training corpus on the deep
parser is minimized (with the exception of the lex-
ical coverage and disambiguation model).
As mentioned in Section 4.4, the performance
of our HPSG parse selection model varies across
different domains. This indicates that, although
the deep grammar embraces domain independent
linguistic knowledge, the lexical coverage and the
disambiguation process among permissible read-
ings is still domain dependent. With the map-
ping between HPSG analyses and their depen-
dency backbones, one can potentially use existing
dependency treebanks to help overcome the insuf-
ficient data problem for deep parse selection mod-
els.
385
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. Map adaptation of stochastic grammars.
Computer speech and language, 20(1):41?68.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the 10th Conference on Computational Natural Lan-
guage Learning (CoNLL-X), New York City, USA.
Stephen Clark and James Curran. 2007. Formalism-
independent parser evaluation with ccg and depbank. In
Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 248?255, Prague,
Czech Republic.
Jason Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of the
16th International Conference on Computational Linguis-
tics (COLING-96), pages 340?345, Copenhagen, Den-
mark.
Dan Flickinger. 2002. On building a more efficient grammar
by exploiting types. In Stephan Oepen, Dan Flickinger,
Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative
Language Engineering, pages 1?17. CSLI Publications.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Language Processing, pages
167?202, Pittsburgh, USA.
Walter Kasper, Bernd Kiefer, Hans-Ulrich Krieger, C.J.
Rupp, and Karsten Worm. 1999. Charting the depths of
robust speech processing. In Proceedings of the 37th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL 1999), pages 405?412, Maryland, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of english: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Efficient HPSG parsing with supertagging and CFG-
filtering. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI 2007), pages
1671?1676, Hyderabad, India.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics, pages
337?344, Sydney, Australia.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 561?568,
Manchester, UK.
Ryan Mcdonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL?05),
pages 91?98, Ann Arbor, Michigan.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005b. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007. To-
wards framework-independent evaluation of deep linguis-
tic parsers. In Proceedings of the GEAF07 Workshop,
pages 238?258, Stanford, CA.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In Pro-
ceedings of ACL-08: HLT, pages 950?958, Columbus,
Ohio, June.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007a.
The CoNLL 2007 shared task on dependency parsing.
In Proceedings of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov, and
Erwin Marsi. 2007b. Maltparser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(1):1?41.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer,
Torbj?rn Nordga?rd, and Victoria Rose?n. 2004. Som a?
kapp-ete med trollet? Towards MRS-Based Norwegian?
English Machine Translation. In Proceedings of the 10th
International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, USA.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007. Hpsg
parsing with shallow dependency constraints. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 624?631, Prague,
Czech Republic.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th Conference
on Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Kristina Toutanova, Christoper D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse
ranking for a rich HPSG grammar. In Proceedings of the
1st Workshop on Treebanks and Linguistic Theories (TLT
2002), pages 253?263, Sozopol, Bulgaria.
Yi Zhang and Valia Kordoni. 2008. Robust Parsing with a
Large HPSG Grammar. In Proceedings of the Sixth Inter-
national Language Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CoNLL 2008), pages 198?202, Manchester, UK.
386
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 31?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hybrid Multilingual Parsing with HPSG for SRL
Yi Zhang
Language Technology
DFKI GmbH, Germany
yzhang@coli.uni-sb.de
Rui Wang
Computational Linguistics
Saarland University, Germany
rwang@coli.uni-sb.de
Stephan Oepen
Informatics
University of Oslo, Norway
oe@ifi.uio.no
Abstract
In this paper we present our syntactic and se-
mantic dependency parsing system submitted
to both the closed and open challenges of the
CoNLL 2009 Shared Task. The system ex-
tends the system of Zhang, Wang, & Uszko-
reit (2008) in the multilingual direction, and
achieves 76.49 average macro F1 Score on the
closed joint task. Substantial improvements
to the open SRL task have been observed that
are attributed to the HPSG parses with hand-
crafted grammars. ?
1 Introduction
The CoNLL 2009 shared task (Hajic? et al, 2009)
continues the exploration on learning syntactic and
semantic structures based on dependency notations
in previous year?s shared task. The new addition
to this year?s shared task is the extension to mul-
tiple languages. Being one of the leading compe-
titions in the field, the shared task received sub-
missions from systems built on top of the state-
of-the-art data-driven dependency parsing and se-
mantic role labeling systems. Although it was
originally designed as a task for machine learning
approaches, CoNLL shared tasks also feature an
?open? track since 2008, which encourages the use
of extra linguistic resources to further improve the
?We are indebted to our DELPH-IN colleagues, specifi-
cally Peter Adolphs, Francis Bond, Berthold Crysmann, and
Montserrat Marimon for numerous hours of support in adapt-
ing their grammars and the PET software to parsing the CoNLL
data sets. The first author thanks the German Excellence Clus-
ter of Multimodal Computing and Interaction for the support
of the work. The second author is funded by the PIRE PhD
scholarship program. Participation of the third author in this
work was supported by the University of Oslo, as part of its re-
search partnership with the Center for the Study of Language
and Information at Stanford University. Our deep parsing ex-
perimentation was executed on the TITAN HPC facilities at the
University of Oslo.
performance. This makes the task a nice testbed for
the cross-fertilization of various language process-
ing techniques.
As an example of such work, Zhang et al (2008)
have shown in the past that deep linguistic parsing
outputs can be integrated to help improve the per-
formance of the English semantic role labeling task.
But several questions remain unanswered. First, the
integration only experimented with the semantic role
labeling part of the task. It is not clear whether
syntactic dependency parsing can also benefit from
grammar-based parsing results. Second, the English
grammar used to achieve the improvement is one of
the largest and most mature hand-crafted linguistic
grammars. It is not clear whether similar improve-
ments can be achieved with less developed gram-
mars. More specifically, the lack of coverage of
hand-crafted linguistic grammars is a major concern.
On the other hand, the CoNLL task is also a good
opportunity for the deep processing community to
(re-)evaluate their resources and software.
2 System Architecture
The overall system architecture is shown in Figure 1.
It is similar to the architecture used by Zhang et al
(2008). Three major components were involved.
The HPSG parsing component utilizes several hand-
crafted grammars for deep linguistic parsing. The
outputs of deep parsings are passed to the syntactic
dependency parser and semantic role labeler. The
syntactic parsing component is composed of a mod-
ified MST parser which accepts HPSG parsing re-
sults as extra features. The semantic role labeler is
comprised of a pipeline of 4 sub-components (pred-
icate identification is not necessary in this year?s
task). Comparing to Zhang et al (2008), this archi-
tecture simplified the syntactic component, and puts
more focus on the integration of deep parsing out-
puts. While Zhang et al (2008) only used seman-
31
SyntacticDependencyParsing
MST Parser
ERG
GG
JaCY
SRG
[incr tsdb()]
PET
HPSG Parsing
Argument Identification
Argument Classification
Predicate Classification
SemanticRoleLabeling
MRS
HPSG Syn.
Syn.Dep.
Figure 1: Joint system architecture.
tic features from HPSG parsing in the SRL task, we
added extra syntactic features from deep parsing to
help both tasks.
3 HPSG Parsing for the CoNLL Data
DELPH-IN (Deep Linguistic Processing with
HPSG) is a repository of open-source software and
linguistic resources for so-called ?deep? grammat-
ical analysis.1 The grammars are rooted in rela-
tively detailed, hand-coded linguistic knowledge?
including lexical argument structure and the linking
of syntactic functions to thematic arguments?and
are intended as general-purpose resources, applica-
ble to both parsing and generation. Semantics in
DELPH-IN is cast in the Minimal Recursion Seman-
tics framework (MRS; Copestake, Flickinger, Pol-
lard, & Sag, 2005), essentially predicate ? argument
structures with provision for underspecified scopal
relations. For the 2009 ?open? task, we used the
DELPH-IN grammars for English (ERG; Flickinger,
2000), German (GG; Crysmann, 2005), Japanese
(JaCY; Siegel & Bender, 2002), and Spanish (SRG;
Marimon, Bel, & Seghezzi, 2007). The grammars
vary in their stage of development: the ERG com-
prises some 15 years of continuous development,
whereas work on the SRG only started about five
years ago, with GG and JaCY ranging somewhere
inbetween.
3.1 Overall Setup
We applied the DELPH-IN grammars to the CoNLL
data using the PET parser (Callmeier, 2002) running
1See http://www.delph-in.net for background.
it through the [incr tsdb()] environment (Oepen &
Carroll, 2000), for parallelization and distribution.
Also, [incr tsdb()] provides facilities for (re-)training
the MaxEnt parse selection models that PET uses for
disambiguation.
The two main challenges in applying DELPH-
IN resources to parsing CoNLL data were (a) mis-
matches in basic assumptions, specifically tokeniza-
tion and the inventory of PoS tags provided as part of
the input, and (b) the need to adapt the resources for
new domains and genres?in particular in terms of
parse disambiguation?as the English and Spanish
grammars at least had not been previously applied
to the corpora used in the CoNLL shared task.
The importance of the first of these two aspects
is often underestimated. A detailed computational
grammar, inevitably, comes with its own assump-
tions about tokenization?the ERG, for example, re-
jects the conventional assumptions underlying the
PTB (and derived tools). It opts for an analysis of
punctuation akin to affixation (rather than as stand-
alone tokens), does not break up contracted negated
auxiliaries, and splits hyphenated words like ill-
advised into two tokens (the hyphen being part of
the first component). Thus, a string like Don?t you!
in the CoNLL data is tokenized as the four-element
sequence ?do, n?t, you, !?,2 whereas the ERG analy-
sis has only two leaf nodes: ?don?t, you!?.
Fortunately, the DELPH-IN toolchain recently
incorporated a mechanism called chart mapping
(Adolphs et al, 2008), which allows one to map
flexibly from ?external? input to grammar-internal
assumptions, while keeping track of external token
identities and their contributions to the final analysis.
The February 2009 release of the ERG already had
this machinery in place (with the goal of supporting
extant, PTB-trained PoS taggers in pre-processing
input to the deep parser), and we found that only a
tiny number of additional chart mapping rules was
required to ?fix up? CoNLL-specific deviations from
the PTB tradition. With the help of the original de-
velopers, we created new chart mapping configura-
tions for the German and Japanese grammars (with
17 and 16 such accomodation rules, respectively) in
a similar spirit. All four DELPH-IN grammars in-
2Note that the implied analogy to a non-contracted variant is
linguistically mis-leading, as ?Do not you! is ungrammatical.
32
clude an account of unknown words, based on un-
derspecified ?generic? lexical entries that are acti-
vated from PoS information.
The Japenese case was interesting, in that
the grammar assumes a different pre-processor
(ChaSen, rather than Juman), such that not only to-
ken boundaries but also PoS tags and morphological
features had to be mapped. From our limited ex-
perience to date, we found the chart mapping ap-
proach adequate in accomodating such discrepan-
cies, and the addition of this extra layer of input
processing gave substantial gains in parser cover-
age (see below). For the Spanish data, on the other
hand, we found it impossible to make effective use
of the PoS and morphological information in the
CoNLL data, due to more fundamental discrepan-
cies (e.g. the treatment of enclitics and multi-word
expressions).
3.2 Retraining Disambiguation Models
The ERG includes a domain-specific parse selection
model (for tourism instructions); GG only a stub
model trained on a handful of test sentences. For
use on the CoNLL data, thus, we had to train new
parse selections models, better adapted to the shared
task corpora. Disambiguation in PET is realized by
conditional MaxEnt models (Toutanova, Manning,
Flickinger, & Oepen, 2005), usually trained on full
HPSG treebanks. Lacking this kind of training ma-
terial, we utilized the CoNLL dependency informa-
tion instead, by defining an unlabeled dependency
accuracy (DA) metric for HPSG analyses, essen-
tially quantifying the degree of overlap in head ?
dependent relations against the CoNLL annotations.
Calculating DA for HPSG trees is similar to the
procedure commonly used for extracting bi-lexical
dependencies from phrase structure trees, in a sense
even simpler as HPSG analyses fully determine
headeness. Taking into account the technical com-
plication of token-level mismatches, our DA met-
ric loosely corresponds to the unlabeled attachment
score. To train CoNLL-specific parse selection mod-
els, we parsed the development sections in 500-best
mode (using the existing models) and then mechani-
cally ?annotated? the HPSG analyses with maximum
DA as preferred, all others as dis-preferred. In other
words, this procedure constructs a ?binarized? em-
pirical distribution where estimation of log-linear
Grammar Coverage Time
ERG 80.4% 10.06 s
GG 28.6% 3.41 s
JaCY 42.7% 2.13 s
SRG 7.5% 0.80 s
Table 1: Performance of the DELPH-IN grammars.
model parameters amounts to adjusting conditional
probabilities towards higher DA values.3
Using the [incr tsdb()] MaxEnt experimentation
facilities, we trained new parse selection models
for English and German, using the first 16,000 sen-
tences of the English training data and the full Ger-
man training corpus; seeing that only inputs that (a)
parse successfully and (b) have multiple readings,
with distinct DA values are relevant to this step, the
final models reflect close to 13,000 sentences for En-
glish, and a little more than 4,000 items for German.
Much like in the SRL component, these experiments
are carried out with the TADM software, using ten-
fold cross-validation and exact match ranking accu-
racy (against the binarized training distribution) to
optimize estimation hyper-parameters
3.3 Deep Parsing Features
HPSG parsing coverage and average cpu time per
input for the four languages with DELPH-IN gram-
mars are summarized in Table 1. The PoS-based
unknown word mechanism was active for all gram-
mars but no other robustness measures (which tend
to lower the quality of results) were used, i.e. only
complete spanning HPSG analyses were accepted.
Parse times are for 1-best parsing, using selective
unpacking (Zhang, Oepen, & Carroll, 2007).
HPSG parsing outputs are available in several dif-
ferent forms. We investigated two types of struc-
tures: syntactic derivations and MRS meaningrep-
resentations. Representative features were extracted
from both structures and selectively used in the sta-
tistical syntactic dependency parsing and semantic
role labeling modules for the ?open? challenge.
3We also experimented with using DA scores directly as em-
pirical probabilities in the training distribution (or some func-
tion of DA, to make it fall off more sharply), but none of
these methods seemed to further improve parse selection per-
formance.
33
Deep Semantic Features Similar to Zhang et al
(2008), we extract a set of features from the seman-
tic outputs (MRS) of the HPSG parses. These fea-
tures represent the basic predicate-argument struc-
ture, and provides a simplified semantic view on the
target sentence.
Deep Syntactic Dependency Features A HPSG
derivation is a tree structure. The internal nodes are
labeled with identifiers of grammar rules, and leaves
with lexical entries. The derivation tree provides
complete information about the actual HPSG anal-
ysis, and can be used together with the grammar to
reproduce complete feature structure and/or MRS.
Given that the shared task adopts dependency rep-
resentation, we further map the derivation trees into
token-token dependencies, labeled by corresponding
HPSG rules, by defining a set of head-finding rules
for each grammar. This dependency structure is dif-
ferent from the dependencies in CoNLL dataset, and
provides an alternative HPSG view on the sentences.
We refer to this structure as the dependency back-
bone (DB) of the HPSG anaylsis. A set of features
were extracted from the deep syntactic dependency
structures. This includes: i) the POS of the DB par-
ent from the predicate and/or argument; ii) DB la-
bel of the argument to its parent (only for AI/AC);
iii) labeled path from predicate to argument in DB
(only for AI/AC); iv) POSes of the predicate?s DB
dependents
4 Syntactic Dependency Parsing
For the syntactic dependency parsing, we use the
MST Parser (McDonald et al, 2005), which is a
graph-based approach. The best parse tree is ac-
quired by searching for a spanning tree which max-
imizes the score on either a partially or a fully con-
nected graph with all words in the sentence as nodes
(Eisner, 1996; McDonald et al, 2005). Based on our
experience last year, we use the second order setting
of the parser, which includes features over pairs of
adjacent edges as well as features over single edges
in the graph. For the projective or non-projective
setting, we compare the results on the development
datasets of different languages. According to the
parser performance, we decide to use non-projective
parsing for German, Japanese, and Czech, and use
projective parsing for the rest.
For the Closed Challenge, we first consider
whether to use the morphological features. We find
that except for Czech, parser performs better with-
out morphological features on other languages (En-
glish and Chinese have no morphological features).
As for the other features (i.e. lemma and pos) given
by the data sets, we also compare the gold standard
features and P-columns. For all languages, the per-
formance decreases in the following order: training
with gold standard features and evaluating with the
gold standard features, training with P-columns and
evaluating with P-columns, training with gold stan-
dard features and testing with P-columns. Conse-
quently, in the final submission, we take the second
combination.
The goal of the Open Challenge is to see whether
using external resources can be helpful for the pars-
ing performance. As we mentioned before, our
deep parser gives us both the syntactic analysis of
the input sentences using the HPSG formalism and
also the semantic analysis using MRS as the repre-
sentation. However, for the syntactic dependency
parsing, we only extract features from the syntac-
tic HPSG analyses and feed them into the MST
Parser. Although, when parsing with gold standard
lemma and POS features, our open system outper-
forms the closed system on out-domain tests (for En-
glish), when parsing with P-columns there is no sub-
stantial improvement observed after using the HPSG
features. Therefore, we did not include it in the final
submission.
5 Semantic Role Labeling
The semantic role labeling component used in the
submitted system is similar to the one described
by Zhang et al (2008). Since predicates are indi-
cated in the data, the predicate identification mod-
ule is removed from this year?s system. Argument
identification, argument classification and predicate
classification are the three sub-components in the
pipeline. All of them are MaxEnt-based classifiers.
For parameter estimation, we use the open source
TADM system (Malouf, 2002).
The active features used in various steps of SRL
are fine tuned separately for different languages us-
ing development datasets. The significance of fea-
ture types varies across languages and datasets.
34
ca zh cs en de ja es
SY
N Closed 82.67 73.63 75.58 87.90 84.57 91.47 82.69
ood - - 71.29 81.50 75.06 - -
SR
L
Closed 67.34 73.20 78.28 77.85 62.95 64.71 67.81
ood - - 77.78 67.07 54.87 - -
Open - - - 78.13 (?0.28) 64.31 (?1.36) 65.95 (?1.24) 68.24 (?0.43)
ood - - - 68.11 (?1.04) 58.42 (?3.55) - -
Table 2: Summary of System Performance on Multiple Languages
In the open challenge, two groups of extra fea-
tures from HPSG parsing outputs, as described in
Section 3.3, were used on languages for which we
have HPSG grammars, that is English, German,
Japanese, and Spanish.
6 Result Analysis
The evaluation results of the submitted system are
summarized in Table 2. The overall ranking of
the system is #7 in the closed challenge, and #2
in the open challenge. While the system achieves
mediocre performance, the clear performance dif-
ference between the closed and open challenges of
the semantic role labeler indicates a substantial gain
from the integration of HPSG parsing outputs. The
most interesting observation is that even with gram-
mars which only achieve very limited coverage, no-
ticeable SRL improvements are obtained. Con-
firming the observation of Zhang et al (2008), the
gain with HPSG features is more significant on out-
domain tests, this time on German as well.
The training of the syntactic parsing models for
all seven languages with MST parser takes about
100 CPU hours with 10 iterations. The dependency
parsing takes 6 ? 7 CPU hours. The training and test-
ing of the semantic role labeler is much more effi-
cient, thanks to the use of MaxEnt models and the
efficient parameter estimation software. The train-
ing of all SRL models for 7 languages takes about 3
CPU hours in total. The total time for semantic role
labeling on test datasets is less than 1 hour.
Figure 2 shows the learning curve of the syntactic
parser and semantic role labeler on the Czech and
English datasets. While most of the systems con-
tinue to improve when trained on larger datasets, an
exception was observed with the Czech dataset on
the out-domain test for syntactic accuracy. In most
of the cases, with the increase of training data, the
out-domain test performance of the syntactic parser
and semantic role labeler improves slowly relative
to the in-domain test. For the English dataset, the
SRL learning curve climbs more quickly than those
of syntactic parsers. This is largely due to the fact
that the semantic role annotation is sparser than the
syntactic dependencies. On the Czech dataset which
has dense semantic annotation, this effect is not ob-
served.
7 Conclusion
In this paper, we described our syntactic parsing and
semantic role labeling system participated in both
closed and open challenge of the (Joint) CoNLL
2009 Shared Task. Four hand-written HPSG gram-
mars of a variety of scale have been applied to parse
the datasets, and the outcomes were integrated as
features into the semantic role labeler of the sys-
tem. The results clearly show that the integration of
HPSG parsing results in the semantic role labeling
task brings substantial performance improvement.
The conclusion of Zhang et al (2008) has been re-
confirmed on multiple languages for which we hand-
built HPSG grammars exist, even where grammati-
cal coverage is low. Also, the gain is more signifi-
cant on out-of-domain tests, indicating that the hy-
brid system is more robust to cross-domain varia-
tion.
References
Adolphs, P., Oepen, S., Callmeier, U., Crysmann, B.,
Flickinger, D., & Kiefer, B. (2008). Some fine points
of hybrid natural language parsing. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation. Marrakech, Morocco.
Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado?, S.,
& Pinkal, M. (2006). The SALSA corpus: a German
corpus resource for lexical semantics. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation. Genoa, Italy.
35
 60
 65
 70
 75
 80
 85
 90
 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
Training Corpus Size (English)
Syn
SRL
Syn-ood
SRL-ood
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
Training Corpus Size (Czech)
Syn
SRL
Syn-ood
SRL-ood
Figure 2: Learning curves of syntactic dependency parser and semantic role labeler on Czech and English datasets
Callmeier, U. (2002). Preprocessing and encoding tech-
niques in PET. In S. Oepen, D. Flickinger, J. Tsujii, &
H. Uszkoreit (Eds.), Collaborative language engineer-
ing. A case study in efficient grammar-based process-
ing. Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Journal of Research on Language and Computa-
tion, 3(4), 281 ? 332.
Crysmann, B. (2005). Relative clause extraposition
in German. An efficient and portable implementation.
Research on Language and Computation, 3(1), 61 ?
82.
Flickinger, D. (2000). On building a more efficient gram-
mar by exploiting types. Natural Language Engineer-
ing, 6 (1), 15 ? 28.
Hajic?, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart??, M. A., Ma`rquez, L., Meyers, A., Nivre, J., Pado?,
S., S?te?pa?nek, J., Stran?a?k, P., Surdeanu, M., Xue, N.,
& Zhang, Y. (2009). The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning. Boulder,
CO, USA.
Hajic?, J., Panevova?, J., Hajic?ova?, E., Sgall, P., Pa-
jas, P., S?te?pa?nek, J., Havelka, J., Mikulova?, M., &
Z?abokrtsky?, Z. (2006). Prague Dependency Treebank
2.0 (Nos. Cat. No. LDC2006T01, ISBN 1-58563-370-
4). Philadelphia, PA, USA: Linguistic Data Consor-
tium.
Kawahara, D., Kurohashi, S., & Hasida, K. (2002). Con-
struction of a Japanese relevance-tagged corpus. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation (pp. 2008?2013).
Las Palmas, Canary Islands.
Malouf, R. (2002). A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings
of the 6th conferencde on natural language learning
(CoNLL 2002) (pp. 49?55). Taipei, Taiwan.
Marimon, M., Bel, N., & Seghezzi, N. (2007). Test suite
construction for a Spanish grammar. In T. H. King &
E. M. Bender (Eds.), Proceedings of the Grammar En-
gineering Across Frameworks workshop (p. 250-264).
Stanford, CA: CSLI Publications.
Oepen, S., & Carroll, J. (2000). Performance profiling for
parser engineering. Natural Language Engineering, 6
(1), 81 ? 97.
Palmer, M., Kingsbury, P., & Gildea, D. (2005). The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1), 71?106.
Palmer, M., & Xue, N. (2009). Adding semantic roles
to the Chinese Treebank. Natural Language Engineer-
ing, 15(1), 143?172.
Siegel, M., & Bender, E. M. (2002). Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd work-
shop on asian language resources and international
standardization at the 19th international conference
on computational linguistics. Taipei, Taiwan.
Surdeanu, M., Johansson, R., Meyers, A., Ma`rquez, L.,
& Nivre, J. (2008). The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proceedings of the 12th Conference on Computa-
tional Natural Language Learning. Manchester, UK.
Taule?, M., Mart??, M. A., & Recasens, M. (2008). An-
Cora: Multilevel Annotated Corpora for Catalan and
Spanish. In Proceedings of the 6th International Con-
ference on Language Resources and Evaluation. Mar-
rakesh, Morroco.
Toutanova, K., Manning, C. D., Flickinger, D., & Oepen,
S. (2005). Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on Language
and Computation, 3(1), 83 ? 105.
Zhang, Y., Oepen, S., & Carroll, J. (2007). Efficiency in
unification-based n-best parsing. In Proceedings of the
10th International Conference on Parsing Technolo-
gies (pp. 48 ? 59). Prague, Czech Republic.
Zhang, Y., Wang, R., & Uszkoreit, H. (2008). Hy-
brid Learning of Dependency Structures from Hetero-
geneous Linguistic Resources. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL 2008) (pp. 198?202). Manch-
ester, UK.
36
Proceedings of the 8th International Conference on Computational Semantics, pages 90?103,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Inference Rules for Recognizing Textual Entailment
Georgiana Dinu
Saarland University
dinu@coli.uni-sb.de
Rui Wang
Saarland University
rwang@coli.uni-sb.de
Abstract
In this paper, we explore the application of inference rules for rec-
ognizing textual entailment (RTE). We start with an automatically
acquired collection and then propose methods to refine it and obtain
more rules using a hand-crafted lexical resource. Following this, we
derive a dependency-based representation from texts, which aims to
provide a proper base for the inference rule application. The evalu-
ation of our approach on the RTE data shows promising results on
precision and the error analysis suggests future improvements.
1 Introduction
Textual inference plays an important role in many natural language pro-
cessing (NLP) tasks, such as question answering [7]. In recent years, the
recognizing textual entailment (RTE) [4] challenge, which focuses on de-
tecting semantic inference, has attracted a lot of attention. Given a text T
(several sentences) and a hypothesis H (one sentence), the goal is to detect
if H can be inferred from T.
Studies such as [3] attest that lexical substitution (e.g. synonyms, anto-
nyms) or simple syntactic variation accounts for the entailment only in a
small number of pairs. Thus, one essential issue is to identify more complex
expressions which, in appropriate contexts, convey the same (or similar)
meaning. More generally, we are also interested in pairs of expressions in
which only a uni-directional inference relation holds
1
.
A typical example is the following RTE pair in which accelerate to in H
is used as an alternative formulation for reach speed of in T.
1
We will use the term inference rule to stand for such concept; the two expressions can
be actual paraphrases if the relation is bi-directional
90
T: The high-speed train, scheduled for a trial run on Tuesday, is able to reach
a maximum speed of up to 430 kilometers per hour, or 119 meters per second.
H: The train accelerates to 430 kilometers per hour.
One way to deal with textual inference is through rule representation,
such as X wrote Y ? X is author of Y. However, manually building col-
lections of inference rules is time-consuming and it is unlikely that humans
can exhaustively enumerate all the rules encoding the knowledge needed in
reasoning with natural languages. Instead, an alternative is to acquire these
rules automatically from large corpora. Furthermore, given such a rule col-
lection, how to successfully use it in NLP applications is the next step to be
focused on.
For the first aspect, we extend and refine an existing collection of infer-
ence rules acquired based on the Distributional Hypothesis (DH). One of the
main advantages of using DH is that the only input needed is a large corpus
of (parsed) text
2
. For this purpose, a hand-crafted lexical resource is used
for augmenting the original inference rule collection and excluding some of
the incorrect rules.
For the second aspect, we focus on applying these rules to the RTE task.
In particular, we use a structure representation derived from the dependency
parse trees of T and H, which aims to capture the essential information they
convey.
The rest of the paper is organized as follows: Section 2 introduces the
inference rule collection we use, based on the Discovery of Inference Rules
from Text (henceforth DIRT) algorithm; we also discuss previous work on
applying it to the RTE task. Section 3 presents our analyses on the RTE
data and discusses two issues: the lack of rules and the difficulty of finding
proper ways of applying them. Section 4 proposes methods to extend and
refine the rule collection aiming at the former issue. To address the latter
issue, Section 5 describes the structure representation we use to identify the
appropriate context for the rule application. The experiments will be pre-
sented in Section 6, followed by an error analysis and discussions in Section
7. Finally, Section 8 will conclude the paper and point out some future
work.
2
Another line of work on acquiring paraphrases uses comparable corpora, for instance
[2], [12]
91
2 Background
A number of automatically acquired inference rule/paraphrase collections
are available, such as [14]. In our work we use the DIRT collection because
it is the largest one and it has a relatively good accuracy (in the 50% range,
[13]). In this section, we describe the DIRT algorithm for acquiring inference
rules. Following that, we will overview the RTE systems which take DIRT
as an external knowledge resource.
2.1 Discovery of Inference Rules from Text
The DIRT algorithm has been introduced by [10] and it is based on what
is called the Extended Distributional Hypothesis. The original DH states
that words occurring in similar contexts have similar meaning, whereas the
extended version hypothesizes that phrases occurring in similar contexts are
similar.
An inference rule in DIRT is a pair of binary relations ? pattern
1
(X,Y ),
pattern
2
(X,Y ) ? which stand in an inference relation. pattern
1
and pattern
2
are chains in Minipar [9] dependency trees while X and Y are placeholders for
nouns at the end of the chains. The two patterns will constitute a candidate
paraphrase if the sets of X and Y values exhibit relevant overlap. An example
is the pair (X
subj
??? prevent
obj
??? Y, X
subj
??? provide
obj
??? protection
mod
???
against
pcomp
????? Y).
Such rules can be defined [13] as directional relations between two text
patterns with variables. The left-hand-side pattern is assumed to entail
the right-hand-side pattern in certain contexts, under the same variable
instantiation. The definition relaxes the intuition of inference, as we only
require the entailment to hold in some but not all contexts, motivated by
the fact that such inferences occur often in natural text.
2.2 Related Work
Intuitively such inference rules should be effective for recognizing textual
entailment. However, only a small number of systems have used DIRT as
a resource in the RTE-3 challenge, and the experimental results have not
shown its great contribution.
In [3]?s approach, semantic parsing in clause representation is performed
and true entailment is decided only if every clause in the semantic repre-
sentation of T semantically matches some clause in H. The only variation
allowed consists of rewritings derived from WordNet and DIRT. Given the
92
preliminary stage of this system, the overall results show very low improve-
ment over a random classification baseline.
[1] implement a proof system using rules for generic linguistic structures,
lexical-based rules, and lexical-syntactic rules (which were obtained with the
DIRT algorithm applied to the first CD of the Reuters RCV1 corpus). Given
a premise p and a hypothesis h, the lexical-syntactic component marks all
lexical noun alignments. For every pair of alignments, the paths between
the two nouns are extracted, and the DIRT algorithm is applied to obtain
a similarity score. If the score is above a threshold, the rule will be ap-
plied. However, these lexical-syntactic rules are only used in about 3% of
the attempted proofs and for most cases there is no lexical variation.
[8] use DIRT in a more relaxed manner. A DIRT rule is employed in the
system if at least one of the anchors match in T and H, i.e. they use them
as unary rules. However, the analysis of the system shows that the DIRT
component is the least relevant one (adding 0.4% to the precision).
In [11]?s system, a paraphrase substitution step is added on top of a sys-
tem based on a tree alignment algorithm. The basic paraphrase substitution
method follows several steps. Initially, the two patterns of a rule are matched
in T and H (instantiations of the anchors X, Y do not have to match). The
T tree is transformed by applying the paraphrase substitution. Following
that, the transformed T tree and H tree are aligned. The coverage (pro-
portion of aligned content words) is computed and if above some threshold,
the entailment holds. The paraphrase component adds 1.0% to the result
on the development set and only 0.5% to the test set, but a more detailed
analysis on the interaction of this component with other components of the
system is not given.
3 Inference Rules for RTE
In this section our goal is to investigate the causes for which a resource
such as DIRT fails to bring clear improvements to RTE. The issues we have
encountered can be divided into two categories. Firstly, given a collection
of correct inference rules, making full use of the knowledge encoded in it is
not a trivial task. Secondly, some of the needed rules still lack even in a
very large collection such as DIRT. Section 4 will tackle the latter issue first
while Section 5 will focus on the former one.
93
3.1 DIRT Rules Found in the RTE Data
To Address this first issue, we begin with a straightforward experiment to
discover the number of pairs in the RTE data which contain rules present
in DIRT
3
.
Following the definition of an entailment rule, we identify RTE pairs in
which pattern
1
(w1, w2) and pattern
2
(w1, w2) are matched, one in T and
the other one in H, and thus, ?pattern
1
(X,Y ), pattern2(X,Y )? is an infer-
ence rule. The pair below is an example of this.
T: The sale was made to pay Yukos US$ 27.5 billion tax bill, Yuganskneftegaz
was originally sold for US$ 9.4 billion to a little known company Baikalfinans-
group which was later bought by the Russian state-owned oil company Rosneft.
H: Baikalfinansgroup was sold to Rosneft.
On average, only 2% of the pairs in the RTE data are subject to such
inference rules. Out of these, approximately 50% are lexical rules (one verb
entailing the other) and in the rest, around 50% are present in WordNet as
a synonym, hypernym or sister relation.
However, given the small number of inference rules identified this way,
we performed another analysis. This aims at determining an upper bound
of the number of pairs featuring entailment phrases present in a collection.
Given DIRT and the RTE data, we compute that in how many pairs two
patterns of a paraphrase can be matched irrespectively of their anchor val-
ues. An example is the following pair,
T: Libyas case against Britain and the US concerns the dispute over their
demand for extradition of Libyans charged with blowing up a Pan Am jet over
Lockerbie in 1988.
H: One case involved the extradition of Libyan suspects in the Pan Am Locker-
bie bombing.
This is a case in which the rule is correct and the entailment is positive.
In order to determine this, a system will have to know that Libya?s case
against Britain and the US in T entails one case in H. Similarly, in this
context, the dispute over their demand for extradition of Libyans charged
with blowing up a Pan Am jet over Lockerbie can be replaced with the
extradition of Libyan suspects in the Pan Am Lockerbie bombing. Altogether
3
For all the experiments in this paper, we use the DIRT collection provided by [10],
derived from the DIRT algorithm applied on 1GB of newstext.
94
X, founded in Y ? X, opened in Y
X launch Y ? X produce Y
X represent Z ? X work for Y
X faces menace from Y ? X endangered by Y
X, peace agreement for Y ? X is formulated to end war in Y
Table 1: Example of inference rules needed in RTE
in around 25% of the pairs, patterns of a rule can be found in this way, and
many times more than one rule in a pair. However, in many of these pairs,
finding out the patterns of an inference rule does not imply that the rule is
truly present in that pair.
Making use of the knowledge encoded with such rules is therefore, not
a trivial task. If rules are used strictly in concordance with their definition,
their utility is limited to a very small number of pairs. For this reason, 1)
instead of forcing the anchor values to be identical as most previous works,
we allow flexible rule matching (similar to [11]) and 2) furthermore, we
control the rule application process using a structure representation derived
from the dependency tree (Section 5).
3.2 Missing Rules
Apart from the issues underlined in the previous section, looking at the
data, we find it quite clear that DIRT lacks rules that many entailment
pairs require.
Table 1 gives a selection of rules that are needed in some entailment pairs.
The first three rows contain rules which are not structurally complex. These,
however, are missing from both DIRT and also other hand-crafted resources
such as WordNet (i.e. there is no short path connecting them). This is
to be expected as they are rules which hold in some specific contexts, but
difficult to be captured by a sense distinction of the lexical items involved.
The more complex rules are even more difficult to be captured by a DIRT-
like algorithm. Some of these do not occur frequently enough even in large
amounts of text to permit the acquirement of them via DH.
4 Extending and Refining DIRT
In order to address the issue of missing rules, we investigate the effects of
combining DIRT with an exact hand-coded lexical resource in order to create
new rules.
95
X face threat of Y X at risk of Y
face ? confront, front, look, face up risk ? danger, hazard, jeopardy
threat ? menace, terror, scourge
endangerment, peril
Table 2: Lexical variations creating new rules based on DIRT rule X face
threat of Y ? X at risk of Y
For this we extended the DIRT rules by adding rules in which any of the
lexical items involved in the patterns can be replaced by WordNet synonyms.
The idea behind this is that a combination of various lexical resources is
needed in order to cover the vast variety of phrases which humans can judge
to be in an inference relation.
In the example above, we consider the DIRT rule X face threat of Y
? X, at risk of Y (Table 2). Of course at this moment due to the lack
of sense disambiguation, our method introduces lots of rules that are not
correct. As one can see, expressions such as front scourge do not make any
sense, therefore any rules containing this will be incorrect. However some
of the new rules created in this example, such as X face threat of Y ? X,
at danger of Y are reasonable ones and the rules which are incorrect often
contain patterns that are very unlikely to occur in natural text.
The method just described allows us to identify the first three rules listed
in Table 1. We also acquire the rule X face menace of Y ? X endangered
by Y (via X face threat of Y ? X threatened by Y, menace ? threat,
threaten ? endanger). However the entailment pair requires a slightly
different version of the rule, involving the phrase face menace from.
Our extension is application-oriented therefore it is not intended to be
evaluated as an independent rule collection, but in an application scenario
such as RTE (Section 6).
Another issue that we address is the one of removing the most systematic
errors present in DIRT. DH algorithms have the main disadvantage that not
only phrases with the same meaning are extracted but also phrases with
opposite meaning.
In order to overcome this problem and since such errors are relatively
easy to detect, we applied a filter to the DIRT rules. This eliminates in-
ference rules which contain WordNet antonyms. To evaluate the precision
of our method, we randomly selected 200 examples of rules eliminated from
DIRT (irrespective of the textual entailment data) and a human evaluator
decided if they are indeed incorrect inference rules. Out of these 92% turned
96
out to be incorrect rules, such as X right about Y ? X wrong about Y. How-
ever, there are also cases of correct rules being eliminated, such as X have
second thoughts about Y ? X lack confidence about Y.
5 Inference Rules on Tree Skeletons
In order to address the issues described in Section 3.1, we choose to apply
the rule collection on a dependency-based representation of T and H. We
will first introduce this representation and the algorithm to derive it, and
following that we will describe how we applied the inference rules on this
structure.
Tree Skeletons
The Tree Skeleton (TS) structure was proposed by [15], and can be
viewed as an extended version of the predicate-argument structure. Since it
contains not only the predicate and its arguments, but also the dependency
paths in-between, it captures the essential part of the sentence.
Following their algorithm, we first preprocess the data using the Minipar
dependency parser and then select overlapping topic words (i.e. nouns) in
T and H (we use fuzzy match at the substring level instead of full match).
Starting with these nouns, we traverse the dependency tree to identify the
lowest common ancestor node (named as root node). This sub-tree without
the inner yield is defined as a Tree Skeleton. Figure 1 shows the TS of T in
the pair:
T For their discovery of ulcer-causing bacteria, Australian doctors Robin War-
ren and Barry Marshall have received the 2005 Nobel Prize in Physiology or Medicine.
H Robin Warren was awarded a Nobel Prize.
Notice that, in order to match the inference rules with two anchors, the
number of the dependency paths from the nouns to the root node should
also be two. In practice, tree skeletons can be extracted from approximately
30% of the T-H pairs.
Applying DIRT on a TS
After extracting the TS, the next step is to find the inference rules which
match the two tree skeletons of a T-H pair. This is done in a rather straight-
forward manner. Given tree skeletons of T and H, we check if the two left
dependency paths, the two right ones or the two root nodes contain the
patterns of a rule.
In the example above, the rule X
obj
??? receive
subj
???? Y ?X
obj2
???? award
obj1
????
97
Figure 1: Dependency structure of text. Tree skeleton in bold
Y satisfies this criterion, as it is matched at the root nodes. Notice that the
rule is correct only in restricted contexts, in which the object of receive is
something which is conferred on the basis of merit.
6 Experiments
Our experiments consist in predicting positive entailment in a very straight-
forward rule-based manner. For each collection we select the RTE pairs in
which we find a tree skeleton and match an inference rule. The first number
in our table entries represents how many of such pairs we have identified, out
of 1600 development and test pairs. For these pairs we simply predict pos-
itive entailment and the second entry represents what percentage of these
pairs are indeed true entailment. Our work does not focus on building a
complete RTE system but we also combine our method with a bag of words
baseline to see the effects on the entire data set.
In the first two columns (Table 3: Dirt
TS
and Dirt+WN
TS
) we consider
DIRT in its original state and DIRT with rules generated with WordNet as
described in Section 4; all precisions are higher than 63%
4
. After adding
WordNet, tree skeletons and rules are matched in approximately twice as
many pairs, while the precision is not harmed. This may indicate that our
method of adding rules does not decrease precision of an RTE system.
In the third column we report the results of using a set of rules containing
only the trivial identity ones (Id
TS
). For our current system, this can be
seen as a precision upper bound for all the other collections, in concordance
4
The RTE task is considered to be difficult. The average accuracy of the systems in
the RTE-3 challenge is around 61% [6]
98
RTE Set Dirt
TS
Dirt+WN
TS
Id
TS
Dirt+Id Dirt+Id
+WN
?
TS
+WN
?
TS
RTE2 55/0.63 103/0.65 45/0.66 136/0.65 90/0.67
RTE3 48/0.66 79/0.65 29/0.79 101/0.69 74/0.71
Table 3: Results on tree skeletons with various rule collections
with the fact that identical rules are nothing but inference rules of highest
possible confidence. The fourth column (Dirt+Id+WN
TS
) contains what
can be considered our best setting. In this setting three times as many
pairs are covered using a collection containing DIRT and identity rules with
WordNet extension. Although the precision results with this setting are
encouraging (65% for RTE2 data and 69% for RTE3 data), the coverage is
still low, 8% for RTE2 and 6% for RTE3. This aspect together with an error
analysis we performed are the focus of Section 7.
Another experiment aimed at improving the precision of our predictions.
For this we further restrict our method: we have a true entailment only if
applying the inference rule to a TS leaves no unmatched lexical items in the
fragment of the dependency path where it has been identified. The more re-
stricted method (Dirt+Id+WN
?
TS
) gives, as expected, better precision with
an approximately 30% loss in coverage.
At last, we also integrate our method with a bag of words baseline,
which calculates the ratio of overlapping words in T and H. For the pairs
that our method covers, we overrule the baseline?s decision. The results are
shown in Table 4. On the full data set, the improvement is still small due
to the low coverage of our method, however on the pairs that are covered by
our method, there is a significant improvement over the overlap baseline.
RTE Test(# pairs) BoW BoW&Main
RTE2 (89) 52.80% 60.67%
RTE2 (800) 56.87% 57.75%
RTE3 (71) 52.11% 59.15%
RTE3 (800) 61.12% 61.75%
Table 4: Results on RTE test data. Covered set and full set.
99
Source of error # pairs % pairs
TS structure 7 23%
Incorrect rules 9 30%
Other 14 47%
Table 5: Error analysis
7 Discussion
In this section we take a closer look at the data in order to better understand
how does our method of combining tree skeletons and inference rules work.
For error analysis we consider the pairs incorrectly classified in the RTE3
data, consisting of a total of 30 pairs. We classify the errors into three main
categories: tree skeleton structure errors, inference rule errors, and other
errors (Table 5).
In the first category, seven T-H pairs are incorrect. In those cases the
tree skeleton fails to match the corresponding anchors of the inference rules.
For instance, if someone founded the Institute of Mathematics (Instituto di
Matematica) at the University of Milan, it does not follow that they founded
The University of Milan.
Approximately 30% of the errors are caused by incorrect inference rules.
Out of these, two are correct in some contexts but not in the entailment
pairs in which they are found. For example, the following rule X generate Y
? X earn Y is used incorrectly, however in the restricted context of money
or income, the two verbs have similar meaning. An example of an incorrect
rule is X issue Y ? X hit Y since it is difficult to find a context in which
this holds.
The last category contains all the other errors. In all these cases, the
additional information conveyed by the text or the hypothesis which cannot
be captured by our current approach, affects the entailment. For example
an imitation diamond is not a diamond, and more than 1,000 members of
the Russian and foreign media does not entail more than 1,000 members
from Russia; these are not trivial, since lexical semantics and fine-grained
analysis of the restrictors are needed.
In a second part of our analysis we discuss the coverage issue, based on
an analysis of uncovered pairs. A main factor in failing to detect pairs in
which entailment rules should be applied is the fact that the tree skeleton
does not find the corresponding lexical items of two rule patterns. In one of
the pairs 78% increase in X entails X rose by 78%. This rule is available,
however the tree skeletons capture reach and rise as key verbal nodes. In
100
another example, information such as the fact that rains are creating flood-
ing and devastating are all necessary to conclude that floods are ravaging
Europe. However a structure like tree skeleton cannot capture all these el-
ements. Issues will occur even if the tree skeleton structure is modified to
align all the corresponding fragments together. Consider constructions with
embedding verbs such as manage, forget, attempt. Our method can detect
if the two embedded verbs convey a similar meaning, however not how the
embedding verbs affect the entailment. Independent of the shortcomings of
our tree skeleton structure, a second factor in failing to detect true entail-
ment still lies in lack of rules (e.g. the last two examples in Table 1 are
entailment pair fragments which can be formulated as inference rules, but
are not straightforward to acquire).
8 Conclusion
Throughout the paper we have identified important issues encountered in
using inference rules for recognizing textual entailment and proposed meth-
ods to solve them. We explored the possibility of combining a collection
obtained in a statistical, unsupervised manner, DIRT, with a hand-crafted
lexical resource in order to make inference rules have a larger contribution to
applications. We also investigated ways of effectively applying these rules.
The experiment results show that although coverage is still not satisfying,
the precision is promising. Therefore our method has the potential to be
successfully integrated into a larger entailment detection framework.
The error analysis points out several possible future directions. The tree
skeleton representation we used needs to be enhanced in order to capture
more accurately the relevant fragments of the text. A different issue remains
the fact that a lot of rules we could use for RTE are still lacking. A proper
study of the limitations of the DH as well as a classification of the knowl-
edge we want to encode as inference rules would be a step forward towards
solving this problem. Furthermore, although all the inference rules we used
aim at recognizing positive entailment cases, it is natural to use them for
detecting negative cases of entailment as well. In general, we can identify
pairs in which the patterns of an inference rule are present but the anchors
are missmatched, or they are not in the correct hypernym/hyponym rela-
tion. This can be the base of a principled method for detecting structural
contradictions [5].
101
References
[1] Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpektor, and Moshe Fried-
man. Semantic inference at the lexical-syntactic level for textual entailment
recognition. In Proceedings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 131?136, Prague, June 2007.
[2] Regina Barzilay and Kathleen R. McKeown. Extracting paraphrases from a
parallel corpus. In Proceedings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 50?57, Toulouse, France, July 2001.
[3] Peter Clark, Phil Harrison, John Thompson, William Murray, Jerry Hobbs,
and Christiane Fellbaum. On the role of lexical and world knowledge in rte3.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 54?59, June 2007.
[4] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising
textual entailment challenge. In Lecture Notes in Computer Science, Vol.
3944, Springer, pages 177?190. Quionero-Candela, J.; Dagan, I.; Magnini, B.;
d?Alch-Buc, F. Machine Learning Challenges, 2006.
[5] Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning.
Finding contradictions in text. In Proceedings of ACL-08: HLT, pages 1039?
1047, Columbus, Ohio, June 2008.
[6] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The
third pascal recognizing textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1?
9, Prague, June 2007.
[7] Sanda Harabagiu and Andrew Hickl. Methods for using textual entailment in
open-domain question answering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual Meeting of the ACL,
pages 905?912, Sydney, Australia, July 2006.
[8] Adrian Iftene and Alexandra Balahur-Dobrescu. Hypothesis transformation
and semantic variability rules used in recognizing textual entailment. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 125?130, Prague, June 2007.
[9] Dekang Lin. Dependency-based evaluation of minipar. In Proc. Workshop on
the Evaluation of Parsing Systems, Granada, 1998.
[10] Dekang Lin and Patrick Pantel. Dirt. discovery of inference rules from text. In
KDD ?01: Proceedings of the seventh ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 323?328, New York, USA,
2001.
102
[11] Erwin Marsi, Emiel Krahmer, and Wauter Bosma. Dependency-based para-
phrasing for recognizing textual entailment. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Paraphrasing, pages 83?88,
Prague, June 2007.
[12] Bo Pang, Kevin Knight, and Daniel Marcu. Syntax-based alignment of mul-
tiple translations: Extracting paraphrases and generating new sentences. In
HLT-NAACL, pages 102?109, 2003.
[13] Idan Szpektor, Eyal Shnarch, and Ido Dagan. Instance-based evaluation of
entailment rule acquisition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages 456?463, Prague, Czech
Republic, June 2007.
[14] Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. Scaling
web-based acquisition of entailment relations. In Proceedings of EMNLP, pages
41?48, 2004.
[15] Rui Wang and Gu?nter Neumann. Recognizing textual entailment using sen-
tence similarity based on dependency tree skeletons. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 36?
41, June 2007.
103
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 916?927, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Using Discourse Information for Paraphrase Extraction
Michaela Regneri
Dept. of Computational Linguistics
Saarland University
Saarbr?cken, Germany
regneri@coli.uni-saarland.de
Rui Wang
Language Technology Lab
DFKI GmbH
Saarbr?cken, Germany
ruiwang@dfki.de
Abstract
Previous work on paraphrase extraction us-
ing parallel or comparable corpora has gener-
ally not considered the documents? discourse
structure as a useful information source. We
propose a novel method for collecting para-
phrases relying on the sequential event or-
der in the discourse, using multiple sequence
alignment with a semantic similarity measure.
We show that adding discourse information
boosts the performance of sentence-level para-
phrase acquisition, which consequently gives
a tremendous advantage for extracting phrase-
level paraphrase fragments from matched sen-
tences. Our system beats an informed baseline
by a margin of 50%.
1 Introduction
It is widely agreed that identifying paraphrases is a
core task for natural language processing, including
applications like document summarization (Barzilay
et al1999), Recognizing Textual Entailment (Da-
gan et al2005), natural language generation (Zhao
et al2010; Ganitkevitch et al2011), and machine
translation (Marton et al2009). As a consequence,
many methods have been proposed for generating
large paraphrase resources (Lin and Pantel, 2001;
Szpektor et al2004; Dolan et al2004). One of
the intuitively appropriate data sources for such col-
lections are parallel or comparable corpora: if two
texts are translations of the same foreign document,
or if they describe the same underlying scenario,
they should contain a reasonable number of sentence
pairs that convey the same meaning.
Most approaches that extract paraphrases from
parallel texts employ some type of pattern match-
ing: sentences with the same meaning are assumed
to share many n-grams (Barzilay and Lee, 2003;
Callison-Burch, 2008, among others), many words
in their context (Barzilay and McKeown, 2001) or
certain slots in a dependency path (Lin and Pantel,
2001; Szpektor et al2004). Discourse structure
has only marginally been considered for this task:
For example, Dolan et al2004) extract the first
sentences from comparable articles and take them
as paraphrases. Another approach (Del?ger and
Zweigenbaum, 2009) matches similar paragraphs in
comparable texts, creating smaller comparable doc-
uments for paraphrase extraction.
We believe that discourse structure delivers im-
portant information for the extraction of para-
phrases. Sentences that play the same role in a cer-
tain discourse and have a similar discourse context
can be paraphrases, even if a semantic similarity
model does not consider them very similar. This ex-
tends the widely applied distributional hypothesis to
the discourse level: According to the distributional
hypothesis, entities are similar if they share similar
contexts. In our case, entities are whole sentences,
and contexts are discourse units.
Based on this assumption, we propose a novel
method for collecting paraphrases from parallel texts
using discourse information. We create a new type
of parallel corpus by collecting multiple summaries
for several TV show episodes. The discourse struc-
tures of those summaries are easy to compare: they
all contain the events in the same order as they
have appeared on the screen. This allows us to
take sentence order as event-based discourse struc-
ture, which is highly parallel for recaps of the same
episode.
In its first step, our system uses a sequence align-
916
ment algorithm combined with a state-of-the-art
similarity measure. The approach outperforms in-
formed baselines on the task of sentential paraphrase
identification. The usage of discourse information
even contributes more to the final performance than
the sentence similarity measure.
As second step, we extract phrase-level para-
phrase fragments from the matched sentences. This
step relies on the alignment algorithm?s output, and
we show that discourse information makes a big dif-
ference for the precision of the extraction. We then
add more discourse-based information by prepro-
cessing the text with a coreference resolution sys-
tem, which results in additional performance im-
provement.
The paper is structured as follows: first we sum-
marize related work (Sec. 2), and then we give an
overview over our perspective on the task and sketch
our system pipeline (Sec. 3). The following two sec-
tions describe the details of the sentence matching
step (Sec. 4) and the subsequent paraphrase frag-
ment extraction (Sec. 5). We present both automatic
and manual evaluation of the two system compo-
nents (Sec. 6). Finally, we conclude the paper and
give some hints for future work (Sec. 7).
2 Related Work
Previous paraphrase extraction approaches can be
roughly characterized under two aspects: 1) data
source and 2) granularity of the output.
Both parallel corpora and comparable corpora
have been quite well studied. Barzilay and McK-
eown (2001) use different English translations of
the same novels (i.e., monolingual parallel corpora),
while others (Quirk et al2004) experiment on mul-
tiple sources of the same news/events, i.e., mono-
lingual comparable corpora. Commonly used (can-
didate) comparable corpora are news articles writ-
ten by different news agencies within a limited time
window (Wang and Callison-Burch, 2011). Other
studies focus on extracting paraphrases from large
bilingual parallel corpora, which the machine trans-
lation (MT) community provides in many varieties.
Bannard and Callison-Burch (2005) as well as Zhao
et al2008) take one language as the pivot and
match two possible translations in the other lan-
guages as paraphrases if they share a common pivot
phrase. As parallel corpora have many alternative
ways of expressing the same foreign language con-
cept, large quantities of paraphrase pairs can be ex-
tracted.
The paraphrasing task is also strongly related to
cross-document event coreference resolution, which
is tackled by similar techniques used by the available
paraphrasing systems (Bagga and Baldwin, 1999;
Tomadaki and Salway, 2005).
Most work in paraphrase acquisition has dealt
with sentence-level paraphrases, e.g., (Barzilay and
McKeown, 2001; Barzilay and Lee, 2003; Dolan et
al., 2004; Quirk et al2004). Our approach for sen-
tential paraphrase extraction is related to the one in-
troduced by Barzilay and Lee (2003), who also em-
ploy multiple sequence alignment (MSA). However,
they use MSA at the sentence level rather than at the
discourse level.
We take some core ideas from our previous work
on mining script information (Regneri et al2010).
In this earlier work, we focused on event structures
and their possible realizations in natural language.
The corpus used in those experiments were short
crowd-sourced descriptions of everyday tasks writ-
ten in bullet point style. We aligned them with a
hand-crafted similarity measure that was specifically
designed for this text type. In this current work,
we target the general task of extracting paraphrases
for events rather than the much more specific script-
related task. The current approach uses a domain-
independent similarity measure instead of a specific
hand-crafted similarity score and is thus applicable
to standard texts.
From an applicational point of view, senten-
tial paraphrases are difficult to use in other NLP
tasks. At the phrasal level, interchangeable patterns
(Shinyama et al2002; Shinyama and Sekine, 2003)
or inference rules (Lin and Pantel, 2001) are ex-
tracted. In both cases, each pattern or rule contains
one or several slots, which are restricted to certain
type of words, e.g., named entities (NE) or content
words. They are quite successful in NE-centered
tasks, like information extraction, but their level of
generalization or coverage is insufficient for appli-
cations like Recognizing Textual Entailment (Dinu
and Wang, 2009).
The research on general paraphrase fragment ex-
traction at the sub-sentential level is mainly based
917
on phrase pair extraction techniques from the MT
literature. Munteanu and Marcu (2006) extract sub-
sentential translation pairs from comparable corpora
using the log-likelihood-ratio of word translation
probability. Quirk et al2007) extract fragments
using a generative model of noisy translations. Our
own work (Wang and Callison-Burch, 2011) extends
the first idea to paraphrase fragment extraction on
monolingual parallel and comparable corpora. Our
current approach also uses word-word alignment,
however, we use syntactic dependency trees to com-
pute grammatical fragments. Our use of dependency
trees is inspired by the constituent-tree-based exper-
iments of Callison-Burch (2008).
3 Paraphrases and Discourse
Previous approaches have shown that comparable
texts provide a good basis for paraphrase extrac-
tion. We want to show that discourse structure is
highly useful for precise and high-yield paraphrase
collection from such corpora. Consider the follow-
ing (made-up) example:
(1) [House keeps focusing on his aching leg.1.1.]
[The psychiatrist suggests him to get a hobby
1.2.] [House joins a cooking class.1.3]
(2) [He tells him that the Ibuprofen is not helping
with the pain.2.1.] [Nolan tells House to take up
a hobby.2.2] [Together with Wilson he goes to a
cookery course.2.3]
Read as a whole, it is clear that the two texts de-
scribe the same three events, in the same order, and
thus, e.g., 1.2 and 2.2 are paraphrases. However,
they share very few n-grams, nor named entities. We
determine three factors that can help to identify such
paraphrases:
1. Consider the sequence of events. A system
which recognizes that the three sentence pairs
occur in the same sequential event order would
have a chance of actually matching the sen-
tences.
2. Do coreference resolution. To determine
which sentence parts actually carry the same
meaning, pronoun resolution is essential (e.g.,
to match ?suggest him? and ?tells House?).
recaps 
of House 
M.D.
parallel corpus 
with parallel 
discourse 
structures
The psychiatrist suggests 
him to get a hobby 
Nolan tells House to take 
up a hobby.
sentence-level paraphrases
 + discourse information
 + semantic similarity 
 + word alignments 
 + coref. resolution
 + dependency trees 
 get a hobby 
take up a hobby
paraphrase 
fragments
1
2
3
Figure 1: System pipeline
3. Try a generic sentence similarity model. Pat-
tern matching or n-gram overlap might not be
sufficient to solve this problem.
Our system pipeline is sketched in Fig. 1:
1. Create a corpus: First, we create a compara-
ble corpus of texts with highly comparable dis-
course structures. Complete discourse struc-
tures like in the RST Discourse Treebank (Carl-
son et al2002) may be very useful for para-
phrase computation, however, they are hard to
obtain. Discourse annotation is difficult and
work-intensive, and full-blown automatic dis-
course parsers are neither robust nor very pre-
cise. To circumvent this problem, we assemble
documents that have parallel discourse struc-
tures by default: We compile multiple plot
summaries of TV show episodes. The textual
order of those summaries typically mirrors the
underlying event order of the episodes, in the
same sequence they happened on screen. We
take sentence sequences of recaps as parallel
discourse structures.
2. Extract sentence-level paraphrases: Our sys-
tem finds sentence pairs that are either para-
phrases themselves, or at least contain para-
phrase fragments. This procedure crucially re-
lies on discourse knowledge: A Multiple Se-
quence Alignment (MSA) algorithm matches
sentences if both their inherent semantic sim-
ilarities and the overall similarity score of their
discourse contexts are high enough.
3. Extract paraphrase fragments: Sentence-
level paraphrases may be too specific for fur-
ther domain-independent applications, as they
918
row recap 1 recap 2 recap 3 recap 4 recap 5
34
She gives Fore-
man one shot.
Cuddy tells Fore-
man he has one
chance to prove to
her he can run the
team.

Cuddy agrees
to give him one
chance to prove
himself.
Foreman insists he de-
serves a chance and
Cuddy gives in, warn-
ing him he gets one
shot.
35   
Foreman, Hadley,
and Taub get the
conference room
ready and Foreman
explains that he?ll
be in charge.
Foreman gives the
news to Thirteen
and Taub and they
unpack the conference
room and go with a
diagnosis of CRPS.
36
They decide that
it might be CRPS
and Foreman or-
ders a spinal stim-
ulation.

Foreman says to
treat him for com-
plex regional pain
syndrome with a
spinal stimulation.
 
Figure 2: Excerpt from an alignment table for 5 exemplary recaps of Episode 2 (Season 6).
contain specific NEs (e.g. ?House?) or time ref-
erences. Thus we take a necessary second step
and extract finer-grained paraphrase fragments
from the sentence pairs matched in step 2. The
resulting matched phrases should be grammat-
ical and interchangeable regardless of context.
We propose and compare different fragment ex-
traction algorithms.
The remainder of the paper shows how both of
the paraphrasing steps benefit from using a corpus
with highly parallel discourse structures: The sys-
tem components employ discourse information ei-
ther directly by using MSA (step 1) or coreference
resolution (step 2), or indirectly, because using MSA
in step 1 results in a high precision gain for the sub-
sequent second step.
4 Sentence Matching with MSA
This section explains how we apply MSA to ex-
tract sentence-level paraphrases from a comparable
corpus. As our input data, we manually collect re-
caps for House M.D. episodes from different sources
on the web1. House episodes have an intermediate
length (?45 min), which results in recaps of a con-
1e.g. http://house.wikia.com ? for a detailed list of
URLs, please check the supplementary material or contact the
authors.
venient size (40 to 150 sentences). The result is one
comparable document collection per episode. We
applied a sentence splitter (Gillick, 2009) to the doc-
uments and treat them as sequences of sentences for
further processing.
Sequence alignment takes as its input two se-
quences consisting of elements of some alphabet,
and an alphabet-specific score function cm over
pairs of sequence elements. For insertions and dele-
tions, the algorithm additionally takes gap costs
(cgap). Multiple Sequence Alignment generalizes
pairwise alignment to arbitrarily many sequences.
MSA has its main application area in bioinformat-
ics, where it is used to identify equivalent parts of
DNA (Durbin et al1998). Our alphabet consists of
sentences, and a sequence is an ordered sentence list
constituting a recap.
A Multiple Sequence Alignment results in a table
like Fig. 2. Each column contains the sentences of
one recap, possibly intermitted with gaps (??), and
each row contains at least one non-gap. If two sen-
tences end up in the same row, they are aligned; we
take aligned sentence to be paraphrases. Aligning a
sentence with a gap can be thought of as an insertion
or deletion. Each alignment has a score which is the
sum of all scores for substitutions and all costs for
insertions and deletions. Informally, the alignment
919
score is the sum of all scores for each pair of cells
(c1, c2), if c1 and c2 are in the same row. If either c1
or c2 is a gap, the pair?s score is cgap. If both cells
contain sentences, the score is cm(c1, c2).
Fern and Stevenson (2009) showed that sophis-
ticated similarity measures improve paraphrasing,
so we apply a state-of-the-art vector space model
(Thater et al2011) as our score function. The vec-
tor space model provides contextualized similarities
of words, i.e. the vector of each word is disam-
biguated by the context the current instance occurs
in. cm(c1, c2) returns the model?s similarity score
for c1 and c2.
We re-implement a standard MSA algorithm
(Needleman and Wunsch, 1970) which approxi-
mates the best MSA given the input sequences, cm
and cgap. This algorithm recursively aligns two se-
quences at a time, treating the resulting alignment
as a new sequence. This does not necessarily result
in the globally optimal alignment, because the order
in which sequences are aligned can change the final
output. Given this constraint, the algorithm finds the
best alignment, which - in our case - is the alignment
with the maximal score. Intuitively, we are looking
for the alignment where the most similar sentences
with the most similar preceding and trailing contexts
end up as paraphrases.
5 Paraphrase Fragment Extraction
Taking the output of the sentence alignment as in-
put, we next extract shorter phrase-level paraphrases
(paraphrase fragments) from the matched sentence
pairs. We try different algorithms for this step, all
relying on word-word alignments.
5.1 Preprocessing
Before extracting paraphrase fragments, we first pre-
process all documents as follows:
Stanford CoreNLP 2 provides a set of natural lan-
guage analysis tools. We use the part-of-
speech (POS) tagger, the named-entity recog-
nizer, the parser (Klein and Manning, 2003),
and the coreference resolution system (Lee et
al., 2011). In particular, the dependency struc-
tures of the parser?s output are used for VP-
2http://nlp.stanford.edu/software/
corenlp.shtml
fragment extraction (Sec. 5.3). The output from
the coreference resolution system is used to
cluster all mentions referring to the same en-
tity and to select one as the representative men-
tion. If the representative mention is not a pro-
noun, we modify the original texts by replac-
ing all pronoun mentions in the cluster with the
syntactic head of the representative mention.
Note that the coreference resolution system is
applied to each recap as a whole.
GIZA++ (Och and Ney, 2003) is a widely used
word aligner for MT systems. We amend the
input data by copying identical word pairs 10
times and adding them as additional ?sentence?
pairs (Byrne et al2003), in order to emphasize
the higher alignment probability between iden-
tical words. We run GIZA++ for bi-directional
word alignment and obtain a lexical translation
table.
5.2 Fragment Extraction
As mentioned in Sec. 2, we choose to use alignment-
based approaches to this task, which allows us to use
many existing MT techniques and tools. We mainly
follow our previous approach (Wang and Callison-
Burch, 2011), which is a modified version of an ap-
proach by Munteanu and Marcu (2006) on trans-
lation fragment extraction. We briefly review the
three-step procedure here and refer the reader to the
original paper for more details:
1. Establish word-word alignment between each
sentence pair using GIZA++;
2. Smooth the alignment based on lexical occur-
rence likelihood;
3. Extract fragment pairs using different heuris-
tics, e.g., non-overlapping n-grams, chunk
boundaries, or dependency trees.
After obtaining a lexical translation table by run-
ning GIZA++, for each word pair, w1 and w2, we
use both positive and negative lexical associations
for the alignment, which are defined as the condi-
tional probabilities p(w1|w2) and p(w1|?w2), re-
spectively. The resulting alignment can be further
constrained by a modified longest common sub-
string (LCS) algorithm, which takes sequences of
920
words instead of letters as input. Smoothing (step 2)
is done for each word by taking the average score of
it and its four neighbor words. All the word align-
ments (excluding stop-words) with positive scores
are selected as candidate fragment elements.
Provided with the candidate fragment elements,
we previously (Wang and Callison-Burch, 2011)
used a chunker3 to finalize the output fragments, in
order to follow the linguistic definition of a (para-)
phrase. We extend this step in the current system
by applying a dependency parser to constrain the
boundary of the fragments (Sec. 5.3). Finally, we
filter out trivial fragment pairs, such as identical or
the original sentence pairs.
5.3 VP-fragment Extraction
To obtain more grammatical output fragments, we
add another layer of linguistic information to our
input sentences. Based on the dependency parses
produced during preprocessing, we extract phrases
containing verbs and their complements. More pre-
cisely, we match two phrases if their respective sub-
trees t1 and t2 satisfy the following conditions:
? The subtrees mirror a complete subset of
the GIZA++ word alignment, i.e., all words
aligned to a given word in t1 are contained in
t2, and vice versa. For empty alignments, we
require an overlap of at least one lemma (ig-
noring stop words).
? The root nodes of t1 and t2 have the same
roles within their trees, e.g., we match clauses
with an xcomp-label only with other xcomp-
labelled clauses.
? Both t1 and t2 contain at least one verb with
at least one complement. To enhance recall,
we additionally extract complete prepositional
phrases.
? We exclude trivial fragment pairs that are pre-
fixes or suffixes of each other (or identical).
The main advantage of this approach lies in the out-
put?s grammaticality, because the subtrees always
match complete phrases. This method also functions
as a filtering mechanism for mistakenly aligned sen-
tences: If only the two sentence nodes are returned
3We use the same OpenNLP chunker (http:
//opennlp.sourceforge.net/) for consistency.
as possible matching partners, the pair is discarded
from the results.
6 Evaluation
We evaluate both sentential paraphrase matching
and paraphrase fragment extraction using manually
labelled gold standards (provided in the supplemen-
tary material). We collect recaps for all 20 episodes
of season 6 of House M.D., taking 8 summaries per
episode (the supplementary material contains a list
of all URLs). This results in 160 documents con-
taining 14735 sentences. For evaluation, we use all
episodes except no. 2, which is held out for parame-
ter optimizations and other development purposes.
6.1 Sentential Paraphrase Evaluation
To evaluate sentence matching, we adapt the base-
lines from our earlier work (Regneri et al2010) and
create a new gold standard. We compute precision,
recall and accuracy of our main system and suggest
baselines that separately show the influence of both
the MSA and the semantic scoring function.
Gold-Standard
We aim to create an evaluation set that contains
a sufficient amount of genuine paraphrases. Find-
ing such sentence pairs with random sampling and
manual annotation is infeasible: There are more than
200, 000, 000 possible sentence pairs, and we ex-
pect less than 1% of them to be paraphrases. We
thus sample pairs that either the system or the base-
lines recognized as paraphrases and try to create an
evaluation set that is not biased towards the actual
system or any of the baselines. The evaluation set
consists of 2000 sentence pairs: 400 that the system
recognized as paraphrases, 400 positively labelled
pairs for each of the three baselines (described in the
following section) and 400 randomly selected pairs.
For the final evaluation, we compute precision, re-
call, f-score and accuracy for our main system and
each baseline on this set.
Two annotators labelled each sentence pair
(S1, S2) with one of the following labels:
1. paraphrases: S1 and S2 refer to exactly the
same event(s).
2. containment: S1 contains all the event infor-
mation mentioned in S2, but refers to at least
921
one additional event, or vice versa.
3. related: S1 and S2 overlap in at least one event
reference, but both refer to at least one addi-
tional event.
4. unrelated: S1 and S2 do not overlap at all.
This scheme has a double purpose: The main objec-
tive is judging whether two sentences contain para-
phrases (1-3) or if they are unrelated (4). We use
this coarser distinction for system evaluation by col-
lapsing the categories 1-3 in one paraphrasecoll cat-
egory. Secondly, the annotation shows how well the
sentences fit each other?s content (1 vs. 2&3), and
how much work needs to be done to extract the sen-
tence parts with the same meaning (2 vs. 3).
The inter-annotator agreement according to Co-
hen?s Kappa (Cohen, 1960) is ? = 0.55 (?mod-
erate agreement?). The distinction between unre-
lated cases and elements of paraphrasecoll reaches
? = 0.71 (?substantial agreement?). For the final
gold standard, a third annotator resolved all conflict
cases.
Among all gold standard sentence pairs, we find
158 paraphrases, 238 containment cases, 194 re-
lated ones and 1402 unrelated. We had to discard 8
sentence pairs because one of the items was invalid
or empty. The high proportion of ?unrelated? cases
results from the 400 random pairs and the low pre-
cision of the baselines. Looking at the paraphrases,
27% of the 590 instances in the paraphrasecoll cate-
gory are proper paraphrases, and 73% of them con-
tain additional information that does not belong to
the paraphrased part.
Experimental Setup
We compute precision, recall and f-score with re-
spect to the gold standard (paraphrases are members
of paraphrasecoll), taking f-score as follows:
f -score =
2 ? precision ? recall
precision+ recall
We also compute accuracy as the overall fraction of
correct labels (negative and positive ones).
Our main system uses MSA (denoted by MSA af-
terwards) with vector-based similarities (VEC) as a
scoring function. The gap costs are optimized for
f-score, resulting in cgap = 0.4
To show the contribution of MSA?s structural
component and compare it to the vector model?s
contribution, we create a second MSA-based sys-
tem that uses MSA with BLEU scores (Papineni et
al., 2002) as scoring function (MSA+BLEU). BLEU
establishes the average 1-to-4-gram overlap of two
sentences. The gap costs for this baseline were opti-
mized separately, ending up with cgap = 1.
In order to quantify the contribution of the align-
ment, we create a discourse-unaware baseline by
dropping the MSA and using a state-of-the-art clus-
tering algorithm (Noack, 2007) fed with the vec-
tor space model scores (CLUSTER+VEC). The algo-
rithm partitions the set of sentences into paraphrase
clusters such that the most similar sentences end up
in one cluster. This does not require any parameter
tuning.
We also show a baseline that uses the cluster-
ing algorithm with BLEU scores (CLUSTER+BLEU).
The comparison of this baseline with the other
clustering-baseline that uses vector similarities helps
to underline the sentence similarities? advantage
compared to pure word overlap. Note that the CLUS-
TER+BLEU system resembles popular n-gram over-
lap measures for paraphrase classification.
We also show the results completely random label
assignment, which constitutes a lower bound for the
baselines and the system.
Results
Overall, our system extracts 20379 paraphrase
pairs. Tab. 1 shows the evaluation results on our
gold-standard.
The MSA based system variants outperform the
two clustering baselines significantly (all levels refer
to p = 0.01 and were tested with a resampling test
(Edgington, 1986)).
The clustering baselines perform significantly
better than a random baseline, especially consider-
ing recall. The more elaborated vector-space mea-
sure even gives 10% more in precision and accu-
racy, and overall 14% more in f-score. This is al-
4Gap costs directly influence precision and recall: ?cheap?
gaps lead to a more restrictive system with higher precision, and
more expensive gaps give more recall. We chose f-score as our
objective.
922
System Prec. Recall F-score Acc.
RANDOM 0.30 0.49 0.37 0.51
CLUSTER+BLEU 0.35 0.63 0.45 0.54
CLUSTER+VEC 0.40 0.68 0.51 0.61
MSA+BLEU 0.73 0.74 0.73 0.84
MSA+VEC 0.79 0.66 0.72 0.85
Table 1: Results for sentence matching.
ready a remarkable improvement compared to the
random baseline, and still a significant one com-
pared to CLUSTER+BLEU.
Adding structural knowledge with MSA im-
proves the clustering?s accuracy performance by
24% (CLUSTER+VEC vs. MSA+VEC), precision
even goes up by 39%.
Intuitively we expected the MSA-based systems
to end up with a higher recall than the clustering
baselines, because sentences can be matched even
if their similarity is moderate or low, but their dis-
course context is highly similar. However, this is
only the case for the system using BLEU scores, but
not for the system based on the vector space model.
One possible explanation lies in picking f-score as
objective for the optimization of the gap costs for
MSA: For the naturally more restrictive word over-
lap measure, this leads to a more recall-oriented
system with a low threshold for aligning sentences,
whereas the gap costs for the vector-based system
favors a more restrictive alignment with more pre-
cise results.
The comparison of the two MSA-based sys-
tems highlights the great benefit of using structural
knowledge: Both MSA+BLEU and MSA+VEC have
comparable f-scores and accuracy. The advantage
from using the vector-space model that is still obvi-
ous for the clustering baselines is nearly evened out
when adding discourse knowledge as a backbone.
However, the vector model still results in nominally
higher precision and accuracy.
It is hard to do a direct comparison with state-
of-the-art paraphrase recognition systems, because
most are evaluated on different corpora, e.g., the
Microsoft paraphrase corpus (Dolan and Brockett,
2005, MSR). We cannot apply our system to the
MSR corpus, because we take complete texts as in-
put, while the MSR corpus solely delivers sentence
pairs. While the MSR corpus is larger than our
collection, the wording variations in its paraphrase
pairs are usually lower than for our examples. Thus
the final numbers of previous approaches might be
vaguely comparable with our results: Das and Smith
(2009) present two systems reaching f-scores of 0.82
and 0.83, with a precision of 0.75 and 0.80. Both
precision and f-scores of our msa-based systems lie
within the same range. Heilman and Smith (2010)
introduce a recall-oriented system, which reaches an
f-score of 0.81 by a precision of 0.76. Compared to
this system, our approach results in better precision
values.
All further computations bases on the system us-
ing MSA and the vector space model (MSA+VEC),
because it achieves the highest precision and accu-
racy values.
6.2 Paraphrase Fragment Evaluation
We also manually evaluate precision on paraphrase
fragments, and additionally describe the productiv-
ity of the different setups, providing some intuition
about the methods? recall.
Gold-Standard
We randomly collect 150 fragment pairs for each
of the five system configurations (explained in the
following section). Each fragment pair (f1, f2) is
annotated with one of the following categories:
1. paraphrases: f1 and f2 convey the same
meaning, i.e., they are well-formed and good
matches on the content level.
2. related: f1 and f2 overlap in their meaning, but
one or both phrases have additional unmatched
information.
3. irrelevant: f1 and f2 are unrelated.
This labeling scheme again assesses precision as
well as paraphrase granularity. For precision rating,
we collapse categories 1&2 into one paraphrasecoll
category. Each pair is labelled by two annotators,
who were shown both the fragments and the whole
sentences they originate from. Overall, the raters
had an agreement of ? = 0.67 (?substantial agree-
ment?), which suggests that the task was easier than
sentence level annotation. The agreement for the
923
distinction between the paraphrasecoll categories
and irrelevant instances reaches a level of ? = 0.88
(also ?substantial agreement?). All conflicts were
again adjudicated by a third annotator. Overall, the
gold standard contains 190 paraphrases, 258 related
pairs and 302 irrelevant instances. Unlike previ-
ous approaches to fragment extraction, we do not
evaluate grammaticality, given that the VP-fragment
method implicitly constrains the output fragments to
be complete phrases.
Configurations & Results
We take the output of the sentence matching sys-
tem MSA+VEC as input for paraphrase fragment ex-
traction. As detailed in Sec. 5, our core fragment
module uses the word-word alignments provided by
GIZA++ and uses a chunker for fragment extrac-
tion. We successively enrich this core module with
more information, either by longest common sub-
string (LCS) matching or by operating on depen-
dency trees (VP). In addition, we evaluate the in-
fluence of coreference resolution by preprocessing
the input to the best performing configuration with
pronoun resolution (COREF).
We mainly compute precision for this task, as the
recall of paraphrase fragments is difficult to define.
However, we do include a measure we call produc-
tivity to indicate the algorithm?s completeness. It is
defined as the ratio between the number of result-
ing fragment pairs and the number of sentence pairs
used as input.
Extraction Method Precision Productivity
MSA 0.57 0.76
MSA+LCS 0.45 0.30
MSA+VP 0.81 0.42
MSA+VP+COREF 0.84 0.45
Table 2: Results of paraphrase fragment extraction.
Tab. 2 shows the evaluation results. We reach
our best precision by using the VP-fragment heuris-
tics, which is still more productive than the LCS
method. The grammatical filter gives us a higher
precision compared to the purely alignment-based
approaches. Enhancing the system with corefer-
ence resolution raises the score even further. We
cannot directly compare this performance to other
systems, as all other approaches have different data
sources. However, precision is usually manually
evaluated, so the figures are at least indicative for
a comparison with previous work: One state-of-the-
art system introduced by Zhao et al2008) extracts
paraphrase fragments from bilingual parallel cor-
pora and reaches a precision of 0.67. We found the
same number using our previous approach (Wang
and Callison-Burch, 2011), which is roughly equiv-
alent to our core module. Our approach outperforms
both by 17% with similar estimated productivity.
As a final comparison, we show how the perfor-
mance of the sentence matching methods directly af-
fects the fragment extraction. We use the VP-based
fragment extraction system (VP), and compare the
performances by using either the outputs from our
main system (MSA+VP) or alternatively the base-
line that replaces MSA with a clustering algorithm
(CLUSTER+VP). Both variants use the vector-based
semantic similarity measure.
Sentence matching Precision Productivity
CLUSTER+VP 0.31 0.04
MSA+VP 0.81 0.42
Table 3: Impact of MSA on fragment extraction
As shown in Tab. 3, the precision gain from using
MSA becomes tremendous during further process-
ing: We beat the baseline by 50% here, and produc-
tivity increases by a factor of 10. This means that the
baseline produces on average 0.01 good fragment
pairs per matched sentence pair, and the final sys-
tem extracts 0.3 of them. Those numbers show that
for any application that acquires paraphrases of arbi-
trary granularity, sequential event information pro-
vides an invaluable source to achieve a lean para-
phrasing method with high precision.
6.3 Example output
Fig. 3 shows exemplary results from our system
pipeline, using the VP?FRAGMENTS method with
full coreference resolution on the sentence pairs ex-
tracted by MSA. The results reflect the importance
of discourse information for this task: Sentences are
correctly matched in spite of not having common de-
924
Sentence 1 [with fragment 1] Sentence 2 [with fragment 2]
1 Taub meets House for dinner and claims [that
Rachel had a pottery class].
Taub shows up for his dinner with House without
Rachel, explaining [that she?s at a ceramics class].
2 House doesn?t want her to go and she doesn?t want
to go either, but [she can?t leave her family.]
Lydia admits that she doesn?t want to leave House but
[she has to stay with her family].
3 Thirteen is in a cab to the airport when she finds
out that [her trip had been canceled].
Hadley discovers that [her reservation has been can-
celled].
4 Nash asks House [for the extra morphine]. The patient is ready [for more morphine].
5 House comes in to tell Wilson that Tucker has can-
cer and [shows him the test results].
House comes in and [informs Wilson that the tests have
proven positive]: Tucker has cancer.
6 Foreman tells him [to confide in Cameron]. When Chase points out they can?t move Donny with-
out alerting Cameron, Foreman tells Chase [to be honest
with his wife].
7 Thirteen breaks [into the old residence] and tells
Taub that she realizes that he?s been with Maya.
Taub and Thirteen break [into Ted?s former residence].
8 He finds [a darkened patch on his right foot near
the big toe].
House finally finds [a tumorous mole on his toe].
Figure 3: Example results; fragments extracted from aligned sentences are bracketed and emphasized.
pendency patterns (e.g., Example 4) or sharing many
n-grams (6-8). Additionally, the coreference resolu-
tion allows us to match Rachel (1) and Wilson (5) to
the correct corresponding pronouns. All examples
show that this technique of matching sentence could
even help to make coreference resolution better, be-
cause we can easily identify Cameron with his wife,
Lydia with the respective pronouns, Nash with The
Patient or the nickname Thirteen with Hadley, the
character?s actual name.
7 Conclusion and Future Work
We presented our work on paraphrase extraction us-
ing discourse information, on a corpus consisting
of recaps of TV show episodes. Our approach first
uses MSA to extract sentential paraphrases, which
are then further processed to compute finer-grained
paraphrase fragments using dependency trees and
pronoun resolution. The experimental results show
great advantages from using discourse information,
beating informed baselines and performing compet-
itively with state-of-the-art systems.
For future work, we plan to use MSA to align
single clauses rather than whole sentences. This
can also help to define the fragment boundaries
more clearly. Additionally, we plan to generalize
the method for other parallel texts by preprocessing
them with a temporal classifier. In a more advanced
step, we will also use the aligned paraphrases to help
resolving discourse structure, e.g. for coreference
resolution, which could lead to a high-performance
bootstrapping system. In a long-term view, it would
be interesting to see how aligned discourse trees
could help to extract paraphrases from arbitrary par-
allel text.
Acknowledgements
The first author was funded by the Cluster
of Excellence ?Multimodal Computing and In-
teraction? in the German Excellence Initiative.
The second Author was funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
No. 287923 (EXCITEMENT, http://www.
excitement-project.eu/). ? We want to
thank Stefan Thater for supplying the semantic sim-
ilarity scores of his algorithm for our data. We are
grateful to Manfred Pinkal, Alexis Palmer and three
anonymous reviewers for their helpful comments on
previous versions of this paper.
925
References
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL 2005.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of HLT-NAACL 2003.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proc.
of ACL 2001.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of
ACL 1999.
W. Byrne, S. Khudanpur, W. Kim, S. Kumar, P. Pecina,
P. Virga, P. Xu, and D. Yarowsky. 2003. The Johns
Hopkins University 2003 Chinese-English machine
translation system. In Proceedings of the MT Summit
IX.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP 2008.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2002. RST Discourse Treebank. LDC.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In MLCW, pages 177?190.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proceedings of ACL-IJCNLP 2009.
Louise Del?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the ACL-IJCNLP BUCC-2009 Workshop.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entailment.
In Proceedings of EACL 2009.
W. B. Dolan and C. Brockett. 2005. Automatically con-
structing a corpus of sentential paraphrases. In Pro-
ceedings of the third International Workshop on Para-
phrasing.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING 2004.
Richard Durbin, Sean Eddy, Anders Krogh, and Graeme
Mitchison. 1998. Biological Sequence Analysis.
Cambridge University Press.
Eugene S Edgington. 1986. Randomization tests. Mar-
cel Dekker, Inc., New York, NY, USA.
Samuel Fern and Mark Stevenson. 2009. A semantic
similarity approach to paraphrase detection. In Pro-
ceedings of the Computational Linguistics UK (CLUK
2008) 11th Annual Research Colloquium.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP
2011.
Dan Gillick. 2009. Sentence boundary detection and the
problem with the u.s. In Proceedings of HLT-NAACL
2009: Companion Volume: Short Papers.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL-HLT 2010.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL 2003.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the CoNLL-2011 Shared Task.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proceedings of the
ACM SIGKDD.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation Using
Monolingually-Derived Paraphrases. In Proceedings
of EMNLP 2009.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting Parallel Sub-Sentential Fragments from Non-
Parallel Corpora. In Proceedings of ACL 2006.
Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
molecular biology, 48(3), March.
Andreas Noack. 2007. Energy models for graph cluster-
ing. Journal of Graph Algorithms and Applications,
11(2):453?480.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
926
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP 2004.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning Script Knowledge with Web
Experiments. In Proceedings of ACL 2010.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Proceedings
of the ACL PARAPHRASE ?03 Workshop.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT 2002.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP
2004.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2011. Word Meaning in Context: A Simple and Effec-
tive Vector Model. In Proceedings of IJCNLP 2011.
Eleftheria Tomadaki and Andrew Salway. 2005. Match-
ing verb attributes for cross-document event corefer-
ence. In Proc. of the Interdisciplinary Workshop on
the Identification and Representation of Verb Features
and Verb Classes.
Rui Wang and Chris Callison-Burch. 2011. Para-
phrase fragment extraction from monolingual compa-
rable corpora. In Proc. of the ACL BUCC-2011 Work-
shop.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase Pat-
terns from Bilingual Corpora. In Proceedings of ACL
2008.
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging Multiple MT Engines for Para-
phrase Generation. In Proceedings of COLING 2010.
927
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845?850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Converting Continuous-Space Language Models into
N-gram Language Models for Statistical Machine Translation
Rui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,3
1 Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2 Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3 MOE-Microsoft Key Lab. for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong Unviersity, Shanghai 200240 China
wangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cn
Abstract
Neural network language models, or
continuous-space language models (CSLMs),
have been shown to improve the performance
of statistical machine translation (SMT)
when they are used for reranking n-best
translations. However, CSLMs have not
been used in the first pass decoding of SMT,
because using CSLMs in decoding takes a lot
of time. In contrast, we propose a method
for converting CSLMs into back-off n-gram
language models (BNLMs) so that we can
use converted CSLMs in decoding. We show
that they outperform the original BNLMs and
are comparable with the traditional use of
CSLMs in reranking.
1 Introduction
Language models are important in natural language
processing tasks such as speech recognition and
statistical machine translation. Traditionally, back-
off n-gram language models (BNLMs) (Chen and
Goodman, 1996; Chen and Goodman, 1998;
Stolcke, 2002) are being widely used for these tasks.
Recently, neural network language models,
or continuous-space language models (CSLMs)
(Bengio et al, 2003; Schwenk, 2007; Le et al, 2011)
are being used in statistical machine translation
(SMT) (Schwenk et al, 2006; Son et al, 2010;
Schwenk et al, 2012; Son et al, 2012; Niehues
and Waibel, 2012). These works have shown that
CSLMs can improve the BLEU (Papineni et al,
2002) scores of SMT when compared with BNLMs,
on the condition that the training data for language
modeling are the same size. However, in practice,
CSLMs have not been widely used in SMT.
One reason is that the computational costs of
training and using CSLMs are very high. Various
methods have been proposed to tackle the training
cost issues (Son et al, 2010; Schwenk et al, 2012;
Mikolov et al, 2011). However, there has been little
work on reducing using costs. Since the using costs
of CSLMs are very high, it is difficult to use CSLMs
in decoding directly.
A common approach in SMT using CSLMs is
the two pass approach, or n-best reranking. In this
approach, the first pass uses a BNLM in decoding
to produce an n-best list. Then, a CSLM is used to
rerank those n-best translations in the second pass.
(Schwenk et al, 2006; Son et al, 2010; Schwenk et
al., 2012; Son et al, 2012)
Another approach is using restricted Boltzmann
machines (RBMs) (Niehues and Waibel, 2012)
instead of using multi-layer neural networks
(Bengio et al, 2003; Schwenk, 2007; Le et al,
2011). Since probability in a RBM can be calculated
very efficiently (Niehues and Waibel, 2012), they
can use the RBM language model in SMT decoding.
However, the RBM was just used in an adaptation of
SMT, not in a large SMT task, because the training
costs of RBMs are very high.
The last approach is using a BNLM to simulate
a CSLM (Deoras et al, 2011; Arsoy et al, 2013).
(Deoras et al, 2011) used a recurrent neural network
language model (RNNLM) to generate a large
amount of text, which was generated by sampling
words from the probability distributions calculated
by the RNNLM. Then, they trained the BNLM
845
from the text using the interpolated Kneser-Ney
smoothing method. (Arsoy et al, 2013) converted
neural network language models of increasing order
to pruned back-off language models, using lower-
order models to constrain the n-grams allowed in
higher-order models.
Both of these methods were used in decoding for
speech recognition. These methods were applied
to not-so-large scale experiments (55 million (M)
words for training their BNLMs) (Arsoy et al,
2013). In contrast, our method is applied to SMT
and can be used to improve a BNLM created from
746 M words by using a CSLM trained from 42 M
words.
Because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs, improving a BNLM by using a CSLM
trained from a smaller corpus is very important.
Actually, a CSLM trained from a smaller corpus
can improve the BLEU scores of SMT if it is used
in the n-best reranking (Schwenk, 2010; Huang et
al., 2013). In contrast, we will demonstrate that a
BNLM simulating a CSLM can improve the BLEU
scores of SMT in the first pass decoding.
Our approach is as follows: (1) First, we train a
CSLM (Schwenk, 2007) from a corpus. (2) Second,
we also train a BNLM from the same corpus or
larger corpus. (3) Finally, we rewrite the probability
of each n-gram of the BNLM with that probability
calculated from the CSLM.We also re-normalize the
probabilities of the BNLM, then use the re-written
BNLM in SMT decoding.
In Section 2, we describe the BNLM and CSLM
(Schwenk, 2010) used for re-writing BNLMs. In
Section 3, we describe the method of converting
a CSLM into a BNLM. In Sections 4 and 5, we
evaluate our method and conclude.
2 Language Models
In this section, we will introduce the standard
BNLM and CSLM structure and probability
calculation.
2.1 Standard back-off ngram language model
A BNLM predicts the probability of a wordwi given
its preceding n ? 1 words hi = wi?1i?n+1. But
it will suffer from data sparseness if the context,
hi, does not appear in the training data. So an
estimation by ?backing-off? to models with smaller
histories is necessary. In the case of the modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
the probability of wi given hi under a BNLM,
Pb(wi|hi), is:
Pb(wi|hi) = P?b(wi|hi) + ?(hi)Pb(wi|wi?1i?n+2) (1)
where P?b(wi|hi) is a discounted probability and
?(hi) is the back-off weight. A BNLM is used with
a CSLM as shown below.
2.2 CSLM structure and probability
calculation
The main structure of a CSLM using a multi-
layer neural network contains four layers: the input
layer projects all words in the context hi onto
the projection layer (the first hidden layer); the
second hidden layer and the output layer achieve the
non-liner probability estimation and calculate the
language model probability P (wi|hi) for the given
context. (Schwenk, 2007).
The CSLM calculates the probabilities of all
words in the vocabulary of the corpus given
the context at once. However, because the
computational complexity of calculating the
probabilities of all words is quite high, the CSLM is
only used to calculate the probabilities of a subset
of the whole vocabulary. This subset is called
a short-list, which consists of the most frequent
words in the vocabulary. The CSLM also calculates
the sum of the probabilities of all words not in the
short-list by assigning a neuron for that purpose.
The probabilities of other words not in the short-list
are obtained from a BNLM (Schwenk, 2007;
Schwenk, 2010).
Let wi, hi be the current word and history. The
CSLM with a BNLM calculates the probability of
wi given hi, P (wi|hi), as follows:
P (wi|hi) =
{
Pc(wi|hi)
1?Pc(o|hi)
Ps(hi) if wi ? short-list
Pb(wi|hi) otherwise
(2)
where Pc(?) is the probability calculated by the
CSLM, Pc(o|hi) is the probability of the neuron
for the words not in the short-list, Pb(?) is the
probability calculated by the BNLM as in Eq. 1,
and
Ps(hi) =
?
v?short-list
Pb(v|hi). (3)
846
It can be considered that the CSLM redistributes
the probability mass of all words in the short-list.
This probability mass is calculated by using the
BNLM.
3 Conversion of CSLM into BNLM
As described in the introduction, we first train a
CSLM from a corpus. We also train a BNLM from
the same corpus or a larger corpus. Then, we rewrite
the probability of each ngram in the BNLM with the
probability calculated from the CSLM.
First, we use the probabilities of 1-grams in
the BNLM as they are. Next, we rewrite the
probabilities of n-grams (n=2,3,4,5) in the BNLM
with the probabilities calculated by using the n-gram
CSLM, respectively. Note that the n-gram CSLM
means that the length of its history is n ? 1. Note
also that we only need to rewrite the probabilities
of n-grams ending with a word in the short-list.
Finally, we re-normalize the probabilities of the
BNLM using the SRILM?s ?-renorm? option.
When we rewrite a BNLM trained from a larger
corpus, the ngrams in the BNLM often contain
unknown words for the CSLM. In that case, we use
the probabilities in the BNLM as they are.
4 Experiments
4.1 Common settings
We used the patent data for the Chinese to English
patent translation subtask from the NTCIR-9 patent
translation task (Goto et al, 2011). The parallel
training, development, and test data consisted of 1
M, 2,000, and 2,000 sentences, respectively.
We followed the settings of the NTCIR-9 Chinese
to English translation baseline system (Goto et al,
2011) except that we used various language models
to compare them. We used the MOSES phrase-
based SMT system (Koehn et al, 2003), together
with Giza++ (Och and Ney, 2003) for alignment and
MERT (Och, 2003) for tuning on the development
data. The translation performance was measured by
the case-insensitive BLEU scores on the tokenized
test data. We used mteval-v13a.pl for
calculating BLEU scores.1
1It is available at http://www.itl.nist.gov/iad/
mig/tests/mt/2009/
We used the 14 standard SMT features: five
translation model scores, one word penalty score,
seven distortion scores and one language model
score. Each of the different language models was
used to calculate the language model score.
As the baseline BNLM, we trained a 5-gram
BNLM with modified Kneser-Ney smoothing using
the English side of the 1 M sentences training data,
which consisted of 42 M words. We did not discard
any n-grams in training this model. That is, we
did not use count cutoffs. We call this BNLM as
BNLM42.
A 5-gram CSLM was trained on the same
1 M training sentences using the CSLM toolkit
(Schwenk, 2010). The settings for the CSLM
were: projection layer of dimension 256 for each
word, hidden layer of dimension 384 and output
layer (short-list) of dimension 8192, which were
recommended in the CSLM toolkit. We call this
CSLM CSLM42. CSLM42 used BNLM42 as the
background BNLM.
We also trained a larger 5-gram BNLM with
modified Kneser-Ney smoothing by adding
sentences from the 2005 US patent data distributed
in the NTCIR-8 patent translation task (Fujii et al,
2010) to the 42 M words. The data consisted of
746 M words. We call this BNLM BNLM746. We
discarded 3,4,5-grams that occurred only once when
we created BNLM746.
Next, we re-wrote BNLM42 with CSLM42 by
using the method described in Section 3. This
re-written BNLM was interpolated with BNLM42.
The interpolation weight was determined by the grid
search. That is, we changed the interpolation weight
to 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated
BNLM. Then we used that BNLM in the SMT
system to tune the weight parameters on the first
half of the development data. Next, we selected
the interpolation weight that obtained the highest
BLEU score on the second half of the development
data. After we selected the interpolation weight,
we applied MERT again to the 2,000 sentence
development data to tune the weight parameters.2
We call this BNLM CONV42. We also obtained
CONV746 by re-writing BNLM746 with CSLM42
2We aware that the interpolation weight might be
determined by minimizing the perplexity on the development
data. However, we opted to directly maximize the BLEU score.
847
in the same way.
The vocabulary of these language models was the
same, which was extracted from the 1 M training
sentences.
4.2 Experimental results
Table 1 shows the percent BLEU scores on the test
data. The figures in the ?1st pass? column show
the BLEU scores in the first pass decoding when
we changed the language model. The figures in the
?reranking? column show the BLEU scores when
we applied CSLM42 to rerank the 100-best lists for
the different language models. When we applied
CSLM42 for reranking, we added the CSLM42
score as the additional 15th feature. The weight
parameters were tuned by using Z-MERT (Zaidan,
2009).
LMs 1st pass rerank
BNLM42 31.60 32.44
CONV42 32.58 32.98
BNLM746 32.83 33.36
CONV746 33.22 33.54
Table 1: Comparison of BLEU scores
We also performed the paired bootstrap re-
sampling test (Koehn, 2004).3 We sampled 2000
samples for each significance test.
Table 2 shows the results of a statistical
significance test, in which the ?1st? is short for
the ?1st pass?. The marks indicate whether the
LM to the left of a mark is significantly better
than that above the mark at a certain level. (???:
significantly better at ? = 0.01, ?>?: ? = 0.05,
???: not significantly better at ? = 0.05)
First, as shown in the tables, the reranking
by applying CSLM42 increased the BLEU scores
for all language models. This observation is in
accordance with those of previous work (Schwenk,
2010; Huang et al, 2013).
Second, the reranking results of BNLM42 (32.44)
were not better than those of the first pass of
BNLM746 (32.83). This indicates that if the
underlying BNLM is made from a small corpus, the
reranking using CSLM can not compensate for it.
3We used the code available at http://www.ark.cs.
cmu.edu/MT/.
BN
LM
74
6
(re
ra
nk
)
CO
N
V
74
6
(1
st)
CO
N
V
42
(re
ra
nk
)
BN
LM
74
6
(1
st)
CO
N
V
42
(1
st)
BN
LM
42
(re
ra
nk
)
BN
LM
42
(1
st)
CONV746 (rerank) ? ? ? ? ? ? ?
BNLM746 (rerank) ? ? > ? ? ?
CONV746 (1st) ? ? ? ? ?
CONV42 (rerank) ? ? ? ?
BNLM746 (1st) ? ? ?
CONV42 (1st) ? ?
BNLM42 (rerank) ?
Table 2: Significance tests for systems with different LMs
Third, CONV42 was better than BNLM42 for
both first-pass and reranking. This also holds in the
case of CONV746 and BNLM746. This indicated
that our conversion method improved the BNLMs,
even if the underlying BNLMwas trained on a larger
corpus than that used for training the CSLM. As
described in the introduction, this is very important
because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs. This observation has not been found in the
previous work.
In addition, the first-pass of CONV42 and
CONV746 (32.58 and 33.22) were comparable with
those of the reranking results of BNLM42 and
BNLM746 (32.44 and 33.36), respectively. That is,
there were no significant differences between these
results. This indicates that our conversion method
preserves the performance of the reranking using
CSLM.
5 Conclusion
We have proposed a method for converting CSLMs
into BNLMs. The method can be used to improve
a BNLM by using a CSLM trained from a smaller
corpus than that used for training the BNLM. We
have also shown that BNLMs created by our method
performs as good as the reranking using CSLMs.
Our future work is to compare our conversion
method with that of (Arsoy et al, 2013).4
4We aware that (Arsoy et al, 2013) compared their method
with the one that is identical with our method. However, the
experiments were conducted on a speech recognition task and
the scale of the experiment was not so large. Since we noticed
their work just before the submission of our paper, we did not
have time to compare their method with our method in SMT.
848
Acknowledgments
We appreciate the helpful discussion with Andrew
Finch and Paul Dixon, and three anonymous
reviewers for many invaluable comments and
suggestions to improve our paper. This work
is supported by the National Natural Science
Foundation of China (Grant No. 60903119, No.
61170114 and No. 61272248), the National
Basic Research Program of China (Grant No.
2013CB329401) and the Science and Technology
Commission of Shanghai Municipality (Grant No.
13511500200).
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural network
language models into back-off language models for
efficient decoding in automatic speech recognition.
In Proc. of IEEE Int. Conf. on Acoustics, Speech
and Signal Processing (ICASSP 2013), Vancouver,
Canada, May. IEEE.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic
language model. Journal of Machine Learning
Research (JMLR), 3:1137?1155, March.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, ACL
?96, pages 310?318, Santa Cruz, California, June.
Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard Univ.
A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat,
and Sanjeev Khudanpur. 2011. Variational
approximation of long-span language models for lvcsr.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 5532?
5535, Prague, Czech Republic, May. IEEE.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meeting
on Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and Cross-
lingual Information Access, pages 293?302, Tokyo,
Japan, June.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9 Workshop Meeting, pages
559?578, Tokyo, Japan, December.
Zhongqiang Huang, Jacob Devlin, and Spyros
Matsoukas. 2013. Bbn?s systems for the chinese-
english sub-task of the ntcir-10 patentmt evaluation.
In NTCIR-10, Tokyo, Japan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural
network language model. In Acoustics, Speech and
Signal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527, Prague, Czech
Republic, May. IEEE.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernock. 2011. Strategies for
training large scale neural network language models.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 196?
201, Prague, Czech Republic, May. IEEE.
Jan Niehues and Alex Waibel. 2012. Continuous space
language models using restricted boltzmann machines.
In Proceedings of the International Workshop for
Spoken Language Translation, IWSLT 2012, pages
311?318, Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
849
318, Philadelphia, Pennsylvania, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL on Main conference poster
sessions, COLING-ACL ?06, pages 723?730, Sydney,
Australia, July. Association for Computational
Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of LanguageModeling for HLT,
WLM ?12, pages 11?19, Montreal, Canada, June.
Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, pages 137?146.
Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski,
and Franc?ois Yvon. 2010. Training continuous
space language models: some practical issues. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 778?788, Cambridge, Massachusetts,
October. Association for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ?12, pages
39?48, Montreal, Canada, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International
Conference on Spoken Language Processing, pages
257?286, November.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
850
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Neural Network Based Bilingual Language Model Growing
for Statistical Machine Translation
Rui Wang
1,3,?
, Hai Zhao
1,3
, Bao-Liang Lu
1,3
, Masao Utiyama
2
and Eiichro Sumita
2
1
Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai, 200240, China
2
Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology,
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3
Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China
wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,
{mutiyama, eiichiro.sumita}@nict.go.jp
Abstract
Since larger n-gram Language Model
(LM) usually performs better in Statistical
Machine Translation (SMT), how to con-
struct efficient large LM is an important
topic in SMT. However, most of the ex-
isting LM growing methods need an extra
monolingual corpus, where additional LM
adaption technology is necessary. In this
paper, we propose a novel neural network
based bilingual LM growing method, only
using the bilingual parallel corpus in SMT.
The results show that our method can im-
prove both the perplexity score for LM e-
valuation and BLEU score for SMT, and
significantly outperforms the existing LM
growing methods without extra corpus.
1 Introduction
?Language Model (LM) Growing? refers to adding
n-grams outside the corpus together with their
probabilities into the original LM. This operation
is useful as it can make LM perform better through
letting it become larger and larger, by only using a
small training corpus.
There are various methods for adding n-grams
selected by different criteria from a monolingual
corpus (Ristad and Thomas, 1995; Niesler and
Woodland, 1996; Siu and Ostendorf, 2000; Si-
ivola et al., 2007). However, all of these approach-
es need additional corpora. Meanwhile the extra
corpora from different domains will not result in
better LMs (Clarkson and Robinson, 1997; Iyer et
al., 1997; Bellegarda, 2004; Koehn and Schroeder,
?
Part of this work was done as Rui Wang visited in NICT.
2007). In addition, it is very difficult or even im-
possible to collect an extra large corpus for some
special domains such as the TED corpus (Cettolo
et al., 2012) or for some rare languages. There-
fore, to improve the performance of LMs, without
assistance of extra corpus, is one of important re-
search topics in SMT.
Recently, Continues Space Language Model
(CSLM), especially Neural Network based Lan-
guage Model (NNLM) (Bengio et al., 2003;
Schwenk, 2007; Mikolov et al., 2010; Le et al.,
2011), is being actively used in SMT (Schwenk
et al., 2006; Son et al., 2010; Schwenk, 2010;
Schwenk et al., 2012; Son et al., 2012; Niehues
and Waibel, 2012). One of the main advantages
of CSLM is that it can more accurately predic-
t the probabilities of the n-grams, which are not in
the training corpus. However, in practice, CSLM-
s have not been widely used in the current SMT
systems, due to their too high computational cost.
Vaswani and colleagues (2013) propose a
method for reducing the training cost of CSLM
and apply it to SMT decoder. However, they do
not show their improvement for decoding speed,
and their method is still slower than the n-gram
LM. There are several other methods for attempt-
ing to implement neural network based LM or
translation model for SMT (Devlin et al., 2014;
Liu et al., 2014; Auli et al., 2013). However, the
decoding speed using n-gram LM is still state-of-
the-art one. Some approaches calculate the prob-
abilities of the n-grams n-grams before decoding,
and store them in the n-gram format (Wang et al.,
2013a; Arsoy et al., 2013; Arsoy et al., 2014). The
?converted CSLM? can be directly used in SMT.
Though more n-grams which are not in the train-
189
ing corpus can be generated by using some of
these ?converting? methods, these methods only
consider the monolingual information, and do not
take the bilingual information into account.
We observe that the translation output of a
phrase-based SMT system is concatenation of
phrases from the phrase table, whose probabilities
can be calculated by CSLM. Based on this obser-
vation, a novel neural network based bilingual LM
growing method is proposed using the ?connecting
phrases?. The remainder of this paper is organized
as follows: In Section 2, we will review the exist-
ing CSLM converting methods. The new neural
network based bilingual LM growing method will
be proposed in Section 3. In Section 4, the exper-
iments will be conducted and the results will be
analyzed. We will conclude our work in Section
5.
2 Existing CSLM Converting Methods
Traditional Backoff N -gram LMs (BNLMs) have
been widely used in many NLP tasks (Zhang and
Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;
Zhang et al., 2012; Xu and Zhao, 2012; Wang et
al., 2013b; Jia and Zhao, 2013; Wang et al., 2014).
Recently, CSLMs become popular because they
can obtain more accurate probability estimation.
2.1 Continues Space Language Model
A CSLM implemented in a multi-layer neural net-
work contains four layers: the input layer projects
(first layer) all words in the context h
i
onto the
projection layer (second layer); the hidden layer
(third layer) and the output layer (fourth layer)
achieve the non-liner probability estimation and
calculate the LM probability P (w
i
|h
i
) for the giv-
en context (Schwenk, 2007).
CSLM is able to calculate the probabilities of
all words in the vocabulary of the corpus given the
context. However, due to too high computational
complexity, CSLM is mainly used to calculate the
probabilities of a subset of the whole vocabulary
(Schwenk, 2007). This subset is called a short-
list, which consists of the most frequent words in
the vocabulary. CSLM also calculates the sum of
the probabilities of all words not included in the
short-list by assigning a neuron with the help of
BNLM. The probabilities of other words not in the
short-list are obtained from an BNLM (Schwenk,
2007; Schwenk, 2010; Wang et al., 2013a).
Let w
i
and h
i
be the current word and history,
respectively. CSLM with a BNLM calculates the
probability P (w
i
|h
i
) of w
i
given h
i
, as follows:
P (w
i
|h
i
) =
?
?
?
P
c
(w
i
|h
i
)
?
w?V
0
P
c
(w|h
i
)
P
s
(h
i
) if w
i
? V
0
P
b
(w
i
|h
i
) otherwise
(1)
where V
0
is the short-list, P
c
(?) is the probabil-
ity calculated by CSLM,
?
w?V
0
P
c
(w|h
i
) is the
summary of probabilities of the neuron for all the
words in the short-list, P
b
(?) is the probability cal-
culated by the BNLM, and
P
s
(h
i
) =
?
v?V
0
P
b
(v|h
i
). (2)
We may regard that CSLM redistributes the
probability mass of all words in the short-list,
which is calculated by using the n-gram LM.
2.2 Existing Converting Methods
As baseline systems, our approach proposed in
(Wang et al., 2013a) only re-writes the probabil-
ities from CSLM into the BNLM, so it can only
conduct a convert LM with the same size as the o-
riginal one. The main difference between our pro-
posed method in this paper and our previous ap-
proach is that n-grams outside the corpus are gen-
erated firstly and the probabilities using CSLM are
calculated by using the same method as our previ-
ous approach. That is, the proposed new method
is the same as our previous one when no grown
n-grams are generated.
The method developed by Arsoy and colleagues
(Arsoy et al., 2013; Arsoy et al., 2014) adds al-
l the words in the short-list after the tail word of
the i-grams to construct the (i+1)-grams. For ex-
ample, if the i-gram is ?I want?, then the (i+1)-
grams will be ?I want *?, where ?*? stands for any
word in the short list. Then the probabilities of
the (i+1)-grams are calculated using (i+1)-CSLM.
So a very large intermediate (i+1)-grams will have
to be grown
1
, and then be pruned into smaller
suitable size using an entropy-based LM pruning
method modified from (Stolcke, 1998). The (i+2)-
grams are grown using (i+1)-grams, recursively.
1
In practice, the probabilities of all the target/tail words
in the short list for the history i-grams can be calculated by
the neurons in the output layer at the same time, which will
save some time. According to our experiments, the time cost
for Arsoy?s growing method is around 4 times more than our
proposed method, if the LMs which are 10 times larger than
the original one are grown with other settings all the same.
190
3 Bilingual LM Growing
The translation output of a phrase-based SMT sys-
tem can be regarded as a concatenation of phrases
in the phrase table (except unknown words). This
leads to the following procedure:
Step 1. All the n-grams included in the phrase
table should be maintained at first.
Step 2. The connecting phrases are defined in
the following way.
The w
b
a
is a target language phrase starting from
the a-th word ending with the b-th word, and ?w
b
a
?
is a phrase includingw
b
a
as a part of it, where ? and
? represent any word sequence or none. An i-gram
phrase w
k
1
w
i
k+1
(1 ? k ? i ? 1) is a connecting
phrase
2
, if :
(1) w
k
1
is the right (rear) part of one phrase ?w
k
1
in the phrase table, or
(2) w
i
k+1
is the left (front) part of one phrase
w
i
k+1
? in the phrase table.
After the probabilities are calculated using C-
SLM (Eqs.1 and 2), we combine the n-grams in
the phrase table from Step 1 and the connecting
phrases from Step 2.
3.1 Ranking the Connecting Phrases
Since the size of connecting phrases is too huge
(usually more than one Terabyte), it is necessary
to decide the usefulness of connecting phrases for
SMT. The more useful connecting phrases can be
selected, by ranking the appearing probabilities of
the connecting phrases in SMT decoding.
Each line of a phrase table can be simplified
(without considering other unrelated scores in the
phrase table) as
f ||| e ||| P (e|f), (3)
where the P (e|f) means the translation probabili-
ty from f(source phrase) to e(target phrase),
which can be calculated using bilingual parallel
training data. In decoding, the probability of a tar-
get phrase e appearing in SMT should be
P
t
(e) =
?
f
P
s
(f) ? P (e|f), (4)
2
We are aware that connecting phrases can be applied to
not only two phrases, but also three or more. However the ap-
pearing probabilities (which will be discussed in Eq. 5 of next
subsection) of connecting phrases are approximately estimat-
ed. To estimate and compare probabilities of longer phrases
in different lengths will lead to serious bias, and the experi-
ments also showed using more than two connecting phrases
did not perform well (not shown for limited space), so only
two connecting phrases are applied in this paper.
where the P
s
(f) means the appearing probability
of a source phrase, which can be calculated using
source language part in the bilingual training data.
Using P
t
(e)
3
, we can select the connecting
phrases e with high appearing probabilities as
the n-grams to be added to the original n-
grams. These n-grams are called ?grown n-
grams?. Namely, we build all the connecting
phrases at first, and then we use the appearing
probabilities of the connecting phrases to decide
which connecting phrases should be selected. For
an i-gram connecting phrasew
k
1
w
i
k+1
, wherew
k
1
is
part of ?w
k
1
and w
i
k+1
is part of w
i
k+1
? (the ?w
k
1
and w
i
k+1
? are from the phrase table), the prob-
ability of the connecting phrases can be roughly
estimated as
P
con
(w
k
1
w
i
k+1
) =
i?1
?
k=1
(
?
?
P
t
(?w
k
1
)?
?
?
P
t
(w
i
k+1
?)).
(5)
A threshold for P
con
(w
k
1
w
i
k+1
) is set, and only
the connecting phrases whose appearing probabil-
ities are higher than the threshold will be selected
as the grown n-grams.
3.2 Calculating the Probabilities of Grown
N -grams Using CSLM
To our bilingual LM growing method, a 5-gram
LM and n-gram (n=2,3,4,5) CSLMs are built by
using the target language of the parallel corpus,
and the phrase table is learned from the parallel
corpus.
The probabilities of unigram in the original n-
gram LM will be maintained as they are. The
n-grams from the bilingual phrase table will be
grown by using the ?connecting phrases? method.
As the whole connecting phrases are too huge, we
use the ranking method to select the more useful
connecting phrases. The distribution of different
n-grams (n=2,3,4,5) of the grown LMs are set as
the same as the original LM.
The probabilities of the grown n-grams
(n=2,3,4,5) are calculated using the 2,3,4,5-
CSLM, respectively. If the tail (target) words of
the grown n-grams are not in the short-list of C-
SLM, the P
b
(?) in Eq. 1 will be applied to calcu-
late their probabilities.
3
This P
t
(e) hence provides more bilingual information,
in comparison with using monolingual target LMs only.
191
We combine the n-grams (n=1,2,3,4,5) togeth-
er and re-normalize the probabilities and backof-
f weights of the grown LM. Finally the original
BNLM and the grown LM are interpolated. The
entire process is illustrated in Figure 1.
Corpus
Phrase Table
Grown n-grams 
with Probabilities
Grown LM
Output
Input
Interpolate
Grown n-grams
CSLM
BNLM
Connecting
Phrases
Figure 1: NN based bilingual LM growing.
4 Experiments and Results
4.1 Experiment Setting up
The same setting up of the NTCIR-9 Chinese to
English translation baseline system (Goto et al.,
2011) was followed, only with various LMs to
compare them. The Moses phrase-based SMT
system was applied (Koehn et al., 2007), togeth-
er with GIZA++ (Och and Ney, 2003) for align-
ment and MERT (Och, 2003) for tuning on the de-
velopment data. Fourteen standard SMT features
were used: five translation model scores, one word
penalty score, seven distortion scores, and one LM
score. The translation performance was measured
by the case-insensitive BLEU on the tokenized test
data.
We used the patent data for the Chinese to En-
glish patent translation subtask from the NTCIR-9
patent translation task (Goto et al., 2011). The par-
allel training, development, and test data sets con-
sist of 1 million (M), 2,000, and 2,000 sentences,
respectively.
Using SRILM (Stolcke, 2002; Stolcke et al.,
2011), we trained a 5-gram LM with the interpo-
lated Kneser-Ney smoothing method using the 1M
English training sentences containing 42M words
without cutoff. The 2,3,4,5-CSLMs were trained
on the same 1M training sentences using CSLM
toolkit (Schwenk, 2007; Schwenk, 2010). The set-
tings for CSLMs were: input layer of the same
dimension as vocabulary size (456K), projection
layer of dimension 256 for each word, hidden lay-
er of dimension 384 and output layer (short-list) of
dimension 8192, which were recommended in the
CSLM toolkit and (Wang et al., 2013a)
4
.
4
Arsoy used around 55 M words as the corpus, including
4.2 Results
The experiment results were divided into four
groups: the original BNLMs (BN), the CSLM
Re-ranking (RE), our previous converting (WA),
the Arsoy?s growing, and our growing methods.
For our bilingual LM growing method, 5 bilingual
grown LMs (BI-1 to 5) were conducted in increas-
ing sizes. For the method of Arsoy, 5 grown LMs
(AR-1 to 5) with similar size of BI-1 to 5 were also
conducted, respectively.
For the CSLM re-ranking, we used CSLM to
re-rank the 100-best lists of SMT. Our previous
converted LM, Arsoy?s grown LMs and bilingual
grown LMs were interpolated with the original
BNLMs, using default setting of SRILM
5
. To re-
duce the randomness of MERT, we used twometh-
ods for tuning the weights of different SMT fea-
tures, and two BLEU scores are corresponding to
these twomethods. TheBLEU-s indicated that the
same weights of the BNLM (BN) features were
used for all the SMT systems. The BLEU-i indi-
cated that the MERT was run independently by
three times and the average BLEU scores were
taken.
We also performed the paired bootstrap re-
sampling test (Koehn, 2004)
6
. Two thousands
samples were sampled for each significance test.
The marks at the right of the BLEU score indicated
whether the LMs were significantly better/worse
than the Arsoy?s grown LMs with the same IDs
for SMT (?++/???: significantly better/worse at
? = 0.01, ?+/??: ? = 0.05, no mark: not signif-
icantly better/worse at ? = 0.05).
From the results shown in Table 1, we can get
the following observations:
(1) Nearly all the bilingual grown LMs outper-
formed both BNLM and our previous converted
LM on PPL and BLEU. As the size of grown LM-
s is increased, the PPL always decreased and the
BLEU scores trended to increase. These indicated
that our proposed method can give better probabil-
ity estimation for LM and better performance for
SMT.
(2) In comparison with the grown LMs in Ar-
84K words as vocabulary, and 20K words as short-list. In this
paper, we used the same setting as our previous work, which
covers 92.89% of the frequency of words in the training cor-
pus, for all the baselines and our method for fair comparison.
5
In our previous work, we used the development data to
tune the weights of interpolation. In this paper, we used the
default 0.5 as the interpolation weights for fair comparison.
6
We used the code available at http://www.ark.cs.
cmu.edu/MT
192
Table 1: Performance of the Grown LMs
LMs n-grams PPL BLEU-s BLEU-i ALH
BN 73.9M 108.8 32.19 32.19 3.03
RE N/A 97.5 32.34 32.42 N/A
WA 73.9M 104.4 32.60 32.62 3.03
AR-1 217.6M 103.3 32.55 32.75 3.14
AR-2 323.8M 103.1 32.61 32.64 3.18
AR-3 458.5M 103.0 32.39 32.71 3.20
AR-4 565.6M 102.8 32.67 32.51 3.21
AR-5 712.2M 102.5 32.49 32.60 3.22
BI-1 223.5M 101.9 32.81+ 33.02+ 3.20
BI-2 343.6M 101.0 32.92+ 33.11++ 3.24
BI-3 464.5M 100.6 33.08++ 33.25++ 3.26
BI-4 571.0M 100.3 33.15++ 33.12++ 3.28
BI-5 705.5M 100.1 33.11++ 33.24++ 3.31
soy?s method, our grown LMs obtained better P-
PL and significantly better BLEU with the sim-
ilar size. Furthermore, the improvement of PPL
and BLEU of the existing methods became satu-
rated much more quickly than ours did, as the LMs
grew.
(3) The last column was the Average Length of
the n-grams Hit (ALH) in SMT decoding for dif-
ferent LMs using the following function
ALH =
5
?
i=1
P
i?gram
? i, (6)
where the P
i?gram
means the ratio of the i-grams
hit in SMT decoding. There were also positive
correlations between ALH, PPL and BLEUs. The
ALH of bilingual grown LM was longer than that
of the Arsoy?s grown LM of the similar size. In
another word, less back-off was used for our pro-
posed grown LMs in SMT decoding.
4.3 Experiments on TED Corpus
The TED corpus is in special domain as discussed
in the introduction, where large extra monolingual
corpora are hard to find. In this subsection, we
conducted the SMT experiments on TED corpora
using our proposed LM growing method, to eval-
uate whether our method was adaptable to some
special domains.
We mainly followed the baselines of the IWSLT
2014 evaluation campaign
7
, only with a few mod-
ifications such as the LM toolkits and n-gram or-
der for constructing LMs. The Chinese (CN) to
English (EN) language pair was chosen, using de-
v2010 as development data and test2010 as evalu-
ation data. The same LM growing method was ap-
7
https://wit3.fbk.eu/
plied on TED corpora as on NTCIR corpora. The
results were shown in Table 2.
Table 2: CN-EN TED Experiments
LMs n-grams PPL BLEU-s
BN 7.8M 87.1 12.41
WA 7.8M 85.3 12.73
BI-1 23.1M 79.2 12.92
BI-2 49.7M 78.3 13.16
BI-3 73.4M 77.6 13.24
Table 2 indicated that our proposed LM grow-
ing method improved both PPL and BLEU in com-
parison with both BNLM and our previous CSLM
converting method, so it was suitable for domain
adaptation, which is one of focuses of the current
SMT research.
5 Conclusion
In this paper, we have proposed a neural network
based bilingual LM growing method by using the
bilingual parallel corpus only for SMT. The results
show that our proposed method can improve both
LM and SMT performance, and outperforms the
existing LM growing methods significantly with-
out extra corpus. The connecting phrase-based
method can also be applied to LM adaptation.
Acknowledgments
We appreciate the helpful discussion with Dr.
Isao Goto and Zhongye Jia, and three anony-
mous reviewers for valuable comments and sug-
gestions on our paper. Rui Wang, Hai Zhao
and Bao-Liang Lu were partially supported by
the National Natural Science Foundation of Chi-
na (No. 60903119, No. 61170114, and No.
61272248), the National Basic Research Program
of China (No. 2013CB329401), the Science and
Technology Commission of Shanghai Municipali-
ty (No. 13511500200), the European Union Sev-
enth Framework Program (No. 247619), the Cai
Yuanpei Program (CSC fund 201304490199 and
201304490171), and the art and science interdis-
cipline funds of Shanghai Jiao Tong University
(A study on mobilization mechanism and alerting
threshold setting for online community, and media
image and psychology evaluation: a computation-
al intelligence approach). The corresponding au-
thor of this paper, according to the meaning given
to this role by Shanghai Jiao Tong University, is
Hai Zhao.
193
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. In Proceedings of ICASSP-2013, Vancouver,
Canada, May. IEEE.
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2014. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. IEEE/ACM Transactions on Audio, Speech,
and Language, 22(1):184?192.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
cessings of EMNLP-2013, pages 1044?1054, Seat-
tle, Washington, USA, October. Association for
Computational Linguistics.
Jerome R Bellegarda. 2004. Statistical language mod-
el adaptation: review and perspectives. Speech
Communication, 42(1):93?108. Adaptation Meth-
ods for Speech Recognition.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR), 3:1137?1155, March.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of EAMT-2012,
pages 261?268, Trento, Italy, May.
Philip Clarkson and A.J. Robinson. 1997. Lan-
guage model adaptation using mixtures and an ex-
ponentially decaying cache. In Proceedings of
ICASSP-1997, volume 2, pages 799?802 vol.2, Mu-
nich,Germany.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL-
2014, pages 1370?1380, Baltimore, Maryland, June.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the paten-
t machine translation task at the NTCIR-9 work-
shop. In Proceedings of NTCIR-9 Workshop Meet-
ing, pages 559?578, Tokyo, Japan, December.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish.
1997. Using out-of-domain data to improve in-
domain language models. Signal Processing Letter-
s, IEEE, 4(8):221?223.
Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a
framework for automatic evaluation of chinese input
method engines. In Proceedings of IJCNLP-2013,
pages 1195?1201, Nagoya, Japan, October. Asian
Federation of Natural Language Processing.
Zhongye Jia and Hai Zhao. 2014. A joint graph mod-
el for pinyin-to-chinese conversion with typo cor-
rection. In Proceedings of ACL-2014, pages 1512?
1523, Baltimore, Maryland, June. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of ACL-2007 Workshop
on Statistical Machine Translation, pages 224?227,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL-2007, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP-2004, pages 388?395, Barcelona, Spain,
July. Association for Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of ICASSP-2011, pages 5524?5527, Prague,
Czech Republic, May. IEEE.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL-2014,
pages 1491?1500, Baltimore, Maryland, June. As-
sociation for Computational Linguistics.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
Proceedings of INTERSPEECH-2010, pages 1045?
1048.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzman-
n machines. In Proceedings of IWSLT-2012, pages
311?318, Hong Kong.
Thomas Niesler and Phil Woodland. 1996. A variable-
length category-based n-gram language model. In
Proceedings of ICASSP-1996, volume 1, pages 164?
167 vol. 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignmen-
t models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of ACL-2003, pages 160?167, Sapporo, Japan, July.
Association for Computational Linguistics.
194
Eric Sven Ristad and Robert G. Thomas. 1995. New
techniques for context modeling. In Proceedings
of ACL-1995, pages 220?227, Cambridge, Mas-
sachusetts. Association for Computational Linguis-
tics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of
COLING ACL-2006, pages 723?730, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, WLM ?12, pages 11?19, Montreal, Canada,
June. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The
Prague Bulletin of Mathematical Linguistics, pages
137?146.
Vesa Siivola, Teemu Hirsimki, and Sami Virpioja.
2007. On growing and pruning kneser-ney s-
moothed n-gram models. IEEE Transactions on Au-
dio, Speech, and Language, 15(5):1617?1624.
Manhung Siu and Mari Ostendorf. 2000. Variable n-
grams and extensions for conversational speech lan-
guage modeling. IEEE Transactions on Speech and
Audio, 8(1):63?75.
Le Hai Son, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc?ois Yvon. 2010. Training con-
tinuous space language models: some practical is-
sues. In Proceedings of EMNLP-2010, pages 778?
788, Cambridge, Massachusetts, October. Associa-
tion for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of NAACL HLT-
2012, pages 39?48, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-
tor Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of INTERSPEECH 2011,
Waikoloa, HI, USA, December.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274, Lansdowne, VA, USA.
Andreas Stolcke. 2002. Srilm-an extensible
language modeling toolkit. In Proceedings of
INTERSPEECH-2002, pages 257?286, Seattle, US-
A, November.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of EMNLP-2013, pages 1387?1392,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013a. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of EMNLP-2013, pages 845?850,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Xiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b.
Labeled alignment for recognizing textual entail-
ment. In Proceedings of IJCNLP-2013, pages 605?
613, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-
Liang Lu. 2014. Parallelized extreme learning ma-
chine ensemble based on minmax modular network.
Neurocomputing, 128(0):31 ? 41.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING-2012, pages 1341?1350,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of IJCAI-2013, pages 2211?
2217. AAAI Press.
Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012.
A machine learning approach to convert CCGbank
to Penn treebank. In Proceedings of COLING-
2012, pages 535?542, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word
segmentation for chinese machine translation. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 248?263.
Springer Berlin Heidelberg.
195
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 7?13,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
The ACL Anthology Searchbench
Ulrich Scha?fer Bernd Kiefer Christian Spurk Jo?rg Steffen Rui Wang
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI)
D-66123 Saarbru?cken, Germany
{ulrich.schaefer,kiefer,cspurk,steffen,wang.rui}@dfki.de
http://www.dfki.de/lt
Abstract
We describe a novel application for structured
search in scientific digital libraries. The ACL
Anthology Searchbench is meant to become a
publicly available research tool to query the
content of the ACL Anthology. The applica-
tion provides search in both its bibliographic
metadata and semantically analyzed full tex-
tual content. By combining these two features,
very efficient and focused queries are possi-
ble. At the same time, the application serves
as a showcase for the recent progress in nat-
ural language processing (NLP) research and
language technology. The system currently
indexes the textual content of 7,500 anthol-
ogy papers from 2002?2009 with predicate-
argument-like semantic structures. It also
provides useful search filters based on bib-
liographic metadata. It will be extended to
provide the full anthology content and en-
hanced functionality based on further NLP
techniques.
1 Introduction and Motivation
Scientists in all disciplines nowadays are faced with
a flood of new publications every day. In addi-
tion, more and more publications from the past be-
come digitally available and thus even increase the
amount. Finding relevant information and avoiding
duplication of work have become urgent issues to be
addressed by the scientific community.
The organization and preservation of scientific
knowledge in scientific publications, vulgo text doc-
uments, thwarts these efforts. From a viewpoint of
a computer scientist, scientific papers are just ?un-
structured information?. At least in our own sci-
entific community, Computational Linguistics, it is
generally assumed that NLP could help to support
search in such document collections.
The ACL Anthology1 is a comprehensive elec-
tronic collection of scientific papers in our own field
(Bird et al, 2008). It is updated regularly with
new publications, but also older papers have been
scanned and are made available electronically.
We have implemented the ACL Anthology
Searchbench2 for two reasons: Our first aim is to
provide a more targeted search facility in this col-
lection than standard web search on the anthology
website. In this sense, the Searchbench is meant to
become a service to our own community.
Our second motivation is to use the developed
system as a showcase for the progress that has been
made over the last years in precision-oriented deep
linguistic parsing in terms of both efficiency and
coverage, specifically in the context of the DELPH-
IN community3. Our system also uses further NLP
techniques such as unsupervised term extraction,
named entity recognition and part-of-speech (PoS)
tagging.
By automatically precomputing normalized se-
mantic representations (predicate-argument struc-
ture) of each sentence in the anthology, the search
space is structured and allows to find equivalent or
related predicates even if they are expressed differ-
1http://www.aclweb.org/anthology
2http://aclasb.dfki.de
3http://www.delph-in.net ? DELPH-IN stands for
DEep Linguistic Processing with HPSG INitiative.
7
ently, e.g. in passive constructions, using synonyms,
etc. By storing the semantic sentence structure along
with the original text in a structured full-text search
engine, it can be guaranteed that recall cannot fall
behind the baseline of a fulltext search.
In addition, the Searchbench also provides de-
tailed bibliographic metadata for filtering as well as
autosuggest texts for input fields computed from the
corpus ? two further key features one can expect
from such systems today, nevertheless very impor-
tant for efficient search in digital libraries.
We describe the offline preprocessing and deep
parsing approach in Section 2. Section 3 concen-
trates on the generation of the semantic search in-
dex. In Section 4, we describe the search interface.
We conclude in Section 5 and present an outlook to
future extensions.
2 Parsing the ACL Anthology
The basis of the search index for the ACL Anthol-
ogy are its original PDF documents, currently 8,200
from the years 2002 through 2009. To overcome
quality problems in text extraction from PDF, we
use a commercial PDF extractor based on OCR tech-
niques. This approach guarantees uniform and high-
quality textual representations even from older pa-
pers in the anthology (before 2000) which mostly
were scanned from printed paper versions.
The general idea of the semantics-oriented ac-
cess to scholarly paper content is to parse each sen-
tence they contain with the open-source HPSG (Pol-
lard and Sag, 1994) grammar for English (ERG;
Flickinger (2002)) and then distill and index seman-
tically structured representations for search.
To make the deep parser robust, it is embedded
in a NLP workflow. The coverage (percentage of
full deeply parsed sentences) on the anthology cor-
pus could be increased from 65 % to now more
than 85 % through careful combination of several
robustness techniques; for example: (1) chart prun-
ing, directed search during parsing to increase per-
formance, and also coverage for longer sentences
(Cramer and Zhang, 2010); (2) chart mapping, a
novel method for integrating preprocessing informa-
tion in exactly the way the deep grammar expects
it (Adolphs et al, 2008); (3) new version of the
ERG with better handling of open word classes; (4)
more fine-grained named entity recognition, includ-
ing recognition of citation patterns; (5) new, better
suited parse ranking model (WeScience; Flickinger
et al (2010)). Because of limited space, we will fo-
cus on (1) and (2) below. A more detailed descrip-
tion and further results are available in (Scha?fer and
Kiefer, 2011).
Except for a small part of the named entity recog-
nition components (citations, some terminology)
and the parse ranking model, there are no further
adaptations to genre or domain of the text corpus.
This implies that the NLP workflow could be easily
and modularly adapted to other (scientific or non-
scientific) domains?mainly thanks to the generic
and comprehensive language modelling in the ERG.
The NLP preprocessing component workflow is
implemented using the Heart of Gold NLP mid-
dleware architecture (Scha?fer, 2006). It starts
with sentence boundary detection (SBR) and regu-
lar expression-based tokenization using its built-in
component JTok, followed by the trigram-based PoS
tagger TnT (Brants, 2000) trained on the Penn Tree-
bank (Marcus et al, 1993) and the named entity rec-
ognizer SProUT (Droz?dz?yn?ski et al, 2004).
2.1 Precise Preprocessing Integration with
Chart Mapping
Tagger output is combined with information from
the named entity recognizer, e.g. delivering hypo-
thetical information on citation expressions. The
combined result is delivered as input to the deep
parser PET (Callmeier, 2000) running the ERG.
Here, citations, for example, can be treated as either
persons, locations or appositions.
Concerning punctuation, the ERG can make use
of information on opening and closing quotation
marks. Such information is often not explicit in the
input text, e.g. when, as in our setup, gained through
OCR which does not distinguish between ? and ? or ?
and ?. However, a tokenizer can often guess (recon-
struct) leftness and rightness correctly. This infor-
mation, passed to the deep parser via chart mapping,
helps it to disambiguate.
2.2 Increased Processing Speed and Coverage
through Chart Pruning
In addition to a well-established discriminative max-
imum entropy model for post-analysis parse selec-
8
tion, we use an additional generative model as de-
scribed in Cramer and Zhang (2010) to restrict the
search space during parsing. This restriction in-
creases efficiency, but also coverage, because the
parse time was restricted to at most 60 CPU seconds
on a standard PC, and more sentences could now be
parsed within these bounds. A 4 GB limit for main
memory consumption was far beyond what was ever
needed. We saw a small but negligible decrease in
parsing accuracy, 5.4 % best parses were not found
due to the pruning of important chart edges.
Ninomiya et al (2006) did a very thorough com-
parison of different performance optimization strate-
gies, and among those also a local pruning strategy
similar to the one used here. There is an important
difference between the systems, in that theirs works
on a reduced context-free backbone first and recon-
structs the results with the full grammar, while PET
uses the HPSG grammar directly, with subsumption
packing and partial unpacking to achieve a similar
effect as the packed chart of a context-free parser.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  20  40  60  80  100
sentences x 1000mean parse time (CPU s)
sentence length ??
Figure 1: Distribution of sentence length and mean parse
times for mild pruning
In total, we parsed 1,537,801 sentences, of which
57,832 (3.8 %) could not be parsed because of lexi-
con errors. Most of them were caused by OCR ar-
tifacts resulting in unexpected punctuation character
combinations. These can be identified and will be
deleted in the future.
Figure 1 displays the average parse time of pro-
cessing with a mild chart pruning setting, together
with the mean quadratic error. In addition, it con-
tains the distribution of input sentences over sen-
tence length. Obviously, the vast majority of sen-
tences has a length of at most 60 words4. The parse
times only grow mildly due to the many optimiza-
tion techniques in the original system, and also the
new chart pruning method. The sentence length dis-
tribution has been integrated into Figure 1 to show
that the predominant part of our real-world corpus
can be processed using this information-rich method
with very low parse times (overall average parse
time < 2 s per sentence).
The large amount of short inputs is at first surpris-
ing, even more so that most of these inputs can not
be parsed. Most of these inputs are non-sentences
such as headings, enumerations, footnotes, table cell
content. There are several alternatives to deal with
such input, one to identify and handle them in a pre-
processing step, another to use a special root con-
dition in the deep analysis component that is able
to combine phrases with well-defined properties for
inputs where no spanning result could be found.
We employed the second method, which has the
advantage that it handles a larger range of phenom-
ena in a homogeneous way. Figure 2 shows the
change in percentage of unparsed and timed out in-
puts for the mild pruning method with and without
the root condition combining fragments.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
strictstrict timeoutstrict+fragmentsstrict+fragments timeout
sentence length ??
Figure 2: Unparsed and timed out sentences with and
without fragment combination
Figure 2 shows that this changes the curve for un-
parsed sentences towards more expected character-
istics and removes the uncommonly high percent-
age of short sentences for which no parse can be
computed. Together with the parses for fragmented
4It has to be pointed out that extremely long sentences also
may be non-sentences resulting from PDF extraction errors,
missing punctuation etc. No manual correction took place.
9
Figure 3: Multiple semantic tuples may be generated for a sentence
input, we get a recall (sentences with at least one
parse) over the whole corpus of 85.9 % (1,321,336
sentences), without a significant change for any of
the other measures, and with potential for further im-
provement.
3 Semantic Tuple Extraction with DMRS
In contrast to shallow parsers, the ERG not only
handles detailed syntactic analyses of phrases, com-
pounds, coordination, negation and other linguistic
phenomena that are important for extracting seman-
tic relations, but also generates a formal semantic
representation of the meaning of the input sentence
in the Minimal Recursion Semantics (MRS) repre-
sentation format (Copestake et al, 2005). It consists
of elementary predications for each word and larger
constituents, connected via argument positions and
variables, from which predicate-argument structure
can be extracted.
MRS representations resulting from deep parsing
are still relatively close to linguistic structures and
contain more detailed information than a user would
like to query and search for. Therefore, an additional
extraction and abstraction step is performed before
storing semantic structures in the search index.
Firstly, MRS is converted to DMRS (Copes-
take, 2009), a dependency-style version of MRS
that eases extraction of predicate-argument struc-
ture using the implementation in LKB (Copestake,
2002). The representation format we devised for the
search index we call semantic tuples, in fact quintu-
ples <subject, predicate, first object, second object,
adjuncts>; example in Figure 3. The basic extrac-
tion algorithm consists of the following three steps:
(1) calculate the closure for each elementary pred-
ication based on the EQ (variable equivalence) re-
lation, and group the predicates and entities in each
closure respectively; (2) extract the relations of the
groups, which results in a graph as a whole; (3) re-
cursively traverse the graph, form one semantic tu-
ple for each predicate, and fill in the corresponding
information under its scope, i.e. subject, object, etc.
In the example shown in Figure 3, entity groups
like ?our systems?, ?the baseline?, and ?good perfor-
mance on the SRL task?, as well as predicate groups
?beating? and ?achieved? are formed at the first step.
In the second step, the graph structure is extracted,
i.e., the relation between the groups. Finally, two
semantic tuples are filled in with both the predicates
and the corresponding information. Notice that the
modifier(s) of the entity belong to the same entity
group, but the modifier(s) of the predicate will be
put into the Adjuncts slot. Similarly, the coordina-
tion of the entities will be put into one entity group,
while the coordination of predicates will form mul-
tiple semantic tuples.
Since we are extracting predicate-argument struc-
ture, syntactic variations such as passive construc-
tions and relative clauses will be all ?normalized?
into the same form. Consequently, ?the book which
I read?, ?I read the book?, and ?the book was read
by me? will form the exact same semantic tuple <I,
read, the book, N/A, N/A>. The resulting tuple
structures along with their associated text are stored
in an Apache Solr/Lucene5 server which receives
queries from the Searchbench user interface.
4 Searchbench User Interface
The Searchbench user interface (UI) is a web appli-
cation running in every modern, JavaScript-enabled
web browser. As can be seen in Figure 4, the UI
is divided into three parts: (1) a sidebar on the left
(Filters View), where different filters can be set that
constrain the list of found documents; (2) a list of
found documents matching the currently set filters
in the upper right part of the UI (Results View); (3)
5http://lucene.apache.org/solr
10
Figure 4: Searchbench user interface with different filters set and currently looking at the debug menu for a sentence.
the Document View in the lower right part of the UI
with different views of the current document.
A focus in the design of the UI has been to al-
low the user to very quickly browse the papers of the
ACL Anthology and then to find small sets of rele-
vant documents based on metadata and content. This
is mainly achieved by these techniques: (i) changes
in the collection of filters automatically update the
Results View; (ii) metadata and searchable content
from both the Results View and the Document View
can easily be used with a single click as new filters;
(iii) filters can easily be removed with a single click;
(iv) manually entering filter items is assisted by sen-
sible autosuggestions computed from the corpus; (v)
accidental filter changes can easily be corrected by
going back in the browser history.
The following kinds of filters are supported:
Statements (filter by semantic statements, i.e., the
actual content of sentences, see Section 4.1), Key-
words (filter by simple keywords with a full-text
search), Topics (filter by topics of the articles that
were extracted with an extended approach of the un-
supervised term extractor of Frantzi et al (1998)),
Publication (filter by publication title/event), Au-
thors (filter by author names), Year (filter by pub-
lication year), Affiliations (filter by affiliation or-
ganizations), Affiliation Sites (filter by affiliation
cities and countries)6. Found papers always match
all currently set filters. For each filter type multi-
ple different filter items can be set; one could search
for papers written jointly by people from different
research institutes on a certain topic, for example.
Matches of the statements filter and the keywords
filter are highlighted in document snippets for each
paper in the Results View and in the currently se-
lected paper of the Document View.
Besides a header displaying the metadata of the
currently selected paper (including the automatically
extracted topics on the right), the Document View
provides three subviews of the selected paper: (1)
the Document Content View is a raw list of the sen-
tences of the paper and provides different kinds of
interaction with these sentences; (2) the PDF View
shows the original PDF version of the paper; (3) the
Citations View provides citation information includ-
6Affiliations have been added using the ACL Anthology
Network data (Radev et al, 2009).
11
ing link to the ACL Anthology Network (Radev et
al., 2009).
Figure 4 shows the search result for a query com-
bining a statement (?obtain improvements?), a topic
?dependency parsing? and the publication year 2008.
As can be seen in the Results View, six papers
match these filters; sentences with semantically sim-
ilar predicates and passive voice are found, too.
4.1 Semantic Search
The main feature which distinguishes the ACL An-
thology Searchbench from other search applications
for scientific papers is the semantic search in paper
content. This enables the search for (semantic) state-
ments in the paper content as opposed to searching
for keywords in the plain text. Our use of the term
?statement? is loosely along the lines of the same
term used in logic. Very simple sentences often
bear a single statement only, while more complex
sentences (especially when having multiple clauses)
contain multiple statements. Each of the semantic
tuples extracted from the papers of the ACL Anthol-
ogy (cf. Section 3) corresponds to a statement.
The Statements filter is responsible for the seman-
tic search. Statements used in filters may be under-
specified, e.g., one may search for statements with a
certain semantic subject but with arbitrary semantic
predicates and objects. There are two ways in which
a new statement filter can be set: (1) entering a state-
ment manually; (2) clicking a sentence in the Doc-
ument Content View and choosing the statements of
this sentence that shall be set as new statement fil-
ters (cf. Figure 5), i.e. it is possible to formulate and
refine queries ?by example?.
Figure 5: Dialog for choosing statements to be used as
new filters (for sentence ?Our systems achieved good per-
formance on the SRL task, easily beating the baseline.?).
Throughout the user interface, no distinction is
made between the different kinds of semantic ob-
jects and adjuncts so as to make it easy also for
non-linguists to use the search and to be more ro-
bust against bad analyses of the parser. Therefore,
the different semantic parts of a statement are high-
lighted in three different colors only, depending on
whether a part is the semantic subject, the semantic
predicate or anything else (object/adjunct).
In order to disengage even further from the con-
crete wording and make the semantic search even
more ?meaning-based?, we additionally search for
synonyms of the semantic predicates in statement
filters. These synonyms have been computed as an
intersection of the most frequent verbs (semantic
predicates) in the anthology corpus with WordNet
synsets (Fellbaum, 1998), the main reason being re-
duction of the number of meanings irrelevant for the
domain. This relatively simple approach could of
course be improved, e.g. by active learning from
user clicks in search results etc.
5 Summary and Outlook
We have described the ACL Anthology Search-
bench, a novel search application for scientific dig-
ital libraries. The system is fully implemented and
indexes 7,500 papers of the 8,200 parsed ones. For
the other 700, bibliographic metadata was missing.
These and the remaining 10,000 papers are currently
being processed and will be added to the search in-
dex. The goal of the Searchbench is both to serve
as a showcase for benefits and improvement of NLP
for text search and at the same time provide a use-
ful tool for researchers in Computational Linguis-
tics. We believe that the tool by now already sup-
ports targeted search in a large collection of digital
research papers better than standard web search en-
gines. An evaluation comparing Searchbench query
results with web search is in progress.
Optionally, the Searchbench runs in a linguistic
debug mode providing NLP output a typical user
would not need. These analyses are accessible from
a context menu on each sentence (cf. Figure 4). Both
a tabular view of the semantic tuples of a sentence
(cf. Figure 3) and different kinds of information re-
lated to the parsing of the sentence (including the
MRS and a parse tree) can be displayed.
Future work, for which we are urgently seek-
ing funding, could include integration of further
12
NLP-based features such as coreference resolution
or question answering, as well as citation classifi-
cation and graphical navigation along the ideas in
Scha?fer and Kasterka (2010).
Acknowledgments
We are indebted to Peter Adolphs, Bart Cramer, Dan
Flickinger, Stephan Oepen, Yi Zhang for their sup-
port with ERG and PET extensions such as chart
mapping and chart pruning. Melanie Reiplinger,
Benjamin Weitz and Leonie Gro?n helped with pre-
processing. We also thank the anonymous review-
ers for their encouraging comments. The work de-
scribed in this paper has been carried out in the
context of the project TAKE (Technologies for Ad-
vanced Knowledge Extraction), funded under con-
tract 01IW08003 by the German Federal Ministry
of Education and Research, and in the context of the
world-wide DELPH-IN consortium.
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Daniel Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural lan-
guage parsing. In Proceedings of LREC-2008, pages
1380?1387, Marrakesh, Morocco.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008. The
ACL anthology reference corpus: A reference dataset
for bibliographic research. In Proceedings of LREC-
2008, pages 1755?1759, Marrakesh, Morocco.
Torsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proc. of ANLP, pages 224?231, Seattle, WA.
Ulrich Callmeier. 2000. PET ? A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99?108.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: an in-
troduction. Research on Language and Computation,
3(2?3):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI publications, Stanford.
Ann Copestake. 2009. Slacker semantics: why superfi-
ciality, dependency and avoidance of commitment can
be the right way to go. In Proc. of EACL, pages 1?9.
Bart Cramer and Yi Zhang. 2010. Constraining robust
constructions for broad-coverage parsing with preci-
sion grammars. In Proceedings of COLING-2010,
pages 223?231, Beijing, China.
Witold Droz?dz?yn?ski, Hans-Ulrich Krieger, Jakub Pisko-
rski, Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow
processing with unification and typed feature struc-
tures ? foundations and applications. Ku?nstliche In-
telligenz, 2004(1):17?23.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods: Syntacto-semantic annotation for
English Wikipedia. In Proceedings of LREC-2010,
pages 1665?1671.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Dan Flickinger,
Stephan Oepen, Hans Uszkoreit, and Jun?ichi Tsujii,
editors, Collaborative Language Engineering. A Case
Study in Efficient Grammar-based Processing, pages
1?17. CSLI Publications, Stanford, CA.
Katerina T. Frantzi, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 1998. The C-value/NC-value method of automatic
recognition for multi-word terms. In Proceedings of
ECDL, pages 585?604.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English. The Penn Treebank. Computational
Linguistics, 19:313?330.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
Kenjiro Taura, and Jun?ichi Tsujii. 2006. Fast and
scalable HPSG parsing. Traitement automatique des
langues (TAL), 46(2).
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In Proceedings of the ACL-2009 Workshop
on Natural Language Processing and Information Re-
trieval for Digital Libraries, Singapore.
Ulrich Scha?fer and Uwe Kasterka. 2010. Scientific
authoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL-HLT 2010
Workshop on Computational Linguistics and Writing,
pages 7?14, Los Angeles, CA.
Ulrich Scha?fer and Bernd Kiefer. 2011. Advances in
deep parsing of scholarly paper content. In Raffaella
Bernardi, Sally Chambers, Bjo?rn Gottfried, Fre?de?rique
Segond, and Ilya Zaihrayeu, editors, Advanced Lan-
guage Technologies for Digital Libraries, LNCS Hot
Topics Series. Springer. to appear.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional NLP markup. In Pro-
ceedings of the EACL-2006 Workshop on Multi-
dimensional Markup in Natural Language Processing,
pages 81?84, Trento, Italy.
13
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction
with Latent Structural SVM
Lizhen Qu
Max Planck Institute
for Informatics
lqu@mpi-inf.mpg.de
Yi Zhang
Nuance Communications
yi.zhang@nuance.com
Rui Wang
DFKI GmbH
mars198356@hotmail.com
Lili Jiang
Max Planck Institute
for Informatics
ljiang@mpi-inf.mpg.de
Rainer Gemulla
Max Planck Institute
for Informatics
rgemulla@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute
for Informatics
weikum@mpi-inf.mpg.de
Abstract
Extracting instances of sentiment-oriented re-
lations from user-generated web documents is
important for online marketing analysis. Un-
like previous work, we formulate this extrac-
tion task as a structured prediction problem
and design the corresponding inference as an
integer linear program. Our latent structural
SVM based model can learn from training cor-
pora that do not contain explicit annotations of
sentiment-bearing expressions, and it can si-
multaneously recognize instances of both bi-
nary (polarity) and ternary (comparative) re-
lations with regard to entity mentions of in-
terest. The empirical evaluation shows that
our approach significantly outperforms state-
of-the-art systems across domains (cameras
and movies) and across genres (reviews and
forum posts). The gold standard corpus that
we built will also be a valuable resource for
the community.
1 Introduction
Sentiment-oriented relation extraction (Choi et al.,
2006) is concerned with recognizing sentiment po-
larities and comparative relations between entities
from natural language text. Identifying such rela-
tions often requires syntactic and semantic analysis
at both sentence and phrase level. Most prior work
on sentiment analysis consider either i) subjective
sentence detection (Yu and K?bler, 2011), ii) po-
larity classification (Johansson and Moschitti, 2011;
Wilson et al., 2005), or iii) comparative relation
identification (Jindal and Liu, 2006; Ganapathib-
hotla and Liu, 2008). In practice, however, differ-
ent types of sentiment-oriented relations frequently
coexist in documents. In particular, we found that
more than 38% of the sentences in our test corpus
contain more than one type of relations. The iso-
lated analysis approach is inappropriate because i) it
sacrifices acuracy by ignoring the intricate interplay
among different types of relations; ii) it could lead to
conflicting predictions such as estimating a relation
candidate as both negative and comparative. There-
fore, in this paper, we identify instances of both sen-
timent polarities and comparative relations for enti-
ties of interest simultaneously. We assume that all
the mentions of entities and attributes are given, and
entities are disambiguated. It is a widely used as-
sumption when evaluating a module in a pipeline
system that the outputs of preceding modules are
error-free.
To the best of our knowledge, the only exist-
ing system capable of extracting both comparisons
and sentiment polarities is a rule-based system pro-
posed by Ding et al. (2009). We argue that it is
better to tackle the task by using a unified model
with structured outputs. It allows us to consider a
set of correlated relation instances jointly and char-
acterize their interaction through a set of soft and
hard constraints. For example, we can encode con-
straints to discourage an attribute to participate in
a polarity relation and a comparative relation at the
same time. As a result, the system extracts a set of
correlated instances of sentiment-oriented relations
from a given sentence. For example, with the sen-
tence about the camera Canon 7D, ?The sensor is
great, but the price is higher than Nikon D7000.?
the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155?168. Action Editor: Janyce Wiebe.
Submitted 6/2013; Revised 11/2013; Published 4/2014. c?2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textit-
price).
However, constructing a fully annotated train-
ing corpus for this task is labor-intensive and re-
quires strong linguistic background. We minimize
this overhead by applying a simplified annotation
scheme, in which annotators mark mentions of en-
tities and attributes, disambiguate the entities, and
label instances of relations for each sentence. Based
on the new scheme, we have created a small Senti-
ment Relation Graph (SRG) corpus for the domains
of cameras and movies, which significantly differs
from the corpora used in prior work (Wei and Gulla,
2010; Kessler et al., 2010; Toprak et al., 2010;
Wiebe et al., 2005; Hu and Liu, 2004) in the follow-
ing ways: i) both sentiment polarities and compar-
ative relations are annotated; ii) all mentioned en-
tities are disambiguated; and iii) no subjective ex-
pressions are annotated, unless they are part of entity
mentions.
The new annotation scheme raises a new chal-
lenge for learning algorithms in that they need to
automatically find textual evidences for each anno-
tated relation during training. For example, with the
sentence ?I like the Rebel a little better, but that is
another price jump?, simply assigning a sentiment-
bearing expression to the nearest relation candidate
is insufficient, especially when the sentiment is not
explicitly expressed.
In this paper, we propose SENTI-LSSVM, a latent
structural SVM based model for sentiment-oriented
relation extraction. SENTI-LSSVM is applied to find
the most likely set of the relation instances expressed
in a given sentence, where the latent variables are
used to assign the most appropriate textual evidences
to the respective instances.
In summary, the contributions of this paper are the
following:
? We propose SENTI-LSSVM: the first unified sta-
tistical model with the capability of extracting
instances of both binary and ternary sentiment-
oriented relations.
? We design a task-specific integer linear pro-
gramming (ILP) formulation for inference.
? We construct a new SRG corpus as a valuable
asset for the evaluation of sentiment relation
extraction.
? We conduct extensive experiments with on-
line reviews and forum posts, showing that
SENTI-LSSVM model can effectively learn from
a training corpus without explicitly annotated
subjective expressions and that its performance
significantly outperforms state-of-the-art sys-
tems.
2 Related Work
There are ample works on analyzing sentiment po-
larities and entity comparisons, but the majority of
them studied the two tasks in isolation.
Most prior approaches for fine-grained sentiment
analysis focus on polarity classification. Super-
vised approaches on expression-level analysis re-
quire the annotation of sentiment-bearing expres-
sions as training data (Jin et al., 2009; Choi
and Cardie, 2010; Johansson and Moschitti, 2011;
Yessenalina and Cardie, 2011; Wei and Gulla,
2010). However, the corresponding annotation pro-
cess is time-consuming. Although sentence-level
annotations are easier to obtain, the analysis at this
level cannot cope with sentences conveying relations
of multiple types (McDonald et al., 2007; T?ckstr?m
and McDonald, 2011; Socher et al., 2012). Lexicon-
based approaches require no training data (Ku et al.,
2006; Kim and Hovy, 2006; Godbole et al., 2007;
Ding et al., 2008; Popescu and Etzioni, 2005; Liu et
al., 2005) but suffer from inferior performance (Wil-
son et al., 2005; Qu et al., 2012). In contrast, our
method requires no annotation of sentiment-bearing
expressions for training and can predict both senti-
ment polarities and comparative relations.
Sentiment-oriented comparative relations have
been studied in the context of user-generated dis-
course (Jindal and Liu, 2006; Ganapathibhotla and
Liu, 2008). Approaches rely on linguistically moti-
vated rules and assume the existence of independent
keywords in sentences which indicate comparative
relations. Therefore, these methods fall short of ex-
tracting comparative relations based on domain de-
pendent information.
Both Johansson and Moschitti (2011) and Wu et
al. (2011) formulate fine-grained sentiment analy-
sis as a learning problem with structured outputs.
However, they focus only on polarity classification
156
of expressions and require annotation of sentiment-
bearing expressions for training as well.
While ILP has been previously applied for infer-
ence in sentiment analysis (Choi and Cardie, 2009;
Somasundaran and Wiebe, 2009; Wu et al., 2011),
our task requires a complete ILP reformulation due
to 1) the absence of annotated sentiment expressions
and 2) the constraints imposed by the joint extrac-
tion of both sentiment polarity and comparative re-
lations.
3 System Overview
This section gives an overview of the whole system
for extracting sentiment-oriented relation instances.
Prior to presenting the system architecture, we in-
troduce the essential concepts and the definitions of
two kinds of directed hypergraphs as the represen-
tation of correlated relation instances extracted from
sentences.
3.1 Concepts and Definitions
Entity. An entity is an abstract or concrete thing,
which needs not be of material existence. An entity
in this paper refers to either a product or a brand.
Attribute. An attribute is an object closely associ-
ated with or belonging to an entity, such as the lens
of digital camera.
Sentiment-Oriented Relation. A sentiment-
oriented relation is either a sentiment polarity or a
comparative relation, defined on tuples of entities
and attributes. A sentiment polarity relation conveys
either a positive or a negative attitude towards enti-
ties or their attributes, whereas a comparative rela-
tion indicates the preference of one entity over the
other entity w.r.t. an attribute.
Relation Instance. An instance of sentiment polar-
ity takes the form r(entity, attribute) with r ? {pos-
itive, negative}, such as positive(Canon 7D, sen-
sor). The polarity instances expressed in the form
of unary relations, such as ?Nikon D7000 is ex-
cellent.?, are denoted as binary relations r(entity,
whole), where the attribute whole indicates the en-
tity as a whole. In contrast, an instance of compar-
ative relation is in the form of preferred{entity, en-
tity, attribute}, e.g. preferred(Canon 7D, Nikon
D7000, price). For brevity, we refer to an instance
set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances
of the remaining relations, we represent them as
other{entity, attribute}, such as textitpartOf{wheel,
car}. These relations include objective relations
and the subjective relations other than sentiment-
oriented relations.
Mention-Based Relation Instances. A mention-
based relation instance refers to a tuple of entity
mentions with a certain relation. This concept is in-
troduced as the representation of instances in a sen-
tence by replacing entities with the corresponding
entity mentions, such as positive(?Canon SD880i?,
?wide angle view?).
Figure 1: An example of MRG.
Mention-Based Relation Graph. A mention-based
relation graph (or MRG ) represents a collection of
mention-based relation instances expressed in a sen-
tence. As illustrated in Figure 1, an MRG is a di-
rected hypergraph G = ?M,E? with a vertex set
M and an edge set E. A vertex mi ? M denotes
a mention of an entity or an attribute occurring ei-
ther within the sentence or in its context. We say
that a mention is from the context if it is mentioned
in the previous sentence or is an attribute implied
in the current sentence. An instance of a binary re-
lation in an MRG takes the form of a binary edge
el = (mi,ma), where mi and ma denote an en-
tity mention and an attribute mention respectively,
and the type l ? {positive, negative, other}. A
ternary edge el indicating comparative relation is
represented as el = (mi,mj ,ma), where two en-
tity mentions mi and mj are compared with respect
to the attribute mention ma. We define the type
l ? {better,worse} to indicate two possible direc-
tions of the relation and assume mi occurs before
mj . As a result, we have a set L of five relation
types: positive, negative, better, worse or other. Ac-
cording to these definitions, the annotations in the
SRG corpus are actually MRGs and disambiguated
entities. If there are multiple mentions referring to
the same entity, annotators are asked to choose the
157
most obvious one because it saves annotation time
and is less demanding for the entity recognition and
diambiguation modules.
Figure 2: An example of eMRG. The textual evi-
dences are wrapped by green dashed boxes.
Evidentiary Mention-Based Relation Graph. An
evidentiary mention-based relation graph, coined
eMRG , extends an MRG by associating each edge
with a textual evidence to support the corresponding
relation assertions (see Figure 2). Consequently, an
edge in an eMRG is denoted by a pair (a, c), where
a represents a mention-based relation instance and
c is the associated textual evidence. It is also re-
ferred to as an evidentiary edge. represented as
el = (mi,mj ,ma), an MRG as an evidentiary MRG
(eMRG) and the edges of eMRGs as evidentiary
edges, as shown in Figure 2.
3.2 System Architecture
Figure 3: System architecture.
As illustrated by Figure 3, at the core of our sys-
tem is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs
from sentences. For a given sentence with known
entity mentions, we select all possible mention sets
as relation candidates, where each set includes at
least one entity mention. Then we associate each
relation candidate with a set of constituents or the
whole sentence as the textual evidence candidates
(cf. Section 6.1). Subsequently, the inference com-
ponent aims to find the most likely eMRG from all
possible combinations of mention-based relation in-
stances and their textual evidences (cf. Section 6.2).
The representation eMRG is chosen because it char-
acterizes exactly the model outputs by letting each
edge correspond to an instance of mention-based re-
lation and the associated textual evidence. Finally,
the model parameters of this model are learned by
an online algorithm (cf. Section 7).
Since instance sets of sentiment-oriented relations
(sSoRs) are the expected outputs, we can obtain
sSoRs from MRGs by using a simple rule-based al-
gorithm. The algorithm essentially maps the men-
tions from an MRG into entities and attributes in an
sSoR and label the corresponding tuples with the re-
lation types of the edges from an MRG. For instances
of comparative relation, the label better or worse is
mapped to the relation type preferred.
4 SENTI-LSSVM Model
The task of sentiment-oriented relation extraction
is to determine the most likely sSoR in a sentence.
Since sSoRs are derived from the corresponding
MRGs as described in Section 3, the task is reduced
to find the most likely MRG for each sentence. Since
an MRG is created by assigning relation types to a
subset of all relation candidates, which are possible
tuples of mentions with unknown relation types, the
number of MRGs can be extremely high.
To tackle the task, one solution is to employ
an edge-factored linear model in the framework of
structural SVM (Martins et al., 2009; Tsochantaridis
et al., 2004). The model suggests that a bag of fea-
tures should be specified for each relation candidate,
and then the model predicts the most likely candi-
date sets along with their relation types to form the
optimal MRGs. As we observed, for a relation can-
didate, the most informative features are the words
near its entity mentions in the original text. How-
158
ever, if we represent a candidate by all these words,
it is very likely that the instances of different relation
types share overly similar features, because a men-
tion is often involved in more than one relation can-
didate, as shown in Figure 2. As a consequence, the
instances of different relations represented by overly
similar features can easily confuse the learning algo-
rithm. Thus, it is critical to select proper constituents
or sentences as textual evidences for each relation
candidate in both training and testing.
Consequently, we divide the task of sentiment-
oriented relation extraction into two subtasks : i)
identifying the most likely MRGs; ii) assigning
proper textual evidences to each edge of MRGs to
support their relation assertions. It is desirable to
carry out the two subtasks jointly as these two sub-
tasks could enhance each other. First, the identifi-
cation of relation types requires proper textual ev-
idences; second, the soft and hard constraints im-
posed by the correlated relation instances facilitate
the recognition of the corresponding textual evi-
dences. Since the eMRGs are created by attaching
every MRG with a set of textual evidences, tackling
the two subtasks simultaneously is equivalent to se-
lecting the most likely eMRG from a set of eMRG
candidates. It is challenging because our SRG corpus
does not contain any annotation of textual evidences.
Formally, let X denote the set of all available sen-
tences, and we define y ? Y(x)(x ? X ) as the set
of labeled edges of an MRG and Y = ?x?XY(x).
Since the assignments of textual evidences are not
observed, an assignment of evidences to y is de-
noted by a latent variable h ? H(x) and H =
?x?XH(x). Then (y, h) corresponds to an eMRG,
and (a, c) ? (y, h) is a labeled edge a attached
with a textual evidence c. Given a labeled dataset
D = {(x1, y1), ..., (xn, yn)} ? (X ? Y)n, we aim
to learn a discriminant function f : X ? Y?H that
outputs the optimal eMRG (y, h) ? Y(x)?H(x) for
a given sentence x.
Due to the introduction of latent variables, we
adopt the latent structural SVM (Yu and Joachims,
2009) for structural classification. Our discriminant
function is defined as
f(x) = argmax(y,h)?Y(x)?H(x)?>?(x, y, h) (1)
where ?(x, y, h) is the feature function of an eMRG
(y, h) and ? is the corresponding weight vector.
To ensure tractability, we also employ edge-based
factorization for our model. Let Mp denote a set of
entity mentions and yr(mi) be a set of edges labeled
with sentiment-oriented relations incident to mi, the
factorization of ?(x, y, h) is given as
?(x, y, h) =
?
(a,c)?(y,h)
?e(x, a, c) + (2)
?
mi?Mp
?
a,a??yr(mi),a 6=a?
?c(a, a?)
where ?e(x, a, c) is a local edge feature function
for a labeled edge a attached with a textual evidence
c and ?c(a, a?) is a feature function capturing co-
occurrence of two labeled edges ami and a?mi inci-dent to an entity mention mi.
5 Feature Space
The following features are used in the feature func-
tions (Equation 2):
Unigrams: As mentioned before, a textual evi-
dence attached to an edge in MRG is either a word,
phrase or sentence. We consider all lemmatized un-
igrams in the textual evidence as unigram features.
Context: Since web users usually express related
sentiments about the same entity across sentence
boundaries, we describe the sentiment flow using a
set of contextual binary features. For example, if en-
tity A is mentioned in both the previous sentence and
the current sentence, a set of contextual binary fea-
tures are used to indicate all possible combinations
of the current and the previous mentioned sentiment-
oriented relations regarding to entity A.
Co-occurrence: We have mentioned the co-
occurrence feature in Equation 2, indicated by
?c(a, a?). It captures the co-occurrence of two la-
beled edges incident to the same entity mention.
Note that the co-occurrence feature function is con-
sidered only if there is a contrast conjunction such as
?but? between the non-shared entity mentions inci-
dent to the two labeled edges.
Senti-predictors: Following the idea of (Qu et
al., 2012), we encode the prediction results from
the rule-based phrase-level multi-relation predic-
tor (Ding et al., 2009) and from the bag-of-opinions
predictor (Qu et al., 2010) as features based on the
textual evidence. The output of the first predictor
is an integer value, while the output of the second
predictor is a sentiment relation, such as ?positive?,
159
?negative?, ?better? or ?worse?. We map the rela-
tional outputs into integer values and then encode
the outputs from both predictors as senti-predictor
features.
Others: The commonly used part-of-speech tags
are also included as features. Moreover, for an edge
candidate, a set of binary features are used to denote
the types of the edge and its entity mentions. For in-
stance, a binary feature indicates whether an edge is
a binary edge related to an entity mentioned in con-
text. To characterize the syntactic dependencies be-
tween two adjacent entity mentions, we use the path
in the dependency tree between the heads of the cor-
responding constituents, the number of words and
other mentions in-between as features. Additionally,
if the textual evidence is a constituent, its feature
w.r.t. an edge is the dependency path to the clos-
est mention of the edge that does not overlap with
this constituent.
6 Structural Inference
In order to find the best eMRG for a given sentence
with a well trained model, we need to determine
the most likely relation type for each relation candi-
date and support the corresponding assertions with
proper textual evidences. We formulate this task
as an Integer Linear Programming (ILP). Instead of
considering all constituents of a sentence, we empir-
ically select a subset as textual evidences for each
relation candidate.
6.1 Textual Evidence Candidates Selection
Textual evidences are selected based on the con-
stituent trees of sentences parsed by the Stanford
parser (Klein and Manning, 2003). For each men-
tion in a sentence, we first locate a constituent in
the tree with the maximal overlap by Jaccard sim-
ilarity. Starting from this constituent, we consider
two types of candidates: type I candidates are con-
stituents at the highest level which contain neither
any word of another mention nor any contrast con-
junctions such as ?but?; type II candidates are con-
stituents at the highest level which cover exactly two
mentions of an edge and do not overlap with any
other mentions. For a binary edge connecting an en-
tity mention and an attribute mention, we consider
a type I candidate starting from the attribute men-
tion. For a binary edge connecting two entity men-
tions, we consider type I candidates starting from
both mentions. Moreover, for a comparative ternary
edge, we consider both type I and type II candidates
starting from the attribute mention. This strategy is
based on our observation that these candidates of-
ten cover the most important information w.r.t. the
covered entity mentions.
6.2 ILP Formulation
We formulate the inference problem of finding the
best eMRG as an ILP problem due to its convenient
integration of both soft and hard constraints.
Given the model parameters ?, we reformulate
the score of an eMRG in the discriminant function
(1) as follows,
?>?(x, y, h) =
?
(a,c)?(y,h)
saczac +
?
mi?Mp
?
a,a??yr(mi),a 6=a?
saa?zaa?
where sac = ?>?e(x, a, c) denotes the score of a
labeled edge a attached with a textual evidence c,
saa? = ?>?c(a, a?) is the edge co-occurrence score,
the binary variable zac indicates the presence or ab-
sence of the corresponding edge, and zaa? indicates
if two edges co-occurr. As not every edge set can
form an eMRG, we require that a valid eMRG should
satisfy a set of linear constraints, which form our
constraint space. Then function (1) is equivalent to
max
z?B s
>z + ?zd
s.t. A
?
?
z
?
?
?
? ? d
z,?, ? ? B
where B = 2S with S = {0, 1}, and ? and ? are
auxiliary binary variables that help define the con-
straint space. The above optimization problem takes
exactly the form of an ILP because both the con-
straints and the objective function are linear, and all
variables take only integer values.
In the following, we consider two types of con-
straint space, 1) an eMRG with only binary edges and
2) an eMRG with both binary and ternary edges.
160
eMRG with only Binary Edges: An eMRG has
only binary edges if a sentence contains no attribute
mention or at most one entity mention. We expect
that each edge has only one relation type and is sup-
ported by a single textual evidence. To facilitate the
formulation of constraints, we introduce ?el to de-
note the presence or absence of a labeled edge el,
and ?ec to indicate if a textual evidence c is assigned
to an unlabeled edge e. Then the binary variable for
the corresponding evidentiary edge zelc = ?ec ? ?el ,
where the ILP formulation of conjunction can be
found in (Martins et al., 2009).
Let Ce denote the set of textual evidence candi-
dates of an unlabeled edge e. The constraint of at
most one textual evidence per edge is formulated as:
?
c?Ce
?ec ? 1 (3)
Once a textual evidence is assigned to an edge,
their relation labels should match and the number
of labeled edges must agree with the number of at-
tached textual evidences. Further, we assume that a
textual evidence c conveys at most one relation so
that an evidence will not be assigned to the relations
of different types, which is the main problem for the
structural SVM based model. Let ?cl indicate that
the textual evidence c is labeled by the relation type
l. The corresponding constraints are expressed as,
?
l?Le
?el =
?
c?Ce
?ec; zelc ? ?cl;
?
l?L
?cl ? 1
where Le denotes the set of all possible labels for
an unlabeled edge e, and L is the set of all relation
types of MRGs (cf. Section 3).
In order to avoid a textual evidence being overly
reused by multiple relation candidates, we first pe-
nalize the assignment of a textual evidence c to a
labeled edge a by associating the corresponding zac
with a fixed negative cost ?? in the objective func-
tion. Then the selection of one textual evidence per
edge a is encouraged by associating ? to zdc in the
objective function, where zdc =
?
e?Sc ?ec and Sc isthe set of edges that the textual evidence c serves as
a candidate. The disjunction zdc is expressed as:
zdc ? ?e, e ? Sc
zdc ?
?
e?Sc
?e
(a) Binary edge structure
(b) Ternary edge structure
Figure 4: Alternative structures associated with an
attribute mention.
This soft constraint not only encourages one textual
evidence per edge, but also keeps it eligible for mul-
tiple assignments.
For any two labeled edge a and a? incident
to the same entity mention, the edge-to-edge co-
occurrence is described by zca,a? = za ? za? .
eMRG with both Binary and Ternary Edges: If
there are more than one entity mentions and at least
one attribute mention in a sentence, an eMRG can
potentially have both binary and ternary edges. In
this case, we assume that each mention of attributes
can participate either in binary relations or in ternary
relations. The assumption holds in more than 99.9%
of the sentences in our SRG corpus, thus we describe
it as a set of hard constraints. Geometrically, the as-
sumption can be visualized as the selection between
two alternative structures incident to the same at-
tribute mention, as shown in Figure 4. Note that,
in the binary edge structure, we include not only the
edges incident to the attribute mention but also the
edge between the two entity mentions.
Let Sbmi be the set of all possible labeled edgesin a binary edge structure of an attribute mention
mi. Variable ? bmi =
?
el?Sbmi
?el indicates whether
the attribute mention is associated with a binary
edge structure or not. In the same manner, we use
? tmi =
?
el?Stmi
?el to indicate the association of the
an attribute mention mi with an ternary edge struc-
ture from the set of all incident ternary edges Stmi .The selection between two alternative structures is
161
formulated as ? bmi + ? tmi = 1. As this influencesonly the edges incident to an attribute mention, we
keep all the constraints introduced in the previous
section unchanged except for constraint (3), which
is modified as
?
c?Ce
?ec ? ? bmi ;
?
c?Ce
?ec ? ? tmi
Therefore, we can have either binary edges or
ternary edges for an attribute mention.
7 Learning Model Parameters
Given a set of training sentences D =
{(x1, y1), . . . , (xn, yn)}, the best weight vec-
tor ? of the discriminant function (1) is found by
solving the following optimization problem:
min
?
1
n
n?
i=1
[ max
(y?,h?)?Y(x)?H(x)
(?>?(x, y?, h?)+?(h?, y?, y))
? max
h??H(x)
?>?(x, y, h?)] + ?|?|] (4)
where ?(h?, y?, y) is a loss function measuring the dis-
crepancies between an eMRG (y, h?) with gold stan-
dard edge labels y and an eMRG (y?, h?) with inferred
labeled edges y? and textual evidences h?. Due to the
sparse nature of the lexical features, we apply L1
regularizer to the weight vector ?, and the degree of
sparsity is controlled by the hyperparameter ?.
Since the L1 norm in the above optimization
problem is not differentiable at zero, we apply the
online forward-backward splitting (FOBOS) algo-
rithm (Duchi and Singer, 2009). It requires two steps
for updating the weight vector ? by using a single
training sentence x on each iteration t.
?t+ 12 = ?t ? ?t?t
?t+1 = arg min
?
1
2?? ? ?t?
2 + ?t?|?|
where ?t is the subgradient computed without con-
sidering the L1 norm and ?t is the learning rate.
For a labeled sentence x, ?t = ?(x, y??, h??) ?
?(x, y, h??), where the feature functions of the corre-
sponding eMRGs are inferred by solving (y??, h??) =
arg max(h?,y?)?H(x)?Y(x)[?
>?(x, y?, h?) + ?(h?, y?, y)]
and (y, h??) = arg maxh??H(x) ?>?(x, y, h?), as in-
dicated in the optimization problem (4).
The former inference problem is similar to the
one we considered in the previous section except
the inclusion of the loss function. We incorporate
the loss function into the ILP formulation by defin-
ing the loss between an MRG (y, h) and a gold stan-
dard MRG as the sum of per-edge costs. In our ex-
periments, we consider a positive cost ? for each
wrongly labeled edge a, so that if an edge a has a
different label from the gold standard, we add ? to
the coefficient sac of the corresponding variable zac
in the objective function of the ILP formulation.
In addition, since the non-positive weights of edge
labels in the initial learning phrase often lead to
eMRGs with many unlabeled edges, which harms the
learning performance, we fix it by adding a con-
straint for the minimal number of labeled edges in
an eMRG, ?
a?A
?
c?Ca
?ac ? ? (5)
where A is the set of all labeled edge candidates and
? denotes the minimal number of labeled edges.
Empirically, the best way to determine ? is to
make it equal to the maximal number of labeled
edges in an eMRG with the restriction that a tex-
tual evidence can be assigned to at most one edge.
By considering all the edge candidates A and all the
textual evidence candidates C as two vertex sets in a
bipartite graph G? = ?V = (A,C), E? (with edges in
E indicating which textual evidence can be assigned
to which edge), ? corresponds to exactly the size of
a maximum matching of the bipartite graph1.
To find the optimal eMRG (y, h??), for the gold la-
bel k of each edge, we consider the following set of
constraints for inference since the labels of the edges
are known for the training data,
?
c?Ce
?ec ? 1; ?ec ? lck
?
k??L
lck? ? 1;
?
e?Sc
?ec ? 1
We include also the soft constraints, which avoid
a textual evidence being overly reused by multiple
relations, and the constraints similar to (5) to ensure
a minimal number of labeled edges and a minimal
number of sentiment-oriented relations.
1It is computed by the Hopcroft-Karp algorithm (Hopcroft
and Karp, 1973) in our implementation.
162
8 SRG Corpus
For evaluation we constructed the SRG corpus,
which in total consists of 1686 manually annotated
online reviews and forum posts in the digital camera
and movie domains2. For each domain, we maintain
a set of attributes and a list of entity names.
The annotation scheme for the sentiment repre-
sentation asserts minimal linguistic knowledge from
our annotators. By focusing on the meanings of the
sentences, the annotators make decisions based on
their language intuition, not restricted by specific
syntactic structures. Taking the example in Figure
2, the annotators only need to mark the mentions of
entities and attributes from both the sentences and
the context, disambiguate them, and label (?Canon
7D?, ?Nikon D7000?, price) as worse and (?Canon
7D?, ?sensor?) as positive, whereas in prior work,
people have annotated the sentiment-bearing expres-
sions such as ?great? and link them to the respective
relation instances as well. This also enables them
to annotate instances of both sentiment polarity and
comparative relaton, which are conveyed by not only
explicit sentiment-bearing expressions like ?excel-
lent performance?, but also factual expressions im-
plying evaluations such as ?The 7V has 10x optical
zoom and the 9V has 16x.?.
Camera Movie
Reviews Forums Reviews Forums
positive 386 1539 879 905
negative 165 363 529 331
comparison 30 480 39 35
Table 1: Distribution of relation instances in SRG corpus.
14 annotators participated in the annotation
project. After a short training period, annotators
worked on randomly assigned documents one at a
time. For product reviews, the system lists all rel-
evant information about the entity and the prede-
fined attributes. For forum posts, the system shows
only the attribute list. For each sentence in a doc-
ument, the annotator first determines if it refers to
an entity of interest. If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Ama-
zon.com; the 667 camera forum posts are downloaded from fo-
rum.digitalcamerareview.com; the 138 movie reviews and 774
forum posts are from imdb.com and boards.ie respectively
as off-topic. Otherwise, the annotator will identify
the most obvious mentions, disambiguate them, and
mark the MRGs. We evaluate the inter-annotator
agreement on sSoRs in terms of Cohen?s Kappa
(?) (Cohen, 1968). An average Kappa value of 0.698
was achieved on a randomly selected set consisting
of 412 sentences.
Table 1 shows the corpus distribution after nor-
malizing them into sSoRs. Camera forum posts con-
tain the largest proportion of comparisons because
they are mainly about the recommendation of dig-
ital cameras. In contrast, web users are much less
interested in comparing movies, in both reviews and
forums. In all subsets, positive relations play a dom-
inant role since web users intend to express more
positive attitudes online than negative ones (Pang
and Lee, 2007).
9 Experiments
This section describes the empirical evaluation of
SENTI-LSSVM together with two competitive base-
lines on the SRG corpus.
9.1 Experimental Setup
We implemented a rule-based baseline (DING-
RULE) and a structural SVM (Tsochantaridis et
al., 2004) baseline (SENTI-SSVM) for comparison.
The former system extends the work of Ding et
al. (2009), which designed several linguistically-
motivated rules based on a sentiment polarity lexi-
con for relation identification and assumes there is
only one type of sentiment relation in a sentence. In
our implementation, we keep all the rules of (Ding et
al., 2009) and add one phrase-level rule when there
are more than one mention in a sentence. The ad-
ditional rule assigns sentiment-bearing words and
negators to its nearest relation candidates based on
the absolute surface distance between the words and
the corresponding mentions. In this case, the phrase-
level sentiment-oriented relations depend only on
the assigned sentiment words and negators. The lat-
ter system is based on a structural SVM and does
not consider the assignment of textual evidences to
relation instances during inference. The textual fea-
tures of a relation candidate are all lexical and sen-
timent predictor features within a surface distance
of four words from the mentions of the candidate.
163
Thus, this baseline does not need the inference con-
straints of SENTI-LSSVM for the selection of textual
evidences. To gain more insights into the model,
we also evaluate the contribution of individual fea-
tures of SENTI-LSSVM. In addition, to show if identi-
fying sentiment polarities and comparative relations
jointly works better than tackling each task on its
own, we train SENTI-LSSVM for each task separately
and combine their predictions according to compat-
ibility rules and the corresponding graph scores.
For each domain and text genre, we withheld 15%
documents for development and use the remaining
for cross validation. The hyperparameters of all sys-
tems are tuned on the development datasets. For all
experiments of SENTI-LSSVM, we use ? = 0.0001
for the L1 regularizer in Eq.(4) and ? = 0.05 for
the loss function; and for SENTI-SSVM, ? = 0.0001
and ? = 0.01. Since the relation type of off-topic
sentences is certainly other, we evaluate all systems
with 5-fold cross-validation only on the on-topic
sentences in the evaluation dataset. Since the same
sSoR can have several equivalent MRGs and the rela-
tion type other is not of our interest, we evaluate the
sSoRs in terms of precision, recall and F-measure.
All reported numbers are averages over the 5 folds.
9.2 Results
Table 2 shows the complete results of all sys-
tems. Here our model SENTI-LSSVM outperformed
all baselines in terms of the average F-measure
scores and recalls by a large margin. The F-measure
on movie reviews is about 14% over the best base-
line. The rule-based system has higher precision
than recall in most cases. However, simply increas-
ing the coverage of the domain independent senti-
ment polarity lexicon might lead to worse perfor-
mance (Taboada et al., 2011) because many sen-
timent oriented relations are conveyed by domain
dependent expressions and factual expressions im-
plying evaluations, such as ?This camera does not
have manual control.? Compared to DING-RULE,
SENTI-SSVM performs better in the camera domain
but worse for the movies due to many misclassi-
fication of negative relation instances as other. It
also wrongly predicted more positive instances as
other than SENTI-LSSVM. We found that the recalls
of these instances are low because they often have
overly similar features with the instances of the type
other linking to the same mentions. The problem
gets worse in the movie domain since i) many sen-
tences contain no explicit sentiment-bearing words;
ii) the prior polarity of the sentiment-bearing words
do not agree with their contextual polarity in the
sentences. Consider the following example from a
forum post about the movie ?Superman Returns?:
?Have a look at Superman: the Animated Series or
Justice League Unlimited . . . that is how the char-
acters of Superman and Lex Luthor should be.?. In
contrast, our model minimizes the overlapping fea-
tures by assigning them to the most likely relation
candidates. This leads to significantly better per-
formance. Although SENTI-SSVM has low recall for
both positive and negative relations, it achieves the
highest recall for the comparative relation among all
systems in the movie domain and camera reviews.
Since less than 1% of all instances are for compara-
tive relations in these document sets and all models
are trained to optimize the overall accuracy, SENTI-
LSSVM intends to trade off the minority class for the
overall better performance. This advantage disap-
pears on the camera forum posts, where the number
of instances of comparative relation is 12 times more
than that in the other data sets.
All systems perform better in predicting positive
relations than the negative ones. This corresponds
well to the empirical findings in (Wilson, 2008) that
people intend to use more complex expressions for
negative sentiments than their affirmative counter-
parts. It is also in accordance with the distribution of
these relations in our SRG corpus which is randomly
sampled from the online documents. For learning
systems, it can also be explained by the fact that the
training data for positive relations are considerably
more than those for negative ones. The comparative
relation is the hardest one to process since we found
that many corresponding expressions do not contain
explicit keywords for comparison.
To understand the performance of the key fea-
ture groups in our model better, we remove each
group from the full SENTI-LSSVM system and eval-
uate the variations with movie reviews and camera
forum posts, which have relatively balanced distri-
bution of relation types. As shown in Table 3, the
features from the sentiment predictors make signif-
icant contributions for both datasets. The differ-
ent drops of the performance indicate that the po-
164
Positive Negative Comparison Micro-average
P R F P R F P R F P R F
Ca
me
ra
Fo
rum
DING-RULE 56.4 39.0 46.1 46.2 24.0 31.6 42.6 14.0 21.0 53.4 30.8 39.0
SENTI-SSVM 60.2 35.6 44.8 44.2 38.5 41.2 28.0 40.1 32.9 43.7 36.7 39.9
SENTI-LSSVM 69.2 38.9 49.8 50.8 39.3 44.3 42.6 35.1 38.5 56.5 38.0 45.4
Ca
me
ra
Re
-
vie
w DING-RULE 83.6 69.0 75.6 68.6 38.8 49.6 30.0 16.9 21.6 81.1 58.6 68.1SENTI-SSVM 72.6 75.4 74.0 63.9 62.5 63.2 28.0 38.9 32.5 68.1 70.4 69.3
SENTI-LSSVM 77.3 85.4 81.2 68.9 61.3 64.9 22.3 20.7 21.6 73.1 73.4 73.7
Mo
vie
Fo
rum
DING-RULE 63.7 37.4 47.1 27.6 34.3 30.6 8.9 5.6 6.8 48.2 35.9 41.2
SENTI-SSVM 66.2 30.1 41.3 25.6 17.3 20.7 44.2 56.7 49.7 53.3 27.9 36.6
SENTI-LSSVM 63.3 44.2 52.1 29.7 45.6 36.0 40.1 45.0 42.4 49.7 44.6 47.0
Mo
vie Re
-
vie
w DING-RULE 66.5 47.2 55.2 42.0 39.1 40.5 31.4 12.0 17.4 56.2 44.0 49.4SENTI-SSVM 61.3 54.0 57.4 45.2 13.7 21.1 24.5 63.3 35.3 54.6 39.2 45.7
SENTI-LSSVM 59.0 79.1 67.6 53.3 51.4 52.3 28.3 34.0 30.9 57.9 68.8 62.9
Table 2: Evaluation results for DING-RULE, SENTI-SSVM and SENTI-LSSVM. Boldface figures are statistically
significantly better than all others in the same comparison group under t-test with p = 0.05.
Feature Models Movie Reviews Camera Forums
full system 62.9 45.4
?unigram 63.2 (+0.3) 41.2 (-4.2)
?context 54.5 (-8.4) 46.0 (+0.6)
?co-occurrence 62.6 (-0.3) 44.9 (-0.5)
?senti-predictors 61.3 (-1.6) 34.3 (-11.1)
Table 3: Micro-average F-measure of SENTI-LSSVM
with different feature models
larities predicted by rules are more consistent in
camera forum posts than in movie reviews. Due
to the complexity of expressions in the movie re-
views our model cannot benefit from the unigram
features but these features are a good compensation
for the sentiment predictor features in camera fo-
rum posts. The sharp drop by removing the context
features from our model on movie reviews indicates
that the sentiments in movie reviews depend highly
on the relations of the previous sentences. In con-
trast, the sentiment-oriented relations of the previ-
ous sentences could be a reason of overfitting for
camera forum data. The edge co-occurrence fea-
tures do not play an important role in our model
since the number of co-occurred sentiment-oriented
relations in the sentences with contrast conjunctions
like ?but? is small. However, we found that allow-
ing the co-occurrence of any sentiment-oriented re-
lations would harm the performance of the model.
In addition, our experiments showed that the sep-
arated approach, which trains a model for senti-
ment polarities and comparative relations respec-
tively, leads to a decrease by almost 1% in terms of
the F-measure averaged over all four datasets. The
largest drop of F-measure is 3% on camera forum
posts, since this dataset contains the largest propor-
tion of comparative relations. We found that the er-
rors are increased when the trained models make
conflicting predictions. In this case, the joint ap-
proach can take all factors into account and make
more consistent decisions than the separated ap-
proaches.
10 Conclusion
We proposed SENTI-LSSVM model for extracting in-
stances of both sentiment polarities and comparative
relations. For evaluating and training the model, we
created an SRG corpus by using a lightweight an-
notation scheme. We showed that our model can
automatically find textual evidences to support its
relation predictions and achieves significantly bet-
ter F-measure scores than alternative state-of-the-art
methods.
References
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
165
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 590?598, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the Annual meeting of
the Association for Computational Linguistics, pages
269?274. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 431?
439, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or Par-
tial Credit. Psychological bulletin, 70(4):213.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining, pages 231?240, New
York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Lei Zhang. 2009. Entity
discovery and assignment for opinion mining applica-
tions. In Proceedings of the ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining, pages
1125?1134.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
The Journal of Machine Learning Research, 10:2899?
2934.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, pages 241?248, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs (system demonstration). In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media.
John E Hopcroft and Richard M Karp. 1973. An n?5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on computing, 2(4):225?231.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Proceedings of the
ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 168?177, New York, NY,
USA. ACM.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: a novel machine learning system for
web opinion mining and extraction. In Proceedings
of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1195?
1204, New York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In Proceedings of the 21st In-
ternational Conference on Artificial Intelligence - Vol-
ume 2, AAAI?06, pages 1331?1336. AAAI Press.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities?
exploration of pipelines and joint models. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics, volume 11, pages 101?106.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sent-
ment corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, SST ?06, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006.
Opinion extraction, summarization and tracking in
news and blog corpora. In AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 100?107.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351, New
York, NY, USA. ACM.
Andr? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Annual meeting of the Association for Computational
Linguistics, pages 342?350.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In Proceed-
ings of the Annual meeting of the Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
166
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In Chu-Ren
Huang and Dan Jurafsky, editors, Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), ACL Anthology, pages 913?
921, Beijing, China. Tsinghua University Press.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum. 2012.
A weakly supervised model for sentence-level seman-
tic orientation analysis with multiple experts. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 149?159,
Jeju Island, Korea, July. Proceedings of the Annual
meeting of the Association for Computational Linguis-
tics.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1201?1211.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
the Joint conference of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, pages 226?234.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267?307.
Oscar T?ckstr?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the 33rd
European conference on Advances in information re-
trieval, ECIR?11, pages 368?374, Berlin, Heidelberg.
Springer-Verlag.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 575?584,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the International
Conference on Machine Learning, pages 104?112.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the Annual meeting of the Association
for Computational Linguistics, pages 404?413.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained subjectivity
and sentiment analysis: recognizing the intensity, po-
larity, and attitudes of private states. Ph.D. thesis,
UNIVERSITY OF PITTSBURGH.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2011. Structural opinion mining for graph-based sen-
timent representation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1332?1341.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of the International Conference on Machine
Learning, page 147.
Ning Yu and Sandra K?bler. 2011. Filling the gap:
Semi-supervised learning for opinion detection across
domains. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning, pages
200?209. Association for Computational Linguistics.
167
168
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 272?275,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
MARS: A Specialized RTE System for Parser Evaluation
Rui Wang
?
Yi Zhang
??
? Department of Computational Linguistics, Saarland University
? LT-Lab, German Research Center for Artificial Intelligence
Im Stadtwald, 66123 Saarbr?ucken, Germany
{rwang,yzhang}@coli.uni-sb.de
Abstract
This paper describes our participation
in the the SemEval-2010 Task #12,
Parser Evaluation using Textual Entail-
ment. Our system incorporated two depen-
dency parsers, one semantic role labeler,
and a deep parser based on hand-crafted
grammars. The shortest path algorithm
is applied on the graph representation of
the parser outputs. Then, different types
of features are extracted and the entail-
ment recognition is casted into a machine-
learning-based classification task. The
best setting of the system achieves 66.78%
of accuracy, which ranks the 3rd place.
1 Introduction
The SemEval-2010 Task #12, Parser Evaluation
using Textual Entailment (PETE) (Yuret et al,
2010), is an interesting task connecting two areas
of research, parsing and recognizing textual entail-
ment (RTE) (Dagan et al, 2005). The former is
usually concerned with syntactic analysis in spe-
cific linguistic frameworks, while the latter is be-
lieved to involve more semantic aspects of the hu-
man languages. However, no clear-cut boundary
can be drawn between syntax and semantics for
both tasks. In recent years, the parsing commu-
nity has been reaching beyond what was usually
accepted as syntactic structures. Many deep lin-
guistic frameworks allow the construction of se-
mantic representations in parallel to the syntactic
structure. Meanwhile, data-driven shallow seman-
tic parsers (or semantic role labelers) are another
popular type of extension to enrich the information
in the parser outputs.
Although entailment is described as a semantic
relation, RTE, in practice, covers linguistic phe-
nomena at various levels, from surface text to the
meaning, even to the context and discourse. One
proposal of solving the problem is to deal with dif-
ferent cases of entailment using different special-
ized RTE modules (Wang and Neumann, 2009).
Then, the PETE data can be naturally classified
into the syntactic and shallow semantic categories.
By participating in this shared task, we aim to
investigate whether different parsing outputs leads
to different RTE accuracy, and on the contrary,
whether the ?application?-based evaluation pro-
vides insights to the parser comparison. Further,
we investigate if strict grammaticality checking
with a linguistic grammar is helpful in this task.
2 System Description
The workflow of the system is shown in Figure
1 and the details of the three components will be
elaborated on in the following sections.
2.1 Preprocessing
In this paper, we generally refer all the linguistic
analyses on the text as preprocessing. The output
of this procedure is a graph representation, which
approximates the meaning of the input text. In par-
ticular, after tokenization and POS tagging, we did
dependency parsing and semantic role labeling. In
addition, HPSG parsing is a filter for ungrammat-
ical hypotheses.
Tokenization and POS Tagging We use the
Penn Treebank style tokenization throughout the
various processing stages. TnT, an HMM-based
POS tagger trained with Wall Street Journal sec-
tions of the PTB, was used to automatically pre-
dict the part-of-speech of each token in the texts
and hypotheses.
Dependency Parsing For obtaining the syntac-
tic dependencies, we use two dependency parsers,
MSTParser (McDonald et al, 2005) and Malt-
Parser (Nivre et al, 2007). MSTParser is a graph-
based dependency parser where the best parse
tree is acquired by searching for a spanning tree
272
Dependency Path
Extraction
Feature-based ClassificationPreprocessing
HPSG
Parsing
Dependency
Parsing
Semantic 
Role 
Labeling
T
H
Dependency
Triple
Extraction
Path 
Extraction
Feature 
Extraction
SVM-based
Classification
Yes/No
No
Figure 1: Workflow of the System
which maximize the score on an either partially or
fully connected dependency graph. MaltParser is
a transition-based incremental dependency parser,
which is language-independent and data-driven. It
contains a deterministic algorithm, which can be
viewed as a variant of the basic shift-reduce al-
gorithm. Both parsers can achieve state-of-the-art
performance and Figure 2 shows the resulting syn-
tactic dependency trees of the following T-H pair,
ID: 2036; Entailment: YES
T: Devotees of the market question the value of
the work national service would perform.
H: Value is questioned.
Semantic Role Labeling The statistical depen-
dency parsers provide shallow syntactic analyses
of the entailment pairs through the limited vocab-
ulary of the dependency relations. In our case, the
CoNLL shared task dataset from 2008 were used
to train the statistical dependency parsing mod-
els. While such dependencies capture interesting
syntactic relations, when compared to the parsing
systems with deeper representations, the contained
information is not as detailed. To compensate for
this, we used a shallow semantic parser to predict
the semantic role relations in the T and H of en-
tailment pairs. The shallow semantic parser was
also trained with CoNLL 2008 shared task dataset,
with semantic roles extracted from the Propbank
and Nombank annotations (Zhang et al, 2008).
Figure 3 shows the resulting semantic dependency
graphs of the T-H pair.
HPSG Parsing We employ the English Re-
source Grammar (Flickinger, 2000), a hand-
written linguistic grammar in the framework of
HPSG, and the PET HPSG parser (Callmeier,
2001) to check the grammaticality of each hy-
pothesis sentence. As the hypotheses in this
PETE shared task were automatically generated,
some ungrammatical hypotheses occur in non-
entailment pairs. the grammaticality checking al-
lows us to quickly identify these instances.
2.2 Dependency Path Extraction
According to the task definition, we need to ver-
ify whether those dependency relations in H also
appear in T. We firstly find out all the impor-
tant dependency triples in H, like <word, depen-
dency relation, word>, excluding those having
stop words. The extracted syntactic dependency
triples of the example T-H pair would be none,
since the only content words ?value? and ?ques-
tioned? have no direct syntactic dependency in-
between (Figure 2). The extracted semantic de-
pendency triples would be <?questioned?, ?A1?,
?value?> (Figure 3).
After that, we use the word pairs contained in
the extracted dependency triples as anchors to find
out the corresponding dependency relations in T.
Notice that it is not necessarily that we can al-
ways find a direct dependency relation in T be-
tween the same word pair, so we need to traverse
the dependency tree or graph to find the depen-
dency paths. In general, we treat all the depen-
dency trees and graphs as undirected graphs with
loops, but keep records for the directions of the
edges we traverse. For the following three repre-
sentations, we apply slightly different algorithms
to find the dependency path between two words,
Syntactic Dependency Tree We simply traverse
the tree and find the corresponding depen-
dency path connecting the two words;
Semantic Dependency Graph We apply Dijk-
stra?s algorithm (Dijkstra, 1959) to find the
shortest path between the two words;
Joint Dependency Graph We assign different
weights to syntactic and semantic dependen-
cies and apply Dijkstra?s algorithm to find the
shortest path (with the lowest cost)
1
.
2.3 Feature-based Classification
Based on the meaning representation we have dis-
cussed above (Section 2.1 and Section 2.2), we ex-
1
In practice, we simply give semantic dependencies 0.5
cost and syntactic dependencies 1.0 cost, to show the prefer-
ences on the former when both exist.
273
T:
H:
Figure 2: Syntactic dependency of the example T-H pair by MaltParser.
T:
H:
Figure 3: Semantic dependency of the example T-H pair by MaltParser and our SRL system.
tract features for the machine-learning-based clas-
sifier. First of all, we should check whether there
are dependency triples extracted from H, other-
wise for our system, there is no meaning repre-
sentation for that sentence. Then we also need to
check whether the same words can be found in T
as well. Only if the corresponding dependency
paths are successfully located in T, we could ex-
tract the following features.
The direction of each dependency relation or
path could be interesting. The direction of the
H-path is clear, so we only need to check the
direction of the T-path. In practice, we simply
use a boolean value to represent whether T-path
contains dependency relations with different di-
rections. For instance, in Figure 3, if we extract
the path from ?market? to ?value?, the directions
of the dependency relations contained in the path
would be? and?, one of which would be incon-
sistent with the dependency relation in H.
Notice that all the dependency paths from H
have length 1
2
, but the lengths of the dependency
paths from T are varied. If the latter length is also
1, we can simply compare the two dependency re-
lations; otherwise, we compare each of the depen-
2
The length of one dependency path is defined as the num-
ber of dependency relations contained in the path.
dency relation contained the T-path with H-path
one by one
3
. By comparison, we mainly focus on
two values, the category of the dependency rela-
tion (e.g. syntactic dependency vs. semantic de-
pendency) and the content of the dependency rela-
tion (e.g. A1 vs. AM-LOC).
We also incorporate the string value of the de-
pendency relation pair and make it boolean ac-
cording to whether it occurs or not. Table 1 shows
the feature types we extract from each T-H pair.
3 Experiments
As we mentioned in the preprocessing section
(Section 2.1), we utilize the open source depen-
dency parsers, MSTParser
4
and MaltParser
5
, our
own semantic role labeler (Zhang et al, 2008), and
the PET HPSG parser
6
. For the shortest path algo-
rithm, we use the jGraphT package
7
; and for the
machine learning toolkit, we use the UniverSVM
3
Enlightened by Wang and Neumann (2007), we ex-
clude some dependency relations like ?CONJ?, ?COORD?,
?APPO?, etc., heuristically, since in most of the cases, they
will not change the relationship between the two words at
both ends of the path.
4
http://sourceforge.net/projects/
mstparser/
5
http://maltparser.org/
6
http://heartofgold.dfki.de/PET.html
7
http://jgrapht.sourceforge.net/
274
H
N
u
l
l
?
T
N
u
l
l
?
D
i
r
M
u
l
t
i
?
D
e
p
S
a
m
e
?
R
e
l
S
i
m
?
R
e
l
S
a
m
e
?
R
e
l
P
a
i
r
Joint + + + + + + + +
No Sem + + + + +
No Syn + + + + + + +
Table 1: Feature types of different settings of the
system. H Null? means whether H has dependencies;
T Null? means whether T has the corresponding paths (us-
ing the same word pairs found in H); Dir is whether the di-
rection of the path T the same as H; Multi? adds a prefix,
m , to the Rel Pair features, if the T-path is longer than one
dependency relation; Dep Same? checks whether the two de-
pendency types are the same, i.e. syntactic and semantic de-
pendencies; Rel Sim? only occurs when two semantic depen-
dencies are compared, meaning whether they have the same
prefixes, e.g. C-, AM-, etc.; Rel Same? checks whether the
two dependency relations are the same; and Rel Pair simple
concatenates the two relation labels together. Notice that, the
first seven feature types all contain boolean values, and for the
last one, we make it boolean as well, by observing whether
that pair of dependency labels appear or not.
package
8
. We test different dependency graphs
and feature sets as mentioned before (Table 1), and
the results are shown in Table 2.
MSTParser+SRL MaltParser+SRL
Joint No Sem No Syn Joint No Sem No Syn
+GC 0.5249
0.5116 0.5050
0.6678
0.5282 0.6346
(-1.3%) (-2.0%) (-14.0%) (-3.3%)
-GC 0.5216 0.5050 0.4950 0.6545 0.5282 0.6179
Table 2: Experiment results of our system with
different settings.
First of all, in almost all the cases, the grammat-
icality checking based on HPSG parsing is help-
ful, if we compare each pair of results at the two
rows, +GC and -GC. In all cases, the joint graph
representation achieves better results. This in-
dicates that features extracted from both syntac-
tic dependency and shallow semantic dependency
are useful for the entailment recognition. For the
MaltParser case, the semantic features show great
importance. Notice that the performance of the
whole system does not necessarily reflect the per-
formance of the parser itself, since it also depends
on our entailment modules. In all, the best setting
of our system ranks the 3rd place in the evaluation.
4 Conclusion
In this paper, we present our system used in the
PETE task, which consists of preprocessing, de-
pendency path extraction, and feature-based clas-
sification. We use MSTParser and MaltParser as
8
http://www.kyb.mpg.de/bs/people/
fabee/universvm.html
dependency parsers, our SRL system as a shallow
semantic parser, and a deep parser based on hand-
crafted grammars for grammaticality checking.
The entailment recognition is done by an SVM-
based classifier using features extracted from the
graph representation of the parser outputs. Based
on the results, we tentatively conclude that both
the syntactic and the shallow semantic features are
useful. A detailed error analysis would be our on-
going work in the near future.
Acknowledgment
The authors thank the PIRE PhD scholarship and
the German Excellence Cluster of MMCI for the
support of the work.
References
Ulrich Callmeier. 2001. Efficient parsing with large-scale
unification grammars. Master?s thesis, Universit?at des
Saarlandes, Saarbr?ucken, Germany.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Qui?nonero-Candela et al, editor, MLCW 2005, volume
LNAI Volume 3944, pages 177?190. Springer-Verlag.
E. W. Dijkstra. 1959. A note on two problems in connexion
with graphs. Numerische Mathematik, 1:269?271.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(1):1?41.
Rui Wang and G?unter Neumann. 2007. Recognizing textual
entailment using a subsequence kernel method. In Pro-
ceedings of AAAI-07, Vancouver, Canada, July.
Rui Wang and G?unter Neumann. 2009. An accuracy-
oriented divide-and-conquer strategy for recognizing tex-
tual entailment. In Proceedings of TAC 2008, Gaithers-
burg, Maryland, USA.
Deniz Yuret, Ayd?n Han, and Zehra Turgut. 2010. Semeval-
2010 task 12: Parser evaluation using textual entailments.
In Proceedings of the SemEval-2010 Evaluation Exercises
on Semantic Evaluation.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hybrid
learning of dependency structures from heterogeneous lin-
guistic resources. In Proceedings of the Twelfth Con-
ference on Computational Natural Language Learning
(CoNLL 2008), pages 198?202, Manchester, UK.
275
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 198?202
Manchester, August 2008
Hybrid Learning of Dependency Structures from Heterogeneous
Linguistic Resources
Yi Zhang
Language Technology Lab
DFKI GmbH
yzhang@coli.uni-sb.de
Rui Wang
Computational Linguistics
Saarland University, Germany
rwang@coli.uni-sb.de
Hans Uszkoreit
Language Technology Lab
DFKI GmbH
uszkoreit@dfki.de
Abstract
In this paper we present our syntactic and
semantic dependency parsing system par-
ticipated in both closed and open compe-
titions of the CoNLL 2008 Shared Task.
By combining the outcome of two state-of-
the-art syntactic dependency parsers, we
achieved high accuracy in syntactic de-
pendencies (87.32%). With MRSes from
grammar-based HPSG parsers, we achieved
significant performance improvement on
semantic role labeling (from 71.31% to
71.89%), especially in the out-domain
evaluation (from 60.16% to 62.11%).
1 Introduction
The CoNLL 2008 shared task (Surdeanu et al,
2008) provides a unique chance of comparing dif-
ferent syntactic and semantic parsing techniques
in one unified open competition. Our contribution
in this joint exercise focuses on the combination
of different algorithms and resources, aiming not
only for state-of-the-art performance in the com-
petition, but also for the dissemination of the learnt
lessons to related sub-fields in computational lin-
guistics.
The so-called hybrid approach we take has two
folds of meaning. For syntactic dependency pars-
ing, we build our system based on state-of-the art
algorithms. Past CoNLL share task results have
shown that transition-based and graph-based algo-
rithms started from radically different ideas, yet
achieved largely comparable results. One of the
question we would like investigate is whether the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
combination of the two approach on the output
level leads to even better results.
For the semantic role labeling (SRL) task, we
would like to build a system that allows us to test
the contribution of different linguistic resources.
To our special interest is to examine the deep
linguistic parsing systems based on hand-crafted
grammars. During the past decades, various large
scale linguistic grammars have been built, some
of which achieved both broad coverage and high
precision. In combination with other advances
in deep linguistic processing, e.g. efficient pars-
ing algorithms, statistical disambiguation models
and robust processing techniques, several systems
have reached mature stage to be deployed in ap-
plications. Unfortunately, due to the difficulties
in cross-framework evaluation, fair comparison of
these systems with state-of-the-art data-driven sta-
tistical parsers is still hard to achieve. More impor-
tantly, it is not even clear whether deep linguistic
analysis is necessary at all for tasks such as shallow
semantic parsing (also known as SRL). Drawing
a conclusion on this latter point with experiments
using latest deep parsing techniques is one of our
objective.
The remainder of the paper is structure as fol-
lows. Section 2 introduces the overall system ar-
chitecture. Section 3 explains the voting mecha-
nism used in the syntactic parser. Section 4 de-
scribes in detail the semantic role labeling com-
ponent. Section 5 presents evaluation results and
error analysis. Section 6 concludes the paper.
2 System Architecture
As shown in Figure 1, our system is a two-stage
pipeline. For the syntactic dependencies, we apply
two state-of-the-art dependency parsers and com-
bined their results based on a voting model. For
198
Parse Selector
(MaltParser)Transition?based DepParser (MST Parser)Graph?based DepParser
Deep Linguistic Parser(ERG/PET)Predicate Identification
Argument Identification
Argument Classification
Predicated Classification
SemanticRoleLabeling
Syn.Dep.
MRS
SyntacticDependencyParsing
Figure 1: System Architecture
the semantic roles, we extracted features from the
previous stage, combined with deep parsing results
(in MRS), and use statistical classification models
to make predictions. In particular, the second part
can be further divided into four stages: predicate
identification (PI), argument identification (AI), ar-
gument classification (AC), and predicate classi-
fication (PC). Maximum entropy-based machine
learning techniques are used in both components
which we will see in detail in the following sec-
tions.
3 Syntactic Dependency Parsing
For obtaining syntactic dependencies, we have
combined the results of two state-of-the-art depen-
dency parsers: the MST parser (McDonald et al,
2005) and the MaltParser (Nivre et al, 2007).
The MST parser formalizes dependency parsing
as searching for maximum spanning trees (MSTs)
in directed graphs. A major advantage of their
framework is the ability to naturally and efficiently
model both projective and non-projective parses.
To learn these structures they used online large-
margin learning that empirically provides state-of-
the-art performance.
The MaltParser is a transition-based incremental
dependency parser, which is language-independent
and data-driven. It contains a deterministic algo-
rithm, which can be viewed as a variant of the ba-
sic shift-reduce algorithm. The learning method
they applied is support vector machine and experi-
mental evaluation confirms that the MaltParser can
achieve robust, efficient and accurate parsing for a
wide range of languages.
Since both their parsing algorithms and machine
learning methods are quite different, we decide to
take advantages of them. After a comparison be-
tween the results of the two parsers
1
, we find that,
1. The MST parser is better at the whole struc-
ture. In several sentences, the MaltParser was
wrong at the root node, but the MST parser is
correct.
2. The MaltParser is better at some dependency
labels (e.g. TMP, LOC, etc.).
These findings motivate us to do a voting based
on both outputs. The features considered in the
voting model are as follows:
? Dependency path: two categories of depen-
dency paths are considered as features: 1)
the POS-Dep-POS style and 2) the Dep-Dep
style. The former consists of part-of-speech
(POS) tags and dependency relations appear-
ing in turns; and the latter only contains de-
pendency relations. The maximum length of
the dependency path is three dependency re-
lations.
? Root attachments: the number of tokens at-
tached to the ROOT node by the parser in one
sentence
? Sentence length: the number of tokens in
each input sentence
? Projectivity: whether the parse is projective
or not
With these features, we apply a statistical model
to predict, for each sentence, we choose the pars-
ing result from which parser. The voted result will
be our syntactic dependency output and be passed
to the later stages.
4 Semantic Role Labeling
4.1 Overview
The semantic role labeling component of our sys-
tem is comprised of a pipeline model with four
1
In this experiment, we use second order features and the
projective decoder for the MST parser trained with 10 iter-
ations, and Arc-eager algorithm with a quadric polynomial
kernel for the MaltParser.
199
sub-components that performs predicate identi-
fication (PI), argument identification (AI), argu-
ment classification (AC) and predicate classifica-
tion (PC) respectively. The output in previous
steps are taken as input information to the follow-
ing stages. All these components are essentially
based on a maximum entropy statistical classifier,
although with different task-specific optimizations
and feature configurations in each step. Depending
on the available information from the input data
structure, the same architecture is used for both
closed and open challenge runs, with different fea-
ture types. Note that our system does not make use
of or predict SU chains.
Predicate Identification The component makes
binary prediction on each input token whether it
forms a predicate in the input sentence. This pre-
dictor precedes other components because it is a
relatively easy task (comparing to the following
components). Also, making this prediction early
helps to cut down the search space in the follow-
ing steps. Based on the observation on the training
data, we limit the PI predictor to only predict for
tokens with certain POS types (POSes marked as
predicates for at least 50 times in the training set).
This helps to significantly improve the system effi-
ciency in both training and prediction time without
sacrificing prediction accuracy.
It should be noted that the prediction of nominal
predicates are generally much more difficult (based
on CoNLL 2008 shared task annotation). The PI
model achieved 96.32 F-score on WSJ with verbal
predicates, but only 84.74 on nominal ones.
Argument Identification After PI, the argu-
ments to the predicted predicates are identified
with the AI component. Similar to the approach
taken in Hacioglu (2004), we use a statistical clas-
sifier to select from a set of candidate nodes in a
dependency tree. However, instead of selecting
from a set of neighboring nodes from the predicate
word
2
, we define the concept of argument path as
a chain of dependency relations from the predicate
to the argument in the dependency tree. For in-
stance, an argument path [??? | ???] indicates that
if the predicate is syntactically depending as ??? on
a node which has a ??? child, then the ??? node
2
Hacioglu (2004) defines a tree-structured family of a
predicate as a measure of locality. It is a set of dependency
relation nodes that consists of the predicate?s parent, chil-
dren, grandchildren, siblings, siblings? children and siblings?
grandchildren with respect to its dependency tree
(sibling to the predicate) is an argument candidate.
While Hacioglu (2004)?s approach focus mainly
on local arguments (with respect to the syntactic
dependencies), our approach is more suitable of
capturing long distance arguments from the pred-
icate. Another minor difference is that we allow
predicate word to be its own argument (which is
frequently the case for nominal predicates) with an
empty argument path [ | ].
The set of effective argument paths are obtained
from the training set, sorted and filtered according
to their frequencies, and used in testing to obtain
the candidate arguments. By setting a frequency
threshold, we are able to select the most useful
argument paths. The lower the threshold is, the
higher coverage one might get in finding candi-
date arguments, accompanied with a higher aver-
age candidate number per predicate and potentially
a more difficult task for the statistical classifier.
By experimenting with different frequency thresh-
olds on the training set, we established a frequency
threshold of 40, which guarantees candidate argu-
ment coverage of 95%, and on average 5.76 candi-
dates per predicate. Given that for the training set
each predicate takes on average 2.13 arguments,
the binary classifier will have relatively balanced
prediction classes.
Argument Classification For each identified ar-
gument, an argument label will be assigned during
the argument classification step. Unlike the binary
classifiers in previous two steps, AC uses a multi-
class classifier that predicts from the inventory of
argument labels. For efficiency reasons, we only
concern the most frequent 25 argument labels.
Predicate Classification The final step in the
SRL component labels the predicted predicate with
a predicate name. Due to the lack of lexical
resources in the closed competition, this step is
scheduled for the last, in order to benefit from the
predictions made in the previous steps. Unlike the
previous steps, the statistical model used in this
step is a ranking model. We obtained a list of can-
didate frames and corresponding rolesets from the
provided PropBank and NomBank data. Each pre-
dicted predicate is mapped onto the potential role-
sets it may take. When the frame for the predicate
word is missing from the list, or there is only one
candidate roleset for it, the predicate name is as-
signed deterministically (word stem concatenated
with ?.01? for frame missing predicates, the unam-
200
biguous roleset name when there is only one can-
didate). When there are more than one candidate
rolesets, a ranking model is trained to select the
most probable roleset for a given predicate given
the syntactic and semantic context.
4.2 Features
The feature types used in our SRL component are
summarized in Table 1, with the configurations of
our submitted ?closed? and ?open? runs marked.
Numerous different configurations with these fea-
ture types have been experimented on training and
development data. The results show that feature
types 1?14 are the best performing ones. Fea-
tures related to the siblings of the predicate only
introduce minor performance variation. We also
find the named entity labels does not lead to im-
mediate improvement of SRL performance. The
WordNet sense feature does achieve minor perfor-
mance increase on PI and PC, although the signif-
icant remains to be further examined. Based on
the pipeline model, we find it difficult to achieve
further improvement by incorporate more features
types from provided annotation. And the vari-
ance of SRL performance with different open fea-
tures is usually less than 1%. To clearly show the
contribution of extra external resources, these less
contributing features (siblings, named entity labels
and WordNet sense) are not used in our submitted
?open? runs.
MRSes as features to SRL As a novel point of
our SRL system, we incorporate parsing results
from a linguistic grammar-based parsing system
in our ?open? competition run. In this experi-
ment, we used English Resource Grammar (ERG,
Flickinger (2000)), a precision HPSG grammar for
English. For parsing, we used PET (Callmeier,
2001), an efficient HPSG parser, together with ex-
tended partial parsing module (Zhang et al, 2007)
for maximum robustness. The grammar is hand-
crafted and the disambiguation models are trained
on Redwoods treebanks. They present general lin-
guistic knowledge, and are not tuned for the spe-
cific domains in this competition.
While the syntactic analysis of the HPSG gram-
mar is largely different from the dependency anno-
tation used in this shared task, the semantic rep-
resentations do share a similar view on predicate-
argument structures. ERG uses as its semantic
representation the Minimal Recursion Semantics
(MRS, Copestake et al (2005)), a non-recursive flat
structure that is suitable for underspecifying scope
ambiguities. A predicate-argument backbone of
MRS can be extracted by identifying shared vari-
ables between elementary predications (??s).
In order to align the HPSG parser?s I/O with
CoNLL?s annotation, extensive mapping scripts
are developed to preprocess the texts, and extract
backbone from output MRSes. The unknown word
handling techniques (Zhang and Kordoni, 2005)
are used to overcome lexical gaps. Only the best
analysis is used for MRS extraction. Without par-
tial parsing, the ERG achieves around 70% of raw
coverage on the training data. When partial pars-
ing is used, almost all the sentences received ei-
ther full or partial analysis (except for several cases
where computational resources are exhausted), and
the SRL performance improves by ?0.5%.
5 Results
Among 20+ participant groups, our system ranked
seventh in the ?closed? competition, and first in
the ?open? challenge. The performance of the syn-
tactic and semantic components of our system are
summarized in Table 2.
In-Domain Out-Domain
Lab. Unlab. Lab. Unlab.
Syntactic Dep. 88.14% 90.78% 80.80% 86.12%
S
R
L
Closed 72.67% 82.68% 60.16% 76.98%
Open 73.08% 83.04% 62.11% 78.48%
Table 2: Labeled and unlabeled attachment scores
in syntactic dependency parsing and F1 score for
semantic role labeling.
The syntactic voting and semantic labeling parts
of our system are implemented in Java together
with a few Perl scripts. Using the open source
TADM for parameter estimation, our the voting
component take no more than 1 minute to train and
10 seconds to run (on WSJ testset). The SRL com-
ponent takes about 1 hour for training, and no more
than 30 seconds for labeling (WSJ testset).
Result analysis shows that the combination of
the two state-of-the-art parsers delivers good syn-
tactic dependencies (ranked 2nd). Error analysis
shows most of the errors are related to preposi-
tions. One category is the syntactic ambiguity of
pp-attachment, e.g. in ?when trading was halted
in Philip Morris?, ?in? can be attached to either
?trading? or ?halted?. The other category is the
LOC and TMP tags in phrases like ?at the end of
the day?, ?at the point of departure?, etc.
201
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
P
l
e
m
m
a
P
P
O
S
P
r
e
l
P
-
p
a
r
e
n
t
P
O
S
A
l
e
m
m
a
A
P
O
S
A
r
e
l
P
-
c
h
i
l
d
r
e
n
P
O
S
e
s
P
-
c
h
i
l
d
r
e
n
r
e
l
P
-
A
p
a
t
h
A
-
c
h
i
l
d
r
e
n
P
O
S
e
s
A
-
c
h
i
l
d
r
e
n
r
e
l
s
P
p
r
e
c
e
d
e
s
A
?
A
?
s
p
o
s
i
t
i
o
n
P
-
s
i
b
l
i
n
g
s
P
O
S
e
s
P
-
s
i
b
l
i
n
g
s
r
e
l
s
P
N
E
l
a
b
e
l
P
W
N
s
e
n
s
e
P
M
R
S
?
?
-
n
a
m
e
P
M
R
S
-
a
r
g
s
l
a
b
e
l
s
P
M
R
S
-
a
r
g
s
P
O
S
e
s
A
M
R
S
?
?
-
n
a
m
e
A
M
R
S
-
p
r
e
d
s
l
a
b
e
l
s
A
M
R
S
-
p
r
e
d
s
P
O
S
e
s
PI ? ? ? ? ? ? ? ?   
AI ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?      
AC ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?      
PC ? ? ? ? ? ? ? ?   
Table 1: Feature types used in semantic role labeling sub-components. Feature types marked with ? are
used in the ?closed? run; feature types marked with  are used in the ?open? run; feature types marked
with ? are used in both runs. P denotes predicate; A denotes semantic argument.
The results on semantic role labeling show,
sometimes, even with syntactic errors of
LOC/TMP tags, the semantic role labeler can
still predict AM-LOC/AM-TMP correctly, which
indicates the robustness of our hybrid approach.
By comparing our ?closed? and ?open? runs, the
MRS features do introduce a clear performance
improvement. The performance gain is even
more significant in out-domain test, showing that
the MRS features from ERG are indeed much less
domain dependent. Another example worth men-
tioning is that, in the sentence ?Scotty regarded the
ear and the grizzled hair around it with a moment
of interest?, it is extremely difficult to know that
?Scotty? is a semantic role of ?interest?.
Also, we are the only group that submitted runs
for both tracks, and achieved better performance
in open competition. Although the best ways of
integrating deep linguistic processing techniques
remain as an open question, the achieved results
at least show that hand-crafted grammars like ERG
do provide heterogeneous linguistic insights that
can potentially find their usage in data-driven NLP
tasks as such.
6 Conclusion
In this paper, we described our hybrid system
on both syntactic and semantic dependencies la-
beling. We built a voting model to combine
the results of two state-of-the-art syntactic depen-
dency parsers, and a pipeline model to combine
deep parsing results for SRL. The experimental re-
sults showed the advantages of our hybrid strat-
egy, especially on the cross-domain data set. Al-
though the optimal ways of combining deep pro-
cessing techniques remains to be explored, the
performance gain achieved by incorporating hand-
crafted grammar outputs shows a promising direc-
tion of study for both fields.
References
Callmeier, Ulrich. 2001. Efficient parsing with large-scale
unification grammars. Master?s thesis, Universit?at des
Saarlandes, Saarbr?ucken, Germany.
Copestake, Ann, Dan Flickinger, Carl J. Pollard, and Ivan A.
Sag. 2005. Minimal recursion semantics: an introduction.
Research on Language and Computation, 3(4):281?332.
Flickinger, Dan. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28.
Hacioglu, Kadri. 2004. Semantic role labeling using de-
pendency trees. In Proceedings of COLING 2004, pages
1273?1276, Geneva, Switzerland, Aug 23?Aug 27.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Nivre, Joakim, Jens Nilsson, Johan Hall, Atanas Chanev,
G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and Er-
win Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(1):1?41.
Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu??s
M`arquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th Conference
on Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Zhang, Yi and Valia Kordoni. 2005. A statistical approach
towards unknown word type prediction for deep grammars.
In Proceedings of the Australasian Language Technology
Workshop 2005, pages 24?31, Sydney, Australia.
Zhang, Yi, Valia Kordoni, and Erin Fitzgerald. 2007. Partial
parse selection for robust deep processing. In Proceed-
ings of ACL 2007 Workshop on Deep Linguistic Process-
ing, pages 128?135, Prague, Czech.
202
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 163?167,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Cheap Facts and Counter-Facts
Rui Wang
Computational Linguistics Department
Saarland University
Room 2.04, Building C 7.4
Saarbruecken, 66123 Germany
rwang@coli.uni-sb.de
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
3400 N. Charles Street (CSEB 226-B)
Baltimore, MD 21218, USA
ccb@cs.jhu.com.edu
Abstract
This paper describes our experiments of us-
ing Amazon?s Mechanical Turk to generate
(counter-)facts from texts for certain named-
entities. We give the human annotators a para-
graph of text and a highlighted named-entity.
They will write down several (counter-)facts
about this named-entity in that context. The
analysis of the results is performed by com-
paring the acquired data with the recognizing
textual entailment (RTE) challenge dataset.
1 Motivation
The task of RTE (Dagan et al, 2005) is to say
whether a person would reasonably infer some short
passage of text, the Hypothesis (H), given a longer
passage, the Text (T). However, collections of such
T-H pairs are rare to find and these resources are the
key to solving the problem.
The datasets used in the RTE task were collected
by extracting paragraphs of news text and manu-
ally constructing hypotheses. For the data collected
from information extraction task, the H is usually
a statement about a relation between two named-
entities (NEs), which is written by expertise. Simi-
larly, the H in question answering data is constructed
using both the question and the (in)correct answers.
Therefore, the research questions we could ask are,
1. Are these hypotheses really those ones people
interested in?
2. Are hypotheses different if we construct them
in other ways?
3. What would be a good negative hypotheses
compared with the positive ones?
In this paper, we address these issues by using
Amazon?s Mechanical Turk (MTurk), online non-
expert annotators (Snow et al, 2008). Instead of
constructing the hypotheses targeted to IE or QA, we
just ask the human annotators to come up with some
facts they consider as relevant to the given text. For
negative hypotheses, we change the instruction and
ask them to write counter-factual but still relevant
statements. In order to narrow down the content of
the generated hypotheses, we give a focused named-
entity (NE) for each text to guide the annotators.
2 Related Work
The early related research was done by Cooper et al
(1996), where they manually construct a textbook-
style corpus aiming at different semantic phenom-
ena involved in inference. However, the dataset is
not large enough to train a robust machine-learning-
based RTE system. The recent research from the
RTE community focused on acquiring large quan-
tities of textual entailment pairs from news head-
lines (Burger and Ferro, 2005) and negative exam-
ples from sequential sentences with transitional dis-
course connectives (Hickl et al, 2006). Although
the quality of the data collected were quite good,
most of the positive examples are similar to summa-
rization and the negative examples are more like a
comparison/contrast between two sentences instead
of a contradiction. Those data are the real sen-
tences used in news articles, but the way of obtain-
ing them is not necessarily the (only) best way to
163
find entailment pairs. In this paper, we investigate
an alternative inexpensive way of collecting entail-
ment/contradiction text pairs by crowdsourcing.
In addition to the information given by the text,
common knowledge is also allowed to be involved
in the inference procedure. The Boeing-Princeton-
ISI (BPI) textual entailment test suite1 is specifically
designed to look at entailment problems requiring
world knowledge. We will also allow this in the de-
sign of our task.
3 Design of the Task
The basic idea of the task is to give the human an-
notators a paragraph of text with one highlighted
named-entity and ask them to write some (counter-
)facts about it. In particular, we first preprocess
an existing RTE corpus using a named-entity rec-
ognizer to mark all the named-entities appearing in
both T and H. When we show the texts to Turkers,
we highlight one named-entity and give them one of
these two sets of instructions:
Facts: Please write several facts about the high-
lighted words according to the paragraph. You
may add additional common knowledge (e.g.
Paris is in France), but please mainly use the
information contained in the text. But please
do not copy and paste!
Counter-Facts: Please write several statements that
are contradictory to the text. Make your state-
ments about the highlighted words. Please use
the information mainly in the text. Avoid using
words like not or never.
Then there are three blank lines given for the annota-
tors to fill in facts or counter-factual statements. For
each HIT, we gather facts or counter-facts for five
texts, and for each text, we ask three annotators to
perform the task. We give Turkers one example as a
guide along with the instructions.
4 Experiments and Results
The texts we use in our experiments are the develop-
ment set of the RTE-5 challenge (Bentivogli et al,
1http://www.cs.utexas.edu/?pclark/
bpi-test-suite/
Total Average (per Text)
Extracted NEs
Facts 244 1.19
Counter-Facts 121 1.11
Generated Hypotheses
Facts 790 3.85
Counter-Facts 203 1.86
Table 1: The statistics of the (valid) data we collect. The
Total column presents the number of extracted NEs and
generated hypotheses and the Average column shows the
average numbers per text respectively.
2009), and we preprocess the data using the Stan-
ford named-entity recognizer (Finkel et al, 2005).
In all, it contains 600 T-H pairs, and we use the texts
to generate facts and counter-facts and hypotheses as
references. We put our task online through Crowd-
Flower2, and on average, we pay one cent for each
(counter-)fact to the Turkers. CrowdFlower can help
with finding trustful Turkers and the data were col-
lected within a few hours.
To get a sense of the quality of the data we collect,
we mainly focus on analyzing the following three
aspects: 1) the statistics of the datasets themselves;
2) the comparison between the data we collect and
the original RTE dataset; and 3) the comparison be-
tween the facts and the counter-facts.
Table 1 show some basic statistics of the data we
collect. After excluding invalid and trivial ones3, we
acquire 790 facts and 203 counter-facts. In general,
the counter-facts seem to be more difficult to obtain
than the facts, since both the total number and the
average number of the counter-facts are less than
those of the facts. Notice that the NEs are not many
since they have to appear in both T and H.
The comparison between our data and the original
RTE data is shown in Table 2. The average length of
the generated hypotheses is longer than the original
hypotheses, for both the facts and the counter-facts.
Counter-facts seem to be more verbose, since addi-
tional (contradictory) information is added. For in-
stance, example ID 425 in Table 4, Counter Fact 1
can be viewed as the more informative but contra-
dictory version of Fact 1 (and the original hypoth-
2http://crowdflower.com/
3Invalid data include empty string or single words; and the
trivial ones are those sentences directly copied from the texts.
164
esis). The average bag-of-words similarity scores
are calculated by dividing the number of overlap-
ping words of T and H by the total number of words
in H. In the original RTE dataset, the entailed hy-
potheses have a higher BoW score than the contra-
dictory ones; while in our data, facts have a lower
score than the counter-facts. This might be caused
by the greater variety of the facts than the counter-
facts. Fact 1 of example ID 425 in Table 4 is almost
the same as the original hypothesis, and Fact 2 of
example ID 374 as well, though the latter has some
slight differences which make the answer different
from the original one. The NE position in the sen-
tence is another aspect to look at. We find that peo-
ple tend to put the NEs at the beginning of the sen-
tences more than other positions, while in the RTE
datasets, NEs appear in the middle more frequently.
In order to get a feeling of the quality of the
data, we randomly sampled 50 generated facts and
counter-facts and manually compared them with the
original hypotheses. Table 3 shows that generated
facts are easier for the systems to recognize, and the
counter-facts have the same difficulty on average.
Although it is subjective to evaluate the difficulty
of the data by human reading, in general, we follow
the criteria that
1. Abstraction is more difficult than extraction;
2. Inference is more difficult than the direct en-
tailment;
3. The more sentences in T are involved, the more
difficult that T-H pair is.
Therefore, we view the Counter Fact 1 in example
ID 16 in Table 4 is more difficult than the original
hypothesis, since it requires more inference than the
direct fact validation. However, in example ID 374,
Fact 1 is easier to be verified than the original hy-
pothesis, and same as those facts in example ID 506.
Similar hypotheses (e.g. Fact 1 in example ID 425
and the original hypothesis) are treated as being at
the same level of difficulty.
After the quantitive analysis, let?s take a closer
look at the examples in Table 4. The facts are usually
constructed by rephrasing some parts of the text (e.g.
in ID 425, ?after a brief inspection? is paraphrased
by ?investigated by? in Fact 2) or making a short
Valid Harder Easier Same
Facts 76% 16% 24% 36%
Counter-Facts 84% 36% 36% 12%
Table 3: The comparison of the generated (counter-)facts
with the original hypotheses. The Valid column shows the
percentage of the valid (counter-)facts; and other columns
present the distribution of harder, easier cases than the
original hypotheses or with the same difficulty.
RTE-5 Our Data
Counter-/Facts 300/300 178/178
All ?YES? 50% 50%
BoW Baseline 57.5% 58.4%
Table 5: The results of baseline RTE systems on the data
we collected, compared with the original RTE-5 dataset.
The Counter-/Facts row shows the number of the T-H
pairs contained in the dataset; and the other scores in per-
centage are accuracy of the systems.
summary (e.g. Fact 1 in ID 374, ?George Stranahan
spoke of Thompson?s death.?). For counter-facts, re-
moving the negation words or changing into another
adjective is one common choice, e.g. in ID 374,
Counter Fact 1 removed ?n?t? and Counter Fact 3
changed ?never? into ?fully?. The antonyms can
also make the contradiction, as ?rotten? to ?great?
in Counter Fact 2 in ID 374.
Example ID 506 in Table 4 is another interest-
ing case. There are many facts about Yemen, but
no valid counter-facts are generated. Furthermore,
if we compare the generated facts with the original
hypothesis, we find that people tend to give straight-
forward facts instead of abstracts4.
At last, we show some preliminary results on test-
ing a baseline RTE system on this dataset. For
the sake of comparison, we extract a subset of the
dataset, which is balanced on entailment and con-
tradiction text pairs, and compare the results with
the same system on the original RTE-5 dataset. The
baseline system uses a simple BoW-based similar-
ity measurement between T and H (Bentivogli et al,
2009) and the results are shown in Table 5.
The results indicate that our data are slightly ?eas-
ier? than the original RTE-5 dataset, which is consis-
tent with our human evaluation on the sampled data
4But this might also be caused by the design of our task.
165
Ave. Length Ave. BoW
NE Position
Head Middle Tail
Original Entailment Hypotheses 7.6 0.76 46% 53% 1%
Facts 9.8 0.68 68% 29% 3%
Original Contradiction Hypotheses 7.5 0.72 44% 56% 0%
Counter-Facts 12.3 0.75 59% 38% 3%
Table 2: The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset. The
Ave. Length column represents the average number of words in each hypothesis; The Ave. BoW shows the average
bag-of-words similarity compared with the text. The three columns on the right are all about the position of the NE
appearing in the sentence, how likely it is at the head, middle, or tail of the sentence.
(Table 3). However, it is still too early to draw con-
clusions based on the simple baseline results.
5 Conclusion and Future Work
In this paper, we report our experience of using
MTurk to collect facts and counter-facts about the
given NEs and texts. We find that the generated hy-
potheses are not entirely the same as the original
hypotheses in the RTE data. One direct extension
would be to use more than one NE at one time, but it
may also cause problems, if those NEs do not have
any relations in-between. Another line of research
would be to test this generated resources using some
real existing RTE systems and compare the results
with the original RTE datasets, and also further ex-
plore the potential application of this resource.
Acknowledgments
The first author is supported by the PIRE scholar-
ship program. The second author is supported by
the EuroMatrixPlusProject (funded by the European
Commission), by the DARPA GALE program under
Contract No. HR0011-06-2-0001, and by the NSF
under grant IIS-0713448. The views and findings
are the authors? alone.
References
L. Bentivogli, B. Magnini, I. Dagan, H.T. Dang, and
D. Giampiccolo. 2009. The fifth pascal recogniz-
ing textual entailment challenge. In Proceedings of
the Text Analysis Conference (TAC 2009) Workshop,
Gaithersburg, Maryland, USA, November. National
Institute of Standards and Technology.
John Burger and Lisa Ferro. 2005. Generating an entail-
ment corpus from news headlines. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 49?54, Ann Arbor,
Michigan, USA. Association for Computational Lin-
guistics.
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,
Johan Van Genabith, Jan Jaspars, Hans Kamp, David
Milward, Manfred Pinkal, Massimo Poesio, and Steve
Pulman. 1996. Using the framework. Technical Re-
port LRE 62-051 D-16, The FraCaS Consortium.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
pascal recognising textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005),
pages 363?370. Association for Computational Lin-
guistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recogniz-
ing textual entailment with lcc?s groundhog system. In
Proceedings of the Second PASCAL Challenges Work-
shop.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
Evaluating non- expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP.
166
ID: 16 Answer: Contradiction
Original Text The father of an Oxnard teenager accused of gunning down a gay classmate who was romanti-
cally attracted to him has been found dead, Ventura County authorities said today. Bill McIner-
ney, 45, was found shortly before 8 a.m. in the living room of his Silver Strand home by a friend,
said James Baroni, Ventura County?s chief deputy medical examiner. The friend was supposed
to drive him to a court hearing in his son?s murder trial, Baroni said. McInerney?s 15-year-old
son, Brandon, is accused of murder and a hate crime in the Feb. 12, 2008, shooting death of
classmate Lawrence ?Larry? King, 15. The two boys had been sparring in the days before the
killing, allegedly because Larry had expressed a romantic interest in Brandon.
Original Hypothesis Bill McInerney is accused of killing a gay teenager.
NE 1: Bill McInerney
Counter Fact 1 Bill McInerney is still alive.
ID: 374 Answer: Contradiction
Original Text Other friends were not surprised at his death. ?I wasn?t surprised,? said George Stranahan, a
former owner of the Woody Creek Tavern, a favourite haunt of Thompson. ?I never expected
Hunter to die in a hospital bed with tubes coming out of him.? Neighbours have said how his
broken leg had prevented him from leaving his house as often as he had liked to. One neighbour
and long-standing friend, Mike Cleverly, said Thompson was clearly hobbled by the broken leg.
?Medically speaking, he?s had a rotten year.?
Original Hypothesis The Woody Creek Tavern is owned by George Stranahan.
NE 1: George Stranahan
Fact 1 George Stranahan spoke of Thompson?s death.
Fact 2 George Stranahan once owned the Woody Creek Tavern.
Counter Fact 1 George Stranahan was surprised by his friend?s death.
Counter Fact 2 Medically, George Stranahan?s friend, Humter Thompson, had a great year.
Counter Fact 3 George Stranahan fully expected Thompson to die in a hospital with tubes coming out of him.
NE 2: Woody Creek Tavern
Fact 1 Woody Creek Tavern was previously owned by George Stranahan.
ID: 425 Answer: Entailment
Original Text Merseyside Police concluded after a brief inspection that the controversial blog Liverpool Evil
Cabal does not break criminal law. However the council officers continue to search for the
editor. The blog has been blocked on computers controlled by Liverpool Direct Ltd, a company
jointly owned by Liverpool City Council and British Telecom. The council?s elected officials
have denied ordering the block and are currently investigating its origin.
Original Hypothesis Liverpool Evil Cabal is the name of an online blog.
NE 1: Liverpool Evil Cabal
Fact 1 Liverpool Evil Cabal is a web blog.
Fact 2 Liverpool Evil Cabal was a blog investigated by the Merseyside Police.
Counter Fact 1 Liverpool Evil Cabal is a blog of Liverpool Direct Ltd.
Counter Fact 2 Liverpool Evil Cabal is freed from the charges of law breaking.
ID: 506 Answer: Entailment
Original Text At least 58 people are now dead as a result of the recent flooding in Yemen, and at least 20,000
in the country have no access to shelter. Five people are also reported missing. The Yemeni
government has pledged to send tents to help the homeless. The flooding is caused by the recent
heavy rain in Yemen, which came as a shock due to the fact that the country only receives several
centimeters of rain per year.
Original Hypothesis Heavy rain caused flooding in Yemen.
NE 1: Yemen
Fact 1 58 people are dead in Yemen because of flooding.
Fact 2 5 people in Yemen are missing.
Fact 3 At least 58 people are dead in Yemen because of flooding.
Table 4: Examples of facts and counter-facts, compared with the original texts and hypotheses. We ask the Turkers to
write several (counter-)facts about the highlighted NEs, and only part of the results are shown here.
167
Discriminative Parse Reranking for Chinese with Homogeneous and
Heterogeneous Annotations
Weiwei Sun?? and Rui Wang? and Yi Zhang??
?Department of Computational Linguistics, Saarland University
?German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
{wsun,rwang,yzhang}@coli.uni-saarland.de
Abstract
Discriminative parse reranking has been
shown to be an effective technique to im-
prove the generative parsing models. In
this paper, we present a series of exper-
iments on parsing the Tsinghua Chinese
Treebank with hierarchically split-merge
grammars and reranked with a perceptron-
based discriminative model. In addition to
the homogeneous annotation on TCT, we
also incorporate the PCTB-based parsing
result as heterogeneous annotation into
the reranking feature model. The rerank-
ing model achieved 1.12% absolute im-
provement on F1 over the Berkeley parser
on a development set. The head labels in
Task 2.1 are annotated with a sequence
labeling model. The system achieved
80.32 (B+C+H F1) in CIPS-SIGHAN-
2010 Task 2.1 (Open Track) and 76.11
(Overall F1) in Task 2.2 (Open Track)1.
1 Introduction
The data-driven approach to syntactic analysis of
natural language has undergone revolutionary de-
velopment in the last 15 years, ever since the
first few large scale syntactically annotated cor-
pora, i.e. treebanks, became publicly available in
the mid-90s of the last century. One and a half
decades later, treebanks remain to be an expensive
type of language resources and only available for
1This result is achieved with a bug-fixed version of the
system and does not correspond to the numbers in the origi-
nal evaluation report.
a small number of languages. The main issue that
hinders large treebank development projects is
the difficulties in creating a complete and consis-
tent annotation guideline which then constitutes
the very basis for sustainable parallel annotation
and quality assurance. While traditional linguistic
studies typically focus on either isolated language
phenomena or limited interaction among a small
groups of phenomena, the annotation scheme in
treebanking project requires full coverage of lan-
guage use in the source media, and proper treat-
ment with an uniformed annotation format. Such
high demand from the practical application of lin-
guistic theory has given rise to a countless num-
ber of attempts and variations in the formaliza-
tion frameworks. While the harsh natural selec-
tion set the bar high and many attempts failed to
even reach the actual annotation phase, a hand-
ful highly competent grammar frameworks have
given birth to several large scale treebanks.
The co-existence of multiple treebanks with
heterogeneous annotation presents a new chal-
lenge to the consumers of such resources. The im-
mediately relevant task is the automated syntactic
analysis, or parsing. While many state-of-the-art
statistical parsing systems are not bound to spe-
cific treebank annotation (assuming the formal-
ism is predetermined independently), almost all
of them assume homogeneous annotation in the
training corpus. Therefore, such treebanks can not
be simply put together when training the parser.
One approach would be to convert them into an
uniformed representation, although such conver-
sion is usually difficult and by its nature error-
prune. The differences in annotations constitute
different generative stories: i.e., when the pars-
ing models are viewed as mechanisms to produce
structured sentences, each treebank model will as-
sociate its own structure with the surface string in-
dependently. On the other hand, if the discrimina-
tive view is adopted, it is possible to use annota-
tions in different treebanks as indication of good-
ness of the tree in the original annotation.
In this paper, we present a series of experi-
ments to improve the Chinese parsing accuracy
on the Tsinghua Chinese Treebank. First, we use
coarse-to-fine parsing with hierarchically split-
merge generative grammars to obtain a list of can-
didate trees in TCT annotation. A discriminative
parse selection model is then used to rerank the
list of candidates. The reranking model is trained
with both homogeneous (TCT) and heterogeneous
(PCTB) data. A sequence labeling system is used
to annotate the heads in Task 2-1.
The remaining part of the paper is organized as
follows. Section 2 reviews the relevant previous
study on generative split-merge parsing and dis-
criminative reranking models. Section 3 describes
the work flow of our system participated in the
CIPS-SIGHAN-2010 bake-off Task 2. Section 4
describes the detailed settings for the evaluation
and the empirical results. Section 5 concludes the
paper.
2 Background
Statistical constituent-based parsing is popular-
ized through the decade-long competition on pars-
ing the Wall Street Journal sections of the English
Penn Treebank. While the evaluation setup has
for long seen its limitation (a frustratingly low
of 2% overall improvement throughout a decade
of research), the value of newly proposed pars-
ing methods along the way has clearly much more
profound merits than the seemly trivial increase in
evaluation figures. In this section we review two
effective techniques in constituent-based statisti-
cal parsing, and their potential benefits in parsing
Chinese.
Comparing with many other languages, statisti-
cal parsing for Chinese has reached early success,
due to the fact that the language has relatively
fixed word order and extremely poor inflectional
morphology. Both facts allow the PCFG-based
statistical modeling to perform well. On the other
hand, the much higher ambiguity between basic
word categories like nouns and verbs makes Chi-
nese parsing interestingly different from the situ-
ation of English.
The type of treebank annotations also affects
the performance of the parsing models. Tak-
ing the Penn Chinese Treebank (PCTB; Xue
et al (2005)) and Tsinghua Chinese Treebank
(TCT; Zhou (2004)) as examples, PCTB is anno-
tated with a much more detailed set of phrase cat-
egories, while TCT uses a more fine-grained POS
tagset. The asymmetry in the annotation informa-
tion is partially due to the difference of linguis-
tic treatment. But more importantly, it shows that
both treebanks have the potential of being refined
with more detailed classification, on either phrasal
or word categories. One data-driven approach to
derive more fine-grained annotation is the hierar-
chically split-merge parsing (Petrov et al, 2006;
Petrov and Klein, 2007), which induces subcat-
egories from coarse-grained annotations through
an expectation maximization procedure. In com-
bination with the coarse-to-fine parsing strategy,
efficient inference can be done with a cascade
of grammars of different granularity. Such pars-
ing models have reached (close to) state-of-the-art
performance for many languages including Chi-
nese and English.
Another effective technique to improve parsing
results is discriminative reranking (Charniak and
Johnson, 2005; Collins and Koo, 2005). While
the generative models compose candidate parse
trees, a discriminative reranker reorders the list
of candidates in favor of those trees which max-
imizes the properties of being a good analysis.
Such extra model refines the original scores as-
signed by the generative model by focusing its de-
cisions on the fine details among already ?good?
candidates. Due to this nature, the set of features
in the reranker focus on those global (and poten-
tially long distance) properties which are difficult
to model with the generative model. Also, since
it is not necessary for the reranker to generate the
candidate trees, one can easily integrate additional
external information to help adjust the ranking of
the analysis. In the following section, we will de-
Berkeley 
Parser
...
Parse 
Reranker
TCT
Head
Classifier
...
H
H
H
A B
C D
C
D B
A
C
Task 2.1
Task 2.2
Open
e.g. ?? ??? ? ??
PCTB
Parser
Figure 1: Workflow of the System
scribe the reranking model we developed for the
CIPS-SIGHAN-2010 parsing tasks. We will also
show how the heterogeneous parsing results can
be integrated through the reranker to further im-
prove the performance of the system.
3 System Description
In this section, we will present our approach
in detail. The whole system consists of three
main components, the Berkeley Parser, the Parse
Reranker, and the Head Classifier. The workflow
is shown in Figure 1. Firstly, we use the Berke-
ley Parser trained on the TCT to parse the in-
put sentence and obtain a list of possible parses;
then, all the parses2 will be re-ranked by the Parse
Reranker; and finally, the Head Classifer will an-
notate the head information for each constituent
2In practice, we only take the top n parses. We have dif-
ferent n values in the experiment settings, and n is up to 50.
Algorithm 1: The Perptron learning proce-
dure.
input : Data {(xt, yt), t = 1, 2, ...,m}
Initialize: w? (0, ..., 0)1
for i = 1, 2, ..., I do2
for t =SHUFFLE (1, ...,m) do3
y?t =4
arg maxy?GENbestn (xt) w
>?(xt, y)
if y?t 6= yt then5
w? w+(?(xt, yt)??(xt, y?t ))6
end7
end8
wi ? w9
end10
return aw = 1I
?I
i=1 wi11
on the best parse tree. For parse reranking, we
can extract features either from TCT-style parses
or together with the PCTB-style parse of the same
sentence. For example, we can check whether
the boundary predictions given by the TCT parser
are agreed by the PCTB parser. Since the PCTB
parser is trained on a different treebank from TCT,
our reranking model can be seen as a method to
use a heterogenous resource. The best parse tree
given by the Parse Reranker will be the result for
Task 2.2; and the final output of the system will
be the result for Task 2.1. Since we have already
mentioned the Berkeley Parser in the related work,
we will focus on the other two modules in the rest
of this section.
3.1 Parse Reranker
We follow Collins and Koo (2005)?s discrimina-
tive reranking model to score possible parse trees
of each sentence given by the Berkeley Parser.
Previous research on English shows that struc-
tured perceptron (Collins, 2002) is one of the
strongest machine learning algorithms for parse
reranking (Collins and Duffy, 2002; Gao et al,
2007). In our system, we use the averaged per-
ceptron algorithm to do parameter estimation. Al-
gorithm 1 illustrates the learning procedure. The
parameter vector w is initialized to (0, ..., 0). The
learner processes all the instances (t is from 1 to
n) in each iteration (i). If current hypothesis (w)
fails to predict xt, the learner update w through
calculating the difference between ?(xt, y?t ) and
?(xt, yt). At the end of each iteration, the learner
save the current model as w + i, and finally all
these models will be added up to get aw.
3.2 Features
We use an example to show the features we extract
in Figure 2.
vp
v
?
eat
np
v
?
buy
uJDE
?
n
??
apple
Figure 2: An Example
Rules The context-free rule itself:
np? v + uJDE + np.
Grandparent Rules Same as the Rules, but
also including the nonterminal above the rule:
vp(np? v + uJDE + np)
Bigrams Pairs of nonterminals from the left to
right of the the rule. The example rule would con-
tribute the bigrams np(STOP, v), np(v,uJDE),
np(uJDE,np) and np(np, STOP).
Grandparent Bigrams Same as Bigrams, but
also including the nonterminal above the bigrams.
For instance, vp(np(STOP, v))
Lexical Bigrams Same as Bigrams, but with
the lexical heads of the two nonterminals also in-
cluded. For instance, np(STOP,?).
Trigrams All trigrams within the rule. The
example rule would contribute the trigrams
np(STOP, STOP, v), np(STOP, v,uJDE),
np(v,uJDE,np), np(uJDE,np,STOP) and
np(np,STOP,STOP).
Combination of Boundary Words and
Rules The first word and the rule (i.e.
?+(np? v + uJDE + np)), the last word
and the rule one word before and the rule, one
word after and the rule, the first word, the last
word and the rule, and the first word?s POS, last
word?s POS and the rule.
Combination of Boundary Words and Phrasal
Category : Same as combination of boundary
words and rules, but substitute the rule with the
category of current phrases.
Two level Rules Same as Rules, but also
including the entire rule above the rule:
vp? v + (np? v + uJDE + np)
Original Rank : The logarithm of the original
rank of n-best candidates.
Affixation features In order to better handle
unknown words, we also extract morphologi-
cal features: character n-gram prefixes and suf-
fixes for n up to 3. For example, for word/tag
pair ????/n, we add the following fea-
tures: (prefix1,?,n), (prefix2,??,n), (prefix3,?
??,n), (suffix1,?,n), (suffix2,??,n), (suf-
fix3,???,n).
Apart from training the reranking model using
the same dataset (i.e. the TCT), we can also use
another treebank (e.g. the PCTB). Although they
have quite different annotations as well as the data
source, it would still be interesting to see whether
a heterogenous resource is helpful with the parse
reranking.
Consist Category If a phrase is also analyzed
as one phrase by the PCTB parser, both the TCT
and PCTB categories are used as two individual
features. The combination of the two categories
are also used.
Inconsist Category If a phrase is not analyzed
as one phrase by the PCTB parser, the TCT cate-
gory is used as a feature.
Number of Consist and Inconsist phrases The
two number are used as two individual featuers.
We also use the ratio of the number of consist
phrases and inconsist phrase (we add 0.1 to each
number for smoothing), the ratio of the number
of consist/inconsist phrases and the length of the
current sentence.
POS Tags For each word, the combination of
TCT and PCTB POS tags (with or without word
content) are used.
3.3 Head Classifier
Following (Song and Kit, 2009), we apply a se-
quence tagging method to find head constituents.
We suggest readers to refer to the original paper
for details of the method. However, since the fea-
ture set is different, we give the discription of
them in this paper. To predict whether current
phrase is a head phrase of its parent, we use the
same example above (Figure 2) for convenience.
If we consider np as our current phrase, the fol-
lowing features are extracted,
Rules The generative rule, vp? v + (np).
Category of the Current Phrase and its Parent
np, vp, and (np, vp).
Bigrams and Trigrams (v, np), (np,STOP),
(STOP, v,np), and (np,STOP,STOP).
Parent Bigrams and Trigrams vp(v, np),
vp(np,STOP), vp(STOP, v, np),
vp(np,STOP,STOP).
Lexical Unigram The first word ?, the last
word ??, and together with the parent, (vp,?)
and (vp,??)
4 Evaluation
4.1 Datasets
The dataset used in the CIPS-ParsEval-2010 eval-
uation is converted from the Tsinghua Chinese
Treebank (TCT). There are two subtasks: (1)
event description sub-sentence analysis and (2)
complete sentence parsing. On the assumption
that the boundaries and relations between these
event description units are determined separately,
the first task aims to identify the local fine-grained
syntactic structures. The goal of the second task
is to evaluate the performance of the automatic
parsers on complete sentences in real texts. The
training dataset is a mixture of several genres, in-
cluding newspaper texts, encyclopedic texts and
novel texts.
The annotation in the dataset is different to
the other frequently used Chinese treebank (i.e.
PCTB) Whereas TCT annotation strongly reflects
early descriptive linguistics, PCTB draws primar-
ily on Government-Binding (GB) theory from
1980s. PCTB annotation differs from TCT anno-
tation from many perspectives:
? TCT and PCTB have different segmentation
standards.
? TCT is somehow branching-rich annota-
tion, while PCTB annotation is category-
rich. Specifically the topological tree struc-
tures is more detailed in TCT, and there
are not many flat structures. However con-
stituents are detailed classified, namely the
number of phrasal categories is small. On the
contrary, though flat structures are very com-
mon in PCTB, the categorization of phrases
is fine-grained. In addition, PCTB contains
functional information. Function tags ap-
pended to constituent labels are used to in-
dicate additional syntactic or semantic infor-
mation.
? TCT contains head indices, making head
identification of each constituent an impor-
tant goal of task 1.
? Following the GB theory, PCTB assume
there are movements, so there are empty cat-
egory annotation. Because of different theo-
retical foundations, there are different expla-
nations for a series of linguistic phenomena
such as the usage of function word ???.
In the reranking experiments, we also use a
parser trained on PCTB to provide more syntac-
tic clues.
4.2 Setting
In order to gain a representative set of training
data, we use cross-validation scheme described in
(Collins, 2000). The dataset is a mixture of three
genres. We equally split every genre data into 10
subsets, and collect three subset of different gen-
res as one fold of the whole data. In this way, we
can divide the whole data into 10 balanced sub-
sets. For each fold data, a complement parser is
trained using all other data to produce multiple hy-
potheses for each sentence. This cross-validation
n 1 2 5 10 20 30 40 50
F1 79.97 81.62 83.51 84.63 85.59 86.07 86.38 86.60
Table 1: Upper bound of f-score as a function of number n of n-best parses.
scheme can prevent the initial model from being
unrealistically ?good? on the training sentences.
We use the first 9 folds as training data and the last
fold as development data for the following exper-
iments. For the final submission of the evaluation
task, we re-train a reranking model using all 10
folds data. All reranking models are trained with
30 iterations.
For parsing experiments, we use the Berkeley
parser3. All parsers are trained with 5 iterations
of split, merge, smooth. To produce PCTB-style
analysis, we train the Berkeley parse with PCTB
5.0 data that contains 18804 sentences and 508764
words. For the evaluation of development experi-
ments, we used the EVALB tool4 for evaluation,
and used labeled recall (LR), labeled precision
(LP) and F1 score (which is the harmonic mean
of LR and LP) to measure accuracy.
For the head classification, we use SVMhmm5,
an implementation of structural SVMs for se-
quence tagging. The main setting of learning pa-
rameter is C that trades off margin size and train-
ing error. In our experiments, the head classifica-
tion is not sensitive to this parameter and we set
it to 1 for all experiments reported. For the kernel
function setting, we use the simplest linear kernel.
4.3 Results
4.3.1 Upper Bound of Reranking
The upper bound of n-best parse reranking is
shown in Table 1. From the 1-best result we see
that the base accuracy of the parser is 79.97. 2-
best and 10-best show promising oracle-rate im-
provements. After that things start to slow down,
and we achieve an oracle rate of 86.60 at 50-best.
4.3.2 Reranking Using Homogeneous Data
Table 2 summarizes the performance of the ba-
sic reranking model. It is evaluated on short sen-
3http://code.google.com/p/
berkeleyparser/
4http://nlp.cs.nyu.edu/evalb/
5http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
tences (less than 40 words) from the development
data of the task 2. When 40 reranking candidates
are used, the model gives a 0.76% absolute im-
provement over the basic Berkeley parser.
POS(%) LP(%) LR(%) F1
Baseline 93.59 85.60 85.36 85.48
n = 2 93.66 85.84 85.54 85.69
n = 5 93.62 86.04 85.73 85.88
n = 10 93.66 86.22 85.85 86.04
n = 20 93.70 86.19 85.87 86.03
n = 30 93.70 86.32 86.00 86.16
n = 40 93.76 86.40 86.09 86.24
n = 50 93.73 86.10 85.81 85.96
Table 2: Reranking performance with different
number of parse candidates on the sentences that
contain no more than 40 words in the development
data.
4.3.3 Reranking Using Heterogeneous Data
Table 3 summarizes the reranking performance
using PCTB data. It is also evaluated on short sen-
tences of the task 2. When 30 reranking candi-
dates are used, the model gives a 1.12% absolute
improvement over the Berkeley parser. Compar-
ison of Table 2 and 3 shows an improvement by
using heterogeneous data.
POS(%) LP(%) LR(%) F1
n = 2 93.70 85.98 85.67 85.82
n = 5 93.75 86.52 86.19 86.35
n = 10 93.77 86.64 86.29 86.47
n = 20 93.79 86.71 86.34 86.53
n = 30 93.80 86.72 86.48 86.60
n = 40 93.80 86.54 86.22 86.38
n = 50 93.89 86.73 86.41 86.57
Table 3: Reranking performance with different
number of parse candidates on the sentences that
contain no more than 40 words in the development
data.
Task 1 ?B+C?-P ?B+C?-R ?B+C?-F1 ?B+C+H?-P ?B+C+H?-R ?B+C+H?-F1 POS
Old data 82.37 83.05 82.71 79.99 80.65 80.32 81.87
Table 4: Final results of task 1.
Task 2 dj-P dj-R dj-F1 fj-P fj-R fj-F1 Avg. POS
Old data 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23
New data 79.60 79.13 79.36 70.01 75.94 72.85 76.11 89.05
Table 5: Final results of task 2.
4.3.4 Head Classification
The head classification performance is evalu-
ated using gold-standard syntactic trees. For each
constituent in a gold parse tree, a structured clas-
sifier is trained to predict whether it is a head con-
stituent of its parent. Table 6 shows the overall
performance of head classification. We can see
that the head classification can achieve a high per-
formance.
P(%) R(%) F?=1
98.59% 98.20% 98.39
Table 6: Head classification performance with
gold trees on the development data.
4.3.5 Final Result
Table 4 and 5 summarize the final results. Here
we use the reranking model with heterogeneous
data. The second line of Table 5 shows the offi-
cal final results. In this submission, we trained a
model using an old version of training data. Note
that, the standard of POS tags of the ?old? version
is different from the latest version which is also
used as test data. For example, the name of some
tags are changed. The third line of Table 46 shows
the results predicted by the newest data7. This re-
sult is comparable to other systems.
5 Conclusion
In this paper, we described our participation of
the CIPS-SIGHAN-2010 parsing task. The gen-
6There are two sentences that are not parsed by the Berke-
ley parser. We use a simple strategy to solve this problem:
We first roughly segment the sentence according to punctu-
ation; Then the parsed sub-sentences are merged as a single
zj.
7We would like to thank the organizer to re-test our new
submission.
erative coarse-to-fine parsing model is integrated
with a discriminative parse reranking model, as
well as a head classifier based on sequence la-
beling. We use the perceptron algorithm to train
the reranking models and experiment with both
homogenous and heterogenous data. The results
show improvements over the baseline in both
cases.
Acknowledgments
The first author is supported by the German Aca-
demic Exchange Service (DAAD). The second
author is supported by the PIRE scholarship pro-
gram; the third author thanks DFKI and the Clus-
ter of Excellence on Multimodal Computing and
Interaction for their support of the work.
References
Charniak, E. and M Johnson. 2005. oarse-to-fine n-
best parsing and maxent discriminative reranking.
In Proceedings of ACL, pages 173?180.
Collins, Michael and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In
Proceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Collins, Michael and Terry Koo. 2005. Discriminative
reranking for natural language parsing. In Compu-
tational Linguistics, volume 31(1), pages 25?69.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Computational Lin-
guistics, pages 175?182. Morgan Kaufmann.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, pages
1?8, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Gao, Jianfeng, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Petrov, S. and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL-2007, Rochester, NY, USA, April.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Song, Yan and Chunyu Kit. 2009. Pcfg parsing with
crf tagging for head recognition. In Proceedings of
the CIPS-ParsEval-2009.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Zhou, Qiang. 2004. Annotation scheme for chinese
treebank (in chinese). Journal of Chinese Informa-
tion Processing, 18(4):1?8.
Paraphrase Fragment Extraction from Monolingual Comparable Corpora
Rui Wang
Language Technology Lab
DFKI GmbH
Stuhlsatzenhausweg 3 / Building D3 2
Saarbruecken, 66123 Germany
rwang@coli.uni-sb.de
Chris Callison-Burch
Computer Science Department
Johns Hopkins University
3400 N. Charles Street (CSEB 226-B)
Baltimore, MD 21218, USA
ccb@cs.jhu.edu
Abstract
We present a novel paraphrase fragment pair
extraction method that uses a monolingual
comparable corpus containing different arti-
cles about the same topics or events. The pro-
cedure consists of document pair extraction,
sentence pair extraction, and fragment pair ex-
traction. At each stage, we evaluate the in-
termediate results manually, and tune the later
stages accordingly. With this minimally su-
pervised approach, we achieve 62% of accu-
racy on the paraphrase fragment pairs we col-
lected and 67% extracted from the MSR cor-
pus. The results look promising, given the
minimal supervision of the approach, which
can be further scaled up.
1 Introduction
Paraphrase is an important linguistic phenomenon
which occurs widely in human languages. Since
paraphrases capture the variations of linguistic ex-
pressions while preserving the meaning, they are
very useful in many applications, such as machine
translation (Marton et al, 2009), document summa-
rization (Barzilay et al, 1999), and recognizing tex-
tual entailment (RTE) (Dagan et al, 2005).
However, such resources are not trivial to ob-
tain. If we make a comparison between para-
phrase and MT, the latter has large parallel bilin-
gual/multilingual corpora to acquire translation pairs
in different granularity; while it is difficult to find a
?naturally? occurred paraphrase ?parallel? corpora.
Furthermore, in MT, certain words can be translated
into a (rather) small set of candidate words in the
target language; while in principle, each paraphrase
can have infinite number of ?target? expressions,
which reflects the variety of each human language.
A variety of paraphrase extraction approaches
have been proposed recently, and they require dif-
ferent types of training data. Some require bilingual
parallel corpora (Callison-Burch, 2008; Zhao et al,
2008), others require monolingual parallel corpora
(Barzilay and McKeown, 2001; Ibrahim et al, 2003)
or monolingual comparable corpora (Dolan et al,
2004).
In this paper, we focus on extracting paraphrase
fragments from monolingual corpora, because this is
the most abundant source of data. Additionally, this
would potentially allow us to extract paraphrases for
a variety of languages that have monolingual cor-
pora, but which do not have easily accessible paral-
lel corpora.
This paper makes the following contributions:
1. We adapt a translation fragment pair extrac-
tion method to paraphrase extraction, i.e., from
bilingual corpora to monolingual corpora.
2. We construct a large collection of para-
phrase fragments from monolingual compara-
ble corpora and achieve similar quality from a
manually-checked paraphrase corpus.
3. We evaluate both intermediate and final results
of the paraphrase collection, using the crowd-
sourcing technique, which is effective, fast, and
cheap.
52
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 52?60,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Corpora Sentence level Sub-sentential level
Paraphrase acquisition
Monolingual Parallel e.g., Barzilay and McKeown (2001) This paperComparable e.g., Quirk et al (2004) e.g., Shinyama et al (2002) & This paper
Bilingual Parallel N/A e.g., Bannard and Callison-Burch (2005)
Statistical machine translation
Bilingual Parallel Most SMT systems SMT phrase tablesComparable e.g., Fung and Lo (1998) e.g., Munteanu and Marcu (2006)
Table 1: Previous work in paraphrase acquisition and machine translation.
2 Related Work
Roughly speaking, there are three dimensions to
characterize the previous work in paraphrase ac-
quisition and machine translation, whether the
data comes from monolingual or bilingual corpora,
whether the corpora are parallel or comparable, and
whether the output is at the sentence level or at the
sub-sentential level. Table 1 gives one example in
each category.
Paraphrase acquisition is mostly done at the
sentence-level, e.g., (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Dolan et al, 2004), which is
not straightforward to be used as a resource for other
NLP applications. Quirk et al (2004) adopted the
MT approach to ?translate? one sentence into a para-
phrased one. As for the corpora, Barzilay and McK-
eown (2001) took different English translations of
the same novels (i.e., monolingual parallel corpora),
while the others experimented on multiple sources
of the same news/events, i.e., monolingual compa-
rable corpora.
At the sub-sentential level, interchangeable pat-
terns (Shinyama et al, 2002; Shinyama and Sekine,
2003) or inference rules (Lin and Pantel, 2001)
are extracted, which are quite successful in named-
entity-centered tasks, like information extraction,
while they are not generalized enough to be applied
to other tasks or they have a rather small coverage,
e.g. RTE (Dinu and Wang, 2009). To our best
knowledge, there is few focused study on general
paraphrase fragments extraction at the sub-sentential
level, from comparable corpora. A recent study
by Belz and Kow (2010) mainly aimed at natural
language generation, which they performed a small
scale experiment on a specific topic, i.e., British
hills.
Given the available parallel corpora from the MT
community, there are studies focusing on extracting
paraphrases from bilingual corpora (Bannard and
Callison-Burch, 2005; Callison-Burch, 2008; Zhao
et al, 2008). The way they do is to treat one lan-
guage as an pivot and equate two phrases in the other
languages as paraphrases if they share a common
pivot phrase. Paraphrase extraction draws on phrase
pair extraction from the translation literature. Since
parallel corpora have many alternative ways of ex-
pressing the same foreign language concept, large
quantities of paraphrase pairs can be extracted.
As for the MT research, the standard statistical
MT systems require large size of parallel corpora for
training and then extract sub-sentential translation
phrases. Apart from the limited parallel corpora,
comparable corpora are non-parallel bilingual cor-
pora whose documents convey the similar informa-
tion are also widely considered by many researchers,
e.g., (Fung and Lo, 1998; Koehn and Knight, 2000;
Vogel, 2003; Fung and Cheung, 2004a; Fung and
Cheung, 2004b; Munteanu and Marcu, 2005; Wu
and Fung, 2005). A recent study by Smith et al
(2010) extracted parallel sentences from comparable
corpora to extend the existing resources.
At the sub-sentential level, Munteanu and
Marcu (2006) extracted sub-sentential translation
pairs from comparable corpora based on the log-
likelihood-ratio of word translation probability.
They exploit the possibility of making use of reports
within a limited time window, which are about the
same event or having overlapping contents, but in
different languages. Quirk et al (2007) extracted
fragments using a generative model of noisy transla-
tions. They show that even in non-parallel corpora,
useful parallel words or phrases can still be found
and the size of such data is much larger than that of
53
Document Pair 
Extraction
Sentence Pair 
Extraction
Fragment Pair 
Extraction
Corpora
(Gigaword)
<doc> .
.. in 
1995 ... 
</doc>
<doc> .
.. Jan., 
1995 ... 
</doc>
Comparability
<sent> 
NATO ... 
in 1995 ... 
</sent>
<sent> In 
1995, 
NATO ... 
</sent>
N-Gram 
Overlapping
<frag> the 
finance chief 
</frag>
<frag> the 
chief financial 
officer </frag>
Interchangeability
Paraphrased 
Fragments
Paraphrase 
Collection
(MSR)
Paraphrase 
Collecton 
(CCB)
Figure 1: A three stage pipeline is used to extract paraphrases from monolingual texts
parallel corpora. In this paper, we adapt ideas from
the MT research on extracting sub-sentential trans-
lation fragments from bilingual comparable corpora
(Munteanu and Marcu, 2006), and use the tech-
niques to extract paraphrases from monolingual par-
allel and comparable corpora.
Evaluation is another challenge for resource col-
lection, which usually requires tremendous labor re-
sources. Both Munteanu and Marcu (2006) and
Quirk et al (2007) evaluated their resources indi-
rectly in MT systems, while in this paper, we make
use of the crowd-sourcing technique to manually
evaluate the quality of the paraphrase collection.
In parcitular, Amazon?s Mechanical Turk1 (MTurk)
provides a way to pay people small amounts of
money to perform tasks that are simple for humans
but difficult for computers. Examples of these Hu-
man Intelligence Tasks (or HITs) range from label-
ing images to moderating blog comments to pro-
viding feedback on relevance of results for a search
query. Using MTurk for NLP task evaluation has
been shown to be significantly cheaper and faster,
and there is a high agreement between aggregate
non-expert annotations and gold-standard annota-
tions provided by the experts (Snow et al, 2008).
1http://www.mturk.com/
3 Fragment Pair Acquisition
Figure 1 shows the pipeline of our paraphrase ac-
quisition method. We evaluate quality at each stage
using Amazon?s Mechanical Turk. In order to en-
sure that the non-expert annotators complete the task
accurately, we used both positive and negative con-
trols. If annotators answered either control incor-
rectly, we excluded their answers. For all the ex-
periments we describe in this paper, we obtain the
answers within a couple of hours or an overnight.
Our focus in this paper is on fragment extraction,
but we briefly describe document and sentence pair
extraction first.
3.1 Document Pair Extraction
Monolingual comparable corpora contain texts
about the same events or subjects, written in one lan-
guage by different authors (Barzilay and Elhadad,
2003). We extract pairs of newswire articles written
by different news agencies from the GIGAWORD cor-
pus, which contains articles from six different agen-
cies. Although the comparable documents are not in
parallel, at the sentential or sub-sentential level, the
paraphrased fragments may still exist.
To quantify the comparability between two doc-
uments, we calculate the number of overlapping
words and give them different weights based on
TF-IDF (Salton and McGill, 1983) using the More-
54
LikeThis2 function provided by Lucene.
After collecting the document pairs, we asked an-
notators, ?Are these two documents about the same
topic??, and allowing them to answer ?Yes?, ?No?,
and ?Not sure?. Each set of six document pairs con-
tained, four to be evaluated, one positive control (a
pair of identical documents) and one negative con-
trol (a pair of random documents). We sampled
400 document pairs with the comparability score be-
tween 0.8 and 0.9, and another 400 pairs greater than
0.9. We presented them in a random order and each
was labeled by three annotations. After excluding
the annotations containing incorrect answers for ei-
ther control, we took a majority vote for every docu-
ment pair, and if three annotations are different from
each other.
We found document pairs with >0.9 were classi-
fied by annotators to be related more than half the
time, and a higher threshold would greatly decrease
the number of document pairs extracted. We per-
formed subsequent steps on the 3896 document pairs
that belonged to this category.
3.2 Sentence Pair Extraction
After extracting pairs of related documents, we
next selected pairs of related sentences from within
paired documents. The motivation behind is that
the standard word alignment algorithms can be eas-
ily applied to the paired sentences instead of docu-
ments. To do so we selected sentences with overlap-
ping n-grams up to length n=4. Obviously for para-
phrasing, we want some of the n-grams to differ, so
we varied the amount of overlap and evaluated sen-
tence pairs with a variety of threshold bands3.
We evaluated 10 pairs of sentences at a time, in-
cluding one positive control and two negative con-
trols. A random pair of sentential paraphrases from
the RTE task acted as the positive control. The
negative controls included one random pair of non-
paraphrased, but highly relevant sentences, and a
random pair of sentences. Annotators classified the
sentence pairs as: paraphrases, related sentences,
2http://lucene.apache.org/java/2_9_1/
api/contrib-queries/org/apache/lucene/
search/similar/MoreLikeThis.html
3In the experiment setting, the thresholds (maximum com-
parability and minimum comparability) for the 4 groups are,
{0.78,0.206}, {0.206,0.138}, {0.138,0.115}, {0.115,0.1}.
0%	 ?
10%	 ?
20%	 ?
30%	 ?
40%	 ?
50%	 ?
60%	 ?
70%	 ?
80%	 ?
90%	 ?
100%	 ?
0.1-??
0.10
8	 ?
0.10
8-??0.
115	 ?
0.11
5-??0.
125	 ?
0.12
5-??0.
138	 ?
0.13
8-??0.
16	 ?
0.16
-??0.2
06	 ?
0.20
6-??0.
78	 ?
invalid	 ?
not_related	 ?
related	 ?
paraphrases	 ?
Figure 2: Results of the sentence pair extraction. The x-
axis is the threshold for the comparability scores; and the
y-axis is the distribution of the annotations.
and non-related sentences.
We uniformly sampled 200 sentence pairs from
each band. They are randomly shuffled into more
than 100 HITs and each HIT got three annotations.
Figure 2 shows the distribution of annotations across
different groups, after excluding answers that failed
the controls.
Our best scoring threshold band was 0.2-0.8. Sen-
tence pairs with this overlap were judged to be para-
phrases 45% of the time, to be related 30% of the
time, and to be unrelated 25% of the time. Although
the F2 heuristic proposed by Dolan et al (2004),
which takes the first two sentences of each document
pair, obtains higher relatedness score (we evalu-
ated F2 sentences as 50% paraphrases, 37% related,
and 13% unrelated), our n-gram overlap method ex-
tracted much more sentence pairs per document pair.
One interesting observation other than the general
increasing tendency is that the portion of the related
sentence pairs is not monotonic, which exactly re-
flects our intuition about a good comparability value
(neither too high nor too low). However, some er-
rors are difficult to exclude. For instance, one sen-
tence says ?The airstrikes were halted for 72 hours
last Thursday...? and the other says ?NATO and UN
officials extended the suspension of airstrikes for a
further 72 hours from late Sunday...?. Without fine-
grained analysis of the temporal expressions, it is
difficult to know whether they are talking about the
same event. The F2 method does provide us a fairly
good way to exclude some unrelated sentence pairs,
but note that the pairs collected by this method are
55
Figure 3: An example of fragment pair extraction. Stop words are all set to 1 initially. Zero is the threshold, and the
underscored phrases are the outputs.
only about 0.5% of using the comparability scores.
We show in Figure 1 that we also use an addi-
tional sentence-level paraphrase corpus as the input
of this module. We take all the positive instances
(i.e. the two sentences in a pair are paraphrase to
each other) and pass them to the later stage as well,
as for comparison with our paraphrase collection ex-
tracted from the comparable sentence pairs. In all,
we used 276,120 sentence pairs to feed our fragment
extraction method.
3.3 Fragment Pair Extraction
The basic procedure is to 1) establish alignments be-
tween words or n-grams and 2) extract target para-
phrase fragments. For the first step, we use two ap-
proaches. One is to change the common substring
alignment problem from string to word sequence
and we extend the longest common substring (LCS)
extraction algorithm to multiple common n-grams.
An alternative way is to use a normal word aligner
(widely used as the first step in MT systems) to ac-
complish the job. For our experiments, we use the
BerkeleyAligner4 (Liang et al, 2006) by feeding it a
dictionary of pairs of identical words along with the
paired sentences. We can also combine these two
methods by performing the LCS alignment first and
adding additional word alignments from the aligner.
These form the three configurations of our system
(Table 2).
Following Munteanu and Marcu (2006), we use
both positive and negative lexical associations for
the alignment. The positive association measures
4http://code.google.com/p/
berkeleyaligner/
how likely one word will be aligned to another
(value from 0 to 1); and the negative associations
indicates how unlikely an alignment exists between
a word pair (from -1 to 0). The basic idea to have
both is that when a word cannot be aligned with any
other words, it will choose the least unlikely one. If
the positive association of w1 being aligned with w2
is defined as the conditional probability p(w1|w2),
the negative associations will simply be p(w1|?w2).
Since we obtain a distribution of all the possible
words aligned with w1 from the word aligner, both
p(w1|w2) and p(w1|?w2) can be calculated; for the
LCS alignment, we simply set p(w1|w2) as 1 and
p(w1|?w2) as -1, if w1 and w2 are aligned; and vice
versa, if not.
After the initialization of all the word alignments
using the two associations, each word takes the av-
erage of the neighboring four words and itself. The
intuition of this smoothing is to tolerate a few un-
aligned parts (if they are surrounded by aligned
parts). Finally, all the word alignments having a pos-
itive score will be selected as the candidate fragment
elements. Figure 3 shows an example of this pro-
cess.
The second step, fragment extraction, is a bit
tricky, since a fragment is not clearly defined like
a document or a sentence. One option is to fol-
low the MT definition of a phrase, which means a
sub-sentential n-gram string (usually n is less than
10). Munteanu and Marcu (2006) adopted this, and
considered all the possible sub-sentential translation
fragments as their targets, i.e. the adjacent n-grams.
For instance, in Figure 3, all the adjacent words
above the threshold (i.e. zero) will form the target
56
Configurations
Aligner+ LCS+ Word+ LCS+Word+
Phrase Extraction Chunk N-Gram Chunk
Our Corpus
PARAPHRASE 15% 36% 32%
RELATED 21% 26% 21%
SUM 36% 62% 53%
The MSR Corpus
PARAPHRASE 38% 44% 49%
RELATED 20% 19% 18%
SUM 58% 63% 67%
Table 2: Distribution of the Extracted Fragment Pairs of
our Corpus and MSR Corpus. We manually evaluated
1051 sentence pairs in all. We use LCS or word aligner as
the initialization and apply n-gram-based or chunk-based
phrase extraction. The first column serves as the baseline.
paraphrase, ?the Bosnian Serbs to pull their heavy
weapons back from? and those aligned words in the
other sentence ?the Bosnian Serbs withdraw their
heavy weapons from? will be the source paraphrase.
The disadvantage of this definition is that the ex-
tracted fragment pairs might not be easy for human
beings to interpret or they are even ungrammatical
(cf. the fourth example in Table 5). An alternative
way is to follow the linguistic definition of a phrase,
e.g. noun phrase (NP), verb phrase (VP), etc. In this
case, we need to use (at least) a chunker to prepro-
cess the text and obtain the proper boundary of each
fragment and we used the OpenNLP chunker.
We finalize our paraphrase collection by filter-
ing out identical fragment pairs, subsumed fragment
pairs (one fragment is fully contained in the other),
and fragment having only one word. Apart from sen-
tence pairs collected from the comparable corpora,
we also did experiments on the existing MSR para-
phrase corpus (Dolan and Brockett, 2005), which is
a collection of manually annotated sentential para-
phrases.
The evaluation on both collections is done by the
MTurk. Each task contains 8 pairs of fragments to
be evaluated, plus one positive control using iden-
tical fragment pairs, and one negative control using
a pair of random fragments. All the fragments are
shown with the corresponding sentences from where
they are extracted5. The question being asked is
5We thought about evaluating pairs of isolated fragments,
?How are the two highlighted phrases related??, and
the possible answers are, ?These phrases refer to the
same thing as each other? (PARAPHRASE), ?These
phrases are overlap but contain different informa-
tion? (RELATED), and ?The phrases are unrelated or
invalid? (INVALID). Table 2 shows the results (ex-
cluding invalid sentence pairs) and Table 5 shows
some examples.
In general, the results on MSR is better than those
on our corpus6. Comparing the different settings,
for our corpus, word alignment with n-gram frag-
ment extraction works better; and for corpora with
higher comparability (e.g. the MSR corpus), the
configuration of using both LCS and word align-
ments and the chunk-based fragment extraction out-
performs the others. In fact, PARAPHRASE and RE-
LATED are not quite comparable7, since the bound-
ary mismatch of the fragments may not be obvious
to the Turkers. Nevertheless, we would assume a
cleaner output from the chunk-based method, and
both approaches achieve similar levels of quality.
Zhao et al (2008) extracted paraphrase frag-
ment pairs from bilingual parallel corpora, and
their log-liner model outperforms Bannard and
Callison-Burch (2005)?s maximum likelihood esti-
mation method with 67% to 60%. Notice that, our
starting corpora are (noisy) comparable corpora in-
stead of parallel ones (for our corpus), and the ap-
proach is almost unsupervised8, so that it can be
easily scaled up to other larger corpora, e.g. the
news websites. Furthermore, we compared our frag-
ment pair collection with Callison-Burch (2008)?s
approach on the same MSR corpus, only about 21%
of the extracted paraphrases appear on both sides,
which shows the potential to combine different re-
sources.
4 Analysis of the Collections
In this section, we present some analysis on the frag-
ment pair collection. We show the basic statistics of
the corpora and then some examples of the output.
but later found out it was difficult to make the judgement.
6A sample of the corpus can be downloaded here:
http://www.coli.uni-saarland.de/?rwang/
resources/paraphrases.
7Thanks to the anonymous reviewer who pointed this out.
8The MTurk annotations can be roughly viewed as a guide
for parameter tuning instead of training the system
57
As for comparison, we choose two other paraphrase
collections, one is acquired from parallel bilingual
corpora (Callison-Burch, 2008) and the other is us-
ing the same fragment extraction algorithm on the
MSR corpus.
4.1 Statistics of the Corpora
Stage Collection Size %
GIGAWORD (1995) 600,000 10%
Documents Retrieved 150,000 2.5%
Document Pairs Selected 10,000 0.25%
Sentence Pairs Extracted 270,000 0.1%
Fragment Pairs Extracted 90,000 0.01%
Table 3: The size of our corpus. We only used ca. 10%
of the GIGAWORD corpus in the experiments and the size
of the collection at each stage are shown in the table.
Table 3 roughly shows the percentage of the ex-
tracted data compared with the original GIGAWORD
corpus at each stage9. In the experiments reported
here, we only use a subset of the news articles in
1995. If we scale to the full GIGAWORD corpus (19
Gigabytes, news from 1994 to 2006), we expect an
order of magnitude more fragment pairs to be col-
lected.
Apart from the size of the corpus, we are also in-
terested in the composition of the corpus. Table 4
shows the proportions of some n-grams contained in
the corpus. Here CCB denotes the paraphrase col-
lection acquired from parallel bilingual corpora re-
ported in (Callison-Burch, 2008), and MSR? denotes
the collection using the same algorithm on the MSR
corpus.
In Table 4, the four columns from the left are
about the fragments (one part of each fragment pair),
and the six columns from the right are about para-
phrases. For example, 1 & 2 indicates the paraphrase
contains one single word on one side and a 2-gram
on the other side. Since we deliberately exclude sin-
gle words, the n-gram distributions of OUR and MSR
are ?flatter? than the other two corpora, but still, 2-
grams fragments occupy more than 40% in all cases.
The n-gram distributions of the paraphrases are even
more diverse for the OUR and MSR corpora. The sum
9All the numbers in the table are roughly estimated, due to
the variations of different settings. This just gives us an impres-
sion of the space for improvement.
of the listed proportions are only around 45%, while
for CCB and MSR?, the sums are about 95%.
4.2 Examples
Table 5 shows some examples from the best two set-
tings. From our corpus, both simple paraphrases
(?Governor ... said? and ?Gov. ... announced?)
and more varied ones (?rose to fame as? and ?the
highlight of his career?) can be extracted. It?s clear
that the smoothing and extraction algorithms do help
with finding non-trivial paraphrases (shown in Fig-
ure 3). The extracted phrase ?campaign was? shows
the disadvantage of n-gram-based phrase extraction
method, since the boundary of the fragment could be
improper. Using a chunker can effectively exclude
such problems, as shown in the lower part of the ta-
ble, where all the extracted paraphrases are gram-
matical phrases. Even from a parallel paraphrase
corpus at the sentence level, the acquired fragment
pairs (w/o context) could be non-paraphrases. For
instance, the second pair from the MSR corpus shows
that one news agency gives more detailed informa-
tion about the launching site than the other, and the
last example is also debatable, whether it?s ?under
$200? or ?around $200? depending on the reliability
of the information source.
5 Summary and Future Work
In this paper, we present our work on paraphrase
fragment pair extraction from monolingual compa-
rable corpora, inspired by Munteanu and Marcu
(2006)?s bilingual method. We evaluate our inter-
mediate results at each of the stages using MTurk.
Both the quality and the quantity of the collected
paraphrase fragment pairs are promising given the
minimal supervision. As for the ongoing work, we
are currently expanding our extraction process to the
whole GIGAWORD corpus, and we plan to apply it
to other comparable corpora as well. For the fu-
ture work, we consider incorporating more linguistic
constraints, e.g. using a syntactic parser (Callison-
Burch, 2008), to further improve the quality of the
collection. More importantly, applying the collected
paraphrase fragment pairs to other NLP applications
(e.g. MT, RTE, etc.) will give us a better view of the
utility of this resource.
58
N-grams Phrases Para-phrases1 2 3 4 1 & 1 1 & 2 2 & 2 1 & 3 2 & 3 3 & 3
OUR N/A 43.4% 30.5% 16.4% N/A N/A 20.0% N/A 16.7% 8.8%
MSR N/A 41.7% 30.5% 16.0% N/A N/A 20.1% N/A 16.6% 9.4%
CCB 10.7% 42.7% 32.0% 10.9% 34.7% 16.3% 24.0% 2.5% 9.4% 6.9%
MSR? 8.1% 41.4% 37.2% 10.0% 29.0% 16.6% 26.8% 2.8% 10.7% 9.6%
Table 4: The (partial) distribution of N-grams (N=1-4) in different paraphrase collections.
From Our Corpus: using word aligner and n-gram-based phrase extraction
... unveiled a detailed peace plan calling for the Bosnian Serbs to pull their heavy weapons back from Sarajevo. ParaphraseIf the Bosnian Serbs withdraw their heavy weapons from Sarajevo?s outskirts, ...
In San Juan, Puerto Rico, Governor Pedro Rosello said the the storm could hit the US territory by Friday, ... ParaphraseIn Puerto Rico, Gov. Pedro Rossello announced that banks will be open only until 11 a.m. Friday and ...
Kunstler rose to fame as the lead attorney for the ?Chicago Seven,? ... ParaphraseThe highlight of his career came when he defended the Chicago Seven ...
... initiated the air attacks in response to Serb shelling of Sarajevo that killed 38 people Monday. InvalidThe campaign was to respond to a shelling of Sarajevo Monday that killed 38 people.
From MSR Corpus: using both LCS and word aligner and chunk-based phrase extraction
O?Brien?s attorney, Jordan Green, declined to comment. ParaphraseJordan Green, the prelate?s private lawyer, said he had no comment.
Iraq?s nuclear program had been dismantled, and there ?was no convincing evidence of its reconstitution.? ParaphraseIraq?s nuclear program had been dismantled and there was no convincing evidence it was being revived, ...
... to blast off between next Wednesday and Friday from a launching site in the Gobi Desert. Related... to blast off as early as tomorrow or as late as Friday from the Jiuquan launching site in the Gobi Desert.
... Super Wireless Media Router, which will be available in the first quarter of 2004, at under $200. RelatedThe router will be available in the first quarter of 2004 and will cost around $200, the company said.
Table 5: Some examples of the extracted paraphrase fragment pairs.
Acknowledgments
The first author would like to thank the EuroMatrix-
Plus project (IST-231720) which is funded by the
European Commission under the Seventh Frame-
work Programme. The second author is supported
by the EuroMatrixPlusProject, by the DARPA
GALE program under Contract No. HR0011-06-
2-0001, and by the NSF under grant IIS-0713448.
The authors would like to thank Mirella Lapata and
Delip Rao for the useful discussions as well as the
anonymous Turkers who helped us to accomplish
the tasks.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
R. Barzilay and N. Elhadad. 2003. Sentence alignment
for monolingual comparable corpora. In Proceedings
of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of
ACL, College Park, MD.
Anja Belz and Eric Kow. 2010. Extracting parallel frag-
ments from comparable corpora for data-to-text gen-
eration. In Proceedings of the 6th International Nat-
ural Language Generation Conference, Stroudsburg,
PA, USA.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. In Pro-
ceedings of the RTE Workshop.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entailment.
59
In Proceedings of EACL, Athens, Greece. Association
of Computational Linguistics.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the IWP2005.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING.
Pascale Fung and Percy Cheung. 2004a. Mining very
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
EMNLP.
Pascale Fung and Percy Cheung. 2004b. Multi-level
bootstrapping for extracting parallel sentences from a
quasi-comparable corpus. In Proceedings of COLING.
P. Fung and Y. Y. Lo. 1998. An IR approach for translat-
ing new words from nonparallel, comparable texts. In
Proceedings of ACL.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
the National Conference on Artificial Intelligence.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of the ACM
SIGKDD.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
EMNLP, Singapore.
Dragos Munteanu and Daniel Marcu. 2005. Improving
machine translation performance by exploiting compa-
rable corpora. Computational Linguistics, 31(4), De-
cember.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of ACL.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of the 2004 Conference on
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, Barcelona, Spain.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
G. Salton and M. J. McGill. 1983. Introduction to
modern information retrieval. ISBN 0-07-054484-0.
McGraw-Hill.
Y. Shinyama and S. Sekine. 2003. Paraphrase acquisition
for information extraction. In Proceedings of Interna-
tional Workshop on Paraphrasing.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles yusuke shinyama satoshi sekine automatic para-
phrase acquisition from news articles. In Proceed-
ings of Human Language Technology Conference, San
Diego, USA.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Stroudsburg, PA,
USA. Association of Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
Evaluating non- expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP.
Stephan Vogel. 2003. Using noisy bilingual data for sta-
tistical machine translation. In Proceedings of EACL.
Dekai Wu and Pascale Fung. 2005. Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora. In Proceedings of
IJCNLP, Jeju Island, Korea.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of ACL.
60
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 119?128,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Linguistically-Augmented Bulgarian-to-English Statistical Machine
Translation Model
Rui Wang
Language Technology Lab
DFKI GmbH
Saarbru?cken, Germany
ruiwang@dfki.de
Petya Osenova and Kiril Simov
Linguistic Modelling Department, IICT
Bulgarian Academy of Sciences
Sofia, Bulgaria
{petya,kivs}@bultreebank.org
Abstract
In this paper, we present our linguistically-
augmented statistical machine translation
model from Bulgarian to English, which
combines a statistical machine translation
(SMT) system (as backbone) with deep lin-
guistic features (as factors). The motiva-
tion is to take advantages of the robust-
ness of the SMT system and the linguis-
tic knowledge of morphological analysis
and the hand-crafted grammar through sys-
tem combination approach. The prelimi-
nary evaluation has shown very promising
results in terms of BLEU scores (38.85) and
the manual analysis also confirms the high
quality of the translation the system deliv-
ers.
1 Introduction
In the recent years, machine translation (MT)
has achieved significant improvement in terms
of translation quality (Koehn, 2010). Both
data-driven approaches (e.g., statistical MT
(SMT)) and knowledge-based (e.g., rule-based
MT (RBMT)) have achieved comparable results
shown in the evaluation campaigns (Callison-
Burch et al, 2011). However, according to the
human evaluation, the final outputs of the MT sys-
tems are still far from satisfactory.
Fortunately, recent error analysis shows that the
two trends of the MT approaches tend to be com-
plementary to each other, in terms of the types
of the errors they made (Thurmair, 2005; Chen et
al., 2009). Roughly speaking, RBMT systems of-
ten have missing lexicon and thus lack of robust-
ness, while handling linguistic phenomena requir-
ing syntactic information better. SMT systems, on
the contrary, are in general more robust, but some-
times output ungrammatical sentences.
In fact, instead of competing with each other,
there is also a line of research trying to com-
bine the advantages of the two sides using a
hybrid framework. Although many systems
can be put under the umbrella of ?hybrid? sys-
tems, there are various ways to do the combi-
nation/integration. Thurmair (2009) summarized
several different architectures of hybrid systems
using SMT and RBMT systems. Some widely
used ones are: 1) using an SMT to post-edit the
outputs of an RBMT; 2) selecting the best trans-
lations from several hypotheses coming from dif-
ferent SMT/RBMT systems; and 3) selecting the
best segments (phrases or words) from different
hypotheses.
For the language pair Bulgarian-English, there
has not been much study on it, mainly due to the
lack of resources, including corpora, preproces-
sors, etc. There was a system published by Koehn
et al (2009), which was trained and tested on the
European Union law data, but not on other do-
mains like news. They reported a very high BLEU
score (Papineni et al, 2002) on the Bulgarian-
English translation direction (61.3), which in-
spired us to further investigate this direction.
In this paper, we focus on the Bulgarian-to-
English translation and mainly explore the ap-
proach of annotating the SMT baseline with lin-
guistic features derived from the preprocessing
and hand-crafted grammars. There are three mo-
tivations behind our approach: 1) the SMT base-
line trained on a decent amount of parallel cor-
pora outputs surprisingly good results, in terms of
both statistical evaluation metrics and preliminary
manual evaluation; 2) the augmented model gives
119
us more space for experimenting with different
linguistic features without losing the ?basic? ro-
bustness; 3) the MT system can profit from con-
tinued advances in the development of the deep
grammars thereby opening up further integration
possibilities.
The rest of the paper will be organized as fol-
lows: Section 2 presents our work on cleaning
the corpora and Section 3 briefly describes the
preprocessing of the data. Section 4 introduces
our factor-based SMT model which allows us
to incorporate various linguistic features into an
SMT baseline, among which those features com-
ing from the MRS are described in Section 5 in
detail. We show our experiments in Section 6 as
well as both automatic and manual evaluation of
the results. Section 7 briefly mentions some re-
lated work and then we summarize this paper in
Section 8.
2 Data Preparation
In our experiments we are using the SETIMES
parallel corpus, which is part of the OPUS parallel
corpus1. The data in the corpus was aligned auto-
matically. Thus, we first checked the consistency
of the automatic alignments. It turned out that
more than 25% of the sentence alignments were
not correct. Since SETIMES appeared to be a
noisy dataset, our effort was directed into cleaning
it as much as possible before the start of the ex-
periments. We first corrected manually more than
25.000 sentence alignments. The the rest of the
data set includes around 135,000 sentences. Al-
together the data set is about 160,000 sentences,
when the manually checked part is added. Thus,
two actions were taken:
1. Improving the tokenization of the Bulgar-
ian part. The observations from the man-
ual check of the set of 25,000 sentences
showed systematic errors in the tokenized
text. Hence, these cases have been detected
and fixed semi-automatically.
2. Correcting and removing the suspicious
alignments. Initially, the ratio of the lengths
of the English and Bulgarian sentences was
calculated in the set of the 25,000 manually
annotated sentences. As a rule, the Bulgarian
1OPUS?an open source parallel corpus,
http://opus.lingfil.uu.se/
sentences are longer than the English ones.
The ratio is 1.34. Then we calculated the ra-
tio for each pair of sentences. After this, the
optimal interval was manually determined,
such that if the ratio for a given pair of sen-
tences is within the interval, then we assume
that the pair is a good one. The interval for
these experiments is set to [0.7; 1.8]. All the
pairs with ratio outside of the interval have
been deleted. Similarly, we have cleaned
EMEA dataset.
The size of the resulting datasets are: 151,718
sentence pairs for the SETIMES dataset. Simi-
lar approach was undertaken for another dataset
from OPUS corpus - EMEA. After the cleaning
704,631 sentence pairs were selected from the
EMEA dataset. Thus, the size of the original
datasets was decreased by 10%.
3 Linguistic Preprocessing
The data in SETIMES dataset was analysed on the
following levels:
? POS tagging. POS tagging is performed by
a pipe of several modules. First we apply
SVM POS tagger which takes as an input
a tokenised text and its output is a tagged
text. The performance is near 91% accuracy.
The SVM POS tagger is implemented us-
ing SVMTool (Gimnez and Mrquez, 2004).
Then we apply a morphological lexicon and
a set of rules. The lexicon add all the pos-
sible tags for the known words. The rules
reduce the ambiguity for some of the sure
cases. The result of this step is a tagged text
with some ambiguities unresolved. The third
step is application of the GTagger (Georgiev
et al, 2012). It is trained on an ambigu-
ous data and select the most appropriate tags
from the suggested ones. The accuracy of the
whole pipeline is 97.83%. In this pipeline
SVM POS Tagger plays the role of guesser
for the GTagger.
? Lemmatization. The lemmatization mod-
ule is based on the same morphological lexi-
con. From the lexicon we extracted functions
which convert each wordform into its basic
form (as a representative of the lemma). The
functions are defined via two operations on
120
wordforms: remove and concatenate. The
rules have the following form:
if tag = Tag then {remove OldEnd; concatenate
NewEnd}
where Tag is the tag of the wordform, Old-
End is the string which has to be removed
from the end of the wordform and NewEnd
is the string which has to be concatenated to
the beginning of the word form in order to
produce the lemma. The rules are for word
forms in the lexicon. Less than 2% of the
wordforms are ambiguous in the lexicon (but
they are very rare in real texts). Similar rules
are defined for unknown words. The accu-
racy of the lemmatizer is 95.23%.
? Dependency parsing. We have trained the
MALT Parser on the dependency version of
BulTreeBank2. We did this work together
with Svetoslav Marinov who has experience
in using the MALT Parser and Johan Hall
who is involved in thedevelopment of Malt
Parser. The trained model achieves 85.6%
labeled parsing accuracy. It is integrated in
a language pipe with the POS tagger and the
lemmatizer.
After the application of the language pipeline,
the result is represented in a table form following
the CoNLL shared task format3.
4 Factor-based SMT Model
Our approach is built on top of the factor-based
SMT model proposed by Koehn and Hoang
(2007), as an extension of the traditional phrase-
based SMT framework. Instead of using only the
word form of the text, it allows the system to take
a vector of factors to represent each token, both
for the source and target languages. The vec-
tor of factors can be used for different levels of
linguistic annotations, like lemma, part-of-speech
(POS), or other linguistic features. Furthermore,
this extension actually allows us to incorporate
various kinds of features if they can be (somehow)
represented as annotations to the tokens.
The process is quite similar to supertagging
(Bangalore and Joshi, 1999), which assigns ?rich
descriptions (supertags) that impose complex
2http://www.bultreebank.org/dpbtb/
3http://ufal.mff.cuni.cz/conll2009-st/
task-description.html
constraints in a local context?. In our case, all
the linguistic features (factors) associated with
each token form a supertag to that token. Singh
and Bandyopadhyay (2010) had a similar idea
of incorporating linguistic features, while they
worked on Manipuri-English bidirectional trans-
lation. Our approach is slightly different from
(Birch et al, 2007) and (Hassan et al, 2007), who
mainly used the supertags on the target language
side, English. We primarily experiment with the
source language side, Bulgarian. This potentially
huge feature space provides us with various possi-
bilities of using our linguistic resources developed
in and out of our project.
In particular, we consider the following factors
on the source language side (Bulgarian):
? WF - word form is just the original text to-
ken.
? LEMMA is the lexical invariant of the orig-
inal word form. We use the lemmatizer
described in Section 3, which operates on
the output from the POS tagging. Thus,
the 3rd person, plural, imperfect tense verb
form ?varvyaha? (?walking-were?, They were
walking) is lemmatized as the 1st person,
present tense verb ?varvya?.
? POS - part-of-speech of the word. We use
the positional POS tag set of the BulTree-
Bank, where the first letter of the tag indi-
cates the POS itself, while the next letters re-
fer to semantic and/or morphosyntactic fea-
tures, such as: Dm - where ?D? stands for
?adverb?, and ?m? stand for ?modal?; Ncmsi
- where ?N? stand for ?noun?, ?c? means
?common?, ?m? is ?masculine?, ?s? is ?singu-
lar?,and ?i? is ?indefinite?.
? LING - other linguistic features derived from
the POS tag in the BulTreeBank tagset (see
above).
In addition to these, we can also incorporate
syntactic structure of the sentence by breaking
down the tree into dependency relations. For in-
stance, a dependency tree can be represented as
a set of triples in the form of <parent, relation,
child>. <loves, subject, John> and <loves, ob-
ject, Mary> will represent the sentence ?John
loves Mary?. Consequently, three additional fac-
tors are included for both languages:
121
? DEPREL - is the dependency relation be-
tween the current word and the parent node.
? HLEMMA is the lemma of the current word?s
parent node.
? HPOS is the POS tag of the current word?s
parent node.
Here is an example of a processed sentence.
The sentence is ?spored odita v elektricheskite
kompanii politicite zloupotrebyavat s dyrzhavnite
predpriyatiya.? The glosses for the words in
the Bulgarian sentence are: spored (according)
odita (audit-the) v (in) elektricheskite (electrical-
the) kompanii (companies) politicite (politicians-
the) zloupotrebyavat (abuse) s (with) dyrzhavnite
(state-the) predpriyatiya (enterprises). The trans-
lation in the original source is : ?electricity au-
dits prove politicians abusing public companies.?
The result from the linguistic processing and the
addition of information about head elements are
presented in the first seven columns of Table 1.
We extend the grammatical features to have the
same size. All the information is concatenated to
the word forms in the text. In the next section we
present how we extend this format to incorporate
the MRS analysis. In the next section we will ex-
tend this example to incorporate the MRS analysis
of the sentence.
5 MRS Supertagging
Our work on Minimal Recursion Semantic anal-
ysis of Bulgarian text is inspired by the work
on MRS and RMRS (Robust Minimal Recursion
Semantic) (see (Copestake, 2003) and (Copes-
take, 2007)) and the previous work on transfer
of dependency analyses into RMRS structures de-
scribed in (Spreyer and Frank, 2005) and (Jakob
et al, 2010). In this section we present first a short
overview of MRS and RMRS. Then we discuss
the new features added on the basis of the RMRS
structures.
MRS is introduced as an underspecified se-
mantic formalism (Copestake et al, 2005). It is
used to support semantic analyses in the English
HPSG grammar ERG (Copestake and Flickinger,
2000), but also in other grammar formalisms like
LFG. The main idea is that the formalism avoids
spelling out the complete set of readings resulting
from the interaction of scope bearing operators
and quantifiers, instead providing a single under-
specified representation from which the complete
set of readings can be constructed. Here we will
present only basic definitions from (Copestake et
al., 2005). For more details the cited publication
should be consulted. An MRS structure is a tu-
ple ? GT , R, C ?, where GT is the top handle,
R is a bag of EPs (elementary predicates) and C
is a bag of handle constraints, such that there is
no handle h that outscopes GT . Each elementary
predication contains exactly four components: (1)
a handle which is the label of the EP; (2) a rela-
tion; (3) a list of zero or more ordinary variable
arguments of the relation; and (4) a list of zero or
more handles corresponding to scopal arguments
of the relation (i.e., holes). RMRS is introduced
as a modification of MRS which to capture the se-
mantics resulting from the shallow analysis. Here
the following assumption is taken into account the
shallow processor does not have access to a lexi-
con. Thus it does not have access to arity of the
relations in EPs. Therefore, the representation has
to be underspecified with respect to the number
of arguments of the relations. The names of rela-
tions are constructed on the basis of the lemma for
each wordform in the text and the main argument
for the relation is specified. This main argument
could be of two types: referential index for nouns
and event for the other part of speeches.
Because in this work we are using only the
RMRS relation and the type of the main argument
as features to the translation model, we will skip
here the explanation of the full structure of RMRS
structures and how they are constructed. Thus, we
firstly do a match between the surface tokens and
the MRS elementary predicates (EPs) and then
extract the following features as extra factors:
? EP - the name of the elementary predicate,
which usually indicates an event or an entity
semantically.
? EOV indicates the current EP is either an
event or a reference variable.
Notice that we do not take all the information
provided by the MRS, e.g., we throw away the
scopal information and the other arguments of the
relations. This kind of information is not straight-
forward to be represented in such ?tagging?-style
models, which will be tackled in the future.
This information for the example sentence is
122
WF Lemma POSex Ling DepRel HLemma HPOS EP EoV
spored spored R adjunct zloupotrebyavam VP spored r e
odita odit Nc npd prepcomp spored R odit n v
v v R mod odit Nc v r e
elektricheskite elektricheski A pd mod kompaniya Nc elekticheski a e
kompanii kompaniya Nc fpi prepcomp v R kompaniya n v
politicite politik Nc mpd subj zloupotrebyavam Vp politik n v
zloupotrebyavat zloupotrebyavam Vp tir3p root - - zloupotrebyavam v e
s s R indobj zloupotrebyavam Vp s r e
dyrzhavnite dyrzhaven A pd mod predpriyatie Nc dyrzhaven a e
predpriyatiya predpriyatie Nc npi prepcomp s R predpriyatie n v
Table 1: The sentence analysis with added head information ? HLemma and HPOS.
represented for each word form in the last two
columns of Table 1.
All these factors encoded within the corpus
provide us with a rich selection of factors for dif-
ferent experiments. Some of them are presented
within the next section. The model of encoding
MRS information in the corpus as additional fea-
tures does not depend on the actual semantic anal-
ysis ? MRS or RMRS, because both of them pro-
vide enough semantic information.
6 Experiments
6.1 Experiments with the Bulgarian raw
corpus
To run the experiments, we use the phrase-based
translation model provided by the open-source
statistical machine translation system, Moses4
(Koehn et al, 2007). For training the translation
model, the parallel corpora (mentioned in Sec-
tion 2) were preprocessed with the tokenizer and
lowercase converter provided by Moses. Then the
procedure is quite standard:
? We run GIZA++ (Och and Ney, 2003) for bi-
directional word alignment, and then obtain
the lexical translation table and phrase table.
? A tri-gram language model is estimated us-
ing the SRILM toolkit (Stolcke, 2002).
? Minimum error rate training (MERT) (Och,
2003) is applied to tune the weights for the
set of feature weights that maximizes the of-
ficial f-score evaluation metric on the devel-
opment set.
The rest of the parameters we use the default
setting provided by Moses.
4http://www.statmt.org/moses/
We split the corpora into the training set, the
development set and the test set. For SETIMES,
the split is 100,000/500/1,000 and for EMEA, it
is 700,000/500/1,000. For reference, we also run
tests on the JRC-Acquis corpus5. The final results
under the standard evaluation metrics are shown
in the following table in terms of BLEU (Papineni
et al, 2002):
Corpora Test Dev Final Drop
SETIMES? SETIMES 34.69 37.82 36.49 /
EMEA? EMEA 51.75 54.77 51.62 /
SETIMES? EMEA 13.37 / / 61.5%
SETIMES? JRC-Acquis 7.19 / / 79.3%
EMEA? SETIMES 7.37 / / 85.8%
EMEA? JRC-Acquis 9.21 / / 82.2%
Table 2: Results of the baseline SMT system
(Bulgarian-English)
As we mentioned before, the EMEA corpus
is mainly about the description of medicine us-
age, and the format is quite fixed. Therefore, it
is not surprising to see high performance on the
in-domain test (2nd row in Table 2). SETIMES,
consisting of news articles, is in a less controlled
setting. The BLEU score is lower6. The results on
the out-of-domain tests are in general much lower
with a drop of more than 60% in BLEU score (the
last column). For the JRC-Acquis corpus, in con-
trast to the in-domain scores given by Koehn et
al. (2009) (61.3), the low out-of-domain results
shows a very similar situation as EMEA. A brief
manual check of the results indicate that the out-
of-domain tests suffer severely from the missing
5http://optima.jrc.it/Acquis/
6Actually, the BLEU score itself is higher than for most
of the other language pairs http://matrix.statmt.
org/. As the datasets are different, the results are not di-
rectly comparable. Here, we just want to get a rough pic-
ture. Achieving better performance for Bulgarian-to-English
translation than for other language pairs is not the focus of
the paper.
123
lexicon, while the in-domain test for the news arti-
cles contains more interesting issues to look into.
The better translation quality also makes the sys-
tem outputs human readable.
6.2 Experiments with the
Linguistically-Augmented Bulgarian
Corpus
As we described the factor-based model in Sec-
tion 4, we also perform experiments to test the
effectiveness of different linguistic annotations.
The different configurations we considered are
shown in the first column of Table 3.
These models can be roughly grouped into
five categories: word form with linguistic fea-
tures; lemma with linguistic features; models
with dependency features; MRS elementary pred-
icates (EP) and the type of the main argument of
the predicate (EOV); and MRS features without
word forms. The setting of the system is mostly
the same as the previous experiment, except for
1) increasing the training data from 100,000 to
150,000 sentence pairs; 2) specifying the factors
during training and decoding; and 3) without do-
ing MERT7. We perform the finer-grained model
only on the SETIMES data, as the language is
more diverse (compared to the other two corpora).
The results are shown in Table 3.
The first model is served as the baseline here.
We show all the n-gram scores besides the final
BLEU, since the some of the differences are very
small. In terms of the numbers, POS seems to
be an effective factor, as Model 2 has the highest
score. Model 3 indicates that linguistic features
also improve the performance. Model 4-6 show
the necessity of including the word form as one
of the factors, in terms of BLEU scores. Model
10 shows significant decrease after incorporating
HLEMMA feature. This may be due to the data
sparsity, as we are actually aligning and translat-
ing bi-grams instead of tokens. This may also in-
dicate that increasing the number of factors does
not guarantee performance enhancement. After
replacing the HLEMMA with HPOS, the result is
close to the others (Model 8). The experiments
with features from the MRS analyses (Model 11-
16) show improvements over the baseline consis-
tently and using only the MRS features (Model
7This is mainly due to the large amount of computation
required. We will perform MERT on the better-performing
configurations in the future.
17-18) also delivers descent results. In future ex-
periments we will consider to include more fea-
ture from the MRS analyses.
So far, incorporating additional linguistic
knowledge has not shown huge improvement in
terms of statistical evaluation metrics. However,
this does not mean that the translations delivered
are the same. In order to fully evaluate the system,
manual analysis is absolutely necessary. We are
still far from drawing a conclusion at this point,
but the preliminary scores calculated already indi-
cate that the system can deliver decent translation
quality consistently.
6.3 Manual Evaluation
We manually validated the output for all the mod-
els mentioned in Table 3. The guideline in-
cludes two aspects of the quality of the transla-
tion: Grammaticality and Content. Grammati-
cality can be evaluated solely on the system out-
put and Content by comparison with the reference
translation. We use a 1-5 score for each aspect as
follows:
Grammaticality
1. The translation is not understandable.
2. The evaluator can somehow guess the mean-
ing, but cannot fully understand the whole
text.
3. The translation is understandable, but with
some efforts.
4. The translation is quite fluent with some mi-
nor mistakes or re-ordering of the words.
5. The translation is perfectly readable and
grammatical.
Content
1. The translation is totally different from the
reference.
2. About 20% of the content is translated, miss-
ing the major content/topic.
3. About 50% of the content is translated, with
some missing parts.
4. About 80% of the content is translated, miss-
ing only minor things.
5. All the content is translated.
For the missing lexicons or not-translated
Cyrillic tokens, we ask the evaluators to score 2
124
ID Model BLEU 1-gram 2-gram 3-gram 4-gram
1 WF 38.61 69.9 44.6 31.5 22.7
2 WF, POS 38.85 69.9 44.8 31.7 23.0
3 WF, LEMMA, POS, LING 38.84 69.9 44.7 31.7 23.0
4 LEMMA 37.22 68.8 43.0 30.1 21.5
5 LEMMA, POS 37.49 68.9 43.2 30.4 21.8
6 LEMMA, POS, LING 38.70 69.7 44.6 31.6 22.8
7 WF, DEPREL 36.87 68.4 42.8 29.9 21.1
8 WF, DEPREL, HPOS 36.21 67.6 42.1 29.3 20.7
9 WF, LEMMA, POS, LING, DEPREL 36.97 68.2 42.9 30.0 21.3
10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 29.57 60.8 34.9 23.0 15.7
11 WF, POS, EP 38.74 69.8 44.6 31.6 22.9
12 WF, POS, LING, EP 38.76 69.8 44.6 31.7 22.9
13 WF, EP, EOV 38.74 69.8 44.6 31.6 22.9
14 WF, POS, EP, EOV 38.74 69.8 44.6 31.6 22.9
15 WF, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9
16 WF, POS, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9
17 EP, EOV 37.22 68.5 42.9 30.2 21.6
18 EP, EOV, LING 38.38 69.3 44.2 31.3 22.7
Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000)
for one Cyrillic token and score 1 for more than
one tokens in the output translation.
The results are shown in the following two ta-
bles, Table 4 and Table 5, respectively. The cur-
rent results from the manual validation are on the
basis of 150 sentence pairs. The numbers shown
in the tables are the number of sentences given the
corresponding scores. The ?Total? column sums
up the scores of all the output sentences by each
model.
The results show that linguistic and seman-
tic analyses definitely improve the quality of the
translation. Exploiting the linguistic processing
on word level ? LEMMA, POS and LING ? pro-
duces the best result. However, the model with
only EP and EOV features also delivers very good
results, which indicates the effectiveness of the
MRS features from the deep hand-crafted gram-
mars. Including more factors (especially the in-
formation from the dependency parsing) drops the
results because of the sparseness effect over the
dataset, which is consistent with the automatic
evaluation BLEU score. The last two rows are
shown for reference. ?Google? shows the results
of using the online translation service provided by
http://translate.google.com/. The
high score (very close to the reference translation)
may be because our test data are not excluded
from their training data. In future we plan to do
the same evaluation with a larger dataset.
The problem with the untranslated Cyrillic to-
kens in our view could be solved in most of the
cases by providing additional lexical information
from a Bulgarian-English lexicon. Thus, we also
evaluated the possible impact of such a lexicon if
it had been available. In order to do this, we sub-
stituted each copied Cyrillic token with its trans-
lation when there was only one possible transla-
tion. We did such substitutions for 189 sentence
pairs. Then we evaluated the result by classify-
ing the translations as acceptable or unacceptable.
The number of the acceptable translations are 140
in this case.
The manual evaluation of the translation mod-
els on a bigger scale is in progress. The current re-
sults are promising. Statistical evaluation metrics
can give us a brief overview of the system perfor-
mance, but the actual translation quality is much
more interesting to us, as in many cases, the dif-
ferent surface translations can convey exactly the
same meaning in the context.
7 Related Work
Our work is also enlightened by another line of
research, transfer-based MT models, which are
seemingly different but actually very close. In this
section, before we mention some previous work
in this research direction, we firstly introduce the
background of the development of the deep HPSG
grammars.
The MRSes are usually delivered together with
the HPSG analyses of the text. There already
125
ID Model 1 2 3 4 5 Total
1 WF 20 47 5 32 46 487
2 WF, POS 20 48 5 37 40 479
3 WF, LEMMA, POS, LING 20 47 6 34 43 483
4 LEMMA 15 34 11 46 44 520
5 LEMMA, POS 15 38 12 51 34 501
6 LEMMA, POS, LING 20 48 5 34 43 482
7 WF, DEPREL 32 48 3 29 38 443
8 WF, DEPREL, HPOS 45 41 7 23 34 410
9 WF, LEMMA, POS, LING, DEPREL 34 47 5 30 34 433
10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 101 32 0 8 9 242
11 WF, POS, EP 19 49 4 34 44 485
12 WF, POS, LING, EP 19 49 3 39 40 482
13 WF, EP, EOV 20 49 2 41 38 478
14 WF, POS, EP, EOV 19 50 3 31 47 487
15 WF, LING, EP, EOV 19 48 5 37 41 483
16 WF, POS, LING, EP, EOV 19 49 5 37 40 480
17 EP, EOV 15 41 10 44 40 503
18 EP, EOV, LING 20 49 7 38 36 471
19 GOOGLE 0 2 20 52 76 652
20 REFERENCE 0 0 5 51 94 689
Table 4: Manual evaluation of the grammaticality
exist quite extensive implemented formal HPSG
grammars for English (Copestake and Flickinger,
2000), German (Mu?ller and Kasper, 2000), and
Japanese (Siegel, 2000; Siegel and Bender, 2002).
HPSG is the underlying theory of the interna-
tional initiative LinGO Grammar Matrix (Bender
et al, 2002). At the moment, precise and lin-
guistically motivated grammars, customized on
the base of the Grammar Matrix, have been or
are being developed for Norwegian, French, Ko-
rean, Italian, Modern Greek, Spanish, Portuguese,
Chinese, etc. There also exists a first version of
the Bulgarian Resource Grammar - BURGER. In
the research reported here, we use the linguistic
modeled knowledge from the existing English and
Bulgarian grammars. Since the Bulgarian gram-
mar has limited coverage on news data, depen-
dency parsing has been performed instead. Then,
mapping rules have been defined for the construc-
tion of RMRSes.
However, the MRS representation is still quite
close to the syntactic level, which is not fully lan-
guage independent. This requires a transfer at the
MRS level, if we want to do translation from the
source language to the target language. The trans-
fer is usually implemented in the form of rewrit-
ing rules. For instance, in the Norwegian LO-
GON project (Oepen et al, 2004), the transfer
rules were hand-written (Bond et al, 2005; Oepen
et al, 2007), which included a large amount of
manual work. Graham and van Genabith (2008)
and Graham et al (2009) explored the automatic
rule induction approach in a transfer-based MT
setting involving two lexical functional grammars
(LFGs), which was still restricted by the perfor-
mance of both the parser and the generator. Lack
of robustness for target side generation is one of
the main issues, when various ill-formed or frag-
mented structures come out after transfer. Oepen
et al (2007) use their generator to generate text
fragments instead of full sentences, in order to in-
crease the robustness. We want to make use of
the grammar resources while keeping the robust-
ness, therefore, we experiment with another way
of transfer involving information derived from the
grammars.
In our approach, we take an SMT system as our
?backbone? which robustly delivers some trans-
lation for any given input. Then, we augment
SMT with deep linguistic knowledge. In general,
what we are doing is still along the lines of previ-
ous work utilizing deep grammars, but we build a
more ?light-weighted? transfer model.
8 Conclusion and Future Work
In this paper, we report our work on build-
ing a linguistically-augmented statistical machine
translation model from Bulgarian to English.
126
ID Model 1 2 3 4 5 Total
1 WF 20 46 5 23 56 499
2 WF, POS 20 48 5 24 53 492
3 WF, LEMMA, POS, LING 20 47 1 24 58 503
4 LEMMA 15 32 5 33 65 551
5 LEMMA, POS 15 35 9 32 59 535
6 LEMMA, POS, LING 20 48 5 22 55 494
7 WF, DEPREL 32 49 4 14 51 453
8 WF, DEPREL, HPOS 45 41 2 21 41 422
9 WF, LEMMA, POS, LING, DEPREL 34 48 3 20 45 444
10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 101 32 0 6 11 244
11 WF, POS, EP 19 49 3 20 59 501
12 WF, POS, LING, EP 19 50 2 20 59 500
13 WF, EP, EOV 19 50 4 16 61 500
14 WF, POS, EP, EOV 19 50 2 23 56 497
15 WF, LING, EP, EOV 19 48 4 18 61 504
16 WF, POS, LING, EP, EOV 19 50 3 24 54 494
17 EP, EOV 14 38 7 31 60 535
18 EP, EOV, LING 19 49 7 20 55 493
19 GOOGLE 1 0 9 42 98 686
20 REFERENCE 1 0 5 37 107 699
Table 5: Manual evaluation of the content
Based on our observations of the previous ap-
proaches on transfer-based MT models, we de-
cide to build a hybrid system by combining an
SMT system with deep linguistic resources. We
perform a preliminary evaluation on several con-
figurations of the system (with different linguis-
tic knowledge). The high BLEU score shows the
high quality of the translation delivered by the
SMT baseline; and manual analysis confirms the
consistency of the system.
There are various aspects we can improve the
ongoing project: 1) The MRSes are not fully ex-
plored yet, since we have only considered the EP
and EOV features. 2) We would like to add factors
on the target language side (English) as well. 3)
The guideline of the manual evaluation needs fur-
ther refinement for considering the missing lexi-
cons as well as how much of the content is truly
conveyed (Farreu?s et al, 2011). 4) We also need
more experiments to evaluate the robustness of
our approach in terms of out-domain tests.
Acknowledgements
This work was supported by the EuroMatrix-
Plus project (IST-231720) funded by the Euro-
pean Community under the Seventh Framework
Programme for Research and Technological De-
velopment. The authors would like to thank Tania
Avgustinova for fruitful discussions and her help-
ful linguistic analysis; and also to Laska Laskova,
Stanislava Kancheva and Ivaylo Radev for doing
the human evaluation of the data.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertag-
ging: an approach to almost parsing supertagging: an ap-
proach to almost parsing supertagging: an approach to
almost parsing. Computational Linguistics, 25(2), June.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar Matrix. An open-source starter-kit
for the rapid development of cross-linguistically consis-
tent broad-coverage precision grammar. In Proceedings
of the Workshop on Grammar Engineering and Evalua-
tion at the 19th International Conference on Computa-
tional Linguistics, Taipei, Taiwan.
Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007.
Ccg supertags in factored statistical machine translation.
In Proceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 9?16, Prague, Czech Republic,
June.
Francis Bond, Stephan Oepen, Melanie Siegel, Ann Copes-
take, and Dan Flickinger. 2005. Open source machine
translation with DELPH-IN. In Proceedings of the Open-
Source Machine Translation Workshop at the 10th Ma-
chine Translation Summit, pages 15 ? 22, Phuket, Thai-
land, September.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and
Omar F. Zaidan. 2011. Findings of the 2011 workshop
on statistical machine translation. In Proceedings of the
6th Workshop on SMT.
Yu Chen, M. Jellinghaus, A. Eisele, Yi Zhang, S. Hunsicker,
S. Theison, Ch. Federmann, and H. Uszkoreit. 2009.
127
Combining multi-engine translations with moses. In Pro-
ceedings of the 4th Workshop on SMT.
Ann Copestake and Dan Flickinger. 2000. An open source
grammar development environment and broad-coverage
english grammar using hpsg. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation, Athens, Greece.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language & Computation, 3(4):281?332.
Ann Copestake. 2003. Robust minimal recursion semantics
(working paper).
Ann Copestake. 2007. Applying robust semantics. In Pro-
ceedings of the 10th Conference of the Pacific Assocation
for Computational Linguistics (PACLING), pages 1?12.
Mireia Farreu?s, Marta R. Costa-jussa`, and Maja Popovic?
Morse. 2011. Study and correlation analysis of linguis-
tic, perceptual and automatic machine translation evalu-
ations. Journal of the American Society for Information
Sciences and Technology, 63(1):174?184, October.
Georgi Georgiev, Valentin Zhikov, Petya Osenova, Kiril
Simov, and Preslav Nakov. 2012. Feature-rich part-of-
speech tagging for morphologically complex languages:
Application to bulgarian. In Proceedings of EACL 2012.
MIT Press, Cambridge, MA, USA.
Jess Gimnez and Llus Mrquez. 2004. Svmtool: A general
pos tagger generator based on support vector machines.
In Proceedings of the 4th LREC.
Yvette Graham and Josef van Genabith. 2008. Packed rules
for automatic transfer-rule induction. In Proceedings of
the European Association of Machine Translation Con-
ference (EAMT 2008), pages 57?65, Hamburg, Germany,
September.
Yvette Graham, Anton Bryl, and Josef van Genabith. 2009.
F-structure transfer-based statistical machine translation.
In Proceedings of the Lexical Functional Grammar Con-
ference, pages 317?328, Cambridge, UK. CSLI Publica-
tions, Stanford University, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation. In
Proceedings of ACL, Prague, Czech Republic, June.
Max Jakob, Marke?ta Lopatkova?, and Valia Kordoni. 2010.
Mapping between dependency structures and composi-
tional semantic representations. In Proceedings of the
7th International Conference on Language Resources and
Evaluation (LREC 2010), pages 2491?2497.
Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proceedings of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of ACL
(demo session).
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe. In
Proceedings of MT Summit XII.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, January.
Stefan Mu?ller and Walter Kasper. 2000. HPSG analy-
sis of German. In Wolfgang Wahlster, editor, Verbmo-
bil. Foundations of Speech-to-Speech Translation, pages
238 ? 253. Springer, Berlin, Germany, artificial intelli-
gence edition.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer,
Torbj?rn Nordga?rd, , and Victoria Rose?n. 2004. Som a?
kapp-ete med trollet? towards MRS-based norwegian to
english machine translation. In Proceedings of the 10th
International Conference on Theoretical and Method-
ological Issues in Machine Translation, Baltimore, MD.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, Victoria Rose?n, and Dan Flickinger. 2007. To-
wards hybrid quality-oriented machine translation ? on
linguistics and probabilities in MT. In Proceedings of the
11th Conference on Theoretical and Methodological Is-
sues in Machine Translation (TMI-07), Skovde, Sweden.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of ACL.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of japanese. In Proceedings of the 19th
International Conference on Computational Linguistics,
Taipei, Taiwan.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil. Foundations of Speech-
to-Speech Translation, pages 265 ? 280. Springer, Berlin,
Germany, artificial intelligence edition.
Thoudam Doren Singh and Sivaji Bandyopadhyay. 2010.
Manipuri-english bidirectional statistical machine trans-
lation systems using morphology and dependency rela-
tions. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 83?91,
Beijing, China, August.
Kathrin Spreyer and Anette Frank. 2005. Projecting RMRS
from TIGER Dependencies. In Proceedings of the HPSG
2005 Conference, pages 354?363, Lisbon, Portugal.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, volume 2.
Gregor Thurmair. 2005. Hybrid architectures for machine
translation systems. Language Resources and Evalua-
tion, 39(1).
Gregor Thurmair. 2009. Comparing different architectures
of hybrid machine translation systems. In Proceedings of
MT Summit XII.
128
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10?19,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Linguistically-Enriched Models for Bulgarian-to-English Machine
Translation
Rui Wang
Language Technology Lab
DFKI GmbH
Saarbru?cken, Germany
ruiwang@dfki.de
Petya Osenova and Kiril Simov
Linguistic Modelling Department, IICT
Bulgarian Academy of Sciences
Sofia, Bulgaria
{petya,kivs}@bultreebank.org
Abstract
In this paper, we present our linguistically-
enriched Bulgarian-to-English statistical ma-
chine translation model, which takes a sta-
tistical machine translation (SMT) system as
backbone various linguistic features as fac-
tors. The motivation is to take advantages of
both the robustness of the SMT system and
the rich linguistic knowledge from morpho-
logical analysis as well as the hand-crafted
grammar resources. The automatic evaluation
has shown promising results and our extensive
manual analysis confirms the high quality of
the translation the system delivers. The whole
framework is also extensible for incorporating
information provided by different sources.
1 Introduction
Incorporating linguistic knowledge into statistical
models is an everlasting topic in natural language
processing. The same story happens in the ma-
chine translation community. Along with the suc-
cess of statistical machine translation (SMT) models
(summarized by Koehn (2010)), various approaches
have been proposed to include linguistic informa-
tion, ranging from early work by Wu (1997) to re-
cent work by Chiang (2010), from deep transfer-
based models (Graham and van Genabith, 2008) to
mapping rules at the syntactic level (Galley et al,
2004; Liu et al, 2006; Zhang et al, 2008). Although
the purely data-driven approaches achieve signifi-
cant results as shown in the evaluation campaigns
(Callison-Burch et al, 2011), according to the hu-
man evaluation, the final outputs of the SMT sys-
tems are still far from satisfactory.
Koehn and Hoang (2007) proposed a factored
SMT model as an extension of the traditional
phrase-based SMT model, which opens up an easy
way to incorporate linguistic knowledge at the to-
ken level. Birch et al (2007) and Hassan et al
(2007) have shown the effectiveness of adding su-
pertags on the target side, and Avramidis and Koehn
(2008) have focused on the source side, translat-
ing a morphologically-poor language (English) to a
morphologically-rich language (Greek). However,
all of them attempt to enrich the English part of
the language pairs being translated. For the lan-
guage pairs like Bulgarian-English, there has not
been much study on it, mainly due to the lack of
resources, including corpora, preprocessors, etc, on
the Bulgarian part. There was a system published
by Koehn et al (2009), which was trained and tested
on the European Union law data, but not on other
popular domains like news. They reported a very
high BLEU score (Papineni et al, 2002) on the
Bulgarian-English translation direction (61.3).
Apart from being morphologically-rich, Bulgar-
ian has a number of challenging linguistic phenom-
ena to consider, including free word order, long dis-
tance dependency, coreference relations, clitic dou-
bling, etc. For instance, the following two sentences:
(1) Momcheto
Boy-the
j
her-dat
go
it-acc
dava
gives
buketa
bouquet-the
na
to
momicheto.
girl-the.
The boy gives the bouquet to the girl.
(2) Momcheto
Boy-the
j
her-dat
go
it-acc
dava.
gives.
The boy gives it to her.
10
are difficult for the traditional phrase-based SMT
system, because the clitic in the first sentence must
not be translated, while in the second case it is oblig-
atory. Via the semantic analysis (e.g., Minimal Re-
cursion Semantics), the clitic information will be in-
corporated in the representation of the correspond-
ing arguments.
In this work, we rely on the linguistic processing
to cope with some of these phenomena and improve
the correspondences between the two languages: 1)
The lemmatization factors out the difference be-
tween word forms and ensures better coverage of the
Bulgarian-English lexicon. 2) The dependency pars-
ing helps to identify the grammatical functions such
as subject, object in sentences with a non-standard
word order. 3) The semantic analysis provides a fur-
ther abstraction which hides some of the language
specific features. Example of the last is the case of
clitic doubling.
As for the Bulgarian-to-English translation
model, we basically ?annotate? the SMT baseline
with various linguistic features derived from the
preprocessing and hand-crafted grammars. There
are three contributions of this work:
? The models trained on a decent amount of par-
allel corpora output surprisingly good results,
in terms of automatic evaluation metrics.
? The enriched models give us more space for ex-
perimenting with different linguistic features
without losing the ?basic? robustness.
? According to our extensive manual analyses,
the approach has shown promising results for
future integration of more knowledge from the
continued advances of the deep grammars.
The rest of the paper will be organized as fol-
lows: Section 2 briefly introduces some background
of the hand-crafted grammar resources we use and
also some previous related work on transfer-based
MT. Section 3 describes the linguistic analyses we
perform on the Bulgarian text, whose output is used
in the factored SMT model. We show our exper-
iments in Section 4 as well as both automatic and
detailed manual evaluation of the results. We sum-
marize this paper in Section 5 and point out several
directions for future work.
2 Machine Translation with Deep
Grammars
Our work is also enlightened by another line of re-
search, transfer-based MT models using deep lin-
guistic knowledge, which are seemingly different
but actually very related. In this section, before
we describe our model of incorporating linguis-
tic knowledge from the hand-crafted grammars, we
firstly introduce the background of such resources as
well as some previous work on MT using them.
Our usage of Minimal Recursion Semantic
(MRS) analysis of Bulgarian text is inspired by the
work on MRS and RMRS (Robust Minimal Recur-
sion Semantic) (see (Copestake, 2003) and (Copes-
take, 2007)) and the previous work on transfer of de-
pendency analyses into RMRS structures described
in (Spreyer and Frank, 2005) and (Jakob et al,
2010). Although being a semantic representation,
MRS is still quite close to the syntactic level, which
is not fully language independent. This requires a
transfer at the MRS level, if we want to do trans-
lation from the source language to the target lan-
guage. The transfer is usually implemented in the
form of rewriting rules. For instance, in the Nor-
wegian LOGON project (Oepen et al, 2004), the
transfer rules were hand-written (Bond et al, 2005;
Oepen et al, 2007), which included a large amount
of manual work. Graham and van Genabith (2008)
and Graham et al (2009) explored the automatic rule
induction approach in a transfer-based MT setting
involving two lexical functional grammars (LFGs)1,
which was still restricted by the performance of both
the parser and the generator. Lack of robustness for
target side generation is one of the main issues, when
various ill-formed or fragmented structures come
out after transfer. Oepen et al (2007) used their
generator to generate text fragments instead of full
sentences, in order to increase the robustness.
In our approach, we want to make use of the
grammar resources while keeping the robustness,
therefore, we experiment with another way of trans-
fer involving information derived from the gram-
mars. In particular, we take a robust SMT system
as our ?backbone? and then we augment it with deep
linguistic knowledge. In general, what we are doing
1Although their grammars are automatically induced from
treebanks, the formalism supports rich linguistic information.
11
is still along the lines of previous work utilizing deep
grammars, but we build a more ?light-weighted? but
yet extensible statistical transfer model.
3 Factor-based SMT Model
Our translation model is built on top of the factored
SMT model proposed by Koehn and Hoang (2007),
as an extension of the traditional phrase-based SMT
framework. Instead of using only the word form
of the text, it allows the system to take a vector of
factors to represent each token, both for the source
and target languages. The vector of factors can be
used for different levels of linguistic annotations,
like lemma, part-of-speech, or other linguistic fea-
tures, if they can be (somehow) represented as an-
notations to each token.
The process is quite similar to supertagging (Ban-
galore and Joshi, 1999), which assigns ?rich descrip-
tions (supertags) that impose complex constraints in
a local context?. In our case, all the linguistic fea-
tures (factors) associated with each token form a
supertag to that token. Singh and Bandyopadhyay
(2010) had a similar idea of incorporating linguis-
tic features, while they worked on Manipuri-English
bidirectional translation. Our approach is slightly
different from (Birch et al, 2007) and (Hassan et al,
2007), who mainly used the supertags on the target
language side, English. Instead, we primarily ex-
periment with the source language side, Bulgarian.
This potentially huge feature space provides us with
various possibilities of using our linguistic resources
developed within and out of our project.
Firstly, the data was processed by the NLP pipe
for Bulgarian (Savkov et al, 2012) including a mor-
phological tagger, GTagger (Georgiev et al, 2012), a
lemmatizer and a dependency parser2. Then we con-
sider the following factors on the source language
side (Bulgarian):
? WF ? word form is just the original text token.
? LEMMA is the lexical invariant of the original word
form. We use the lemmatizer, which operates on
the output from the POS tagging. Thus, the 3rd per-
son, plural, imperfect tense verb form ?varvyaha?
(?walking-were?, They were walking) is lemmatized
as the 1st person, present tense verb ?varvya?.
2We have trained the MaltParser3 (Nivre et al, 2007)
on the dependency version of BulTreeBank: http://www.
bultreebank.org/dpbtb/. The trained model achieves
85.6% labeled parsing accuracy.
? POS ? part-of-speech of the word. We use the po-
sitional POS tag set of the BulTreeBank, where the
first letter of the tag indicates the POS itself, while
the next letters refer to semantic and/or morphosyn-
tactic features, such as: Dm - where ?D? stands for
?adverb?, and ?m? stand for ?modal?; Ncmsi - where
?N? stand for ?noun?, ?c? means ?common?, ?m? is
?masculine?, ?s? is ?singular?,and ?i? is ?indefinite?.
? LING ? other linguistic features derived from the
POS tag in the BulTreeBank tagset.
? DEPREL is the dependency relation between the
current word and the parent node.
? HLEMMA is the lemma of the parent node.
? HPOS is the POS tag of the parent node.
Here is an example of a processed sentence. The
sentence is ?spored odita v elektricheskite kompanii
politicite zloupotrebyavat s dyrzhavnite predpriy-
atiya.? The glosses for the words in the Bulgarian
sentence are: spored (according) odita (audit-the) v
(in) elektricheskite (electrical-the) kompanii (com-
panies) politicite (politicians-the) zloupotrebyavat
(abuse) s (with) dyrzhavnite (state-the) predpriy-
atiya (enterprises). The translation in the original
source is : ?electricity audits prove politicians abus-
ing public companies.? The result from the linguistic
processing are presented in Table 1.
As for the deep linguistic knowledge, we also ex-
tract features from the semantic analysis ? Minimal
Recursion Semantics (MRS). MRS is introduced as
an underspecified semantic formalism (Copestake et
al., 2005). It is used to support semantic analyses
in the English HPSG grammar ERG (Copestake and
Flickinger, 2000), but also in other grammar for-
malisms like LFG. The main idea is that the for-
malism avoids spelling out the complete set of read-
ings resulting from the interaction of scope bearing
operators and quantifiers, instead providing a single
underspecified representation from which the com-
plete set of readings can be constructed. Here we
will present only basic definitions from (Copestake
et al, 2005). For more details the cited publication
should be consulted.
An MRS structure is a tuple ? GT , R, C ?, where
GT is the top handle, R is a bag of EPs (ele-
mentary predicates) and C is a bag of handle con-
straints, such that there is no handle h that outscopes
GT . Each elementary predicate contains exactly
four components: 1) a handle which is the label of
12
No WF Lemma POS Ling DepRel HLemma HPOS
1 spored spored R adjunct zloupotrebyavam VP
2 odita odit Nc npd prepcomp spored R
3 v v R mod odit Nc
4 elektricheskite elektricheski A pd mod kompaniya Nc
5 kompanii kompaniya Nc fpi prepcomp v R
6 politicite politik Nc mpd subj zloupotrebyavam Vp
7 zloupotrebyavat zloupotrebyavam Vp tir3p root - -
8 s s R indobj zloupotrebyavam Vp
9 dyrzhavnite dyrzhaven A pd mod predpriyatie Nc
10 predpriyatiya predpriyatie Nc npi prepcomp s R
Table 1: The sentence analysis with added head information ? HLemma and HPOS.
No EP EoV EP1 /POS1 EP2 /POS2 EP3 /POS3
1 spored r e zloupotrebyavam v/Vp odit n/Nc -
2 odit n v - - -
3 v r e odit n/Nc kompaniya n/Nc -
4 elekticheski a e kompaniya n/Nc - -
5 kompaniya n v - - -
6 politik n v - - -
7 zloupotrebyavam v e politik n/Nc - s r/R
8 s r e zloupotrebyavam v/Vp predpriyatie n/Nc -
9 dyrzhaven a e predpriyatie n/Nc - -
10 predpriyatie n v - - -
Table 2: Representation of MRS factors for each wordform in the sentence.
the EP; 2) a relation; 3) a list of zero or more or-
dinary variable arguments of the relation; and 4) a
list of zero or more handles corresponding to scopal
arguments of the relation (i.e., holes).
Robust MRS (RMRS) is introduced as a modifica-
tion of MRS which captures the semantics resulting
from the shallow analysis. Here the following as-
sumption is taken into account: the shallow proces-
sor does not have access to a lexicon. Thus it does
not have access to the arity of the relations in EPs.
Therefore, the representation has to be underspeci-
fied with respect to the number of arguments of the
relations. The names of relations are constructed on
the basis of the lemma for each wordform in the text
and the main argument for the relation is specified.
This main argument could be of two types: referen-
tial index for nouns and event for the other parts of
speech. Because in this work we are using only the
RMRS relation and the type of the main argument as
features to the translation model, we will skip here
the explanation of the full RMRS structures and how
they are constructed.
As for the factors, we firstly do a match between
the surface tokens and the MRS elementary predi-
cates (EPs) and then extract the following features
as extra factors:
? EP ? the name of the elementary predicate, which
usually indicates an event or an entity semantically.
? EOV indicates the current EP is either an event or a
reference variable.
? ARGnEP indicates the elementary predicate of the
argument which belongs to the predicate. n is usu-
ally from 1 to 3.
? ARGnPOS indicates the POS tag of the argument
which belongs to the predicate.
Notice that we do not take all the information pro-
vided by the MRS, e.g., we throw away the scopal
information and the other arguments of the relations.
Those kinds of information is not straightforward to
be represented in such ?tagging?-style models, which
will be tackled in the future.
The extra information for the example sentence
is represented in Table 2. All these factors encoded
13
within the corpus provide us with a rich selection of
features for different experiments.
4 Experiments
To run the experiments, we use the phrase-based
translation model provided by the open-source sta-
tistical machine translation system, Moses4 (Koehn
et al, 2007). For training the translation model,
the SETIMES parallel corpus has been used, which
is part of the OPUS parallel corpus5. As for the
choice of the datasets, the language is more diverse
in the news articles, compared with other corpora in
more controlled settings, e.g., the JRC-Acquis cor-
pus6 used by Koehn et al (2009).
We split the corpus into the training set and the
test set by 150,000 and 1,000 sentence pairs re-
spectively7. Both datasets are preprocessed with
the tokenizer and lowercase converter provided by
Moses. Then the procedure is quite standard: We
run GIZA++ (Och and Ney, 2003) for bi-directional
word alignment, and then obtain the lexical trans-
lation table and phrase table. A tri-gram language
model is estimated using the SRILM toolkit (Stol-
cke, 2002). For the rest of the parameters we use the
default setting provided by Moses.
Notice that, since on the target language side (i.e.,
English) we do not have any other factors than the
word form, the factor-based models we use here
only differentiate from each other in the translation
phase, i.e., there is no ?generation? models involved.
4.1 Automatic Evaluation Metrics
The baseline results (non-factored model) under the
standard evaluation metrics are shown in the first
row of Table 3 in terms of BLEU (Papineni et al,
2002) and METEOR (Denkowski and Lavie, 2011).
We then design various configurations to test the
effectiveness of different linguistic annotations de-
scribed in Section 3. The detailed configurations we
considered are shown in the first column of Table 3.
The first impression is that the BLEU scores in
general are high. These models can be roughly
4http://www.statmt.org/moses/
5OPUS ? an open source parallel corpus, http://
opus.lingfil.uu.se/
6http://optima.jrc.it/Acquis/
7We did not preform MERT (Och, 2003), as it is quite com-
putationally heavy for such various configurations.
grouped into six categories (separated by double
lines): word form with linguistic features; lemma
with linguistic features; models with dependency
features; MRS elementary predicates (EP) and the
type of the main argument of the predicate (EOV);
EP features without word forms; and EP features
with MRS ARGn features.
In terms of the resulting scores, POS and Lemma
seem to be effective features, as Model 2 has the
highest BLEU score and Model 4 the best METEOR
score. Model 3 indicates that linguistic features also
improve the performance. Model 4-6 show the ne-
cessity of including the word form as one of the
factors. Incorporating HLEMMA feature largely de-
creases the results due to the vastly increasing vo-
cabulary, i.e., aligning and translating bi-grams in-
stead of tokens. Therefore, we did not include the
results in the table. After replacing the HLEMMA
with HPOS, the result is close to the others (Model
8). Model 9 may also indicate that increasing the
number of factors does not guarantee performance
enhancement. The experiments with predicate fea-
tures (EP and EOV) from the MRS analyses (Model
10-12) show improvements over the baseline con-
sistently and using only the MRS features (Model
13-14) also delivers descent results. Concerning
the MRS ARGn features, the models with ARGnEP
again suffer from the sparseness problem as the de-
pendency HLEMMA features, but the models with
ARGnPOS (Model 15-16) achieve better perfor-
mance than those with dependency HPOS features.
This is mainly because the dependency information
is encoded together with the (syntactically) depen-
dent word, while the MRS arguments are grouped
around the semantic heads.
So far, incorporating additional linguistic knowl-
edge has not shown huge improvement in terms of
statistical evaluation metrics. However, this does not
mean that the translations delivered are the same. In
order to fully evaluate the system, manual analysis is
absolutely necessary. We are still far from drawing a
conclusion at this point, but the automatic evaluation
scores already indicate that the system can deliver
decent translation quality consistently.
4.2 Manual Evaluation
We manually validated the output for all the models
mentioned in Table 3. The guideline includes two
14
ID Model BLEU 1-gram 2-gram 3-gram 4-gram METEOR
1 WF (Baseline) 38.61 69.9 44.6 31.5 22.7 0.3816
2 WF, POS 38.85 69.9 44.8 31.7 23.0 0.3812
3 WF, LEMMA, POS, LING 38.84 69.9 44.7 31.7 23.0 0.3803
4 LEMMA 37.22 68.8 43.0 30.1 21.5 0.3817
5 LEMMA, POS 37.49 68.9 43.2 30.4 21.8 0.3812
6 LEMMA, POS, LING 38.70 69.7 44.6 31.6 22.8 0.3800
7 WF, DEPREL 36.87 68.4 42.8 29.9 21.1 0.3627
8 WF, DEPREL, HPOS 36.21 67.6 42.1 29.3 20.7 0.3524
9 WF, LEMMA, POS, LING, DEPREL 36.97 68.2 42.9 30.0 21.3 0.3610
10 WF, POS, EP 38.74 69.8 44.6 31.6 22.9 0.3807
11 WF, EP, EOV 38.74 69.8 44.6 31.6 22.9 0.3807
12 WF, POS, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9 0.3802
13 EP, EOV 37.22 68.5 42.9 30.2 21.6 0.3711
14 EP, EOV, LING 38.38 69.3 44.2 31.3 22.7 0.3691
15 EP, EOV, ARGnPOS 36.21 67.4 41.9 29.2 20.9 0.3577
16 WF, EP, EOV, ARGnPOS 37.37 68.4 43.2 30.3 21.8 0.3641
Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000/1,000)
aspects of the quality of the translation: Grammati-
cality and Content. Grammaticality can be evaluated
solely on the system output and Content by compar-
ison with the reference translation. We use a 1-5
score for each aspect as follows:
Grammaticality
1. The translation is not understandable.
2. The evaluator can somehow guess the meaning, but
cannot fully understand the whole text.
3. The translation is understandable, but with some ef-
forts.
4. The translation is quite fluent with some minor mis-
takes or re-ordering of the words.
5. The translation is perfectly readable and grammati-
cal.
Content
1. The translation is totally different from the refer-
ence.
2. About 20% of the content is translated, missing the
major content/topic.
3. About 50% of the content is translated, with some
missing parts.
4. About 80% of the content is translated, missing only
minor things.
5. All the content is translated.
For the missing lexicons or not-translated Cyril-
lic tokens, we ask the evaluators to score 2 for one
Cyrillic token and score 1 for more than one tokens
in the output translation. We have two annotators
achieving the inter-annotator agreement according
to Cohen?s Kappa (Cohen, 1960) ? = 0.73 for gram-
maticality and ? = 0.75 for content, both of which
are substantial agreement. For the conflict cases,
we take the average value of both annotators and
rounded the final score up or down in order to have
an integer.
The current results from the manual validation
are on the basis of randomly sampled 150 sentence
pairs. The numbers shown in Table 4 are the number
of sentences given the corresponding scores. The
?Sum? column shows the average score of all the out-
put sentences by each model and the ?Final? column
shows the average of the two ?Sum? scores.
The results show that linguistic and semantic
analyses definitely improve the quality of the trans-
lation. Exploiting the linguistic processing on
word level ? LEMMA, POS and LING ? pro-
duces the best result. However, the model with
only EP and EOV features also delivers very good
results, which indicates the effectiveness of the
MRS features from the deep hand-crafted gram-
mars, although incorporating the MRS ARGn fea-
tures shows similar performance drops as depen-
dency features. Including more factors in general
reduces the results because of the sparseness effect
over the dataset, which is consistent with the au-
tomatic evaluation. The last two rows are shown
15
ID Model
Grammaticality Content
Final
1 2 3 4 5 Sum 1 2 3 4 5 Sum
1 WF (Baseline) 20 47 5 32 46 3.25 20 46 5 23 56 3.33 3.29
2 WF, POS 20 48 5 37 40 3.19 20 48 5 24 53 3.28 3.24
3 WF, LEMMA, POS, LING 20 47 6 34 43 3.22 20 47 1 24 58 3.35 3.29
4 LEMMA 15 34 11 46 44 3.47 15 32 5 33 65 3.67 3.57
5 LEMMA, POS 15 38 12 51 34 3.34 15 35 9 32 59 3.57 3.45
6 LEMMA, POS, LING 20 48 5 34 43 3.21 20 48 5 22 55 3.29 3.25
7 WF, DEPREL 32 48 3 29 38 2.95 32 49 4 14 51 3.02 2.99
8 WF, DEPREL, HPOS 45 41 7 23 34 2.73 45 41 2 21 41 2.81 2.77
9 WF, LEMMA, POS, LING, DEPREL 34 47 5 30 34 2.89 34 48 3 20 45 2.96 2.92
10 WF, POS, EP 19 49 4 34 44 3.23 19 49 3 20 59 3.34 3.29
11 WF, EP, EOV 20 49 2 41 38 3.19 19 50 4 16 61 3.33 3.26
12 WF, POS, LING, EP, EOV 19 49 5 37 40 3.20 19 50 3 24 54 3.29 3.25
13 EP, EOV 15 41 10 44 40 3.35 14 38 7 31 60 3.57 3.46
14 EP, EOV, LING 20 49 7 38 36 3.14 19 49 7 20 55 3.29 3.21
15 EP, EOV, ARGnPOS 23 49 9 34 35 3.06 23 47 8 33 39 3.12 3.09
16 WF, EP, EOV, ARGnPOS 34 47 10 30 29 2.82 34 47 10 20 39 2.89 2.85
* GOOGLE 0 2 20 52 76 4.35 1 0 9 42 98 4.57 4.46
* REFERENCE 0 0 5 51 94 4.59 1 0 5 37 107 4.66 4.63
Table 4: Manual evaluation of the grammaticality and the content
for reference. ?Google? shows the results of using
the online translation service provided by http://
translate.google.com/ on 06.02.2012. The
high score (very close to the reference translation)
may be because our test data are not excluded from
their training data. In future we plan to do the same
evaluation with a larger dataset.
Concerning the impact from the linguistic pro-
cessing pipeline to the final translation results,
Lemma and MRS elementary predicates help at the
level of rich morphology. For example, the baseline
model correctly translates the adjective ?Egyptian?
in ?Egyptian Scientists? (plural), but not in ?Egyp-
tian Government, as in the second phrase the adjec-
tive has a neutral gender. Model 4 and Model 13 are
correct for both.
Generally speaking, if we roughly divide the lin-
guistic processing pipeline in two categories: statis-
tical processing (POS tagger and dependency parser)
and rule-based processing (lemmatizer and MRS
construction), the latter category (almost perfect)
highly relies on the former one. For example, the
lemma depends on the word form and the tag, and
the result is unambiguous in more than 98% of the
morphological lexicon and in text this is almost
100% (because the ambiguous cases are very rare).
The errors come mainly from new words and errors
in the tagger. Similarly, the RMRS rules are good
when the parser is correct. Here, the main problems
are duplications of the ROOT elements and the sub-
ject elements, which we plan to fix using heuristics
in the future.
4.3 Question-Based Evaluation
Although the reported manual evaluation in the pre-
vious section demonstrates that linguistic knowl-
edge improves the translation, we notice that the
evaluators tend to give marks at the two ends of
scale, and less in the middle. Generally, this is
because the measurement is done on the basis of
the content that the evaluators extract from the Bul-
garian sentence using there own cognitive capacity.
Then they start to overestimate or underestimate the
translation, knowing in advance what has to be trans-
lated. In order to avoid this subjectivity, we design
a different manual evaluation in which the evalua-
tor does not know the original Bulgarian sentences.
Then the evaluation is based only on the content rep-
resented within the English translation.
In order to do this, we represent the content of the
Bulgarian sentences as a set of questions that have
a list of possible answers, assigned to them. During
the judgement of the content transfer, the evaluators
16
need to answer these questions. As the list of an-
swers also contains false answers, the evaluators are
forced to select the right answer which can be in-
ferred from the English translation.
The actual questions are created semi-
automatically from the dependency analysis of
the sentences. We defined a set of rules for genera-
tion of the questions on the basis of the dependency
relations. For example, if a sentence has only a
subject relation presented within the analysis, the
question will be about who is doing the event. If
the analysis presents subject and direct object, the
question will be about who is doing something with
what/whom. These automatically generated ques-
tions are manually investigated and, if necessary,
edited. Also, additional answers are formulated on
the basis of general language knowledge. The main
idea is that the possible answers are conceptually
close to each other, but not in a hypernymy relation.
Always there is an answer ?none?.
Then the questions are divided into small groups
and distributed to be answered by three evaluators
in such a way that each question is answered by two
evaluators, but no evaluator answers the whole set of
questions for a given sentence. In this way, we try
to minimize the influence of one question to the an-
swers of the next questions. The answers are com-
pared to the true answers of the questions for each
given sentence. We evaluated 192 questions for each
model and sum up the scores (correctly answered
questions) in Table 5.
This evaluation is more expensive, but we expect
them to be more objective. As for a related work,
(Yuret et al, 2010) used textual entailment to eval-
uate different parser outputs. The way they con-
structed the hypotheses is similar to our creation of
questions (based on dependency relations). How-
ever, they focused on the automatic evaluation and
we adopt it for the manual evaluation.
5 Conclusion and Future Work
In this paper, we report our work on building a
linguistically-enriched statistical machine transla-
tion model from Bulgarian to English. Based on our
observations of the previous approaches on transfer-
based MT models, we decide to build a factored
model by feeding an SMT system with deep lin-
ID Model Score
1 WF (Baseline) 127
2 WF, POS 126
3 WF, LEMMA, POS, LING 131
4 LEMMA 133
5 LEMMA, POS 133
6 LEMMA, POS, LING 128
7 WF, DEPREL 131
8 WF, DEPREL, HPOS 120
9 WF, LEMMA, POS, LING, DEPREL 124
10 WF, POS, EP 125
11 WF, EP, EOV 126
12 WF, POS, LING, EP, EOV 128
13 EP, EOV 138
14 EP, EOV, LING 122
15 EP, EOV, ARGnPOS 130
16 WF, EP, EOV, ARGnPOS 121
Table 5: Question-based evaluation
guistic features. We perform various experiments on
several configurations of the system (with different
linguistic knowledge). The high BLEU score shows
the high quality of the translation delivered by the
SMT baseline; and various manual analyses confirm
the consistency of the system.
There are various aspects of the current approach
we can improve: 1) The MRSes are not fully ex-
plored yet, although we have considered the most
important predicate and argument features. 2) We
would like to add factors on the target language side
(English) as well to fulfill a ?complete? transfer. 3)
Incorporating reordering rules on the Bulgarian side
may help the alignment and larger language mod-
els on the English side should also help improving
the translation results. 4) Due to the morphologi-
cal complexity of the Bulgarian language, the other
translation direction, from Bulgarian to English, is
also worth investigation in this framework.
Acknowledgements
This work was partially supported by the EuroMa-
trixPlus project (IST-231720) funded by the Euro-
pean Community?s Seventh Framework Programme.
The authors would like to thank Laska Laskova,
Stanislava Kancheva and Ivaylo Radev for doing the
human evaluation of the data.
17
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2), June.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. Ccg supertags in factored statistical machine
translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 9?16, Prague,
Czech Republic, June.
Francis Bond, Stephan Oepen, Melanie Siegel, Ann
Copestake, and Dan Flickinger. 2005. Open source
machine translation with DELPH-IN. In Proceedings
of the Open-Source Machine Translation Workshop at
the 10th Machine Translation Summit, pages 15 ? 22,
Phuket, Thailand, September.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the 6th Workshop on SMT.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL, pages 1443?
1452.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
Ann Copestake and Dan Flickinger. 2000. An open
source grammar development environment and broad-
coverage english grammar using hpsg. In Proceedings
of the 2nd International Conference on Language Re-
sources and Evaluation, Athens, Greece.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics: An in-
troduction. Research on Language & Computation,
3(4):281?332.
Ann Copestake. 2003. Robust minimal recursion seman-
tics (working paper).
Ann Copestake. 2007. Applying robust semantics. In
Proceedings of the 10th Conference of the Pacific As-
socation for Computational Linguistics (PACLING),
pages 1?12.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proceedings of HLT-NAACL, Boston, Massachusetts,
USA, May.
G. Georgiev, V. Zhikov, P. Osenova, K. Simov, and
P. Nakov. 2012. Feature-rich part-of-speech tagging
for morphologically complex languages: Application
to bulgarian. In EACL 2012.
Yvette Graham and Josef van Genabith. 2008. Packed
rules for automatic transfer-rule induction. In Pro-
ceedings of the European Association of Machine
Translation Conference (EAMT 2008), pages 57?65,
Hamburg, Germany, September.
Y. Graham, A. Bryl, and J. van Genabith. 2009. F-
structure transfer-based statistical machine translation.
In Proceedings of the Lexical Functional Grammar
Conference, pages 317?328, Cambridge, UK. CSLI
Publications, Stanford University, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL, Prague, Czech Republic, June.
Max Jakob, Marke?ta Lopatkova?, and Valia Kordoni.
2010. Mapping between dependency structures and
compositional semantic representations. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC 2010), pages 2491?
2497.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL (demo session).
P. Koehn, A. Birch, and R. Steinberger. 2009. 462 ma-
chine translation systems for europe. In Proceedings
of MT Summit XII.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, January.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
18
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, , and Victoria Rose?n.
2004. Som a? kapp-ete med trollet? towards MRS-
based norwegian to english machine translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, Victoria Rose?n, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine translation
? on linguistics and probabilities in MT. In Pro-
ceedings of the 11th Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-
07), Skovde, Sweden.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
Aleksandar Savkov, Laska Laskova, Stanislava
Kancheva, Petya Osenova, and Kiril Simov. 2012.
Linguistic processing pipeline for bulgarian. In
Proceedings of LREC, Istanbul, Turkey.
Thoudam Doren Singh and Sivaji Bandyopadhyay.
2010. Manipuri-english bidirectional statistical ma-
chine translation systems using morphology and de-
pendency relations. In Proceedings of the Fourth
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 83?91, Beijing, China, August.
Kathrin Spreyer and Anette Frank. 2005. Projecting
RMRS from TIGER Dependencies. In Proceedings of
the HPSG 2005 Conference, pages 354?363, Lisbon,
Portugal.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, volume 2.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Septem-
ber.
Deniz Yuret, Ayd?n Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the SemEval-2010
Evaluation Exercises on Semantic Evaluation.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proceedings of ACL-HLT, pages 559?567.
19
