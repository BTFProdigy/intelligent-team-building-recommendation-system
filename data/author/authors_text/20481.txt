Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 2?11,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Power-Law Distributions for Paraphrases Extracted from Bilingual
Corpora
Spyros Martzoukos Christof Monz
Informatics Institute, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{s.martzoukos, c.monz}@uva.nl
Abstract
We describe a novel method that extracts
paraphrases from a bitext, for both the
source and target languages. In order
to reduce the search space, we decom-
pose the phrase-table into sub-phrase-tables
and construct separate clusters for source
and target phrases. We convert the clus-
ters into graphs, add smoothing/syntactic-
information-carrier vertices, and compute
the similarity between phrases with a ran-
dom walk-based measure, the commute
time. The resulting phrase-paraphrase
probabilities are built upon the conversion
of the commute times into artificial co-
occurrence counts with a novel technique.
The co-occurrence count distribution be-
longs to the power-law family.
1 Introduction
Paraphrase extraction has emerged as an impor-
tant problem in NLP. Currently, there exists an
abundance of methods for extracting paraphrases
from monolingual, comparable and bilingual cor-
pora (Madnani and Dorr, 2010; Androutsopou-
los and Malakasiotis, 2010); we focus on the lat-
ter and specifically on the phrase-table that is ex-
tracted from a bitext during the training stage of
Statistical Machine Translation (SMT). Bannard
and Callison-Burch (2005) introduced the pivot-
ing approach, which relies on a 2-step transition
from a phrase, via its translations, to a paraphrase
candidate. By incorporating the syntactic struc-
ture of phrases (Callison-Burch, 2005), the qual-
ity of the paraphrases extracted with pivoting can
be improved. Kok and Brockett (2010) (hence-
forth KB) used a random walk framework to de-
termine the similarity between phrases, which
was shown to outperform pivoting with syntac-
tic information, when multiple phrase-tables are
used. In SMT, extracted paraphrases with asso-
ciated pivot-based (Callison-Burch et al 2006;
Onishi et al 2010) and cluster-based (Kuhn et
al., 2010) probabilities have been found to im-
prove the quality of translation. Pivoting has also
been employed in the extraction of syntactic para-
phrases, which are a mixture of phrases and non-
terminals (Zhao et al 2008; Ganitkevitch et al
2011).
We develop a method for extracting para-
phrases from a bitext for both the source and tar-
get languages. Emphasis is placed on the qual-
ity of the phrase-paraphrase probabilities as well
as on providing a stepping stone for extracting
syntactic paraphrases with equally reliable prob-
abilities. In line with previous work, our method
depends on the connectivity of the phrase-table,
but the resulting construction treats each side sep-
arately, which can potentially be benefited from
additional monolingual data.
The initial problem in harvesting paraphrases
from a phrase-table is the identification of the
search space. Previous work has relied on breadth
first search from the query phrase with a depth
of 2 (pivoting) and 6 (KB). The former can be
too restrictive and the latter can lead to excessive
noise contamination when taking shallow syntac-
tic information features into account. Instead, we
choose to cluster the phrase-table into separate
source and target clusters and in order to make this
task computationally feasible, we decompose the
phrase-table into sub-phrase-tables. We propose
a novel heuristic algorithm for the decomposition
of the phrase-table (Section 2.1), and use a well-
established co-clustering algorithm for clustering
2
each sub-phrase-table (Section 2.2).
The underlying connectivity of the source
and target clusters gives rise to a natural graph
representation for each cluster (Section 3.1).
The vertices of the graphs consist of phrases
and features with a dual smoothing/syntactic-
information-carrier role. The latter allow (a) re-
distribution of the mass for phrases with no appro-
priate paraphrases and (b) the extraction of syn-
tactic paraphrases. The proximity among vertices
of a graph is measured by means of a randomwalk
distance measure, the commute time (Aldous and
Fill, 2001). This measure is known to perform
well in identifying similar words on the graph of
WordNet (Rao et al 2008) and a related measure,
the hitting time is known to perform well in har-
vesting paraphrases on a graph constructed from
multiple phrase-tables (KB).
Generally in NLP, power-law distributions are
typically encountered in the collection of counts
during the training stage. The distances of Sec-
tion 3.1 are converted into artificial co-occurrence
counts with a novel technique (Section 3.2). Al-
though they need not be integers, the main chal-
lenge is the type of the underlying distributions;
it should ideally emulate the resulting count dis-
tributions from the phrase extraction stage of a
monolingual parallel corpus (Dolan et al 2004).
These counts give rise to the desired probability
distributions by means of relative frequencies.
2 Sub-phrase-tables & Clustering
2.1 Extracting Connected Components
For the decomposition of the phrase-table into
sub-phrase-tables it is convenient to view the
phrase-table as an undirected, unweighted graph
P with the vertex set being the source and target
phrases and the edge set being the phrase-table en-
tries. For the rest of this section, we do not distin-
guish between source and target phrases, i.e. both
types are treated equally as vertices of P . When
referring to the size of a graph, we mean the num-
ber of vertices it contains.
A trivial initial decomposition of P is achieved
by identifying all its connected components (com-
ponents for brevity), i.e. the mutually disjoint
connected subgraphs, {P0, P1, ..., Pn}. It turns
out (see Section 4.1) that the largest component,
say P0, is of significant size. We call P0 giant
and it needs to be further decomposed. This is
done by identifying all vertices such that, upon
removal, the component becomes disconnected.
Such vertices are called articulation points or cut-
vertices. Cut-vertices of high connectivity degree
are removed from the giant component (see Sec-
tion 4.1). For the remaining vertices of the giant
component, new components are identified and
we proceed iteratively, while keeping track of the
cut-vertices that are removed at each iteration, un-
til the size of the largest component is less than a
certain threshold ? (see Section 4.1).
Note that at each iteration, when removing cut-
vertices from a giant component, the resulting col-
lection of components may include graphs con-
sisting of a single vertex. We refer to such ver-
tices as residues. They are excluded from the re-
sulting collection and are considered for separate
treatment, as explained later in this section.
The cut-vertices need to be inserted appropri-
ately back to the components: Starting from the
last iteration step, the respective cut-vertices are
added to all the components of P0 which they
used to ?glue? together; this process is performed
iteratively, until there are no more cut-vertices to
add. By ?addition? of a cut-vertex to a component,
we mean the re-establishment of edges between
the former and other vertices of the latter. The
result is a collection of components whose total
number of unique vertices is less than the number
of vertices of the initial giant component P0.
These remaining vertices are the residues. We
then construct the graph R which consists of
the residues together with all their translations
(even those that are included in components of
the above collection) and then identify its compo-
nents {R0, ..., Rm}. It turns out, that the largest
component, say R0, is giant and we repeat the de-
composition process that was performed on P0.
This results in a new collection of components
as well as new residues: The components need
to be pruned (see Section 4.1) and the residues
give rise to a new graph R? which is constructed
in the same way asR. We proceed iteratively until
the number of residues stops changing. For each
remaining residue u, we identify its translations,
and for each translation v we identify the largest
component of which v is a member and add u to
that component.
The final result is a collection C = D ? F ,
where D is the collection of components emerg-
ing from the entire iterative decomposition of P0
3
and R, and F = {P1, ..., Pn}. Figure 1 shows
the decomposition of a connected graph G0; for
simplicity we assume that only one cut-vertex is
removed at each iteration and ties are resolved ar-
bitrarily. In Figure 2 the residue graph is con-
structed and its two components are identified.
The iterative insertion of the cut vertices is also
depicted. The resulting two components together
with those from R form the collection D for G0.
The addition of cut-vertices into multiple com-
ponents, as well as the construction method of the
residue-based graph R, can yield the occurrences
of a vertex in multiple components in D. We ex-
ploit this property in two ways:
(a) In order to mitigate the risk of excessive de-
composition (which implies greater risk of good
paraphrases being in different components), as
well as to reduce the size of D, a conserva-
tive merging algorithm of components is em-
ployed. Suppose that the elements of D are
ranked according to size in ascending order as
D = {D1, ..., Dk, Dk+1, ..., D|D|}, where |Di| ?
?, for i = 1, ..., k, and some threshold ? (see Sec-
tion 4.1). Each component Di with i ? {1, ..., k}
is examined as follows: For each vertex of Di the
number of its occurrences inD is inspected; this is
done in order to identify an appropriate vertex b to
act as a bridge between Di and other components
of which b is a member. Note that translations of
a vertex b with smaller number of occurrences in
D are less likely to capture their full spectrum of
paraphrases. We thus choose a vertex b from Di
with the smallest number of occurrences in D ,
resolving ties arbitrarily, and proceed with merg-
ing Di with the largest component, say Dj with
j ? {1, ..., |D| ? 1}, of which b is also a member.
The resulting merged component Dj? contains all
vertices and edges of Di and Dj and new edges,
which are formed according to the rule: if u is a
vertex of Di and v is a vertex of Dj and (u, v) is
a phrase-table entry, then (u, v) is an edge in Dj? .
As long as no connected component has identi-
fied Di as the component with which it should be
merged, then Di is deleted from the collection D.
(b) We define an idf -inspired measure for each
phrase pair (x, x?) of the same type (source or tar-
get) as
idf(x, x?) =
1
log |D|
log
(
2c(x, x?)|D|
c(x) + c(x?)
)
, (1)
where c(x, x?) is the number of components in
which the phrases x and x? co-occur, and equiv-
alently for c(?). The purpose of this measure is
for pruning paraphrase candidates and its use is
explained in Section 3.1. Note that idf(x, x?) ?
[0, 1].
The merging process and the idf measure are
irrelevant for phrases belonging to the compo-
nents of F , since the vertex set of each compo-
nent of F is mutually disjoint with the vertex set
of any other component in C.
  
G0 s1s2s3s4 t1t 2t3 c0={s2 } G11r={t 2 }
s1s4 t1 G12s3s4G12 G21s3 t 4c1={t3} r? r?{s4 }
t 4 s3 t3t 4t3t 4
Figure 1: The decomposition of G0 with vertices
si and tj : The cut-vertex of the ith iteration is de-
noted by ci, and r collects the residues after each
iteration. The task is completed in Figure 2.
  
G s0s1 t 0t2 s0s1 t 0t2s2 t 1 =c3 s3 t3 =c4=c4s2 t 1t2 s2 t2t 1s3s0 t3 s0s2 t2t 1
Figure 2: Top: Residue graph with its components
(no further decomposition is required). Bottom:
Adding cut-vertices back to their components.
2.2 Clustering Connected Components
The aim of this subsection is to generate sep-
arate clusters for the source and target phrases
of each sub-phrase-table (component) C ? C.
For this purpose the Information-Theoretic Co-
Clustering (ITC) algorithm (Dhillon et al 2003)
is employed, which is a general principled cluster-
ing algorithm that generates hard clusters (i.e. ev-
4
ery element belongs to exactly one cluster) of two
interdependent quantities and is known to per-
form well on high-dimensional and sparse data.
In our case, the interdependent quantities are the
source and target phrases and the sparse data is
the phrase-table.
ITC is a search algorithm similar to K-means,
in the sense that a cost function, is minimized at
each iteration step and the number of clusters for
both quantities are meta-parameters. The number
of clusters is set to the most conservative initial-
ization for both source and target phrases, namely
to as many clusters as there are phrases. At each
iteration, new clusters are constructed based on
the identification of the argmin of the cost func-
tion for each phrase, which gradually reduces the
number of clusters.
We observe that conservative choices for the
meta-parameters often result in good paraphrases
being in different clusters. To overcome this prob-
lem, the hard clusters are converted into soft (i.e.
an element may belong to several clusters): One
step before the stopping criterion is met, we mod-
ify the algorithm so that instead of assigning a
phrase to the cluster with the smallest cost we se-
lect the bottom-X clusters ranked by cost. Addi-
tionally, only a certain number of phrases is cho-
sen for soft clustering. Both selections are done
conservatively with criteria based on the proper-
ties of the cost functions.
The formation of clusters leads to a natural re-
finement of the idf measure defined in eqn. (1):
The quantity c(x, x?) is redefined as the number
of components in which the phrases x and x? co-
occur in at least one cluster.
3 Monolingual Graphs & Counts
We proceed with converting the clusters into di-
rected, weighted graphs and then extract para-
phrases for both the source and target side. For
brevity we explain the process restricted to the
source clusters of a sub-phrase-table, but the same
method applies for the target side and for all sub-
phrase-tables in the collection C.
3.1 Monolingual graphs
Each source cluster is converted into a graph G as
follows: The vertex set consists of the phrases of
the cluster and an edge between s and s? exists, if
(a) s and s? have at least one translation from the
same target cluster, and (b) idf(s, s?) is greater
than some threshold ? (see Section 4.1). If two
phrases that satisfy condition (b) and have trans-
lations in more than one common target cluster,
a distinct such edge is established. All edges are
bi-directional with distinct weights for both direc-
tions.
Figure 3 depicts an example of such a construc-
tion; a link between a phrase si and a target cluster
implies the existence of at least one translation for
si in that cluster. We are not interested in the tar-
get phrases and they are thus not shown. For sim-
plicity we assume that condition (b) is always sat-
isfied and the extracted graph contains the maxi-
mum possible edges. Observe that phrases s3 and
s4 have two edges connecting them, (due to tar-
get clusters Tc and Td) and that the target cluster
Ta is irrelevant to the construction of the graph,
since s1 is the only phrase with translations in it.
This conversion of a source cluster into a graph G
  
s1 s2 s4 s5s3 s8s7s6Ta Tb Tc Td Te Tf
s2
s1 s3 s4s5 s6
s7 s8
Figure 3: Top: A source cluster containing
phrases s1,..., s8 and the associated target clusters
Ta,..., Tf . Bottom: The extracted graph from the
source cluster. All edges are bi-directional.
results in the formation of subgraphs in G, where
each subgraph is generated by a target cluster. In
general, if condition (b) is not always satisfied,
then G need not be connected and each connected
component is treated as a distinct graph.
Analogous to KB, we introduce feature vertices
to G: For each phrase vertex s, its part-of-speech
(POS) tag sequence and stem sequence are iden-
tified and inserted into G as new vertices with
bi-directional weighted edges connected to s. If
phrase vertices s and s? have the same POS tag se-
quence, then they are connected to the same POS
tag feature vertex. Similarly for stem feature ver-
tices. See Figure 4 for an example. Note that we
do not allow edges between POS tag and stem fea-
5
  
s124534
876
Tab Tabcd8e
f?53??
??cd8e
????87?
f?53?
???87?
Figure 4: Adding feature vertices to the extracted
graph (has) ?? (owns) ?? (i have) ?? (i had).
Phrase, POS tag feature and stem feature ver-
tices are drawn in circles, dotted rectangles and
solid rectangles respectively. All edges are bi-
directional.
ture vertices. The purpose of the feature vertices,
unlike KB, is primarily for smoothing and secon-
darily for identifying paraphrases with the same
syntactic information and this will become clear
in the description of the computation of weights.
The set of all phrase vertices that are adja-
cent to s is written as ?(s), and referred to
as the neighborhood of s. Let n(s, t) denote
the co-occurrence count of a phrase-table entry
(s, t) (Koehn, 2009). We define the strength of
s in the subgraph generated by cluster T as
n(s;T ) =
?
t?T
n(s, t), (2)
which is simply a partial occurrence count for s.
We proceed with computing weights for all edges
of G:
Phrase??phrase weights: Inspired by the
notion of preferential attachment (Yule, 1925),
which is known to produce power-law weight dis-
tributions for evolving weighted networks (Barrat
et al 2004), we set the weight of a directed
edge from s to s? to be proportional to the
strengths of s? in all subgraphs in which both
s and s? are members. Thus, in the random
walk framework, s is more likely to visit
a stronger (more reliable) neighbor. If Ts,s? =
{T |s and s? coexist in subgraph generated by T},
then the weight w(s ? s?) of the directed edge
from s to s? is given by
w(s ? s?) =
?
T?Ts,s?
n(s?;T ), (3)
if s? ? ?(s) and 0 otherwise.
Phrase??feature weights: As mentioned
above, feature vertices have the dual role of car-
rying syntactic information and smoothing. From
eqn. (3) it can be deduced that, if for a phrase
s, the amount of its outgoing weights is close to
the amount of its incoming weights, then this is
an indication that at least a significant part of its
neighborhood is reliable; the larger the strengths,
the more certain the indication. Otherwise, either
s or a significant part of its neighborhood is
unreliable. The amount of weight from s to its
feature vertices should depend on this observation
and we thus let
net(s) =
?
?
?
?
?
?
?
s???(s)
(w(s ? s?)? w(s? ? s))
?
?
?
?
?
?
+ ,
(4)
where  prevents net(s) from becoming 0 (see
Section 4.1). The net weight of a phrase vertex
s is distributed over its feature vertices as
w(s ? fX) =< w(s ? s
?) > +net(s), (5)
where the first summand is the average weight
from s to its neighboring phrase vertices and
X = POS,STEM. If s has multiple POS tag
sequences, we distribute the weight of eqn. (5)
relatively to the co-occurrences of s with the re-
spective POS tag feature vertices. The quantity
< w(s ? s?) > accounts for the basic smoothing
and is augmented by a value net(s) that measures
the reliability of s?s neighborhood; the more unre-
liable the neighborhood, the larger the net weight
and thus larger the overall weights to the feature
vertices.
The choice for the opposite direction is trivial:
w(fX ? s) =
1
|{s? : (fX , s?) is an edge }|
, (6)
where X = POS,STEM. Note the effect of
eqns. (4)?(6) in the case where the neighborhood
of s has unreliable strengths: In a random walk
the feature vertices of s will be preferred and the
resulting similarities between s and other phrase
vertices will be small, as desired. Nonetheless,
if the syntactic information is the same with any
other phrase vertex inG, then the paraphrases will
be captured.
The transition probability from any vertex u to
any other vertex v in G, i.e., the probability of
6
hopping from u to v in one step, is given by
p(u ? v) =
w(u ? v)
?
v? w(u ? v
?)
, (7)
where we sum over all vertices adjacent to u inG.
We can thus compute the similarity between any
two vertices u and v in G by their commute time,
i.e., the expected number of steps in a round trip,
in a random walk from u to v and then back to u,
which is denoted by ?(u, v) (see Section 4.1 for
the method of computation of ?). Since ?(u, v) is
a distance measure, the smaller its value, the more
similar u and v are.
3.2 Counts
We convert the distance ?(u, v) of a vertex pair
u, v in a graph G into a co-occurrence count
nG(u, v) with a novel technique: In order to as-
sess the quality of the pair u, v with respect to G
we compare ?(u, v) with ?(u, x) and ?(v, x) for
all other vertices x in G. We thus consider the av-
erage distance of u with the other vertices of G
other than v, and similarly for v. This quantity is
denoted by ?(u; v) and ?(v;u) respectively, and
by definition it is given by
?(i; j) =
?
x?G
x 6=j
?(i, x)pG(x|i) (8)
where pG(x|i) ? p(x|G, i) is a yet unknown
probability distribution with respect to G. The
quantity (?(u; v)+?(v;u))/2 can then be viewed
as the average distance of the pair u, v to the rest
of the graph G. The co-occurrence count of u and
v in G is thus defined by
nG(u, v) =
?(u; v) + ?(v;u)
2?(u, v)
. (9)
In order to calculate the probabilities pG(?|?) we
employ the following heuristic: Starting with a
uniform distribution p(0)G (?|?) at timestep t = 0,
we iterate
?(t)(i; j) =
?
x?G
x 6=j
?(i, x)p(t)G (x|i) (10)
n(t)G (u, v) =
?(t)(u; v) + ?(t)(v;u)
2?(u, v)
(11)
p(t+1)G (v|u) =
n(t)G (u, v)
?
x?G n
(t)
G (u, v)
(12)
for all pairs of vertices u, v in G until conver-
gence. Experimentally, we find that convergence
is always achieved. After the execution of this it-
erative process we divide each count by the small-
est count in order to achieve a lower bound of 1.
A pair u, v may appear in multiple graphs in the
same sub-phrase-tableC. The total co-occurrence
count of u and v in C and the associated condi-
tional probabilities are thus given by
nC(u, v) =
?
G?C
nG(u, v) (13)
pC(v|u) =
nC(u, v)
?
x?C nC(u, x)
. (14)
A pair u, v may appear in multiple sub-phrase-
tables and for the calculation of the final count
n(u, v) we need to average over the associated
counts from all sub-phrase-tables. Moreover, we
have to take into account the type of the vertices:
For the simplest case where both u and v repre-
sent phrase vertices, their expected count is, by
definition, given by
n(s, s?) =
?
C
nC(s, s
?)p(C|s, s?). (15)
On the other hand, if at least one of u or v is
a feature vertex, then we have to consider the
phrase vertex that generates this feature: Suppose
that u is the phrase vertex s=?acquire? and v the
POS tag vertex f=?NN? and they co-occur in two
sub-phrase-tables C and C ? with positive counts
nC(s, f) and nC?(s, f) respectively; the feature
vertex f is generated by the phrase vertices ?own-
ership? in C and by ?possession? in C ?. In that
case, an interpolation of the counts nC(s, f) and
nC?(s, f) as in eqn. (15) would be incorrect and
a direct sum nC(s, f) + nC?(s, f) would provide
the true count. As a result we have
n(s, f) =
?
s?
?
C
nC(s, f(s
?))p(C|s, f(s?)),
(16)
where the first summation is over all phrase ver-
tices s? such that f(s?) = f . With a similar argu-
ment we can write
n(f, f ?) =
?
s,s?
?
C
nC(f(s), f(s
?))?
? p(C|f(s), f(s?)). (17)
7
For the interpolants, from standard probability we
find
p(C|u, v) =
pC(v|u)p(C|u)
?
C? pC?(v|u)p(C
?|u)
, (18)
where the probabilities p(C|u) can be computed
by considering the likelihood function
`(u) =
N?
i=1
p(xi|u) =
N?
i=1
?
C
pC(xi|u)p(C|u)
and by maximizing the average log-likelihood
1
N log `(u), where N is the total number of ver-
tices with which u co-occurs with positive counts
in all sub-phrase-tables.
Finally, the desired probability distributions are
given by the relative frequencies
p(v|u) =
n(u, v)
?
x n(u, x)
, (19)
for all pairs of vertices u, v.
4 Experiments
4.1 Setup
The data for building the phrase-table P
is drawn from DE-EN bitexts crawled from
www.project-syndicate.org, which is
a standard resource provider for the WMT
campaigns (News Commentary bitexts, see,
e.g. (Callison-Burch et al 2007) ). The filtered
bitext consists of 125K sentences; word align-
ment was performed running GIZA++ in both di-
rections and generating the symmetric alignments
using the ?grow-diag-final-and? heuristics. The
resulting P has 7.7M entries, 30% of which are
?1-1?, i.e. entries (s, t) that satisfy p(s|t) =
p(t|s) = 1. These entries are irrelevant for para-
phrase harvesting for both the baseline and our
method, and are thus excluded from the process.
The initial giant component P0 contains 1.7M
vertices (Figure 5), of which 30% become
residues and are used to construct R. At each it-
eration of the decomposition of a giant compo-
nent, we remove the top 0.5% ? size cut-vertices
ranked by degree of connectivity, where size is
the number of vertices of the giant component and
set ? = 2500 as the stopping criterion. The latter
choice is appropriate for the subsequent step of
co-clustering the components, for both time com-
plexity and performance of the ITC algorithm.
100 102 104 106100
101102
103104
105106
107
rank
size 100 102 104 106100
105P0
Figure 5: Log-log plot of ranked components ac-
cording to their size (number of source and target
phrases) for: (a) Components extracted from P .
?1-1? components are not shown. (b) Components
extracted from the decomposition of P0.
In the components emerging from the decompo-
sition of R0, we observe an excessive number
of cut-vertices. Note that vertices that consist
these components can be of two types: i) for-
mer residues, i.e., residues that emerged from the
decomposition of P0, and ii) other vertices of
P0. Cut-vertices can be of either type. For each
component, we remove cut-vertices that are not
translations of the former residues of that com-
ponent. Following this pruning strategy, the de-
generacy of excessive cut-vertices does not reap-
pear in the subsequent iterations of decompos-
ing components generated by new residues, but
the emergence of two giant components was ob-
served: One consisting mostly of source type ver-
tices and one of target type vertices. Without go-
ing into further details, the algorithm can extend
to multiple giant components straightforwardly.
For the merging process of the collection D we
set ? = 5000, to avoid the emergence of a giant
component. The sizes of the resulting sub-phrase-
tables are shown in Figure 6. For the ITC algo-
rithm we use the smoothing technique discussed
in (Dhillon and Guan, 2003) with ? = 106.
For the monolingual graphs, we set ? = 0.65
and discard graphs with more than 20 phrase ver-
tices, as they contain mostly noise. Thus, the sizes
of the graphs allow us to use analytical methods
to compute the commute times: For a graph G,
we form the transition matrix P , whose entries
P (u, v) are given by eqn. (7), and the fundamen-
8
100 102 104 106100
101102
103104
105106
rank
size
 
 
before mergingafter merging
Figure 6: Log-log plot of ranked sub-phrase-
tables according to their size (number of source
and target phrases).
tal matrix (Grinstead and Snell, 2006; Boley et al
2011) Z = (I?P +1piT )?1, where I is the iden-
tity matrix, 1 denotes the vector of all ones and pi
is the vector of stationary probabilities (Aldous
and Fill, 2001) which is such that piTP = piT
and piT1 = 1 and can be computed as in (Hunter,
2000). The commute time between any vertices u
and v in G is then given by (Grinstead and Snell,
2006)
?(u, v) = (Z(v, v)? Z(u, v))/pi(v) +
+ (Z(u, u)? Z(v, u))/pi(u). (20)
For the parameter of eqn. (4), an appropriate
choice is  = |?(s)| + 1; for reliable neighbor-
hoods, this quantity is insignificant. POS tags and
lemmata are generated with TreeTagger1.
Figure 7 depicts the most basic type of graph
that can be extracted from a cluster; it includes
two source phrase vertices a, b, of different syn-
tactic information. Suppose that both a and
b are highly reliable with strengths n(a;T ) =
n(b;T ) = 40, for some target cluster T . The re-
sulting conditional probabilities adequately repre-
sent the proximity of the involved vertices. On
the other hand, the range of the co-occurrence
counts is not compatible with that of the strengths.
This is because i) there are no phrase vertices with
small strength in the graph, and ii) eqn. (9) is es-
sentially a comparison between a pair of vertices
and the rest of the graph. To overcome this prob-
lem inflation vertices ia and ib of strength 1 with
accompanying feature vertices are introduced to
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
the graph. Figure 8 depicts the new graph, where
the lengths of the edges represent the magnitude
of commute times. Observe that the quality of
the probabilities is preserved but the counts are
inflated, as required.
In general, if a source phrase vertex s has at
least one translation t such that n(s, t) ? 3, then a
triplet (is, f(is), g(is)) is added to the graph as in
Figure 8. The inflation vertex is establishes edges
with all other phrase and inflation vertices in the
graph and weights are computed as in Section 3.1.
The pipeline remains the same up to eqn. (13),
where all counts that include inflation vertices are
ignored.
  
G st =G{c=G{
t =s {
c=s {r =s}G{ 0 123r =t =G{}G{ 0 124r=c =G{}G{ 0 124r =t =s {}G{ 0 1?r=c=s {}G{ 0 1?
? =G? s { 0 213? =G? t =G{{ 0 21?? =G? c=G{{ 0 21?? =G? t =s {{ 0 ??? =G? c =s {{ 0 ??
Figure 7: Top: A graph with source phrase ver-
tices a and b, both of strength 40, with accom-
panying distinct POS sequence vertices f(?) and
stem sequence vertices g(?). Bottom: The result-
ing co-occurrence counts and conditional proba-
bilities for a.
  
G=s{t } 0 122G=c=t }{t } 0 123G=r =t }{t } 0 123G=c=s }{t } 0 14?G=r =s }{t } 0 14?
tsc=t }c=s }
r =t }
r =s }
? t? s c=? t }
r =? t }
c=? s }r =? s }? =t ? s } 0 441?? =t ? c=t }} 0 4?1?? =t ? r =t }} 0 4?1?? =t ? c=s }} 0 31?? =t ? r =s }} 0 31?
Figure 8: The inflated version of Figure 7.
9
4.2 Results
Our method generates conditional probabilities
for any pair chosen from {phrase, POS sequence,
stem sequence}, but for this evaluation we restrict
ourselves to phrase pairs. For a phrase s, the qual-
ity of a paraphrase s? is assessed by
P (s?|s) ? p(s?|s) + p(f1(s
?)|s) + p(f2(s
?)|s),
(21)
where f1(s?) and f2(s?) denote the POS tag se-
quence and stem sequence of s? respectively. All
three summands of eqn. (21) are computed from
eqn. (19). The baseline is given by pivoting (Ban-
nard and Callison-Burch, 2005),
P (s?|s) =
?
t
p(t|s)p(s?|t), (22)
where p(t|s) and p(s?|t) are the phrase-based rel-
ative frequencies of the translation model.
We select 150 phrases (an equal number for
unigrams, bigrams and trigrams), for which we
expect to see paraphrases, and keep the top-10
paraphrases for each phrase, ranked by the above
measures. We follow (Kok and Brockett, 2010;
Metzler et al 2011) in the evaluation of the ex-
tracted paraphrases: Each phrase-paraphrase pair
is manually annotated with the following options:
0) Different meaning; 1) (i) Same meaning, but
potential replacement of the phrase with the para-
phrase in a sentence ruins the grammatical struc-
ture of the sentence. (ii) Tokens of the paraphrase
are morphological inflections of the phrase?s to-
kens. 2) Samemeaning. Although useful for SMT
purposes, ?super/substrings of? are annotated with
0 to achieve an objective evaluation.
Both methods are evaluated in terms of the
Mean Expected Precision (MEP) at k; the Ex-
pected Precision for each selected phrase s at
rank k is computed by Es[p@k] = 1k
?k
i=1 pi,
where pi is the proportion of positive annotations
for item i. The desired metric is thus given by
MEP@k = 1150
?
s Es[p@k]. The contribution
to pi can be restricted to perfect paraphrases only,
which leads to a strict strategy for harvesting para-
phrases. Table 1 summarizes the results of our
evaluation and
we deduce that our method can lead to improve-
ments over the baseline.
An important accomplishment of our method
is that the distribution of counts n(u, v), (as given
Method
Lenient MEP Strict MEP
@1 @5 @10 @1 @5 @10
Baseline .58 .47 .41 .43 .33 .28
Graphs .72 .61 .52 .53 .40 .33
Table 1: Mean Expected Precision (MEP) at k un-
der lenient and strict evaluation criteria.
by eqns. (15)?(17)) for all vertices u and v, be-
longs to the power-law family (Figure 9). This is
evidence that the monolingual graphs can simu-
late the phrase extraction process of a monolin-
gual parallel corpus. Intuitively, we may think of
the German side of the DE?EN parallel corpus as
the ?English? approximation to a ?EN??EN par-
allel corpus, and the monolingual graphs as the
word alignment process.
100 102 104 106 108100
101
102
103
104
105
rank
co?o
ccur
renc
e co
unt
Figure 9: Log-log plot of ranked pairs of English
vertices according to their counts
5 Conclusions & Future Work
We have described a new method that harvests
paraphrases from a bitext, generates artificial
co-occurrence counts for any pair chosen from
{phrase, POS sequence, stem sequence}, and po-
tentially identifies patterns for the syntactic infor-
mation of the phrases. The quality of the para-
phrases? ranked lists outperforms that of a stan-
dard baseline. The quality of the resulting condi-
tional probabilities is promising and will be eval-
uated implicitly via an application to SMT.
This research was funded by the European
Commission through the CoSyne project FP7-
ICT- 4-248531.
10
References
David Aldous and James A. Fill. 2001. Reversible
Markov Chains and Random Walks on Graphs.
http://www.stat.berkeley.edu/?aldous/RWG/
book.html
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proc.
ACL, pp. 597?604.
Alain Barrat, Marc Barthlemy, and Alessandro Vespig-
nani. 2004. Modeling the Evolution of Weighted
Networks. Phys. Rev. Lett., 92.
Daniel Boley, Gyan Ranjan, and Zhi-Li Zhang. 2011.
Commute Times for a Directed Graph using an
Asymmetric Laplacian. Linear Algebra and its Ap-
plications, Issue 2, pp. 224?242.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora.
Proc. EMNLP, pp. 196?205.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007
(Meta-) Evaluation of Machine Translation. Proc.
Workshop on Statistical Machine Translation, pp.
136?158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006 Improved statistical machine trans-
lation using paraphrases. Proc. HLT/NAACL, pp.
17?24.
Inderjit S. Dhillon and Yuqiang Guan. 2003. Informa-
tion Theoretic Clustering of Sparse Co-Occurrence
Data. Proc. IEEE Int?l Conf. Data Mining, pp. 517?
520.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-Theoretic
Coclustering. Proc. ACM SIGKDD Int?l Conf.
Knowledge Discovery and Data Mining, pp. 89?98.
William Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised construction of large para-
phrase corpora: Exploiting massively parallel news
sources. Proc. COLING, pp. 350-356.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme 2011. Learn-
ing Sentential Paraphrases from Bilingual Paral-
lel Corpora for Text-to-Text Generation. Proc.
EMNLP, pp. 1168?1179.
Charles Grinstead and Laurie Snell. 2006. Introduc-
tion to Probability. Second ed., American Mathe-
matical Society.
Jeffrey J. Hunter. 2000. A Survey of Generalized In-
verses and their Use in Stochastic Modelling. Res.
Lett. Inf. Math. Sci., Vol. 1, pp. 25?36.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Stanley Kok and Chris Brockett. 2010. Hitting the
Right Paraphrases in Good Time. Proc. NAACL,
pp.145?153.
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase Clustering for Smoothing
TM Probabilities: or, how to Extract Paraphrases
from Phrase Tables. Proc. COLING, pp.608?616.
Nitin Madnani and Bonnie Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of
Data-Driven Methods. Computational Linguistics,
36(3):341?388.
Donald Metzler, Eduard Hovy, and Chunliang
Zhang. 2011. An Empirical Evaluation of Data-
Driven Paraphrase Generation Techniques. Proc.
ACL:Short Papers, pp. 546?551.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. Proc. ACL:Short Papers, pp. 1?5.
Delip Rao, David Yarowsky, and Chris Callison-
Burch. 2008. Affinity Measures based on the Graph
Laplacian. Proc. Textgraphs Workshop on Graph-
based Algorithms for NLP at COLING, pp. 41?48.
George U. Yule. 1925. A Mathematical Theory of
Evolution, based on the Conclusions of Dr. J. C.
Willis, F.R.S. Philos. Trans. R. Soc. London, B 213,
pp. 21?87.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. Proc. ACL, pp.
780?788.
11
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?38,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Maximizing Component Quality in Bilingual Word-Aligned Segmentations
Spyros Martzoukos Christophe Costa Flor
?
encio Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.CostaFlorencio, C.Monz}@uva.nl
Abstract
Given a pair of source and target language
sentences which are translations of each other
with known word alignments between them,
we extract bilingual phrase-level segmenta-
tions of such a pair. This is done by identi-
fying two appropriate measures that assess the
quality of phrase segments, one on the mono-
lingual level for both language sides, and one
on the bilingual level. The monolingual mea-
sure is based on the notion of partition refine-
ments and the bilingual measure is based on
structural properties of the graph that repre-
sents phrase segments and word alignments.
These two measures are incorporated in a ba-
sic adaptation of the Cross-Entropy method
for the purpose of extracting an N -best list
of bilingual phrase-level segmentations. A
straight-forward application of such lists in
Statistical Machine Translation (SMT) yields
a conservative phrase pair extraction method
that reduces phrase-table sizes by 90% with
insignificant loss in translation quality.
1 Introduction
Given a pair of source and target language sen-
tences which are translations of each other with
known word alignments between them, the problem
of extracting high quality bilingual phrase segmen-
tations is defined as follows: Maximize the quality
of phrase segments, i.e., groupings of consecutive
words, in both language sides, subject to constraints
imposed by the underlying word alignments. The
purpose of this work is to provide a solution to this
maximization problem and investigate the effect of
the resulting high quality bilingual phrase segments
on SMT. For brevity, ?phrase-level sentence segmen-
tation? and ?phrase segment? will henceforth be sim-
ply referred to as ?segmentation? and ?segment? re-
spectively.
The exact definition of segments? quality depends
on the application. Our notion of a segmentation of
maximum quality is defined as the set of consecutive
words of the sentence that captures maximum col-
locational and/or grammatical characteristics. This
implies that a sequence of tokens is identified as a
segment if its fully compositional expressive power
is higher than the expressive power of any combina-
tion of partial compositions. Since this definition is
fairly general it is thus suitable for most NLP tasks.
In particular, it is tailored to the type of segments
that are suitable for the purposes of SMT and is in
line with previous work (Blackwood et al., 2008;
Paul et al., 2010).
With this definition in mind, we introduce a
monolingual segment quality measure that is based
on assessing the cost of converting one segmentation
into another by means of an elementary operation.
This operation, namely the ?splitting? of a segment
into two segments, together with all possible seg-
mentations of a sentence are known to form a par-
tially ordered set (Guo, 1997). Such a construction
is known as partition refinement and gives rise to the
desired monolingual surface quality measure.
The presence of word alignments between the
sentence pair provides additional structure which
should not be ignored. In the language of graph the-
ory, a segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment?s words and
30
an edge between two words exists if and only if these
words are consecutive. Then, a bilingual segmenta-
tion is represented by the graph that is formed by all
its source and target language chains together with
edges induced by word alignments. Motivated by
the phrase pair extraction methods of SMT (Och et
al., 1999; Koehn et al., 2003), we focus on the con-
nected components, or simply components of such a
representation. We explain that the extent to which
we can delete word alignments from a component
without violating its component status, gives rise to
a bilingual, purely structural quality measure.
The surface and structural measures are incorpo-
rated in one algorithm that extracts an N -best list
of bilingual word-aligned segmentations. This algo-
rithm, which is an adaptation of the Cross-Entropy
method (Rubinstein, 1997), performs joint maxi-
mization of surface (in both languages) and struc-
tural quality measures. Components of graph repre-
sentations of the resulting N -best lists give rise to
high quality translation units. These units, which
form a small subset of all possible (continuous) con-
sistent phrase pairs, are used to construct SMT mod-
els. Results on Czech?English and German?English
datasets show a 90% reduction in phrase-table sizes
with insignificant loss in translation quality which
are in line with other pruning techniques in SMT
(Johnson et al., 2007; Zens et al., 2012).
2 Monolingual Surface Quality Measure
Given a sentence s
1
s
2
...s
k
that consists of words
s
i
, 1 ? i ? k, we introduce an empirical count-
based measure that assesses the quality of its seg-
mentations. By fixing a segmentation ?, we are in-
terested in assessing the cost of perturbing ? and
generating another segmentation ?
?
. A perturbation
of ? is achieved by splitting a segment of ? into
two new segments, while keeping all other segments
fixed. For example, for a sentence with five words, if
? : (s
1
s
2
)(s
3
s
4
s
5
), where brackets are used to dis-
tinguish the segments s
1
s
2
and s
3
s
4
s
5
, then ? can
be perturbed in three different ways:
? ?
?
: (s
1
)(s
2
)(s
3
s
4
s
5
), by splitting the first seg-
ment of ?.
? ?
??
: (s
1
s
2
)(s
3
)(s
4
s
5
), by splitting at the first
position of the second segment of ?.
? ?
???
: (s
1
s
2
)(s
3
s
4
)(s
5
), by splitting at the sec-
ond position of the second segment of ?,
so that ?
?
, ?
??
and ?
???
are the perturbations of ?.
Such perturbations are known as partition refine-
ments in the literature (Stanley, 1997). The set of all
segmentations of a sentence, equipped with the split-
ting operation forms a partially ordered set (Guo,
1997), and its visual representation is known as the
Hasse diagram. Figure 1 shows such a partially or-
dered set for a sentence with four words.
  
?s1 s2 s3 s4??s1??s2 s3 s4? ? s1 s2?? s3 s4? ?s1 s2 s3??s4?
?s1??s2?? s3 s4? ?s1??s2 s3?? s4? ?s1 s2??s3?? s4??s1??s2?? s3??s4?
Figure 1: Hasse diagram of segmentation refine-
ments for a sentence with four words.
The cost of perturbing a segmentation into an-
other, i.e., the weight of a directed edge in the Hasse
diagram, is calculated from n-gram counts that are
extracted from a monolingual training corpus. Let
n(s) be the empirical count of phrase s in the corpus.
Given a segmentation ? of a sentence, let seg(?) de-
note the set of ??s segments. In the above example
we have for instance seg(?
??
) = {s
1
s
2
, s
3
, s
4
s
5
}.
The probability of s in ? is given by relative fre-
quencies
p
?
(s) =
n(s)
?
s
?
?seg(?)
n(s
?
)
. (1)
The cost of perturbing ? into ?
?
by splitting a seg-
ment ss? of ? into segments s and s? is defined by
cost
???
?
(s, s?) = log
p
?
(ss?)
p
?
?
(s)p
?
?
(s?)
, (2)
and we say that s and s? are co-responsible for the
perturbation ? ? ?
?
. Intuitively, this cost function
yields the amount of energy (log of probability) that
is lost when performing a perturbation. On a more
31
technical level, it is closely related to metric spaces
on partially ordered sets (Monjardet, 1981; Orum
and Joslyn, 2009), but we do not go into further de-
tails here.
The cost function admits a measure for the seg-
ments that are co-responsible for perturbing ? into
?
?
and we define the gain of s from the perturbation
? ? ?
?
as
gain
???
?
(s) = ?cost
???
?
(s, s?). (3)
A segment smay be co-responsible for different per-
turbations, and we have to consider all such pertur-
bations. Let
R(s) = {? ? ?
?
: s /? seg(?), s ? seg(?
?
)} (4)
denote the set of perturbations for which s is co-
responsible. Then, the average gain of s in the sen-
tence is given by
gain(s) =
1
|R(s)|
?
{???
?
}?R(s)
gain
???
?
(s). (5)
Intuitively, gain(s) measures how difficult it is to
break phrase s into sub-phrases. Finally, the surface
quality measure of a segmentation ? of a sentence is
given by
g(?) =
?
s?seg(?)
gain(s). (6)
Note that g is a real number. The relation g(?) >
g(?
?
) implies that ? is a better segmentation than ?
?
.
We conclude this section with two remarks: (i)
The exact computation of gain(s) for each possi-
ble segment s is computationally expensive since
all perturbations need to be considered. In prac-
tice we can simply generate a random sample of no
more than 1500 segmentations and compute gain(?)
based on that sample only. (ii) Each sentence of
the monolingual training corpus (from which the n-
gram counts are extracted) should have the begin-
ning and end-of-sentence tokens. The count for each
of them is equal to the number of sentences in the
corpus, and they are treated as regular words. With-
out going into further details they provide the pur-
pose of normalization.
3 Bilingual Structural Quality Measure
Given a word-aligned sentence pair, we introduce a
purely structural measure that assesses the quality of
its bilingual segmentations. By ?purely structural?
we mean that the focus is entirely on combinatorial
aspects of the bilingual segmentations and the word
alignments. For that reason we turn to a graph theo-
retic framework.
A segment can also be viewed as a chain, i.e., a
graph in which vertices are the segment?s words and
an edge between two words exists if and only if these
words are consecutive. Then, a source segmentation
? and a target segmentation ? are graphs that con-
sist of source chains and target chains respectively.
The graph formed by ?, ? and the translation edges
induced by word alignments is thus a graph repre-
sentation of a bilingual word-aligned segmentation.
We focus on a particular type of subgraphs of this
representation, namely its connected components, or
simply components. A component is a graph such
that (a) there exists a path between any two of its
vertices, and (b) there does not exist a path between
a vertex of the component and a vertex outside the
component. Condition (a) means, both technically
and intuitively, that a component is connected and
Condition (b) requires connectivity to be maximal.
Components play a key role in SMT. The most
widely used strategy for extracting high quality
phrase-level translations without linguistic informa-
tion, namely the consistency method (Och et al.,
1999; Koehn et al., 2003) is entirely based on com-
ponents of word aligned unsegmented sentence pairs
(Martzoukos et al., 2013). In particular, each ex-
tracted translation is either a component or the union
of components. Since an unsegmented sentence
pair is just one possible configuration of all possi-
ble bilingual segmentations, we consequently have
no direct reason to investigate further than compo-
nents.
In order to get an intuition of the measure that will
be introduced in this section, we begin with an ex-
ample. Figure 2, shows two different configurations
of the pair (?, ?) for the same sentence pair with
known and fixed word alignments. Both configu-
rations have the same number of edges that connect
source vertices (3) and the same number of edges
that connect target vertices (2). However, one would
32
  
1 2 3 4 ? ? ?1 2 3 4 ? ?1 2 3 4 ? ? ?1 2 3 4 ? ?
Figure 2: Graph representations of two bilingual
segmentations with fixed word alignments. Source
and target vertices are shown with circles and
squares respectively.
expect the top configuration to represent a better
bilingual segmentation. This is because it has more
components (4 opposed to 2 for the bottom config-
uration) and because it consists of ?tighter? clusters,
i.e., ?tighter? components.
A general measure that would capture this obser-
vation requires a balance between the number of
edges of source and target chains, the number of
components and the number of translation edges, all
coupled with how these edges and vertices are con-
nected. This might seem as a daunting task that can
be tackled with a combination of heuristics, but there
is actually a graph-theoretic measure that can fully
describe the sought structure. We proceed with in-
troducing this measure.
Let C denote the set of components of the graph
representation of a bilingual word-aligned segmen-
tation. We are interested in measuring the extent to
which we can delete translation edges from c ? C,
while retaining its component status. Let a
c
denote
the subset of translation edges that are restricted to
the component c. We define the positive integer
gain(c) = number of ways of
deleting translation edges from a
c
,
while keeping c connected, (7)
where the option of deleting nothing is counted. In-
tuitively, by keeping the edges of the chains fixed
the quantity gain(c) measures how difficult it is to
perturb a component from its connected state to a
disconnected state.
Figure 3 shows two components c and c
?
that sat-
isfy gain(c) = gain(c
?
) = 3. Both components
are equally difficult to be perturbed into a discon-
nected state, but only superficially. The actual struc-
tural quality of c is revealed when it is ?compared? to
component c? that consists of the same source and tar-
get vertices, the same translation edges but its source
vertices form exactly one chain and similarly for its
target vertices; c? is essentially the ?upper bound? of
c. In general, the maximum value of gain(c), with
  
cc '?c
Figure 3: Superficially similar components c and c
?
.
Comparing c with c? yields c?s true structural quality.
respect to a fixed set of source and target vertices
and translation edges, is attained when it consists
of exactly one source chain and exactly one target
chain. It is not difficult to see that the desired max-
imum value is always 2
|a
c
|
? 1. In the example of
Figure 3, the structural quality of c and c
?
is thus
3/(2
5
? 1) = 9.7% and 3/(2
2
? 1) = 100% respec-
tively. Hence, the measure that evaluates the struc-
tural quality of a bilingual word-aligned segmenta-
tion (?, ?) is given by
f(?, ?) =
(
?
c?C
gain(c)
2
|a
c
|
? 1
)
1
|C|
, (8)
which takes values in (0, 1]. The relation f(?, ?) >
f(?
?
, ?
?
) implies that (?, ?) is a better bilingual seg-
mentation than (?
?
, ?
?
).
We conclude this section with two remarks: (i) A
component with no translation edges, i.e., a source
or target segment whose words are all unaligned, has
a contribution of 1/0 in (8). In practice we exclude
such components from C. (ii) In graph theory the
quantity gain(c) is known as the number of con-
nected spanning subgraphs (CSSGs) of graph c and
is the key quantity of network reliability (Valiant,
1979; Coulbourn, 1987). Finding the number of
CSSGs of a general graph is a known #P-hard prob-
lem (Welsh, 1997). In our setting, graphs have spe-
cific formation (source and target chains connected
via translation edges) and we are interested in the
deletion of translation edges only; it is possible to
33
compute gain(?) in polynomial time, but we do not
go into further details here.
4 Extracting Bilingual Segmentations with
the Cross-Entropy Method
Equipped with the measures of Sections 2 and 3 we
turn to extracting anN -best list of bilingual segmen-
tations for a given sentence pair. The search space is
exponential in the total number of words of the sen-
tence pair. We propose a new approach for this task,
by noting a direct connection with the combinato-
rial problems that can be solved efficiently and ef-
fectively with the Cross-Entropy (CE) method (Ru-
binstein, 1997).
The CE method is an iterative self-tuning sam-
pling method that has applications in various com-
binatorial and continuous global optimization prob-
lems as well as in rare event detection. A detailed
account on the CE method is beyond the scope of
this work, and we thus simply describe its applica-
tion to our problem.
In particular, we first establish the connection be-
tween the most basic form of the CE method and the
problem of finding the best monolingual segmen-
tation of a sentence, with respect to some scoring
function (not necessarily the one that was introduced
in Section 2). This connection yields a simple, ef-
ficient and effective algorithm for the monolingual
maximization problem. Then, the transition to the
bilingual level is done by incorporating the measure
of Section 3 in the algorithm, thus performing joint
maximization of surface and structural quality. Fi-
nally, the generation of theN -best list will be trivial.
A segmentation of a given sentence has a bit-
string representation in the following way: If two
consecutive words in the sentence belong to the
same segment in the segmentation, then this pair of
words is encoded by ?1?, otherwise by ?0?. Such a
representation is bijective and, thus, for the rest of
this section, we do not distinguish between a seg-
mentation and its bit-string representation. In this
setting, the CE method takes its most basic form
(De Boer et al., 2005). In a nutshell, it is a re-
peated application of (a) sampling bit-strings from
a parametrized probability mass function, (b) scor-
ing them and keeping only a small high-performing
subsample, and (c) updating the parameters of the
probability mass function based on that subsample
only.
We assume no prior knowledge on the quality
of bit-strings, so that they are all equally likely. In
other words, each position of a randomly chosen
bit-string can be either a ?0? or a ?1? with probability
1/2. The aim is to tune these position probabilities
towards the best bit-string, with respect to some
scoring function g. In particular, let the sentence
have n words and let ` = n ? 1 be the length of
bit-strings. A bit-string labeled by an integer i is
denoted by b
i
and its jth bit by b
ij
. The algorithm is
as follows:
0. Initialize the bit-string position probabilities
p
0
= (p
0
1
, ..., p
0
`
) = (1/2, ..., 1/2) and set M = 20`
(sample size), ? = d1%Me (keep top 1% of
samples), ? = 0.7 (smoothing parameter) and t = 1
(iteration).
1. Generate a sample b
1
, ..., b
M
of bit-strings, each
of length `, such that b
ij
?Bernoulli(p
t?1
j
), for all
i = 1, ...,M and j = 1, ..., `.
1.1 Compute scores g(b
1
), ..., g(b
M
).
1.2 Order them descendingly as g(b
pi(1)
) > ... >
g(b
pi(M)
).
2. Focus on the best performing ones: Compute
?
t
= g(b
pi(?)
); samples performing less than this
threshold will be ignored.
3. Use the best performing sub-sample of b
1
, ..., b
M
to update position probabilities:
p
t
j
=
?
M
i=1
I
i
(?
t
)b
ij
?
M
i=1
I
i
(?
t
)
, j = 1, ..., `, (9)
where the choice function I
i
is given by
I
i
(?
t
) =
{
1, if g(b
i
) > ?
t
0, otherwise.
4. Smooth the updated position probabilities as
p
t
j
:= ?p
t
j
+ (1 ? ?)p
t?1
j
, j = 1, ..., `. (10)
E. If for some t > 5we have ?
t
= ?
t?1
= ... = ?
t?5
then stop. Else, t := t + 1 and go to Step 1.
34
The values for the parameters M , ? and ? re-
ported here are in line with the ones suggested in the
literature (Rubinstein and Kroese, 2004) for combi-
natorial problems such as this one. After the execu-
tion of the algorithm, the updated vector of position
probabilities converges to sequence of ?0?s and ?1?s,
which corresponds to the best segmentation under g.
The extension to bilingual level is done by incor-
porating the structural quality measure of Section 3.
The setting is similar, i.e., samples are again bit-
strings, but of length ` = n + m ? 2, where n and
m are the number of words in the source and tar-
get sentence respectively. The first n ? 1 bits corre-
spond to the source sentence and the rest to the target
sentence. The surface quality score of such a bit-
string is given by the harmonic mean of its source
and target surface quality scores.
1
The bit-string
scoring function throughout Steps 1 ? 3 is given by
the harmonic mean of surface and structural quality
scores. Finally, N -best lists are trivially generated,
simply by collecting the top-N performing accumu-
lated samples of a maximization process.
5 Experiments
Given a sentence pair with known and fixed word
alignments, the result of the method described in
Section 4 is an N -best list of bilingual segmenta-
tions of such a pair. The objective function provides
a balance between compositional expressive power
of segments in both languages and synchronization
via word alignments. Thus, each (continuous) com-
ponent of such a bilingual segmentation leads to the
extraction of a high quality phrase pair.
As was mentioned in Section 3, each extracted
phrase pair of standard phrase-based SMT is con-
structed from a component or from the union of
components of an unsegmented word-aligned sen-
tence pair. For each sentence pair, all possible
(continuous) components and (continuous) unions
of components give rise to the extracted (contin-
uous) phrase pairs. In this section we investigate
the impact to SMT models and translation quality,
when extracting phrase pairs (from the N -best lists)
1
As it was mentioned in Section 2 the surface quality score
in (6) is a real number. At each iteration of the algorithm the
surface score of a segmentation can be converted into a number
in [0, 1] via Min-Max normalization. This holds for both source
and target sides of a bit-string (independently).
Cz?En De?En
Europarl (v7) 642,505 1,889,791
News Commentary (v8) 139,679 177,079
Total 782,184 2,066,870
Table 1: Number of filtered parallel sentences for
Czech?English and German?English.
that correspond to components only. A reduction
in phrase-table size is guaranteed because we are
essentially extracting only a subset of all possible
continuous phrase pairs. The challenge is to verify
whether this subset can provide a sufficient transla-
tion model.
Both the baseline and our system are standard
phrase-basedMT systems. Bidirectional word align-
ments are generated with GIZA++ (Och and Ney,
2003) and ?grow-diag-final-and?. These are used
to construct a phrase-table with bidirectional phrase
probabilities, lexical weights and a reordering model
with monotone, swap and discontinuous orienta-
tions, conditioned on both the previous and the next
phrase. 4-gram interpolated language models with
Kneser-Ney smoothing are built with SRILM (Stol-
cke, 2002). A distortion limit of 6 and a phrase-
penalty are also used. All model parameters are
tuned with MERT (Och, 2003). Decoding during
tuning and testing is done with Moses (Koehn et. al,
2007). Since our system only affects which phrases
are extracted, lexical weights and reordering orien-
tations are the same for both systems.
Datasets are from the WMT?13 translation task
(Bojar et al., 2013): Translation and reordering
models are trained on Czech?English and German?
English corpora (Table 1). Language models and
segment measures gain , as defined in (5), are trained
on 35.3M Czech, 50.0M German and 94.5M En-
glish sentences from the provided monolingual data.
Tuning is done on newstest2010 and performance
is evaluated on newstest2008, newstest2009, new-
stest2011 and newstest2012 with BLEU (Papineni
et al., 2001).
In our experiments the size of anN -best list varies
according to the total number of words in the sen-
tence pair, say w. For the purposes of phrase ex-
traction in SMT we would ideally require all local
maxima to be part of an N -best list. This would
35
Method
Czech?English English?Czech Czech?English
?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)
Baseline 19.6 20.6 22.6 20.6 14.8 15.6 16.6 14.9 44.6M (100%)
N -best 19.7 20.4 22.4 20.3 14.4 15.2 16.3 14.3 4.4M (9.8%)
N -best & unseg. 19.6 20.5 22.6 20.7 14.6 15.4 16.8 14.7 4.6M (10.4%)
Table 2: BLEU scores and phrase-table (PT) sizes for Czech?English. Phrase-table of ?Baseline? is con-
structed from all consistent phrase pairs. Phrase-table of ?N -best? is constructed from consistent phrase
pairs that are components of the top-N bilingual word-aligned segmentations of each sentence pair. Simi-
larly for ?N -best & unseg.?, but consistent phrase pairs that are components of each (unsegmented) sentence
pair are also included.
Method
German?English English?German German?English
?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)
Baseline 21.4 20.8 21.3 22.1 15.1 15.1 16.0 16.5 102.3M (100%)
N -best 21.3 20.6 21.3 21.8 15.0 15.0 15.6 16.0 9.4M (9.2%)
N -best & unseg. 21.5 20.8 21.5 22.0 15.4 15.2 15.7 16.2 9.9M (9.7%)
Table 3: Similar to Table 2, but for German?English.
guarantee the extraction of all high quality phrase
pairs, with (empirically) desired variations, while
keeping N small. Since the CE method performs
global optimization, the resulting members of an N -
best list are in the vicinity of the global maximum.
Consequently, we cannot guarantee the inclusion of
local maxima. We set N = d30%we so that at
least some variation from the global maximum is in-
cluded, but is not large enough to contaminate the
lists with noisy bilingual segmentations. The result-
ing lists have 22 bilingual segmentations on aver-
age for both language pairs. Figure 4 shows typical
German?English best performing bilingual segmen-
tations.
BLEU scores are reported in Tables 2 and 3 for
Czech?English and German?English respectively.
Methods ?Baseline? and ?N -best? are the ones de-
scribed above. Phrase-table sizes are reduced as
expected and performance when translating to En-
glish is comparable. The significant drops in new-
stest2012 when translating from the morphologi-
cally poorer language (English) prompts us to in-
clude more ?basic? phrase pairs in the phrase-tables.
This leads to augmenting each N -best list by its un-
segmented sentence pair. Consequently, method ?N -
best & unseg.? extracts the same phrase pairs as ?N -
best?, together with those from components of the
unsegmented sentence pairs. As a result, transla-
tion quality is comparable to ?Baseline? across all
language directions and small phrase-table sizes are
retained.
6 Discussion and Future Work
This work can also be viewed as an attempt to un-
derstand bilinguality as a generalization of mono-
linguality. There is conceptual common ground on
what gain(x) for phrase x (Section 2) or component
x (Section 3) computes. In both cases it measures
how ?stable? a unit is. The stability of a phrase x is
determined by how difficult it is to split x into multi-
ple phrases. The partially ordered set framework of
partition refinements is the natural setting for such
computations. In order to determine the stability
of a component we turn to empirical evidence from
SMT: ?good? phrase pairs are extracted from com-
ponents or unions of components of the graph that
represents word-aligned sentence pairs. The stabil-
ity of a component x is therefore determined by how
difficult it is to break x into multiple components. It
is thus interesting to investigate whether there exists
a general approach that unifies partition refinements
and network reliability for the purpose of identifying
highly stable multilingual units.
36
  
12 34??? ? ? ? ? 34 ??4 ?? ?? ?? ?2??? ?3? ? ?
3?? ? ? ? ?34??4 ?? ? ?
?? ? ? 2?4??
?? ?? ??4?41?
14 ?? ? 3?? ?23?? ? ? ? ? ? ? ? ? ? ? ? ? ?3? 3 ?? ? ? ? ?3? ? ?
?4??? ? ?34?14? 3???4? ?? ? ? ? ? ? ?
?3? ?? ? ? ?3? ?? ? ? ? ? ?4 12 432?4 ?? ?221??1?4
2? ?? ? ?4 ?3? ?? ? ? ? ? ? ? ?4 ??3?? ? ? ? ? ? 4?1??221?
14 ???34???4??4 ??1??? ?4 ?? ? ? ? ? ?1? ?4? ?? ? ? ? ? ? ?1?? ? ?4 ?41?4
4??? ?13?1?4? ?? ? ? ? ?4 ??1?? ? ? ?34? 34? ?? ? ? ? ? ? ? ?34 ?41?4 ???? ? ? ? ? 14
??? ? ?? ?1?421?1???1??? ?32?1?3?
?? ? ? ? ? ?4??21?1?3??32?1?3??
?? ?
?? ?4?
3???? ? ? ?4??? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ? ? ? ? ?4?4???? ?1??? ?4??1?
?? ? ? ??? ? ? ??? ?3143?????? ?2?4?3???34???? ? ? ?2?4??? ??4
??44?4
?34
?3???? ? ?4?1??? ? ? ? ? ? ?4?1??1??3??421??1?
??3???? ? ?? ?3???? ??1??? ? ?? ? ?2??? ? ?? ? ?
?
?? ?
3?
3???11?????? ? ?2???3?1????1?1??21??
??3412???34?2??1????2???3?1??? ? ??3??3213??4???4??
Figure 4: Typical fragments from best performing
German?English segmentations.
The focus has been on bilingual segmentations,
but as was mentioned in Section 2, it is possible
to apply the CE method for generating monolingual
segmentations. By using (6) as the objective func-
tion, we observed that the resulting segmentations
yield promising applications in n-gram topic model-
ing, named entity recognition and Chinese segmen-
tation. However, in the spirit of Ries et al. (1996),
attempts to minimize perplexity instead of maximiz-
ing (6), resulted in larger segments and the segment
quality definition of Section 1 was not met.
The sizes of the resulting phrase-tables together
with the type of phrase pairs that are extracted lead
to applications involving discontinuous phrase pairs.
In (Galley and Manning, 2010) there was evidence
that discontinuous phrase pairs that are extracted
from discontinuous components of word-aligned
sentence pairs can improve translation quality.
1
As
the number of such components is much bigger than
the continuous ones, (Gimpel and Smith, 2011) pro-
pose a Bayesian nonparametric model for finding the
most probable discontinuous phrase pairs. This can
also be done from the N -best lists that are generated
in Section 4, and it would be interesting to see the
effect of such phrase pairs in our existing models.
In a longer version of this work we intend to
study the effect in translation quality when varying
some of the parameters (size of N -best lists, sample
sizes for training gain in Section 2 and for the CE
method), as well as when extracting source-driven
bilingual segmentations as in (Sanchis-Trilles et al.,
2011).
7 Conclusions
In this work, we have presented a solution to the
problem of extracting bilingual segmentations in the
presence of word alignments. Two measures that as-
sess the quality of bilingual segmentations based on
the expressive power of segments in both languages
and their synchronization via word alignments have
been introduced. We have established the link be-
tween the CE method and finding the best monolin-
gual and bilingual segmentations. These measures
formed the objective function of the CE method
whose maximization resulted in an N -best list of
bilingual segmentations for a given sentence pair.
By extracting only phrase pairs that correspond to
components from bilingual segmentations of those
lists, we found that phrase table sizes can be reduced
with insignificant loss in translation quality.
Acknowledgements
This research was funded in part by the Euro-
pean Commission through the CoSyne project FP7-
ICT-4-248531 and the Netherlands Organisation
for Scientific Research (NWO) under project nr.
639.022.213.
1
By ?discontinuous component? we mean a component
whose source or target words (vertices) form a discontinuous
substring in the source or target sentence respectively.
37
References
Graeme Blackwood, Adria de Gispert, and William
Byrne. 2008. Phrasal Segmentation Models for Sta-
tistical Machine Translation. In COLING.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp Koehn,
Christof Monz, Matt Post, Radu Soricut, and Lucia
Specia. 2013. Findings of the 2013 Workshop on Sta-
tistical Machine Translation. In WMT.
Charlie J. Coulbourn. 1987. The Combinatorics of Net-
work Reliability. Oxford University Press.
Pieter-Tjerk De Boer, Dirk P. Kroese, Shie Mannor, and
Reuven Y. Rubinstein. 2005. A Tutorial on the Cross-
Entropy Method. Annals of Operations Research,
vol. 134, pages 19?67.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
NAACL.
Kevin Gimpel and Noah A. Smith. 2011. Generative
Models of Monolingual and Bilingual Gappy Patterns.
In WMT.
Jin Guo. 1997. Critical Tokenization and its Properties.
Computational Linguistics, vol. 23(4), pages 569?596.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by discard-
ing most of the phrase-table. In EMNLP-CoNLL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond?rej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL,
demonstration session.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL.
Spyros Martzoukos, Christophe Costa Flor?encio, and
Christof Monz. 2013. Investigating Connectivity and
Consistency Criteria for Phrase Pair Extraction in Sta-
tistical Machine Translation. In Meeting on Mathe-
matics of Language.
Bernard Monjardet. 1981. Metrics on partially ordered
sets ? a survey. Discrete Mathematics, vol. 35, pages
173?184.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, vol. 29 (1), pages 19?51.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In EMNLP-VLC.
Chris Orum and Cliff A. Joslyn. 2009. Valuations and
Metrics on Partially Ordered Sets. Computing Re-
search Repository - CORR, vol. abs/0903.2.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2010.
Integration of Multiple Bilingually-Learned Segmen-
tation Schemes into Statistical Machine Translation.
In WMT and MetricsMATR.
Klaus Ries, Finn Dag Bu, and Alex Waibel. 1996. Class
phrase models for language modeling. In ICSLP.
Reuven Y. Rubinstein. 1997. Optimization of Computer
Simulation Models with Rare Events. European Jour-
nal of Operations Research, vol. 99, pages 89?112.
Reuven Y. Rubinstein and Dirk P. Kroese. 2004. The
Cross-Entropy Method: A Unified Approach to Com-
binatorial Optimization, Monte-Carlo Simulation and
Machine Learning. Springer-Verlag, New York.
Germ?an Sanchis-Trilles, Daniel Ortiz-Mart??nez, Jes?us
Gonz?alez-Rubio, Jorge Gonz?alez, and Francisco
Casacuberta. 2011. Bilingual segmentation for
phrasetable pruning in Statistical Machine Translation.
In EAMT.
Richard P. Stanley. 1997. Enumerative Combinatorics,
Volume 1. Cambridge University Press.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In ICSLP.
Leslie G. Valiant. 1979. The complexity of enumeration
and reliability problems. SIAM Journal on Comput-
ing, vol. 8, pages 410?421.
Dominic J. A. Welsh. 1997. Approximate counting.
Surveys in Combinatorics, London Math. Soc. Lecture
Notes Ser., 241, pages 287?324.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
Systematic Comparison of Phrase Table Pruning Tech-
niques. In EMNLP.
38
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 93?101,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Investigating Connectivity and Consistency Criteria for Phrase Pair
Extraction in Statistical Machine Translation
Spyros Martzoukos, Christophe Costa Flore?ncio and Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam
Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.Monz}@uva.nl, chriscostafl@gmail.com
Abstract
The consistency method has been estab-
lished as the standard strategy for extract-
ing high quality translation rules in statis-
tical machine translation (SMT). However,
no attention has been drawn to why this
method is successful, other than empiri-
cal evidence. Using concepts from graph
theory, we identify the relation between
consistency and components of graphs that
represent word-aligned sentence pairs. It
can be shown that phrase pairs of interest
to SMT form a sigma-algebra generated
by components of such graphs. This con-
struction is generalized by allowing seg-
mented sentence pairs, which in turn gives
rise to a phrase-based generative model. A
by-product of this model is a derivation of
probability mass functions for random par-
titions. These are realized as cases of con-
strained, biased sampling without replace-
ment and we provide an exact formula for
the probability of a segmentation of a sen-
tence.
1 Introduction
A parallel corpus, i.e., a collection of sentences in
a source and a target language, which are trans-
lations of each other, is a core ingredient of ev-
ery SMT system. It serves the purpose of training
data, i.e., data from which translation rules are ex-
tracted. In its most basic form, SMT does not re-
quire the parallel corpus to be annotated with lin-
guistic information, and human supervision is thus
restricted to the construction of the parallel corpus.
The extraction of translation rules is done by ap-
propriately collecting statistics from the training
data. The pioneering work of (Brown et al, 1993)
identified the minimum assumptions that should
be made in order to extract translation rules and
developed the relevant models that made such ex-
tractions possible.
These models, known as IBMmodels, are based
on standard machine learning techniques. Their
output is a matrix of word alignments for each sen-
tence pair in the training data. These word align-
ments provide the input for later approaches that
construct phrase-level translation rules which may
(Wu, 1997; Yamada and Knight, 2001) or may not
(Och et al, 1999; Marcu and Wong, 2002) rely on
linguistic information.
The method developed in (Och et al, 1999),
known as the consistency method, is a simple yet
effective method that has become the standard way
of extracting (source, target)-pairs of phrases as
translation rules. The development of consistency
has been done entirely on empirical evidence and
it has thus been termed a heuristic.
In this work we show that the method of (Och
et al, 1999) actually encodes a particular type of
structural information induced by the word align-
ment matrices. Moreover, we show that the way in
which statistics are extracted from the associated
phrase pairs is insufficient to describe the underly-
ing structure.
Based on these findings we suggest a phrase-
level model in the spirit of the IBM models. A key
aspect of the model is that it identifies the most
likely partitions, rather than alignment maps, asso-
ciated with appropriately chosen segments of the
training data. For that reason, we provide a gen-
eral construction of probability mass functions for
partitions and, in particular, an exact formula for
the probability of a segmentation of a sentence.
93
2 Definition of Consistency
In this section we provide the definition of consis-
tency, which was introduced in (Och et al, 1999),
refined in (Koehn et al, 2003), and we follow
(Koehn, 2009) in our description. We start with
some preliminary definitions.
Let S = s1...s|S| be a source sentence, i.e., a
string that consists of consecutive source words;
each word si is drawn from a source language vo-
cabulary and i indicates the position of the word
in S. The operation of string extraction from the
words of S is defined as the construction of the
string s = si1 ...sin from the words of S, with
1 ? i1 < ... < in ? |S|. If i1, ..., in are consecu-
tive, which implies that s is a substring of S, then
s is called a source phrase and we write s ? S.
As a shorthand we also write sini1 for the phrase
si1 ...sin . Similar definitions apply to the target
side and we denote by T, tj and t a target sen-
tence, word and phrase respectively.
Let (S = s1s2...s|S|, T = t1t2...t|T |) be a sen-
tence pair and letA denote the |S|?|T |matrix that
encodes the existence/absence of word alignments
in (S, T ) as
A(i, j) =
{
1, if si and tj are aligned
0, otherwise,
(1)
for all i = 1, ..., |S| and j = 1, ..., |T |. Un-
aligned words are allowed. A pair of strings (s =
si1 ...si|s| , t = tj1 ...tj|t|) that is extracted from
(S, T ) is termed consistent with A, if the follow-
ing conditions are satisfied:
1. s ? S and t ? T .
2. ?k ? {1, ..., |s|} such that A(ik, j) = 1, then
j ? {j1, ..., j|t|}.
3. ?l ? {1, ..., |t|} such that A(i, jl) = 1, then
i ? {i1, ..., i|s|}.
4. ?k ? {1, ..., |s|} and ?l ? {1, ..., |t|} such
that A(ik, jl) = 1.
Condition 1 guarantees that (s, t) is a phrase
pair and not just a pair of strings. Condition 2 says
that if a word in s is aligned to one or more words
in T , then all such target words must appear in t.
Condition 3 is the equivalent of Condition 2 for the
target words. Condition 4 guarantees the existence
of at least one word alignment in (s, t).
For a sentence pair (S, T ), the set of all consis-
tent pairs with an alignment matrix A is denoted
by P (S, T ). Figure 1(a) shows an example of a
sentence pair with an alignment matrix together
with all its consistent pairs.
In SMT the extraction of each consistent pair
(s, t) from (S, T ) is followed by a statistic
f(s, t;S, T ). Typically f(s, t;S, T ) counts the oc-
currences of (s, t) in (S, T ). By considering all
sentence pairs in the training data, the translation
probability is constructed as
p(t|s) =
?
(S,T ) f(s, t;S, T )
?
(S,T )
?
t? f(s, t
?;S, T )
, (2)
and similarly for p(s|t). Finally, the entries of the
phrase table consist of all extracted phrase pairs,
their corresponding translation probabilities and
other models which we do not discuss here.
3 Consistency and Components
For a given sentence pair (S, T ) and a fixed word
alignment matrixA, our aim is to show the equiva-
lence between consistency and connectivity prop-
erties of the graph formed by (S, T ) and A. More-
over, we explain that the way in which measure-
ments are performed is not compatible , in princi-
ple, with the underlying structure. We start with
some basic definitions from graph theory (see for
example (Harary, 1969)).
Let G = (V,E) be a graph with vertex set V
and edge set E. Throughout this work, vertices
represent words and edges represent word align-
ments, but the latter will be further generalized in
Section 4. A subgraph H = (V ?, E?) of G is a
graph with V ? ? V , E? ? E and the property
that for each edge in E?, both its endpoints are in
V ?. A path in G is a sequence of edges which con-
nect a sequence of distinct vertices. Two vertices
u, v ? V are called connected if G contains a path
from u to v. G is said to be connected if every pair
of vertices in G is connected.
A connected component, or simply component,
of G is a maximal connected subgraph of G. G
is called bipartite if V can be partitioned in sets
VS and VT , such that every edge in E connects a
vertex in VS to one in VT . The disjoint union of
graphs, or simply union, is an operation on graphs
defined as follows. For n graphs with disjoint ver-
tex sets V1, ..., Vn (and hence disjoint edge sets),
their union is the graph (?ni=1Vi,?
n
i=1Ei).
Consider the graph G whose vertices are the
words of the source and target sentences, and
whose edges are induced by the non-zero entries
94
  
t1 t2 t3 t4 t5 t6 t7s1s2s3s4s5 s1 s2
t1 t2 t3
s3 s4
t4 t5 t6
s5
t7s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7,{ }C1= G
s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7{ }C2= s1 s3t1 t4 t5 t6 s5t7 s2 s4t2 t3 t6 s2 s4t2 t3 s5t7s1 s3t1 t4 t5
s1 s3t1 t4 t5 s2 s4t2 t3 s5t7{ }C3= s1 s3t1 t4 t5 t6 s5t7s2 s4t2 t3 t6s2 s4t2 t3 s5t7s1 s3t1 t4 t5t6
s1 s3t1 t4 t5 s2 s4t2 t3{ }C 4= t6 s5t7
(s5 , t7) ,(s14 , t15) ,(s5 , t67) ,(s14 , t16) ,(S ,T )
(a)
(b)
P (S ,T )= { }
, ,, , , , ,, , ,
Figure 1: (a) Left: Sentence pair with an alignment matrix. Dots indicate existence of word alignments.
Right: All consistent pairs. (b) The graph representation of the matrix in (a), and the sets generated by
components of the graph. Dark shading indicates consistency.
of the matrix A. There are no edges between
any two source-type vertices nor between any two
target-type vertices. Moreover, the source and tar-
get language vocabularies are assumed to be dis-
joint and thus G is bipartite. The set of all com-
ponents of G is defined as C1 and let k denote its
cardinality, i.e., |C1| = k. From the members of
C1 we further construct sets C2, ..., Ck as follows:
For each i, 2 ? i ? k, any member ofCi is formed
by the union of any i distinct members of C1. In
other words, any member of Ci is a graph with i
components and each such component is a mem-
ber of C1. The cardinality of Ci is clearly
(k
i
)
, for
every i, 1 ? i ? k.
Note that Ck = {G}, since G is the union of
all members of C1. Moreover, observe that C? =
?ki=1Ci is the set of graphs that can be generated
by all possible unions of G?s components. In that
sense
C = {?} ? C? (3)
is the power set of G. Indeed we have |C| = 1 +
?k
i=1
(k
i
)
= 2k as required.1
Figure 1(b) shows the graph G and the associ-
ated sets Ci of (S, T ) and A in Figure 1(a). Note
the bijective correspondence between consistent
1Here we used the fact that for any set X with |X| =
n, the set of all subsets of X , i.e., the power set of X , has
cardinality
Pn
i=0
`n
i
?
= 2n.
pairs and the phrase pairs that can be extracted
from the vertices of the members of the sets Ci.
This is a consequence of consistency Conditions 2
and 3, since they provide the sufficient conditions
for component formation.
In general, if a pair of strings (s, t) satisfies the
consistency Conditions 2 and 3, then it can be ex-
tracted from the vertices of a graph inCi, for some
i. Moreover, if Conditions 1 and 4 are also satis-
fied, i.e., if (s, t) is consistent, then we can write
P (S, T ) =
k?
i=1
{
(SH , TH) : H ? Ci,
SH ? S, TH ? T
}
,
(4)
where SH denotes the extracted string from the
source-type vertices of H , and similarly for TH .
Having established this relationship, when refer-
ring to members of C, we henceforth mean either
consistent pairs or inconsistent pairs. The latter
are pairs (SH , TH) for some H ? C such that at
least either SH 6? S or TH 6? T .
The construction above shows that phrase pairs
of interest to SMT are part of a carefully con-
structed subclass of all possible string pairs that
can be extracted from (S, T ). The power set C
of G gives rise to a small, possibly minimal, set
95
in which consistent and inconsistent pairs can be
measured.1 In other words, since C is (by con-
struction) a sigma-algebra, the pair (C1, C) is a
measurable space. Furthermore, one can construct
a measure space (C1, C, f), with an appropriately
chosen measure f : C ? [0,?).
Is the occurrence-counting measure f of Sec-
tion 2 a good choice? Fix an ordering for Ci, and
let Ci,j denote the jth member of Ci, for all i,
1 ? i ? k. Furthermore, let ?(x, y) = 1, if x = y
and 0, otherwise. We argue by contradiction that
the occurrence-counting measure
f(H) =
?
{H?: H??C, H? is consistent}
?(H,H ?), (5)
fails to form a measure space. Suppose that more
than one component of G is consistent, i.e., sup-
pose that
1 <
k?
j=1
f(C1,j) ? k. (6)
By construction of C, it is guaranteed that
1 = f(G) = f(Ck,1) = f(?
k
j=1 C1,j). (7)
The members of C1 are pairwise disjoint, because
each of them is a component ofG. Thus, since f is
assumed to be a measure, sigma-additivity should
be satisfied, i.e., we must have
f(?kj=1 C1,j) =
k?
j=1
f(C1,j) > 1, (8)
which is a contradiction.
In practice, the deficiency of using eq. 5 as
a statistic could possibly be explained by the
fact that the so-called lexical weights are used as
smoothing.
4 Consistency, Components and
Segmentations
In Section 3 the only relation that was assumed
among source (target) words/vertices was the or-
der of appearance in the source (target) sentence.
As a result, the graph representation G of (S, T )
and A was bipartite. There are several, linguisti-
cally motivated, ways in which a general graph can
be obtained from the bipartite graph G. We ex-
plain that the minimal linguistic structure, namely
1See Appendix for definitions.
sentence segmentations, can provide a generaliza-
tion of the construction introduced in Section 3.
Let X be a finite set of consecutive integers. A
consecutive partition of X is a partition of X such
that each part consists of integers consecutive in
X . A segmentation ? of a source sentence S is a
consecutive partition of {1, ..., |S|}. A part of ?,
i.e., a segment, is intuitively interpreted as a phrase
in S. In the graph representation G of (S, T ) and
A, a segmentation ? of S is realised by the ex-
istence of edges between consecutive source-type
vertices whose labels, i.e., word positions in S, ap-
pear in the same segment of ?. The same argument
holds for a target sentence and its words; a target
segmentation is denoted by ? .
Clearly, there are 2|S|?1 possible ways to seg-
ment S and, given a fixed alignment matrix A,
the number of all possible graphs that can be con-
structed is thus 2|S|+|T |?2. The bipartite graph
of Section 3 is just one possible configuration,
namely the one in which each segment of ? con-
sists of exactly one word, and similarly for ? . We
denote this segmentation pair by (?0, ?0).
We now turn to extracting consistent pairs in
this general setting from all possible segmenta-
tions (?, ?) for a sentence pair (S, T ) and a fixed
alignment matrix A. As in Section 3, we con-
struct graphs G?,? , associated sets C?,?i , for all i,
1 ? i ? k?,? , and C?,? , for all (?, ?). Consistent
pairs are extracted in lieu of eq. 4, i.e.,
P ?,? (S, T ) =
k?,??
i=1
{
(SH , TH) : H ? C
?,?
i ,
SH ? S, TH ? T
}
, (9)
and it is trivial to see that
{(S, T )} ? P ?,? (S, T ) ? P (S, T ), (10)
for all (?, ?). Note that P (S, T ) = P ?0,?0(S, T )
and, depending on the details of A, it is possible
for other pairs (?, ?) to attain equality. Moreover,
each consistent pair in P (S, T ) can be be extracted
from a member of at least one C?,? .
We focus on the sets C?,?1 , i.e., the components
of G?,? , for all (?, ?). In particular, we are inter-
ested in the relation between P (S, T ) and C?,?1 ,
for all (?, ?). Each consistent H ? C?0,?0 can
be converted into a single component by appropri-
ately forming edges between consecutive source-
type vertices and/or between consecutive target-
type vertices. The resulting component will evi-
dently be a member of C?,?1 , for some (?, ?). It
96
is important to note that the conversion of a con-
sistent H ? C?0,?0 into a single component need
not be unique; see Figure 2 for a counterexam-
ple. Since (a) such conversions are possible for
all consistent H ? C?0,?0 and (b) P (S, T ) =
P ?0,?0(S, T ), it can be deduced that all possible
consistent pairs can be traced in the sets C?,?1 , for
all (?, ?). In other words, we have:
P (S, T ) =
?
?,?
{
(SH , TH) : H ? C
?,?
1 ,
SH ? S, TH ? T
}
. (11)
The above equation says that by taking sen-
tence segmentations into account, we can recover
all possible consistent pairs, by inspecting only the
components of the underlying graphs.
It would be interesting to investigate the re-
lation between measure spaces (C?,?1 , C
?,? , f?,? )
and different configurations for A. We leave that
for future work and focus on the advantages pro-
vided by eq. 11.
  
t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3
Figure 2: A graph with three components (top),
and four possible conversions into a single compo-
nent by forming edges between contiguous words.
5 Towards a phrase-level model that
respects consistency
The aim of this section is to exploit the relation
established in eq. 11 between consistent pairs and
components of segmented sentence pairs. It was
also shown in Section 2 that the computation of the
translation models is inappropriate to describe the
underlying structure. We thus suggest a phrase-
based generative model in the spirit of the IBM
word-based models, which is compatible with the
construction of the previous sections.
5.1 Hidden variables
All definitions from the previous sections are car-
ried over, and we introduce a new quantity that is
associated with components. Let G?,? and C?,?1 ,
for some (?, ?) be as in Section 4, then the set
K is defined as follows: Each member of K is
a pair of (source, target) sets of segments that cor-
responds to the pair of (source, target) vertices of
a consistent member of C?,?1 . In other words, K is
a bisegmentation of a pair of segmented sentences
that respects consistency.
Figure 3 shows three possible ways to con-
struct consistent graphs from (S, T ) = (s41, t
6
1),
? = {{1, 2}, {3}, {4}} ? {x1, x2, x3} and ? =
{{1}, {2, 3, 4}, {5}, {6}} ? {y1, y2, y3, y4}. In
each case the exact alignment information is un-
known and we have:
(a) K =
{ (
{x1}, {y1}
)
,
(
{x2}, {y2}
)
,
(
{x3}, {y3, y4}
) }
.
(b) K =
{ (
{x1, x2}, {y1, y2, y3}
)
,
(
{x3}, {y4}
)}
.
(c) K =
{ (
{x1}, {y3, y4}
)
,
(
{x2, x3}, {y1, y2}
)}
.
  
t1s1 s2 s3t 2 t3 t4 s5t 5 t6
t1 s1 s2 s3t 2 t3 t4t 5 t6s5
t1s1 s2 s3t 2 t3t4 t 5t6 s5
(a)
(b)
(c)
Figure 3: Three possible ways to construct con-
sistent graphs for (s41, t
6
1) and a given segmenta-
tion pair. Exact word alignment information is un-
known.
In the proposed phrase-level generative model
the random variables whose instances are ?, ? and
97
K are hidden variables. As with the IBM mod-
els, they are associated with the positions of words
in a sentence, rather than the words themselves.
Alignment information is implicitly identified via
the consistent bisegmentation K.
Suppose we have a corpus that consists of pairs
of parallel sentences (S, T ), and let fS,T denote
the occurrence count of (S, T ) in the corpus. Also,
let lS = |S| and lT = |T |. The aim is to maximize
the corpus log-likelihood function
` =
?
S,T
fS,T log p?(T |S)
=
?
S,T
fS,T log
?
?,?,K
p?(T, ?, ?,K|S), (12)
where ?, ? and K are hidden variables parameter-
ized by a vector ? of unknown weights, whose val-
ues are to be determined. The expectation max-
imization algorithm (Dempster et al, 1977) sug-
gests that an iterative application of
?n+1 = argmax
?
?
S,T
fS,T
?
?,?,K
p?n(?, ?,K|S, T )?
log p?(T, ?, ?,K|S),
(13)
provides a good approximation for the maximum
value of `. As with the IBM models we seek prob-
ability mass functions (PMFs) of the form
p?(T, ?, ?,K|S) = p?(lT |S)p?(?, ?,K|lT , S)?
p?(T |?, ?,K, lT , S),
(14)
and decompose further as
p?(?, ?,K|lT , S) = p?(?, ? |lT , S)p?(K|?, ?, lT , S)
(15)
A further simplification of p?(?, ? |lT , S) =
p?(?|S)p?(? |lT ) may not be desirable, but will
help us understand the relation between ? and the
PMFs. In particular, we give a formal description
of p?(?|S) and then explain that p?(K|?, ?, lT , S)
and p?(T |?, ?,K, lT , S) can be computed in a
similar way.
5.2 Constrained, biased sampling without
replacement
The probability of a segmentation given a sentence
can be realised in two different ways. We first pro-
vide a descriptive approach which is more intu-
itive, and we use the sentence S = s41 as an ex-
ample whenever necessary. The set of all possi-
ble segments of S is denoted by seg(S) and triv-
ially |seg(S)| = |S|
(
|S| + 1
)
/2. Each segment
x ? seg(S) has a nonnegative weight ?(x|lS) such
that ?
x?seg(S)
?(x|lS) = 1. (16)
Suppose we have an urn that consists of
|seg(S)| weighted balls; each ball corresponds to
a segment of S. We sample without replacement
with the aim of collecting enough balls to form a
segmentation of S. When drawing a ball x we si-
multaneously remove from the urn all other balls
x? such that x ? x? 6= ?. We stop when the urn
is empty. In our example, let the urn contain 10
balls and suppose that the first draw is {1, 2}. In
the next draw, we have to choose from {3}, {4}
and {3, 4} only, since all other balls contain a ?1?
and/or a ?2? and are thus removed. The sequence
of draws that leads to a segmentation is thus a path
in a decision tree. Since ? is a set, there are |?|!
different paths that lead to its formation. The set
of all possible segmentations, in all possible ways
that each segmentation can be formed, is encoded
by the collection of all such decision trees.
The second realisation, which is based on the
notions of cliques and neighborhoods, is more
constructive and will give rise to the desired PMF.
A clique in a graph is a subset U of the vertex set
such that for every two vertices u, v ? U , there ex-
ists an edge connecting u and v. For any vertex u
in a graph, the neighborhood of u is defined as the
set N(u) = {v : {u, v} is an edge}. A maximal
clique is a clique U that is not a subset of a larger
clique: For each u ? U and for each v ? N(u) the
set U ? {v} is not a clique.
Let G be the graph whose vertices are all seg-
ments of S and whose edges satisfy the condition
that any two vertices x and x? form an edge iff
x ? x? = ?; see Figure 4 for an example. G es-
sentially provides a compact representation of the
decision trees discussed above.
It is not difficult to see that a maximal clique
also forms a segmentation. Moreover, the set of all
maximal cliques in G is exactly the set of all pos-
sible segmentations for S. Thus, p?(?|S) should
satisfy
p?(?|S) = 0, if ? is not a clique in G, (17)
and ?
?
p?(?|S) = 1, (18)
98
  
{1}
{2}
{3}
{4}
{253}{154}
{351}{35154} {25351}
{2535154}
Figure 4: The graph whose vertices are the seg-
ments of s41 and whose edges are formed by non-
overlapping vertices.
where the sum is over all maximal cliques in G.
In our example p?
(
{ {1}, {1, 2} }|S
)
= 0, be-
cause there is no edge connecting segments {1}
and {1, 2} so they are not part of any clique.
In order to derive an explicit formula for
p?(?|S) we focus on a particular type of paths
in G. A path is called clique-preserving, if ev-
ery vertex in the path belongs to the same clique.
Our construction should be such that each clique-
preserving path has positive probability of occur-
ring, and all other paths should have probability
0. We proceed with calculating probabilities of
clique-preserving paths based on the structure of
G and the constraint of eq. 16.
The probability p?(?|S) can be viewed as
the probability of generating all clique-preserving
paths on the maximal clique ? in G. Since
? is a clique, there are |?|! possible paths that
span its vertices. Let ? = {x1, ..., x|?|},
and let pi denote a permutation of {1, ..., |?|}.
We are interested in computing the probabil-
ity q?(xpi(1), ..., xpi(|?|)) of generating a clique-
preserving path xpi(1), ..., xpi(|?|) in G. Thus,
p?(?|S) = p?({x1, ..., x|?|}|S)
=
?
pi
q?(xpi(1), ..., xpi(|?|))
=
?
pi
q?(xpi(1)) q?(xpi(2)|xpi(1))? ...
...? q?(xpi(|?|)|xpi(1), ..., xpi(|?|?1)).
(19)
The probabilities q?(?) can be explicitly calcu-
lated by taking into account the following ob-
servation. A clique-preserving path on a clique
? can be realised as a sequence of vertices
xpi(1), ..., xpi(i), ..., xpi(|?|) with the following con-
straint: If at step i ? 1 of the path we are at ver-
tex xpi(i?1), then the next vertex xpi(i) should be a
neighbor of all of xpi(1), ..., xpi(i?1). In other words
we must have
xpi(i) ? Npi,i ?
i?1?
l=1
N(xpi(l)). (20)
Thus, the probability of choosing xpi(i) as the next
vertex of the path is given by
q?(xpi(i)|xpi(1), ..., xpi(i?1)) =
?(xpi(i)|lS)
?
x?Npi,i
?(x|lS)
,
(21)
if xpi(i) ? Npi,i and 0, otherwise. When choosing
the first vertex of the path (the root in the deci-
sion tree) we have Npi,1 = seg(S), which gives
q?(xpi(1)) = ?(xpi(1)|lS), as required. Therefore
eq. 19 can be written compactly as
p?(?|S) =
?
?
|?|?
i=1
?(xi|lS)
?
?
?
pi
1
Q?(?, pi;S)
,
(22)
where
Q?(?, pi;S) =
|?|?
i=1
?
x?Npi,i
?(x|lS) . (23)
The construction above can be generalized in
order to derive a PMF for any random variable
whose values are partitions of a set. Indeed, by al-
lowing the vertices of G to be a subset of a power
set, and keeping the condition of edge formation
the same, probabilities of clique-preserving paths
can be calculated in the same way. Figure 5 shows
the graph G that represents all possible instances of
K with (S, T ) = (s41, t
5
1), ? =
{
{1, 2}, {3}, {4}
}
and ? =
{
{1}, {2, 3, 4}, {5}
}
. Again each maxi-
mal clique is a possible consistent bisegmentation.
In order for this model to be complete, one
should solve the maximization step of eq. 13 and
calculate the posterior p?n(?, ?,K|S, T ). We are
not bereft of hope, as relevant techniques have
been developed (see Section 6).
6 Related Work
To our knowledge, this is the first attempt to inves-
tigate formal motivations behind the consistency
method.
99
  
t 12 s, 23t 4 s,5 t 3 s, 5
t 12 s, 1
t 4 s,1 t 3 s, 1
t 12 s, 5
t 3 s, 23 t 4 s, 23t 14 s,13
t 14 s, 25
t 43 s, 25
t 13 s, 15
t 14 s,1 t 3 s, 25 t 14 s,5 t 3 s, 13
t 43 s, 13
Figure 5: Similar to Figure 4 but for consistent
bisegmentations with (S, T ) = (s41, t
5
1) and a
given segmentation pair (see text). For clarity, we
show the phrases that are formed from joining con-
tiguous segments in each pair, rather than the seg-
ments themselves.
Several phrase-level generative models have
been proposed, almost all relying on multinomial
distributions for the phrase alignments (Marcu and
Wong, 2002; Zhang et al, 2003; Deng and Byrne
2005; DeNero et al, 2006; Birch et al, 2006).
This is a consequence of treating alignments as
functions rather than partitions.
Word alignment and phrase extraction via In-
version Transduction Grammars (Wu, 1997), is a
linguistically motivated method that relies on si-
multaneous parsing of source and target sentences
(DeNero and Klein, 2010; Cherry and Lin 2007;
Neubig et al, 2012).
The partition probabilities we introduced in
Section 5.2 share the same tree structure discussed
in (Dennis III, 1991), which has found applica-
tions in Information Retrieval (Haffari and Teh,
2009).
7 Conclusions
We have identified the relation between consis-
tency and components of graphs that represent
word-aligned sentence pairs. We showed that
phrase pairs of interest to SMT form a sigma-
algebra generated by components of such graphs,
but the existing occurrence-counting statistics are
inadequate to describe this structure. A general-
ization of our construction via sentence segmenta-
tions lead to a realisation of random partitions as
cases of constrained, biased sampling without re-
placement. As a consequence, we derived an exact
formula for the probability of a segmentation of a
sentence.
Appendix: Measure Space
The following standard definitions can be found
in, e.g., (Feller, 1971). LetX be a set. A collection
B of subsets of X is called a sigma-algebra if the
following conditions hold:
1. ? ? B.
2. If E is in B, then so is its complement X \E.
3. If {Ei} is a countable collection of sets in B,
then so is their union ?iEi.
Condition 1 guarantees that B is non-empty and
Conditions 2 and 3 say thatB is closed under com-
plementation and countable unions respectively.
The pair (X,B) is called a measurable space.
A function f : B ? [0,?) is called a measure
if the following conditions hold:
1. f(?) = 0.
2. If {Ei} is a countable collection of pairwise
disjoint sets in B, then
f(?iEi) =
?
i
f(Ei).
Condition 2 is known as sigma-additivity. The
triple (X,B, f) is called a measure space.
Acknowledgments
This research was supported by the European
Union?s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (GALATEAS) and by the EC funded
project CoSyne (FP7-ICT-4-24853).
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne and Philipp Koehn. 2006. Constraining the
Phrase-Based, Joint Probability Statistical Transla-
tion Model. In Proc. of the Workshop on Statistical
Machine Translation, pages 154?157.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation.
Computational Linguistics, vol.19(2), pages 263?
312.
100
Colin Cherry and Dekang Lin. 2007. Inversion Trans-
duction Grammar for Joint Phrasal TranslationMod-
eling. In Proc. of SSST, NAACL-HLT / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 17?24.
A.P. Dempster, N.M. Laird and D.B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B (Methodological) 39(1), pages 1?38.
John DeNero, Dan Gillick, James Zhang and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proc. of the Work-
shop on Statistical Machine Translation, pages 31?
38.
John DeNero and Dan Klein. 2010. Discriminative
Modeling of Extraction Sets for Machine Transla-
tion. In Proc. of the Association for Computational
Linguistics (ACL), pages 1453?1463.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proc. of the Conference on Empir-
ical Methods in Natural Language Processing and
Human Language Technology (HLT-EMNLP), pages
169?176.
Samuel Y. Dennis III. 1991. On the Hyper-Dirichlet
Type 1 and Hyper-Liouville Distributions. Commu-
nications in Statistics - Theory and Methods, 20(12),
pages 4069?4081.
William Feller. 1971. An Introduction to Probability
Theory and its Applications, Volume II. John Wiley,
New York.
Gholamreza Haffari and Yee Whye Teh. 2009. Hi-
erarchical Dirichlet Trees for Information Retrieval.
In Proc. of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL), pages 173?181.
Frank Harary. 1969. Graph Theory. Addison?Wesley,
Reading, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (HLT-NAACL), pages
48?54.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 133?139.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori and Tatsuya Kawahara. 2012. Joint
Phrase Alignment and Extraction for Statistical Ma-
chine Translation. Journal of Information Process-
ing, vol. 20(2), pages 512?523.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of the Joint Con-
ference of Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP-VLC),
pages 20?28.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23, pages 377?
404.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Association for Computational Linguistics (ACL),
pages 523?530.
Ying Zhang, Stephan Vogel and Alex Waibel. 2003.
Integrated Phrase Segmentation and Alignment Al-
gorithm for Statistical Machine Translation. In
Proc. of the International Conference on Natural
Language Processing and Knowledge Engineering
(NLP-KE).
101
