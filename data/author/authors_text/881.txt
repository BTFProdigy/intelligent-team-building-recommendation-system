c? 2004 Association for Computational Linguistics
Learning Subjective Language
Janyce Wiebe? Theresa Wilson?
University of Pittsburgh University of Pittsburgh
Rebecca Bruce? Matthew Bell?
University of North Carolina University of Pittsburgh
at Asheville
Melanie Martin?
New Mexico State University
Subjectivity in natural language refers to aspects of language used to express opinions, evalua-
tions, and speculations. There are numerous natural language processing applications for which
subjectivity analysis is relevant, including information extraction and text categorization. The
goal of this work is learning subjective language from corpora. Clues of subjectivity are gener-
ated and tested, including low-frequency words, collocations, and adjectives and verbs identified
using distributional similarity. The features are also examined working together in concert. The
features, generated from different data sets using different procedures, exhibit consistency in
performance in that they all do better and worse on the same data sets. In addition, this article
shows that the density of subjectivity clues in the surrounding context strongly affects how likely
it is that a word is subjective, and it provides the results of an annotation study assessing the
subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion
piece recognition (a type of text categorization and genre detection) to demonstrate the utility of
the knowledge acquired in this article.
1. Introduction
Subjectivity in natural language refers to aspects of language used to express opin-
ions, evaluations, and speculations (Banfield 1982; Wiebe 1994). Many natural lan-
guage processing (NLP) applications could benefit from being able to distinguish
subjective language from language used to objectively present factual information.
Current extraction and retrieval technology focuses almost exclusively on the sub-
ject matter of documents. However, additional aspects of a document influence its
relevance, including evidential status and attitude (Kessler, Nunberg, Schu?tze 1997).
Information extraction systems should be able to distinguish between factual infor-
mation (which should be extracted) and nonfactual information (which should be
? Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260.
E-mail{wiebe,mbell}@cs.pitt.edu.
? Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260. Email: twilson@cs.pitt.edu.
? Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804.
E-mail: bruce@cs.unca.edu
? Department of Computer Science, New Mexico State University, Las Cruces, NM 88003. E-mail:
mmartin@cs.nmsu.edu.
Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted for
publication: 23 January 2004
278
Computational Linguistics Volume 30, Number 3
discarded or labeled as uncertain). Question-answering systems should distinguish
between factual and speculative answers. Multi-perspective question answering aims
to present multiple answers to the user based upon speculation or opinions derived
from different sources (Carbonell 1979; Wiebe et al 2003). Multidocument summa-
rization systems should summarize different opinions and perspectives. Automatic
subjectivity analysis would also be useful to perform flame recognition (Spertus 1997;
Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intel-
lectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio
broadcasts (Barzialy et al 2000), review mining (Terveen et al 1997), review classifi-
cation (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy
1987), and clustering documents by ideological point of view (Sack 1995). In general,
nearly any information-seeking system could benefit from knowledge of how opin-
ionated a text is and whether or not the writer purports to objectively present factual
material.
To perform automatic subjectivity analysis, good clues must be found. A huge
variety of words and phrases have subjective usages, and while some manually de-
veloped resources exist, such as dictionaries of affective language (General-Inquirer
2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the atti-
tude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no
comprehensive dictionary of subjective language. In addition, many expressions with
subjective usages have objective usages as well, so a dictionary alone would not suffice.
An NLP system must disambiguate these expressions in context.
The goal of our work is learning subjective language from corpora. In this article,
we generate and test subjectivity clues and contextual features and use the knowledge
we gain to recognize subjective sentences and opinionated documents.
Two kinds of data are available to us: a relatively small amount of data manually
annotated at the expression level (i.e., labels on individual words and phrases) of Wall
Street Journal and newsgroup data and a large amount of data with existing document-
level annotations from the Wall Street Journal (opinion pieces, such as editorials and
reviews, versus nonopinion pieces). Both are used as training data to identify clues
of subjectivity. In addition, we cross-validate the results between the two types of
annotation: The clues learned from the expression-level data are evaluated against the
document-level annotations, and those learned using the document-level annotations
are evaluated against the expression-level annotations.
There were a number of motivations behind our decision to use document-level
annotations, in addition to our manual annotations, to identify and evaluate clues
of subjectivity. The document-level annotations were not produced according to our
annotation scheme and were not produced for the purpose of training and evaluating
an NLP system. Thus, they are an external influence from outside the laboratory. In
addition, there are a great number of these data, enabling us to evaluate the results
on a larger scale, using multiple large test sets. This and cross-training between the
two types of annotations allows us to assess consistency in performance of the various
identification procedures. Good performance in cross-validation experiments between
different types of annotations is evidence that the results are not brittle.
We focus on three types of subjectivity clues. The first are hapax legomena, the set
of words that appear just once in the corpus. We refer to them here as unique words.
The set of all unique words is a feature with high frequency and significantly higher
precision than baseline (Section 3.2).
The second are collocations (Section 3.3). We demonstrate a straightforward method
for automatically identifying collocational clues of subjectivity in texts. The method is
first used to identify fixed n-grams, such as of the century and get out of here. Interest-
279
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
ingly, many include noncontent words that are typically on stop lists of NLP systems
(e.g., of, the, get, out, here in the above examples). The method is then used to identify
an unusual form of collocation: One or more positions in the collocation may be filled
by any word (of an appropriate part of speech) that is unique in the test data.
The third type of subjectivity clue we examine here are adjective and verb fea-
tures identified using the results of a method for clustering words according to dis-
tributional similarity (Lin 1998) (Section 3.4). We hypothesized that two words may
be distributionally similar because they are both potentially subjective (e.g., tragic, sad,
and poignant are identified from bizarre). In addition, we use distributional similarity
to improve estimates of unseen events: A word is selected or discarded based on the
precision of it together with its n most similar neighbors.
We show that the various subjectivity clues perform better and worse on the same
data sets, exhibiting an important consistency in performance (Section 4.2).
In addition to learning and evaluating clues associated with subjectivity, we ad-
dress disambiguating them in context, that is, identifying instances of clues that are
subjective in context (Sections 4.3 and 4.4). We find that the density of clues in the
surrounding context is an important influence. Using two types of annotations serves
us well here, too. It enables us to use manual judgments to identify parameters for
disambiguating instances of automatically identified clues. High-density clues are high
precision in both the expression-level and document-level data. In addition, we give
the results of a new annotation study showing that most high-density clues are in sub-
jective text spans (Section 4.5). Finally, we use the clues together to perform document-
level classification, to further demonstrate the utility of the acquired knowledge (Sec-
tion 4.6).
At the end of the article, we discuss related work (Section 5) and conclusions
(Section 6).
2. Subjectivity
Subjective language is language used to express private states in the context of a
text or conversation. Private state is a general covering term for opinions, evaluations,
emotions, and speculations (Quirk et al 1985). The following are examples of subjective
sentences from a variety of document types.
The first two examples are from Usenet newsgroup messages:
(1) I had in mind your facts, buddy, not hers.
(2) Nice touch. ?Alleges? whenever facts posted are not in your persona of
what is ?real.?
The next one is from an editorial:
(3) We stand in awe of the Woodstock generation?s ability to be unceasingly
fascinated by the subject of itself. (?Bad Acid,? Wall Street Journal,
August 17, 1989)
The next example is from a book review:
(4) At several different layers, it?s a fascinating tale. (George Melloan,
?Whose Spying on Our Computers?? Wall Street Journal, November 1,
1989)
280
Computational Linguistics Volume 30, Number 3
The last one is from a news story:
(5) ?The cost of health care is eroding our standard of living and sapping
industrial strength,? complains Walter Maher, a Chrysler
health-and-benefits specialist. (Kenneth H. Bacon, ?Business and Labor
Reach a Consensus on Need to Overhaul Health-Care System,? Wall
Street Journal, November 1, 1989)
In contrast, the following are examples of objective sentences, sentences without sig-
nificant expressions of subjectivity:
(6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents a
share.
(7) Northwest Airlines settled the remaining lawsuits filed on behalf of 156
people killed in a 1987 crash, but claims against the jetliner?s maker are
being pursued, a federal judge said. (?Northwest Airlines Settles Rest of
Suits,? Wall Street Journal, November 1, 1989)
A particular model of linguistic subjectivity underlies the current and past re-
search in this area by Wiebe and colleagues. It is most fully presented in Wiebe and
Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994). It was developed to support NLP
research and combines ideas from several sources in fields outside NLP, especially
linguistics and literary theory. The most direct influences on the model were Dolezel
(1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda
(1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse),
Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic
description of opaque contexts), and especially Banfield (1982) (theory of subjectivity
versus communication).1
The remainder of this section sketches our conceptualization of subjectivity and
describes the annotation projects it underlies.
Subjective elements are linguistic expressions of private states in context. Subjec-
tive elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3)
and eroding, sapping, and complains in (5)). They may be single words (e.g., complains)
or more complex expressions (e.g., stand in awe, what a NP). Purely syntactic or mor-
phological devices may also be subjective elements (e.g., fronting, parallelism, changes
in aspect).
A subjective element expresses the subjectivity of a source, who may be the writer
or someone mentioned in the text. For example, the source of fascinating in (4) is
the writer, while the source of the subjective elements in (5) is Maher (according to
the writer). In addition, a subjective element usually has a target, that is, what the
subjectivity is about or directed toward. In (4), the target is a tale; in (5), the target of
Maher?s subjectivity is the cost of health care.
Note our parenthetical above??according to the writer??concerning Maher?s
subjectivity. Maher is not directly speaking to us but is being quoted by the writer.
Thus, the source is a nested source, which we notate (writer, Maher); this represents
the fact that the subjectivity is being attributed to Maher by the writer. Since sources
1 For additional citations to relevant work from outside NLP, please see Banfield (1982), Fludernik (1993),
Wiebe (1994), and Stein and Wright (1995).
281
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
are not directly addressed by the experiments presented in this article, we merely
illustrate the idea here with an example, to give the reader an idea:
The Foreign Ministry said Thursday that it was ?surprised, to put it
mildly? by the U.S. State Department?s criticism of Russia?s human
rights record and objected in particular to the ?odious? section on
Chechnya. (Moscow Times, March 8, 2002]
Let us consider some of the subjective elements in this sentence, along with their
sources:
surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry)
to put it mildly: (writer, Foreign Ministry)
criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department)
objected: (writer, Foreign Ministry)
odious: (writer, Foreign Ministry)
Consider surprised, to put it mildly. This refers to a private state of the Foreign Ministry
(i.e., it is very surprised). This is in the context of The Foreign Ministry said, which is in
a sentence written by the writer. This gives us the three-level source (writer, Foreign
Ministry, Foreign Ministry). The phrase to put it mildly, which expresses sarcasm, is
attributed to the Foreign Ministry by the writer (i.e., according to the writer, the Foreign
Ministry said this). So its source is (writer, Foreign Ministry). The subjective element
criticism has a deeply nested source: According to the writer, the Foreign Ministry said
it is surprised by the U.S. State Department?s criticism.
The nested-source representation allows us to pinpoint the subjectivity in a sen-
tence. For example, there is no subjectivity attributed directly to the writer in the
above sentence: At the level of the writer, the sentence merely says that someone
said something and objected to something (without evaluating or questioning this).
If the sentence started The magnificent Foreign Ministry said. . . , then we would have an
additional subjective element, magnificent, with source (writer).
Note that subjective does not mean not true. Consider the sentence John criticized
Mary for smoking. The verb criticized is a subjective element, expressing negative eval-
uation, with nested source (writer, John). But this does not mean that John does not
believe that Mary smokes. (In addition, the fact that John criticized Mary is being
presented as true by the writer.)
Similarly, objective does not mean true. A sentence is objective if the language used
to convey the information suggests that facts are being presented; in the context of
the discourse, material is objectively presented as if it were true. Whether or not the
source truly believes the information, and whether or not the information is in fact
true, are considerations outside the purview of a theory of linguistic subjectivity.
An aspect of subjectivity highlighted when we are working with NLP applications
is ambiguity. Many words with subjective usages may be used objectively. Examples
are sapping and eroding. In (5), they are used subjectively, but one can easily imagine
objective usages, in a scientific domain, for example. Thus, an NLP system may not
merely consult a list of lexical items to accurately identify subjective language but
must disambiguate words, phrases, and sentences in context. In our terminology, a
potential subjective element (PSE) is a linguistic element that may be used to express
282
Computational Linguistics Volume 30, Number 3
Table 1
Data Sets and Annotations used in Experiments. Annotators M, MM, and T are
co-authors of this paper. D and R are not.
Name Source Number of Words Annotators Type of
annotation
WSJ-SE Wall Street Journal 18,341 D,M Subjective elements
NG-SE Newsgroup 15,413 M Subjective elements
NG-FE Newsgroup 88,210 MM,R Flame elements
OP1 Wall Street Journal 640,975 M,T Documents
Composed of 4 data sets: W9-4,W9-10,W9-22,W-33
OP2 Wall Street Journal 629,690 M,T Documents
Composed of 4 data sets: W9-2,W9-20,W9-21,W-23
subjectivity. A subjective element is an instance of a potential subjective element, in a
particular context, that is indeed subjective in that context (Wiebe 1994).
In this article, we focus on learning lexical items that are associated with subjec-
tivity (i.e., PSEs) and then using them in concert to disambiguate instances of them
(i.e., to determine whether the instances are subjective elements).
2.1 Manual Annotations
In our subjectivity annotation projects, we do not give the annotators lists of particular
words and phrases to look for. Rather, we ask them to label sentences according to
their interpretations in context. As a result, the annotators consider a large variety of
expressions when performing annotations.
We use data that have been manually annotated at the expression level, the sen-
tence level, and the document level. For diversity, we use data from the Wall Street
Journal Treebank as well as data from a corpus of Usenet newsgroup messages. Table
1 summarizes the data sets and annotations used in this article. None of the datasets
overlap. The annotation types listed in the table are those used in the experiments
presented in this article.
In our first subjectivity annotation project (Wiebe, Bruce, and O?Hara 1999; Bruce
and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus
(Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated
at the sentence level by multiple judges. The judges were instructed to classify a sen-
tence as subjective if it contained any significant expressions of subjectivity, attributed
to either the writer or someone mentioned in the text, and to classify the sentence as
objective, otherwise. After multiple rounds of training, the annotators independently
annotated a fresh test set of 500 sentences from WSJ-SE. They achieved an average
pairwise kappa score of 0.70 over the entire test set, an average pairwise kappa score
of 0.80 for the 85% of the test set for which the annotators were somewhat sure of
their judgments, and an average pairwise kappa score of 0.88 for the 70% of the test
set for which the annotators were very sure of their judgments.
We later asked the same annotators to identify the subjective elements in WSJ-
SE. Specifically, each annotator was given the subjective sentences he identified in
283
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
the previous study and asked to put brackets around the words he believed caused
the sentence to be classified as subjective.2 For example (subjective elements are in
parentheses):
They paid (yet) more for (really good stuff).
(Perhaps you?ll forgive me) for reposting his response.
No other instructions were given to the annotators and no training was performed for
the expression-level task. A single round of tagging was performed, with no commu-
nication between annotators. There are techniques for analyzing agreement when an-
notations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera,
and Amorortu 1999), but our focus in this article is on words. Thus, our analyses are
at the word level: Each word is classified as either appearing in a subjective element
or not. Punctuation and numbers are excluded from the analyses. The kappa value for
word agreement in this study is 0.42.
Another two-level annotation project was performed in Wiebe et al (2001), this
time involving document-level and expression-level annotations of newsgroup data
(NG-FE in Table 1). In that project, we were interested in annotating flames, inflam-
matory messages in newsgroups or listservs. Note that inflammatory language is a
kind of subjective language. The annotators were instructed to mark a message as
a flame if the main intention of the message is a personal attack and the message
contains insulting or abusive language.
After multiple rounds of training, three annotators independently annotated a
fresh test set of 88 messages from NG-FE. The average pairwise percentage agreement
is 92% and the average pairwise kappa value is 0.78. These results are comparable to
those of Spertus (1997), who reports 98% agreement on noninflammatory messages
and 64% agreement on inflammatory messages.
Two of the annotators were then asked to identify the flame elements in the entire
corpus NG-FE. Flame elements are the subset of subjective elements that are perceived
to be inflammatory. The two annotators were asked to do this in the entire corpus, even
those messages not identified as flames, because messages that were not judged to be
flames at the document level may contain some individual inflammatory phrases. As
above, no training was performed for the expression-level task, and a single round of
tagging was performed, without communication between annotators. Agreement was
measured in the same way as in the subjective-element study above. The kappa value
for flame element annotations in corpus NG-FE is 0.46.
An additional annotation project involved a single annotator, who performed
subjective-element annotations on the newsgroup corpus NG-SE.
The agreement results above suggest that good levels of agreement can be achieved
at higher levels of classification (sentence and document), but agreement at the expres-
sion level is more challenging. The agreement values are lower for the expression-level
annotations but are still much higher than that expected by chance.
Note that our word-based analysis of agreement is a tough measure, because it
requires that exactly the same words be identified by both annotators. Consider the
following example from WSJ-SE:
D: (played the role well) (obligatory ragged jeans a thicket of long hair
and rejection of all things conventional)
2 We are grateful to Aravind Joshi for suggesting this level of annotation.
284
Computational Linguistics Volume 30, Number 3
M: played the role (well) (obligatory) (ragged) jeans a (thicket) of long
hair and (rejection) of (all things conventional)
Judge D in the example consistently identifies entire phrases as subjective, while judge
M prefers to select discrete lexical items.
Despite such differences between annotators, the expression-level annotations
proved very useful for exploring hypotheses and generating features, as described
below.
Since this article was written, a new annotation project has been completed. A
10,000-sentence corpus of English-language versions of world news articles has been
annotated with detailed subjectivity information as part of a project investigating
multiple-perspective question answering (Wiebe et al 2003). These annotations are
much more detailed than the annotations used in this article (including, for example,
the source of each private state). The interannotator agreement scores for the new
corpus are high and are improvements over the results of the studies described above
(Wilson and Wiebe 2003).
The current article uses existing document-level subjective classes, namely edito-
rials, letters to the editor, Arts & Leisure reviews, and Viewpoints in the Wall Street
Journal. These are subjective classes in the sense that they are text categories for which
subjectivity is a key aspect. We refer to them collectively as opinion pieces. All other
types of documents in the Wall Street Journal are collectively referred to as nonopinion
pieces.
Note that opinion pieces are not 100% subjective. For example, editorials contain
objective sentences presenting facts supporting the writer?s argument, and reviews
contain sentences objectively presenting facts about the product beign reviewed. Sim-
ilarly, nonopinion pieces are not 100% objective. News reports present opinions and
reactions to reported events (van Dijk 1988); they often contain segments starting with
expressions such as critics claim and supporters argue. In addition, quoted-speech sen-
tences in which individuals express their subjectivity are often included (Barzilay et
al. 2000). For concreteness, let us consider WSJ-SE, which, recall, has been manually
annotated at the sentence level. In WSJ-SE, 70% of the sentences in opinion pieces
are subjective and 30% are objective. In nonopinion pieces, 44% of the sentences are
subjective and only 56% are objective. Thus, while there is a higher concentration of
subjective sentences in opinion versus nonopinion pieces, there are many subjective
sentences in nonopinion pieces and objective sentences in opinion pieces.
An inspection of some data reveals that some editorial and review articles are not
marked as such by the Wall Street Journal. For example, there are articles whose purpose
is to present an argument rather than cover a news story, but they are not explicitly
labeled as editorials by the Wall Street Journal. Thus, the opinion piece annotations of
data sets OP1 and OP2 in Table 1 have been manually refined. The annotation instruc-
tions were simply to identify any additional opinion pieces that were not marked as
such. To test the reliability of this annotation, two judges independently annotated
two Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000
words. This is an ?annotation lite? task: With no training, the annotators achieved
kappa values of 0.94 and 0.95, and each spent an average of three hours per Wall
Street Journal file.
3. Generating and Testing Subjective Features
3.1 Introduction
The goal in this section is to learn lexical subjectivity clues of various types, single
words as well as collocations. Some require no training data, some are learned us-
285
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
ing the expression-level subjective-element annotations as training data, and some
are learned using the document-level opinion piece annotations as training data (i.e.,
opinion piece versus nonopinion piece). All of the clues are evaluated with respect to
the document-level opinion piece annotations. While these evaluations are our focus,
because many more opinion piece than subjective-element data exist, we do evaluate
the clues learned from the opinion piece data on the subjective-element data as well.
Thus, we cross-validate the results both ways between the two types of annotations.
Throughout this section, we evaluate sets of clues directly, by measuring the pro-
portion of clues that appear in subjective documents or expressions, seeking those that
appear more often than expected. In later sections, the clues are used together to find
subjective sentences and to perform text categorization.
The following paragraphs give details of the evaluation and experimental design
used in this section.
The proportion of clues in subjective documents or expressions is their precision.
Specifically, the precision of a set S with respect to opinion pieces is
prec(S) =
number of instances of members of S in opinion pieces
total number of instances of members of S in the data
The precision of a set S with respect to subjective elements is
prec(S) =
number of instances of members of S in subjective elements
total number of instances of members of S in the data
In the above, S is a set of types (not tokens). The counts are of tokens (i.e., instances
or occurrences) of members of S.
Why use a set rather than individual items? Many good clues of subjectivity occur
with low frequency (Wiebe, McKeever, and Bruce 1998). In fact, as we shall see below,
uniqueness in the corpus is an informative feature for subjectivity classification. Thus,
we do not want to discard low-frequency clues, because they are a valuable source of
information, and we do not want to evaluate individual low-frequency lexical items,
because the results would be unreliable. Our strategy is thus to identify and evaluate
sets of words and phrases, rather than individual items.
What kinds of results may we expect? We cannot expect absolutely high precision
with respect to the opinion piece classifications, even for strong clues, for three reasons.
First, for our purposes, the data are noisy. As mentioned above, while the proportion
of subjective sentences is higher in opinion than in nonopinion pieces, the proportions
are not 100 and 0: Opinion pieces contain objective sentences, and nonopinion pieces
contain subjective sentences.
Second, we are trying to learn lexical items associated with subjectivity, that is,
PSEs. As discussed above, many words and phrases with subjective usages have ob-
jective usages as well. Thus, even in perfect data with no noise, we would not expect
100% precision. (This is the motivation for the work on density presented in section
4.4.)
Third, the distribution of opinions and nonopinions is highly skewed in favor of
nonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinion
pieces.
In this work, increases in precision over a baseline precision are used as evidence
that promising sets of PSEs have been found. Our main baseline for comparison is
the number of word instances in opinion pieces, divided by the total number of word
instances:
Baseline Precision =
number of word instances in opinion pieces
total number of word instances
286
Computational Linguistics Volume 30, Number 3
Table 2
Frequencies and increases in precision of unique
words in subjective-element data. Baseline
frequency is the total number of words, and
baseline precision is the proportion of words in
subjective elements.
WSJ-SE
D M
freq +prec +prec
Unique words 2,615 +.07 +.12
Baseline 18,341 .07 .08
Words and phrases with higher proportions than this appear more than expected in
opinion pieces.
To further evaluate the quality of a set of PSEs, we also perform the following
significance test. For a set of PSEs in a given data set, we test the significance of the
difference between (1) the proportion of words in opinion pieces that are PSEs and (2)
the proportion of words in nonopinion pieces that are PSEs, using the z-significance
test for two proportions.
Before we continue, there are a few more technical items to mention concerning
the data preparation and experimental design:
? All of the data sets are stemmed using Karp?s morphological analyzer
(Karp et al 1994) and part-of-speech tagged using Brill?s (1992) tagger.
? When the opinion piece classifications are used for training, the existing
classifications, assigned by the Wall Street Journal, are used. Thus, the
processes using them as training data may be applied to more data to
learn more clues, without requiring additional manual annotation.
? When the opinion piece data are used for testing, the manually refined
classifications (described at the end of Section 2.1) are used.
? OP1 and OP2 together comprise eight treebank files. Below, we often
give results separately for the component files, allowing us to assess the
consistency of results for the various types of clues.
3.2 Unique Words
In this section, we show that low-frequency words are associated with subjectivity in
both the subjective-element and opinion piece data. Apparently, people are creative
when they are being opinionated.
Table 2 gives results for unique words in subjective-element data. Recall that
unique words are those that appear just once in the corpus, that is, hapax legomena.
The first row of Table 2 gives the frequency of unique words in WSJ-SE, followed
by the percentage-point improvements in precision over baseline for unique words in
subjective elements marked by two annotators (denoted as D and M in the table). The
second row gives baseline frequency and precisions. Baseline frequency is the total
number of words in WSJ-SE. Baseline precision for an annotator is the proportion of
words included in subjective elements by that annotator. Specifically, consider anno-
tator M. The baseline precision of words in subjective elements marked by M is 0.08,
287
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 3
Frequencies and increases in precision for words that appear exactly once in the data sets
composing OP1. For each data set, baseline frequency is the total number of words, and
baseline precision is the proportion of words in opinion pieces.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
Unique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11
Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14
but the precision of unique words in these same annotations is 0.20, 0.12 points higher
than the baseline. This is a 150% improvement over the baseline.
The number of unique words in opinion pieces is also higher than expected. Table
3 compares the precision of the set of unique words to the baseline precision (i.e.,
the precision of the set of all words that appear in the corpus) in the four WSJ files
composing OP1. Before this analysis was performed, numbers were removed from the
data (we are not interested in the fact that, say, the number 163,213.01 appears just once
in the corpus). The number of words in each data set and baseline precisions are listed
at the bottom of the table. The freq columns give total frequencies. The +prec columns
show the percentage-point improvements in precision over baseline. For example, in
W9-10, unique words have precision 0.34: 0.18 baseline plus an improvement over
baseline of 0.16. The difference in the proportion of words that are unique in opinion
pieces and the proportion of words that are unique in nonopinion pieces is highly
significant, with p < 0.001 (z ? 22) for all of the data sets. Note that not only does the
set of unique words have higher than baseline precision, the set is a frequent feature.
The question arises, how does corpus size affect the precision of the set of unique
words? Presumably, uniqueness in a larger corpus is more meaningful than uniqueness
in a smaller one. The results in Figure 1 provide evidence that it is. The y-axis in Figure
1 represents increase in precision over baseline and the x-axis represents corpus size.
Five graphs are plotted, one for the set of words that appear exactly once (uniques),
one for the set of words that appear exactly twice ( freq2), one for the set of words that
appear exactly three times ( freq3), etc.
In Figure 1, increases in precision are given for corpora of size n, where n =
20, 40, . . . , 2420, 2440 documents. Each data point is an average over 25 sample corpora
of size n. The sample corpora were chosen from the concatenation of OP1 and OP2, in
which 9% of the documents are opinion pieces. The sample corpora were created by
randomly selecting documents from the large corpus, preserving the 9% distribution
of opinion pieces. At the smallest corpus size (containing 20 documents), the average
number of words is 9,617. At the largest corpus size (containing 2440 documents), the
average is 1,225,186 words.
As can be seen in the figure, the precision of unique and other low-frequency
words increases with corpus size, with increases tapering off at the largest corpus size
tested. Words with frequency 2 also realize a nice increase, although one that is not as
dramatic, in precision over baseline. Even words of frequency 3, 4, and 5 show modest
increases.
To help us understand the importance of low-frequency words in large as opposed
to small data sets, we can consider the following analogy. With collectible trading
cards, rare cards are the most valuable. However, if we have some cards and are
trying to determine thier value, looking in only a few packs of cards will not tell us if
288
Computational Linguistics Volume 30, Number 3
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
20 620 1220 1820 2420
Corpus Size (documents)
Increase in 
Precision
uniques freq2 freq3 freq4 freq5
Figure 1
Precision of low-frequency words as corpus size increases.
any of our cards are valuable. Only by looking at many packs of cards can we make
a determination as to which are the rare ones. Only in samples of sufficient size is
uniqueness informative.
The results in this section suggest that an NLP system using uniqueness features
to recognize subjectivity should determine uniqueness with respect to the test data
augmented with an additional store of (unannotated) data.
3.3 Identifying Potentially Subjective Collocations from Subjective-Element and
Flame-Element Annotations
In this section, we describe experiments in identifying potentially subjective colloca-
tions.
Collocations are selected from the subjective-element data (i.e., NG-SE, NG-FE, and
WSJ-SE), using the union of the annotators? tags for the data sets tagged by multiple
taggers. The results are then evaluated on opinion piece data.
The selection procedure is as follows. First, all 1-grams, 2-grams, 3-grams, and
4-grams are extracted from the data. In this work, each constituent of an n-gram is
a word-stem, part-of-speech pair. For example, (in-prep the-det can-noun) is a 3-gram
that matches trigrams consisting of preposition in, followed by determiner the, and
ending with noun can.
A subset of the n-grams are then selected based on precision. The precision of an
n-gram is the number of subjective instances of that n-gram in the data divided by
the total number of instances of that n-gram in the data. An instance of an n-gram is
subjective if each word occurs in a subjective element in the data.
n-grams are selected based on two criteria. First, the precision of the n-gram must
be greater than the baseline precision (i.e., the proportion of all word instances that
289
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
are in subjective elements). Second, the precision of the n-gram must be greater than
the maximum precision of its constituents. This criterion is used to avoid selecting
unnecessarily long collocations. For example, scumbag is a strongly subjective clue. If
be a scumbag does not have higher precision than scumbag alone, we do not want to
select it.
Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2.
(W1,W2) is identified as a potential subjective element if prec(W1, W2) ? 0.1 and:
prec(W1, W2) > max(prec(W1), prec(W2))
For trigrams, we extend the second condition as follows. Let (W1, W2, W3) be a trigram
consisting of consecutive words W1, W2, and W3. The condition is then
prec(W1, W2, W3) > max(prec(W1, W2), prec(W3))
or
prec(W1, W2, W3) > max(prec(W1), prec(W2, W3))
The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gram
first with the maximum of the precisions of word W1 and trigram (W2, W3, W4) and
then with the maximum of the precisions of trigram (W1,W2,W3) and word W4. We
call the n-gram collocations identified as above fixed-n-grams.
We also define a type of collocation called a unique generalized n-gram (ugen-n-
gram). Such collocations have placeholders for unique words. As will be seen below,
these are our highest-precision features.
To find and select such generalized collocations, we first find every word that
appears just once in the corpus and replace it with a new word, UNIQUE (but re-
membering the part of speech of the original word). In essence, we treat the set of
single-instance words as a single, frequently occurring word (which occurs with var-
ious parts of speech). Precisely the same method used for extracting and selecting
n-grams above is used to obtain the potentially subjective collocations with one or
more positions filled by a UNIQUE, part-of-speech pair.
To test the ugen-n-grams extracted from the subjective-element training data using
the method outlined above, we assess their precision with respect to opinion piece
data. As with the training data, all unique words in the test data are replaced by
UNIQUE. When a ugen-n-gram is matched against the test data, the UNIQUE fillers
match words (of the appropriate parts of speech) that are unique in the test data.
Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patterns
identified as described above on the four data sets composing OP1. The freq columns
give total frequencies, and the +prec columns show the improvements in precision
from the baseline. The number of words in each data set and baseline precisions are
given at the bottom of the table. For all n-gram features besides the fixed-4-grams and
ugen-4-grams, the proportion of features in opinion pieces is significantly greater than
the proportion of features in nonopinion pieces.3
The question arises, how much overlap is there between instances of fixed-n-grams
and instances of ugen-n-grams? In the test data of Table 4, there are a total of 8,577
fixed-n-grams instances. Only 59 of these, fewer than 1% are contained (wholly or in
part) in ugen-n-gram instances. This small intersection set shows that two different
types of potentially subjective collocations are being recognized.
3 Specifically, the difference between (1) the number of feature instances in opinion pieces divided by the
number of words in opinion pieces and (2) the number of feature instances in nonopinion pieces
divided by the number of words in nonopinion pieces is significant (p < 0.05) for all data sets.
290
Computational Linguistics Volume 30, Number 3
Table 4
Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocations
learned from the subjective-element data. For each data set, baseline frequency is the total
number of words, and baseline precision is the proportion of words in opinion pieces.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
fixed-2-grams 1,840 +.07 1,972 +.07 1,933 +.04 1,839 +.05
ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17
fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05
ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15
fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07
ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25
baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14
Randomly selected examples of our learned collocations that appear in the test
data are given in Tables 5 and 6. It is interesting to note that the unique generalized
collocations were learned from the training data by their matching different unique
words from the ones they match in the test data.
3.4 Generating Features from Document-Level Annotations Using Distributional
Similarity
In this section, we identify adjective and verb PSEs using distributional similarity.
Opinion-piece data are used for training, and (a different set of) opinion-piece data
and the subjective-element data are used for testing.
With distributional similarity, words are judged to be more or less similar based
on their distributional patterning in text (Lee 1999; Lee and Pereira 1999). Our
Table 5
Random sample of fixed-3-gram collocations in OP1.
one-noun of-prep his-det worst-adj of-prep all-det
quality-noun of-prep the-det to-prep do-verb so-adverb
in-prep the-det company-noun you-pronoun and-conj your-pronoun
have-verb taken-verb the-det rest-noun of-prep us-pronoun
are-verb at-prep least-adj but-conj if-prep you-pronoun
as-prep a-det weapon-noun continue-verb to-to do-verb
purpose-noun of-prep the-det could-modal have-verb be-verb
it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep
have-verb be-verb the-det do-verb something-noun about-prep
cause-verb you-pronoun to-to evidence-noun to-to back-adverb
that-prep you-pronoun are-verb i-pronoun be-verb not-adverb
of-prep the-det century-noun of-prep money-noun be-prep
291
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 6
Random sample of unique generalized collocations in OP1. U: UNIQUE.
Pattern Instances
U-adj as-prep: drastic as; perverse as; predatory as
U-adj in-prep: perk in; unsatisfying in; unwise in
U-adverb U-verb: adroitly dodge; crossly butter; unceasingly fascinate
U-noun back-adverb: cutting back; hearken back
U-verb U-adverb: coexist harmoniously; flouncing tiresomely
ad-noun U-noun: ad hoc; ad valorem
any-det U-noun: any over-payment; any tapings; any write-off
are-verb U-noun: are escapist; are lowbrow; are resonance
but-conj U-noun: but belch; but cirrus; but ssa
different-adj U-noun: different ambience; different subconferences
like-prep U-noun: like hoffmann; like manute; like woodchuck
national-adj U-noun: national commonplace; national yonhap
particularly-adverb U-adj: particularly galling; particularly noteworthy
so-adverb U-adj: so monochromatic; so overbroad; so permissive
this-det U-adj: this biennial; this inexcusable; this scurrilous
your-pronoun U-noun: your forehead; your manuscript; your popcorn
U-adj and-conj U-adj: arduous and raucous; obstreperous and abstemious
U-noun be-verb a-det: acyclovir be a; siberia be a
U-noun of-prep its-pronoun: outgrowth of its; repulsion of its
U-verb and-conj U-verb: wax and brushed; womanize and booze
U-verb to-to a-det: cling to a; trek to a
are-verb U-adj to-to: are opaque to; are subject to
a-det U-noun and-conj: a blindfold and; a rhododendron and
a-det U-verb U-noun: a jaundice ipo; a smoulder sofa
it-pronoun be-verb U-adverb: it be humanly; it be sooo
than-prep a-det U-noun: than a boob; than a menace
the-det U-adj and-conj: the convoluted and; the secretive and
the-det U-noun that-prep: the baloney that; the cachet that
to-to a-det U-adj: to a gory; to a trappist
to-to their-pronoun U-noun: to their arsenal; to their subsistence
with-prep an-det U-noun: with an alias; with an avalanche
292
Computational Linguistics Volume 30, Number 3
trainingPrec(s) is the precision of s in the training data
validationPrec(s) is the precision of s in the validation data
testPrec(s) is the precision of s in the test data
(similarly for trainingFreq, validationFreq, and testFreq)
S = the set of all adjectives (verbs) in the training data
for T in [0.01,0.04,. . .,0.70]:
for n in [2,3,. . .,40]:
retained = {}
For si in S:
if trainingPrec({si} ? Ci,n) > T:
retained = retained ? {si} ? Ci,n
RT,n = retained
ADJpses = {} (VERBpses = {})
for T in [0.01,0.04,. . .,0.70]:
for n in [2,3,. . .,40]:
if validationPrec(RT,n) ? 0.28 (0.23 for verbs)
and validationFreq(RT,n) ? 100:
ADJpses = ADJpses ? RT,n (VERBpses = VERBpses ? RT,n)
Results in Table 7 show testPrec(ADJpses) and testFreq(ADJpses).
Figure 2
Algorithm for selecting adjective and verb features using distributional similarity.
motivation for experimenting with it to identify PSEs was twofold. First, we hypoth-
esized that words might be distributionally similar because they share pragmatic us-
ages, such as expressing subjectivity, even if they are not close synonyms. Second,
as shown above, low-frequency words appear more often in subjective texts than ex-
pected. We did not want to discard all low-frequency words from consideration but
cannot effectively judge the suitability of individual words. Thus, to decide whether
to retain a word as a PSE, we consider the precision not of the individual word, but
of the word together with a cluster of words similar to it.
Many variants of distributional similarity have been used in NLP (Lee 1999; Lee
and Pereira 1999). Dekang Lin?s (1998) method is used here. In contrast to many
implementations, which focus exclusively on verb-noun relationships, Lin?s method
incorporates a variety of syntactic relations. This is important for subjectivity recogni-
tion, because PSEs are not limited to verb-noun relationships. In addition, Lin?s results
are freely available.
A set of seed words begins the process. For each seed si, the precision of the set
{si}?Ci,n in the training data is calculated, where Ci,n is the set of n words most similar
to si, according to Lin?s (1998) method. If the precision of {si} ? Ci,n is greater than a
threshold T, then the words in this set are retained as PSEs. If it is not, neither si nor
the words in Ci,n are retained. The union of the retained sets will be denoted RT,n, that
is, the union of all sets {si} ? Ci,n with precision on the training set > T.
In Wiebe (2000), the seeds (the sis) were extracted from the subjective-element
annotations in corpus WSJ-SE. Specifically, the seeds were the adjectives that appear
at least once in a subjective element in WSJ-SE. In this article, the opinion piece corpus
is used to move beyond the manual annotations and small corpus of the earlier work,
and a much looser criterion is used to choose the initial seeds: All of the adjectives
(verbs) in the training data are used.
The algorithm for the process is given in Figure 2. There is one small difference
for adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for
293
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 7
Frequencies and increases in precision for adjective and verb features identified
using distributional similarity with filtering. For each test data set, baseline
frequency is the total number of words, and baseline precision is the proportion of
words in opinion pieces.
Baseline ADJpses VERBpses
Training Validation Test freq prec freq +prec freq +prec
W9-10 W9-22
W9-22 W9-10 W9-33 153,634 .14 1,576 +.12 1,490 +.11
W9-10 W9-33
W9-33 W9-10 W9-22 155,135 .13 859 +.15 535 +.11
W9-22 W9-33
W9-33 W9-22 W9-10 156,334 .18 249 +.22 224 +.10
All pairings of W9-10,
W9-22,W9-33 W9-4 156,421 .19 1,872 +.17 1,777 +.15
adjectives versus 0.23 for verbs. These thresholds were determined using validation
data.
Seeds and their clusters are assessed on a training set for many parameter settings
(cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70
by .03). As mentioned above, each (n, T) parameter pair yields a set of adjectives RT,n,
that is, the union of all sets {si}?Ci,n with precision on the training set > T. A subset,
ADJpses, of those sets is chosen based on precision and frequency in a validation set.
Finally, the ADJpses are tested on the test set.
Table 7 shows the results for four opinion piece test sets. Multiple training-
validation data set pairs are used for each test set, as given in Table 7. The results
are for the union of the adjectives (verbs) chosen for each pair. The freq columns give
total frequencies, and the +prec columns show the improvements in precision from
the baseline. For each data set, the difference between the proportion of instances
of ADJpses in opinion pieces and the proportion in nonopinion pieces is significant
(p < 0.001, z ? 9.2). The same is true for VERBpses (p < 0.001, z ? 4.1).
In the interests of testing consistency, Table 8 shows the results of assessing the
adjective and verb features generated from opinion piece data (ADJpses and VERBpses
Table 8
Average frequencies and increases in precision in subjective-element data
of the sets tested in Table 7. The baselines are the precisions of
adjectives/verbs that appear in subjective elements in the
subjective-element data.
Adj baseline Verb baseline ADJpses VERBpses
freq prec freq prec freq +prec freq +prec
WSJ-SE-D 1,632 .13 2,980 .15 136 +.16 151 +.10
WSJ-SE-M 1,632 .19 2,980 .12 136 +.24 151 +.13
NG-SE 1,104 .37 2,629 .15 185 +.25 275 +.08
294
Computational Linguistics Volume 30, Number 3
Table 9
Frequencies and increases in precision for all features. For each data set, baseline
frequency is the total number of words, and baseline precision is the proportion of
words in opinion pieces. freq: total frequency; +prec: increase in precision over baseline.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
Unique words 4794 +.15 4763 +.16 4274 +.11 4567 +.11
Fixed-2-grams 1840 +.07 1972 +.07 1933 +.04 1839 +.05
ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17
Fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05
ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15
Fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07
ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25
Adjectives 1872 +.17 249 +.22 859 +.15 1576 +.12
Verbs 1777 +.15 224 +.10 535 +.11 1490 +.11
Baseline 156421 .19 156334 .18 155135 .13 153634 .14
in Table 7) on the subjective-element data. The left side of the table gives baseline
figures for each set of subjective-element annotations. The right side of the table gives
the average frequencies and increases in precision over baseline for the ADJpses and
VERBpses sets on the subjective-element data. The baseline figures in the table are the
frequencies and precisions of the sets of adjectives and verbs that appear at least once
in a subjective element. Since these sets include words that appear just once in the
corpus (and thus have 100% precision), the baseline precision is a challenging one.
Testing the VERBpses and ADJpses on the subjective-element data reveals some inter-
esting consistencies for these subjectivity clues. The precision increases of the VERBpses
on the subjective-element data are comparable to their increases on the opinion piece
data. Similarly, the precision increases of the ADJpses on the subjective-element data
are as good as or better than the performance of this set of PSEs on the opinion piece
data. Finally, the precisions increases for the ADJpses are higher than for the VERBpses
on all data sets. This is again consistent with the higher performance of the ADJpses
sets in the opinion piece data sets.
4. Features Used in Concert
4.1 Introduction
In this section, we examine the various types of clues used together. In preparation for
this work, all instances in OP1 and OP2 of all of the PSEs identified as described in
Section 3 have been automatically identified. All training to define the PSE instances
in OP1 was performed on data separate from OP1, and all training to define the PSE
instances in OP2 was performed on data separate from OP2.
4.2 Consistency in Precision among Data Sets
Table 9 summarizes the results from previous sections in which the opinion piece data
are used for testing. The performance of the various features is consistently good or
bad on the same data sets: the performance is better for all features on W9-10 and
W9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with very
low frequency, and the verbs, which have low frequency in W9-10). This is so despite
the fact that the features were generated using different procedures and data: The
295
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
0. PSEs = all adjs, verbs, modals, nouns, and adverbs that appear at least
once in an SE (except not, will, be, have).
1. PSEinsts = the set of all instances of PSEs
2. HiDensity = {}
3. For P in PSEinsts:
4. leftWin(P) = the W words before P
5. rightWin(P) = the W words after P
6. density(P) = number of SEs whose first or last
word is in leftWin(P) or rightWin(P)
7. if density(P) ? T:
HiDensity = HiDensity ? {P}
8. prec(PSEinsts) =
number of PSEinsts in subject elements
|PSEinsts|
9. prec(HiDensity) =
number of HiDensity in subject elements
|HiDensity|
Figure 3
Algorithm for calculating density in subjective-element data.
adjectives and verbs were generated from WSJ document-level opinion piece classifi-
cations; the n-gram features were generated from newsgroup and WSJ expression-level
subjective-element classifications; and the unique unigram feature requires no training.
This consistency in performance suggests that the results are not brittle.
4.3 Choosing Density Parameters from Subjective-Element Data
In Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, on
how subjective the surrounding context is. We explore this idea in the current work,
assessing whether PSEs are more likely to be subjective if they are surrounded by sub-
jective elements. In particular, we experiment with a density feature to decide whether
or not a PSE instance is subjective: If a sufficient number of subjective elements are
nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded.
The density parameters are a window size W and a frequency threshold T.
In this section, we explore the density of manually annotated PSEs in subjective-
element data and choose density parameters to use in Section 4.4, in which we apply
them to automatically identified PSEs in opinion piece data.
The process for calculating density in the subjective-element data is given in Fig-
ure 3. The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs that
appear at least once in a subjective element, with the exception of some stop words
(line 0 of Figure 3). Note that these PSEs depend only on the subjective-element man-
ual annotations, not on the automatically identified features used elsewhere in the
article or on the document-level opinion piece classes. PSEinsts is the set of PSE
instances to be disambiguated (line 1). HiDensity (initialized on line 2) will be the
subset of PSEinsts that are retained. In the loop, the density of each PSE instance
P is calculated. This is the number of subjective elements that begin or end in the
W words preceding or following P (line 6). P is retained if its density is at least T
(line 7).
Lines 8?9 of the algorithm assess the precision of the original (PSEinsts) and new
(HiDensity) sets of PSE instances. If prec(HiDensity) is greater than prec(PSEinsts), then
296
Computational Linguistics Volume 30, Number 3
Table 10
Most frequent entry in the top three precision intervals for each
subjective-element data set.
WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE
Baseline freq 1,566 1,245 1,167 1,108 3,303
Baseline prec .49 .47 .41 .36 .51
Range .87?.92 .95?1.0 .95?1.0 .95?1.0 .95?1.0
T, W 10, 20 12, 50 20, 50 14, 100 10, 10
freq 76 12 1 1 3
prec .89 1.0 1.0 1.0 1.0
Range .82?.87 .90?.95 .73?.78 .51?.56 .67?.72
T, W 6, 10 12, 60 46, 190 22, 370 26, 90
freq 63 22 53 221 664
prec .84 .91 .78 .51 .67
Range .77?.82 .84?.89 .66?.71 .46?.51 .63?.67
T, W 12, 40 12, 80 18, 60 16, 310 8, 30
freq 292 42 53 358 1504
prec .78 .88 .68 .47 .63
there is evidence that the number of subjective elements near a PSE instance is related
to its subjectivity in context.
To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1
and WSJ-SE2) and annotations of the two judges are considered separately. WSJ-SE2-D,
for example, refers to D?s annotations of WSJ-SE2. The process in Figure 3 was repeated
for different parameter settings (T in [1, 2, 4, . . . , 48] and W in [1, 10, 20, . . . , 490]) on each
of the SE data sets. To find good parameter settings, the results for each data set were
sorted into five-point precision intervals and then sorted by frequency within each
interval. Information for the top three precision intervals for each data set are shown
in Table 10, specifically, the parameter values (i.e., T and W) and the frequency and
precision of the most frequent result in each interval. The intervals are in the rows
labeled Range. For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92,
0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92). The
top of Table 10 gives baseline frequencies and precisions, which are |PSEinsts| and
prec(PSEinsts), respectively, in line 8 of Figure 3.
The parameter values exhibit a range of frequencies and precisions, with the ex-
pected trade-off between precision and frequency. We choose the following parameters
to test in Section 4.4: For each data set, for each precision interval whose lower bound
is at least 10 percentage points higher than the baseline for that data set, the top
two (T, W) pairs yielding the highest frequencies in that interval are chosen. Among
the five data sets, a total of 45 parameter pairs were so selected. This exercise was
completed once, without experimenting with different parameter settings.
4.4 Density for Disambiguation
In this section, density is exploited to find subjective instances of automatically iden-
tified PSEs. The process is shown in Figure 4. There are only two differences between
the algorithms in Figures 3 and 4. First, in Figure 3, density is defined in terms of
the number of subjective elements nearby. However, subjective-element annotations
are not available in test data. Thus in Figure 4, density is defined in terms of the
297
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
0. PSEinsts = the set of instances in the test
data of all PSEs described in Section 3
1. HiDensity = {}
2. For P in PSEinsts:
3. leftWin(P) = the W words before P
4. rightWin(P) = the W words after P
5. density(P) = number of PSEinsts whose first or last
word is in leftWin(P) or rightWin(P)
6. if density(P) ? T:
HiDensity = HiDensity ? {P}
7. prec(PSEinsts) = # of PSEinsts in OPs|PSEinsts|
8. prec(HiDensity) = # of HiDensity in OPs|HiDensity|
Figure 4
Algorithm for calculating density in opinion piece (OP) data
number of other PSE instances nearby, where PSEinsts consists of all instances of the
automatically identified PSEs described in Section 3, for which results are given in
Table 9.
Second, in Figure 4, we assess precision with respect to the document-level classes
(lines 7?8). The test data are OP1.
An interesting question arose when we were defining the PSE instances: What
should be done with words that are identified to be PSEs (or parts of PSEs) according
to multiple criteria? For example, sunny, radiant, and exhilarating are all unique in
corpus OP1, and are all members of the adjective PSE feature defined for testing
on OP1. Collocations add additional complexity. For example, consider the sequence
and splendidly, which appears in the test data. The sequence and splendidly matches
the ugen-2-gram (and-conj U-adj), and the word splendidly is unique. In addition, a
sequence may match more than one n-gram feature. For example, is it that matches
three fixed-n-gram features: is it, is it that, and it that.
In the current experiments, the more PSEs a word matches, the more weight it
is given. The hypothesis behind this treatment is that additional matches represent
additional evidence that a PSE instance is subjective. This hypothesis is realized as
follows: Each match of each member of each type of PSE is considered to be a PSE
instance. Thus, among them, there are 11 members in PSEinsts for the five phrases
sunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matches
mentioned above.
The process in Figure 4 was conducted with the 45 parameter pair values (T and
W) chosen from the subjective-element data as described in Section 4.3. Table 11 shows
results for a subset of the 45 parameters, namely, the most frequent parameter pair
chosen from the top three precision intervals for each training set. The bottom of the
table gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts|
and prec(PSEinsts), respectively, in line 7 of Figure 4.
The density features result in substantial increases in precision. Of the 45 parameter
pairs, the minimum percentage increase over baseline is 22%. Fully 24% of the 45
parameter pairs yield increases of 200% or more; 38% yield increases between 100%
298
Computational Linguistics Volume 30, Number 3
Table 11
Results for high-density PSEs in test data OP1 using parameters chosen
from subjective-element data.
WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE
T, W 10, 20 12, 50 20, 50 14, 100 10, 10
freq 237 3,176 170 10,510 8
prec .87 .72 .97 .57 1.0
T, W 6, 10 12, 60 46, 190 22, 370 26, 90
freq 459 5,289 1,323 21,916 787
prec .68 .68 .95 .37 .92
T, W 12, 40 12, 80 18, 60 16, 310 8, 30
freq 1,398 9,662 906 24,454 3,239
prec .79 .58 .87 .34 .67
PSE baseline: freq = 30,938, prec = .28
and 199%, and 38% yield increases between 22% and 99%. In addition, the increases
are significant. Using the set of high-density PSEs defined by the parameter pair with
the least increase over baseline, we tested the difference in the proportion of PSEs
in opinion pieces that are high-density and the proportion of PSEs in nonopinion
pieces that are high-density. The difference between these two proportions is highly
significant (z = 46.2, p < 0.0001).
Notice that, except for one blip (T, W = 6, 10 under WSJ-SE-M), the precisions
decrease and the frequencies increase as we go down each column in Table 11. The
same pattern can be observed with all 45 parameter pairs (results not included here
because of space considerations). But the parameter pairs are ordered in Table 11
based on performance in the manually annotated subjective-element data, not based
on performance in the test data. For example, the entry in the first row, first column
(T, W = 10, 20) is the parameter pair giving the highest frequency in the top precision
interval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process of
Figure 3). Thus, the relative precisions and frequencies of the parameter pairs are
carried over from the training to the test data. This is quite a strong result, given that
the PSEs in the training data are from manual annotations, while the PSEs in the test
data are our automatically identified features.
4.5 High-Density Sentence Annotations
To assess the subjectivity of sentences with high-density PSEs, we extracted the 133
sentences in corpus OP2 that contain at least one high-density PSE and manually
annotated them. We refer to these sentences as the system-identified sentences.
We chose the density-parameter pair (T, W = 12, 30), based on its precision and
frequency in OP1. This parameter setting yields results that have relatively high pre-
cision and low frequency. We chose a low-frequency setting to make the annotation
study feasible.
The extracted sentences were independently annotated by two judges. One is a
coauthor of this article (judge 1), and the other has performed subjectivity annota-
tion before, but is not otherwise involved in this research (judge 2). Sentences were
annotated according to the coding instructions of Wiebe, Bruce, and O?Hara (1999)
which, recall, are to classify a sentence as subjective if there is a significant expression
of subjectivity of either the writer or someone mentioned in the text, in the sentence.
299
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 12
Examples of system-identified sentences.
(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and oo
Somali soldiers.
(2.a) But now the refugees are streaming across the border and alarming the world. ss
(2.b) In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation. oo
(2.c) It is becoming more and more obvious that his gallstone-age communism is dying with him: . . . ss
(3.a) Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ss
at Hiltons and Holiday Inns, but creditably and with the air of someone for whom
?Ten Cents a Dance? was more than a bit autobiographical.
(3.b) ?It was an exercise of blending Michelle?s singing with Susie?s singing,? explained Ms. Stevens. oo
(4) Enlisted men and lower-grade officers were meat thrown into a grinder. ss
(5) ?If you believe in God and you believe in miracles, there?s nothing particularly crazy about that.? ss
(6) He was much too eager to create ?something very weird and dynamic,? ss
?catastrophic and jolly? like ?this great and coily thing? ?Lolita.?
(7) The Bush approach of mixing confrontation with conciliation strikes some people as sensible, perhaps ss
even inevitable, because Mr. Bush faces a Congress firmly in the hands of the opposition.
(8) Still, despite their efforts to convince the world that we are indeed alone, the visitors do seem to keep ss
coming and, like the recent sightings, there?s often a detail or two that suggests they may
actually be a little on the dumb side.
(9) As for the women, they?re pathetic. ss
(10) At this point, the truce between feminism and sensationalism gets might uneasy. ss
(11) MMPI?s publishers say the test shouldn?t be used alone to diagnose ss
psychological problems or in hiring; it should be given in conjunction with other tests.
(12) While recognizing that professional environmentalists may feel threatened, ss
I intend to urge that UV-B be monitored whenever I can.
Table 13
Sentence annotation contingency table; judge 1 counts are in rows and
judge 2 counts are in columns.
Subjective Objective Unsure
Subjective 98 2 3
Objective 2 14 0
Unsure 2 11 1
In addition to the subjective and objective classes, a judge can tag a sentence as unsure
if he or she is unsure of his or her rating or considers the sentence to be borderline.
An equal number (133) of other sentences were randomly selected from the corpus
to serve as controls. The 133 system-identified sentences and the 133 control sentences
were randomly mixed together. The judges were asked to annotate all 266 sentences,
not knowing which were system-identified and which were control. Each sentence
was presented with the sentence that precedes it and the sentence that follows it in
the corpus, to provide some context for interpretation.
Table 12 shows examples of the system-identified sentences. Sentences classified
by both judges as objective are marked oo and those classified by both judges as
subjective are marked ss.
300
Computational Linguistics Volume 30, Number 3
Table 14
Examples of subjective sentences adjacent to system-identified sentences.
Bathed in cold sweat, I watched these Dantesque scenes, holding tightly the
damp hand of Edek or Waldeck who, like me, were convinced that there was no God.
?The Japanese are amazed that a company like this exists in Japan,? says Kimindo
Kusaka, head of the Softnomics Center, a Japanese management-research organization.
And even if drugs were legal, what evidence do you have that the habitual drug user
wouldn?t continue to rob and steal to get money for clothes, food or shelter?
The moral cost of legalizing drugs is great, but it is a cost that apparently lies
outside the narrow scope of libertarian policy prescriptions.
I doubt that one exists.
They were upset at his committee?s attempt to pacify the program critics by
cutting the surtax paid by the more affluent elderly and making up the loss by
shifting more of the burden to the elderly poor and by delaying some benefits by a year.
Judge 1 classified 103 of the system-identified sentences as subjective, 16 as ob-
jective, and 14 as unsure. Judge 2 classified 102 of the system-identified sentences as
subjective, 27 as objective; and 4 as unsure. The contingency table is given in Table 13.4
The kappa value using all three classes is 0.60, reflecting the highly skewed distri-
bution in favor of subjective sentences, and the disagreement on the lower-frequency
classes (unsure and objective). Consistent with the findings in Wiebe, Bruce, and
O?Hara (1999), the kappa value for agreement on the sentences for which neither
judge is unsure is very high: 0.86.
A different breakdown of the sentences is illuminating. For 98 of the sentences (call
them SS), judges 1 and 2 tag the sentence as subjective. Among the other sentences, 20
appear in a block of contiguous system-identified sentences that includes a member of
SS. For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block of
subjective sentences as they are. Similarly, (3.a) is in SS and (3.b) is in the same block.
Among the remaining 15 sentences, 6 are adjacent to subjective sentences that
were not identified by our system (so were not annotated by the judges). All of those
sentences contain significant expressions of subjectivity of the writer or someone men-
tioned in the text, the criterion used in this work for classifying a sentence as subjective.
Samples are shown in Table 14.
Thus, 93% of the sentences identified by the system are subjective or are near
subjective sentences. All the sentences, together with their tags and the sentences
adjacent to them, are available on the Web at www.cs.pitt.edu/? wiebe.
4.6 Using Features for Opinion Piece Recognition
In this section, we assess the usefulness of the PSEs identified in Section 3 and listed
in Table 9 by using them to perform document-level classification of opinion pieces.
Opinion-piece classification is a difficult task for two reasons. First, as discussed in Sec-
tion 2.1, both opinionated and factual documents tend to be composed of a mixture of
subjective and objective language. Second, the natural distribution of documents in our
data is heavily skewed toward nonopinion pieces. Despite these hurdles, using only
4 In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2
classified only 47 (36%) of them as subjective.
301
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
our PSEs, we achieve positive results in opinion-piece classification using the basic k-
nearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997).
Given a document, the basic KNN algorithm classifies the document according to
the majority classification of the document?s k closest neighbors. For our purposes, each
document is characterized by one feature, the count of all PSE instances (regardless
of type) in the document, normalized by document length in words. The distance
between two documents is simply the absolute value of the difference between the
normalized PSE counts for the two documents.
With leave-one-out cross-validation, the set of n documents to be classified is
divided into a training set of size n?1 and a validation set of size 1. The one document
in the validation set is then classified according to the majority classification of its k
closest-neighbor documents in the training set. This process is repeated until every
document is classified.
Which value to use for k is chosen during a preprocessing phase. During the pre-
processing phase, we run the KNN algorithm with leave-one-out cross-validation on
a separate training set, for odd values of k from 1 to 15. The value of k that results in
the best classification during the preprocessing phase is the one used for later KNN
classification.
For the classification experiment, the data set OP1 was used in the preprocess-
ing phase to select the value of k, and then classification was performed on the 1,222
documents in OP2. During training on OP1, k equal to 15 resulted in the best classifi-
cation. On the test set, OP2, we achieved a classification accuracy of 0.939; the baseline
accuracy for choosing the most frequent class (nonopinion pieces) was 0.915. Our clas-
sification accuracy represents a 28% reduction in error and is significantly better than
baseline according to McNemar?s test (Everitt 1997).
The positive results from the opinion piece classification show the usefulness of
the various PSE features when used together.
5. Relation to Other Work
There has been much work in other fields, including linguistics, literary theory, psy-
chology, philosophy, and content analysis, involving subjective language. As men-
tioned in Section 2, the conceptualization underlying our manual annotations is based
on work in literary theory and linguistics, most directly Dolez?el (1973), Uspensky
(1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield
(1982). We also mentioned existing knowledge resources such as affective lexicons
(General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons
(e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]).
Such knowledge may be used in future work to complement the work presented in
this article, for example, to seed the distributional-similarity process described in Sec-
tion 3.4.
There is also work in fields such as content analysis and psychology on statisti-
cally characterizing texts in terms of word lists manually developed for distinctions
related to subjectivity. For example, Hart (1984) performs counts on a manually de-
veloped list of words and rhetorical devices (e.g., ?sacred? terms such as freedom)
in political speeches to explore potential reasons for public reactions. Anderson and
McMaster (1998) use fixed sets of high-frequency words to assign connotative scores
to documents and sections of documents along dimensions such as how pleasant,
acrimonious, pious, or confident, the text is.
What distinguishes our work from work on subjectivity in other fields is that
we focus on (1) automatically learning knowledge from corpora, (2) automatically
302
Computational Linguistics Volume 30, Number 3
performing contextual disambiguation, and (3) using knowledge of subjectivity in
NLP applications. This article expands and integrates the work reported in Wiebe and
Wilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al (2001) and Wiebe (2000).
Previous work in NLP on the same or related tasks includes sentence-level and
document-level subjectivity classifications. At the sentence level, Wiebe, Bruce, and
O?Hara (1999) developed a machine learning system to classify sentences as subjec-
tive or objective. The accuracy of the system was more than 20 percentage points
higher than a baseline accuracy. Five part-of-speech features, two lexical features, and
a paragraph feature were used. These results suggested to us that there are clues to
subjectivity that might be learned automatically from text and motivated the work
reported in the current article. The system was tested in 10-fold cross validation ex-
periments using corpus WSJ-SE, a small corpus of only 1,001 sentences. As discussed
in Section 1, a main goal of our current work is to exploit existing document-level
annotations, because they enable us to use much larger data sets, they were created
outside our research group, and they allow us to assess consistency of performance
by cross-validating between our manual annotations and the existing document-level
annotations. Because the document-level data are not annotated at the sentence level,
sentence-level classification is not highlighted in this article. The new sentence annota-
tion study to evaluate sentences with high-density features (Section 4.5) uses different
data from WSJ-SE, because some of the features (n-grams and density parameters)
were identified using WSJ-SE as training data.
Other previous work in NLP has addressed related document-level classifications.
Spertus (1997) developed a system for recognizing inflammatory messages. As men-
tioned earlier in the article, inflammatory language is a type of subjective language,
so the task she addresses is closely related to ours. She uses machine learning to
select among manually developed features. In contrast, the focus in our work is on
automatically identifying features from the data.
A number of projects investigating genre detection include editorials as one of the
targeted genres. For example, in Karlgren and Cutting (1994), editorials are one of fif-
teen categories, and in Kessler, Nunberg, and Schu?tze (1997), editorials are one of six.
Given the goal of these works to perform genre detection in general, they use low-level
features that are not specific to editorials. Neither shows significant improvements for
editorial recognition. Argamon, Koppel, and Avneri (1998) address a slightly different
task, though it does involve editorials. Their goal is to distinguish not only, for ex-
ample, news from editorials, but also these categories in different publications. Their
best results are distinguishing among the news categories of different publications;
their lowest results involve editorials. Because we focus specifically on distinguishing
opinion pieces from nonopinion pieces, our results are better than theirs for those
categories. In addition, in contrast to the above studies, the focus of our work is on
learning features of subjectivity. We perform opinion piece recognition in order to
assess the usefulness of the various features when used together.
Other previous NLP research has used features similar to ours for other NLP tasks.
Low-frequency words have been used as features in information extraction (Weeber,
Vos, and Baayen 2000) and text categorization (Copeck et al 2000). A number of
researchers have worked on mining collocations from text to extend lexicographic
resources for machine translation and word sense disambiguation (e.g., Smajda 1993;
Lin 1999; Biber 1993).
In Samuel, Carberry, and Vijay-Shanker?s (1998) work on identifying collocations
for dialog-act recognition, a filter similar to ours was used to eliminate redundant
n-gram features: n-grams were eliminated if they contained substrings with the same
entropy score as or a better entropy score than the n-gram.
303
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
While it is common in studies of collocations to omit low-frequency words and
expressions from analysis, because they give rise to invalid or unrealistic statistical
measures (Church and Hanks, 1990), we are able to identify higher-precision colloca-
tions by including placeholders for unique words (i.e., the ugen-n-grams). We are not
aware of other work that uses such collocations as we do.
Features identified using distributional similarity have previously been used for
syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994)
and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999).
We are not aware of other work identifying and using density parameters as
described in this article.
Since our experiments, other related work in NLP has been performed. Some of
this work addresses related but different classification tasks. Three studies classify
reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002;
Dave, Lawrence, Pennock 2003). The input is assumed to be a review, so this task
does not include finding subjective documents in the first place. The first study listed
above (Turney 2002) uses a variation of the semantic similarity procedure presented
in Wiebe (2000) (Section 3.4). The third (Dave, Lawrence, and Pennock 2003) uses n-
gram features identified with a variation of the procedure presented in Wiebe, Wilson,
and Bell (2001) (Section 3.3). Tong (2001) addresses finding sentiment timelines, that
is, tracking sentiments over time in multiple documents. For clues of subjectivity, he
uses manually developed lexical rules, rather than automatically learning them from
corpora. Similarly, Gordon et al (2003) use manually developed grammars to detect
some types of subjective language. Agrawal et al (2003) partition newsgroup authors
into camps based on quotation links. They do not attempt to recognize subjective
language.
The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloff
and Wiebe (2003) and Yu and Hatzivassiloglou (2003). The first two focus on finding
additional types of subjective clues (nouns and extraction patterns identified using
extraction pattern bootstrapping). Yu and Hatzivassiloglou (2003) perform opinion text
classification. They also use existing WSJ document classes for training and testing,
but they do not include the entire corpus in their experiments, as we do. Their opinion
piece class consists only of editorials and letters to the editor, and their nonopinion
class consists only of business and news. They report an average F-measure of 96.5%.
Our result of 94% accuracy on document level classification is almost comparable.
They also perform sentence-level classification.
We anticipate that knowledge of subjective language may be usefully exploited in
a number of NLP application areas and hope that the work presented in this article will
encourage others to experiment with subjective language in their applications. More
generally, there are many types of artificial intelligence systems for which state-of-
affairs types such as beliefs and desires are central, including systems that perform plan
recognition for understanding narratives (Dyer 1982; Lehnert et al 1983), for argument
understanding (Alvarado, Dyer, and Flowers 1986), for understanding stories from
different perspectives (Carbonell 1979), and for generating language under different
pragmatic constraints (Hovy 1987). Knowledge of linguistic subjectivity could enhance
the abilities of such systems to recognize and generate expressions referring to such
states of affairs in natural text.
6. Conclusions
Knowledge of subjective language promises to be beneficial for many NLP applica-
tions including information extraction, question answering, text categorization, and
304
Computational Linguistics Volume 30, Number 3
summarization. This article has presented the results of an empirical study in ac-
quiring knowledge of subjective language from corpora in which a number of fea-
ture types were learned and evaluated on different types of data with positive re-
sults.
We showed that unique words are subjective more often than expected and that
unique words are valuable clues to subjectivity. We also presented a procedure for au-
tomatically identifying potentially subjective collocations, including fixed collocations
and collocations with placeholders for unique words. In addition, we used the results
of a method for clustering words according to distributional similarity (Lin 1998) to
identify adjectival and verbal clues of subjectivity.
Table 9 summarizes the results of testing all of the above types of PSEs. All show
increased precision in the evaluations. Together, they show consistency in performance.
In almost all cases they perform better or worse on the same data sets, despite the
fact that different kinds of data and procedures are used to learn them. In addition,
PSEs learned using expression-level subjective-element data have precisions higher
than baseline on document-level opinion piece data, and vice versa.
Having a large stable of PSEs, it was important to disambiguate whether or not
PSE instances are subjective in the contexts in which they appear. We discovered that
the density of other potentially subjective expressions in the surrounding context is
important. If a clue is surrounded by a sufficient number of other clues, then it is
more likely to be subjective than if there were not. Parameter values were selected
using training data manually annotated at the expression level for subjective elements
and then tested on data annotated at the document level for opinion pieces. All of
the selected parameters led to increases in precision on the test data, and most lead to
increases over 100%. Once again we found consistency between expression-level and
document-level annotations. PSE sets defined by density have high precision in both
the subjective-element data and the opinion piece data. The large differences between
training and testing suggest that our results are not brittle.
Using a density feature selected from a training set, sentences containing high-
density PSEs were extracted from a separate test set, and manually annotated by two
judges. Fully 93% of the sentences extracted were found to be subjective or to be near
subjective sentences. Admittedly, the chosen density feature is a high-precision, low-
frequency one. But since the process is fully automatic, the feature could be applied to
more unannotated text to identify regions containing subjective sentences. In addition,
because the precision and frequency of the density features are stable across data sets,
lower-precision but higher-frequency options are available.
Finally, the value of the various types of PSEs was demonstrated with the task
of opinion piece classification. Using the k-nearest-neighbor classification algorithm
with leave-one-out cross-validation, a classification accuracy of 94% was achieved on
a large test set, with a reduction in error of 28% from the baseline.
Future work is required to determine how to exploit density features to improve
the performance of text categorization algorithms. Another area of future work is
searching for clues to objectivity, such as the politeness features used by Spertus (1997).
Still another is identifying the type of a subjective expression (e.g., positive or neg-
ative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) on
classifying lexemes to the classification of instances in context (compare, e.g., ?great!?
and ?oh great.?)
In addition, it would be illuminating to apply our system to data annotated with
discourse trees (Carlson, Marcu, and Okurowski 2001). We hypothesize that most ob-
jective sentences identified by our system are dominated in the discourse by subjective
sentences and that we are moving toward identifying subjective discourse segments.
305
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Acknowledgments
We thank the anonymous reviewers for
their helpful and constructive comments.
This research was supported in part by the
Office of Naval Research under grants
N00014-95-1-0776 and N00014-01-1-0381.
References
Agrawal, Rakesh, Sridhar Rajagopalan,
Ramakrishnan Srikant, and Yirong Xu.
2003. Mining newsgroups using networks
arising from social behavior. In
Proceedings of the 12th International World
Wide Web Conference (WWW2003),
Budapest, May 20-24.
Alvarado, Sergio J., Michael G. Dyer, and
Margot Flowers. 1986. Editorial
comprehension in oped through
argument units. In Proceedings of the Fifth
National Conference on Artificial Intelligence
(AAAI-86), Philadelphia, August 11?15,
pages 250?256.
Anderson, Clifford W. and George C.
McMaster. 1989. Quantification of
rewriting by the Brothers Grimm: A
comparison of successive versions of
three tales. Computers and the Humanities,
23(4?5):341?346.
Aone, Chinatsu, Mila Ramos-Santacruz, and
William J. Niehaus. 2000. Assentor: An
NLP-based solution to e-mail monitoring.
In Proceedings of the 12th Innovative
Applications of Artificial Intelligence
Conference (IAAI-2000), Austin, TX,
August 1?3, pages 945?950.
Argamon, Shlomo, Moshe Koppel, and
Galit Avneri. 1998. Routing documents
according to style. In Proceedings of the
First International Workshop on Innovative
Internet Information Systems (IIIS-98), Pisa,
Italy, June 8?9.
Banfield, Ann. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
Barzilay, Regina, Michael Collins, Julia
Hirschberg, and Steve Whittaker. 2000.
The rules behind roles: Identifying
speaker role in radio broadcasts. In
Proceedings of the 17th National Conference on
Artificial Intelligence (AAAI-2000), Austin,
TX, July 30?August 3, pages 679?684.
Biber, Douglas. 1993. Co-occurrrence
patterns among collocations: A tool for
corpus-based lexical knowledge
acquisition. Computational Linguistics,
19(3):531?538.
Brill, Eric. 1992. A simple rule-based part of
speech tagger. In Proceedings of the 3rd
Conference on Applied Natural Language
Processing (ANLP-92), Trenton, Italy, April
1?3 pages 152?155.
Bruce, Rebecca and Janyce Wiebe. 1999.
Recognizing subjectivity: A case study of
manual tagging. Natural Language
Engineering, 5(2):187?205.
Carbonell, Jaime G. 1979. Subjective
Understanding: Computer Models of Belief
Systems. Ph.D. thesis, and Technical
Report no. 150, Department of Computer
Science, Yale University, New Haven, CT.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2001. Building a
discourse-tagged corpus in the
framework of rhetorical structure theory.
In Proceedings of the Second SIG dial
Workshop on Discourse and Dialogue
(SIGdial-2001), Aalborg, Denmark,
September 1?2, pages 30?39.
Chatman, Seymour. 1978. Story and
Discourse: Narrative Structure in Fiction and
Film. Cornell University Press, Ithaca, NY.
Church, Kenneth W. and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16:22?29.
Cohn, Dorrit. 1978. Transparent Minds:
Narrative Modes for Representing
Consciousness in Fiction. Princeton
University Press, Princeton, NJ.
Copeck, Terry, Kim Barker, Sylvain Delisle,
and Stan Szpakowicz. 2000. Automating
the measurement of linguistic features to
help classify texts as technical. In
Proceedings of the Seventh Conference on
Automatic NLP (TALN-2000), Lausanne,
Switzerland, October 16?18, pages
101?110.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word cooccurrence probabilities. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics
(ACL-94), Las Cruces, NM, June 27?30,
pages 272?278.
Dave, Kushal, Steve Lawrence, and
David M. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and
semantic classification of produce
reviews. In Proceedings of the 12th
International World Wide Web Conference
(WWW2003), Budapest, May 20?24.
Dolez?el, Lubomir. 1973. Narrative Modes in
Czech Literature. University of Toronto
Press, Toronto, Ontario, Canada.
Dyer, Michael G. 1982. Affect processing for
narratives. In Proceedings of the Second
National Conference on Artificial Intelligence
(AAAI-82), Pittsburgh, August 18?20,
pages 265?268.
Everitt, Brian S. 1977. The Analysis of
Contingency Tables. Chapman and Hall,
London.
306
Computational Linguistics Volume 30, Number 3
Fludernik, Monika. 1993. The Fictions of
Language and the Languages of Fiction.
Routledge, London.
Fodor, Janet Dean. 1979. The Linguistic
Description of Opaque Contexts, volume 13
of Outstanding Dissertations in Linguistics.
Garland, New York and London.
General-Inquirer, The. 2000. Available at
http://www.wjh.harvard.edu/?
inquirer/spreadsheet guide.htm.
Gordon, Andrew, Abe Kazemzadeh, Anish
Nair, and Milena Petrova. 2003.
Recognizing expressions of commonsense
psychology in English text. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03),
Sapporo, Japan, July 7?12, pages 208?215.
Hart, Roderick P. 1984. Systematic analysis
of political discourse: The development of
diction. In K. Sanders et al, editors,
Political Communication Yearbook: 1984.
Southern Illinois University Press,
Carbondale, pages 97?134.
Hatzivassiloglou, Vasileios and Kathy
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics (ACL-97),
Madrid, July 12, pages 174?181.
Heise, David. 2000. Affect control theory.
Available at
http://www.indiana.edu/socpsy/ACT/
index.htm.
Hindle, Don. 1990. Noun classification from
predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-90), Pittsburgh, June 6?9, pages
268?275.
Hovy, Eduard. 1987. Generating Natural
Language under Pragmatic Constraints. Ph.D.
thesis, Yale University, New Haven, CT.
Karlgren, Jussi and Douglass Cutting. 1994.
Recognizing text genres with simple
metrics using discriminant analysis. In
Proceedings of the Fifteenth International
Conference on Computational Linguistics
(COLING-94), pages 1071?1075.
Karp, Daniel, Yves Schabes, Martin Zaidel,
and Dania Egedi. 1994. A freely available
wide coverage morphological analyzer for
English. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING-94), Nantes, France
pages 922?928.
Kaufer, David. 2000. Flaming: A white paper.
Available at www.eudora.com.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Schu?tze. 1997. Automatic
detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics (ACL-97),
Madrid, July 7?12, pages 32?38.
Kuroda, S.-Y. 1973. Where epistemology,
style and grammar meet: A case study
from the Japanese. In P. Kiparsky and
S. Anderson, editors, A Festschrift for
Morris Halle. Holt, Rinehart & Winston,
New York, pages 377?391.
Kuroda, S.-Y. 1976. Reflections on the
foundations of narrative theory?from a
linguistic point of view. In T. A. van Dijk,
editor, Pragmatics of Language and
Literature. North-Holland, Amsterdam,
pages 107?140.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99), College Park, MD,
pages 25?32.
Lee, Lillian and Fernando Pereira. 1999.
Distributional similarity models:
Clustering vs. nearest neighbors. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), College Park, MD,
pages 33?40.
Lehnert, Wendy G., Michael Dyer, Peter
Johnson, C. J. Yang, and Steve Harley.
1983. BORIS: An Experiment in In-Depth
Understanding of Narratives. Artificial
Intelligence, 20:15?62.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics (ACL-98),
Montreal, August 10?14, pages 768?773.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), College Park, MD, pages
317?324.
Litman, Diane J. and Rebecca J. Passonneau.
1995. Combining multiple knowledge
sources for discourse segmentation. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics
(ACL-95), Cambridge, MA, June 26?30,
pages 108?115.
Macleod, Catherine, Ralph Grishman, and
Adam Meyers. 1998. Complex syntax
reference manual. Technical report, New
York University.
Marcu, Daniel, Magdalena Romera, and
Estibaliz Amorrortu. 1999. Experiments in
constructing a corpus of discourse trees:
Problems, annotation choices, issues. In
Proceedings of the International Workshop on
Levels of Representation in Discourse
(LORID-99), Edinburgh, July 6?9 pages
71?78.
307
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Marcus, Mitch, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mitchell, Tom. 1997. Machine Learning.
McGraw-Hill, Boston.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002),
Philadelphia, July 6?7, pages 79?86.
Quirk, Randolph, Sidney Greenbaum,
Geoffry Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, New York.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level Bootstrapping. In Proceedings
of the 16th National Conference on Artificial
Intelligence (AAAI-1999), Orlando, FL, July
18?22, pages 474?479.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for subjective
expressions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), Sapporo, Japan,
July 11?12, pages 105?112.
Riloff, Ellen, Janyce Wiebe, and Theresa
Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping. In
Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL-2003),
Edmonton, Alberta, Canada, May 31?June
1, pages 25?32.
Sack, Warren. 1995. Representing and
recognizing point of view. In Proceedings
of the AAAI Fall Symposium on AI
Applications in Knowledge Navigation and
Retrieval, Cambridge, MA, page 152.
Samuel, Ken, Sandra Carberry, and
K. Vijay-Shanker. 1998. Dialogue act
tagging with transformation-based
learning. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics (ACL-98), Montreal, August
10?14, pages 1150?1156.
Smajda, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19:143?177.
Spertus, Ellen. 1997. Smokey: Automatic
recognition of hostile messages. In
Proceedings of the Ninth Annual Conference
on Innovative Applications of Artificial
Intelligence (IAAI-97), Providence, RI, July
27?31, pages 1058?1065.
Stein, Dieter and Susan Wright, editors.
1995. Subjectivity and Subjectivisation.
Cambridge University Press, Cambridge.
Terveen, Loren, Will Hill, Brian Amento,
David McDonald, and Josh Creter. 1997.
Building task-specific interfaces to high
volume conversational data. In
Proceedings of the Conference on Human
Factors in Computing Systems (CHI-97), Los
Angeles, April 18?23, pages 226?233.
Teufel, Simone and Marc Moens. 2000.
What?s yours and what?s mine:
Determining intellectual attribution in
scientific texts. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing and the Workshop on
Very Large Corpora (EMNLP/VLC-2000),
Hong Kong, October 7?8, pages 9?17.
Tong, Richard. 2001. An operational system
for detecting and tracking opinions in
on-line discussions. In Working Notes of the
SIGIR Workshop on Operational Text
Classification, New Orleans, September
9?13, pages 1?6.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL-2000), Philadelphia, July 7?12, pages
417?424.
Uspensky, Boris. 1973. A Poetics of
Composition. University of California
Press, Berkeley, and Los Angeles.
van Dijk, Teun A. 1988. News as Discourse.
Erlbaum, Hillsdale, NJ.
Weeber, Marc, Rein Vos, and R. Harald
Baayen. 2000. Extracting the
lowest-frequency words: Pitfalls and
possibilities. Computational Linguistics,
26(3):301?317.
Wiebe, Janyce and Theresa Wilson. 2002.
Learning to disambiguate potentially
subjective expressions. In Proceedings of the
Sixth Conference on Natural Language
Learning (CoNLL-2002), Taipei, Taiwan,
pages 112?118.
Wiebe, Janyce. 1990. Recognizing Subjective
Sentences: A Computational Investigation of
Narrative Text. Ph.D. thesis, State
University of New York at Buffalo.
Wiebe, Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233?287.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the 17th National Conference on Artificial
Intelligence (AAAI-2000), Austin, TX, July
30?August 3, pages 735?740.
Wiebe, Janyce, Eric Breck, Chris Buckley,
Claire Cardie, Paul Davis, Bruce Fraser,
Diane Litman, David Pierce, Ellen Riloff,
Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and
308
Computational Linguistics Volume 30, Number 3
organizing opinions expressed in the
world press. In Working Notes of the AAAI
Spring Symposium in New Directions in
Question Answering, Palo Alto, CA, pages
12?19.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second ACL SIGdial Workshop on Discourse
and Dialogue (SIGdial-2001), Aalborg,
Denmark, September 1?2, pages 186?195.
Wiebe, Janyce, Rebecca Bruce, and Thomas
O?Hara. 1999. Development and use of a
gold standard data set for subjectivity
classifications. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99), College
Park, MD, pages 246?253.
Wiebe, Janyce, Kenneth McKeever, and
Rebecca Bruce. 1998. Mapping
collocational properties into machine
learning features. In Proceedings of the Sixth
Workshop on Very Large Corpora (WVLC-98),
Montreal, August 15?16, pages 225?233.
Wiebe, Janyce and William J. Rapaport.
1986. Representing de re and de dicto belief
reports in discourse and narrative.
Proceedings of the IEEE, 74:1405?1413.
Wiebe, Janyce and William J. Rapaport.
1988. A computational theory of
perspective and reference in narrative. In
Proceedings of the 26th Annual Meeting of the
Association for Computational Linguistics
(ACL-88), Buffalo, NY, pages 131?138.
Wiebe, Janyce M. and William J. Rapaport.
1991. References in narrative text. Nou?s,
25(4):457?486.
Wiebe, Janyce, Theresa Wilson, and
Matthew Bell. 2001. Identifying
collocations for recognizing opinions. In
Proceedings of the ACL-01 Workshop on
Collocation: Computational Extraction,
Analysis, and Exploitation, Toulouse,
France, July 7, pages 24?31.
Wilson, Theresa and Janyce Wiebe. 2003.
Annotating opinions in the world press.
In Proceedings of the Fourth SIGdial Workshop
on Discourse and Dialogue (SIGdial-2003),
Sapporo, Japan, July 5?6, pages 13?22.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), Sapporo, Japan,
July 11?12, pages 129?136.
A Corpus Study of Evaluative and Speculative Language
Janyce Wiebe

, Rebecca Bruce
y
, Matthew Bell

, Melanie Martin
z
, Theresa Wilson

University of Pittsburgh

, University of North Carolina at Asheville
y
, New Mexico State University
z
wiebe,mbell,twilson@cs.pitt.edu, bruce@cs.unca.edu, mmartin@cs.nmsu.edu
Abstract
This paper presents a corpus study
of evaluative and speculative language.
Knowledge of such language would be
useful in many applications, such as
text categorization and summarization.
Analyses of annotator agreement and of
characteristics of subjective language are
performed. This study yields knowl-
edge needed to design eective machine
learning systems for identifying subjec-
tive language.
1 Introduction
Subjectivity in natural language refers to aspects
of language used to express opinions and evalua-
tions (Baneld, 1982; Wiebe, 1994). Subjectivity
tagging is distinguishing sentences used to present
opinions and other forms of subjectivity (subjec-
tive sentences) from sentences used to objectively
present factual information (objective sentences).
This task is especially relevant for news report-
ing and Internet forums, in which opinions of var-
ious agents are expressed. There are numerous
applications for which subjectivity tagging is rele-
vant. Two are information retrieval and informa-
tion extraction. Current extraction and retrieval
technology focuses almost exclusively on the sub-
ject matter of documents. However, additional
aspects of a document inuence its relevance, in-
cluding, e.g., the evidential status of the material
presented, and the attitudes expressed about the
topic (Kessler et al, 1997). Knowledge of subjec-
tive language would also be useful in ame recog-
nition (Spertus, 1997; Kaufer, 2000), email clas-
sication (Aone et al, 2000), intellectual attribu-
tion in text (Teufel and Moens, 2000), recogniz-
ing speaker role in radio broadcasts (Barzilay et
al., 2000), review mining (Terveen et al, 1997),
generation and style (Hovy, 1987), clustering doc-
uments by ideological point of view (Sack, 1995),
and any other application that would benet from
knowledge of how opinionated the language is, and
whether or not the writer purports to objectively
present factual material.
To use subjectivity tagging in applications,
good linguistic clues must be found. As with many
pragmatic and discourse distinctions, existing lex-
ical resources are not comprehensively coded for
subjectivity. The goal of our current work is learn-
ing subjectivity clues from corpora. This paper
contributes to this goal by empirically examin-
ing subjectivity. We explore annotating subjectiv-
ity at dierent levels (expression, sentence, docu-
ment) and produce corpora annotated at dierent
levels. Annotator agreement is analyzed to un-
derstand and assess the viability of such annota-
tions. In addition, because expression-level anno-
tations are ne-grained and thus very informative,
these annotations are examined to gain knowledge
about subjectivity.
We also use our annotations and existing ed-
itorial annotations to generate and test features
of subjectivity. Altogether, the observations and
results of these studies provide valuable informa-
tion that will facilitate designing eective machine
learning systems for recognizing subjectivity.
The remainder of this paper rst provides back-
ground about subjectivity, then presents results
for document-level annotations, followed by an
analysis of expression-level annotations. Results
for features generated using document-level anno-
tations are next, ending with conclusions.
2 Subjectivity
Sentence (1) is an example of a simple subjective
sentence, and (2) is an example of a simple objec-
tive sentence:
1
(1) At several dierent layers, it's a fascinating
tale.
1
The term subjectivity is due to Ann Baneld
(1982). For references to work on subjectivity, please
see (Baneld, 1982; Fludernik, 1993; Wiebe, 1994;
Stein and Wright, 1995).
(2) Bell Industries Inc. increased its quarterly to
10 cents from 7 cents a share.
The main types of subjectivity are:
1. Evaluation. This category includes emotions
such as hope and hatred as well as evalua-
tions, judgements, and opinions. Examples
of expressions involving positive evaluation
are enthused, wonderful, and great product!.
Examples involving negative evaluation are
complained, you idiot!, and terrible product.
2. Speculation. This category includes anything
that removes the presupposition of events oc-
curring or states holding, such as speculation
and uncertainty. Examples of speculative ex-
pressions are speculated, and maybe.
Following are examples of strong negative
evaluative language from a corpus of Usenet
newsgroup messages:
(3a) I had in mind your facts, buddy, not hers.
(3b) Nice touch. \Alleges" whenever facts posted
are not in your persona of what is \real".
Following is an example of opinionated, edito-
rial language, taken from an editorial in the Wall
Street Journal:
(4) We stand in awe of the Woodstock genera-
tion's ability to be unceasingly fascinated by the
subject of itself.
Sentences (5) and (6) illustrate the fact that
sentences about speech events may be subjective
or objective:
(5) Northwest Airlines settled the remaining
lawsuits led on behalf of 156 people killed in
a 1987 crash, but claims against the jetliner's
maker are being pursued, a federal judge said.
(6) \The cost of health care is eroding our stan-
dard of living and sapping industrial strength,"
complains Walter Maher, a Chrysler health-and-
benets specialist.
In (5), the material about lawsuits and claims is
presented as factual information, and a federal
judge is given as the source of information. In
(6), in contrast, a complaint is presented. An NLP
system performing information extraction on (6)
should not treat the material in the quoted string
as factual information, with the complainer as a
source of information, whereas a corresponding
treatment of sentence (5) would be appropriate.
Subjective sentences often contain individual
expressions of subjectivity. Examples are fasci-
nating in (1), and eroding, sapping, and complains
in (6). The following paragraphs mention aspects
of subjectivity expressions that are relevant for
NLP applications.
First, although some expressions, such as !, are
subjective in all contexts, many, such as sapping
and eroding, may or may not be subjective, de-
pending on the context in which they appear. A
potential subjective element (PSE) is a linguistic
element that may be used to express subjectivity.
A subjective element is an instance of a potential
subjective element, in a particular context, that is
indeed subjective in that context (Wiebe, 1994).
Second, a subjective element expresses the sub-
jectivity of a source, who may be the writer or
someone mentioned in the text. For example, the
source of fascinating in (1) is the writer, while
the source of the subjective elements in (6) is Ma-
her. In addition, a subjective element has a tar-
get, i.e., what the subjectivity is about or directed
toward. In (1), the target is a tale; in (6), the tar-
get of Maher's subjectivity is the cost of health
care. These are examples of object-centric sub-
jectivity, which is about an object mentioned in
the text (other examples: \I love this project";
\The software is horrible"). Subjectivity may also
be addressee-oriented, i.e., directed toward the lis-
tener or reader (e.g., \You are an idiot").
Third, there may be multiple subjective ele-
ments in a sentence, possibly of dierent types
and attributed to dierent sources and targets.
For example, in (4), subjectivity of the Woodstock
generation is described (specically, its fascina-
tion with itself). In addition, subjectivity of the
writer is expressed (e.g., `we stand in awe'). As de-
scribed below, individual subjective elements were
annotated as part of this work, rening previous
work on sentence-level annotations. Finally, PSEs
may be complex expressions such as `village id-
iot', `powers that be', `You' NP, and `What a'
NP. There is a great variety of such expressions,
including many studied under the rubric of idioms
(see, for example, (Nunberg et al, 1994)). We ad-
dress learning such expressions in another project.
3 Previous Work on Subjectivity
Tagging
In previous work (Wiebe et al, 1999; Bruce and
Wiebe, 1999), a corpus of sentences from the Wall
Street Journal Treebank Corpus (Marcus et al,
1993) was manually annotated with subjectivity
classications by multiple judges. The judges were
instructed to consider a sentence to be subjective
if they perceived any signicant expression of sub-
jectivity (of any source) in the sentence, and to
consider the sentence to be objective, otherwise.
Agreement was summarized in terms of Cohen's
 (Cohen, 1960), which compares the total proba-
bility of agreement to that expected if the taggers'
classications were statistically independent (i.e.,
\chance agreement"). After two rounds of tag-
ging by three judges, an average pairwise  value
of .69 was achieved on a test set. The EM learn-
ing algorithm was used to produce corrected tags
representing the consensus opinions of the taggers
(Goodman, 1974; Dawid and Skene, 1979). An
automatic system to perform subjectivity tagging
was developed using the new tags as training and
testing data. In 10-fold cross validation experi-
ments, a probabilistic classier obtained an aver-
age accuracy on subjectivity tagging of 72.17%,
more than 20 percentage points higher than a
baseline accuracy obtained by always choosing the
more frequent class. Five part-of-speech features,
two lexical features, and a paragraph feature were
used.
To identify richer features, (Wiebe, 2000) used
Lin's (1998) method for clustering words accord-
ing to distributional similarity, seeded by a small
amount of detailed manual annotation, to auto-
matically identify adjective PSEs. There are two
parameters of this process, neither of which was
varied in (Wiebe, 2000): C, the cluster size con-
sidered, and FT , a ltering threshold, such that, if
the seed word and the words in its cluster have, as
a set, lower precision than the ltering threshold
on the training data, the entire cluster, includ-
ing the seed word, is ltered out. This process is
adapted for use in the current paper, as described
in section 7.
4 Choices in Annotation
In expression-level annotation, the judges rst
identify the sentences they believe are subjective.
They next identify the subjective elements in
the sentence, i.e., the expressions they feel are
responsible for the subjective classication. For
example (subjective elements are in parentheses):
They promised (yet) more for (really good stu).
(Perhaps you'll forgive me) for reposting his
response.
Subjective-element (expression-level) annota-
tions are probably the most natural. Ultimately,
we would like to recognize the subjective elements
in a text, and their types, targets, and sources.
However, both manual and automatic tagging at
this level are dicult because the tags are very
ne-grained, and there is no predetermined clas-
sication unit; a subjective element may be a sin-
gle word or a large expression. Thus, in the short
term, it is probably best to use subjective-element
annotations for knowledge acquisition (analysis,
training, feature generation) alone, and not target
automatic classication of subjective elements.
In this work, document-level subjectivity anno-
tations are text categories of which subjectivity
is a key aspect. We use three text categories:
editorials (Kessler et al, 1997), reviews, and
\ames", i.e., hostile messages (Spertus, 1997;
Kaufer, 2000). For ease of discussion, we group
editorials and reviews together under the term
opinion pieces.
There are benets to using such document-level
annotations. First, they are more directly re-
lated to applications (e.g., ltering hostile mes-
sages and mining reviews from Internet forums).
Second, there are existing annotations to be ex-
ploited, such as editorials and arts reviews marked
as such by newspapers, as well as on-line product
reviews accompanied by formal numerical ratings
(e.g., 4 on a scale from 1 to 5).
However, a challenging aspect of such data is
that opinion pieces and ames contain objective
sentences, while documents in other text cate-
gories contain subjective sentences. News reports
present reactions to and attitudes toward reported
events (van Dijk 1988); they often contain seg-
ments starting with expressions such as critics
claim and supporters argue. In addition, quoted-
speech sentences in which individuals express their
subjectivity are often included (Barzilay et al,
2000). On the other hand, editorials contain ob-
jective sentences presenting facts supporting the
writer's argument, and reviews contain sentences
objectively presenting facts about the product.
This \impure" aspect of opinionated text cate-
gories must be considered when such data is used
for training and testing. Some specic results are
given below in section 7.
We believe that sentence-level classications
will continue to provide an important level of
analysis. The sentence provides a prespeci-
ed classication unit
2
and, while sentence-level
judgements are not as ne-grained as subjective-
2
While sentence boundaries are not always unam-
biguous in unedited text or spoken language, the data
can always be segmented into sentence-like units be-
fore subjectivity tagging is performed.
element judgements, they do not involve the large
amount of noise we face with document-level an-
notations.
5 Document-Level Annotation
Results
5.1 Flame Annotations
In this study, newsgroup messages were assigned
the tags ame or not-ame. The corpus con-
sists of 1140 Usenet newsgroup messages, bal-
anced among the categories alt, sci, comp, and rec
in the Usenet hierarchy. The corpus was divided,
preserving the category balance, into a training set
of 778 messages and a test set of 362 messages.
The annotators were instructed to mark a mes-
sage as a ame if the \main intention of the mes-
sage is a personal attack, containing insulting or
abusive language." A number of policy decisions
were made in the instructions, dealing, primarily,
with included messages (part or all of a previous
message, included in the current message as part
of a reply). Some additional issues addressed in
the instructions were who the attack was directed
at, nonsense, sarcasm, humor, rants, and raves.
During the training phase, two annotators, MM
and R, participated in multiple rounds of tagging,
revising the annotation instructions as they pro-
ceeded. During the testing phase, MM and R in-
dependently annotated the test set, achieving a 
value on these messages of 0.69. A third annota-
tor, L, trained on 492 messages from the training
set, and then annotated 88 of the messages in the
test set. The pairwise  values on this set of 88
are: MM & R: 0.80; MM & L: 0.75; R & MM:
0.80; for an average pairwise  of .78.
This study provides evidence for the viability
of document-level ame annotation. We plan to
build a ame-recognition system in the future. As
will be seen below, MM and R also tagged this
data at the subjective-element level.
5.2 Opinion-Piece Classications
Our opinion-piece classications are built on exist-
ing annotations in the Wall Street Journal. Specif-
ically, there are articles explicitly identied to be
Editorials, Letters to the Editor, Arts & Leisure,
and Viewpoints; together, we call these opinion
pieces. This data is a good resource for subjectiv-
ity recognition. However, an inspection of some
data revealed that some editorials and reviews are
not marked as such. For example, there are arti-
cles written in the rst person, and the purpose of
the article is to present an argument rather than
cover a news story, but there is no explicit indi-
cation that they are editorials. To create high
quality test data, two judges manually annotated
WSJ data for opinion pieces. The instructions
were to nd any additional opinion pieces that
are not marked as such. The annotators also had
the option of disagreeing with the existing anno-
tations, but did not opt to do so in any instances.
One judge annotated all articles in four datasets
of the Wall Street Journal Treebank corpus (Mar-
cus et al, 1993) (W9-4, W9-10, W9-22, and W9-
33, each approximately 160K words) as well as
the corpus of Wall Street Journal articles used in
(Wiebe et al, 1999) (called WSJ-SE below). An-
other judge annotated all articles in two of the
datasets (W9-22 and W9-33).
This annotation task appears to be relatively
easy. With no training at all, the  values are very
high: .94 for dataset W9-33 and .95 for dataset
W9-22.
The agreement data for W9-22 is given in Table
1 in the form of a contingency table. In section
7, this data is used to generate and test candidate
potential subjective elements (PSEs).
6 Subjective-Element Annotation
Results and Analyses
6.1 Annotations and Data
These subsections analyze subjective element an-
notations performed on three datasets, WSJ-SE,
NG-FE, and NG-SE.
WSJ-SE is the corpus of 1001 sentences of the
Wall Street Journal Treebank Corpus referred to
above in section 3. Recall that the sentences of
this corpus were manually annotated with subjec-
tivity classications as described in (Wiebe et al,
1999; Bruce and Wiebe, 1999).
For this paper, two annotators (D and M ) were
asked to identify the subjective elements in WSJ-
SE. Specically, the taggers were given the sub-
jective sentences identied in the previous study,
and asked to put brackets around the words they
believe cause the sentence to be classied as sub-
jective.
Note that inammatory language is a kind of
subjective language. NG-FE is a subset of the
Usenet newsgroup corpus used in the document-
level ame-annotation study described in section
5.1. Specically, NG-FE consists of the 362-
message test set for taggers R and MM. For this
study, R and MM were asked to identify the ame
elements in NG-FE. Flame elements are the sub-
set of subjective elements that are perceived to
be inammatory. R and MM were asked to do
this in all 362 messages, because some messages
that were not judged to be ames at the message
level do contain individual inammatory phrases
Tagger 2
Op Not Op
Tagger 1 Op n
11
= 23 n
12
= 0 n
1+
= 23
Not Op n
21
= 2 n
22
= 268 n
2+
= 270
n
+1
= 25 n
+2
= 268 n
++
= 293
Table 1: Contingency Table for Opinion Piece Agreement in W9-22
(in these cases, the tagger does not believe that
these phrases express the main intent of the mes-
sage).
In addition to the above annotations, tagger M
performed subjective-element tagging on a dier-
ent set of Usenet newsgroup messages, corpus NG-
SE. The size of this corpus is 15413 words.
In datasets WSJ-SE and NG-SE, the taggers
were also asked to specify one of ve subjective
element types: e+ (positive evaluative), e  (neg-
ative evaluative), e? (some other type of evalua-
tion), u (uncertainty), and o (none of the above),
with the option to assign multiple types to an in-
stance. All corpora were stemmed (Karp et al,
1992) and part-of-speech tagged (Brill, 1992).
6.2 Agreement Among Taggers
There are techniques for analyzing agreement
when annotations involve segment boundaries
(Litman and Passonneau, 1995; Marcu et al,
1999), but our focus in this paper is on words.
Thus, our analyses are at the word level: each
word is classied as either appearing in a subjec-
tive element or not. Punctuation is excluded from
our analyses. The WSJ data is divided into two
subsets in this section, Exp1 and Exp2.
As mentioned above, in WSJ-SE Exp1 and
Exp2, the taggers also classied subjective ele-
ments with respect to the type of subjectivity
being expressed. Subjectivity type agreement is
again analyzed at the word level, but, in this anal-
ysis, only the words classied as belonging to sub-
jective elements by both taggers are considered.
Table 2 provides  values for word agreement
in NG-FE (the ame data) as well as for WSJ-SE
Exp1 and Exp2. The task of identifying subjec-
tive elements in a body of text is dicult, and the
agreement results reect this fact; agreement is
much stronger than that expected by chance, but
less than what we would like to see when verify-
ing a new classication. Further renement of the
coding manual is required. Additionally, it may be
possible to rene the classications automatically
using methods such as those described in (Wiebe
et al, 1999). In this analysis, we explore the pat-
terns of agreement exhibited by the taggers in an
eort to better understand the classication.
We begin by looking at word agreement. Word
agreement is higher in the ame experiment
(NG-FE) than it is in either WSJ experiment
(WSJ-SE Exp1 and Exp2). Looking at the WSJ
data provides one plausible explanation for the
lower word agreement in the WSJ experiments.
As exhibited in the subjective elements identied
for the single clause below,
D: (e+ played the role well) (e? obligatory
ragged jeans a thicket of long hair and rejection
of all things conventional)
M : (e+ well) (e? obligatory) (e- ragged) (e?
thicket) (e- rejection) (e- all things conventional)
tagger D consistently identies entire phrases
as subjective, while Tagger M prefers to select
discrete lexical items. This dierence in inter-
pretation of the tagging instructions does not
occur in the ame experiment. Nonetheless, even
within the ame data, there are many instances
where both taggers identify the same segment of
a sentence as forming a subjective element but
disagree on the boundaries of that segment, as in
the example below.
R: (classic case of you deliberately misinterpret-
ing my comments)
MM : (you deliberately misinterpreting my
comments)
These patterns of partial agreement are also evi-
dent in the  values for words from specic syn-
tactic categories (see Table 2 again). In the WSJ
data, agreement on determiners is particularly low
because they are often included as part of a phrase
by tagger D but typically not included in the spe-
cic lexical items chosen by tagger M. Interest-
ingly, in the WSJ experiments, the taggers most
frequently agreed on the selection of modals and
adjectives, while in the ame experiment, agree-
ment was highest on nouns and adjectives. The
high agreement on adjectives in both genres is con-
All Words Nouns Verbs Modals Adj's Adverbs Det's
NG-FE 0:4657 0:5213 0.4571 0:4008 0:5011 0:3576 0:4286
WSJ-SE, Exp1 0:4228 0:3999 0.4235 0:6992 0:6000 0:4328 0:2661
WSJ-SE, Exp2 0:3703 0:3705 0.4261 0:4298 0:4294 0:2256 0:1234
Table 2:  Values for Word Agreement
sistent with results from other work (Bruce and
Wiebe, 1999; Wiebe et al, 1999), but high agree-
ment on nouns in the ame data verses high agree-
ment on modals in the WSJ data suggests a genre
specic usage of these categories. This would be
the case if, for example, modals were most fre-
quently used to express uncertainty, a type of sub-
jectivity that would be relatively rare in ames.
Turning to subjective-element type, in both
WSJ experiments, the  values for type agreement
are comparable to those for word agreement. Re-
call that multiple types may be assigned to a single
subjective instance. All such instances in the WSJ
data are u in combination with an evaluative tag
(i.e., e+, e- and e?), and they are not common:
each tagger assigned multiple tags to fewer than
7% of the subjective instances. However, if partial
matches between type tags are recognized, i.e., if
they share a common tag, then the  values im-
prove signicantly. Table 3 shows both types of
results.
It is interesting to note the variation in type agree-
ment for words of dierent syntactic categories.
Agreement on adjectives is consistently high while
the agreement on the type of subjectivity ex-
pressed by modals and adverbs is consistently low.
This contrasts with the fact that word agreement
for modals, in particular, and, to a lesser extent,
adverbs was high. This lack of agreement sug-
gests that the type of subjectivity expressed by
adjectives is more easily distinguished than that
of modals or adverbs. This is particularly impor-
tant because the number of adjectives included in
subjective elements is high. In contrast, the num-
bers of modals and adverbs are relatively low.
Additional insight can be gained by combining
the 3 evaluative classications (i.e., e+, e- and
e?) to form a single tag, e, representing any
form of evaluative expression. Table 4 presents
type agreement results for the tag set e, u, o.
In contrasting Tables 3 and 4, it is surprising
to note that most of the  values decrease when
the distinction among the evaluative types is re-
moved. This suggests that the three evaluative
types are natural classications. Only for adverbs
does type agreement improve with the smaller
tag set; this indicates that it is dicult to dis-
tinguish the evaluative nature of adverbs. Note
also that agreement for modals is not impacted
by the change in tag sets. This fact supports the
hypothesis that modals are used primary to ex-
press uncertainty. As a nal point, we look at
patterns of agreement in type classication using
the models of symmetry, marginal homogeneity,
quasi-independence, and quasi-symmetry. Each
model tests for a specic pattern of agreement:
symmetry tests the interchangeability of taggers,
marginal homogeneity veries the absence of bias
among taggers, quasi-independence veries that
the taggers act independently when they disagree,
and quasi-symmetry tests for the presence of any
pattern in their disagreements. For a more com-
plete description of these models and their use
in analyzing intercoder reliability see (Bruce and
Wiebe, 1999). In short, the results presented in
Table 5 indicate that the taggers are not inter-
changeable: they exhibit biases in their type clas-
sications, and there is a pattern of correlated dis-
agreement in the assignment of the original type
tags. Surprisingly, the taggers appear to act in-
dependently when they disagree in assigning the
compressed type tags (i.e., tags e, u and o). This
shift in the pattern of disagreement between tag-
gers again suggests that the compression of the
evaluative tags was inappropriate. Additionally,
these ndings suggest that it may be possible to
automatically correct the type biases expressed
by the taggers using the technique described in
(Bruce and Wiebe, 1999), a topic that will be in-
vestigated in future work.
6.3 Uniqueness
Based on previous work (Wiebe et al, 1998), we
hypothesized that low-frequency words are associ-
ated with subjectivity. Table 6 provides evidence
that the number of unique words (words that ap-
pear just once) in subjective elements is higher
than expected. The rst row gives information
for all words and the second gives information for
words that appear just once. The gures in the
Num columns are total counts, and the gures in
the P columns give the proportion that appear in
subjective elements. The Agree columns give in-
All Words Nouns Verbs Modals Adj's Adverbs Det's
Exp1 Full Match 0:4216 0:4228 0.2933 0:1422 0:5919 0:1207 0:5000
Partial Match 0:5156 0:4570 0.4447 0:3011 0:6607 0:3305 0:5000
Exp2 Full Match 0:3041 0:2353 0.2765 0:1429 0:5794 0:1207 0:0000
Partial Match 0:4209 0:2353 0.3994 0:3494 0:6719 0:4439 0:1429
Table 3:  Values for Type Agreement Using All Types in the WSJ Data
All Words Nouns Verbs Modals Adj's Adverbs Det's
Exp1 Full Match 0:3377 0:0440 0.1648 0:1968 0:5443 0:3810 0:0000
Partial Match 0:5287 0:1637 0.3765 0:4903 0:8125 0:3810 0:0000
Exp2 Full Match 0:2569 0:0000 0.1923 0:1509 0:4783 0:1707 0:1429
Partial Match 0:4789 0:0000 0.4167 0:4000 0:8056 0:7671 0:4000
Table 4:  Values for Type Agreement Using E,O,U in the WSJ Data
Sym. M.H. Q.S. Q.I.
Exp1 All Types G
2
112:351 92:447 19:904 66:771
Sig. 0:000 0:000 0:527 0:007
e,o,u G
2
85:478 84:142 1:336 12:576
Sig. 0:000 0:000 0:248 0:027
Exp2 All Types G
2
94:669 76:247 18:422 58:892
Sig. 0:000 0:000 0:241 0:001
e,o,u G
2
66:822 66:819 0:003 0:0003
Sig. 0:000 0:000 0:986 0:987
Table 5: Tests for Patterns of Agreement in WSJ Type-Tagged Data
WSJ-SE NG-FE
D M Agree Agree R MM
Num P Num P Num P Num P Num P Num P
All words 18341 .07 18341 .08 16857 .04 15413 .15 86279 .01 88210 .02
unique 2615 .14 2615 .20 2522 .15 2348 .17 5060 .07 4836 .03
Table 6: Proportions of Unique Words in Subjective Elements
formation for the subset of the corresponding data
set upon which the two annotators agree.
Comparison of rows 1 and 2 across columns
shows that the proportion of unique words that
are subjective is higher than the proportion of all
words that are subjective. In all cases, this dier-
ence in proportions is highly statistically signi-
cant.
6.4 Types and Context
An interesting question is, when a word appears
in multiple subjective elements, are those subjec-
tive elements all the same type? Table 7 shows
that a signicant portion are used in more than
one type. Each item considered in the table is a
word-POS pair that appears more than once in the
corpus. The gures shown are the total number of
word-POS items that appear more than once (the
columns labeled MultInst) and the proportion of
those items that appear in more than one type
of subjective element (the columns labeled Mult-
Type). These results highlight the need for contex-
tual disambiguation. For example, one thinks of
great as a positive evaluative term, but its polarity
depends on the context; it can be used negatively
evaluatively in a context such as \Just great." A
goal of performing subjective-element annotations
is to support learning such local contextual inu-
ences.
7 Generating and Testing PSEs
using Document-Level
Annotations
This section uses the opinion-piece annotations to
expand our set of PSEs beyond those that can be
derived from the subjective-element annotations.
Precision is used to assess feature quality. The
precision of feature F for class C is the number
of Fs that occur in units of class C over the total
number of Fs that occur anywhere in the data.
An important motivation for using the opinion-
piece data is that there is a large amount of it,
and manually rening existing annotations as de-
scribed in section 5.2 is much easier and more re-
liable than other types of subjectivity annotation.
However, we cannot expect absolutely high pre-
cisions for two reasons. First, the distribution of
opinions and non-opinions is highly skewed in fa-
vor of non-opinions. For example, in Table 1, tag-
ger 1 classies only 23 of 293 articles as opinion
pieces. Second, as discussed in section 4, opin-
ion pieces contain objective sentences and non
opinion-pieces contain subjective sentences. For
example, in WSJ-SE, which has been annotated
at the sentence and document levels, 70% of the
sentences in opinion pieces are subjective and 30%
are objective. In non-opinion pieces, 44% of the
sentences are subjective and only 56% are objec-
tive.
To give an idea of expected precisions, let us
consider the precision of subjective sentences with
respect to opinion pieces. Suppose that 15% of
the sentences in the dataset are in opinions, 85%
in non-opinions. Let us assume the proportions of
subjective and objective sentences in opinion and
non-opinion pieces given just above. Let N be the
total number of sentences. The desired precision
is the number of subjective sentences in opinions
over the total number of subjective sentences. It
is .22:
p=.15 * N * .70 / (.15 * N * .70 + .85 * N * .44).
In addition, we are assessing PSEs, which are
only potentially subjective; many have objective
as well as subjective uses.
Thus, even if precisions are much lower than 1,
we use increases in precision over a baseline as ev-
idence of promising PSEs. The baseline for com-
parison is the number of word instances in opin-
ion pieces, divided by the total number of word
instances. Table 8 shows the precisions for three
types of PSEs. The freq columns give total fre-
quencies, and the +prec columns show the im-
provements in precision from the baseline. The
baseline precisions are given at the bottom of the
table.
As mentioned above, (Wiebe, 2000) showed suc-
cess automatically identifying adjective PSEs us-
ing Lin's method, seeded by a small amount of de-
tailed manual annotations. Desiring to move away
from manually annotated data, for this paper the
same process is used, but the seed words are all
the adjectives (verbs) in the training data. In ad-
dition, in the current setting, there are no a priori
values to use for parameters C (cluster size) and
FT (ltering threshold), as there were in (Wiebe,
2000), and results vary with dierent parameter
settings. Thus, a train-validate-test process is ap-
propriate. In Table 8, the numbers given under,
e.g., W9-10, are the results obtained when W9-10
is used as the test set. One of the other datasets,
say W9-22, was used as the training set, meaning
that all the adjectives (verbs) in that dataset are
the seed words, and all ltering was performed us-
ing only that data. The seed-ltering process was
repeated with dierent settings of C and FT , pro-
ducing a dierent set of adjectives (verbs) for each
setting. A third dataset, say W9-33, was used as a
validation set, i.e., among all the sets of adjectives
generated from the training set, those with good
performance on the validation set were selected as
WSJ-SE-M WSJ-SE-D NG-SE-M
MultInst MultType MultInst MultType MultInst MultType
413 .17 378 .16 571 .29
Table 7: Word-POS-Types Used in Multiple Types of Subjective Elements
W9-10 W9-22 W9-33 W9-04
freq +prec freq +prec freq +prec freq +prec
adjectives 373 .21 1340 .11 2137 .09 2537 .14
verbs 721 .16 1436 .08 3139 .07 3720 .11
unique words 6065 .10 5441 .07 6045 .06 6171 .09
baseline precision .17 .13 .14 .18
freq: Total frequency +prec: Increase in precision over baseline
Table 8: Frequencies and Increases in Precision
the PSEs to test on the test set. A set was consid-
ered to have good performance on the validation
set if its precision is at least .25 and its frequency
is at least 100. Since this process is meant to
be a method for mining existing document-level
annotations for PSEs, the existing opinion-piece
annotations were used for training and validation.
Our manual opinion-piece annotations were used
for testing.
The row labeled unique words shows the preci-
sion on the test set of the individual words that
are unique in the test set. The increase over base-
line precision shows that low-frequency words can
be informative for recognizing subjectivity.
Note that the features all do better and worse
on the same data sets. This shows that the subjec-
tivity is somehow harder to identify in, say, W9-33
than in W9-10; it also shows an important consis-
tency among the features, even though they are
identied in dierent ways.
8 Conclusions
This paper presents the results of an empirical ex-
amination of subjectivity at the dierent levels of
a text: the expression level, the sentence level,
and the document level. While analysis of subjec-
tivity is perhaps most natural and precise at the
expression level, document-level annotations are
freely available from a number of sources and are
appropriate for many applications. The sentence-
level annotation is a workable intermediate level:
sentence-level judgments are not as ne-grained as
expression-level judgments, and they don't involve
the large amount of noise found at the document
level.
As part of this examination, we present a study
of annotator agreement characterizing the di-
culty of identifying subjectivity at the dierent
levels of a text. The results demonstrate that not
only can subjectivity be identied at the docu-
ment level with high reliability, but that it is also
possible to identify expression-level subjectivity,
albeit with lower reliability.
Using manual annotations, we are able to char-
acterize subjective language. At the expression
level, we found that it is natural to distinguish
among positively evaluative, negatively evalua-
tive, and speculative uses of a word. We also
found that subjective text contains a high pro-
portion of unique word occurrences, much more so
than ordinary text. Rather than ignoring or dis-
carding unique words, we demonstrate that the
occurrence of a unique word is a PSE. We also
found that agreement is higher for some syntac-
tic word classes, e.g., for adjectives in comparison
with determiners.
Finally, we are able to mine PSEs from text
tagged at the document level. Given the diculty
of evaluating PSEs in document-level subjectiv-
ity classication due to the mix of subjective and
objective sentences, the PSEs identied in this
study exhibit relatively high precision. In future
work, we will investigate document-level classi-
cation using these PSEs, as well as other methods
for extracting PSEs from text tagged at the doc-
ument level; methods to be investigated include
mutual-bootstrapping and/or co-training.
References
C. Aone, M. Ramos-Santacruz, and W. Niehaus.
2000. Assentor: An nlp-based solution to e-mail
monitoring. In Proc. IAAI-2000, pages 945{
950.
A. Baneld. 1982. Unspeakable Sentences. Rout-
ledge and Kegan Paul, Boston.
R. Barzilay, M. Collins, J. Hirschberg, and
S. Whittaker. 2000. The rules behind roles:
Identifying speaker role in radio broadcasts. In
Proc. AAAI.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of the 3rd Conference on Ap-
plied Natural Language Processing (ANLP-92),
pages 152{155.
R. Bruce and J. Wiebe. 1999. Recognizing subjec-
tivity: A case study of manual tagging. Natural
Language Engineering, 5(2).
J. Cohen. 1960. A coecient of agreement for
nominal scales. Educational and Psychological
Meas., 20:37{46.
A. P. Dawid and A. M. Skene. 1979. Max-
imum likelihood estimation of observer error-
rates using the EM algorithm. Applied Statis-
tics, 28:20{28.
M. Fludernik. 1993. The Fictions of Language
and the Languages of Fiction. Routledge, Lon-
don.
L. Goodman. 1974. Exploratory latent structure
analysis using both identiable and unidenti-
able models. Biometrika, 61:2:215{231.
E. Hovy. 1987. Generating Natural Language un-
der Pragmatic Constraints. Ph.D. thesis, Yale
University.
D. Karp, Y. Schabes, M. Zaidel, and D. Egedi.
1992. A freely available wide coverage mor-
phological analyzer for English. In Proc. of
the 14th International Conference on Compu-
tational Linguistics (COLING-92).
D. Kaufer. 2000. Flaming: A White Paper.
www.eudora.com.
B. Kessler, G. Nunberg, and H. Schutze. 1997.
Automatic detection of text genre. In Proc.
ACL-EACL-97.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. COLING-ACL '98,
pages 768{773.
Diane J. Litman and R. J. Passonneau. 1995.
Combining multiple knowledge sources for dis-
course segmentation. In Proc. 33rd Annual
Meeting of the Association for Computational
Linguistics (ACL-95), pages 108{115. Associa-
tion for Computational Linguistics, june.
D. Marcu, M. Romera, and E. Amorrortu. 1999.
Experiments in constructing a corpus of dis-
course trees: Problems, annotation choices, is-
sues. In The Workshop on Levels of Represen-
tation in Discourse, pages 71{78.
M. Marcus, Santorini, B., and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The penn treebank. Computational Lin-
guistics, 19(2):313{330.
G. Nunberg, I. Sag, and T. Wasow. 1994. Idioms.
Language, 70:491{538.
W. Sack. 1995. Representing and recognizing
point of view. In Proc. AAAI Fall Symposium
on AI Applications in Knowledge Navigation
and Retrieval.
E. Spertus. 1997. Smokey: Automatic recogni-
tion of hostile messages. In Proc. IAAI.
D. Stein and S. Wright, editors. 1995. Subjectiv-
ity and Subjectivisation. Cambridge University
Press, Cambridge.
L. Terveen, W. Hill, B. Amento, D. McDonald,
and J. Creter. 1997. Building task-specic in-
terfaces to high volume conversational data. In
Proc. CHI 97, pages 226{233.
S. Teufel and M. Moens. 2000. What's yours and
what's mine: Determining intellectual attribu-
tion in scientic texts. In Proc. Joint SIGDAT
Converence on EMNLP and VLC.
J. Wiebe, K. McKeever, and R. Bruce. 1998.
Mapping collocational properties into machine
learning features. In Proc. 6th Workshop on
Very Large Corpora (WVLC-98), pages 225{
233, Montreal, Canada, August. ACL SIGDAT.
J. Wiebe, R. Bruce, and T. O'Hara. 1999. Devel-
opment and use of a gold standard data set for
subjectivity classications. In Proc. 37th An-
nual Meeting of the Assoc. for Computational
Linguistics (ACL-99), pages 246{253, Univer-
sity of Maryland, June. ACL.
J. Wiebe. 1994. Tracking point of view in narra-
tive. Computational Linguistics, 20(2):233{287.
J. Wiebe. 2000. Learning subjective adjectives
from corpora. In 17th National Conference on
Articial Intelligence (AAAI-2000).
Class-based Collocations for Word-Sense Disambiguation
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003-8001
tomohara@cs.nmsu.edu
Rebecca Bruce
Department of Computer Science
University of North Carolina at Asheville
Asheville, NC 28804-3299
bruce@cs.unca.edu
Jeff Donner
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003-8001
jdonner@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260-4034
wiebe@cs.pitt.edu
Abstract
This paper describes the NMSU-Pitt-UNCA
word-sense disambiguation system participat-
ing in the Senseval-3 English lexical sample
task. The focus of the work is on using seman-
tic class-based collocations to augment tradi-
tional word-based collocations. Three separate
sources of word relatedness are used for these
collocations: 1) WordNet hypernym relations;
2) cluster-based word similarity classes; and 3)
dictionary definition analysis.
1 Introduction
Supervised systems for word-sense disambigua-
tion (WSD) often rely upon word collocations
(i.e., sense-specific keywords) to provide clues
on the most likely sense for a word given the
context. In the second Senseval competition,
these features figured predominantly among the
feature sets for the leading systems (Mihalcea,
2002; Yarowsky et al, 2001; Seo et al, 2001).
A limitation of such features is that the words
selected must occur in the test data in order for
the features to apply. To alleviate this problem,
class-based approaches augment word-level fea-
tures with category-level ones (Ide and Ve?ronis,
1998; Jurafsky and Martin, 2000). When ap-
plied to collocational features, this approach ef-
fectively uses class labels rather than wordforms
in deriving the collocational features.
This research focuses on the determination
of class-based collocations to improve word-
sense disambiguation. We do not address refine-
ment of existing algorithms for machine learn-
ing. Therefore, a commonly used decision tree
algorithm is employed to combine the various
features when performing classification.
This paper describes the NMSU-Pitt-
UNCA system we developed for the third
Senseval competition. Section 2 presents an
overview of the feature set used in the system.
Section 3 describes how the class-based colloca-
tions are derived. Section 4 shows the results
over the Senseval-3 data and includes detailed
analysis of the performance of the various col-
locational features.
2 System Overview
We use a decision tree algorithm for word-sense
disambiguation that combines features from the
local context of the target word with other lex-
ical features representing the broader context.
Figure 1 presents the features that are used
in this application. In the first Senseval com-
petition, we used the first two groups of fea-
tures, Local-context features and Collocational
features, with competitive results (O?Hara et al,
2000).
Five of the local-context features represent
the part of speech (POS) of words immediately
surrounding the target word. These five fea-
tures are POS?i for i from -2 to +2 ), where
POS+1, for example, represents the POS of the
word immediately following the target word.
Five other local-context features represent
the word tokens immediately surrounding the
target word (Word?i for i from ?2 to +2).
Each Word?i feature is multi-valued; its values
correspond to all possible word tokens.
There is a collocation feature WordColl
s
de-
fined for each sense s of the target word. It
is a binary feature, representing the absence or
presence of any word in a set specifically chosen
for s. A word w that occurs more than once in
the training data is included in the collocation
set for sense s if the relative percent gain in the
conditional probability over the prior probabil-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Local-context features
POS: part-of-speech of target word
POS?i: part-of-speech of word at offset i
WordForm: target wordform
Word?i: stem of word at offset i
Collocational features
WordColl
s
: word collocation for sense s
WordColl
?
wordform of non-sense-specific
collocation (enumerated)
Class-based collocational features
HyperColl
s
: hypernym collocation for s
HyperColl
?,i
: non-sense-specific hypernym collo-
cation
SimilarColl
s
: similarity collocation for s
DictColl
s
: dictionary collocation for s
Figure 1: Features for word-sense disambigua-
tion. All collocational features are binary indi-
cators for sense s, except for WordColl
?
.
ity is 20% or higher:
(P (s|w) ? P (s))
P (s) ? 0.20.
This threshold was determined to be effective
via an optimization search over the Senseval-2
data. WordColl
?
represents a set of non-sense-
specific collocations (i.e., not necessarily indica-
tive of any one sense), chosen via the G2 criteria
(Wiebe et al, 1998). In contrast to WordColl
s
,
each of which is a separate binary feature, the
words contained in the set WordColl
?
serve as
values in a single enumerated feature.
These features are augmented with class-
based collocational features that represent in-
formation about word relationships derived
from three separate sources: 1) WordNet
(Miller, 1990) hypernym relations (HyperColl);
2) cluster-based word similarity classes (Simi-
larColl); and 3) relatedness inferred from dictio-
nary definition analysis (DictColl). The infor-
mation inherent in the sources from which these
class-based features are derived allows words
that do not occur in the training data context
to be considered as collocations during classifi-
cation.
3 Class-based Collocations
The HyperColl features are intended to capture
a portion of the information in the WordNet hy-
pernyms links (i.e., is-a relations). Hypernym-
based collocations are formulated by replacing
each word in the context of the target word (e.g.,
in the same sentence as the target word) with
its complete hypernym ancestry from WordNet.
Since context words are not sense-tagged, each
synset representing a different sense of a context
word is included in the set of hypernyms replac-
ing that word. Likewise, in the case of multiple
inheritance, each parent synset is included.
The collocation variable HyperColl
s
for each
sense s is binary, corresponding to the absence
or presence of any hypernym in the set chosen
for s. This set of hypernyms is chosen using the
ratio of conditional probability to prior prob-
ability as described for the WordColl
s
feature
above. In contrast, HyperColl
?,i
selects non-
sense-specific hypernym collocations: 10 sepa-
rate binary features are used based on the G2
selection criteria. (More of these features could
be used, but they are limited for tractability.)
For more details on hypernym collocations, see
(O?Hara, forthcoming).
Word-similarity classes (Lin, 1998) derived
from clustering are also used to expand the
pool of potential collocations; this type of se-
mantic relatedness among words is expressed in
the SimilarColl feature. For the DictColl fea-
tures, definition analysis (O?Hara, forthcoming)
is used to determine the semantic relatedness of
the defining words. Differences between these
two sources of word relations are illustrated by
looking at the information they provide for ?bal-
lerina?:
word-clusters:
dancer:0.115 baryshnikov:0.072
pianist:0.056 choreographer:0.049
... [18 other words]
nicole:0.041 wrestler:0.040
tibetans:0.040 clown:0.040
definition words:
dancer:0.0013 female:0.0013 ballet:0.0004
This shows that word clusters capture a wider
range of relatedness than the dictionary def-
initions at the expense of incidental associa-
tions (e.g., ?nicole?). Again, because context
words are not disambiguated, the relations for
all senses of a context word are conflated. For
details on the extraction of word clusters, see
(Lin, 1998); and, for details on the definition
analysis, see (O?Hara, forthcoming).
When formulating the features SimilarColl
and DictColl, the words related to each con-
text word are considered as potential colloca-
tions (Wiebe et al, 1998). Co-occurrence fre-
Sense Distinctions Precision Recall
Fine-grained .566 .565
Course-grained .660 .658
Table 1: Results for Senseval-3 test data.
99.72% of the answers were attempted. All fea-
tures from Figure 1 were used.
quencies f(s,w) are used in estimating the con-
ditional probability P (s|w) required by the rel-
ative conditional probability selection scheme
noted earlier. However, instead of using a unit
weight for each co-occurrence, the relatedness
weight is used (e.g., 0.056 for ?pianist?); and,
because a given related-word might occur with
more than one context word for the same target-
word sense, the relatedness weights are added.
The conditional probability of the sense given
the relatedness collocation is estimated by di-
viding the weighted frequency by the sum of all
such weighted co-occurrence frequencies for the
word:
P (s|w)? wf (s,w)?
s
?
wf (s?, w)
Here wf(s, w) stands for the weighted co-
occurrence frequency of the related-word collo-
cation w and target sense s.
The relatedness collocations are less reliable
than word collocations given the level of indi-
rection involved in their extraction. Therefore,
tighter constraints are used in order to filter out
extraneous potential collocations. In particular,
the relative percent gain in the conditional ver-
sus prior probability must be 80% or higher, a
threshold again determined via an optimization
search over the Senseval-2 data. In addition,
the context words that they are related to must
occur more than four times in the training data.
4 Results and Discussion
Disambiguation is performed via a decision tree
formulated using Weka?s J4.8 classifier (Witten
and Frank, 1999). For the system used in the
competition, the decision tree was learned over
the entire Senseval-3 training data and then ap-
plied to the test data. Table 1 shows the results
of our system in the Senseval-3 competition.
Table 2 shows the results of 10-fold cross-
validation just over the Senseval-3 training data
(using Naive Bayes rather than decision trees.)
To illustrate the contribution of the three types
Experiment Precision
?Local +Local
Local - .593
WordColl .490 .599
HyperColl .525 .590
DictColl .532 .570
SimilarColl .534 .586
HyperColl+WordColl .525 .611
DictColl+WordColl .501 .606
SimilarColl+WordColl .518 .596
All Collocations .543 .608
#Words: 57 Avg. Entropy: 1.641
Avg. #Senses: 5.3 Baseline: 0.544
Table 2: Results for Senseval-3 training data.
All values are averages, except #Words, which
is the number of distinct word types classified.
Baseline always uses the most-frequent sense.
of class-based collocations, the table shows re-
sults separately for systems developed using a
single feature type, as well as for all features in
combination. In addition, the performance of
these systems are shown with and without the
use of the local features (Local), as well as with
and without the use of standard word colloca-
tions (WordColl). As can be seen, the related-
word and definition collocations perform better
than hypernym collocations when used alone.
However, hypernym collocations perform bet-
ter when combined with other features. Fu-
ture work will investigate ways of ameliorat-
ing such interactions. The best overall system
(HyperColl+WordColl+Local) uses the com-
bination of local-context features, word colloca-
tions, and hypernym collocations. The perfor-
mance of this system compared to a more typi-
cal system for WSD (WordColl+Local) is sta-
tistically significant at p < .05, using a paired
t-test.
We analyzed the contributions of the various
collocation types to determine their effective-
ness. Table 3 shows performance statistics for
each collocation type taken individually over the
training data. Precision is based on the num-
ber of correct positive indicators versus the to-
tal number of positive indicators, whereas recall
is the number correct over the total number of
training instances (7706). This shows that hy-
pernym collocations are nearly as effective as
word collocations. We also analyzed the occur-
rence of unique positive indicators provided by
the collocation types over the training data. Ta-
Total Total
Feature #Corr. #Pos. Recall Prec.
DictColl 273 592 .035 .461
HyperColl 2932 6479 .380 .453
SimilarColl 528 1535 .069 .344
WordColl 3707 7718 .481 .480
Table 3: Collocation performance statistics.
Total #Pos. is number of positive indicators for
the collocation in the training data, and Total
#Corr. is the number of these that are correct.
Unique Unique
Feature #Corr. #Pos. Prec.
DictColl 110 181 .608
HyperColl 992 1795 .553
SimilarColl 198 464 .427
DictColl 1244 2085 .597
Table 4: Analysis of unique positive indicators.
Unique #Pos. is number of training instances
with the feature as the only positive indicator,
and Unique #Corr. is number of these correct.
ble 4 shows how often each feature type is pos-
itive for a particular sense when all other fea-
tures for the sense are negative. This occurs
fairly often, suggesting that the different types
of collocations are complementary and thus gen-
erally useful when combined for word-sense dis-
ambiguation. Both tables illustrate coverage
problems for the definition and related word
collocations, which will be addressed in future
work.
References
Nancy Ide and Jean Ve?ronis. 1998. Introduc-
tion to the special issue on word sense dis-
ambiguation: the state of the art. Computa-
tional Linguistics, 24(1):1?40.
Daniel Jurafsky and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, New Jersey.
Dekang Lin. 1998. Automatic retrieval
and clustering of similar words. In Proc.
COLING-ACL 98, pages 768?764, Montreal.
August 10-14.
Rada Mihalcea. 2002. Instance based learning
with automatic feature selection applied to
word sense disambiguation. In Proceedings of
the 19th International Conference on Com-
putational Linguistics (COLING 2002), Tai-
wan. August 26-30.
George Miller. 1990. Introduction. Interna-
tional Journal of Lexicography, 3(4): Special
Issue on WordNet.
Tom O?Hara, Janyce Wiebe, and Rebecca F.
Bruce. 2000. Selecting decomposable models
for word-sense disambiguation: The grling-
sdm system. Computers and the Humanities,
34(1-2):159?164.
Thomas P. O?Hara. forthcoming. Empirical ac-
quisition of conceptual distinctions via dictio-
nary definitions. Ph.D. thesis, Department of
Computer Science, New Mexico State Univer-
sity.
Hee-Cheol Seo, Sang-Zoo Lee, Hae-Chang
Rim, and Ho Lee. 2001. KUNLP sys-
tem using classification information model at
SENSEVAL-2. In Proceedings of the Second
International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-
2), pages 147?150, Toulouse. July 5-6.
Janyce Wiebe, Kenneth McKeever, and Re-
becca F. Bruce. 1998. Mapping collocational
properties into machine learning features. In
Proc. 6th Workshop on Very Large Corpora
(WVLC-98), pages 225?233, Montreal, Que-
bec, Canada. Association for Computational
Linguistics. SIGDAT.
Ian H. Witten and Eibe Frank. 1999. Data
Mining: Practical Machine Learning Tools
and Techniques with Java Implementations.
Morgan Kaufmann, San Francisco, CA.
David Yarowsky, Silviu Cucerzan, Radu Flo-
rian, Charles Schafer, and Richard Wicen-
towski. 2001. The Johns Hopkins SENSE-
VAL2 system descriptions. In Proceedings of
the Second International Workshop on Eval-
uating Word Sense Disambiguation Systems
(SENSEVAL-2), pages 163?166, Toulouse.
July 5-6.
