Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 112?119,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Multilingual Transliteration Using Feature based Phonetic Method  
Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat  
University of Illinois at Urbana-Champaign 
{syoon9,kkim36,rws}@uiuc.edu 
 
 
Abstract 
In this paper we investigate named entity 
transliteration based on a phonetic scoring 
method. The phonetic method is computed 
using phonetic features and carefully 
designed pseudo features. The proposed 
method is tested with four languages ? 
Arabic, Chinese, Hindi and Korean ? and 
one source language ? English, using 
comparable corpora. The proposed method 
is developed from the phonetic method 
originally proposed in Tao et al (2006). In 
contrast to the phonetic method in Tao et al 
(2006) constructed on the basis of pure 
linguistic knowledge, the method in this 
study is trained using the Winnow machine 
learning algorithm. There is salient 
improvement in Hindi and Arabic 
compared to the previous study. Moreover, 
we demonstrate that the method can also 
achieve comparable results, when it is 
trained on language data different from the 
target language. The method can be applied 
both with minimal data, and without target 
language data for various languages.  
1 Introduction. 
In this paper, we develop a multi-lingual 
transliteration system for named entities. Named 
entity transliteration is the process of producing, 
for a name in a source language, a set of one or 
more transliteration candidates in a target language. 
The correct transliteration of named entities is 
crucial, since they are frequent and important key 
words in information retrieval. In addition, 
requests in retrieving relevant documents in 
multiple languages require the development of the 
multi-lingual system.  
The system is constructed using paired 
comparable texts. The comparable texts are about 
the same or related topics, but are not, in general, 
translations of each other. Using this data, the 
transliteration method aims to find transliteration 
correspondences in the paired languages. For 
example, if there were an English and Arabic 
newspaper on the same day, each of the 
newspapers would contain articles about the same 
important international events. From these 
comparable articles across the paired languages, 
the same named entities are expected to be found. 
Thus, from the named entities in an English 
newspaper, the method would find transliteration 
correspondences in comparable texts in other 
languages. 
The multi-lingual transliteration system entails 
solving several problems which are very 
challenging. First, it should show stable 
performance for many unrelated languages. The 
transliteration will be influenced by the difference 
in the phonological systems of the language pairs, 
and the process of transliteration differs according 
to the languages involved. For example, in Arabic 
texts, short vowels are rarely written while long 
vowels are written. When transliterating English 
names, the vowels are disappeared or written as 
long vowels. For example London is transliterated 
as lndn ?????????, and both vowels are not represented 
in the transliteration. However, Washington is 
often transliterated as  wSnjTwn ?????? ????????????? , and 
the final vowel is realized with long vowel. 
Transliterations in Chinese are very different from 
the original English pronunciation due to the 
112
limited syllable structure and phoneme inventory 
of Chinese. For example, Chinese does not allow 
consonant clusters or coda consonants except [n,N], 
and this results in deletion, substitution of 
consonants or insertion of vowels. Thus while a 
syllable initial /d/ may surface as in Baghdad 
??? ba-ge-da, note that the syllable final /d/ is 
not represented. Multi-lingual transliteration 
system should solve these language dependent 
characteristics.  
One of the most important concerns in a 
multilingual transliteration system is its 
applicability given a small amount of training data, 
or even no training data: for arbitrary language 
pairs, one cannot in general assume resources such 
as name dictionaries. Indeed, for some rarely 
spoken languages, it is practically impossible to 
find enough training data. Therefore, the proposed 
method aims to obtain comparable performance 
with little training data.  
2 Previous Work 
Previous work ? e.g. (Knight and Graehl, 1998; 
Meng et al, 2001; Al-Onaizan and Knight, 2002; 
Gao et al, 2004) ? has mostly assumed that one 
has a training lexicon of transliteration pairs, from 
which one can learn a model, often a source-
channel or MaxEnt-based model. 
Comparable corpora have been studied 
extensively in the literature, but transliteration in 
the context of comparable corpora has not been 
well addressed. In our work, we adopt the method 
proposed in (Tao et al, 2006) and apply it to the 
problem of transliteration. 
Measuring phonetic similarity between words 
has been studied for a long time. In many studies, 
two strings are aligned using a string alignment 
algorithm, and an edit distance (the sum of the cost 
for each edit operation), is used as the phonetic 
distance between them. The resulting distance 
depends on the costs of the edit operation. There 
are several approaches that use distinctive features 
to determine the costs of the edit operation. Gildea 
and Jurafsky (1996) counted the number of 
features whose values are different, and used them 
as a substitution cost. However, this approach has a 
crucial limitation: the cost does not consider the 
importance of the features. Nerbonne and Heeringa 
(1997) assigned a weight for each feature based on 
entropy and information gain, but the results were 
even less accurate than the method without weight. 
3 Phonetic transliteration method 
In this paper, the phonetic transliteration is 
performed using the following steps:  
1) Generation of the pronunciation for 
English words and target words: 
a. Pronunciations for English words are obtained 
using the Festival text-to-speech system (Taylor et 
al., 1998).  
b. Target words are automatically converted into 
their phonemic level transcriptions by various 
language-dependent means. In the case of 
Mandarin Chinese, this is based on the standard 
Pinyin transliteration system. Arabic words are 
converted based on orthography, and the resulting 
transcriptions are reasonably correct except for the 
fact that short vowels were not represented. 
Similarly, the pronunciation of Hindi and Korean 
can be well-approximated based on the standard 
orthographic representation. All pronunciations are 
based on the WorldBet transliteration system 
(Hieronymus, 1995), an ascii-only version of the 
IPA. 
2) Training a linear classifier using the 
Winnow algorithm: 
A linear classifier is trained using the training 
data which is composed of transliteration pairs and 
non-transliteration pairs. Transliteration pairs are 
extracted from the transliteration dictionary, while 
non-transliteration pairs are composed of an 
English named entity and a random word from the 
target language newspaper.  
a. For all the training data, the pairs of 
pronunciations are aligned using standard string 
alignment algorithm based on Kruskal (1999). The 
substitution/insertion/deletion cost for the string 
alignment algorithm is based on the baseline cost 
from (Tao et al 2006). 
b. All phonemes in the pronunciations are 
decomposed into their features. The features used 
in this study will be explained in detail in part 3.1.  
c. For every phoneme pair (p1, p2) in the aligned 
pronunciations, a feature xi has a ?+1? value or a ??
1? value: 
 
xi =   +1   when p1 and p2  have the same 
values for feature xi 
?1   otherwise 
113
d. A linear classifier is trained using the 
Winnow algorithm from the SNoW toolkit 
(Carlson et al, 1999).  
 
3) Scoring English-target word pair: 
a. For a given English word, the score between it 
and a target word is computed using the linear 
classifier. 
b. The score ranges from 0 to any positive 
number, and the candidate with the highest score is 
selected as the transliteration of the given English 
name.  
 
3.1  Feature set 
Halle and Clements (1983)?s distinctive features 
are used in order to model the substitution/ 
insertion/deletion costs for the string-alignment 
algorithm and linear classifier. A distinctive 
feature is a feature that describes the phonetic 
characteristics of phonetic segments. 
However, distinctive features alone are not 
enough to model the frequent sound change 
patterns that occur when words are adapted across 
languages. For example, stop and fricative 
consonants such as /p, t, k, b, d, g, s, z/ are 
frequently deleted when they appear in the coda 
position. This tendency is extremely salient when 
the target languages do not allow coda consonants 
or consonant clusters. For example, since Chinese 
only allows /n, N/ in coda position, stop consonants 
in the coda position are frequently lost; Stanford is 
transliterated as sitanfu, with the final /d/ lost. 
Since traditional distinctive features do not 
consider the position in the syllable, this pattern 
cannot be captured by distinctive features alone. 
To capture these sound change patterns, additional 
features such as ?deletion of stop/fricative 
consonant in the coda position? must be considered.  
Based on the pronunciation error data of learners 
of English as a second language as reported in 
(Swan and Smith, 2002), we propose the use of 
what we will term pseudofeatures. The pseudo 
features in this study are same as in Tao et al 
(2006). Swan & Smith (2002)?s study covers 25 
languages including Asian languages such as Thai, 
Korean, Chinese and Japanese, European 
languages such as German, Italian, French and 
Polish, and Middle East languages such as Arabic 
and Farsi. The substitution/insertion/deletion errors 
of phonemes were collected from this data. The 
following types of errors frequently occur in 
second language learners? speech production.  
(1) Substitution: If the learner?s first language 
does not have a particular phoneme found in 
English, it is substituted by the most similar 
phoneme in their first language. 
(2) Insertion: If the learner?s first language does 
not have a particular consonant cluster in English, 
a vowel is inserted. 
(3) Deletion: If the learner?s first language does 
not have a particular consonant cluster in English, 
one consonant in the consonant cluster is deleted. 
The same substitution/deletion/insertion patterns 
in a second language learner?s errors also appear in 
the transliteration of foreign names. The deletion 
of the stop consonant which appears in English-
Chinese transliterations occurs frequently in the 
English pronunciation spoken by Chinese speakers. 
Therefore, the error patterns in second language 
learners? can be used in transliteration. 
Based on (1) ~ (3), 21 pseudo features were 
designed. All features have binary values. Using 
these 21 pseudo features and 20 distinctive features, 
a linear classifier is trained. Some examples of 
pseudo features are presented in Table 1.  
 
Pseudo-  
Feature Description Example 
Consonant-
coda 
Substitution 
of consonant 
feature in 
coda position 
 
Sonorant-
coda 
Substitution 
of sonorant 
feature in 
coda position 
Substitution 
between [N] and 
[g] in coda 
position in Arabic
Labial-coda
Substitution 
of labial 
feature in 
coda position 
Substitution 
between [m] and 
[n] in coda 
position in Chinese
j-exception
Substitution 
of [j] and [dZ] 
Spanish/Catalan 
and Festival error
w-exception Substitution of [v] and [w] 
Chinese/Farsi and 
Festival error 
Table 1. Examples of pseudo features  
 
114
3.2 Scoring the English-target word pair  
A linear classifier is trained using the Winnow 
algorithm from the SNoW toolkit.  
The Winnow algorithm is one of the update 
rules for linear classifier. A linear classifier is an 
algorithm to find a linear function that best 
separates the data. For the set of features X and set 
of weights W, the linear classifier is defined as [1] 
(Mitchell, T., 1997) 
1 2
1 2
0 1 1 2 2
  { , ,  ... }
  { , , ... } 
( )   1        ...    0   
             -1  
n
n
n n
X x x x
W w w w
f x if w wx w x w x
otherwise
=
=
= + + + + >
[1] 
 
The linear function assigns label +1 when the 
paired target language word is the transliteration of 
given English word, while it assigns label ?1 when 
it is not a transliteration of given English word.  
The score of an English word and target word 
pair is computed using equation [2] which is part 
of the definition of f(x) in equation [1]. 
0
1
n
i i
i
w w x
=
+?   [2] 
The output of equation [2] is termed the target 
node activation. If this value is high, class 1 is 
more activated, and the pair is more likely to be a 
transliteration pair. To illustrate, let us assume 
there are two candidates in target language (t1 and 
t2) for an English word e. If the score of (e, t1) is 
higher than the score of (e, t2), the pair (e, t1) has 
stronger activation than (e, t2). It means that t1  
scores higher as the transliteration of e than t2. 
Therefore, the candidate with the highest score (in 
this case t1) is selected as the transliteration of the 
given English name. 
4 Experiment and Results 
The linear function was trained for each 
language, separately. 500 transliteration pairs were 
randomly selected from each transliteration 
dictionary, and used as positive examples in the 
training procedure. This is quite small compared to 
previous approaches such as Knight and Graehl 
(1998) or Gao et al (2004). In addition, 1500 
words were randomly selected from the newspaper 
in the target languages, and paired with English 
words in the positive examples. A total of 750,000 
pairs (500 English words? 1500 target words) were 
generated, and used as negative examples in the 
training procedure. 
Table 2 presents the source of training data for 
each language.  
 
 Transliteration pair Target word 
Arabic New Mexico State University 
Xinhua Arabic 
newswire 
Chinese Behavior Design Corporation 
Xinhua  
Chinese  
newswire 
Hindi Naidunia Hindi newswire  
Naidunia Hindi 
newswire 
Korean
the National  
Institute of the 
Korean language 
Chosun  
Korean  
newspaper 
Table 2. Sources of the training data 
The phonetic transliteration method was 
evaluated using comparable corpora, consisting of 
newspaper articles in English and the target 
languages?Arabic, Chinese, Hindi, and Korean?
from the same day, or almost the same day. Using 
comparable corpora, the named-entities for persons 
and locations were extracted from the English text; 
in this paper, the English named-entities were 
extracted using the named-entity recognizer 
described in Li et al (2004), based on the SNoW 
machine learning toolkit (Carlson et al, 1999).  
The transliteration task was performed using the 
following steps:  
1) English text was tagged using the named-
entity recognizer. The 200 most frequent named 
entities were extracted from seven days? worth of 
the English newswire text. Among pronunciations 
of words generated by the Festival text-to speech 
system, 3% contained errors representing 
monophthongs instead of diphthongs or vice versa. 
1.5% of all cases misrepresented single consonant, 
and 6% showed errors in the vowels. Overall, 
10.5% of the tokens contained pronunciation errors 
which could trigger errors in transliteration. 
2) To generate the Arabic and Hindi candidates, 
all words from the same seven days were extracted. 
In the case of Korean corpus, the collection of 
newspapers was from every five days, unlike the 
other three language corpora which were collected 
every day; therefore, candidates of Korean were 
115
generated from one month of newspapers, since 
seven days of newspaper articles did not show a 
sufficient number of transliteration candidates. 
This caused the total number of candidates to be 
much bigger than for the other languages.  
The words were stemmed all possible ways 
using simple hand-developed affix lists: for 
example, given a Hindi word c1c2c3, if both c3 
and c2c3 are in the suffix and ending list, then this 
single word generated three possible candidates: c1, 
c1c2, and c1c2c3.  
3) Segmenting Chinese sentences requires a 
dictionary or supervised segmenter. Since the goal 
is to use minimal knowledge or data from the 
target language, using supervised methods is 
inappropriate for our approach. Therefore, Chinese 
sentences were not segmented. Using the 495 
characters that are frequently used for 
transliterating foreign names (Sproat et al, 1996), 
a sequence of three of more characters from the list 
was taken as a possible candidate for Chinese. 
4) For the given 200 English named entities and 
target language candidate lists, all the possible 
pairings of English and target-language name were 
considered as possible transliteration pairs.  
The number of candidates for each target 
language is presented in Table 3. 
 
Language The number of candidates 
Arabic 12,466 
Chinese 6,291 
Hindi 10,169 
Korean 42,757 
Table 3. Number of candidates for each target 
language. 
5) Node activation scores were calculated for 
each pair in the test data, and the candidates were 
ranked by their score. The candidate with the 
highest node activation score was selected as the 
transliteration of the given English name.  
Some examples of English words and the top 
three ranking candidates among all of the potential 
target-language candidates were given in Tables 4, 
5. Starred entries are correct. 
 
Candidate English 
Word Rank Script Romanization 
Arafat 
*1 
2 
3 
????
????
??? 
a-la-fa-te 
la-fa-di-ao
la-wei-qi 
Table 4. Examples of the top-3 candidates in the 
transliteration of English ? Chinese 
Candidate English 
Word Rank
Script Romanization 
*1 ??? be-thu-nam
2 ???? be-thu-nam-chug Vietnam 
3 ???? pyo-jun-e-wa 
*1 ??????? 
o-su-thu-
ley-il-li-a 
2 ??? us-tol-la Australia
3 ????????? 
o-su-thu-
ley-il-li-a-
ey-se 
Table 5. Examples of the top-3 candidates in the 
transliteration of English-Korean 
To evaluate the proposed transliteration methods 
quantitatively, the Mean Reciprocal Rank (MRR), 
a measure commonly used in information retrieval 
when there is precisely one correct answer (Kandor 
and Vorhees, 2000) was measured, following Tao 
and Zhai (2005).  
 
Since the evaluation data obtained from the 
comparable corpus was small, the systems were 
evaluated using both held-out data from the 
transliteration dictionary and comparable corpus.  
 
First, the results of the held-out data will be 
presented. For a given English name and target 
language candidates, all possible combinations 
were generated. Table 6 presents the size of held-
out data, and Table 7 presents MRR of the held-out 
data.  
 
116
 Number 
of English 
named 
entities 
Number of 
Candidates 
in target 
language 
Number of 
total pairs 
used in the 
evaluation
Arabic 500 1,500 750,000 
Chinese 500 1,500 750,000 
Hindi 100 1,500 150,000 
Korean 100 1,500 150,000 
Table 6. Size of the test data 
Winnow 
 Baseline  Total 
feature 
distinctive 
feature 
only 
Arabic 0.66 0.74 0.70 
Chinese 0.74 0.74 0.72 
Hindi 0.87 0.91 0.91 
Korean 0.82 0.85 0.82 
Table 7. MRRs of the phonetic transliteration 
The baseline was computed using the phonetic 
transliteration method proposed in Tao et al 
(2006). In contrast to the method in this study, the 
baseline system is purely based on linguistic 
knowledge. In the baseline system, the edit 
distance, which was the result of the string 
alignment algorithm, was used as the score of an 
English-target word pair. The performance of the 
edit distance was dependent on insertion/deletion/ 
substitution costs. These costs were determined 
based on the distinctive features and pseudo 
features, based on the pure linguistic knowledge 
without training data. As illustrated in Table 7, the 
phonetic transliteration method using features 
worked adequately for multilingual data, as 
phonetic features are universal, unlike the 
phonemes which are composed of them. Adopting 
phonetic features as the units for transliteration 
yielded the baseline performance.  
In order to evaluate the effectiveness of pseudo 
features, the method was trained using two 
different feature sets: a total feature set and a 
distinctive feature-only set. For Arabic, Chinese 
and Korean, the MRR of the total feature set was 
higher than the MRR of the distinctive feature-only 
set. The improvement of the total set was 4% for 
Arabic, 2.6% for Chinese, 2.4% for Korean. There 
was no improvement of the total set in Hindi. In 
general, the pseudo features improved the accuracy 
of the transliteration. 
For all languages, the MRR of the Winnow 
algorithm with the total feature set was higher than 
the baseline. There was 7% improvement for 
Arabic, 0.7% improvement for Chinese, 4% 
improvement for Hindi and 3% improvement for 
Korean.  
 
We turn now to the results on comparable 
corpora. We attempted to create a complete set of 
answers for the 200 English names in our test set, 
but part of the English names did not seem to have 
any standard transliteration in the target language 
according to the native speaker?s judgment. 
Accordingly, we removed these names from the 
evaluation set. Thus, the resulting list was less than 
200 English names, as shown in the second column 
of Table 8; (Table 8 All). Furthermore, some 
correct transliterations were not found in our 
candidate list for the target languages, since the 
answer never occurred in the target news articles; 
(Table 8 Missing). Thus this results in a smaller 
number of candidates to evaluate. This smaller 
number is given in the fourth column of Table 8; 
(Table 8 Core).  
 
Language # All # Missing #Core 
Arabic 192 121 71 
Chinese 186 92 94 
Hindi 144 83 61 
Korean 195 114 81 
Table 8. Number of evaluated English Name 
 
MRRs were computed on the two sets 
represented by the count in column 2, and the 
smaller set represented by the count in column 4. 
We termed the former MRR ?AllMRR? and the 
latter ?CoreMRR?. In Table 9, ?CoreMRR? and 
?AllMRR? of the method were presented.  
 
117
Baseline  Winnow  
 All-
MRR 
Core
MRR 
All-
MRR 
Core
MRR
Arabic 0.20 0.53 0.22 0.61
Chinese 0.25 0.49 0.25 0.50
Hindi 0.30 0.69 0.36 0.86
Korean 0.30 0.71 0.29 0.69
Table 9. MRRs of the phonetic transliteration 
In both methods, CoreMRRs were higher than 
0.49 for all languages. That is, if the answer is in 
the target language texts, then the method finds the 
correct answer within the top 2 words.  
As with the previously discussed results, there 
were salient improvements in Arabic and Hindi 
when using the Winnow algorithm. The MRRs of 
the Winnow algorithm except Korean were higher 
than the baseline. There was 7% improvement for 
Arabic and 17% improvement for Hindi in 
CoreMRR. In contrast to the 3% improvement in 
held-out data, there was a 2% decrease in Korean: 
the MRRs of Korean from the Winnow algorithm 
were lower than baseline, possibly because of the 
limited size of the evaluation data. Similar to the 
results of held-out data, the improvement in 
Chinese was small (1%).  
The MRRs of Hindi and the MRRs of Korean 
were higher than the MRRs of Arabic and Chinese. 
The lower MRRs of Arabic and Chinese may result 
from the phonological structures of the languages. 
In general, transliteration of English word into 
Arabic and Chinese is much more irregular than 
the transliteration into Hindi and Korean in terms 
of phonetics.  
 
To test the applicability to languages for which 
training data is not available, we also investigated 
the use of models trained on language pairs 
different from the target language pair. Thus, for 
each test language pair, we evaluated the 
performance of models trained on each of the other 
language pairs. For example, three models were 
trained using Chinese, Hindi, and Korean, and they 
were tested with Arabic data. The CoreMRRs of 
this experiment were presented in Table 10. Note 
that the diagonal in this Table represents the 
within-language-pair training and testing scenario 
that we reported on above. 
test data 
 
Arabic Chinese Hindi
Kore
an 
Arabic 0.61 0.50 0.86 0.63
Chinese 0.59 0.50 0.80 0.66
Hindi 0.59 0.54 0.86 0.67
train
-ing 
data
Korean 0.56 0.51 0.76 0.69
Table 10. MRRs for the phonetic transliteration 2  
For Arabic, Hindi, and Korean, MRRs were 
indeed the highest when the methods were trained 
using data from the same language, as indicated by 
the boldface MRR scores on the diagonal. In 
general, however, the MRRs were not saliently 
lower across the board when using different 
language data than using same-language data in 
training and testing. For all languages, MRRs for 
the cross-language case were best when the 
methods were trained using Hindi. The differences 
between MRRs of the method trained from Hindi 
and MRRs of the method by homogeneous 
language data were 2% for Arabic and Korean. In 
the case of Chinese, MRRs of the method trained 
by Hindi was actually better than MRRs obtained 
by Chinese training data. Hindi has a large 
phoneme inventory compared to Korean, Arabic, 
and Chinese, so the relationship between English 
phonemes and Hindi phonemes is relatively regular, 
and only small number of language specific 
transliteration rules exist. That is, the language 
specific influences from Hindi are smaller than 
those from other languages. This characteristic of 
Hindi may result in the high MRRs for other 
languages. What these results imply is that named 
entity transliteration could be performed without 
training data for the target language with phonetic 
feature as a unit. This approach is especially 
valuable for languages for which training data is 
minimal or lacking. 
 
5 Conclusion 
In this paper, a phonetic method for multilingual 
transliteration was proposed. The method was 
based on string alignment, and linear classifiers 
trained using the Winnow algorithm. In order to 
learn both language-universal and language-
specific transliteration characteristics, distinctive 
118
features and pseudo features were used in training. 
The method can be trained using a small amount of 
training data, and the performance decreases only 
by a small degree when it is trained with a 
language different from the test data. Therefore, 
this method is extremely useful for 
underrepresented languages for which training data 
is difficult to find. 
Acknowledgments 
This work was funded the National Security 
Agency contract NBCHC040176 (REFLEX) and a 
Google Research grant.  
 
References 
Y. Al-Onaizan and K. Knight. 2002. Machine 
transliteration of names in Arabic text. In 
Proceedings of the ACL Workshop on Computational 
Approaches to Semitic Languages, Philadelphia, PA. 
Andrew J. Carlson, Chad M. Cumby, Jeff L. Rosen, and 
Dan Roth. 1999. The SNoW learning architecture. 
Technical Report UIUCDCS-R-99-2101, UIUC CS 
Dept. 
Wei Gao, Kam-Fai Wong, and Wai Lam. 2004. 
Phoneme based transliteration of foreign names for 
OOV problem. Proceeding of IJCNLP, 374?381. 
Daniel Gildea and Daniel Jurafsky. 1996. Learning Bias 
and Phonological-Rule Induction. Computational 
Linguistics 22(4):497?530. 
Morris Halle and G.N. Clements. 1983. Problem book 
in phonology. MIT press, Cambridge. 
James Hieronymus. 1995. Ascii phonetic symbols for 
the world?s languages: Worldbet. 
http://www.ling.ohio-tate.edu/ edwards/worldbet.pdf. 
Paul B. Kantor and Ellen B. Voorhees. 2000. The 
TREC-5 confusion track: Comparing retrieval 
methods for scanned text. Information Retrieval, 2: 
165?176. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
transliteration. Computational Linguistics, 24(4). 
Joseph B. Kruskal. 1999. An overview of sequence 
comparison. Time Warps, String Edits, and 
Macromolecules, CSLI, 2nd edition, 1?44. 
Xin Li, Paul Morie, and Dan Roth. 2004. Robust 
reading: Identification and tracing of ambiguous 
names. Proceeding of NAACL-2004. 
H.M. Meng, W.K Lo, B. Chen, and K. Tang. 2001. 
Generating phonetic cognates to handle named 
entities in English-Chinese cross-language spoken 
document retrieval. In Proceedings of the Automatic 
Speech Recognition and Understanding Workshop. 
Tom M. Mitchell. 1997. Machine Learning, McCraw-
Hill, Boston. 
John Nerbonne and Wilbert Heeringa. 1997. Measuring 
Dialect Distance Phonetically. Proceedings of the 3rd 
Meeting of the ACL Special Interest Group in 
Computational Phonology. 
Richard Sproat, Chilin. Shih, William A. Gale, and 
Nancy Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational 
Linguistics, 22(3).  
Michael Swan and Bernard Smith. 2002. Learner 
English, Cambridge University Press, Cambridge . 
Tao Tao and ChengXiang Zhai. 2005. Mining 
comparable bilingual text corpora for cross-language 
information integration. Proceeding of the eleventh 
ACM SIGKDD international conference on 
Knowledge discovery in data mining, 691?696. 
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard 
Sproat and ChengXiang Zhai. "Unsupervised Named 
Entity Transliteration Using Temporal and Phonetic 
Correlation." EMNLP, July 22-23, 2006, Sydney, 
Australia. 
Paul A. Taylor, Alan Black, and Richard Caley. 1998. 
The architecture of the Festival speech synthesis 
system. Proceedings of the Third ESCAWorkshop on 
SpeechSynthesis, 147?151. 
119
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 250?257,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Named Entity Transliteration Using Temporal and Phonetic
Correlation
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat and ChengXiang Zhai
University of Illinois at Urbana-Champaign
{syoon9,afister2,rws}@uiuc.edu, {taotao,czhai}@cs.uiuc.edu
Abstract
In this paper we investigate unsuper-
vised name transliteration using compara-
ble corpora, corpora where texts in the two
languages deal in some of the same top-
ics ? and therefore share references to
named entities ? but are not translations
of each other. We present two distinct
methods for transliteration, one approach
using an unsupervised phonetic translit-
eration method, and the other using the
temporal distribution of candidate pairs.
Each of these approaches works quite
well, but by combining the approaches
one can achieve even better results. We
believe that the novelty of our approach
lies in the phonetic-based scoring method,
which is based on a combination of care-
fully crafted phonetic features, and empiri-
cal results from the pronunciation errors of
second-language learners of English. Un-
like previous approaches to transliteration,
this method can in principle work with any
pair of languages in the absence of a train-
ing dictionary, provided one has an esti-
mate of the pronunciation of words in text.
1 Introduction
As a part of a on-going project on multilingual
named entity identification, we investigate unsu-
pervised methods for transliteration across lan-
guages that use different scripts. Starting from
paired comparable texts that are about the same
topic, but are not in general translations of each
other, we aim to find the transliteration correspon-
dences of the paired languages. For example, if
there were an English and Arabic newspaper on
the same day, each of the newspapers would likely
contain articles about the same important inter-
national events. From these comparable articles
across the two languages, the same named enti-
ties such as persons and locations would likely be
found. For at least some of the English named
entities, we would therefore expect to find Ara-
bic equivalents, many of which would in fact be
transliterations.
The characteristics of transliteration differ ac-
cording to the languages involved. In particular,
the exact transliteration of say, an English name
is highly dependent on the language since this will
be influenced by the difference in the phonological
systems of the language pairs. In order to show the
reliability of a multi-lingual transliteration model,
it should be tested with a variety of different lan-
guages. We have tested our transliteration meth-
ods with three unrelated target languages ? Ara-
bic, Chinese and Hindi, and a common source lan-
guage ? English. Transliteration from English to
Arabic and Chinese is complicated (Al-Onaizan
and Knight, 2002). For example, while Arabic or-
thography has a conventional way of writing long
vowels using selected consonant symbols ? ba-
sically <w>, <y> and <?>, in ordinary text
short vowels are rarely written. When transliter-
ating English names there is the option of repre-
senting the vowels as either short (i.e. unwrit-
ten) or long (i.e. written with one of the above
three mentioned consonant symbols). For exam-
ple London is transliterated as     lndn, with no
vowels; Washington often as  
	   wSnjTwn,
with <w> representing the final <o>. Transliter-
ations in Chinese are very different from the orig-
inal English pronunciation due to the limited syl-
lable structure and phoneme inventory of Chinese.
For example, Chinese does not allow consonant
clusters or coda consonants except [n, N], and this
results in deletion, substitution of consonants or
insertion of vowels. Thus while a syllable initial
/d/ may surface as in Baghdad  ba-ge-da,
note that the syllable final /d/ is not represented.
250
Hindi transliteration is not well-studied, but it is
in principle easier than Arabic and Chinese since
Hindi phonotactics is much more similar to that of
English.
2 Previous Work
Named entity transliteration is the problem of pro-
ducing, for a name in a source language, a set
of one or more transliteration candidates in a tar-
get language. Previous work ? e.g. (Knight and
Graehl, 1998; Meng et al, 2001; Al-Onaizan and
Knight, 2002; Gao et al, 2004) ? has mostly as-
sumed that one has a training lexicon of translit-
eration pairs, from which one can learn a model,
often a source-channel or MaxEnt-based model.
Comparable corpora have been studied exten-
sively in the literature ? e.g.,(Fung, 1995; Rapp,
1995; Tanaka and Iwasaki, 1996; Franz et al,
1998; Ballesteros and Croft, 1998; Masuichi et
al., 2000; Sadat et al, 2004), but transliteration
in the context of comparable corpora has not been
well addressed. The general idea of exploiting
time correlations to acquire word translations from
comparable corpora has been explored in several
previous studies ? e.g., (Fung, 1995; Rapp, 1995;
Tanaka and Iwasaki, 1996). Recently, a Pearson
correlation method was proposed to mine word
pairs from comparable corpora (Tao and Zhai,
2005); this idea is similar to the method used in
(Kay and Roscheisen, 1993) for sentence align-
ment. In our work, we adopt the method proposed
in (Tao and Zhai, 2005) and apply it to the problem
of transliteration; note that (Tao and Zhai, 2005)
compares several different metrics for time corre-
lation, as we also note below ? and see (Sproat et
al., 2006).
3 Transliteration with Comparable
Corpora
We start from comparable corpora, consisting of
newspaper articles in English and the target lan-
guages for the same time period. In this paper, the
target languages are Arabic, Chinese and Hindi.
We then extract named-entities in the English text
using the named-entity recognizer described in (Li
et al, 2004), which is based on the SNoW machine
learning toolkit (Carlson et al, 1999). To perform
transliteration, we use the following general ap-
proach: 1 Extract named entities from the English
corpus for each day; 2 Extract candidates from the
same day?s newspapers in the target language; 3
For each English named entity, score and rank the
target-language candidates as potential transliter-
ations. We apply two unsupervised methods ?
time correlation and pronunciation-based methods
? independently, and in combination.
3.1 Candidate scoring based on
pronunciation
Our phonetic transliteration score uses a standard
string-alignment and alignment-scoring technique
based on (Kruskal, 1999) in that the distance is de-
termined by a combination of substitution, inser-
tion and deletion costs. These costs are computed
from a language-universal cost matrix based on
phonological features and the degree of phonetic
similarity. (Our technique is thus similar to other
work on phonetic similarity such as (Frisch, 1996)
though details differ.) We construct a single cost
matrix, and apply it to English and all target lan-
guages. This technique requires the knowledge of
the phonetics and the sound change patterns of the
language, but it does not require a transliteration-
pair training dictionary. In this paper we assume
the WorldBet transliteration system (Hieronymus,
1995), an ASCII-only version of the IPA.
The cost matrix is constructed in the following
way. All phonemes are decomposed into stan-
dard phonological features. However, phonolog-
ical features alone are not enough to model the
possible substution/insertion/deletion patterns of
languages. For example, /h/ is more frequently
deleted than other consonants, whereas no single
phonological feature allows us to distinguish /h/
from other consonants. Similarly, stop and frica-
tive consonants such as /p, t, k, b, d, g, s, z/ are
frequently deleted when they appear in the coda
position. This tendency is very salient when the
target languages do not allow coda consonants or
consonant clusters. So, Chinese only allows [n,
N] in coda position, and stop consonants in coda
position are frequently lost; Stanford is translit-
erated as sitanfu, with the final /d/ lost. Since
phonological features do not consider the posi-
tion in the syllable, this pattern cannot be cap-
tured by conventional phonological features alone.
To capture this, an additional feature ?deletion
of stop/fricative consonant in the coda position?
is added. We base these observations, and the
concomitant pseudofeatures on pronunciation er-
ror data of learners of English as a second lan-
guage, as reported in (Swan and Smith, 2002). Er-
251
rors in second language pronunciation are deter-
mined by the difference in the phonological sys-
tem of learner?s first and second language. The
same substitution/deletion/insertion patterns in the
second language learner?s errors appear also in
the transliteration of foreign names. For exam-
ple, if the learner?s first language does not have
a particular phoneme found in English, it is sub-
stituted by the most similar phoneme in their first
language. Since Chinese does not have /v/, it is
frequently substituted by /w/ or /f/. This sub-
stitution occurs frequently in the transliteration
of foreign names in Chinese. Swan & Smith?s
study covers 25 languages, and includes Asian
languages such as Thai, Korean, Chinese and
Japanese, European languages such as German,
Italian, French, and Polish and Middle Eastern
languages such as Arabic and Farsi. Frequent sub-
stitution/insertion/deletion patterns of phonemes
are collected from these data. Some examples are
presented in Table 1.
Twenty phonological features and 14 pseud-
ofeatures are used for the construction of the cost
matrix. All features are classified into 5 classes.
There are 4 classes of consonantal features ?
place, manner, laryngeality and major (conso-
nant, sonorant, syllabicity), and a separate class
of vocalic features. The purpose of these classes
is to define groups of features which share the
same substitution/insertion/deletion costs. For-
mally, given a class C, and a cost CC , for each
feature f ? C, CC defines the cost of substitut-
ing a different value for f than the one present in
the source phoneme. Among manner features, the
feature continuous is classified separately, since
the substitution between stop and fricative con-
sonants is very frequent; but between, say, nasals
and fricatives such substitution is much less com-
mon. The cost for frequent sound change pat-
terns should be low. Based on our intuitions, our
pseudofeatures are classified into one or another
of the above-mentioned five classes. The substitu-
tion/deletion/insertion cost for a pair of phonemes
is the sum of the individual costs of the features
which are different between the two phonemes.
For example, /n/ and /p/ are different in sonorant,
labial and coronal features. Therefore, the substi-
tution cost of /n/ for /p/ is the sum of the sonorant,
labial and coronal cost (20+10+10 = 40). Features
and associated costs are shown in Table 2. Sam-
ple substitution, insertion, and deletion costs for
/g/ are presented in Table 3.
The resulting cost matrix based on these prin-
ciples is then used to calculate the edit distance
between two phonetic strings. Pronunciations for
English words are obtained using the Festival text-
to-speech system (Taylor et al, 1998), and the tar-
get language words are automatically converted
into their phonemic level transcriptions by various
language-dependent means. In the case of Man-
darin Chinese this is based on the standard pinyin
transliteration system. For Arabic this is based
on the orthography, which works reasonably well
given that (apart from the fact that short vowels
are no represented) the script is fairly phonemic.
Similarly, the pronunciation of Hindi can be rea-
sonably well-approximated based on the standard
Devanagari orthographic representation. The edit
cost for the pair of strings is normalized by the
number of phonemes. The resulting score ranges
from zero upwards; the score is used to rank can-
didate transliterations, with the candidate having
the lowest cost being considered the most likely
transliteration. Some examples of English words
and the top three ranking candidates among all of
the potential target-language candidates are given
in Table 4.1 Starred entries are correct.
3.2 Candidate scoring based on time
correlation
Names of the same entity that occur in different
languages often have correlated frequency patterns
due to common triggers such as a major event. For
example, the 2004 tsunami disaster was covered
in news articles in many different languages. We
would thus expect to see a peak of frequency of
names such as Sri Lanka, India, and Indonesia in
news articles published in multiple languages in
the same time period. In general, we may expect
topically related names in different languages to
tend to co-occur together over time. Thus if we
have comparable news articles over a sufficiently
long time period, it is possible to exploit such cor-
relations to learn the associations of names in dif-
ferent languages.
The idea of exploiting time correlation has been
well studied. We adopt the method proposed in
(Tao and Zhai, 2005) to represent the source name
and each name candidate with a frequency vector
and score each candidate by the similarity of the
1We describe candidate selection for each of the target
languages later.
252
Input Output Position
D D, d, z everywhere
T T, t, s everywhere
N N, n, g everywhere
p/t/k deletion coda
Table 1: Substitution/insertion/deletion patterns for phonemes based on English second-language
learner?s data reported in (Swan and Smith, 2002). Each row shows an input phoneme class, possi-
ble output phonemes (including null), and the positions where the substitution (or deletion) is likely to
occur.
Class Feature Cost
Major features and Consonant Del consonant 20
sonorant
consonant deletion
Place features and Vowel Del coronal 10
vowel del/ins
stop/fricative consonant del at coda position
h del/ins
Manner features nasal 5
dorsal feature for palatal consonants
Vowel features and Exceptions vowel height 3
vowel place
exceptional
Manner/ Laryngeal features continuous 1.5
voicing
Table 2: Examples of features and associated costs. Pseudofeatures are shown in boldface. Exceptional
denotes a situation such as the semivowel [j] substituting for the affricate [dZ]. Substitutions between
these two sounds actually occur frequently in second-language error data.
two frequency vectors. This is very similar to the
case in information retrieval where a query and a
document are often represented by a term vector
and documents are ranked by the similarity be-
tween their vectors and the query vector (Salton
and McGill, 1983). But the vectors are very dif-
ferent and should be constructed in quite differ-
ent ways. Following (Tao and Zhai, 2005), we
also normalize the raw frequency vector so that
it becomes a frequency distribution over all the
time points. In order to compute the similarity be-
tween two distribution vectors ~x = (x1, ..., xT )
and ~y = (y1, ..., yT ), the Pearson correlation co-
efficient was used in (Tao and Zhai, 2005). We
also consider two other commonly used measures
? cosine (Salton and McGill, 1983), and Jensen-
Shannon divergence (Lin, 1991), though our re-
sults show that Pearson correlation coefficient per-
forms better than these two other methods. Since
the time correlation method and the phonetic cor-
respondence method exploit distinct resources, it
makes sense to combine them. We explore two ap-
proaches to combining these two methods, namely
score combination and rank combination. These
will be defined below in Section 4.2.
4 Experiments
We evaluate our algorithms on three compara-
ble corpora: English/Arabic, English/Chinese, and
English/Hindi. Data statistics are shown in Ta-
ble 5.
From each data set in Table 5, we picked out all
news articles from seven randomly selected days.
We identified about 6800 English names using the
entity recognizer from (Carlson et al, 1999), and
chose the most frequent 200 names as our English
named entity candidates. Note that we chose the
most frequent names because the reliability of the
statistical correlation depends on the size of sam-
ple data. When a name is rare in a collection,
253
Source Target Cost Target Cost
g g 0 r 40.5
kh 2.5 e 44.5
cCh 5.5 del 24
tsh 17.5 ins 20
N 26.5
Table 3: Substitution/deletion/insertion costs for /g/.
English Candidate
Script Worldbet
Philippines 1       
 
 f l b y n
*2     
	    
 
 f l b y n y t
3            f l b y n a
Megawati *1 
 
 
  m h a f th
2          m i j a w a t a
3        m a k w z a
English Candidate
Script Romanization Worldbet
Belgium *1 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 600?608, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Assessment of ESL Learners? Syntactic Competence Based on Similarity
Measures
Su-Youn Yoon
Educational Testing Service
Princeton, NJ 08541
syoon@ets.org
Suma Bhat
Beckman Institute,
Urbana, IL 61801
spbhat2@illinois.edu
Abstract
This study presents a novel method that
measures English language learners? syntac-
tic competence towards improving automated
speech scoring systems. In contrast to most
previous studies which focus on the length of
production units such as the mean length of
clauses, we focused on capturing the differ-
ences in the distribution of morpho-syntactic
features or grammatical expressions across
proficiency. We estimated the syntactic com-
petence through the use of corpus-based NLP
techniques. Assuming that the range and so-
phistication of grammatical expressions can
be captured by the distribution of Part-of-
Speech (POS) tags, vector space models of
POS tags were constructed. We use a large
corpus of English learners? responses that are
classified into four proficiency levels by hu-
man raters. Our proposed feature measures
the similarity of a given response with the
most proficient group and is then estimates the
learner?s syntactic competence level.
Widely outperforming the state-of-the-art
measures of syntactic complexity, our method
attained a significant correlation with human-
rated scores. The correlation between human-
rated scores and features based on manual
transcription was 0.43 and the same based on
ASR-hypothesis was slightly lower, 0.42. An
important advantage of our method is its ro-
bustness against speech recognition errors not
to mention the simplicity of feature genera-
tion that captures a reasonable set of learner-
specific syntactic errors.
1 Introduction
This study provides a novel method that measures
ESL (English as a second language) learners? com-
petence in grammar usage (syntactic competence).
Being interdisciplinary in nature, it shows how to
combine the core findings in the ESL literature with
various empirical NLP techniques for the purpose of
automated scoring.
Grammar usage is one of the dimensions of lan-
guage ability that is assessed during non-native pro-
ficiency level testing in a foreign language. Overall
proficiency in the target language can be assessed
by testing the abilities in various areas including flu-
ency, pronunciation, and intonation; grammar and
vocabulary; and discourse structure. Testing rubrics
for human raters contain descriptors used for the
subjective assessment of several of these features.
With the recent move towards the objective assess-
ment of language ability (spoken and written), it is
imperative that we develop methods for quantifying
these abilities and measuring them automatically.
Ortega (2003) indicated that ?the range of forms
that surface in language production and the degree
of sophistication of such forms? were two impor-
tant areas in grammar usage and called the combina-
tion of these two areas ?syntactic complexity.? Fea-
tures that measure syntactic complexity have been
frequently studied in ESL literature and have been
found to be highly correlated with students? profi-
ciency levels in writing.
Studies in automated speech scoring have focused
on fluency (Cucchiarini et al2000; Cucchiarini et
al., 2002), pronunciation (Witt and Young, 1997;
600
Witt, 1999; Franco et al1997; Neumeyer et al
2000), and intonation (Zechner et al2009), and rel-
atively fewer studies have been conducted on gram-
mar usage. More recently, Lu (2010), Chen and
Yoon (2011) and Chen and Zechner (2011) have
measured syntactic competence in speech scoring.
Chen and Yoon (2011) estimated the complexity of
sentences based on the average length of the clauses
or sentences. In addition to these length measures,
Lu (2010) and Chen and Zechner (2011) measured
the parse-tree based features such as the mean depth
of parsing tree levels. However, these studies found
that these measures did not show satisfactory empir-
ical performance in automatic speech scoring (Chen
and Yoon, 2011; Chen and Zechner, 2011) when the
features were calculated from the output of a speech
recognition engine.
This study considers new features that measure
syntactic complexity and is novel in two important
ways. First, in contrast to most features that in-
fer syntactic complexity based upon the length of
the unit, we directly measure students? sophistica-
tion and range in grammar usage. Second, instead
of rating a student?s response using a scale based on
native speech production, our experiments compare
it with a similar body of learners? speech. Elicit-
ing native speakers? data and rating it for grammar
usage (supervised approach) can be arbitrary, since
there can be a very wide range of possible grammat-
ical structures that native speakers utilize. Instead,
we proceed in a semi-supervised fashion. A large
amount of learners? spoken responses were collected
and classified into four groups according to their
proficiency level. We then sought to find how dis-
tinct the proficiency classes were based on the distri-
bution of POS tags. Given a student?s response, we
calculated the similarity with a sample of responses
for each score level based on the proportion and dis-
tribution of Part-of-Speech using NLP techniques.
POS tag distribution has been used in various
tasks such as text genre classification (Feldman et
al., 2009); in a language testing context, it has been
used in grammatical error detection (Chodorow and
Leacock, 2000; Tetreault and Chodorow, 2008) and
essay scoring. Recently, Roark et al2011) ex-
plored POS tag distribution to capture the differ-
ences in syntactic complexity between healthy sub-
jects and subjects with mild cognitive impairment,
but no other research has used POS tag distribution
in measuring syntactic complexity, to the best of au-
thors? knowledge.
An assessment of ESL learners? syntactic compe-
tence should consider the structure of sentences as a
whole - a task which may not be captured by the sim-
plistic POS tag distribution. However, studies of Lu
(2010) and Chen and Zechner (2011) showed that
more complex syntactic features are unreliable in
ASR-based scoring system. Furthermore, we show
that POS unigrams or bigrams indeed capture a rea-
sonable portion of learners? range and sophistication
of grammar usage in our discussion in Section 7.
This paper will proceed as follows: we will re-
view related work in Section 2 and present the
method to calculate syntactic complexity in Section
3. Data and experiment setup will be explained in
Section 4 and Section 5. The results will be pre-
sented in Section 6. Finally, in Section 7, we discuss
the levels of syntactic competence that are captured
using our proposed measure.
2 Related Work
Second Language Acquisition (SLA) researchers
have developed many quantitative measures to es-
timate the level of acquisition of syntactic compe-
tence. Bardovi-Harlig and Bofman (1989) classi-
fied these measures into two groups. The first group
is related to the acquisition of specific morphosyn-
tactic features or grammatical expressions. Tests of
negations or relative clauses - whether these expres-
sions occurred in the test responses without errors -
fell into this group (hereafter, the expression-based
group). The second group is related to the length of
the clause or the relationship between clauses and
hence not tied to particular structures (hereafter, the
length-based group). Examples of the second group
measures include the average length of clause unit
and dependent clauses per sentence unit.
These syntactic measures have been extensively
studied in ESL writing. Ortega (2003) synthesized
25 research studies which employed syntactic mea-
sures on ESL writing and reported a significant re-
lationship between the proposed features and writ-
ing proficiency. He reported that a subset of features
such as the mean length of the clause unit increased
with students? proficiency. More recently, Lu (2010)
601
has conducted a more systematic study using an au-
tomated system. He applied 14 syntactic measures
to a large database of Chinese learners? writing sam-
ples and found that syntactic measures were strong
predictors of students? writing proficiency.
Studies in the area of automated speech scor-
ing have only recently begun to actively investi-
gate the usefulness of syntactic measures for scoring
spontaneous speech (Chen et al2010; Bernstein
et al2010). These have identified clause bound-
aries (identified from manual annotations and au-
tomatically) and obtained length-based features. In
addition to these conventional syntactic complexity
features, Lu (2009) implemented an automated sys-
tem that calculates the revised Developmental Level
(D-Level) Scale (Covington et al2006) using nat-
ural language processing (NLP) techniques. The
original D-Level Scale was proposed by Rosenberg
and Abbeduto (1987) based primarily on observa-
tions of child language acquisition. They classified
children?s grammatical acquisition into 7 different
groups according to the presence of certain types of
complex sentences. The revised D-Level Scale clas-
sified sentences into the eight levels according to the
presence of particular grammatical expressions. For
instance, level 0 is comprised of simple sentences,
while level 5 is comprised of sentences joined by
subordinating conjunction or nonfinite clauses in an
adjunct position. The D-Level Scale has been less
studied in the speech scoring. To our knowledge,
Chen and Zechner (2011) is the only study that ap-
plied the D-Level analyzer to ESL learners? spoken
responses.
In contrast to ESL writing, applying syntactic
complexity features, both conventional length-based
features and D-Level features, presents serious ob-
stacles for speaking. First, the length of the spo-
ken responses are typically shorter than written re-
sponses. Most measures are based on sentence or
sentence-like units, and in speaking tests that elicit
only a few sentences the measures are less reli-
able. Chen and Yoon (2011) observed a marked
decrease in correlation between syntactic measures
and proficiency as response length decreased. In
addition, speech recognition errors only worsen the
situation. Chen and Zechner (2011) showed that
the significant correlation between syntactic mea-
sures and speech proficiency (correlation coefficient
= 0.49) became insignificant when they were applied
to the speech recognition word hypotheses. Errors
in speech recognition seriously influenced the mea-
sures and decreased the performance. Due to these
problems, the existing syntactic measures do not
seem reliable enough for being used in automated
speech proficiency scoring.
In this study, we propose novel syntactic measures
which are relatively robust against speech recogni-
tion errors and are reliable in short responses. In
contrast to recent studies focusing on length-based
features, we focus on capturing differences in the
distribution of morphosyntactic features or gram-
matical expressions across proficiency levels. We in-
vestigate the distribution of a broader class of gram-
matical forms through the use of corpus-based NLP
techniques.
3 Method
Many previous studies, that assess syntactic com-
plexity based on the distribution of morpho-
syntactic features and grammatical expressions, lim-
ited their experiments to a few grammatical expres-
sions. Covington et al2006) and Lu (2009) cov-
ered all sentence types, but their approaches were
based on expert observation (supervised rubrics),
and descriptions of each level were brief and ab-
stract. It is important to develop a more detailed and
refined scale, but developing scales in a supervised
way is difficult due to the subjectivity and the com-
plexity of structures involved.
In order to overcome this problem, we employed
NLP technology and a corpus-based approach. We
hypothesize that the level of acquired grammatical
forms is signaled by the distribution of the POS tags,
and the differences in grammatical proficiency re-
sult in differences in POS tag distribution. Based on
this assumption, we collected large amount of ESL
learners? spoken responses and classified them into
four groups according to their proficiency levels.
The syntactic competence was estimated based on
the similarity between the test responses and learn-
ers? corpus.
A POS-based vector space model (VSM), in
which the response belonging to separate profi-
ciency levels were converted to vectors and the sim-
ilarity between vectors were calculated using cosine
602
similarity measure and tf-idf weighting, was em-
ployed. Such a score-category-based VSM has been
used in automated essay scoring. Attali and Burstein
(2006) to assess the lexical content of an essay by
comparing the words in the test essay with the words
in a sample essays from each score category. We
extend this to assessment of grammar usage using
vectors of POS tags.
Proficient speakers use complicated grammati-
cal expressions, while beginners use simple expres-
sions and sentences with frequent grammatical er-
rors. POS tags (or sequences) capturing these ex-
pressions may be seen in corresponding proportions
in each score group. These distributional differences
are captured by inverse-document frequency.
In addition, we identify frequent POS tag se-
quences as those having high mutual information
and include them in our experiments. Temple (2000)
pointed out that the proficient learners are charac-
terized by increased automaticity in speech produc-
tion. These speakers tend to memorize frequently
used multi-word sequences as a chunk and retrieve
the whole chunk as a single unit. The degree of auto-
maticity can be captured by the frequent occurrence
of POS sequences with high mutual information.
We quantify the usefulness of the generated fea-
tures for the purpose of automatic scoring by first
considering its correlation with the human scores.
We then compare the performance of our features
with those in Lu (2011), where the features are a
collection of measures of syntactic complexity that
have shown promising directions in previous stud-
ies.
4 Data
Two different sets of data were used in this study:
the AEST 48K dataset and AEST balanced dataset.
Both were collections of responses from the AEST,
a high-stakes test of English proficiency and had
no overlaps. The AEST assessment consists of 6
items in which speakers are prompted to provide re-
sponses lasting between 45 and 60 seconds per item.
In summary, approximately 3 minutes of speech is
collected per speaker.
Among the 6 items, two items are tasks that ask
examinees to provide information or opinions on fa-
miliar topics based on their personal experience or
background knowledge. The four remaining items
are integrated tasks that include other language skills
such as listening and reading. All items extract
spontaneous, unconstrained natural speech. The
size, purpose, and speakers? native language infor-
mation for each dataset is summarized in Table 1.
Each response was rated by trained human raters
using a 4-point scoring scale, where 1 indicates
a low speaking proficiency and 4 indicates a high
speaking proficiency. In order to evaluate the relia-
bility of the human ratings, the data should be scored
by two raters. Since none of the AEST balanced
data was double scored the inter-rater agreement ra-
tio was estimated using a large (41K) double-scored
dataset using the same scoring guidelines and scor-
ing process. The Pearson correlation coefficient was
0.63 suggesting a reasonable inter-rater agreement.
The distribution of the scores for this data can be
found in Table 2.
We used the AEST 48K dataset as the training
data and the AEST balanced dataset as the evalua-
tion data.
5 Experiments
5.1 Overview
Our experimental procedure is as follows. All tran-
scriptions were tagged using the POS tagger de-
scribed in Section 5.3 and POS tag sequences were
extracted. Next, the POS-based VSMs (one for
each score class) were created using the AEST 48K
dataset. Finally, for a given test response in the
AEST balanced dataset, similarity features were
generated.
A score-class-specific POS-based VSM was cre-
ated using POS tags generated from the manual tran-
scriptions. For evaluation, two different types of
transcriptions (manual transcription and word hy-
potheses from the speech recognizer described in
Section 5.2) were used in order to investigate the in-
fluence of speech recognition errors in the feature
performance.
5.2 Speech recognition
An HMM recognizer was trained on AEST 48K
dataset - approximately 733 hours of non-native
speech collected from 7872 speakers. A gender in-
dependent triphone acoustic model and combination
603
Corpus name Purpose Number of
speakers
Number of
responses
Native languages Size
(Hrs)
AEST 48K
data
ASR training and
POS model train-
ing
7872 47227 China (20%), Korea (19%),
Japanese (7%), India (7%), oth-
ers (46%)
733
AEST bal-
anced data
Feature develop-
ment and evalua-
tion
480 2880 Korean (15%), Chinese (14%),
Japanese (7%), Spanish (9%),
Others (55%)
44
Table 1: Data size and speakers native languages
Corpus name Size Score1 Score2 Score3 Score4
AEST 48K data Number of files 1953 16834 23106 5334
(%) 4 36 49 11
AEST balanced data Number of files 141 1133 1266 340
(%) 5 40 45 12
Table 2: Proficiency scores and data sizes
of bigram, trigram, and four-gram language models
were used. The word error rate (WER) on the held-
out test dataset was 27%.
5.3 POS tagger
POS tags were generated using the POS tagger im-
plemented in the OpenNLP toolkit. It was trained
on the Switchboard (SWBD) corpus. This POS tag-
ger was trained on about 528K word/tag pairs and
achieved a tagging accuracy of 96.3% on a test set
of 379K words. The Penn POS tag set was used in
the tagger.
5.4 Unit generation using mutual information
POS bigrams with high mutual information were se-
lected and used as a single unit. First, all POS bi-
grams which occurred less than 50 times were fil-
tered out. Next, the remaining POS tag bigrams
were sorted by their mutual information scores, and
two different sets (top50 and top110) were selected.
The selected POS pairs were transformed into com-
pound tags. As a result, we generated three sets
of POS units by this process: the original POS set
without the compound unit (Base), the original set
and an additional 50 compound units (Base+mi50),
and the original set and an additional 110 units
(Base+mi110).
Finally, unigram, bigram and trigram were gener-
ated for each set separately. The size of total terms
in each condition was presented in table 3.
Base Base+mi50 Base+mi110
Unigram 42 93 151
Bigram 1366 4284 9691
Trigram 21918 54856 135430
Table 3: Number of terms used in VSMs
5.5 Building VSMs
For each ngram, three sets of VSMs were built us-
ing three sets of tags as terms, yielding a total of
nine VSMs. The results were based on the individ-
ual model and we did not combine any models.
5.6 Cosine similarity-based features
The cosine similarity has been frequently used in
the information retrieval field to identify the relevant
documents for the given query. This measures the
similarity between a given query and a document by
measuring the cosine of the angle between vectors in
a high-dimensional space, whereby each term in the
query and documents corresponding to a unique di-
mension. If a document is relevant to the query, then
it shares many terms resulting in a small angle. In
this study, the term was a single or compound POS
tag (unigram,bigram or trigram) weighted by its tf-
idf, and the document was the response.
First, the inverse document frequency was calcu-
lated from the training data, and each response was
treated as a document. Next, responses in the same
604
Unigram Bigram Trigram
Base Base
+mi50
Base
+mi110
Base Base
+mi50
Base
+mi110
Base Base
+mi50
Base
+mi110
Trans-
cription
0.301** 0.297** 0.329** 0.427** 0.361** 0.366** 0.402** 0.322** 0.295**
ASR 0.246** 0.272** 0.304** 0.415** 0.348** 0.347** 0.373** 0.311** 0.282**
Table 4: Pearson correlation coefficients between ngram-based features and expert proficiency scores
** Correlation is significant at the 0.01 level
score group were concatenated, and a single vector
was generated for each score group. A total of 4
vectors were generated using training data. For each
test response, a similarity score was calculated as
follows:
cos(~q, ~dj) =
nP
i=1
qidji
nP
i=1
qi2
nP
i=1
di2
qi ? tf(ti, ~q)? log
(
N
df(ti)
)
dji ? tf(ti, ~dj)? log
(
N
df(ti)
)
where ~q is a vector of the test response,
~dj is a vector of the scoreGroupj ,
n is the total number of POS tags,
tf(ti, ~q) is the term frequency of POS tag ti in the
test response,
tf(ti, ~dj) is the term frequency of POS tag ti in the
scoreGroupj ,
N is the total number of training responses,
df(ti) is the document frequency of POS tag ti in
the total training responses
Finally, a total of 4 cos scores (one per score
group) were generated. Among these four values,
the cos4, the similarity score to the responses in the
score group 4, was selected as a feature with the fol-
lowing intuition. cos4 measures the similarity of a
given test response to the representative vector of
score class 4; the larger the value, the closer it would
be to score class 4.
6 Results
6.1 Correlation
Table 4 shows correlations between cosine similarity
features and proficiency scores rated by experts.
The bigram-based features outperformed both
unigram-based and trigram-based features. In par-
ticular, the similarities using the base tag set with
bigrams achieved the best performance. By adding
the mutual information-based compound units to the
original POS tag sets, the performance of features
improved in the unigram models. However, there
was no performance gain in either bigram or tri-
gram models; on the contrary, there was a large
drop in performance. Unigrams have good coverage
but limited power in distinguishing different score
levels. On the other hand, trigrams have opposite
characteristics. Bigrams seem to strike a balance
in both coverage and complexity (from among the
three considered here) and may thus have resulted in
the best performace.
The performance of ASR-based features were
comparable to that of transcription-based features.
The best performing feature among ASR-based-
features were from the bigram and base set, with
correlations nearly the same as the best performing
one among the transcription-based-features. See-
ing how close the correlations were in the case of
transcription-based and ASR-hypothesis based fea-
ture extraction, we conclude that the proposed mea-
sure is robust to ASR errors.
6.2 Comparison with other Measures of
Syntactic Complexity
We compared the performance of our features with
the features of syntactic complexity proposed in (Lu,
2011). Towards this, the clause boundaries of the
ASR hypotheses, were automatically detected using
the automated clause boundary detection method1.
1The automated clause boundary detection method in this
study was a Maximum Entropy Model based on word bigrams,
POS tag bigrams, and pause features. The method achieved an
605
The utterances were then parsed using the Stanford
Parser, and a total of 22 features including both
length-related features and parse-tree based features
were generated using (Lu, 2011). Finally, we calcu-
lated Pearson correlation coefficients between these
features and human proficiency scores.
Study Feature Correlation
Current study bigram based cos4 0.41**
(Lu, 2011) DCC 0.14**
Table 5: Comparison between (Lu, 2011) and this study
** Correlation is significant at the 0.01 level
As indicated in Table 5, the best performing fea-
ture was mean number of dependent clauses per
clause (DCC) and the correlation r was 0.14. No
features other than DCC achieved statistically sig-
nificant correlation. Our best performing feature (bi-
gram based cos4) widely outperformed the best of
Lu (2011)?s features (correlations approximately 0.3
apart).
A logical explanation for the poor performance of
Lu (2011)?s features is that the features are gener-
ated using multi-stage automated process, and the
errors in each process contributes the low feature
performance. For instance, the errors in the auto-
mated clause boundary detection may result in a se-
rious drop in the performance. With the spoken re-
sponses being particularly short (a typical response
in the data set had 10 clauses on average), even one
error in clause boundary detection can seriously af-
fect the reliability of features.
7 Discussion
While the measure of syntactic competence that we
study here is an abstraction of the overall syntactic
competence, without consideration of specific con-
structions, we analyzed the results further with the
intention of casting light on the level of details of
syntactic competence that can be explained using
our measure. Furthermore, this section will show
that bigram POS sequences can yield significant in-
formation on the range and sophistication of gram-
mar usage in the specific assessment context (spon-
F-score of 0.60 on the non-native speakers? ASR hypotheses.
A detailed description of the method is presented in (Chen and
Zechner, 2011)
taneous speech comprised of only declarative sen-
tences).
ESL speakers with high proficiency scores are ex-
pected to use more complicated grammatical expres-
sions that result in a high proportion of POS tags
related to these expressions in that score group. The
distribution of POS tags was analyzed in detail in or-
der to investigate whether there were systematic dis-
tributional changes according to proficiency levels.
Owing to space constraints, we restrict our discus-
sion to the analysis using unigrams (base and com-
pund). For each score group, the POS tags were
sorted based on the frequencies in training data, and
the rank orders were calculated. The more frequent
the POS tag, the higher its rank.
A total of 150 POS tags, including the original
POS tag set and top 110 compound tags, were clas-
sified into 5 classes:
? Absence-of-low-proficiency (ABS): Group of
POS tags that appear in all score groups except
the lowest proficiency group;
? Increase (INC): Group of POS tags whose
ranks increase consistently as proficiency in-
creases;
? Decrease (DEC): Group of POS tags whose
ranks decrease consistently as proficiency in-
creases;
? Constant (CON): Group of POS tags whose
ranks remain same despite change in profi-
ciency;
? Mix: Group of POS tags of with no consistent
pattern in the ranks.
Table 6 presents the number of POS tags in each
class.
ABS INC DEC CON Mix
14 37 33 18 48
Table 6: Tag distribution and proficiency scores
The ?ABS? class mostly consists of ?WP? and
?WDT?; more than 50% of tags in this class are re-
lated to these two tags. ?WP? is a Wh-pronoun while
?WDT? is a Wh-determiner. Since most sentences in
606
our data are declarative sentences, ?Wh? phrase sig-
nals the use of relative clause. Therefore, the lack
of these tags strongly support the hypothesis that the
speakers in score group 1 showed incompetence in
the use of relative clauses or their use in limited sit-
uations.
The ?INC? class can be sub-classified into three
groups: verb, comparative, and relative clause. Verb
group is includes the infinitive (TO VB), passive
(VB VBN, VBD VBN, VBN, VBN IN, VBN RP),
and gerund forms (VBG, VBG RP, VBG TO). Next,
the comparative group encompasses comparative
constructions. Finally, the relative clause group sig-
nals the presence of relative clauses. The increased
proportion of these tags reflects the use of more
complicated tense forms and modal forms as well
as more frequent use of relative clauses. It supports
the hypothesis that speakers with higher proficiency
scores tend to use more complicated grammatical
expressions.
The ?DEC? class can be sub-classified into five
groups: noun, simple tense verb, GW and UH,
non-compound, and comparative. The noun group
is comprised of many noun or proper noun-related
expressions, and their high proportions are consis-
tent with the tendency that less proficient speakers
use nouns more frequently. Secondly, the simple
tense verb group is comprised of the base form (VB)
and simple present and past forms(PRP VBD, VB,
VBD TO, VBP TO, VBZ). The expressions in these
groups are simpler than those in ?Increase? group.
The ?UH? tag is for interjection and filler words
such as ?uh? and ?um?, while the ?GW? tag is for
word-fragments. These two spontaneous speech
phenomena are strongly related to fluency, and it
signals problems in speech production. Frequent
occurrences of these two tags are evidence of fre-
quent planning problems and their inclusion in the
?DEC? class suggests that instances of speech plan-
ning problems decrease with increased proficiency.
Tags in the non-compound group, such as ?DT?,
?MD?, ?RBS?, and ?TO?, have related compound
tags. The non-compound tags are associated with
the expressions that do not co-occur with strongly
related words, and they tend to be related to errors.
For instance, the non-compound ?MD? tag signals
that there is an expression that a modal verb is not
followed by ?VB? (base form) and as seen in the ex-
amples, ?the project may can change? and ?the others
must can not be good?, they are related to grammat-
ical errors.
Finally, the comparative group includes
?RBR JJR?. The decrease of ?RBR JJR? is re-
lated to the correct acquisition of the comparative
form. ?RBR? is for comparative adverbs and ?JJR? is
for comparative adjectives, and the combination of
two tags is strongly related to double-marked errors
such as ?more easier?. In the intermediate stage in
the acquisition of comparative form, learners tend
to use the double-marked form. The compound tags
correctly capture this erroneous stage.
The ?Decrease? class also includes three Wh-
related tags (WDT NN, WDT VBP, WRB), but the
proportion is much smaller than the ?Increase? class.
The above analysis shows that the combination of
original and compound POS tags correctly capture
systematic changes in the grammatical expressions
according to changes in proficiency levels.
The robust performance of our proposed mea-
sure to speech recognition errors may be better ap-
preciated in the context of similar studies. Com-
pared with the state-of-the art measures of syntac-
tic complexity proposed in Lu (2011) our features
achieve significantly better performance especially
when generated from ASR hypotheses. It is to
be noted that the performance drop between the
transcription-based feature and the ASR hypothesis-
based feature was marginal.
8 Conclusions
In this paper, we presented features that measure
syntactic competence for the automated speech scor-
ing. The features measured the range and sophisti-
cation of grammatical expressions based on POS tag
distributions. A corpus with a large number of learn-
ers? responses was collected and classified into four
groups according to proficiency levels. The syntac-
tic competence of the test response was estimated by
identifying the most similar group from the learners?
corpus. Furthermore, speech recognition errors only
resulted in a minor performance drop. The robust-
ness against speech recognition errors is an impor-
tant advantage of our method.
607
Acknowledgments
The authors would like to thank Shasha Xie, Klaus
Zechner, and Keelan Evanini for their valuable com-
ments, help with data preparation and experiments.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technology,
Learning, and Assessment, 4(3).
Kathleen Bardovi-Harlig and Theodora Bofman. 1989.
Attainment of syntactic and morphological accuracy
by advanced language learners. Studies in Second
Language Acquisition, 11:17?34.
Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010.
Fluency and structural complexity as predictors of L2
oral proficiency. In Proceedings of InterSpeech 2010,
Tokyo, Japan, September.
Lei Chen and Su-Youn Yoon. 2011. Detecting structural
events for assessing non-native speech. In Proceed-
ings of the 6th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 38?45.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics 2011, pages 722?731.
Lei Chen, Joel Tetreault, and Xiaoming Xi. 2010.
Towards using structural events to assess non-native
speech. In Proceedings of the NAACL HLT 2010 Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 74?79.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
In Proceedings of NAACL00, pages 140?147.
Michael A. Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? A proposed revision of the Rosen-
berg and Abbeduto D-Level Scale. Technical report,
CASPR Research Report 2006-01, Athens, GA: The
University of Georgia, Artificial Intelligence Center.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 111(6):2862?2873.
Sergey Feldman, M.A. Marin, Maria Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms for
genre classification of text. In Acoustics, Speech and
Signal Processing, 2009. ICASSP 2009. IEEE Interna-
tional Conference on, pages 4781 ?4784, april.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scoring
for language instruction. In Proceedings of ICASSP
97, pages 1471?1474.
Xiaofei Lu. 2009. Automatic measurement of syntac-
tic complexity in child language acquisition. Interna-
tional Journal of Corpus Linguistics, 14(1):3?28.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2011. L2 syntactic complex-
ity analyze. Retrieved March 17, 2012 from
http://www.personal.psu.edu/xxl13/
downloads/l2sca.html/.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college?level L2 writing. Applied Lin-
guistics, 24(4):492?518.
Brian. Roark, Margaret Mitchell, John-Paul. Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild cog-
nitive impairment. Audio, Speech, and Language
Processing, IEEE Transactions on, 19(7):2081 ?2090,
sept.
Sheldon Rosenberg and Leonard Abbeduto. 1987. Indi-
cators of linguistic competence in the peer group con-
versational behavior of mildly retarded adults. Applied
Psycholinguistics, 8:19?32.
Liz Temple. 2000. Second language learner speech pro-
duction. Studia Linguistica, pages 288?297.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl writing.
In In Proceedings of COLING.
Silke Witt and Steve Young. 1997. Performance mea-
sures for phone-level pronunciation teaching in CALL.
In Proceedings of the Workshop on Speech Technology
in Language Learning, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883?895, October.
608
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1305?1315,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Shallow Analysis Based Assessment of Syntactic Complexity for
Automated Speech Scoring
Suma Bhat
Beckman Institute,
University of Illinois,
Urbana, IL
spbhat2@illinois.edu
Huichao Xue
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA
hux10@cs.pitt.edu
Su-Youn Yoon
Educational Testing Service
Princeton, NJ
syoon@ets.org
Abstract
Designing measures that capture various
aspects of language ability is a central
task in the design of systems for auto-
matic scoring of spontaneous speech. In
this study, we address a key aspect of lan-
guage proficiency assessment ? syntactic
complexity. We propose a novel measure
of syntactic complexity for spontaneous
speech that shows optimum empirical per-
formance on real world data in multiple
ways. First, it is both robust and reliable,
producing automatic scores that agree well
with human rating compared to the state-
of-the-art. Second, the measure makes
sense theoretically, both from algorithmic
and native language acquisition points of
view.
1 Introduction
Assessment of a speaker?s proficiency in a second
language is the main task in the domain of au-
tomatic evaluation of spontaneous speech (Zech-
ner et al, 2009). Prior studies in language ac-
quisition and second language research have con-
clusively shown that proficiency in a second lan-
guage is characterized by several factors, some of
which are, fluency in language production, pro-
nunciation accuracy, choice of vocabulary, gram-
matical sophistication and accuracy. The design of
automated scoring systems for non-native speaker
speaking proficiency is guided by these studies in
the choice of pertinent objective measures of these
key aspects of language proficiency.
The focus of this study is the design and per-
formance analysis of a measure of the syntactic
complexity of non-native English responses for
use in automatic scoring systems. The state-of-
the art automated scoring system for spontaneous
speech (Zechner et al, 2009; Higgins et al, 2011)
currently uses measures of fluency and pronuncia-
tion (acoustic aspects) to produce scores that are in
reasonable agreement with human-rated scores of
proficiency. Despite its good performance, there
is a need to extend its coverage to higher order as-
pects of language ability. Fluency and pronunci-
ation may, by themselves, already be good indi-
cators of proficiency in non-native speakers, but
from a construct validity perspective
1
, it is neces-
sary that an automatic assessment model measure
higher-order aspects of language proficiency. Syn-
tactic complexity is one such aspect of proficiency.
By ?syntactic complexity?, we mean a learner?s
ability to use a wide range of sophisticated gram-
matical structures.
This study is different from studies that fo-
cus on capturing grammatical errors in non-native
speakers (Foster and Skehan, 1996; Iwashita et al,
2008). Instead of focusing on grammatical errors
that are found to be highly representative of lan-
guage proficiency, our interest is in capturing the
range of forms that surface in language production
and the degree of sophistication of such forms,
collectively referred to as syntactic complexity in
(Ortega, 2003).
The choice and design of objective measures of
language proficiency is governed by two crucial
constraints:
1. Validity: a measure should show high dis-
criminative ability between various levels of
language proficiency, and the scores pro-
duced by the use of this measure should show
high agreement with human-assigned scores.
2. Robustness: a measures should be derived
automatically and should be robust to errors
in the measure generation process.
A critical impediment to the robustness con-
straint in the state-of-the-art is the multi-stage au-
1
Construct validity is the degree to which a test measures
what it claims, or purports, to be measuring and an important
criterion in the development and use of assessments or tests.
1305
tomated process, where errors in the speech recog-
nition stage (the very first stage) affect subsequent
stages. Guided by studies in second language de-
velopment, we design a measure of syntactic com-
plexity that captures patterns indicative of profi-
cient and non-proficient grammatical structures by
a shallow-analysis of spoken language, as opposed
to a deep syntactic analysis, and analyze the per-
formance of the automatic scoring model with its
inclusion. We compare and contrast the proposed
measure with that found to be optimum in Yoon
and Bhat (2012).
Our primary contributions in this study are:
? We show that the measure of syntactic com-
plexity derived from a shallow-analysis of
spoken utterances satisfies the design con-
straint of high discriminative ability between
proficiency levels. In addition, including our
proposed measure of syntactic complexity in
an automatic scoring model results in a statis-
tically significant performance gain over the
state-of-the-art.
? The proposed measure, derived through a
completely automated process, satisfies the
robustness criterion reasonably well.
? In the domain of native language acquisition,
the presence or absence of a grammatical
structure indicates grammatical development.
We observe that the proposed approach ele-
gantly and effectively captures this presence-
based criterion of grammatical development,
since the feature indicative of presence or ab-
sence of a grammatical structure is optimal
from an algorithmic point of view.
2 Related Work
Speaking in a non-native language requires diverse
abilities, including fluency, pronunciation, into-
nation, grammar, vocabulary, and discourse. In-
formed by studies in second language acquisition
and language testing that regard these factors as
key determiners of spoken language proficiency,
some researchers have focused on the objective
measurement of these aspects of spoken language
in the context of automatic assessment of language
ability. Notable are studies that have focused on
assessment of fluency (Cucchiarini et al, 2000;
Cucchiarini et al, 2002), pronunciation (Witt and
Young, 1997; Witt, 1999; Franco et al, 1997;
Neumeyer et al, 2000), and intonation (Zechner
et al, 2009). The relative success of these studies
has yielded objective measures of acoustic aspects
of speaking ability, resulting in a shift in focus
to more complex aspects of assessment of gram-
mar (Bernstein et al, 2010; Chen and Yoon, 2011;
Chen and Zechner, 2011), topic development (Xie
et al, 2012), and coherence (Wang et al, 2013).
In an effort to assess grammar and usage in a
second language learning environment, numerous
studies have focused on identifying relevant quan-
titative measures. These measures have been used
to estimate proficiency levels in English as a sec-
ond language (ESL) writing with reasonable suc-
cess. Wolf-Quintero et al (1998), Ortega (2003),
and Lu (2010) found that measures such as mean
length of T-unit
2
and dependent clauses per clause
(henceforth termed as length-based measures) are
well correlated with holistic proficiency scores
suggesting that these quantitative measures can be
used as objective indices of grammatical develop-
ment.
In the context of spoken ESL, these measures
have been studied as well but the results have been
inconclusive. The measures could only broadly
discriminate between students? proficiency levels,
rated on a scale with moderate to weak correla-
tions, and strong data dependencies on the par-
ticipant groups were observed (Halleck, 1995;
Iwashita et al, 2008; Iwashita, 2010).
With the recent interest in the area of auto-
matic assessment of speech, there is a concur-
rent need to assess the grammatical development
of ESL students automatically. Studies that ex-
plored the applicability of length-based measures
in an automated scoring system (Chen and Zech-
ner, 2011; Chen and Yoon, 2011) observed another
important drawback of these measures in that set-
ting. Length-based measures do not meet the con-
straints of the design, that, in order for measures
to be effectively incorporated in the automated
speech scoring system, they must be generated in
a fully automated manner, via a multi-stage au-
tomated process that includes speech recognition,
part of speech (POS) tagging, and parsing.
A major bottleneck in the multi-stage process
of an automated speech scoring system for second
language is the stage of automated speech recog-
nition (ASR). Automatic recognition of non-native
speakers? spontaneous speech is a challenging task
as evidenced by the error rate of the state-of-the-
2
T-units are defined as ?the shortest grammatically allow-
able sentences into which writing can be split.? (Hunt, 1965)
1306
art speech recognizer. For instance, Chen and
Zechner (2011) reported a 50.5% word error rate
(WER) and Yoon and Bhat (2012) reported a 30%
WER in the recognition of ESL students? spoken
responses. These high error rates at the recogni-
tion stage negatively affect the subsequent stages
of the speech scoring system in general, and in
particular, during a deep syntactic analysis, which
operates on a long sequence of words as its con-
text. As a result, measures of grammatical com-
plexity that are closely tied to a correct syntac-
tic analysis are rendered unreliable. Not surpris-
ingly, Chen and Zechner (2011) studied measures
of grammatical complexity via syntactic parsing
and found that a Pearson?s correlation coefficient
of 0.49 between syntactic complexity measures
(derived from manual transcriptions) and profi-
ciency scores, was drastically reduced to near non-
existence when the measures were applied to ASR
word hypotheses. This suggests that measures
that rely on deep syntactic analysis are unreliable
in current ASR-based scoring systems for sponta-
neous speech.
In order to avoid the problems encountered
with deep analysis-based measures, Yoon and
Bhat (2012) explored a shallow analysis-based ap-
proach, based on the assumption that the level of
grammar sophistication at each proficiency level
is reflected in the distribution of part-of-speech
(POS) tag bigrams. The idea of capturing dif-
ferences in POS tag distributions for classification
has been explored in several previous studies. In
the area of text-genre classification, POS tag dis-
tributions have been found to capture genre differ-
ences in text (Feldman et al, 2009; Marin et al,
2009); in a language testing context, it has been
used in grammatical error detection and essay
scoring (Chodorow and Leacock, 2000; Tetreault
and Chodorow, 2008). We will see next what as-
pects of syntactic complexity are captured by such
a shallow-analysis.
3 Shallow-analysis approach to
measuring syntactic complexity
The measures of syntactic complexity in this ap-
proach are POS bigrams and are not obtained by a
deep analysis (syntactic parsing) of the structure of
the sentence. Hence we will refer to this approach
as ?shallow analysis?. In a shallow-analysis ap-
proach to measuring syntactic complexity, we rely
on the distribution of POS bigrams at every profi-
ciency level to be representative of the range and
sophistication of grammatical constructions at that
level. At the outset, POS-bigrams may seem too
simplistic to represent any aspect of true syntactic
complexity. We illustrate to the contrary, that they
are indeed able to capture certain grammatical er-
rors and sophisticated constructions by means of
the following instances. Consider the two sentence
fragments below taken from actual responses (the
bigrams of interest and their associated POS tags
are bold-faced).
1. They can/MD to/TO survive . . .
2. They created the culture/NN that/WDT
now/RB is common in the US.
We notice that Example 1 is not only less gram-
matically sophisticated than Example 2 but also
has a grammatical error. The error stems from the
fact that it has a modal verb followed by the word
?to?. On the other hand, Example 2 contains a
relative clause composed of a noun introduced by
?that?. Notice how these grammatical expressions
(one erroneous and the other sophisticated) can be
detected by the POS bigrams ?MD-TO? and ?NN-
WDT?, respectively.
The idea that the level of syntactic complex-
ity (in terms of its range and sophistication) can
be assessed based on the distribution of POS-tags
is informed by prior studies in second language
acquisition. It has been shown that the usage of
certain grammatical constructions (such as that of
the embedded relative clause in the second sen-
tence above) are indicators of specific milestones
in grammar development (Covington et al, 2006).
In addition, studies such as Foster and Skehan
(1996) have successfully explored the utility of
frequency of grammatical errors as objective mea-
sures of grammatical development.
Based on this idea, Yoon and Bhat (2012) de-
veloped a set of features of syntactic complex-
ity based on POS sequences extracted from a
large corpus of ESL learners? spoken responses,
grouped by human-assigned scores of proficiency
level. Unlike previous studies, it did not rely
on the occurrence of normative grammatical con-
structions. The main assumption was that each
score level is characterized by different types of
prominent grammatical structures. These repre-
sentative constructions are gathered from a collec-
tion of ESL learners? spoken responses rated for
overall proficiency. The syntactic complexity of
a test spoken response was estimated based on its
1307
similarity to the proficiency groups in the refer-
ence corpus with respect to the score-specific con-
structions. A score was assigned to the response
based on how similar it was to the high score
group. In Section 4.1, we go over the approach
in further detail.
Our current work is inspired by the shallow
analysis-based approach of Yoon and Bhat (2012)
and operates under the same assumptions of cap-
turing the range and sophistication of grammati-
cal constructions at each score level. However,
the approaches differ in the way in which a spo-
ken response is assigned to a score group. We
first analyze the limitations of the model studied in
(Yoon and Bhat, 2012) and then describe how our
model can address those limitations. The result is
a new measure based on POS bigrams to assess
ESL learners? mastery of syntactic complexity.
4 Models for Measuring Grammatical
Competence
We mentioned that the measure proposed in this
study is derived from assumptions similar to those
studied in (Yoon and Bhat, 2012). Accordingly,
we will summarize the previously studied model,
outline its limitations, show how our proposed
measure addresses those limitations and compare
the two measures for the task of automatic scoring
of speech.
4.1 Vector-Space Model based approach
Yoon and Bhat (2012) explored an approach in-
spired by information retrieval. They treat the con-
catenated collection of responses from a particular
score-class as a ?super? document. Then, regard-
ing POS bigrams as terms, they construct POS-
based vector space models for each score-class
(there are four score classes denoting levels of pro-
ficiency as will be explained in Section 5.2), thus
yielding four score-specific vector-space models
(VSMs). The terms of the VSM are weighted by
the term frequency-inverse document frequency
(tf -idf ) weighting scheme (Salton et al, 1975).
The intuition behind the approach is that responses
in the same proficiency level often share similar
grammar and usage patterns. The similarity be-
tween a test response and a score-specific vector is
then calculated by a cosine similarity metric. Al-
though a total of 4 cosine similarity scores (one
per score group) were generated, only cos
4
from
among the four similarity scores, and cosmax,
were selected as features.
? cos
4
: the cosine similarity score between the
test response and the vector of POS bigrams
for the highest score class (level 4); and,
? cosmax: the score level of the VSM with
which the given response shows maximum
similarity.
Of these, cos
4
was selected based on its empir-
ical performance (it showed the strongest corre-
lation with human-assigned scores of proficiency
among the distance-based measures). In addition,
an intuitive justification for the choice is that the
score-4 vector is a grammatical ?norm? represent-
ing the average grammar usage distribution of the
most proficient ESL students. The measure of syn-
tactic complexity of a response, cos
4
, is its simi-
larity to the highest score class.
The study found that the measures showed rea-
sonable discriminative ability across proficiency
levels. Despite its encouraging empirical perfor-
mance, the VSM method of capturing grammati-
cal sophistication has the following limitations.
First, the VSM-based method is likely to over-
estimate the contribution of the POS bigrams
when highly correlated bigrams occur as terms in
the VSM. Consider the presence of a grammar pat-
tern represented by more than one POS bigram.
For example, both ?NN-WDT? and ?WDT-RB? in
Sentence 2 reflect the learner?s usage of a relative
clause. However, we note that the two bigrams are
correlated and including them both results in an
over-estimation of their contribution. The VSM
set-up has no mechanism to handle correlated fea-
tures.
Second, the tf -idf weighting scheme for rela-
tively rare POS bigrams does not adequately cap-
ture their underlying distribution with respect to
score groups. Grammatical expressions that occur
frequently in one score level but rarely in other
levels can be assumed to be characteristic of a
specific score level. Therefore, the more uneven
the distribution of a grammatical expression across
score classes, the more important that grammatical
expression should be as an indicator of a particular
score class. However, the simple idf scheme can-
not capture this uneven distribution. A pattern that
occurs rarely but uniformly across different score
groups can get the same weight as a pattern which
is unevenly distributed to one score group. Mar-
tineau and Finin (2009) observed this weakness of
the tf -idf weighting in the domain of sentiment
1308
analysis. When using tf -idf weighting to extract
words that were strongly associated with positive
sentiment in a movie review corpus (they consid-
ered each review as a document and a word as a
term), it was found that a substantial proportion
of words with the highest tf -idf were rare words
(e.g., proper nouns) which were not directly asso-
ciated with the sentiment.
We propose to address these important limita-
tions of the VSM approach by the use of a method
that accounts for each of the deficiencies. This is
done by resorting to a maximum entropy model
based approach, to which we turn next.
4.2 Maximum Entropy-Based model
In order to address the limitations discussed in 4.1,
we propose a classification-based approach. Tak-
ing an approach different from previous studies,
we formulate the task of assigning a score of syn-
tactic complexity to a spoken response as a classi-
fication problem: given a spoken response, assign
the response to a proficiency class. A classifier is
trained in an inductive fashion, using a large cor-
pus of learner responses that is divided into pro-
ficiency scores as the training data and then used
to test data that is similar to the training data. A
distinguishing feature of the current study is that
the measure is based on a comparison of charac-
teristics of the test response to models trained on
large amounts of data from each score point, as op-
posed to measures that are simply characteristics
of the responses themselves (which is how mea-
sures have been considered in prior studies).
The inductive classifier we use here is the
maximum-entropy model (MaxEnt) which has
been used to solve several statistical natural lan-
guage processing problems with much success
(Berger et al, 1996; Borthwick et al, 1998; Borth-
wick, 1999; Pang et al, 2002; Klein et al, 2003;
Rosenfeld, 2005). The productive feature en-
gineering aspects of incorporating features into
the discriminative MaxEnt classifier motivate the
model choice for the problem at hand. In partic-
ular, the ability of the MaxEnt model?s estimation
routine to handle overlapping (correlated) features
makes it directly applicable to address the first lim-
itation of the VSM model. The second limitation,
related to the ineffective weighting of terms via
the the tf -idf scheme, seems to be addressed by
the fact that the MaxEnt model assigns a weight
to each feature (in our case, POS bigrams) on a
per-class basis (in our case, score group), by tak-
ing every instance into consideration. Therefore,
a MaxEnt model has an advantage over the model
described in 4.1 in that it uses four different weight
schemes (one per score level) and each scheme is
optimized for each score level. This is beneficial
in situations where the features are not evenly im-
portant across all score levels.
5 Experimental Setup
Our experiments seek answers to the following
questions.
1. To what extent does a MaxEnt-score of syn-
tactic complexity discriminate between levels
of proficiency?
2. What is the effect of including the proposed
measure of syntactic complexity in the state-
of-the-art automatic scoring model?
3. How robust is the measure to errors in the var-
ious stages of automatic generation?
5.1 Tasks
In order to answer the motivating questions of the
study, we set-up two tasks. In the first task, we
compare the extent to which the VSM-based mea-
sure and the MaxEnt-based measure (outlined in
4.1 and 4.2 above) discriminate between levels of
syntactic complexity. Additionally, we compare
the performance of an automatic scoring model of
overall proficiency that includes the measures of
syntactic complexity from each of the two mod-
els being compared and analyze the gains with re-
spect to the state-of-the-art. In the second task, we
study the measures? robustness to errors incurred
by ASR.
5.2 Data
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment consisted of questions to
which speakers were prompted to provide sponta-
neous spoken responses lasting approximately 45-
60 seconds per question. Test takers read and/or
listened to stimulus materials and then responded
to questions based on the stimuli. All questions so-
licited spontaneous, unconstrained natural speech.
A small portion of the available data with inad-
equate audio quality and lack of student response
was excluded from the study. The remaining re-
sponses were partitioned into two datasets: the
ASR set and the scoring model training/test (SM)
1309
set. The ASR set, with 47,227 responses, was
used for ASR training and POS similarity model
training. The SM set, with 2,950 responses, was
used for feature evaluation and automated scoring
model evaluation. There was no overlap in speak-
ers between the ASR set and the SM set.
Each response was rated for overall proficiency
by trained human scorers using a 4-point scoring
scale, where 1 indicates low speaking proficiency
and 4 indicated high speaking proficiency. The
distribution of proficiency scores, along with other
details of the data sets, are presented in Table 1.
As seen in Table 1, there is a strong bias towards
the middle scores (score 2 and 3) with approxi-
mately 84-85% of the responses belonging to these
two score levels. Although the skewed distribution
limits the number of score-specific instances for
the highest and lowest scores available for model
training, we used the data without modifying the
distribution since it is representative of responses
in a large-scale language assessment scenario.
Human raters? extent of agreement in the sub-
jective task of rating responses for language pro-
ficiency constrains the extent to which we can ex-
pect a machine?s score to agree with that of hu-
mans. An estimate of the extent to which human
raters agree on the subjective task of proficiency
assessment, is obtained by two raters scoring ap-
proximately 5% of data (2,388 responses from
ASR set and 140 responses from SM set). Pear-
son correlation r between the scores assigned by
the two raters was 0.62 in ASR set and 0.58 in SM
set. This level of agreement will guide the evalua-
tion of the human-machine agreement on scores.
5.3 Stages of Automatic Grammatical
Competence Assessment
Here we outline the multiple stages involved in the
automatic syntactic complexity assessment. The
first stage, ASR, yields an automatic transcription,
which is followed by the POS tagging stage. Sub-
sequently, the feature extraction stage (a VSM or
a MaxEnt model as the case may be) generates the
syntactic complexity feature which is then incor-
porated in a multiple linear regression model to
generate a score.
The steps for automatic assessment of overall
proficiency follow an analogous process (either in-
cluding the POS tagger or not), depending on the
objective measure being evaluated. The various
objective measures are then combined in the mul-
tiple regression scoring model to generate an over-
all score of proficiency.
5.3.1 Automatic Speech Recognizer
An HMM recognizer was trained using ASR set
(approximately 733 hours of non-native speech
collected from 7,872 speakers). A gender inde-
pendent triphone acoustic model and combination
of bigram, trigram, and four-gram language mod-
els were used. A word error rate (WER) of 31%
on the SM dataset was observed.
5.3.2 POS tagger
POS tags were generated using the POS tagger
implemented in the Open-NLP toolkit
3
. It was
trained on the Switchboard (SWBD) corpus. This
POS tagger was trained on about 528K word/tag
pairs. A combination of 36 tags from the Penn
Treebank tag set and 6 tags generated for spoken
languages were used in the tagger.
The tagger achieved a tagging accuracy of
96.3% on a Switchboard evaluation set composed
of 379K words, suggesting high accuracy of the
tagger. However, due to substantial amount of
speech recognition errors in our data, the POS
error rate (resulting from the combined errors of
ASR and automated POS tagger) is expected to be
higher.
5.3.3 VSM-based Model
We used the ASR data set to train a POS-bigram
VSM for the highest score class and generated
cos
4
and cosmax reported in Yoon and Bhat
(2012), for the SM data set as outlined in Sec-
tion 4.1.
5.3.4 Maximum Entropy Model Classifier
The input to the classifier is a set of POS bi-
grams (1366 bigrams in all) obtained from the
POS-tagged output of the data. We considered
binary-valued features (whether a POS bigram oc-
curred or not), occurrence frequency, and relative
frequency as input for the purpose of experimen-
tation. We used the maximum entropy classifier
implementation in the MaxEnt toolkit
4
. The clas-
sifier was trained using the LBFGS algorithm for
parameter estimation and used equal-scale gaus-
sian priors for smoothing. The results that fol-
low are based on MaxEnt classifier?s parameter
settings initialized to zero. Since a preliminary
3
http://opennlp.apache.org
4
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
1310
Data set No. of No. of Score Score distribution
responses speakers Mean SD 1 2 3 4
ASR 47,227 7,872 2.67 0.73 1,953 16,834 23,106 5,334
4% 36% 49% 11%
SM 2,950 500 2.61 0.74 166 1,103 1,385 296
6% 37% 47% 10%
Table 1: Data size and score distribution
analysis of the effect of varying the feature (bi-
nary or frequency) revealed that the binary-valued
feature was optimal (in terms of yielding the best
agreement between human and machine scores),
we only report our results for this case. The ASR
data set was used to train the MaxEnt classifier and
the features generated from the SM data set were
used for evaluation.
One straightforward way of using the maximum
entropy classifier?s prediction for our case is to
directly use its predicted score-level ? 1, 2, 3 or
4. However, this forces the classifier to make a
coarse-grained choice and may over-penalize the
classifier?s scoring errors. To illustrate this, con-
sider a scenario where the classifier assigns two
responses A and B to score level 2 (based on the
maximum a posteriori condition). Suppose that,
for response A, the score class with the second
highest probability corresponds to score level 1
and that, for response B, it corresponds to score
level 3. It is apparent that the classifier has an
overall tendency to assign a higher score to B, but
looking at its top preference alone (2 for both re-
sponses), masks this tendency.
We thus capture the classifier?s finer-grained
scoring tendency by calculating the expected value
of the classifier output. For a given response, the
MaxEnt classifier calculates the conditional prob-
ability of a score-class given the response, in turn
yielding conditional probabilities of each score
group given the observation ? p
i
for score group
i ? {1, 2, 3, 4}. In our case, we consider the pre-
dicted score of syntactic complexity to be the ex-
pected value of the class label given the observa-
tion as, mescore = 1?p
1
+2?p
2
+3?p
3
+4?p
4
.
This permits us to better represent the score as-
signed by the MaxEnt classifier as a relative pref-
erence over score assignments.
5.3.5 Automatic Scoring System
We consider a multiple regression automatic scor-
ing model as studied in Zechner et al (2009; Chen
and Zechner (2011; Higgins et al (2011). In its
state-of-the-art set-up, the following model uses
the features ? HMM acoustic model score (global
normalized), speaking rate, word types per sec-
ond, average chunk length in words and language
model score (global normalized). We use these
features by themselves (Base), and also in con-
junction with the VSM-based feature (cva4) and
the MaxEnt-based feature (mescore).
5.4 Evaluation Metric
We evaluate the measures using the metrics cho-
sen in previous studies (Zechner et al, 2009; Chen
and Zechner, 2011; Yoon and Bhat, 2012). A
measure?s utility has been evaluated according to
its ability to discriminate between levels of pro-
ficiency assigned by human raters. This is done
by considering the Pearson correlation coefficient
between the feature and the human scores. In an
ideal situation, we would have compared machine
score with scores of grammatical skill assigned by
human raters. In our case, however, with only
access to the overall proficiency scores, we use
scores of language proficiency as those of gram-
matical skill.
A criterion for evaluating the performance of
the scoring model is the extent to which the au-
tomatic scores of overall proficiency agree with
the human scores. As in prior studies, here too
the level of agreement is evaluated by means of
the weighted kappa measure as well as unrounded
and rounded Pearson?s correlations between ma-
chine and human scores (since the output of the re-
gression model can either be rounded or regarded
as is). The feature that maximizes this degree of
agreement will be preferred.
6 Experimental Results
First, we compare the discriminative ability of
measures of syntactic complexity (VSM-model
based measure with that of the MaxEnt-based
measure) across proficiency levels. Table 2 sum-
marizes our experimental results for this task. We
1311
Features Manual Transcriptions ASR
mescore 0.57 0.52
cos
4
0.48 0.43
cosmax - 0.31
Table 2: Pearson correlation coefficients between measures and holistic proficiency scores. All values
are significant at level 0.01. Only the measures cos
4
and mescore were compared for robustness using
manual and ASR transcriptions.
notice that of the measures compared, mescore
shows the highest correlation with scores of syn-
tactic complexity. The correlation was approxi-
mately 0.1 higher in absolute value than that of
cos
4
, which was the best performing feature in the
VSM-based model and the difference is statisti-
cally significant.
Seeking to study the robustness of the mea-
sures derived using a shallow analysis, we next
compare the two measures studied here, with re-
spect to the impact of speech recognition errors on
their correlation with scores of syntactic complex-
ity. Towards this end, we compare mescore and
cos
4
when POS bigrams are extracted from man-
ual transcriptions (ideal ASR) and ASR transcrip-
tions.
In Table 2, noticing that the correlations de-
crease going along a row, we can say that the er-
rors in the ASR system caused both mescore and
cos
4
to under-perform. However, the performance
drop (around 0.05) resulting from a shallow anal-
ysis is relatively small compared to the drop ob-
served while employing a deep syntactic analysis.
Chen and Zechner (2011) found that while using
measures of syntactic complexity obtained from
transcriptions, errors in ASR transcripts caused
over 0.40 drop in correlation from that found with
manual transcriptions
5
. This comparison suggests
that the current POS-based shallow analysis ap-
proach is more robust to ASR errors compared to
a syntactic analysis-based approach.
The effect of the measure of syntactic complex-
ity is best studied by including it in an automatic
scoring model of overall proficiency. We com-
pare the performance gains over the state-of-the-
art with the inclusion of additional features (VSM-
based and MaxEnt-based, in turn). Table 3 shows
the system performance with different grammar
sophistication measures. The results reported are
averaged over a 5-fold cross validation of the mul-
tiple regression model, where 80% of the SM data
5
Due to differences in the dataset and ASR system, a di-
rect comparison between the current study and the cited prior
study was not possible.
set is used to train the model and the evaluation is
done using 20% of the data in every fold.
As seen in Table 3, using the proposed measure,
mescore, leads to an improved agreement be-
tween human and machine scores of proficiency.
Comparing the unrounded correlation results in
Table 3 we notice that the model Base+mescore
shows the highest correlation of predicted scores
with human scores. In addition, we test the sig-
nificance of the difference between two depen-
dent correlations using Steiger?s Z-test (via the
paired.r function in the R statistical package
(Revelle, 2012)). We note that the performance
gain of Base+mescore over Base as well as over
Base + cos4 is statistically significant at level =
0.01. The performance gain of Base+cos4 over
Base, however, is not statistically significant at
level = 0.01. Thus, the inclusion of the MaxEnt-
based measure of syntactic complexity results in
improved agreement between machine and hu-
man scores compared to the state-of-the-art model
(here, Base).
7 Discussions
We now discuss some of the observations and re-
sults of our study with respect to the following
items.
Improved performance: We sought to verify
empirically that the MaxEnt model really outper-
forms the VSM in the case of correlated POS
bigrams. To see this, we separate the test set
into three subsets A,B,C. Set A contains re-
sponses where MaxEnt outperforms VSM; set B
contains responses where VSM outperforms Max-
Ent; set C contains responses where their predic-
tions are comparable. For each group of responses
s ? {A,B,C}, we calculate the percentage of re-
sponses P
s
where two highly correlated POS bi-
grams occur
6
. We found that the percentages fol-
low the order: P
A
= 12.93% > P
C
= 7.29% >
6
We consider two POS bigrams to be highly correlated,
when the their pointwise-mutual information is higher than
4.
1312
Evaluation method Base Base+cos4 Base+mescore
Weighted kappa 0.503 0.524 0.546
Correlation (unrounded) 0.548 0.562 0.592
Correlation (rounded) 0.482 0.492 0.519
Table 3: Comparison of scoring model performances using features of syntactic complexity studied in
this paper along with those available in the state-of-the-art. Here, Base is the scoring model without the
measures of syntactic complexity. All correlations are significant at level 0.01.
P
B
= 4.41%. This suggests that when correlated
POS bigrams occur, MaxEnt is more likely to pro-
vide better score predictions than VSM does.
Feature design: In the case of MaxEnt,
the observation that binary-valued features (pres-
ence/absence of POS bigrams) yield better perfor-
mance than features indicative of the occurrence
frequency of the bigram has interesting implica-
tions. This was also observed in Pang et al (2002)
where it was interpreted to mean that overall senti-
ment is indicated by the presence/absence of key-
words, as opposed to topic of a text, which is in-
dicated by the repeated use of the same or simi-
lar terms. An analogous explanation is applicable
here.
At first glance, the use of the presence/absence
of grammatical structures may raise concerns
about a potential loss of information (e.g. the dis-
tinction between an expression that is used once
and another that is used multiple times is lost).
However, when considered in the context of lan-
guage acquisition studies, this approach seems to
be justified. Studies in native language acquisi-
tion, have considered multiple grammatical devel-
opmental indices that represent the grammatical
levels reached at various stages of language acqui-
sition. For instance, Covington et al (2006) pro-
posed the revised D-level scale which was origi-
nally studied by Rosenberg and Abbeduto (1987).
The D-Level Scale categorizes grammatical de-
velopment into 8 levels according to the pres-
ence of a set of diverse grammatical expressions
varying in difficulty (for example, level 0 con-
sists of simple sentences, while level 5 consists
of sentences joined by a subordinating conjunc-
tion). Similarly, Scarborough (1990) proposed
the Index of Productive Syntax (IPSyn), accord-
ing to which, the presence of particular grammati-
cal structures, from a list of 60 structures (ranging
from simple ones such as including only subjects
and verbs, to more complex constructions such as
conjoined sentences) is evidence of language ac-
quisition milestones.
Despite the functional differences between the
indices, there is a fundamental operational simi-
larity - that they both use the presence or absence
of grammatical structures, rather than their oc-
currence count, as evidence of acquisition of cer-
tain grammatical levels. The assumption that a
presence-based view of grammatical level acquisi-
tion is also applicable to second language assess-
ment helps validate our observation that binary-
valued features yield a better performance when
compared with frequency-valued features.
Generalizability: The training and test sets
used in this study had similar underlying distribu-
tions ? they both sought unconstrained responses
to a set of items with some minor differences in
item type. Looking ahead, an important question
is the extent to which our measure is sensitive to a
mismatch between training and test data.
8 Conclusions
Seeking alternatives to measuring syntactic com-
plexity of spoken responses via syntactic parsers,
we study a shallow-analysis based approach for
use in automatic scoring.
Empirically, we show that the proposed mea-
sure, based on a maximum entropy classification,
satisfied the constraints of the design of an objec-
tive measure to a high degree. In addition, the pro-
posed measure was found to be relatively robust to
ASR errors. The measure outperformed a related
measure of syntactic complexity (also based on
shallow-analysis of spoken response) previously
found to be well-suited for automatic scoring. In-
cluding the measure of syntactic complexity in
an automatic scoring model resulted in statisti-
cally significant performance gains over the state-
of-the-art. We also make an interesting observa-
tion that the impressionistic evaluation of syntactic
complexity is better approximated by the presence
or absence of grammar and usage patterns (and
not by their frequency of occurrence), an idea sup-
ported by studies in native language acquisition.
1313
References
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
Speech, pages 1241?1244.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting diverse
knowledge sources via maximum entropy in named
entity recognition. In Proc. of the Sixth Workshop
on Very Large Corpora.
Andrew Borthwick. 1999. A maximum entropy ap-
proach to named entity recognition. Ph.D. thesis,
New York University.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
IUNLPBEA ?11, pages 38?45, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of NAACL, pages 140?147.
Michael A Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? a proposed revision of the rosen-
berg and abbeduto d-level scale. ReVision. Wash-
ington, DC http://www. ai. uga. edu/caspr/2006-01-
Covington. pdf.(Accessed May 10, 2010.).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. The Journal of the Acoustical Soci-
ety of America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: comparisons between read and sponta-
neous speech. The Journal of the Acoustical Society
of America, 111(6):2862?2873.
Sergey Feldman, M.A. Marin, Mari Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms
for genre classification of text. In Proceedings of
ICASSP, pages 4781 ?4784.
Pauline Foster and Peter Skehan. 1996. The influence
of planning and task type on second language per-
formance. Studies in Second Language Acquisition,
18:299?324.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scor-
ing for language instruction. In Proceedings of
ICASSP, pages 1471?1474.
Gene B Halleck. 1995. Assessing oral proficiency: a
comparison of holistic and objective measures. The
Modern Language Journal, 79(2):223?234.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech & Language, 25(2):282?
306.
Kellogg W Hunt. 1965. Grammatical structures writ-
ten at three grade levels. ncte research report no. 3.
Noriko Iwashita, Annie Brown, Tim McNamara, and
Sally O?Hagan. 2008. Assessed levels of second
language speaking proficiency: How distinct? Ap-
plied Linguistics, 29(1):24?49.
Noriko Iwashita. 2010. Features of oral proficiency in
task performance by efl and jfl learners. In Selected
proceedings of the Second Language Research Fo-
rum, pages 32?47.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003-Volume 4, pages 180?183. Asso-
ciation for Computational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
M.A Marin, Sergey Feldman, Mari Ostendorf, and
Maya R. Gupta. 2009. Filtering web text to match
target genres. In Proceedings of ICASSP, pages
3705?3708.
Justin Martineau and Tim Finin. 2009. Delta tfidf: An
improved feature space for sentiment analysis. In
ICWSM.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college?level L2 writing. Applied Lin-
guistics, 24(4):492?518.
1314
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. As-
sociation for Computational Linguistics.
William Revelle, 2012. psych: Procedures for Psycho-
logical, Psychometric, and Personality Research.
Northwestern University, Evanston, Illinois. R
package version 1.2.1.
Sheldon Rosenberg and Leonard Abbeduto. 1987. In-
dicators of linguistic competence in the peer group
conversational behavior of mildly retarded adults.
Applied Psycholinguistics, 8:19?32.
Ronald Rosenfeld. 2005. Adaptive statistical language
modeling: a maximum entropy approach. Ph.D. the-
sis, IBM.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Hollis S Scarborough. 1990. Index of productive syn-
tax. Applied Psycholinguistics, 11(1):1?22.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING, pages 865?
872.
Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Pro-
ceedings of NAACL-HLT, pages 814?819.
Silke Witt and Steve Young. 1997. Performance
measures for phone-level pronunciation teaching in
CALL. In Proceedings of STiLL, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Kate Wolf-Quintero, Shunji Inagaki, and Hae-Young
Kim. 1998. Second language development in writ-
ing: Measures of fluency, accuracy, and complexity.
Technical Report 17, Second Language Teaching
and curriculum Center, The University of Hawai?i,
Honolulu, HI.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the NAACL-HLT, pages
103?111.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
esl learners? syntactic competence based on similar-
ity measures. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 600?608. Association for Compu-
tational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883?895.
1315
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 38?45,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Detecting Structural Events for Assessing Non-Native Speech
Lei Chen
Educational Testing Service
Princeton NJ USA
LChen@ets.org
Su-Youn Yoon
Educational Testing Service
Princeton NJ USA
SYoon@ets.org
Abstract
Structural events, (i.e., the structure of clauses
and disfluencies) in spontaneous speech, are
important components of human speaking and
have been used to measure language devel-
opment. However, they have not been ac-
tively used in automated speech assessment
research. Given the recent substantial progress
on automated structural event detection on
spontaneous speech, we investigated the de-
tection of clause boundaries and interruption
points of edit disfluencies on transcriptions
of non-native speech data and extracted fea-
tures from the detected events for speech
assessment. Compared to features com-
puted on human-annotated events, the features
computed on machine-generated events show
promising correlations to holistic scores that
reflect speaking proficiency levels.
1 Introduction
Spontaneous speech utterances are organized in a
structured way and generated dynamically with op-
tional disfluencies. In second language acquisition
(SLA) research, information related to the structure
of utterances and profile of disfluencies has been
widely used to monitor speakers? language develop-
ment processes (Iwashita, 2006). However, struc-
tural events in human conversations have not been
actively used in the automated speech assessment re-
search. For example, most research that used Auto-
matic Speech Recognition (ASR) technology to au-
tomatically score speaking proficiency (Neumeyer
et al, 2000; Zechner et al, 2007) focused on word-
level cues for fluency and accuracy.
In the last decade, a large amount of research (Go-
toh and Renals, 2000; Shriberg et al, 2000; Liu,
2004; Ostendorf et al, 2008) has been conducted
on structural event detection (i.e., sentence and dis-
fluency structure). This research has resulted in
better models for structural event detection. The
detected structural events have been found to help
many of the following natural language processing
(NLP) tasks: speech parsing, information retrieval,
machine translation, and extractive speech summa-
rization (Ostendorf et al, 2008).
Because structural event information: (1) is im-
portant for understanding/processing speech, (2)
has been successfully used in monitoring language
development, which will be summarized in Sec-
tion 2, (3) has received limited attention in auto-
mated speech assessment, and (4) has been actively
investigated in the speech research domain in the
past decade, it is worthwhile investigating the util-
ity of using structural event detection on automated
speech assessment. Because of the fairly low word
accuracy currently achieved when recognizing spon-
taneous non-native speech of mixed proficiency lev-
els and native language backgrounds, this study will
focus on the transcribed words rather than speech
recognition outputs.
This paper is organized as follows: Section 2 re-
views previous research; Section 3 reports on the
data used in the paper, including the collection, scor-
ing, transcription, and annotation processes; Sec-
tion 4 discusses the methods we utilized for struc-
tural event detection; Section 5 describes the exper-
iments of structural event detection; Section 6 de-
scribed the features derived from the event sequence
38
for assessing speech and evaluation results on these
features; Section 7 discusses the findings of our re-
search and plans for future directions.
2 Previous Research
In the SLA and child language development research
fields, language development is measured accord-
ing to fluency, accuracy, and complexity (Iwashita,
2006). Structural events are used to derive the fea-
tures measuring syntactic complexity. For example,
typical metrics for measuring syntactic complexity
include: length of production units (e.g., T-units1,
clauses, verb phrases, and sentences), amount of
embedding, subordination and coordination, range
of structural types, and structural sophistication.
Iwashita (2006) investigated several measures of
syntactic complexity on data generated by learners
of Japanese. The author reported that some mea-
surements (e.g., T-unit length, the number of clauses
per T-unit, and the number of independent clauses
per T-unit) were good at predicting learners? profi-
ciency levels.
In addition, speech disfluencies are used to mea-
sure language development. For example, Lennon
(1990) used a dozen features related to speed,
pauses, and several disfluency markers, such as
filled pauses per T-unit, to measure the improvement
of English proficiency for four German-speaking
women during a six-month study in England. He
found a significant change in filled pauses per T-unit
during the study process.
These two types of features derived from struc-
tural events were combined in other previous stud-
ies. For example, Mizera (2006) used fluency fac-
tors related to speed, voiced smoothness (frequency
of repetitions or self-corrections), pauses, syntactic
complexity (mean length of T-units), and accuracy,
to measure speaking proficiency on 20 non-native
English speakers. In this experiment, disfluency-
related factors, such as the total number of voiced
disfluencies, correlated strongly with the fluency
score (r = ?0.45); however, the syntactic com-
plexity factor only showed a moderate correlation
(r = 0.310).
There have been previous efforts in using NLP
1A T-unit is defined as essentially a main clause plus any
other clauses which are dependent upon it (Hunt, 1970).
technology to automatically calculate syntactic com-
plexity metrics on learners? writing data. For exam-
ple, Lu (2009) and Sagae et al (2005) used parsing
to get structural information on written texts; how-
ever, such efforts have not been undertaken in as-
sessing speech data.
Chen et al (2010) annotated structural events
(such as clause structure and disfluencies) on En-
glish language learners? speech transcriptions and
extracted features based on the structural event pro-
file. They found that the features derived from struc-
tural event profile show promising correlation to hu-
man holistic scores. Berstein et al (2010) also com-
puted the features related to sentence lengths and
the counts of syntactic entities. They found the ex-
tracted features were highly correlated to holistic
scores measuring test-takers? language proficiency
in both English and Spanish.
In the speech research domain, a large amount
of research has been conducted to detect struc-
tural events in speech transcriptions and recognized
words using lexical and prosodic cues. Using a lan-
guage model (LM) trained on words combined with
the events of interest is a popular technique for us-
ing textual information for structural event detec-
tion. For example, Heeman and Allen (1999) devel-
oped a LM including part of speech (POS) tags, dis-
course markers (e.g., right, anyway), speech repairs,
and intonational phrases. In this way, structural in-
formation (e.g., speech repairs), could be predicted
using a traditional speech recognition approach.
Prosodic information has been widely used to fur-
ther improve textual models. For example, a sim-
ple prosodic feature, pause duration between words,
was used in Gotoh and Renals (2000) to detect sen-
tence boundaries. It was found that the pause dura-
tion model alone was better than using an LM alone,
and the combination of the two models further im-
proved the performance.
More advanced prosody models were used in
other research on sentence boundary and speech re-
pair detections (Shriberg et al, 2000; Shriberg and
Stolcke, 2004). A general framework was built com-
bining textual and prosodic cues to detect various
kinds of structural events in speech, including sen-
tence boundaries, disfluencies, topic boundaries, di-
alog acts, emotion, etc. Shriberg and Stolcke (2004)
extracted prosodic features such as pause, phone du-
39
ration, rhyme duration, and F0 features. Using all
of these features, a decision tree was built to de-
tect possible structural events. An LM augmented
with structural event tokens was also used to de-
tect structural events based on textual cues. Fi-
nally, a Hidden Markov Model (HMM) was used
to combine estimations from the textual model (an
augmented LM with structural events) and prosodic
model (decision-tree based on prosodic features).
Research on structural event detection has been
strongly affected by the DARPA EARS pro-
gram (EARS, 2002). As in Shriberg et al (2000), the
structural event detection (e.g., sentence units (SUs)
and speech repairs) investigated in EARS was a clas-
sification task utilizing both prosodic and textual
knowledge sources. New approaches for combin-
ing the two knowledge sources, including maximum
entropy (MaxEnt) and conditional random fields
(CRFs), were studied to address the weaknesses of
the generative HMM approach (Liu et al, 2004). Liu
et al (2005) concluded that ?adding textual infor-
mation, building a more robust prosodic model, us-
ing conditional modeling approaches (Maxent and
CRF), and system combination all yield perfor-
mance gains.?
3 Non-native Structural Event Corpus
Non-native speech data were collected from the
TOEFL Practice Test Online (TPO) (ETS, 2006).
In each TPO test, test-takers were required to re-
spond to six speaking test items, in which they were
required to provide information or opinions on fa-
miliar topics, based on their personal experience or
background knowledge. For example, the test-takers
were asked to describe their opinions about living on
or off campus.
A total of 1066 responses were collected from ex-
aminees. Then, a group of experienced human raters
scored these items based on the scoring rubrics de-
signed for scoring the TPO test. For each item, two
human raters independently assigned 4-point holis-
tic scores for test-takers? English proficiency levels.
The speaking content was transcribed by a pro-
fessional transcribing agency. On the transcrip-
tions, structural event annotations were added, in-
cluding (1) locations of clause boundaries, (2) types
of clauses (e.g., noun clauses, adjective clauses, ad-
verb clauses, etc.), and (3) disfluencies.
Disfluencies can further be sub-classified into sev-
eral groups: silent pauses, filled pauses (e.g., uh and
um), false starts, repetitions, and repairs. The repeti-
tions and repairs were denoted as ?edit disfluency?,
which were comprised of a reparandum, an optional
editing term, and a correction. The reparandum is
the part of an utterance that a speaker wants to re-
peat or change, while the correction contains the
speaker?s correction. The editing term can be a
filled pause (e.g., um) or an explicit expression (e.g.,
sorry). The interruption point (IP), occurring at the
end of the reparandum, is where the fluent speech is
interrupted to prepare for the correction.
For the research reported in this paper, we focus
on two structural events: the locations of clause-
ending boundaries (CBs) and interruption points
(IPs) of edit disfluencies. Note that if several clauses
(in different layers of a clause hierarchy) end at the
same word boundary, these clause boundaries were
collapsed into one CB event.
Two persons annotated the corpus separately and
their annotation quality was monitored by using sev-
eral Kappa computations. For CBs, ? ranges from
0.85 to 0.90; for IPs, ? ranges from 0.63 to 0.83.
Generally, a ? greater than 0.8 indicates a good
between-rater agreement and ? in the range of 0.6
to 0.8 indicates acceptable agreement (Landis and
Koch, 1977). Therefore, we believe that our human
annotations are sufficiently reliable to be used in the
following experiments.
4 Methods of Structural Event Detection
4.1 Features for structural event detection
In previous research (Gotoh and Renals, 2000;
Shriberg et al, 2000; Liu, 2004), prosodic cues
were found to be helpful, however, such findings
on native speech data may not work well with non-
native speech data. Anderson-Hsieh and Venkata-
giri (1994) compared the pause frequencies of three
groups of speakers (native, high-scoring, and low-
scoring non-native speakers). They found that pause
frequency was higher for groups of speakers with
lower speaking skills. For native speakers, a long
pause after a word-ending boundary is an impor-
tant cue for signaling the existence of a sentence or
clause boundary. However, the fact that there are
40
more frequent pauses in non-native speech obscures
this relationship.
On our non-native speech corpus, we conducted
a pilot study on a widely-used prosodic feature, the
pause duration2 after a word, for its predictive abil-
ity to detect clause boundaries. If the duration of the
pause after a word boundary is longer than 0.15 sec-
ond, we call it a long pause. We measured the likeli-
hood of being a CB event on the words followed by a
long pause. For each score level, the likelihoods are:
15% for a score of 1, 22% for a score of 2, 28% for
a score of 3, and 35% for a score of 4. Clearly, for
low-proficiency speakers (i.e., speakers with a score
of 1), long pauses in their utterances are not tightly
linked to CBs. Therefore, more research is needed
to utilize prosodic cues on non-native speech; in this
paper, we focus on lexical features.
4.2 Statistical models
Based on lexical features, the structural event detec-
tion task can be generalized as follows:
E? = argmax
E
P (E|W )
Given that E denotes the between-word event se-
quence and W denotes the corresponding lexical
cues, the goal is to find the event sequence that has
the greatest probability, given the observed features.
Recently, conditional modeling approaches were
successfully used in sentence units (SUs) and speech
repairs detection (Liu, 2004). Hence, we use the
Maximum Entropy (MaxEnt) (Berger et al, 1996)
and Conditional Random Fields (CRFs) (Lafferty et
al., 2001) approaches to build statistical models for
structural event detection.
5 Structural Event Detection Experiment
5.1 Setup
In our experiment, the whole corpus described in
Section 3 was split into a training set (train), a devel-
opment test set (dev), and testing set (test), without
speaker overlap between any pair of sets. Table 1
summarizes the numbers of items and words, as well
as structural events of each dataset.
2Pause durations were obtained by running forced alignment
using speech and transcriptions on a tri-phone HMM speech
recognizer
train dev test
# item 664 101 301
# word 71523 10509 33754
# CB 6121 918 2852
# IP 1767 267 1112
Table 1: The number of items, words, and structural
events of the three sets in the TPO corpus
On average, each item contains about 108.6
words, 9.3 CBs, and 3.0 IPs. 9% of the word bound-
aries are associated with a CB event and 3% of the
word boundaries are associated with an IP event.
Clearly, these CB and IP events are sparse and such
a skewed distribution of structural events increases
the difficulty of structural event detection.
5.2 Models
The following two conditional models were built to
detect CB and IP events:
? MaxEnt: Given wi as the word token at po-
sition i, the word n-gram features include:
?wi?, ?wi?1, wi?, ?wi, wi+1?, ?wi?2, wi?1, wi?,
?wi, wi+1, wi+2?, and ?wi?1, wi, wi+1?. Given
ti as the POS tag3 at position i, the POS
n-gram features include: ?ti?, ?ti?1, ti?,
?ti, ti+1?, ?ti?2, ti?1, ti?, ?ti, ti+1, ti+2?, and
?ti?1, ti, ti+1?.
For IP detection, in addition to the n-gram fea-
tures described above, another four features
that capture syntactic pattern of disfluencies are
utilized:
? filled pause adjacency: This feature has
a binary value showing whether a filled
pause such as uh or um was adjacent to
the current word (wi).
? word repetition: This feature has a binary
value showing whether the current word
(wi) was repeated in the following 5 words
or not.
3POS tags were obtained by tagging words using a MaxEnt
POS tagger, which was implemented in the OpenNLP toolkit
and trained on the Switchboard (SWBD) corpus. This POS tag-
ger was trained on about 528K word/tag pairs and achieved an
tagging accuracy of 96.3% on a test set of 379K words.
41
? similarity: This feature has a continuous
value which measures the similarity be-
tween the reparandum and correction. As-
suming that wi was the end of the reparan-
dum, the start point and the end point of
the reparandum and correction were es-
timated, and the string edit distance be-
tween the reparandum and correction was
calculated. The start point and the end
point of the reparandum and correction
were estimated as follows; if wi appeared
in the following 5 words, the second oc-
currence was defined as the end of the cor-
rection. Otherwise, wi+5 was defined as
the end of correction. Secondly, N , the
length of the correction was calculated,
and wi?N+1 was defined as the start point
of the reparandum. During the calculation
of the string edit distance, a word frag-
ment was considered to be the same as
a word whose initial character sequences
matched it.
? length of correction: This feature counts
the number of words in the correction.
The first two features are similar to the features
used in (Liu, 2004) while the last two features
provide important keys in distinguishing edit
disfluencies from fluent speech. Since the cor-
rection is composed of word sequences that are
similar to the reparandum, these two features
are higher than zero when the target word is a
part of the edit disfluency. In addition, these
two numeric features were discretized by using
an equal-distance binning approach.
Using n-gram features for CB detection and all
these lexical features for IP detection, we used
the Maxent toolkit designed by Zhang (2005) to
build MaxEnt models. The L-BFGS parameter
estimation method is used, with the Gaussian-
prior smoothing technique to avoid over-fitting.
The Gaussian prior is estimated on the dev set.
? CRF: All features which were described in
building MaxEnt models were used in the CRF
model. We used the Java-based NLP package
Mallet (McCallum, 2005) to build CRF mod-
els. Similar to MaxEnt models, Gaussian-prior
smoothing was used with the priors estimated
on the dev set.
These models were trained using the train set. Be-
sides Gaussian priors, other parameters in the model
training (i.e., the training iteration number as well as
the cutting-point for event decisions) were estimated
using the dev set. Finally, the trained models were
evaluated on the test set.
5.3 Evaluation of event detection
Since structural event detection was treated as a clas-
sification task in this paper, four standard evaluation
metrics were used:
accuracy =
TP + TN
TP + FP + TN + FN
precision =
TP
TP + FP
recall =
TP
TP + FN
F1 = 2?
recall ? precision
recall + precision
where, TP and FP denote the number of true pos-
itives and false positives, and TN and FN denote
the number of true negatives and false negatives. A
structural event (a CB or IP boundary) is treated as a
positive class. In our experiment, since we treated
precision and recall as equally important, the F1
measurement was used.
For each model, if the estimated probability,
P (Ei|W ), is larger than a threshold, the correspond-
ing word boundary will be estimated to be a positive
class. The threshold was chosen when a maximal
F1 score was achieved on the dev set.
A model that always predicts the majority class
(a no-event in this study) was treated as a baseline
model. For CB detection, this type of baseline model
resulted in an accuracy of 91.6%; for IP detection,
this type of baseline model resulted in an accuracy
of 96.7%.
5.4 Results of structural event detection
Table 2 summarizes the performance of the two
models on the CB and IP detection tasks.
For CB detection, two conditional models are su-
perior to the baseline CB detection (with an accuracy
of 91.6%); they achieved relatively high F1 scores
42
Acc. Pre. Rec. F1
CB
MaxEnt 94.5 66.1 71.8 0.689
CRF 96.1 82.3 68.6 0.749
IP
MaxEnt 98.1 61.8 55.2 0.583
CRF 98.4 76.9 48.0 0.591
Table 2: Experimental results of the CB and IP detection
measurement using accuracy (Acc.), precision (Pre.), re-
call (Rec.) and F1 measurement (F1) on the TPO data
ranging from 0.689 to 0.749. Between the two mod-
els, the CRF model achieved the higher F1 score
at 0.749, The lower F-score of the MaxEnt model
may be caused by the fact that the MaxEnt model
does not use event history information in its decod-
ing process.
However, these two models achieved lower per-
formance on the task of detecting IPs for editing dis-
fluencies. F-scores became about 0.58 to 0.59 for
IP detections. The degraded performance may be
caused by the extremely low IP distribution (only
3%) in our data. Between the two modeling ap-
proaches, consistent with the result shown for CB
detection, the CRF model achieved a higher F1
score (0.591).
6 Using Detected Structural Events for
Speech Assessment
6.1 Features assessing proficiency
Many previous SLA studies used the length of pro-
duction units and frequency of disfluencies as met-
rics to measure language development (Iwashita,
2006; Lennon, 1990; Mizera, 2006). Our automated
structural event detection provides the locations of
CBs and IPs, which can be used to compute these
features for use in speech assessment.
Using Nw to represent the total number of words
in the spoken response (without pruning the reparan-
dums and edit terms in the edit disfluencies), NC
as the total number of CBs, and NIP as the total
number of IPs detected on transcriptions of speech
streams, the following features (i.e, mean length of
clause (MLC), interruption points per clause (IPC),
and interruption points per word (IPW)) were de-
rived:
MLC = Nw/NC
IPC = NIP /NC
IPW = NIP /Nw
The IPW can be treated as the IPC normalized
by the MLC. The reason for this normalization is
that disfluency behavior is influenced by various fac-
tors, such as speakers? proficiency levels as well as
the difficulty of utterances? structure. For example,
Roll et al (2007) found that the complexity of ex-
pression, computed based on the language?s parsing-
tree structure, influenced the frequency of disflu-
encies in their experiment on Swedish responses.
Therefore, the fact that IPW is the IPC normalized
by MLC (a feature related to complexity of utter-
ances? structure) helps to reduce the impact of utter-
ances? structure and to highlight contributions from
the speaker?s proficiency.
6.2 Results of measuring the derived features
On the test set, we produced CB and IP event se-
quences estimated by the MaxEnt and CRF models,
respectively. These machine-generated events were
evaluated by comparison with human annotations,
which were denoted as REF.
The proposed features described in Section 6.1
were computed on the word/event sequence of each
item. In addition, given the fact that each item only
covers approximately one-minute of speech and the
content is quite limited, we also extracted features
on the test-taker level by combining the detected
events of all of the items spoken by each test-taker.
Then, according to the score handling protocol used
in TPO, the human-holistic scores from the first hu-
man rater were used as item scores to compute Pear-
son correlation coefficients (rs) with the features.
For the test-taker level evaluation, we used the aver-
age score for each test-taker from all of his/her item
scores.
Table 3 reports on the evaluation results of the
features derived from the structural event estima-
tions. Compared to rs computed on the speaker
level using multiple (as many as 6) items, rs com-
puted on the item level are generally lower. This
is because words and events are limited in this one-
minute long response. Among the three features, the
43
Model rMLC rIPC rIPW
Per item
REF 0.003 ?0.369 ?0.402
MaxEnt ?0.012 ?0.329 ?0.343
CRF ?0.042 ?0.328 ?0.335
Per speaker
REF 0.066 ?0.453 ?0.516
MaxEnt 0.055 ?0.396 ?0.417
CRF 0.043 ?0.355 ?0.366
Table 3: Correlation coefficients (rs) between the fea-
tures derived from structural events with human scores
on the item and speaker levels
MLC shows the lowest r to human holistic scores. In
contrast, the two features derived from interruption
points show promising rs to human holistic scores.
Between them, the IPW always shows a higher r
than the IPC. Compared to the features extracted on
human annotations, the features derived from struc-
tural events automatically estimated by the two NLP
models show a lower but sufficiently high r. The
features derived from the MaxEnt model?s estima-
tions on the test-taker level show a greater r than the
features derived from the CRF model estimations.
7 Discussion
Three features measuring syntactic complexity and
disfluency profile of speaking, MLC, IPC, and IPW,
were extracted on the structural event sequences es-
timated by the developed models. Compared to the
features extracted from the human-annotated struc-
tural events, the features derived from machine-
generated event sequences show promisingly close
correlations.
Applying automated structural event detection to
spontaneous speech brings many benefits for auto-
matic speech assessment. First, obtaining informa-
tion beyond the word level, such as the structure of
clauses and disfluencies, can expand and improve
the construct4 coverage of speech features. Second,
knowing the structure of utterances helps to facili-
tate the application of more NLP processing meth-
ods (e.g., collocation detection that requires infor-
mation about sentence boundaries), to speech con-
4A construct is the set of knowledge, skills, and abilities
measured by a test.
tent. In this study, using only simple word and
POS based n-gram features, CBs can be detected
relatively well (with an F1 score of approximately
0.70). More lexical features reflecting repair proper-
ties were found to help improve IP detection perfor-
mance. In addition, IP-based features derived from
machine-generated event sequences show promis-
ing correlation with human holistic scores. Results
in detection of clause boundaries and interruption
points support the approach of utilizing automated
structural event detection on speech assessment.
We plan to continue our research in the following
three directions. First, we will investigate integrat-
ing prosodic cues to further improve the structural
event detection performance on non-native speech.
Second, we will investigate estimating structural
events directly on speech recognition results. Third,
other aspects of syntactic complexity, such as the
embedding of clauses, will be studied to provide a
broader set of features for speech assessment.
References
J. Anderson-Hsieh and H. Venkatagiri. 1994. Syllable
duration and pausing in the speech of chinese ESL
speakers. TESOL Quarterly, pages 807?812.
A. Berger, S. Pietra, and V. Pietra. 1996. A maximum en-
tropy approach to natural language processing. Com-
putational Linguistics, 22:39?72.
J. Berstein, J. Cheng, and M. Suzuki. 2010. Fluency and
Structural Complexity as Predictors of L2 Oral Profi-
ciency. In Proc. of InterSpeech.
L. Chen, J. Tetreault, and X. Xi. 2010. Towards using
structural events to assess non-native speech. In Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, page 74.
EARS. 2002. DARPA EARS Program. http://
projects.ldc.upenn.edu/EARS/.
ETS. 2006. TOEFL Practice Online Test (TPO).
Y. Gotoh and S. Renals. 2000. Sentence boundary de-
tection in broadcast speech transcript. In Proceed-
ings of the International Speech Communication As-
sociation (ISCA) Workshop: Automatic Speech Recog-
nition: Challenges for the new Millennium ASR-2000.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrased and discourse markers: Modeling
speakers? utterances in spoken dialogue. Computa-
tional Linguistics.
K. W. Hunt. 1970. Syntactic maturity in school chil-
dren and adults. In Monographs of the Society for Re-
44
search in Child Development. University of Chicago
Press, Chicago, IL.
N. Iwashita. 2006. Syntactic complexity measures and
their relation to oral proficiency in Japanese as a for-
eign language. Language Assessment Quarterly: An
International Journal, 3(2):151?169.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random field: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML).
J. R Landis and G. G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
pages 159?174.
P. Lennon. 1990. Investigating fluency in EFL: A quanti-
tative approach. Language Learning, 40(3):387?417.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2004.
Comparing and combining generative and poste-
rior probability models: Some advances in sentence
boundary detection in speech. In Proceedings of the
Empirical Methods in Natural Language Processing
(EMNLP).
Y. Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, Hillard
D., M. Ostendorf, M. Tomalin, P. Woodland, and
M. Harper. 2005. Structural Metadata Research in the
EARS Program. In Proceedings of the International
Conference of Acoustics, Speech, and Signal Process-
ing (ICASSP).
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
X. Lu. 2009. Automatic measurement of syntactic com-
plexity in child language acquisition. International
Journal of Corpus Linguistics, 14(1):3?28.
A. McCallum. 2005. Mallet: A machine learning toolkit
for language. http://mallet.cs.umass.edu.
G. J. Mizera. 2006. Working memory and L2 oral flu-
ency. Ph.D. thesis, University of Pittsburgh.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 30:83?93.
M. Ostendorf, B. Favre, R. Grishman, D. Hakkani-
Tur, M. Harper, D. Hillard, J. Hirschberg, Heng
Ji, J.G. Kahn, Yang Liu, S. Maskey, E. Matusov,
H. Ney, A. Rosenberg, E. Shriberg, Wen Wang, and
C. Woofers. 2008. Speech segmentation and spoken
document processing. Signal Processing Magazine,
IEEE, 25(3):59?69, May.
M. Roll, J. Frid, and M. Horne. 2007. Measuring syntac-
tic complexity in spontaneous spoken Swedish. Lan-
guage and Speech, 50(2):227.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Auto-
matic measurement of syntactic development in child
language. In Proc. of ACL, volume 100.
E. Shriberg and A. Stolcke. 2004. Direct modeling of
prosody: An overview of applications in automatic
speech processing. In Proceedings of the International
Conference on Speech Prosody.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cation, 32(1-2):127?154.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
L. Zhang. 2005. Maximum Entropy Model-
ing Toolkit for Python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html.
45
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?160,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Non-scorable Response Detection for Automated Speaking Proficiency
Assessment
Su-Youn Yoon, Keelan Evanini, Klaus Zechner
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{syoon,kevanini,kzechner}@ets.org
Abstract
We present a method that filters out non-
scorable (NS) responses, such as responses
with a technical difficulty, in an automated
speaking proficiency assessment system. The
assessment system described in this study first
filters out the non-scorable responses and then
predicts a proficiency score using a scoring
model for the remaining responses.
The data were collected from non-native
speakers in two different countries, using two
different item types in the proficiency assess-
ment: items that elicit spontaneous speech and
items that elicit recited speech. Since the pro-
portion of NS responses and the features avail-
able to the model differ according to the item
type, an item type specific model was trained
for each item type. The accuracy of the mod-
els ranged between 75% and 79% in spon-
taneous speech items and between 95% and
97% in recited speech items.
Two different groups of features, signal pro-
cessing based features and automatic speech
recognition (ASR) based features, were im-
plemented. The ASR based models achieved
higher accuracy than the non-ASR based mod-
els.
1 Introduction
We developed a method that filters out non-scorable
(NS) responses as a supplementary module to an
automated speech proficiency assessment system.
In this study, the method was developed for a
telephony-based assessment of English proficiency
for non-native speakers. The examinees? responses
were collected from several different environmen-
tal conditions, and many of the utterances contain
background noise from diverse sources. In ad-
dition to the presence of noise, many responses
have other sub-optimal characteristics. For exam-
ple, some responses contain uncooperative behav-
ior from the speakers, such as non-English speech,
whispered speech, and non-responses. These types
of responses make it difficult to provide a valid as-
sessment of a speaker?s English proficiency. There-
fore, in order to address the diverse types of causes
for these problematic responses, we used a two step
approach: first, these problematic responses were
filtered out by a ?filtering model,? and only the re-
maining responses were scored using the automated
scoring model.
The overall architecture of our method, includ-
ing the automated speech proficiency scoring sys-
tem, is as follows: for a given spoken response,
the system performs speech recognition, yielding a
word hypothesis and time stamps. In addition to
word recognition, the system computes pitch and
power to generate prosodic features; the system cal-
culates descriptive statistics such as the mean and
standard deviation of pitch and power at both the
word level and response level. Given the word hy-
potheses and pitch/power features, it derives features
for automated proficiency scoring. Next, the non-
ASR based features are calculated separately using
signal processing techniques. Finally, given both
sets of features, the filtering model identifies NS re-
sponses.
This paper will proceed as follows: we will re-
view previous studies (Section 2), present the data
152
(Section 3), and then describe the structure of the
filtering model (Section 4). Next, the results will
be presented (Section 5), followed by a discussion
(Section 6), and we will conclude with a summary
of the importance of the findings (Section 7).
2 Previous Work
Higgins et al (2011) developed a ?filtering model?
that is conceptually similar to the one in this pa-
per. The model was trained and tested on a corpus
containing responses from non-native speakers to an
English proficiency assessment. This system used
a regression model based on four features which
were originally designed for automated speech pro-
ficiency scoring: the number of distinct words in the
speech recognition output, the average speech rec-
ognizer confidence score, the average power of the
speech signal, and the mean absolute deviation of
the speech signal power. This model was able to
identify responses which were also identified as NS
responses by human raters with an approximately
98% accuracy when a false positive rate (the propor-
tion of responses without technical difficulties that
were incorrectly flagged as problematic) was lower
than 1%.
Although there are few other studies which are di-
rectly related to the task of filtering out non-scorable
responses in the domain of automated speech profi-
ciency assessment, several signal processing studies
are related to this work. Traditionally, the Signal to
Noise Ratio (SNR) has been used to detect speech
with a large amount of background noise. This
method measures the ratio between the total energy
of the speech signal and the total energy of the noise;
if the SNR is low, then the speech contains loud
background noise. A low SNR results in lower in-
telligibility and increases the difficulty for both hu-
man and automated scoring. Furthermore, spectral
characteristics can be also applied to detect speech
with loud background noise, since noise has differ-
ent spectral characteristics than speech (noise tends
to have no or few peaks in the spectral domain).
If a response contains loud background noise, then
the spectral characteristics of the speech may be ob-
scured by noise and it may have similar character-
istics with the noise. These differences in spectral
characteristics have been used in audio information
retrieval Lu and Hankinson (1998).
Secondly, responses without valid speech can be
identified using Voice Activity Detection (VAD).
VAD is a technique which distinguishes human
speech from non-speech. When speech is clean,
VAD can be calculated by simply computing the
zero-crossing rate which signals the existence of
cyclic waves such as vowels. However, if the re-
sponse also contains loud background noises, more
sophisticated methods are required. In order to re-
move the influence of noise, Chang and Kim (2003),
Chang et al (2006), Shin et al (2005) and Sohn et
al. (1999) estimated the characteristics of the noise
spectrum and the distribution of noise, and compen-
sated for them when speech is identified. The perfor-
mance of these systems is heavily-influenced by the
accuracy of estimating characteristics of the back-
ground noise.
In this study, we used a set of ASR based fea-
tures and non-ASR based features. ASR based fea-
tures were similar to the ones used by Zechner et al
(2009). In addition to the features based on ASR
hypotheses, the ASR based feature set contained ba-
sic pitch and power related features since the ASR
system in this study also produced pitch and power
measurements in order to generate prosodic features.
The non-ASR based features were comprised of four
groups of features based on signal processing tech-
niques such as SNR, VAD, and pitch and power.
Features related to pitch and power were included in
both the ASR based features and the non-ASR based
features. Since the non-ASR based features were
originally implemented as an independent module
from the ASR-based system (it was implemented for
the case where the appropriate recognizer is unavail-
able), there is some degree of overlap between the
two feature sets.
3 Data
The data for this experiment were drawn from a pro-
totype of a telephony-based English language as-
sessment. Non-native speakers of English each re-
sponded to 40 test items designed to evaluate their
level of English proficiency. The test was composed
of items that elicited both spontaneous speech (here-
after SS) and recited speech (hereafter RS). In this
study, 8 items (four SS and four RS) were used for
153
each speaker.
Participants used either a cell phone or a land
line to complete the assessment, and the participants
were compensated for their time. The motivation
level of the participants was thus lower than in the
case of an actual high stakes assessment, where a
participant?s performance could have a substantial
impact on their future. In addition, the data collec-
tion procedure was less controlled than in an op-
erational testing environment; for example, some
recordings exhibited higher levels of ambient noise
than others. These two facts led to the quality of
some of the responses being lower than would be
expected in an operational assessment.
The data for this study were collected from partic-
ipants in two countries: India and China. For India,
4900 responses from 638 speakers were collected.
For China, 5565 responses from 702 speakers were
collected (some of the participants did not provide
responses to all 8 test items). Each response is ap-
proximately 45 sec in duration.
After the data was collected, all of the responses
were given scores on a three-point scale by trained
raters. The raters also labeled responses as ?non-
scorable? (NS), when appropriate. NS responses are
ones that could not be given a score according to the
rubrics of the three-point scale. These were due to
either a technical difficulty obscuring the content of
the response or an inappropriate response from the
participant.
The proportion of NS responses differs markedly
between the two countries. 852 of the responses in
the India data set (17% of the total) were labeled as
NS, compared to 1548 responses (28%) in the China
data set.
Table 1 provides the different types of NS re-
sponses that were annotated by the raters, along with
the relative frequency of each NS category com-
pared to the others.
Excluding the category ?Other?, background
noise, non-responses, and unrelated topic were the
most frequent types of NS response for both data
sets. However, the relative proportions of each type
differed somewhat between the two countries. For
example, the most frequent NS type in India was
background noise; 33% of NS responses were of this
type, 1.7 times higher than in China.
The proportion of unrelated topic responses was
NS Type India (%) China (%)
Background noise 33.2 19.6
Other 25.0 15.4
Unrelated response 18.9 40.1
Non-response 10.6 8.8
Non-English speech 4.9 6.4
Too soft 2.8 1.0
Background speech 2.0 1.9
Missing samples 1.5 4.0
Too loud 0.8 0.1
Cheating 0.3 2.7
Table 1: Different types of NS responses and their relative
frequency, in % of all NS for each country (ranked by
frequency of occurrence in India)
Data Partition
India China
# of re-
sponses
NS
(%)
# of re-
sponses
NS
(%)
SS-train 1114 31.6 1382 32.2
SS-eval 1271 27.5 1391 33.8
RS-train 1253 8.0 1392 22.4
RS-eval 1275 4.8 1400 22.9
Table 2: Item-type specific training and evaluation data
also high in both countries, but it was much higher
in the China data set: it was 19% in the responses
from India and 40% for China (more than twice as
high as in India). All responses which were not di-
rectly related to the prompt fell into this category.
For SS items, the majority included responses about
a different topic. For RS items, responses in which
the speakers read different prompts were classified
into this category.
The responses were divided into training and test-
ing for NS response detection. Due to the significant
difference in the proportion of NS responses and rel-
ative frequencies of NS types in the two data sets, fil-
tering models were trained separately for each coun-
try. In addition, since the proportions of NS re-
sponses and the available features varied according
to the item type, training and testing data were fur-
ther classified by item types. The proportions of NS
responses and the sizes of the partitions, along with
the percent of NS responses in each item type, are
shown in Table 2.
154
The partitions for testing the filtering model were
selected to maximize the number of speakers with
complete sets of responses; however, this constraint
was not able to be met for the training partitions in
the India data set (due to insufficient data). This ex-
plains the lower proportion of NS responses in the
India test partitions, since speakers with complete
sets of responses were less likely to provide bad re-
sponses. As Table 2 shows, NS responses were more
frequent among SS items than RS items: the pro-
portion of NS responses in SS items was four times
higher than in RS items in India and 1.5 times in
China.
4 Method
4.1 Overview
In this study, two different sets of features were used
in the model training process; ASR-based features
and non-ASR based features. For each item-type,
an item-type-specific filtering model was developed
using these two sets of features.
4.2 Feature generation
4.2.1 ASR based features
For this feature set, we used the features from
an automated speech proficiency scoring system.
This scoring system used an ASR engine containing
word-internal triphone acoustic models and item-
type-specific language models. Separate acoustic
models were trained for the data sets from the two
countries. The acoustic training data for the two
models consisted of 45.5 hours of speech from In-
dia and 123.1 hours of speech from China. In addi-
tion, separate language models were trained for the
SS and RS items for each country; for the RS items,
the language models also incorporated the texts of
the prompts.
A total of 61 features were available. Among
these features, many features were conceptually
similar but based on different normalization meth-
ods. These features showed a strong intercorrela-
tion. For this study, 30 features were selected and
classified into four groups according to their char-
acteristics: basic features, fluency features, ASR-
confidence features, and Word Error Rate (WER)
features.
The basic features are related to power and pitch,
and they capture the overall distribution of pitch and
power values in a speaker?s response using mean and
variance calculations. These features are relevant
since NS responses may have an abnormal distribu-
tion in energy. For instance, non-responses contain
very low energy. In order to detect these abnormal-
ities in speech signal, pitch and power related fea-
tures were calculated.
The fluency features measure the length of a re-
sponse in terms of duration and number of words.
In addition, this group contains features related to
speaking rate and silences, such as mean duration
and number of silence. In particular, these features
are effective in identifying Non-responses which
contain zero or only a few words.
The ASR-confidence group contains features pre-
dicting the performance of the speech recognizer.
Low speech recognition accuracy may be indicated
by low confidence scores.
Finally, the WER group provides features esti-
mating the similarity between the prompts and the
recognition output. In addition to the conventional
word error rate (WER), term error rate (TER) was
also implemented for the filtering model. TER is
a metric commonly used in spoken information re-
trieval, and it only accounts for errors in content
words. This measure may be more effective in iden-
tifying NS responses than conventional WER; for in-
stance, the overlap in function words between off-
topic responses and prompts can be correctly ig-
nored. TER was calculated according to the follow-
ing formula:
dif(Wc) =
{
0 ifCref (Wc) < Chyp(Wc)
Cref (Wc)? Chyp(Wc) otherwise
TER =
?
c?WC
dif(Wc)
?
c?WC
Cref (Wc)
(1)
where Cref (Wc) is the number of occurrences of
the word Wc in reference, Chyp(Wc) is the number
of occurrences of the word Wc in hypothesis, and
WC is the set of content words in reference.
Formula 1 differs from the conventional method
155
Group List of features
Basic mean/standard deviation/minimum/maximum of power, difference between maxi-
mum and minimum in power, mean/standard deviation/minimum/maximum of pitch,
difference between maximum and minimum in pitch
Fluency duration of whole speech part, number of words, speaking rate (word per sec),
mean/standard deviation of silence duration, number of silences, silences per sec and
silences per word
ASR score mean of confidence score, normalized Acoustic Model score by word length, normal-
ized Language Model score by number of words
Word Error Rate the word accuracy between prompt and ASR word hypothesis, correct words per
minute, term error rate
Table 3: List of ASR based features
of calculating TER in two ways. Firstly, content
words which occurred only in the word hypothesis
are ignored in the formula. Secondly, if a word oc-
curred in the word hypothesis more frequently than
in the reference, the difference is ignored. These
modifications were made to address characteristics
of the responses in the data. On the one hand, speak-
ers occasionally inserted a few words such as ?too
difficult? at the end of a response. In addition, a few
speakers repeated words contained in the prompt
multiple times. The two modifications to TER ad-
dress both of these issues.
All features from the four groups are summarized
in Table 3.
4.2.2 Non-ASR based features
A total of 12 features from four different groups
were implemented using non-ASR based methods
such as VAD and SNR. These features are listed in
Table 4.
Feature Category Feature
VAD proportion of voiced
frames in response, num-
ber and total duration of
voiced regions
Syllable number of syllables
Amplitude maximum, minimum,
mean, standard deviation
SNR SNR, speech peak
Table 4: List of non-ASR based features
VAD related features were implemented using the
ESPS speech analysis program. For every 10 mil-
lisecond interval, the voice frame detector deter-
mined whether the interval was voiced or not. Three
features were implemented using this voiced interval
information: the number of voiced intervals, ratio of
voiced intervals in the entire response, and the total
duration of voiced intervals.
In addition, the number of syllables was estimated
based on the flow of energy. The energy of the syl-
lable tends to reach its peak in the nucleus and the
dip in the boundaries. By counting the number of
such fluctuations in energy measurements, the num-
ber of syllables can be estimated. The Praat script
from De Jong and Wempe (2009) was used for this
purpose.
In order to detect the abnormalities in energy, am-
plitude based features were calculated. These fea-
tures were similar to the basic features in ASR based
features.
Finally, if a response contains loud background
noise, the ratio of speech to noise is low. SNR, the
mean noise level, and the peak speech level were
computed using the NIST audio quality assurance
package (NIST, 2009).
The VAD and syllable feature groups were de-
signed to estimate the number of syllables, the pro-
portion of speech to non-speech, and the total dura-
tion of speech intervals. These features were similar
to the number of words and duration of speech fea-
tures in the ASR-based feature set. Despite the con-
ceptual similarity, these features were implemented
since the two types of features were calculated us-
ing different characteristics of the spoken response.
156
The VAD and syllable features are based on the flow
of energy and the zero crossing rate and the ASR-
based features are based on the speech recognition.
In particular, the speech recognizer tends to gener-
ate word hypotheses even for responses that contain
no speech input, but VAD does not have such a ten-
dency. Due to this difference, VAD based features
may be more robust in the responses with no valid
speech.
4.3 Model building
For each response, both ASR features and non-ASR
features were calculated. In contrast to non-ASR
features, which were available for all responses,
ASR features (except the Basic group) were un-
available for some responses, namely, responses for
which the ASR system did not generate any word
hypotheses because no tokens received scores above
the rejection threshold. This causes a missing value
problem; about 7% of the responses did not have a
complete set of attributes.
Missing values are a common problem in machine
learning. One of the popular approaches is to replace
a missing value with a unique value such as the at-
tribute?s mean. Ding and Simonoff (2008) proposed
a method that replaces a missing value with an arbi-
trary unique value. This method is preferable when
missing of a value depends on the target value and
this relationship holds in both training and test data.
In this study, the missing values were replaced
with unique values due to the relationship between
the missing values and the target label; if the speech
recognizer did not produce any word hypotheses, the
response was highly likely to be a NS response. 63%
of the responses where the speech recognizer failed
to generate word hypotheses were NS responses.
Since all ASR-based features were continuous val-
ues, we used two real values: 0.0 for fluency features
and ASR features and 100.0 for word error rate fea-
tures. The fluency features and ASR features tend to
be 0.0 while the word error rate features tend to be
100.0 when the responses are NS responses.
A total of 42 features were used in the model
building. The only exception was WER; since WER
features were only available for the model based
on recited speech, they were calculated only for RS
items. Decision tree models were trained using the
J48 algorithm (WEKA implementation of C4.5) of
WEKA machine learning toolkit (Hall et al, 2009).
5 Results
For each item-type, three models were built to in-
vestigate the impact of each feature group: a model
using non-ASR features, a model using ASR fea-
tures, and a model using both features (the ?Com-
bined? model). Tables 5 and 6 present the accuracy
of the SS models and Tables 7 and 8 present the ac-
curacy of the RS models. In all tables, the base-
line was calculated using majority voting, and rep-
resented a system in which no responses were clas-
sified as NS; since the majority class was scorable,
the baseline using the majority voting did not predict
any response as non-scorable response. Therefore,
precision, recall, F-score are all 0 in this case.
Model Acc. Pre. Rec. F-score
Baseline 72.5 0 0 0
Non-ASR 77.0 0.645 0.364 0.465
ASR 79.0 0.683 0.444 0.538
Combined 78.6 0.657 0.461 0.542
Table 5: Performance of the SS model in India
Model Acc. Pre. Rec. F-score
Baseline 66.2 0 0 0
Non-ASR 68.9 0.601 0.240 0.343
ASR 72.9 0.718 0.326 0.448
Combined 72.9 0.720 0.323 0.446
Table 6: Performance of the SS model in China
Model Acc. Pre. Rec. F-score
Baseline 94.8 0 0 0
Non-ASR 95.7 0.684 0.210 0.321
ASR 97.2 0.882 0.484 0.625
Combined 96.8 0.769 0.484 0.594
Table 7: Performance of the RS model in India
In both item-types, the models using ASR-based
features achieved the best performance. The SS
model achieved 79% accuracy in India and 73% ac-
curacy in China, representing improvements of ap-
proximately 7% over the baseline. In both data sets,
the RS model achieved high accuracies: 97% accu-
racy in India and 96% accuracy in China. In India,
157
Model Acc. Pre. Rec. F-score
Baseline 77.1 0 0 0
Non-ASR 78.3 0.555 0.268 0.361
ASR 95.6 0.942 0.860 0.899
Combined 95.1 0.912 0.872 0.892
Table 8: Performance of the RS model in China
this represents a 2.4% improvement over the base-
line. Although the absolute value of this error re-
duction is not very large, the relative error reduc-
tion is 46%. In China, the improvement was more
salient; there was 18% improvement over baseline,
corresponding to a relative error reduction of 78%.
Additional experiments were conducted to deter-
mine the robustness of the filtering models to evalu-
ation data from a country not included in the train-
ing data. The evaluation sets from both item types
(SS and RS) in both countries (India and China)
were processed using three different models: 1) a
model trained using the ASR-based features for the
responses from the same country (the ?Same? con-
dition, whose results are identical to the ?ASR? re-
sults in Tables 5 - 8), 2) a model trained using the
ASR-based features for the responses from the other
country (the ?Different? condition), and 3) a model
trained using the ASR-based features for the re-
sponses from both countries (the ?Both? condition).
Table 9 presents the accuracy results for these four
sets of experiments.
Model
India China
SS RS SS RS
Same 79.0 97.2 72.9 95.6
Different 80.1 95.4 73.5 93.8
Both 80.0 96.5 74.0 95.9
Table 9: Accuracy results using training and evaluation
data from different countries
These results show that the models are quite ro-
bust to evaluation data from a different country. In
all cases, there is at most a small decline in perfor-
mance when training data from the other country is
used (in the case of the SS responses, there is even a
slight increase in performance). Table 9 also shows
that the RS models performed worse in the Different
Country condition (compared to the Same Country
condition) than the SS models. This difference is
likely due to the difference in the number of NS re-
sponses among the RS data in the two countries (as
shown in Table 2). However, the decline is still rel-
atively small, suggesting that it would be reasonable
to extend the filtering models to responses from ad-
ditional countries that were not seen in the training
data.
6 Discussion
Approximately 40 features were available for the
model building, but not all features had a signifi-
cant impact on the detection of NS responses. For
each item-type, the importance of features were fur-
ther investigated using a logistic regression analysis.
The training data of India and China were combined,
and a stepwise logistic regression analysis was per-
formed using the SPSS statistical analysis program.
For each item-type, the top 3 features are pre-
sented in Table 10; the features are presented in the
same order selected in the models.
Model RS SS
ASR TER,
speaking
rate, s.d. of
pitch
mean of confi-
dence scores,
speaking rate,
s.d. of power
Non-ASR number of
syllables,
number
and du-
ration of
voiced
regions
number of sylla-
bles, s.d. and
mean of ampli-
tude
Combined TER,
speaking
rate, s.s.dd.
pitch
mean of confi-
dence scores,
speaking rate,
number of
voiced regions
Table 10: Top 3 features in stepwise logistic regression
model
For the RS items, TER was the best feature and it
was the top feature for both the ASR feature based
model and the combined model. The top 3 features
in the combined model were the same as the ASR
feature based model, and non-ASR features were not
158
selected. In non-ASR based features, the number of
syllables was the best feature, followed by the VAD
based features.
For the SS items, the top 2 features were the same
in both the ASR feature based model and the com-
bined model. The combined model selected one
non-ASR based feature, namely, a VAD based fea-
ture. As with the RS items, the number of syllables
was the best feature, followed by the energy related
feature.
These results show the importance of WER fea-
tures. Most of the current features are designed for
signal level abnormalities such as responses with
large background noise or non-responses. For in-
stance, fluency features and VAD features are effec-
tive for non-response detection, since they can deter-
mine whether the responses contain valid speech or
not. SNR and pitch/power related features are use-
ful for identifying responses with large background
noise. However, no features except the WER group
can identify content-level abnormalities such as un-
related topic and non-English responses. The high
proportion of these two types of responses (24%
in India and 46% in China) may be the major ex-
planation for the lower accuracy of the model for
SS responses than for RS responses. In the future,
content-related features should also be developed for
spontaneous speech.
The features selected the first time in the logis-
tic model differed according to item-types. The re-
sults support the item-type-specific model approach
adopted in this paper; item-type-specific models can
assign strong weights to the item-type-specific fea-
tures that are most important.
As shown in Tables 5 - 8, the combination of non-
ASR and ASR features could not achieve any fur-
ther improvement over the model consisting only of
ASR based features. However, in all cases, the non-
ASR based model did lead to some improvement
over the baseline. The magnitude of this improve-
ment was greater in SS items than RS items; in par-
ticular, it was greatest among the SS items in the
India data set. This difference may be due to the dif-
ferent distributions of the NS types among the data
sets. The non-ASR based features can cover only
limited types of NS responses such as non-responses
and responses with background noise, and the pro-
portion of these types is much higher among the SS
responses from India.
In addition, in RS items, the poor performance of
the combined model may be related to the high per-
formance of TER. The stepwise regression analysis
showed that the combined model did not select any
of non-ASR based features.
7 Conclusion
In this study, filtering models were implemented as a
supplementary module to an automated proficiency
scoring system. Due to the difference in the avail-
able features and proportion of NS responses, item-
type specific models were trained.
The item-types heavily influenced the overall
characteristics of the filtering models. First, the pro-
portion of NS responses was significantly different
according to item-type; it was much higher in spon-
taneous speech items than recited speech items. Sec-
ondly, the word error rate feature group was only
available for recited speech. Although the word er-
ror rate feature group contained three features, they
improved the performance of the filtering model sig-
nificantly.
ASR feature based models outperformed non-
ASR feature based models, but non-ASR based fea-
tures may be useful for new tests. Finally, experi-
ments demonstrated that the country-specific mod-
els using the ASR-based features are relatively ro-
bust to responses from a different country. This re-
sult suggests that this approach can generalize well
to speakers from different countries.
In this study, large numbers of features (42 for RS
items and 39 for SS items) were used in the model
training, but some features were conceptually simi-
lar and not all of them were significantly important;
the logistic regression analsysis using traning data
showed that there was no significant improvement
after selecting 5 features for RS items and 13 fea-
tures for SS items. Use of non-significant features
in the model training may result in the overfitting
problems. In future research, the features will be
classified into subgroups based on their conceptual
similarities; groups of features with high intercorre-
lations will be reduced to include only the best per-
forming feature in each group. Thus, based on care-
ful pre-selection procedures, only high performing
features will be selected, and the model will be re-
159
trained.
In addition, many different types of NS responses
were lumped into one big category (NS); this may
increase the confusion between scorable and non-
scorable responses and decrease the model?s perfor-
mance. Some of NS types have very different char-
acteristics compared to other NS types and this fact
caused critical differences in the feature values. For
instance, non-responses contained zero or close to
zero words, whereas non-English responses and off-
topic responses typically had a word count similar to
scorable responses. This difference may reduce the
effectiveness of this feature. In order to avoid this
type of problem, we will classify NS types into small
numbers of subgroups and build a seperate model for
each subgroup.
References
Joon-Hyuk Chang and Nam Soo Kim. 2003. Voice ac-
tivity detection based on complex Laplacian model.
Electronics Letters, 39(7):632?634.
Joon-Hyuk Chang, Nam Soo Kim, and Sanjit K. Mitra.
2006. Voice activity detection based on multiple sta-
tistical models. IEEE Transactions on Signal Process-
ing, 54(6):1965?1976.
Nivja H. De Jong and Ton Wempe. 2009. Praat script
to detect syllable nuclei and measure speech rate au-
tomatically. Behavior research methods, 41(2):385?
390.
Yufeng Ding and Jeffrey S. Simonoff. 2008. An investi-
gation of missing data methods for classification trees.
Statistics Working Papers Series.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
In SIGKDD Explorations, volume 11.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech and Language, 25:282?
306, April.
Guojun Lu and Templar Hankinson. 1998. A technique
towards automatic audio classification and retrieval.
In Proceedings of the 4th International Conference on
Signal Processing, volume 2, pages 1142?1145.
NIST. 2009. The NIST SPeech Quality Assurance
(SPQA) Package Version 2.3. from http://www.
nist.gov/speech/tools/index.htm.
Jong Won Shin, Hyuk Jin Kwon, Suk Ho Jin, and
Nam Soo Kim. 2005. Voice activity detection based
on generalized gamma distribution. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 781?784.
Jongseo Sohn, Nam Soo Kim, and Wonyong Sung.
1999. A statistical model-based voice activity detec-
tion. IEEE Signal Processing Letter, 6(1):1?3.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
160
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 161?169,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Non-English Response Detection Method for Automated Proficiency Scoring
System
Su-Youn Yoon and Derrick Higgins
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{syoon,dhiggins}@ets.org
Abstract
This paper presents a method for identifying
non-English speech, with the aim of support-
ing an automated speech proficiency scoring
system for non-native speakers.
The method uses a popular technique from the
language identification domain, a single phone
recognizer followed by multiple language-
dependent language models. This method
determines the language of a speech sample
based on the phonotactic differences among
languages.
The method is intended for use with non-
native English speakers. Therefore, the
method must be able to distinguish non-
English responses from non-native speakers?
English responses. This makes the task more
challenging, as the frequent pronunciation er-
rors of non-native speakers may weaken the
phonetic and phonotactic distinction between
English responses and non-English responses.
In order to address this issue, the speaking
rate measure was used to complement the
language identification based features in the
model.
The accuracy of the method was 98%, and
there was 45% relative error reduction over
a system based on the conventional language
identification technique. The model using
both feature sets furthermore demonstrated an
improvement in accuracy for speakers at all
English proficiency levels.
1 Introduction
We developed a non-English response identifica-
tion method as a supplementary module for the au-
tomated speech proficiency scoring of non-native
speakers. The method can identify speech samples
of test takers who try to game the system by speak-
ing in their native languages. For the items that
elicited spontaneous speech, fluency features such
as speaking rate have been one of the most impor-
tant features in the automated scoring. By speak-
ing in their native languages, speakers can generate
fluent speech, and the automated proficiency scor-
ing system may assign a high score. This problem
has been rarely recognized, and none of research has
focused on it as to the authors? knowledge. In or-
der to address this issue, the automated proficiency
scoring system in this study first filters out the re-
sponses in non-English languages, and for the re-
maining responses, it predicts the proficiency score
using a scoring model.
Non-English detection is strongly related to lan-
guage identification(Lamel and Gauvain, 1993;
Zissman, 1996; Li et al, 2007); language identifi-
cation is the process of determining which language
a spoken response is in, while non-English detec-
tion makes a binary decision whether the spoken re-
sponse is in English or not. Due to the strong simi-
larity between the two tasks, the language identifica-
tion method was used here for non-English response
detection.
In contrast to previous research, the method de-
scribed here was intended for use with non-native
speakers, and the English responses for model train-
ing and evaluation were accordingly collected from
non-native speakers. Among other differences,
non-native speakers? speech tends to display non-
standard pronunciation characteristics which can
161
make the task of language identification more chal-
lenging. For instance, when native Korean speak-
ers speak English, they may replace some English
phonemes not in their language with their native
phones, and epenthesize vowels within consonant
clusters. Such processes tend to reduce the pho-
netic and phonotactic distinction between English
and other languages. The frequency of these pro-
nunciation errors is influenced by speakers? na-
tive language and proficiency level, with lower-
proficiency speakers likely to exhibit the greatest
degree of divergence from standard pronunciation.
Language identification method may not effectively
distinguish non-fluent speakers? English responses
from non-English responses. In order to address
these non-native speech characteristics, the model
described here includes the speaking rate feature,
which has been found to be an indicator of speak-
ing proficiency in previous research(Strik and Cuc-
chiarini, 1999; Zechner et al, 2009). Non-fluent
speakers? English responses can be distinguished
from non-English responses by slow speaking rate.
This paper will proceed as follows: we first re-
view previous studies in section 2, then describe the
data in section 3, and present the experiment in sec-
tion 4. The results and discussion are presented in
section 5, and the conclusions are presented in sec-
tion 6.
2 Previous Work
Many previous studies in language identification
focused on phonetic and phonotactic differences
among languages. The frequencies of phones and
phone sequences differ according to languages and
some phone sequences occur only in certain lan-
guages. The literature in language identification
captured this characteristic using the likelihood
score of speech recognizers, which signals the de-
gree of a match between the test sentences and
speech recognizer models. Both the language model
(hereafter, LM) and acoustic model (hereafter, AM)
of a phone recognizer are optimized for the acoustic
characteristics and the phoneme distribution of the
training data. If a spoken response is recognized us-
ing a recognizer trained on a different language, it
may result in a low likelihood score due to a mis-
match between the test sentences and the models.
Lamel and Gauvain (1993) trained multiple
language-dependent-phone-recognizers and se-
lected the language with the highest matching score
as the input language (hereafter, parallel PRLM).
For instance, if the test data contained English and
Hindi speech data, the English-phone-recognizer
and the Hindi-phone-recognizer were trained in-
dependently. In the test, the given speech samples
were recognized using two phone recognizers,
and the language that had a higher matching
score was selected. However, training multiple
phone recognizers was time-consuming and labor
intensive; therefore, Zissman (1996) proposed a
system using single-language phone recognition
followed by multiple language-dependent language
modeling (hereafter, PRLM). PRLM was able to
achieve comparable performance to parallel PRLM
for long speech (longer than 30 seconds), and in a
two-language situation, the error rate was between
5 and 7%.
Instead of language-dependent LM, Li et al
(2007) used vector space modeling (VSM). They
applied metrics frequently used in information re-
trieval. As with the PRLM method, the speech was
converted into phone sequences using the phone rec-
ognizer, and cooccurrence statistics such as term fre-
quency (TF) and inverse document frequency (IDF)
were calculated. The method outperformed the
PRLM approach for long speech.
These methods can be challenging and time-
consuming to implement, as they require implemen-
tation of methods beyond those typically available
in a standard word-based recognition system. In
particular, the application of the phone recognizer
increases the processing time substantially. Be-
cause of this problem, Lim et al (2004) presented a
method based on the features that were readily avail-
able for speech recognizers: a confidence score and
the cross-entropy of the LM. The confidence scoring
method measured the acoustic match between the
word hypotheses and the real sound, while the cross-
entropy measured how well a sentence matched a
given language model. If the test sentence was rec-
ognized by the speech recognizer in a different lan-
guage, the phonetic and lexical mismatches between
two languages resulted in a low confidence score and
a high cross-entropy. Using this methodology, Lim
et al (2004) achieved 99.8% accuracy in their three-
162
way classification task.
The current study can be distinguished from the
previous studies in the following points. First of all,
special features were implemented to model non-
native speech since the method was developed for
non-native speech. In our study, the data contained
non-native speakers? English speech, characterized
by inaccurate pronunciation. It resulted in a mis-
match between the speech-recognizer models and
test sentences, even for utterances in English. In par-
ticular, the mismatch was more salient in non-fluent
speakers? speech, which comprised a high propor-
tion of our data. In order to address this issue, speak-
ing rate, which has achieved good performance in
the estimation of non-native speakers? speaking pro-
ficiency (Strik and Cucchiarini, 1999; Zechner et
al., 2009), was implemented as an additional feature.
Secondly, in contrast to previous studies that deter-
mined which language the speech was in, we made
a binary decision whether the speech was in English
or not. Finally, the method was developed as part of
a language assessment system.
3 Data
The OGI Multi-language corpus (Muthusamy et al,
1992), a standard language identification develop-
ment data set, was used in the training and evalua-
tion of the system. It contains a total of 1,957 calls
from speakers of 10 different languages (English,
Farsi, French, German, Japanese, Korean, Mandarin
Chinese, Spanish, Tamil, and Vietnamese). The cor-
pus was composed of short speech and long speech;
the short files contained approximately 10 seconds
speech, while the long files contained speech ranged
from 30 seconds to 50 seconds.
The method described here was implemented to
distinguish non-English responses from non-native
speakers? English responses. Therefore, the English
data used to train and evaluate the model for non-
English response detection was collected from non-
native speakers. In particular, responses to the En-
glish Practice Test (EPT) were used. The EPT is
an online practice test which allows students to gain
familiarity with the format of a high-stakes test of
English proficiency and receive immediate feedback
on their test responses based on automated scor-
ing methods. The speaking section of the EPT as-
sessment consists of 6 items in which speakers are
prompted to provide open-ended responses of 45-60
seconds in length. The scoring scale of each item is
discrete from 1 to 4, where 4 indicates high speaking
proficiency and 1 low proficiency.
The non-English detection task is composed of
two major components: training of PRLM, and
training of the classifier which makes a binary de-
cision about whether a speech sample is in the En-
glish language, given PRLM-based features and the
speaking rate.
The OGI corpus was used in training of both
PRLM and the classifier; a total of 9,033 short files
from the OGI corpus were used in PRLM training,
and 158 long files were used in classifier training.
(The small number of long files in the OGI corpus
limited the number of samples comparable in length
to our English-language data described below, so
that only these 158 OGI samples could be used in
classifier training and evaluation.) For English, only
short samples were selected for use in this experi-
ment.
In addition, a total of 3,021 EPT responses were
used in classifier training. As the English profi-
ciency levels of speakers may have an influence
on the accuracy of non-English response detection,
the EPT responses were selected to include simi-
lar numbers of responses for each score level. Re-
sponses were classified into four groups according
to their proficiency scores and 1000 responses were
randomly selected from each group. For score 1
and 4, where the number of available responses was
smaller than 1000, all available responses were se-
lected. Ultimately, 156 responses for score 1, 1000
responses for score 2 and score 3, and 865 responses
for score 4 were selected.
The resultant training and evaluation data sets are
summarized in Table 1.
Due to the lack of non-Engilsh responses in EPT
data, 158 non-English utterances in OGI data were
used in both training and evaluation of non-English
detection. EPT responses were collected from many
different countries, and speakers with 75 different
native languages were participated in the data collec-
tion. Due to the large variations, many of their native
languages were not covered by OGI data. However,
all 9 languages in OGI data were in top 15 L1 lan-
guages and covered approximately 60% of speakers?
163
Partition name Purpose Number of
English files
Number of non-
English files
PRLM-train Training of Language-
dependent LM
1,716 (OGI) 7,317 (OGI)
EN-detection Training and evaluation of non-
English detection classifier
3,021 (EPT) 158 (OGI)
Table 1: Data partition
native languages.
4 Experiment
4.1 Overview
Due to the efficiency in processing time and im-
plementation, a PRLM was implemented instead of
a parallel PRLM. However, the difference between
PRLM and parallel PRLM in this study may not be
significant since PRLM has been shown to be com-
parable to parallel PRLM for test samples longer
than 30 seconds, and the duration of test instances in
this study was longer than 30 seconds. In addition
to PRLM, speaking rate was calculated as a feature.
4.2 PRLM based features
The PRLM based method in this study is composed
of three parts: training of a phone recognizer, train-
ing of language-dependent LMs, and generation of
PRLM-based features. In contrast to the conven-
tional language identification approach that only fo-
cused on identifying the language with the highest
matching score, 6 additional features were imple-
mented to capture the difference between English
model and other models.
Phone recognizer: An English triphone acoustic
model was trained on 30 hours of non-native English
speech (EPT data) using the HTK toolkit (Young et
al., 2002). The model contained 43 monophones
and 4,887 triphones. Due to the difference in the
sampling rate of EPT (11,025 Hz) and the OGI cor-
pus (8,000 Hz), the EPT data was down-sampled to
8,000 Hz and the acoustic model was trained using
the down-sampled data. In order to avoid the in-
fluence of English in phone hypothesis generation,
a triphone bigram language model with a uniform
probability distribution was used as the LM. (All
possible combinations of two triphones were gener-
ated and a uniform probability was assigned to each
combination.) The phone recognition accuracy rate
was 42.7% on the 94 held-out EPT test samples.
This phone recognizer was used in phone hypoth-
esis generation for all data; the same recognizer was
used for all languages.
Language-dependent LMs: For responses in the
PRLM-train partition, phone hypothesis was gener-
ated using the English recognizer. Instead of the
manual transcription, a language-dependent phone
bigram LM was trained using the phone hypothe-
sis. In order to avoid a data sparseness problem, tri-
phones were converted into monophones by remov-
ing left and right context phones, and a bigram LM
with closed vocabulary was trained. 10 language-
dependent bigram LMs, including one for English,
were trained.
PRLM based feature generation: For each re-
sponse in the EN-detection partition, phone hypoth-
esis was generated, and triphones were converted
into monophones. Given monophone hypothesis, an
LM score was calculated for each language using a
language-dependent LM. A total of 10 LM scores
were calculated.
Since the LM score increases as the number of
phones increases, the LM score was normalized by
the number of phones in each response, in order
to avoid the influence of hypothesis length. 7 fea-
tures were generated based on these normalized LM
scores:
? MaxLanguage: The language with the maxi-
mum LM score
? SecondLanguage: The language with the
second-largest LM score.
? MaxScore: Normalized LM score of the
MaxLanguage.
164
? MaxDifference: Difference between normal-
ized English LM score and MaxScore
? MaxRatio: Ratio between normalized English
LM score and MaxScore
? AverageDifference: Difference between nor-
malized English LM score and the average of
normalized LM scores for languages other than
English
? AverageRatio: Ratio between normalized En-
glish LM score and the average of normalized
LM scores for languages other than English
Among above 6 features, 4 features (MaxDiffer-
ence, MaxRatio, AverageDifference, and AverageR-
atio) were designed to measure the difference be-
tween matching of a test responses with English
model and it with the other models. These features
may be particularly effective when MaxLanguage of
the English response is not English; these values will
be close to 0 when the divergence due to non-native
characteristics result in only slightly better match
with other language than that with English.
4.3 Speaking rate calculation
The speaking rate was calculated as a feature rele-
vant to establishing speakers? proficiency level, as
established in previous research. Speaking rate was
calculated from the phone hypothesis as the number
of phones divided by the duration of responses (cf.
Strik and Cucchiarini (1999)).
4.4 Model building
For each response, both PRLM-based features and
speaking rate were calculated, and a decision tree
model was trained to predict binary values (0 for En-
glish and 1 for non-English) using the J48 algorithm
(WEKA implementation of C4.5) of the WEKA ma-
chine learning toolkit (Hall et al, 2009).
Due to the limited number of non-English re-
sponses in the EN-detection partition, three-fold
cross validation was performed during classifier
training and evaluation. The 3,179 responses were
partitioned into three sets to include approximately
same numbers of non-English responses and English
responses for each proficiency score group. Each
partition contained 52 ? 53 non-English responses
and 1007 English responses. In each fold, the de-
cision tree was trained using two of these partitions
and tested on the remaining one.
5 Evaluation
First, the accuracy of the PRLM method was eval-
uated based on multiple forced-choice experiments
with two alternatives using OGI data; in addition
to non-English responses in EN-detection partition,
English responses from the OGI data were used in
this experiment. For each response (in English and
one other language), phone hypothesis was gener-
ated and two normalized LM scores were calculated
using the English LM and the LM for the other lan-
guage. The MaxLanguage was hypothesized as the
source language of the speech. The same experiment
was performed for 9 combinations of English and
other languages. Each experiment was comprised
of 17 English utterances and 17 non-English utter-
ances1. The majority class baseline was thus 0.5.
The mean accuracy of the 9 experiments in this study
was 0.943, which is comparable to (1996)?s perfor-
mance: in his study, the best performing PRLM
exhibited an average accuracy of 0.950. This ini-
tial evaluation used the same data and feature as
Zissman (1996). (Of the seven PRLM-based fea-
tures listed above, only MaxLanguage was used in
(1996)?s study.)
Table 2 summarizes the evaluation results of the
non-English response detection experiments using
three-fold cross-validation within the EN-detection
partition. In order to investigate the impact of dif-
ferent types of features, the features were classi-
fied into four sets?MaxLanguage only, PRLM
(encompassing all PRLM features), SpeakingRate,
and all?and models were trained using each set.
The baseline using majority voting demonstrated an
accuracy of 0.95 by classifying all responses as En-
glish responses.
All models achieved improvements over baseline.
In particular, the model using all features achieved
a 66% relative error reduction over the baseline of
0.95. Furthermore, the all-features model outper-
formed the model based only on PRLM or speaking
1Due to the languages where the available responses were
only 17, the same 17 English responses were used in the all
experiment although 18 responses were available
165
Features Acc. Pre. Rec. F-
score
Base-
line
0.950 0.000 0.000 0.000
Max-
Language
0.969 0.943 0.411 0.572
PRLM 0.966 0.675 0.633 0.649
Speaking-
Rate
0.962 0.886 0.278 0.415
All 0.983 0.909 0.746 0.816
Table 2: Performance of non-English response detection
rate; the accuracy of the all-features model was ap-
proximately 1-2% higher than other models in abso-
lute value and represented approximately a 45-50%
relative error reduction over these models.
The PRLM-based model had higher overall accu-
racy than the speaking rate-based model, and the dif-
ference was even more salient by the F-score mea-
sure: the PRLM-based model achieved an F-score
approximately 24% higher than the speaking rate-
based model.
The model based on all PRLM features did not
achieve a higher accuracy than the model based on
only MaxLanguage. However, there was a clear im-
provement in F-score by using the additional fea-
tures. The PRLM-based model achieved an F-score
approximately 8% higher than the model based only
on MaxLanguage.
In order to investigate the influence of speakers?
proficiency on the accuracy of non-English detec-
tion, the responses in EN-detection were divided
into 4 groups according to proficiency score, and the
performance was calculated for each score group;
the performance of each score group was calcu-
lated using subset comprised of all non-English re-
sponses and English responses with the correspond-
ing scores.
A majority class baseline (classifying all re-
sponses as English) was again used. Table 3 sum-
marizes the results observed, by score level, for the
baseline model and for four different models used in
Table 2. Note that the baseline is lower in Table 3
than in Table 2, because the ratio of English to non-
English responses is lower for each of the subsets of
the EN-detection partitions used for the evaluations
Figure 1: Relationship between proficiency score and
MaxDifference
at a given score level.
For all score groups, the model using all features
achieved high accuracy. The model?s accuracy on all
data sets except for score group 1 was approximately
0.96 and the F-score approximately 0.85. The accu-
racy on score group 1 was 0.87, relatively lower than
other score groups. This is largely due to the smaller
number of English responses available at score level
1, and the consequent lower baseline on this data
set. However, the relative error reduction was much
larger; it was 74% for score group 1.
For all score groups, the PRLM-based mod-
els outperformed MaxLanguage based models and
speaking rate based models. Additional PRLM
features improved the performance over the mod-
els only based on MaxLanguage (conventional lan-
guage identification method). In addition, the com-
bination of both types of features resulted in further
improvement.
The consistent improvement of the model using
both PRLM and speaking rate features suggests a
compensatory relationship between these features.
In order to investigate this relationship in further de-
tail, two representative features, MaxDifference and
AverageDifference were selected, and boxplots were
created. Figure 1 and Figure 2 show the relationship
between proficiency score and PRLM features. In
these figures, the label ?NE? is used to indicate the
non-English group, while the labels 1, 2, 3, and 4
correspond to each score group.
Figure 1 shows that MaxDifference decreases as
166
Score Features Acc. Pre. Rec. F-score
1 Baseline 0.497 0.000 0.000 0.000
MaxLanguage 0.696 0.970 0.411 0.577
PRLM 0.792 0.936 0.633 0.752
SpeakingRate 0.636 1.000 0.278 0.432
All 0.869 0.992 0.746 0.851
2 Baseline 0.865 0.000 0.000 0.000
MaxLanguage 0.919 0.983 0.411 0.579
PRLM 0.930 0.811 0.633 0.709
SpeakingRate 0.901 1.000 0.278 0.432
All 0.962 0.971 0.746 0.843
3 Baseline 0.865 0.000 0.000 0.000
MaxLanguage 0.920 1.000 0.411 0.582
PRLM 0.939 0.903 0.633 0.738
SpeakingRate 0.901 0.983 0.278 0.430
All 0.963 0.976 0.746 0.845
4 Baseline 0.846 0.000 0.000 0.000
MaxLanguage 0.908 0.987 0.411 0.579
PRLM 0.936 0.934 0.633 0.752
SpeakingRate 0.882 0.896 0.278 0.417
All 0.955 0.956 0.746 0.837
Table 3: Performance of non-English detection according to speakers? proficiency level
Figure 2: Relationship between proficiency score and Av-
erageDifference
the speaker?s proficiency decreases, although the
feature displays a large variance. The feature mean
for non-English responses is lower than for score
groups 2, 3, and 4, but the distinction between
non-English and English becomes smaller as the
proficiency score decreases. The feature mean for
score group 1 is even lower than for non-English re-
sponses. This obscures the distinction between En-
glish responses and non-English responses at lower
score levels.
As Figure 2 shows, AverageDifference is rela-
tively stable across score levels, compared to MaxD-
ifference. Although the mean feature value de-
creases as the proficiency score decreases, the de-
crease is smaller than for MaxDifference. In addi-
tion, the mean feature values of the English groups
are consistently higher than those for non-English
responses.
Figure 3 shows the relationship between profi-
ciency score and speaking rate.
For the speaking rate feature, the distinction be-
tween non-English and English responses increases
as speakers? proficiency level decreases, as shown
167
Figure 3: Relationship between proficiency score and
SpeakingRate
in Figure 3. The speaking rate of non-English re-
sponses is the highest among all groups compared,
and the speaking rate decreases for English re-
sponses as the speaker?s proficiency score decreases.
Thus, the PRLM features tend to display better dis-
crimination between English and non-English re-
sponses at the higher end of the proficiency scale,
while the SpeakingRate feature provides better dis-
crimination at the lower end of the scale. By com-
bining both feature classes, we are able to produce
a model which outperforms both a PRLM-based
model and a model using speaking rate alone.
6 Conclusion
In this study, we presented a non-English response
detection method for non-native speakers? speech. A
decision tree model was trained using PRLM-based
features and speaking rate.
The method was intended for use as a supple-
mentary module of an automated speech proficiency
scoring system. The characteristics of non-native
English speech (frequent pronunciation errors) re-
duced the phonetic distinction between English re-
sponses and non-English responses, and correspond-
ingly, the differences between the feature values for
non-English and English speech decreased as well.
In order to address this issue, a speaking rate fea-
ture was added to the model. This feature was spe-
cialized for second language (L2) learners? speech,
as speaking rate has previously proved useful in es-
timating non-native speakers? speaking proficiency.
In contrast to PRLM-based features, the speaking
rate feature showed increasing discrimination be-
tween non-English and English speech samples as
speakers? proficiency level decreased. The com-
plementary relationship between PRLM-based fea-
tures and speaking rate led to an improvement in
the model when these features were combined. Im-
provements resulting from the combined feature set
extended across speakers at all proficiency levels
studied in the context of this paper.
The speaking rate becomes less effective if test
takers speak slowly in their native languages. How-
ever, the test takers are unlikely to use this strategy,
since it will result in a low score although they can
game the system.
Due to lack of non-English responses in EPT data,
non-English utterances were extracted from OGI
data. Since the features in this study were not di-
rectly related to acoustic scores, the acoustic dif-
ferences between EPT and OGI data may not give
significant impact on the results. However, in order
to avoid any influence by differences between cor-
pora, the non-English responses will be collected us-
ing EPT setup and the evaluation will be performed
using new data in future.
References
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
In SIGKDD Explorations, volume 11.
Lori F. Lamel and Jean-Luc Gauvain. 1993. Cross-
lingual experiments with phone recognition. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume 2,
pages 507?510.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A vec-
tor space modeling approach to spoken language iden-
tification. Audio, Speech and Language Processing,
15:271 ? 284.
Boon Pang Lim, Haizhou Li, and Yu Chen. 2004. Lan-
guage identification through large vocabulary continu-
ous speech recognition. In Proceedings of the 2004 In-
ternational Symposium on Chinese Spoken Language
Processing, pages 49 ? 52.
Yeshwant K. Muthusamy, Ronald A. Cole, and Beat-
rice T. Oshika. 1992. The OGI multi-language tele-
phone speech corpus. In Proceedings of the Inter-
168
national Conference on Spoken Language Processing,
pages 895?898.
Helmer Strik and Catia Cucchiarini. 1999. Automatic as-
sessment of second language learners? fluency. In Pro-
ceedings of the 14th International Congress of Pho-
netic Sciences, pages 759?762, San Francisco, USA.
Steve Young, Gunnar Evermann, Dan Kershaw, Gareth
Moore, Julian Odell, Dave Ollason, Dan Povey,
Valtcho Valtchev, and Phil Woodland. 2002. The HTK
Book (for HTK Version3.2). Microsoft Corporation
and Cambridge University Engineering Department.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883 ? 895.
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of tele-
phone speech. Speech and Audio Processing, 4:31 ?
44.
169
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 180?189,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Vocabulary Profile as a Measure of Vocabulary Sophistication
Su-Youn Yoon, Suma Bhat*, Klaus Zechner
Educational Testing Service, 660 Rosedale Road, Princeton, NJ, USA
{syoon,kzechner}@ets.org
* University of Illinois, Urbana-Champaign, IL, USA
sumapramod@gmail.com
Abstract
This study presents a method that assesses
ESL learners? vocabulary usage to improve
an automated scoring system of sponta-
neous speech responses by non-native English
speakers. Focusing on vocabulary sophistica-
tion, we estimate the difficulty of each word
in the vocabulary based on its frequency in
a reference corpus and assess the mean diffi-
culty level of the vocabulary usage across the
responses (vocabulary profile).
Three different classes of features were gen-
erated based on the words in a spoken re-
sponse: coverage-related, average word rank
and the average word frequency and the extent
to which they influence human-assigned lan-
guage proficiency scores was studied. Among
these three types of features, the average word
frequency showed the most predictive power.
We then explored the impact of vocabulary
profile features in an automated speech scor-
ing context, with particular focus on the im-
pact of two factors: genre of reference corpora
and the characteristics of item-types.
The contribution of the current study lies in
the use of vocabulary profile as a measure of
lexical sophistication for spoken language as-
sessment, an aspect heretofore unexplored in
the context of automated speech scoring.
1 Introduction
This study provides a method that measures ESL
(English as a second language) learners? compe-
tence in vocabulary usage.
Spoken language assessments typically measure
multiple dimensions of language ability. Overall
proficiency in the target language can be assessed
by testing the abilities in various areas including flu-
ency, pronunciation, and intonation; grammar and
vocabulary; and discourse structure. With the recent
move toward the objective assessment of language
ability (spoken and written), it is imperative that we
develop methods for quantifying these abilities and
measuring them automatically.
A majority of the studies in automated speech
scoring have focused on fluency (Cucchiarini et al,
2000; Cucchiarini et al, 2002), pronunciation (Witt
and Young, 1997; Witt, 1999; Franco et al, 1997;
Neumeyer et al, 2000), and intonation (Zechner et
al., 2011). More recently, Chen and Yoon (2011)
and Chen and Zechner (2011) have measured syn-
tactic competence in speech scoring. However, only
a few have explored features related to vocabulary
usage and they have been limited to type-token ratio
(TTR) related features (e.g., Lu (2011)). In addi-
tion, Bernstein et al (2010) developed vocabulary
features that measure the similarity between the vo-
cabulary in the test responses and the vocabulary in
the pre-collected texts in the same topic. However,
their features assessed content and topicality, not vo-
cabulary usage.
The speaking construct of vocabulary usage com-
prises two sub-constructs: sophistication and preci-
sion. The aspect of vocabulary that we intend to
measure in this paper is that of lexical sophistication,
also termed lexical diversity and lexical richness in
second language studies. Measures of lexical so-
phistication attempt to quantify the degree to which
a varied and large vocabulary is used (Laufer and
Nation, 1995). In order to assess the degree of lex-
180
ical sophistication, we employ a vocabulary profile-
based approach (partly motivated from the results of
a previous study, as will be explained in Section 2).
By a vocabulary profile, it is meant that the fre-
quency of each vocabulary item is calculated from
a reference corpus covering the language variety of
the target situation. The degree of lexical sophisti-
cation is captured by the word frequency - low fre-
quency words are considered to be more difficult,
and therefore more sophisticated. We then design
features that capture the difficulty level of vocabu-
lary items in test takers? responses. Finally, we per-
form correlation analyses between these new fea-
tures and human proficiency scores and assess the
feature?s importance with respect to the other fea-
tures in an automatic scoring module. The novelty
of this study lies in the use of vocabulary profile in
an automatic scoring set-up to assess lexical sophis-
tication.
This paper will proceed as follows: we will re-
view related work in Section 2. Data and experiment
setup will be explained in Section 3 and Section 4.
Next, we will present the results in Section 5, discuss
them in Section 6, and conclude with a summary of
the importance of our findings in Section 7.
2 Related Work
Measures of lexical richness have been the focus of
several studies involving assessment of L1 and L2
language abilities (Laufer and Nation, 1995; Ver-
meer, 2000; Daller et al, 2003; Kormos and Denes,
2004). The types of measures considered in these
studies can be grouped into quantitative and qualita-
tive measures.
The quantitative measures give insight into the
number of words known, but do not distinguish them
from one another based on their category or fre-
quency in language use. They have evolved to make
up for the widely applied measure type-token-ratio
(TTR). However, owing to its sensitivity to the num-
ber of tokens, TTR has been considered as an un-
stable measure in differing proficiency levels of lan-
guage learners. The Guiraud index, Uber index, and
Herdan index (Vermeer, 2000; Daller et al, 2003;
Lu, 2011) are some measures in this category mostly
derived from TTR as either simpler transformations
of the TTR or its scaled versions to ameliorate the
effect of differing token cardinalities.
Qualitative measures, on the other hand, dis-
tinguish themselves from those derived from TTR
since they take into account distinctions between
words such as their parts of speech or difficulty lev-
els. Adding a qualitative dimension gives more in-
sight into lexical aspects of language ability than
the purely quantitative measures such as TTR-based
measures. Some measures in this category in-
clude a derived form of the limiting relative diver-
sity (LRD) given by
?
D(verbs)/D(nouns) using
the D-measure proposed in (Malvern and Richards,
1997), Lexical frequency profile (LFP) (Laufer and
Nation, 1995) and P-Lex (Meara and Bell, 2003).
LFP uses a vocabulary profile (VP) for a given
body of written text or spoken utterance and gives
the percentage of words used at different frequency
levels (such as from the one-thousand most com-
mon words, the next thousand most common words)
where the words themselves come from a pre-
compiled vocabulary list, such as the Academic
Word List (AWL) with its associated frequency dis-
tribution on words by Coxhead(1998). Frequency
level refers to a class of words (or appropriately cho-
sen word units) that are grouped based on their fre-
quencies of actual usage in corpora. P-Lex is an-
other approach that uses the frequency level of the
words to assess lexical richness. These measures are
based on the differing frequencies of lexical items
and hence rely on the availability of frequency lists
for the language being considered.
These two different types of measures have been
used in the analysis of essays written by second lan-
guage learners of English (ESL). Laufer and Nation
(1995) have shown that LFP correlates well with an
independent measure of vocabulary knowledge and
that it is possible to categorize learners according to
different proficiency levels using this measure. In
another study seeking to understand the extent to
which VP based on students? essays predicted their
academic performance (Morris and Cobb, 2004), it
was observed that students? vocabulary profile re-
sults correlated significantly with their grades. Ad-
ditionally, VP was found to be indicative of finer dis-
tinctions in the language skills of high proficiency
nonnative speakers than oral interviews can cover.
Furthermore, these measures have been employed
in automated essay scoring. Attali and Burstein
181
(2006) used average word frequency and average
word length in characters across the words in the
essay. In addition to the average word frequency
measure, the average word length measure was im-
plemented to assess the average difficulty of the
word used in the essay under the assumption that
the words with more characters were more difficult
than the words with fewer characters. These fea-
tures showed promising performance in estimating
test takers? proficiency levels.
In contrast to qualitative measures, quantitative
measures did not achieve promising performance.
Vermeer (2000) showed that quantitative measures
achieve neither the validity nor the reliability of the
measures, regardless of the transformations and cor-
rections.
More recently, the relationship of lexical rich-
ness to ESL learners? speaking task performance
has been studied by Lu (2011). The comprehensive
study was aimed at measuring lexical richness along
the three dimensions of lexical density, sophistica-
tion, and variation, using 25 different metrics (be-
longing to both the qualitative and quantitative cate-
gories above) available in the language acquisition
literature. His results, based on the manual tran-
scription of a spoken corpus of English learners, in-
dicate that a) lexical variation (the number of word
types) correlated most strongly with the raters? judg-
ments of the quality of ESL learners? oral narratives,
b) lexical sophistication only had a very small ef-
fect, and c) lexical density (indicative of proportion
of lexical words) in an oral narrative did not appear
to relate to its quality.
In this study, we seek to quantify vocabulary us-
age in terms of measures of lexical sophistication:
VP based on a set of reference word lists. The nov-
elty of the current study lies in the use of VP as
a measure of lexical sophistication for spoken lan-
guage assessment. It derives support from other
studies (Morris and Cobb, 2004; Laufer and Nation,
1995) but is carried out in a completely different
context, that of automatic scoring of proficiency lev-
els in spontaneous speech, an area not explored thus
far in existing literature.
Furthermore, we investigate the impact of the
genre of the reference corpus on the performance of
these lexical measures. For this purpose, three dif-
ferent corpora will be used to generate reference fre-
quency levels. Finally, we will investigate how the
characteristics of the item types influence the perfor-
mance of these measures.
3 Data
The AEST balanced data set, a collection of re-
sponses from the AEST, is used in this study.
AEST is a high-stakes test of English proficiency,
and it consists of 6 items in which speakers are
prompted to provide responses lasting between 45
and 60 seconds per item, yielding approximately 5
minutes of spoken content per speaker.
Among the 6 items, two items elicit information
or opinions on familiar topics based on the exam-
inees? personal experience or background knowl-
edge. These constitute the independent (IND) items.
The four remaining items are integrated tasks that
include other language skills such as listening and
reading. These constitute the integrated (INT)
items. Both sets of items extract spontaneous and
unconstrained natural speech. The primary dif-
ference between the two elicitation types is that
IND items only provide a prompt whereas INT items
provide a prompt, a reading passage, and a listening
stimulus. The size, purpose, and speakers? native
language information for each dataset are summa-
rized in Table 1. All items extract spontaneous, un-
constrained natural speech.
Each response was rated by a trained human rater
using a 4-point scoring scale, where 1 indicates
a low speaking proficiency and 4 indicates a high
speaking proficiency. The scoring guideline is sum-
marized in the AEST rubrics.
Since none of the AEST balanced data was
double-scored, we estimate the inter-rater agreement
ratio of the corpus by using a large double-scored
dataset which used the same scoring guidelines and
scoring process; using the 41K double-scored re-
sponses collected from AEST, we calculate the Pear-
son correlation coefficient to be 0.63, suggesting a
reasonable agreement. The distribution of scores for
this data can be found in Table 2.
4 Experiments
4.1 Overview
In this study, we developed vocabulary profile fea-
tures. From a reference corpus, we pre-compiled
182
Corpus
name
Purpose # of
speakers
# of re-
sponses
Native languages Size
(Hrs)
AEST bal-
anced data
Feature evaluation, Scor-
ing model training and
evaluation
480 2880 Korean (15%), Chinese (14%),
Japanese (7%), Spanish (9%),
Others (55%)
44
Table 1: Data size and speakers? native languages
Size Score1 Score2 Score3 Score4
Number
of files
141 1133 1266 340
(%) 5 40 45 12
Table 2: Distribution of proficiency scores in the dataset
multiple sets of vocabulary lists (e.g., a list of the
100 most frequent words in a reference corpus).
Next, for each test response, a transcription was gen-
erated using the speech recognizer. For each re-
sponse with respect to each reference word list, vo-
cabulary profile features were calculated. In addi-
tion to vocabulary profile features, type-token ratio
(TTR) was calculated as a baseline feature. Despite
its instability, TTR has been employed in the auto-
mated speech scoring systems such as (Zechner et
al., 2009), and its use here allows a direct compar-
ison of the performance of the features with the re-
sults of previous studies.
4.2 Vocabulary list generation
The three reference corpora we used in this study
are presented in Table 3: The General Service
List (GSL), the TOEFL 2000 Spoken and Written
Academic Language Corpus (T2K-SWAL) and the
AEST data.
Corpus Genre Tokens Types
GSL Written - 2,284
T2K-SWAL Spoken 1,869,346 28,855
AEST data Spoken 5,520,375 23,165
Table 3: Three reference corpora used in this study
GSL (West, 1953) comprises 2,284 words se-
lected to be of ?general service? to learners of En-
glish. In this study, we used the version with fre-
quency information from (Bauman, 1995). The orig-
inal version did not include word frequency and
was ?enhanced? by John Bauman and Brent Culli-
gan with the frequency information obtained from
the Brown Corpus, a collection of written texts.
T2K-SWAL (Biber et al, 2002) is a collection of
spoken and written texts covering a broad language
variety and use in the academic setting. In this study,
only its spoken texts were used. The spoken corpus
included manual transcriptions of discussions, con-
versations, and lectures that occurred in class ses-
sions, study-group meetings, office hours, and ser-
vice encounters.
Finally, AEST data is a collection of manual tran-
scriptions of spoken responses from the AEST for
non-native English speakers. Although there was no
overlap between AEST data and the evaluation data
(AEST balanced data), the vocabulary lists in AEST
data might be a closer match to the vocabulary lists
in the evaluation data since both of them come from
the same test products. From a content perspective,
this dataset is likely to better reflect characteristics
of non-native English speakers than the other two
reference corpora.
For T2K-SWAL and AEST, all transcriptions
were normalized; all the tokens were further de-
capitalized and removed of all non-alphanumeric
characters except for dash and quote. The morpho-
logical variants were considered as different words.
All words were sorted by the word occurrences in
the corpus, and a set of 6 lists were generated:
top-100 words (TOP1), word frequency ranks 101-
300 (TOP2), ranks 301-700 (TOP3), ranks 701-1500
(TOP4), ranks 1501-3000 (TOP5), and all other
words with ranks of 3001 and above (TOP6). For
GSL, a set of 5 lists was generated; TOP6 was
not generated since GSL only included about 2200
words.
Compared to written texts, speakers tended to use
a much smaller vocabulary in speech. For instance,
the percentage of words within the top-1000 words
on the total word types of AEST data responses was
over 90% on average, and they were similar across
183
proficiency levels. This is the reason why we sub-
classified the top 1000 words into three lists, unlike
the vocabulary profile features using top-1000 words
as one list like (Morris and Cobb, 2004), which did
not have any power to differentiate between profi-
ciency levels.
4.3 Transcription generation for evaluation
data
A Hidden Markov Model (HMM) speech recognizer
was trained on the AEST dataset, approximately
733 hours of non-native speech collected from 7872
speakers. A gender independent triphone acoustic
model and a combination of bigram, trigram, and
four-gram language models was used. The word
error rate (WER) on the held-out test dataset was
27%. For each response in the evaluation partition,
an ASR-based transcription was generated using the
speech recognizer.
4.4 Feature generation
Each response comprised less than 60 seconds of
speech with an average of 113 word tokens. Due
to the short response length, there was wide varia-
tion in the proportion of low-frequency word types
for the same speaker. In order to address this issue,
for each speaker, two responses from the same item-
type (IND/INT) were concatenated and used as one
large response. As a result, three concatenated re-
sponses (one IND response and two INT responses)
were generated for each speaker, yielding a total of
480 concatenated responses for IND items and 960
concatenated responses for INT items for our exper-
iment.
First, a list of word types was generated from
the ASR hypothesis of each concatenated response.
IND items provide only a one-sentence prompt,
while INT items provide stimuli including a prompt,
a reading passage, and a listening stimulus. In order
to minimize the influence of the vocabulary in the
stimuli on that of the speakers, we excluded the con-
tent words that occurred in the prompts or stimuli
from the word type list1.
1This process prevents to measure the content relevance;
whether the response is off-topic or not. However, this is not
problematic since the features in this study will be used in the
conjunction with the features that measure the accuracy of the
aspects of content and topicality such as (Xie et al, 2012)?s fea-
Table 4: List of features.
Feature # of Feature Description
features type
TTR 1 Ratio Type-token ratio
TOPn 5 or 6a Listrel Proportion of types
that occurred both
the response and
TOPn list in the to-
tal types of the re-
sponse.
aRank 1 Rank Avg. word rankb
aFreq 1 Freq Avg. word freq.c
lFreq 1 Freq Avg. log(word
freq)d
a For GSL, five different features were created using
TOP1-TOP5 lists, but TOP6 was not created. For
T2K-SWAL and AEST data, six different features were
created using TOP1-TOP6 lists separately.
b ?rank? is the ordinal number of words in a list that is sorted in
descending order of word frequency; words not present in the
reference corpus get the default rank of RefMaxRank+1.
c Avg. word frequency is the sum of the word-frequencies of
word types in the reference corpus divided by the total
number of words in the reference corpus; words not in the
reference corpus get assigned a default frequency of 1.
d Same as feature aFreq, but the logarithm of the word
frequency is taken here
Next, we generated five types of features using
three reference vocabulary lists. A maximum of 10
features were generated for each reference list. The
feature-types are tabulated in Table 4.
All features above were generated from word
types, not word tokens, i.e., multiple occurrences of
the same word in a response were only counted once.
Below we delineate the step-by-step process with
a sample response that leads to the feature genera-
tion outlined in Table 5.
? Step 1: Generate ASR hypothesis for the given
speech response. e.g: Every student has dif-
ferent perspective about how to relax. Playing
xbox.
? Step 2: Generate type list from ASR hypoth-
esis. For the response above we get the list
- about, how, different, xbox, to, relax, every,
perspective, student, has, playing.
tures.
184
word
freq. in
reference
corpus
word rank in
the reference
corpus
TOPn
about 25672 30 TOP1
how 8944 96 TOP1
has 18105 53 TOP1
to 218976 2 TOP1
different 5088 153 TOP2
every 2961 236 TOP2
playing 798 735 TOP4
perspec-
tive
139 1886 TOP5
xbox 1 20000 No
Table 5: An example of feature calculation.
? Step 3: Generate type list excluding words that
occurred in the prompt - about, how, different,
xbox, to, every, perspective, has, playing.
From the ASR hypotheses (result of Step 1), the
corresponding type list was generated (Step 2) and
two words (?student?, ?relax?) were excluded from
the final list due to overlap with the prompt. The
final word list used in the feature generation has 9
types (Step 3).
Next, for each word in the above type list, if it oc-
curs in the reference corpus (a list of words sorted
by frequency), its word frequency, word rank and
the TOPn information (whether the word belonged
to the TOPn list or not) are obtained. If it did not oc-
cur in the reference corpus, the default frequency (1)
and the default word rank (20000) were assigned. In
5, the default values were assigned for ?xbox? since
it was not in the reference corpus.
Finally, the average of the word frequencies and
the average of the the word ranks were calculated
(aFreq and aRank). For lFreq, the log value of each
frequency was calculated and then averaged. For
TOPn features, we obtain the proportion of the word
types that belong to the TOPn category. For the
above sample, the TOP1 feature value was 0.444
since 4 words belong to TOP1 and the total number
of word types was 9 (4/9=0.444).
5 Results
5.1 Correlation
We analyzed the relationship between the proposed
features and human proficiency scores to assess their
influence on predicting the proficiency score. The
reference proficiency score for a concatenated re-
sponse was estimated by summing up the two scores
of the constituent responses. Thus, the new score
scale was 2-8. Table 6 presents Pearson correlation
coefficients (r).
The best performing feature was aFreq followed
by TOP1. Both features showed statistically signif-
icant negative correlations with human proficiency
scores. TOP6 also showed statistically significant
correlation with human scores, but it was 10-20%
lower than TOP1. This suggests that a human rater
more likely assigned high scores when the vocabu-
lary of the response was not limited to a few most
frequent words. However, the use of difficult words
(low-frequency) shows a weaker relationship with
the proficiency scores.
Features based on AEST data outperformed fea-
tures based on T2K-SWAL or GSL. The correlation
of the AEST data-based aFreq feature was ?0.61
for the IND items and?0.51 for the INT items; they
were approximately 0.1 higher than the correlations
of T2K-SWAL or GSL-based features. A similar
tendency was found for the TOP1-TOP6 features,
although differences between AEST data-based fea-
tures and other reference-based features were less
salient overall.
For top-performing vocabulary profile features
including aFreq and TOP1, the correlations of
INT items were weaker than those of the IND items.
In general, the correlations of INT items were 10-
20% lower than those of the IND items in absolute
value.
aFreq and TOP1 consistently achieved better
performance than TTR across all item-types.
5.2 Scoring model building
To arrive at an automatic scoring model, we included
the new vocabulary profile features with other fea-
tures previously found to be useful in a multiple lin-
ear regression (MLR) framework. A total of 80 fea-
tures were generated by the automated speech pro-
ficiency scoring system from Zechner et al (2009),
185
Reference TTR TOP1 TOP2 TOP3 TOP4 TOP5 TOP6 aRank aFreq lFreq
IND GSL -.147 -.347 .027 .078 .000 .053 - .266 -.501 -.260
T2K-SWAL -.147 -.338 .085 .207 .055 .020 .168 .142 -.509 -.159
ATEST -.147 -.470 .014 .275 .172 .187 .218 .236 -.613 -.232
INT GSL -.245 -.255 -.086 -.019 -.068 -.031 - .316 -.404 -.318
T2K-SWAL -.245 -.225 .010 .094 .047 .079 .124 .087 -.405 -.198
ATEST -.245 -.345 -.092 .156 .135 .188 .194 .214 -.507 -.251
Table 6: Correlations between features and human proficiency scores
and they were classified into 5 sub-groups: fluency,
pronunciation, prosody, vocabulary complexity, and
grammar usage. For each sub-group, at least one
feature that correlated well with human scores but
had a low inter-correlation with other features was
selected. A total of following 6 features were se-
lected and used in the base model (base):
? wdpchk (fluency): Average chunk length in words;
a chunk is a segment whose boundaries are set by
long silences
? tpsecutt (fluency): Number of types per sec.
? normAM (pronunciation): Average acoustic model
score normalized by the speaking rate
? phn shift (pronunciation): Average absolute dis-
tance of the normalized vowel durations compared
to standard normalized vowel durations estimated
on a native speech corpus
? stretimdev (prosody): Mean deviation of distance
between stressed syllables in sec.
? lmscore (grammar): Average language model score
normalized by number of words
We first calculated correlations between these fea-
tures and human proficiency scores and compared
them with the most predictive vocabulary profile
features. Table 7 presents Pearson correlation co-
efficients (r) of these features.
In both item-types, the most correlated features
represented the aspect of fluency in production.
While tpsecutt was the best feature in IND items
and the correlation with human scores was approx-
imately 0.66, in INT items, wdpchk was the best
feature and the correlation was even higher, 0.73.
The performance of aFreq was particularly high
in IND items; it was the second best feature and only
marginally lower than the best feature (by 0.04).
aFreq also achieved promising performance in INT;
Features IND INT
wdpchk .538 .612
tpsecutt .659 .729
normAM .467 .429
phn shift -.503 -.535
stretimemdev -.442 -.397
lmscore .257 .312
aFreq -.613 -.507
TOP1 -.470 -.345
TTR -.147 -.245
Table 7: Comparison of feature-correlations with human-
assigned proficiency scores.
it was the fourth best feature. However, the perfor-
mance was considerably lower than the the best fea-
ture, and the difference between the best feature and
aFreq was approximately 22%.
We compared the performances of this base
model with an augmented model (base + TTR + all
vocabulary profile features) whose feature set was
the base augmented with our proposed measures of
vocabulary sophistication. Item-type specific multi-
ple linear regression models were trained using five-
fold cross validation. The 480 IND responses 960
INT responses were partitioned into five sets, sepa-
rately. In each fold, an item-type specific regression
model was trained using four of these partitions and
tested on the remaining one.
The averages of the five-fold models are sum-
marized in Table 8, showing weighted kappa to
indicate agreement between automatic scores and
human-assigned scores and also the Pearson?s cor-
relation (r) of the unrounded (un-rnd) and rounded
(rnd) scores with the human-assigned scores. We
used the correlation and weighted kappa as perfor-
mance evaluation measures to maintain the consis-
tency with the previous studies such as (Zechner
et al, 2009). In addition, the correlation metric
186
matches better with our goal to investigate the rela-
tionship between the predicted scores and the actual
scores rather than the difference between the pre-
dicted scores and the actual scores.
Features un-
rnd
corr.
rnd
corr.
weighted
kappa
IND base 0.66 0.62 0.55
base + TTR 0.66 0.63 0.56
base + TTR +
all
0.66 0.64 0.57
INT base 0.76 0.73 0.69
base + TTR 0.76 0.74 0.70
base + TTR +
all
0.77 0.74 0.70
Table 8: Performance of item-type specific multiple lin-
ear regression based scoring models.
The new scores show slightly better agreement
with human-assigned scores, but the improvement
was small in both item-types, approximately 1%.
6 Discussion
In general, we found that the test takers used a fairly
small number of vocabulary items in the spoken re-
sponses. On average, the total types used in the
responses was 87.21 for IND items and 98.52 for
INT items. Furthermore, the proportions of high
frequency words on test takers? spoken responses
were markedly high. The proportion of top-100
words was almost 50% and the proportion of top-
1500 words (summation of TOP1-TOP4) was over
89% on average. This means that only 1500 words
represent almost 90% of the active vocabulary of
the test takers in their spontaneous speech. Figure
1 presents the average TOP1-TOP6 features across
all proficiency levels.
The values of INT items were similar to IND
items, but the TOP3-TOP6 values were slightly
higher than IND items; INT items tended to include
more low frequency words. In order to investigate
the impact of the higher proportion of low frequency
words in INT items, we selected two features (aFreq
and TOP1) and further analyzed them.
Table 9 provides the mean of aFreq and TOP1 for
each score level. The features were generated using
AEST as a reference.
Figure 1: Proportion of top-N frequent words on average
Score aFreq TOP1
IND INT IND INT
2 43623 36175 .60 .52
3 38165 32493 .55 .49
4 33861 28884 .51 .48
5 30599 27118 .49 .46
6 28485 26327 .46 .45
7 27358 25093 .45 .43
8 26065 24711 .43 .43
Table 9: Mean of vocabulary profile features for each
score level
On average, the differences between adjacent
score levels in INT items were smaller than those
in IND items. The weaker distinction between score
levels may result in the lower performance of vo-
cabulary profile features in INT items. Particularly,
the differences were smaller in lower score levels (2-
4) than in higher score levels (5-8). The relatively
high proportion of low frequency words in the low
score level reduced the predictive power of vocabu-
lary profile features.
This difference between the item-types strongly
supports item-type-specific modeling. We combined
the IND and INT item responses and computed
a correlation between the features and the profi-
ciency scores over the entirety of data sets. De-
spite increase in sample sizes, the correlations were
lower than both the corresponding correlations of
the IND items and the INT items. For instance, the
correlation of the T2K-SWAL-based aFreq feature
was?0.393, and that of the AEST data-based aFreq
was?0.50, which was approximately 3% lower than
the INT items and 10% lower than the IND items.
The difference in the vocabulary distributions be-
tween the two item-types decreased the performance
187
of the features.
In this study, AEST data-based features outper-
formed T2K-SWAL-based features. Although no
items in the evaluation data overlapped with items
in AEST data, the similarity in the speakers? profi-
ciency levels and task types might have resulted in
a better match between the vocabulary and its dis-
tributions of AEST data with AEST balanced data,
finally the AEST data-based features achieved the
best performance.
In order to explore the degree to which AEST bal-
anced data (test responses) and the reference cor-
pora matched, we calculated the proportion of word
types that occurred in test responses and reference
corpora (the coverage of reference list). The ASR
hypotheses of AEST balanced data comprised 6,024
word types. GSL covered 73%, T2K-SWAL cov-
ered 99%, and AEST data covered close to 100%.
Considering the fact that, a) despite high coverage
of both T2K-SWAL and AEST data, T2K-SWAL-
based features achieved much lower performance
than AEST data, and, b) despite huge differences
in the coverage between T2K-SWAL and GSL, the
performance of features based on these reference
corpora were comparable, coverage was not likely
to have been a factor having a strong impact on the
performance. The large differences in the perfor-
mance of TOP1 across reference lists support the
possibility of the strong influence of high frequency
word types on proficiency; the kinds of word types
that were in the TOP1 bins were an important factor
that influenced the performance of vocabulary pro-
file features. Finally, genre differences (spoken texts
vs. written texts) in reference corpora did not have
strong impact on the predictive ability of the fea-
tures; the performance of features based on written
reference corpus (GSL) were comparable to those
based on a spoken reference corpus (T2K-SWAL).
Despite the high correlation shown by the indi-
vidual features (such as aFreq), we do not see a cor-
responding increase in the performance of the scor-
ing model with all the best performing features. The
most likely explanation to this is the small training
data size; in each fold, only about 380 responses for
IND and about 760 responses for INT were used
in the scoring model training. Another possibility
is overlap with the existing features; the aspect that
vocabulary profile features are modeling may be al-
ready covered to some extent in existing feature set.
In future research, we will further investigate this as-
pect in details.
7 Conclusions
In this study, we presented features that measure
ESL learners? vocabulary usage. In particular, we
focused on vocabulary sophistication, and explored
the suitability of vocabulary profile features to cap-
ture sophistication. From three different reference
corpora, the frequency of vocabulary items was cal-
culated which was then used to estimate the sophis-
tication of test takers? vocabulary. Among the three
different reference corpora, features based on AEST
data, a collections of responses similar to that of the
test set, showed the best performance. A total of 29
features were generated, and the average word fre-
quency (aFreq) achieved the best correlation with
human proficiency scores. In general, vocabulary
profile features showed strong correlations with hu-
man proficiency scores, but when used in an auto-
matic scoring model in combination with an existing
set of predictors of language proficiency, the aug-
mented feature set showed marginal improvement in
predicting human-assigned scores of proficiency.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technology,
Learning, and Assessment, 4(3).
John Bauman. 1995. About the GSL. Retrieved
March 17, 2012 from http://jbauman.com/
gsl.html.
Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010.
Fluency and structural complexity as predictors of L2
oral proficiency. In Proceedings of InterSpeech 2010,
Tokyo, Japan, September.
Douglas Biber, Susan Conrad, Randi Reppen, Pat Byrd,
and Marie Helt. 2002. Speaking and writing in the
university: A multidimensional comparison. TESOL
Quarterly, 36:9?48.
Lei Chen and Su-Youn Yoon. 2011. Detecting structural
events for assessing non-native speech. In Proceed-
ings of the 6th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 38?45.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
188
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics 2011, pages 722?731.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: Comparisons between read and spontaneous
speech. The Journal of the Acoustical Society of Amer-
ica, 111(6):2862?2873.
Helmut Daller, Roeland van Hout, and Jeanine Treffers-
Daller. 2003. Lexical richness in the spontaneous
speech of bilinguals. Applied Linguistics, 24(2):197?
222.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scoring
for language instruction. In Proceedings of ICASSP
97, pages 1471?1474.
Judit Kormos and Mariann Denes. 2004. Exploring mea-
sures and perceptions of fluency in the speech of sec-
ond language learners. System, 32:145?164.
Batia Laufer and Paul Nation. 1995. Vocabulary size and
use: lexical richness in L2 written production. Applied
Linguistics, 16:307?322.
Xiaofei Lu. 2011. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Language Journal.
David D. Malvern and Brian J. Richards. 1997. A
new measure of lexical diversity. In Evolving mod-
els of language: Papers from the Annual Meeting of
the British Association of Applied Linguists held at the
University of Wales, Swansea, September, pages 58?
71.
Paul Meara and Huw Bell. 2003. P lex: A simple and
effective way of describing the lexical characteristics
of short L2 texts. Applied Linguistics, 24(2):197?222.
Lori Morris and Tom Cobb. 2004. Vocabulary profiles
as predictors of the academic performance of teaching
english as a second language trainees. System, 32:75?
87.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88?93.
Anne Vermeer. 2000. Coming to grips with lexical rich-
ness in spontaneous speech data. Language Testing,
17(1):65?83.
Michael West. 1953. A General Service List of English
Words. Longman, London.
Silke Witt and Steve Young. 1997. Performance mea-
sures for phone-level pronunciation teaching in CALL.
In Proceedings of the Workshop on Speech Technology
in Language Learning, pages 99?102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of the NAACL-HLT, Montreal,
July.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883?895, October.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011. Eval-
uating prosodic features for automated scoring of non-
native read speech. In IEEE Workshop on Automatic
Speech Recognition and Understanding 2011, Hawaii,
December.
189
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 116?123,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Similarity-Based Non-Scorable Response Detection for Automated Speech
Scoring
Su-Youn Yoon
Educational Testing Service
Princeton, NJ, USA
syoon@ets.org
Shasha Xie
Microsoft
Sunnyvale, CA, USA
shxie@microsoft.com
Abstract
This study provides a method that iden-
tifies problematic responses which make
automated speech scoring difficult. When
automated scoring is used in the context
of a high stakes language proficiency as-
sessment, for which the scores are used to
make consequential decisions, some test
takers may have an incentive to try to game
the system in order to artificially inflate
their scores. Since many automated pro-
ficiency scoring systems use fluency fea-
tures such as speaking rate as one of the
important features, students may engage
in strategies designed to manipulate their
speaking rate as measured by the system.
In order to address this issue, we de-
veloped a method which filters out non-
scorable responses based on text similar-
ity measures. Given a test response, the
method generated a set of features which
calculated the topic similarity with the
prompt question or the sample responses
including relevant content. Next, an au-
tomated filter which identified these prob-
lematic responses was implemented us-
ing the similarity features. This filter im-
proved the performance of the baseline
filter in identifying responses with topic
problems.
1 Introduction
In spoken language proficiency assessment, some
responses may include sub-optimal characteristics
which make it difficult for the automated scor-
ing system to provide a valid score. For instance,
some test takers may try to game the system by
speaking in their native languages or by citing
memorized responses for unrelated topics. Oth-
ers may repeat questions or part of questions with
modifications instead of generating his/her own
response. Hereafter, we call these problematic
responses non-scorable (NS) responses. By us-
ing these strategies, test takers can generate flu-
ent speech, and the automated proficiency scoring
system, which utilizes fluency as one of the im-
portant factors, may assign a high score. In or-
der to address this issue, the automated proficiency
scoring system in this study used a two-step ap-
proach: these problematic responses were filtered
out by a ?filtering model,? and only the remaining
responses were scored using the automated scor-
ing model. By filtering out these responses, the
robustness of the automated scoring system can be
improved.
The proportion of NS responses, in the assess-
ment of which the responses are scored by human
raters, are likely to be low. For instance, the pro-
portion of NS responses in the international En-
glish language assessment used in this study was
2%. Despite this low proportion, it is a serious
problem which has a strong impact on the validity
of the test. In addition, the likelihood of students
engaging in gaming strategies may increase with
the use of automated scoring. Therefore, an au-
tomated filtering model with a high accuracy is a
necessary step to use the automated scoring sys-
tem as a sole rater.
Both off-topic and copy responses have topic-
related problems, although they are at the two ex-
tremes in the degree of similarity. Focusing on
the intermediate levels of similarity, Metzler et al.
(2005) presented a hierarchy of five similarity lev-
els: unrelated, on the general topic, on the spe-
cific topic, same facts, and copied. In the auto-
mated scoring of spontaneous speech, responses
that fell into unrelated can be considered as off-
topic, while the ones that fell into copied can be
considered as repetition or plagiarism. Follow-
ing this approach, we developed a non-scorable
response identification method utilizing similar-
116
Figure 1: A diagram of the overall architecture of
our method.
ity measures. We will show that this similarity
based method is highly efficient in identifying off-
topic or repetition responses. Furthermore, we
will show that the method can effectively detect
NS responses that are not directly related to the
topicality issue (e.g, non-English responses).
Figure 1 shows the overall architecture of our
method including the automated speech profi-
ciency scoring system. For a given spoken re-
sponse, the system performs speech processing in-
cluding speech recognition and generates a word
hypotheses and time stamps. In addition, the sys-
tem computes pitch and power; the system calcu-
lates descriptive statistics such as the mean and
standard deviation of pitch and power at both the
word level and response level. Given the word hy-
potheses and descriptive features of pitch/power,
it derives features for automated proficiency scor-
ing. In addition, the similarity features are gener-
ated based on the word hypotheses and topic mod-
els. Finally, given both sets of features, the filter-
ing model filters out non-scorable responses, and
the remainder of the responses are scored using a
scoring model. A detailed description of the sys-
tem is available from Zechner et al. (2009). In this
study, we will only focus on the filtering model.
This paper will proceed as follows: we first re-
view previous studies in section 2, then describe
the data in section 3, and present the method and
experiment set-up in sections 4 and 5. The results
and discussion are presented in section 6, and the
conclusions are presented in section 7.
2 Related Work
Filtering of NS responses for automated speech
scoring has been rarely recognized. Only a few
pieces of research have focused on this task,
and most studies have targeted highly restricted
speech. van Doremalen et al. (2009) and Lo et
al. (2010) used normalized confidence scores of
a speech recognizer in recasting speech. They
identified non-scorable responses with promising
performances (equal error rates ranged from 10
to 20%). Cheng and Shen (2011) extended these
studies and combined an acoustic model score, a
language model score, and a garbage model score
with confidence scores. They applied this new fil-
ter to less constrained items (e.g., picture descrip-
tion) and identified off-topic responses with an ac-
curacy rate of 90%with a false positive rate of 5%.
Although normalized confidence scores
achieved promising performances in restricted
speech, they may not be appropriate for the items
that elicit unconstrained spontaneous speech.
Low confidence scores signal the use of words
or phrases not covered by the language model
(LM) and this is strongly associated with off-topic
responses in restricted speech in which the target
sentence is given. However, in spontaneous
speech, this is not trivial; it may be associated
with not only off-topic speech but also mismatch
between the LM and speech input due to the low
coverage of the LM. Due to the latter case, the
decision based on the confidence score may not
be effective in measuring topic similarity.
The topic similarity between two documents
has been frequently modeled by relative-frequency
measures (Hoad and Zobel, 2003; Shivakumar and
Garcia-Molina, 1995), document fingerprinting
(Brin et al., 1995; Shivakumar and Garcia-Molina,
1995; Shivakumar and Garcia-Molina, 1996)), and
query based information retrieval methods using
vector space models or language model (Sander-
son, 1997; Hoad and Zobel, 2003).
Document similarity measures have been ap-
plied in automated scoring. Foltz et al. (1999)
evaluated the content of written essays using latent
semantic analysis (LSA) by comparing the test es-
says with essays of known quality in regard to their
degree of conceptual relevance and the amount of
relevant content. In another approach, the lexical
content of an essay was evaluated by comparing
the words contained in each essay to the words
found in a sample of essays from each score cat-
egory (Attali and Burstein, 2006). More recently,
Xie et al. (2012) used a similar approach in au-
tomated speech scoring; they measured the sim-
ilarity using three similarity measures, including
a lexical matching method (Vector Space Model)
and two semantic similarity measures (Latent Se-
mantic Analysis and Pointwise Mutual Informa-
tion). They showed moderately high correlations
117
between the similarity features and human profi-
ciency scores on even the output of an automatic
speech recognition system. Similarity measures
have also been used in off-topic detection for non-
native speakers? essays. Higgins et al. (2006) cal-
culated overlaps between the question and content
words from the essay and obtained an error rate of
10%.
Given the promising performance in both auto-
mated scoring and off-topic essay detection, we
will expand these similarity measures in NS re-
sponse detection for speech scoring.
3 Data
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment was composed of items in
which speakers were prompted to provide sponta-
neous speech.
Approximately 48,000 responses from 8,000
non-native speakers were collected and used for
training the automated speech recognizer (ASR
set). Among 24 items in the ASR set, four items
were randomly selected. For these items, a total
of 11,560 responses were collected and used for
the training and evaluation of filtering model (FM
set). Due to the extremely skewed distribution of
NS responses (2% in the ASR set), it was not easy
to train and evaluate the filtering model. In or-
der to address this issue, we modified the distribu-
tion of NS responses in the FM set. Initially, we
collected 90,000 responses including 1,560 NS re-
sponses. While maintaining all NS responses, we
downsampled the scorable responses in the FM set
to include 10,000 responses. Finally, the propor-
tion of NS responses was 6 times higher in FM
set (13%) than ASR set. This artificial increase of
the NS responses reduces the current problem of
the skewed NS distribution and may make the task
easier. However, the likelihood of students engag-
ing in gaming strategies may increase with the use
of automated scoring, and this increased NS dis-
tribution may be close to this situation.
Each response was rated by trained human
raters using a 4-point scoring scale, where 1 indi-
cated a low speaking proficiency and 4 indicated a
high speaking proficiency. The raters also labeled
responses as NS, when appropriate. NS responses
are defined as responses that cannot be given a
score according to the rubrics of the four-point
scale. NS responses were responses with tech-
nical difficulties (TDs) that obscured the content
of the responses or responses that would receive
a score of 0 due to participants? inappropriate be-
haviors. The speakers, item information, and dis-
tribution of proficiency scores are presented in Ta-
ble 1. There was no overlap in the sets of speakers
in the ASR and FM sets.
In addition, 1,560 NS responses from the FM
set were further classified into six types by two
raters with backgrounds in linguistics using the
rubrics presented in Table 2. This annotation was
used for the purpose of analysis: to identify the
frequent types of NS responses and prioritize the
research effort.
Type Proportion
in total
NSs
Description
NR 73% No response. Test taker doesn?t
speak.
OR 16% Off-topic responses. The re-
sponse is not related to the
prompt.
TR 5% Generic responses. The re-
sponse only include filler words
or generic responses such as, ?I
don?t know, it is too difficult to
answer, well?, etc.
RE 4% Question copy. Full or partial
repetition of question.
NE 1% Non-English. Responses is in a
language other than English.
OT 1% Others
Table 2: Types of zero responses and proportions
Some responses belonged to more than one
type, and this increased complexity of the anno-
tation task. For instance, one response was com-
prised of a question copy and generic sentences,
while another response was comprised of a ques-
tion copy and off-topic sentences. An example of
this type was presented in Table 3. This was a re-
sponse for the question ?Talk about an interesting
book that you read recently. Explain why it was
interesting
1
.?
For these responses, annotators first segmented
them into sentences and assigned the type that was
most dominant.
Each rater annotated approximately 1,000 re-
sponses, and 586 responses were rated by both
1
In order to not reveal the real test question administered
in the operational test, we invented this question. Based on
the question, we also modified a sample response; the ques-
tion copy part was changed to avoid disclosure of the test
question, but the other part remained the same as the original
response.
118
Data set Num. responses Num. speakers Num. items Average score Score distribution
NS 1 2 3 4
ASR 48,000 8,000 24 2.63 773 1953 16834 23106 5334
2% 4% 35% 48% 11%
FM 11,560 11,390 4 2.15 1560 734 4328 4263 675
13% 6% 37% 37% 6%
Table 1: Data size and score distribution
Sentence Type
Well in my opinion are the inter-
esting books that I read recently
is.
RE
Talking about a interesting book. RE
One interesting book oh God in-
teresting book that had read re-
cently.
RE
Oh my God. TR
I really don?t know how to an-
swer this question.
TR
Well I don?t know. TR
Sorry. TR
Table 3: Manual transcription of complex-type re-
sponse
raters. The Cohen?s kappa between two raters was
0.76. Among five different NS responses, non-
response was the most frequent type (73%), fol-
lowed by off-topic (16%). The combination of the
two types was approximately 90% of the entire NS
responses.
4 Method
In this study, we generated two different types of
features. First, we developed similarity features
(both chunk-based and response-based) to identify
the responses with problems in topicality. Sec-
ondly, we generated acoustic, fluency, and ASR-
confidence features using a state-of-art automated
speech scoring system. Finally, using both feature
sets, classifiers were trained to make a binary dis-
tinction of NS response vs. scorable response.
4.1 Chunk-based similarity features
Some responses in this study included more than
two different types of the topicality problems. For
instance, the first three sentences in Table 3 be-
longed to the ?copied? category, while the other
sentences fell into ?unrelated?. If the similarity
features were calculated based on the entire re-
sponse, the feature values may fall into neither
the ?copied? nor ?unrelated? range because of the
trade-off between the two types at two extremes.
In order to address this issue, we calculated chunk-
based similarity features similar to Metzler et al.
(2005)?s sentence-based features.
First, the response was split into the chunks
which were surrounded by long silences with du-
rations longer than 0.6 sec. For each chunk,
the proportion of word overlap with the question
(WOL) was calculated based on the formula (1).
Next, chunks with a WOL higher than 0.5 were
considered as question copies.
WOL =
|S?Q|
|S|
where S is a response and Q is a question,
|S ? Q| is the number of word types that appear
both in S and Q,
|S| is the number of word types in S
(1)
Finally, the following three features were de-
rived for each response based on the chunk-based
WOL.
? numwds: the number of word tokens after re-
moving question copies, fillers, and typical
generic sentences
2
;
? copyR: the proportion of question copies in
the response in terms of number of word to-
kens;
? meanWOL: the mean ofWOLs for all chunks
in the response.
4.2 Response-based similarity features
We implemented three features based on a vector
space model (VSM) using cosine similarity and
term frequency-inverse document frequency (tf -
idf ) weighting to estimate the topic relevance at
the response-level.
2
Five sentences ?it is too difficult?, ?thank you?, ?I don?t
know?, ?I am sorry?, and ?oh my God? were stored as typical
sentences and removed from responses
119
Since the topics of each question were differ-
ent from each other, we trained a VSM for each
question separately. For the four items in the
FM set, we selected a total of 485 responses (125
responses per item) from the ASR set for topic
model training. Assuming that the responses with
the highest proficiency scores contain the most di-
verse and appropriate words related to the topic,
we only selected responses with a score of 4.
We obtained the manual transcriptions of the re-
sponses, and all responses about the same ques-
tion were converted into a single vector. In this
study, the term was a unigram word, and the doc-
ument was the response. idf was trained from the
entire set of 48,000 responses in the ASR training
partition, while tf was trained from the question-
specific topic model training set.
In addition to the response-based VSM, we
trained a question-based VSM. Each question was
composed of two sentences. Each question was
converted into a single vector, and a total of four
VSMs were trained. idf was trained in the same
way as the response-based VSMs, while tf was
trained only using the question sentences.
Using these two different types of VSMs, the
following three features were generated for each
response.
? sampleCosine: a similarity score based on
the response-based VSM. Assuming that two
documents with the same topic shared com-
mon words, it measured the similarity in the
words used in a test response and the sample
responses. The feature was implemented to
identify off-topic responses (OR);
? qCosine: a similarity score based on the
question-based VSM. It measured the simi-
larity between a test response and its ques-
tion. The feature was implemented to iden-
tify both off-topic responses (OR) and ques-
tion copy responses (RE); a low score is
highly likely to be an off-topic response,
while a high score signals a full or partial
copy;
? meanIDF : mean of idfs for all word tokens
in the response. Generic responses (TR) tend
to include many high frequency words such
as articles and pronouns, and the mean idf
value of these responses may be low.
4.3 Features from the automated speech
scoring system
A total of 61 features (hereafter, A/S features)
were generated using a state-of-the-art automated
speech scoring system. A detailed description
of the system is available from (Jeon and Yoon,
2012). Among these features, many features were
conceptually similar but based on different nor-
malization methods, and they showed a strong
inter-correlation. For this study, 30 features were
selected and classified into three groups according
to their characteristics: acoustic features, fluency
features, and ASR-confidence features.
The acoustic features were related to power,
pitch, and MFCC. First, power, pitch and
MFCC were extracted at each frame using
Praat (Boersma, 2002). Next, we generated
response-level features from these frame-level fea-
tures by calculating mean and variation. These
features captured the overall distribution of energy
and voiced regions in a speaker?s response. These
features are relevant since NS responses may have
an abnormal distribution in energy. For instance,
non-responses contain very low energy. In order
to detect these abnormalities in the speech signal,
pitch and power related features were calculated.
The fluency features measure the length of a re-
sponse in terms of duration and number of words.
In addition, this group contains features related
to speaking rate and silences, such as mean du-
ration and number of silences. In particular, these
features are effective in identifying non-responses
which contain zero or only a few words.
The ASR-confidence group contains features
predicting the performance of the speech recog-
nizer. Low confidence scores signal low speech
recognition accuracy.
4.4 Model training
Three filtering models were trained to investigate
the impact of each feature group: a filtering model
using similarity features (hereafter, the Similarity-
filter), a filtering model using A/S features (here-
after, the A/S-filter), and a filtering model using a
combination of the two groups of features (here-
after, the Combined-filter).
5 Experiments
AnHMM-based speech recognizer was trained us-
ing the ASR set. A gender independent triphone
acoustic model and a combination of bigram, tri-
120
gram, and four-gram language models were used.
A word error rate (WER) of 27% on the held-out
test dataset was observed.
For each response in the FM set, the word
hypotheses was generated using this recognizer.
From this ASR-based transcription, the six simi-
larity features were generated. In addition, the 30
A/S features described in 4.3 were generated.
Using these two sets of features, filtering mod-
els were trained using the Support Vector Ma-
chine algorithm (SVM) with the RBF kernel of
the WEKA machine-learning toolkit (Hall et al.,
2009). A 10 fold cross-validation was conducted
using the FM dataset.
6 Results and discussion
First, we will report the performance for the sub-
set only topic-related NS responses. The sim-
ilarity features were designed to detect NS re-
sponses with topicality issues, but the majority in
the FM set were non-response (73%). The topic-
related NS responses (off-topic responses, generic
responses, and question copy responses) were only
25%. In the entire set, the advantage of the simi-
larity features over the A/S features might not be
salient due to the high proportion of non-response.
In order to investigate the performance of the sim-
ilarity features in the topic related NS responses,
we excluded all responses other than ?OR?, ?TR?,
and ?RE? from the FM set and conducted a 10 fold
cross-validation.
Table 4 presents the average of the 10 fold
cross-validation results in this subset. In this set,
the total number of NS responses is 314, and the
accuracy of the majority voting (to classify all re-
sponses as scorable responses) is 0.962.
acc. prec. recall fscore
Similarity-
filter
0.975 0.731 0.548 0.626
A/S-filter 0.971 0.767 0.341 0.472
Combined-
filter
0.977 0.780 0.566 0.656
Table 4: Performance of filters in topic-related NS
detection
Not surprisingly, the Similarity-filter outper-
formed the A/S-filter: the F-score was approxi-
mately 0.63 which was 0.15 higher than that of
the A/S-filter in absolute value. The lack of fea-
tures specialized for detection of topic abnormal-
ity resulted in the low recall of the A/S-filter. The
combination of the two features achieved a slight
improvement: the F-score was 0.66 and it was 0.03
higher than the Similarity-filter.
In Metzler et al. (2005)?s study, the system us-
ing both sentence-based features and document-
based features did not achieve further improve-
ment over the system based on the document-
based features alone. In order to explore the im-
pact of chunk-based features, similarity features
were classified into two groups (chunk-based fea-
tures vs. document-based features), and two fil-
ters were trained using each group separately. Ta-
ble 5 compares the performance of the two filters
(Similarity-chunk and Similarity-doc) with the fil-
ter using all similarity features (Similarity).
acc. prec. recall fscore
Similarity-
chunk
0.972 0.700 0.442 0.542
Similarity-
doc
0.971 0.730 0.396 0.514
Similarity 0.975 0.731 0.548 0.626
Table 5: Comparison of chunk-based and
document-based similarity features
In this study, the chunk-based features were
comparable to the document-based features. Fur-
thermore, combination of the two features im-
proved F-score. The performance improvement
mostly resulted from higher recall.
Finally, Table 6 presents the results using the
entire FM set, including the OR, TR, and RE re-
sponses that were not included in the previous
experiment. The accuracy of the majority class
baseline (classifying all responses as scorable re-
sponses) is 0.865.
acc. prec. recall fscore
Similarity-
filter
0.976 0.926 0.895 0.910
A/S-filter 0.974 0.953 0.849 0.898
Combined-
filter
0.977 0.941 0.884 0.911
Table 6: Performance of filters in all types of NS
detection
Both the Similarity-filter and the A/S-
filter achieved high performance. Both accuracies
and F-scores were similar and the difference
121
between the two filters was approximately 0.01.
The Similarity-filter achieved better performance
than the A/S-filter in recall: it was 0.89, which
was substantially higher than the A/S-filter (0.85).
It is an encouraging result that the Similarity-
filter could achieve a performance comparable
to the A/S-filter, which was based on multi-
ple resources such as signal processing, forced-
alignment, and ASR. But, the combination of the
two feature groups did not achieve further im-
provement: the increase in both accuracy and F-
measure was less than 0.01.
7 Conclusions
In this study, filtering models were implemented
as a supplementary module for an automated
speech proficiency scoring system. In addition to
A/S features, which have shown promising perfor-
mance in previous studies, a set of similarity fea-
tures were implemented and a filtering model was
developed. The Similarity-filter was more accu-
rate than the A/S-filter in identifying the responses
with topical problems. This result is encouraging
since the proportion of these responses is likely to
increase when the automated speech scoring sys-
tem becomes a sole rater of the assessment.
Although the Similarity-filter achieved better
performance than the A/S-filter, it should be fur-
ther improved. The recall of the system was low,
and approximately 45% of NS responses could
not be identified. In addition, the model requires
substantial amount of sample responses for each
item, and it will cause serious difficulty when it is
used the real test situation. In future, we will ex-
plore the similarity features trained only using the
prompt question or the additional prompt materi-
als such as visual and audio materials.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technol-
ogy, Learning, and Assessment, 4(3).
Paul Boersma. 2002. Praat, a system for doing phonet-
ics by computer. Glot International, 5(9/10):341?
345.
Sergey Brin, James Davis, and Hector Garcia-Molina.
1995. Copy detection mechanisms for digital docu-
ments. In ACM SIGMOD Record, volume 24, pages
398?409. ACM.
Jian Cheng and Jianqiang Shen. 2011. Off-topic detec-
tion in automated speech assessment applications.
In Proceedings of InterSpeech, pages 1597?1600.
IEEE.
Peter W. Foltz, Darrell Laham, and Thomas K. Lan-
dauer. 1999. The Intelligent Essay Assessor: Appli-
cations to educational technology. Interactive mul-
timedia Electronic Journal of Computer-Enhanced
Learning, 1(2).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineer-
ing, 12(02):145?159.
Timothy C Hoad and Justin Zobel. 2003. Meth-
ods for identifying versioned and plagiarized doc-
uments. Journal of the American society for infor-
mation science and technology, 54(3):203?215.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of the InterSpeech, pages 1275?1278.
Wai-Kit Lo, Alissa M Harrison, and Helen Meng.
2010. Statistical phone duration modeling to filter
for intact utterances in a computer-assisted pronun-
ciation training system. In Proceedings of Acous-
tics Speech and Signal Processing (ICASSP), 2010
IEEE International Conference on, pages 5238?
5241. IEEE.
Donald Metzler, Yaniv Bernstein, W Bruce Croft, Al-
istair Moffat, and Justin Zobel. 2005. Similarity
measures for tracking information flow. In Proceed-
ings of the 14th ACM international conference on In-
formation and knowledge management, pages 517?
524. ACM.
Mark Sanderson. 1997. Duplicate detection in the
reuters collection. ? Technical Report (TR-1997-5)
of the Department of Computing Science at the Uni-
versity of Glasgow G12 8QQ, UK?.
Narayanan Shivakumar and Hector Garcia-Molina.
1995. Scam: A copy detection mechanism for digi-
tal documents.
Narayanan Shivakumar and Hector Garcia-Molina.
1996. Building a scalable and accurate copy detec-
tion mechanism. In Proceedings of the first ACM
international conference on Digital libraries, pages
160?168. ACM.
Joost van Doremalen, Helmet Strik, and Cartia Cuc-
chiarini. 2009. Utterance verification in language
learning applications. In Proceedings of the SLaTE.
122
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111. Association for Computa-
tional Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
123
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
