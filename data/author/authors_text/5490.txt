Building a Sense Tagged Corpus with
Open Mind Word Expert
Timothy Chklovski
Artificial Intelligence Laboratory
Massachusetts Institute of Technology
timc@mit.edu
Rada Mihalcea
Department of Computer Science
University of Texas at Dallas
rada@utdallas.edu
Abstract
Open Mind Word Expert is an imple-
mented active learning system for col-
lecting word sense tagging from the
general public over the Web. It is avail-
able at http://teach-computers.org. We
expect the system to yield a large vol-
ume of high-quality training data at a
much lower cost than the traditional
method of hiring lexicographers. We
thus propose a Senseval-3 lexical sam-
ple activity where the training data is
collected via Open Mind Word Expert.
If successful, the collection process can
be extended to create the definitive cor-
pus of word sense information.
1 Introduction
Most of the efforts in the Word Sense Disam-
biguation (WSD) field have concentrated on su-
pervised learning algorithms. These methods usu-
ally achieve the best performance at the cost of
low recall. The main weakness of these meth-
ods is the lack of widely available semantically
tagged corpora and the strong dependence of dis-
ambiguation accuracy on the size of the training
corpus. The tagging process is usually done by
trained lexicographers, and consequently is quite
expensive, limiting the size of such corpora to a
handful of tagged texts.
This paper introduces Open Mind Word Ex-
pert, a Web-based system that aims at creating
large sense tagged corpora with the help of Web
users. The system has an active learning compo-
nent, used for selecting the most difficult exam-
ples, which are then presented to the human tag-
gers. We expect that the system will yield more
training data of comparable quality and at a sig-
nificantly lower cost than the traditional method
of hiring lexicographers.
Open Mind Word Expert is a newly born project
that follows the Open Mind initiative (Stork,
1999). The basic idea behind Open Mind is to
use the information and knowledge that may be
collected from the existing millions of Web users,
to the end of creating more intelligent software.
This idea has been used in Open Mind Common
Sense, which acquires commonsense knowledge
from people. A knowledge base of about 400,000
facts has been built by learning facts from 8,000
Web users, over a one year period (Singh, 2002).
If Open Mind Word Expert experiences a similar
learning rate, we expect to shortly obtain a cor-
pus that exceeds the size of all previously tagged
data. During the first fifty days of activity, we col-
lected about 26,000 tagged examples without sig-
nificant efforts for publicizing the site. We expect
this rate to gradually increase as the site becomes
more widely known and receives more traffic.
2 Sense Tagged Corpora
The availability of large amounts of semanti-
cally tagged data is crucial for creating successful
WSD systems. Yet, as of today, only few sense
tagged corpora are publicly available.
One of the first large scale hand tagging efforts
is reported in (Miller et al, 1993), where a subset
of the Brown corpus was tagged with WordNet
                   July 2002, pp. 116-122.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
senses. The corpus includes a total of 234,136
tagged word occurrences, out of which 186,575
are polysemous. There are 88,058 noun occur-
rences of which 70,214 are polysemous.
The next significant hand tagging task was re-
ported in (Bruce and Wiebe, 1994), where 2,476
usages of interest were manually assigned with
sense tags from the Longman Dictionary of Con-
temporary English (LDOCE). This corpus was
used in various experiments, with classification
accuracies ranging from 75% to 90%, depending
on the algorithm and features employed.
The high accuracy of the LEXAS system (Ng
and Lee, 1996) is due in part to the use of large
corpora. For this system, 192,800 word occur-
rences have been manually tagged with senses
from WordNet. The set of tagged words consists
of the 191 most frequently occurring nouns and
verbs. The authors mention that approximately
one man-year of effort was spent in tagging the
data set.
Lately, the SENSEVAL competitions provide a
good environment for the development of su-
pervised WSD systems, making freely available
large amounts of sense tagged data for about
100 words. During SENSEVAL-1 (Kilgarriff and
Palmer, 2000), data for 35 words was made avail-
able adding up to about 20,000 examples tagged
with respect to the Hector dictionary. The size
of the tagged corpus increased with SENSEVAL-2
(Kilgarriff, 2001), when 13,000 additional exam-
ples were released for 73 polysemous words. This
time, the semantic annotations were performed
with respect to WordNet.
Additionally, (Kilgarriff, 1998) mentions the
Hector corpus, which comprises about 300 word
types with 300-1000 tagged instances for each
word, selected from a 17 million word corpus.
Sense tagged corpora have thus been central to
accurate WSD systems. Estimations made in (Ng,
1997) indicated that a high accuracy domain inde-
pendent system for WSD would probably need a
corpus of about 3.2 million sense tagged words.
At a throughput of one word per minute (Ed-
monds, 2000), this would require about 27 man-
years of human annotation effort.
With Open Mind Word Expert we aim at creat-
ing a very large sense tagged corpus, by making
use of the incredible resource of knowledge con-
stituted by the millions of Web users, combined
with techniques for active learning.
3 Open Mind Word Expert
Open Mind Word Expert is a Web-based interface
where users can tag words with their WordNet
senses. Tagging is organized by word. That is,
for each ambiguous word for which we want to
build a sense tagged corpus, users are presented
with a set of natural language (English) sentences
that include an instance of the ambiguous word.
Initially, example sentences are extracted from
a large textual corpus. If other training data is
not available, a number of these sentences are pre-
sented to the users for tagging in Stage 1. Next,
this tagged collection is used as training data, and
active learning is used to identify in the remaining
corpus the examples that are ?hard to tag?. These
are the examples that are presented to the users for
tagging in Stage 2. For all tagging, users are asked
to select the sense they find to be the most appro-
priate in the given sentence, from a drop-down
list that contains all WordNet senses, plus two
additional choices, ?unclear? and ?none of the
above?. The results of any automatic classifica-
tion or the classification submitted by other users
are not presented so as to not bias the contrib-
utor?s decisions. Based on early feedback from
both researchers and contributors, a future version
of Open Mind Word Expert may allow contribu-
tors to specify more than one sense for any word.
A prototype of the system has been imple-
mented and is available at http://www.teach-
computers.org. Figure 1 shows a screen shot from
the system interface, illustrating the screen pre-
sented to users when tagging the noun ?child?.
3.1 Data
The starting corpus we use is formed by a mix
of three different sources of data, namely the
Penn Treebank corpus (Marcus et al, 1993), the
Los Angeles Times collection, as provided during
TREC conferences1 , and Open Mind Common
Sense2, a collection of about 400,000 common-
sense assertions in English as contributed by vol-
unteers over the Web. A mix of several sources,
each covering a different spectrum of usage, is
1http://trec.nist.gov
2http://commonsense.media.mit.edu
Figure 1: Screen shot from Open Mind Word Expert
used to increase the coverage of word senses and
writing styles. While the first two sources are well
known to the NLP community, the Open Mind
Common Sense constitutes a fairly new textual
corpus. It consists mostly of simple single sen-
tences. These sentences tend to be explanations
and assertions similar to glosses of a dictionary,
but phrased in a more common language and with
many sentences per sense. For example, the col-
lection includes such assertions as ?keys are used
to unlock doors?, and ?pressing a typewriter key
makes a letter?. We believe these sentences may
be a relatively clean source of keywords that can
aid in disambiguation. For details on the data and
how it has been collected, see (Singh, 2002).
3.2 Active Learning
To minimize the amount of human annotation ef-
fort needed to build a tagged corpus for a given
ambiguous word, Open Mind Word Expert in-
cludes an active learning component that has the
role of selecting for annotation only those exam-
ples that are the most informative.
According to (Dagan et al, 1995), there are two
main types of active learning. The first one uses
memberships queries, in which the learner con-
structs examples and asks a user to label them. In
natural language processing tasks, this approach
is not always applicable, since it is hard and
not always possible to construct meaningful un-
labeled examples for training. Instead, a second
type of active learning can be applied to these
tasks, which is selective sampling. In this case,
several classifiers examine the unlabeled data and
identify only those examples that are the most in-
formative, that is the examples where a certain
level of disagreement is measured among the clas-
sifiers.
We use a simplified form of active learning
with selective sampling, where the instances to be
tagged are selected as those instances where there
is a disagreement between the labels assigned by
two different classifiers. The two classifiers are
trained on a relatively small corpus of tagged data,
which is formed either with (1) Senseval training
examples, in the case of Senseval words, or (2)
examples obtained with the Open Mind Word Ex-
pert system itself, when no other training data is
available.
The first classifier is a Semantic Tagger with
Active Feature Selection (STAFS). This system
(previously known as SMUls) is one of the top
ranked systems in the English lexical sample task
at SENSEVAL-2. The system consists of an in-
stance based learning algorithm improved with
a scheme for automatic feature selection. It re-
lies on the fact that different sets of features
have different effects depending on the ambigu-
ous word considered. Rather than creating a gen-
eral learning model for all polysemous words,
STAFS builds a separate feature space for each
individual word. The features are selected from a
pool of eighteen different features that have been
previously acknowledged as good indicators of
word sense, including: part of speech of the am-
biguous word itself, surrounding words and their
parts of speech, keywords in context, noun be-
fore and after, verb before and after, and others.
An iterative forward search algorithm identifies
at each step the feature that leads to the highest
cross-validation precision computed on the train-
ing data. More details on this system can be found
in (Mihalcea, 2002b).
The second classifier is a COnstraint-BAsed
Language Tagger (COBALT). The system treats
every training example as a set of soft constraints
on the sense of the word of interest. WordNet
glosses, hyponyms, hyponym glosses and other
WordNet data is also used to create soft con-
straints. Currently, only ?keywords in context?
type of constraint is implemented, with weights
accounting for the distance from the target word.
The tagging is performed by finding the sense that
minimizes the violation of constraints in the in-
stance being tagged. COBALT generates confi-
dences in its tagging of a given instance based on
how much the constraints were satisfied and vio-
lated for that instance.
Both taggers use WordNet 1.7 dictionary
glosses and relations. The performance of the two
systems and their level of agreement were eval-
uated on the Senseval noun data set. The two
systems agreed in their classification decision in
54.96% of the cases. This low agreement level
is a good indication that the two approaches are
fairly orthogonal, and therefore we may hope for
high disambiguation precision on the agreement
Precision
System (fine grained) (coarse grained)
STAFS 69.5% 76.6%
COBALT 59.2% 66.8%
STAFS   COBALT 82.5% 86.3%
STAFS - STAFS   COBALT 52.4% 63.3%
COBALT - STAFS   COBALT 30.09% 42.07%
Table 1: Disambiguation precision for the two in-
dividual classifiers and their agreement and dis-
agreement sets
set. Indeed, the tagging accuracy measured on
the set where both COBALT and STAFS assign
the same label is 82.5%, a figure that is close
to the 85.5% inter-annotator agreement measured
for the SENSEVAL-2 nouns (Kilgarriff, 2002).
Table 1 lists the precision for the agreement
and disagreement sets of the two taggers. The
low precision on the instances in the disagreement
set justifies referring to these as ?hard to tag?. In
Open Mind Word Expert, these are the instances
that are presented to the users for tagging in the
active learning stage.
3.3 Ensuring Quality
Collecting from the general public holds the
promise of providing much data at low cost. It
also makes attending to two aspects of data col-
lection more important: (1) ensuring contribution
quality, and (2) making the contribution process
engaging to the contributors.
We have several steps already implemented and
have additional steps we propose to ensure qual-
ity.
First, redundant tagging is collected for each
item. Open Mind Word Expert currently uses the
following rules in presenting items to volunteer
contributors:
 Two tags per item. Once an item has two
tags associated with it, it is not presented for
further tagging.
 One tag per item per contributor. We allow
contributors to submit tagging either anony-
mously or having logged in. Anonymous
contributors are not shown any items already
tagged by contributors (anonymous or not)
from the same IP address. Logged in con-
tributors are not shown items they have al-
ready tagged.
Second, inaccurate sessions will be discarded.
This can be accomplished in two ways, roughly
by checking agreement and precision:
 Using redundancy of tags collected for each
item, any given session (a tagging done all
in one sitting) will be checked for agreement
with tagging of the same items collected out-
side of this session.
 If necessary, the precision of a given contrib-
utor with respect to a preexisting gold stan-
dard (such as SemCor or Senseval training
data) can be estimated directly by presenting
the contributor with examples from the gold
standard. This will be implemented if there
are indications of need for this in the pilot;
it will help screen out contributors who, for
example, always select the first sense (and
are in high agreement with other contribu-
tors who do the same).
In all, automatic assessment of the quality of
tagging seems possible, and, based on the ex-
perience of prior volunteer contribution projects
(Singh, 2002), the rate of maliciously misleading
or incorrect contributions is surprisingly low.
Additionally, the tagging quality will be esti-
mated by comparing the agreement level among
Web contributors with the agreement level that
was already measured in previous sense tagging
projects. An analysis of the semantic annotation
task performed by novice taggers as part of the
SemCor project (Fellbaum et al, 1997) revealed
an agreement of about 82.5% among novice tag-
gers, and 75.2% among novice taggers and lexi-
cographers.
Moreover, since we plan to use paid, trained
taggers to create a separate test corpus for each
of the words tagged with Open Mind Word Ex-
pert, these same paid taggers could also validate
a small percentage of the training data for which
no gold standard exists.
3.4 Engaging the Contributors
We believe that making the contribution process
as engaging and as ?game-like? for the contrib-
utors as possible is crucial to collecting a large
volume of data. With that goal, Open Mind Word
Expert tracks, for each contributor, the number of
items tagged for each topic. When tagging items,
a contributor is shown the number of items (for
this word) she has tagged and the record number
of items tagged (for this word) by a single user.
If the contributor sets a record, it is recognized
with a congratulatory message on the contribution
screen, and the user is placed in the Hall of Fame
for the site. Also, the user can always access a
real-time graph summarizing, by topic, their con-
tribution versus the current record for that topic.
Interestingly, it seems that relatively sim-
ple word games can enjoy tremendous
user acceptance. For example, WordZap
(http://wordzap.com), a game that pits players
against each other or against a computer to be the
first to make seven words from several presented
letters (with some additional rules), has been
downloaded by well over a million users, and
the reviewers describe the game as ?addictive?.
If sense tagging can enjoy a fraction of such
popularity, very large tagged corpora will be
generated.
Additionally, NLP instructors can use the site
as an aid in teaching lexical semantics. An in-
structor can create an ?activity code?, and then,
for users who have opted in as participants of that
activity (by entering the activity code when cre-
ating their profiles), access the amount tagged by
each participant, and the percentage agreement of
the tagging of each contributor who opted in for
this activity. Hence, instructors can assign Open
Mind Word Expert tagging as part of a homework
assignment or a test.
Also, assuming there is a test set of already
tagged examples for a given ambiguous word, we
may add the capability of showing the increase
in disambiguation precision on the test set, as it
results from the samples that a user is currently
tagging.
4 Proposed Task for SENSEVAL-3
The Open Mind Word Expert system will be used
to build large sense tagged corpora for some of
the most frequent ambiguous words in English.
The tagging will be collected over the Web from
volunteer contributors. We propose to organize a
task in SENSEVAL-3 where systems will disam-
biguate words using the corpus created with this
system.
We will initially select a set of 100 nouns,
and collect for each of them  	
 tagged
samples (Edmonds, 2000), where  is the num-
ber of senses of the noun. It is worth mention-
ing that, unlike previous SENSEVAL evaluations,
where multi-word expressions were considered
as possible senses for an constituent ambiguous
word, we filter these expressions apriori with an
automatic tool for collocation extraction. There-
fore, the examples we collect refer only to single
ambiguous words, and hence we expect a lower
inter-tagger agreement rate and lower WSD tag-
ging precision when only single words are used,
since usually multi-word expressions are not am-
biguous and they constitute some of the ?easy
cases? when doing sense tagging.
These initial set of tagged examples will then
be used to train the two classifiers described in
Section 3.2, and annotate an additional set of
 
examples. From these, the users will be
presented only with those examples where there
is a disagreement between the labels assigned by
the two classifiers. The final corpus for each am-
biguous word will be created with (1) the original
set of
 	

tagged examples, plus (2) the
examples selected by the active learning compo-
nent, sense tagged by users.
Words will be selected based on their frequen-
cies, as computed on SemCor. Once the tag-
ging process of the initial set of 100 words is
completed, additional nouns will be incremen-
tally added to the Open Mind Word Expert inter-
face. As we go along, words with other parts of
speech will be considered as well.
To enable comparison with Senseval-2, the set
of words will also include the 29 nouns used in
the Senseval-2 lexical sample tasks. This would
allow us to assess how much the collected data
helps on the Senseval-2 task.
As shown in Section 3.3, redundant tags will be
collected for each item, and overall quality will be
assessed. Moreover, starting with the initial set of
 	

examples labeled for each word, we
will create confusion matrices that will indicate
the similarity between word senses, and help us
create the sense mappings for the coarse grained
evaluations.
One of the next steps we plan to take is to re-
place the ?two tags per item? scheme with the
?tag until at least two tags agree? scheme pro-
posed and used during the SENSEVAL-2 tagging
(Kilgarriff, 2002). Additionally, the set of mean-
ings that constitute the possible choices for a cer-
tain ambiguous example will be enriched with
groups of similar meanings, which will be de-
termined either based on some apriori provided
sense mappings (if any available) or based on the
confusion matrices mentioned above.
For each word with sense tagged data created
with Open Mind Word Expert, a test corpus will
be built by trained human taggers, starting with
examples extracted from the corpus mentioned in
Section 3.1. This process will be set up indepen-
dently of the Open Mind Word Expert Web in-
terface. The test corpus will be released during
SENSEVAL-3.
5 Conclusions and future work
Open Mind Word Expert pursues the poten-
tial of creating a large tagged corpus. WSD
can also benefit in other ways from the Open
Mind approach. We are considering using a
AutoASC/GenCor type of approach to generate
sense tagged data with a bootstrapping algorithm
(Mihalcea, 2002a). Web contributors can help
this process by creating the initial set of seeds,
and exercising control over the quality of the
automatically generated seeds.
Acknowledgments
We would like to thank the Open Mind Word Ex-
pert contributors who are making all this work
possible. We are also grateful to Adam Kilgar-
riff for valuable suggestions and interesting dis-
cussions, to Randall Davis and to the anonymous
reviewers for useful comments on an earlier ver-
sion of this paper, and to all the Open Mind Word
Expert users who have emailed us with their feed-
back and suggestions, helping us improve this ac-
tivity.
References
R. Bruce and J. Wiebe. 1994. Word sense disam-
biguation using decomposable models. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-94), pages
139?146, LasCruces, NM, June.
I. Dagan, , and S.P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers.
In International Conference on Machine Learning,
pages 150?157.
P. Edmonds. 2000. Designing a task for
Senseval-2, May. Available online at
http://www.itri.bton.ac.uk/events/senseval.
C. Fellbaum, J. Grabowski, and S. Landes. 1997.
Analysis of a hand-tagging task. In Proceedings
of ANLP-97 Workshop on Tagging Text with Lexi-
cal Semantics: Why, What, and How?, Washington
D.C.
A. Kilgarriff and M. Palmer, editors. 2000. Com-
puter and the Humanities. Special issue: SENSE-
VAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
A. Kilgarriff. 1998. Gold standard datasets for eval-
uating word sense disambiguation programs. Com-
puter Speech and Language, 12(4):453?472.
A. Kilgarriff, editor. 2001. SENSEVAL-2, Toulouse,
France, November.
A. Kilgarriff. 2002. English lexical sample task de-
scription. In Proceedings of Senseval-2, ACL Work-
shop.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguis-
tics, 19(2):313?330.
R. Mihalcea. 2002a. Bootstrapping large sense tagged
corpora. In Proceedings of the Third International
Conference on Language Resources and Evaluation
LREC 2002, Canary Islands, Spain, May. (to ap-
pear).
R. Mihalcea. 2002b. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING-ACL 2002), Taipei, Taiwan, August. (to
appear).
G. Miller, C. Leacock, T. Randee, and R. Bunker.
1993. A semantic concordance. In Proceedings
of the 3rd DARPA Workshop on Human Language
Technology, pages 303?308, Plainsboro, New Jer-
sey.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
examplar-based approach. In Proceedings of the
34th Annual Meeting of the Association for Com-
putational Linguistics (ACL-96), Santa Cruz.
H.T. Ng. 1997. Getting serious about word sense dis-
ambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?, pages 1?7, Washington.
P. Singh. 2002. The public acquisition of common-
sense knowledge. In Proceedings of AAAI Spring
Symposium: Acquiring (and Using) Linguistic (and
World) Knowledge for Information Access., Palo
Alto, CA. AAAI.
D. Stork. 1999. The Open Mind initiative. IEEE Ex-
pert Systems and Their Applications, 14(3):19?20.
 	
PageRank on Semantic Networks,
with Application to Word Sense Disambiguation
Rada Mihalcea, Paul Tarau, Elizabeth Figa
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu, tarau@unt.edu, efiga@unt.edu
Abstract
This paper presents a new open text word sense
disambiguation method that combines the use
of logical inferences with PageRank-style algo-
rithms applied on graphs extracted from natu-
ral language documents. We evaluate the ac-
curacy of the proposed algorithm on several
sense-annotated texts, and show that it consis-
tently outperforms the accuracy of other pre-
viously proposed knowledge-based word sense
disambiguation methods. We also explore and
evaluate methods that combine several open-text
word sense disambiguation algorithms.
1 Introduction
Google?s PageRank link-analysis algorithm (Brin
and Page, 1998), and variants like Kleinberg?s HITS
algorithm (Kleinberg, 1999), have been used for an-
alyzing the link-structure of the World Wide Web
to provide global, content independent ranking of
Web pages. Arguably, PageRank can be singled
out as a key element of the paradigm-shift Google
has triggered in the field of Web search technol-
ogy, by providing a Web page ranking mechanism
that relies on the collective knowledge of Web ar-
chitects rather than content analysis of individual
Web pages. In short, PageRank is a way of decid-
ing on the importance of a vertex within a graph, by
taking into account global information recursively
computed from the entire graph, rather than relying
only on local vertex-specific information. Apply-
ing a similar line of thinking to lexical and semantic
knowledge graphs like WordNet (Miller, 1995) sug-
gests using the implicit knowledge incorporated in
their link structure for language processing applica-
tions, where knowledge drawn from an entire text
can be used in making local ranking/selection deci-
sions.
In this paper, we explore the applicability of
PageRank to semantic networks, and show that such
graph-based ranking algorithms can be successfully
used in language processing applications. In partic-
ular, we propose and experiment with a new unsu-
pervised knowledge-based word sense disambigua-
tion algorithm, which succeeds in identifying the
sense of all words in open text with a precision
significantly higher than other previously proposed
knowledge-based algorithms.
The paper is organized as follows. Section 2 re-
views the problem of word sense disambiguation,
and surveys related work. Section 3 briefly describes
the PageRank algorithm, and shows how this algo-
rithm can be adapted to the WordNet graph. Sec-
tion 4 introduces the PageRank-based word sense
disambiguation algorithm. Combinations with other
known algorithms are explored in Section 5. A
thorough empirical evaluation of the proposed algo-
rithms on several sense-annotated texts is provided
in section 6.
2 Open Text Word Sense Disambiguation
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a poly-
semous word within a given context. Applications
such as machine translation, knowledge acquisition,
common sense reasoning, and others, require knowl-
edge about word meanings, and word sense disam-
biguation is considered essential for all these appli-
cations.
Most of the efforts in solving this problem
were concentrated so far toward targeted supervised
learning, where each sense tagged occurrence of a
particular word is transformed into a feature vector,
which is then used in an automatic learning process.
The applicability of such supervised algorithms is
however limited only to those few words for which
sense tagged data is available, and their accuracy
is strongly connected to the amount of labeled data
available at hand.
Instead, open-text knowledge-based approaches
have received significantly less attention1 . While the
performance of such methods is usually exceeded by
their supervised corpus-based alternatives, they have
however the advantage of providing larger coverage.
1We use the term knowledge-based to denote methods that
involve logical inferences and derivation of global properties
that extend the data in a dictionary and/or a corpus with new
knowledge. In our definition of knowledge-based approaches,
the use of a corpus is not excluded.
Knowledge-based methods for word sense disam-
biguation are usually applicable to all words in open
text, while supervised corpus-based techniques tar-
get only few selected words for which large corpora
are made available. Four main types of knowledge-
based methods have been developed so far for word
sense disambiguation.
Lesk algorithms. First introduced by (Lesk,
1986), these algorithms attempt to identify the most
likely meanings for the words in a given context
based on a measure of contextual overlap between
the dictionary definitions of the ambiguous words,
or between the current context and dictionary defi-
nitions provided for a given target word.
Semantic similarity. Measures of semantic simi-
larity computed on semantic networks (Rada et al,
1989). Depending on the size of the context they
span, these measures are in turn divided into two
main categories:
(1) Local context ? where the semantic measures are
used to disambiguate words additionally connected
by syntactic relations (Stetina et al, 1998).
(2) Global context ? where the semantic measures
are employed to derive lexical chains, which are
threads of meaning often drawn throughout an en-
tire text (Morris and Hirst, 1991).
Selectional preferences. Automatically or semi-
automatically acquired selectional preferences, as
means for constraining the number of possible
senses that a word might have, based on the relation
it has with other words in context (Resnik, 1997).
Heuristic-based methods. These methods consist
of simple rules that can reliably assign a sense to
certain word categories: one sense per collocation
(Yarowsky, 1993), and one sense per discourse (Gale
et al, 1992).
In this paper, we propose a new open-text dis-
ambiguation algorithm that combines information
drawn from a semantic network (WordNet) with
graph-based ranking algorithms (PageRank). We
compare our method with other open-text word
sense disambiguation algorithms, and show that the
accuracy achieved through our new PageRank-based
method exceeds the performance obtained by other
knowledge-based methods.
3 PageRank on Semantic Networks
In this section, we briefly describe PageRank (Brin
and Page, 1998), and describe the view of WordNet
as a graph, which facilitates the application of the
graph-based ranking algorithm on this semantic net-
work.
3.1 The PageRank Algorithm
Iterative graph-based ranking algorithms are essen-
tially a way of deciding the importance of a vertex
within a graph; in the context of search engines, it
is a way of deciding how important a page is on the
Web. In this model, when one vertex links to another
one, it is casting a vote for that other vertex. The
higher the number of votes that are cast for a vertex,
the higher the importance of the vertex. Moreover,
the importance of the vertex casting the vote deter-
mines how important the vote itself is, and this in-
formation is also taken into account by the ranking
model. Hence, the score associated with a vertex is
determined based on the votes that are cast for it, and
the score of the vertices casting these votes.
Let G = (V,E) be a directed graph with the
set of vertices V and set of edges E, where E is a
subset of V ? V . For a given vertex Vi, let In(Vi)
be the set of vertices that point to it, and let Out(Vi)
be the set of edges going out of vertex Vi. The
PageRank score of vertex Vi is defined as follows:
S(Vi) = (1 ? d) + d ?
?
j?In(Vi)
S(Vj)
|Out(Vj)|
where d is a damping factor that can be set between
0 and 1 2.
Starting from arbitrary values assigned to each
node in the graph, the PageRank computation it-
erates until convergence below a given threshold
is achieved. After running the algorithm, a fast
in-place sorting algorithm is applied to the ranked
graph vertices to sort them in decreasing order.
PageRank can be also applied on undirected
graphs, in which case the out-degree of a vertex is
equal to the in-degree of the vertex, and convergence
is usually achieved after a fewer number of itera-
tions.
3.2 WordNet as a Graph
WordNet is a lexical knowledge base for English
that defines words, meanings, and relations between
them. The basic unit in WordNet is a synset, which
is a set of synonym words or word phrases, and
represents a concept. WordNet defines several se-
mantic relations between synsets, including ISA
relations (hypernym/hyponym), PART-OF relations
(meronym/holonym), entailment, and others.
To represent WordNet as a graph, we use an
instance-centric data representation, which defines
2The role of the damping factor d is to incorporate into the
PageRank model the probability of jumping from a given ver-
tex to another random vertex in the graph. In the context of
Web surfing, PageRank implements the ?random surfer model?,
where a user clicks on links at random with a probability d, and
jumps to a completely new page with probability 1 ? d. The
factor d is usually set at 0.85 (Brin and Page, 1998), and this is
the value we are also using in our implementation.
synsets as vertices, and relations or sets of relations
as edges. The graph can be constructed as an undi-
rected graph, with no orientation defined for edges,
or as a directed graph, in which case a direction is ar-
bitrarily established for each relation (e.g. hyponym
? hypernym).
Given a subset of the WordNet synsets, as iden-
tified in a given text or by other selectional crite-
ria, and given a semantic relation, a graph is con-
structed by identifying all the synsets (vertices) in
the given subset that can be linked by the given rela-
tion (edges). Relations can be also combined, for in-
stance a graph can be constructed so that it accounts
for both the ISA and the PART-OF relations between
the vertices in the graph.
4 PageRank-based Word Sense
Disambiguation
In this section, we describe a new unsupervised
open-text word sense disambiguation algorithm that
relies on PageRank-style algorithms applied on se-
mantic networks.
4.1 Building the Text Synset Graph
To enable the application of PageRank-style algo-
rithms to the disambiguation of all words in open
text, we have to build a graph that represents the text
and interconnects the words with meaningful rela-
tions.
Since no a-priori semantic information is avail-
able for the words in the text, we start with the as-
sumption that every possible sense of a word is a
potentially correct sense, and therefore all senses for
all words are to be included in the initial search set.
The synsets pertaining to all word senses form there-
fore the vertices of the graph. The edges between the
nodes are drawn using synset relations available in
WordNet, either explicitly encoded in the network,
or derived by various means (see Sections 4.2, 4.3).
Note that not all WordNet arcs are suitable for
combination with PageRank, as they sometimes
identify competing word senses which tend to share
targets of incoming or outgoing links. As our ob-
jective is to differentiate between senses, we want to
focus on specific rather than shared links. We call
two synsets colexical if they represent two senses of
the same word ? that is, if they share one identical
lexical unit. For a given word or word phrase, colex-
ical synsets will be listed as competing senses, from
which a given disambiguation algorithm should se-
lect one.
To ensure that colexical synsets do not ?contam-
inate? each other?s PageRank values, we have to
make sure that they are not linked together, and
hence they compete through disjoint sets of links.
This means that relations between synsets pertaining
to various senses of the same word or word phrase
are not added to the graph. Consider for instance
the verb travel: it has six senses defined in Word-
Net, with senses 2 and 3 linked by an ISA relation
(travel#2 ISA travel#3). Since the synsets pertain-
ing to these two senses are colexical (they share the
lexical unit travel), this ISA link is not added to the
text graph.
4.2 Basic Semantic Relations
WordNet explicitly encodes a set of basic se-
mantic relations, including hypernymy, hyponymy,
meronymy, holonymy, entailment, causality, at-
tribute, pertainimy. WordNet 2.0 has also introduced
nominalizations ? which link verbs and nouns per-
taining to the same semantic class, and domain links
? a first step toward the classification of synsets,
based on the ?ontology? in which a given synset is
relevant to. While the domain relations usually add
a small number of links, their use tends to help fo-
cusing on a dominant field which was observed to
help the disambiguation process.
4.3 Derived Semantic Relations
Two or more basic WordNet relations can be com-
bined together to form a new relation. For in-
stance, we can combine hypernymy and hyponymy
to obtain the coordinate relation ? which identifies
synsets that share the same hypernym. For example,
dog#1 and wolf#1 are coordinates, since they share
the same hypernym canine#1.
It is worth mentioning the composite relation
xlink, which is a new global relation that we define,
which integrates all the basic relations (nominaliza-
tions and domain links included) and the coordinate
relation. Shortly, two synsets are connected by an
xlink relation if any WordNet-defined relation or a
coordinate relation can be identified between them.
4.4 The PageRank Disambiguation Algorithm
The input to the disambiguation algorithm consists
of raw text. The output is a text with word mean-
ing annotations for all open-class words. Given a
semantic relation SR, which can be a basic or com-
posite relation, the algorithm consists of the follow-
ing main steps:
Step 1: Preprocessing.
During preprocessing, the text is tokenized and an-
notated with parts of speech. Collocations are iden-
tified using a sliding window approach, where a col-
location is considered to be a sequence of words
that forms a compound concept defined in WordNet.
Named entities are also identified at this stage.
Step 2: Graph construction.
Build the text synset graph: for all open class words
in the text, identify all synsets defined in Word-
Net, and add them as vertices in the graph. Words
previously assigned with a named entity tag, and
modal/auxiliary verbs are not considered. For the
given semantic relation SR, add an edge between
all vertices in the graph that can be linked by the
relation SR.
Step 3: PageRank.
Assign an initial small value to each vertex in the
graph. Iterate the PageRank computation until it
converges - usually for 25-30 iterations. In our im-
plementation, vertices are initially assigned with a
value of 1. Notice that the final values obtained af-
ter PageRank runs to completion are not affected by
the choice of the initial value, only the number of
iterations to convergence may be different.
Step 4: Assign word meanings.
For each ambiguous word in the text, find the
synset that has the highest PageRank score, which
is uniquely identifying the sense of the word. If
none of the synsets corresponding to the meanings
of a word could be connected with other synsets in
the graph using the given relation SR, the word is
assigned with a random sense (when the WordNet
sense order is not considered), or with the first sense
in WordNet (when a sense order is available).
The algorithm can be run on the entire text at
once, in which case the resulting graph is fairly large
? usually more than two thousands vertices ? and
has high connectivity. Alternatively, it can be run
on smaller sections of the text, and in this case the
graphs have lower number of vertices and lower con-
nectivity. In the experiments reported in this paper,
we are using the first option, since it results in richer
synset graphs and ensures that most of the words are
assigned a meaning using the PageRank sense dis-
ambiguation algorithm.
5 Related Algorithms
We overview in this section two other word sense
disambiguation algorithms that address all words in
open text: Lesk algorithm, and the most frequent
sense algorithm3 . We also propose two new hybrid
algorithms that combine the PageRank word sense
disambiguation method with the Lesk algorithm and
the most frequent sense algorithm.
5.1 The Lesk algorithm
The Lesk algorithm (Lesk, 1986) is one of the first
algorithms used for the semantic disambiguation of
all words in open text. The only resource required
by the algorithm is a set of dictionary entries, one for
each possible word sense, and knowledge about the
immediate context where the sense disambiguation
is performed.
3The reason for choosing these algorithms over the other
methods mentioned in section 2 is the fact that they address all
open class words in a text.
The main idea behind the original definition of
the algorithm is to disambiguate words by finding
the overlap among their sense definitions. Namely,
given two words, W1 and W2, each with NW1 and
NW2 senses defined in a dictionary, for each pos-
sible sense pair W i1 and W
j
2 , i=1..NW1, j=1..NW2,
first determine their definitions overlap, by counting
the number of words they have in common. Next,
the sense pair with the highest overlap is selected,
and consequently a sense is assigned to each of the
two words involved in the initial pair.
When applied to open text, the original defini-
tion of the algorithm faces an explosion of word
sense combinations4 , and alternative solutions are
required. One solution is to use simulated anneal-
ing, as proposed in (Cowie et al, 1992). Another
solution ? which we adopt in our experiments ? is
to use a variation of the Lesk algorithm (Kilgarriff
and Rosenzweig, 2000), where meanings of words
in the text are determined individually, by finding
the highest overlap between the sense definitions of
each word and the current context. Rather than seek-
ing to simultaneously determine the meanings of all
words in a given text, this approach determines word
senses individually, and therefore it avoids the com-
binatorial explosion of senses.
5.2 Most Frequent Sense
WordNet keeps track of the frequency of each word
meaning within a sense-annotated corpus. This
introduces an additional knowledge-element that
can significantly improve the disambiguation perfor-
mance.
A very simple algorithm that relies on this infor-
mation consists of picking the most frequent sense
for any given word as the correct one. Given that
sense frequency distributions tend to decrease expo-
nentially for less frequent senses, this guess usually
outperforms methods that use exclusively the con-
tent of the document and associated dictionary in-
formation.
5.3 Combining PageRank and Lesk
When combining two different algorithms, we have
to ensure that their effects accumulate without dis-
turbing each algorithms internal workings.
The PageRank+Lesk algorithm consists in pro-
viding a default ordering by Lesk (possibly after
shuffling WordNet senses to remove the sense fre-
quency bias), and then applying PageRank, which
4Consider for instance the text ?I saw a man who is 108
years old and can still walk and tell jokes?, with nine open class
words, each with several possible senses : see(26), man(11),
year(4), old(8), can(5), still(4), walk(10), tell(8), joke(3). Given
the total of 43,929,600 possible sense combinations, finding the
optimal combination using definition overlaps is not a tractable
approach.
Size(words) Random Lesk PageRank PageRank+Lesk
SEMCOR
law 825 37.12% 39.62% 46.42% 49.36%
sports 808 29.95% 33.00% 40.59% 46.18%
education 898 37.63% 41.33% 46.88% 52.00%
debates 799 40.17% 42.38% 47.80% 50.52%
entertainment 802 39.27% 43.05% 43.89% 49.31%
AVERAGE 826 36.82% 39.87% 45.11% 49.47%
SENSEVAL-2
d00 471 28.97% 43.94% 43.94% 47.77%
d01 784 45.47% 52.65% 54.46% 57.39%
d02 514 39.24% 49.61% 54.28% 56.42%
AVERAGE 590 37.89% 48.73% 50.89% 53.86%
AVERAGE (ALL) 740 37.22% 43.19% 47.27% 51.16%
Table 1: Word Sense Disambiguation accuracy for PageRank, Lesk, PageRank+Lesk, and Random (no sense
order)
will eventually reorder the senses. With this ap-
proach, senses that have similar PageRank values
will keep their Lesk ordering. As PageRank over-
rides Lesk one can notice that in this case we pri-
oritize PageRank, which tends to outperform Lesk.
The resulting algorithm provides a combination
which improves over both algorithms individually,
as shown in Section 6.
5.4 Combining PageRank with the Sense
Frequency
The combination of PageRank with the WordNet
sense frequency information is done in two steps:
? introduce the WordNet frequency ordering by re-
moving the random permutation of senses
? use a formula which combines PageRank and ac-
tual WordNet sense frequency information
While a simple product of the two ranks already
provides an improvement over both algorithms the
following formula which prioritizes the first sense
provides the best results:
Rank =
{ 4? FR ? PR if N = 1
FR ? PR if N > 1
where FR represents the WordNet sense frequency,
PR represents the rank computed by PageRank, N
is the position in the frequency ordered synset list,
and Rank represents the combined rank.
6 Experimental Evaluation
We evaluate the accuracy of the word sense dis-
ambiguation algorithms on a benchmark of sense-
annotated texts, in which each open-class word is
mapped to the meaning selected by a lexicographer
as being the most appropriate one in the context of
a sentence. We are using a subset of the SemCor
texts (Miller et al, 1993) ? five randomly selected
files covering different topics: news, sports, enter-
tainment, law, and debates ? as well as the data
set provided for the English all words task during
SENSEVAL-2.
The average size of a file is 600-800 open class
words. On each file, we run two sets of evaluations.
(1) One set consisting of the basic ?uninformed?
version of the knowledge-based algorithms, where
the sense ordering provided by the dictionary is not
taken into account at any point. (2) A second set of
experiments consisting of ?informed? disambigua-
tion algorithms, which incorporate the sense order
rendered by the dictionary.
6.1 Uninformed Algorithms
Given that word senses are ordered in WordNet by
decreasing frequency of their occurrence in large
sense annotated data, we explicitly remove this or-
dering by applying a random permutation of the
senses with uniform distribution. This randomiza-
tion step ensures that any eventual bias introduced
by the sense ordering is removed, and it enables us to
evaluate the impact of the disambiguation algorithm
when no information about sense frequency is avail-
able. In this setting, the following dictionary-based
algorithms are evaluated and compared: PageRank,
Lesk, combined PageRank-Lesk, and the random
baseline:
PageRank. The algorithm introduced in this paper,
which selects the most likely sense of a word based
on the PageRank score assigned to the synsets cor-
responding to the given word within the text graph.
While experiments were performed using all seman-
tic relations listed in Sections 4.2 and 4.3, we report
here on the results obtained with the xlink relation,
which was found to perform best as compared to
other semantic relations.
Lesk. We are also experimenting with the Lesk al-
gorithm described in section 5.1, which decides on
the correct sense of a word based on the highest
Size(words) MFS Lesk PageRank PageRank+Lesk
SEMCOR
law 825 69.09% 72.65% 73.21% 73.97%
sports 808 57.30% 64.21% 68.31% 68.31%
education 898 64.03% 69.33% 71.65% 71.53%
debates 799 66.33% 70.07% 71.14% 71.67%
entertainment 802 59.72% 64.98% 66.02% 66.16%
AVERAGE 826 63.24% 68.24% 70.06% 70.32%
SENSEVAL-2
d00 471 51.70% 53.07% 58.17% 57.74%
d01 784 60.80% 64.28% 67.85% 68.11%
d02 514 55.97% 62.84% 63.81% 64.39%
AVERAGE 590 56.15% 60.06% 63.27% 63.41%
AVERAGE (ALL) 740 60.58% 65.17% 67.51% 67.72%
Table 2: Word Sense Disambiguation accuracy for PageRank, Lesk, PageRank+Lesk, and Most Frequent
Sense (WordNet sense order integrated)
overlap between the dictionary sense definitions and
the context where the word occurs.
PageRank + Lesk. The PageRank and Lesk algo-
rithms can be combined into one hybrid algorithm,
as described in section 5.3. First, we order the senses
based on the score assigned by the the Lesk algo-
rithm, and then apply PageRank on this reordered
set of senses.
Random. Finally, we are running a very simple
sense annotation algorithm, which assigns a random
sense to each word in the text, and which represents
a baseline for this set of ?uninformed? word sense
disambiguation algorithms.
Table 1 lists the disambiguation precision ob-
tained by each of these algorithms on the evalua-
tion benchmark. On average, PageRank gives an ac-
curacy of 47.27%, which brings a significant 7.7%
error reduction with respect to the Lesk algorithm,
and 19.0% error reduction over the random baseline.
The best performance is achieved by a combined
PageRank and Lesk algorithm: 51.16% accuracy,
which brings a 28.5% error reduction with respect
to the random baseline. Notice that all these algo-
rithms rely exclusively on information drawn from
dictionaries, and do not require any information on
sense frequency, which makes them highly portable
to other languages.
6.2 Informed Algorithms
In a second set of experiments, we allow the dis-
ambiguation algorithms to incorporate the sense or-
der provided by WordNet. While this class of
algorithms is informed by the use of global fre-
quency information, it does not use any specific
corpus annotations and therefore it leans in gray
area between supervised and unsupervised methods.
We are again evaluating four different algorithms:
PageRank, Lesk, combined PageRank ? Lesk, and a
baseline consisting of assigning by default the most
frequent sense.
PageRank. The PageRank-based algorithm intro-
duced in this paper, combined with the WordNet
sense frequency, as described in Section 5.4.
Lesk. The Lesk algorithm described in section 5.1,
applied on an ordered set of senses. This means
that words that have two or more senses with a sim-
ilar score identified by Lesk, will keep the WordNet
sense ordering.
PageRank + Lesk. A hybrid algorithm, that com-
bines PageRank, Lesk, and the dictionary sense or-
der. This algorithm consists of the method described
in Section 5.3, applied on the ordered set of senses.
Most frequent sense. Finally, we are running a sim-
ple ?informed? sense annotation algorithm, which
assigns by default the most frequent sense to each
word in the text (i.e. sense number one in WordNet).
Table 2 lists the accuracy obtained by each of
these informed algorithms on the same benchmark.
Again, the PageRank algorithm exceeds the other
knowledge-based algorithms by a significant mar-
gin: it brings an error rate reduction of 21.3% with
respect to the most frequent sense baseline, and a
7.2% error reduction over the Lesk algorithm. Inter-
estingly, combining PageRank and Lesk under this
informed setting does not bring any significant im-
provements over the individual algorithms: 67.72%
obtained by the combined algorithm compared with
67.51% obtained with PageRank only.
6.3 Discussion
Regardless of the setting ? fully unsupervised algo-
rithms with no a-priori knowledge about sense or-
der, or informed methods where the sense order ren-
dered by the dictionary is taken into account ? the
PageRank-based word sense disambiguation algo-
rithm exceeds the baseline by a large margin, and
always outperforms the Lesk algorithm. Moreover,
a hybrid algorithm that combines the PageRank and
Lesk methods into one single algorithm is found to
improve over the individual algorithms in the first
setting, but brings no significant changes when the
sense frequency is also integrated into the disam-
biguation algorithm. This may be explained by the
fact that the additional knowledge element intro-
duced by the sense order in WordNet increases the
redundancy of information in these two algorithms
to the point where their combination cannot improve
over the individual algorithms.
The most closely related method is perhaps the
lexical chains algorithm (Morris and Hirst, 1991) ?
where threads of meaning are identified throughout a
text. Lexical chains however only take into account
possible relations between concepts in a static way,
without considering the importance of the concepts
that participate in a relation, which is recursively
determined by PageRank. Another related line of
work is the word sense disambiguation algorithm
proposed in (Veronis and Ide, 1990), where a large
neural network is built by relating words through
their dictionary definitions.
The Analogy. In the context of Web surfing,
PageRank implements the ?random surfer model?,
where a user surfs the Web by following links from
any given Web page. In the context of text meaning,
PageRank implements the concept of text cohesion
(Halliday and Hasan, 1976), where from a certain
concept C in a text, we are likely to ?follow? links
to related concepts ? that is, concepts that have a se-
mantic relation with the current concept C .
Intuitively, PageRank-style algorithms work well
for finding the meaning of all words in open text
because they combine together information drawn
from the entire text (graph), and try to identify those
synsets (vertices) that are of highest importance for
the text unity and understanding.
The meaning selected by PageRank from a set of
possible meanings for a given word can be seen as
the one most recommended by related meanings in
the text, with preference given to the ?recommen-
dations? made by most influential ones, i.e. the ones
that are in turn highly recommended by other related
meanings. The underlying hypothesis is that in a co-
hesive text fragment, related meanings tend to occur
together and form a ?Web? of semantic connections
that approximates the model humans build about a
given context in the process of discourse understand-
ing.
7 Conclusions
In this paper, we showed that iterative graph-
based ranking algorithms ? originally designed for
content-independent Web link analysis or for social
networks ? turn into a useful source of information
for natural language tasks when applied on semantic
networks. In particular, we proposed and evaluated
a new approach for unsupervised knowledge-based
word-sense disambiguation that relies on PageRank-
style algorithms applied on a WordNet-based con-
cepts graph, and showed that the accuracy achieved
through our algorithm exceeds the performance ob-
tained by other knowledge-based algorithms.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation grant IIS-0336793.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale hyper-
textual Web search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
J. Cowie, L. Guthrie, and J. Guthrie. 1992. Lexical disam-
biguation using simulated annealing. In Proceedings of the
5th International Conference on Computational Linguistics
COLING-92, pages 157?161.
W. Gale, K. Church, and D. Yarowsky. 1992. One sense per
discourse. In Proceedings of the DARPA Speech and Natural
Language Workshop, Harriman, New York.
M. Halliday and R. Hasan. 1976. Cohesion in English. Long-
man.
A. Kilgarriff and R. Rosenzweig. 2000. Framework and re-
sults for English SENSEVAL. Computers and the Humani-
ties, 34:15?48.
J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604?632.
M.E. Lesk. 1986. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from an
ice cream cone. In Proceedings of the SIGDOC Conference
1986, Toronto, June.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993. A
semantic concordance. In Proceedings of the 3rd DARPA
Workshop on Human Language Technology, pages 303?308,
Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Communication
of the ACM, 38(11):39?41.
J. Morris and G. Hirst. 1991. Lexical cohesion, the the-
saurus, and the structure of text. Computational Linguistics,
17(1):21?48.
R. Rada, H. Mili, E. Bickell, and B. Blettner. 1989. Devel-
opment and application of a metric on semantic nets. IEEE
Transactions on Systems, Man and Cybernetics, 19:17?30,
Jan/Feb.
P. Resnik. 1997. Selectional preference and sense disambigua-
tion. In Proceedings of ACL Siglex Workshop on Tagging
Text with Lexical Semantics, Why, What and How?, Wash-
ington DC, April.
J. Stetina, S. Kurohashi, and M. Nagao. 1998. General word
sense disambiguation method based on a full sentential con-
text. In Usage of WordNet in Natural Language Processing,
Proceedings of COLING-ACL Workshop, Montreal, Canada,
July.
J. Veronis and N. Ide. 1990. Word sense disambiguation with
very large neural networks extracted from machine read-
able dictionaries. In Proceedings of the 13th International
Conference on Computational Linguistics (COLING 1990),
Helsinki, Finland, August.
D. Yarowsky. 1993. One sense per collocation. In Proceedings
of the ARPA Human Language Technology Workshop.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 380?389, Prague, June 2007. c?2007 Association for Computational Linguistics
Explorations in Automatic Book Summarization
Rada Mihalcea and Hakan Ceylan
Department of Computer Science
University of North Texas
rada@cs.unt.edu, hakan@unt.edu
Abstract
Most of the text summarization research car-
ried out to date has been concerned with
the summarization of short documents (e.g.,
news stories, technical reports), and very lit-
tle work if any has been done on the sum-
marization of very long documents. In this
paper, we try to address this gap and ex-
plore the problem of book summarization.
We introduce a new data set specifically de-
signed for the evaluation of systems for book
summarization, and describe summarization
techniques that explicitly account for the
length of the documents.
1 Introduction
Books represent one of the oldest forms of written
communication and have been used since thousands
of years ago as a means to store and transmit
information. Despite this fact, given that a large
fraction of the electronic documents available online
and elsewhere consist of short texts such as Web
pages, news articles, scientific reports, and others,
the focus of natural language processing techniques
to date has been on the automation of methods tar-
geting short documents. We are witnessing however
a change: an increasingly larger number of books
become available in electronic format, in projects
such as Gutenberg (http://www.gutenberg.org),
Google Book Search (http://books.google.com),
or the Million Books project
(http://www.archive.org/details/millionbooks).
Similarly, a large number of the books published in
recent years are often available ? for purchase or
through libraries ? in electronic format. This means
that the need for language processing techniques
able to handle very large documents such as books
is becoming increasingly important.
In this paper, we address the problem of book
summarization. While there is a significant body
of research that has been carried out on the task
of text summarization, most of this work has been
concerned with the summarization of short doc-
uments, with a particular focus on news stories.
However, books are different in both length and
genre, and consequently different summarization
techniques are required. In fact, the straight-forward
application of a current state-of-the-art summariza-
tion tool leads to poor results ? a mere 0.348 F-
measure compared to the baseline of 0.325 (see the
following sections for details). This is not surprising
since these systems were developed specifically for
the summarization of short news documents.
The paper makes two contributions. First, we
introduce a new data set specifically designed for
the evaluation of book summaries. We describe
the characteristics of a new benchmark consisting
of books with manually constructed summaries, and
we calculate and provide lower and upper perfor-
mance bounds on this data set. Second, after briefly
describing a summarization system that has been
successfully used for the summarization of short
documents, we show how techniques that take into
account the length of the documents can be used to
significantly improve the performance of this sys-
tem.
2 Related Work
Automatic summarization has received a lot of atten-
tion from the natural language processing commu-
380
nity, ever since the early approaches to automatic ab-
straction that laid the foundations of the current text
summarization techniques (Luhn, 1958; Edmunson,
1969). The literature typically distinguishes be-
tween extraction, concerned with the identification
of the information that is important in the input text;
and abstraction, which involves a generation step to
add fluency to a previously compressed text (Hovy
and Lin, 1997). Most of the efforts to date have been
concentrated on the extraction step, which is perhaps
the most critical component of a successful summa-
rization algorithm, and this is the focus of our cur-
rent work as well.
To our knowledge, no research work to date was
specifically concerned with the automatic summa-
rization of books. There is, however, a large and
growing body of work concerned with the summa-
rization of short documents, with evaluations typ-
ically focusing on news articles. In particular, a
significant number of summarization systems have
been proposed during the recent Document Under-
standing Conference exercises (DUC) ? annual eval-
uations that usually draw the participation of 20?30
teams every year.
There are two main trends that can be identified
in the summarization literature: supervised systems,
that rely on machine learning algorithms trained on
pre-existing document-summary pairs, and unsuper-
vised techniques, based on properties and heuristics
derived from the text.
Among the unsupervised techniques, typical sum-
marization methods account for both the weight of
the words in sentences, as well as the sentence posi-
tion inside a document. These techniques have been
successfully implemented in the centroid approach
(Radev et al, 2004), which extends the idea of tf.idf
weighting (Salton and Buckley, 1997) by introduc-
ing word centroids, as well as integrating other fea-
tures such as position, first-sentence overlap and
sentence length. More recently, graph-based meth-
ods that rely on sentence connectivity have also been
found successful, using algorithms such as node de-
gree (Salton et al, 1997) or eigenvector centrality
(Mihalcea and Tarau, 2004; Erkan and Radev, 2004;
Wolf and Gibson, 2004).
In addition to unsupervised methods, supervised
machine learning techniques have also been used
with considerable success. Assuming the avail-
ability of a collection of documents and their cor-
responding manually constructed summaries, these
methods attempt to identify the key properties of a
good summary, such as the presence of named enti-
ties, positional scores, or the location of key phrases.
Such supervised techniques have been successfully
used in the systems proposed by e.g. (Teufel and
Moens, 1997; Hirao et al, 2002; Zhou and Hovy,
2003; D?Avanzo and Magnini, 2005).
In addition to short news documents, which have
been the focus of most of the summarization systems
proposed to date, work has been also carried out on
the summarization of other types of documents. This
includes systems addressing the summarization of e-
mail threads (Wan and McKeown, 2004), online dis-
cussions (Zhou and Hovy, 2005), spoken dialogue
(Galley, 2006), product reviews (Hu and Liu, 2004),
movie reviews (Zhuang et al, 2006), or short literary
fiction stories (Kazantseva and Szpakowicz, 2006).
As mentioned before, we are not aware of any work
addressing the task of automatic book summariza-
tion.
3 A Data Set for the Evaluation of
Book Summarization
A first challenge we encountered when we started
working on the task of book summarization was the
lack of a suitable data set, designed specifically for
the evaluation of summaries of long documents. Un-
like the summarization of short documents, which
benefits from the data sets made available through
the annual DUC evaluations, we are not aware of
any publicly available data sets that can be used for
the evaluation of methods for book summarization.
The lack of such data sets is perhaps not sur-
prising since even for humans the summarization of
books is more difficult and time consuming than the
summarization of short news documents. Moreover,
books are often available in printed format and are
typically protected by copyright laws that do not al-
low their reproduction in electronic format, which
consequently prohibits their public distribution.
We constructed a data set starting from the ob-
servation that several English and literature courses
make use of books that are sometimes also avail-
able in the form of abstracts ? meant to ease the
access of students to the content of the books. In
381
particular, we have identified two main publish-
ers that make summaries available online for books
studied in the U.S. high-school and college sys-
tems: Grade Saver (http://www.gradesaver.com) and
Cliff?s Notes (http://www.cliffsnotes.com/). Fortu-
nately, many of these books are classics that are al-
ready in the public domain, and thus for most of
them we were able to find the online electronic ver-
sion of the books on sites such as Gutenberg or On-
line Literature (http://www.online-literature.com).
For instance, the following is an example drawn
from Cliff?s Notes summary of Bleak House by
Charles Dickens.
On a raw November afternoon, London is en-
shrouded in heavy fog made harsher by chimney
smoke. The fog seems thickest in the vicinity of
the High Court of Chancery. The court, now in ses-
sion, is hearing an aspect of the case of Jarndyce
and Jarndyce. A ?little mad old woman? is, as al-
ways, one of the spectators. Two ruined men, one
a ?sallow prisoner,? the other a man from Shrop-
shire, appear before the court ? to no avail. Toward
the end of the sitting, the Lord High Chancellor an-
nounces that in the morning he will meet with ?the
two young people? and decide about making them
wards of their cousin....
Starting with the set of books that had a sum-
mary available from Cliff?s Notes, we removed all
the books that did not have an online version, and
further eliminated those that did not have a summary
available from Grade Saver. This left us with a ?gold
standard? data set of 50 books, each of them with
two manually created summaries.
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 0  100000  200000  300000  400000
Su
m
m
ar
y 
le
ng
th
Book length
Figure 1: Summary and book lengths for 50 books
The books in this collection have an average
length of 92,000 words, with summaries with an
average length of 6,500 words (Cliff?s Notes) and
7,500 words (Grade Saver). Figure 1 plots the length
of the summaries (averaged over the two manual
summaries) with respect to the length of the books.
As seen in the plot, most of the books have a length
of 50,000-150,000 words, with a summary of 2,000?
6,000 words, corresponding to a compression rate of
about 5-15%. There are also a few very long books,
with more than 150,000 words, for which the sum-
maries tend to become correspondingly longer.
3.1 Evaluation Metrics
For the evaluation, we use the ROUGE evaluation
toolkit. ROUGE is a method based on Ngram statis-
tics, found to be highly correlated with human eval-
uations (Lin and Hovy, 2003).1 Throughout the pa-
per, the evaluations are reported using the ROUGE-
1 setting, which seeks unigram matches between
the generated and the reference summaries, and
which was found to have high correlation with hu-
man judgments at a 95% confidence level. Addi-
tionally, the final system is also evaluated using the
ROUGE-2 (bigram matches) and ROUGE-SU4 (non-
contiguous bigrams) settings, which have been fre-
quently used in the DUC evaluations.
In most of the previous summarization evalua-
tions, the data sets were constructed specifically for
the purpose of enabling system evaluations, and thus
the length of the reference and the generated sum-
maries was established prior to building the data set
and prior to the evaluations. For instance, some
of the previous DUC evaluations provided refer-
ence summaries of 100-word each, and required the
participating systems to generate summaries of the
same length.
However, in our case we have to deal with
pre-existing summaries, with large summary-length
variations across the 50 books and across the two
reference summaries. To address this problem, we
decided to keep one manual summary as the main
reference (Grade Saver), and use the other summary
(Cliff?s Notes) as a way to decide on the length of
the generated summaries. This means that for a
given book, the Cliff?s Notes summary and all the
1ROUGE is available at http://haydn.isi.edu/ROUGE/
382
automatically generated summaries have the same
length, and they are all evaluated against the (pos-
sibly with a different length) Grade Saver summary.
This way, we can also calculate an upper bound by
comparing the two manual summaries against each
other, and at the same time ensure a fair comparison
between the automatically generated summaries and
this upper bound.2
3.2 Lower and Upper Bounds
To determine the difficulty of the task on the 50 book
data set, we calculate and report lower and upper
bounds. The lower bound is determined by using a
baseline summary constructed by including the first
sentences in the book (also known in the literature
as the lead baseline).3 As mentioned in the previ-
ous section, all the generated summaries ? includ-
ing this baseline ? have a length equal to the Cliff?s
Notes manual summary. The upper bound is calcu-
lated by evaluating Cliff?s Notes manual summary
against the reference Grade Saver summary. Table
1 shows the precision (P), recall (R), and F-measure
(F) for these lower and upper bounds, calculated as
average across the 50 books.
P R F
Lower bound (lead baseline) 0.380 0.284 0.325
Upper bound (manual summary) 0.569 0.493 0.528
Table 1: Lower and upper bounds for the book sum-
marization task, calculated on the 50 book data set
An automatic system evaluated on this data set is
therefore expected to have an F-measure higher than
the lower bound of 0.325, and it is unlikely to exceed
the upper bound of 0.528 obtained with a human-
generated summary.
4 An Initial Summarization System
Our first book summarization experiment was done
using a re-implementation of an existing state-of-
the-art summarization system. We decided to use the
2An alternative solution would be to determine the length
of the generated summaries using a predefined compression
rate (e.g., 10%). However, this again implies great variations
across the lengths of the generated versus the manual sum-
maries, which can result in large and difficult to interpret varia-
tions across the ROUGE scores.
3A second baseline that accounts for text segments is also
calculated and reported in section 6.
centroid-based method implemented in the MEAD
system (Radev et al, 2004), for three main reasons.
First, MEAD was shown to lead to good perfor-
mance in several DUC evaluations, e.g., (Radev et
al., 2003; Li et al, 2005). Second, it is an unsuper-
vised method which, unlike supervised approaches,
does not require training data (not available in our
case). Finally, the centroid-based techniques imple-
mented in MEAD can be optimized and made very
efficient, which is an important aspect in the sum-
marization of very long documents such as books.
The latest version of MEAD4 uses features, clas-
sifiers and re-rankers to determine the sentences to
include in the summary. The default features are
centroid, position and sentence length. The centroid
value of a sentence is the sum of the centroid val-
ues of the words in the sentence. The centroid value
of a word is calculated by multiplying the term fre-
quency (tf) of a word by the word?s inverse docu-
ment frequency (idf) obtained from the Topic Detec-
tion and Tracking (TDT) corpus. The tf of a word
is calculated by dividing the frequency of a word in
a document cluster by the number of documents in
the cluster. The positional value Pi of a sentence is
calculated using the formula (Radev et al, 2004):
Pi =
n? i+ 1
n ? Cmax (1)
where n represents the number of sentences in the
document, i represents the position of the sentence
inside the text, and Cmax is the score of the sentence
that has the maximum centroid value.
The summarizer combines these features to give
a score to each sentence. The default setting con-
sists of a linear combination of features that assigns
equal weights to the centroid and the positional val-
ues, and only scores sentences that have more than
nine words. After the sentences are scored, the re-
rankers are used to modify the scores of a sentence
depending on its relation with other sentences. The
default re-ranker implemented in MEAD first ranks
the sentences by their scores in descending order
and iteratively adds the top ranked sentence if the
sentence is not too similar to the already added sen-
tences. This similarity is computed as a cosine sim-
ilarity and by default the sentences that exhibit a co-
sine similarity higher than 0.7 are not added to the
4MEAD 3.11, http://www.summarization.com/mead/
383
summary. Note that although the MEAD distribution
also includes an optional feature calculated using the
LexRank graph-based algorithm (Erkan and Radev,
2004), this feature could not be used since it takes
days to compute for very long documents such as
ours, and thus its application was not tractable.
Although the MEAD system is publicly available
for download, in order to be able to make continu-
ous modifications easily and efficiently to the system
as we develop new methods, we decided to write
our own implementation. Our implementation dif-
fers from the original one in certain aspects. First,
we determine document frequency counts using the
British National Corpus (BNC) rather than the TDT
corpus. Second, we normalize the sentence scores
by dividing the score of a sentence by the length of
the sentence, and instead we eliminate the sentence
length feature used by MEAD. Note also that we do
not take stop words into account when calculating
the length of a sentence. Finally, since we are not
doing multi-document summarization, we do not use
a re-ranker in our implementation.
P R F
MEAD (original download) 0.423 0.296 0.348
MEAD (our implementation) 0.435 0.323 0.369
Table 2: Summarization results using the MEAD
system
Table 2 shows the results obtained on the 50 book
data set using the original MEAD implementation,
as well as our implementation. Although the per-
formance of this system is clearly better than the
baseline (see Table 1), it is nonetheless far below the
upper bound. In the following section, we explore
techniques for improving the quality of the gener-
ated summaries by accounting for the length of the
documents.
5 Techniques for Book Summarization
We decided to make several changes to our initial
system, in order to account for the specifics of the
data set we work with. In particular, our data set
consists of very large documents, and correspond-
ingly the summarization of such documents requires
techniques that account for their length.
5.1 Sentence Position In Very Large
Documents
The general belief in the text summarization litera-
ture (Edmunson, 1969; Mani, 2001) is that the posi-
tion of sentences in a text represents one of the most
important sources of information for a summariza-
tion system. In fact, a summary constructed using
the lead sentences was often found to be a compet-
itive baseline, with only few systems exceeding this
baseline during the recent DUC summarization eval-
uations.
Although the position of sentences in a document
seems like a pertinent heuristic for the summariza-
tion of short documents, and in particular for the
newswire genre as used in the DUC evaluations, our
hypothesis is that this heuristic may not hold for
the summarization of very long documents such as
books. The style and topic may change several times
throughout a book, and thus the leading sentences
will not necessarily overlap with the essence of the
document.
To test this hypothesis, we modified our initial
system so that it does not account for the position
of the sentences inside a document, but it only ac-
counts for the weight of the constituent words. Cor-
respondingly, the score of a sentence is determined
only as a function of the word centroids, and ex-
cludes the positional score. Table 3 shows the av-
erage ROUGE scores obtained using the summariza-
tion system with and without the position scores.
P R F
With positional scores 0.435 0.323 0.369
Without positional scores 0.459 0.329 0.383
Table 3: Summarization results with and without po-
sitional scores
As suspected, removing the position scores leads
to a better overall performance, with an increase ob-
served in both the precision and the recall of the
system. Although the position in a document is a
heuristic that helps the summarization of news sto-
ries and other short documents, it appears that the
sentences located toward the beginning of a book are
not necessarily useful for building the summary of a
book.
384
5.2 Text Segmentation
A major difference between short and long docu-
ments stands in the frequent topic shifts typically
observed in the later. While short stories are usu-
ally concerned with one topic at a time, long doc-
uments such as books often cover more than one
topic. Thus, the intuition is that a summary should
include content covering the important aspects of
all the topics in the document, as opposed to only
generic aspects relevant to the document as a whole.
A system for the summarization of long documents
should therefore extract key concepts from all the
topics in the document, and this task is better per-
formed when the topic boundaries are known prior
to the summarization step.
To accomplish this, we augment our system with
a text segmentation module that attempts to deter-
mine the topic shifts, and correspondingly splits
the document into smaller segments. Note that al-
though chapter boundaries are available in some of
the books in our data set, this is not always the case
as there are also books for which the chapters are not
explicitly identified. To ensure an uniform treatment
of the entire data set, we decided not to use chap-
ter boundaries, and instead apply an automatic text
segmentation algorithm.
While several text segmentation systems have
been proposed to date, we decided to use a graph-
based segmentation algorithm using normalized-
cuts (Malioutov and Barzilay, 2006), shown to ex-
ceed the performance of alternative segmentation
methods. Briefly, the segmentation algorithm starts
by modeling the text as a graph, where sentences
are represented as nodes in the graph, and inter-
sentential similarities are used to draw weighted
edges. The similarity between sentences is calcu-
lated using cosine similarity, with a smoothing fac-
tor that adds the counts of the words in the neighbor
sentences. Words are weighted using an adaptation
of the tf.idf metric, where a document is uniformly
split into chunks that are used for the tf.idf computa-
tion. There are two parameters that have to be set in
this algorithm: (1) the length in words of the blocks
approximating sentences; and (2) the cut-off value
for drawing edges between nodes. Since the method
was originally developed for spoken lecture segmen-
tation, we were not able to use the same parameters
as suggested in (Malioutov and Barzilay, 2006). In-
stead, we used a development set of three books, and
determined the optimal sentence word-length as 20
and the optimal cut-off value as 25, and these are the
values used throughout our experiments.
Once the text is divided into segments, we gener-
ate a separate summary for each segment, and con-
sequently create a final summary by collecting sen-
tences from the individual segment summaries in
a round-robin fashion. That is, starting with the
ranked list of sentences generated by the summa-
rization algorithm for each segment, we pick one
sentence at a time from each segment summary until
we reach the desired book-summary length.
A useful property of the normalized-cut segmen-
tation algorithm is that one can decide apriori the
number of segments to be generated, and so we can
evaluate the summarization algorithm for different
segmentation granularities. Figure 2 shows the av-
erage ROUGE-1 F-measure score obtained for sum-
maries generated using one to 50 segments.
 0.37
 0.38
 0.39
 0.4
 0  5  10  15  20  25  30  35  40  45  50
R
ou
ge
-1
 F
-m
ea
su
re
Number of segments
Figure 2: Summarization results for different seg-
mentation granularities.
As seen in the figure, segmenting the text helps
the summarization process. The average ROUGE-1
F-measure score raises to more than 0.39 F-measure
for increasingly larger number of segments, with a
plateau reached at approximately 15?25 segments,
followed by a decrease when more than 30 segments
are used.
In all the following evaluations, we segment each
book into a constant number of 15 segments; in fu-
ture work, we plan to consider more sophisticated
methods for finding the optimal number of segments
individually for each book.
385
5.3 Modified Term Weighting
An interesting characteristic of documents with
topic shifts is that words do not have an uniform dis-
tribution across the entire document. Instead, their
distribution can vary with the topic, and thus the
weight of the words should change accordingly.
To account for the distribution of the words in-
side the entire book, as well as inside the individual
topics (segments), we devised a weighting scheme
that accounts for four factors: the segment term
frequency (stf), calculated as the number of occur-
rences of a word inside a segment; the book term
frequency (tf), determined as the number of occur-
rences of a word inside a book; the inverse segment
frequency (isf), measured as the inverse of the num-
ber of segments containing the word; and finally, the
inverse document frequency (idf), which takes into
account the distribution of a word in a large exter-
nal corpus (as before, we use the BNC corpus). A
word weight is consequently determined by multi-
plying the book term frequency with the segment
term frequency, and the result is then multiplied with
the inverse segment frequency and the inverse docu-
ment frequency. We refer to this weighting scheme
as tf.stf.idf.isf.
Using this weighting scheme, we prevent a word
from having the same score across the entire book,
and instead we give a higher weight to its occur-
rences in segments where the word has a high fre-
quency. For instance, the word doctor occurs 30
times in one of the books in our data set, which leads
to a constant tf.idf score of 36.76 across the entire
book. Observing that from these 30 occurrences, 19
appear in just one segment, the tf.stf.idf.isf weight-
ing scheme will lead to a weight of 698.49 for that
segment, much higher than e.g. the weight of 36
calculated for other segments that have only a few
occurrences of this word.
P R F
tf.idf weighting 0.463 0.339 0.391
tf.stf.idf.isf weighting 0.464 0.349 0.398
Table 4: Summarization results using a weighting
scheme accounting for the distribution of words in-
side and across segments
Table 4 shows the summarization results obtained
for the new weighting scheme (recall that all the re-
sults are calculated for a text segmentation into 15
segments).
5.4 Combining Summarization Methods
The next improvement we made was to bring an
additional source of knowledge into the system, by
combining the summarization provided by our cur-
rent system with the summarization obtained from a
different method.
We implemented a variation of a centrality graph-
based algorithm for unsupervised summarization,
which was successfully used in the past for the
summarization of short documents. Very briefly,
the TextRank system (Mihalcea and Tarau, 2004)
? similar in spirit with the concurrently proposed
LexRank method (Erkan and Radev, 2004) ? works
by building a graph representation of the text, where
sentences are represented as nodes, and weighted
edges are drawn using inter-sentential word overlap.
An eigenvector centrality algorithm is then applied
on the graph (e.g., PageRank), leading to a rank-
ing over the sentences in the document. An imped-
iment we encountered was the size of the graphs,
which become intractably large and dense for very
large documents such as books. In our implemen-
tation we decided to use a cut-off value for drawing
edges between nodes, and consequently removed all
the edges between nodes that are farther apart than
a given threshold. We use a threshold value of 75,
found to work best using the same development set
of three books used before.
P R F
Our system 0.464 0.349 0.398
TextRank 0.449 0.356 0.397
COMBINED 0.464 0.363 0.407
Table 5: Summarization results for individual and
combined summarization algorithms
Using the same segmentation as before (15 seg-
ments), the TextRank method by itself did not lead to
improvements over our current centroid-based sys-
tem. Instead, since we noticed that the summaries
generated with our system and with TextRank cov-
ered different sentences, we implemented a method
that combines the top ranked sentences from the
two methods. Specifically, the combination method
picks one sentence at a time from the summary gen-
erated by our system for each segment, followed by
386
one sentence selected from the summary generated
by the TextRank method, and so on. The combi-
nation method also specifically avoids redundancy.
Table 5 shows the results obtained with our current
centroid-based system the TextRank method, as well
as the combined method.
5.5 Segment Ranking
In the current system, all the segments identified in
a book have equal weight. However, this might not
always be the case, as there are sometimes topics
inside the book that have higher importance, and
which consequently should be more heavily repre-
sented in the generated summaries.
To account for this intuition, we implemented a
segment ranking method that assigns to each seg-
ment a score reflecting its importance inside the
book. The ranking is performed with a method sim-
ilar to TextRank, using a random-walk model over
a graph representing segments and segment simi-
larities. The resulting segment scores are multi-
plied with the sentence scores obtained from the
combined method described before, normalized over
each segment, resulting in a new set of scores. The
top ranked sentences over the entire book are then
selected for inclusion in the summary. Table 6 shows
the results obtained by using segment ranking.
P R F
COMBINED 0.464 0.363 0.407
COMBINED + Segment Ranking 0.472 0.366 0.412
Table 6: Summarization results using segment rank-
ing
6 Discussion
In addition to the ROUGE-1 metric, the quality of the
summaries generated with our final summarization
system was also evaluated using the ROUGE-2 and
the ROUGE-SU4 metrics, which are frequently used
in the DUC evaluations. Table 7 shows the figures
obtained with ROUGE-1, ROUGE-2 and ROUGE-
SU4 for our final system, for the original MEAD
download, as well as for the lower and upper bounds.
The table also shows an additional baseline deter-
mined by selecting the first sentences in each seg-
ment, using the segmentation into 15 segments as
determined before. As it can be seen from the F-
P R F
ROUGE-1
Lower bound 0.380 0.284 0.325 [0.306,0.343]
Segment baseline 0.402 0.301 0.344 [0.328,0.366]
MEAD 0.423 0.296 0.348 [0.329,0.368]
Our system 0.472 0.366 0.412 [0.394,0.428]
Upper bound 0.569 0.493 0.528 [0.507,0.548]
ROUGE-2
Lower bound 0.035 0.027 0.031 [0.027,0.035]
Segment baseline 0.040 0.031 0.035 [0.031,0.038]
MEAD 0.039 0.029 0.033 [0.028,0.037]
Our system 0.069 0.054 0.061 [0.055,0.067]
Upper bound 0.112 0.097 0.104 [0.096,0.111]
ROUGE-SU4
Lower bound 0.096 0.073 0.083 [0.076,0.090]
Segment baseline 0.102 0.079 0.089 [0.082,0.093]
MEAD 0.106 0.076 0.088 [0.081,0.095]
Our system 0.148 0.115 0.129 [0.121,0.138]
Upper bound 0.210 0.182 0.195 [0.183,0.206]
Table 7: Evaluation of our final book summariza-
tion system using different ROUGE metrics. The ta-
ble also shows: the lower bound (first sentences in
the book); the segment baseline (first sentences in
each segment); MEAD (original system download);
the upper bound (manual summary). Confidence in-
tervals for F-measure are also included.
measure confidence intervals also shown in the ta-
ble, the improvements obtained by our system with
respect to both baselines and with respect to the
MEAD system are statistically significant (as the
confidence intervals do not overlap).
Additionally, to determine the robustness of the
results with respect to the number of reference sum-
maries, we ran a separate evaluation where both the
Grade Saver and the Cliff?s Notes summaries were
used as reference. As before, the length of the gener-
ated summaries was determined based on the Cliff?s
Notes summary. The F-measure figures obtained
in this case using our summarization system were
0.402, 0.057 and 0.127 using ROUGE-1, ROUGE-2
and ROUGE-SU4 respectively. The F-measure fig-
ures calculated for the baseline using the first sen-
tences in each segment were 0.340, 0.033 and 0.085.
These figures are very close to those listed in Table
7 where only one summary was used as a reference,
suggesting that the use of more than one reference
summary does not influence the results.
Regardless of the evaluation metric used, the per-
formance of our book summarization system is sig-
nificantly higher than the one of an existing summa-
rization system that has been designed for the sum-
387
marization of short documents (MEAD). In fact, if
we account for the upper bound of 0.528, the rela-
tive error rate reduction for the ROUGE-1 F-measure
score obtained by our system with respect to MEAD
is a significant 34.44%.
The performance of our system is mainly due to
features that account for the length of the document:
exclusion of positional scores, text segmentation and
segment ranking, and a segment-based weighting
scheme. An additional improvement is obtained by
combining two different summarization methods. It
is also worth noting that our system is efficient, tak-
ing about 200 seconds to apply the segmentation al-
gorithm, plus an additional 65 seconds to generate
the summary of one book.5
To assess the usefulness of our system with re-
spect to the length of the documents, we analyzed
the individual results obtained for books of different
sizes. Averaging the results obtained for the shorter
books in our collection, i.e., 17 books with a length
between 20,000 and 50,000 words, the lead base-
line gives a ROUGE-1 F-measure score of 0.337,
our system leads to 0.378, and the upper bound is
measured at 0.498, indicating a relative error rate
reduction of 25.46% obtained by our system with
respect to the lead baseline (accounting for the max-
imum achievable score given by the upper bound).
Instead, when we consider only the books with a
length over 100,000 words (16 books in our data set
fall under this category), the lead baseline is deter-
mined as 0.347, our system leads to 0.418, and the
upper bound is calculated as 0.552, which results in
a higher 34.64% relative error rate reduction. This
suggests that our system is even more effective for
longer books, due perhaps to the features that specif-
ically take into account the length of the books.
There are also cases where our system does not
improve over the baseline. For instance, for the sum-
marization of Candide by Franc?ois Voltaire, our sys-
tem achieves a ROUGE-1 F-measure of 0.361, which
is slightly worse than the lead baseline of 0.368. In
other cases however, the performance of our system
comes close to the upper bound, as it is the case with
the summarization of The House of the Seven Gables
by Nathaniel Hawthorne, which has a lead baseline
5Running times measured on a Pentium IV 3GHz, 2GB
RAM.
of 0.296, an upper bound of 0.457, and our system
obtains 0.404. This indicates that a possible avenue
for future research is to account for the characteris-
tics of a book, and devise summarization methods
that can adapt to the specifics of a given book such
as length, genre, and others.
7 Conclusions
Although there is a significant body of work that has
been carried out on the task of text summarization,
most of the research to date has been concerned with
the summarization of short documents. In this paper,
we tried to address this gap and tackled the problem
of book summarization.
We believe this paper made two important con-
tributions. First, it introduced a new summariza-
tion benchmark, specifically targeting the evalua-
tion of systems for book summarization.6 Second,
it showed that systems developed for the summa-
rization of short documents do not fare well when
applied to very long documents such as books, and
instead a better performance can be achieved with
a system that accounts for the length of the docu-
ments. In particular, the book summarization sys-
tem we developed was found to lead to more than
30% relative error rate reduction with respect to an
existing state-of-the-art summarization tool.
Given the increasingly large number of books
available in electronic format, and correspondingly
the growing need for tools for book summarization,
we believe that the topic of automatic book sum-
marization will become increasingly important. We
hope that this paper will encourage and facilitate the
development of an active line of research concerned
with book summarization.
Acknowledgments
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for useful discussions and feed-
back, and in particular to Carmen Banea for sug-
gesting Cliff?s Notes as a source of book summaries.
This work was supported in part by a research grant
from Google Inc. and by a grant from the Texas Ad-
vanced Research Program (#003594).
6The data set is publicly available and can be downloaded
from http://lit.csci.unt.edu/index.php/Downloads
388
References
E. D?Avanzo and B. Magnini. 2005. A keyphrase-based
approach to summarization: The Lake system at DUC
2005. In Proceedings of the Document Understanding
Conference (DUC 2005).
H.P. Edmunson. 1969. New methods in automatic ex-
tracting. Journal of the ACM, 16(2):264?285.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2004), Barcelona,
Spain, July.
M. Galley. 2006. Automatic summarization of conversa-
tional multi-party speech. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI
2006), AAAI/SIGART Doctoral Consortium, Boston.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002.
NTT?s text summarization system for DUC-2002. In
Proceedings of the Document Understanding Confer-
ence 2002 (DUC 2002).
E. Hovy and C. Lin, 1997. Automated text summarization
in SUMMARIST. Cambridge Univ. Press.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, Seattle, Washington.
A. Kazantseva and S. Szpakowicz. 2006. Challenges in
evaluating summaries of short stories. In Proceedings
of the Workshop on Task-Focused Summarization and
Question Answering, Sydney, Australia.
W. Li, W. Li, B. Li, Q. Chen, and M. Wu. 2005. The
Hong Kong Polytechnic University at DUC 2005. In
Proceedings of the Document Understanding Confer-
ence (DUC 2005), Vancouver, Canada.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development,
2(2):159?165.
I. Malioutov and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL 2006), pages 9?16.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), Barcelona, Spain.
D. Radev, J. Otterbacher, H. Qi, and D. Tam. 2003.
MEAD ReDUCs: Michigan at DUC 2003. In Pro-
ceedings of the Document Understanding Conference
(DUC 2003).
D. Radev, H. Jing, M. Stys, and D. Tam. 2004. Centroid-
based summarization of multiple documents. Informa-
tion Processing and Management, 40.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
S. Teufel and M. Moens. 1997. Sentence extraction as a
classification task. In ACL/EACL workshop on intelli-
gent and scalable text summarization, Madrid, Spain.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of the 20th International Conference on
Computational Linguistics, Geneva, Switzerland.
F. Wolf and E. Gibson. 2004. Paragraph-, word-, and
coherence-based approaches to sentence ranking: A
comparison of algorithm and human performance. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2004), Barcelona,
Spain, July.
L. Zhou and E. Hovy. 2003. A Web-trained extrac-
tion summarization system. In Proceedings of Hu-
man Language Technology Conference (HLT-NAACL
2003), Edmonton, Canada, May.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet re-
lay chats. In Proceedings of Association for Computa-
tional Linguistics (ACL 2005), Ann Arbor.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the ACM international conference on Information and
knowledge management (CIKM 2006), Arlington, Vir-
ginia.
389
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127?135,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multilingual Subjectivity Analysis Using Machine Translation
Carmen Banea and Rada Mihalcea
University of North Texas
carmenb@unt.edu, rada@cs.unt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Samer Hassan
University of North Texas
samer@unt.edu
Abstract
Although research in other languages is in-
creasing, much of the work in subjectivity
analysis has been applied to English data,
mainly due to the large body of electronic re-
sources and tools that are available for this lan-
guage. In this paper, we propose and evalu-
ate methods that can be employed to transfer a
repository of subjectivity resources across lan-
guages. Specifically, we attempt to leverage
on the resources available for English and, by
employing machine translation, generate re-
sources for subjectivity analysis in other lan-
guages. Through comparative evaluations on
two different languages (Romanian and Span-
ish), we show that automatic translation is a
viable alternative for the construction of re-
sources and tools for subjectivity analysis in
a new target language.
1 Introduction
We have seen a surge in interest towards the ap-
plication of automatic tools and techniques for the
extraction of opinions, emotions, and sentiments in
text (subjectivity). A large number of text process-
ing applications have already employed techniques
for automatic subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 2005), text semantic analysis (Wiebe and Mihal-
cea, 2006; Esuli and Sebastiani, 2006), tracking sen-
timent timelines in on-line forums and news (Lloyd
et al, 2005; Balog et al, 2006), mining opinions
from product reviews (Hu and Liu, 2004), and ques-
tion answering (Yu and Hatzivassiloglou, 2003).
A significant fraction of the research work to date
in subjectivity analysis has been applied to English,
which led to several resources and tools available for
this language. In this paper, we explore multiple
paths that employ machine translation while lever-
aging on the resources and tools available for En-
glish, to automatically generate resources for sub-
jectivity analysis for a new target language. Through
experiments carried out with automatic translation
and cross-lingual projections of subjectivity annota-
tions, we try to answer the following questions.
First, assuming an English corpus manually an-
notated for subjectivity, can we use machine trans-
lation to generate a subjectivity-annotated corpus in
the target language? Second, assuming the availabil-
ity of a tool for automatic subjectivity analysis in
English, can we generate a corpus annotated for sub-
jectivity in the target language by using automatic
subjectivity annotations of English text and machine
translation? Finally, third, can these automatically
generated resources be used to effectively train tools
for subjectivity analysis in the target language?
Since our methods are particularly useful for lan-
guages with only a few electronic tools and re-
sources, we chose to conduct our initial experiments
on Romanian, a language with limited text process-
ing resources developed to date. Furthermore, to
validate our results, we carried a second set of ex-
periments on Spanish. Note however that our meth-
ods do not make use of any target language specific
knowledge, and thus they are applicable to any other
language as long as a machine translation engine ex-
ists between the selected language and English.
127
2 Related Work
Research in sentiment and subjectivity analysis has
received increasingly growing interest from the nat-
ural language processing community, particularly
motivated by the widespread need for opinion-based
applications, including product and movie reviews,
entity tracking and analysis, opinion summarization,
and others.
Much of the work in subjectivity analysis has
been applied to English data, though work on other
languages is growing: e.g., Japanese data are used
in (Kobayashi et al, 2004; Suzuki et al, 2006;
Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006).
In addition, several participants in the Chinese
and Japanese Opinion Extraction tasks of NTCIR-
6 (Kando and Evans, 2007) performed subjectivity
and sentiment analysis in languages other than En-
glish.
In general, efforts on building subjectivity analy-
sis tools for other languages have been hampered by
the high cost involved in creating corpora and lexical
resources for a new language. To address this gap,
we focus on leveraging resources already developed
for one language to derive subjectivity analysis tools
for a new language. This motivates the direction of
our research, in which we use machine translation
coupled with cross-lingual annotation projections to
generate the resources and tools required to perform
subjectivity classification in the target language.
The work closest to ours is the one reported in
(Mihalcea et al, 2007), where a bilingual lexicon
and a manually translated parallel text are used to
generate the resources required to build a subjectiv-
ity classifier in a new language. In that work, we
found that the projection of annotations across par-
allel texts can be successfully used to build a cor-
pus annotated for subjectivity in the target language.
However, parallel texts are not always available for
a given language pair. Therefore, in this paper we
explore a different approach where, instead of rely-
ing on manually translated parallel corpora, we use
machine translation to produce a corpus in the new
language.
3 Machine Translation for Subjectivity
Analysis
We explore the possibility of using machine transla-
tion to generate the resources required to build sub-
jectivity annotation tools in a given target language.
We focus on two main scenarios. First, assuming a
corpus manually annotated for subjectivity exists in
the source language, we can use machine translation
to create a corpus annotated for subjectivity in the
target language. Second, assuming a tool for auto-
matic subjectivity analysis exists in the source lan-
guage, we can use this tool together with machine
translation to create a corpus annotated for subjec-
tivity in the target language.
In order to perform a comprehensive investiga-
tion, we propose three experiments as described be-
low. The first scenario, based on a corpus manu-
ally annotated for subjectivity, is exemplified by the
first experiment. The second scenario, based on a
corpus automatically annotated with a tool for sub-
jectivity analysis, is subsequently divided into two
experiments depending on the direction of the trans-
lation and on the dataset that is translated.
In all three experiments, we use English as a
source language, given that it has both a corpus man-
ually annotated for subjectivity (MPQA (Wiebe et
al., 2005)) and a tool for subjectivity analysis (Opin-
ionFinder (Wiebe and Riloff, 2005)).
3.1 Experiment One: Machine Translation of
Manually Annotated Corpora
In this experiment, we use a corpus in the source
language manually annotated for subjectivity. The
corpus is automatically translated into the target lan-
guage, followed by a projection of the subjectivity
labels from the source to the target language. The
experiment is illustrated in Figure 1.
We use the MPQA corpus (Wiebe et al, 2005),
which is a collection of 535 English-language news
articles from a variety of news sources manually an-
notated for subjectivity. Although the corpus was
originally annotated at clause and phrase level, we
use the sentence-level annotations associated with
the dataset (Wiebe and Riloff, 2005). From the total
of 9,700 sentences in this corpus, 55% of the sen-
tences are labeled as subjective while the rest are
objective. After the automatic translation of the cor-
128
Figure 1: Experiment one: machine translation of man-
ually annotated training data from source language into
target language
pus and the projection of the annotations, we obtain
a large corpus of 9,700 subjectivity-annotated sen-
tences in the target language, which can be used to
train a subjectivity classifier.
3.2 Experiment Two: Machine Translation of
Source Language Training Data
In the second experiment, we assume that the only
resources available are a tool for subjectivity anno-
tation in the source language and a collection of raw
texts, also in the source language. The source lan-
guage text is automatically annotated for subjectiv-
ity and then translated into the target language. In
this way, we produce a subjectivity annotated cor-
pus that we can use to train a subjectivity annotation
tool for the target language. Figure 2 illustrates this
experiment.
In order to generate automatic subjectivity anno-
tations, we use the OpinionFinder tool developed by
(Wiebe and Riloff, 2005). OpinionFinder includes
two classifiers. The first one is a rule-based high-
precision classifier that labels sentences based on the
presence of subjective clues obtained from a large
lexicon. The second one is a high-coverage classi-
fier that starts with an initial corpus annotated us-
ing the high-precision classifier, followed by several
bootstrapping steps that increase the size of the lex-
icon and the coverage of the classifier. For most of
our experiments we use the high-coverage classifier.
Figure 2: Experiment two: machine translation of raw
training data from source language into target language
Table 1 shows the performance of the two Opinion-
Finder classifiers as measured on the MPQA corpus
(Wiebe and Riloff, 2005).
P R F
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 1: Precision (P), Recall (R) and F-measure (F) for
the two OpinionFinder classifiers, as measured on the
MPQA corpus
As a raw corpus, we use a subset of the SemCor
corpus (Miller et al, 1993), consisting of 107 docu-
ments with roughly 11,000 sentences. This is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others. The reason
for working with this collection is the fact that we
also have a manual translation of the SemCor docu-
ments from English into one of the target languages
used in the experiments (Romanian), which enables
comparative evaluations of different scenarios (see
Section 4).
Note that in this experiment the annotation of sub-
jectivity is carried out on the original source lan-
guage text, and thus expected to be more accurate
than if it were applied on automatically translated
text. However, the training data in the target lan-
guage is produced by automatic translation, and thus
likely to contain errors.
129
3.3 Experiment Three: Machine Translation of
Target Language Training Data
The third experiment is similar to the second one,
except that we reverse the direction of the transla-
tion. We translate raw text that is available in the
target language into the source language, and then
use a subjectivity annotation tool to label the auto-
matically translated source language text. After the
annotation, the labels are projected back into the tar-
get language, and the resulting annotated corpus is
used to train a subjectivity classifier. Figure 3 illus-
trates this experiment.
Figure 3: Experiment three: machine translation of raw
training data from target language into source language
As before, we use the high-coverage classifier
available in OpinionFinder, and the SemCor corpus.
We use a manual translation of this corpus available
in the target language.
In this experiment, the subjectivity annotations
are carried out on automatically generated source
text, and thus expected to be less accurate. How-
ever, since the training data was originally written
in the target language, it is free of translation errors,
and thus training carried out on this data should be
more robust.
3.4 Upper bound: Machine Translation of
Target Language Test Data
For comparison purposes, we also propose an ex-
periment which plays the role of an upper bound on
the methods described so far. This experiment in-
volves the automatic translation of the test data from
the target language into the source language. The
source language text is then annotated for subjectiv-
ity using OpinionFinder, followed by a projection of
the resulting labels back into the target language.
Unlike the previous three experiments, in this
experiment we only generate subjectivity-annotated
resources, and we do not build and evaluate a stan-
dalone subjectivity analysis tool for the target lan-
guage. Further training of a machine learning algo-
rithm, as in experiments two and three, is required in
order to build a subjectivity analysis tool. Thus, this
fourth experiment is an evaluation of the resources
generated in the target language, which represents
an upper bound on the performance of any machine
learning algorithm that would be trained on these re-
sources. Figure 4 illustrates this experiment.
Figure 4: Upper bound: machine translation of test data
from target language into source language
4 Evaluation and Results
Our initial evaluations are carried out on Romanian.
The performance of each of the three methods is
evaluated using a dataset manually annotated for
subjectivity. To evaluate our methods, we generate a
Romanian training corpus annotated for subjectivity
on which we train a subjectivity classifier, which is
then used to label the test data.
We evaluate the results against a gold-standard
corpus consisting of 504 Romanian sentences man-
ually annotated for subjectivity. These sentences
represent the manual translation into Romanian of
a small subset of the SemCor corpus, which was
removed from the training corpora used in experi-
ments two and three. This is the same evaluation
dataset as used in (Mihalcea et al, 2007). Two
Romanian native speakers annotated the sentences
individually, and the differences were adjudicated
130
through discussions. The agreement of the two an-
notators is 0.83% (? = 0.67); when the uncertain an-
notations are removed, the agreement rises to 0.89
(? = 0.77). The two annotators reached consensus
on all sentences for which they disagreed, resulting
in a gold standard dataset with 272 (54%) subjective
sentences and 232 (46%) objective sentences. More
details about this dataset are available in (Mihalcea
et al, 2007).
In order to learn from our annotated data, we ex-
periment with two different classifiers, Na??ve Bayes
and support vector machines (SVM), selected for
their performance and diversity of learning method-
ology. For Na??ve Bayes, we use the multinomial
model (McCallum and Nigam, 1998) with a thresh-
old of 0.3. For SVM (Joachims, 1998), we use the
LibSVM implementation (Fan et al, 2005) with a
linear kernel.
The automatic translation of the MPQA and of
the SemCor corpus was performed using Language
Weaver,1 a commercial statistical machine transla-
tion software. The resulting text was post-processed
by removing diacritics, stopwords and numbers. For
training, we experimented with a series of weight-
ing schemes, yet we only report the results obtained
for binary weighting, as it had the most consistent
behavior.
The results obtained by running the three experi-
ments on Romanian are shown in Table 2. The base-
line on this data set is 54.16%, represented by the
percentage of sentences in the corpus that are sub-
jective, and the upper bound (UB) is 71.83%, which
is the accuracy obtained under the scenario where
the test data is translated into the source language
and then annotated using the high-coverage Opin-
ionFinder tool.
Perhaps not surprisingly, the SVM classifier out-
performs Na??ve Bayes by 2% to 6%, implying that
SVM may be better fitted to lessen the amount of
noise embedded in the dataset and provide more ac-
curate classifications.
The first experiment, involving the automatic
translation of the MPQA corpus enhanced with man-
ual annotations for subjectivity at sentence level,
does not seem to perform well when compared to the
experiments in which automatic subjectivity classi-
1http://www.languageweaver.com/
Romanian
Exp Classifier P R F
E1 Na??ve Bayes 60.91 60.91 60.91
SVM 66.07 66.07 66.07
E2 Na??ve Bayes 63.69 63.69 63.69
SVM 69.44 69.44 69.44
E3 Na??ve Bayes 65.87 65.87 65.87
SVM 67.86 67.86 67.86
UB OpinionFinder 71.83 71.83 71.83
Table 2: Precision (P), Recall (R) and F-measure (F) for
Romanian experiments
fication is used. This could imply that a classifier
cannot be so easily trained on the cues that humans
use to express subjectivity, especially when they are
not overtly expressed in the sentence and thus can
be lost in the translation. Instead, the automatic
annotations produced with a rule-based tool (Opin-
ionFinder), relying on overt mentions of words in
a subjectivity lexicon, seems to be more robust to
translation, further resulting in better classification
results. To exemplify, consider the following sub-
jective sentence from the MPQA corpus, which does
not include overt clues of subjectivity, but was an-
notated as subjective by the human judges because
of the structure of the sentence: It is the Palestini-
ans that are calling for the implementation of the
agreements, understandings, and recommendations
pertaining to the Palestinian-Israeli conflict.
We compare our results with those obtained by
a previously proposed method that was based on
the manual translation of the SemCor subjectivity-
annotated corpus. In (Mihalcea et al, 2007), we
used the manual translation of the SemCor corpus
into Romanian to form an English-Romanian par-
allel data set. The English side was annotated us-
ing the Opinion Finder tool, and the subjectivity la-
bels were projected on the Romanian text. A Na??ve
Bayes classifier was then trained on the subjectivity
annotated Romanian corpus and tested on the same
gold standard as used in our experiments. Table 3
shows the results obtained in those experiments by
using the high-coverage OpinionFinder classifier.
Among our experiments, experiments two and
three are closest to those proposed in (Mihalcea
et al, 2007). By using machine translation, from
131
OpinionFinder classifier P R F
high-coverage 67.85 67.85 67.85
Table 3: Precision (P), Recall (R) and F-measure (F) for
subjectivity analysis in Romanian obtained by using an
English-Romanian parallel corpus
English into Romanian (experiment two) or Roma-
nian into English (experiment three), and annotating
this dataset with the high-coverage OpinionFinder
classifier, we obtain an F-measure of 63.69%, and
65.87% respectively, using Na??ve Bayes (the same
machine learning classifier as used in (Mihalcea et
al., 2007)). This implies that at most 4% in F-
measure can be gained by using a parallel corpus as
compared to an automatically translated corpus, fur-
ther suggesting that machine translation is a viable
alternative to devising subjectivity classification in a
target language leveraged on the tools existent in a
source language.
As English is a language with fewer inflections
when compared to Romanian, which accommodates
for gender and case as a suffix to the base form of a
word, the automatic translation into English is closer
to a human translation (experiment three). Therefore
labeling this data using the OpinionFinder tool and
projecting the labels onto a fully inflected human-
generated Romanian text provides more accurate
classification results, as compared to a setup where
the training is carried out on machine-translated Ro-
manian text (experiment two).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 5: Experiment two: Machine learning F-measure
over an incrementally larger training set
We also wanted to explore the impact that the cor-
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 6: Experiment three: Machine learning F-measure
over an incrementally larger training set
pus size may have on the accuracy of the classifiers.
We re-ran experiments two and three with 20% cor-
pus size increments at a time (Figures 5 and 6). It
is interesting to note that a corpus of approximately
6000 sentences is able to achieve a high enough F-
measure (around 66% for both experiments) to be
considered viable for training a subjectivity classi-
fier. Also, at a corpus size over 10,000 sentences, the
Na??ve Bayes classifier performs worse than SVM,
which displays a directly proportional trend between
the number of sentences in the data set and the ob-
served F-measure. This trend could be explained
by the fact that the SVM classifier is more robust
with regard to noisy data, when compared to Na??ve
Bayes.
5 Portability to Other Languages
To test the validity of the results on other languages,
we ran a portability experiment on Spanish.
To build a test dataset, a native speaker of Span-
ish translated the gold standard of 504 sentences into
Spanish. We maintain the same subjectivity anno-
tations as for the Romanian dataset. To create the
training data required by the first two experiments,
we translate both the MPQA corpus and the Sem-
Cor corpus into Spanish using the Google Transla-
tion service,2 a publicly available machine transla-
tion engine also based on statistical machine transla-
tion. We were therefore able to implement all the ex-
periments but the third, which would have required
2http://www.google.com/translate t
132
a manually translated version of the SemCor corpus.
Although we could have used a Spanish text to carry
out a similar experiment, due to the fact that the
dataset would have been different, the results would
not have been directly comparable.
The results of the two experiments exploring the
portability to Spanish are shown in Table 4. Inter-
estingly, all the figures are higher than those ob-
tained for Romanian. We assume this occurs be-
cause Spanish is one of the six official United Na-
tions languages, and the Google translation engine
is using the United Nations parallel corpus to train
their translation engine, therefore implying that a
better quality translation is achieved as compared to
the one available for Romanian. We can therefore
conclude that the more accurate the translation en-
gine, the more accurately the subjective content is
translated, and therefore the better the results. As it
was the case for Romanian, the SVM classifier pro-
duces the best results, with absolute improvements
over the Na??ve Bayes classifier ranging from 0.2%
to 3.5%.
Since the Spanish automatic translation seems to
be closer to a human-quality translation, we are not
surprised that this time the first experiment is able
to generate a more accurate training corpus as com-
pared to the second experiment. The MPQA corpus,
since it is manually annotated and of better quality,
has a higher chance of generating a more reliable
data set in the target language. As in the experiments
on Romanian, when performing automatic transla-
tion of the test data, we obtain the best results with
an F-measure of 73.41%, which is also the upper
bound on our proposed experiments.
Spanish
Exp Classifier P R F
E1 Na??ve Bayes 65.28 65.28 65.28
SVM 68.85 68.85 68.85
E2 Na??ve Bayes 62.50 62.50 62.50
SVM 62.70 62.70 62.70
UB OpinionFinder 73.41 73.41 73.41
Table 4: Precision (P), Recall (R) and F-measure (F) for
Spanish experiments
6 Discussion
Based on our experiments, we can conclude that ma-
chine translation offers a viable approach to gener-
ating resources for subjectivity annotation in a given
target language. The results suggest that either a
manually annotated dataset or an automatically an-
notated one can provide sufficient leverage towards
building a tool for subjectivity analysis.
Since the use of parallel corpora (Mihalcea et al,
2007) requires a large amount of manual labor, one
of the reasons behind our experiments was to asses
the ability of machine translation to transfer subjec-
tive content into a target language with minimal ef-
fort. As demonstrated by our experiments, machine
translation offers a viable alternative in the construc-
tion of resources and tools for subjectivity classifica-
tion in a new target language, with only a small de-
crease in performance as compared to the case when
a parallel corpus is available and used.
To gain further insights, two additional experi-
ments were performed. First, we tried to isolate the
role played by the quality of the subjectivity anno-
tations in the source-language for the cross-lingual
projections of subjectivity. To this end, we used the
high-precision OpinionFinder classifier to annotate
the English datasets. As shown in Table 1, this clas-
sifier has higher precision but lower recall as com-
pared to the high-coverage classifier we used in our
previous experiments. We re-ran the second exper-
iment, this time trained on the 3,700 sentences that
were classified by the OpinionFinder high-precision
classifier as either subjective or objective. For Ro-
manian, we obtained an F-measure of 69.05%, while
for Spanish we obtained an F-measure of 66.47%.
Second, we tried to isolate the role played by
language-specific clues of subjectivity. To this end,
we decided to set up an experiment which, by com-
parison, can suggest the degree to which the lan-
guages are able to accommodate specific markers for
subjectivity. First, we trained an English classifier
using the SemCor training data automatically anno-
tated for subjectivity with the OpinionFinder high-
coverage tool. The classifier was then applied to the
English version of the manually labeled test data set
(the gold standard described in Section 4). Next, we
ran a similar experiment on Romanian, using a clas-
sifier trained on the Romanian version of the same
133
SemCor training data set, annotated with subjectiv-
ity labels projected from English. The classifier was
tested on the same gold standard data set. Thus, the
two classifiers used the same training data, the same
test data, and the same subjectivity annotations, the
only difference being the language used (English or
Romanian).
The results for these experiments are compiled in
Table 5. Interestingly, the experiment conducted on
Romanian shows an improvement of 3.5% to 9.5%
over the results obtained on English, which indi-
cates that subjective content may be easier to learn
in Romanian versus English. The fact that Roma-
nian verbs are inflected for mood (such as indicative,
conditional, subjunctive, presumptive), enables an
automatic classifier to identify additional subjective
markers in text. Some moods such as conditional
and presumptive entail human judgment, and there-
fore allow for clear subjectivity annotation. More-
over, Romanian is a highly inflected language, ac-
commodating for forms of various words based on
number, gender, case, and offering an explicit lex-
icalization of formality and politeness. All these
features may have a cumulative effect in allowing
for better classification. At the same time, English
entails minimal inflection when compared to other
Indo-European languages, as it lacks both gender
and adjective agreement (with very few notable ex-
ceptions such as beautiful girl and handsome boy).
Verb moods are composed with the aid of modals,
while tenses and expressions are built with the aid
of auxiliary verbs. For this reason, a machine learn-
ing algorithm may not be able to identify the same
amount of information on subjective content in an
English versus a Romanian text. It is also interesting
to note that the labeling of the training set was per-
formed using a subjectivity classifier developed for
English, which takes into account a large, human-
annotated, subjectivity lexicon also developed for
English. One would have presumed that any clas-
sifier trained on this annotated text would therefore
provide the best results in English. Yet, as explained
earlier, this was not the case.
7 Conclusion
In this paper, we explored the use of machine trans-
lation for creating resources and tools for subjec-
Exp Classifier P R F
En Na??ve Bayes 60.32 60.32 60.32
SVM 60.32 60.32 60.32
Ro Na??ve Bayes 67.85 67.85 67.85
SVM 69.84 69.84 69.84
Table 5: Precision (P), Recall (R) and F-measure (F) for
identifying language specific information
tivity analysis in other languages, by leveraging on
the resources available in English. We introduced
and evaluated three different approaches to generate
subjectivity annotated corpora in a given target lan-
guage, and exemplified the technique on Romanian
and Spanish.
The experiments show promising results, as they
are comparable to those obtained using manually
translated corpora. While the quality of the trans-
lation is a factor, machine translation offers an effi-
cient and effective alternative in capturing the sub-
jective semantics of a text, coming within 4% F-
measure as compared to the results obtained using
human translated corpora.
In the future, we plan to explore additional
language-specific clues, and integrate them into the
subjectivity classifiers. As shown by some of our
experiments, Romanian seems to entail more subjec-
tivity markers compared to English, and this factor
motivates us to further pursue the use of language-
specific clues of subjectivity.
Our experiments have generated corpora of about
20,000 sentences annotated for subjectivity in Ro-
manian and Spanish, which are available for down-
load at http://lit.csci.unt.edu/index.php/Downloads,
along with the manually annotated data sets.
Acknowledgments
The authors are grateful to Daniel Marcu and Lan-
guageWeaver for kindly providing access to their
Romanian-English and English-Romanian machine
translation engines. This work was partially sup-
ported by a National Science Foundation grant IIS-
#0840608.
134
References
C. Ovesdotter Alm, D. Roth, and R. Sproat. 2005.
Emotions from text: Machine learning for text-based
emotion prediction. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in blog
mood levels. In Proceedings of the 11th Meeting of
the European Chapter of the Association for Compu-
tational Linguistics (EACL-2006).
A. Esuli and F. Sebastiani. 2006. Determining term sub-
jectivity and term orientation for opinion mining. In
Proceedings the 11th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 193?200, Trento, IT.
R. Fan, P. Chen, and C. Lin. 2005. Working set selection
using the second order information for training svm.
Journal of Machine Learning Research, 6:1889?1918.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2004 (KDD 2004), pages 168?177, Seattle, Wash-
ington.
Y. Hu, J. Duan, X. Chen, B. Pei, and R. Lu. 2005. A new
method for sentiment classification in text retrieval. In
IJCNLP, pages 1?9.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
H. Kanayama and T. Nasukawa. 2006. Fully automatic
lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 355?363, Sydney, Australia.
N. Kando and D. Kirk Evans, editors. 2007. Proceed-
ings of the Sixth NTCIR Workshop Meeting on Evalua-
tion of Information Access Technologies: Information
Retrieval, Question Answering, and Cross-Lingual In-
formation Access, 2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, Japan, May. National Institute of In-
formatics.
S.-M. Kim and E. Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
pages 200?207, New York, New York.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. In Proceedings of the 1st
International Joint Conference on Natural Language
Processing (IJCNLP-04).
L. Lloyd, D. Kechagias, and S. Skiena. 2005. Lydia: A
system for large-scale news analysis. In String Pro-
cessing and Information Retrieval (SPIRE 2005).
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
Y. Suzuki, H. Takamura, and M. Okumura. 2006. Ap-
plication of semi-supervised learning to evaluative ex-
pression classification. In Proceedings of the 7th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2006),
pages 502?513, Mexico City, Mexico.
H. Takamura, T. Inui, and M. Okumura. 2006. Latent
variable models for semantic orientations of phrases.
In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2006), Trento, Italy.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of COLING-ACL 2006.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2005) ( invited paper), Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
135
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190?199,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Subjectivity Word Sense Disambiguation
Cem Akkaya and Janyce Wiebe
University of Pittsburgh
{cem,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
This paper investigates a new task, subjec-
tivity word sense disambiguation (SWSD),
which is to automatically determine which
word instances in a corpus are being used
with subjective senses, and which are be-
ing used with objective senses. We pro-
vide empirical evidence that SWSD is
more feasible than full word sense dis-
ambiguation, and that it can be exploited
to improve the performance of contextual
subjectivity and sentiment analysis sys-
tems.
1 Introduction
The automatic extraction of opinions, emotions,
and sentiments in text (subjectivity analysis) to
support applications such as product review min-
ing, summarization, question answering, and in-
formation extraction is an active area of research
in NLP.
Many approaches to opinion, sentiment, and
subjectivity analysis rely on lexicons of words that
may be used to express subjectivity. Examples of
such words are the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
Knowing the meaning (and thus subjectivity) of
these words would help a system recognize the
negative sentiments in these sentences.
Most subjectivity lexicons are compiled as lists
of keywords, rather than word meanings (senses).
However, many keywords have both subjective
and objective senses. False hits ? subjectivity
clues used with objective senses ? are a signifi-
cant source of error in subjectivity and sentiment
analysis. For example, even though the follow-
ing sentence contains all of the negative keywords
above, it is nevertheless objective, as they are all
false hits:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
To tackle this source of error, we define a
new task, subjectivity word sense disambigua-
tion (SWSD), which is to automatically determine
which word instances in a corpus are being used
with subjective senses, and which are being used
with objective senses. We hypothesize that SWSD
is more feasible than full word sense disambigua-
tion, because it is more coarse grained ? often, the
exact sense need not be pinpointed. We also hy-
pothesize that SWSD can be exploited to improve
the performance of contextual subjectivity analy-
sis systems via sense-aware classification.
The paper consists of two parts. In the first
part, we build and evaluate a targeted supervised
SWSD system that aims to disambiguate members
of a subjectivity lexicon. It labels clue instances as
having a subjective sense or an objective sense in
context. The system relies on common machine
learning features for word sense disambiguation
(WSD). The performance is substantially above
both baseline and the performance of full WSD
on the same data, suggesting that the task is feasi-
ble, and that subjectivity provides a natural coarse-
grained grouping of senses.
The second part demonstrates the promise of
SWSD for contextual subjectivity analysis. First,
we show that subjectivity sense ambiguity is
highly prevalent in the MPQA opinion-annotated
corpus (Wiebe et al, 2005; Wilson, 2008), thus
establishing the potential benefit of performing
SWSD. Then, we exploit SWSD to improve per-
formance on several subjectivity analysis tasks,
from subjective/objective sentence-level classi-
fication to positive/negative/neutral expression-
level classification. To our knowledge, this is the
190
first attempt to explicitly use sense-level subjec-
tivity tags in contextual subjectivity and sentiment
analysis.
2 Background
We adopt the definitions of subjective and objec-
tive from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2008). Subjective expres-
sions are words and phrases being used to ex-
press mental and emotional states, such as spec-
ulations, evaluations, sentiments, and beliefs. A
general covering term for such states is private
state (Quirk et al, 1985), an internal state that
cannot be directly observed or verified by others.
(Wiebe and Mihalcea, 2006) give the following
examples:
(3) His alarm grew.
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President?s verbal assault on Israel.
What?s the catch?
Polarity (also called semantic orientation) is
also important to NLP applications. In review
mining, for example, we want to know whether
an opinion about a product is positive or negative.
Nonetheless, as argued by (Wiebe and Mihalcea,
2006; Su and Markert, 2008), there are also mo-
tivations for a separate subjective/objective (S/O)
classification.
First, expressions may be subjective but not
have any particular polarity. An example given by
(Wilson et al, 2005a) is Jerome says the hospi-
tal feels no different than a hospital in the states.
An NLP application system may want to find a
wide range of private states attributed to a person,
such as their motivations, thoughts, and specula-
tions, in addition to their positive and negative sen-
timents. Second, benefits for sentiment analysis
can be realized by decomposing the problem into
S/O (or neutral versus polar) and polarity classifi-
cation (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Wilson et al, 2005a; Kim and Hovy,
2006). We will see further evidence of this in Sec-
tion 4.2.3 in this paper.
The contextual subjectivity analysis experi-
ments in Section 4 include both S/O and polarity
classifications. The data used in those experiments
is from the MPQA Corpus (Wiebe et al, 2005;
Wilson, 2008),1 which consists of texts from the
world press annotated for subjective expressions.
1Available at http://www.cs.pitt.edu/mpqa
In the MPQA Corpus, subjective expressions of
varying lengths are marked, from single words to
long phrases. In addition, other properties are an-
notated, including polarity.
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjec-
tivity. As noted in (Wiebe and Mihalcea, 2006),
sentences containing objective senses may not be
objective. Thus, objective senses are defined as
follows: Classifying a sense as O means that,
when the sense is used in a text or conversation,
one does not expect it to express subjectivity and,
if the phrase or sentence containing it is subjective,
the subjectivity is due to something else. Finally,
classifying a sense as B means it covers both sub-
jective and objective usages.
The following subjective examples are given in
(Wiebe and Mihalcea, 2006):
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in
anticipation of some specific pain or danger (usually ac-
companied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
They give the following objective examples:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals
the occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particu-
lar purpose; ?the device is small enough to wear on your
wrist?; ?a device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was
only 10 fish?)
=> indefinite quantity ? (an estimated quantity)
Wiebe and Mihalcea performed an agreement
study and report that good agreement (?=0.74) can
be achieved between human annotators labeling
the subjectivity of senses. For a similar task, (Su
and Markert, 2008) also report good agreement
(?=0.79).
191
3 Subjectivity Word Sense
Disambiguation
3.1 Task Definition and Method
We now turn to SWSD, and our method for per-
forming it.
Note that SWSD is midway between pure dic-
tionary classification and pure contextual interpre-
tation. For SWSD, the context of the word is con-
sidered in order to perform the task, but the sub-
jectivity is determined solely by the dictionary. In
contrast, full contextual interpretation can deviate
from a sense?s subjectivity label in the dictionary.
As noted above, words used with objective senses
may appear in subjective expressions. For exam-
ple, an SWSD system would label the following
examples of alarm as S, O and O, respectively. On
the other hand, a sentence-level subjectivity clas-
sifier would label the sentences as S, S, and O, re-
spectively.
(4) His alarm grew.
Will someone shut that darn alarm off?
The alarm went off.
We use a supervised approach to SWSD. We
train a different classifier for each lexicon entry
for which we have training data. Thus, our ap-
proach is like targeted WSD (in contrast to all-
words WSD), with two labels: S and O.
We borrow machine learning features which
have been successfully used in WSD. Specifically,
given an ambiguous target word, we use the fol-
lowing features from (Mihalcea, 2002):
CW : the target word itself
CP : POS of the target word
CF : surrounding context of 3 words and their POS
HNP : the head of the noun phrase to which the
target word belongs
NB : the first noun before the target word
VB : the first verb before the target word
NA : the first noun after the target word
VA : the first verb after the target word
SK : at most 10 context words occurring at least 5
times; determined for each sense
3.2 Lexicon and Data
Our target words are members of a subjectivity
lexicon, because, since they are in such a lexicon,
we know they have subjective usages. Specifically,
we use the lexicon of (Wilson et al, 2005b; Wil-
son, 2008).2 The entries have been divided into
2Available at http://www.cs.pitt.edu/mpqa
those that are strongly subjective (strongsubj) and
those that are weakly subjective (weaksubj), re-
flecting their reliability as subjectivity clues. The
sources of the entries in the lexicon are identified
in (Wilson, 2008). In the second part of this pa-
per, we evaluate systems against the MPQA cor-
pus. Wilson also uses this corpus for her eval-
uations. To enable this, entries were added to
the lexicon independently from the MPQA corpus
(that is, none of the entries were derived using the
MPQA corpus).
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, indi-
cating whether they are used with a subjective or
objective sense. Because we do not have data la-
beled with the S/O coarse-grained senses and we
did not want to undertake the annotation effort at
this stage, we created an annotated corpus by com-
bining two types of sense annotations: (1) labels
of senses within a dictionary as S or O (i.e., sub-
jectivity sense labels), and (2) sense tags of word
instances in a corpus (i.e., sense-tagged data). The
subjectivity sense labels are used to collapse the
sense labels in the sense-tagged data into the two
new senses, S and O.
Our sense-tagged data are the lexical sample
corpora (training and test data) from SENSEVAL1
(Kilgarriff and Palmer, 2000), SENSEVAL2 (Preiss
and Yarowsky, 2001), and SENSEVAL3 (Mihal-
cea and Edmonds, 2004). We selected all of the
SENSEVAL words that are also in the subjectivity
lexicon, and labeled their dictionary senses as S,
O, or B according to the annotation scheme de-
scribed above in Section 2. We did this subjectiv-
ity sense labeling according to the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3).
Among the words, we found that 11 are not
ambiguous - either they have only S or only O
senses (in the corresponding sense inventory), or
the senses of their instances in the SENSEVAL data
are all S or all O. So as not to inflate our results, we
removed those 11 from the data, leaving 39 words.
In addition, we excluded the senses labeled B (a to-
tal of 10 senses). This leaves a total of 372 senses:
9 words (64 senses) from SENSEVAL1, 18 words
(201 senses) from SENSEVAL2, and 12 words (107
senses) from SENSEVAL3.
192
Base Acc SP SR SF OP OR OF IB EB(%)
All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8
S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2
S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8
S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0
Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,
SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute
improvement in Acc over Base; EB is percent error reduction in Acc.
3.3 SWSD Experiments
In this section, we evaluate our SWSD system, and
compare its performance to an WSD system on the
same data.
Note that, although generally in the SENSEVAL
datasets, training and test data are provided sep-
arately, a few target words from SENSEVAL1 do
not have both training and testing data. Thus, we
opted to combine the training and test data into one
dataset, and then perform 10-fold cross validation
experiments.
For our classifier, we use the SVM classifier
from the Weka package (Witten and Frank., 2005)
with its default settings.
We were interested in how well the system
would perform on more and less ambiguous
words. Thus, we split the words into three sub-
sets according to their majority-class baselines,
and report separate results: S1 (9 words), S2 (18
words), and S3 (12 words) have majority-class
baselines in the intervals [50%,70%) , [70%,90%),
and [90%,100%), respectively.
Table 1 contains the results, giving the overall
results (micro averages), as well as results for the
subsets S1, S2, and S3.
The improvement for SWSD over baseline is
especially high for the less skewed set, S1. This
is very encouraging because these words are the
more ambiguous words, and thus are the ones that
most need SWSD (assuming the SENSEVAL pri-
ors are similar to the priors in the corpus). The
average error reduction over baseline for S1 words
is 54.2%. Even for the more skewed sets S2 and
S3, reductions are 32.8% and 28.0%, respectively,
with an overall reduction of 41.8%.
To compare SWSD with WSD, we re-ran the
10-fold cross validation experiments, but this time
using the original sense labels, rather than S
and O. The (micro-averaged) accuracy is 67.9%,
much lower than the overall accuracy for SWSD
(88.3%).
The positive results provide evidence that
SWSD is a feasible variant of WSD, and that the
S/O sense groupings are natural ones, since the
system is able to learn to distinguish between them
with high accuracy. There is also potential for im-
provement by using a richer feature set, including
subjectivity features.
4 Opinion Analysis with Subjectivity
Word Sense Disambiguation
In this section, we explore the promise of SWSD
for contextual subjectivity analysis. First, we pro-
vide evidence that a subjectivity lexicon can have
substantial coverage of the subjective expressions
in a corpus, yet still be responsible for significant
subjectivity sense ambiguity in that corpus. Then,
we exploit SWSD in several contextual opinion
analysis systems, comparing the performance of
sense-aware and non-sense-aware versions. They
are all variations of components of the Opinion-
Finder opinion recognition system.3
4.1 Coverage and Ambiguity of Lexicon
Entries in the MPQA Corpus
In this section, we consider the distribution of lex-
icon entries in the MPQA corpus.
The lexicon covers a substantial subset of the
subjective expressions in the corpus: 67.1% of the
subjective expressions contain one or more lexi-
con entries.
On the other hand, fully 42.9% of the instances
of the lexicon entries in the MPQA corpus are
not in subjective expressions. An instance that
is not in a subjective expression is, by definition,
being used with an objective sense. Thus, these
instances are false hits of subjectivity clues. As
mentioned above, the entries in the lexicon have
been pre-classified as either more (strongsubj) or
less (weaksubj) reliable. We see this difference re-
flected in their degree of ambiguity ? 53% of the
3Available at http://www.cs.pitt.edu/opin
193
weaksubj instances are false hits, while only 22%
of the strongsubj instances are.
The high coverage of the lexicon demonstrates
its potential usefulness for opinion analysis sys-
tems, while its degree of ambiguity, in the form of
false hits in a subjectivity annotated corpus, shows
the potential benefit to opinion analysis of per-
forming SWSD.
As mentioned above, our experiments involve
only lexicon entries that are covered by the SEN-
SEVAL data, as we did not perform manual sense
tagging for this work. We have hope to expand
the system?s coverage in the future, as more word-
sense tagged data is produced (e.g., ONTONOTES
(Hovy et al, 2006)). We also have evidence that a
moderate amount of manual annotation would be
worth the effort. For example, let us order the lexi-
con entries from highest to lowest by frequency in
the MPQA corpus. The top 20 are responsible for
25% of all false hits in the corpus; the top 40 are
responsible for 34%; and the top 80 are responsi-
ble for 44%. If the SWSD system could be trained
for these words, the potential impact on reducing
false hits could be substantial, especially consid-
ering the good performance of the SWSD system
on the more ambiguous words. Note that we do
not want to simply discard these clues. The top 20
cover 9.4% of all subjective expressions; the top
40 cover 15.4%; and the top 80 cover 29.5%. Note
that SWSD only needs the data annotated with the
coarse-grained binary labels, which should be less
time consuming to produce than full word sense
tags.
4.2 Contextual Classification
We found in Section 3.3 that SWSD is a feasible
task and then in Section 4.1 that there is a great
deal of subjectivity sense ambiguity in a standard
subjectivity-annotated corpus (MPQA). We now
turn to exploiting the results of SWSD to automat-
ically recognize subjectivity and sentiment in the
MPQA corpus.
A motivation for using the MPQA data is that
many types of classifiers have been evaluated on
it, and we can directly test the effect of SWSD on
these classifiers.
Note that, for the SWSD experiments, the num-
ber of words does not limit the amount of data,
as SENSEVAL provides data for each word. How-
ever, the only parts of the MPQA corpus for which
SWSD could affect performance is the subset con-
taining instances of the words in the SWSD sys-
tem?s coverage. Thus, for the classifiers in this
section, the data used is the SenMPQA dataset,
which consists of the sentences in the MPQA Cor-
pus that contain at least one instance of the 39 key-
words. There are 689 such sentences (containing,
in total, 723 instances of the 39 keywords).
Even though this dataset is smaller than the one
used above, it gives us enough data to draw con-
clusions according to McNemar?s test for statisti-
cal significance.
4.2.1 Rule-based Classifier
We first apply SWSD to the rule-based classifier
from (Riloff and Wiebe, 2003). The classifier,
which is a sentence-level S/O classifier, has low
subjective and objective recall but high subjective
and objective precision. It is useful for creating
training data for subsequent processing by apply-
ing it to large amounts of unannotated data.
The classifier is a good candidate for directly
measuring the effects of SWSD on contextual sub-
jectivity analysis, because it classifies sentences
only by looking for the presence of subjectivity
keywords. Performance will improve if false hits
can be ignored.
The classifier labels a sentence as S if it contains
two or more strongsubj clues. On the other hand,
it considers three conditions to classify a sentence
as O: there are no strongsubj clues in the current
sentence, there are together at most one strongsubj
clue in the previous and next sentence, and there
are together at most 2 weaksubj clues in the cur-
rent, previous, and next sentence. A sentence that
is not labeled S or O is labeled unknown.
The rule-based classifier is made sense aware
by making it blind to the target word instances la-
beled O by the SWSD system, as these represent
false hits of subjectivity keywords. We compare
this sense-aware method (SE), with the original
classifier (O
RB
), in order to see if SWSD would
improve performance. We also built another modi-
fied rule-based classifier RE to demonstrate the ef-
fect of randomly ignoring subjectivity keywords.
RE ignores a keyword instance randomly with a
probability of 0.429, the expected value of false
hits in the MPQA corpus. The results are listed in
Table 2.
The rule-based classifier looks for the presence
of the keywords to find subjective sentences and
for the absence of the keywords to find objective
sentences. It is obvious that a variant working on
194
Acc OP OR OF SP SR SF
O
RB
27.0 50.0 4.1 7.6 92.7 36.0 51.8
SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6
RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2
Table 2: Effect of SWSD on the rule-based classi-
fiers.
fewer keyword instances than O
RB
will always
have the same or higher objective recall and the
same or lower subjective recall than O
RB
. That is
the case for both SE and RE. The real benefit we
see is in objective precision, which is substantially
higher for SE than O
RB
. For our experiments, OP
gives a better idea of the impact of SWSD, be-
cause most of the keyword instances SWSD dis-
ambiguates are weaksubj clues, and weaksubj key-
words figure more prominently in objective classi-
fication. On the other hand, RE has both lower OP
and SP than O
RB
. Note that accuracy for all three
systems is low, because all unknown predictions
are counted as incorrect.
These findings suggest that SWSD performs
well on disambiguating keyword instances in the
MPQA corpus,4 and demonstrates a positive im-
pact of SWSD on sentence-level subjectivity clas-
sification.
4.2.2 Subjective/Objective Classifier
We now move to more fine-grained expression-
level subjectivity classification. Since sentences
often contain multiple subjective expressions,
expression-level classification is more informative
than sentence-level classification.
The classifier in this section is an implementa-
tion of the neutral/polar supervised classifier of
(Wilson et al, 2005a) (using the same features),
except that the classes are S/O rather than neu-
tral/polar. These classifiers label instances of lex-
icon entries. The gold standard is defined on the
MPQA Corpus as follows: If an instance is in a
subjective expression, it is contextually S. If the
instance is in an objective expression, it is contex-
tually O. We evaluate the system on the 723 clue
instances in the SenMPQA dataset.
We incorporate SWSD information into the
contextual subjectivity classifier in a straight-
forward fashion: outputs are modified according
to simple, intuitive rules.
4which we cannot evaluate directly, as the MPQA corpus
is not sense tagged.
Our strategy is defined by the relation between
sense subjectivity and contextual subjectivity and
involves two rules, R1 and R2.
We know that a keyword instance used with a
S sense must be in a subjective expression. R1 is
to simply trust SWSD: If the contextual classifier
labels an instance as O, but SWSD determines that
it has an S sense, then R1 flips the contextual clas-
sifier?s label to S.
Things are not as simple in the case of O senses,
since they may appear in both subjective and ob-
jective expressions. We will state R2, and then ex-
plain it: If the contextual classifier labels an in-
stance as S, but (1) SWSD determines that it has
an O sense, (2) the contextual classifier?s confi-
dence is low, and (3) there is no other subjective
keyword in the same expression, then R2 flips the
contextual classifier?s label to O. First, consider
confidence: though a keyword with an O sense
may appear in either subjective or objective ex-
pressions, it is more likely to appear in an objec-
tive expression. We assume that this is reflected
to some extent in the contextual classifier?s confi-
dence. Second, if a keyword with an O sense ap-
pears in a subjective expression, then the subjec-
tivity is not due to that keyword but rather due to
something else. Thus, the presence of another lex-
icon entry ?explains away? the presence of the O
sense in the subjective expression, and we do not
want SWSD to overrule the contextual classifier.
Only when the contextual classifier isn?t certain
and only when there isn?t another keyword does
R2 flip the label to O.
Our definition of low confidence is in terms
of the label weights assigned by BoosTexter
(Schapire and Singer, 2000), which is the under-
lying machine learning algorithm of the classifier.
We use the difference between the largest label
weight and the second largest label weight as a
measure of confidence, as suggested in the Boos-
Texter documentation. The threshold we use is
0.0008.5
We apply the contextual classifier and the
SWSD system to the data, and compare the per-
formance of the original system (O
S/O
) and three
sense-aware variants: one using only R1, one us-
5As will be noted below, we experimented with three
thresholds for the classifier in Section 4.2.3, with no signif-
icant difference in accuracy. Here, we simply adopt 0.0008,
without further experimentation. In addition, we did not ex-
periment with other conditions than those incorporated in the
two rules in this section and the two rules in Section 4.2.3
below.
195
Acc OP OR OF SP SR SF
O
S/O
75.4 68.0 62.9 65.4 79.2 82.7 80.9
R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4
R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0
R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8
Table 3: Effect of SWSD on the subjec-
tive/objective classifier
ing only R2, and one using both (R1R2). The re-
sults are in Table 3. The R1 variant shows an im-
provement of 2.3 points in accuracy (a 9.4% error
reduction). The R2 variant shows an improvement
of 3.6 points in accuracy (a 14.6% error reduc-
tion). Applying both rules (R1R2) gives an im-
provement of 5.9 percentage points in accuracy (a
24% error reduction).
In our case, a paired t-test is not appropriate
to measure statistical significance, as we are not
doing multiple runs. Thus, we apply McNemar?s
test, which is a non-parametric method for algo-
rithms that can be executed only once, meaning
training once and testing once (Dietterich, 1998).
For R1, the improvement in accuracy is statisti-
cally significant at the p < .05 level. For R2 and
R1R2, the improvement in accuracy is statistically
significant at the p < .01 level. Moreover, in all
cases, we see improvement in both objective and
subjective F-measure.
4.2.3 Contextual Polarity Classifier
We now apply SWSD to contextual polarity clas-
sification (positive/negative/neutral), in the hope
that avoiding false hits of subjectivity keywords
will also lead to performance improvement in con-
textual sentiment analysis.
We use an implementation of the classifier of
(Wilson et al, 2005a). This classifier labels in-
stances of lexicon entries. The gold standard is
defined on the MPQA Corpus as follows: If an
instance is in a positive subjective expression, it
is contextually positive (Ps); if in a negative sub-
jective expression, it is contextually negative (Ng);
and if it is in an objective expression or a neu-
tral subjective expression, then it is contextually
N(eutral). As above, we evaluate the system on
the keyword instances in the SenMPQA dataset.
Wilson et al use a two step approach. The first
step classifies keyword instances as being in a po-
lar (positive or negative) or a neutral context. The
first step is performed by the neutral/polar classi-
fier mentioned above in Section 4.2.2. The sec-
ond step decides the contextual polarity (positive
or negative) of the instances classified as polar in
the first step, and is performed by a separate clas-
sifier.
To make a sense-aware version of the system,
we use rules to change some of the answers of the
neutral/polar classifier.
Unfortunately, we cannot simply trust SWSD
when it labels a keyword as an S sense, because an
S sense might be in a N(eutral) expression (since
there are neutral subjective expressions). But, an
S sense is more likely to appear in a P(olar) ex-
pression. Thus, we consider confidence (rule R3):
If the contextual classifier labels an instance as N,
but SWSD determines it has an S sense and the
contextual classifier?s confidence is low,6 then R3
flips the contextual classifier?s label to P.
Rule R4 is analogous to R2 in the previous sec-
tion: If the contextual classifier labels an instance
as P, but (1) SWSD determines that it has an O
sense, (2) the contextual classifier?s confidence is
low, and (3) there is no other subjective keyword in
the same expression, then R2 flips the contextual
classifier?s label to N.
We compare the performance of the original
neutral/polar classifier (O
N/P
) and sense-aware
variants using R3 and R4. The results are in Table
4. This time, the table does not include a combined
method, because only R4 improves performance.
This is consistent with the finding in (Wilson et
al., 2005a) that most errors are caused by subjec-
tivity keywords with non-neutral prior polarity ap-
pearing in phrases with neutral contextual polarity.
R4 targets these cases. It is promising to see that
SWSD provides enough information to fix some of
them. There is a 2.6 point improvement in accu-
racy (a 12.4% error reduction). The improvement
in accuracy is statistically significant at the p <
.01 level with McNemar?s test. The improvement
in accuracy is accompanied by improvements in
both neutral and polar F-measure.
We wanted to see if the improvements in the
6As in the previous section, low confidence is defined
in terms of the difference between the largest label weight
and the second largest label weight assigned by BoosTexter.
We tried three thresholds, 0.0007, 0.0008, and 0.0009, re-
sulting in only a slight difference in accuracy: 0.0007 and
0.0009 both give 81.5 accuracy compared to 81.6 accuracy
for 0.0008. We report results using 0.0008, though the ac-
curacy using the other thresholds is statistically significantly
better than the accuracy of the original classifier at the same
level.
196
Acc NP NR NF NgP NgR NgF PsP PsR PsF
O
Ps/Ng/N
77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0
R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0
Table 5: Effect of SWSD on the contextual polarity classifier
Acc NP NR NF PP PR PF
O
N/P
79.0 81.5 92.5 86.7 65.8 40.7 50.3
R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8
R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3
Table 4: Effect of SWSD on the neutral/polar clas-
sifier
first step of Wilson et als system can be propa-
gated to their second step, yielding an overall im-
provement in positive /negative/neutral (Ps/Ng/N)
classification.
The sense-aware variant of the overall two-part
system is the same as the original except that we
apply R4 to the output of the first step (flipping
some of the neutral/polar classifier?s P labels to
N). Thus, since the second step in Wilson et al?s
classifier processes only those instances labeled P
in the first step, in the sense-aware system, fewer
instances are passed from the first to the second
step.
Table 5 reports results for the original sys-
tem (O
Ps/Ng/N
) and the sense-aware variant (R4).
These results are for the entire SenMPQA dataset,
not just those labeled P in the first step.
The accuracy improves 3 percentage points (a
13.4% error reduction). The improvement in accu-
racy is statistically significant at the p < .01 level
with McNemar?s test. We see the real benefit when
we look at the precision of the positive and neg-
ative classes. Negative precision goes from 60.4
to 82.1 and positive precision goes from 52.2 to
68.6, with no loss in recall. This is evidence that
the SWSD system is doing a good job of removing
some false hits of subjectivity clues that harm the
original version of the system.
5 Comparisons to Previous Work
Several researchers exploit lexical resources for
contextual subjectivity and sentiment analysis.
These systems typically look for the presence of
subjective or sentiment-bearing words in the text.
They may rely only on this information (e.g.,
(Turney, 2002; Whitelaw et al, 2005; Riloff and
Wiebe, 2003)), or they may combine it with addi-
tional information as well (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Bloom et al,
2007; Wilson et al, 2005a)). We apply SWSD to
some of those systems to show the effect of SWSD
on contextual subjectivity and sentiment analysis.
Another set of related work is on subjectivity
and polarity labeling of word senses (e.g. (Esuli
and Sebastiani, 2006; Andreevskaia and Bergler,
2006; Wiebe and Mihalcea, 2006; Su and Markert,
2008)). They label senses of words in a dictionary.
In comparison, we label senses of word instances
in a corpus.
Moreover, our work extends findings in (Wiebe
and Mihalcea, 2006) and (Su and Markert, 2008).
(Wiebe and Mihalcea, 2006) demonstrates that
subjectivity is a property that can be associated
with word senses. We show that it is a natural
grouping of word senses and that it provides a
principled way for clustering senses. They also
demonstrate that subjectivity helps with WSD. We
show that a coarse-grained WSD variant (SWSD)
helps with subjectivity and sentiment analysis.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) show that even reliable subjectiv-
ity clues have objective senses. We demonstrate
that this ambiguity is also prevalent in a corpus.
Several researchers (e.g., (Palmer et al, 2004;
Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)) work on reducing the granularity of sense
inventories for WSD. They aim for a more coarse-
grained sense inventory to overcome performance
shortcomings related to fine-grained sense distinc-
tions. Our work is similar in the sense that we
reduce all senses of a word to two senses (S/O).
The difference is the criterion driving the group-
ing. Related work concentrates on syntactic and
semantic similarity between senses to group them.
In contrast, our grouping is driven by subjectivity
with a specific application area in mind, namely
subjectivity and sentiment analysis.
6 Conclusions and Future Work
We introduced the task of subjectivity word sense
disambiguation (SWSD), and evaluated a super-
vised method inspired by research in WSD. The
197
system achieves high accuracy, especially on
highly ambiguous words, and substantially outper-
forms WSD on the same data. The positive results
provide evidence that SWSD is a feasible variant
of WSD, and that the S/O sense groupings are nat-
ural ones.
We also explored the promise of SWSD for con-
textual subjectivity analysis. We showed that a
subjectivity lexicon can have substantial coverage
of the subjective expressions in the corpus, yet
still be responsible for significant sense ambiguity.
This demonstrates the potential benefit to opin-
ion analysis of performing SWSD. We then ex-
ploit SWSD in several contextual opinion analysis
systems, including positive/negative/neutral senti-
ment classification. Improvements in performance
were realized for all of the systems.
We plan several future directions which promise
to further increase the impact of SWSD on sub-
jectivity and sentiment analysis. We will manu-
ally annotate a moderate number of strategically
chosen words, namely frequent ones which are
highly ambiguous. In addition, we will add fea-
tures to the SWSD system reflecting the subjec-
tivity of the surrounding context. Finally, there
are more sophisticated strategies to explore for
improving subjectivity and sentiment analysis via
SWSD than the simple, intuitive rules we began
with in this paper.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0840632
and #0840608. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
References
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for a fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In (EACL-2006).
K. Bloom, N. Garg, and S. Argamon. 2007. Extracting
appraisal expressions. In HLT-NAACL 2007, pages
308?315, Rochester, NY.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895?1923.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In (LREC-06), Genova, IT.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, New York City.
A. Kilgarriff and M. Palmer, editors. 2000. Com-
puter and the Humanities. Special issue: SENSE-
VAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
S.-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In (COLING 2004), pages 1267?
1373, Geneva, Switzerland.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In (HLT/NAACL-06), pages
200?207, New York, New York.
R. Mihalcea and P. Edmonds, editors. 2004. Pro-
ceedings of SENSEVAL-3, Association for Compu-
tational Linguistics Workshop, Barcelona, Spain.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applica-
tions. In HLT-NAACL 2004 Workshop: 2nd Work-
shop on Scalable Natural Language Understanding,
Boston, Massachusetts.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In (ACL-04), pages 271?
278, Barcelona, ES. Association for Computational
Linguistics.
J. Preiss and D. Yarowsky, editors. 2001. Pro-
ceedings of SENSEVAL-2, Association for Compu-
tational Linguistics Workshop, Toulouse, France.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In (EMNLP-2003),
pages 105?112, Sapporo, Japan.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), Prague,
Czech Republic.
F. Su and K. Markert. 2008. From word to sense: a
case study of subjectivity recognition. In (COLING-
2008), Manchester.
198
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417?424, Philadelphia.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Pro-
ceedings of CIKM-05, the ACM SIGIR Conference
on Information and Knowledge Management, Bre-
men, DE.
J. Wiebe and R. Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, Syd-
ney, Australia.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Anno-
tating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005a. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In (HLT/EMNLP-2005), pages 347?354,
Vancouver, Canada.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005b. OpinionFinder: A system for subjec-
tivity analysis. In Proc. Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP-
2005) Companion Volume (software demonstration).
T. Wilson. 2008. Fine-grained Subjectivity and Sen-
timent Analysis: Recognizing the Intensity, Polarity,
and Attitudes of private states. Ph.D. thesis, Intelli-
gent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-03), pages 129?
136, Sapporo, Japan.
199
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1192?1201,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge
Samer Hassan and Rada Mihalcea
Department of Computer Science
University of North Texas
samer@unt.edu, rada@cs.unt.edu
Abstract
In this paper, we address the task of cross-
lingual semantic relatedness. We intro-
duce a method that relies on the informa-
tion extracted from Wikipedia, by exploit-
ing the interlanguage links available be-
tween Wikipedia versions in multiple lan-
guages. Through experiments performed
on several language pairs, we show that
the method performs well, with a perfor-
mance comparable to monolingual mea-
sures of relatedness.
1 Motivation
Given the accelerated growth of the number of
multilingual documents on the Web and else-
where, the need for effective multilingual and
cross-lingual text processing techniques is becom-
ing increasingly important. In this paper, we
address the task of cross-lingual semantic relat-
edness, and introduce a method that relies on
Wikipedia in order to calculate the relatedness of
words across languages. For instance, given the
word factory in English and the word lavoratore
in Italian (En. worker), the method can measure
the relatedness of these two words despite the fact
that they belong to two different languages.
Measures of cross-language relatedness are use-
ful for a large number of applications, including
cross-language information retrieval (Nie et al,
1999; Monz and Dorr, 2005), cross-language text
classification (Gliozzo and Strapparava, 2006),
lexical choice in machine translation (Och and
Ney, 2000; Bangalore et al, 2007), induction
of translation lexicons (Schafer and Yarowsky,
2002), cross-language annotation and resource
projections to a second language (Riloff et al,
2002; Hwa et al, 2002; Mohammad et al, 2007).
The method we propose is based on a measure
of closeness between concept vectors automati-
cally built from Wikipedia, which are mapped via
the Wikipedia interlanguage links. Unlike previ-
ous methods for cross-language mapping, which
are typically limited by the availability of bilingual
dictionaries or parallel texts, the method proposed
in this paper can be used to measure the related-
ness of word pairs in any of the 250 languages for
which a Wikipedia version exists.
The paper is organized as follows. We first pro-
vide a brief overview of Wikipedia, followed by
a description of the method to build concept vec-
tors based on this encyclopedic resource. We then
show how these concept vectors can be mapped
across languages for a cross-lingual measure of
word relatedness. Through evaluations run on six
language pairs, connecting English, Spanish, Ara-
bic and Romanian, we show that the method is ef-
fective at capturing the cross-lingual relatedness of
words, with results comparable to the monolingual
measures of relatedness.
2 Wikipedia
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative ef-
fort of a large number of volunteer contributors.
Virtually any Internet user can create or edit a
Wikipedia webpage, and this ?freedom of contri-
bution? has a positive impact on both the quantity
(fast-growing number of articles) and the quality
(potential errors are quickly corrected within the
collaborative environment) of this online resource.
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or
an event, and consists of a hypertext document
with hyperlinks to other pages within or outside
Wikipedia. The role of the hyperlinks is to guide
the reader to pages that provide additional infor-
mation about the entities or events mentioned in
an article. Articles are organized into categories,
which in turn are organized into hierarchies. For
instance, the article automobile is included in the
category vehicle, which in turn has a parent cate-
1192
Language Articles Users
English 2,221,980 8,944,947
German 864,049 700,980
French 765,350 546,009
Polish 579,170 251,608
Japanese 562,295 284,031
Italian 540,725 354,347
Dutch 519,334 216,938
Portuguese 458,967 503,854
Spanish 444,696 966,134
Russian 359,677 226,602
Table 1: Top ten largest Wikipedias
gory named machine, and so forth.
Each article in Wikipedia is uniquely referenced
by an identifier, consisting of one or more words
separated by spaces or underscores and occasion-
ally a parenthetical explanation. For example, the
article for bar with the meaning of ?counter for
drinks? has the unique identifier bar (counter).
Wikipedia editions are available for more than
250 languages, with a number of entries vary-
ing from a few pages to two millions articles or
more per language. Table 1 shows the ten largest
Wikipedias (as of December 2008), along with
the number of articles and approximate number of
contributors.1
Relevant for the work described in this paper are
the interlanguage links, which explicitly connect
articles in different languages. For instance, the
English article for bar (unit) is connected, among
others, to the Italian article bar (unit?a di misura)
and the Polish article bar (jednostka). On average,
about half of the articles in a Wikipedia version
include interlanguage links to articles in other lan-
guages. The number of interlanguage links per ar-
ticle varies from an average of five in the English
Wikipedia, to ten in the Spanish Wikipedia, and as
many as 23 in the Arabic Wikipedia.
3 Concept Vector Representations using
Explicit Semantic Analysis
To calculate the cross-lingual relatedness of two
words, we measure the closeness of their con-
cept vector representations, which are built from
Wikipedia using explicit semantic analysis (ESA).
Encyclopedic knowledge is typically organized
into concepts (or topics), each concept being
further described using definitions, examples,
1http://meta.wikimedia.org/wiki/List of Wikipedias
#Grand Total
and possibly links to other concepts. ESA
(Gabrilovich and Markovitch, 2007) relies on the
distribution of words inside the encyclopedic de-
scriptions, and builds semantic representations for
a given word in the form of a vector of the encyclo-
pedic concepts in which the word appears. In this
vector representation, each encyclopedic concept
is assigned with a weight, calculated as the term
frequency of the given word inside the concept?s
article.
Formally, let C be the set of all the Wikipedia
concepts, and let a be any content word. We define
~a as the ESA concept vector of term a:
~a = {w
c
1
, w
c
2
...w
c
n
} , (1)
where w
c
i
is the weight of the concept c
i
with re-
spect to a. ESA assumes the weight w
c
i
to be the
term frequency tf
i
of the word a in the article cor-
responding to concept c
i
.
We use a revised version of the ESA algorithm.
The original ESA semantic relatedness between
the words in a given word pair a ? b is defined as
the cosine similarity between their corresponding
vectors:
Relatedness(a, b) =
~a ?
~
b
?~a?
?
?
?
~
b
?
?
?
. (2)
To illustrate, consider for example the construc-
tion of the ESA concept vector for the word bird.
The top ten concepts containing this word, along
with the associated weight (calculated using equa-
tion 7), are listed in table 2. Note that the the ESA
vector considers all the possible senses of bird, in-
cluding Bird as a surname as in e.g., ?Larry Bird.?
Weight Wikipedia concept
51.4 Lists Of Birds By Region
44.8 Bird
40.3 British Birds Rarities Committee
32.8 Origin Of Birds
31.5 Ornithology
30.1 List Of Years In Birding And Ornithology
29.8 Bird Vocalization
27.4 Global Spread Of H5n1 In 2006
26.5 Larry Bird
22.3 Birdwatching
Table 2: Top ten Wikipedia concepts for the word
?bird?
In our ESA implementation, we make three
changes with respect to the original ESA algo-
rithm. First, we replace the cosine similarity with
1193
a Lesk-like metric (Lesk, 1986), which places less
emphasis on the distributional differences between
the vector weights and more emphasis on the over-
lap (mutual coverage) between the vector features,
and thus it is likely to be more appropriate for the
sparse ESA vectors, and for the possible asymme-
try between languages. Let a and b be two terms
with the corresponding ESA concept vectors ~A
and ~B respectively. Let A and B represent the sets
of concepts with a non-zero weight encountered in
~
A and ~B respectively. The coverage of ~A by ~B is
defined as:
G(
~
B|
~
A) =
?
i?B
w
a
i
(3)
and similarly, the coverage of ~B by ~A is:
G(
~
A|
~
B) =
?
i?A
w
b
i
(4)
where wa
i
and wb
i
represent the weight associ-
ated with concept c
i
in vectors ~A and ~B respec-
tively. By averaging these two asymmetric scores,
we redefine the relatedness as:
Relatedness(a, b) =
G(
~
B|
~
A) + G(
~
A|
~
B)
2
(5)
Second, we refine the ESA weighting schema
to account for the length of the articles describing
the concept. Since some concepts have lengthy
descriptions, they may be favored due to their high
term frequencies when compared to more compact
descriptions. To eliminate this bias, we calculate
the weight associated with a concept c
i
as follows:
w
c
i
= tf
i
? log(M/ |c
i
|), (6)
where tf
i
represents the term frequency of the
word a in concept c
i
, M is a constant representing
the maximum vocabulary size of Wikipedia con-
cepts, and |c
i
| is the size of the vocabulary used in
the description of concept c
i
.
Finally, we use the Wikipedia category graph
to promote category-type concepts in our feature
vectors. This is done by scaling the concept?s
weight by the inverse of the distance d
i
to the
root category. The concepts that are not categories
are treated as leaves, and therefore their weight is
scaled down by the inverse of the maximum depth
in the category graph. The resulting weighting
scheme is:
w
c
i
= tf
i
? log(M/ |c
i
|)/d
i
(7)
4 Cross-lingual Relatedness
We measure the relatedness of concepts in differ-
ent languages by using their ESA concept vector
representations in their own languages, along with
the Wikipedia interlanguage links that connect ar-
ticles written in a given language to their corre-
sponding Wikipedia articles in other languages.
For example, the English Wikipedia article moon
contains interlanguage links to Q ?

? in the Ara-
bic Wikipedia, luna in the Spanish Wikipedia, and
luna? in the Romanian Wikipedia. The interlan-
guage links can map concepts across languages,
and correspondingly map concept vector represen-
tations in different languages.
Formally, let C
x
and C
y
be the sets of all
Wikipedia concepts in languages x and y, with
corresponding translations in the y and x lan-
guages, respectively. If tr
xy
() is a translation
function that maps a concept c
i
? C
x
into the con-
cept c?
i
? C
y
via the interlanguage links, we can
write:
tr
xy
(c
i
) = c
?
i
, (8)
The projection of the ESA vector ~t from lan-
guage x onto y can be written as:
tr
xy
(
~
t) =
{
w
tr
xy
(c
1
)
...w
tr
xy
(c
n
)
}
. (9)
Using equations 5, 7, and 9, we can calculate the
cross-lingual semantic relatedness between any
two content terms a
x
and b
y
in given languages
x and y as:
sim(a
x
, b
y
) =
G(tr
yx
(
~
B)|
~
A) + G(
~
A|tr
yx
(
~
B))
2
.
(10)
Note that the weights assigned to Wikipedia
concepts inside the concept vectors are language
specific. That is, two Wikipedia concepts from
different languages, mapped via an interlanguage
link, can, and often do have different weights.
Intuitively, the relation described by the inter-
language links should be reflective and transi-
tive. However, due to Wikipedia?s editorial pol-
icy, which accredits users with the responsibility
1194
of maintaining the articles, these properties are not
always met. Table 3 shows real cases where the
transitive and the reflective properties fail due to
missing interlanguage links.
Relation Exists
Reflectivity
Kafr-El-Dawwar Battle(en) 7? P@ ?Y? @ Q ?? ??Q??(ar) Yes
P@

?Y? @ Q

??

??Q??(ar) 7? Kafr-El-Dawwar Battle(en) No
Transitivity
Intifada(en) 7? Intifada(es) Yes
Intifada(es) 7? ? ?A ?J K @(ar) Yes
Intifada(en) 7? ? ?A ?J K @(ar) No
Table 3: Reflectivity and transitivity in Wikipedia
We solve this problem by iterating over the
translation tables and extracting all the missing
links by enforcing the reflectivity and the transi-
tivity properties. Table 4 shows the initial number
of interlanguage links and the discovered links for
the four languages used in our experiments. The
table also shows the coverage of the interlanguage
links, measured as the ratio between the total num-
ber of interlanguage links (initial plus discovered)
originating in the source language towards the tar-
get language, divided by the total number of arti-
cles in the source language.
Interlanguage links
Language pair Initial Discov. Cover.
English ? Spanish 293,957 12,659 0.14
English ? Romanian 86,719 4,641 0.04
English ? Arabic 56,233 3,916 0.03
Spanish ? English 294,266 7,328 0.58
Spanish ? Romanian 39,830 3,281 0.08
Spanish ? Arabic 33,889 3,319 0.07
Romanian ? English 75,685 6,783 0.46
Romanian ? Spanish 36,002 3,546 0.22
Romanian ? Arabic 15,777 1,698 0.10
Arabic ? English 46,072 3,170 0.33
Arabic ? Spanish 28,142 3,109 0.21
Arabic ? Romanian 15,965 1,970 0.12
Table 4: Interlanguage links (initial and discov-
ered) and their coverage in Wikipedia versions in
four languages.
5 Experiments and Evaluations
We run our experiments on four languages: En-
glish, Spanish, Romanian and Arabic. For each
of these languages, we use a Wikipedia down-
load from October 2008. The articles were pre-
processed using Wikipedia Miner (Milne, 2007)
to extract structural information such as general-
ity, and interlanguage links. Furthermore, arti-
cles were also processed to remove numerical con-
tent, as well as any characters not included in the
given language?s alphabet. The content words are
stemmed, and words shorter than three characters
are removed (a heuristic which we use as an ap-
proximation for stopword removal). Table 5 shows
the number of articles in each Wikipedia version
and the size of their vocabularies, as obtained af-
ter the pre-processing step.
Articles Vocabulary
English 2, 221, 980 1, 231, 609
Spanish 520, 154 406, 134
Arabic 149, 340 216, 317
Romanian 179, 440 623, 358
Table 5: Number of articles and size of vocabulary
for the four Wikipedia versions
After pre-processing, the articles are indexed
to generate the ESA concept vectors. From each
Wikipedia version, we also extract other features
including article titles, interlanguage links, and
Wikipedia category graphs. The interlanguage
links are further processed to recover any missing
links, as described in the previous section.
5.1 Data
For the evaluation, we build several cross-lingual
datasets based on the standard Miller-Charles
(Miller and Charles, 1998) and WordSimilarity-
353 (Finkelstein et al, 2001) English word relat-
edness datasets.
The Miller-Charles dataset (Miller and Charles,
1998) consists of 30-word pairs ranging from syn-
onymy pairs (e.g., car - automobile) to completely
unrelated terms (e.g., noon - string). The relat-
edness of each word pair was rated by 38 hu-
man subjects, using a scale from 0 (not-related)
to 4 (perfect synonymy). The dataset is avail-
able only in English and has been widely used
in previous semantic relatedness evaluations (e.g.,
(Resnik, 1995; Hughes and Ramage, 2007; Zesch
et al, 2008)).
The WordSimilarity-353 dataset (also known as
Finkelstein-353) (Finkelstein et al, 2001) consists
of 353 word pairs annotated by 13 human experts,
on a scale from 0 (unrelated) to 10 (very closely
related or identical). The Miller-Charles set is a
subset in the WordSimilarity-353 data set. Unlike
the Miller-Charles data set, which consists only of
1195
Word pair
English coast - shore car - automobile brother - monk
Spanish costa - orilla coche - automovil hermano - monje
Arabic ?gA ? - Z?

?A


?

?PA

J

? - ?K
.
Q?

?J


?

? - I
.
?@

P
Romanian t?a?rm - mal mas?fina? - automobil frate - ca?luga?r
Table 6: Word pair translation examples
single words, the WordSimilarity-353 set alo fea-
tures phrases (e.g., ?Wednesday news?), therefore
posing an additional degree of difficulty for a re-
latedness metric applied on this data.
Native speakers of Spanish, Romanian and Ara-
bic, who were also highly proficient in English,
were asked to translate the words in the two data
sets. The annotators were provided one word pair
at a time, and asked to provide the appropriate
translation for each word while taking into account
their relatedness within the word pair. The relat-
edness was meant as a hint to disambiguate the
words, when multiple translations were possible.
The annotators were also instructed not to use
multi-word expressions in their translations. They
were also allowed to use replacement words to
overcome slang or culturally-biased terms. For ex-
ample, in the case of the word pair dollar-buck,
annotators were allowed to use PA


JK

X
2 as a transla-
tion for buck.
To test the ability of the bilingual judges to pro-
vide correct translations by using this annotation
setting, we carried out the following experiment.
We collected Spanish translations from five differ-
ent human judges, which were then merged into
a single selection based on the annotators? trans-
lation agreement; the merge was done by a sixth
human judge, who also played the role of adjudi-
cator when no agreement was reached between the
initial annotators.
Subsequently, five additional human experts re-
scored the word-pair Spanish translations by using
the same scale that was used in the construction of
the English data set. The correlation between the
2Arabic for dinars ? the commonly used currency in the
Middle East.
relatedness scores assigned during this experiment
and the scores assigned in the original English ex-
periment was 0.86, indicating that the translations
provided by the bilingual judges were correct and
preserved the word relatedness.
For the translations provided by the five human
judges, in more than 74% of the cases at least three
human judges agreed on the same translation for a
word pair. When the judges did not provide iden-
tical translations, they typically used a close syn-
onym. The high agreement between their trans-
lations indicates that the annotation setting was
effective in pinpointing the correct translation for
each word, even in the case of ambiguous words.
Motivated by the validation of the annotation
setting obtained for Spanish, we used only one hu-
man annotator to collect the translations for Arabic
and Romanian. Table 6 shows examples of trans-
lations in the three languages for three word pairs
from our data sets.
Using these translations, we create six cross-
lingual data sets, one for each possible language
pair (English-Spanish, English-Arabic, English-
Romanian, Spanish-Arabic, Spanish-Romanian,
Arabic-Romanian). Given a source-target lan-
guage pair, a data set is created by first using the
source language for the first word and the target
language for the second word, and then reversing
the order, i.e., using the source language for the
second word and the target language for the first
word. The size of the data sets is thus doubled
in this way (e.g., the 30 word pairs in the English
Miller-Charles set are transformed into 60 word
pairs in the English-Spanish Miller-Charles set).
5.2 Results
We evaluate the cross-lingual measure of related-
ness on each of the six language pairs. For com-
parison purposes, we also evaluate the monolin-
gual relatedness on the four languages.
For the evaluation, we use the Pearson (r)
and Spearman (?) correlation coefficients, which
are the standard metrics used in the past for the
evaluation of semantic relatedness (Finkelstein et
1196
al., 2001; Zesch et al, 2008; Gabrilovich and
Markovitch, 2007). While the Pearson correla-
tion is highly dependent on the linear relationship
between the distributions in question, Spearman
mainly emphasizes the ability of the distributions
to maintain their relative ranking.
Tables 7 and 8 show the results of the evalua-
tions of the cross-lingual relatedness, when using
an ESA concept vector with a size of maximum
10,000 concepts.3
English Spanish Arabic Romanian
Miller-Charles
English 0.58 0.43 0.32 0.50
Spanish 0.44 0.20 0.38
Arabic 0.36 0.32
Romanian 0.58
WordSimilarity-353
English 0.55 0.32 0.31 0.29
Spanish 0.45 0.32 0.28
Arabic 0.28 0.25
Romanian 0.30
Table 7: Pearson correlation for cross-
lingual relatedness on the Miller-Charles and
WordSimilarity-353 data sets
English Spanish Arabic Romanian
Miller-Charles
English 0.75 0.56 0.27 0.55
Spanish 0.64 0.17 0.32
Arabic 0.33 0.21
Romanian 0.61
WordSimilarity-353
English 0.71 0.55 0.35 0.38
Spanish 0.50 0.29 0.30
Arabic 0.26 0.20
Romanian 0.28
Table 8: Spearman correlation for cross-
lingual relatedness on the Miller-Charles and
WordSimilarity-353 data sets
As a validation of our ESA implementation, we
compared the results obtained for the monolingual
English relatedness with other results reported in
the past for the same data sets. Gabrilovich and
Markovitch (2007) reported a Spearman correla-
tion of 0.72 for the Miller-Charles data set and
0.75 for the WordSimilarity-353 data set, respec-
3The concepts are selected in reversed order of their
weight inside the vector in the respective language. Note that
the cross-lingual mapping between the concepts in the ESA
vectors is done after the selection of the top 10,000 concepts
in each language.
tively. Zesch et al (2008) reported a Spear-
man correlation of 0.67 for the Miller-Charles set.
These values are comparable to the Spearman cor-
relation scores obtained in our experiments for the
English data sets (see Table 8), with a fairly large
improvement obtained on the Miller-Charles data
set when using our implementation.
6 Discussion
Overall, our method succeeds in capturing the
cross-lingual semantic relatedness between words.
As a point of comparison, one can use the mono-
lingual measures of relatedness as reflected by the
diagonals in Tables 7 and 8.
Looking at the monolingual evaluations, the re-
sults seem to be correlated with the Wikipedia size
for the corresponding language, with the English
measure scoring the highest. These results are not
surprising, given the direct relation between the
Wikipedia size and the sparseness of the ESA con-
cept vectors. A similar trend is observed for the
cross-lingual relatedness, with higher results ob-
tained for the languages with large Wikipedia ver-
sions (e.g., English-Spanish), and lower results for
the languages with a smaller size Wikipedia (e.g.,
Arabic-Spanish).
For comparison, we ran two additional experi-
ments. In the first experiment, we compared the
coverage of our cross-lingual relatedness method
to a direct use of the translation links available in
Wikipedia. The cross-lingual relatedness is turned
into a monolingual relatedness by using the in-
terlanguage Wikipedia links to translate the first
of the two words in a cross-lingual pair into the
language of the second word in the pair.4 From
the total of 433 word pairs available in the two
data sets, this method can produce translations
for an average of 103 word pairs per language
pair. This means that the direct Wikipedia inter-
language links allow the cross-lingual relatedness
measure to be transformed into a monolingual re-
latedness in about 24% of the cases, which is a
low coverage compared to the full coverage that
can be obtained with our cross-lingual method of
relatedness.
In an attempt to raise the coverage of the trans-
lation, we ran a second experiment where we used
a state-of-the-art translation engine to translate the
first word in a pair into the language of the sec-
4We use all the interlanguage links obtained by combining
the initial and the discovered links, as described in Section 4.
1197
ond word in the pair. We use Google Translate,
which is a statistical machine translation engine
that relies on large parallel corpora, to find the
most likely translation for a given word. Unlike
the previous experiment, this time we can achieve
full translation coverage, and thus we are able to
produce data sets of equal size that can be used
for a comparison between relatedness measures.
Specifically, using the translation produced by the
machine translation engine for the first word in a
pair, we calculate the relatedness within the space
of the language of the second word using a mono-
lingual ESA also based on Wikipedia. The results
obtained with this method are compared against
the results obtained with our cross-lingual ESA re-
latedness.
Using a Pearson correlation, our cross-lingual
relatedness method achieves an average score
across all six language pairs of 0.36 for the Miller-
Charles data set and 0.30 for the WordSimilarity-
353 data set,5 which is higher than the 0.33 and
0.28 scores achieved for the same data sets when
using a translation obtained with Google Trans-
late followed by a monolingual measure of re-
latedness. These results are encouraging, also
given that the translation-based method is limited
to those language pairs for which a translation en-
gine exists (e.g., Google Translate covers 40 lan-
guages), whereas our method can be applied to any
language pair from the set of 250 languages for
which a Wikipedia version exists.
To gain further insights, we also determined the
impact of the vector length in the ESA concept
vector representation, by calculating the Pearson
correlation for vectors of different lengths. Fig-
ures 1 and 2 show the Pearson score as a func-
tion of the vector length for the Miller-Charles
and WordSimilarity-353 data sets. The plots show
that the cross-lingual measure of relatedness is not
significantly affected by the reduction or increase
of the vector length. Thus, the use of vectors of
length 10,000 (as used in most of our experiments)
appears as a reasonable tradeoff between accuracy
and performance.
Furthermore, by comparing the performance of
the proposed Lesk-like model to the traditional
cosine-similarity (Figures 3 and 4), we note that
the Lesk-like model outperforms the cosine model
on most language pairs. We believe that this is
5This average considers all the cross-lingual relatedness
scores listed in Table 7; it does not include the monolingual
scores listed on the table diagonal.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 5000  10000  15000  20000  25000  30000
Pe
ar
so
n 
co
rre
la
tio
n
Vector length
ar?ar
ar?en
ar?es
ar?ro
en?en
en?es
es?es
es?ro
en?ro
ro?ro
Figure 1: Pearson correlation vs. ESA vector
length on the Miller-Charles data set
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 5000  10000  15000  20000  25000  30000
Av
er
ag
e 
Pe
ar
so
n
Vector Size
ar?ar
ar?en
ar?es
ar?ro
en?en
en?es
es?es
es?ro
en?ro
ro?ro
Figure 2: Pearson correlation vs. ESA vector
length on the WordSimilarity-353 data set
due to the stricter correlation conditions imposed
by the cosine-metric in such sparse vector-based
representations, as compared to the more relaxed
hypothesis used by the Lesk model.
Finally, we also looked at the relation between
the number of interlanguage links found for the
concepts in a vector and the length of the vector.
Figures 5 and 6 display the average number of in-
terlanguage links as a function of the concept vec-
tor length.
By analyzing the effect of the average number
of interlanguage links found per word in the given
datasets (Figures 5 and 6), we notice that these
links increase proportionally with the vector size,
as expected. However, this increase does not lead
to any significant improvements in accuracy (Fig-
ures 1 and 2). This implies that while the presence
of interlanguage links is a prerequisite for the mea-
1198
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 5000  10000  15000  20000  25000  30000
Pe
ar
so
n
Vector Size
lsk(ar?ar)
lsk(en?en)
lsk(es?es)
lsk(ro?ro)
cos(ar?ar)
cos(en?en)
cos(es?es)
cos(ro?ro)
Figure 3: Lesk vs. cosine similarity for the Miller-
Charles data set
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 5000  10000  15000  20000  25000  30000
Pe
ar
so
n
Vector Size
lsk(ar?ar)
lsk(en?en)
lsk(es?es)
lsk(ro?ro)
cos(ar?ar)
cos(en?en)
cos(es?es)
cos(ro?ro)
Figure 4: Lesk vs. cosine similarity for the
WordSimilarity-353 data set
sure of relatedness,6 their effect is only significant
for the top ranked concepts in a vector. Therefore,
increasing the vectors size to maximize the match-
ing of the projected dimensions does not necessar-
ily lead to accuracy improvements.
7 Related Work
Measures of word relatedness were found useful in
a large number of natural language processing ap-
plications, including word sense disambiguation
(Patwardhan et al, 2003), synonym identification
(Turney, 2001), automated essay scoring (Foltz et
al., 1999), malapropism detection (Budanitsky and
Hirst, 2001), coreference resolution (Strube and
Ponzetto, 2006), and others. Most of the work to
date has focused on measures of word relatedness
for English, by using methods applied on knowl-
6Two languages with no interlanguage links between
them will lead to a relatedness score of zero for any word
pair across these languages, no matter how strongly related
the words are.
 0
 500
 1000
 1500
 2000
 5000  10000  15000  20000  25000  30000
N
um
be
r o
f i
nt
er
la
ng
ua
ge
 lin
ks
Vector length
ar?en
ar?es
ar?ro
en?es
en?ro
es?ro
Figure 5: Number of interlanguage links vs. vec-
tor length for the Miller-Charles data set
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 5000  10000  15000  20000  25000  30000
N
um
be
r o
f i
nt
er
la
ng
ua
ge
 lin
ks
Vector length
ar?en
ar?es
ar?ro
en?es
en?ro
es?ro
Figure 6: Number of interlanguage links vs. vec-
tor length for the WordSimilarity-353 data set
edge bases (Lesk, 1986; Wu and Palmer, 1994;
Resnik, 1995; Jiang and Conrath, 1997; Hughes
and Ramage, 2007) or on large corpora (Salton
et al, 1997; Landauer et al, 1998; Turney, 2001;
Gabrilovich and Markovitch, 2007).
Although to a lesser extent, measures of word
relatedness have also been applied on other lan-
guages, including German (Zesch et al, 2007;
Zesch et al, 2008; Mohammad et al, 2007), Chi-
nese (Wang et al, 2008), Dutch (Heylen et al,
2008) and others. Moreover, assuming resources
similar to those available for English, e.g., Word-
Net structures or large corpora, the measures of
relatedness developed for English can be in prin-
ciple applied to other languages as well.
All these methods proposed in the past have
been concerned with monolingual word related-
ness calculated within the boundaries of one lan-
guage, as opposed to cross-lingual relatedness,
which is the focus of our work.
The research area closest to the task of cross-
1199
lingual relatedness is perhaps cross-language in-
formation retrieval, which is concerned with
matching queries posed in one language to docu-
ment collections in a second language. Note how-
ever that most of the approaches to date for cross-
language information retrieval have been based on
direct translations obtained for words in the query
or in the documents, by using bilingual dictionar-
ies (Monz and Dorr, 2005) or parallel corpora (Nie
et al, 1999). Such explicit translations can iden-
tify a direct correspondence between words in two
languages (e.g., they will find that fabbrica (It.)
and factory (En.) are translations of each other),
but will not capture similarities of a different de-
gree (e.g., they will not find that lavoratore (It.;
worker in En.) is similar to factory (En.).
Also related are the areas of word alignment
for machine translation (Och and Ney, 2000),
induction of translation lexicons (Schafer and
Yarowsky, 2002), and cross-language annotation
projections to a second language (Riloff et al,
2002; Hwa et al, 2002; Mohammad et al,
2007). As with cross-language information re-
trieval, these areas have primarily considered di-
rect translations between words, rather than an en-
tire spectrum of relatedness, as we do in our work.
8 Conclusions
In this paper, we addressed the problem of
cross-lingual semantic relatedness, which is a
core task for a number of applications, includ-
ing cross-language information retrieval, cross-
language text classification, lexical choice for ma-
chine translation, cross-language projections of re-
sources and annotations, and others.
We introduced a method based on concept vec-
tors built from Wikipedia, which are mapped
across the interlanguage links available between
Wikipedia versions in multiple languages. Ex-
periments performed on six language pairs, con-
necting English, Spanish, Arabic and Romanian,
showed that the method is effective at captur-
ing the cross-lingual relatedness of words. The
method was shown to be competitive when com-
pared to methods based on a translation using the
direct Wikipedia links or using a statistical trans-
lation engine. Moreover, our method has wide ap-
plicability across languages, as it can be used for
any language pair from the set of 250 languages
for which a Wikipedia version exists.
The cross-lingual data sets introduced
in this paper can be downloaded from
http://lit.csci.unt.edu/index.php/Downloads.
Acknowledgments
The authors are grateful to Carmen Banea for her
help with the construction of the data sets. This
material is based in part upon work supported by
the National Science Foundation CAREER award
#0747340. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion.
References
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statis-
tical machine translation through global lexical se-
lection and sentence reconstruction. In Proceedings
of the Annual Meeting of the Association of Compu-
tational Linguistics, Prague, Czech Republic.
A. Budanitsky and G. Hirst. 2001. Semantic distance
in WordNet: An experimental, application-oriented
evaluation of five measures. In Proceedings of the
NAACL Workshop on WordNet and Other Lexical
Resources, Pittsburgh.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In
WWW, pages 406?414.
P. Foltz, D. Laham, and T. Landauer. 1999. Automated
essay scoring: Applications to educational technol-
ogy. In Proceedings of World Conference on Edu-
cational Multimedia, Hypermedia and Telecommu-
nications, Chesapeake, Virginia.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611.
A. Gliozzo and C. Strapparava. 2006. Exploiting com-
parable corpora and bilingual dictionaries for cross-
language text categorization. In Proceedings of the
Conference of the Association for Computational
Linguistics, Sydney, Australia.
K. Heylen, Y. Peirsman, D. Geeraerts, and D. Speel-
man. 2008. Modelling word similarity: an evalu-
ation of automatic synonymy extraction algorithms.
In Proceedings of the Sixth International Language
Resources and Evaluation, Marrakech, Morocco.
T. Hughes and D. Ramage. 2007. Lexical semantic
knowledge with random graph walks. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, Prague, Czech Republic.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2002), Philadelphia, July.
1200
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Intro-
duction to latent semantic analysis. Discourse Pro-
cesses, 25.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
G. Miller and W. Charles. 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1).
D. Milne. 2007. Computing semantic relatedness us-
ing wikipedia link structure. In European Language
Resources Association (ELRA), editor, In Proceed-
ings of the New Zealand Computer Science Re-
search Student Conference (NZCSRSC 2007), New
Zealand.
S. Mohammad, I. Gurevych, G. Hirst, and T. Zesch.
2007. Cross-lingual distributional profiles of
concepts for measuring semantic distance. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL-2007), Prague, Czech Republic.
C. Monz and B.J. Dorr. 2005. Iterative translation
disambiguation for cross-language information re-
trieval. In Proceedings of the 28th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, Salvador,
Brazil.
J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999.
Cross-language information retrieval based on paral-
lel texts and automatic mining of parallel texts from
the Web. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval.
F. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), Saar-
brucken, Germany, August.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico
City, February.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. In-
ducing information extraction systems for new lan-
guages via cross-language projection. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, Taipei, Taiwan, August.
G. Salton, A. Wong, and C.S. Yang. 1997. A vec-
tor space model for automatic indexing. In Read-
ings in Information Retrieval, pages 273?280. Mor-
gan Kaufmann Publishers, San Francisco, CA.
C. Schafer and D. Yarowsky. 2002. Inducing trans-
lation lexicons via diverse similarity measures and
bridge languages. In Proceedings of the 6th Confer-
ence on Natural Language Learning (CoNLL 2003),
Taipei, Taiwan, August.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedeness using Wikipedia. In
Proceedings of the American Association for Artifi-
cial Intelligence, Boston, MA.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), Freiburg, Germany.
X. Wang, S. Ju, and S. Wu. 2008. A survey of chi-
nese text similarity computation. In Proceedings of
the Asia Information Retrieval Symposium, Harbin,
China.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, Las Cruces, New Mexico.
T. Zesch, I. Gurevych, and M. Mu?hlha?user. 2007.
Comparing Wikipedia and German Wordnet by
Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings of Human Language Tech-
nologies: The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
T. Zesch, C. Mu?ller, and I. Gurevych. 2008. Using
Wiktionary for Computing Semantic Relatedness.
In Proceedings of the American Association for Ar-
tificial Intelligence, Chicago.
1201
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 567?575,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Text-to-text Semantic Similarity for Automatic Short Answer Grading
Michael Mohler and Rada Mihalcea
Department of Computer Science
University of North Texas
mgm0038@unt.edu, rada@cs.unt.edu
Abstract
In this paper, we explore unsupervised
techniques for the task of automatic short
answer grading. We compare a number of
knowledge-based and corpus-based mea-
sures of text similarity, evaluate the effect
of domain and size on the corpus-based
measures, and also introduce a novel tech-
nique to improve the performance of the
system by integrating automatic feedback
from the student answers. Overall, our
system significantly and consistently out-
performs other unsupervised methods for
short answer grading that have been pro-
posed in the past.
1 Introduction
One of the most important aspects of the learn-
ing process is the assessment of the knowledge
acquired by the learner. In a typical examination
setting (e.g., an exam, assignment or quiz), this
assessment implies an instructor or a grader who
provides students with feedback on their answers
to questions that are related to the subject mat-
ter. There are, however, certain scenarios, such
as the large number of worldwide sites with lim-
ited teacher availability, or the individual or group
study sessions done outside of class, in which an
instructor is not available and yet students need an
assessment of their knowledge of the subject. In
these instances, we often have to turn to computer-
assisted assessment.
While some forms of computer-assisted assess-
ment do not require sophisticated text understand-
ing (e.g., multiple choice or true/false questions
can be easily graded by a system if the correct so-
lution is available), there are also student answers
that consist of free text which require an analy-
sis of the text in the answer. Research to date has
concentrated on two main subtasks of computer-
assisted assessment: the grading of essays, which
is done mainly by checking the style, grammati-
cality, and coherence of the essay (cf. (Higgins
et al, 2004)), and the assessment of short student
answers (e.g., (Leacock and Chodorow, 2003; Pul-
man and Sukkarieh, 2005)), which is the focus of
this paper.
An automatic short answer grading system is
one which automatically assigns a grade to an an-
swer provided by a student through a comparison
with one or more correct answers. It is important
to note that this is different from the related task of
paraphrase detection, since a requirement in stu-
dent answer grading is to provide a grade on a cer-
tain scale rather than a binary yes/no decision.
In this paper, we explore and evaluate a set of
unsupervised techniques for automatic short an-
swer grading. Unlike previous work, which has
either required the availability of manually crafted
patterns (Sukkarieh et al, 2004; Mitchell et al,
2002), or large training data sets to bootstrap such
patterns (Pulman and Sukkarieh, 2005), we at-
tempt to devise an unsupervised method that re-
quires no human intervention. We address the
grading problem from a text similarity perspec-
tive and examine the usefulness of various text-
to-text semantic similarity measures for automati-
cally grading short student answers.
Specifically, in this paper we seek answers to
the following questions. First, given a number
of corpus-based and knowledge-based methods as
previously proposed in the past for word and text
semantic similarity, what are the measures that
work best for the task of short answer grading?
Second, given a corpus-based measure of similar-
ity, what is the impact of the domain and the size
of the corpus on the accuracy of the measure? Fi-
nally, can we use the student answers themselves
to improve the quality of the grading system?
2 Related Work
There are a number of approaches that have been
proposed in the past for automatic short answer
grading. Several state-of-the-art short answer
graders (Sukkarieh et al, 2004; Mitchell et al,
2002) require manually crafted patterns which, if
matched, indicate that a question has been an-
swered correctly. If an annotated corpus is avail-
567
able, these patterns can be supplemented by learn-
ing additional patterns semi-automatically. The
Oxford-UCLES system (Sukkarieh et al, 2004)
bootstraps patterns by starting with a set of key-
words and synonyms and searching through win-
dows of a text for new patterns. A later implemen-
tation of the Oxford-UCLES system (Pulman and
Sukkarieh, 2005) compares several machine learn-
ing techniques, including inductive logic program-
ming, decision tree learning, and Bayesian learn-
ing, to the earlier pattern matching approach with
encouraging results.
C-Rater (Leacock and Chodorow, 2003)
matches the syntactical features of a student
response (subject, object, and verb) to that of a
set of correct responses. The method specifically
disregards the bag-of-words approach to take
into account the difference between ?dog bites
man? and ?man bites dog? while trying to detect
changes in voice (?the man was bitten by a dog?).
Another short answer grading system, AutoTu-
tor (Wiemer-Hastings et al, 1999), has been de-
signed as an immersive tutoring environment with
a graphical ?talking head? and speech recogni-
tion to improve the overall experience for students.
AutoTutor eschews the pattern-based approach en-
tirely in favor of a bag-of-words LSA approach
(Landauer and Dumais, 1997). Later work on Au-
toTutor (Wiemer-Hastings et al, 2005; Malatesta
et al, 2002) seeks to expand upon the original bag-
of-words approach which becomes less useful as
causality and word order become more important.
These methods are often supplemented with
some light preprocessing, e.g., spelling correc-
tion, punctuation correction, pronoun resolution,
lemmatization and tagging. Likewise, in order to
facilitate their goals of providing feedback to the
student more robust than a simple ?correct? or ?in-
correct,? several systems break the gold-standard
answers into constituent concepts that must indi-
vidually be matched for the answer to be consid-
ered fully correct (Callear et al, 2001). In this way
the system can determine which parts of an answer
a student understands and which parts he or she is
struggling with.
Automatic short answer grading is closely re-
lated to the task of text similarity. While more
general than short answer grading, text similarity
is essentially the problem of detecting and com-
paring the features of two texts. One of the earli-
est approaches to text similarity is the vector-space
model (Salton et al, 1997) with a term frequency
/ inverse document frequency (tf.idf) weighting.
This model, along with the more sophisticated
LSA semantic alternative (Landauer and Dumais,
1997), has been found to work well for tasks such
as information retrieval and text classification.
Another approach (Hatzivassiloglou et al,
1999) has been to use a machine learning algo-
rithm in which features are based on combina-
tions of simple features (e.g., a pair of nouns ap-
pear within 5 words from one another in both
texts). This method also attempts to account for
synonymy, word ordering, text length, and word
classes.
Another line of work attempts to extrapolate
text similarity from the arguably simpler prob-
lem of word similarity. (Mihalcea et al, 2006)
explores the efficacy of applying WordNet-based
word-to-word similarity measures (Pedersen et al,
2004) to the comparison of texts and found them
generally comparable to corpus-based measures
such as LSA.
An interesting study has been performed at the
University of Adelaide (Lee et al, 2005), compar-
ing simpler word and n-gram feature vectors to
LSA and exploring the types of vector similarity
metrics (e.g., binary vs. count vectors, Jaccard
vs. cosine vs. overlap distance measure, etc.).
In this case, LSA was shown to perform better
than the word and n-gram vectors and performed
best at around 100 dimensions with binary vectors
weighted according to an entropy measure, though
the difference in measures was often subtle.
SELSA (Kanejiya et al, 2003) is a system that
attempts to add context to LSA by supplementing
the feature vectors with some simple syntactical
features, namely the part-of-speech of the previous
word. Their results indicate that SELSA does not
perform as well as LSA in the best case, but it has
a wider threshold window than LSA in which the
system can be used advantageously.
Finally, explicit semantic analysis (ESA)
(Gabrilovich and Markovitch, 2007) uses
Wikipedia as a source of knowledge for text
similarity. It creates for each text a feature vector
where each feature maps to a Wikipedia article.
Their preliminary experiments indicated that ESA
was able to significantly outperform LSA on some
text similarity tasks.
3 Data Set
In order to evaluate the methods for short answer
grading, we have created a data set of questions
from introductory computer science assignments
with answers provided by a class of undergradu-
ate students. The assignments were administered
as part of a Data Structures course at the Univer-
sity of North Texas. For each assignment, the stu-
dent answers were collected via the WebCT online
learning environment.
568
The evaluations reported in this paper are car-
ried out on the answers submitted for three of the
assignments in this class. Each assignment con-
sisted of seven short-answer questions.1 Thirty
students were enrolled in the class and submitted
answers to these assignments. Thus, the data set
we work with consists of a total of 630 student an-
swers (3 assignments x 7 questions/assignment x
30 student answers/question).
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both hu-
man judges were graduate computer science stu-
dents; one was the teaching assistant in the Data
Structures class, while the other is one of the au-
thors of this paper. Table 1 shows two question-
answer pairs with three sample student answers
each. The grades assigned by the two human
judges are also included.
The evaluations are run using Pearson?s corre-
lation coefficient measured against the average of
the human-assigned grades on a per-question and
a per-assignment basis. In the per-question set-
ting, every question and the corresponding student
answer is considered as an independent data point
in the correlation, and thus the emphasis is placed
on the correctness of the grade assigned to each
answer. In the per-assignment setting, each data
point is an assignment-student pair created by to-
taling the scores given to the student for each ques-
tion in the assignment. In this setting, the em-
phasis is placed on the overall grade a student re-
ceives for the assignment rather than on the grade
received for each independent question.
The correlation between the two human judges
is measured using both settings. In the per-
question setting, the two annotators correlated at
(r=0.6443). For the per-assignment setting, the
correlation was (r=0.7228).
A deeper look into the scores given by the
two annotators indicates the underlying subjectiv-
ity in grading short answer assignments. Of the
630 grades given, only 358 (56.8%) were exactly
agreed upon by the annotators. Even more strik-
ing, a full 107 grades (17.0%) differed by more
than one point on the five point scale, and 19
grades (3.0%) differed by 4 points or more. 2
1In addition, the assignments had several programming
exercises which have not been considered in any of our ex-
periments.
2An example should suffice to explain this discrepancy in
annotator scoring: Question: What does a function signature
include? Answer: The name of the function and the types of
the parameters. Student: input parameters and return type.
Scores: 1, 5. This example suggests that the graders were
not always consistent in comparing student answers to the in-
structor answer. Additionally, the instructor answer may be
insufficient to account for correct student answers, as ?return
Furthermore, on the occasions when the annota-
tors disagreed, the same annotator gave the higher
grade 79.8% of the time.
Over the course of this work, much attention
was given to our choice of correlation metric.
Previous work in text similarity and short-answer
grading seems split on the use of Pearson?s and
Spearman?s metric. It was not initially clear
that the underlying assumptions necessary for the
proper use of Pearson?s metric (e.g. normal dis-
tribution, interval measurement level, linear cor-
relation model) would be met in our experimental
setup. We considered both Spearman?s and sev-
eral less often used metrics (e.g. Kendall?s tau,
Goodman-Kruskal?s gamma), but in the end, we
have decided to follow previous work using Pear-
son?s so that our scores can be more easily com-
pared.3
4 Automatic Short Answer Grading
Our experiments are centered around the use of
measures of similarity for automatic short answer
grading. In particular, we carry out three sets
of experiments, seeking answers to the following
three research questions.
First, what are the measures of semantic sim-
ilarity that work best for the task of short an-
swer grading? To answer this question, we run
several comparative evaluations covering a num-
ber of knowledge-based and corpus-based mea-
sures of semantic similarity. While previous work
has considered such comparisons for the related
task of paraphrase identification (Mihalcea et al,
2006), to our knowledge no comprehensive eval-
uation has been carried out for the task of short
answer grading which includes all the similarity
measures proposed to date.
Second, to what extent do the domain and the
size of the data used to train the corpus-based
measures of similarity influence the accuracy of
the measures? To address this question, we run
a set of experiments which vary the size and do-
main of the corpus used to train the LSA and the
ESA metrics, and we measure their effect on the
accuracy of short answer grading.
Finally, given a measure of similarity, can we
integrate the answers with the highest scores and
improve the accuracy of the measure? We use
a technique similar to the pseudo-relevance feed-
back method used in information retrieval (Roc-
chio, 1971) and augment the correct answer with
type? does seem to be a valid component of a ?function sig-
nature? according to some literature on the web.
3Consider this an open call for discussion in the NLP
community regarding the proper usage of correlation metrics
with the ultimate goal of consistency within the community.
569
Sample questions, correct answers, and student answers Grade
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior of portions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
more manageable problems. 4, 4
Table 1: Two sample questions with short answers provided by students and the grades assigned by the
two human judges
the student answers receiving the best score ac-
cording to a similarity measure.
In all the experiments, the evaluations are run
on the data set described in the previous section.
The results are compared against a simple baseline
that assigns a grade based on a measurement of
the cosine similarity between the weighted vector-
space representations of the correct answer and the
candidate student answer. The Pearson correla-
tion for this model, using an inverse document fre-
quency derived from the British National Corpus
(BNC), is r=0.3647 for the per-question evaluation
and r=0.4897 for the per-assignment evaluation.
5 Text-to-text Semantic Similarity
We run our comparative evaluations using eight
knowledge-based measures of semantic similarity
(shortest path, Leacock & Chodorow, Lesk, Wu
& Palmer, Resnik, Lin, Jiang & Conrath, Hirst &
St. Onge), and two corpus-based measures (LSA
and ESA). For the knowledge-based measures, we
derive a text-to-text similarity metric by using the
methodology proposed in (Mihalcea et al, 2006):
for each open-class word in one of the input texts,
we use the maximum semantic similarity that can
be obtained by pairing it up with individual open-
class words in the second input text. More for-
mally, for each word W of part-of-speech class C
in the instructor answer, we find maxsim(W,C):
maxsim(W,C) = maxSIMx(W,wi)
where wi is a word in the student answer of class
C and the SIMx function is one of the functions
described below. All the word-to-word similarity
scores obtained in this way are summed up and
normalized with the length of the two input texts.
We provide below a short description for each of
these similarity metrics.
5.1 Knowledge-Based Measures
The shortest path similarity is determined as:
Simpath =
1
length (1)
where length is the length of the shortest path be-
tween two concepts using node-counting (includ-
ing the end nodes).
The Leacock & Chodorow (Leacock and
Chodorow, 1998) similarity is determined as:
Simlch = ? log
length
2 ?D (2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D
is the maximum depth of the taxonomy.
The Lesk similarity of two concepts is defined as
a function of the overlap between the correspond-
ing definitions, as provided by a dictionary. It is
based on an algorithm proposed by Lesk (1986) as
a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) simi-
larity metric measures the depth of two given con-
cepts in the WordNet taxonomy, and the depth of
the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
returns the information content (IC) of the LCS of
two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
570
The measure introduced by Lin (Lin, 1998) builds
on Resnik?s measure of similarity, and adds a
normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
Finally, we consider the Hirst & St. Onge (Hirst
and St-Onge, 1998) measure of similarity, which
determines the similarity strength of a pair of
synsets by detecting lexical chains between the
pair in a text using the WordNet hierarchy.
5.2 Corpus-Based Measures
Corpus-based measures differ from knowledge-
based methods in that they do not require any en-
coded understanding of either the vocabulary or
the grammar of a text?s language. In many of
the scenarios where CAA would be advantageous,
robust language-specific resources (e.g. Word-
Net) may not be available. Thus, state-of-the-art
corpus-based measures may be the only available
approach to CAA in languages with scarce re-
sources.
One corpus-based measure of semantic similar-
ity is latent semantic analysis (LSA) proposed by
Landauer (Landauer and Dumais, 1997). In LSA,
term co-occurrences in a corpus are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-
by-document matrix T representing the corpus.
For the experiments reported in this section, we
run the SVD operation on several corpora includ-
ing the BNC (LSA BNC) and the entire English
Wikipedia (LSA Wikipedia).4
Explicit semantic analysis (ESA) (Gabrilovich
and Markovitch, 2007) is a variation on the stan-
dard vectorial model in which the dimensions of
the vector are directly equivalent to abstract con-
cepts. Each article in Wikipedia represents a con-
cept in the ESA vector. The relatedness of a term
to a concept is defined as the tf*idf score for the
term within the Wikipedia article, and the related-
ness between two words is the cosine of the two
concept vectors in a high-dimensional space. We
refer to this method as ESA Wikipedia.
4Throughout this paper, the references to the Wikipedia
corpus refer to a version downloaded in September 2007.
5.3 Implementation
For the knowledge-based measures, we use the
WordNet-based implementation of the word-to-
word similarity metrics, as available in the Word-
Net::Similarity package (Patwardhan et al, 2003).
For latent semantic analysis, we use the InfoMap
package.5 For ESA, we use our own imple-
mentation of the ESA algorithm as described in
(Gabrilovich and Markovitch, 2006). Note that
all the word similarity measures are normalized so
that they fall within a 0?1 range. The normaliza-
tion is done by dividing the similarity score pro-
vided by a given measure with the maximum pos-
sible score for that measure.
Table 2 shows the results obtained with each of
these measures on our evaluation data set.
Measure Correlation
Knowledge-based measures
Shortest path 0.4413
Leacock & Chodorow 0.2231
Lesk 0.3630
Wu & Palmer 0.3366
Resnik 0.2520
Lin 0.3916
Jiang & Conrath 0.4499
Hirst & St-Onge 0.1961
Corpus-based measures
LSA BNC 0.4071
LSA Wikipedia 0.4286
ESA Wikipedia 0.4681
Baseline
tf*idf 0.3647
Table 2: Comparison of knowledge-based and
corpus-based measures of similarity for short an-
swer grading
6 The Role of Domain and Size
One of the key considerations when applying
corpus-based techniques is the extent to which size
and subject matter affect the overall performance
of the system. In particular, based on the underly-
ing processes involved, the LSA and ESA corpus-
based methods are expected to be especially sen-
sitive to changes in domain and size. Building the
language models depends on the relatedness of the
words in the training data which suggests that, for
instance, in a computer science domain the terms
?object? and ?oriented? will be more closely re-
lated than in a more general text. Similarly, a large
amount of training data will lead to less sparse
5http://infomap-nlp.sourceforge.net/
571
vector spaces, which in turn is expected to affect
the performance of the corpus-based methods.
With this in mind, we developed two training
corpora for use with the corpus-based measures
that covered the computer science domain. The
first corpus (LSA slides) consists of several online
lecture notes associated with the class textbook,
specifically covering topics that are used as ques-
tions in our sample. The second domain-specific
corpus is a subset of Wikipedia (LSA Wikipedia
CS) consisting of articles that contain any of the
following words: computer, computing, computa-
tion, algorithm, recursive, or recursion.
The performance on the domain-specific cor-
pora is compared with the one observed on the
open-domain corpora mentioned in the previ-
ous section, namely LSA Wikipedia and ESA
Wikipedia. In addition, for the purpose of running
a comparison with the LSA slides corpus, we also
created a random subset of the LSA Wikipedia
corpus approximately matching the size of the
LSA slides corpus. We refer to this corpus as LSA
Wikipedia (small).
Table 3 shows an overview of the various cor-
pora used in the experiments, along with the Pear-
son correlation observed on our data set.
Measure - Corpus Size Correlation
Training on generic corpora
LSA BNC 566.7MB 0.4071
LSA Wikipedia 1.8GB 0.4286
LSA Wikipedia (small) 0.3MB 0.3518
ESA Wikipedia 1.8GB 0.4681
Training on domain-specific corpora
LSA Wikipedia CS 77.1MB 0.4628
LSA slides 0.3MB 0.4146
ESA Wikipedia CS 77.1MB 0.4385
Table 3: Corpus-based measures trained on cor-
pora from different domains and of different sizes
Assuming a corpus of comparable size, we ex-
pect a measure trained on a domain-specific cor-
pus to outperform one that relies on a generic one.
Indeed, by comparing the results obtained with
LSA slides to those obtained with LSA Wikipedia
(small), we see that by using the in-domain com-
puter science slides we obtain a correlation of
r=0.4146, which is higher than the correlation
of r=0.3518 obtained with a corpus of the same
size but open-domain. The effect of the domain
is even more pronounced when we compare the
performance obtained with LSA Wikipedia CS
(r=0.4628) with the one obtained with the full LSA
Wikipedia (r=0.4286).6 The smaller, domain-
6The difference was found significant using a paired t-test
specific corpus performs better, despite the fact
that the generic corpus is 23 times larger and is a
superset of the smaller corpus. This suggests that
for LSA the quality of the texts is vastly more im-
portant than their quantity.
When using the domain-specific subset of
Wikipedia, we observe decreased performance
with ESA compared to the full Wikipedia space.
We suggest that for ESA the high-dimensionality
of the concept space7 is paramount, since many re-
lations between generic words may be lost to ESA
that can be detected latently using LSA.
In tandem with our exploration of the effects
of domain-specific data, we also look at the effect
of size on the overall performance. The main in-
tuitive trends are there, i.e., the performance ob-
tained with the large LSA-Wikipedia is better than
the one that can be obtained with LSA Wikipedia
(small). Similarly, in the domain-specific space,
the LSA Wikipedia CS corpus leads to better per-
formance than the smaller LSA slides data set.
However, an analysis carried out at a finer grained
scale, in which we calculate the performance ob-
tained with LSA when trained on 5%, 10%, ...,
100% fractions of the full LSA Wikipedia corpus,
does not reveal a close correlation between size
and performance, which suggests that further anal-
ysis is needed to determine the exact effect of cor-
pus size on performance.
7 Relevance Feedback based on Student
Answers
The automatic grading of student answers im-
plies a measure of similarity between the answers
provided by the students and the correct answer
provided by the instructor. Since we only have
one correct answer, some student answers may be
wrongly graded because of little or no similarity
with the correct answer that we have.
To address this problem, we introduce a novel
technique that feeds back from the student an-
swers themselves in a way similar to the pseudo-
relevance feedback used in information retrieval
(Rocchio, 1971). In this way, the paraphrasing that
is usually observed across student answers will en-
hance the vocabulary of the correct answer, while
at the same time maintaining the correctness of the
gold-standard answer.
Briefly, given a metric that provides similarity
scores between the student answers and the cor-
rect answer, scores are ranked from most similar
(p<0.001).
7In ESA, all the articles in Wikipedia are used as dimen-
sions, which leads to about 1.75 million dimensions in the
ESA Wikipedia corpus, compared to only 55,000 dimensions
in the ESA Wikipedia CS corpus.
572
to least. The words of the top N ranked answers
are then added to the gold standard answer. The
remaining answers are then rescored according the
the new gold standard vector. In practice, we hold
the scores from the first run (i.e., with no feed-
back) constant for the top N highest-scoring an-
swers, and the second-run scores for the remaining
answers are multiplied by the first-run score of the
Nth highest-scoring answer. In this way, we keep
the original scores for the top N highest-scoring
answers (and thus prevent them from becoming ar-
tificially high), and at the same time, we guarantee
that none of the lower-scored answers will get a
new score higher than the best answers.
The effects of relevance feedback are shown in
Figure 9, which plots the Pearson correlation be-
tween automatic and human grading (Y axis) ver-
sus the number of student answers that are used
for relevance feedback (X axis).
Overall, an improvement of up to 0.047 on
the 0-1 Pearson scale can be obtained by using
this technique, with a maximum improvement ob-
served after about 4-6 iterations on average. Af-
ter an initial number of high-scored answers, it is
likely that the correctness of the answers degrades,
and thus the decrease in performance observed af-
ter an initial number of iterations. Our results in-
dicate that the LSA and WordNet similarity met-
rics respond more favorably to feedback than the
ESA metric. It is possible that supplementing the
bag-of-words in ESA (with e.g. synonyms and
phrasal differences) does not drastically alter the
resultant concept vector, and thus the overall ef-
fect is smaller.
8 Discussion
Our experiments show that several knowledge-
based and corpus-based measures of similarity
perform comparably when used for the task of
short answer grading. However, since the corpus-
based measures can be improved by account-
ing for domain and corpus size, the highest per-
formance can be obtained with a corpus-based
measure (LSA) trained on a domain-specific cor-
pus. Further improvements were also obtained
by integrating the highest-scored student answers
through a relevance feedback technique.
Table 4 summarizes the results of our experi-
ments. In addition to the per-question evaluations
that were reported throughout the paper, we also
report the per-assignment evaluation, which re-
flects a cumulative score for a student on a single
assignment, as described in Section 3.
Overall, in both the per-question and per-
assignment evaluations, we obtained the best per-
formance by using an LSA measure trained on
Correlation
Measure per-quest. per-assign.
Baselines
tf*idf 0.3647 0.4897
LSA BNC 0.4071 0.6465
Relevance Feedback based on Student Answers
WordNet shortest path 0.4887 0.6344
LSA Wikipedia CS 0.5099 0.6735
ESA Wikipedia full 0.4893 0.6498
Annotator agreement 0.6443 0.7228
Table 4: Summary of results obtained with vari-
ous similarity measures, with relevance feedback
based on six student answers. We also list the
tf*idf and the LSA trained on BNC baselines (no
feedback), as well as the annotator agreement up-
per bound.
a medium size domain-specific corpus obtained
from Wikipedia, with relevance feedback from
the four highest-scoring student answers. This
method improves significantly over the tf*idf
baseline and also over the LSA trained on BNC
model, which has been used extensively in previ-
ous work. The differences were found to be sig-
nificant using a paired t-test (p<0.001).
To gain further insights, we made an additional
analysis where we determined the ability of our
system to make a binary accept/reject decision. In
this evaluation, we map the 0-5 human grading of
the data set to an accept/reject annotation by us-
ing a threshold of 2.5. Every answer with a grade
higher than 2.5 is labeled as ?accept,? while ev-
ery answer below 2.5 is labeled as ?reject.? Next,
we use our best system (LSA trained on domain-
specific data with relevance feedback), and run a
ten-fold cross-validation on the data set. Specif-
ically, for each fold, the system uses the remain-
ing nine folds to automatically identify a thresh-
old to maximize the matching with the gold stan-
dard. The threshold identified in this way is used
to automatically annotate the test fold with ?ac-
cept?/?reject? labels. The ten-fold cross validation
resulted in an accuracy of 92%, indicating the abil-
ity of the system to automatically make a binary
accept/reject decision.
9 Conclusions
In this paper, we explored unsupervised tech-
niques for automatic short answer grading.
We believe the paper made three important con-
tributions. First, while there are a number of word
and text similarity measures that have been pro-
posed in the past, to our knowledge no previ-
ous work has considered a comprehensive evalu-
573
 0.35
 0.4
 0.45
 0.5
 0.55
 0  5  10  15  20
Co
rre
la
tio
n
Number of student answers used for feedback
LSA-Wiki-full
LSA-Wiki-CS
LSA-slides-CS
ESA-Wiki-full
ESA-Wiki-CS
WN-JCN
WN-PATH
TF*IDF
LSA-BNC
Figure 1: Effect of relevance feedback on performance
ation of all the measures for the task of short an-
swer grading. We filled this gap by running com-
parative evaluations of several knowledge-based
and corpus-based measures on a data set of short
student answers. Our results indicate that when
used in their original form, the results obtained
with the best knowledge-based (WordNet short-
est path and Jiang & Conrath) and corpus-based
measures (LSA and ESA) have comparable per-
formance. The benefit of the corpus-based ap-
proaches over knowledge-based approaches lies in
their language independence and the relative ease
in creating a large domain-sensitive corpus versus
a language knowledge base (e.g., WordNet).
Second, we analysed the effect of domain and
corpus size on the effectiveness of the corpus-
based measures. We found that significant im-
provements can be obtained for the LSA measure
when using a medium size domain-specific corpus
built from Wikipedia. In fact, when using LSA,
our results indicate that the corpus domain may be
significantly more important than corpus size once
a certain threshold size has been reached.
Finally, we introduced a novel technique for in-
tegrating feedback from the student answers them-
selves into the grading system. Using a method
similar to the pseudo-relevance feedback tech-
nique used in information retrieval, we were able
to improve the quality of our system by a few per-
centage points.
Overall, our best system consists of an LSA
measure trained on a domain-specific corpus built
on Wikipedia with feedback from student answers,
which was found to bring a significant absolute
improvement on the 0-1 Pearson scale of 0.14 over
the tf*idf baseline and 0.10 over the LSA BNC
model that has been used in the past.
In future work, we intend to expand our analy-
sis of both the gold-standard answer and the stu-
dent answers beyond the bag-of-words paradigm
by considering basic logical features in the text
(i.e., AND, OR, NOT) as well as the existence
of shallow grammatical features such as predicate-
argument structure(Moschitti et al, 2007) as well
as semantic classes for words. Furthermore, it may
be advantageous to expand upon the existing mea-
sures by applying machine learning techniques to
create a hybrid decision system that would exploit
the advantages of each measure.
The data set introduced in this paper, along with
the human-assigned grades, can be downloaded
from http://lit.csci.unt.edu/index.php/Downloads.
Acknowledgments
This work was partially supported by a National
Science Foundation CAREER award #0747340.
The authors are grateful to Samer Hassan for mak-
ing available his implementation of the ESA algo-
rithm.
References
D. Callear, J. Jerrams-Smith, and V. Soh. 2001.
CAA of Short Non-MCQ Answers. Proceedings of
574
the 5th International Computer Assisted Assessment
conference.
E. Gabrilovich and S. Markovitch. 2006. Overcoming
the brittleness bottleneck using Wikipedia: Enhanc-
ing text categorization with encyclopedic knowl-
edge. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), Boston.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Ex-
plicit Semantic Analysis. Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence, pages 6?12.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999.
Detecting text similarity over short passages: Ex-
ploring linguistic feature combinations via machine
learning. Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile.
2004. Evaluating multiple aspects of coherence in
student essays. In Proceedings of the annual meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as rep-
resentations of contexts for the detection and correc-
tion of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
D. Kanejiya, A. Kumar, and S. Prasad. 2003. Au-
tomatic evaluation of students? answers using syn-
tactically enhanced LSA. Proceedings of the HLT-
NAACL 03 workshop on Building educational appli-
cations using natural language processing-Volume
2, pages 53?60.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Au-
tomated Scoring of Short-Answer Questions. Com-
puters and the Humanities, 37(4):389?405.
M.D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document simi-
larity. Proceedings of the 27th Annual Conference
of the Cognitive Science Society, pages 1254?1259.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to
text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence
(AAAI 2006), Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of
free-text responses. Proceedings of the 6th Interna-
tional Computer Assisted Assessment (CAA) Confer-
ence.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Conference of the Association for Computa-
tional Linguistics.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico
City, February.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference
on Artificial Intelligence, pages 1024?1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
Short Answer Marking. ACL WS Bldg Ed Apps us-
ing NLP.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Wong, and C.S. Yang. 1997. A vec-
tor space model for automatic indexing. In Read-
ings in Information Retrieval, pages 273?280. Mor-
gan Kaufmann Publishers, San Francisco, CA.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004.
Auto-Marking 2: An Update on the UCLES-Oxford
University research into using Computational Lin-
guistics to Score Short, Free Text Responses. In-
ternational Association of Educational Assessment,
Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tutor?s
comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535?542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton.
2005. Initial results and mixed directions for re-
search methods tutor. In AIED2005 - Supplementary
Proceedings of the 12th International Conference on
Artificial Intelligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, Las Cruces, New Mexico.
575
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 411?418, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Unsupervised Large-Vocabulary Word Sense Disambiguation
with Graph-based Algorithms for Sequence Data Labeling
Rada Mihalcea
Department of Computer Science
University of North Texas
rada@cs.unt.edu
Abstract
This paper introduces a graph-based algo-
rithm for sequence data labeling, using ran-
dom walks on graphs encoding label de-
pendencies. The algorithm is illustrated
and tested in the context of an unsuper-
vised word sense disambiguation problem,
and shown to significantly outperform the
accuracy achieved through individual label
assignment, as measured on standard sense-
annotated data sets.
1 Introduction
Many natural language processing tasks consist of la-
beling sequences of words with linguistic annotations,
e.g. word sense disambiguation, part-of-speech tag-
ging, named entity recognition, and others. Typical
labeling algorithms attempt to formulate the annota-
tion task as a traditional learning problem, where the
correct label is individually determined for each word
in the sequence using a learning process, usually con-
ducted independent of the labels assigned to the other
words in the sequence. Such algorithms do not have
the ability to encode and thereby exploit dependen-
cies across labels corresponding to the words in the
sequence, which potentially limits their performance
in applications where such dependencies can influence
the selection of the correct set of labels.
In this paper, we introduce a graph-based sequence
data labeling algorithm well suited for such natural
language annotation tasks. The algorithm simultane-
ously annotates all the words in a sequence by ex-
ploiting relations identified among word labels, us-
ing random walks on graphs encoding label dependen-
cies. The random walks are mathematically modeled
through iterative graph-based algorithms, which are
applied on the label graph associated with the given
sequence of words, resulting in a stationary distribu-
tion over label probabilities. These probabilities are
then used to simultaneously select the most probable
set of labels for the words in the input sequence.
The annotation method is illustrated and tested on
an unsupervised word sense disambiguation prob-
lem, targeting the annotation of all open-class words
in unrestricted text using information derived exclu-
sively from dictionary definitions. The graph-based
sequence data labeling algorithm significantly outper-
forms the accuracy achieved through individual data
labeling, resulting in an error reduction of 10.7%, as
measured on standard sense-annotated data sets. The
method is also shown to exceed the performance of
other previously proposed unsupervised word sense
disambiguation algorithms.
2 Iterative Graphical Algorithms for
Sequence Data Labeling
In this section, we introduce the iterative graphical al-
gorithm for sequence data labeling. The algorithm is
succinctly illustrated using a sample sequence for a
generic annotation problem, with a more extensive il-
lustration and evaluation provided in Section 3.
Given a sequence of words W = {w1, w2, ..., wn},
each word wi with corresponding admissible labels
Lwi = {l1wi , l2wi , ..., l
Nwiwi }, we define a label graph G
= (V,E) such that there is a vertex v ? V for every pos-
sible label ljwi , i = 1..n, j = 1..Nwi . Dependencies
between pairs of labels are represented as directed or
indirected edges e ? E, defined over the set of vertex
pairs V ? V . Such label dependencies can be learned
from annotated data, or derived by other means, as il-
lustrated later. Figure 1 shows an example of a graph-
411
1w
1
2
3
2
1 1 1
2
4
w 2 w 3 w 4
w1
w1
3
w1l
l
l
w2
w2
l
l
w3l w4
w4
w4
w4l
l
l
l
1.1
0.4
0.2
0.5
0.2
0.1
1.3
0.9
0.6
0.7
1.6
[1.12]
[1.39]
[0.86]
[1.13]
[1.38]
[1.56] [0.40]
[1.05]
[0.58]
[0.48]
Figure 1: Sample graph built on the set of possible
labels (shaded nodes) for a sequence of four words
(unshaded nodes). Label dependencies are indicated
as edge weights. Scores computed by the graph-based
algorithm are shown in brackets, next to each label.
ical structure derived over the set of labels for a se-
quence of four words. Note that the graph does not
have to be fully connected, as not all label pairs can
be related by a dependency.
Given such a label graph associated with a sequence
of words, the likelihood of each label can be recur-
sively determined using an iterative graph-based rank-
ing algorithm, which runs over the graph of labels and
identifies the importance of each label (vertex) in the
graph. The iterative graphical algorithm is modeling a
random walk, leading to a stationary distribution over
label probabilities, represented as scores attached to
vertices in the graph. These scores are then used to
identify the most probable label for each word, result-
ing in the annotation of all the words in the input se-
quence. For instance, for the graph drawn in Figure 1,
the word w1 will be assigned with label l1w1 , since the
score associated with this label (1.39) is the maximum
among the scores assigned to all admissible labels as-
sociated with this word.
A remarkable property that makes these iterative
graphical algorithms appealing for sequence data la-
beling is the fact that they take into account global
information recursively drawn from the entire graph,
rather than relying on local vertex-specific informa-
tion. Through the random walk performed on the la-
bel graph, these iterative algorithms attempt to collec-
tively exploit the dependencies drawn between all la-
bels in the graph, which makes them superior to other
approaches that rely only on local information, indi-
vidually derived for each word in the sequence.
2.1 Graph-based Ranking
The basic idea implemented by an iterative graph-
based ranking algorithm is that of ?voting? or ?recom-
mendation?. When one vertex links to another one, it
is basically casting a vote for that other vertex. The
higher the number of votes that are cast for a vertex,
the higher the importance of the vertex. Moreover,
the importance of the vertex casting a vote determines
how important the vote itself is, and this information
is also taken into account by the ranking algorithm.
While there are several graph-based ranking algo-
rithms previously proposed in the literature, we focus
on only one such algorithm, namely PageRank (Brin
and Page, 1998), as it was previously found success-
ful in a number of applications, including Web link
analysis, social networks, citation analysis, and more
recently in several text processing applications.
Given a graph G = (V,E), let In(Va) be the set
of vertices that point to vertex Va (predecessors), and
let Out(Va) be the set of vertices that vertex Va points
to (successors). The PageRank score associated with
the vertex Va is then defined using a recursive function
that integrates the scores of its predecessors:
P (Va) = (1 ? d) + d ?
?
Vb?In(Va)
P (Vb)
|Out(Vb)|
(1)
where d is a parameter that is set between 0 and 11.
This vertex scoring scheme is based on a random
walk model, where a walker takes random steps on the
graph G, with the walk being modeled as a Markov
process ? that is, the decision on what edge to follow
is solely based on the vertex where the walker is cur-
rently located. Under certain conditions, this model
converges to a stationary distribution of probabilities,
associated with vertices in the graph. Based on the
Ergodic theorem for Markov chains (Grimmett and
Stirzaker, 1989), the algorithm is guaranteed to con-
verge if the graph is both aperiodic and irreducible.
The first condition is achieved for any graph that is a
non-bipartite graph, while the second condition holds
for any strongly connected graph ? property achieved
by PageRank through the random jumps introduced
by the (1 ? d) factor. In matrix notation, the PageR-
ank vector of stationary probabilities is the principal
eigenvector for the matrix Arow, which is obtained
from the adjacency matrix A representing the graph,
with all rows normalized to sum to 1: (P = ATrowP ).
Intuitively, the stationary probability associated
with a vertex in the graph represents the probability
1The typical value for d is 0.85 (Brin and Page, 1998), and this
is the value we are also using in our implementation.
412
of finding the walker at that vertex during the ran-
dom walk, and thus it represents the importance of the
vertex within the graph. In the context of sequence
data labeling, the random walk is performed on the
label graph associated with a sequence of words, and
thus the resulting stationary distribution of probabili-
ties can be used to decide on the most probable set of
labels for the given sequence.
2.2 Ranking on Weighted Graphs
In a weighted graph, the decision on what edge to fol-
low during a random walk is also taking into account
the weights of outgoing edges, with a higher likeli-
hood of following an edge that has a larger weight.
The weighted version of the ranking algorithm is
particularly useful for sequence data labeling, since
the dependencies between pairs of labels are more
naturally modeled through weights indicating their
strength, rather than binary 0/1 values. Given a set of
weights wab associated with edges connecting vertices
Va and Vb, the weighted PageRank score is determined
as:
WP (Va) = (1?d)+d
?
Vb?In(Va)
wba
?
Vc?Out(Vb)
wbc
WP (Vb) (2)
2.3 Algorithm for Sequence Data Labeling
Given a sequence of words with their corresponding
admissible labels, the algorithm for sequence data la-
beling seeks to identify a graph of label dependencies
on which a random walk can be performed, resulting
in a set of scores that can be used for label assignment.
Algorithm 1 shows the pseudocode for the labeling
process. The algorithm consists of three main steps:
(1) construction of label dependencies graph; (2) la-
bel scoring using graph-based ranking algorithms; (3)
label assignment.
First, a weighted graph of label dependencies is
built by adding a vertex for each admissible label, and
an edge for each pair of labels for which a dependency
is identified. A maximum allowable distance can be
set (MaxDist), indicating a constraint over the dis-
tance between words for which a label dependency
is sought. For instance, if MaxDist is set to 3, no
edges will be drawn between labels corresponding to
words that are more than three words apart, counting
all running words. Label dependencies are determined
through the Dependency function, whose definition
depends on the application and type of resources avail-
able (see Section 2.4).
Next, scores are assigned to vertices using a graph-
based ranking algorithm. Current experiments are
Algorithm 1 Graph-based Sequence Data Labeling
Input: Sequence W = {wi|i = 1..N}
Input: Admissible labels Lwi = {ltwi |t = 1..Nwi},i = 1..N
Output: Sequence of labels L = {lwi |i = 1..N}, with label lwi
corresponding to word wi from the input sequence.
Build graph G of label dependencies
1: for i = 1 to N do
2: for j = i + 1 to N do
3: if j ? i > MaxDist then
4: break
5: end if
6: for t = 1 to Nwi do
7: for s = 1 to Nwj do
8: weight? Dependency(ltwi , lswj , wi, wj)
9: if weight > 0 then
10: AddEdge(G, ltwi , lswj , weight)
11: end if
12: end for
13: end for
14: end for
15: end for
Score vertices in G
1: repeat
2: for all Va ? V ertices(G) do
3: WP (Va) = (1? d) + d?
?
Vb?In(Va)
wbaWP (Vb)/
?
Vc?Out(Vb)
wbc
4: end for
5: until convergence of scores WP (Va)
Label assignment
1: for i = 1 to N do
2: lwi ? argmax{WP (ltwi)|t = 1..Nwi}
3: end for
based on PageRank, but other ranking algorithms can
be used as well.
Finally, the most likely set of labels is determined
by identifying for each word the label that has the
highest score. Note that all admissible labels corre-
sponding to the words in the input sequence are as-
signed with a score, and thus the selection of two or
more most likely labels for a word is also possible.
2.4 Label Dependencies
Label dependencies can be defined in various ways,
depending on the application at hand and on the
knowledge sources that are available. If an annotated
corpus is available, dependencies can be defined as
label co-occurrence probabilities approximated with
frequency counts P (ltwi , lswj ), or as conditional prob-
abilities P (ltwi |lswj ). Optionally, these dependencies
can be lexicalized by taking into account the corre-
sponding words in the sequence, e.g. P (ltwi |lswj ) ?
P (wi|ltwi). In the absence of an annotated corpus, de-
pendencies can be derived by other means, e.g. part-
413
of-speech probabilities can be approximated from a
raw corpus as in (Cutting et al, 1992), word-sense de-
pendencies can be derived as definition-based similar-
ities, etc. Label dependencies are set as weights on
the arcs drawn between corresponding labels. Arcs
can be directed or undirected for joint probabilities or
similarity measures, and are usually directed for con-
ditional probabilities.
2.5 Labeling Example
Consider again the example from Figure 1, consisting
of a sequence of four words, and their possible cor-
responding labels. In the first step of the algorithm,
label dependencies are determined, and let us assume
that the values for these dependencies are as indicated
through the edge weights in Figure 1. Next, vertices
in the graph are scored using an iterative ranking al-
gorithm, resulting in a score attached to each label,
shown in brackets next to each vertex. Finally, the
most probable label for each word is selected. Word
w1 is thus assigned with label l1w1 , since the score of
this label (1.39) is the maximum among the scores as-
sociated with all its possible labels (1.39, 1.12, 0.86).
Similarly, word w2 is assigned with label l2w2 , w3 with
label l1w3 , and w4 receives label l2w4 .
2.6 Efficiency Considerations
For a sequence of words W = {w1, w2, ..., wn}, each
word wi with Nwi admissible labels, the running time
of the graph-based sequence data labeling algorithm
is proportional with O(C
n
?
i=1
i+MaxDist
?
j=i+1
(Nwi ? Nwj ))
(the time spent in building the label graph and iterating
the algorithm for a constant number of times C). This
is order of magnitudes better than the running time
of O(
n
?
i=1
Nwi) for algorithms that attempt to select the
best sequence of labels by searching through the en-
tire space of possible label combinations, although it
can be significantly higher than the running time of
O(
n
?
i=1
Nwi) for individual data labeling.
2.7 Other Algorithms for Sequence Data
Labeling
It is interesting to contrast our algorithm with previ-
ously proposed models for sequence data labeling, e.g.
Hidden Markov Models, Maximum Entropy Markov
Models, or Conditional Random Fields. Although
they differ in the model used (generative, discrimina-
tive, or dual), and the type of probabilities involved
(joint or conditional), these previous algorithms are
all parameterized algorithms that typically require pa-
rameter training through maximization of likelihood
on training examples. In these models, parameters that
maximize sequence probabilities are learned from a
corpus during a training phase, and then applied to
the annotation of new unseen data. Instead, in the
algorithm proposed in this paper, the likelihood of a
sequence of labels is determined during test phase,
through random walks performed on the label graph
built for the data to be annotated. While current eval-
uations of our algorithm are performed on an unsuper-
vised labeling task, future work will consider the eval-
uation of the algorithm in the presence of an annotated
corpus, which will allow for direct comparison with
these previously proposed models for sequence data
labeling.
3 Experiments in Word Sense
Disambiguation
The algorithm for sequence data labeling is illustrated
and tested on an all-words word sense disambiguation
problem. Word sense disambiguation is a labeling task
consisting of assigning the correct meaning to each
open-class word in a sequence (usually a sentence).
Most of the efforts for solving this problem were con-
centrated so far toward targeted supervised learning,
where each sense tagged occurrence of a particular
word is transformed into a feature vector used in an
automatic learning process. The applicability of such
supervised algorithms is however limited to those few
words for which sense tagged data is available, and
their accuracy is strongly connected to the amount of
labeled data available at hand. Instead, algorithms that
attempt to disambiguate all-words in unrestricted text
have received significantly less attention, as the devel-
opment and success of such algorithms has been hin-
dered by both (a) lack of resources (training data), and
(b) efficiency aspects resulting from the large size of
the problem.
3.1 Graph-based Sequence Data Labeling for
Unsupervised Word Sense Disambiguation
To apply the graph-based sequence data labeling algo-
rithm to the disambiguation of an input text, we need
information on labels (word senses) and dependencies
(word sense dependencies). Word senses can be eas-
ily obtained from any sense inventory, e.g. WordNet
or LDOCE. Sense dependencies can be derived in var-
ious ways, depending on the type of resources avail-
able for the language and/or domain at hand. In this
paper, we explore the unsupervised derivation of sense
414
dependencies using information drawn from machine
readable dictionaries, which is general and can be ap-
plied to any language or domain for which a sense in-
ventory is available.
Relying exclusively on a machine readable dictio-
nary, a sense dependency can be defined as a measure
of similarity between word senses. There are several
metrics that can be used for this purpose, see for in-
stance (Budanitsky and Hirst, 2001) for an overview.
However, most of them rely on measures of seman-
tic distance computed on semantic networks, and thus
they are limited by the availability of explicitly en-
coded semantic relations (e.g. is-a, part-of). To
maintain the unsupervised aspect of the algorithm, we
chose instead to use a measure of similarity based on
sense definitions, which can be computed on any dic-
tionary, and can be evaluated across different parts-of-
speech.
Given two word senses and their corresponding def-
initions, the sense similarity is determined as a func-
tion of definition overlap, measured as the number of
common tokens between the two definitions, after run-
ning them through a simple filter that eliminates all
stop-words. To avoid promoting long definitions, we
also use a normalization factor, and divide the content
overlap of the two definitions with the length of each
definition. This sense similarity measure is inspired
by the definition of the Lesk algorithm (Lesk, 1986).
Starting with a sense inventory and a function for
computing sense dependencies, the application of the
sequence data labeling algorithm to the unsupervised
disambiguation of a new text proceeds as follows.
First, for the given text, a label graph is built by
adding a vertex for each possible sense for all open-
class words in the text. Next, weighted edges are
drawn using the definition-based semantic similarity
measure, computed for all pairs of senses for words
found within a certain distance (MaxDist, as defined
in Algorithm 1). Once the graph is constructed, the
graph-based ranking algorithm is applied, and a score
is determined for all word senses in the graph. Finally,
for each open-class word in the text, we select the ver-
tex in the label graph which has the highest score, and
label the word with the corresponding word sense.
3.2 An Example
Consider the task of assigning senses to the words
in the text The church bells no longer rung on Sun-
days2. For the purpose of illustration, we assume at
2Example drawn from the data set provided during the
SENSEVAL-2 English all-words task. Manual sense annotations
The church bells no longer rung on Sundays.
church
1: one of the groups of Christians who have their own beliefs
and forms of worship
2: a place for public (especially Christian) worship
3: a service conducted in a church
bell
1: a hollow device made of metal that makes a ringing sound
when struck
2: a push button at an outer door that gives a ringing or buzzing
signal when pushed
3: the sound of a bell
ring
1: make a ringing sound
2: ring or echo with sound
3: make (bells) ring, often for the purposes of musical edifica-
tion
Sunday
1: first day of the week; observed as a day of rest and worship
by most Christians
bell ring
[1.46]
[0.99]
[0.96] [2.56]
[0.63]
[0.58]
[0.42]
[0.67]
Sundaychurch
S2
S1
s3
s2
s3
s2
S3
s1 S1s1
0.35
0.501.06
0.40
0.19
0.34
1.01
0.55 [0.73]
0.30
[0.93]
0.35
0.31
0.80
0.85
0.23
Figure 2: The label graph for assigning senses to
words in the sentence The church bells no longer rung
on Sundays.
most three senses for each word, which are shown in
Figure 2. Word senses and definitions are obtained
from the WordNet sense inventory (Miller, 1995). All
word senses are added as vertices in the label graph,
and weighted edges are drawn as dependencies among
word senses, derived using the definition-based sim-
ilarity measure (no edges are drawn between word
senses with a similarity of zero). The resulting label
graph is an undirected weighted graph, as shown in
Figure 2. After running the ranking algorithm, scores
are identified for each word-sense in the graph, indi-
cated between brackets next to each node. Selecting
for each word the sense with the largest score results in
the following sense assignment: The church#2 bells#1
were also made available for this data.
415
no longer rung#3 on Sundays#1, which is correct ac-
cording to annotations performed by professional lex-
icographers.
3.3 Results and Discussion
The algorithm was primarily evaluated on the
SENSEVAL-2 English all-words data set, consisting
of three documents from Penn Treebank, with 2,456
open-class words (Palmer et al, 2001). Unlike other
sense-annotated data sets, e.g. SENSEVAL-3 or Sem-
Cor, SENSEVAL-2 is the only testbed for all-words
word sense disambiguation that includes a sense map,
which allows for additional coarse-grained sense eval-
uations. Moreover, there is a larger body of previous
work that was evaluated on this data set, which can be
used as a base of comparison.
The performance of our algorithm is compared with
the disambiguation accuracy obtained with a variation
of the Lesk algorithm3 (Lesk, 1986), which selects the
meaning of an open-class word by finding the word
sense that leads to the highest overlap between the cor-
responding dictionary definition and the current con-
text. Similar to the definition similarity function used
in the graph-based disambiguation algorithm (Section
3.1), the overlap measure used in the Lesk implemen-
tation does not take into account stop-words, and it is
normalized with the length of each definition to avoid
promoting longer definitions.
We are thus comparing the performance of se-
quence data labeling, which takes into account label
dependencies, with individual data labeling, where a
label is selected independent of the other labels in
the text. Note that both algorithms rely on the same
knowledge source, i.e. dictionary definitions, and thus
they are directly comparable. Moreover, none of the
algorithms take into account the dictionary sense order
(e.g. the most frequent sense provided by WordNet),
and therefore they are both fully unsupervised.
Table 1 shows precision and recall figures4 for a
3Given a sequence of words, the original Lesk algorithm at-
tempts to identify the combination of word senses that maxi-
mizes the redundancy (overlap) across all corresponding defini-
tions. The algorithm was later improved through a method for
simulated annealing (Cowie et al, 1992), which solved the com-
binatorial explosion of word senses, while still finding an optimal
solution. However, recent comparative evaluations of different
variants of the Lesk algorithm have shown that the performance
of the original algorithm is significantly exceeded by an algorithm
variation that relies on the overlap between word senses and cur-
rent context (Vasilescu et al, 2004). We are thus using this latter
Lesk variant in our implementation.
4Recall is particularly low for each individual part-of-speech
because it is calculated with respect to the entire data set. The
overall precision and recall figures coincide, reflecting the 100%
coverage of the algorithm.
context size (MaxDist) equal to the length of each
sentence, using: (a) sequence data labeling with itera-
tive graph-based algorithms; (b) individual data label-
ing with a version of the Lesk algorithm; (c) random
baseline. Evaluations are run for both fine-grained
and coarse-grained sense distinctions, to determine
the algorithm performance under different classifica-
tion granularities.
The accuracy of the graph-based sequence data la-
beling algorithm exceeds by a large margin the indi-
vidual data labeling algorithm, resulting in 10.7% er-
ror rate reduction for fine-grained sense distinctions,
which is statistically significant (p < 0.0001, paired
t-test). Performance improvements are equally dis-
tributed across all parts-of-speech, with comparable
improvements obtained for nouns, verbs, and adjec-
tives. A similar error rate reduction of 11.0% is ob-
tained for coarse-grained sense distinctions, which
suggests that the performance of the graph-based se-
quence data labeling algorithm does not depend on
classification granularity, and similar improvements
over individual data labeling can be obtained regard-
less of the average number of labels per word.
We also measured the variation of performance with
context size, and evaluated the disambiguation ac-
curacy for both algorithms for a window size rang-
ing from two words to an entire sentence. The win-
dow size parameter limits the number of surround-
ing words considered when seeking label dependen-
cies (sequence data labeling), or the words counted
in the measure of definition?context overlap (individ-
ual data labeling). Figure 3 plots the disambiguation
accuracy of the two algorithms as a function of con-
text size. As seen in the figure, both algorithms ben-
efit from larger contexts, with a steady increase in
performance observed for increasingly larger window
sizes. Although the initial growth observed for the se-
quence data labeling algorithm is somewhat sharper,
the gap between the two curves stabilizes for window
sizes larger than five words, which suggests that the
improvement in performance achieved with sequence
data labeling over individual data labeling does not de-
pend on the size of available context.
The algorithm was also evaluated on two other
data sets, SENSEVAL-3 English all-words data
(Snyder and Palmer, 2004) and a subset of SemCor
(Miller et al, 1993), although only fine-grained sense
evaluations could be conducted on these test sets.
The disambiguation precision on the SENSEVAL-3
data was measured at 52.2% using sequence data
labeling, compared to 48.1% obtained with individual
416
Fine-grained sense distinctions Coarse-grained sense distinctions
Random Individual Sequence Random Individual Sequence
Part-of baseline (Lesk) (graph-based) baseline (Lesk) (graph-based)
speech P R P R P R P R P R P R
Noun 41.4% 19.4% 50.3% 23.6% 57.5% 27.0% 42.7% 20.0% 51.4% 24.1% 58.8% 27.5%
Verb 20.7% 3.9% 30.5% 5.7% 36.5% 6.9% 22.8% 4.3% 31.9% 6.0% 37.9% 7.1%
Adjective 41.3% 9.3% 49.1% 11.0% 56.7% 12.7% 42.6% 42.6% 49.8% 11.2% 57.6% 12.9%
Adverb 44.6% 5.2% 64.6% 7.6% 70.9% 8.3% 40.7% 4.8% 65.3% 7.7% 71.9% 8.5%
ALL 37.9% 37.9% 48.7% 48.7% 54.2% 54.2% 38.7% 38.7% 49.8% 49.8% 55.3% 55.3%
Table 1: Precision and recall for graph-based sequence data labeling, individual data labeling, and random
baseline, for fine-grained and coarse-grained sense distinctions.
 35
 40
 45
 50
 55
 60
 0  5  10  15  20  25  30
D
is
am
bi
gu
at
io
n 
pr
ec
isi
on
 (%
)
Window size
sequence
individual
random
Figure 3: Disambiguation results using sequence data
labeling, individual labeling, and random baseline, for
various context sizes.
data labeling, and 34.3% achieved through random
sense assignment. The average disambiguation figure
obtained on all the words in a random subset of 10
SemCor documents, covering different domains, was
56.5% for sequence data labeling, 47.4% for individ-
ual labeling, and 35.3% for the random baseline.
Comparison with Related Work
For a given sequence of ambiguous words, the origi-
nal definition of the Lesk algorithm (Lesk, 1986), and
more recent improvements based on simulated anneal-
ing (Cowie et al, 1992), seek to identify the combina-
tion of senses that maximizes the overlap among their
dictionary definitions. Tests performed with this algo-
rithm on the SENSEVAL-2 data set resulted in a dis-
ambiguation accuracy of 39.5%. This precision is ex-
ceeded by the Lesk algorithm variation used in the ex-
periments reported in this paper, which measures the
overlap between sense definitions and the current con-
text, for a precision of 48.7% on the same data set (see
Table 1). In the SENSEVAL-2 evaluations, the best
performing fully unsupervised algorithm5 was devel-
oped by (Litkowski, 2001), who combines analysis of
multiword units and contextual clues based on collo-
cations and content words from dictionary definitions
and examples, for an overall precision and recall of
45.1%. More recently, (McCarthy et al, 2004) reports
one of the best results on the SENSEVAL-2 data set,
using an algorithm that automatically derives the most
frequent sense for a word using distributional similari-
ties learned from a large raw corpus, for a disambigua-
tion precision of 53.0% and a recall of 49.0%.
Another related line of work consists of the disam-
biguation algorithms based on lexical chains (Morris
and Hirst, 1991), and the more recent improvements
reported in (Galley and McKeown, 2003) ? where
threads of meaning are identified throughout a text.
Lexical chains however only take into account con-
nections between concepts identified in a static way,
without considering the importance of the concepts
that participate in a relation, which is recursively de-
termined in our algorithm. Moreover, the construction
of lexical chains requires structured dictionaries such
as WordNet, with explicitly defined semantic relations
between word senses, whereas our algorithm can also
work with simple unstructured dictionaries that pro-
vide only word sense definitions. (Galley and McK-
eown, 2003) evaluated their algorithm on the nouns
from a subset of SEMCOR, reporting 62.09% dis-
ambiguation precision. The performance of our al-
gorithm on the same subset of SEMCOR nouns was
measured at 64.2%6. Finally, another disambiguation
method relying on graph algorithms that exploit the
5Algorithms that integrate the most frequent sense in Word-
Net are not considered here, since this represents a supervised
knowledge source (WordNet sense frequencies are derived from a
sense-annotated corpus).
6Note that the results are not directly comparable, since (Gal-
ley and McKeown, 2003) used the WordNet sense order to break
the ties, whereas we assume that such sense order frequency is not
available, and thus we break the ties through random choice.
417
structure of semantic networks was proposed in (Mi-
halcea et al, 2004), with a disambiguation accuracy of
50.9% measured on all the words in the SENSEVAL-2
data set.
Although it relies exclusively on dictionary defini-
tions, the graph-based sequence data labeling algo-
rithm proposed in this paper, with its overall perfor-
mance of 54.2%, exceeds significantly the accuracy
of all these previously proposed unsupervised word
sense disambiguation methods, proving the benefits of
taking into account label dependencies when annotat-
ing sequence data. An additional interesting benefit of
the algorithm is that it provides a ranking over word
senses, and thus the selection of two or more most
probable senses for each word is also possible.
4 Conclusions
We proposed a graphical algorithm for sequence data
labeling that relies on random walks on graphs encod-
ing label dependencies. Through the label graphs it
builds for a given sequence of words, the algorithm ex-
ploits relations between word labels, and implements
a concept of recommendation. A label recommends
other related labels, and the strength of the recom-
mendation is recursively computed based on the im-
portance of the labels making the recommendation.
In this way, the algorithm simultaneously annotates
all the words in an input sequence, by identifying the
most probable (most recommended) set of labels.
The algorithm was illustrated and tested on an unsu-
pervised word sense disambiguation problem, target-
ing the annotation of all words in unrestricted texts.
Through experiments performed on standard sense-
annotated data sets, the graph-based sequence data la-
beling algorithm was shown to significantly outper-
form the accuracy achieved through individual data la-
beling, resulting in a statistically significant error rate
reduction of 10.7%. The disambiguation method was
also shown to exceed the performance of previously
proposed unsupervised word sense disambiguation al-
gorithms. Moreover, comparative results obtained un-
der various experimental settings have shown that the
algorithm is robust to changes in classification granu-
larity and context size.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation grant IIS-0336793.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
A. Budanitsky and G. Hirst. 2001. Semantic distance in
wordnet: An experimental, application-oriented evalu-
ation of five measures. In Proceedings of the NAACL
Workshop on WordNet and Other Lexical Resources,
Pittsburgh.
J. Cowie, L. Guthrie, and J. Guthrie. 1992. Lexical disam-
biguation using simulated annealing. In Proceedings of
the 5th International Conference on Computational Lin-
guistics (COLING 1992).
D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. 1992.
A practical part-of-speech tagger. In Proceedings of
the Third Conference on Applied Natural Language Pro-
cessing ANLP-92.
M. Galley and K. McKeown. 2003. Improving word sense
disambiguation in lexical chaining. In Proceedings of
the 18th International Joint Conference on Artificial In-
telligence (IJCAI 2003), Acapulco, Mexico, August.
G. Grimmett and D. Stirzaker. 1989. Probability and Ran-
dom Processes. Oxford University Press.
M.E. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In Proceedings of the SIGDOC
Conference 1986, Toronto.
K. Litkowski. 2001. Use of machine readable dictionaries
in word sense disambiguation for Senseval-2. In Pro-
ceedings of ACL/SIGLEX Senseval-2, Toulouse, France.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Using automatically acquired predominant senses
for word sense disambiguation. In Proceedings of
ACL/SIGLEX Senseval-3, Barcelona, Spain.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on se-
mantic networks, with application to word sense disam-
biguation. In Proceedings of the 20st International Con-
ference on Computational Linguistics (COLING 2004).
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Communi-
cation of the ACM, 38(11):39?41.
J. Morris and G. Hirst. 1991. Lexical cohesion, the the-
saurus, and the structure of text. Computational Lin-
guistics, 17(1):21?48.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H.T.
Dang. 2001. English tasks: all-words and verb lexi-
cal sample. In Proceedings of ACL/SIGLEX Senseval-2,
Toulouse, France.
B. Snyder and M. Palmer. 2004. The English all-
words task. In Proceedings of ACL/SIGLEX Senseval-3,
Barcelona, Spain.
F. Vasilescu, P. Langlais, and G. Lapalme. 2004. Evalu-
ating variants of the Lesk approach for disambiguating
words. In Proceedings of the Conference of Language
Resources and Evaluations (LREC 2004).
418
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 531?538, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Making Computers Laugh:
Investigations in Automatic Humor Recognition
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX, 76203, USA
rada@cs.unt.edu
Carlo Strapparava
Istituto per la Ricerca Scientifica e Tecnologica
ITC ? irst
I-38050, Povo, Trento, Italy
strappa@itc.it
Abstract
Humor is one of the most interesting and
puzzling aspects of human behavior. De-
spite the attention it has received in fields
such as philosophy, linguistics, and psy-
chology, there have been only few at-
tempts to create computational models for
humor recognition or generation. In this
paper, we bring empirical evidence that
computational approaches can be success-
fully applied to the task of humor recogni-
tion. Through experiments performed on
very large data sets, we show that auto-
matic classification techniques can be ef-
fectively used to distinguish between hu-
morous and non-humorous texts, with sig-
nificant improvements observed over apri-
ori known baselines.
1 Introduction
... pleasure has probably been the main goal all along. But I hesitate
to admit it, because computer scientists want to maintain their image
as hard-working individuals who deserve high salaries. Sooner or
later society will realize that certain kinds of hard work are in fact
admirable even though they are more fun than just about anything
else. (Knuth, 1993)
Humor is an essential element in personal com-
munication. While it is merely considered a way
to induce amusement, humor also has a positive ef-
fect on the mental state of those using it and has the
ability to improve their activity. Therefore computa-
tional humor deserves particular attention, as it has
the potential of changing computers into a creative
and motivational tool for human activity (Stock et
al., 2002; Nijholt et al, 2003).
Previous work in computational humor has fo-
cused mainly on the task of humor generation (Stock
and Strapparava, 2003; Binsted and Ritchie, 1997),
and very few attempts have been made to develop
systems for automatic humor recognition (Taylor
and Mazlack, 2004). This is not surprising, since,
from a computational perspective, humor recogni-
tion appears to be significantly more subtle and dif-
ficult than humor generation.
In this paper, we explore the applicability of
computational approaches to the recognition of ver-
bally expressed humor. In particular, we investigate
whether automatic classification techniques are a vi-
able approach to distinguish between humorous and
non-humorous text, and we bring empirical evidence
in support of this hypothesis through experiments
performed on very large data sets.
Since a deep comprehension of humor in all of
its aspects is probably too ambitious and beyond
the existing computational capabilities, we chose
to restrict our investigation to the type of humor
found in one-liners. A one-liner is a short sen-
tence with comic effects and an interesting linguistic
structure: simple syntax, deliberate use of rhetoric
devices (e.g. alliteration, rhyme), and frequent use
of creative language constructions meant to attract
the readers attention. While longer jokes can have
a relatively complex narrative structure, a one-liner
must produce the humorous effect ?in one shot?,
with very few words. These characteristics make
this type of humor particularly suitable for use in an
automatic learning setting, as the humor-producing
features are guaranteed to be present in the first (and
only) sentence.
We attempt to formulate the humor-recognition
531
problem as a traditional classification task, and feed
positive (humorous) and negative (non-humorous)
examples to an automatic classifier. The humor-
ous data set consists of one-liners collected from
the Web using an automatic bootstrapping process.
The non-humorous data is selected such that it
is structurally and stylistically similar to the one-
liners. Specifically, we use three different nega-
tive data sets: (1) Reuters news titles; (2) proverbs;
and (3) sentences from the British National Corpus
(BNC). The classification results are encouraging,
with accuracy figures ranging from 79.15% (One-
liners/BNC) to 96.95% (One-liners/Reuters). Re-
gardless of the non-humorous data set playing the
role of negative examples, the performance of the
automatically learned humor-recognizer is always
significantly better than apriori known baselines.
The remainder of the paper is organized as fol-
lows. We first describe the humorous and non-
humorous data sets, and provide details on the Web-
based bootstrapping process used to build a very
large collection of one-liners. We then show experi-
mental results obtained on these data sets using sev-
eral heuristics and two different text classifiers. Fi-
nally, we conclude with a discussion and directions
for future work.
2 Humorous and Non-humorous Data Sets
To test our hypothesis that automatic classification
techniques represent a viable approach to humor
recognition, we needed in the first place a data set
consisting of both humorous (positive) and non-
humorous (negative) examples. Such data sets can
be used to automatically learn computational mod-
els for humor recognition, and at the same time eval-
uate the performance of such models.
2.1 Humorous Data
For reasons outlined earlier, we restrict our attention
to one-liners, short humorous sentences that have the
characteristic of producing a comic effect in very
few words (usually 15 or less). The one-liners hu-
mor style is illustrated in Table 1, which shows three
examples of such one-sentence jokes.
It is well-known that large amounts of training
data have the potential of improving the accuracy of
the learning process, and at the same time provide
insights into how increasingly larger data sets can
affect the classification precision. The manual con-
enumerations matching
stylistic constraint (2)?
yes
yes
seed one?liners
automatically identified
        one?liners
Web search
webpages matching 
thematic constraint (1)?
candidate
webpages
Figure 1: Web-based bootstrapping of one-liners.
struction of a very large one-liner data set may be
however problematic, since most Web sites or mail-
ing lists that make available such jokes do not usu-
ally list more than 50?100 one-liners. To tackle this
problem, we implemented a Web-based bootstrap-
ping algorithm able to automatically collect a large
number of one-liners starting with a short seed list,
consisting of a few one-liners manually identified.
The bootstrapping process is illustrated in Figure
1. Starting with the seed set, the algorithm auto-
matically identifies a list of webpages that include at
least one of the seed one-liners, via a simple search
performed with a Web search engine. Next, the web-
pages found in this way are HTML parsed, and ad-
ditional one-liners are automatically identified and
added to the seed set. The process is repeated sev-
eral times, until enough one-liners are collected.
An important aspect of any bootstrapping algo-
rithm is the set of constraints used to steer the pro-
cess and prevent as much as possible the addition of
noisy entries. Our algorithm uses: (1) a thematic
constraint applied to the theme of each webpage;
and (2) a structural constraint, exploiting HTML an-
notations indicating text of similar genre.
The first constraint is implemented using a set
of keywords of which at least one has to appear
in the URL of a retrieved webpage, thus poten-
tially limiting the content of the webpage to a
theme related to that keyword. The set of key-
words used in the current implementation consists
of six words that explicitly indicate humor-related
content: oneliner, one-liner, humor, humour, joke,
532
One-liners
Take my advice; I don?t use it anyway.
I get enough exercise just pushing my luck.
Beauty is in the eye of the beer holder.
Reuters titles
Trocadero expects tripling of revenues.
Silver fixes at two-month high, but gold lags.
Oil prices slip as refiners shop for bargains.
BNC sentences
They were like spirits, and I loved them.
I wonder if there is some contradiction here.
The train arrives three minutes early.
Proverbs
Creativity is more important than knowledge.
Beauty is in the eye of the beholder.
I believe no tales from an enemy?s tongue.
Table 1: Sample examples of one-liners, Reuters ti-
tles, BNC sentences, and proverbs.
funny. For example, http://www.berro.com/Jokes
or http://www.mutedfaith.com/funny/life.htm are the
URLs of two webpages that satisfy this constraint.
The second constraint is designed to exploit the
HTML structure of webpages, in an attempt to iden-
tify enumerations of texts that include the seed one-
liner. This is based on the hypothesis that enumer-
ations typically include texts of similar genre, and
thus a list including the seed one-liner is likely to
include additional one-line jokes. For instance, if a
seed one-liner is found in a webpage preceded by the
HTML tag <li> (i.e. ?list item?), other lines found
in the same enumeration preceded by the same tag
are also likely to be one-liners.
Two iterations of the bootstrapping process,
started with a small seed set of ten one-liners, re-
sulted in a large set of about 24,000 one-liners.
After removing the duplicates using a measure of
string similarity based on the longest common sub-
sequence metric, we were left with a final set of
approximately 16,000 one-liners, which are used in
the humor-recognition experiments. Note that since
the collection process is automatic, noisy entries are
also possible. Manual verification of a randomly se-
lected sample of 200 one-liners indicates an average
of 9% potential noise in the data set, which is within
reasonable limits, as it does not appear to signifi-
cantly impact the quality of the learning.
2.2 Non-humorous Data
To construct the set of negative examples re-
quired by the humor-recognition models, we tried
to identify collections of sentences that were non-
humorous, but similar in structure and composition
to the one-liners. We do not want the automatic clas-
sifiers to learn to distinguish between humorous and
non-humorous examples based simply on text length
or obvious vocabulary differences. Instead, we seek
to enforce the classifiers to identify humor-specific
features, by supplying them with negative examples
similar in most of their aspects to the positive exam-
ples, but different in their comic effect.
We tested three different sets of negative exam-
ples, with three examples from each data set illus-
trated in Table 1. All non-humorous examples are
enforced to follow the same length restriction as the
one-liners, i.e. one sentence with an average length
of 10?15 words.
1. Reuters titles, extracted from news articles pub-
lished in the Reuters newswire over a period of
one year (8/20/1996 ? 8/19/1997) (Lewis et al,
2004). The titles consist of short sentences with
simple syntax, and are often phrased to catch
the readers attention (an effect similar to the
one rendered by one-liners).
2. Proverbs extracted from an online proverb col-
lection. Proverbs are sayings that transmit, usu-
ally in one short sentence, important facts or
experiences that are considered true by many
people. Their property of being condensed, but
memorable sayings make them very similar to
the one-liners. In fact, some one-liners attempt
to reproduce proverbs, with a comic effect, as
in e.g. ?Beauty is in the eye of the beer holder?,
derived from ?Beauty is in the eye of the be-
holder?.
3. British National Corpus (BNC) sentences, ex-
tracted from BNC ? a balanced corpus covering
different styles, genres and domains. The sen-
tences were selected such that they were similar
in content with the one-liners: we used an in-
formation retrieval system implementing a vec-
torial model to identify the BNC sentence most
similar to each of the 16,000 one-liners1 . Un-
like the Reuters titles or the proverbs, the BNC
sentences have typically no added creativity.
However, we decided to add this set of negative
examples to our experimental setting, in order
1The sentence most similar to a one-liner is identified by
running the one-liner against an index built for all BNC sen-
tences with a length of 10?15 words. We use a tf.idf weighting
scheme and a cosine similarity measure, as implemented in the
Smart system (ftp.cs.cornell.edu/pub/smart)
533
to observe the level of difficulty of a humor-
recognition task when performed with respect
to simple text.
To summarize, the humor recognition experiments
rely on data sets consisting of humorous (positive)
and non-humorous (negative) examples. The posi-
tive examples consist of 16,000 one-liners automat-
ically collected using a Web-based bootstrapping
process. The negative examples are drawn from: (1)
Reuters titles; (2) Proverbs; and (3) BNC sentences.
3 Automatic Humor Recognition
We experiment with automatic classification tech-
niques using: (a) heuristics based on humor-specific
stylistic features (alliteration, antonymy, slang); (b)
content-based features, within a learning framework
formulated as a typical text classification task; and
(c) combined stylistic and content-based features,
integrated in a stacked machine learning framework.
3.1 Humor-Specific Stylistic Features
Linguistic theories of humor (Attardo, 1994) have
suggested many stylistic features that characterize
humorous texts. We tried to identify a set of fea-
tures that were both significant and feasible to im-
plement using existing machine readable resources.
Specifically, we focus on alliteration, antonymy, and
adult slang, which were previously suggested as po-
tentially good indicators of humor (Ruch, 2002; Bu-
caria, 2004).
Alliteration. Some studies on humor appreciation
(Ruch, 2002) show that structural and phonetic prop-
erties of jokes are at least as important as their con-
tent. In fact one-liners often rely on the reader?s
awareness of attention-catching sounds, through lin-
guistic phenomena such as alliteration, word repeti-
tion and rhyme, which produce a comic effect even if
the jokes are not necessarily meant to be read aloud.
Note that similar rhetorical devices play an impor-
tant role in wordplay jokes, and are often used in
newspaper headlines and in advertisement. The fol-
lowing one-liners are examples of jokes that include
one or more alliteration chains:
Veni, Vidi, Visa: I came, I saw, I did a little shopping.
Infants don?t enjoy infancy like adults do adultery.
To extract this feature, we identify and count the
number of alliteration/rhyme chains in each exam-
ple in our data set. The chains are automatically ex-
tracted using an index created on top of the CMU
pronunciation dictionary2 .
Antonymy. Humor often relies on some type of
incongruity, opposition or other forms of apparent
contradiction. While an accurate identification of
all these properties is probably difficult to accom-
plish, it is relatively easy to identify the presence of
antonyms in a sentence. For instance, the comic ef-
fect produced by the following one-liners is partly
due to the presence of antonyms:
A clean desk is a sign of a cluttered desk drawer.
Always try to be modest and be proud of it!
The lexical resource we use to identify antonyms
is WORDNET (Miller, 1995), and in particular the
antonymy relation among nouns, verbs, adjectives
and adverbs. For adjectives we also consider an in-
direct antonymy via the similar-to relation among
adjective synsets. Despite the relatively large num-
ber of antonymy relations defined in WORDNET,
its coverage is far from complete, and thus the
antonymy feature cannot always be identified. A
deeper semantic analysis of the text, such as word
sense disambiguation or domain disambiguation,
could probably help detecting other types of seman-
tic opposition, and we plan to exploit these tech-
niques in future work.
Adult slang. Humor based on adult slang is very
popular. Therefore, a possible feature for humor-
recognition is the detection of sexual-oriented lexi-
con in the sentence. The following represent exam-
ples of one-liners that include such slang:
The sex was so good that even the neighbors had a cigarette.
Artificial Insemination: procreation without recreation.
To form a lexicon required for the identification of
this feature, we extract from WORDNET DOMAINS3
all the synsets labeled with the domain SEXUALITY.
The list is further processed by removing all words
with high polysemy (? 4). Next, we check for the
presence of the words in this lexicon in each sen-
tence in the corpus, and annotate them accordingly.
Note that, as in the case of antonymy, WORDNET
coverage is not complete, and the adult slang fea-
ture cannot always be identified.
Finally, in some cases, all three features (alliteration,
2Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict
3WORDNET DOMAINS assigns each synset in WORDNET
with one or more ?domain? labels, such as SPORT, MEDICINE,
ECONOMY. See http://wndomains.itc.it.
534
antonymy, adult slang) are present in the same sen-
tence, as for instance the following one-liner:
Behind every greatal manant is a greatal womanant, and
behind every greatal womanant is some guy staring at her
behindsl!
3.2 Content-based Learning
In addition to stylistic features, we also experi-
mented with content-based features, through ex-
periments where the humor-recognition task is for-
mulated as a traditional text classification problem.
Specifically, we compare results obtained with two
frequently used text classifiers, Na??ve Bayes and
Support Vector Machines, selected based on their
performance in previously reported work, and for
their diversity of learning methodologies.
Na??ve Bayes. The main idea in a Na??ve Bayes text
classifier is to estimate the probability of a category
given a document using joint probabilities of words
and documents. Na??ve Bayes classifiers assume
word independence, but despite this simplification,
they perform well on text classification. While there
are several versions of Na??ve Bayes classifiers (vari-
ations of multinomial and multivariate Bernoulli),
we use the multinomial model, previously shown to
be more effective (McCallum and Nigam, 1998).
Support Vector Machines. Support Vector Ma-
chines (SVM) are binary classifiers that seek to find
the hyperplane that best separates a set of posi-
tive examples from a set of negative examples, with
maximum margin. Applications of SVM classifiers
to text categorization led to some of the best results
reported in the literature (Joachims, 1998).
4 Experimental Results
Several experiments were conducted to gain insights
into various aspects related to an automatic hu-
mor recognition task: classification accuracy using
stylistic and content-based features, learning rates,
impact of the type of negative data, impact of the
classification methodology.
All evaluations are performed using stratified ten-
fold cross validations, for accurate estimates. The
baseline for all the experiments is 50%, which rep-
resents the classification accuracy obtained if a label
of ?humorous? (or ?non-humorous?) would be as-
signed by default to all the examples in the data set.
Experiments with uneven class distributions were
also performed, and are reported in section 4.4.
4.1 Heuristics using Humor-specific Features
In a first set of experiments, we evaluated the classi-
fication accuracy using stylistic humor-specific fea-
tures: alliteration, antonymy, and adult slang. These
are numerical features that act as heuristics, and the
only parameter required for their application is a
threshold indicating the minimum value admitted for
a statement to be classified as humorous (or non-
humorous). These thresholds are learned automat-
ically using a decision tree applied on a small subset
of humorous/non-humorous examples (1000 exam-
ples). The evaluation is performed on the remaining
15,000 examples, with results shown in Table 24.
One-liners One-liners One-liners
Heuristic Reuters BNC Proverbs
Alliteration 74.31% 59.34% 53.30%
Antonymy 55.65% 51.40% 50.51%
Adult slang 52.74% 52.39% 50.74%
ALL 76.73% 60.63% 53.71%
Table 2: Humor-recognition accuracy using allitera-
tion, antonymy, and adult slang.
Considering the fact that these features represent
stylistic indicators, the style of Reuters titles turns
out to be the most different with respect to one-
liners, while the style of proverbs is the most sim-
ilar. Note that for all data sets the alliteration feature
appears to be the most useful indicator of humor,
which is in agreement with previous linguistic find-
ings (Ruch, 2002).
4.2 Text Classification with Content Features
The second set of experiments was concerned with
the evaluation of content-based features for humor
recognition. Table 3 shows results obtained using
the three different sets of negative examples, with
the Na??ve Bayes and SVM text classifiers. Learning
curves are plotted in Figure 2.
One-liners One-liners One-liners
Classifier Reuters BNC Proverbs
Na ??ve Bayes 96.67% 73.22% 84.81%
SVM 96.09% 77.51% 84.48%
Table 3: Humor-recognition accuracy using Na??ve
Bayes and SVM text classifiers.
4We also experimented with decision trees learned from a
larger number of examples, but the results were similar, which
confirms our hypothesis that these features are heuristics, rather
than learnable properties that improve their accuracy with addi-
tional training data.
535
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Naive Bayes
SVM
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Naive Bayes
SVM
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Naive Bayes
SVM
(a) (b) (c)
Figure 2: Learning curves for humor-recognition using text classification techniques, with respect to three
different sets of negative examples: (a) Reuters; (b) BNC; (c) Proverbs.
Once again, the content of Reuters titles appears
to be the most different with respect to one-liners,
while the BNC sentences represent the most simi-
lar data set. This suggests that joke content tends to
be very similar to regular text, although a reasonably
accurate distinction can still be made using text clas-
sification techniques. Interestingly, proverbs can be
distinguished from one-liners using content-based
features, which indicates that despite their stylistic
similarity (see Table 2), proverbs and one-liners deal
with different topics.
4.3 Combining Stylistic and Content Features
Encouraged by the results obtained in the first
two experiments, we designed a third experiment
that attempts to jointly exploit stylistic and con-
tent features for humor recognition. The feature
combination is performed using a stacked learner,
which takes the output of the text classifier, joins it
with the three humor-specific features (alliteration,
antonymy, adult slang), and feeds the newly created
feature vectors to a machine learning tool. Given
the relatively large gap between the performance
achieved with content-based features (text classifi-
cation) and stylistic features (humor-specific heuris-
tics), we decided to implement the second learning
stage in the stacked learner using a memory based
learning system, so that low-performance features
are not eliminated in the favor of the more accu-
rate ones5. We use the Timbl memory based learner
(Daelemans et al, 2001), and evaluate the classifica-
tion using a stratified ten-fold cross validation. Table
5Using a decision tree learner in a similar stacked learning
experiment resulted into a flat tree that takes a classification de-
cision based exclusively on the content feature, ignoring com-
pletely the remaining stylistic features.
4 shows the results obtained in this experiment, for
the three different data sets.
One-liners One-liners One-liners
Reuters BNC Proverbs
96.95% 79.15% 84.82%
Table 4: Humor-recognition accuracy for combined
learning based on stylistic and content features.
Combining classifiers results in a statistically sig-
nificant improvement (p < 0.0005, paired t-test)
with respect to the best individual classifier for the
One-liners/Reuters and One-liners/BNC data sets,
with relative error rate reductions of 8.9% and 7.3%
respectively. No improvement is observed for the
One-liners/Proverbs data set, which is not surpris-
ing since, as shown in Table 2, proverbs and one-
liners cannot be clearly differentiated using stylistic
features, and thus the addition of these features to
content-based features is not likely to result in an
improvement.
4.4 Discussion
The results obtained in the automatic classification
experiments reveal the fact that computational ap-
proaches represent a viable solution for the task of
humor-recognition, and good performance can be
achieved using classification techniques based on
stylistic and content features.
Despite our initial intuition that one-liners are
most similar to other creative texts (e.g. Reuters ti-
tles, or the sometimes almost identical proverbs),
and thus the learning task would be more difficult in
relation to these data sets, comparative experimental
results show that in fact it is more difficult to distin-
guish humor with respect to regular text (e.g. BNC
536
sentences). Note however that even in this case the
combined classifier leads to a classification accuracy
that improves significantly over the apriori known
baseline.
An examination of the content-based features
learned during the classification process reveals in-
teresting aspects of the humorous texts. For in-
stance, one-liners seem to constantly make reference
to human-related scenarios, through the frequent use
of words such as man, woman, person, you, I. Simi-
larly, humorous texts seem to often include negative
word forms, such as the negative verb forms doesn?t,
isn?t, don?t, or negative adjectives like wrong or bad.
A more extensive analysis of content-based humor-
specific features is likely to reveal additional humor-
specific content features, which could also be used in
studies of humor generation.
In addition to the three negative data sets, we also
performed an experiment using a corpus of arbitrary
sentences randomly drawn from the three negative
sets. The humor recognition with respect to this neg-
ative mixed data set resulted in 63.76% accuracy for
stylistic features, 77.82% for content-based features
using Na??ve Bayes and 79.23% using SVM. These
figures are comparable to those reported in Tables 2
and 3 for One-liners/BNC, which suggests that the
experimental results reported in the previous sec-
tions do not reflect a bias introduced by the negative
data sets, since similar results are obtained when the
humor recognition is performed with respect to ar-
bitrary negative examples.
As indicated in section 2.2, the negative exam-
ples were selected structurally and stylistically sim-
ilar to the one-liners, making the humor recognition
task more difficult than in a real setting. Nonethe-
less, we also performed a set of experiments where
we made the task even harder, using uneven class
distributions. For each of the three types of nega-
tive examples, we constructed a data set using 75%
non-humorous examples and 25% humorous exam-
ples. Although the baseline in this case is higher
(75%), the automatic classification techniques for
humor-recognition still improve over this baseline.
The stylistic features lead to a classification accu-
racy of 87.49% (One-liners/Reuters), 77.62% (One-
liners/BNC), and 76.20% (One-liners/Proverbs),
and the content-based features used in a Na??ve
Bayes classifier result in accuracy figures of 96.19%
(One-liners/Reuters), 81.56% (One-liners/BNC),
and 87.86% (One-liners/Proverbs).
Finally, in addition to classification accuracy, we
were also interested in the variation of classifica-
tion performance with respect to data size, which
is an aspect particularly relevant for directing fu-
ture research. Depending on the shape of the learn-
ing curves, one could decide to concentrate future
work either on the acquisition of larger data sets, or
toward the identification of more sophisticated fea-
tures. Figure 2 shows that regardless of the type of
negative data, there is significant learning only un-
til about 60% of the data (i.e. about 10,000 positive
examples, and the same number of negative exam-
ples). The rather steep ascent of the curve, especially
in the first part of the learning, suggests that humor-
ous and non-humorous texts represent well distin-
guishable types of data. An interesting effect can
be noticed toward the end of the learning, where for
both classifiers the curve becomes completely flat
(One-liners/Reuters, One-liners/Proverbs), or it even
has a slight drop (One-liners/BNC). This is probably
due to the presence of noise in the data set, which
starts to become visible for very large data sets6.
This plateau is also suggesting that more data is not
likely to help improve the quality of an automatic
humor-recognizer, and more sophisticated features
are probably required.
5 Related Work
While humor is relatively well studied in scientific
fields such as linguistics (Attardo, 1994) and psy-
chology (Freud, 1905; Ruch, 2002), to date there
is only a limited number of research contributions
made toward the construction of computational hu-
mour prototypes.
One of the first attempts is perhaps the work de-
scribed in (Binsted and Ritchie, 1997), where a for-
mal model of semantic and syntactic regularities was
devised, underlying some of the simplest types of
puns (punning riddles). The model was then ex-
ploited in a system called JAPE that was able to au-
tomatically generate amusing puns.
Another humor-generation project was the HA-
HAcronym project (Stock and Strapparava, 2003),
whose goal was to develop a system able to au-
tomatically generate humorous versions of existing
6We also like to think of this behavior as if the computer
is losing its sense of humor after an overwhelming number of
jokes, in a way similar to humans when they get bored and stop
appreciating humor after hearing too many jokes.
537
acronyms, or to produce a new amusing acronym
constrained to be a valid vocabulary word, starting
with concepts provided by the user. The comic ef-
fect was achieved mainly by exploiting incongruity
theories (e.g. finding a religious variation for a tech-
nical acronym).
Another related work, devoted this time to the
problem of humor comprehension, is the study re-
ported in (Taylor and Mazlack, 2004), focused on
a very restricted type of wordplays, namely the
?Knock-Knock? jokes. The goal of the study was
to evaluate to what extent wordplay can be automati-
cally identified in ?Knock-Knock? jokes, and if such
jokes can be reliably recognized from other non-
humorous text. The algorithm was based on auto-
matically extracted structural patterns and on heuris-
tics heavily based on the peculiar structure of this
particular type of jokes. While the wordplay recog-
nition gave satisfactory results, the identification of
jokes containing such wordplays turned out to be
significantly more difficult.
6 Conclusion
A conclusion is simply the place where you got tired of thinking.
(anonymous one-liner)
The creative genres of natural language have been
traditionally considered outside the scope of any
computational modeling. In particular humor, be-
cause of its puzzling nature, has received little atten-
tion from computational linguists. However, given
the importance of humor in our everyday life, and
the increasing importance of computers in our work
and entertainment, we believe that studies related to
computational humor will become increasingly im-
portant.
In this paper, we showed that automatic classifi-
cation techniques can be successfully applied to the
task of humor-recognition. Experimental results ob-
tained on very large data sets showed that computa-
tional approaches can be efficiently used to distin-
guish between humorous and non-humorous texts,
with significant improvements observed over apriori
known baselines. To our knowledge, this is the first
result of this kind reported in the literature, as we
are not aware of any previous work investigating the
interaction between humor and techniques for auto-
matic classification.
Finally, through the analysis of learning curves
plotting the classification performance with respect
to data size, we showed that the accuracy of the au-
tomatic humor-recognizer stops improving after a
certain number of examples. Given that automatic
humor-recognition is a rather understudied problem,
we believe that this is an important result, as it pro-
vides insights into potentially productive directions
for future work. The flattened shape of the curves
toward the end of the learning process suggests that
rather than focusing on gathering more data, fu-
ture work should concentrate on identifying more
sophisticated humor-specific features, e.g. semantic
oppositions, ambiguity, and others. We plan to ad-
dress these aspects in future work.
References
S. Attardo. 1994. Linguistic Theory of Humor. Mouton de
Gruyter, Berlin.
K. Binsted and G. Ritchie. 1997. Computational rules for pun-
ning riddles. Humor, 10(1).
C. Bucaria. 2004. Lexical and syntactic ambiguity as a source
of humor. Humor, 17(3).
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner, ver-
sion 4.0, reference guide. Technical report, University of
Antwerp.
S. Freud. 1905. Der Witz und Seine Beziehung zum Unbe-
wussten. Deutike, Vienna.
T. Joachims. 1998. Text categorization with Support Vector
Machines: learning with many relevant features. In Pro-
ceedings of the European Conference on Machine Learning.
D.E. Knuth. 1993. The Stanford Graph Base: A Platform for
combinatorial computing. ACM Press.
D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1: A new
benchmark collection for text categorization research. The
Journal of Machine Learning Research, 5:361?397.
A. McCallum and K. Nigam. 1998. A comparison of event
models for Naive Bayes text classification. In Proceedings
of AAAI-98 Workshop on Learning for Text Categorization.
G. Miller. 1995. Wordnet: A lexical database. Communication
of the ACM, 38(11):39?41.
A. Nijholt, O. Stock, A. Dix, and J. Morkes, editors. 2003. Pro-
ceedings of CHI-2003 workshop: Humor Modeling in the
Interface, Fort Lauderdale, Florida.
W. Ruch. 2002. Computers with a personality? lessons to be
learned from studies of the psychology of humor. In Pro-
ceedings of the The April Fools Day Workshop on Computa-
tional Humour.
O. Stock and C. Strapparava. 2003. Getting serious about the
development of computational humour. In Proceedings of
the 8th International Joint Conference on Artificial Intelli-
gence (IJCAI-03), Acapulco, Mexico.
O. Stock, C. Strapparava, and A. Nijholt, editors. 2002. Pro-
ceedings of the The April Fools Day Workshop on Computa-
tional Humour, Trento.
J. Taylor and L. Mazlack. 2004. Computationally recognizing
wordplay in jokes. In Proceedings of CogSci 2004, Chicago.
538
A Language Independent Algorithm for
Single and Multiple Document Summarization
Rada Mihalcea and Paul Tarau
Department of Computer Science and Engineering
University of North Texas
{rada,tarau}@cs.unt.edu
Abstract
This paper describes a method for lan-
guage independent extractive summariza-
tion that relies on iterative graph-based
ranking algorithms. Through evalua-
tions performed on a single-document
summarization task for English and Por-
tuguese, we show that the method per-
forms equally well regardless of the lan-
guage. Moreover, we show how a meta-
summarizer relying on a layered appli-
cation of techniques for single-document
summarization can be turned into an ef-
fective method for multi-document sum-
marization.
1 Introduction
Algorithms for extractive summarization are typi-
cally based on techniques for sentence extraction,
and attempt to identify the set of sentences that are
most important for the overall understanding of a
given document. Some of the most successful ap-
proaches consist of supervised algorithms that at-
tempt to learn what makes a good summary by
training on collections of summaries built for a rela-
tively large number of training documents, e.g. (Hi-
rao et al, 2002), (Teufel and Moens, 1997). How-
ever, the price paid for the high performance of
such supervised algorithms is their inability to eas-
ily adapt to new languages or domains, as new train-
ing data are required for each new data type. In
this paper, we show that a method for extractive
summarization relying on iterative graph-based al-
gorithms, as previously proposed in (Mihalcea and
Tarau, 2004) can be applied to the summarization
of documents in different languages without any re-
quirements for additional data. Additionally, we
also show that a layered application of this single-
document summarization method can result into an
efficient multi-document summarization tool.
Earlier experiments with graph-based ranking al-
gorithms for text summarization, as previously re-
ported in (Mihalcea and Tarau, 2004) and (Erkan
and Radev, 2004), were either limited to single-
document English summarization, or they were ap-
plied to English multi-document summarization,
but in conjunction with other extractive summariza-
tion techniques that did not allow for a clear evalua-
tion of the impact of the graph algorithms alone. In
this paper, we show that a method exclusively based
on graph-based algorithms can be successfully ap-
plied to the summarization of single and multiple
documents in any language, and show that the re-
sults are competitive with those of state-of-the-art
summarization systems.
The paper is organized as follows. Section 2
briefly overviews two iterative graph-based ranking
algorithms, and shows how these algorithms can be
applied to single and multiple document summa-
rization. Section 3 describes the data sets used in
the summarization experiments and the evaluation
methodology. Experimental results are presented in
Section 4, followed by discussions, pointers to re-
lated work, and conclusions.
2 Iterative Graph-based Algorithms for
Extractive Summarization
In this section, we shortly describe two graph-based
ranking algorithms and their application to the task
of extractive summarization. Ranking algorithms,
such as Kleinberg?s HITS algorithm (Kleinberg,
1999) or Google?s PageRank (Brin and Page, 1998),
have been traditionally and successfully used in
Web-link analysis (Brin and Page, 1998), social net-
works, and more recently in text processing appli-
cations (Mihalcea and Tarau, 2004), (Mihalcea et
al., 2004), (Erkan and Radev, 2004). In short, a
graph-based ranking algorithm is a way of decid-
ing on the importance of a vertex within a graph, by
taking into account global information recursively
computed from the entire graph, rather than relying
19
only on local vertex-specific information. The ba-
sic idea implemented by the ranking model is that
of ?voting? or ?recommendation?. When one vertex
links to another one, it is basically casting a vote for
that other vertex. The higher the number of votes
that are cast for a vertex, the higher the importance
of the vertex.
Let G = (V,E) be a directed graph with the set
of vertices V and set of edges E, where E is a sub-
set of V ?V . For a given vertex Vi, let In(Vi) be the
set of vertices that point to it (predecessors), and let
Out(Vi) be the set of vertices that vertex Vi points
to (successors).
PageRank. PageRank (Brin and Page, 1998)
is perhaps one of the most popular ranking algo-
rithms, and was designed as a method for Web link
analysis. Unlike other graph ranking algorithms,
PageRank integrates the impact of both incoming
and outgoing links into one single model, and there-
fore it produces only one set of scores:
PR(Vi) = (1? d) + d ?
?
Vj?In(Vi)
PR(Vj)
|Out(Vj)| (1)
where d is a parameter set between 0 and 1.
HITS. HITS (Hyperlinked Induced Topic
Search) (Kleinberg, 1999) is an iterative algorithm
that was designed for ranking Web pages according
to their degree of ?authority?. The HITS algo-
rithm makes a distinction between ?authorities?
(pages with a large number of incoming links) and
?hubs? (pages with a large number of outgoing
links). For each vertex, HITS produces two sets
of scores ? an ?authority? score, and a ?hub? score:
HITSA(Vi) =
?
Vj?In(Vi)
HITSH(Vj) (2)
HITSH(Vi) =
?
Vj?Out(Vi)
HITSA(Vj) (3)
For each of these algorithms, starting from arbitrary
values assigned to each node in the graph, the com-
putation iterates until convergence below a given
threshold is achieved. After running the algorithm,
a score is associated with each vertex, which rep-
resents the ?importance? or ?power? of that vertex
within the graph.
In the context of Web surfing or citation analy-
sis, it is unusual for a vertex to include multiple or
partial links to another vertex, and hence the orig-
inal definition for graph-based ranking algorithms
is assuming unweighted graphs. However, when
the graphs are built starting with natural language
texts, they may include multiple or partial links be-
tween the units (vertices) that are extracted from
text. It may be therefore useful to integrate into
the model the ?strength? of the connection between
two vertices Vi and Vj as a weight wij added to
the corresponding edge that connects the two ver-
tices. The ranking algorithms are thus adapted to
include edge weights, e.g. for PageRank the score
is determined using the following formula (a similar
change can be applied to the HITS algorithm):
PRW (Vi) = (1?d)+d?
?
Vj?In(Vi)
wji PR
W (Vj)?
Vk?Out(Vj)
wkj
(4)
[1] Watching the new movie, ?Imagine: John Lennon,? was
very painful for the late Beatle?s wife, Yoko Ono.
[2] ?The only reason why I did watch it to the end is because
I?m responsible for it, even though somebody else made it,?
she said.
[3] Cassettes, film footage and other elements of the acclaimed
movie were collected by Ono.
[4] She also took cassettes of interviews by Lennon, which
were edited in such a way that he narrates the picture.
[5] Andrew Solt (?This Is Elvis?) directed, Solt and David L.
Wolper produced and Solt and Sam Egan wrote it.
[6] ?I think this is really the definitive documentary of John
Lennon?s life,? Ono said in an interview.
1
2
3
4
5
6 0.16
0.30
0.46 0.15
[1.34]
[1.75]
[0.70]
[0.74]
[0.52]
[0.91]
0.15
0.29
0.32
0.15
Figure 1: Graph of sentence similarities built on a
sample text. Scores reflecting sentence importance
are shown in brackets next to each sentence.
While the final vertex scores (and therefore rank-
ings) for weighted graphs differ significantly as
compared to their unweighted alternatives, the num-
ber of iterations to convergence and the shape of the
convergence curves is almost identical for weighted
and unweighted graphs.
2.1 Single Document Summarization
For the task of single-document extractive summa-
rization, the goal is to rank the sentences in a given
text with respect to their importance for the overall
understanding of the text. A graph is therefore con-
structed by adding a vertex for each sentence in the
text, and edges between vertices are established us-
ing sentence inter-connections. These connections
20
are defined using a similarity relation, where ?simi-
larity? is measured as a function of content overlap.
Such a relation between two sentences can be seen
as a process of ?recommendation?: a sentence that
addresses certain concepts in a text gives the reader
a ?recommendation? to refer to other sentences in
the text that address the same concepts, and there-
fore a link can be drawn between any two such sen-
tences that share common content.
The overlap of two sentences can be determined
simply as the number of common tokens between
the lexical representations of two sentences, or it
can be run through syntactic filters, which only
count words of a certain syntactic category. More-
over, to avoid promoting long sentences, we use a
normalization factor, and divide the content overlap
of two sentences with the length of each sentence.
The resulting graph is highly connected, with a
weight associated with each edge, indicating the
strength of the connections between various sen-
tence pairs in the text. The graph can be repre-
sented as: (a) simple undirected graph; (b) directed
weighted graph with the orientation of edges set
from a sentence to sentences that follow in the text
(directed forward); or (c) directed weighted graph
with the orientation of edges set from a sentence to
previous sentences in the text (directed backward).
After the ranking algorithm is run on the graph,
sentences are sorted in reversed order of their score,
and the top ranked sentences are selected for inclu-
sion in the extractive summary. Figure 1 shows an
example of a weighted graph built for a sample text
of six sentences.
2.2 Multiple Document Summarization
Multi-document summaries are built using a ?meta?
summarization procedure. First, for each document
in a given cluster of documents, a single document
summary is generated using one of the graph-based
ranking algorithms. Next, a ?summary of sum-
maries? is produced using the same or a different
ranking algorithm. Figure 2 illustrates the meta-
summarization process used to generate a multi-
document summary starting with a cluster of N
documents.
Unlike single documents ? where sentences with
highly similar content are very rarely if at all en-
countered ? it is often the case that clusters of mul-
tiple documents, all addressing the same or related
topics, would contain very similar or even identical
sentences. To avoid such pairs of sentences, which
may decrease the readability and the amount of in-
formation conveyed by a summary, we introduce a
maximum threshold on the sentence similarity mea-
sure. Consequently, in the graph construction stage,
no link (edge) is added between sentences (ver-
tices) whose similarity exceeds this threshold. In
Single?document 
summarization Single?document summarization
Summary Document 1
Summary Document 2
Summary Document N
......
Single?document 
summarization
Single?document 
summarization
Document 1 Document 2 Document N
Meta?document
Multi?document summary
Figure 2: Generation of a multi-document summary
using meta-summarization.
the experiments reported in this paper, this similar-
ity threshold was empirically set to 0.5.
3 Materials and Evaluation Methodology
Single and multiple English document summariza-
tion experiments are run using the summarization
test collection provided in the framework of the
Document Understanding Conference (DUC). In
particular, we use the data set of 567 news arti-
cles made available during the DUC 2002 evalu-
ations (DUC, 2002), and the corresponding 100-
word summaries generated for each of these doc-
uments (single-document summarization), or the
100-word summaries generated for each of the 59
document clusters formed on the same data set
(multi-document summarization). These are the
summarization tasks undertaken by other systems
participating in the DUC 2002 document summa-
rization evaluations.
To test the language independence aspect of the
algorithm, in addition to the English test collection,
we also use a Brazilian Portuguese data set con-
sisting of 100 news articles and their correspond-
ing manually produced summaries. We use the
TeMa?rio test collection (Pardo and Rino, 2003),
containing newspaper articles from online Brazilian
newswire: 40 documents from Jornal de Brasil and
60 documents from Folha de Sa?o Paulo. The doc-
uments were selected to cover a variety of domains
(e.g. world, politics, foreign affairs, editorials), and
manual summaries were produced by an expert in
Brazilian Portuguese. Unlike the summaries pro-
duced for the English DUC documents ? which had
a length requirement of approximately 100 words,
the length of the summaries in the TeMa?rio data
set is constrained relative to the length of the corre-
21
sponding documents, i.e. a summary has to account
for about 25-30% of the original document. Con-
sequently, the automatic summaries generated for
the documents in this collection are not restricted to
100 words, as in the English experiments, but are
required to have a length comparable to the corre-
sponding manual summaries, to ensure a fair evalu-
ation.
For evaluation, we are using the ROUGE evalu-
ation toolkit1, which is a method based on Ngram
statistics, found to be highly correlated with human
evaluations (Lin and Hovy, 2003a). The evaluation
is done using the Ngram(1,1) setting of ROUGE,
which was found to have the highest correlation
with human judgments, at a confidence level of
95%.
4 Experimental Results
The extractive summarization algorithm is evalu-
ated in the context of: (1) A single-document sum-
marization task, where a summary is generated for
each of the 567 English news articles provided dur-
ing the Document Understanding Evaluations 2002
(DUC, 2002), and for each of the 100 Portuguese
documents in the TeMa?rio data set; and (2) A multi-
document summarization task, where a summary is
generated for each of the 59 document clusters in
the DUC 2002 data. Since document clusters and
multi-document summaries are not available for the
Portuguese documents, a multi-document summa-
rization evaluation could not be conducted on this
data set. Note however that the multi-document
summarization tool is based on the single-document
summarization method (see Figure 2), and thus high
performance in single-document summarization is
expected to result into a similar level of perfor-
mance in multi-document summarization.
4.1 Single Document Summarization for
English
For single-document summarization, we evaluate
the extractive summaries produced using each of
the two graph-based ranking algorithms described
in Section 2 (HITS and PageRank). Table 1
shows the results obtained for the 100-words au-
tomatically generated summaries for the English
DUC 2002 data set. The table shows results us-
ing the two graph algorithms described in Section
2 when using graphs that are: (a) undirected, (b)
directed forward, or (c) directed backward2.
For a comparative evaluation, Table 2 shows the
results obtained on this data set by the top 5 (out
1ROUGE is available at http://www.isi.edu/?cyl/ROUGE/.
2Note that the first two rows in the table are in fact redun-
dant, since the ?hub? variation of the HITS algorithm can be
derived from its ?authority? counterpart by reversing the edge
orientation in the graphs.
Graph
Algorithm Undir. Forward Backward
HITSWA 49.12 45.84 50.23
HITSWH 49.12 50.23 45.84
PageRankW 49.04 42.02 50.08
Table 1: Results for English single-document sum-
marization.
of 15) performing systems participating in the sin-
gle document summarization task at DUC 2002. It
also lists the baseline performance, computed for
100-word summaries generated by taking the first
sentences in each article.
Top 5 systems (DUC, 2002)
S27 S31 S28 S21 S29 Baseline
50.11 49.14 48.90 48.69 46.81 47.99
Table 2: Results for top 5 DUC 2002 single docu-
ment summarization systems, and baseline.
4.2 Single Document Summarization for
Portuguese
The single-document summarization tool was also
evaluated on the TeMa?rio collection of Portuguese
newspaper articles. We used the same graph set-
tings as in the English experiments: graph-based
ranking algorithms consisting of either HITS or
PageRank, relying on graphs that are undirected,
directed forward, or directed backward. As men-
tioned in Section 3, the length of each automatically
generated summary was constrained to match the
length of the corresponding manual summary, for a
fair comparison. Table 3 shows the results obtained
on this data set, evaluated using the ROUGE evalu-
ation toolkit. A baseline was also computed, using
the first sentences in each document, and evaluated
at 0.4963.
Graph
Algorithm Undir. Forward Backward
HITSWA 48.14 48.34 50.02
HITSWH 48.14 50.02 48.34
PageRankW 49.39 45.74 51.21
Table 3: Results for Portuguese single-document
summarization.
4.3 Multiple Document Summarization
We evaluate multi-document summaries gener-
ated using combinations of the graph-based rank-
ing algorithms that were found to work best in
the single document summarization experiments ?
PageRankW and HITSWA , on undirected or di-
rected backward graphs. Although the single docu-
ment summaries used in the ?meta? summarization
22
process may conceivably be of any size, in this eval-
uation their length is limited to 100 words.
As mentioned earlier, different graph algorithms
can be used for producing the single document sum-
mary and the ?meta? summary; Table 4 lists the
results for multi-document summarization experi-
ments using various combinations of graph algo-
rithms. For comparison, Table 5 lists the results ob-
tained by the top 5 (out of 9) performing systems
in the multi-document summarization task at DUC
2002, and a baseline generated by taking the first
sentence in each article.
Since no multi-document clusters and associ-
ated summaries were available for the other lan-
guage considered in our experiments, the multi-
document summarization experiments were con-
ducted only on the English data set. However, since
the multi-doc summarization technique consists of
a layered application of single-document summa-
rization, we believe that the performance achieved
in single-document summarization for Portuguese
would eventually result into similar performance
figures when applied to the summarization of clus-
ters of documents.
Top 5 systems (DUC, 2002)
S26 S19 S29 S25 S20 Baseline
35.78 34.47 32.64 30.56 30.47 29.32
Table 5: Results for top 5 DUC 2002 multi-
document summarization systems, and baseline.
4.4 Discussion
The graph-based extractive summarization algo-
rithm succeeds in identifying the most important
sentences in a text (or collection of texts) based on
information exclusively drawn from the text itself.
Unlike other supervised systems, which attempt to
learn what makes a good summary by training on
collections of summaries built for other articles, the
graph-based method is fully unsupervised, and re-
lies only on the given texts to derive an extractive
summary.
For single document summarization, the
HITSWA and PageRankW algorithms, run on
a graph structure encoding a backward direction
across sentence relations, provide the best per-
formance. These results are consistent across
languages ? with similar performance figures
observed on both the English DUC data set and
on the Portuguese TeMa?rio data set. The setting
that is always exceeding the baseline by a large
margin is PageRankW on a directed backward
graph, with clear improvements over the simple
(but powerful) first-sentence selection baseline.
Moreover, comparative evaluations performed
with respect to other systems participating in the
DUC 2002 evaluations revealed the fact that the
performance of the graph-based extractive summa-
rization method is competitive with state-of-the-art
summarization systems.
Interestingly, the ?directed forward? setting is
consistently performing worse than the baseline,
which can be explained by the fact that both data
sets consist of newspaper articles, which tend to
concentrate the most important facts toward the be-
ginning of the document, and therefore disfavor a
forward direction set across sentence relations.
For multiple document summarization, the best
?meta? summarizer is the PageRankW algorithm
applied on undirected graphs, in combination with
a single summarization system using the HITSWA
ranking algorithm, for a performance similar to the
one of the best system in the DUC 2002 multi-
document summarization task.
The results obtained during all these experiments
prove that graph-based ranking algorithms, previ-
ously found successful in Web link analysis and so-
cial networks, can be turned into a state-of-the-art
tool for extractive summarization when applied to
graphs extracted from texts. Moreover, the method
was also shown to be language independent, lead-
ing to similar results when applied to the summa-
rization of documents in different languages.
The better results obtained by algorithms like
HITSWA and PageRank on graphs containing only
backward edges are likely to come from the fact that
recommendations flowing toward the beginning of
the text take advantage of the bias giving higher
summarizing value of sentences occurring at the be-
ginning of the document.
Another important aspect of the method is that
it gives a ranking over all sentences in a text (or
a collection of texts) ? which means that it can be
easily adapted to extracting very short summaries,
or longer more explicative summaries.
4.5 Related Work
Extractive summarization is considered an impor-
tant first step for more sophisticated automatic text
summarization. As a consequence, there is a large
body of work on algorithms for extractive summa-
rization undertaken as part of the DUC evaluation
exercises (http://www-nlpir.nist.gov/projects/duc/).
Previous approaches include supervised learning
(Hirao et al, 2002), (Teufel and Moens, 1997), vec-
torial similarity computed between an initial ab-
stract and sentences in the given document, intra-
document similarities (Salton et al, 1997), or graph
algorithms (Mihalcea and Tarau, 2004), (Erkan and
Radev, 2004), (Wolf and Gibson, 2004). It is also
notable the study reported in (Lin and Hovy, 2003b)
discussing the usefulness and limitations of auto-
matic sentence extraction for text summarization,
23
Single document ?Meta? summarization algorithm
summarization algo. PageRankW -U PageRankW -DB HITSWA -U HITSWA -DB
PageRankW -U 35.52 34.99 34.56 34.65
PageRankW -DB 35.02 34.48 35.19 34.39
HITSWA -U 33.68 32.59 32.12 34.23
HITSWA -DB 35.72 35.20 34.62 34.73
Table 4: Results for multi-document summarization (U = Undirected; DB = Directed Backward)
which emphasizes the need of accurate tools for
sentence extraction as an integral part of automatic
summarization systems.
5 Conclusions
Intuitively, iterative graph-based ranking algo-
rithms work well on the task of extractive summa-
rization because they do not only rely on the local
context of a text unit (vertex), but they rather take
into account information recursively drawn from
the entire text (graph). Through the graphs it builds
on texts, a graph-based ranking algorithm identifies
connections between various entities in a text, and
implements the concept of recommendation. A text
unit recommends other related text units, and the
strength of the recommendation is recursively com-
puted based on the importance of the units making
the recommendation. In the process of identifying
important sentences in a text, a sentence recom-
mends another sentence that addresses similar con-
cepts as being useful for the overall understanding
of the text. Sentences that are highly recommended
by other sentences are likely to be more informa-
tive for the given text, and will be therefore given a
higher score.
In this paper, we showed that a previously pro-
posed method for graph-based extractive summa-
rization can be successfully applied to the sum-
marization of documents in different languages,
without any requirements for additional knowl-
edge or corpora. Moreover, we showed how a
meta-summarizer relying on a layered application
of techniques for single-document summarization
can be turned into an effective method for multi-
document summarization. Experiments performed
on standard data sets have shown that the results ob-
tained with this method are comparable with those
of state-of-the-art systems for automatic summa-
rization, while at the same time providing the bene-
fits of a robust language independent algorithm.
Acknowledgments
We are grateful to Lucia Helena Machado Rino for
making available the TeMa?rio summarization test
collection and for her help with this data set.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
DUC. 2002. Document understanding conference 2002.
http://www-nlpir.nist.gov/projects/duc/.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige in
multi-document text summarization. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain, July.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002.
Ntt?s text summarization system for duc-2002. In
Proceedings of the Document Understanding Confer-
ence 2002.
J.M. Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604?
632.
C.Y. Lin and E.H. Hovy. 2003a. Automatic evalua-
tion of summaries using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology
Conference (HLT-NAACL 2003), Edmonton, Canada,
May.
C.Y. Lin and E.H. Hovy. 2003b. The potential and lim-
itations of sentence extraction for summarization. In
Proceedings of the HLT/NAACL Workshop on Auto-
matic Summarization, Edmonton, Canada, May.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), Barcelona, Spain.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageR-
ank on semantic networks, with application to word
sense disambiguation. In Proceedings of the 20st In-
ternational Conference on Computational Linguistics
(COLING 2004), Geneva, Switzerland.
T.A.S. Pardo and L.H.M. Rino. 2003. TeMario: a cor-
pus for automatic text summarization. Technical re-
port, NILC-TR-03-09.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
S. Teufel and M. Moens. 1997. Sentence extraction
as a classification task. In ACL/EACL workshop on
?Intelligent and scalable Text summarization?, pages
58?65, Madrid, Spain.
F. Wolf and E. Gibson. 2004. Paragraph-, word-,
and coherence-based approaches to sentence ranking:
A comparison of algorithm and human performance.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics, Barcelona, Spain,
July.
24
How to Add a New Language on the NLP Map:
Building Resources and Tools for Languages with Scarce Resources
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Vivi Nastase
EML Research gGmbH
Vivi.Nastase@eml-r.villa-bosch.de
Abstract
Those of us whose mother tongue is not English or
are curious about applications involving other lan-
guages, often find ourselves in the situation where
the tools we require are not available. According
to recent studies there are about 7200 different lan-
guages spoken worldwide ? without including vari-
ations or dialects ? out of which very few have auto-
matic language processing tools and machine read-
able resources.
In this tutorial we will show how we can take
advantage of lessons learned from frequently stud-
ied and used languages in NLP, and of the wealth
of information and collaborative efforts mediated by
the World Wide Web. We structure the presentation
around two major themes: mono-lingual and cross-
lingual approaches. Within the mono-lingual area,
we show how to quickly assemble a corpus for sta-
tistical processing, how to obtain a semantic network
using on-line resources ? in particular Wikipedia ?
and how to obtain automatically annotated corpora
for a variety of applications. The cross-lingual half
of the tutorial shows how to build upon NLP meth-
ods and resources for other languages, and adapt
them for a new language. We will review automatic
construction of parallel corpora, projecting annota-
tions from one side of the parallel corpus to the
other, building language models, and finally we will
look at how all these can come together in higher-
end applications such as machine translation and
cross-language information retrieval.
Biographies
Rada Mihalcea is an Assistant Professor of Com-
puter Science at the University of North Texas. Her
research interests are in lexical semantics, multi-
lingual natural language processing, minimally su-
pervised natural language learning, and graph-based
algorithms for natural language processing. She
serves on the editorial board of the Journal of Com-
putational Linguistics, the Journal of Language Re-
sources and Evaluations, the Journal of Natural Lan-
guage Engineering, the Journal of Research in Lan-
guage in Computation, and the recently established
Journal of Interesting Negative Results in Natural
Language Processing and Machine Learning.
Vivi Nastase is a post-doctoral fellow at EML Re-
search gGmbH, Heidelberg, Germany. Her research
interests are in lexical semantics, semantic relations,
knowledge extraction, multi-document summariza-
tion, graph-based algorithms for natural language
processing, multilingual natural language process-
ing. She is a co-founder of the Journal of Interest-
ing Negative Results in Natural Language Process-
ing and Machine Learning.
938
Book Reviews
The Text Mining Handbook:
Advanced Approaches to Analyzing Unstructured Data
Ronen Feldman and James Sanger
(Bar-Ilan University and ABS Ventures)
Cambridge, England: Cambridge University Press, 2007, xii+410 pp; hardbound,
ISBN 0-521-83657-3, $70.00
Reviewed by
Rada Mihalcea
University of North Texas
Text mining is the process of discovering information in large text collections, and
automatically identifying interesting patterns and relationships in textual data. It is
a relatively new research area, which has recently raised much interest among the
research and industry communities, mainly due to the continuously increasing amount
of information available on the Web and elsewhere. Text mining is a highly interdisci-
plinary research area, bringing together research insights from the fields of data mining,
natural language processing, machine learning, and information retrieval. In particular,
text mining is closely related to the older area of data mining, which targets the extrac-
tion of interesting information from data records, although text mining is allegedlymore
difficult, as the source data consists of unstructured collections of documents rather than
structured databases.
The book by Feldman and Sanger is a thorough introduction to text mining, cov-
ering the general architecture of text mining systems, along with the main techniques
used by such systems. It addresses both the theory and practice of text mining, and it
illustrates the different techniques with real-world scenarios and practical applications.
It is particularly relevant for students and professional practitioners, being structured
as a self-contained handbook that does not require previous experience in any of the
research fields involved.
The book is structured into twelve chapters, which gradually introduce the area of
text mining and related topics, starting with an introduction to the task of text mining,
and ending with examples of practical applications from three different domains.
The first chapter can be regarded as an overview of the book. It starts by defining the
problem of text mining and the key elements in text mining: the document collections,
the document features (words, terms, and concepts), and the role of background knowl-
edge in text mining. It then briefly touches upon the possible applications of text mining,
such as pattern discovery and trend analysis, and shortly discusses the interface layer of
text mining systems. The second part of the chapter lays down the general architecture
of a text mining system, which also serves as a rough guide for the rest of the book, as
it describes the main components of a text mining system that are described in detail in
subsequent chapters.
Chapter 2 is one of the longest chapters in the book, and also one of the most
dense in terms of newly introduced concepts. Despite being a more difficult read
compared to the other chapters in the book, I found it to be the most informative with
respect to operations specific to text mining. The chapter starts by defining the core
text mining operations, including distributions, sets, and associations, and introduces
Computational Linguistics Volume 34, Number 1
the main techniques for isolating interesting patterns and analyzing trends over time.
The second part of the chapter overviews the role of background knowledge in
text mining. The authors describe several ontologies and lexicons, and show with
evidence from a real-world example (the FACT system they developed in the late
1990s) how background knowledge can be effectively integrated into text mining
systems. A shortcoming of this section is the interchangeable use of ?domain ontol-
ogy? and ?background knowledge,? which can be confusing for computational lin-
guists, who typically make a distinction between these terms. Finally, the third part
of the chapter briefly describes query languages, which are later addressed in detail in
Chapter 9.
Chapter 3 is meant as a short introduction to text preprocessing techniques, includ-
ing tokenization, tagging, and parsing. This chapter, as well as several of the following
chapters addressing text classification and information extraction, were most likely
included in the book because of the authors? intention to make the book self-contained
and appealing even for those with no background in computational linguistics.
Chapters 4 and 5 describe techniques for text classification and clustering, which
are relevant to the selection of documents addressed by a text mining system. Chapter
4 describes the representation of documents for the purpose of text categorization,
and introduces several machine-learning algorithms, including decision trees, naive
Bayes, and SVMs, as well as committees of classifiers through bagging and boosting.
Chapter 5 introduces several clustering algorithms, including agglomerative clustering,
expectation maximization, and K-means, as well as techniques specific to clustering
textual data such as latent semantic analysis.
The next three chapters, 6, 7, and 8, address the task of information extraction, which
is a key element in any text mining system. Chapter 6 begins by defining the problem of
information extraction, the architecture of a typical information extraction system, and
the main knowledge-based and structural approaches to information extraction. Chap-
ter 7 is dedicated to probabilistic models, including maximum entropy, hidden Markov
models, and conditional random fields. Chapter 8 then describes text preprocessing
using these probabilistic models, including part-of-speech tagging, shallow parsing,
and named-entity tagging. The chapter also addresses bootstrapping techniques for
information extraction. One drawback of this section of the book is the fact that the
division of the material among these three chapters is not always very clear, and some
parts could have been organized differently. For instance, the probabilistic models and
their applications are split among Chapters 7 and 8, although they could have been
combined under one chapter. Chapter 8 includes material on bootstrapping approaches
for information extraction, which seems unrelated to the rest of this chapter and instead
could have been included in Chapter 6.
Chapter 9 addresses techniques for interfacing with text mining systems, includ-
ing browsing and displaying of distributions, associations, and hierarchies, as well as
query languages and query refinement. Visualization techniques are then addressed in
Chapter 10, which describes visual interfaces to text mining systems, such as association
graphs, histograms, and self-organizing maps. The chapter is rich in illustrations, which
contribute to a better understanding of the differences between the various visualization
methods.
The relationships discovered by a text mining system can be further explored by
using techniques for link analysis, which are described in Chapter 11. The chapter
begins with a brief introduction to graph theory, followed by a description of several
graph centrality methods and algorithms for network partitioning. The chapter also
provides pointers to software packages for link analysis.
126
Book Reviews
Finally, Chapter 12 illustrates the application of the text mining concepts introduced
in the book to three practical text mining problems: industry literature mining, patent
analysis, and protein interactions. By showing examples of concrete applications, the
authors demonstrate the applicability of the text mining theory introduced in the book
to practical real-world scenarios.
Overall, The Text Mining Handbook is a good introduction to text mining, written by
leading experts in the field. The book is well written and addresses both the theory and
practice of text mining, whichmakes it appealing for researchers and practitioners alike.
By being self-contained, with several chapters addressing in detail the main topics
relevant to text mining, the book is highly recommended to those who would like to
start delving into the area of text mining without having any previous background
in computational linguistics. Although experts in computational linguistics will most
likely find that they can safely skip over several of the text processing chapters (e.g., the
introductory chapters on text preprocessing, or the chapters on text classification and
information extraction), they will certainly find a lot of value in the chapters addressing
the specific task of text mining (mainly Chapters 2, 10, 11, and 12).
Rada Mihalcea is an Assistant Professor of Computer Science at the University of North Texas. Her
research interests are in lexical semantics, multilingual natural language processing, minimally
supervised natural language learning, and graph-based algorithms for natural language process-
ing. She served as the president of the ACL Special Interest Group on the Lexicon (SIGLEX)
from 2004 to 2007, and she is currently a board member of the ACL Special Interest Group on
Natural Language Learning (SIGNLL). Mihalcea?s address is: Department of Computer Science,
University of North Texas, P.O. Box 311366, Denton, TX, 76203; e-mail: rada@cs.unt.edu.
127

Open Text Semantic Parsing Using FrameNet and WordNet
Lei Shi and Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
leishi@unt.edu, rada@cs.unt.edu
Abstract
This paper describes a rule-based semantic parser
that relies on a frame dataset (FrameNet), and a
semantic network (WordNet), to identify seman-
tic relations between words in open text, as well
as shallow semantic features associated with con-
cepts in the text. Parsing semantic structures al-
lows semantic units and constituents to be ac-
cessed and processed in a more meaningful way
than syntactic parsing, moving the automation of
understanding natural language text to a higher
level.
1 Introduction
The goal of the semantic parser is to analyze the semantic
structure of a natural language sentence. Similar in spirit
with the syntactic parser ? whose goal is to parse a valid nat-
ural language sentence into a parse tree indicating how the
sentence can be syntactically decomposed into smaller syn-
tactic constituents ? the purpose of the semantic parser is to
analyze the structure of sentence meaning. Sentence mean-
ing is composed by entities and interactions between enti-
ties, where entities are assigned semantic roles, and can be
further modified by other modifiers. The meaning of a sen-
tence is decomposed into smaller semantic units connected
by various semantic relations by the principle of composi-
tionality, and the parser represents the semantic structure ?
including semantic units as well as semantic relations, con-
necting them into a formal format.
One major problem faced by many natural language un-
derstanding applications that rely on syntactic analysis of
text, is the fact that similar syntactic patterns may introduce
different semantic interpretations. Likewise, similar mean-
ings can be syntactically realized in many different ways.
The semantic parser attempts to solve this problem, and
produces a syntax-independent representation of sentence
meaning, so that semantic constituents can be accessed and
processed in a more meaningful and flexible way, avoiding
the sometimes rigid interpretations produced by a syntactic
analyzer. For instance, the sentences I boil water and water
boils contain a similar relation between water and boil, even
though they have different syntactic structures.
In this paper, we describe the main components of the se-
mantic parser, and illustrate the basic procedures involved
in parsing semantically open text. Our semantic parser de-
parts from current approaches in statistics-based annotations
of semantic structures. Instead, we are using publicly avail-
able lexical resources (FrameNet and WordNet) as a starting
point to derive rules for a rule-based semantic parser.
2 Semantic Structure
Semantics is the denotation of a string of symbols, either
a sentence or a word. Similar to a syntactic parser, which
shows how a larger string is formed by smaller strings from
a formal point of view, the semantic parser shows how the
denotation of a larger string ? sentence, is formed by deno-
tations of smaller strings ? words. Syntactic relations can be
described using a set of rules about how a sentence string
is formally generated using word strings. Instead, seman-
tic relations between semantic constituents depend on our
understanding of the world, which is across languages and
syntax.
We can model the sentence semantics as describing enti-
ties and interactions between entities. Entities can represent
physical objects, as well as time, places, or ideas, and are
usually formally realized as nouns or noun phrases. Inter-
actions, usually realized as verbs, describe relationships or
interactions between participating entities. Note that a par-
ticipant can also be an interaction, which can be regarded
as an entity nominalized from an interaction. We assign se-
mantic roles to participants and their semantic relations are
identified by the case frame introduced by their interaction.
In a sentence, participants and interactions can be further
modified by various modifiers, including descriptive mod-
ifiers that describe attributes such as drive slowly, restric-
tive modifiers that enforce a general denotation to become
more specific such as musical instrument, referential modi-
fiers that indicate particular instances such as the pizza I or-
dered. Other semantic relations can also be identified, such
as coreference, complement, and others. Based on the prin-
ciple of compositionality, the sentence semantic structure is
recursive, similar to a tree.
Note that the semantic parser analyzes shallow-level se-
mantics, which is derived directly from linguistic knowl-
edge, such as rules about semantic role assignment, lexi-
cal semantic knowledge, and syntactic-semantic mappings,
without taking into account any context or common sense
knowledge. Hence, the parser can be used as an interme-
diate semantic processing level before higher levels of text
understanding.
3 Knowledge Bases for Semantic Parsing
The parser relies on two main types of knowledge ? about
words, and about relations between words. The first type of
knowledge is drawn from WordNet ? a large lexical database
with rich information about words and concepts. We refer
to this as word-level knowledge. The latter is derived from
FrameNet ? a resource that contains information about dif-
ferent situations, called frames, in which semantic relations
are syntactically realized in natural language sentences. We
call this sentence-level knowledge. In addition to these two
lexical knowledge bases, the parser also utilizes a set of man-
ually defined rules, which encode mappings from syntactic
structures to semantic relations, and which are used to han-
dle those structures not explicitly addressed by FrameNet or
WordNet. In this section, we describe the type of informa-
tion extracted from these knowledge bases, and show how
this information is encoded in a format accessible to the se-
mantic parser.
3.1 Sentence Level Knowledge
FrameNet (Johnson et al, 2002) provides the knowl-
edge needed to identify case frames and semantic roles.
FrameNet is based on the theory of frame semantics, and de-
fines a sentence level ontology. In frame semantics, a frame
corresponds to an interaction and its participants, both of
which denote a scenario, in which participants play some
kind of roles. A frame has a name, and we use this name
to identify the semantic relation that groups together the se-
mantic roles. Nouns, verbs and adjectives can be used to
identify frames.
Each annotated sentence in FrameNet exemplifies a pos-
sible syntactic realization for the semantic roles associated
with a frame for a given target word. By extracting the syn-
tactic features and corresponding semantic roles from all an-
notated sentences in the FrameNet corpus, we are able to au-
tomatically build a large set of rules that encode the possible
syntactic realizations of semantic frames.
3.1.1 Rules Learned from FrameNet
FrameNet data ?is meant to be lexicographically relevant,
not statistically representative? (Johnson et al, 2002), and
therefore we are using FrameNet as a starting point to derive
rules for a rule-based semantic parser.
To build the rules, we are extracting several syntactic fea-
tures. Some are explicitly encoded in FrameNet, such as the
grammatical function (GF) and phrase type (PT) features.
In addition, other syntactic features are extracted from the
sentence context. One such feature is the relative position
(RP) to the target word. Another feature is the voice of the
sentence. If the phrase type is prepositional phrase (PP), we
also record the actual preposition that precedes the phrase.
After we extract all these syntactic features, the semantic
role is appended to the rule, which creates a mapping from
syntactic features to semantic roles.
Feature sets are arranged in a list, the order of which is
identical to that in the sentence. Altogether, the rule for a
possible realization of a frame exemplified by a tagged sen-
tence is an ordered sequence of syntactic features with their
semantic roles. For example, the corresponding formalized
rule for the sentence I had chased Selden over the moor is:
[active, [ext,np,before,theme], [obj,np,after,goal],
[comp,pp,after,over,path]]
In FrameNet, there are multiple annotated sentences for
each frame to demonstrate multiple possible syntactic real-
izations. All possible realizations of a frame are collected
and stored in a list for that frame, which also includes the tar-
get word, its syntactic category, and the name of the frame.
All the frames defined in FrameNet are transformed into this
format, so that they can be easily handled by the rule-based
semantic parser.
3.2 Word Level Knowledge
WordNet (Miller, 1995) is the resource used to identify shal-
low semantic features that can be attached to lexical units.
For instance, attribute relations, adjective/adverb classifica-
tions, and others, are semantic features extracted from Word-
Net and stored together with the words, so that they can be
directly used in the parsing process.
All words are uniformly defined, regardless of their class.
Features are assigned to each word, including syntactic and
shallow semantic features, indicating the functions played
by the word. Syntactic features are used by the feature-
augmented syntactic analyzer to identify grammatical errors
and produce syntactic information for semantic role assign-
ment. Semantic features encode lexical semantic informa-
tion extracted from WordNet that is used to determine se-
mantic relations between words in various situations.
Features can be arbitrarily defined, as long as there are
rules to handle them. The features we define encode infor-
mation about the syntactic category of a word, number and
countability for nouns, transitivity and form for verbs, type,
degree, and attribute for adjectives and adverbs, and others.
For example, for the adjective slow, the entry in the lexi-
con is defined as:
lex(slow,W):- W= [parse:slow, cat:adj, attr:speed,
degree:base, type:descriptive].
Here, the category (cat) is defined as adjective, the type
is descriptive, degree is base form. We also record the attr
feature, which is derived from the attribute relation in Word-
Net, and links a descriptive adjective to the attribute (noun)
it modifies, such as slow   speed.
4 The Semantic Parser
The parsing algorithm is implemented as a rule-based sys-
tem. The general procedure of semantic parsing consists of
three main steps: (1) syntactic parsing into an intermedi-
ate format, using a feature-augmented syntactic parser, and
assignment of shallow semantic features; (2) semantic role
assignment; (3) application of default rules.
4.1 Feature Augmented Syntactic/Semantic Analyzer
The semantic parser is based on dependencies between
words that are identified using a structure analyzer. The an-
alyzer generates an intermediate format, where target words
and syntactic arguments are explicitly identified, so that they
can be matched against the rules derived from FrameNet.
The intermediate format also encodes some shallow seman-
tic features, including word level semantics (e.g. attribute,
gender), and semantic relations that have direct syntactic
correspondence (e.g. modifier types). The function of the
sentence is also identified, as assertion, query, yn-query,
command.
The analyzer is based on a feature augmented grammar,
and has the capability of detecting if a sentence is gram-
matically correct (unlike statistical parsers, which attempt to
parse any sentence, regardless of their well-formness). Con-
stituents are assigned with features, and the grammar con-
sists of a set of rules defining how constituents can connect
to each other, based on the values of their features.
Since features can contain both syntactic and semantic in-
formation, the analyzer can reject some grammatically in-
correct sentences such as: I have much apples, You has my
car, or even some semantically incorrect sentences: The
technology is very military1.
4.2 Semantic Role Assignment
In the process of semantic role assignment, we first start by
identifying all possible frames, according to the target word.
Next, a matching algorithm is used to find the most likely
match among all rules derived for these frames, to identify
the correct frame (if several are possible), and assign seman-
tic roles.
In a sentence describing an interaction, we usually select
the verb or predicative adjective as the target word, which
triggers the sentence level frame. A noun can also play the
role of target word, but only within the scope of the noun
phrase it belongs to, and it can be used to assign semantic
roles only to its modifiers.
The matching algorithm relies on a scoring function to
evaluate the similarity between two sequences of syntactic
features. The matching starts from left to right. Whenever
an exact match is found, the score will be increased by 1.
It should be noted that the search sequence is uni-directional
which means that once you find a match, you can go ahead to
check features to the right, but you cannot go back to check
1Since military is not a descriptive adjective, it cannot be con-
nected to the degree modifier very.
rules you have already checked. This guarantees that syntac-
tic features are matched in the right order, and the order of
sequence in the rule is maintained. Since the frame of a tar-
get word may have multiple possible syntactic realizations,
which are exemplified by different sentences in the corpus,
we try to match the syntactic features in the intermediate for-
mat with all the rules available for the target word, and com-
pare their matching scores. The rule with the highest score
is selected, and used for semantic role assignment. Through
this scoring scheme, the matching algorithm tries to maxi-
mize the number of syntactic realizations for semantic roles
defined in FrameNet rules.
Notice that the semantic role assignment is performed re-
cursively, until all roles within frames triggered by all target
words are assigned.
4.2.1 Walk-Through Example
Assume the following two rules, derived from FrameNet for
the target word come:
1:[[ext,np,before,active,theme],
[obj,np,after,active,goal],
[comp,pp,after,active,by,mode_of_transportation]]
2:[[ext,np,before,active,theme],
[obj,np,after,active,goal],
[comp,pp,after,active,from,source]]
And the sentences:
A: I come here by train.
B: I come here from home.
The syntactic features identified by the syntactic analyzer for
these two sentences are:
A?:[[ext,np,before,active], [obj,np,after,active],
$[$comp,pp,after,active,by]]
B?:[[ext,np,before,active], [obj,np,after,active],
$[$comp,pp,after,active,from]]
Using the matching/scoring algorithm, the score for match-
ing A? to rule 1 is determined as 3, and to rule 2 as 2.
Hence, the matching algorithm selects rule 1, and the se-
mantic role for train is mode of transportation. Similarly,
when we match B? to rule 1, we obtain a score of 2, and a
larger score of 3 for matching with rule 2. Therefore, for the
second case, the role assigned to home is source.
4.3 Applying Default Rules
In a sentence, semantic roles are played by the subject, ob-
jects, and the prepositional phrases attached to the inter-
action described by the sentence. However, FrameNet de-
fines roles only for some of these elements, and therefore
the meaning of some sentence constituents cannot be deter-
mined using the rules extracted from FrameNet. In order to
handle these constituents, and allow for a complete seman-
tic interpretation of the sentence, we have defined a set of
default rules that are applied as a last step in the process of
semantic parsing. For example, FrameNet defines a role for
the prepositional phrase on him in ?I depend on him?, but it
does not define a role for the phrase on the street in ?I walk
on the street?. To handle the interpretation of this phrase,
we apply the default rule that ?on something? modifies the
location attribute of an interaction.
We have defined about 100 such default rules, which are
assigned in the last step of the semantic parsing process, if
no other rule could be applied in previous steps. After this
step, the semantic structure of the sentence is produced.
5 Parser Output and Evaluation
The semantic parser is demonstrated in this conference,
which is perhaps the best evaluation we can offer. We
illustrate here the output of the semantic parser on a natural
language sentence, and show the corresponding semantic
structure and tree. For example, for the sentence I like to
eat Mexican food because it is spicy, the semantic parser
produces the following encoding of sentence type, frames,
semantic constituents and roles, and various attributes and
modifiers:
T = assertion
P =
[[experiencer, [[entity, [i], reference(first)],
[modification(attribute), quantity(single)]]],
[interaction(experiencer\_subj),[love]],
[modification(attribute), time(present)],
[content, [
[interaction(ingestion), [eat]],
[ingestibles, [entity, [food]]
[[modification(restriction), [mexican]],
]]]],
[reason, [[agent, [[entity, [it], reference(third)],
[modification(attribute), quantity(single)]]],
[description,
[modification(attribute), time(present)]],
[modification(attribute), taste\_property(spicy)]]]
]
The corresponding semantic tree is shown in Figure 1.
ingestion ), [eat]interaction(
I love to eat Mexican food, because it is spicy.
{[I], reference(first)}
S?[assertion]
interaction( experiencer_subj ), [love]
{[it], reference(third)}
time(present)
quantity(single) {food}
{mexican}
taste_property(spicy)
ingestibles
experiencer
content reason
am am 
sm 
am
Figure 1: Semantic parse tree (am = attributive modifier, rm =
referential modifier, sm = restrictive modifier)
We have conducted evaluations of the semantic role as-
signment algorithm on 350 sentences randomly selected
from FrameNet. The test sentences were removed from
the FrameNet corpus, and the rules-learning procedure de-
scribed earlier in the paper was invoked on this reduced cor-
pus. All test sentences were then semantically parsed, and
full semantic annotations were produced for each sentence.
Notice that the evaluation is conducted only for semantic
role assignment ? since this is the only information avail-
able in FrameNet. The other semantic annotations produced
by the parser (e.g. attribute, gender, countability) are not
evaluated at this point, since there are no hand-validated an-
notations of this kind available in current resources.
Both frames and frame elements are automatically identi-
fied by the parser. Out of all the elements correctly iden-
tified, we found that 74.5% were assigned with the cor-
rect role (this is therefore the accuracy of role assignment),
which compares favorably with previous results reported in
the literature for this task. Notice also that since this is a
rule-based approach, the parser does not need large amounts
of annotated data, but it works well the same for words for
which only one or two sentences are annotated.
6 Related Work
All previous work in semantic parsing has exclusively fo-
cused on labeling semantic roles, rather than analyzing the
full structure of sentence semantics, and is usually based on
statistical models - e.g. (Gildea and Jurafsky, 2000), (Fleis-
chman et al, 2003). To our knowledge, there was no pre-
vious attempt on performing semantic annotations using al-
ternative rule-based algorithms. However, a rule-based ap-
proach is closer to the way humans interpret the semantic
structure of a sentence. Moreover, as mentioned earlier, the
FrameNet data is not meant to be ?statistically representa-
tive?, but rather illustrative for various language constructs,
and therefore a rule-based approach is more suitable for this
lexical resource.
7 Conclusions
We described a rule-based approach to open text seman-
tic parsing. The semantic parser has the capability to an-
alyze the semantic structure of a sentence, and show how
the meaning of the entire sentence is composed of smaller
semantic units, linked by various semantic relations. The
parsing process relies on rules derived from a frame dataset
(FrameNet) and a semantic network (WordNet). We believe
that the semantic parser will prove useful for a range of
language processing applications that require knowledge of
text meaning, including word sense disambiguation, infor-
mation extraction, question answering, machine translation,
and others.
References
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum en-
tropy models for FrameNet classification. In Proceedings of
2003 Conference on Empirical Methods in Natural Language
Processing EMNLP-2003, Sapporo, Japan.
D. Gildea and D. Jurafsky. 2000. Automatic labeling of semantic
roles. In Proceedings of the 38th Annual Conference of the As-
sociation for Computational Linguistics (ACL-00), pages 512?
520, Hong Kong, October.
C. Johnson, C. Fillmore, M. Petruck, C. Baker, M. Ellsworth,
J. Ruppenhofer, and E. Wood. 2002. FrameNet: Theory and
Practice. http://www.icsi.berkeley.edu/ framenet.
G. Miller. 1995. Wordnet: A lexical database. Communication of
the ACM, 38(11):39?41.
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 303?304,
New York City, June 2006. c?2006 Association for Computational Linguistics
3. Graph-Based Algorithms For Natural Language Processing And Information Retrieval
Rada Mihalcea, University of North Texas, and Dragomir Radev, University of Michigan
Graph theory is a well studied discipline, and so are the fields of natural language processing and in-
formation retrieval. However, most of the times, they are perceived as different disciplines, with different
algorithms, different applications, and different potential end-users.
The goal of this tutorial is to provide an overview of methods and applications in natural language
processing and information retrieval that rely on graph-based algorithms. This will include techniques for
graph traversal, minimum path length, min-cut algorithms, minimum spanning trees, random walks, etc. and
their application to information retrieval and Web search, text understanding (word sense disambiguation
and semantic classes), parsing, text summarization, keyword extraction, text clustering, and others.
3.1 Tutorial Outline
1. Graph-based Algorithms Basics
* Vectors, matrices, graphs
* Graph representations and notations
? Traversal, min-cut/max-flow, matching
* Algorithms for graph traversal
* Minimum path length
* Minimum spanning trees
* Min-cut/max-flow algorithms
* Graph-matching algorithms
? Ranking, clustering, learning
* Eigenvector analysis
* Node-ranking algorithms
* Graph-based centrality
* Graph-based clustering
* Machine learning on graphs
2. Information Retrieval applications
* Web-page ranking
* Text classification and clustering
3. Natural language processing applications
? Semantics
* Word sense disambiguation
* Semantic classes
* Textual entailment
* Sentiment classification
? Syntax, Summarization
* Dependency parsing
* Prepositional attachment
* Keyword extraction
* Text summarization
3.2 Target Audience
This tutorial is intended for researchers and practitioners who seek a general understanding of the appli-
cation of graph-theoretical representations and algorithms to natural language processing and information
303
retrieval. It is introductory in nature, no special knowledge or background is required.
Rada Mihalcea is an Assistant Professor of Computer Science at the University of North Texas. Her research
interests are in lexical semantics, graph-based algorithms for natural language processing and information
retrieval, minimally supervised natural language learning, and multilingual natural language processing.
She has published more than 80 articles in books, journals, and proceedings, in these and related areas. She
is the president of the ACL Special Group on the Lexicon (SIGLEX), and a board member for the ACL
Special Group on Natural Language Learning (SIGNLL). She serves on the editorial board of the journal
of Computational Linguistics, the journal of Language Resources and Evaluations, and the recently estab-
lished journal of Interesting Negative Results in Natural Language Processing and Machine Learning. Her
research is supported by NSF, Google, and the state of Texas.
Dragomir Radev is an Associate Professor of Information, of Computer Science and Engineering, and of
Linguistics at the University of Michigan. He has a PhD in Computer Science from Columbia University.
He has held numerous posts within NAACL and ACL. He is on the editorial boards of Information Retrieval
and the Journal of Artificial Intelligence Research and was recently nominated to the board of the Journal
of Natural Language Engineering. He has co-chaired 5 ACL/NAACL workshops and given 6 tutorials at
venues like SIGIR, AAAI, and RANLP. Dragomir?s current interests are in text summarization, information
extraction, information retrieval, graph models, semi-supervised learning, and machine translation. He has
more than 50 peer-reviewed papers as well as more than 50 talks at various universities and other venues.
Dragomir?s work has been funded by NSF, NIH, and ONR.
304
Proceedings of NAACL HLT 2007, pages 196?203,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Using Wikipedia for Automatic Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science
University of North Texas
rada@cs.unt.edu
Abstract
This paper describes a method for generat-
ing sense-tagged data using Wikipedia as
a source of sense annotations. Through
word sense disambiguation experiments,
we show that the Wikipedia-based sense
annotations are reliable and can be used to
construct accurate sense classifiers.
1 Introduction
Ambiguity is inherent to human language. In partic-
ular, word sense ambiguity is prevalent in all natural
languages, with a large number of the words in any
given language carrying more than one meaning.
For instance, the English noun plant can mean green
plant or factory; similarly the French word feuille
can mean leaf or paper. The correct sense of an am-
biguous word can be selected based on the context
where it occurs, and correspondingly the problem of
word sense disambiguation is defined as the task of
automatically assigning the most appropriate mean-
ing to a polysemous word within a given context.
Among the various knowledge-based (Lesk,
1986; Galley and McKeown, 2003; Navigli and Ve-
lardi, 2005) and data-driven (Yarowsky, 1995; Ng
and Lee, 1996; Pedersen, 2001) word sense dis-
ambiguation methods that have been proposed to
date, supervised systems have been constantly ob-
served as leading to the highest performance. In
these systems, the sense disambiguation problem
is formulated as a supervised learning task, where
each sense-tagged occurrence of a particular word
is transformed into a feature vector which is then
used in an automatic learning process. Despite their
high performance, these supervised systems have an
important drawback: their applicability is limited to
those few words for which sense tagged data is avail-
able, and their accuracy is strongly connected to the
amount of labeled data available at hand.
To address the sense-tagged data bottleneck prob-
lem, different methods have been proposed in the
past, with various degrees of success. This includes
the automatic generation of sense-tagged data using
monosemous relatives (Leacock et al, 1998; Mi-
halcea and Moldovan, 1999; Agirre and Martinez,
2004), automatically bootstrapped disambiguation
patterns (Yarowsky, 1995; Mihalcea, 2002), paral-
lel texts as a way to point out word senses bear-
ing different translations in a second language (Diab
and Resnik, 2002; Ng et al, 2003; Diab, 2004),
and the use of volunteer contributions over the Web
(Chklovski and Mihalcea, 2002).
In this paper, we investigate a new approach for
building sense tagged corpora using Wikipedia as a
source of sense annotations. Starting with the hy-
perlinks available in Wikipedia, we show how we
can generate sense annotated corpora that can be
used for building accurate and robust sense clas-
sifiers. Through word sense disambiguation ex-
periments performed on the Wikipedia-based sense
tagged corpus generated for a subset of the SENSE-
VAL ambiguous words, we show that the Wikipedia
annotations are reliable, and the quality of a sense
tagging classifier built on this data set exceeds by a
large margin the accuracy of an informed baseline
that selects the most frequent word sense by default.
The paper is organized as follows. We first pro-
196
vide a brief overview of Wikipedia, and describe the
view of Wikipedia as a sense tagged corpus. We then
show how the hyperlinks defined in this resource
can be used to derive sense annotated corpora, and
we show how a word sense disambiguation system
can be built on this dataset. We present the results
obtained in the word sense disambiguation experi-
ments, and conclude with a discussion of the results.
2 Wikipedia
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
webpage, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential mis-
takes are quickly corrected within the collaborative
environment) of this online resource. Wikipedia edi-
tions are available for more than 200 languages, with
a number of entries varying from a few pages to
more than one million articles per language.1
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or an
event, and consists of a hypertext document with hy-
perlinks to other pages within or outside Wikipedia.
The role of the hyperlinks is to guide the reader to
pages that provide additional information about the
entities or events mentioned in an article.
Each article in Wikipedia is uniquely referenced
by an identifier, which consists of one or more words
separated by spaces or underscores, and occasion-
ally a parenthetical explanation. For example, the
article for bar with the meaning of ?counter for
drinks? has the unique identifier bar (counter).2
The hyperlinks within Wikipedia are created us-
ing these unique identifiers, together with an an-
chor text that represents the surface form of the hy-
perlink. For instance, ?Henry Barnard, [[United
States|American]] [[educationalist]], was born in
[[Hartford, Connecticut]]? is an example of a sen-
tence in Wikipedia containing links to the articles
United States, educationalist, and Hartford, Con-
1In the experiments reported in this paper, we use a down-
load from March 2006 of the English Wikipedia, with approxi-
mately 1 million articles, and more than 37 millions hyperlinks.
2The unique identifier is also used to form the article URL,
e.g. http://en.wikipedia.org/wiki/Bar (counter)
necticut. If the surface form and the unique iden-
tifier of an article coincide, then the surface form
can be turned directly into a hyperlink by placing
double brackets around it (e.g. [[educationalist]]).
Alternatively, if the surface form should be hyper-
linked to an article with a different unique identi-
fier, e.g. link the word American to the article on
United States, then a piped link is used instead, as in
[[United States|American]].
One of the implications of the large number of
contributors editing the Wikipedia articles is the
occasional lack of consistency with respect to the
unique identifier used for a certain entity. For in-
stance, the concept of circuit (electric) is also re-
ferred to as electronic circuit, integrated circuit,
electric circuit, and others. This has led to the so-
called redirect pages, which consist of a redirection
hyperlink from an alternative name (e.g. integrated
circuit) to the article actually containing the descrip-
tion of the entity (e.g. circuit (electric)).
Finally, another structure that is particularly rel-
evant to the work described in this paper is the
disambiguation page. Disambiguation pages are
specifically created for ambiguous entities, and con-
sist of links to articles defining the different mean-
ings of the entity. The unique identifier for a dis-
ambiguation page typically consists of the paren-
thetical explanation (disambiguation) attached to
the name of the ambiguous entity, as in e.g. cir-
cuit (disambiguation) which is the unique identifier
for the disambiguation page of the entity circuit.
3 Wikipedia as a Sense Tagged Corpus
A large number of the concepts mentioned in
Wikipedia are explicitly linked to their correspond-
ing article through the use of links or piped links.
Interestingly, these links can be regarded as sense
annotations for the corresponding concepts, which
is a property particularly valuable for entities that
are ambiguous. In fact, it is precisely this observa-
tion that we rely on in order to generate sense tagged
corpora starting with the Wikipedia annotations.
For example, ambiguous words such as e.g. plant,
bar, or chair are linked to different Wikipedia ar-
ticles depending on their meaning in the context
where they occur. Note that the links are manually
created by the Wikipedia users, which means that
they are most of the time accurate and referencing
197
the correct article. The following represent five ex-
ample sentences for the ambiguous word bar, with
their corresponding Wikipedia annotations (links):
In 1834, Sumner was admitted to the [[bar
(law)|bar]] at the age of twenty-three, and entered
private practice in Boston.
It is danced in 3/4 time (like most waltzes), with
the couple turning approx. 180 degrees every [[bar
(music)|bar]].
Vehicles of this type may contain expensive au-
dio players, televisions, video players, and [[bar
(counter)|bar]]s, often with refrigerators.
Jenga is a popular beer in the [[bar
(establishment)|bar]]s of Thailand.
This is a disturbance on the water surface of a river
or estuary, often cause by the presence of a [[bar
(landform)|bar]] or dune on the riverbed.
To derive sense annotations for a given ambigu-
ous word, we use the links extracted for all the hy-
perlinked Wikipedia occurrences of the given word,
and map these annotations to word senses. For in-
stance, for the bar example above, we extract five
possible annotations: bar (counter), bar (establish-
ment), bar (landform), bar (law), and bar (music).
Although Wikipedia provides the so-called dis-
ambiguation pages that list the possible meanings of
a given word, we decided to use instead the anno-
tations collected directly from the Wikipedia links.
This decision is motivated by two main reasons.
First, a large number of the occurrences of ambigu-
ous words are not linked to the articles mentioned
by the disambiguation page, but to related concepts.
This can happen when the annotation is performed
using a concept that is similar, but not identical to the
concept defined. For instance, the annotation for the
word bar in the sentence ?The blues uses a rhyth-
mic scheme of twelve 4/4 [[measure (music)|bars]]?
is measure (music), which, although correct and di-
rectly related to the meaning of bar (music), is not
listed in the disambiguation page for bar.
Second, most likely due to the fact that Wikipedia
is still in its incipient phase, there are several in-
consistencies that make it difficult to use the disam-
biguation pages in an automatic system. For exam-
ple, for the word bar, the Wikipedia page with the
identifier bar is a disambiguation page, whereas for
the word paper, the page with the identifier paper
contains a description of the meaning of paper as
?material made of cellulose,? and a different page
paper (disambiguation) is defined as a disambigua-
tion page. Moreover, in other cases such as e.g. the
entries for the word organization, no disambiguation
page is defined; instead, the articles corresponding
to different meanings of this word are connected by
links labeled as ?alternative meanings.?
Therefore, rather than using the senses listed in
a disambiguation page as the sense inventory for
a given ambiguous word, we chose instead to col-
lect all the annotations available for that word in
the Wikipedia pages, and then map these labels to
a widely used sense inventory, namely WordNet.3
3.1 Building Sense Tagged Corpora
Starting with a given ambiguous word, we derive a
sense-tagged corpus following three main steps:
First, we extract all the paragraphs in Wikipedia
that contain an occurrence of the ambiguous word
as part of a link or a piped link. We select para-
graphs based on the Wikipedia paragraph segmen-
tation, which typically lists one paragraph per line.4
To focus on the problem of word sense disambigua-
tion, rather than named entity recognition, we ex-
plicitly avoid named entities by considering only
those word occurrences that are spelled with a lower
case. Although this simple heuristic will also elim-
inate examples where the word occurs at the begin-
ning of a sentence (and therefore are spelled with an
upper case), we decided nonetheless to not consider
these examples so as to avoid any possible errors.
Next, we collect all the possible labels for the
given ambiguous word by extracting the leftmost
component of the links. For instance, in the
piped link [[musical notation|bar]], the label musi-
cal notation is extracted. In the case of simple links
(e.g. [[bar]]), the word itself can also play the role
of a valid label if the page it links to is not deter-
mined as a disambiguation page.
Finally, the labels are manually mapped to their
corresponding WordNet sense, and a sense tagged
3Alternatively, the Wikipedia annotations could also play
the role of a sense inventory, without the mapping to WordNet.
We chose however to perform this mapping for the purpose of
allowing evaluations using a widely used sense inventory.
4The average length of a paragraph is 80 words.
198
Word sense Labels in Wikipedia Wikipedia definition WordNet definition
bar (establishment) bar (establishment), nightclub a retail establishment which serves a room or establishment where
gay club, pub alcoholic beverages alcoholic drinks are served
over a counter
bar (counter) bar (counter) the counter from which drinks a counter where you can obtain
are dispensed food or drink
bar (unit) bar (unit) a scientific unit of pressure a unit of pressure equal to a million
dynes per square centimeter
bar (music) bar (music), measure music a period of music musical notation for a repeating
musical notation pattern of musical beats
bar (law) bar association, bar law the community of persons engaged the body of individuals qualified to
law society of upper canada in the practice of law practice law in a particular
state bar of california jurisdiction
bar (landform) bar (landform) a type of beach behind which lies a submerged (or partly submerged)
a lagoon ridge in a river or along a shore
bar (metal) bar metal, pole (object) - a rigid piece of metal or wood
bar (sports) gymnastics uneven bars, - a horizontal rod that serves as a
handle bar support for gymnasts as they
perform exercises
bar (solid) candy bar, chocolate bar - a block of solid substance
Table 1: Word senses for the word bar, based on annotation labels used in Wikipedia
corpus is created. This mapping process is very fast,
as a relatively small number of labels is typically
identified for a given word. For instance, for the
dataset used in the experiments reported in Section
5, an average of 20 labels per word was extracted.
To ensure the correctness of this last step, for
the experiments reported in this paper we used two
human annotators who independently mapped the
Wikipedia labels to their corresponding WordNet
sense. In case of disagreement, a consensus was
reached through adjudication by a third annotator.
In a mapping agreement experiment performed on
the dataset from Section 5, an inter-annotator agree-
ment of 91.1% was observed with a kappa statistics
of ?=87.1, indicating a high level of agreement.
3.2 An Example
As an example, consider the ambiguous word bar,
with 1,217 examples extracted from Wikipedia
where bar appeared as the rightmost component of
a piped link or as a word in a simple link. Since
the page with the identifier bar is a disambigua-
tion page, all the examples containing the single
link [[bar]] are removed, as the link does not re-
move the ambiguity. This process leaves us with
1,108 examples, from which 40 different labels are
extracted. These labels are then manually mapped
to nine senses in WordNet. Figure 1 shows the la-
bels extracted from the Wikipedia annotations for
the word bar, the corresponding WordNet definition,
as well as the Wikipedia definition (when the sense
was defined in the Wikipedia disambiguation page).
4 Word Sense Disambiguation
Provided a set of sense-annotated examples for a
given ambiguous word, the task of a word sense dis-
ambiguation system is to automatically learn a dis-
ambiguation model that can predict the correct sense
for a new, previously unseen occurrence of the word.
We use a word sense disambiguation system that
integrates local and topical features within a ma-
chine learning framework, similar to several of the
top-performing supervised word sense disambigua-
tion systems participating in the recent SENSEVAL
evaluations (http://www.senseval.org).
The disambiguation algorithm starts with a pre-
processing step, where the text is tokenized and an-
notated with part-of-speech tags. Collocations are
identified using a sliding window approach, where
a collocation is defined as a sequence of words that
forms a compound concept defined in WordNet.
Next, local and topical features are extracted from
the context of the ambiguous word. Specifically, we
use the current word and its part-of-speech, a local
context of three words to the left and right of the am-
biguous word, the parts-of-speech of the surround-
ing words, the verb and noun before and after the
ambiguous words, and a global context implemented
through sense-specific keywords determined as a list
of at most five words occurring at least three times
199
in the contexts defining a certain word sense.
This feature set is similar to the one used by (Ng
and Lee, 1996), as well as by a number of state-of-
the-art word sense disambiguation systems partici-
pating in the SENSEVAL-2 and SENSEVAL-3 evalu-
ations. The features are integrated in a Naive Bayes
classifier, which was selected mainly for its perfor-
mance in previous work showing that it can lead to
a state-of-the-art disambiguation system given the
features we consider (Lee and Ng, 2002).
5 Experiments and Results
To evaluate the quality of the sense annotations gen-
erated using Wikipedia, we performed a word sense
disambiguation experiment on a subset of the am-
biguous words used during the SENSEVAL-2 and
SENSEVAL-3 evaluations. Since the Wikipedia an-
notations are focused on nouns (associated with the
entities typically defined by Wikipedia), the sense
annotations we generate and the word sense disam-
biguation experiments are also focused on nouns.
Starting with the 49 ambiguous nouns used during
the SENSEVAL-2 (29) and SENSEVAL-3 (20) evalu-
ations, we generated sense tagged corpora follow-
ing the process outlined in Section 3.1. We then re-
moved all those words that have only one Wikipedia
label (e.g. detention, which occurs 58 times, but
appears as a single link [[detention]] in all the oc-
currences), or which have several labels that are all
mapped to the same WordNet sense (e.g. church,
which has 2,198 occurrences with several differ-
ent labels such as Roman church, Christian church,
Catholic church, which are all mapped to the mean-
ing of church, Christian church as defined in Word-
Net). This resulted in a set of 30 words that have
their Wikipedia annotations mapped to at least two
senses according to the WordNet sense inventory.
Table 2 shows the disambiguation results using
the word sense disambiguation system described in
Section 4, using ten-fold cross-validation. For each
word, the table also shows the number of senses, the
total number of examples, and two baselines: a sim-
ple informed baseline that selects the most frequent
sense by default,5 and a more refined baseline that
5Note that this baseline assumes the availability of a sense
tagged corpus in order to determine the most frequent sense of
a word. The baseline is therefore ?informed,? as compared to a
random, ?uninformed? sense selection.
baselines word sense
word #s #ex MFS LeskC disambig.
argument 2 114 70.17% 73.63% 89.47%
arm 3 291 61.85% 69.31% 84.87%
atmosphere 3 773 54.33% 56.62% 71.66%
bank 3 1074 97.20% 97.20% 97.20%
bar 10 1108 47.38% 68.09% 83.12%
chair 3 194 67.57% 65.78% 80.92%
channel 5 366 51.09% 52.50% 71.85%
circuit 4 327 85.32% 85.62% 87.15%
degree 7 849 58.77% 73.05% 85.98%
difference 2 24 75.00% 75.00% 75.00%
disc 3 73 52.05% 52.05% 71.23%
dyke 2 76 77.63% 82.00% 89.47%
fatigue 3 123 66.66% 70.00% 93.22%
grip 3 34 44.11% 77.00% 70.58%
image 2 84 69.04% 74.50% 80.28%
material 3 223 95.51% 95.51% 95.51%
mouth 2 409 94.00% 94.00% 95.35%
nature 2 392 98.72% 98.72% 98.21%
paper 5 895 96.98% 96.98% 96.98%
party 3 764 68.06% 68.28% 75.91%
performance 2 271 95.20% 95.20% 95.20%
plan 3 83 77.10% 81.00% 81.92%
post 5 33 54.54% 62.50% 51.51%
restraint 2 9 77.77% 77.77% 77.77%
sense 2 183 95.10% 95.10% 95.10%
shelter 2 17 94.11% 94.11% 94.11%
sort 2 11 81.81% 90.90% 90.90%
source 3 78 55.12% 81.00% 92.30%
spade 3 46 60.86% 81.50% 80.43%
stress 3 565 53.27% 54.28% 86.37%
AVERAGE 3.31 316 72.58% 78.02% 84.65%
Table 2: Word sense disambiguation results, in-
cluding two baselines (MFS = most frequent sense;
LeskC = Lesk-corpus) and the word sense disam-
biguation system. Number of senses (#s) and num-
ber of examples (#ex) are also indicated.
implements the corpus-based version of the Lesk al-
gorithm (Kilgarriff and Rosenzweig, 2000).
6 Discussion
Overall, the Wikipedia-based sense annotations
were found reliable, leading to accurate sense classi-
fiers with an average relative error rate reduction of
44% compared to the most frequent sense baseline,
and 30% compared to the Lesk-corpus baseline.
There were a few exceptions to this general trend.
For instance, for some of the words for which only
a small number of examples could be collected from
Wikipedia, e.g. restraint or shelter, no accuracy im-
provement was observed compared to the most fre-
quent sense baseline. Similarly, several words in the
200
 76
 78
 80
 82
 84
 86
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Cl
as
sif
ie
r a
cc
ur
ac
y
Fraction of data
Word sense disambiguation learning curve
Figure 1: Learning curve on the Wikipedia data set.
data set have highly skewed sense distributions, such
as e.g. bank, which has a total number of 1,074 ex-
amples out of which 1,044 examples pertain to the
meaning of financial institution, or the word mate-
rial with 213 out of 223 examples annotated with
the meaning of substance.
One aspect that is particularly relevant for any su-
pervised system is the learning rate with respect to
the amount of available data. To determine the learn-
ing curve, we measured the disambiguation accu-
racy under the assumption that only a fraction of the
data were available. We ran ten fold cross-validation
experiments using 10%, 20%, ..., 100% of the data,
and averaged the results over all the words in the
data set. The resulting learning curve is plotted in
Figure 1. Overall, the curve indicates a continuously
growing accuracy with increasingly larger amounts
of data. Although the learning pace slows down after
a certain number of examples (about 50% of the data
currently available), the general trend of the curve
seems to indicate that more data is likely to lead to
increased accuracy. Given that Wikipedia is growing
at a fast pace, the curve suggests that the accuracy of
the word sense classifiers built on this data is likely
to increase for future versions of Wikipedia.
Another aspect we were interested in was the cor-
relation in terms of sense coverage with respect to
other sense annotated data currently available. For
the set of 30 nouns in our data set, we collected
all the word senses that were defined in either the
Wikipedia-based sense-tagged corpus or in the SEN-
SEVAL corpus. We then determined the percentage
covered by each sense with respect to the entire data
set available for a given ambiguous word. For in-
stance, the noun chair appears in Wikipedia with
senses #1 (68.0%), #2 (31.9%), and #4(0.1%), and
in SENSEVAL with senses #1 (87.7%), #2 (6.3%),
and #3 (6.0%). The senses that do not appear are in-
dicated with a 0% coverage. The correlation is then
measured between the relative sense frequencies of
all the words in our dataset, as observed in the two
corpora. Using the Pearson (r) correlation factor, we
found an overall correlation of r = 0.51 between the
sense distributions in the Wikipedia corpus and the
SENSEVAL corpus, which indicates a medium cor-
relation. This correlation is much lower than the
one observed between the sense distributions in the
training data and in the test data in the SENSEVAL
corpus, which was measured at a high r = 0.95.
This suggests that the sense coverage in Wikipedia
follows a different distribution than in SENSEVAL,
mainly reflecting the difference between the gen-
res of the two corpora: an online collection of en-
cyclopedic pages as available from Wikipedia, ver-
sus the manually balanced British National Cor-
pus used in SENSEVAL. It also suggests that using
the Wikipedia-based sense tagged corpus to disam-
biguate words in the SENSEVAL data or viceversa
would require a change in the distribution of senses
as previously done in (Agirre and Martinez, 2004).
baselines word sense
Dataset #s #ex MFS LeskC disambig.
SENSEVAL 4.60 226 51.53% 58.33% 68.13%
WIKIPEDIA 3.31 316 72.58% 78.02% 84.65%
Table 3: Average number of senses and exam-
ples, most frequent sense and Lesk-corpus baselines,
and word sense disambiguation performance on the
SENSEVAL and WIKIPEDIA datasets.
Table 3 shows the characteristics of the SEN-
SEVAL and the WIKIPEDIA datasets for the nouns
listed in Table 2. The table also shows the most
frequent sense baseline, the Lesk-corpus baseline,
as well as the accuracy figures obtained on each
dataset using the word sense disambiguation system
described in Section 4.6
6As a side note, the accuracy obtained by our system on the
SENSEVAL data is comparable to that of the best participating
systems. Using the output of the best systems: the JHUR sys-
tem on the SENSEVAL-2 words, and the HLTS3 system on the
201
Overall the sense distinctions identified in
Wikipedia are fewer and typically coarser than those
found in WordNet. As shown in Table 3, for the
set of ambiguous words listed in Table 2, an aver-
age of 4.6 senses were used in the SENSEVAL an-
notations, as compared to about 3.3 senses per word
found in Wikipedia. This is partly due to a differ-
ent sense coverage and distribution in the Wikipedia
data set (e.g. the meaning of ambiance for the am-
biguous word atmosphere does not appear at all in
the Wikipedia corpus, although it has the highest fre-
quency in the SENSEVAL data), and partly due to the
coarser sense distinctions made in Wikipedia (e.g.
Wikipedia does not make the distinction between the
act of grasping and the actual hold for the noun grip,
and occurrences of both of these meanings are anno-
tated with the label grip (handle)).
There are also cases when Wikipedia makes dif-
ferent or finer sense distinctions than WordNet. For
instance, there are several Wikipedia annotations for
image as copy, but this meaning is not even defined
in WordNet. Similarly, Wikipedia makes the distinc-
tion between dance performance and theatre perfor-
mance, but both these meanings are listed under one
single entry in WordNet (performance as public pre-
sentation). However, since at this stage we are map-
ping the Wikipedia annotations to WordNet, these
differences in sense granularity are diminished.
7 Related Work
In word sense disambiguation, the line of work most
closely related to ours consists of methods trying to
address the sense-tagged data bottleneck problem.
A first set of methods consists of algorithms that
generate sense annotated data using words semanti-
cally related to a given ambiguous word (Leacock et
al., 1998; Mihalcea and Moldovan, 1999; Agirre and
Martinez, 2004). Related non-ambiguous words,
such as monosemous words or phrases from dictio-
nary definitions, are used to automatically collect
examples from the Web. These examples are then
turned into sense-tagged data by replacing the non-
ambiguous words with their ambiguous equivalents.
Another approach proposed in the past is based on
the idea that an ambiguous word tends to have dif-
SENSEVAL-3 words, an average accuracy of 71.31% was mea-
sured (the output of the systems participating in SENSEVAL is
publicly available from http://www.senseval.org).
ferent translations in a second language (Resnik and
Yarowsky, 1999). Starting with a collection of paral-
lel texts, sense annotations were generated either for
one word at a time (Ng et al, 2003; Diab, 2004), or
for all words in unrestricted text (Diab and Resnik,
2002), and in both cases the systems trained on these
data were found to be competitive with other word
sense disambiguation systems.
The lack of sense-tagged corpora can also be cir-
cumvented using bootstrapping algorithms, which
start with a few annotated seeds and iteratively gen-
erate a large set of disambiguation patterns. This
method, initially proposed by (Yarowsky, 1995),
was successfully evaluated in the context of the
SENSEVAL framework (Mihalcea, 2002).
Finally, in an effort related to the Wikipedia col-
lection process, (Chklovski and Mihalcea, 2002)
have implemented the Open Mind Word Expert sys-
tem for collecting sense annotations from volunteer
contributors over the Web. The data generated using
this method was then used by the systems participat-
ing in several of the SENSEVAL-3 tasks.
Notably, the method we propose has several ad-
vantages over these previous methods. First, our
method relies exclusively on monolingual data, thus
avoiding the possible constraints imposed by meth-
ods that require parallel texts, which may be difficult
to find. Second, the Wikipedia-based annotations
follow a natural Zipfian sense distribution, unlike the
equal distributions typically obtained with the meth-
ods that rely on the use of monosemous relatives
or bootstrapping methods. Finally, the grow pace
of Wikipedia is much faster than other more task-
focused and possibly less-engaging activities such
as Open Mind Word Expert, and therefore has the
potential to lead to significantly higher coverage.
With respect to the use of Wikipedia as a re-
source for natural language processing tasks, the
work that is most closely related to ours is per-
haps the name entity disambiguation algorithm pro-
posed in (Bunescu and Pasca, 2006), where an SVM
kernel is trained on the entries found in Wikipedia
for ambiguous named entities. Other language pro-
cessing tasks with recently proposed solutions re-
lying on Wikipedia are co-reference resolution us-
ing Wikipedia-based measures of word similarity
(Strube and Ponzetto, 2006), enhanced text classi-
fication using encyclopedic knowledge (Gabrilovich
202
and Markovitch, 2006), and the construction of com-
parable corpora using the multilingual editions of
Wikipedia (Adafre and de Rijke, 2006).
8 Conclusions
In this paper, we described an approach for us-
ing Wikipedia as a source of sense annotations for
word sense disambiguation. Starting with the hy-
perlinks available in Wikipedia, we showed how we
can generate a sense annotated corpus that can be
used to train accurate sense classifiers. Through ex-
periments performed on a subset of the SENSEVAL
words, we showed that the Wikipedia sense annota-
tions can be used to build a word sense disambigua-
tion system leading to a relative error rate reduction
of 30?44% as compared to simpler baselines.
Despite some limitations inherent to this approach
(definitions and annotations in Wikipedia are avail-
able almost exclusively for nouns, word and sense
distributions are sometime skewed, the annotation
labels are occasionally inconsistent), these limi-
tations are overcome by the clear advantage that
comes with the use of Wikipedia: large sense tagged
data for a large number of words at virtually no cost.
We believe that this approach is particularly
promising for two main reasons. First, the size of
Wikipedia is growing at a steady pace, which conse-
quently means that the size of the sense tagged cor-
pora that can be generated based on this resource
is also continuously growing. While techniques for
supervised word sense disambiguation have been re-
peatedly criticized in the past for their limited cover-
age, mainly due to the associated sense-tagged data
bottleneck, Wikipedia seems a promising resource
that could provide the much needed solution for this
problem. Second, Wikipedia editions are available
for many languages (currently about 200), which
means that this method can be used to generate sense
tagged corpora and build accurate word sense clas-
sifiers for a large number of languages.
References
S. F. Adafre and M. de Rijke. 2006. Finding similar sentences
across multiple languages in wikipedia. In Proceedings of
the EACL Workshop on New Text, Trento, Italy.
E. Agirre and D. Martinez. 2004. Unsupervised word sense
disambiguation based on automatically retrieved examples:
The importance of bias. In Proceedings of EMNLP 2004,
Barcelona, Spain, July.
R. Bunescu and M. Pasca. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In Proceedings of
EACL 2006, Trento, Italy.
T. Chklovski and R. Mihalcea. 2002. Building a sense tagged
corpus with Open Mind Word Expert. In Proceedings of the
ACL 2002 Workshop on ?Word Sense Disambiguation: Re-
cent Successes and Future Directions?, Philadelphia, July.
M. Diab and P. Resnik. 2002. An unsupervised method for
word sense tagging using parallel corpora. In Proceedings
of ACL 2002, Philadelphia.
M. Diab. 2004. Relieving the data acquisition bottleneck in
word sense disambiguation. In Proceedings of ACL 2004,
Barcelona, Spain.
E. Gabrilovich and S. Markovitch. 2006. Overcoming the brit-
tleness bottleneck using wikipedia: Enhancing text catego-
rization with encyclopedic knowledge. In Proceedings of
AAAI 2006, Boston.
M. Galley and K. McKeown. 2003. Improving word sense
disambiguation in lexical chaining. In Proceedings of IJCAI
2003, Acapulco, Mexico.
A. Kilgarriff and R. Rosenzweig. 2000. Framework and re-
sults for English SENSEVAL. Computers and the Humani-
ties, 34:15?48.
C. Leacock, M. Chodorow, and G.A. Miller. 1998. Using cor-
pus statistics and WordNet relations for sense identification.
Computational Linguistics, 24(1):147?165.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of knowl-
edge sources and learning algorithms for word sense disam-
biguation. In Proceedings of EMNLP 2002, Philadelphia.
M.E. Lesk. 1986. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from an
ice cream cone. In Proceedings of the SIGDOC Conference
1986, Toronto, June.
R. Mihalcea and D.I. Moldovan. 1999. An automatic method
for generating sense tagged corpora. In Proceedings of AAAI
1999, Orlando.
R. Mihalcea. 2002. Bootstrapping large sense tagged corpora.
In Proceedings of LREC 2002, Canary Islands, Spain.
R. Navigli and P. Velardi. 2005. Structural semantic intercon-
nections: a knowledge-based approach to word sense dis-
ambiguation. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 27.
H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge
sources to disambiguate word sense: An examplar-based ap-
proach. In Proceedings of ACL 1996, New Mexico.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting parallel
texts for word sense disambiguation: An empirical study. In
Proceedings of ACL 2003, Sapporo, Japan.
T. Pedersen. 2001. A decision tree of bigrams is an accurate
predictor of word sense. In Proceedings of NAACL 2001,
Pittsburgh.
P. Resnik and D. Yarowsky. 1999. Distinguishing sys-
tems and distinguishing senses: new evaluation methods for
word sense disambiguation. Natural Language Engineering,
5(2):113?134.
M. Strube and S. P. Ponzetto. 2006. Wikirelate! computing
semantic relatedeness using Wikipedia. In Proceedings of
AAAI 2006, Boston.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proceedings of ACL 1995,
Cambridge.
203
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrating Knowledge for Subjectivity Sense Labeling
Yaw Gyamfi and Janyce Wiebe
University of Pittsburgh
{anti,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Abstract
This paper introduces an integrative approach
to automatic word sense subjectivity annota-
tion. We use features that exploit the hier-
archical structure and domain information in
lexical resources such as WordNet, as well as
other types of features that measure the sim-
ilarity of glosses and the overlap among sets
of semantically related words. Integrated in a
machine learning framework, the entire set of
features is found to give better results than any
individual type of feature.
1 Introduction
Automatic extraction of opinions, emotions, and
sentiments in text (subjectivity analysis) to support
applications such as product review mining, sum-
marization, question answering, and information ex-
traction is an active area of research in NLP.
Many approaches to opinion, sentiment, and sub-
jectivity analysis rely on lexicons of words that may
be used to express subjectivity. However, words may
have both subjective and objective senses, which is
a source of ambiguity in subjectivity and sentiment
analysis. We show that even words judged in pre-
vious work to be reliable clues of subjectivity have
significant degrees of subjectivity sense ambiguity.
To address this ambiguity, we present a method
for automatically assigning subjectivity labels to
word senses in a taxonomy, which uses new features
and integrates more diverse types of knowledge than
in previous work. We focus on nouns, which are
challenging and have received less attention in auto-
matic subjectivity and sentiment analysis.
A common approach to building lexicons for sub-
jectivity analysis is to begin with a small set of
seeds which are prototypically subjective (or posi-
tive/negative, in sentiment analysis), and then fol-
low semantic links in WordNet-like resources. By
far, the emphasis has been on horizontal relations,
such as synonymy and antonymy. Exploiting vertical
links opens the door to taking into account the infor-
mation content of ancestor concepts of senses with
known and unknown subjectivity. We develop novel
features that measure the similarity of a target word
sense with a seed set of senses known to be sub-
jective, where the similarity between two concepts
is determined by the extent to which they share in-
formation, measured by the information content as-
sociated with their least common subsumer (LCS).
Further, particularizing the LCS features to domain
greatly reduces calculation while still maintaining
effective features.
We find that our new features do lead to signif-
icant improvements over methods proposed in pre-
vious work, and that the combination of all features
gives significantly better performance than any sin-
gle type of feature alone.
We also ask, given that there are many approaches
to finding subjective words, if it would make sense
for word- and sense-level approaches to work in tan-
dem, or should we best view them as competing ap-
proaches? We give evidence suggesting that first
identifying subjective words and then disambiguat-
ing their senses would be an effective approach to
subjectivity sense labeling.
10
There are several motivations for assigning sub-
jectivity labels to senses. First, (Wiebe and Mi-
halcea, 2006) provide evidence that word sense la-
bels, together with contextual subjectivity analysis,
can be exploited to improve performance in word
sense disambiguation. Similarly, given subjectivity
sense labels, word-sense disambiguation may poten-
tially help contextual subjectivity analysis. In addi-
tion, as lexical resources such as WordNet are devel-
oped further, subjectivity labels would provide prin-
cipled criteria for refining word senses, as well as for
clustering similar meanings to create more course-
grained sense inventories.
For many opinion mining applications, polarity
(positive, negative) is also important. The overall
framework we envision is a layered approach: clas-
sifying instances as objective or subjective, and fur-
ther classifying the subjective instances by polar-
ity. Decomposing the problem into subproblems has
been found to be effective for opinion mining. This
paper addresses the first of these subproblems.
2 Background
We adopt the definitions of subjective and objective
from Wiebe and Mihalcea (2006) (hereafter WM).
Subjective expressions are words and phrases being
used to express opinions, emotions, speculations,
etc. WM give the following examples:
His alarm grew.
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President?s verbal assault on Israel.
What?s the catch?
Polarity (also called semantic orientation) is also
important to NLP applications in sentiment analysis
and opinion extraction. In review mining, for exam-
ple, we want to know whether an opinion about a
product is positive or negative. Even so, we believe
there are strong motivations for a separate subjec-
tive/objective (S/O) classification as well.
First, expressions may be subjective but not have
any particular polarity. An example given by (Wil-
son et al, 2005) is Jerome says the hospital feels
no different than a hospital in the states. An NLP
application system may want to find a wide range
of private states attributed to a person, such as their
motivations, thoughts, and speculations, in addition
to their positive and negative sentiments.
Second, distinguishing S and O instances has of-
ten proven more difficult than subsequent polarity
classification. Researchers have found this at vari-
ous levels of analysis, including the manual anno-
tation of phrases (Takamura et al, 2006), sentiment
classification of phrases (Wilson et al, 2005), sen-
timent tagging of words (Andreevskaia and Bergler,
2006b), and sentiment tagging of word senses (Esuli
and Sebastiani, 2006a). Thus, effective methods for
S/O classification promise to improve performance
for sentiment classification. In fact, researchers in
sentiment analysis have realized benefits by decom-
posing the problem into S/O and polarity classifica-
tion (Yu and Hatzivassiloglou, 2003; Pang and Lee,
2004; Wilson et al, 2005; Kim and Hovy, 2006).
One reason is that different features may be relevant
for the two subproblems. For example, negation fea-
tures are more important for polarity classification
than for subjectivity classification.
Note that some of our features require vertical
links that are present in WordNet for nouns and
verbs but not for other parts of speech. Thus we ad-
dress nouns (leaving verbs to future work). There
are other motivations for focusing on nouns. Rela-
tively little work in subjectivity and sentiment anal-
ysis has focused on subjective nouns. Also, a study
(Bruce and Wiebe, 1999) showed that, of the major
parts of speech, nouns are the most ambiguous with
respect to the subjectivity of their instances.
Turning to word senses, we adopt the definitions
from WM. First, subjective: ?Classifying a sense as
S means that, when the sense is used in a text or con-
versation, we expect it to express subjectivity; we
also expect the phrase or sentence containing it to
be subjective [WM, pp. 2-3].?
In WM, it is noted that sentences containing ob-
jective senses may not be objective, as in the sen-
tence Will someone shut that darn alarm off? Thus,
objective senses are defined as follows: ?Classifying
a sense as O means that, when the sense is used in a
text or conversation, we do not expect it to express
subjectivity and, if the phrase or sentence containing
it is subjective, the subjectivity is due to something
else [WM, p 3].?
The following subjective examples are given in
11
WM:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
The following objective examples are given in WM:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
WM performed an agreement study and report
that good agreement (?=0.74) can be achieved be-
tween human annotators labeling the subjectivity of
senses. For a similar task, (Su and Markert, 2008)
also report good agreement.
3 Related Work
Many methods have been developed for automati-
cally identifying subjective (opinion, sentiment, at-
titude, affect-bearing, etc.) words, e.g., (Turney,
2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004;
Taboada et al, 2006; Takamura et al, 2006).
Five groups have worked on subjectivity sense la-
beling. WM and Su and Markert (2008) (hereafter
SM) assign S/O labels to senses, while Esuli and Se-
bastiani (hereafter ES) (2006a; 2007), Andreevskaia
and Bergler (hereafter AB) (2006b; 2006a), and
(Valitutti et al, 2004) assign polarity labels.
WM, SM, and ES have evaluated their systems
against manually annotated word-sense data. WM?s
annotations are described above; SM?s are similar.
In the scheme ES use (Cerini et al, 2007), senses
are assigned three scores, for positivity, negativity,
and neutrality. There is no unambiguous mapping
between the labels of WM/SM and ES, first because
WM/SM use distinct classes and ES use numerical
ratings, and second because WM/SM distinguish be-
tween objective senses on the one hand and neutral
subjective senses on the other, while those are both
neutral in the scheme used by ES.
WM use an unsupervised corpus-based approach,
in which subjectivity labels are assigned to word
senses based on a set of distributionally similar
words in a corpus annotated with subjective expres-
sions. SM explore methods that use existing re-
sources that do not require manually annotated data;
they also implement a supervised system for com-
parison, which we will call SMsup. The other three
groups start with positive and negative seed sets and
expand them by adding synonyms and antonyms,
and traversing horizontal links in WordNet. AB, ES,
and SMsup additionally use information contained
in glosses; AB also use hyponyms; SMsup also uses
relation and POS features. AB perform multiple
runs of their system to assign fuzzy categories to
senses. ES use a semi-supervised, multiple-classifier
learning approach. In a later paper, (Esuli and Se-
bastiani, 2007), ES again use information in glosses,
applying a random walk ranking algorithm to a
graph in which synsets are linked if a member of
the first synset appears in the gloss of the second.
Like ES and SMsup, we use machine learning, but
with more diverse sources of knowledge. Further,
several of our features are novel for the task. The
LCS features (Section 6.1) detect subjectivity by
measuring the similarity of a candidate word sense
with a seed set. WM also use a similarity measure,
but as a way to filter the output of a measure of distri-
butional similarity (selecting words for a given word
sense), not as we do to cumulatively calculate the
subjectivity of a word sense. Another novel aspect
of our similarity features is that they are particular-
ized to domain, which greatly reduces calculation.
The domain subjectivity LCS features (Section 6.2)
are also novel for our task. So is augmenting seed
sets with monosemous words, for greater coverage
without requiring human intervention or sacrificing
quality. Note that none of our features as we specif-
ically define them has been used in previous work;
combining them together, our approach outperforms
previous approaches.
12
4 Lexicon and Annotations
We use the subjectivity lexicon of (Wiebe and Riloff,
2005)1 both to create a subjective seed set and to
create the experimental data sets. The lexicon is a
list of words and phrases that have subjective uses,
though only word entries are used in this paper (i.e.,
we do not address phrases at this point). Some en-
tries are from manually developed resources, includ-
ing the General Inquirer, while others were derived
from corpora using automatic methods.
Through manual review and empirical testing on
data, (Wiebe and Riloff, 2005) divided the clues into
strong (strongsubj) and weak (weaksubj) subjectiv-
ity clues. Strongsubj clues have subjective meanings
with high probability, and weaksubj clues have sub-
jective meanings with lower probability.
To support our experiments, we annotated the
senses2 of polysemous nouns selected from the lex-
icon, using WM?s annotation scheme described in
Section 2. Due to time constraints, only some of the
data was labeled through consensus labeling by two
annotators; the rest was labeled by one annotator.
Overall, 2875 senses for 882 words were anno-
tated. Even though all are senses of words from the
subjectivity lexicon, only 1383 (48%) of the senses
are subjective.
The words labeled strongsubj are in fact less am-
biguous than those labeled weaksubj in our analysis,
thus supporting the reliability classifications in the
lexicon. 55% (1038/1924) of the senses of strong-
subj words are subjective, while only 36% (345/951)
of the senses of weaksubj words are subjective.
For the analysis in Section 7.3, we form subsets
of the data annotated here to test performance of our
method on different data compositions.
5 Seed Sets
Both subjective and objective seed sets are used to
define the features described below. For seeds, a
large number is desirable for greater coverage, al-
though high quality is also important. We begin to
build our subjective seed set by adding the monose-
mous strongsubj nouns of the subjectivity lexicon
(there are 397 of these). Since they are monose-
mous, they pose no problem of sense ambiguity. We
1Available at http://www.cs.pitt.edu/mpqa
2In WordNet 2.0
then expand the set with their hyponyms, as they
were found useful in previous work by AB (2006b;
2006a). This yields a subjective seed set of 645
senses. After removing the word senses that belong
to the same synset, so that only one word sense per
synset is left, we ended up with 603 senses.
To create the objective seed set, two annotators
manually annotated 800 random senses from Word-
Net, and selected for the objective seed set the ones
they both agreed are clearly objective. This creates
an objective seed set of 727. Again we removed
multiple senses from the same synset leaving us with
722. The other 73 senses they annotated are added
to the mixed data set described below. As this sam-
pling shows, WordNet nouns are highly skewed to-
ward objective senses, so finding an objective seed
set is not difficult.
6 Features
6.1 Sense Subjectivity LCS Feature
This feature measures the similarity of a target sense
with members of the subjective seed set. Here, sim-
ilarity between two senses is determined by the ex-
tent to which they share information, measured by
using the information content associated with their
least common subsumer. For an intuition behind this
feature, consider this example. In WordNet, the hy-
pernym of the ?strong criticism? sense of attack is
criticism. Several other negative subjective senses
are descendants of criticism, including the relevant
senses of fire, thrust, and rebuke. Going up one
more level, the hypernym of criticism is the ?ex-
pression of disapproval? meaning of disapproval,
which has several additional negative subjective de-
scendants, such as the ?expression of opposition and
disapproval? sense of discouragement. Our hypoth-
esis is that the cases where subjectivity is preserved
in the hypernym structure, or where hypernyms do
lead from subjective senses to others, are the ones
that have the highest least common subsumer score
with the seed set of known subjective senses.
We calculate similarity using the information-
content based measure proposed in (Resnik, 1995),
as implemented in the WordNet::Similarity pack-
age (using the default option in which LCS values
are computed over the SemCor corpus).3 Given a
3http://search.cpan.org/dist/WordNet-Similarity/
13
taxonomy such as WordNet, the information con-
tent associated with a concept is determined as the
likelihood of encountering that concept, defined as
?log(p(C)), where p(C) is the probability of see-
ing concept C in a corpus. The similarity between
two concepts is then defined in terms of information
content as: LCSs(C1, C2) = max[?log(p(C))],
where C is the concept that subsumes both C1 and
C2 and has the highest information content (i.e., it is
the least common subsumer (LCS)).
For this feature, a score is assigned to a target
sense based on its semantic similarity to the mem-
bers of a seed set; in particular, the maximum such
similarity is used.
For a target sense t and a seed set S, we could
have used the following score:
Score(t, S) = max
s?S
LCSs(t, s)
However, several researchers have noted that sub-
jectivity may be domain specific. A version of
WordNet exists, WordNet Domains (Gliozzo et al,
2005), which associates each synset with one of the
domains in the Dewey Decimal library classifica-
tion. After sorting our subjective seed set into differ-
ent domains, we observed that over 80% of the sub-
jective seed senses are concentrated in six domains
(the rest are distributed among 35 domains).
Thus, we decided to particularize the semantic
similarity feature to domain, such that only the sub-
set of the seed set in the same domain as the tar-
get sense is used to compute the feature. This in-
volves much less calculation, as LCS values are cal-
culated only with respect to a subset of the seed set.
We hypothesized that this would still be an effec-
tive feature, while being more efficient to calculate.
This will be important when this method is applied
to large resources such as the entire WordNet.
Thus, for seed set S and target sense t which is
in domain D, the feature is defined as the following
score:
SenseLCSscore(t,D, S) = max
d?D?S
LCSs(t, d)
The seed set is a parameter, so we could have
defined a feature reflecting similarity to the objec-
tive seed set as well. Since WordNet is already
highly skewed toward objective noun senses, any
naive classifier need only guess the majority class
for high accuracy for the objective senses. We in-
cluded only a subjective feature to put more empha-
sis on the subjective senses. In the future, features
could be defined with respect to objectivity, as well
as polarity and other properties of subjectivity.
6.2 Domain Subjectivity LCS Score
We also include a feature reflecting the subjectivity
of the domain of the target sense. Domains are
assigned scores as follows. For domain D and seed
set S:
DomainLCSscore(D,S) =
aved?D?SMemLCSscore(d,D, S)
where:
MemLCSscore(d,D, S) =
max
di?D?S,di 6=d
LCSs(d, di)
The value of this feature for a sense is the score
assigned to that sense?s domain.
6.3 Common Related Senses
This feature is based on the intersection between the
set of senses related (via WordNet relations) to the
target sense and the set of senses related to members
of a seed set. First, for the target sense and each
member of the seed set, a set of related senses is
formed consisting of its synonyms, antonyms and di-
rect hypernyms as defined by WordNet. For a sense
s, R(s) is s together with its related senses.
Then, given a target sense t and a seed set S we
compute an average percentage overlap as follows:
RelOverlap(t, S) =
?
si?S
|R(t)?R(si)|
max (|R(t)|,|R(si)|)
|S|
The value of a feature is its score. Two features
are included in the experiments below, one for each
of the subjective and objective seed sets.
6.4 Gloss-based features
These features are Lesk-style features (Lesk, 1986)
that exploit overlaps between glosses of target and
seed senses. We include two types in our work.
6.4.1 Average Percentage Gloss Overlap
Features
For a sense s, gloss(s) is the set of stems in the
gloss of s (excluding stop words). Then, given a tar-
14
get sense t and a seed set S, we compute an average
percentage overlap as follows:
GlOverlap(t, S) =
?
si?S
|gloss(t)??r?R(si)gloss(r)|
max (|gloss(t)|,|?r?R(si)gloss(r)|)
|S|
As above, R(s) is considered for each seed sense
s, but now only the target sense t is considered, not
R(t). We did this because we hypothesized that the
gloss can provide sufficient context for a given target
sense, so that the addition of related words is not
necessary.
We include two features, one for each of the sub-
jective and objective seed sets.
6.4.2 Vector Gloss Overlap Features
For this feature we also consider overlaps of
stems in glosses (excluding stop words). The over-
laps considered are between the gloss of the tar-
get sense t and the glosses of R(s) for all s in a
seed set (for convenience, we will refer to these as
seedRelationSets).
A vector of stems is created, one for each stem
(excluding stop words) that appears in a gloss of
a member of seedRelationSets. If a stem in the
gloss of the target sense appears in this vector, then
the vector entry for that stem is the total count of
that stem in the glosses of the target sense and all
members of seedRelationSets.
A feature is created for each vector entry whose
value is the count at that position. Thus, these fea-
tures consider counts of individual stems, rather than
average proportions of overlaps, as for the previous
type of gloss feature.
Two vectors of features are used, one where the
seed set is the subjective seed set, and one where it
is the objective seed set.
6.5 Summary
In summary, we use the following features (here, SS
is the subjective seed set and OS is the objective
one).
1. SenseLCSscore(t,D, SS)
2. DomainLCSscore(D,SS)
3. RelOverlap(t, SS)
4. RelOverlap(t, OS)
5. GlOverlap(t, SS)
6. GlOverlap(t, OS)
Features Acc P R F
All 77.3 72.8 74.3 73.5
Standalone Ablation Results
All 77.3 72.8 74.3 73.5
LCS 68.2 69.3 44.2 54.0
Gloss vector 74.3 71.2 68.5 69.8
Overlaps 69.4 75.8 40.6 52.9
Leave-One-Out Ablation Results
All 77.3 72.8 74.3 73.5
LCS 75.2 70.9 70.6 70.7
Gloss vector 75.0 74.4 61.8 67.5
Overlaps 74.8 71.9 73.8 72.8
Table 1: Results for the mixed corpus (2354 senses,
57.82% O))
7. Vector of gloss words (SS)
8. Vector of gloss words (OS)
7 Experiments
We perform 10-fold cross validation experiments
on several data sets, using SVM light (Joachims,
1999)4 under its default settings.
Based on our random sampling of WordNet, it
appears that WordNet nouns are highly skewed to-
ward objective senses. (Esuli and Sebastiani, 2007)
argue that random sampling from WordNet would
yield a corpus mostly consisting of objective (neu-
tral) senses, which would be ?pretty useless as a
benchmark for testing derived lexical resources for
opinion mining [p. 428].? So, they use a mixture of
subjective and objective senses in their data set.
To create a mixed corpus for our task, we anno-
tated a second random sample from WordNet (which
is as skewed as the previously mentioned one). We
added together all of the senses of words in the lexi-
con which we annotated, the leftover senses from the
selection of objective seed senses, and this new sam-
ple. We removed duplicates, multiple senses from
the same synset, and any senses belonging to the
same synset in either of the seed sets. This resulted
in a corpus of 2354 senses, 993 (42.18%) of which
are subjective and 1361 (57.82%) of which are ob-
jective.
The results with all of our features on this mixed
corpus are given in Row 1 of Table 1. In Table 1, the
4http://svmlight.joachims.org/
15
first column identifies the features, which in this case
is all of them. The next three columns show overall
accuracy, and precision and recall for finding sub-
jective senses. The baseline accuracy for the mixed
data set (guessing the more frequent class, which is
objective) is 57.82%. As the table shows, the accu-
racy is substantially above baseline.5
7.1 Analysis and Discussion
In this section, we seek to gain insights by perform-
ing ablation studies, evaluating our method on dif-
ferent data compositions, and comparing our results
to previous results.
7.2 Ablation Studies
Since there are several features, we divided them
into sets for the ablation studies. The vector-of-
gloss-words features are the most similar to ones
used in previous work. Thus, we opted to treat
them as one ablation group (Gloss vector). The
Overlaps group includes the RelOverlap(t, SS),
RelOverlap(t, OS), GlOverlap(t, SS), and
GlOverlap(t, OS) features. Finally, the LCS
group includes the SenseLCSscore and the
DomainLCSscore features.
There are two types of ablation studies. In the
first, one group of features at a time is included.
Those results are in the middle section of Table 1.
Thus, for example, the row labeled LCS in this sec-
tion is for an experiment using only the LCS fea-
tures. In comparison to performance when all fea-
tures are used, F-measure for the Overlaps and LCS
ablations is significantly different at the p < .01
level, and, for the Gloss Vector ablation, it is sig-
nificantly different at the p = .052 level (one-tailed
t-test). Thus, all of the features together have better
performance than any single type of feature alone.
In the second type of ablation study, we use all
the features minus one group of features at a time.
The results are in the bottom section of Table 1.
Thus, for example, the row labeled LCS in this sec-
tion is for an experiment using all but the LCS fea-
tures. F-measures for LCS and Gloss vector are sig-
nificantly different at the p = .056 and p = .014 lev-
els, respectively. However, F-measure for the Over-
laps ablation is not significantly different (p = .39).
5Note that, because the majority class is O, baseline recall
(and thus F-measure) is 0.
Data (#senses) Acc P R F
mixed (2354 57.8% O) 77.3 72.8 74.3 73.5
strong+weak (1132) 77.7 76.8 78.9 77.8
weaksubj (566) 71.3 70.3 71.1 70.7
strongsubj (566) 78.6 78.8 78.6 78.7
Table 2: Results for different data sets (all are 50% S,
unless otherwise notes)
These results provide evidence that LCS and Gloss
vector are better together than either of them alone.
7.3 Results on Different Data Sets
Several methods have been developed for identify-
ing subjective words. Perhaps an effective strategy
would be to begin with a word-level subjectivity lex-
icon, and then perform subjectivity sense labeling
to sort the subjective from objective senses of those
words. We also wondered about the relative effec-
tiveness of our method on strongsubj versus weak-
subj clues.
To answer these questions, we apply the full
model (again in 10-fold cross validation experi-
ments) to data sets composed of senses of polyse-
mous words in the subjectivity lexicon. To support
comparison, all of the data sets in this section have
a 50%-50% objective/subjective distribution.6 The
results are presented in Table 2.
For comparison, the first row repeats the results
for the mixed corpus from Table 1. The second
row shows results for a corpus of senses of a mix-
ture of strongsubj and weaksubj words. The corpus
was created by selecting a mixture of strongsubj and
weaksubj words, extracting their senses and the S/O
labels applied to them in Section 4, and then ran-
domly removing senses of the more frequent class
until the distribution is uniform. We see that the
results on this corpus are better than on the mixed
data set, even though the baseline accuracy is lower
and the corpus is smaller. This supports the idea
that an effective strategy would be to first identify
opinion-bearing words, and then apply our method
to those words to sort out their subjective and objec-
tive senses.
The third row shows results for a weaksubj subset
6As with the mixed data set, we removed from these data
sets multiple senses from the same synset and any senses in the
same synset in either of the seed sets.
16
Method P R F
Our method 56.8 66.0 61.1
WM, 60% recall 44.0 66.0 52.8
SentiWordNet mapping 60.0 17.3 26.8
Table 3: Results for WM Corpus (212 senses, 76% O)
Method A P R F
Our Method 81.3% 60.3% 63.3% 61.8%
SM CV* 82.4% 70.8% 41.1% 52.0%
SM SL* 78.3% 53.0% 57.4% 54.9%
Table 4: Results for SM Corpus (484 senses, 76.9% O)
of the strong+weak corpus and the fourth shows re-
sults for a strongsubj subset that is of the same size.
As expected, the results for the weaksubj senses
are lower while those for the strongsubj senses are
higher, as weaksubj clues are more ambiguous.
7.4 Comparisons with Previous Work
WM and SM address the same task as we do. To
compare our results to theirs, we apply our full
model (in 10-fold cross validation experiments) to
their data sets.7
Table 3 has the WM data set results. WM rank
their senses and present their results in the form of
precision recall curves. The second row of Table 3
shows their results at the recall level achieved by our
method (66%). Their precision at that level is sub-
stantially below ours.
Turning to ES, to create S/O annotations, we ap-
plied the following heuristic mapping (which is also
used by SM for the purpose of comparison): any
sense for which the sum of positive and negative
scores is greater than or equal to 0.5 is S, otherwise
it is O. We then evaluate the mapped tags against the
gold standard of WM. The results are in Row 3 of
Table 3. Note that this mapping is not fair to Sen-
tiWordNet, as the tasks are quite different, and we
do not believe any conclusions can be drawn. We
include the results to eliminate the possibility that
their method is as good ours on our task, despite the
differences between the tasks.
Table 4 has the results for the noun subset of SM?s
7The WM data set is available at
http://www.cs.pitt.edu/www.cs.pitt.edu/?wiebe. ES applied
their method in (2006b) to WordNet, and made the results
available as SentiWordNet at http://sentiwordnet.isti.cnr.it/.
data set, which is the data set used by ES, reanno-
tated by SM. CV* is their supervised system and
SL* is their best non-supervised one. Our method
has higher F-measure than the others.8 Note that the
focus of SM?s work is not supervised machine learn-
ing.
8 Conclusions
In this paper, we introduced an integrative approach
to automatic subjectivity word sense labeling which
combines features exploiting the hierarchical struc-
ture and domain information of WordNet, as well
as similarity of glosses and overlap among sets
of semantically related words. There are several
contributions. First, we learn several things. We
found (in Section 4) that even reliable lists of sub-
jective (opinion-bearing) words have many objec-
tive senses. We asked if word- and sense-level ap-
proaches could be used effectively in tandem, and
found (in Section 7.3) that an effective strategy is to
first identify opinion-bearing words, and then apply
our method to sort out their subjective and objective
senses. We also found (in Section 7.2) that the entire
set of features gives better results than any individ-
ual type of feature alone.
Second, several of the features are novel for
our task, including those exploiting the hierarchical
structure of a lexical resource, domain information,
and relations to seed sets expanded with monose-
mous senses.
Finally, the combination of our particular features
is effective. For example, on senses of words from
a subjectivity lexicon, accuracies range from 20 to
29 percentage points above baseline. Further, our
combination of features outperforms previous ap-
proaches.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation awards #0840632 and #0840608.
The authors are grateful to Fangzhong Su and Katja
Markert for making their data set available, and to
the three paper reviewers for their helpful sugges-
tions.
8We performed the same type of evaluation as in SM?s paper.
That is, we assign a subjectivity label to one word sense for each
synset, which is the same as applying a subjectivity label to a
synset as a whole as done by SM.
17
References
Alina Andreevskaia and Sabine Bergler. 2006a. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006b. Sen-
timent tag extraction from wordnet glosses. In Pro-
ceedings of 5th International Conference on Language
Resources and Evaluation.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: A case study of manual tagging. Natural
Language Engineering, 5(2):187?205.
S. Cerini, V. Campagnoni, A. Demontis, M. Formentelli,
and C. Gandini. 2007. Micro-wnop: A gold standard
for the evaluation of automatically compiled lexical re-
sources for opinion mining. In Language resources
and linguistic theory: Typology, second language ac-
quisition, English linguistics. Milano.
Andrea Esuli and Fabrizio Sebastiani. 2006a. Determin-
ing term subjectivity and term orientation for opinion
mining. In 11th Meeting of the European Chapter of
the Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006b. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Conference
on Language Resources and Evaluation, Genova, IT.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 424?
431, Prague, Czech Republic, June.
A. Gliozzo, C. Strapparava, E. d?Avanzo, and
B. Magnini. 2005. Automatic acquisition of
domain specific lexicons. Tech. report, IRST, Italy.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burgess, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector
Learning, Cambridge, MA. MIT-Press.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twentieth
International Conference on Computational Linguis-
tics, pages 1267?1373, Geneva, Switzerland.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
Empirical Methods in Natural Language Processing,
pages 200?207, New York.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics , pages 271?278, Barcelona, ES. Association for
Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In Proc. Inter-
national Joint Conference on Artificial Intelligence.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Conference on
Empirical Methods in Natural Language Processing,
pages 105?112.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics, Manchester.
M. Taboada, C. Anthony, and K. Voll. 2006. Methods
for creating semantic orientation databases. In Pro-
ceedings of 5th International Conference on Language
Resources and Evaluation .
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orienta-
tions of phrases. In Proceedings of the 11th Meeting
of the European Chapter of the Association for Com-
putational Linguistics , Trento, Italy.
P. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of re-
views. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
417?424, Philadelphia.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology Journal, 2(1):61?83.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics, Sydney, Aus-
tralia.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Com-
putational Linguistics , pages 486?497, Mexico City,
Mexico.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technologies Conference/Conference on Empir-
ical Methods in Natural Language Processing , pages
347?354, Vancouver, Canada.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing , pages 129?136, Sapporo,
Japan.
18
Proceedings of NAACL HLT 2009: Short Papers, pages 117?120,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Topic Identification Using Wikipedia Graph Centrality
Kino Coursey
University of North Texas and Daxtron Laboratories, Inc.
kino@daxtron.com
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
This paper presents a method for automatic
topic identification using a graph-centrality al-
gorithm applied to an encyclopedic graph de-
rived from Wikipedia. When tested on a data
set with manually assigned topics, the system
is found to significantly improve over a sim-
pler baseline that does not make use of the ex-
ternal encyclopedic knowledge.
1 Introduction
Document topics have been used for a long time by
librarians to improve the retrieval of a document,
and to provide background or associated information
for browsing by users. They can also assist search,
background information gathering and contextual-
ization tasks, and enhanced relevancy measures.
The goal of the work described in this paper is to
automatically find topics that are relevant to an input
document. We refer to this task as ?topic identifica-
tion? (Medelyan and Witten, 2008). For instance,
starting with a document on ?United States in the
Cold War,? we want to identify relevant topics, such
as ?history,? ?Global Conflicts,? ?Soviet Union,? and
so forth. We propose an unsupervised method for
topic identification, based on a biased graph cen-
trality algorithm applied to a large knowledge graph
built from Wikipedia.
The task of topic identification goes beyond key-
word extraction, since relevant topics may not be
necessarily mentioned in the document, and instead
have to be obtained from some repositories of ex-
ternal knowledge. The task is also different from
text classification, since the topics are either not
known in advance or are provided in the form of
a controlled vocabulary with thousands of entries,
and thus no classification can be performed. In-
stead, with topic identification, we aim to find topics
(or categories1) that are relevant to the document at
hand, which can be used to enrich the content of the
document with relevant external knowledge.
2 Dynamic Ranking of Topic Relevance
Our method is based on the premise that external
encyclopedic knowledge can be used to identify rel-
evant topics for a given document.
The method consists of two main steps. In the first
step, we build a knowledge graph of encyclopedic
concepts based on Wikipedia, where the nodes in the
graph are represented by the entities and categories
that are defined in this encyclopedia. The edges be-
tween the nodes are represented by their relation of
proximity inside the Wikipedia articles. The graph
is built once and then it is stored offline, so that it
can be efficiently use for the identification of topics
in new documents.
In the second step, for each input document, we
first identify the important encyclopedic concepts in
the text, and thus create links between the content of
the document and the external encyclopedic graph.
Next, we run a biased graph centrality algorithm on
the entire graph, so that all the nodes in the exter-
nal knowledge repository are ranked based on their
relevance to the input document.
2.1 Wikipedia
Wikipedia (http://en.wikipedia.org) is a free online
encyclopedia, representing the outcome of a contin-
uous collaborative effort of a large number of vol-
unteer contributors. The basic entry is an article,
which defines an entity or an event, and consists of a
hypertext document with hyperlinks to other pages
within or outside Wikipedia. In addition to arti-
1Throughout the paper, we use the terms ?topic? and ?cate-
gory? interchangeably.
117
cles, Wikipedia also includes a large number of cat-
egories, which represent topics that are relevant to
a given article (the July 2008 version of Wikipedia
includes more than 350,000 such categories).
We use the entire English Wikipedia to build an
encyclopedic graph for use in the topic identification
process. The nodes in the graph are represented by
all the article and category pages in Wikipedia, and
the edges between the nodes are represented by their
relation of proximity inside the articles. The graph
contains 5.8 million nodes, and 65.5 million edges.
2.2 Wikify!
In order to automatically identify the important en-
cyclopedic concepts in an input text, we use the un-
supervised system Wikify! (Mihalcea and Csomai,
2007), which identifies the concepts in the text that
are likely to be highly relevant for the input docu-
ment, and links them to Wikipedia concepts.
Wikify! works in three steps, namely: (1) candi-
date extraction, (2) keyword ranking, and (3) word
sense disambiguation. The candidate extraction step
parses the input document and extracts all the pos-
sible n-grams that are also present in the vocabulary
used in the encyclopedic graph (i.e., anchor texts for
links inside Wikipedia or article or category titles).
Next, the ranking step assigns a numeric value to
each candidate, reflecting the likelihood that a given
candidate is a valuable keyword. Wikify! uses a
?keyphraseness? measure to estimate the probabil-
ity of a term W to be selected as a keyword in
a document by counting the number of documents
where the term was already selected as a keyword
count(Dkey) divided by the total number of docu-
ments where the term appeared count(DW ). These
counts are collected from all the Wikipedia articles.
P (keyword|W ) ? count(Dkey)count(DW )
(1)
Finally, a simple word sense disambiguation
method is applied, which identifies the most likely
article in Wikipedia to which a concept should be
linked to. The algorithm is based on statistical meth-
ods that identify the frequency of meanings in text,
combined with symbolic methods that attempt to
maximize the overlap between the current document
and the candidate Wikipedia articles. See (Mihalcea
and Csomai, 2007) for more details.
2.3 Biased Ranking of the Wikipedia Graph
Starting with the graph of encyclopedic knowledge,
and knowing the nodes that belong to the input doc-
ument, we want to rank all the nodes in the graph
so that we obtain a score that indicates their impor-
tance relative to the given document. We can do this
by using a graph-ranking algorithm biased toward
the nodes belonging to the input document.
Graph-based ranking algorithms such as PageR-
ank are essentially a way of deciding the importance
of a vertex within a graph, based on global informa-
tion recursively drawn from the entire graph. One
formulation is in terms of a random walk through a
directed graph. A ?random surfer? visits nodes of
the graph, and has some probability of jumping to
some other random node of the graph. The rank of
a node is an indication of the probability that one
would find the surfer at that node at any given time.
Formally, let G = (V,E) be a directed graph with
the set of vertices V and set of edges E, where E is
a subset of V ? V . For a given vertex Vi, let In(Vi)
be the set of vertices that point to it (predecessors),
and let Out(Vi) be the set of vertices that vertex Vi
points to (successors). The PageRank score of a ver-
tex Vi is defined as follows (Brin and Page, 1998):
S(Vi) = (1? d) + d ?
?
j?In(Vi)
1
|Out(Vj)|S(Vj)
where d is a damping factor usually set to 0.85.
In a ?random surfer? interpretation of the ranking
process, the (1 ? d) portion represents the proba-
bility that a surfer navigating the graph will jump
to a given node from any other node at random, and
the summation portion indicates that the process will
enter the node via edges directly connected to it. Us-
ing a method inspired by earlier work (Haveliwala,
2002), we modify the formula so that the (1 ? d)
component also accounts for the importance of the
concepts found in the input document, and it is sup-
pressed for all the nodes that are not found in the
input document.
S(Vi) = (1?d)?Bias(Vi)+d?
?
j?In(Vi)
1
|Out(Vj)|S(Vj)
where Bias(Vi) is only defined for those nodes ini-
tially identified in the input document:
Bias(Vi) = f(Vi)?
j?InitalNodeSet
f(Vj)
and 0 for all other nodes in the graph.
InitalNodeSet is the set of nodes belonging
to the input document.
118
Note that f(Vi) can vary in complexity from a de-
fault value of 1 to a complex knowledge-based es-
timation. In our implementation, we use a combi-
nation of the ?keyphraseness? score assigned to the
node Vi and its distance from the ?Fundamental?
category in Wikipedia.
3 Experiments
We run two experiments, aimed at measuring the rel-
evancy of the automatically identified topics with re-
spect to a manually annotated gold standard data set.
In the first experiment, the identification of the
important concepts in the input text (used to bias the
topic ranking process) is performed manually, by the
Wikipedia users. In the second experiment, the iden-
tification of these important concepts is done auto-
matically with the Wikify! system. In both experi-
ments, the ranking of the concepts from the encyclo-
pedic graph is performed using the dynamic ranking
process described in Section 2.
We use a data set consisting of 150 articles from
Wikipedia, which have been explicitly removed
from the encyclopedic graph. All the articles in
this data set include manual annotations of the rele-
vant categories, as assigned by the Wikipedia users,
against which we can measure the quality of the au-
tomatic topic assignments. The 150 articles have
been randomly selected while following the con-
straint that they each contain at least three article
links and at least three category links. Our task is
to rediscover the relevant categories for each page.
Note that the task is non-trivial, since there are more
than 350,000 categories to choose from. We eval-
uate the quality of our system through the standard
measures of precision and recall.
3.1 Manual Annotation of the Input Text
In this first experiment, the articles in the gold stan-
dard data set alo include manual annotations of the
important concepts in the text, i.e., the links to other
Wikipedia articles as created by the Wikipedia users.
Thus, in this experiment we only measure the accu-
racy of the dynamic topic ranking process, without
interference from the Wikify! system.
There are two main parameters that can be set dur-
ing a system run. First, the set of initial nodes used
as bias in the ranking can include: (1) the initial set
of articles linked to by the original document (via
the Wikipedia links); (2) the categories listed in the
articles linked to by the original document2; and (3)
both. Second, the dynamic ranking process can be
run through propagation on an encyclopedic graph
that includes (1) all the articles from Wikipedia; (2)
all the categories from Wikipedia; or (3) all the arti-
cles and the categories from Wikipedia.
Figures 1 and 2 show the precision and recall for
the various settings. Bias and Propagate indicate
the selections made for the two parameters, which
can be set to either Articles, Categories, or Both.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0  20  40  60  80  100
P
re
ci
si
on
Top N topics returned
BiasArticles- PropCategories
BiasCategories- PropArticles PropCategories
BiasArticles BiasCategories- PropCategories
BiasArticles- PropArticles PropCategories
BiasArticles BiasCategories- PropArticles
BiasCategories- PropArticles
BiasArticles- PropArticles
BiasArticles BiasCategories- PropArticles PropCategories
BiasCategories- PropCategories
Baseline
Figure 1: Precision for manual input text annotations.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  20  40  60  80  100
R
ec
al
l
Top N topics returned
BiasArticles- PropCategories
BiasCategories- PropArticles PropCategories
BiasArticles BiasCategories- PropCategories
BiasArticles- PropArticles PropCategories
BiasArticles BiasCategories- PropArticles
BiasCategories- PropArticles
BiasArticles- PropArticles
BiasArticles BiasCategories- PropArticles PropCategories
BiasCategories- PropCategories
Baseline
Figure 2: Recall for manual input text annotations.
As seen in the figures, the best results are obtained
for a setting where both the initial bias and the prop-
agation include all the available nodes, i.e., both ar-
ticles and categories. Although the primary task is
the identification of the categories, the addition of
the article links improves the system performance.
2These should not be confused with the categories included
in the document itself, which represent the gold standard anno-
tations and are not used at any point.
119
To place results in perspective, we also calculate a
baseline (labeled as ?Baseline? in the plots), which
selects by default all the categories listed in the arti-
cles linked to by the original document.
3.2 Automatic Annotation of the Input Text
The second experiment is similar to the first one, ex-
cept that rather than using the manual annotations
of the important concepts in the input document,
we use instead the Wikify! system that automat-
ically identifies these important concepts by using
the method briefly described in Section 2.2. The ar-
ticle links identified by Wikify! are treated in the
same way as the human anchor annotations from the
previous experiment. In this experiment, we have
an additional parameter, which consists of the per-
centage of links selected by Wikify! out of the total
number of words in the document. We refer to this
parameter as keyRatio. The higher the keyRatio, the
more terms are added, but also the higher the poten-
tial of noise due to mis-disambiguation.
Figures 3 and 4 show the effect of varying the
value of the keyRatio parameter on the precision and
recall of the system. Note that in this experiment, we
only use the best setting for the other two parameters
as identified in the previous experiment, namely an
initial bias and a propagation step that include all
available nodes, i.e., both articles and categories.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  20  40  60  80  100
P
re
ci
si
on
Top N topics returned
keyRatio= 0.01
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.06
keyRatio= 0.08
keyRatio= 0.16
keyRatio= 0.32
Baseline keyRatio= 0.04
Figure 3: Precision for automatic input text annotations
The system?s best performance occurs for a key
ratio of 0.04 to 0.06, which coincides with the ratio
found to be optimal in previous experiments using
the Wikify! system (Mihalcea and Csomai, 2007).
Overall, the system manages to find many relevant
topics for the documents in the evaluation data set,
despite the large number of candidate topics (more
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0  20  40  60  80  100
R
ec
al
l
Top N topics returned
keyRatio= 0.01
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.06
keyRatio= 0.08
keyRatio= 0.16
keyRatio= 0.32
Baseline keyRatio= 0.04
Figure 4: Recall for automatic input text annotations
than 350,000). Additional experiments performed
against a set of documents from a source other than
Wikipedia are reported in (Coursey et al, 2009).
4 Conclusions
In this paper, we presented an unsupervised system
for automatic topic identification, which relies on a
biased graph centrality algorithm applied on a graph
built from Wikipedia. Our experiments demonstrate
the usefulness of external encyclopedic knowledge
for the task of topic identification.
Acknowledgments
This work has been partially supported by award
#CR72105 from the Texas Higher Education Coor-
dinating Board and by an award from Google Inc.
The authors are grateful to the Waikato group for
making their data set available.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
K. Coursey, R. Mihalcea, and W. Moen. 2009. Using
encyclopedic knowledge for automatic topic identifi-
cation. In Proceedings of the Conference on Natural
Language Learning, Boulder, CO.
T. Haveliwala. 2002. Topic-sensitive PageRank. In
Proceedings of the Eleventh International World Wide
Web Conference, May.
O. Medelyan and I. H. Witten. 2008. Topic indexing
with Wikipedia. In Proceedings of the AAAI WikiAI
workshop.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In Proceedings
of the Sixteenth ACM Conference on Information and
Knowledge Management, Lisbon, Portugal.
120
Graph-based Ranking Algorithms for Sentence Extraction,
Applied to Text Summarization
Rada Mihalcea
Department of Computer Science
University of North Texas
rada@cs.unt.edu
Abstract
This paper presents an innovative unsupervised
method for automatic sentence extraction using graph-
based ranking algorithms. We evaluate the method in
the context of a text summarization task, and show
that the results obtained compare favorably with pre-
viously published results on established benchmarks.
1 Introduction
Graph-based ranking algorithms, such as Klein-
berg?s HITS algorithm (Kleinberg, 1999) or Google?s
PageRank (Brin and Page, 1998), have been tradition-
ally and successfully used in citation analysis, social
networks, and the analysis of the link-structure of the
World Wide Web. In short, a graph-based ranking al-
gorithm is a way of deciding on the importance of a
vertex within a graph, by taking into account global in-
formation recursively computed from the entire graph,
rather than relying only on local vertex-specific infor-
mation.
A similar line of thinking can be applied to lexical
or semantic graphs extracted from natural language
documents, resulting in a graph-based ranking model
called TextRank (Mihalcea and Tarau, 2004), which
can be used for a variety of natural language process-
ing applications where knowledge drawn from an en-
tire text is used in making local ranking/selection de-
cisions. Such text-oriented ranking methods can be
applied to tasks ranging from automated extraction
of keyphrases, to extractive summarization and word
sense disambiguation (Mihalcea et al, 2004).
In this paper, we investigate a range of graph-
based ranking algorithms, and evaluate their applica-
tion to automatic unsupervised sentence extraction in
the context of a text summarization task. We show
that the results obtained with this new unsupervised
method are competitive with previously developed
state-of-the-art systems.
2 Graph-Based Ranking Algorithms
Graph-based ranking algorithms are essentially a way
of deciding the importance of a vertex within a graph,
based on information drawn from the graph structure.
In this section, we present three graph-based ranking
algorithms ? previously found to be successful on a
range of ranking problems. We also show how these
algorithms can be adapted to undirected or weighted
graphs, which are particularly useful in the context of
text-based ranking applications.
Let G = (V,E) be a directed graph with the set of
vertices V and set of edges E, where E is a subset
of V ? V . For a given vertex Vi, let In(Vi) be the
set of vertices that point to it (predecessors), and let
Out(Vi) be the set of vertices that vertex Vi points to
(successors).
2.1 HITS
HITS (Hyperlinked Induced Topic Search) (Klein-
berg, 1999) is an iterative algorithm that was designed
for ranking Web pages according to their degree of
?authority?. The HITS algorithm makes a distinction
between ?authorities? (pages with a large number of
incoming links) and ?hubs? (pages with a large num-
ber of outgoing links). For each vertex, HITS pro-
duces two sets of scores ? an ?authority? score, and a
?hub? score:
HITSA(Vi) =
?
Vj?In(Vi)
HITSH(Vj) (1)
HITSH(Vi) =
?
Vj?Out(Vi)
HITSA(Vj) (2)
2.2 Positional Power Function
Introduced by (Herings et al, 2001), the positional
power function is a ranking algorithm that determines
the score of a vertex as a function that combines both
the number of its successors, and the score of its suc-
cessors.
POSP (Vi) = 1|V |
?
Vj?Out(Vi)
(1 + POSP (Vj)) (3)
The counterpart of the positional power function is
the positional weakness function, defined as:
POSW (Vi) =
1
|V |
?
Vj?In(Vi)
(1 + POSW (Vj)) (4)
2.3 PageRank
PageRank (Brin and Page, 1998) is perhaps one of the
most popular ranking algorithms, and was designed as
a method for Web link analysis. Unlike other ranking
algorithms, PageRank integrates the impact of both in-
coming and outgoing links into one single model, and
therefore it produces only one set of scores:
PR(Vi) = (1 ? d) + d ?
?
Vj?In(Vi)
PR(Vj)
|Out(Vj)|
(5)
where d is a parameter that is set between 0 and 1 1.
For each of these algorithms, starting from arbitrary
values assigned to each node in the graph, the compu-
tation iterates until convergence below a given thresh-
old is achieved. After running the algorithm, a score is
associated with each vertex, which represents the ?im-
portance? or ?power? of that vertex within the graph.
Notice that the final values are not affected by the
choice of the initial value, only the number of itera-
tions to convergence may be different.
2.4 Undirected Graphs
Although traditionally applied on directed graphs, re-
cursive graph-based ranking algorithms can be also
applied to undirected graphs, in which case the out-
degree of a vertex is equal to the in-degree of the ver-
tex. For loosely connected graphs, with the number of
edges proportional with the number of vertices, undi-
rected graphs tend to have more gradual convergence
curves. As the connectivity of the graph increases
(i.e. larger number of edges), convergence is usually
achieved after fewer iterations, and the convergence
curves for directed and undirected graphs practically
overlap.
2.5 Weighted Graphs
In the context of Web surfing or citation analysis, it
is unusual for a vertex to include multiple or partial
links to another vertex, and hence the original defini-
tion for graph-based ranking algorithms is assuming
unweighted graphs.
However, in our TextRank model the graphs are
build from natural language texts, and may include
multiple or partial links between the units (vertices)
that are extracted from text. It may be therefore use-
ful to indicate and incorporate into the model the
?strength? of the connection between two vertices Vi
and Vj as a weight wij added to the corresponding
edge that connects the two vertices.
Consequently, we introduce new formulae for
graph-based ranking that take into account edge
weights when computing the score associated with a
vertex in the graph.
1The factor d is usually set at 0.85 (Brin and Page, 1998), and
this is the value we are also using in our implementation.
HITSWA (Vi) =
?
Vj?In(Vi)
wjiHITSWH (Vj) (6)
HITSWH (Vi) =
?
Vj?Out(Vi)
wijHITSWA (Vj) (7)
POSWP (Vi) =
1
|V |
?
Vj?Out(Vi)
(1 + wijPOSWP (Vj)) (8)
POSWW (Vi) =
1
|V |
?
Vj?In(Vi)
(1 + wjiPOSWW (Vj)) (9)
PRW (Vi) = (1? d) + d ?
?
Vj?In(Vi)
wji
PRW (Vj)
?
Vk?Out(Vj)
wkj
(10)
While the final vertex scores (and therefore rank-
ings) for weighted graphs differ significantly as com-
pared to their unweighted alternatives, the number of
iterations to convergence and the shape of the conver-
gence curves is almost identical for weighted and un-
weighted graphs.
3 Sentence Extraction
To enable the application of graph-based ranking al-
gorithms to natural language texts, TextRank starts by
building a graph that represents the text, and intercon-
nects words or other text entities with meaningful re-
lations. For the task of sentence extraction, the goal
is to rank entire sentences, and therefore a vertex is
added to the graph for each sentence in the text.
To establish connections (edges) between sen-
tences, we are defining a ?similarity? relation, where
?similarity? is measured as a function of content over-
lap. Such a relation between two sentences can be
seen as a process of ?recommendation?: a sentence
that addresses certain concepts in a text, gives the
reader a ?recommendation? to refer to other sentences
in the text that address the same concepts, and there-
fore a link can be drawn between any two such sen-
tences that share common content.
The overlap of two sentences can be determined
simply as the number of common tokens between
the lexical representations of the two sentences, or it
can be run through syntactic filters, which only count
words of a certain syntactic category. Moreover,
to avoid promoting long sentences, we are using a
normalization factor, and divide the content overlap
of two sentences with the length of each sentence.
Formally, given two sentences Si and Sj , with a
sentence being represented by the set of Ni words
that appear in the sentence: Si = W i1,W i2, ...,W iNi ,
the similarity of Si and Sj is defined as:
Similarity(Si, Sj) = |Wk|Wk?Si&Wk?Sj |log(|Si|)+log(|Sj |)
The resulting graph is highly connected, with a
weight associated with each edge, indicating the
strength of the connections between various sentence
pairs in the text2. The text is therefore represented as
a weighted graph, and consequently we are using the
weighted graph-based ranking formulae introduced in
Section 2.5. The graph can be represented as: (a) sim-
ple undirected graph; (b) directed weighted graph with
the orientation of edges set from a sentence to sen-
tences that follow in the text (directed forward); or (c)
directed weighted graph with the orientation of edges
set from a sentence to previous sentences in the text
(directed backward).
After the ranking algorithm is run on the graph, sen-
tences are sorted in reversed order of their score, and
the top ranked sentences are selected for inclusion in
the summary.
Figure 1 shows a text sample, and the associated
weighted graph constructed for this text. The figure
also shows sample weights attached to the edges con-
nected to vertex 93, and the final score computed for
each vertex, using the PR formula, applied on an undi-
rected graph. The sentences with the highest rank are
selected for inclusion in the abstract. For this sample
article, sentences with id-s 9, 15, 16, 18 are extracted,
resulting in a summary of about 100 words, which ac-
cording to automatic evaluation measures, is ranked
the second among summaries produced by 15 other
systems (see Section 4 for evaluation methodology).
4 Evaluation
The TextRank sentence extraction algorithm is eval-
uated in the context of a single-document summa-
rization task, using 567 news articles provided dur-
ing the Document Understanding Evaluations 2002
(DUC, 2002). For each article, TextRank generates
a 100-words summary ? the task undertaken by other
systems participating in this single document summa-
rization task.
For evaluation, we are using the ROUGE evaluation
toolkit, which is a method based on Ngram statistics,
found to be highly correlated with human evaluations
(Lin and Hovy, 2003a). Two manually produced ref-
erence summaries are provided, and used in the eval-
uation process4 .
2In single documents, sentences with highly similar content
are very rarely if at all encountered, and therefore sentence redun-
dancy does not have a significant impact on the summarization of
individual texts. This may not be however the case with multiple
document summarization, where a redundancy removal technique
? such as a maximum threshold imposed on the sentence similar-
ity ? needs to be implemented.
3Weights are listed to the right or above the edge they cor-
respond to. Similar weights are computed for each edge in the
graph, but are not displayed due to space restrictions.
4The evaluation is done using the Ngram(1,1) setting of
ROUGE, which was found to have the highest correlation with hu-
man judgments, at a confidence level of 95%. Only the first 100
words in each summary are considered.
10: The storm was approaching from the southeast with sustained winds of 75 mph gusting
      to 92 mph.
11: "There is no need for alarm," Civil Defense Director Eugenio Cabral said in a television 
      alert shortly after midnight Saturday.
12: Cabral said residents of the province of Barahona should closely follow Gilbert?s movement.
13: An estimated 100,000 people live in the province, including 70,000 in the city of Barahona,
      about 125 miles west of Santo Domingo.
14. Tropical storm Gilbert formed in the eastern Carribean and strenghtened into a hurricaine
      Saturday night.
15: The National Hurricaine Center in Miami reported its position at 2 a.m. Sunday at latitude
      16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles
      southeast of Santo Domingo.
16: The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westard
      at 15 mph with a "broad area of cloudiness and heavy weather" rotating around the center 
      of the storm.
17. The weather service issued a flash flood watch for Puerto Rico and the Virgin Islands until
       at least 6 p.m. Sunday.
18: Strong winds associated with the Gilbert brought coastal flooding, strong southeast winds,
      and up to 12 feet to Puerto Rico?s south coast.
19: There were no reports on casualties.
20: San Juan, on the north coast, had heavy rains and gusts Saturday, but they subsided during 
      the night.
21: On Saturday, Hurricane Florence was downgraded to a tropical storm, and its remnants 
      pushed inland from the U.S. Gulf Coast. 
22: Residents returned home, happy to find little damage from 90 mph winds and sheets of rain.
23: Florence, the sixth named storm of the 1988 Atlantic storm season, was the second hurricane.
24: The first, Debby, reached minimal hurricane strength briefly before hitting the Mexican coast
      last month.
8: Santo Domingo, Dominican Republic (AP)
9: Hurricaine Gilbert Swept towrd the Dominican Republic Sunday, and the Civil Defense
    alerted its heavily populated south coast to prepare for high winds, heavy rains, and high seas.
4: BC?Hurricaine Gilbert, 0348
3: BC?HurricaineGilbert, 09?11 339
5: Hurricaine Gilbert heads toward Dominican Coast
6: By Ruddy Gonzalez
7: Associated Press Writer
22
23
0.15
0.30
0.59
0.15
0.14
0.27
0.15
0.16
0.29
0.15
0.35
0.55
0.19
0.15
[1.83]
[1.20]
[0.99]
[0.56]
[0.70]
[0.15]
[0.15]
[0.93]
[0.76]
[1.09][1.36]
[1.65]
[0.70]
[1.58]
[0.80]
[0.15]
[0.84]
[1.02]
[0.70]
24 [0.71][0.50]
21
20
19
18
17
16
15 14 13
12
11
10
9
8
7
6
54
Figure 1: Sample graph build for sentence extraction
from a newspaper article.
We evaluate the summaries produced by TextRank
using each of the three graph-based ranking algo-
rithms described in Section 2. Table 1 shows the re-
sults obtained with each algorithm, when using graphs
that are: (a) undirected, (b) directed forward, or (c) di-
rected backward.
For a comparative evaluation, Table 2 shows the re-
sults obtained on this data set by the top 5 (out of 15)
performing systems participating in the single docu-
ment summarization task at DUC 2002 (DUC, 2002).
It also lists the baseline performance, computed for
100-word summaries generated by taking the first sen-
tences in each article.
Discussion. The TextRank approach to sentence ex-
traction succeeds in identifying the most important
sentences in a text based on information exclusively
Graph
Algorithm Undirected Dir. forward Dir. backward
HITSWA 0.4912 0.4584 0.5023
HITSWH 0.4912 0.5023 0.4584
POSWP 0.4878 0.4538 0.3910
POSWW 0.4878 0.3910 0.4538
PageRank 0.4904 0.4202 0.5008
Table 1: Results for text summarization using Text-
Rank sentence extraction. Graph-based ranking al-
gorithms: HITS, Positional Function, PageRank.
Graphs: undirected, directed forward, directed back-
ward.
Top 5 systems (DUC, 2002)
S27 S31 S28 S21 S29 Baseline
0.5011 0.4914 0.4890 0.4869 0.4681 0.4799
Table 2: Results for single document summarization
for top 5 (out of 15) DUC 2002 systems, and baseline.
drawn from the text itself. Unlike other supervised
systems, which attempt to learn what makes a good
summary by training on collections of summaries built
for other articles, TextRank is fully unsupervised, and
relies only on the given text to derive an extractive
summary.
Among all algorithms, the HITSA and PageRank
algorithms provide the best performance, at par with
the best performing system from DUC 20025. This
proves that graph-based ranking algorithms, previ-
ously found successful in Web link analysis, can be
turned into a state-of-the-art tool for sentence extrac-
tion when applied to graphs extracted from texts.
Notice that TextRank goes beyond the sentence
?connectivity? in a text. For instance, sentence 15 in
the example provided in Figure 1 would not be iden-
tified as ?important? based on the number of connec-
tions it has with other vertices in the graph6, but it is
identified as ?important? by TextRank (and by humans
? according to the reference summaries for this text).
Another important advantage of TextRank is that it
gives a ranking over all sentences in a text ? which
means that it can be easily adapted to extracting very
short summaries, or longer more explicative sum-
maries, consisting of more than 100 words.
5 Related Work
Sentence extraction is considered to be an important
first step for automatic text summarization. As a con-
sequence, there is a large body of work on algorithms
5Notice that rows two and four in Table 1 are in fact redundant,
since the ?hub? (?weakness?) variations of the HITS (Positional)
algorithms can be derived from their ?authority? (?power?) coun-
terparts by reversing the edge orientation in the graphs.
6Only seven edges are incident with vertex 15, less than e.g.
eleven edges incident with vertex 14 ? not selected as ?important?
by TextRank.
for sentence extraction undertaken as part of the DUC
evaluation exercises. Previous approaches include su-
pervised learning (Teufel and Moens, 1997), vectorial
similarity computed between an initial abstract and
sentences in the given document, or intra-document
similarities (Salton et al, 1997). It is also notable the
study reported in (Lin and Hovy, 2003b) discussing
the usefulness and limitations of automatic sentence
extraction for summarization, which emphasizes the
need of accurate tools for sentence extraction, as an
integral part of automatic summarization systems.
6 Conclusions
Intuitively, TextRank works well because it does not
only rely on the local context of a text unit (ver-
tex), but rather it takes into account information re-
cursively drawn from the entire text (graph). Through
the graphs it builds on texts, TextRank identifies con-
nections between various entities in a text, and im-
plements the concept of recommendation. A text unit
recommends other related text units, and the strength
of the recommendation is recursively computed based
on the importance of the units making the recommen-
dation. In the process of identifying important sen-
tences in a text, a sentence recommends another sen-
tence that addresses similar concepts as being useful
for the overall understanding of the text. Sentences
that are highly recommended by other sentences are
likely to be more informative for the given text, and
will be therefore given a higher score.
An important aspect of TextRank is that it does
not require deep linguistic knowledge, nor domain
or language specific annotated corpora, which makes
it highly portable to other domains, genres, or lan-
guages.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web
search engine. Computer Networks and ISDN Systems, 30(1?7).
DUC. 2002. Document understanding conference 2002. http://www-
nlpir.nist.gov/projects/duc/.
P.J. Herings, G. van der Laan, and D. Talman. 2001. Measuring the power
of nodes in digraphs. Technical report, Tinbergen Institute.
J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked environ-
ment. Journal of the ACM, 46(5):604?632.
C.Y. Lin and E.H. Hovy. 2003a. Automatic evaluation of summaries using
n-gram co-occurrence statistics. In Proceedings of Human Language
Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.
C.Y. Lin and E.H. Hovy. 2003b. The potential and limitations of sentence
extraction for summarization. In Proceedings of the HLT/NAACL
Workshop on Automatic Summarization, Edmonton, Canada, May.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing order into texts.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on semantic net-
works, with application to word sense disambiguation. In Proceed-
ings of the 20st International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland, August.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text
structuring and summarization. Information Processing and Manage-
ment, 2(32).
S. Teufel and M. Moens. 1997. Sentence extraction as a classification
task. In ACL/EACL workshop on ?Intelligent and scalable Text sum-
marization?, pages 58?65, Madrid, Spain.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 49?52, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Language Independent Extractive Summarization
Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu
Abstract
We demonstrate TextRank ? a system for
unsupervised extractive summarization that
relies on the application of iterative graph-
based ranking algorithms to graphs encod-
ing the cohesive structure of a text. An im-
portant characteristic of the system is that
it does not rely on any language-specific
knowledge resources or any manually con-
structed training data, and thus it is highly
portable to new languages or domains.
1 Introduction
Given the overwhelming amount of information avail-
able today, on the Web and elsewhere, techniques
for efficient automatic text summarization are essen-
tial to improve the access to such information. Al-
gorithms for extractive summarization are typically
based on techniques for sentence extraction, and at-
tempt to identify the set of sentences that are most
important for the understanding of a given document.
Some of the most successful approaches to extractive
summarization consist of supervised algorithms that
attempt to learn what makes a good summary by train-
ing on collections of summaries built for a relatively
large number of training documents, e.g. (Hirao et
al., 2002), (Teufel and Moens, 1997). However, the
price paid for the high performance of such super-
vised algorithms is their inability to easily adapt to
new languages or domains, as new training data are
required for each new type of data. TextRank (Mi-
halcea and Tarau, 2004), (Mihalcea, 2004) is specifi-
cally designed to address this problem, by using an ex-
tractive summarization technique that does not require
any training data or any language-specific knowledge
sources. TextRank can be effectively applied to the
summarization of documents in different languages
without any modifications of the algorithm and with-
out any requirements for additional data. Moreover,
results from experiments performed on standard data
sets have demonstrated that the performance of Text-
Rank is competitive with that of some of the best sum-
marization systems available today.
2 Extractive Summarization
Ranking algorithms, such as Kleinberg?s HITS al-
gorithm (Kleinberg, 1999) or Google?s PageRank
(Brin and Page, 1998) have been traditionally and suc-
cessfully used in Web-link analysis, social networks,
and more recently in text processing applications. In
short, a graph-based ranking algorithm is a way of de-
ciding on the importance of a vertex within a graph,
by taking into account global information recursively
computed from the entire graph, rather than relying
only on local vertex-specific information. The basic
idea implemented by the ranking model is that of vot-
ing or recommendation. When one vertex links to an-
other one, it is basically casting a vote for that other
vertex. The higher the number of votes that are cast
for a vertex, the higher the importance of the vertex.
These graph ranking algorithms are based on a
random walk model, where a walker takes random
steps on the graph, with the walk being modeled as a
Markov process ? that is, the decision on what edge to
follow is solely based on the vertex where the walker
is currently located. Under certain conditions, this
49
model converges to a stationary distribution of prob-
abilities associated with vertices in the graph, repre-
senting the probability of finding the walker at a cer-
tain vertex in the graph. Based on the Ergodic theorem
for Markov chains (Grimmett and Stirzaker, 1989),
the algorithms are guaranteed to converge if the graph
is both aperiodic and irreducible. The first condition
is achieved for any graph that is a non-bipartite graph,
while the second condition holds for any strongly con-
nected graph. Both these conditions are achieved in
the graphs constructed for the extractive summariza-
tion application implemented in TextRank.
While there are several graph-based ranking algo-
rithms previously proposed in the literature, we fo-
cus on two algorithms, namely PageRank (Brin and
Page, 1998) and HITS (Kleinberg, 1999).
Let G = (V, E) be a directed graph with the set of
vertices V and set of edges E, where E is a subset
of V ? V . For a given vertex Vi, let In(Vi) be the
set of vertices that point to it (predecessors), and let
Out(Vi) be the set of vertices that vertex Vi points to
(successors).
2.1 PageRank
PageRank (Brin and Page, 1998) is perhaps one
of the most popular ranking algorithms, and was
designed as a method for Web link analysis. Un-
like other graph ranking algorithms, PageRank inte-
grates the impact of both incoming and outgoing links
into one single model, and therefore it produces only
one set of scores:
PR(Vi) = (1 ? d) + d ?
?
Vj?In(Vi)
PR(Vj)
|Out(Vj)|
(1)
where d is a parameter that is set between 0 and 1,
and has the role of integrating random jumps into the
random walking model.
2.2 HITS
HITS (Hyperlinked Induced Topic Search) (Klein-
berg, 1999) is an iterative algorithm that was designed
for ranking Web pages according to their degree of
?authority?. The HITS algorithm makes a distinc-
tion between ?authorities? (pages with a large num-
ber of incoming links) and ?hubs? (pages with a large
number of outgoing links). For each vertex, HITS
produces two sets of scores ? an ?authority? score, and
a ?hub? score:
HITSA(Vi) =
?
Vj?In(Vi)
HITSH(Vj) (2)
HITSH(Vi) =
?
Vj?Out(Vi)
HITSA(Vj) (3)
Starting from arbitrary values assigned to each node
in the graph, the ranking algorithm iterates until con-
vergence below a given threshold is achieved. After
running the algorithm, a score is associated with each
vertex, which represents the importance of that ver-
tex within the graph. Note that the final values are
not affected by the choice of the initial value, only the
number of iterations to convergence may be different.
When the graphs are built starting with natural lan-
guage texts, it may be useful to integrate into the graph
model the strength of the connection between two ver-
tices Vi and Vj , indicated as a weight wij added to
the corresponding edge. Consequently, the ranking
algorithm is adapted to include edge weights, e.g. for
PageRank the score is determined using the follow-
ing formula (a similar change can be applied to the
HITS algorithm):
PRW (Vi) = (1?d)+d?
?
Vj?In(Vi)
wji
PRW (Vj)
?
Vk?Out(Vj)
wkj
(4)
While the final vertex scores (and therefore rank-
ings) for weighted graphs differ significantly as com-
pared to their unweighted alternatives, the number of
iterations to convergence and the shape of the con-
vergence curves is almost identical for weighted and
unweighted graphs.
For the task of single-document extractive summa-
rization, the goal is to rank the sentences in a given
text with respect to their importance for the overall
understanding of the text. A graph is therefore con-
structed by adding a vertex for each sentence in the
text, and edges between vertices are established us-
ing sentence inter-connections, defined using a simple
similarity relation measured as a function of content
overlap. Such a relation between two sentences can be
seen as a process of recommendation: a sentence that
addresses certain concepts in a text gives the reader
a recommendation to refer to other sentences in the
50
text that address the same concepts, and therefore a
link can be drawn between any two such sentences
that share common content.
The overlap of two sentences can be determined
simply as the number of common tokens between the
lexical representations of the two sentences, or it can
be run through filters that e.g. eliminate stopwords,
count only words of a certain category, etc. Moreover,
to avoid promoting long sentences, we use a normal-
ization factor and divide the content overlap of two
sentences with the length of each sentence.
The resulting graph is highly connected, with a
weight associated with each edge, indicating the
strength of the connections between various sentence
pairs in the text. The graph can be represented as: (a)
simple undirected graph; (b) directed weighted graph
with the orientation of edges set from a sentence to
sentences that follow in the text (directed forward);
or (c) directed weighted graph with the orientation of
edges set from a sentence to previous sentences in the
text (directed backward).
After the ranking algorithm is run on the graph,
sentences are sorted in reversed order of their score,
and the top ranked sentences are selected for inclu-
sion in the summary. Figure 1 shows an example of a
weighted graph built for a short sample text.
[1] Watching the new movie, ?Imagine: John Lennon,? was very
painful for the late Beatle?s wife, Yoko Ono.
[2] ?The only reason why I did watch it to the end is because I?m
responsible for it, even though somebody else made it,? she said.
[3] Cassettes, film footage and other elements of the acclaimed
movie were collected by Ono.
[4] She also took cassettes of interviews by Lennon, which were
edited in such a way that he narrates the picture.
[5] Andrew Solt (?This Is Elvis?) directed, Solt and David L.
Wolper produced and Solt and Sam Egan wrote it.
[6] ?I think this is really the definitive documentary of John
Lennon?s life,? Ono said in an interview.
3 Evaluation
English document summarization experiments are run
using the summarization test collection provided in
the framework of the Document Understanding Con-
ference (DUC). In particular, we use the data set of
567 news articles made available during the DUC
2002 evaluations (DUC, 2002), and the correspond-
ing 100-word summaries generated for each of these
documents. This is the single document summariza-
tion task undertaken by other systems participating in
1
2
3
4
5
6
0.16
0.30
0.46 0.15
[1.34]
[1.75]
[0.70]
[0.74]
[0.52]
[0.91]
0.15
0.29
0.32
0.15
Figure 1: Graph of sentence similarities built on a
sample text. Scores reflecting sentence importance are
shown in brackets next to each sentence.
the DUC 2002 document summarization evaluations.
To test the language independence aspect of the al-
gorithm, in addition to the English test collection, we
also use a Brazilian Portuguese data set consisting of
100 news articles and their corresponding manually
produced summaries. We use the TeMa?rio test col-
lection (Pardo and Rino, 2003), containing newspa-
per articles from online Brazilian newswire: 40 docu-
ments from Jornal de Brasil and 60 documents from
Folha de Sa?o Paulo. The documents were selected to
cover a variety of domains (e.g. world, politics, for-
eign affairs, editorials), and manual summaries were
produced by an expert in Brazilian Portuguese. Unlike
the summaries produced for the English DUC docu-
ments ? which had a length requirement of approxi-
mately 100 words, the length of the summaries in the
TeMa?rio data set is constrained relative to the length
of the corresponding documents, i.e. a summary has
to account for about 25-30% of the original document.
Consequently, the automatic summaries generated for
the documents in this collection are not restricted to
100 words, as in the English experiments, but are re-
quired to have a length comparable to the correspond-
ing manual summaries, to ensure a fair evaluation.
For evaluation, we are using the ROUGE evaluation
toolkit1, which is a method based on Ngram statistics,
found to be highly correlated with human evaluations
(Lin and Hovy, 2003). The evaluation is done using
the Ngram(1,1) setting of ROUGE, which was found
to have the highest correlation with human judgments,
at a confidence level of 95%.
Table 2 shows the results obtained on these two data
sets for different graph settings. The table also lists
baseline results, obtained on summaries generated by
1ROUGE is available at http://www.isi.edu/?cyl/ROUGE/.
51
Graph
Algorithm Undirected Forward Backward
HITSWA 0.4912 0.4584 0.5023
HITSWH 0.4912 0.5023 0.4584
PageRankW 0.4904 0.4202 0.5008
Baseline 0.4799
Table 1: English single-document summarization.
Graph
Algorithm Undirected Forward Backward
HITSWA 0.4814 0.4834 0.5002
HITSWH 0.4814 0.5002 0.4834
PageRankW 0.4939 0.4574 0.5121
Baseline 0.4963
Table 2: Portuguese single-document summarization.
taking the first sentences in each document. By ways
of comparison, the best participating system in DUC
2002 was a supervised system that led to a ROUGE
score of 0.5011.
For both data sets, TextRank applied on a directed
backward graph structure exceeds the performance
achieved through a simple (but powerful) baseline.
These results prove that graph-based ranking algo-
rithms, previously found successful in Web link anal-
ysis and social networks, can be turned into a state-
of-the-art tool for extractive summarization when ap-
plied to graphs extracted from texts. Moreover, due
to its unsupervised nature, the algorithm was also
shown to be language independent, leading to similar
results and similar improvements over baseline tech-
niques when applied on documents in different lan-
guages. More extensive experimental results with the
TextRank system are reported in (Mihalcea and Tarau,
2004), (Mihalcea, 2004).
4 Conclusion
Intuitively, iterative graph-based ranking algorithms
work well on the task of extractive summarization be-
cause they do not only rely on the local context of a
text unit (vertex), but they also take into account infor-
mation recursively drawn from the entire text (graph).
Through the graphs it builds on texts, a graph-based
ranking algorithm identifies connections between var-
ious entities in a text, and implements the concept of
recommendation. In the process of identifying impor-
tant sentences in a text, a sentence recommends other
sentences that address similar concepts as being use-
ful for the overall understanding of the text. Sentences
that are highly recommended by other sentences are
likely to be more informative for the given text, and
will be therefore given a higher score.
An important aspect of the graph-based extractive
summarization method is that it does not require deep
linguistic knowledge, nor domain or language specific
annotated corpora, which makes it highly portable to
other domains, genres, or languages.
Acknowledgments
We are grateful to Lucia Helena Machado Rino for
making available the TeMa?rio summarization test col-
lection and for her help with this data set.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
DUC. 2002. Document understanding conference 2002.
http://www-nlpir.nist.gov/projects/duc/.
G. Grimmett and D. Stirzaker. 1989. Probability and Ran-
dom Processes. Oxford University Press.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002. Ntt?s
text summarization system for duc-2002. In Proceed-
ings of the Document Understanding Conference 2002.
J.M. Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604?
632.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing order
into texts. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing (EMNLP
2004), Barcelona, Spain.
R. Mihalcea. 2004. Graph-based ranking algorithms for
sentence extraction, applied to text summarization. In
Proceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Lingusitics (ACL 2004) (com-
panion volume), Barcelona, Spain.
T.A.S. Pardo and L.H.M. Rino. 2003. TeMario: a cor-
pus for automatic text summarization. Technical report,
NILC-TR-03-09.
S. Teufel and M. Moens. 1997. Sentence extraction as a
classification task. In ACL/EACL workshop on ?Intel-
ligent and scalable Text summarization?, pages 58?65,
Madrid, Spain.
52
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 53?56, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseLearner: Word Sense Disambiguation
for All Words in Unrestricted Text
Rada Mihalcea and Andras Csomai
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu, ac0225@unt.edu
Abstract
This paper describes SENSELEARNER ? a
minimally supervised word sense disam-
biguation system that attempts to disam-
biguate all content words in a text using
WordNet senses. We evaluate the accu-
racy of SENSELEARNER on several stan-
dard sense-annotated data sets, and show
that it compares favorably with the best re-
sults reported during the recent SENSEVAL
evaluations.
1 Introduction
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a polyse-
mous word within a given context. Applications such
as machine translation, knowledge acquisition, com-
mon sense reasoning, and others, require knowledge
about word meanings, and word sense disambiguation
is considered essential for all these applications.
Most of the efforts in solving this problem were
concentrated so far toward targeted supervised learn-
ing, where each sense tagged occurrence of a particu-
lar word is transformed into a feature vector, which is
then used in an automatic learning process. The appli-
cability of such supervised algorithms is however lim-
ited only to those few words for which sense tagged
data is available, and their accuracy is strongly con-
nected to the amount of labeled data available at hand.
Instead, methods that address all words in unre-
stricted text have received significantly less attention.
While the performance of such methods is usually
exceeded by their supervised lexical-sample alterna-
tives, they have however the advantage of providing
larger coverage.
In this paper, we present a method for solving the
semantic ambiguity of all content words in a text. The
algorithm can be thought of as a minimally supervised
word sense disambiguation algorithm, in that it uses
a relatively small data set for training purposes, and
generalizes the concepts learned from the training data
to disambiguate the words in the test data set. As a
result, the algorithm does not need a separate classi-
fier for each word to be disambiguated, but instead it
learns global models for general word categories.
2 Background
For some natural language processing tasks, such as
part of speech tagging or named entity recognition,
regardless of the approach considered, there is a con-
sensus on what makes a successful algorithm. Instead,
no such consensus has been reached yet for the task
of word sense disambiguation, and previous work has
considered a range of knowledge sources, such as lo-
cal collocational clues, common membership in se-
mantically or topically related word classes, semantic
density, and others.
In recent SENSEVAL-3 evaluations, the most suc-
cessful approaches for all words word sense disam-
biguation relied on information drawn from annotated
corpora. The system developed by (Decadt et al,
2004) uses two cascaded memory-based classifiers,
combined with the use of a genetic algorithm for joint
parameter optimization and feature selection. A sep-
arate ?word expert? is learned for each ambiguous
word, using a concatenated corpus of English sense-
53
New raw
text
Feature vector
construction
(POS, NE, MWE)
Preprocessing 
Semantic model
learning
Sense?tagged
text
semantic models
SenseLearner
definitions
Word sense
disambiguation
Trained semantic
models
Sense?tagged
texts
Figure 1: Semantic model learning in SENSE-
LEARNER
tagged texts, including SemCor, SENSEVAL data sets,
and a corpus built from WordNet examples. The per-
formance of this system on the SENSEVAL-3 English
all words data set was evaluated at 65.2%.
Another top ranked system is the one developed by
(Yuret, 2004), which combines two Naive Bayes sta-
tistical models, one based on surrounding collocations
and another one based on a bag of words around the
target word. The statistical models are built based on
SemCor and WordNet, for an overall disambiguation
accuracy of 64.1%.
A different version of our own SENSELEARNER
system (Mihalcea and Faruque, 2004), using three of
the semantic models described in this paper, combined
with semantic generalizations based on syntactic de-
pendencies, achieved a performance of 64.6%.
3 SenseLearner
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm general
enough to be able to disambiguate as many content
words as possible in a text, and efficient enough so
that large amounts of text can be annotated in real
time. SENSELEARNER is attempting to learn general
semantic models for various word categories, starting
with a relatively small sense-annotated corpus. We
base our experiments on SemCor (Miller et al, 1993),
a balanced, semantically annotated dataset, with all
content words manually tagged by trained lexicogra-
phers.
The input to the disambiguation algorithm consists
of raw text. The output is a text with word meaning
annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with part-of-
speech tags; collocations are identified using a sliding
window approach, where a collocation is defined as
a sequence of words that forms a compound concept
defined in WordNet (Miller, 1995); named entities are
also identified at this stage1.
Next, a semantic model is learned for all predefined
word categories, which are defined as groups of words
that share some common syntactic or semantic prop-
erties. Word categories can be of various granulari-
ties. For instance, using the SENSELEARNER learn-
ing mechanism, a model can be defined and trained to
handle all the nouns in the test corpus. Similarly, us-
ing the same mechanism, a finer-grained model can be
defined to handle all the verbs for which at least one
of the meanings is of type <move>. Finally, small
coverage models that address one word at a time, for
example a model for the adjective small, can be also
defined within the same framework. Once defined and
trained, the models are used to annotate the ambigu-
ous words in the test corpus with their corresponding
meaning. Section 4 below provides details on the vari-
ous models that are currently implemented in SENSE-
LEARNER, and information on how new models can
be added to the SENSELEARNER framework.
Note that the semantic models are applicable only
to: (1) words that are covered by the word category
defined in the models; and (2) words that appeared at
least once in the training corpus. The words that are
not covered by these models (typically about 10-15%
of the words in the test corpus) are assigned with the
most frequent sense in WordNet.
An alternative solution to this second step was sug-
gested in (Mihalcea and Faruque, 2004), using seman-
tic generalizations learned from dependencies identi-
fied between nodes in a conceptual network. Their
approach however, although slightly more accurate,
conflicted with our goal of creating an efficient WSD
system, and therefore we opted for the simpler back-
off method that employs WordNet sense frequencies.
1We only identify persons, locations, and groups, which are
the named entities specifically identified in SemCor.
54
4 Semantic Models
Different semantic models can be defined and trained
for the disambiguation of different word categories.
Although more general than models that are built in-
dividually for each word in a test corpus (Decadt et
al., 2004), the applicability of the semantic models
built as part of SENSELEARNER is still limited to
those words previously seen in the training corpus,
and therefore their overall coverage is not 100%.
Starting with an annotated corpus consisting of all
annotated files in SemCor, a separate training data set
is built for each model. There are seven models pro-
vided with the current SENSELEARNER distribution,
implementing the following features:
4.1 Noun Models
modelNN1: A contextual model that relies on the first
noun, verb, or adjective before the target noun, and
their corresponding part-of-speech tags.
modelNNColl: A collocation model that implements
collocation-like features based on the first word to the
left and the first word to the right of the target noun.
4.2 Verb Models
modelVB1 A contextual model that relies on the first
word before and the first word after the target verb,
and their part-of-speech tags.
modelVBColl A collocation model that implements
collocation-like features based on the first word to the
left and the first word to the right of the target verb.
4.3 Adjective Models
modelJJ1 A contextual model that relies on the first
noun after the target adjective.
modelJJ2 A contextual model that relies on the first
word before and the first word after the target adjec-
tive, and their part-of-speech tags.
modelJJColl A collocation model that implements
collocation-like features using the first word to the left
and the first word to the right of the target adjective.
4.4 Defining New Models
New models can be easily defined and trained fol-
lowing the same SENSELEARNER learning method-
ology. In fact, the current distribution of SENSE-
LEARNER includes a template for the subroutine re-
quired to define a new semantic model, which can be
easily adapted to handle new word categories.
4.5 Applying Semantic Models
In the training stage, a feature vector is constructed
for each sense-annotated word covered by a semantic
model. The features are model-specific, and feature
vectors are added to the training set pertaining to the
corresponding model. The label of each such feature
vector consists of the target word and the correspond-
ing sense, represented as word#sense. Table 1 shows
the number of feature vectors constructed in this learn-
ing stage for each semantic model.
To annotate new text, similar vectors are created for
all content-words in the raw text. Similar to the train-
ing stage, feature vectors are created and stored sepa-
rately for each semantic model.
Next, word sense predictions are made for all test
examples, with a separate learning process run for
each semantic model. For learning, we are using the
Timbl memory based learning algorithm (Daelemans
et al, 2001), which was previously found useful for
the task of word sense disambiguation (Hoste et al,
2002), (Mihalcea, 2002).
Following the learning stage, each vector in the test
data set is labeled with a predicted word and sense.
If several models are simultaneously used for a given
test instance, then all models have to agree in the la-
bel assigned, for a prediction to be made. If the word
predicted by the learning algorithm coincides with the
target word in the test feature vector, then the pre-
dicted sense is used to annotate the test instance. Oth-
erwise, if the predicted word is different than the tar-
get word, no annotation is produced, and the word is
left for annotation in a later stage.
5 Evaluation
The SENSELEARNER system was evaluated on the
SENSEVAL-2 and SENSEVAL-3 English all words
data sets, each data set consisting of three texts from
the Penn Treebank corpus annotated with WordNet
senses. The SENSEVAL-2 corpus includes a total of
2,473 annotated content words, and the SENSEVAL-
3 corpus includes annotations for an additional set
of 2,081 words. Table 1 shows precision and recall
figures obtained with each semantic model on these
two data sets. A baseline, computed using the most
frequent sense in WordNet, is also indicated. The
best results reported on these data sets are 69.0% on
SENSEVAL-2 data (Mihalcea and Moldovan, 2002),
55
Training SENSEVAL-2 SENSEVAL-3
Model size Precision Recall Precision Recall
modelNN1 88058 0.6910 0.3257 0.6624 0.3027
modelNNColl 88058 0.7130 0.3360 0.6813 0.3113
modelVB1 48328 0.4629 0.1037 0.5352 0.1931
modelVBColl 48328 0.4685 0.1049 0.5472 0.1975
modelJJ1 35664 0.6525 0.1215 0.6648 0.1162
modelJJ2 35664 0.6503 0.1211 0.6593 0.1153
modelJJColl 35664 0.6792 0.1265 0.6703 0.1172
model*1/2 207714 0.6481 0.6481 0.6184 0.6184
model*Coll 172050 0.6622 0.6622 0. 6328 0.6328
Baseline 63.8% 63.8% 60.9% 60.9%
Table 1: Precision and recall for the SENSELEARNER
semantic models, measured on the SENSEVAL-2 and
SENSEVAL-3 English all words data. Results for com-
binations of contextual (model*1/2) and collocational
(model*Coll) models are also included.
and 65.2% on SENSEVAL-3 data (Decadt et al, 2004).
Note however that both these systems rely on signifi-
cantly larger training data sets, and thus the results are
not directly comparable.
In addition, we also ran an experiment where a sep-
arate model was created for each individual word in
the test data, with a back-off method using the most
frequent sense in WordNet when no training exam-
ples were found in SEMCOR. This resulted into sig-
nificantly higher complexity, with a very large num-
ber of models (about 900?1000 models for each of
the SENSEVAL-2 and SENSEVAL-3 data sets), while
the performance did not exceed the one obtained with
the more general semantic models.
The average disambiguation precision obtained
with SENSELEARNER improves significantly over the
simple but competitive baseline that selects by de-
fault the ?most frequent sense? from WordNet. Not
surprisingly, the verbs seem to be the most difficult
word class, which is most likely explained by the large
number of senses defined in WordNet for this part of
speech.
6 Conclusion
In this paper, we described and evaluated an efficient
algorithm for minimally supervised word-sense dis-
ambiguation that attempts to disambiguate all content
words in a text using WordNet senses. The results ob-
tained on both SENSEVAL-2 and SENSEVAL-3 data
sets are found to significantly improve over the sim-
ple but competitive baseline that chooses by default
the most frequent sense, and are proved competitive
with the best published results on the same data sets.
SENSELEARNER is publicly available for download
at http://lit.csci.unt.edu/?senselearner.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation grant IIS-0336793.
References
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner,
version 4.0, reference guide. Technical report, Univer-
sity of Antwerp.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. Gambl, genetic algorithm optimization of
memory-based wsd. In Senseval-3: Third International
Workshop on the Evaluation of Systems for the Semantic
Analysis of Text, Barcelona, Spain, July.
V. Hoste, W. Daelemans, I. Hendrickx, and A. van den
Bosch. 2002. Evaluating the results of a memory-based
word-expert approach to unrestricted word sense dis-
ambiguation. In Proceedings of the ACL Workshop on
?Word Sense Disambiguatuion: Recent Successes and
Future Directions?, Philadelphia, July.
R. Mihalcea and E. Faruque. 2004. SenseLearner: Min-
imally supervised word sense disambiguation for all
words in open text. In Proceedings of ACL/SIGLEX
Senseval-3, Barcelona, Spain, July.
R. Mihalcea and D. Moldovan. 2002. Pattern learning
and active feature selection for word sense disambigua-
tion. In Senseval 2001, ACL Workshop, pages 127?130,
Toulouse, France, July.
R. Mihalcea. 2002. Instance based learning with automatic
feature selection applied to Word Sense Disambiguation.
In Proceedings of the 19th International Conference on
Computational Linguistics (COLING 2002), Taipei, Tai-
wan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
pages 303?308, Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Communi-
cation of the ACM, 38(11):39?41.
D. Yuret. 2004. Some experiments with a naive bayes wsd
system. In Senseval-3: Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain, July.
56
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1065?1072,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Sense and Subjectivity
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
rada@cs.unt.edu
Abstract
Subjectivity and meaning are both impor-
tant properties of language. This paper ex-
plores their interaction, and brings empir-
ical evidence in support of the hypotheses
that (1) subjectivity is a property that can
be associated with word senses, and (2)
word sense disambiguation can directly
benefit from subjectivity annotations.
1 Introduction
There is growing interest in the automatic extrac-
tion of opinions, emotions, and sentiments in text
(subjectivity), to provide tools and support for var-
ious NLP applications. Similarly, there is continu-
ous interest in the task of word sense disambigua-
tion, with sense-annotated resources being devel-
oped for many languages, and a growing num-
ber of research groups participating in large-scale
evaluations such as SENSEVAL.
Though both of these areas are concerned with
the semantics of a text, over time there has been
little interaction, if any, between them. In this pa-
per, we address this gap, and explore possible in-
teractions between subjectivity and word sense.
There are several benefits that would motivate
such a joint exploration. First, at the resource
level, the augmentation of lexical resources such
as WordNet (Miller, 1995) with subjectivity labels
could support better subjectivity analysis tools,
and principled methods for refining word senses
and clustering similar meanings. Second, at the
tool level, an explicit link between subjectivity and
word sense could help improve methods for each,
by integrating features learned from one into the
other in a pipeline approach, or through joint si-
multaneous learning.
In this paper we address two questions about
word sense and subjectivity. First, can subjectiv-
ity labels be assigned to word senses? To address
this question, we perform two studies. The first
(Section 3) investigates agreement between anno-
tators who manually assign the labels subjective,
objective, or both to WordNet senses. The second
study (Section 4) evaluates a method for automatic
assignment of subjectivity labels to word senses.
We devise an algorithm relying on distributionally
similar words to calculate a subjectivity score, and
show how it can be used to automatically assess
the subjectivity of a word sense.
Second, can automatic subjectivity analysis be
used to improve word sense disambiguation? To
address this question, the output of a subjectivity
sentence classifier is input to a word-sense disam-
biguation system, which is in turn evaluated on the
nouns from the SENSEVAL-3 English lexical sam-
ple task (Section 5). The results of this experiment
show that a subjectivity feature can significantly
improve the accuracy of a word sense disambigua-
tion system for those words that have both subjec-
tive and objective senses.
A third obvious question is, can word sense dis-
ambiguation help automatic subjectivity analysis?
However, due to space limitations, we do not ad-
dress this question here, but rather leave it for fu-
ture work.
2 Background
Subjective expressions are words and phrases
being used to express opinions, emotions, evalu-
ations, speculations, etc. (Wiebe et al, 2005). A
general covering term for such states is private
state, ?a state that is not open to objective obser-
1065
vation or verification? (Quirk et al, 1985).1 There
are three main types of subjective expressions:2
(1) references to private states:
His alarm grew.
He absorbed the information quickly.
He was boiling with anger.
(2) references to speech (or writing) events ex-
pressing private states:
UCC/Disciples leaders roundly con-
demned the Iranian President?s verbal
assault on Israel.
The editors of the left-leaning paper at-
tacked the new House Speaker.
(3) expressive subjective elements:
He would be quite a catch.
What?s the catch?
That doctor is a quack.
Work on automatic subjectivity analysis falls
into three main areas. The first is identifying
words and phrases that are associated with sub-
jectivity, for example, that think is associated with
private states and that beautiful is associated with
positive sentiments (e.g., (Hatzivassiloglou and
McKeown, 1997; Wiebe, 2000; Kamps and Marx,
2002; Turney, 2002; Esuli and Sebastiani, 2005)).
Such judgments are made for words. In contrast,
our end task (in Section 4) is to assign subjectivity
labels to word senses.
The second is subjectivity classification of sen-
tences, clauses, phrases, or word instances in the
context of a particular text or conversation, ei-
ther subjective/objective classifications or posi-
tive/negative sentiment classifications (e.g.,(Riloff
and Wiebe, 2003; Yu and Hatzivassiloglou, 2003;
Dave et al, 2003; Hu and Liu, 2004)).
The third exploits automatic subjectivity anal-
ysis in applications such as review classification
(e.g., (Turney, 2002; Pang and Lee, 2004)), min-
ing texts for product reviews (e.g., (Yi et al, 2003;
Hu and Liu, 2004; Popescu and Etzioni, 2005)),
summarization (e.g., (Kim and Hovy, 2004)), in-
formation extraction (e.g., (Riloff et al, 2005)),
1Note that sentiment, the focus of much recent work in the
area, is a type of subjectivity, specifically involving positive
or negative opinion, emotion, or evaluation.
2These distinctions are not strictly needed for this paper,
but may help the reader appreciate the examples given below.
and question answering (e.g., (Yu and Hatzivas-
siloglou, 2003; Stoyanov et al, 2005)).
Most manual subjectivity annotation research
has focused on annotating words, out of context
(e.g., (Heise, 2001)), or sentences and phrases in
the context of a text or conversation (e.g., (Wiebe
et al, 2005)). The new annotations in this pa-
per are instead targeting the annotation of word
senses.
3 Human Judgment of Word Sense
Subjectivity
To explore our hypothesis that subjectivity may
be associated with word senses, we developed a
manual annotation scheme for assigning subjec-
tivity labels to WordNet senses,3 and performed
an inter-annotator agreement study to assess its
reliability. Senses are classified as S(ubjective),
O(bjective), or B(oth). Classifying a sense as S
means that, when the sense is used in a text or con-
versation, we expect it to express subjectivity; we
also expect the phrase or sentence containing it to
be subjective.
We saw a number of subjective expressions in
Section 2. A subset is repeated here, along with
relevant WordNet senses. In the display of each
sense, the first part shows the synset, gloss, and
any examples. The second part (marked with =>)
shows the immediate hypernym.
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in
anticipation of some specific pain or danger (usually ac-
companied by a desire to flee or fight))
He was boiling with anger.
seethe, boil ? (be in an agitated emotional state; ?The cus-
tomer was seething with anger?)
=> be ? (have the quality of being; (copula, used with an
adjective or a predicate noun); ?John is rich?; ?This is not
a good answer?)
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
That doctor is a quack.
quack ? (an untrained person who pretends to be a physician
and who dispenses medical advice)
=> doctor, doc, physician, MD, Dr., medico
Before specifying what we mean by an objec-
tive sense, we give examples.
3All our examples and data used in the experiments are
from WordNet 2.0.
1066
The alarm went off.
alarm, warning device, alarm system ? (a device that signals
the occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particu-
lar purpose; ?the device is small enough to wear on your
wrist?; ?a device intended to conserve water?)
The water boiled.
boil ? (come to the boiling point and change from a liquid to
vapor; ?Water boils at 100 degrees Celsius?)
=> change state, turn ? (undergo a transformation or a
change of position or action; ?We turned from Socialism
to Capitalism?; ?The people turned against the President
when he stole the election?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was
only 10 fish?)
=> indefinite quantity ? (an estimated quantity)
The duck?s quack was loud and brief.
quack ? (the harsh sound of a duck)
=> sound ? (the sudden occurrence of an audible event;
?the sound awakened them?)
While we expect phrases or sentences contain-
ing subjective senses to be subjective, we do not
necessarily expect phrases or sentences containing
objective senses to be objective. Consider the fol-
lowing examples:
Will someone shut that damn alarm off?
Can?t you even boil water?
While these sentences contain objective senses
of alarm and boil, the sentences are subjective
nonetheless. But they are not subjective due to
alarm and boil, but rather to punctuation, sentence
forms, and other words in the sentence. Thus, clas-
sifying a sense as O means that, when the sense is
used in a text or conversation, we do not expect
it to express subjectivity and, if the phrase or sen-
tence containing it is subjective, the subjectivity is
due to something else.
Finally, classifying a sense as B means it covers
both subjective and objective usages, e.g.:
absorb, suck, imbibe, soak up, sop up, suck up, draw, take in,
take up ? (take in, also metaphorically; ?The sponge absorbs
water well?; ?She drew strength from the minister?s words?)
Manual subjectivity judgments were added to
a total of 354 senses (64 words). One annotator,
Judge 1 (a co-author), tagged all of them. A sec-
ond annotator (Judge 2, who is not a co-author)
tagged a subset for an agreement study, presented
next.
3.1 Agreement Study
For the agreement study, Judges 1 and 2 indepen-
dently annotated 32 words (138 senses). 16 words
have both S and O senses and 16 do not (according
to Judge 1). Among the 16 that do not have both
S and O senses, 8 have only S senses and 8 have
only O senses. All of the subsets are balanced be-
tween nouns and verbs. Table 1 shows the contin-
gency table for the two annotators? judgments on
this data. In addition to S, O, and B, the annotation
scheme also permits U(ncertain) tags.
S O B U Total
S 39 O O 4 43
O 3 73 2 4 82
B 1 O 3 1 5
U 3 2 O 3 8
Total 46 75 5 12 138
Table 1: Agreement on balanced set (Agreement:
85.5%, ?: 0.74)
Overall agreement is 85.5%, with a Kappa (?)
value of 0.74. For 12.3% of the senses, at least
one annotator?s tag is U. If we consider these cases
to be borderline and exclude them from the study,
percent agreement increases to 95% and ? rises to
0.90. Thus, annotator agreement is especially high
when both are certain.
Considering only the 16-word subset with both
S and O senses (according to Judge 1), ? is .75,
and for the 16-word subset for which Judge 1 gave
only S or only O senses, ? is .73. Thus, the two
subsets are of comparable difficulty.
The two annotators also independently anno-
tated the 20 ambiguous nouns (117 senses) of the
SENSEVAL-3 English lexical sample task used in
Section 5. For this tagging task, U tags were not
allowed, to create a definitive gold standard for the
experiments. Even so, the ? value for them is 0.71,
which is not substantially lower. The distributions
of Judge 1?s tags for all 20 words can be found in
Table 3 below.
We conclude this section with examples of
disagreements that illustrate sources of uncer-
tainty. First, uncertainty arises when subjec-
tive senses are missing from the dictionary.
The labels for the senses of noun assault are
(O:O,O:O,O:O,O:UO).4 For verb assault there is
a subjective sense:
attack, round, assail, lash out, snipe, assault (attack in speech
or writing) ?The editors of the left-leaning paper attacked the
new House Speaker?
However, there is no corresponding sense for
4I.e., the first three were labeled O by both annotators. For
the fourth sense, the second annotator was not sure but was
leaning toward O.
1067
noun assault. A missing sense may lead an anno-
tator to try to see subjectivity in an objective sense.
Second, uncertainty can arise in weighing hy-
pernym against sense. It is fine for a synset to
imply just S or O, while the hypernym implies
both (the synset specializes the more general con-
cept). However, consider the following, which
was tagged (O:UB).
attack ? (a sudden occurrence of an uncontrollable condition;
?an attack of diarrhea?)
=> affliction ? (a cause of great suffering and distress)
While the sense is only about the condition, the
hypernym highlights subjective reactions to the
condition. One annotator judged only the sense
(giving tag O), while the second considered the
hypernym as well (giving tag UB).
4 Automatic Assessment of Word Sense
Subjectivity
Encouraged by the results of the agreement study,
we devised a method targeting the automatic an-
notation of word senses for subjectivity.
The main idea behind our method is that we can
derive information about a word sense based on in-
formation drawn from words that are distribution-
ally similar to the given word sense. This idea re-
lates to the unsupervised word sense ranking algo-
rithm described in (McCarthy et al, 2004). Note,
however, that (McCarthy et al, 2004) used the in-
formation about distributionally similar words to
approximate corpus frequencies for word senses,
whereas we target the estimation of a property of
a given word sense (the ?subjectivity?).
Starting with a given ambiguous word w, we
first find the distributionally similar words using
the method of (Lin, 1998) applied to the automat-
ically parsed texts of the British National Corpus.
Let DSW = dsw1, dsw2, ..., dswn be the list of
top-ranked distributionally similar words, sorted
in decreasing order of their similarity.
Next, for each sense wsi of the word w, we de-
termine the similarity with each of the words in the
list DSW , using a WordNet-based measure of se-
mantic similarity (wnss). Although a large num-
ber of such word-to-word similarity measures ex-
ist, we chose to use the (Jiang and Conrath, 1997)
measure, since it was found both to be efficient
and to provide the best results in previous exper-
iments involving word sense ranking (McCarthy
et al, 2004)5. For distributionally similar words
5Note that unlike the above measure of distributional sim-
Algorithm 1 Word Sense Subjectivity Score
Input: Word sense wi
Input: Distributionally similar words DSW = {dswj |j =
1..n}
Output: Subjectivity score subj(wi)
1: subj(wi) = 0
2: totalsim = 0
3: for j = 1 to n do
4: Instsj = all instances of dswj in the MPQA corpus
5: for k in Instsj do
6: if k is in a subj. expr. in MPQA corpus then
7: subj(wi) += sim(wi,dswj)
8: else if k is not in a subj. expr. in MPQA corpus
then
9: subj(wi) -= sim(wi,dswj)
10: end if
11: totalsim += sim(wi,dswj)
12: end for
13: end for
14: subj(wi) = subj(wi) / totalsim
that are themselves ambiguous, we use the sense
that maximizes the similarity score. The similar-
ity scores associated with each word dswj are nor-
malized so that they add up to one across all possi-
ble senses of w, which results in a score described
by the following formula:
sim(wsi, dswj) = wnss(wsi,dswj)?
i??senses(w)
wnss(wsi? ,dswj)
where
wnss(wsi, dswj) = max
k?senses(dswj)
wnss(wsi, dswkj )
A selection process can also be applied so that
a distributionally similar word belongs only to
one sense. In this case, for a given sense wi we
use only those distributionally similar words with
whom wi has the highest similarity score across all
the senses of w. We refer to this case as similarity-
selected, as opposed to similarity-all, which refers
to the use of all distributionally similar words for
all senses.
Once we have a list of similar words associated
with each sense wsi and the corresponding simi-
larity scores sim(wsi, dswj), we use an annotated
corpus to assign subjectivity scores to the senses.
The corpus we use is the MPQA Opinion Corpus,
which consists of over 10,000 sentences from the
world press annotated for subjective expressions
(all three types of subjective expressions described
in Section 2).6
ilarity which measures similarity between words, rather than
word senses, here we needed a similarity measure that also
takes into account word senses as defined in a sense inven-
tory such as WordNet.
6The MPQA corpus is described in (Wiebe et al, 2005)
and available at www.cs.pitt.edu/mpqa/databaserelease/.
1068
Algorithm 1 is our method for calculating sense
subjectivity scores. The subjectivity score is a
value in the interval [-1,+1] with +1 correspond-
ing to highly subjective and -1 corresponding to
highly objective. It is a sum of sim scores, where
sim(wi,dswj) is added for each instance of dswj
that is in a subjective expression, and subtracted
for each instance that is not in a subjective expres-
sion.
Note that the annotations in the MPQA corpus
are for subjective expressions in context. Thus, the
data is somewhat noisy for our task, because, as
discussed in Section 3, objective senses may ap-
pear in subjective expressions. Nonetheless, we
hypothesized that subjective senses tend to appear
more often in subjective expressions than objec-
tive senses do, and use the appearance of words in
subjective expressions as evidence of sense sub-
jectivity.
(Wiebe, 2000) also makes use of an annotated
corpus, but in a different approach: given a word
w and a set of distributionally similar words DSW,
that method assigns a subjectivity score to w equal
to the conditional probability that any member of
DSW is in a subjective expression. Moreover, the
end task of that work was to annotate words, while
our end task is the more difficult problem of anno-
tating word senses for subjectivity.
4.1 Evaluation
The evaluation of the algorithm is performed
against the gold standard of 64 words (354 word
senses) using Judge 1?s annotations, as described
in Section 3.
For each sense of each word in the set of 64
ambiguous words, we use Algorithm 1 to deter-
mine a subjectivity score. A subjectivity label is
then assigned depending on the value of this score
with respect to a pre-selected threshold. While a
threshold of 0 seems like a sensible choice, we per-
form the evaluation for different thresholds rang-
ing across the [-1,+1] interval, and correspond-
ingly determine the precision of the algorithm at
different points of recall7. Note that the word
senses for which none of the distributionally sim-
ilar words are found in the MPQA corpus are not
7Specifically, in the list of word senses ranked by their
subjectivity score, we assign a subjectivity label to the top N
word senses. The precision is then determined as the number
of correct subjectivity label assignments out of all N assign-
ments, while the recall is measured as the correct subjective
senses out of all the subjective senses in the gold standard
data set. By varying the value of N from 1 to the total num-
ber of senses in the corpus, we can derive precision and recall
curves.
included in this evaluation (excluding 82 senses),
since in this case a subjectivity score cannot be
calculated. The evaluation is therefore performed
on a total of 272 word senses.
As a baseline, we use an ?informed? random as-
signment of subjectivity labels, which randomly
assigns S labels to word senses in the data set,
such that the maximum number of S assignments
equals the number of correct S labels in the gold
standard data set. This baseline guarantees a max-
imum recall of 1 (which under true random condi-
tions might not be achievable). Correspondingly,
given the controlled distribution of S labels across
the data set in the baseline setting, the precision
is equal for all eleven recall points, and is deter-
mined as the total number of correct subjective as-
signments divided by the size of the data set8.
Number Break-even
Algorithm of DSW point
similarity-all 100 0.41
similarity-selected 100 0.50
similarity-all 160 0.43
similarity-selected 160 0.50
baseline - 0.27
Table 2: Break-even point for different algorithm
and parameter settings
There are two aspects of the sense subjectivity
scoring algorithm that can influence the label as-
signment, and correspondingly their evaluation.
First, as indicated above, after calculating the
semantic similarity of the distributionally similar
words with each sense, we can either use all the
distributionally similar words for the calculation
of the subjectivity score of each sense (similarity-
all), or we can use only those that lead to the high-
est similarity (similarity-selected). Interestingly,
this aspect can drastically affect the algorithm ac-
curacy. The setting where a distributionally simi-
lar word can belong only to one sense significantly
improves the algorithm performance. Figure 1
plots the interpolated precision for eleven points of
recall, for similarity-all, similarity-selected, and
baseline. As shown in this figure, the precision-
recall curves for our algorithm are clearly above
the ?informed? baseline, indicating the ability of
our algorithm to automatically identify subjective
word senses.
Second, the number of distributionally similar
words considered in the first stage of the algo-
rithm can vary, and might therefore influence the
8In other words, this fraction represents the probability of
making the correct subjective label assignment by chance.
1069
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Precision recall curves
selected
all
baseline
Figure 1: Precision and recall for automatic sub-
jectivity annotations of word senses (DSW=160).
output of the algorithm. We experiment with two
different values, namely 100 and 160 top-ranked
distributionally similar words. Table 2 shows the
break-even points for the four different settings
that were evaluated,9 with results that are almost
double compared to the informed baseline. As
it turns out, for weaker versions of the algorithm
(i.e., similarity-all), the size of the set of distribu-
tionally similar words can significantly impact the
performance of the algorithm. However, for the al-
ready improved similarity-selected algorithm ver-
sion, this parameter does not seem to have influ-
ence, as similar results are obtained regardless of
the number of distributionally similar words. This
is in agreement with the finding of (McCarthy et
al., 2004) that, in their word sense ranking method,
a larger set of neighbors did not influence the al-
gorithm accuracy.
5 Automatic Subjectivity Annotations for
Word Sense Disambiguation
The final question we address is concerned with
the potential impact of subjectivity on the quality
of a word sense classifier. To answer this ques-
tion, we augment an existing data-driven word
sense disambiguation system with a feature re-
flecting the subjectivity of the examples where the
ambiguous word occurs, and evaluate the perfor-
mance of the new subjectivity-aware classifier as
compared to the traditional context-based sense
classifier.
We use a word sense disambiguation system
that integrates both local and topical features.
9The break-even point (Lewis, 1992) is a standard mea-
sure used in conjunction with precision-recall evaluations. It
represents the value where precision and recall become equal.
Specifically, we use the current word and its part-
of-speech, a local context of three words to the
left and right of the ambiguous word, the parts-of-
speech of the surrounding words, and a global con-
text implemented through sense-specific keywords
determined as a list of at most five words occurring
at least three times in the contexts defining a cer-
tain word sense. This feature set is similar to the
one used by (Ng and Lee, 1996), as well as by a
number of SENSEVAL systems. The parameters
for sense-specific keyword selection were deter-
mined through cross-fold validation on the train-
ing set. The features are integrated in a Naive
Bayes classifier, which was selected mainly for
its performance in previous work showing that it
can lead to a state-of-the-art disambiguation sys-
tem given the features we consider (Lee and Ng,
2002).
The experiments are performed on the set of
ambiguous nouns from the SENSEVAL-3 English
lexical sample evaluation (Mihalcea et al, 2004).
We use the rule-based subjective sentence classi-
fier of (Riloff and Wiebe, 2003) to assign an S,
O, or B label to all the training and test examples
pertaining to these ambiguous words. This sub-
jectivity annotation tool targets sentences, rather
than words or paragraphs, and therefore the tool is
fed with sentences. We also include a surrounding
context of two additional sentences, because the
classifier considers some contextual information.
Our hypothesis motivating the use of a
sentence-level subjectivity classifier is that in-
stances of subjective senses are more likely to be
in subjective sentences, and thus that sentence sub-
jectivity is an informative feature for the disam-
biguation of words having both subjective and ob-
jective senses.
For each ambiguous word, we perform two sep-
arate runs: one using the basic disambiguation
system described earlier, and another using the
subjectivity-aware system that includes the addi-
tional subjectivity feature. Table 3 shows the re-
sults obtained for these 20 nouns, including word
sense disambiguation accuracy for the two differ-
ent systems, the most frequent sense baseline, and
the subjectivity/objectivity split among the word
senses (according to Judge 1). The words in the
top half of the table are the ones that have both S
and O senses, and those in the bottom are the ones
that do not. If we were to use Judge 2?s tags in-
stead of Judge 1?s, only one word would change:
source would move from the top to the bottom of
the table.
1070
Sense Data Classifier
Word Senses subjectivity train test Baseline basic + subj.
Words with subjective senses
argument 5 3-S 2-O 221 111 49.4% 51.4% 54.1%
atmosphere 6 2-S 4-O 161 81 65.4% 65.4% 66.7%
difference 5 2-S 3-O 226 114 40.4% 54.4% 57.0%
difficulty 4 2-S 2-O 46 23 17.4% 47.8% 52.2%
image 7 2-S 5-O 146 74 36.5% 41.2% 43.2%
interest 7 1-S 5-O 1-B 185 93 41.9% 67.7% 68.8%
judgment 7 5-S 2-O 62 32 28.1% 40.6% 43.8%
plan 3 1-S 2-O 166 84 81.0% 81.0% 81.0%
sort 4 1-S 2-O 1-B 190 96 65.6% 66.7% 67.7%
source 9 1-S 8-O 64 32 40.6% 40.6% 40.6%
Average 46.6% 55.6% 57.5%
Words with no subjective senses
arm 6 6-O 266 133 82.0% 85.0% 84.2%
audience 4 4-O 200 100 67.0% 74.0% 74.0%
bank 10 10-O 262 132 62.6% 62.6% 62.6%
degree 7 5-O 2-B 256 128 60.9% 71.1% 71.1%
disc 4 4-O 200 100 38.0% 65.6% 66.4%
organization 7 7-O 112 56 64.3% 64.3% 64.3%
paper 7 7-O 232 117 25.6% 49.6% 48.0%
party 5 5-O 230 116 62.1% 62.9% 62.9%
performance 5 5-O 172 87 26.4% 34.5% 34.5%
shelter 5 5-O 196 98 44.9% 65.3% 65.3%
Average 53.3% 63.5% 63.3%
Average for all words 50.0% 59.5% 60.4%
Table 3: Word Sense Disambiguation with and
without subjectivity information, for the set of am-
biguous nouns in SENSEVAL-3
For the words that have both S and O senses,
the addition of the subjectivity feature alone can
bring a significant error rate reduction of 4.3%
(p < 0.05 paired t-test). Interestingly, no improve-
ments are observed for the words with no subjec-
tive senses; on the contrary, the addition of the
subjectivity feature results in a small degradation.
Overall for the entire set of ambiguous words, the
error reduction is measured at 2.2% (significant at
p < 0.1 paired t-test).
In almost all cases, the words with both S and O
senses show improvement, while the others show
small degradation or no change. This suggests that
if a subjectivity label is available for the words in
a lexical resource (e.g. using Algorithm 1 from
Section 4), such information can be used to decide
on using a subjectivity-aware system, thereby im-
proving disambiguation accuracy.
One of the exceptions is disc, which had a small
benefit, despite not having any subjective senses.
As it happens, the first sense of disc is phonograph
record.
phonograph record, phonograph recording, record, disk, disc,
platter ? (sound recording consisting of a disc with continu-
ous grooves; formerly used to reproduce music by rotating
while a phonograph needle tracked in the grooves)
The improvement can be explained by observ-
ing that many of the training and test sentences
containing this sense are labeled subjective by the
classifier, and indeed this sense frequently occurs
in subjective sentences such as ?This is anyway a
stunning disc.?
Another exception is the noun plan, which did
not benefit from the subjectivity feature, although
it does have a subjective sense. This can perhaps
be explained by the data set for this word, which
seems to be particularly difficult, as the basic clas-
sifier itself could not improve over the most fre-
quent sense baseline.
The other word that did not benefit from the
subjectivity feature is the noun source, for which
its only subjective sense did not appear in the
sense-annotated data, leading therefore to an ?ob-
jective only? set of examples.
6 Conclusion and Future Work
The questions posed in the introduction concern-
ing the possible interaction between subjectivity
and word sense found answers throughout the pa-
per. As it turns out, a correlation can indeed be
established between these two semantic properties
of language.
Addressing the first question of whether subjec-
tivity is a property that can be assigned to word
senses, we showed that good agreement (?=0.74)
can be achieved between human annotators la-
beling the subjectivity of senses. When uncer-
tain cases are removed, the ? value is even higher
(0.90). Moreover, the automatic subjectivity scor-
ing mechanism that we devised was able to suc-
cessfully assign subjectivity labels to senses, sig-
nificantly outperforming an ?informed? baseline
associated with the task. While much work re-
mains to be done, this first attempt has proved
the feasibility of correctly assigning subjectivity
labels to the fine-grained level of word senses.
The second question was also positively an-
swered: the quality of a word sense disambigua-
tion system can be improved with the addition
of subjectivity information. Section 5 provided
evidence that automatic subjectivity classification
may improve word sense disambiguation perfor-
mance, but mainly for words with both subjective
and objective senses. As we saw, performance
may even degrade for words that do not. Tying
the pieces of this paper together, once the senses
in a dictionary have been assigned subjectivity la-
bels, a word sense disambiguation system could
consult them to decide whether it should consider
or ignore the subjectivity feature.
There are several other ways our results could
impact future work. Subjectivity labels would
be a useful source of information when manually
augmenting the lexical knowledge in a dictionary,
1071
e.g., when choosing hypernyms for senses or de-
ciding which senses to eliminate when defining a
coarse-grained sense inventory (if there is a sub-
jective sense, at least one should be retained).
Adding subjectivity labels to WordNet could
also support automatic subjectivity analysis. First,
the input corpus could be sense tagged and the
subjectivity labels of the assigned senses could be
exploited by a subjectivity recognition tool. Sec-
ond, a number of methods for subjectivity or sen-
timent analysis start with a set of seed words and
then search through WordNet to find other subjec-
tive words (Kamps and Marx, 2002; Yu and Hatzi-
vassiloglou, 2003; Hu and Liu, 2004; Kim and
Hovy, 2004; Esuli and Sebastiani, 2005). How-
ever, such searches may veer off course down ob-
jective paths. The subjectivity labels assigned to
senses could be consulted to keep the search trav-
eling along subjective paths.
Finally, there could be different strategies
for exploiting subjectivity annotations and word
sense. While the current setting considered a
pipeline approach, where the output of a subjec-
tivity annotation system was fed to the input of a
method for semantic disambiguation, future work
could also consider the role of word senses as a
possible way of improving subjectivity analysis,
or simultaneous annotations of subjectivity and
word meanings, as done in the past for other lan-
guage processing problems.
Acknowledgments We would like to thank
Theresa Wilson for annotating senses, and the
anonymous reviewers for their helpful comments.
This work was partially supported by ARDA
AQUAINT and by the NSF (award IIS-0208798).
References
K. Dave, S. Lawrence, and D. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proc.
WWW-2003, Budapest, Hungary. Available at
http://www2003.org.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis.
In Proc. CIKM-2005.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc.
ACL-97, pages 174?181.
D. Heise. 2001. Project magellan: Collecting cross-
cultural affective meanings via the internet. Elec-
tronic Journal of Sociology, 5(3).
M. Hu and B. Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of ACM
SIGKDD.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical tax onomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
J. Kamps and M. Marx. 2002. Words with attitude. In
Proc. 1st International WordNet Conference.
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proc. Coling 2004.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation
of knowledge sources and learning algo rithms for
word sense disambiguation. In Proc. EMNLP 2002.
D. Lewis. 1992. An evaluation of phrasal and clus-
tered representations on a text categorization task.
In Proceedings of ACM SIGIR.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL,
Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant senses in untagged text.
In Proc. ACL 2004.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Proc.
ACL/SIGLEX Senseval-3.
G. Miller. 1995. Wordnet: A lexical database. Com-
munication of the ACM, 38(11):39?41.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word se nse: An
examplar-based approach. In Proc. ACL 1996.
B. Pang and L. Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
riza tion based on minimum cuts. In Proc. ACL
2004.
A. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc. of
HLT/EMNLP 2005.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. EMNLP
2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting
subjectivity classification to improve information ex
traction. In Proc. AAAI 2005.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
perspective question answering using the opqa cor-
pus. In Proc. HLT/EMNLP 2005.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL 2002.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 1(2).
J. Wiebe. 2000. Learning subjective adjectives from
corpora. In Proc. AAAI 2000.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natu ral language processing tech-
niques. In Proc. ICDM 2003.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. EMNLP 2003.
1072
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976?983,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Multilingual Subjective Language via Cross-Lingual Projections
Rada Mihalcea and Carmen Banea
Department of Computer Science
University of North Texas
rada@cs.unt.edu, carmenb@unt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
This paper explores methods for generating
subjectivity analysis resources in a new lan-
guage by leveraging on the tools and re-
sources available in English. Given a bridge
between English and the selected target lan-
guage (e.g., a bilingual dictionary or a par-
allel corpus), the methods can be used to
rapidly create tools for subjectivity analysis
in the new language.
1 Introduction
There is growing interest in the automatic extraction
of opinions, emotions, and sentiments in text (sub-
jectivity), to provide tools and support for various
natural language processing applications. Most of
the research to date has focused on English, which
is mainly explained by the availability of resources
for subjectivity analysis, such as lexicons and man-
ually labeled corpora.
In this paper, we investigate methods to auto-
matically generate resources for subjectivity analy-
sis for a new target language by leveraging on the
resources and tools available for English, which in
many cases took years of work to complete. Specif-
ically, through experiments with cross-lingual pro-
jection of subjectivity, we seek answers to the fol-
lowing questions.
First, can we derive a subjectivity lexicon for a
new language using an existing English subjectivity
lexicon and a bilingual dictionary? Second, can we
derive subjectivity-annotated corpora in a new lan-
guage using existing subjectivity analysis tools for
English and a parallel corpus? Finally, third, can we
build tools for subjectivity analysis for a new target
language by relying on these automatically gener-
ated resources?
We focus our experiments on Romanian, selected
as a representative of the large number of languages
that have only limited text processing resources de-
veloped to date. Note that, although we work with
Romanian, the methods described are applicable to
any other language, as in these experiments we (pur-
posely) do not use any language-specific knowledge
of the target language. Given a bridge between En-
glish and the selected target language (e.g., a bilin-
gual dictionary or a parallel corpus), the methods
can be applied to other languages as well.
After providing motivations, we present two ap-
proaches to developing sentence-level subjectivity
classifiers for a new target language. The first uses a
subjectivity lexicon translated from an English one.
The second uses an English subjectivity classifier
and a parallel corpus to create target-language train-
ing data for developing a statistical classifier.
2 Motivation
Automatic subjectivity analysis methods have been
used in a wide variety of text processing applica-
tions, such as tracking sentiment timelines in on-
line forums and news (Lloyd et al, 2005; Balog
et al, 2006), review classification (Turney, 2002;
Pang et al, 2002), mining opinions from product
reviews (Hu and Liu, 2004), automatic expressive
text-to-speech synthesis (Alm et al, 2005), text se-
mantic analysis (Wiebe and Mihalcea, 2006; Esuli
and Sebastiani, 2006), and question answering (Yu
and Hatzivassiloglou, 2003).
976
While much recent work in subjectivity analysis
focuses on sentiment (a type of subjectivity, namely
positive and negative emotions, evaluations, and
judgments), we opt to focus on recognizing subjec-
tivity in general, for two reasons.
First, even when sentiment is the desired focus,
researchers in sentiment analysis have shown that
a two-stage approach is often beneficial, in which
subjective instances are distinguished from objec-
tive ones, and then the subjective instances are fur-
ther classified according to polarity (Yu and Hatzi-
vassiloglou, 2003; Pang and Lee, 2004; Wilson et
al., 2005; Kim and Hovy, 2006). In fact, the prob-
lem of distinguishing subjective versus objective in-
stances has often proved to be more difficult than
subsequent polarity classification, so improvements
in subjectivity classification promise to positively
impact sentiment classification. This is reported in
studies of manual annotation of phrases (Takamura
et al, 2006), recognizing contextual polarity of ex-
pressions (Wilson et al, 2005), and sentiment tag-
ging of words and word senses (Andreevskaia and
Bergler, 2006; Esuli and Sebastiani, 2006).
Second, an NLP application may seek a wide
range of types of subjectivity attributed to a per-
son, such as their motivations, thoughts, and specu-
lations, in addition to their positive and negative sen-
timents. For instance, the opinion tracking system
Lydia (Lloyd et al, 2005) gives separate ratings for
subjectivity and sentiment. These can be detected
with subjectivity analysis but not by a method fo-
cused only on sentiment.
There is world-wide interest in text analysis appli-
cations. While work on subjectivity analysis in other
languages is growing (e.g., Japanese data are used in
(Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006)),
much of the work in subjectivity analysis has been
applied to English data. Creating corpora and lexical
resources for a new language is very time consum-
ing. In general, we would like to leverage resources
already developed for one language to more rapidly
create subjectivity analysis tools for a new one. This
motivates our exploration and use of cross-lingual
lexicon translations and annotation projections.
Most if not all work on subjectivity analysis has
been carried out in a monolingual framework. We
are not aware of multi-lingual work in subjectivity
analysis such as that proposed here, in which subjec-
tivity analysis resources developed for one language
are used to support developing resources in another.
3 A Lexicon-Based Approach
Many subjectivity and sentiment analysis tools rely
on manually or semi-automatically constructed lex-
icons (Yu and Hatzivassiloglou, 2003; Riloff and
Wiebe, 2003; Kim and Hovy, 2006). Given the suc-
cess of such techniques, the first approach we take
to generating a target-language subjectivity classi-
fier is to create a subjectivity lexicon by translating
an existing source language lexicon, and then build
a classifier that relies on the resulting lexicon.
Below, we describe the translation process and
discuss the results of an annotation study to assess
the quality of the translated lexicon. We then de-
scribe and evaluate a lexicon-based target-language
classifier.
3.1 Translating a Subjectivity Lexicon
The subjectivity lexicon we use is from Opinion-
Finder (Wiebe and Riloff, 2005), an English sub-
jectivity analysis system which, among other things,
classifies sentences as subjective or objective. The
lexicon was compiled from manually developed re-
sources augmented with entries learned from cor-
pora. It contains 6,856 unique entries, out of which
990 are multi-word expressions. The entries in the
lexicon have been labeled for part of speech, and for
reliability ? those that appear most often in subjec-
tive contexts are strong clues of subjectivity, while
those that appear less often, but still more often than
expected by chance, are labeled weak.
To perform the translation, we use two bilingual
dictionaries. The first is an authoritative English-
Romanian dictionary, consisting of 41,500 entries,1
which we use as the main translation resource for the
lexicon translation. The second dictionary, drawn
from the Universal Dictionary download site (UDP,
2007) consists of 4,500 entries written largely by
Web volunteer contributors, and thus is not error
free. We use this dictionary only for those entries
that do not appear in the main dictionary.
1Unique English entries, each with multiple Romanian
translations.
977
There were several challenges encountered in the
translation process. First, although the English sub-
jectivity lexicon contains inflected words, we must
use the lemmatized form in order to be able to trans-
late the entries using the bilingual dictionary. How-
ever, words may lose their subjective meaning once
lemmatized. For instance, the inflected form of
memories becomes memory. Once translated into
Romanian (as memorie), its main meaning is ob-
jective, referring to the power of retaining informa-
tion as in Iron supplements may improve a woman?s
memory.
Second, neither the lexicon nor the bilingual dic-
tionary provides information on the sense of the in-
dividual entries, and therefore the translation has to
rely on the most probable sense in the target lan-
guage. Fortunately, the bilingual dictionary lists the
translations in reverse order of their usage frequen-
cies. Nonetheless, the ambiguity of the words and
the translations still seems to represent an impor-
tant source of error. Moreover, the lexicon some-
times includes identical entries expressed through
different parts of speech, e.g., grudge has two sepa-
rate entries, for its noun and verb roles, respectively.
On the other hand, the bilingual dictionary does not
make this distinction, and therefore we have again
to rely on the ?most frequent? heuristic captured by
the translation order in the bilingual dictionary.
Finally, the lexicon includes a significant number
(990) of multi-word expressions that pose transla-
tion difficulties, sometimes because their meaning
is idiomatic, and sometimes because the multi-word
expression is not listed in the bilingual dictionary
and the translation of the entire phrase is difficult
to reconstruct from the translations of the individual
words. To address this problem, when a translation
is not found in the dictionary, we create one using
a word-by-word approach. These translations are
then validated by enforcing that they occur at least
three times on the Web, using counts collected from
the AltaVista search engine. The multi-word expres-
sions that are not validated in this process are dis-
carded, reducing the number of expressions from an
initial set of 990 to a final set of 264.
The final subjectivity lexicon in Romanian con-
tains 4,983 entries. Table 1 shows examples of en-
tries in the Romanian lexicon, together with their
corresponding original English form. The table
Romanian English attributes
??nfrumuset?a beautifying strong, verb
notabil notable weak, adj
plin de regret full of regrets strong, adj
sclav slaves weak, noun
Table 1: Examples of entries in the Romanian sub-
jectivity lexicon
also shows the reliability of the expression (weak or
strong) and the part of speech ? attributes that are
provided in the English subjectivity lexicon.
Manual Evaluation.
We want to assess the quality of the translated lexi-
con, and compare it to the quality of the original En-
glish lexicon. The English subjectivity lexicon was
evaluated in (Wiebe and Riloff, 2005) against a cor-
pus of English-language news articles manually an-
notated for subjectivity (the MPQA corpus (Wiebe et
al., 2005)). According to this evaluation, 85% of the
instances of the clues marked as strong and 71.5% of
the clues marked as weak are in subjective sentences
in the MPQA corpus.
Since there is no comparable Romanian corpus,
an alternate way to judge the subjectivity of a Ro-
manian lexicon entry is needed.
Two native speakers of Romanian annotated the
subjectivity of 150 randomly selected entries. Each
annotator independently read approximately 100 ex-
amples of each drawn from the Web, including a
large number from news sources. The subjectivity
of a word was consequently judged in the contexts
where it most frequently appears, accounting for its
most frequent meanings on the Web.
The tagset used for the annotations consists of
S(ubjective), O(bjective), and B(oth). A W(rong) la-
bel is also used to indicate a wrong translation. Table
2 shows the contingency table for the two annota-
tors? judgments on this data.
S O B W Total
S 53 6 9 0 68
O 1 27 1 0 29
B 5 3 18 0 26
W 0 0 0 27 27
Total 59 36 28 27 150
Table 2: Agreement on 150 entries in the Romanian
lexicon
Without counting the wrong translations, the
agreement is measured at 0.80, with a Kappa ? =
978
0.70, which indicates consistent agreement. After
the disagreements were reconciled through discus-
sions, the final set of 123 correctly translated entries
does include 49.6% (61) subjective entries, but fully
23.6% (29) were found in the study to have primar-
ily objective uses (the other 26.8% are mixed).
Thus, this study suggests that the Romanian sub-
jectivity clues derived through translation are less re-
liable than the original set of English clues. In sev-
eral cases, the subjectivity is lost in the translation,
mainly due to word ambiguity in either the source
or target language, or both. For instance, the word
fragile correctly translates into Romanian as fragil,
yet this word is frequently used to refer to breakable
objects, and it loses its subjective meaning of del-
icate. Other words, such as one-sided, completely
lose subjectivity once translated, as it becomes in
Romanian cu o singura latura?, meaning with only
one side (as of objects).
Interestingly, the reliability of clues in the English
lexicon seems to help preserve subjectivity. Out of
the 77 entries marked as strong, 11 were judged to be
objective in Romanian (14.3%), compared to 14 ob-
jective Romanian entries obtained from the 36 weak
English clues (39.0%).
3.2 Rule-based Subjectivity Classifier Using a
Subjectivity Lexicon
Starting with the Romanian lexicon, we developed
a lexical classifier similar to the one introduced by
(Riloff and Wiebe, 2003). At the core of this method
is a high-precision subjectivity and objectivity clas-
sifier that can label large amounts of raw text using
only a subjectivity lexicon. Their method is further
improved with a bootstrapping process that learns
extraction patterns. In our experiments, however, we
apply only the rule-based classification step, since
the extraction step cannot be implemented without
tools for syntactic parsing and information extrac-
tion not available in Romanian.
The classifier relies on three main heuristics to la-
bel subjective and objective sentences: (1) if two
or more strong subjective expressions occur in the
same sentence, the sentence is labeled Subjective;
(2) if no strong subjective expressions occur in a
sentence, and at most two weak subjective expres-
sions occur in the previous, current, and next sen-
tence combined, then the sentence is labeled Objec-
tive; (3) otherwise, if none of the previous rules ap-
ply, the sentence is labeled Unknown.
The quality of the classifier was evaluated on a
Romanian gold-standard corpus annotated for sub-
jectivity. Two native Romanian speakers (Ro1 and
Ro2) manually annotated the subjectivity of the sen-
tences of five randomly selected documents (504
sentences) from the Romanian side of an English-
Romanian parallel corpus, according to the anno-
tation scheme in (Wiebe et al, 2005). Agreement
between annotators was measured, and then their
differences were adjudicated. The baseline on this
data set is 54.16%, which can be obtained by as-
signing a default Subjective label to all sentences.
(More information about the corpus and annotations
are given in Section 4 below, where agreement be-
tween English and Romanian aligned sentences is
also assessed.)
As mentioned earlier, due to the lexicon projec-
tion process that is performed via a bilingual dictio-
nary, the entries in our Romanian subjectivity lex-
icon are in a lemmatized form. Consequently, we
also lemmatize the gold-standard corpus, to allow
for the identification of matches with the lexicon.
For this purpose, we use the Romanian lemmatizer
developed by Ion and Tufis? (Ion, 2007), which has
an estimated accuracy of 98%.2
Table 3 shows the results of the rule-based classi-
fier. We show the precision, recall, and F-measure
independently measured for the subjective, objec-
tive, and all sentences. We also evaluated a vari-
ation of the rule-based classifier that labels a sen-
tence as objective if there are at most three weak ex-
pressions in the previous, current, and next sentence
combined, which raises the recall of the objective
classifier. Our attempts to increase the recall of the
subjective classifier all resulted in significant loss in
precision, and thus we kept the original heuristic.
In its original English implementation, this sys-
tem was proposed as being high-precision but low
coverage. Evaluated on the MPQA corpus, it has
subjective precision of 90.4, subjective recall of
34.2, objective precision of 82.4, and objective re-
call of 30.7; overall, precision is 86.7 and recall is
32.6 (Wiebe and Riloff, 2005). We see a similar be-
havior on Romanian for subjective sentences. The
subjective precision is good, albeit at the cost of low
2Dan Tufis?, personal communication.
979
Measure Subjective Objective All
subj = at least two strong; obj = at most two weak
Precision 80.00 56.50 62.59
Recall 20.51 48.91 33.53
F-measure 32.64 52.52 43.66
subj = at least two strong; obj = at most three weak
Precision 80.00 56.85 61.94
Recall 20.51 61.03 39.08
F-measure 32.64 58.86 47.93
Table 3: Evaluation of the rule-based classifier
recall, and thus the classifier could be used to har-
vest subjective sentences from unlabeled Romanian
data (e.g., for a subsequent bootstrapping process).
The system is not very effective for objective classi-
fication, however. Recall that the objective classifier
relies on the weak subjectivity clues, for which the
transfer of subjectivity in the translation process was
particularly low.
4 A Corpus-Based Approach
Given the low number of subjective entries found in
the automatically generated lexicon and the subse-
quent low recall of the lexical classifier, we decided
to also explore a second, corpus-based approach.
This approach builds a subjectivity-annotated cor-
pus for the target language through projection, and
then trains a statistical classifier on the resulting
corpus (numerous statistical classifiers have been
trained for subjectivity or sentiment classification,
e.g., (Pang et al, 2002; Yu and Hatzivassiloglou,
2003)). The hypothesis is that we can eliminate
some of the ambiguities (and consequent loss of sub-
jectivity) observed during the lexicon translation by
accounting for the context of the ambiguous words,
which is possible in a corpus-based approach. Ad-
ditionally, we also hope to improve the recall of the
classifier, by addressing those cases not covered by
the lexicon-based approach.
In the experiments reported in this section, we
use a parallel corpus consisting of 107 documents
from the SemCor corpus (Miller et al, 1993) and
their manual translations into Romanian.3 The cor-
pus consists of roughly 11,000 sentences, with ap-
proximately 250,000 tokens on each side. It is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others.
3The translation was carried out by a Romanian native
speaker, student in a department of ?Foreign Languages and
Translations? in Romania.
Below, we begin with a manual annotation study
to assess the quality of annotation and preservation
of subjectivity in translation. We then describe the
automatic construction of a target-language training
set, and evaluate a classifier trained on that data.
Annotation Study.
We start by performing an agreement study meant
to determine the extent to which subjectivity is pre-
served by the cross-lingual projections. In the study,
three annotators ? one native English speaker (En)
and two native Romanian speakers (Ro1 and Ro2) ?
first trained on 3 randomly selected documents (331
sentences). They then independently annotated the
subjectivity of the sentences of two randomly se-
lected documents from the parallel corpus, account-
ing for 173 aligned sentence pairs. The annotators
had access exclusively to the version of the sen-
tences in their language, to avoid any bias that could
be introduced by seeing the translation in the other
language.
Note that the Romanian annotations (after all dif-
ferences between the Romanian annotators were ad-
judicated) of all 331 + 173 sentences make up the
gold standard corpus used in the experiments re-
ported in Sections 3.2 and 4.1.
Before presenting the results of the annotation
study, we give some examples. The following are
English subjective sentences and their Romanian
translations (the subjective elements are shown in
bold).
[en] The desire to give Broglio as many starts as
possible.
[ro] Dorint?a de a-i da lui Broglio ca?t mai multe
starturi posibile.
[en] Suppose he did lie beside Lenin, would it be
permanent ?
[ro] Sa? presupunem ca? ar fi as?ezat ala?turi de Lenin,
oare va fi pentru totdeauna?
The following are examples of objective parallel
sentences.
[en]The Pirates have a 9-6 record this year and the
Redbirds are 7-9.
[ro] Pirat?ii au un palmares de 9 la 6 anul acesta si
Pa?sa?rile Ros?ii au 7 la 9.
[en] One of the obstacles to the easy control of a
2-year old child is a lack of verbal communication.
[ro] Unul dintre obstacolele ??n controlarea unui
copil de 2 ani este lipsa comunica?rii verbale.
980
The annotators were trained using the MPQA
annotation guidelines (Wiebe et al, 2005). The
tagset consists of S(ubjective), O(bjective) and
U(ncertain). For the U tags, a class was also given;
OU means, for instance, that the annotator is uncer-
tain but she is leaning toward O. Table 4 shows the
pairwise agreement figures and the Kappa (?) calcu-
lated for the three annotators. The table also shows
the agreement when the borderline uncertain cases
are removed.
all sentences Uncertain removed
pair agree ? agree ? (%) removed
Ro1 & Ro2 0.83 0.67 0.89 0.77 23
En & Ro1 0.77 0.54 0.86 0.73 26
En & Ro2 0.78 0.55 0.91 0.82 20
Table 4: Agreement on the data set of 173 sentences.
Annotations performed by three annotators: one na-
tive English speaker (En) and two native Romanian
speakers (Ro1 and Ro2)
When all the sentences are included, the agree-
ment between the two Romanian annotators is mea-
sured at 0.83 (? = 0.67). If we remove the border-
line cases where at least one annotator?s tag is Un-
certain, the agreement rises to 0.89 with ? = 0.77.
These figures are somewhat lower than the agree-
ment observed during previous subjectivity anno-
tation studies conducted on English (Wiebe et al,
2005) (the annotators were more extensively trained
in those studies), but they nonetheless indicate con-
sistent agreement.
Interestingly, when the agreement is conducted
cross-lingually between an English and a Romanian
annotator, the agreement figures, although some-
what lower, are comparable. In fact, once the
Uncertain tags are removed, the monolingual and
cross-lingual agreement and ? values become al-
most equal, which suggests that in most cases the
sentence-level subjectivity is preserved.
The disagreements were reconciled first between
the labels assigned by the two Romanian annotators,
followed by a reconciliation between the resulting
Romanian ?gold-standard? labels and the labels as-
signed by the English annotator. In most cases, the
disagreement across the two languages was found
to be due to a difference of opinion about the sen-
tence subjectivity, similar to the differences encoun-
tered in monolingual annotations. However, there
are cases where the differences are due to the sub-
jectivity being lost in the translation. Sometimes,
this is due to several possible interpretations for the
translated sentence. For instance, the following sen-
tence:
[en] They honored the battling Billikens last night.
[ro] Ei i-au celebrat pe Billikens seara trecuta?.
is marked as Subjective in English (in context, the
English annotator interpreted honored as referring
to praises of the Billikens). However, the Romanian
translation of honored is celebrat which, while cor-
rect as a translation, has the more frequent interpre-
tation of having a party. The two Romanian annota-
tors chose this interpretation, which correspondingly
lead them to mark the sentence as Objective.
In other cases, in particular when the subjectivity
is due to figures of speech such as irony, the trans-
lation sometimes misses the ironic aspects. For in-
stance, the translation of egghead was not perceived
as ironic by the Romanian annotators, and conse-
quently the following sentence labeled Subjective in
English is annotated as Objective in Romanian.
[en] I have lived for many years in a Connecti-
cut commuting town with a high percentage of [...]
business executives of egghead tastes.
[ro] Am tra?it mult?i ani ??ntr-un oras? din apropiere de
Connecticut ce avea o mare proport?ie de [...] oa-
meni de afaceri cu gusturi intelectuale.
4.1 Translating a Subjectivity-Annotated
Corpus and Creating a Machine Learning
Subjectivity Classifier
To further validate the corpus-based projection of
subjectivity, we developed a subjectivity classifier
trained on Romanian subjectivity-annotated corpora
obtained via cross-lingual projections.
Ideally, one would generate an annotated Roma-
nian corpus by translating English documents man-
ually annotated for subjectivity such as the MPQA
corpus. Unfortunately, the manual translation of this
corpus would be prohibitively expensive, both time-
wise and financially. The other alternative ? auto-
matic machine translation ? has not yet reached a
level that would enable the generation of a high-
quality translated corpus. We therefore decided to
use a different approach where we automatically
annotate the English side of an existing English-
Romanian corpus, and subsequently project the an-
notations onto the Romanian side of the parallel cor-
981
Precision Recall F-measure
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 5: Precision, recall, and F-measure for the
two OpinionFinder classifiers, as measured on the
MPQA corpus.
pus across the sentence-level alignments available in
the corpus.
For the automatic subjectivity annotations, we
generated two sets of the English-side annotations,
one using the high-precision classifier and one using
the high-coverage classifier available in the Opinion-
Finder tool. The high-precision classifier in Opin-
ionFinder uses the clues of the subjectivity lexicon
to harvest subjective and objective sentences from
a large amount of unannotated text; this data is then
used to automatically identify a set of extraction pat-
terns, which are then used iteratively to identify a
larger set of subjective and objective sentences.
In addition, in OpinionFinder, the high-precision
classifier is used to produce an English labeled data
set for training, which is used to generate its Naive
Bayes high-coverage subjectivity classifier. Table
5 shows the performance of the two classifiers on
the MPQA corpus as reported in (Wiebe and Riloff,
2005). Note that 55% of the sentences in the MPQA
corpus are subjective ? which represents the baseline
for this data set.
The two OpinionFinder classifiers are used to la-
bel the training corpus. After removing the 504 test
sentences, we are left with 10,628 sentences that
are automatically annotated for subjectivity. Table
6 shows the number of subjective and objective sen-
tences obtained with each classifier.
Classifier Subjective Objective All
high-precision 1,629 2,334 3,963
high-coverage 5,050 5,578 10,628
Table 6: Subjective and objective training sentences
automatically annotated with OpinionFinder.
Next, the OpinionFinder annotations are pro-
jected onto the Romanian training sentences, which
are then used to develop a probabilistic classifier for
the automatic labeling of subjectivity in Romanian
sentences.
Similar to, e.g., (Pang et al, 2002), we use a
Naive Bayes algorithm trained on word features co-
occurring with the subjective and the objective clas-
sifications. We assume word independence, and we
use a 0.3 cut-off for feature selection. While re-
cent work has also considered more complex syn-
tactic features, we are not able to generate such fea-
tures for Romanian as they require tools currently
not available for this language.
We create two classifiers, one trained on each
data set. The quality of the classifiers is evaluated
on the 504-sentence Romanian gold-standard corpus
described above. Recall that the baseline on this data
set is 54.16%, the percentage of sentences in the cor-
pus that are subjective. Table 7 shows the results.
Subjective Objective All
projection source: OF high-precision classifier
Precision 65.02 69.62 64.48
Recall 82.41 47.61 64.48
F-measure 72.68 56.54 64.68
projection source: OF high-coverage classifier
Precision 66.66 70.17 67.85
Recall 81.31 52.17 67.85
F-measure 72.68 56.54 67.85
Table 7: Evaluation of the machine learning classi-
fier using training data obtained via projections from
data automatically labeled by OpinionFinder (OF).
Our best classifier has an F-measure of 67.85,
and is obtained by training on projections from
the high-coverage OpinionFinder annotations. Al-
though smaller than the 74.70 F-measure obtained
by the English high-coverage classifier (see Ta-
ble 5), the result appears remarkable given that no
language-specific Romanian information was used.
The overall results obtained with the machine
learning approach are considerably higher than
those obtained from the rule-based classifier (except
for the precision of the subjective sentences). This
is most likely due to the lexicon translation process,
which as mentioned in the agreement study in Sec-
tion 3.1, leads to ambiguity and loss of subjectivity.
Instead, the corpus-based translations seem to better
account for the ambiguity of the words, and the sub-
jectivity is generally preserved in the sentence trans-
lations.
5 Conclusions
In this paper, we described two approaches to gener-
ating resources for subjectivity annotations for a new
982
language, by leveraging on resources and tools avail-
able for English. The first approach builds a target
language subjectivity lexicon by translating an exist-
ing English lexicon using a bilingual dictionary. The
second generates a subjectivity-annotated corpus in
a target language by projecting annotations from an
automatically annotated English corpus.
These resources were validated in two ways.
First, we carried out annotation studies measuring
the extent to which subjectivity is preserved across
languages in each of the two resources. These stud-
ies show that only a relatively small fraction of the
entries in the lexicon preserve their subjectivity in
the translation, mainly due to the ambiguity in both
the source and the target languages. This is con-
sistent with observations made in previous work
that subjectivity is a property associated not with
words, but with word meanings (Wiebe and Mihal-
cea, 2006). In contrast, the sentence-level subjectiv-
ity was found to be more reliably preserved across
languages, with cross-lingual inter-annotator agree-
ments comparable to the monolingual ones.
Second, we validated the two automatically gen-
erated subjectivity resources by using them to build
a tool for subjectivity analysis in the target language.
Specifically, we developed two classifiers: a rule-
based classifier that relies on the subjectivity lexi-
con described in Section 3.1, and a machine learn-
ing classifier trained on the subjectivity-annotated
corpus described in Section 4.1. While the highest
precision for the subjective classification is obtained
with the rule-based classifier, the overall best result
of 67.85 F-measure is due to the machine learning
approach. This result is consistent with the anno-
tation studies, showing that the corpus projections
preserve subjectivity more reliably than the lexicon
translations.
Finally, neither one of the classifiers relies on
language-specific information, but rather on knowl-
edge obtained through projections from English. A
similar method can therefore be used to derive tools
for subjectivity analysis in other languages.
References
Alina Andreevskaia and Sabine Bergler. Mining wordnet for
fuzzy sentiment: Sentiment tag extraction from WordNet
glosses. In Proceedings of EACL 2006.
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005.
Emotions from text: Machine learning for text-based emo-
tion prediction. In Proceedings of HLT/EMNLP 2005.
Krisztian Balog, Gilad Mishne, and Maarten de Rijke. 2006.
Why are they excited? identifying and explaining spikes in
blog mood levels. In EACL-2006.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determining term
subjectivity and term orientation for opinion mining. In Pro-
ceedings the EACL 2006.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD.
Yi Hu, Jianyong Duan, Xiaoming Chen, Bingzhen Pei, and
Ruzhan Lu. 2005. A new method for sentiment classifi-
cation in text retrieval. In Proceedings of IJCNLP.
Radu Ion. 2007. Methods for automatic semantic disambigua-
tion. Applications to English and Romanian. Ph.D. thesis,
The Romanian Academy, RACAI.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of EMNLP 2006.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of HLT/NAACL
2006.
Levon Lloyd, Dimitrios Kechagias, and Steven Skiena. 2005.
Lydia: A system for large-scale news analysis. In Proceed-
ings of SPIRE 2005.
George Miller, Claudia Leacock, Tangee Randee, and Ross
Bunker. 1993. A semantic concordance. In Proceedings
of the DARPA Workshop on Human Language Technology.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL 2004.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP 2002.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of EMNLP
2003.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2006.
Latent variable models for semantic orientations of phrases.
In Proceedings of EACL 2006.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL 2002.
Universal Dictionary. 2007. Available at
www.dicts.info/uddl.php.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of COLING-ACL 2006.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts. In
Proceedings of CICLing 2005 (invited paper). Available at
www.cs.pitt.edu/mpqarequest.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2/3):164?
210. Available at www.cs.pitt.edu/mpqa.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of HLT/EMNLP 2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP 2003.
983
Proceedings of ACL-08: HLT, pages 932?940,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Linguistically Motivated Features for Enhanced Back-of-the-Book Indexing
Andras Csomai and Rada Mihalcea
Department of Computer Science
University of North Texas
csomaia@unt.edu,rada@cs.unt.edu
Abstract
In this paper we present a supervised method
for back-of-the-book index construction. We
introduce a novel set of features that goes be-
yond the typical frequency-based analysis, in-
cluding features based on discourse compre-
hension, syntactic patterns, and information
drawn from an online encyclopedia. In exper-
iments carried out on a book collection, the
method was found to lead to an improvement
of roughly 140% as compared to an existing
state-of-the-art supervised method.
1 Introduction
Books represent one of the oldest forms of writ-
ten communication and have been used since thou-
sands of years ago as a means to store and trans-
mit information. Despite this fact, given that a
large fraction of the electronic documents avail-
able online and elsewhere consist of short texts
such as Web pages, news articles, scientific reports,
and others, the focus of natural language process-
ing techniques to date has been on the automa-
tion of methods targeting short documents. We
are witnessing however a change: more and more
books are becoming available in electronic for-
mat, in projects such as the Million Books project
(http://www.archive.org/details/millionbooks), the
Gutenberg project (http://www.gutenberg.org), or
Google Book Search (http://books.google.com).
Similarly, a large number of the books published
in recent years are often available ? for purchase
or through libraries ? in electronic format. This
means that the need for language processing tech-
niques able to handle very large documents such as
books is becoming increasingly important.
This paper addresses the problem of automatic
back-of-the-book index construction. A back-of-
the-book index typically consists of the most impor-
tant keywords addressed in a book, with pointers to
the relevant pages inside the book. The construc-
tion of such indexes is one of the few tasks related
to publishing that still requires extensive human la-
bor. Although there is a certain degree of computer
assistance, consisting of tools that help the profes-
sional indexer to organize and edit the index, there
are no methods that would allow for a complete or
nearly-complete automation.
In addition to helping professional indexers in
their task, an automatically generated back-of-the-
book index can also be useful for the automatic stor-
age and retrieval of a document; as a quick reference
to the content of a book for potential readers, re-
searchers, or students (Schutze, 1998); or as a start-
ing point for generating ontologies tailored to the
content of the book (Feng et al, 2006).
In this paper, we introduce a supervised method
for back-of-the-book index construction, using a
novel set of linguistically motivated features. The
algorithm learns to automatically identify important
keywords in a book based on an ensemble of syntac-
tic, discourse-based and information-theoretic prop-
erties of the candidate concepts. In experiments per-
formed on a collection of books and their indexes,
the method was found to exceed by a large margin
the performance of a previously proposed state-of-
the-art supervised system for keyword extraction.
2 Supervised Back-of-the-Book Indexing
We formulate the problem of back-of-the-book in-
dexing as a supervised keyword extraction task, by
making a binary yes/no classification decision at the
932
level of each candidate index entry. Starting with a
set of candidate entries, the algorithm automatically
decides which entries should be added to the back-
of-the-book index, based on a set of linguistic and
information theoretic features. We begin by iden-
tifying the set of candidate index entries, followed
by the construction of a feature vector for each such
candidate entry. In the training data set, these fea-
ture vectors are also assigned with a correct label,
based on the presence/absence of the entry in the
gold standard back-of-the-book index provided with
the data. Finally, a machine learning algorithm is
applied, which automatically classifies the candidate
entries in the test data for their likelihood to belong
to the back-of-the-book index.
The application of a supervised algorithm re-
quires three components: a data set, which is de-
scribed next; a set of features, which are described in
Section 3; and a machine learning algorithm, which
is presented in Section 4.
2.1 Data
We use a collection of books and monographs from
the eScholarship Editions collection of the Univer-
sity of California Press (UC Press),1 consisting of
289 books, each with a manually constructed back-
of-the-book index. The average length of the books
in this collection is 86053 words, and the average
length of the indexes is 820 entries. A collection
of 56 books was previously introduced in (Csomai
and Mihalcea, 2006); however, that collection is too
small to be split in training and test data to support
supervised keyword extraction experiments.
The UC Press collection was provided in a stan-
dardized XML format, following the Text Encoding
Initiative (TEI) recommendations, and thus it was
relatively easy to process the collection and separate
the index from the body of the text.
In order to use this corpus as a gold standard
collection for automatic index construction, we had
to eliminate the inversions, which are typical in
human-built indexes. Inversion is a method used by
professional indexers by which they break the order-
ing of the words in each index entry, and list the head
first, thereby making it easier to find entries in an
alphabetically ordered index. As an example, con-
sider the entry indexing of illustrations, which, fol-
lowing inversion, becomes illustrations, indexing of.
To eliminate inversion, we use an approach that gen-
1http://content.cdlib.org/escholarship/
erates each permutation of the composing words for
each index entry, looks up the frequency of that per-
mutation in the book, and then chooses the one with
the highest frequency as the correct reconstruction
of the entry. In this way, we identify the form of the
index entries as appearing in the book, which is the
form required for the evaluation of extraction meth-
ods. Entries that cannot be found in the book, which
were most likely generated by the human indexers,
are preserved in their original ordering.
For training and evaluation purposes, we used a
random split of the collection into 90% training and
10% test. This yields a training corpus of 259 docu-
ments and a test data set of 30 documents.
2.2 Candidate Index Entries
Every sequence of words in a book represents a po-
tential candidate for an entry in the back-of-the-book
index. Thus, we extract from the training and the test
data sets all the n-grams (up to the length of four),
not crossing sentence boundaries. These represent
the candidate index entries that will be used in the
classification algorithm. The training candidate en-
tries are then labeled as positive or negative, depend-
ing on whether the given n-gram was found in the
back-of-the-book index associated with the book.
Using a n-gram-based method to extract candidate
entries has the advantage of providing high cover-
age, but the unwanted effect of producing an ex-
tremely large number of entries. In fact, the result-
ing set is unmanageably large for any machine learn-
ing algorithm. Moreover, the set is extremely unbal-
anced, with a ratio of positive and negative exam-
ples of 1:675, which makes it unsuitable for most
machine learning algorithms. In order to address
this problem, we had to find ways to reduce the size
of the data set, possibly eliminating the training in-
stances that will have the least negative effect on the
usability of the data set.
The first step to reduce the size of the data set was
to use the candidate filtering techniques for unsuper-
vised back-of-the-book index construction that we
proposed in (Csomai and Mihalcea, 2007). Namely,
we use the commonword and comma filters, which
are applied to both the training and the test collec-
tions. These filters work by eliminating all the n-
grams that begin or end with a common word (we
use a list of 300 most frequent English words), as
well as those n-grams that cross a comma. This re-
sults in a significant reduction in the number of neg-
933
positive negative total positive:negative ratio
Training data
All (original) 71,853 48,499,870 48,571,723 1:674.98
Commonword/comma filters 66,349 11,496,661 11,563,010 1:173.27
10% undersampling 66,349 1,148,532 1,214,881 1:17.31
Test data
All (original) 7,764 6,157,034 6,164,798 1:793.02
Commonword/comma filters 7,225 1,472,820 1,480,045 1:203.85
Table 1: Number of training and test instances generated from the UC Press data set
ative examples, from 48 to 11 million instances, with
a loss in terms of positive examples of only 7.6%.
The second step is to use a technique for balanc-
ing the distribution of the positive and the negative
examples in the data sets. There are several meth-
ods proposed in the existing literature, focusing on
two main solutions: undersampling and oversam-
pling (Weiss and Provost, 2001). Undersampling
(Kubat and Matwin, 1997) means the elimination of
instances from the majority class (in our case nega-
tive examples), while oversampling focuses on in-
creasing the number of instances of the minority
class. Aside from the fact that oversampling has
hard to predict effects on classifier performance, it
also has the additional drawback of increasing the
size of the data set, which in our case is undesirable.
We thus adopted an undersampling solution, where
we randomly select 10% of the negative examples.
Evidently, the undersampling is applied only to the
training set.
Table 1 shows the number of positive and neg-
ative entries in the data set, for the different pre-
processing and balancing phases.
3 Features
An important step in the development of a super-
vised system is the choice of features used in the
learning process. Ideally, any property of a word or
a phrase indicating that it could be a good keyword
should be represented as a feature and included in
the training and test examples. We use a number
of features, including information-theoretic features
previously used in unsupervised keyword extraction,
as well as a novel set of features based on syntactic
and discourse properties of the text, or on informa-
tion extracted from external knowledge repositories.
3.1 Phraseness and Informativeness
We use the phraseness and informativeness features
that we previously proposed in (Csomai and Mihal-
cea, 2007). Phraseness refers to the degree to which
a sequence of words can be considered a phrase. We
use it as a measure of lexical cohesion of the com-
ponent terms and treat it as a collocation discovery
problem. Informativeness represents the degree to
which the keyphrase is representative for the docu-
ment at hand, and it correlates to the amount of in-
formation conveyed to the user.
To measure the informativeness of a keyphrase,
various methods can be used, some of which were
previously proposed in the keyword extraction liter-
ature:
? tf.idf, which is the traditional information re-
trieval metric (Salton and Buckley, 1997), em-
ployed in most existing keyword extraction ap-
plications. We measure inverse document fre-
quency using the article collection of the online
encyclopedia Wikipedia.
? ?2 independence test, which measures the de-
gree to which two events happen together more
often than by chance. In our work, we use the
?2 in a novel way. We measure the informa-
tiveness of a keyphrase by finding if a phrase
occurs in the document more frequently than
it would by chance. The information required
for the ?2 independence test can be typically
summed up in a contingency table (Manning
and Schutze, 1999):
count(phrase in count(all other phrases
document) in document)
count(phrase in other count(all other phrases
documents) in all other documents)
The independence score is calculated based on
the observed (O) and expected (E) counts:
?2 =
?
i,j
(Oi,j ? Ei,j)2
Ei,j
where i, j are the row and column indices of the
934
contingency table. The O counts are the cells of
the table. The E counts are calculated from the
marginal probabilities (the sum of the values of
a column or a row) converted into proportions
by dividing them with the total number of ob-
served events (N ):
N = O1,1 + O1,2 + O2,1 + O2,2
Then the expected count for seeing the phrase
in the document is:
E1,1 =
O1,1 + O1,2
N ?
O1,1 + O2,1
N ?N
To measure the phraseness of a candidate phrase
we use a technique based on the ?2 independence
test. We measure the independence of the events
of seeing the components of the phrase in the text.
This method was found to be one of the best per-
forming models in collocation discovery (Pecina and
Schlesinger, 2006). For n-grams where N > 2
we apply the ?2 independence test by splitting the
phrase in two (e.g. for a 4-gram, we measure the
independence of the composing bigrams).
3.2 Discourse Comprehension Features
Very few existing keyword extraction methods look
beyond word frequency. Except for (Turney and
Littman, 2003), who uses pointwise mutual infor-
mation to improve the coherence of the keyword set,
we are not aware of any other work that attempts
to use the semantics of the text to extract keywords.
The fact that most systems rely heavily on term fre-
quency properties poses serious difficulties, since
many index entries appear only once in the docu-
ment, and thus cannot be identified by features based
solely on word counts. For instance, as many as 52%
of the index entries in our training data set appeared
only once in the books they belong to. Moreover,
another aspect not typically covered by current key-
word extraction methods is the coherence of the key-
word set, which can also be addressed by discourse-
based properties.
In this section, we propose a novel feature for
keyword extraction inspired by work on discourse
comprehension. We use a construction integration
framework, which is the backbone used by many
discourse comprehension theories.
3.2.1 Discourse Comprehension
Discourse comprehension is a field in cognitive
science focusing on the modeling of mental pro-
cesses associated with reading and understanding
text. The most widely accepted theory for discourse
comprehension is the construction integration the-
ory (Kintsch, 1998). According to this theory,
the elementary units of comprehension are proposi-
tions, which are defined as instances of a predicate-
argument schema. As an example, consider the sen-
tence The hemoglobin carries oxygen, which gener-
ates the predicate CARRY[HEMOGLOBIN,OXIGEN].
The processing cycle of the construction integra-
tion model processes one proposition at a time, and
builds a local representation of the text in the work-
ing memory, called the propositional network.
During the construction phase, propositions are
extracted from a segment of the input text (typ-
ically a single sentence) using linguistic features.
The propositional network is represented as a graph,
with nodes consisting of propositions, and weighted
edges representing the semantic relations between
them. All the propositions generated from the in-
put text are inserted into the graph, as well as all the
propositions stored in the short term memory. The
short term memory contains the propositions that
compose the representation of the previous few sen-
tences. The second phase of the construction step
is the addition of past experiences (or background
knowledge), which is stored in the long term mem-
ory. This is accomplished by adding new elements
to the graph, usually consisting of the set of closely
related propositions from the long term memory.
After processing a sentence, the integration step
establishes the role of each proposition in the mean-
ing representation of the current sentence, through a
spreading activation applied on the propositions de-
rived from the original sentence. Once the weights
are stabilized, the set of propositions with the high-
est activation values give the mental representation
of the processed sentence. The propositions with
the highest activation values are added to the short
term memory, the working memory is cleared and
the process moves to the next sentence. Figure 3.2.1
shows the memory types used in the construction in-
tegration process and the main stages of the process.
3.2.2 Keyword Extraction using Discourse
Comprehension
The main purpose of the short term memory is to
ensure the coherence of the meaning representation
across sentences. By keeping the most important
propositions in the short term memory, the spreading
activation process transfers additional weight to se-
935
Semantic
Memory
Short-term
Memory
Add
Associates
AddPrevious
Propositions
Decay
Integration
Working
Memory
Next
Proposition
Figure 1: The construction integration process
mantically related propositions in the sentences that
follow. This also represents a way of alleviating one
of the main problems of statistical keyword extrac-
tion, namely the sole dependence on term frequency.
Even if a phrase appears only once, the construc-
tion integration process ensures the presence of the
phrase in the short term memory as long as it is rele-
vant to the current topic, thus being a good indicator
of the phrase importance.
The construction integration model is not directly
applicable to keyword extraction due to a number of
practical difficulties. The first implementation prob-
lem was the lack of a propositional parser. We solve
this problem by using a shallow parser to extract
noun phrase chunks from the original text (Munoz
et al, 1999). Second, since spreading activation is
a process difficult to control, with several parame-
ters that require fine tuning, we use instead a dif-
ferent graph centrality measure, namely PageRank
(Brin and Page, 1998).
Finally, to represent the relations inside the long
term semantic memory, we use a variant of latent
semantic analysis (LSA) (Landauer et al, 1998) as
implemented in the InfoMap package,2 trained on a
corpus consisting of the British National Corpus, the
English Wikipedia, and the books in our collection.
To alleviate the data sparsity problem, we also use
the pointwise mutual information (PMI) to calculate
the relatedness of the phrases based on the book be-
ing processed.
The final system works by iterating the following
steps: (1) Read the text sentence by sentence. For
each new sentence, a graph is constructed, consist-
ing of the noun phrase chunks extracted from the
original text. This set of nodes is augmented with
all the phrases from the short term memory. (2) A
2http://infomap.stanford.edu/
weighted edge is added between all the nodes, based
on the semantic relatedness measured between the
phrases by using LSA and PMI. We use a weighted
combination of these two measures, with a weight of
0.9 assigned to LSA and 0.1 to PMI. For the nodes
from the short term memory, we adjust the connec-
tion weights to account for memory decay, which is
a function of the distance from the last occurrence.
We implement decay by decreasing the weight of
both the outgoing and the incoming edges by n ? ?,
where n is the number of sentences since we last saw
the phrase and ? = 0.1. (3) Apply PageRank on
the resulting graph. (4) Select the 10 highest ranked
phrases and place them in the short term memory.
(5) Read the next sentence and go back to step (1).
Three different features are derived based on the
construction integration model:
? CI short term memory frequency (CI short-
term), which measures the number of iterations
that the phrase remains in the short term mem-
ory, which is seen as an indication of the phrase
importance.
? CI normalized short term memory fre-
quency (CI normalized), which is the same as
CI shortterm, except that it is normalized by the
frequency of the phrase. Through this normal-
ization, we hope to enhance the effect of the se-
mantic relatedness of the phrase to subsequent
sentences.
? CI maximum score (CI maxscore), which
measures the maximum centrality score the
phrase achieves across the entire book. This
can be thought of as a measure of the impor-
tance of the phrase in a smaller coherent seg-
ment of the document.
3.3 Syntactic Features
Previous work has pointed out the importance of
syntactic features for supervised keyword extraction
(Hulth, 2003). The construction integration model
described before is already making use of syntactic
patterns to some extent, through the use of a shal-
low parser to identify noun phrases. However, that
approach does not cover patterns other than noun
phrases. To address this limitation, we introduce a
new feature that captures the part-of-speech of the
words composing a candidate phrase.
936
There are multiple ways to represent such a fea-
ture. The simplest is to create a string feature con-
sisting of the concatenation of the part-of-speech
tags. However, this representation imposes limita-
tions on the machine learning algorithms that can
be used, since many learning systems cannot handle
string features. The second solution is to introduce
a binary feature for each part-of-speech tag pattern
found in the training and the test data sets. In our
case this is again unacceptable, given the size of the
documents we work with and the large number of
syntactic patterns that can be extracted. Instead, we
decided on a novel solution which, rather than us-
ing the part-of-speech pattern directly, determines
the probability of a phrase with a certain tag pattern
to be selected as a keyphrase. Formally:
P (pattern) = C(pattern, positive)C(pattern)
where C(pattern, positive) is the number of dis-
tinct phrases having the tag pattern pattern and be-
ing selected as keyword, and C(pattern) represents
the number of distinct phrases having the tag pattern
pattern. This probability is estimated based on the
training collection, and is used as a numeric feature.
We refer to this feature as part-of-speech pattern.
3.4 Encyclopedic Features
Recent work has suggested the use of domain
knowledge to improve the accuracy of keyword ex-
traction. This is typically done by consulting a vo-
cabulary of plausible keyphrases, usually in the form
of a list of subject headings or a domain specific
thesaurus. The use of a vocabulary has the addi-
tional benefit of eliminating the extraction of incom-
plete phrases (e.g. ?States of America?). In fact,
(Medelyan and Witten, 2006) reported an 110% F-
measure improvement in keyword extraction when
using a domain-specific thesaurus.
In our case, since the books can cover several do-
mains, the construction and use of domain-specific
thesauruses is not plausible, as the advantage of such
resources is offset by the time it usually takes to
build them. Instead, we decided to use encyclope-
dic information, as a way to ensure high coverage in
terms of domains and concepts.
We use Wikipedia, which is the largest and the
fastest growing encyclopedia available today, and
whose structure has the additional benefit of being
particularly useful for the task of keyword extrac-
tion. Wikipedia includes a rich set of links that con-
nect important phrases in an article to their corre-
sponding articles. These links are added manually
by the Wikipedia contributors, and follow the gen-
eral guidelines of annotation provided by Wikipedia.
The guidelines coincide with the goals of keyword
extraction, and thus the Wikipedia articles and their
link annotations can be treated as a vast keyword an-
notated corpus.
We make use of the Wikipedia annotations in two
ways. First, if a phrase is used as the title of a
Wikipedia article, or as the anchor text in a link,
this is a good indicator that the given phrase is well
formed. Second, we can also estimate the proba-
bility of a term W to be selected as a keyword in
a new document by counting the number of docu-
ments where the term was already selected as a key-
word (count(Dkey)) divided by the total number of
documents where the term appeared (count(DW )).
These counts are collected from the entire set of
Wikipedia articles.
P (keyword|W ) ? count(Dkey)count(DW )
(1)
This probability can be interpreted as ?the more
often a term was selected as a keyword among its
total number of occurrences, the more likely it is that
it will be selected again.? In the following, we will
refer to this feature as Wikipedia keyphraseness.
3.5 Other Features
In addition to the features described before, we add
several other features frequently used in keyword
extraction: the frequency of the phrase inside the
book (term frequency (tf)); the number of documents
that include the phrase (document frequency (df)); a
combination of the two (tf.idf); the within-document
frequency, which divides a book into ten equally-
sized segments, and counts the number of segments
that include the phrase (within document frequency);
the length of the phrase (length of phrase); and fi-
nally a binary feature indicating whether the given
phrase is a named entity, according to a simple
heuristic based on word capitalization.
4 Experiments and Evaluation
We integrate the features described in the previous
section in a machine learning framework. The sys-
tem is evaluated on the data set described in Sec-
tion 2.1, consisting of 289 books, randomly split into
937
90% training (259 books) and 10% test (30 books).
We experiment with three learning algorithms, se-
lected for the diversity of their learning strategy:
multilayer perceptron, SVM, and decision trees. For
all three algorithms, we use their implementation as
available in the Weka package.
For evaluation, we use the standard information
retrieval metrics: precision, recall, and F-measure.
We use two different mechanisms for selecting the
number of entries in the index. In the first evaluation
(ratio-based), we use a fixed ratio of 0.45% from the
number of words in the text; for instance, if a book
has 100,000 words, the index will consist of 450 en-
tries. This number was estimated based on previous
observations regarding the typical size of a back-of-
the-book index (Csomai and Mihalcea, 2006). In
order to match the required number of entries, we
sort all the candidates in reversed order of the confi-
dence score assigned by the machine learning algo-
rithm, and consequently select the top entries in this
ranking. In the second evaluation (decision-based),
we allow the machine learning algorithm to decide
on the number of keywords to extract. Thus, in this
evaluation, all the candidates labeled as keywords
by the learning algorithm will be added to the index.
Note that all the evaluations are run using a train-
ing data set with 10% undersampling of the negative
examples, as described before.
Table 2 shows the results of the evaluation. As
seen in the table, the multilayer perceptron and the
decision tree provide the best results, for an over-
all average F-measure of 27%. Interestingly, the re-
sults obtained when the number of keywords is auto-
matically selected by the learning method (decision-
based) are comparable to those when the number of
keywords is selected a-priori (ratio-based), indicat-
ing the ability of the machine learning algorithm to
correctly identify the correct keywords.
Additionally, we also ran an experiment to de-
termine the amount of training data required by the
system. While the learning curve continues to grow
with additional amounts of data, the steepest part of
the curve is observed for up to 10% of the training
data, which indicates that a relatively small amount
of data (about 25 books) is enough to sustain the sys-
tem.
It is worth noting that the task of creating back-
of-the-book indexes is highly subjective. In order
to put the performance figures in perspective, one
should also look at the inter-annotator agreement be-
tween human indexers as an upper bound of per-
formance. Although we are not aware of any com-
prehensive studies for inter-annotator agreement on
back-of-the-book indexing, we can look at the con-
sistency studies that have been carried out on the
MEDLINE corpus (Funk and Reid, 1983), where an
inter-annotator agreement of 48% was found on an
indexing task using a domain-specific controlled vo-
cabulary of subject headings.
4.1 Comparison with Other Systems
We compare the performance of our system with two
other methods for keyword extraction. One is the
tf.idf method, traditionally used in information re-
trieval as a mechanism to assign words in a text with
a weight reflecting their importance. This tf.idf base-
line system uses the same candidate extraction and
filtering techniques as our supervised systems. The
other baseline is the KEA keyword extraction system
(Frank et al, 1999), a state-of-the-art algorithm for
supervised keyword extraction. Very briefly, KEA is
a supervised system that uses a Na??ve Bayes learn-
ing algorithm and several features, including infor-
mation theoretic features such as tf.idf and positional
features reflecting the position of the words with re-
spect to the beginning of the text. The KEA system
was trained on the same training data set as used in
our experiments.
Table 3 shows the performance obtained by these
methods on the test data set. Since none of these
methods have the ability to automatically determine
the number of keywords to be extracted, the evalua-
tion of these methods is done under the ratio-based
setting, and thus for each method the top 0.45%
ranked keywords are extracted.
Algorithm P R F
tf.idf 8.09 8.63 8.35
KEA 11.18 11.48 11.32
Table 3: Baseline systems
4.2 Performance of Individual Features
We also carried out experiments to determine the
role played by each feature, by using the informa-
tion gain weight as assigned by the learning algo-
rithm. Note that ablation studies are not appropri-
ate in our case, since the features are not orthogonal
(e.g., there is high redundancy between the construc-
tion integration and the informativeness features),
and thus we cannot entirely eliminate a feature from
the system.
938
ratio-based decision-based
Algorithm P R F P R F
Multilayer perceptron 27.98 27.77 27.87 23.93 31.98 27.38
Decision tree 27.06 27.13 27.09 22.75 34.12 27.30
SVM 20.94 20.35 20.64 21.76 30.27 25.32
Table 2: Evaluation results
Feature Weight
part-of-speech pattern 0.1935
CI shortterm 0.1744
Wikipedia keyphraseness 0.1731
CI maxscore 0.1689
CI shortterm normalized 0.1379
ChiInformativeness 0.1122
document frequency (df) 0.1031
tf.idf 0.0870
ChiPhraseness 0.0660
length of phrase 0.0416
named entity heuristic 0.0279
within document frequency 0.0227
term frequency (tf) 0.0209
Table 4: Information gain feature weight
Table 4 shows the weight associated with each
feature. Perhaps not surprisingly, the features
with the highest weight are the linguistically moti-
vated features, including syntactic patterns and the
construction integration features. The Wikipedia
keyphraseness also has a high score. The smallest
weights belong to the information theoretic features,
including term and document frequency.
5 Related Work
With a few exceptions (Schutze, 1998; Csomai and
Mihalcea, 2007), very little work has been carried
out to date on methods for automatic back-of-the-
book index construction.
The task that is closest to ours is perhaps keyword
extraction, which targets the identification of the
most important words or phrases inside a document.
The state-of-the-art in keyword extraction is cur-
rently represented by supervised learning methods,
where a system is trained to recognize keywords in a
text, based on lexical and syntactic features. This ap-
proach was first suggested in (Turney, 1999), where
parameterized heuristic rules are combined with a
genetic algorithm into a system for keyphrase ex-
traction (GenEx) that automatically identifies key-
words in a document. A different learning algo-
rithm was used in (Frank et al, 1999), where a Naive
Bayes learning scheme is applied on the document
collection, with improved results observed on the
same data set as used in (Turney, 1999). Neither Tur-
ney nor Frank report on the recall of their systems,
but only on precision: a 29.0% precision is achieved
with GenEx (Turney, 1999) for five keyphrases ex-
tracted per document, and 18.3% precision achieved
with Kea (Frank et al, 1999) for fifteen keyphrases
per document. Finally, in recent work, (Hulth, 2003)
proposes a system for keyword extraction from ab-
stracts that uses supervised learning with lexical and
syntactic features, which proved to improve signifi-
cantly over previously published results.
6 Conclusions and Future Work
In this paper, we introduced a supervised method for
back-of-the-book indexing which relies on a novel
set of features, including features based on discourse
comprehension, syntactic patterns, and information
drawn from an online encyclopedia. According to
an information gain measure of feature importance,
the new features performed significantly better than
the traditional frequency-based techniques, leading
to a system with an F-measure of 27%. This rep-
resents an improvement of 140% with respect to a
state-of-the-art supervised method for keyword ex-
traction. Our system proved to be successful both
in ranking the phrases in terms of their suitability as
index entries, as well as in determining the optimal
number of entries to be included in the index. Fu-
ture work will focus on developing methodologies
for computer-assisted back-of-the-book indexing, as
well as on the use of the automatically extracted in-
dexes in improving the browsing of digital libraries.
Acknowledgments
We are grateful to Kirk Hastings from the Califor-
nia Digital Library for his help in obtaining the UC
Press corpus. This research has been partially sup-
ported by a grant from Google Inc. and a grant from
the Texas Advanced Research Program (#003594).
939
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
A. Csomai and R. Mihalcea. 2006. Creating a testbed
for the evaluation of automatically generated back-of-
the-book indexes. In Proceedings of the International
Conference on Computational Linguistics and Intelli-
gent Text Processing, pages 19?25, Mexico City.
A. Csomai and R. Mihalcea. 2007. Investigations in
unsupervised back-of-the-book indexing. In Proceed-
ings of the Florida Artificial Intelligence Research So-
ciety, Key West.
D. Feng, J. Kim, E. Shaw, and E. Hovy. 2006. Towards
modeling threaded discussions through ontology-
based analysis. In Proceedings of National Confer-
ence on Artificial Intelligence.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proceedings of the 16th In-
ternational Joint Conference on Artificial Intelligence.
M. E. Funk and C.A. Reid. 1983. Indexing consistency
in medline. Bulletin of the Medical Library Associa-
tion, 71(2).
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing, Japan, August.
W. Kintsch. 1998. Comprehension: A paradigm for cog-
nition. Cambridge Uniersity Press.
M. Kubat and S. Matwin. 1997. Addressing the curse
of imbalanced training sets: one-sided selection. In
Proceedings of the 14th International Conference on
Machine Learning.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
C. Manning and H. Schutze. 1999. Foundations of Natu-
ral Language Processing. MIT Press.
O. Medelyan and I. H. Witten. 2006. Thesaurus based
automatic keyphrase indexing. In Proceedings of the
Joint Conference on Digital Libraries.
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In Pro-
ceedings of the Conference on Empirical Methods for
Natural Language Processing.
P. Pecina and P. Schlesinger. 2006. Combining asso-
ciation measures for collocation extraction. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 651?658, Sydney, Australia.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
H. Schutze. 1998. The hypertext concordance: a better
back-of-the-book index. In Proceedings of Comput-
erm, pages 101?104.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from as-
sociation. ACM Transactions on Information Systems,
4(21):315?346.
P. Turney. 1999. Learning to extract keyphrases from
text. Technical report, National Research Council, In-
stitute for Information Technology.
G. Weiss and F. Provost. 2001. The effect of class distri-
bution on classifier learning. Technical Report ML-TR
43, Rutgers University.
940
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309?312,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
The Lie Detector: Explorations in the Automatic Recognition
of Deceptive Language
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Carlo Strapparava
FBK-IRST
strappa@fbk.eu
Abstract
In this paper, we present initial experi-
ments in the recognition of deceptive lan-
guage. We introduce three data sets of true
and lying texts collected for this purpose,
and we show that automatic classification
is a viable technique to distinguish be-
tween truth and falsehood as expressed in
language. We also introduce a method for
class-based feature analysis, which sheds
some light on the features that are charac-
teristic for deceptive text.
You should not trust the devil, even if he tells the truth.
? Thomas of Aquin (medieval philosopher)
1 Introduction and Motivation
The discrimination between truth and falsehood
has received significant attention from fields as
diverse as philosophy, psychology and sociology.
Recent advances in computational linguistics mo-
tivate us to approach the recognition of deceptive
language from a data-driven perspective, and at-
tempt to identify the salient features of lying texts
using natural language processing techniques.
In this paper, we explore the applicability of
computational approaches to the recognition of
deceptive language. In particular, we investigate
whether automatic classification techniques repre-
sent a viable approach to distinguish between truth
and lies as expressed in written text. Although
acoustic and other non-linguistic features were
also found to be useful for this task (Hirschberg
et al, 2005), we deliberately focus on written lan-
guage, since it represents the type of data most fre-
quently encountered on the Web (e.g., chats, fo-
rums) or in other collections of documents.
Specifically, we try to answer the following two
questions. First, are truthful and lying texts sep-
arable, and does this property hold for different
datasets? To answer this question, we use three
different data sets that we construct for this pur-
pose ? consisting of true and false short statements
on three different topics ? and attempt to automat-
ically separate them using standard natural lan-
guage processing techniques.
Second, if truth and lies are separable, what are
the distinctive features of deceptive texts? In an-
swer to this second question, we attempt to iden-
tify some of the most salient features of lying texts,
and analyse their occurrence in the three data sets.
The paper is organized as follows. We first
briefly review the related work, followed by a de-
scription of the three data sets that we constructed.
Next, we present our experiments and results using
automatic classification, and introduce a method
for the analysis of salient features in deceptive
texts. Lastly, we conclude with a discussion and
directions for future work.
2 Related Work
Very little work, if any, has been carried out on the
automatic detection of deceptive language in writ-
ten text. Most of the previous work has focused
on the psychological or social aspects of lying, and
there are only a few previous studies that have con-
sidered the linguistic aspects of falsehood.
In psychology, it is worthwhile mentioning the
study reported in (DePaulo et al, 2003), where
more than 100 cues to deception are mentioned.
However, only a few of them are linguistic in na-
ture, as e.g., word and phrase repetitions, while
most of the cues involve speaker?s behavior, in-
cluding facial expressions, eye shifts, etc. (New-
man et al, 2003) also report on a psycholinguistic
study, where they conduct a qualitative analysis of
true and false stories by using word counting tools.
Computational work includes the study of
(Zhou et al, 2004), which studied linguistic cues
for deception detection in the context of text-based
asynchronous computer mediated communication,
and (Hirschberg et al, 2005) who focused on de-
ception in speech using primarily acoustic and
prosodic features.
Our work is also related to the automatic clas-
sification of text genre, including work on author
profiling (Koppel et al, 2002), humor recognition
309
TRUTH LIE
ABORTION
I believe abortion is not an option. Once a life has been
conceived, it is precious. No one has the right to decide
to end it. Life begins at conception,because without con-
ception, there is no life.
A woman has free will and free choice over what goes
on in her body. If the child has not been born, it is under
her control. Often the circumstances an unwanted child
is born into are worse than death. The mother has the
responsibility to choose the best course for her child.
DEATH PENALTY
I stand against death penalty. It is pompous of anyone
to think that they have the right to take life. No court of
law can eliminate all possibilities of doubt. Also, some
circumstances may have pushed a person to commit a
crime that would otherwise merit severe punishment.
Death penalty is very important as a deterrent against
crime. We live in a society, not as individuals. This
imposes some restrictions on our actions. If a person
doesn?t adhere to these restrictions, he or she forfeits her
life. Why should taxpayers? money be spent on feeding
murderers?
BEST FRIEND
I have been best friends with Jessica for about seven
years now. She has always been there to help me out.
She was even in the delivery room with me when I had
my daughter. She was also one of the Bridesmaids in
my wedding. She lives six hours away, but if we need
each other we?ll make the drive without even thinking.
I have been friends with Pam for almost four years now.
She?s the sweetest person I know. Whenever we need
help she?s always there to lend a hand. She always has
a kind word to say and has a warm heart. She is my
inspiration.
Table 1: Sample true and deceptive statements
(Mihalcea and Strapparava, 2006), and others.
3 Data Sets
To study the distinction between true and decep-
tive statements, we required a corpus with explicit
labeling of the truth value associated with each
statement. Since we were not aware of any such
data set, we had to create one ourselves. We fo-
cused on three different topics: opinions on abor-
tion, opinions on death penalty, and feelings about
the best friend. For each of these three topics
an annotation task was defined using the Amazon
Mechanical Turk service.
For the first two topics (abortion and death
penalty), we provided instructions that asked the
contributors to imagine they were taking part in
a debate, and had 10-15 minutes available to ex-
press their opinion about the topic. First, they were
asked to prepare a brief speech expressing their
true opinion on the topic. Next, they were asked
to prepare a second brief speech expressing the op-
posite of their opinion, thus lying about their true
beliefs about the topic. In both cases, the guide-
lines asked for at least 4-5 sentences and as many
details as possible.
For the third topic (best friend), the contributors
were first asked to think about their best friend and
describe the reasons for their friendship (including
facts and anecdotes considered relevant for their
relationship). Thus, in this case, they were asked
to tell the truth about how they felt about their best
friend. Next, they were asked to think about a per-
son they could not stand, and describe it as if s/he
were their best friend. In this second case, they
had to lie about their feelings toward this person.
As before, in both cases the instructions asked for
at least 4-5 detailed sentences.
We collected 100 true and 100 false statements
for each topic, with an average of 85 words per
statement. Previous work has shown that data
collected through the Mechanical Turk service is
reliable and comparable in quality with trusted
sources (Snow et al, 2008). We also made a man-
ual verification of the quality of the contributions,
and checked by hand the quality of all the contri-
butions. With two exceptions ? two entries where
the true and false statements were identical, which
were removed from the data ? all the other entries
were found to be of good quality, and closely fol-
lowing our instructions.
Table 1 shows an example of true and deceptive
language for each of the three topics.
4 Experimental Setup and Results
For the experiments, we used two classifiers:
Na??ve Bayes and SVM, selected based on their
performance and diversity of learning methodolo-
gies. Only minimal preprocessing was applied
to the three data sets, which included tokeniza-
tion and stemming. No feature selection was per-
formed, and stopwords were not removed.
Table 2 shows the ten-fold cross-validation re-
sults using the two classifiers. Since all three data
sets have an equal distribution between true and
false statements, the baseline for all the topics is
50%. The average classification performance of
70% ? significantly higher than the 50% baseline
? indicates that good separation can be obtained
310
between true and deceptive language by using au-
tomatic classifiers.
Topic NB SVM
ABORTION 70.0% 67.5%
DEATH PENALTY 67.4% 65.9%
BEST FRIEND 75.0% 77.0%
AVERAGE 70.8% 70.1%
Table 2: Ten-fold cross-validation classification
results, using a Na??ve Bayes (NB) or Support Vec-
tor Machines (SVM) classifier
To gain further insight into the variation of ac-
curacy with the amount of data available, we also
plotted the learning curves for each of the data
sets, as shown in Figure 1. The overall growing
trend indicates that more data is likely to improve
the accuracy, thus suggesting the collection of ad-
ditional data as a possible step for future work.
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Cl
as
sif
ic
at
io
n 
ac
cu
ra
cy
 (%
)
Fraction of data (%)
Classification learning curves
Abortion
Death penalty
Best friend
Figure 1: Classification learning curves.
We also tested the portability of the classifiers
across topics, using two topics as training data and
the third topic as test. The results are shown in Ta-
ble 3. Although below the in-topic performance,
the average accuracy is still significantly higher
than the 50% baseline, indicating that the learning
process relies on clues specific to truth/deception,
and it is not bound to a particular topic.
5 Identifying Dominant Word Classes in
Deceptive Text
In order to gain a better understanding of the char-
acteristics of deceptive text, we devised a method
to calculate a score associated with a given class
of words, as a measure of saliency for the given
word class inside the collection of deceptive (or
truthful) texts.
Given a class of words C = {W1,W2, ...,WN},
we define the class coverage in the deceptive cor-
pus D as the percentage of words from D belong-
ing to the class C:
CoverageD(C) =
?
Wi?C
FrequencyD(Wi)
SizeD
where FrequencyD(Wi) represents the total
number of occurrences of word Wi inside the cor-
pus D, and SizeD represents the total size (in
words) of the corpus D.
Similarly, we define the class C coverage for the
truthful corpus T :
CoverageT (C) =
?
Wi?C
FrequencyT (Wi)
SizeT
The dominance score of the class C in the de-
ceptive corpus D is then defined as the ratio be-
tween the coverage of the class in the corpus D
with respect to the coverage of the same class in
the corpus T :
DominanceD(C) =
CoverageD(C)
CoverageT (C)
(1)
A dominance score close to 1 indicates a similar
distribution of the words in the class C in both the
deceptive and the truthful corpus. Instead, a score
significantly higher than 1 indicates a class that is
dominant in the deceptive corpus, and thus likely
to be a characteristic of the texts in this corpus.
Finally, a score significantly lower than 1 indicates
a class that is dominant in the truthful corpus, and
unlikely to appear in the deceptive corpus.
We use the classes of words as defined in
the Linguistic Inquiry and Word Count (LIWC),
which was developed as a resource for psycholin-
guistic analysis (Pennebaker and Francis, 1999).
The 2001 version of LIWC includes about 2,200
words and word stems grouped into about 70
broad categories relevant to psychological pro-
cesses (e.g., EMOTION, COGNITION). The LIWC
lexicon has been validated by showing significant
correlation between human ratings of a large num-
ber of written texts and the rating obtained through
LIWC-based analyses of the same texts.
All the word classes from LIWC are ranked ac-
cording to the dominance score calculated with
formula 1, using a mix of all three data sets to
create the D and T corpora. Those classes that
have a high score are the classes that are dom-
inant in deceptive text. The classes that have a
small score are the classes that are dominant in
truthful text and lack from deceptive text. Table 4
shows the top ranked classes along with their dom-
inance score and a few sample words that belong
to the given class and also appeared in the decep-
tive (truthful) texts.
Interestingly, in both truthful and deceptive lan-
guage, three of the top five dominant classes are
related to humans. In deceptive texts however, the
311
Training Test NB SVM
DEATH PENALTY + BEST FRIEND ABORTION 62.0% 61.0%
ABORTION + BEST FRIEND DEATH PENALTY 58.7% 58.7%
ABORTION + DEATH PENALTY BEST FRIEND 58.7% 53.6%
AVERAGE 59.8% 57.8%
Table 3: Cross-topic classification results
Class Score Sample words
Deceptive Text
METAPH 1.71 god, die, sacred, mercy, sin, dead, hell, soul, lord, sins
YOU 1.53 you, thou
OTHER 1.47 she, her, they, his, them, him, herself, himself, themselves
HUMANS 1.31 person, child, human, baby, man, girl, humans, individual, male, person, adult
CERTAIN 1.24 always, all, very, truly, completely, totally
Truthful Text
OPTIM 0.57 best, ready, hope, accepts, accept, determined, accepted, won, super
I 0.59 I, myself, mine
FRIENDS 0.63 friend, companion, body
SELF 0.64 our, myself, mine, ours
INSIGHT 0.65 believe, think, know, see, understand, found, thought, feels, admit
Table 4: Dominant word classes in deceptive text, along with sample words.
human-related word classes (YOU, OTHER, HU-
MANS) represent detachment from the self, as if
trying not to have the own self involved in the
lies. Instead, the classes of words that are closely
connected to the self (I, FRIENDS, SELF) are lack-
ing from deceptive text, being dominant instead in
truthful statements, where the speaker is comfort-
able with identifying herself with the statements
she makes.
Also interesting is the fact that words related
to certainty (CERTAIN) are more dominant in de-
ceptive texts, which is probably explained by the
need of the speaker to explicitly use truth-related
words as a means to emphasize the (fake) ?truth?
and thus hide the lies. Instead, belief-oriented vo-
cabulary (INSIGHT), such as believe, feel, think,
is more frequently encountered in truthful state-
ments, where the presence of the real truth does
not require truth-related words for emphasis.
6 Conclusions
In this paper, we explored automatic techniques
for the recognition of deceptive language in writ-
ten texts. Through experiments carried out on
three data sets, we showed that truthful and ly-
ing texts are separable, and this property holds
for different data sets. An analysis of classes of
salient features indicated some interesting patterns
of word usage in deceptive texts, including detach-
ment from the self and vocabulary that emphasizes
certainty. In future work, we plan to explore the
role played by affect and the possible integration
of automatic emotion analysis into the recognition
of deceptive language.
References
B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,
K. Charlton, and H. Cooper. 2003. Cues to decep-
tion. Psychological Bulletin, 129(1):74?118.
J. Hirschberg, S. Benus, J. Brenier, F. Enos, S. Fried-
man, S. Gilman, C. Girand, M. Graciarena,
A. Kathol, L. Michaelis, B. Pellom, E. Shriberg,
and A. Stolcke. 2005. Distinguishing decep-
tive from non-deceptive speech. In Proceedings of
INTERSPEECH-2005, Lisbon, Portugal.
M. Koppel, S. Argamon, and A. Shimoni. 2002. Au-
tomatically categorizing written texts by author gen-
der. Literary and Linguistic Computing, 4(17):401?
412.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Computational Intelligence,
22(2):126?142.
M. Newman, J. Pennebaker, D. Berry, and J. Richards.
2003. Lying words: Predicting deception from lin-
guistic styles. Personality and Social Psychology
Bulletin, 29:665?675.
J. Pennebaker and M. Francis. 1999. Linguistic in-
quiry and word count: LIWC. Erlbaum Publishers.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good? evaluating non-
expert annotations for natural language tasks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Honolulu,
Hawaii.
L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.
2004. Automating linguistics-based cues for detect-
ing deception in text-based asynchronous computer-
mediated communication. Group Decision and Ne-
gotiation, 13:81?106.
312
                     	 
       	   	            
                    
        An Evaluation Exercise for Word Alignment
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
This paper presents the task definition, re-
sources, participating systems, and compara-
tive results for the shared task on word align-
ment, which was organized as part of the
HLT/NAACL 2003 Workshop on Building and
Using Parallel Texts. The shared task in-
cluded Romanian-English and English-French
sub-tasks, and drew the participation of seven
teams from around the world.
1 Defining a Word Alignment Shared Task
The task of word alignment consists of finding correspon-
dences between words and phrases in parallel texts. As-
suming a sentence aligned bilingual corpus in languages
L1 and L2, the task of a word alignment system is to indi-
cate which word token in the corpus of language L1 cor-
responds to which word token in the corpus of language
L2.
As part of the HLT/NAACL 2003 workshop on ?Build-
ing and Using Parallel Texts: Data Driven Machine
Translation and Beyond?, we organized a shared task on
word alignment, where participating teams were provided
with training and test data, consisting of sentence aligned
parallel texts, and were asked to provide automatically
derived word alignments for all the words in the test set.
Data for two language pairs were provided: (1) English-
French, representing languages with rich resources (20
million word parallel texts), and (2) Romanian-English,
representing languages with scarce resources (1 million
word parallel texts). Similar with the Machine Transla-
tion evaluation exercise organized by NIST1, two sub-
tasks were defined, with teams being encouraged to par-
ticipate in both subtasks.
1http://www.nist.gov/speech/tests/mt/
1. Limited resources, where systems are allowed to use
only the resources provided.
2. Unlimited resources, where systems are allowed to
use any resources in addition to those provided.
Such resources had to be explicitly mentioned in the
system description.
Test data were released one week prior to the deadline
for result submissions. Participating teams were asked
to produce word alignments, following a common format
as specified below, and submit their output by a certain
deadline. Results were returned to each team within three
days of submission.
1.1 Word Alignment Output Format
The word alignment result files had to include one line
for each word-to-word alignment. Additionally, lines in
the result files had to follow the format specified in Fig.1.
While the  and confidence fields overlap in their
meaning, the intent of having both fields available is to
enable participating teams to draw their own line on what
they consider to be a Sure or Probable alignment. Both
these fields were optional, with some standard values as-
signed by default.
1.1.1 A Running Word Alignment Example
Consider the following two aligned sentences:
[English] s snum=18 They had gone . /s
[French] s snum=18 Ils etaient alles . /s
A correct word alignment for this sentence is
18 1 1
18 2 2
18 3 3
18 4 4
stating that: all the word alignments pertain to sentence
18, the English token 1 They aligns with the French to-
ken 1 Ils, the English token 2 had, aligns with the French
token 2 etaient, and so on. Note that punctuation is also
sentence no position L1 position L2 [ ] [confidence]
where:
? sentence no represents the id of the sentence within the
test file. Sentences in the test data already have an id as-
signed. (see the examples below)
? position L1 represents the position of the token that is
aligned from the text in language L1; the first token in each
sentence is token 1. (not 0)
? position L2 represents the position of the token that is
aligned from the text in language L2; again, the first token
is token 1.
?  can be either S or P, representing a Sure or Probable
alignment. All alignments that are tagged as S are also con-
sidered to be part of the P alignments set (that is, all align-
ments that are considered ?Sure? alignments are also part of
the ?Probable? alignments set). If the  field is missing, a
value of S will be assumed by default.
? confidence is a real number, in the range (0-1] (1 meaning
highly confident, 0 meaning not confident); this field is op-
tional, and by default confidence number of 1 was assumed.
Figure 1: Word Alignment file format
aligned (English token 4 aligned with French token 4),
and counts towards the final evaluation figures.
Alternatively, systems could also provide an 
marker and/or a confidence score, as shown in the fol-
lowing example:
18 1 1 1
18 2 2 P 0.7
18 3 3 S
18 4 4 S 1
with missing  fields considered by default to be S,
and missing confidence scores considered by default 1.
1.2 Annotation Guide for Word Alignments
The annotation guide and illustrative word alignment ex-
amples were mostly drawn from the Blinker Annotation
Project. Please refer to (Melamed, 1999, pp. 169?182)
for additional details.
1. All items separated by a white space are considered
to be a word (or token), and therefore have to be
aligned. (punctuation included)
2. Omissions in translation use the NULL token, i.e.
token with id 0. For instance, in the examples below:
[English]: s snum=18 And he said , appoint me
thy wages , and I will give it . /s
[French]: s snum=18 fixe moi ton salaire , et je
te le donnerai . /s
and he said from the English sentence has no cor-
responding translation in French, and therefore all
these words are aligned with the token id 0.
...
18 1 0
18 2 0
18 3 0
18 4 0
...
3. Phrasal correspondences produce multiple word-to-
word alignments. For instance, in the examples be-
low:
English: s snum=18 cultiver la terre /s
French: s snum=18 to be a husbandman /s
since the words do not correspond one to one, and
yet the two phrases mean the same thing in the given
context, the phrases should be linked as wholes, by
linking each word in one to each word in another.
For the example above, this translates into 12 word-
to-word alignments:
18 1 1 18 1 2
18 1 3 18 1 4
18 2 1 18 2 2
18 2 3 18 2 4
18 3 1 18 3 2
18 3 3 18 3 4
2 Resources
The shared task included two different language pairs:
the alignment of words in English-French parallel texts,
and in Romanian-English parallel texts. For each lan-
guage pair, training data were provided to participants.
Systems relying only on these resources were considered
part of the Limited Resources subtask. Systems making
use of any additional resources (e.g. bilingual dictionar-
ies, additional parallel corpora, and others) were classi-
fied under the Unlimited Resources category.
2.1 Training Data
Two sets of training data were made available.
1. A set of Romanian-English parallel texts, consist-
ing of about 1 million Romanian words, and about
the same number of English words. These data con-
sisted of:
 Parallel texts collected from the Web using a
semi-supervised approach. The URLs format
for pages containing potential parallel transla-
tions were manually identified (mainly from
the archives of Romanian newspapers). Next,
texts were automatically downloaded and sen-
tence aligned. A manual verification of the
alignment was also performed. These data col-
lection process resulted in a corpus of about
850,000 Romanian words, and about 900,000
English words.
 Orwell?s 1984, aligned within the MULTEXT-
EAST project (Erjavec et al, 1997), with about
130,000 Romanian words, and a similar num-
ber of English words.
 The Romanian Constitution, for about 13,000
Romanian words and 13,000 English words.
2. A set of English-French parallel texts, consisting of
about 20 million English words, and about the same
number of French words. This is a subset of the
Canadian Hansards, processed and sentence aligned
by Ulrich Germann at ISI (Germann, 2001).
All data were pre-tokenized. For English and French,
we used a version of the tokenizers provided within the
EGYPT Toolkit2. For Romanian, we used our own tok-
enizer. Identical tokenization procedures were used for
training, trial, and test data.
2.2 Trial Data
Two sets of trial data were made available at the same
time training data became available. Trial sets consisted
of sentence aligned texts, provided together with man-
ually determined word alignments. The main purpose
of these data was to enable participants to better under-
stand the format required for the word alignment result
files. Trial sets consisted of 37 English-French, and 17
Romanian-English aligned sentences.
2.3 Test Data
A total of 447 English-French aligned sentences (Och
and Ney, 2000), and 248 Romanian-English aligned sen-
tences were released one week prior to the deadline. Par-
ticipants were required to run their word alignment sys-
tems on these two sets, and submit word alignments.
Teams were allowed to submit an unlimited number of
results sets for each language pair.
2.3.1 Gold Standard Word Aligned Data
The gold standard for the two language pair alignments
were produced using slightly different alignment proce-
dures, which allowed us to study different schemes for
producing gold standards for word aligned data.
For English-French, annotators where instructed to as-
sign a Sure or Probable tag to each word alignment they
produced. The intersection of the Sure alignments pro-
duced by the two annotators led to the final Sure aligned
set, while the reunion of the Probable alignments led to
the final Probable aligned set. The Sure alignment set is
2http://www.clsp.jhu.edu/ws99/projects/mt/toolkit/
guaranteed to be a subset of the Probable alignment set.
The annotators did not produce any NULL alignments.
Instead, we assigned NULL alignments as a default back-
up mechanism, which forced each word to belong to at
least one alignment. The English-French aligned data
were produced by Franz Och and Hermann Ney (Och and
Ney, 2000).
For Romanian-English, annotators were instructed to
assign an alignment to all words, with specific instruc-
tions as to when to assign a NULL alignment. Annota-
tors were not asked to assign a Sure or Probable label.
Instead, we had an arbitration phase, where a third anno-
tator judged the cases where the first two annotators dis-
agreed. Since an inter-annotator agreement was reached
for all word alignments, the final resulting alignments
were considered to be Sure alignments.
3 Evaluation Measures
Evaluations were performed with respect to four differ-
ent measures. Three of them ? precision, recall, and F-
measure ? represent traditional measures in Information
Retrieval, and were also frequently used in previous word
alignment literature. The fourth measure was originally
introduced by (Och and Ney, 2000), and proposes the no-
tion of quality of word alignment.
Given an alignment , and a gold standard alignment
, each such alignment set eventually consisting of two
sets 

, 

, and 

, 

corresponding to Sure and
Probable alignments, the following measures are defined
(where  is the alignment type, and can be set to either S
or P).











(1)











(2)
	










(3)

  




 






 


(4)
Each word alignment submission was evaluated in
terms of the above measures. Moreover, we conducted
two sets of evaluations for each submission:
 NULL-Align, where each word was enforced to be-
long to at least one alignment; if a word did not be-
long to any alignment, a NULL Probable alignment
was assigned by default. This set of evaluations per-
tains to full coverage word alignments.
 NO-NULL-Align, where all NULL alignments were
removed from both submission file and gold stan-
dard data.
Team System name Description
Language Technologies Institute, CMU BiBr (Zhao and Vogel, 2003)
MITRE Corporation Fourday (Henderson, 2003)
RALI - Universite? the Montre?al Ralign (Simard and Langlais, 2003)
Romanian Academy Institute of Artificial Intelligence RACAI (Tufis? et al, 2003)
University of Alberta ProAlign (Lin and Cherry, 2003)
University of Minnesota, Duluth UMD (Thomson McInnes and Pedersen, 2003)
Xerox Research Centre Europe XRCE (Dejean et al, 2003)
Table 1: Teams participating in the word alignment shared task
We conducted therefore 14 evaluations for each
submission file: AER, Sure/Probable Precision,
Sure/Probable Recall, and Sure/Probable F-measure,
with a different figure determined for NULL-Align and
NO-NULL-Align alignments.
4 Participating Systems
Seven teams from around the world participated in the
word alignment shared task. Table 1 lists the names of
the participating systems, the corresponding institutions,
and references to papers in this volume that provide de-
tailed descriptions of the systems and additional analysis
of their results.
All seven teams participated in the Romanian-English
subtask, and five teams participated in the English-French
subtask.3 There were no restrictions placed on the num-
ber of submissions each team could make. This resulted
in a total of 27 submissions from the seven teams, where
14 sets of results were submitted for the English-French
subtask, and 13 for the Romanian-English subtask. Of
the 27 total submissions, there were 17 in the Limited re-
sources subtask, and 10 in the Unlimited resources sub-
task. Tables 2 and 3 show all of the submissions for each
team in the two subtasks, and provide a brief description
of their approaches.
While each participating system was unique, there
were a few unifying themes.
Four teams had approaches that relied (to varying de-
grees) on an IBM model of statistical machine translation
(Brown et al, 1993). UMD was a straightforward imple-
mentation of IBM Model 2, BiBr employed a boosting
procedure in deriving an IBM Model 1 lexicon, Ralign
used IBM Model 2 as a foundation for their recursive
splitting procedure, and XRCE used IBM Model 4 as a
base for alignment with lemmatized text and bilingual
lexicons.
Two teams made use of syntactic structure in the text
to be aligned. ProAlign satisfies constraints derived from
a dependency tree parse of the English sentence being
3The two teams that did not participate in English-French
were Fourday and RACAI.
aligned. BiBr also employs syntactic constraints that
must be satisfied. However, these come from parallel text
that has been shallowly parsed via a method known as
bilingual bracketing.
Three teams approached the shared task with baseline
or prototype systems. Fourday combines several intuitive
baselines via a nearest neighbor classifier, RACAI car-
ries out a greedy alignment based on an automatically
extracted dictionary of translations, and UMD?s imple-
mentation of IBMModel 2 provides an experimental plat-
form for their future work incorporating prior knowledge
about cognates. All three of these systems were devel-
oped within a short period of time before and during the
shared task.
5 Results and Discussion
Tables 4 and 5 list the results obtained by participating
systems in the Romanian-English task. Similarly, results
obtained during the English-French task are listed in Ta-
bles 6 and 7.
For Romanian-English, limited resources, XRCE sys-
tems (XRCE.Nolem-56k.RE.2 and XRCE.Trilex.RE.3)
seem to lead to the best results. These are systems that
are based on GIZA++, with or without additional re-
sources (lemmatizers and lexicons). For unlimited re-
sources, ProAlign.RE.1 has the best performance.
For English-French, Ralign.EF.1 has the best perfor-
mance for limited resources, while ProAlign.EF.1 has
again the largest number of top ranked figures for unlim-
ited resources.
To make a cross-language comparison, we paid partic-
ular attention to the evaluation of the Sure alignments,
since these were collected in a similar fashion (an agree-
ment had to be achieved between two different anno-
tators). The results obtained for the English-French
Sure alignments are significantly higher (80.54% best F-
measure) than those for Romanian-English Sure align-
ments (71.14% best F-measure). Similarly, AER for
English-French (5.71% highest error reduction) is clearly
better than the AER for Romanian-English (28.86% high-
est error reduction).
This difference in performance between the two data
sets is not a surprise. As expected, word alignment, like
many other NLP tasks (Banko and Brill, 2001), highly
benefits from large amounts of training data. Increased
performance is therefore expected when larger training
data sets are available.
The only evaluation set where Romanian-English data
leads to better performance is the Probable alignments
set. We believe however that these figures are not di-
rectly comparable, since the English-French Probable
alignments were obtained as a reunion of the align-
ments assigned by two different annotators, while for
the Romanian-English Probable set two annotators had
to reach an agreement (that is, an intersection of their in-
dividual alignment assignments).
Interestingly, in an overall evaluation, the limited re-
sources systems seem to lead to better results than those
with unlimited resources. Out of 28 different evaluation
figures, 20 top ranked figures are provided by systems
with limited resources. This suggests that perhaps using
a large number of additional resources does not seem to
improve a lot over the case when only parallel texts are
employed.
Ranked results for all systems are plotted in Figures 2
and 3. In the graphs, systems are ordered based on their
AER scores. System names are preceded by a marker to
indicate the system type: L stands for Limited Resources,
and U stands for Unlimited Resources.
6 Conclusion
A shared task on word alignment was organized as part
of the HLT/NAACL 2003 Workshop on Building and
Using Parallel Texts. In this paper, we presented the
task definition, and resources involved, and shortly de-
scribed the participating systems. The shared task in-
cluded Romanian-English and English-French sub-tasks,
and drew the participation of seven teams from around the
world. Comparative evaluations of results led to interest-
ing insights regarding the impact on performance of (1)
various alignment algorithms, (2) large or small amounts
of training data, and (3) type of resources available. Data
and evaluation software used in this exercise are available
online at http://www.cs.unt.edu/?rada/wpt.
Acknowledgments
There are many people who contributed greatly to mak-
ing this word alignment evaluation task possible. We are
grateful to all the participants in the shared task, for their
hard work and involvement in this evaluation exercise.
Without them, all these comparative analyses of word
alignment techniques would not be possible.
We are very thankful to Franz Och from ISI and Her-
mann Ney from RWTH Aachen for kindly making their
English-French word aligned data available to the work-
shop participants; the Hansards made available by Ul-
rich Germann from ISI constituted invaluable data for
the English-French shared task. We would also like to
thank the student volunteers from the Department of En-
glish, Babes-Bolyai University, Cluj-Napoca, Romania
who helped creating the Romanian-English word aligned
data.
We are also grateful to all the Program Committee
members of the current workshop, for their comments
and suggestions, which helped us improve the definition
of this shared task. In particular, we would like to thank
Dan Melamed for suggesting the two different subtasks
(limited and unlimited resources), and Michael Carl and
Phil Resnik for initiating interesting discussions regard-
ing phrase-based evaluations.
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Lingusitics (ACL-2001), Toulouse,
France, July.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji
Yamada. 2003. Reducing parameter space for word
alignment. In HLT-NAACL 2003 Workshop: Building
and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 23?26, Edmonton, Alberta,
Canada, May 31. Association for Computational Lin-
guistics.
T. Erjavec, N. Ide, and D. Tufis?. 1997. Encoding and
parallel alignment of linguistic corpora in six central
and Eastern European languages. In Proceedings of
the Joint ACH/ALL Conference, Queen?s University,
Kingston, Ontario, June.
U. Germann. 2001. Aligned hansards of the 36th
parliament of canada. http://www.isi.edu/natural-
language/download/hansard/.
John C. Henderson. 2003. Word alignment baselines. In
HLT-NAACL 2003Workshop: Building and Using Par-
allel Texts: Data Driven Machine Translation and Be-
yond, pages 27?30, Edmonton, Alberta, Canada, May
31. Association for Computational Linguistics.
Dekang Lin and Colin Cherry. 2003. Proalign: Shared
task system description. In HLT-NAACL 2003 Work-
shop: Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond, pages 11?14, Ed-
monton, Alberta, Canada, May 31. Association for
Computational Linguistics.
D.I. Melamed. 1999. Empirical Methods for Exploiting
Parallel Texts. MIT Press.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics (COLING-ACL 2000), Saarbrucken,
Germany, August.
Michel Simard and Philippe Langlais. 2003. Statisti-
cal translation alignment with compositionality con-
straints. In HLT-NAACL 2003 Workshop: Building
and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 19?22, Edmonton, Alberta,
Canada, May 31. Association for Computational Lin-
guistics.
Bridget Thomson McInnes and Ted Pedersen. 2003. The
duluth word alignment system. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 40?
43, Edmonton, Alberta, Canada, May 31. Association
for Computational Linguistics.
Dan Tufis?, Ana-Maria Barbu, and Radu Ion. 2003. Treq-
al: A word alignment system with limited language
resources. In HLT-NAACL 2003 Workshop: Building
and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 36?39, Edmonton, Alberta,
Canada, May 31. Association for Computational Lin-
guistics.
D Tufis?. 2002. A cheap and fast way to build useful
translation lexicons. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics,
pages 1030?1036, Taipei, August.
Bing Zhao and Stephan Vogel. 2003. Word alignment
based on bilingual bracketing. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 15?
18, Edmonton, Alberta, Canada, May 31. Association
for Computational Linguistics.
System Resources Description
BiBr.EF.1 Limited baseline of bilingual bracketing
BiBr.EF.2 Unlimited baseline of bilingual bracketing + English POS tagging
BiBr.EF.3 Unlimited baseline of bilingual bracketing + English POS tagging and base NP
BiBr.EF.4 Limited reverse direction of BiBr.EF.1
BiBr.EF.5 Unlimited reverse direction of BiBr.EF.2
BiBr.EF.6 Unlimited reverse direction of BiBr.EF.3
BiBr.EF.7 Limited intersection of BiBr.EF.1 & BiBr.EF.3
BiBr.EF.8 Unlimited intersection of BiBr.EF.3 & BiBr.EF.6
ProAlign.EF.1 Unlimited cohesion between source and target language + English parser +
distributional similarity for English words
Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation
UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4
XRCE.Base.EF.1 Limited GIZA++ (IBM Model 4) with English and French lemmatizer
XRCE.Nolem.EF.2 Limited GIZA++ only (IBM Model 4), trained with 1/4 of the corpus
XRCE.Nolem.EF.3 Limited GIZA++ only (IBM Model 4), trained with 1/2 of the corpus
Table 2: Short description for English-French systems
System Resources Description
BiBr.RE.1 Limited baseline of bilingual bracketing
BiBr.RE.2 Unlimited baseline of bilingual bracketing + English POS tagging
BiBr.RE.3 Unlimited baseline of bilingual bracketing + English POS tagging and base NP
Fourday.RE.1 Limited nearest neighbor combination of baseline measures
ProAlign.RE.1 Unlimited cohesion between source and target language + English parser +
distributional similarity for English words
RACAI.RE.1 Unlimited translation equivalence dictionary (Tufis?, 2002) + POS tagging
Ralign.RE.1 Limited Giza (IBM Model 2) + recursive parallel segmentation
UMD.RE.1 Limited IBM Model 2, trained with all the corpus, distortion 4, iterations 4
UMD.RE.2 Limited IBM Model 2, trained with all the corpus, distortion 2, iterations 4
XRCE.Base.RE.1 Limited GIZA++ (IBM Model 4), with English lemmatizer
XRCE.Nolem.RE.2 Limited GIZA++ only (IBM Model 4)
XRCE.Trilex.RE.3 Limited GIZA++ only (IBM Model 4), with English lemmatizer and trinity lexicon
XRCE.Trilex.RE.4 Limited GIZA++ only (IBM Model 4), with English lemmatizer and trinity lexicon
Table 3: Short description for Romanian-English systems
System 



	





	

AER
Limited Resources
BiBr.RE.1 70.65% 55.75% 62.32% 59.60% 57.65% 58.61% 41.39%
Fourday.RE.1 0.00% 0.00% 0.00% 52.83% 42.86% 47.33% 52.67%
Ralign.RE.1 92.00% 45.06% 60.49% 63.63% 65.92% 64.76% 35.24%
UMD.RE.1 57.67% 49.70% 53.39% 57.67% 49.70% 53.39% 46.61%
UMD.RE.2 58.29% 49.99% 53.82% 58.29% 49.99% 53.82% 46.18%
XRCE.Base.RE.1 79.28% 61.14% 69.03% 79.28% 61.14% 69.03% 30.97%
XRCE.Nolem-56K.RE.2 82.65% 62.44% 71.14% 82.65% 62.44% 71.14% 28.86%
XRCE.Trilex.RE.3 80.97% 61.89% 70.16% 80.97% 61.89% 70.16% 29.84%
XRCE.Trilex.RE.4 79.76% 61.31% 69.33% 79.76% 61.31% 69.33% 30.67%
Unlimited Resources
BiBr.RE.2 70.46% 55.51% 62.10% 58.40% 57.59% 57.99% 41.39%
BiBr.RE.3 70.36% 55.47% 62.04% 58.17% 58.12% 58.14% 41.86%
RACAI.RE.1 81.29% 60.26% 69.21% 81.29% 60.26% 69.21% 30.79%
ProAlign.RE.1 88.22% 58.91% 70.64% 88.22% 58.91% 70.64% 29.36%
Table 4: Results for Romanian-English, NO-NULL-Align
System 



	





	

AER
Limited Resources
BiBr.RE.1 70.65% 48.32% 57.39% 57.38% 52.62% 54.90% 45.10%
Fourday.RE.1 0.00% 0.00% 0.00% 35.85% 45.88% 40.25% 59.75%
Ralign.RE.1 92.00% 39.05% 54.83% 63.63% 57.13% 60.21% 39.79%
UMD.RE.1 56.21% 43.17% 48.84% 45.51% 47.76% 46.60% 53.40%
UMD.RE.2 56.58% 43.45% 49.15% 46.00% 47.88% 46.92% 53.08%
XRCE.Base.RE.1 79.28% 52.98% 63.52% 61.59% 61.50% 61.54% 38.46%
XRCE.Nolem-56K.RE.2 82.65% 54.12% 65.41% 61.59% 61.50% 61.54% 38.46%
XRCE.Trilex.RE.3 80.97% 53.64% 64.53% 63.64% 61.58% 62.59% 37.41%
XRCE.Trilex.RE.4 79.76% 53.14% 63.78% 62.22% 61.37% 61.79% 38.21%
Unlimited Resources
BiBr.RE.2 70.46% 48.11% 57.18% 56.01% 52.26% 54.07% 45.93%
BiBr.RE.3 70.36% 48.08% 57.12% 56.05% 52.87% 54.42% 45.58%
RACAI.RE.1 60.30% 62.38% 61.32% 59.87% 62.42% 61.12% 38.88%
ProAlign.RE.1 88.22% 51.06% 64.68% 61.71% 62.05% 61.88% 38.12%
Table 5: Results for Romanian-English, NULL-Align
System 



	





	

AER
Limited Resources
BiBr.EF.1 49.85% 79.45% 61.26% 67.23% 29.24% 40.76% 28.23%
BiBr.EF.4 51.46% 82.42% 63.36% 66.65% 32.68% 43.86% 28.01%
BiBr.EF.7 63.03% 74.59% 68.32% 66.11% 30.06% 41.33% 29.38%
Ralign.EF.1 72.54% 80.61% 76.36% 77.56% 38.19% 51.18% 18.50%
UMD.EF.1 37.98% 64.66% 47.85% 59.69% 23.53% 33.75% 38.47%
XRCE.Base.EF.1 50.89% 84.67% 63.57% 83.22% 32.05% 46.28% 16.23%
XRCE.Nolem.EF.2 55.54% 93.46% 69.68% 89.65% 34.92% 50.27% 8.93%
XRCE.Nolem.EF.3 55.43% 93.81% 69.68% 90.09% 35.30% 50.72% 8.53%
Unlimited Resources
BiBr.EF.2 50.05% 79.89% 61.54% 66.92% 29.14% 40.60% 28.24%
BiBr.EF.3 50.21% 80.26% 61.80% 63.79% 30.52% 41.29% 30.38%
BiBr.EF.5 51.27% 82.17% 63.15% 67.22% 32.56% 43.87% 27.71%
BiBr.EF.6 51.91% 83.26% 63.95% 62.21% 34.58% 44.45% 31.32%
BiBr.EF.8 66.34% 74.86% 70.34% 61.62% 31.37% 41.57% 32.48%
ProAlign.EF.1 71.94% 91.48% 80.54% 96.49% 28.41% 43.89% 5.71%
Table 6: Results for English-French, NO-NULL-Align
System 



	





	

AER
Limited Resources
BiBr.EF.1 49.85% 79.45% 61.26% 60.32% 29.12% 39.28% 33.37%
BiBr.EF.4 51.46% 82.42% 63.36% 61.64% 32.41% 42.48% 31.91%
BiBr.EF.7 63.03% 74.59% 68.32% 51.35% 30.45% 38.23% 40.97%
Ralign.EF.1 72.54% 80.61% 76.36% 77.56% 36.79% 49.91% 18.50%
UMD.EF.1 37.19% 64.66% 47.22% 41.93% 24.08% 30.59% 51.71%
XRCE.Base.EF.1 50.89% 84.67% 63.57% 64.96% 32.73% 43.53% 28.99%
XRCE.Nolem.EF.2 55.54% 93.46% 69.68% 70.98% 35.61% 47.43% 22.10%
XRCE.Nolem.EF.3 55.43% 93.81% 69.68% 72.01% 36.00% 48.00% 21.27%
Unlimited Resources
BiBr.EF.2 50.05% 79.89% 61.54% 59.89% 28.96% 39.04% 33.48%
BiBr.EF.3 50.21% 80.26% 61.80% 57.85% 30.28% 39.75% 35.03%
BiBr.EF.5 51.27% 82.17% 63.15% 62.05% 32.23% 42.43% 31.69%
BiBr.EF.6 51.91% 83.26% 63.95% 58.41% 34.20% 43.14% 34.47%
BiBr.EF.8 66.34% 74.86% 70.34% 48.50% 31.76% 38.38% 43.37%
ProAlign.EF.1 71.94% 91.48% 80.54% 56.02% 30.05% 39.62% 33.71%
Table 7: Results for English-French, NULL-Align
Figure 2: Ranked results for Romanian-English data
Figure 3: Ranked results for English-French data
The SENSEVAL?3 Multilingual English?Hindi Lexical Sample Task
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
timc@isi.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX 76203
rada@cs.unt.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Amruta Purandare
Department of Computer Science
University of Minnesota
Duluth, MN 55812
pura0010@d.umn.edu
Abstract
This paper describes the English?Hindi Multilingual
lexical sample task in SENSEVAL?3. Rather than
tagging an English word with a sense from an En-
glish dictionary, this task seeks to assign the most
appropriate Hindi translation to an ambiguous tar-
get word. Training data was solicited via the Open
Mind Word Expert (OMWE) from Web users who
are fluent in English and Hindi.
1 Introduction
The goal of the MultiLingual lexical sample task
is to create a framework for the evaluation of sys-
tems that perform Machine Translation, with a fo-
cus on the translation of ambiguous words. The
task is very similar to the lexical sample task, ex-
cept that rather than using the sense inventory from
a dictionary we follow the suggestion of (Resnik and
Yarowsky, 1999) and use the translations of the tar-
get words into a second language. In this task for
SENSEVAL-3, the contexts are in English, and the
?sense tags? for the English target words are their
translations in Hindi.
This paper outlines some of the major issues that
arose in the creation of this task, and then describes
the participating systems and summarizes their re-
sults.
2 Open Mind Word Expert
The annotated corpus required for this task was
built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted for mul-
tilingual annotations 1.
To overcome the current lack of tagged data and
the limitations imposed by the creation of such data
using trained lexicographers, the Open Mind Word
1Multilingual Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/english-hindi
Expert system enables the collection of semantically
annotated corpora over the Web. Tagged examples
are collected using a Web-based application that al-
lows contributors to annotate words with their mean-
ings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, together with all
possible translations for the given target word. Users
are asked to select the most appropriate translation
for the target word in each sentence. The selection
is made using check-boxes, which list all possible
translations, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one translation per word, the se-
lection of two or more translations is also possible.
The results of the classification submitted by other
users are not presented to avoid artificial biases.
3 Sense Inventory Representation
The sense inventory used in this task is the set of
Hindi translations associated with the English words
in our lexical sample. Selecting an appropriate
English-Hindi dictionary was a major decision early
in the task, and it raised a number of interesting is-
sues.
We were unable to locate any machine readable
or electronic versions of English-Hindi dictionaries,
so it became apparent that we would need to manu-
ally enter the Hindi translations from printed mate-
rials. We briefly considered the use of Optical Char-
acter Recognition (OCR), but found that our avail-
able tools did not support Hindi. Even after deciding
to enter the Hindi translations manually, it wasn?t
clear how those words should be encoded. Hindi is
usually represented in Devanagari script, which has
a large number of possible encodings and no clear
standard has emerged as yet.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
We decided that Romanized or transliterated
Hindi text would be the the most portable encoding,
since it can be represented in standard ASCII text.
However, it turned out that the number of English?
Hindi bilingual dictionaries is much less than the
number of Hindi?English, and the number that use
transliterated text is smaller still.
Still, we located one promising candidate, the
English?Hindi Hippocrene Dictionary (Raker and
Shukla, 1996), which represents Hindi in a translit-
erated form. However, we found that many English
words only had two or three translations, making it
too coarse grained for our purposes2 .
In the end we selected the Chambers English?
Hindi dictionary (Awasthi, 1997), which is a high
quality bilingual dictionary that uses Devanagari
script. We identified 41 English words from the
Chambers dictionary to make up our lexical sam-
ple. Then one of the task organizers, who is
fluent in English and Hindi, manually transliter-
ated the approximately 500 Hindi translations of
the 41 English words in our lexical sample from
the Chambers dictionary into the ITRANS format
(http://www.aczone.com/itrans/). ITRANS software
was used to generate Unicode for display in the
OMWE interfaces, although the sense tags used in
the task data are the Hindi translations in transliter-
ated form.
4 Training and Test Data
The MultiLingual lexical sample is made up of 41
words: 18 nouns, 15 verbs, and 8 adjectives. This
sample includes English words that have varying de-
grees of polysemy as reflected in the number of pos-
sible Hindi translations, which range from a low of
3 to a high of 39.
Text samples made up of several hundred in-
stances for each of 31 of the 41 words were drawn
from the British National Corpus, while samples for
the other 10 words came from the SENSEVAL-2 En-
glish lexical sample data. The BNC data is in a
?raw? text form, where the part of speech tags have
been removed. However, the SENSEVAL-2 data in-
cludes the English sense?tags as determined by hu-
man taggers.
After gathering the instances for each word in
the lexical sample, we tokenized each instance and
removed those that contain collocations of the tar-
get word. For example, the training/test instances
for arm.n do not include examples for contact arm,
2We have made available transcriptions of the entries for
approximately 70 Hippocrene nouns, verbs, and adjectives
at http://www.d.umn.edu/?pura0010/hindi.html, although these
were not used in this task.
pickup arm, etc., but only examples that refer to arm
as a single lexical unit (not part of a collocation). In
our experience, disambiguation accuracy on collo-
cations of this sort is close to perfect, and we aimed
to concentrate the annotation effort on the more dif-
ficult cases.
The data was then annotated with Hindi transla-
tions by web volunteers using the Open Mind Word
Expert (bilingual edition). At various points in time
we offered gift certificates as a prize for the most
productive tagger in a given day, in order to spur
participation. A total of 40 volunteers contributed to
this task.
To create the test data we collected two indepen-
dent tags per instance, and then discarded any in-
stances where the taggers disagreed. Thus, each
instance that remains in the test data has complete
agreement between two taggers. For the training
data, we only collected one tag per instance, and
therefore this data may be noisy. Participating sys-
tems could choose to apply their own filtering meth-
ods to identify and remove the less reliably anno-
tated examples.
After tagging by the Web volunteers, there were
two data sets provided to task participants: one
where the English sense of the target word is un-
known, and another where it is known in both the
training and test data. These are referred to as the
translation only (t) data and the translation and sense
(ts) data, respectively. The t data is made up of in-
stances drawn from the BNC as described above,
while the ts data is made up of the instances from
SENSEVAL-2. Evaluations were run separately for
each of these two data sets, which we refer to as the
t and ts subtasks.
The t data contains 31 ambiguous words: 15
nouns, 10 verbs, and 6 adjectives. The ts data con-
tains 10 ambiguous words: 3 nouns, 5 verbs, and 2
adjectives, all of which have been used in the En-
glish lexical sample task of SENSEVAL-2. These
words, the number of possible translations, and the
number of training and test instances are shown in
Table 1. The total number of training instances in
the two sub-tasks is 10,449, and the total number of
test instances is 1,535.
5 Participating Systems
Five teams participated in the t subtask, submitting
a total of eight systems. Three teams (a subset of
those five) participated in the ts subtask, submitting
a total of five systems. All submitted systems em-
ployed supervised learning, using the training ex-
amples provided. Some teams used additional re-
sources as noted in the more detailed descriptions
Table 1: Target words in the SENSEVAL-3 English-Hindi task
Lexical Unit Translations Train Test Lexical Unit Translations Train Test Lexical Unit Translations Train Test
TRANSLATION ONLY (T?DATA)
band.n 8 224 91 bank.n 21 332 52 case.n 13 348 42
different.a 4 320 25 eat.v 3 271 48 field.n 14 300 100
glass.n 8 379 13 hot.a 18 348 32 line.n 39 360 11
note.v 11 220 12 operate.v 9 280 50 paper.n 8 264 73
plan.n 8 210 35 produce.v 7 265 67 rest.v 14 172 10
rule.v 8 160 18 shape.n 8 320 32 sharp.a 16 248 48
smell.v 5 210 17 solid.a 16 327 37 substantial.a 15 250 100
suspend.v 4 370 28 table.n 21 378 16 talk.v 6 341 35
taste.n 6 350 40 terrible.a 4 200 99 tour.n 5 240 9
vision.n 14 318 20 volume.n 9 309 54 watch.v 10 300 100
way.n 16 331 22 TOTAL 348 8945 1336
TRANSLATION AND SENSE ONLY (TS?DATA)
bar.n 19 278 39 begin.v 6 360 15 channel.n 6 92 16
green.a 9 175 26 nature.n 15 71 14 play.v 14 152 10
simple.a 9 166 19 treat.v 7 100 32 wash.v 16 10 11
work.v 24 100 17 TOTAL 125 1504 199
below.
5.1 NUS
The NUS team from the National University of Sin-
gapore participated in both the t and ts subtasks. The
t system (nusmlst) uses a combination of knowledge
sources as features, and the Support Vector Machine
(SVM) learning algorithm. The knowledge sources
used include part of speech of neighboring words,
single words in the surrounding context, local col-
locations, and syntactic relations. The ts system
(nusmlsts) does the same, but adds the English sense
of the target word as a knowledge source.
5.2 LIA-LIDILEM
The LIA-LIDILEM team from the Universite? d?
Avignon and the Universite? Stendahl Grenoble had
two systems which participated in both the t and ts
subtasks. In the ts subtask, only the English sense
tags were used, not the Hindi translations.
The FL-MIX system uses a combination of three
probabilistic models, which compute the most prob-
able sense given a six word window of context. The
three models are a Poisson model, a Semantic Clas-
sification Tree model, and a K nearest neighbors
search model. This system also used a part of speech
tagger and a lemmatizer.
The FC-MIX system is the same as the FL-MIX
system, but replaces context words by more gen-
eral synonym?like classes computed from a word
aligned English?French corpus which number ap-
proximately 850,000 words in each language.
5.3 HKUST
The HKUST team from the Hong Kong University
of Science and Technology had three systems that
participated in both the t and ts subtasks
The HKUST me t and HKUST me ts sys-
tems are maximum entropy classifiers. The
HKUST comb t and HKUST comb ts systems
are voted classifiers that combine a new Kernel
PCA model with a maximum entropy model and
a boosting?based model. The HKUST comb2 t
and HKUST comb2 ts are voted classifiers that
combine a new Kernel PCA model with a maximum
entropy model, a boosting?based model, and a
Naive Bayesian model.
5.4 UMD
The UMD team from the University of Maryland en-
tered (UMD?SST) in the t task. UMD?SST is a su-
pervised sense tagger based on the Support Vector
Machine learning algorithm, and is described more
fully in (Cabezas et al, 2001).
5.5 Duluth
The Duluth team from the University of Minnesota,
Duluth had one system (Duluth-ELSS) that partici-
pated in the t task. This system is an ensemble of
three bagged decision trees, each based on a differ-
ent type of lexical feature. This system was known
as Duluth3 in SENSEVAL-2, and it is described more
fully in (Pedersen, 2001).
6 Results
All systems attempted all of the test instances, so
precision and recall are identical, hence we report
Table 2: t Subtask Results
System Accuracy
nusmlst 63.4
HKUST comb t 62.0
HKUST comb2 t 61.4
HKUST me t 60.6
FL-MIX 60.3
FC-MIX 60.3
UMD-SST 59.4
Duluth-ELSS 58.2
Baseline (majority) 51.9
Table 3: ts Subtask Results
System Accuracy
nusmlsts 67.3
FL-MIX 64.1
FC-MIX 64.1
HKUST comb ts 63.8
HKUST comb2 ts 63.8
HKUST me ts 60.8
Baseline (majority) 55.8
the single Accuracy figure. Tables 2 and 3 show re-
sults for the t and ts subtasks, respectively.
We note that the participating systems all ex-
ceeded the baseline (majority) classifier by some
margin, suggesting that the sense distinctions made
by the translations are clear and provide sufficient
information for supervised methods to learn effec-
tive classifiers.
Interestingly, the average results on the ts data are
higher than the average results on the t data, which
suggests that sense information is likely to be helpful
for the task of targeted word translation. Additional
investigations are however required to draw some fi-
nal conclusions.
7 Conclusion
The Multilingual Lexical Sample task in
SENSEVAL-3 featured English ambiguous words
that were to be tagged with their most appropriate
Hindi translation. The objective of this task is to
determine feasibility of translating words of various
degrees of polysemy, focusing on translation of
specific lexical items. The results of five teams
that participated in this event tentatively suggest
that machine learning techniques can significantly
improve over the most frequent sense baseline.
Additionally, this task has highlighted creation
of testing and training data by leveraging the
knowledge of bilingual Web volunteers. The
training and test data sets used in this exercise are
available online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the Mul-
tilingual Open Mind Word Expert project, making
this task possible. We are also grateful to all the par-
ticipants in this task, for their hard work and involve-
ment in this evaluation exercise. Without them, all
these comparative analyses would not be possible.
We are particularly grateful to a research grant
from the University of North Texas that provided the
funding for contributor prizes, and to the National
Science Foundation for their support of Amruta Pu-
randare under a Faculty Early CAREER Develop-
ment Award (#0092784).
References
S. Awasthi, editor. 1997. Chambers English?Hindi
Dictionary. South Asia Books, Columbia, MO.
C. Cabezas, P. Resnik, and J. Stevens. 2001. Su-
pervised sense tagging using Support Vector Ma-
chines. In Proceedings of the Senseval-2 Work-
shop, Toulouse, July.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with the Open Mind Word
Expert. In Proceedings of the ACL Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions, Philadelphia.
T. Pedersen. 2001. Machine learning with lexical
features: The Duluth approach to Senseval-2. In
Proceedings of the Senseval-2 Workshop, pages
139?142, Toulouse, July.
J. Raker and R. Shukla, editors. 1996. Hip-
pocrene Standard Dictionary English-Hindi
Hindi-English (With Romanized Pronunciation).
Hippocrene Books, New York, NY.
P. Resnik and D. Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New eval-
uation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113?133.
The SENSEVAL?3 English Lexical Sample Task
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Adam Kilgarriff
Information Technology Research Institute
University of Brighton
Brighton, UK
Adam.Kilgarriff@itri.brighton.ac.uk
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for
the English lexical sample task, which was orga-
nized as part of the SENSEVAL-3 evaluation exer-
cise. The task drew the participation of 27 teams
from around the world, with a total of 47 systems.
1 Introduction
We describe in this paper the task definition, re-
sources, participating systems, and comparative re-
sults for the English lexical sample task, which was
organized as part of the SENSEVAL-3 evaluation ex-
ercise. The goal of this task was to create a frame-
work for evaluation of systems that perform targeted
Word Sense Disambiguation.
This task is a follow-up to similar tasks organized
during the SENSEVAL-1 (Kilgarriff and Palmer,
2000) and SENSEVAL-2 (Preiss and Yarowsky,
2001) evaluations. The main changes in this
year?s evaluation consist of a new methodology for
collecting annotated data (with contributions from
Web users, as opposed to trained lexicographers),
and a new sense inventory used for verb entries
(Wordsmyth).
2 Building a Sense Tagged Corpus with
Volunteer Contributions over the Web
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002) 1. To overcome the
current lack of sense tagged data and the limitations
imposed by the creation of such data using trained
lexicographers, the OMWE system enables the col-
lection of semantically annotated corpora over the
Web. Sense tagged examples are collected using
1Open Mind Word Expert can be accessed at http://teach-
computers.org/
a Web-based application that allows contributors to
annotate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one meaning per word, the se-
lection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
Similar to the annotation scheme used for the En-
glish lexical sample at SENSEVAL-2, we use a ?tag
until two agree? scheme, with an upper bound on the
number of annotations collected for each item set to
four.
2.1 Source Corpora
The data set used for the SENSEVAL-3 English
lexical sample task consists of examples extracted
from the British National Corpus (BNC). Ear-
lier versions of OMWE also included data from
the Penn Treebank corpus, the Los Angeles Times
collection as provided during TREC conferences
(http://trec.nist.gov), and Open Mind Common Sense
(http://commonsense.media.mit.edu).
2.2 Sense Inventory
The sense inventory used for nouns and adjec-
tives is WordNet 1.7.1 (Miller, 1995), which
is consistent with the annotations done for the
same task during SENSEVAL-2. Verbs are in-
stead annotated with senses from Wordsmyth
(http://www.wordsmyth.net/). The main reason mo-
tivating selection of a different sense inventory is the
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Class Nr of Avg senses Avg senses
words (fine) (coarse)
Nouns 20 5.8 4.35
Verbs 32 6.31 4.59
Adjectives 5 10.2 9.8
Total 57 6.47 4.96
Table 1: Summary of the sense inventory
weak verb performance of systems participating in
the English lexical sample in SENSEVAL-2, which
may be due to the high number of senses defined for
verbs in the WordNet sense inventory. By choos-
ing a different set of senses, we hope to gain insight
into the dependence of difficulty of the sense disam-
biguation task on sense inventories.
Table 1 presents the number of words under each
part of speech, and the average number of senses for
each class.
2.3 Multi-Word Expressions
For this evaluation exercise, we decided to isolate the
task of semantic tagging from the task of identifying
multi-word expressions; we applied a filter that re-
moved all examples pertaining to multi-word expres-
sions prior to the tagging phase. Consequently, the
training and test data sets made available for this task
do not contain collocations as possible target words,
but only single word units. This is a somewhat dif-
ferent definition of the task as compared to previous
similar evaluations; the difference may have an im-
pact on the overall performance achieved by systems
participating in the task.
2.4 Sense Tagged Data
The inter-tagger agreement obtained so far is closely
comparable to the agreement figures previously re-
ported in the literature. Kilgarriff (2002) mentions
that for the SENSEVAL-2 nouns and adjectives there
was a 66.5% agreement between the first two tag-
gings (taken in order of submission) entered for
each item. About 12% of that tagging consisted of
multi-word expressions and proper nouns, which are
usually not ambiguous, and which are not consid-
ered during our data collection process. So far we
measured a 62.8% inter-tagger agreement between
the first two taggings for single word tagging, plus
close-to-100% precision in tagging multi-word ex-
pressions and proper nouns (as mentioned earlier,
this represents about 12% of the annotated data).
This results in an overall agreement of about 67.3%
which is reasonable and closely comparable with
previous figures. Note that these figures are col-
lected for the entire OMWE data set build so far,
which consists of annotated data for more than 350
words.
In addition to raw inter-tagger agreement, the
kappa statistic, which removes from the agreement
rate the amount of agreement that is expected by
chance(Carletta, 1996), was also determined. We
measure two figures: micro-average   , where num-
ber of senses, agreement by chance, and   are de-
termined as an average for all words in the set,
and macro-average   , where inter-tagger agreement,
agreement by chance, and   are individually deter-
mined for each of the words in the set, and then
combined in an overall average. With an average of
five senses per word, the average value for the agree-
ment by chance is measured at 0.20, resulting in a
micro-   statistic of 0.58. For macro-   estimations,
we assume that word senses follow the distribution
observed in the OMWE annotated data, and under
this assumption, the macro-   is 0.35.
3 Participating Systems
27 teams participated in this word sense disambigua-
tion task. Tables 2 and 3 list the names of the partic-
ipating systems, the corresponding institutions, and
the name of the first author ? which can be used
as reference to a paper in this volume, with more
detailed descriptions of the systems and additional
analysis of the results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of 47 submissions were received for this task.
Tables 2 and 3 show all the submissions for each
team, gives a brief description of their approaches,
and lists the precision and recall obtained by each
system under fine and coarse grained evaluations.
The precision/recall baseline obtained for this task
under the ?most frequent sense? heuristic is 55.2%
(fine grained) and 64.5% (coarse grained). The per-
formance of most systems (including several unsu-
pervised systems, as listed in Table 3) is significantly
higher than the baseline, with the best system per-
forming at 72.9% (79.3%) for fine grained (coarse
grained) scoring.
Not surprisingly, several of the top performing
systems are based on combinations of multiple clas-
sifiers, which shows once again that voting schemes
that combine several learning algorithms outperform
the accuracy of individual classifiers.
4 Conclusion
The English lexical sample task in SENSEVAL-
3 featured English ambiguous words that were to
be tagged with their most appropriate WordNet or
Wordsmyth sense. The objective of this task was
to: (1) Determine feasibility of reliably finding the
Fine Coarse
System/Team Description P R P R
htsa3 A Naive Bayes system, with correction of the a-priori frequencies, by
U.Bucharest (Grozea) dividing the output confidence of the senses by  	
	 (   ) 72.9 72.9 79.3 79.3
IRST-Kernels Kernel methods for pattern abstraction, paradigmatic and syntagmatic info.
ITC-IRST (Strapparava) and unsupervised term proximity (LSA) on BNC, in an SVM classifier. 72.6 72.6 79.5 79.5
nusels A combination of knowledge sources (part-of-speech of neighbouring words,
Nat.U. Singapore (Lee) words in context, local collocations, syntactic relations), in an SVM classifier. 72.4 72.4 78.8 78.8
htsa4 Similar to htsa3, with different correction function of a-priori frequencies. 72.4 72.4 78.8 78.8
BCU comb An ensemble of decision lists, SVM, and vectorial similarity, improved
Basque Country U. with a variety of smoothing techniques. The features consist 72.3 72.3 78.9 78.9
(Agirre & Martinez) of local collocations, syntactic dependencies, bag-of-words, domain features.
htsa1 Similar to htsa3, but with smaller number of features. 72.2 72.2 78.7 78.7
rlsc-comb A regularized least-square classification (RLSC), using local and topical
U.Bucharest (Popescu) features, with a term weighting scheme. 72.2 72.2 78.4 78.4
htsa2 Similar to htsa4, but with smaller number of features. 72.1 72.1 78.6 78.6
BCU english Similar to BCU comb, but with a vectorial space model learning. 72.0 72.0 79.1 79.1
rlsc-lin Similar to rlsc-comb, with a linear kernel, and a binary weighting scheme. 71.8 71.8 78.4 78.4
HLTC HKUST all A voted classifier combining a new kernel PCA method, a Maximum Entropy
HKUST (Carpuat) model, and a boosting-based model, using syntactic and collocational features 71.4 71.4 78.6 78.6
TALP A system with per-word feature selection, using a rich feature set. For
U.P.Catalunya learning, it uses SVM, and combines two binarization procedures: 71.3 71.3 78.2 78.2
(Escudero et al) one vs. all, and constraint learning.
MC-WSD A multiclass averaged perceptron classifier with two components: one
Brown U. trained on the data provided, the other trained on this data, and on 71.1 71.1 78.1 78.1
(Ciaramita & Johnson) WordNet glosses. Features consist of local and syntactic features.
HLTC HKUST all2 Similar to HLTC HKUST all, also adds a Naive Bayes classifier. 70.9 70.9 78.1 78.1
NRC-Fine Syntactic and semantic features, using POS tags and pointwise mutual infor-
NRC (Turney) mation on a terabyte corpus. Five basic classifiers are combined with voting. 69.4 69.4 75.9 75.9
HLTC HKUST me Similar to HLTC HKUST all, only with a maximum entropy classifier. 69.3 69.3 76.4 76.4
NRC-Fine2 Similar to NRC-Fine, with a different threshold for dropping features 69.1 69.1 75.6 75.6
GAMBL A cascaded memory-based classifier, using two classifiers based on global
U. Antwerp (Decadt) and local features, with a genetic algorithm for parameter optimization. 67.4 67.4 74.0 74.0
SinequaLex Semantic classification trees, built on short contexts and document se-
Sinequa Labs (Crestan) mantics, plus a decision system based on information retrieval techniques. 67.2 67.2 74.2 74.2
CLaC1 A Naive Bayes approach using a context window around the target word, 67.2 67.2 75.1 75.1
Concordia U. (Lamjiri) which is dynamically adjusted
SinequaLex2 A cumulative method based on scores of surrounding words. 66.8 66.8 73.6 73.6
UMD SST4 Supervised learning using Support Vector Machines, using local and
U. Maryland (Cabezas) wide context features, and also grammatical and expanded contexts. 66.0 66.0 73.7 73.7
Prob1 A probabilistic modular WSD system, with individual modules based on
Cambridge U. (Preiss) separate known approaches to WSD (26 different modules) 65.1 65.1 71.6 71.6
SyntaLex-3 A supervised system that uses local part of speech features and bigrams,
U.Toronto (Mohammad) in an ensemble classifier using bagged decision trees. 64.6 64.6 72.0 72.0
UNED A similarity-based system, relying on the co-occurrence of nouns and
UNED (Artiles) adjectives in the test and training examples. 64.1 64.1 72.0 72.0
SyntaLex-4 Similar to SyntaLex-3, but with unified decision trees. 63.3 63.3 71.1 71.1
CLaC2 Syntactic and semantic (WordNet hypernyms) information of neighboring
words, fed to a Maximum Entropy learner. See also CLaC1 63.1 63.1 70.3 70.3
SyntaLex-1 Bagged decision trees using local POS features. See also SyntaLex-3. 62.4 62.4 69.1 69.1
SyntaLex-2 Similar to SyntaLex-1, but using broad context part of speech features. 61.8 61.8 68.4 68.4
Prob2 Similar to Prob1, but invokes only 12 modules. 61.9 61.9 69.3 69.3
Duluth-ELSS An ensemble approach, based on three bagged decision trees, using
U.Minnesota (Pedersen) unigrams, bigrams, and co-occurrence features 61.8 61.8 70.1 70.1
UJAEN A Neural Network supervised system, using features based on semantic
U.Jae?n (Garc??a-Vega) relations from WordNet extracted from the training data 61.3 61.3 69.5 69.5
R2D2 A combination of supervised (Maximum Entropy, HMM Models, Vector
U. Alicante (Vazquez) Quantization, and unsupervised (domains and conceptual density) systems. 63.4 52.1 69.7 57.3
IRST-Ties A generalized pattern abstraction system, based on boosted wrapper
ITC-IRST (Strapparava) induction, using only few syntagmatic features. 70.6 50.5 76.7 54.8
NRC-Coarse Similar to NRC-Fine; maximizes the coarse score, by training on coarse senses. 48.5 48.5 75.8 75.8
NRC-Coarse2 Similar to NRC-Coarse, with a different threshold for dropping features. 48.4 48.4 75.7 75.7
DLSI-UA-LS-SU A maximum entropy method and a bootstrapping algorithm (?re-training?) with,
U.Alicante (Vazquez) iterative feeding of training cycles with new high-confidence examples. 78.2 31.0 82.8 32.9
Table 2: Performance and short description of the supervised systems participating in the SENSEVAL-3
English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both
fine grained and coarse grained scoring. Corresponding team and reference to system description (in this
volume) are indicated for the first system for each team.
Fine Coarse
System/Team Description P R P R
wsdiit An unsupervised system using a Lesk-like similarity between context
IIT Bombay of ambiguous words, and dictionary definitions. Experiments are 66.1 65.7 73.9 74.1
(Ramakrishnan et al) performed for various window sizes, various similarity measures
Cymfony A Maximum Entropy model for unsupervised clustering, using neighboring
(Niu) words and syntactic structures as features. A few annotated instances 56.3 56.3 66.4 66.4
are used to map context clusters to WordNet/Worsmyth senses.
Prob0 A combination of two unsupervised modules, using basic part of speech
Cambridge U. (Preiss) and frequency information. 54.7 54.7 63.6 63.6
clr04-ls An unsupervised system relying on definition properties (syntactic, semantic,
CL Research subcategorization patterns, other lexical information), as given in a dictionary. 45.0 45.0 55.5 55.5
(Litkowski) Performance is generally a function of how well senses are distinguished.
CIAOSENSO An unsupervised system that combines the conceptual density idea with the
U. Genova (Buscaldi) frequency of words to disambiguate; information about domains is also 50.1 41.7 59.1 49.3
taken into account.
KUNLP An algorithm that disambiguates the senses of a word by selecting a substituent
Korea U. (Seo) among WordNet relatives (antonyms, hypernyms, etc.). The selection 40.4 40.4 52.8 52.8
is done based on co-occurrence frequencies, measured on a large corpus.
Duluth-SenseRelate An algorithm that assigns the sense to a word that is most related to the
U.Minnesota (Pedersen) possible senses of its neighbors, using WordNet glosses to measure 40.3 38.5 51.0 48.7
relatedness between senses.
DFA-LS-Unsup A combination of three heuristics: similarity between synonyms and the context,
UNED (Fernandez) according to a mutual information measure; lexico-syntactic patterns extracted 23.4 23.4 27.4 27.4
from WordNet glosses; the first sense heuristic.
DLSI-UA-LS-NOSU An unsupervised method based on (Magnini & Strapparava 2000) WordNet
U.Alicante (Vazquez) domains; it exploits information contained in glosses of WordNet domains, and 19.7 11.7 32.2 19.0
uses ?Relevant Domains?, obtained from association ratio over domains and words.
Table 3: Performance and short description for the Unsupervised systems participating in the SENSEVAL-3
English lexical sample task.
appropriate sense for words with various degrees of
polysemy, using different sense inventories; and (2)
Determine the usefulness of sense annotated data
collected over the Web (as opposed to other tradi-
tional approaches for building semantically anno-
tated corpora).
The results of 47 systems that participated in this
event tentatively suggest that supervised machine
learning techniques can significantly improve over
the most frequent sense baseline, and also that it is
possible to design unsupervised techniques for reli-
able word sense disambiguation. Additionally, this
task has highlighted creation of testing and training
data by leveraging the knowledge of Web volunteers.
The training and test data sets used in this exercise
are available online from http://www.senseval.org
and http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Open Mind Word Expert project, making this task
possible. In particular, we are grateful to Gwen
Lenker ? our most productive contributor. We are
also grateful to all the participants in this task, for
their hard work and involvement in this evaluation
exercise. Without them, all these comparative anal-
yses would not be possible.
We are indebted to the Princeton WordNet team,
for making WordNet available free of charge, and to
Robert Parks from Wordsmyth, for making available
the verb entries used in this evaluation.
We are particularly grateful to the National Sci-
ence Foundation for their support under research
grant IIS-0336793, and to the University of North
Texas for a research grant that provided funding for
contributor prizes.
References
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249?
254.
T. Chklovski and R. Mihalcea. 2002. Building a sense tagged
corpus with Open Mind Word Expert. In Proceedings of the
ACL 2002 Workshop on ?Word Sense Disambiguation: Re-
cent Successes and Future Directions?, Philadelphia, July.
A. Kilgarriff and M. Palmer, editors. 2000. Computer and
the Humanities. Special issue: SENSEVAL. Evaluating Word
Sense Disambiguation programs, volume 34, April.
G. Miller. 1995. Wordnet: A lexical database. Communication
of the ACM, 38(11):39?41.
J. Preiss and D. Yarowsky, editors. 2001. Proceedings of
SENSEVAL-2, Association for Computational Linguistics
Workshop, Toulouse, France.
An Evaluation Exercise for Romanian Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Vivi Na?stase
School of Computer Science
University of Ottawa
Ottawa, ON, Canada
vnastase@site.uottawa.ca
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Doina Ta?tar
Department of Computer Science
Babes?-Bolyai University
Cluj-Napoca, Romania
dtatar@ubb.ro
Dan Tufis?
Romanian Academy Center
for Artificial Intelligence
Bucharest, Romania
tufis@racai.ro
Florentina Hristea
Department of Computer Science
University of Bucharest
Bucharest, Romania
fhristea@mailbox.ro
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for a
Romanian Word Sense Disambiguation task, which
was organized as part of the SENSEVAL-3 evaluation
exercise. Five teams with a total of seven systems
were drawn to this task.
1 Introduction
SENSEVAL is an evaluation exercise of the lat-
est word-sense disambiguation (WSD) systems. It
serves as a forum that brings together researchers in
WSD and domains that use WSD for various tasks.
It allows researchers to discuss modifications that
improve the performance of their systems, and an-
alyze combinations that are optimal.
Since the first edition of the SENSEVAL competi-
tions, a number of languages were added to the orig-
inal set of tasks. Having the WSD task prepared for
several languages provides the opportunity to test
the generality of WSD systems, and to detect dif-
ferences with respect to word senses in various lan-
guages.
This year we have proposed a Romanian WSD
task. Five teams with a total of seven systems have
tackled this task. We present in this paper the data
used and how it was obtained, and the performance
of the participating systems.
2 Open Mind Word Expert
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted to Roma-
nian1.
To overcome the current lack of sense tagged
data and the limitations imposed by the creation of
such data using trained lexicographers, the Open
Mind Word Expert system enables the collection
of semantically annotated corpora over the Web.
Sense tagged examples are collected using a Web-
based application that allows contributors to anno-
tate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are en-
couraged to select only one meaning per word, the
selection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
3 Sense inventory
For the Romanian WSD task, we have chosen a set
of words from three parts of speech - nouns, verbs
and adjectives. Table 1 presents the number of
words under each part of speech, and the average
number of senses for each class.
The senses were (manually) extracted from a Ro-
manian dictionary (Dict?ionarul EXplicativ al limbii
roma?ne - DEX (Coteanu et al, 1975)). These senses
1Romanian Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/romanian
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Number Avg senses Avg senses
Class words (fine) (coarse)
Nouns 25 8.92 4.92
Verbs 9 8.7 4.6
Adjectives 5 9 4
Total 39 8.875 4.725
Table 1: Sense inventory
and their dictionary definitions were incorporated in
the Open Mind Word Expert. For each annotation
task, the contributors could choose from this list of
39 words. For each chosen word, the system dis-
plays the associated senses, together with their def-
initions, and a short (1-4 words) description of the
sense. After the user gets familiarized with these
senses, the system displays each example sentence,
and the list of senses together with their short de-
scription, to facilitate the tagging process.
For the coarse grained WSD task, we had the op-
tion of using the grouping provided by the dictio-
nary. A manual analysis however showed that some
of the senses in the same group are quite distinguish-
able, while others that were separated were very
similar.
For example, for the word circulatie (roughly, cir-
culation). The following two senses are grouped in
the dictionary:
2a. movement, travel along a communication
line/way
2b. movement of the sap in plants or the cytoplasm
inside cells
Sense 2a fits better with sense 1 of circulation:
1. the event of moving about
while sense 2b fits better with sense 3:
3. movement or flow of a liquid, gas, etc. within a
circuit or pipe.
To obtain a better grouping, a linguist clustered
the similar senses for each word in our list of forty.
The average number of senses for each class is al-
most halved.
Notice that Romanian is a language that uses dia-
critics, and the the presence of diacritics may be cru-
cial for distinguishing between words. For example
peste without diacritics may mean fish or over. In
choosing the list of words for the Romanian WSD
task, we have tried to avoid such situations. Al-
though some of the words in the list do have dia-
critics, omitting them does not introduce new ambi-
guities.
4 Corpus
Examples are extracted from the ROCO corpus, a
400 million words corpus consisting of a collection
of Romanian newspapers collected on the Web over
a three years period (1999-2002).
The corpus was tokenized and part-of-speech
tagged using RACAI?s tools (Tufis, 1999). The to-
kenizer recognizes and adequately segments various
constructs: clitics, dates, abbreviations, multiword
expressions, proper nouns, etc. The tagging fol-
lowed the tiered tagging approach with the hidden
layer of tagging being taken care of by Thorsten
Brants? TNT (Brants, 2000). The upper level of
the tiered tagger removed from the assigned tags all
the attributes irrelevant for this WSD exercise. The
estimated accuracy of the part-of-speech tagging is
around 98%.
5 Sense Tagged Data
While several sense annotation schemes have been
previously proposed, including single or dual anno-
tations, or the ?tag until two agree? scheme used dur-
ing SENSEVAL-2, we decided to use a new scheme
and collect four tags per item, which allowed us
to conduct and compare inter-annotator agreement
evaluations for two-, three-, and four-way agree-
ment. The agreement rates are listed in Table 3.
The two-way agreement is very high ? above 90%
? and these are the items that we used to build the
annotated data set. Not surprisingly, four-way agree-
ment is reached for a significantly smaller number of
cases. While these items with four-way agreement
were not explicitly used in the current evaluation,
we believe that this represents a ?platinum standard?
data set with no precedent in the WSD research com-
munity, which may turn useful for a range of future
experiments (for bootstrapping, in particular).
Agreement type Total (%)
TOTAL ITEMS 11,532 100%
At least two agree 10,890 94.43%
At least three agree 8,192 71.03%
At least four agree 4,812 41.72%
Table 3: Inter-agreement rates for two-, three-, and
four-way agreement
Table 2 lists the target words selected for this task,
together with their most common English transla-
tions. For each word, we also list the number of
senses, as defined in the DEX sense inventory (col-
locations included), and the number of annotated ex-
amples made available to task participants.
Word Main English senses senses Train Test Word Main English senses senses Train Test
translation (fine) (coarse) size size translation (fine) (coarse) size size
NOUNS
ac needle 16 7 127 65 accent accent 5 3 172 87
actiune action 10 7 261 128 canal channel 6 5 134 66
circuit circuit 7 5 200 101 circulatie circulation 9 3 221 114
coroana crown 15 11 252 126 delfin doplhin 5 4 31 15
demonstratie demonstration 6 3 229 115 eruptie eruption 2 2 54 27
geniu genius 5 3 106 54 nucleu nucleus 7 5 64 33
opozitie opposition 12 7 266 134 perie brush 5 3 46 24
pictura painting 5 2 221 111 platforma platform 11 8 226 116
port port 7 3 219 108 problema problem 6 4 262 131
proces process 11 3 166 82 reactie reaction 7 6 261 131
stil style 14 4 199 101 timbru stamp 7 3 231 116
tip type 7 4 263 131 val wave 15 9 242 121
valoare value 23 9 251 125
VERBS
cistiga win 5 4 227 115 citi read 10 4 259 130
cobori descend 11 6 252 128 conduce drive 7 6 265 134
creste grow 14 6 209 103 desena draw 3 3 54 27
desface untie 11 5 115 58 fierbe boil 11 4 83 43
indulci sweeten 7 4 19 10
ADJECTIVES
incet slow 6 3 224 113 natural natural 12 5 242 123
neted smooth 7 3 34 17 oficial official 5 3 185 96
simplu simple 15 6 153 82
Table 2: Target words in the SENSEVAL-3 Romanian Lexical Sample task
Team System name Reference (this volume)
Babes-Bolyai University, Cluj-Napoca (1) ubb nbc ro (Csomai, 2004)
Babes-Bolyai University, Cluj-Napoca (2) UBB (Serban and Tatar, 2004)
Swarthmore College swat-romanian (Wicentowski et al, 2004a)
Swarthmore College / Hong Kong Polytechnic University swat-hk-romanian (Wicentowski et al, 2004b)
Hong Kong University of Science and Technology romanian-swat hk-bo
University of Maryland, College Park UMD SST6 (Cabezas et al, 2004)
University of Minnesota, Duluth Duluth-RomLex (Pedersen, 2004)
Table 4: Teams participating in the SENSEVAL-3 Romanian Word Sense Disambiguation task
In addition to sense annotated examples, partici-
pants have been also provided with a large number
of unlabeled examples. However, among all partici-
pating systems, only one system ? described in (Ser-
ban and Ta?tar 2004) ? attempted to integrate this ad-
ditional unlabeled data set into the learning process.
6 Participating Systems
Five teams participated in this word sense disam-
biguation task. Table 4 lists the names of the par-
ticipating systems, the corresponding institutions,
and references to papers in this volume that provide
detailed descriptions of the systems and additional
analysis of their results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of seven submissions was received for this task.
Table 5 shows all the submissions for each team, and
gives a brief description of their approaches.
7 Results and Discussion
Table 6 lists the results obtained by all participating
systems, and the baseline obtained using the ?most
frequent sense? (MFS) heuristic. The table lists pre-
cision and recall figures for both fine grained and
coarse grained scoring.
The performance of all systems is significantly
higher than the baseline, with the best system per-
forming at 72.7% (77.1%) for fine grained (coarse
grained) scoring, which represents a 35% (38%) er-
ror reduction with respect to the baseline.
The best system (romanian-swat hk-bo) relies on
a Maximum Entropy classifier with boosting, using
local context (neighboring words, lemmas, and their
part of speech), as well as bag-of-words features for
surrounding words.
Not surprisingly, several of the top perform-
ing systems are based on combinations of multi-
ple sclassifiers, which shows once again that voting
System Description
romanian-swat hk-bo Supervised learning using Maximum Entropy with boosting, using bag-of-words
and n-grams around the head word as features
swat-hk-romanian The swat-romanian and romanian-swat hk-bo systems combined with majority voting.
Duluth-RLSS An ensemble approach that takes a vote among three bagged decision trees,
based on unigrams, bigrams and co-occurrence features
swat-romanian Three classifiers: cosine similarity clustering, decision list, and Naive Bayes,
using bag-of-words and n-grams around the head word as features
combined with a majority voting scheme.
UMD SST6 Supervised learning using Support Vector Machines, using contextual features.
ubb nbc ro Supervised learning using a Naive Bayes learning scheme, and features extracted
using a bag-of-words approach.
UBB A k-NN memory-based learning approach, with bag-of-words features.
Table 5: Short description of the systems participating in the SENSEVAL-3 Romanian Word Sense Disam-
biguation task. All systems are supervised.
Fine grained Coarse grained
System P R P R
romanian-swat hk-bo 72.7% 72.7% 77.1% 77.1%
swat-hk-romanian 72.4% 72.4% 76.1% 76.1%
Duluth-RLSS 71.4% 71.4% 75.2% 75.2%
swat-romanian 71.0% 71.0% 74.9% 74.9%
UMD SST6 70.7% 70.7% 74.6% 74.6%
ubb nbc ro 71.0% 68.2% 75.0% 72.0%
UBB 67.1% 67.1% 72.2% 72.2%
Baseline (MFS) 58.4% 58.4% 62.9% 62.9%
Table 6: System results on the Romanian Word Sense Disambiguation task.
schemes that combine several learning algorithms
outperform the accuracy of individual classifiers.
8 Conclusion
A Romanian Word Sense Disambiguation task
was organized as part of the SENSEVAL-3 eval-
uation exercise. In this paper, we presented
the task definition, and resources involved, and
shortly described the participating systems. The
task drew the participation of five teams, and in-
cluded seven different systems. The sense an-
notated data used in this exercise is available
online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Romanian Open Mind Word Expert project, mak-
ing this task possible. Special thanks to Bog-
dan Harhata, from the Institute of Linguistics Cluj-
Napoca, for building a coarse grained sense map.
We are also grateful to all the participants in this
task, for their hard work and involvement in this
evaluation exercise. Without them, all these com-
parative analyses would not be possible.
References
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP
Conference, ANLP-2000, Seattle, WA, May.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Ex-
pert. In Proceedings of the Workshop on ?Word
Sense Disambiguation: Recent Successes and Fu-
ture Directions?, ACL 2002, Philadelphia, July.
I. Coteanu, L. Seche, M. Seche, A. Burnei,
E. Ciobanu, E. Contras?, Z. Cret?a, V. Hristea,
L. Mares?, E. St??ngaciu, Z. S?tefa?nescu, T. T?ugulea,
I. Vulpescu, and T. Hristea. 1975. Dict?ionarul
Explicativ al Limbii Roma?ne. Editura Academiei
Republicii Socialiste Roma?nia.
D. Tufis. 1999. Tiered tagging and combined classi-
fiers. In Text, Speech and Dialogue, Lecture Notes
in Artificial Intelligence.
SenseLearner: Minimally Supervised Word Sense Disambiguation
for All Words in Open Text
Rada Mihalcea and Ehsanul Faruque
Department of Computer Science
University of North Texas
 
rada,faruque  @cs.unt.edu
Abstract
This paper introduces SENSELEARNER ? a mini-
mally supervised sense tagger that attempts to dis-
ambiguate all content words in a text using the
senses from WordNet. SENSELEARNER partici-
pated in the SENSEVAL-3 English all words task,
and achieved an average accuracy of 64.6%.
1 Introduction
The task of word sense disambiguation consists
of assigning the most appropriate meaning to a
polysemous word within a given context. Appli-
cations such as machine translation, knowledge
acquisition, common sense reasoning, and oth-
ers, require knowledge about word meanings, and
word sense disambiguation is considered essential
for all these applications.
Most of the efforts in solving this problem
were concentrated so far toward targeted super-
vised learning, where each sense tagged occur-
rence of a particular word is transformed into a
feature vector, which is then used in an automatic
learning process. The applicability of such super-
vised algorithms is however limited only to those
few words for which sense tagged data is avail-
able, and their accuracy is strongly connected to
the amount of labeled data available at hand.
Instead, methods that address all words in
open-text have received significantly less atten-
tion. While the performance of such methods is
usually exceeded by their supervised corpus-based
alternatives, they have however the advantage of
providing larger coverage.
In this paper, we introduce a new method for
solving the semantic ambiguity of all content
words in a text. The algorithm can be thought of
as a minimally supervised WSD algorithm in that
it uses a small data set for training purposes, and
generalizes the concepts learned from the training
data to disambiguate the words in the test data set.
As a result, the algorithm does not need a sepa-
rate classifier for each word to be disambiguated.
Moreover, it does not requires thousands of occur-
rences of the same word to be able to disambiguate
the word; in fact, it can successfully disambiguate
a content word even if it did not appear in the train-
ing data.
2 Background
For some natural language processing tasks, such
as part of speech tagging or named entity recogni-
tion, regardless of the approach considered, there
is a consensus on what makes a successful algo-
rithm (Resnik and Yarowsky, 1997). Instead, no
such consensus has been reached yet for the task
of word sense disambiguation, and previous work
has considered a range of knowledge sources, such
as local collocational clues, membership in a se-
mantically or topically related word class, seman-
tic density, etc. Other related work has been mo-
tivated by the intuition that syntactic information
in a sentence contains enough information to be
able to infer the semantics of words. For example,
according to (Gomez, 2001), the syntax of many
verbs is determined by their semantics, and thus it
is possible to get the later from the former. On the
other hand, (Lin, 1997) proposes a disambigua-
tion algorithm that relies on the basic intuition that
if two occurrences of the same word have identi-
cal meanings, then they should have similar local
context. He then extends this assumption one step
further and proposes an algorithm based on the in-
tuition that two different words are likely to have
similar meanings if they occur in an identical local
context.
3 SenseLearner
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm gen-
eral enough to be able to disambiguate all content
words in a text. We are therefore using (1) SemCor
(Miller et al, 1993) ? a balanced, semantically an-
notated dataset, with all content words manually
tagged by trained lexicographers ? to learn a se-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
mantic language model for the words seen in the
training corpus; and (2) information drawn from
WordNet (Miller, 1995), to derive semantic gen-
eralizations for those words that did not appear in
the annotated corpus.
The input to the disambiguation algorithm con-
sists of raw text. The output is a text with word
meaning annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with
parts of speech; collocations are identified using
a sliding window approach, where a collocation is
considered to be a sequence of words that forms
a compound concept defined in WordNet; named
entities are also identified at this stage.
Next, the following two main steps are applied
sequentially:
1. Semantic Language Model. In the first step, a
semantic language model is learned for each
part of speech, starting with the annotated
corpus. These models are then used to anno-
tate words in the test corpus with their cor-
responding meaning. This step is applica-
ble only to those words that appeared at least
once in the training corpus.
2. Semantic Generalizations using Syntactic
Dependencies and a Conceptual Network.
This method is applied to those words not
covered by the semantic language model.
Through the semantic generalizations it
makes, this second step is able to annotate
words that never appeared in the training cor-
pus.
3.1 Semantic Language Model
The role of this first module is to learn a global
model for each part of speech, which can be used
to disambiguate content words in any input text.
Although significantly more general than models
that are built individually for each word in a test
corpus as in e.g. (Hoste et al, 2002) ? the models
can only handle words that were previously seen
in the training corpus, and therefore their coverage
is not 100%.
Starting with an annotated corpus formed by all
annotated files in SemCor, a separate training data
set is built for each part of speech. The following
features are used to build the training models.
Nouns   The first noun, verb, or adjective be-
fore the target noun, within a window of
at most five words to the left, and its part
of speech.
Verbs   The first word before and the first
word after the target verb, and its part
of speech.
Adj   One relying on the first noun after the
target adjective, within a window of at
most five words.
  A second model relying on the first
word before and the first word after the
target adjective, and its part of speech.
The two models for adjectives are applied in-
dividually, and then combined through vot-
ing.
For each open-class word in the training cor-
pus (i.e. SemCor), a feature vector is built and
added to the corresponding training set. The la-
bel of each such feature vector consists of the tar-
get word and the corresponding sense, represented
as word#sense. Using this procedure, a total of
170,146 feature vectors are constructed: 86,973
vectors in the noun model, 47,838 in the verb
model, and 35,335 vectors in each of the two ad-
jective models.
To annotate new text, similar vectors are cre-
ated for all content-words in the raw text. The
vectors are stored in different files based on their
syntactic class, and a separate learning process is
run for each part-of-speech. For learning, we are
using the Timbl memory based learning algorithm
(Daelemans et al, 2001), which was previously
found useful for the task of word sense disam-
biguation (Mihalcea, 2002).
Following the learning stage, each vector in the
test data set ? and thus each content word ? is la-
beled with a predicted word and sense. If the word
predicted by the learning algorithm coincides with
the target word in the test feature vector, then the
predicted sense is used to annotate the test in-
stance. Otherwise, if the predicted word is dif-
ferent than the target word, no annotation is pro-
duced, and the word is left for annotation in a later
stage.
During the evaluations on the SENSEVAL-3 En-
glish all-words data set, 1,782 words were tagged
using the semantic language model, resulting in an
average coverage of 85.6%.
3.2 Semantic Generalizations using Syntactic
Dependencies and a Conceptual Network
Similar to (Lin, 1997), we consider the syn-
tactic dependency of words, but we also con-
sider the conceptual hierarchy of a word obtained
through the WordNet semantic network ? as a
means for generalization, capable to handle un-
seen words. Thus, this module can disambiguate
multiple words using the same knowledge source.
Moreover, the algorithm is able to disambiguate a
word even if it does not appear in the training cor-
pus. For instance, if we have a verb-object depen-
dency pair, ?drink water? in the training corpus,
using the conceptual hierarchy, we will be able
to successfully disambiguate the verb-object pair
?take tea?, even if this particular pair did not ap-
pear in the training corpus. This is done via the
generalization learned from the semantic network
? ?drink water? allows us to infer a more general
relation ?take-in liquid?, which in turn will help
disambiguate the pair ?take tea?, as a specializa-
tion for ?take-in liquid?.
The semantic generalization algorithm is di-
vided into two phases: training phase and test
phase.
Training Phase As mentioned above, we use
the annotated data provided in SemCor for train-
ing purposes. In order to combine the syntactic
dependency of words and the conceptual hierar-
chies through WordNet hypernymy relations, the
following steps are performed:
1. Remove the SGML tags from SemCor, and
produce a raw file with one sentence per line.
2. Parse the sentence using the Link parser
(Sleator and Temperley, 1993), and save all
the dependency-pairs.
3. Add part-of-speech and sense information (as
provided by SemCor) to each open word in
the dependency-pairs.
4. For each noun or verb in a dependency-pair,
obtain the WordNet hypernym tree of the
word. We build a vector consisting of the
words themselves, their part-of-speech, their
WordNet sense, and a reference to all the hy-
pernym synsets in WordNet. The reason for
attaching hypernym information to each de-
pendency pair is to allow for semantic gener-
alizations during the learning phase.
5. For each dependency-pair, we generate posi-
tive feature vectors for the senses that appear
in the training set, and negative feature vec-
tors for all the remaining possible senses.
Test Phase After training, we can use the gen-
eralized feature vector to assign the appropriate
sense to new words in a test data set. In the test
phase, we complete the following steps:
1. Parse each sentences of the test file using
the Link parser, and save all the dependency-
pairs.
2. Start from the leftmost open word in the sen-
tence and retrieve all the other open words it
connects to.
3. For each such dependency-pair, create fea-
ture vectors for all possible combinations of
senses. For example, if the first open word in
the pair has two possible senses and the sec-
ond one has three possible senses, this results
in a total of six possible feature vectors.
4. Finally, we pass all these feature vectors to a
memory based learner, Timbl (Daelemans et
al., 2001), which will attempt to label each
feature vector with a positive or negative la-
bel, based on information learned from the
training data.
An Example Consider the following sentence
from SemCor: The Fulton County Grand Jury
said Friday an investigation of Atlanta?s recent
primary election produced ?no evidence? that any
irregularities took place. As mentioned before,
the first step consists of parsing the sentence and
collecting all possible dependency-pairs among
words, such as subject-verb, verb-object, etc. For
simplicity, let us focus on the verb-object rela-
tion between produce and evidence. We extract
the proper senses of the two words from Sem-
Cor. Thus, at this point, combining the syntac-
tic knowledge from the parser, and the semantic
knowledge extracted from SemCor, we know that
there is a object-verb link/relation between pro-
duced#v#4 and evidence#n#1.
We now look up the hypernym tree for each of
the words involved in the current dependency-pair,
and create a feature vector as follows:
Os, produce#v#4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, produce#v#4, expose#v#3, show#v#4, evidence#n#1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, evidence#n#1, informa-
tion#n#3, cognition#n#1, psychological feature#n#1
where Os indicates an object-verb relation, and the
null elements are used to pad the feature vector for
a constant size of 20 elements per word.
Assuming the following sentence in the test
data: ?expose meaningful information.?, we iden-
tify an object-verb relation between expose and in-
formation. Although none of the words in the pair
?expose information? appear in the training cor-
pus, by looking up the IS-A hierarchy from Word-
Net, we will be able to successfully disambiguate
this pair, as both ?expose? and ?information? ap-
pear in the feature vector (see the vector above).
4 Evaluation
The SENSELEARNER system was evaluated on
the SENSEVAL-3 English all words data ? a data
set consisting of three texts from the Penn Tree-
bank corpus, with a total of 2,081 annotated con-
tent words. Table 1 shows precision figures for
each part-of-speech (nouns, verbs, adjectives), and
contribution of each word class toward total recall.
Fraction of
Class Precision Recall
Nouns 69.4 31.0
Verbs 56.1 20.2
Adjectives 71.6 12.2
Total 64.6 64.6
Table 1: SENSELEARNER results in the
SENSEVAL-3 English all words task
The average precision of 64.6% compares fa-
vorably with the ?most frequent sense? baseline,
which was computed at 60.9%. Not surprisingly,
the verbs seem to be the most difficult word class,
which is most likely explained by the large num-
ber of senses defined in WordNet for this part of
speech.
5 Conclusion
In this paper, we proposed and evaluated a new al-
gorithm for minimally supervised word-sense dis-
ambiguation that attempts to disambiguate all con-
tent words in a text using the senses from Word-
Net. The algorithm was implemented in a system
called SENSELEARNER, which participated in the
SENSEVAL-3 English all words task and obtained
an average accuracy of 64.6% ? a significant im-
provement over the most frequent sense baseline
of 60.9%.
Acknowledgments
This work was partially supported by a National
Science Foundation grant IIS-0336793.
References
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2001. Timbl: Tilburg mem-
ory based learner, version 4.0, reference guide.
Technical report, University of Antwerp.
F. Gomez. 2001. An algorithm for aspects
of semantic interpretation using an enhanced
Wordnet. In Proceedings of the North Ameri-
can Association for Computational Linguistics
(NAACL 2001), Pittsburgh, PA.
V. Hoste, W. Daelemans, I. Hendrickx, and
A. van den Bosch. 2002. Evaluating the re-
sults of a memory-based word-expert approach
to unrestricted word sense disambiguation. In
Proceedings of the ACL Workshop on ?Word
Sense Disambiguatuion: Recent Successes and
Future Directions?, Philadelphia, July.
D. Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In
Proceedings of the Association for Computa-
tional Linguistics, Madrid, Spain.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word
Sense Disambiguation. In Proceedings of the
19th International Conference on Computa-
tional Linguistics (COLING 2002), Taipei, Tai-
wan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker.
1993. A semantic concordance. In Proceedings
of the 3rd DARPA Workshop on Human Lan-
guage Technology, pages 303?308, Plainsboro,
New Jersey.
G. Miller. 1995. Wordnet: A lexical database.
Communication of the ACM, 38(11):39?41.
P. Resnik and D. Yarowsky. 1997. A perspec-
tive on word sense disambiguation methods and
their evaluation. In Proceedings of ACL Siglex
Workshop on Tagging Text with Lexical Seman-
tics, Why, What and How?, Washington DC,
April.
D. Sleator and D. Temperley. 1993. Parsing En-
glish with a Link grammar. In Third Interna-
tional Workshop on Parsing Technologies.
An Algorithm for Open Text Semantic Parsing
Lei Shi and Rada Mihalcea
Department of Computer Science
University of North Texas
leishi@unt.edu, rada@cs.unt.edu
Abstract
This paper describes an algorithm for open text shal-
low semantic parsing. The algorithm relies on a
frame dataset (FrameNet) and a semantic network
(WordNet), to identify semantic relations between
words in open text, as well as shallow semantic fea-
tures associated with concepts in the text. Parsing
semantic structures allows semantic units and con-
stituents to be accessed and processed in a more
meaningful way than syntactic parsing, moving the
automation of understanding natural language text
to a higher level.
1 Introduction
The goal of the semantic parser is to analyze the
semantic structure of a natural language sentence.
Similar in spirit with the syntactic parser ? whose
goal is to parse a valid natural language sentence
into a parse tree indicating how the sentence can
be syntactically decomposed into smaller syntactic
constituents ? the purpose of the semantic parser is
to analyze the structure of sentence meaning. Sen-
tence meaning is composed by entities and interac-
tions between entities, where entities are assigned
semantic roles, and can be further modified by other
modifiers. The meaning of a sentence is decom-
posed into smaller semantic units connected by var-
ious semantic relations by the principle of compo-
sitionality, and the parser represents the semantic
structure ? including semantic units as well as se-
mantic relations, connecting them into a formal for-
mat.
In this paper, we describe the main components
of the semantic parser, and illustrate the basic pro-
cedures involved in parsing semantically open text.
We believe that such structures, reflecting various
levels of semantic interpretation of the text, can be
used to improve the quality of text processing appli-
cations, by taking into account the meaning of text.
The paper is organized as follows. We first de-
scribe the semantic structure of English sentences,
as the basis for semantic parsing. We then intro-
duce the knowledge bases utilized by the parser, and
show how we use this knowledge in the process of
semantic parsing. Next, we describe the parsing
algorithm and elaborate on each of the three main
steps involved in the process of semantic parsing:
(1) syntactic and shallow semantic analysis, (2) se-
mantic role assignment, and (3) application of de-
fault rules. Finally, we illustrate the parsing process
with several examples, and show how the semantic
parsing algorithm can be integrated into other lan-
guage processing systems.
2 Semantic Structure
Semantics is the denotation of a string of symbols,
either a sentence or a word. Similar to a syn-
tactic parser, which shows how a larger string is
formed by smaller strings from a formal point of
view, the semantic parser shows how the denotation
of a larger string ? sentence, is formed by deno-
tations of smaller strings ? words. Syntactic rela-
tions can be described using a set of rules about how
a sentence string is formally generated using word
strings. Instead, semantic relations between seman-
tic constituents depend on our understanding of the
world, which is across languages and syntax.
We can model the sentence semantics as describ-
ing entities and interactions between entities. Enti-
ties can represent physical objects, as well as time,
places, or ideas, and are usually formally realized
as nouns or noun phrases. Interactions, usually real-
ized as verbs, describe relationships or interactions
between participating entities. Note that a partic-
ipant can also be an interaction, which can be re-
garded as an entity nominalized from an interaction.
We assign semantic roles to participants, and their
semantic relations are identified by the case frame
introduced by their interaction. In a sentence, par-
ticipants and interactions can be further modified
by various modifiers, including descriptive modi-
fiers that describe attributes such as drive slowly,
restrictive modifiers that enforce a general denota-
tion to become more specific such as musical in-
strument, referential modifiers that indicate partic-
ular instances such as the pizza I ordered. Other
semantic relations can also be identified, such as
coreference, complement, and others. Based on the
principle of compositionality, the sentence semantic
structure is recursive, similar to a tree.
The semantic parser analyzes shallow-level se-
mantics, which is derived directly from linguis-
tic knowledge, such as rules about semantic
role assignment, lexical semantic knowledge, and
syntactic-semantic mappings, without taking into
account any context or common sense knowledge.
The parser can be used as an intermediate semantic
processing tool before higher levels of text under-
standing.
3 Knowledge Bases for Semantic Parsing
One major problem faced by many natural language
understanding applications that rely on syntactic
analysis of text, is the fact that similar syntactic pat-
terns may introduce different semantic interpreta-
tions. Likewise, similar meanings can be syntac-
tically realized in many different ways. The seman-
tic parser attempts to solve this problem, and pro-
duces a syntax-independent representation of sen-
tence meaning, so that semantic constituents can be
accessed and processed in a more meaningful and
flexible way, avoiding the sometimes rigid interpre-
tations produced by a syntactic analyzer. For in-
stance, the sentences I boil water and water boils
contain a similar relation between water and boil,
even though they have different syntactic structures.
To deal with the large number of cases where the
same syntactic relation introduces different seman-
tic relations, we need knowledge about how to map
syntax to semantics. To this end, we use two main
types of knowledge ? about words, and about rela-
tions between words. The first type of knowledge
is drawn from WordNet ? a large lexical database
with rich information about words and concepts.
We refer to this as word-level knowledge. The lat-
ter is derived from FrameNet ? a resource that con-
tains information about different situations, called
frames, in which semantic relations are syntacti-
cally realized in natural language sentences. We
call this sentence-level knowledge. In addition to
these two lexical knowledge bases, the parser also
utilizes a set of manually defined rules, which en-
code mappings from syntactic structures to seman-
tic relations, and which are also used to handle those
structures not explicitly addressed by FrameNet or
WordNet.
In this section, we describe the type of infor-
mation extracted from these knowledge bases, and
show how this information is encoded in a format
accessible to the semantic parser.
3.1 Frame Identification and Semantic Role
Assignment
FrameNet (Johnson et al, 2002) provides the
knowledge needed to identify case frames and se-
mantic roles. FrameNet is based on the theory of
frame semantics, and defines a sentence level on-
tology. In frame semantics, a frame corresponds to
an interaction and its participants, both of which
denote a scenario, in which participants play some
kind of roles. A frame has a name, and we use this
name to identify the semantic relation that groups
together the semantic roles. In FrameNet, nouns,
verbs and adjectives can be used to identify frames.
Each annotated sentence in FrameNet exempli-
fies a possible syntactic realization for the seman-
tic roles associated with a frame for a given target
word. By extracting the syntactic features and cor-
responding semantic roles from all annotated sen-
tences in the FrameNet corpus, we are able to auto-
matically build a large set of rules that encode the
possible syntactic realizations of semantic frames.
In our implementation, we use only verbs as
target words for frame identification. Currently,
FrameNet defines about 1700 verbs attached to 230
different frames. To extend the parser coverage to
a larger subset of English verbs, we are using Verb-
Net (Kipper et al, 2000), which allows us to handle
a significantly larger set of English verbs.
VerbNet is a verb lexicon compatible with Word-
Net, but with explicitly stated syntactic and se-
mantic information using Levin?s verb classification
(Levin, 1993). The fundamental assumption is that
the syntactic frames of a verb as an argument-taking
element are a direct reflection of the underlying se-
mantics. Therefore verbs in the same VerbNet class
usually share common FrameNet frames, and have
the same syntactic behavior. Hence, rules extracted
from FrameNet for a given verb can be easily ex-
tended to verbs in the same VerbNet class. To en-
sure a correct outcome, we have manually validated
the FrameNet-VerbNet mapping, and corrected the
few discrepancies that were observed between Verb-
Net classes and FrameNet frames.
3.1.1 Rules Learned from FrameNet
FrameNet data ?is meant to be lexicographically rel-
evant, not statistically representative? (Johnson et
al., 2002), and therefore we are using FrameNet as
a starting point to derive rules for a rule-based se-
mantic parser.
To build the rules, we are extracting several syn-
tactic features. Some are explicitly encoded in
FrameNet, such as the grammatical function (GF)
and phrase type (PT) features.
In addition, other syntactic features are extracted
from the sentence context. One such feature is the
relative position (RP) to the target word. Sometimes
the same syntactic constituent may play different se-
mantic roles according to its position with respect
to the target word. For instance the sentences: I pay
you. and You pay me. have different roles assigned
to the same lexical unit you based on the relative
position with respect to the target word pay.
Another feature is the voice of the sentence. Con-
sider these examples: I paid Mary 500 dollars. and
I was paid by Mary 500 dollars. In these two sen-
tences, I has the same values for the features GF, PT
and RP, but it plays completely different roles in the
same frame because of the difference of voice.
If the phrase type is prepositional phrase (PP), we
also record the actual preposition that precedes the
phrase. Consider these examples: I was paid for my
work. and I was paid by Mary. The prepositional
phrases in these examples have the same values for
the features GF, PT, and RP, but different preposi-
tions differentiate the roles they should play.
After we extract all these syntactic features, the
semantic role is appended to the rule, which creates
a mapping from syntactic features to semantic roles.
Feature sets are arranged in a list, the order of
which is identical to that in the sentence. The or-
der of sets within the list is important, as illustrated
by the following example: ?I give the boy a ball.?
Here, the boy and a ball have the same features
as described above, but since the boy occurs be-
fore a ball, then the boy plays the role of recipi-
ent. Altogether, the rule for a possible realization
of a frame exemplified by a tagged sentence is an
ordered sequence of syntactic features with their se-
mantic roles.
For instance, Table 1 lists the syntactic and se-
mantic features extracted from FrameNet for the
sentence I had chased Selden over the moor.
I had chased Selden over the moor
GF Ext obj comp
PT NP Target NP PP
Position before after after
Voice active
PP over
Role Theme Goal Path
Table 1: Example sentence with syntactic and se-
mantic features
The corresponding formalized rule for this sen-
tence is:
[active, [ext,np,before,theme], [obj,np,
after,goal], [comp,pp,after,over,path]]
In FrameNet, there are multiple annotated sen-
tences for each frame to demonstrate multiple pos-
sible syntactic realizations. All possible realizations
of a frame are collected and stored in a list for that
frame, which also includes the target word, its syn-
tactic category, and the name of the frame. All the
frames defined in FrameNet are transformed into
this format, so that they can be easily handled by
the rule-based semantic parser.
3.2 Word Level Knowledge
WordNet (Miller, 1995) is the resource used to iden-
tify shallow semantic features that can be attached
to lexical units. For instance, attribute relations,
adjective/adverb classifications, and others, are se-
mantic features extracted from WordNet and stored
together with the words, so that they can be directly
used in the parsing process.
All words are uniformly defined, regardless of
their class. Features are assigned to each word, in-
cluding syntactic and shallow semantic features, in-
dicating the functions played by the word. Syntactic
features are used by the feature-augmented syntac-
tic analyzer to identify grammatical errors and pro-
duce syntactic information for semantic role assign-
ment. Semantic features encode lexical semantic in-
formation extracted from WordNet that is used to
determine semantic relations between words in var-
ious situations.
Features can be arbitrarily defined, as long as
there are rules to handle them. The features we
define encode information about the syntactic
category of a word, number and countability for
nouns, transitivity and form for verbs, type, degree,
and attribute for adjectives and adverbs, and others.
Table 2 lists the main features used for content
words.
Feature Values
Nouns
Number singular/plural
Countability countable/uncountable
Verbs
Transitivity transitive/intransitive/double transitive
Form normal/infi nitive/present
participle/past participle
Adjectives
Type descriptive/restrictive/referential
Attribute arbitrary
Degree base/comparative/superlative
Adverbs
Type descriptive/restrictive/referential
Attribute arbitrary
Degree base/comparative/superlative
Table 2: Features for content words
For example, for the word dog, the entry in the
lexicon is defined as:
lex(dog,W):- W= [parse:dog, cat:noun,
num:singular, count:countable].
Here, the category (cat) is defined as noun, the
number (num) is singular, and we also record the
countability (count)1.
For adjectives, the value of the attribute feature
is also stored, which is provided by the attribute re-
lation in WordNet. This relation links a descriptive
adjective to the attribute (noun) it modifies, such as
slow ? speed. For example, for the adjective slow,
the entry in the lexicon is defined as:
lex(slow,W):- W= [parse:slow, cat:adj,
attr:speed, degree:base, type:descriptive].
Here, the category (cat) is defined as adjective,
the type is descriptive, degree is base form. We also
record the attr feature, which is derived from the at-
tribute relation in WordNet, and links a descriptive
adjective to the attribute (noun) it modifies, such as
slow? speed.
We are also exploiting the transitional relations
from adverbs to adjectives and to nouns. We noticed
that some descriptive adverbs have correspondence
to descriptive adjectives, which in turn are linked to
nouns by the attribute relation. Using these transi-
tional links, we derive relations like: slowly? slow
? speed. A typical descriptive adverb is defined as
follows:
lex(slowly,W):- W= [parse:slowly, cat:adv,
attr:speed, degree:base, type:descriptive].
In addition to incorporating semantic information
from WordNet into the lexicon, this word level on-
tology is also used to derive default rules, as dis-
cussed later.
3.3 Hand-coded Knowledge
The FrameNet database encodes various syntac-
tic realizations only for semantic roles within a
frame. Syntax-semantics mappings other than se-
mantic roles are manually encoded as rules inte-
grated in the syntactic-semantic analyzer. The an-
alyzer determines the syntactic structure of the sen-
tence, and once a particular syntactic constituent
is identified, its corresponding mapping rules are
immediately applied. The syntactic constituent is
1The value for the feature (countability) is obtained
from word properties stored in the Link parser dictionaries
(http://www.link.cs.cmu.edu/link/). The Link dictionaries are
also used to derive the lists of words to be stored in the lexi-
con. Note however that the Link parser itself is not used in the
parsing process.
then translated into its corresponding semantic con-
stituent, together with the relevant semantic infor-
mation.
Some semantic relations can be directly derived
from syntactic patterns. For example, a restrictive
relative clause such as ?the man that you see? serves
as a referential modifier. An adverbial clause be-
ginning with ?because? is a modifier describing the
?reason? of the interaction. The inflection from
?apple? to ?apples? adds an attributive modifier of
quantity to the entity ?apple?.
However, syntactic relations may often introduce
semantic ambiguity, with multiple possible interpre-
tations. To handle these cases, we encode rules that
describe all possible interpretations of any given
structure, and then use lexical semantic informa-
tion as selectional restrictions for ambiguity reso-
lution. For instance, in ?a book on Chinese his-
tory?, on Chinese history describes the topic of the
book and this interpretation can be uniquely deter-
mined by noting that history is not a physical object,
and thus the interpretation of on Chinese history as
describing location is semantically anomalous. In-
stead, in ?a book on the computer?, on the computer
may describe a location, but it could also describe
the book topic, and hence the correct interpretation
of this sentence cannot be determined without ad-
ditional context. In such cases, the semantic parser
produces all possible interpretations, allowing sys-
tems that use the semantic parser?s output to deter-
mine the right interpretation that best fits the appli-
cation at hand.
Selectional restrictions ? as part of the hand-
coded knowledge ? are used for both semantic
role identification and syntax-semantics translation.
These additional rules are needed to supplement the
information encoded in FrameNet, since FrameNet
only annotates syntactic features, which often times
do not provide enough information for identifying
correct semantic roles.
Consider for example ?I break the window? vs.
?The hammer breaks the window?. According to
our semantic parser, the participants in the interac-
tion ?break? have exactly the same syntactic fea-
tures in both sentences, but they play different se-
mantic roles (?I? plays the agent role while ?ham-
mer? plays the instrument role), since they belong
to different ontological categories: ?I? refers to a
person and ?hammer? refers to a tool. This interpre-
tation is not possible using only FrameNet informa-
tion, and thus we fill the gap by attaching selectional
restrictions to the rules extracted from FrameNet.
The definition of selectional restriction is based
on WordNet 2.0 noun hierarchy. We say that en-
tity E belongs to the ontological category C if the
noun E is a child node of C in the WordNet seman-
tic hierarchy of nouns. For example, if we define
the ontological category for the role ?instrument? as
instrumentality, then all hyponyms of instrumental-
ity can play this role, while other nouns like ?boy?,
which are not part of the instrumentality category
will be rejected. Selectional restrictions are defined
using a Disjunctive Normal Form (DNF) in the fol-
lowing format:
[Onto(ID,P),Onto(ID,P),...],[Onto(ID,P),...],...
Here, ?Onto? is a noun and ID is its Word-
Net sense, which uniquely identifies Onto as a
node in the semantic network. ?P? can be set
to p (positive) or n (negative), denoting if a noun
should belong to the given category or not. For
example, [person(1,n),object(1,p)],[substance(1,p)]
means that the noun should belong to object(sense
#1) but not person(sense #1)2, or it should belong
to substance(sense #1). This information is added
to the rules derived from FrameNet, and therefore
after this step, a complete FrameNet rule entry is:
[Voice,[GF,PT,SelectionalRestriction,Role],...].
4 Semantic Parsing
The general procedure of semantic parsing consists
of three main steps3: (1) The syntactic-semantic
analyzer analyzes the syntactic structure, and uses
hand-coded rules as well as lexical semantic knowl-
edge to identify some semantic relations between
constituents. It also prepares syntactic features for
semantic role assignment in the next step. (2) The
role assigner uses rules extracted from FrameNet,
and assigns semantic roles for identified partici-
pants, based on their syntactic features as produced
in the first step. (3) For those constituents not exem-
plified in FrameNet, we apply default rules to decide
their default meaning.
4.1 Feature Augmented Syntactic-Semantic
Analyzer
The analyzer is implemented as a bottom-up chart
parsing algorithm based on features. We include
rules of syntax-semantics mappings in the unifica-
tion based formalism. The parser analyzes syntac-
tic relations and immediately applies corresponding
mapping rules to obtain semantic relations when a
2person(sense #1) is a child node of object(sense #1) in
WordNet
3The parsing algorithm is implemented as a rule-based sys-
tem using a declarative programming language Prolog.
syntactic relation is identified. Most semantic rela-
tions (e.g. various modifiers) are identified in this
step, except semantic role annotation and applica-
tion of default rules, which are postponed for a later
stage. The analyzer generates an intermediate for-
mat, where target words and arguments are explic-
itly tagged with their syntactic and semantic fea-
tures, so that they can be matched against the rules
derived from FrameNet. We are using a feature-
based analyzer that accomplishes three main tasks:
4.1.1 Check if the sentence is grammatically
correct
The syntactic analyzer is based on a feature aug-
mented grammar, and therefore has the capability of
detecting if a sentence is grammatically correct (un-
like statistical parsers, which attempt to parse any
sentence, regardless of their well-formness). The
grammar consists of a set of rules defining how con-
stituents with different syntactic or semantic fea-
tures can unify with each other.
By defining a grammar in this way, using fea-
tures, once the right features are selected, the an-
alyzer can reject some grammatically incorrect sen-
tences such as: I have much apples., You has my
car., and some semantically anomalous sentences:
The technology is very military.4.
4.1.2 Provide features for semantic role
assignment
Through syntactic-semantic analysis in the first
step, sentences are transformed into a format
in which target words and syntactic constituents
are explicitly tagged with their features. Unlike
FrameNet ? which may also assign roles to adverbs,
we only use the subject, object(s) and prepositional
phrases as potential participants in the interaction
for semantic role labeling5. The analyzer marks
verbs as target words for frame identification, iden-
tifies constituents for semantic role assignment, and
produces features such as GF, PT, Voice, Preposi-
tion, as well as ontological categories for each con-
stituent, in a format identical to the rules extracted
from FrameNet, so that they can be matched with
the frame definitions.
The ontological categories of constituents are
used to match selectional restrictions, and are au-
tomatically derived from the head word of the noun
phrase, or the head word of the noun phrase of the
prepositional phrase. For other constituents that
act like nouns, such as pronouns, infinitive forms,
gerunds, or noun clauses, we have manually defined
4Since military is not a descriptive adjective, it cannot be
modifi ed by very and predicative use is forbidden.
5Adverbs are treated as modifi ers.
ontological categories. For example, ?book? is the
ontological category of the phrase ?the interesting
book? and ?on the book?. ?person? is the ontolog-
ical category we manually define for the pronoun
?he?. We have also defined several special onto-
logical categories that are not in WordNet such as
any, which can be matched to any selectional re-
striction, nonperson, which means everything ex-
cept person, and others. Note that this matching
procedure also plays the role of a word sense dis-
ambiguation tool, by selecting only those categories
that match the current frame constituents. After
this step, target words and syntactic constituents can
be assigned with the corresponding case frame and
semantic roles during the second step of semantic
parsing.
4.1.3 Identify some semantic relations
Some semantic relations can be identified in this
phase. These semantic relations include word level
semantic relations, and some semantic relations
that have direct syntactic correspondence by using
syntax-semantics mapping rules. This phase can
also identify the function of the sentence such as
assertion, query, yn-query, command etc, based on
syntactic patterns of the sentence.
The output of the analyzer is an intermediate for-
mat suitable for the semantic parser, which contains
syntactic features and identified semantic relations.
For example, the output for the sentence ?He kicked
the old dog.? is:
[assertion,
[[tag, ext, np, person,
[[entity, [he], reference(third)],
[modification(attribute), quantity(single)],
[modification(attribute), gender (male)]]],
[target, v, kick, active, [kick]],
[modification(attribute), time (past)],
[tag, obj, np, dog,
[[modification(reference), reference(the)],
[modification(attribute), age(old)],
[target, n, dog, [dog]]]]]
]
4.2 Semantic Role Assignment
In the process of semantic role assignment, we first
start by identifying all possible frames, according
to the target word. Next, a matching algorithm is
used to find the most likely match among all rules
of these frames, to identify the correct frame (or
frames if several are possible), and assign semantic
roles.
In a sentence describing an interaction, we select
the verb as the target word, which triggers the sen-
tence level frame and uses the FrameNet rules of
that target word for matching. If the verb is not
defined in FrameNet and VerbNet, we use Word-
Net synonymy relation to check if any of its syn-
onyms is defined in FrameNet or VerbNet. If such
synonyms exist, their rules are applied to the tar-
get word. This approach is based on the idea in-
troduced by Levin that ?what enables a speaker to
determine the behavior of a verb is its meaning?
(Levin, 1993). Synonymous verbs always intro-
duce the same semantic frame and usually have the
same syntactic behavior. To minimize information
in the verb lexicon, non-frequently used verbs usu-
ally inherit a subset of the syntactic behavior of
their frequently used synonyms. Since VerbNet has
defined a framework of syntactic-semantic behav-
ior for these frequently used verbs, the behavior of
other related verbs can be quite accurately predicted
by using WordNet synonymy relations. Using this
approach, we achieve a coverage of more than 3000
verbal lexical units.
The matching algorithm relies on a scoring
scheme to evaluate the similarity between two se-
quences of features. The matching starts from the
first constituent of the sentence. It looks through
the list of entries in the rule and when a match is
found, it moves to the next constituent looking for
a new match. A match involves match of syntactic
features, as well as match of selectional restrictions.
An exact match means that both syntactic features
and selectional restrictions are matched, which in-
crements the score of matching by 3. We apply
selectional restriction by looking up the WordNet
noun hierarchies. If the node of the ontological cat-
egory is within the areas that the selectional restric-
tion describes, this is regarded as a match. When
applying selectional restrictions, due to polysemy
of the ontological entries, we try all possible senses,
starting from the most frequently used sense accord-
ing to WordNet, until one sense meets the selec-
tional restriction. If the syntactic features match ex-
actly, but none of the possible word senses meet the
selectional restrictions, this is regarded as a partial
match, which increments the score by 2.
Partial matching is also possible, for a relaxed
application of selectional restriction. This enables
anaphora and metaphor resolution, in which the
constituents have either unknown ontological cate-
gory, or inherit features from other ontological cat-
egories (by applying high level knowledge such as
personification). The number of subjects and ob-
jects as well as their relative positions should be
strictly obeyed, since any variations may result in
significant differences for semantic role labeling.
Prepositional phrases are free in their location be-
cause the preposition is already a unique identi-
fier. Finally, after all constituents have found their
match, if there are still remaining entries in the
rule, the total score is decreased by 1. This is a
penalty paid by partial matches, since additional
constituents may indicate different semantic role la-
beling, which may change the interpretation of the
entire sentence.
A polysemous verb may belong to multiple
frames, and a frame pertaining to a given target
word may have multiple possible syntactic realiza-
tions, exemplified by different sentences in the cor-
pus. We try to match the syntactic features in the in-
termediate format with all the rules of all the frames
available for the target word, and compare their
matching scores. The rule with the highest score
is selected, and used for semantic role assignment.
Through this scoring scheme, the matching algo-
rithm tries to maximize the utilization of syntactic
and semantic information available in the sentence,
to correctly identify case frames and semantic roles.
4.2.1 Walk-Through Example
Assume the following two rules, triggered for the
target word break:
1: [active,[ext,np,[[person(1,p)]],agent],
[obj,np,[[object(1,p)]],theme],
[comp,pp,with,[[instrumentality(3,p)]],
instrument]]
2: [[ext,np,[[instrumentality(3,p)]],instrument],
[obj,np,[[person(1,n),object(1,p)]],theme]]
3: [[ext,np,[[person(1,n),object(1,p)]],theme]]
And the sentences:
A: I break the window with a hammer
B: The hammer breaks the window
C: The window breaks on the wall
The features identified by the analyzer are:
A?:[[ext,np,active,person],
[obj,np,active,window],
[comp,pp,active,with,hammer]]
B?:[[ext,np,active,hammer],
[obj,np,active,window]]
C?:[[ext,np,active,window],
[comp,pp,on,wall]]
Using the matching/scoring algorithm, the score
for matching A? to rule 1 is determined as 9 since
there are 3 exact matches, and to rule 2 as 5 since
there is an exact match for ?the window? but a par-
tial match for ?I?. Hence, the matching algorithm
selects rule 1, and the semantic role for ?I? is agent.
Similarly, when we match B? to rule 1, we obtain a
score of 4, since there is an exact match for ?the
window?, a partial match for ?the hammer?, and
rule 1 has an additional entry for a prepositional
phrase, which decrements the score by 1. It makes
a larger score of 6 for matching with rule 2. There-
fore, for the second case, the role assigned to ?the
hammer? is instrument. Rule 3 is not applied to the
first two sentences since they have additional ob-
jects; similarly, rule 1 and 2 cannot be applied to
sentence C for the same reason. The first constituent
in C finds an exact match in rule 3 with a total score
of 3, and hence ?the window? is assigned the correct
role theme. The prepositional phrase ?on the wall?,
for which no entry for labeling a role is found in rule
3, will be handled by default rules (see Section 4.3).
Based on the principle of compositionality, mod-
ifiers and constituents assigned semantic roles can
describe interactions, so the semantic role assign-
ment is performed recursively, until all roles within
frames triggered by all target words are assigned.
4.3 Applying Default Rules
We always assign semantic roles to subjects and ob-
jects6, but only some prepositional phrases can in-
troduce semantic roles, as defined in the FrameNet
case frames. Other prepositional phrases function
as modifiers; in order to handle these constituents,
and allow for a complete semantic interpretation of
the sentence, we have defined a set of default rules
that are applied as the last step of the semantic pars-
ing process. For example, FrameNet defines a role
for the prepositional phrase on him in ?I depend
on him? but not for on the street in ?I walk on the
street?, because it does not play a role, but it is a
modifier describing a location. Since the role for
the prepositional phrase beginning with on is not de-
fined for the target word walk in FrameNet, we ap-
ply the default rule that ?on something? modifies the
location attribute of the interaction walk. Note that
we include selectional restriction in the default rule
since constituents with the same syntactic features
such as ?on Tuesday? and ?on the table? may have
obviously different semantic interpretations. An ex-
ample of a default rule is shown below, indicating
that the interpretation of a prepositional phrase fol-
lowed by a time period (where time period is an
ontological category from WordNet) is that of time
modifier:
DefaultRule([_,pp,_,on,Onto,_],time):-
SelectionalRestriction
(Onto,1,
[[time_period(1,p)]])
We have defined around 100 such default rules,
which are applied during the last step of the seman-
6Where a subject and object are usually realized by noun
phrases, noun clauses, or infi nitive forms.
tic parsing process.
5 Parser Output and Evaluation
We illustrate here the output of the semantic parser
on a natural language sentence, and show the
corresponding semantic structure and tree7. For
example, for the sentence I like to eat Mexican food
because it is spicy, the semantic parser produces the
following encoding of sentence type, frames, se-
mantic constituents and roles, and various attributes
and modifiers:
T = assertion
P =
[[experiencer, [[entity, [i], reference(first)],
[modification(attribute), quantity(single)]]],
[interaction(experiencer_subj),[love]],
[modification(attribute), time(present)],
[content, [
[interaction(ingestion), [eat]],
[ingestibles, [entity, [food]]
[[modification(restriction), [mexican]],
]]]],
[reason, [[agent, [[entity, [it],
reference(third)],
[modification(attribute), quantity(single)]]],
[description,
[modification(attribute), time(present)]],
[modification(attribute),
taste_property(spicy)]]]
]
The corresponding parse tree is shown in Figure 1.
ingestion ), [eat]interaction(
I love to eat Mexican food, because it is spicy.
{[I], reference(first)}
S?[assertion]
interaction( experiencer_subj ), [love]
{[it], reference(third)}
time(present)
quantity(single) {food}
{mexican}
taste_property(spicy)
ingestibles
experiencer content reason
am am
sm
am
Figure 1: Semantic parse tree (am = attributive modifi er,
rm = referential modifi er, sm = restrictive modifi er)
We have conducted evaluations of the semantic
role assignment algorithm on 350 sentences ran-
domly selected from FrameNet. The test sentences
were removed from the FrameNet corpus, and the
rules-extraction procedure described earlier in the
paper was invoked on this reduced corpus. All test
sentences were then semantically parsed, and full
semantic annotations were produced for each sen-
tence. Notice that the evaluation is conducted only
7The semantic parser was demonstrated in a major Natural
Language Processing conference, and can be also demonstrated
during the workshop.
for semantic role assignment ? since this is the only
information available in FrameNet. The other se-
mantic annotations produced by the parser (e.g. at-
tribute, gender, countability) are not evaluated at
this point, since there are no hand-validated anno-
tations of this kind available in current resources.
Both frames and frame elements are automati-
cally identified by the parser. Out of all the elements
correctly identified, we found that 74.5% were as-
signed with the correct role (this is therefore the
accuracy of role assignment), which compares fa-
vorably with previous results reported in the liter-
ature for this task. Notice also that since this is a
rule-based approach, the parser does not need large
amounts of annotated data, and it works well the
same for words for which only one or two sentences
are annotated.
6 Interface and Integration to Other
Systems
The semantic algorithm uses linguistic knowledge,
such as syntactic realization of semantic roles in a
case frame, syntax-semantics mappings, and lexical
semantic knowledge, to parse the semantic structure
of open text. It can be regarded as a shallow se-
mantic analyzer, which provides partial results for
higher level understanding systems that can effec-
tively utilize context, commonsense, and other types
of knowledge, to achieve final accurate meaning in-
terpretations, or use custom defined rules for high
level processing in particular domains.
The matching/scoring scheme integrated in our
algorithm can effectively identify the right semantic
interpretation, but some semantic ambiguity cannot
be resolved without enough context and common-
sense knowledge. For example, although the fa-
mous meaningless sentence ?colorless green ideas
sleep furiously? can be correctly identified as se-
mantically anomalous by the semantic parser, by
analyzing the syntactic behavior of ?sleep? and the
selectional restrictions that we attach to this frame,
the sentence ?I saw the man in the park with the
telescope? has several semantic interpretations. Ac-
cording to the commonsense knowledge that we en-
code in the semantic parser (mostly drawn from
WordNet), telescope is defined as a tool to see some-
thing, and we may infer that ?with telescope? in this
sentence describes an instrument of ?see?. How-
ever, without enough context, not even humans can
rule out the possibility that the ?telescope? is the
man?s possession, rather than an instrument for the
interaction ?see?. The semantic parser maintains all
possible interpretations that cannot be rejected by
their syntactic and shallow semantic patterns, and
rank all of them by their scores as the likelihood of
being the correct interpretation. Other systems can
use high level knowledge such as common sense,
context or user defined rules to choose the right in-
terpretation.
As an integral part of the parsing system, we pro-
vide several interfaces that allow other systems or
additional modules to change the behavior of the
parser based on their rules and knowledge. One
such interface is the ontochg predicate, which is
called whenever the ontological category is identi-
fied for a constituent during the syntactic-semantic
analysis. By default, it outputs the same ontolog-
ical category as identified by the parser, but other
systems can change the content of this predicate to
replace the ontological category identified by the
parser with other categories, according to their rules
and knowledge. This is particularly useful for inte-
grating add-ons capable of anaphora and metaphor
resolution. The adjatt predicate is another interface
for add-ons that can resolve polysemy of descriptive
adjectives and adverbs. Due to polysemy, some de-
scriptive adjectives and adverbs may modify differ-
ent attributes in different situations and sometimes
the resolution requires high level understanding us-
ing commonsense knowledge and context. These
interfaces make the semantic parser more flexible,
robust, and easier to integrate into other systems that
achieve high level meaning processing and under-
standing.
7 Related Work
There are several statistical approaches for auto-
matic semantic role labeling based on PropBank
and FrameNet. (Gildea and Jurafsky, 2000) pro-
posed a statistical approach based on FrameNet I
data for annotation of semantic roles. Fleischman
(Fleischman et al, 2003) used FrameNet annota-
tions in a maximum entropy framework. A more
flexible generative model is proposed in (Thomp-
son et al, 2003), where null-instantiated roles can
be also identified, and frames are not assumed to be
known a-priori. These approaches exclusively focus
on semantic roles labeling based on statistical meth-
ods, rather than analysis of the full structure of sen-
tence semantics. However, a rule-based approach
is closer to the way humans interpret the semantic
structure of a sentence. Moreover, as mentioned
earlier, the FrameNet data is not meant to be ?sta-
tistically representative? (Johnson et al, 2002), but
rather illustrative for various language constructs,
and therefore a rule-based approach is more suitable
for this lexical resource.
8 Conclusions
In this paper, we proposed an algorithm for open
text shallow semantic parsing. The algorithm has
the capability to analyze the semantic structure of
a sentence, and show how the meaning of the en-
tire sentence is composed of smaller semantic units,
linked by various semantic relations. The parsing
process utilizes linguistic knowledge, consisting of
rules derived from a frame dataset (FrameNet), a se-
mantic network (WordNet), as well as hand-coded
rules of syntax-semantics mappings, which encode
natural selectional restrictions. Parsing semantic
structures allows semantic units and constituents to
be accessed and processed in a more meaningful
way than syntactic parsing, and enables higher-level
text understanding applications. We believe that the
semantic parser will prove useful for a range of lan-
guage processing applications that require knowl-
edge of text meaning, including word sense disam-
biguation, information retrieval, question answer-
ing, machine translation, and others.
References
M. Fleischman, N. Kwon, and E. Hovy. 2003.
Maximum entropy models for FrameNet classi-
fication. In Proceedings of 2003 Conference on
Empirical Methods in Natural Language Pro-
cessing EMNLP-2003, Sapporo, Japan.
D. Gildea and D. Jurafsky. 2000. Automatic label-
ing of semantic roles. In Proceedings of the 38th
Annual Conference of the Association for Com-
putational Linguistics (ACL 2000), pages 512?
520, Hong Kong, October.
C. Johnson, C. Fillmore, M. Petruck, C. Baker,
M. Ellsworth, J. Ruppenhofer, and E. Wood.
2002. FrameNet: Theory and Practice.
http://www.icsi.berkeley.edu/ framenet.
K. Kipper, H.T.Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceed-
ings of Seventeenth National Conference on Arti-
ficial Intelligence AAAI 2000, Austin,TX, July.
B. Levin. 1993. English Verb Classes and Alterna-
tion: A Preliminary Investigation. The Univer-
sity of Chicago Press.
G. Miller. 1995. Wordnet: A lexical database.
Communication of the ACM, 38(11):39?41.
C. Thompson, R. Levy, and C. Manning. 2003. A
generative model for FrameNet semantic role la-
beling. In Proceedings of the Fourteenth Euro-
pean Conference on Machine Learning ECML-
2003, Croatia.
Co-training and Self-training for Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu
Abstract
This paper investigates the application of co-
training and self-training to word sense disam-
biguation. Optimal and empirical parameter se-
lection methods for co-training and self-training
are investigated, with various degrees of error
reduction. A new method that combines co-
training with majority voting is introduced, with
the effect of smoothing the bootstrapping learn-
ing curves, and improving the average perfor-
mance.
1 Introduction
The task of word sense disambiguation consists in assign-
ing the most appropriate meaning to a polysemous word
within a given context. Most of the efforts in solving
this problem were concentrated so far towards supervised
learning, where each sense tagged occurrence of a par-
ticular word is transformed into a feature vector, which
is then used in an automatic learning process. While
these algorithms usually achieve the best performance, as
compared to their unsupervised or knowledge-based al-
ternatives, there is an important shortcoming associated
with these methods: their applicability is limited only to
those words for which sense tagged data is available, and
their accuracy is strongly connected to the amount of la-
beled data available at hand. In this paper, we investigate
methods for building sense classifiers when only relatively
few annotated examples are available. We explore boot-
strapping methods using co-training and self-training, and
evaluate their performance on the SENSEVAL-2 nouns.
We show that classifiers built for different words have dif-
ferent behavior during the bootstrapping process. If the
right parameters for co-training and self-training can be
identified (growth size, pool size, and number of itera-
tions, as explained later in the paper), an average error
reduction of 25.5% is achieved, with similar performance
observed for both co-training and self-training. However,
with empirical settings, the error reduction is significantly
smaller, with a 9.8% error rate reduction achieved for a
new method that combines co-training with majority vot-
ing.
We first overview the general approach of bootstrap-
ping for natural language learning using co-training and
self-training. We then introduce the problem of super-
vised word sense disambiguation, and define several local
and topical basic classifiers. We investigate the applica-
bility of co-training and self-training to supervised word
sense disambiguation, starting with these basic classifiers,
and perform comparative evaluations of optimal and em-
pirical bootstrapping parameter settings.
2 Co-training and Self-training for Natural
Language Learning
Co-training and self-training are bootstrapping methods
that aim to improve the performance of a supervised learn-
ing algorithm by incorporating large amounts of unlabeled
data into the training data set.
2.1 Co-training
Starting with a set of labeled data, co-training algorithms
(Blum and Mitchell, 1998) attempt to increase the amount
of annotated data using some (large) amounts of unlabeled
data. Shortly, co-training algorithms work by generating
several classifiers trained on the input labeled data, which
are then used to tag new unlabeled data. From this newly
annotated data, the most confident predictions are sought,
and subsequently added to the set of labeled data. The
process may continue for several iterations.
In natural language learning, co-training was applied
to statistical parsing (Sarkar, 2001), reference resolution
(Ng and Cardie, 2003), part of speech tagging (Clark et
al., 2003), and others, and was generally found to bring
improvement over the case when no additional unlabeled
data are used.
One important aspect of co-training consists in the re-
lation between the views used in learning. In the original
definition of co-training, (Blum and Mitchell, 1998) state
conditional independence of the views as a required cri-
terion for co-training to work. In recent work, (Abney,
2002) shows that the independence assumption can be re-
laxed, and co-training is still effective under a weaker in-
dependence assumption. He is proposing a greedy algo-
rithm to maximize agreement on unlabelled data, which
produces good results in a co-training experiment for
named entity classification. Moreover, (Clark et al, 2003)
show that a naive co-training process that does not explic-
itly seek to maximize agreement on unlabelled data can
lead to similar performance, at a much lower computa-
tional cost. In this work, we apply co-training by identify-
ing two different feature sets based on a ?local versus topi-
cal? feature split, which represent potentially independent
views for word sense classification, as shown in Section 4.
2.2 Self-training
While there is a common agreement on the definition
of co-training, the literature provides several sometimes
conflicting definitions for self-training. (Ng and Cardie,
2003) define self-training as a ?single-view weakly super-
vised algorithm?, build by training a committee of clas-
sifiers using bagging, combined with majority voting for
final label selection. (Clark et al, 2003) provide a differ-
ent definition: self-training is performed using ?a tagger
that is retrained on its own labeled cache on each round?.
We adopt this second definition, which also agrees with
the definition given in (Nigam and Ghani, 2000).
Self-training starts with a set of labeled data, and builds
a classifier, which is then applied on the set of unlabeled
data. Only those instances with a labeling confidence ex-
ceeding a certain threshold are added to the labeled set.
The classifier is then retrained on the new set of labeled
examples, and the process continues for several iterations.
Notice that only one classifier is required, with no split of
features.
Figure 1 illustrates the general bootstrapping process.
Starting with a set of labeled and unlabeled data, the boot-
strapping algorithm aims to improve the classification
performance, by integrating examples from the unlabeled
data into the labeled data set. At each iteration, the class
distribution in the labeled data is maintained, by keeping
a constant ratio across classes between already labeled
examples and newly added examples; the role of this step
is to avoid introducing imbalance in the training data
set. For co-training, the algorithm requires two different
views (two different classifiers   and   ) that interact
in the bootstrapping process. By limiting the number of
views to one (one general classifier   ), co-training is
transformed into a self-training process, where one single
classifier learns from its own output.
0. Given:
- A set L of labeled training examples
- A set U of unlabeled examples
- Classifiers 
1. Create a pool U? of examples by choosing P random ex-
amples from U
2. Loop for I iterations:
2.1 Use L to individually train the classifiers   , and la-
bel the examples in U?
2.2 For each classifier  select G most confidently ex-
amples and add them to L, while maintaining the
class distribution in L
2.3 Refill U? with examples from U, to keep U? at a con-
stant size of P examples
Figure 1: General bootstrapping process using labeled and
unlabeled data
Co-training and self-training parameters
Three different parameters can be set in the bootstrapping
process, and usually the performance achieved through
bootstrapping depends on the value chosen for these pa-
rameters.
 Iterations (I) Number of iterations.
 Pool size (P) Number of examples selected from the
unlabeled set U for annotation at each iteration.
 Growth size (G) Number of most confidently labeled
examples that are added at each iteration to the set of
labeled data L.
As previously noticed (Ng and Cardie, 2003), there
is no principled method for selecting optimal values for
these parameters, which is an important disadvantage of
these algorithms. In the following, we show that there is
a big gap between the performance achieved for some op-
timal parameter settings, selected through measurements
performed on the test data, and the performance level
when these parameters are set empirically, suggesting that
more research is required to narrow this gap, and make
these bootstrapping algorithms useful for practical appli-
cations.
First, we describe the general framework of supervised
word sense disambiguation, and introduce several basic
sense classifiers that are used in co-training and self-
training experiments. Next, through several experiments,
(1) we determine the optimal parameter settings for co-
training and self-training, and (2) explore various algo-
rithms for empirical selection of these three parameters
for best performance.
3 Supervised Word Sense Disambiguation
Supervised word sense disambiguation systems work un-
der the assumption that several annotated examples are
available for a target ambiguous word. These examples
are used to build a classifier that automatically learns clues
useful for the disambiguation of the given polysemous
word, and then applies these clues to the classification of
new unlabeled instances.
First, the examples are pre-processed and annotated
with morphological or syntactic tags. Next, each sense-
tagged example is transformed into a feature vector, suit-
able for an automatic learning process. There are two
main decisions that one takes in the construction of such
a classifier: (1) What features to extract from the exam-
ples provided, to best model the behavior of the given am-
biguous word; (2) What learning algorithm to use for best
performance.
3.1 Preprocessing
During preprocessing, SGML tags are eliminated, the text
is tokenized and annotated with parts of speech. Collo-
cations are identified using a sliding window approach,
where a collocation is considered to be a sequence of
words that forms a compound concept defined in Word-
Net. During this process, all collocations that include the
target word are identified, and the examples that use a
collocation are removed from the training/test data. For
instance, examples referring to short circuit are removed
from the data set for circuit, so that a separate learning
process is performed for each lexical unit.
Feat. Description
CW (L) The word   itself
CP (L) The part of speech of the word  
CF (L) Word forms and their part of speech for a window of K words
surrounding  
COL (L) Collocations formed with maximum K words surrounding  
HNP (L) The head of the noun phrase to which   belongs, if any
SK (T) Maximum of M keywords occurring at least N times are
determined for each sense of the ambiguous word. The value
of this feature is either 0 or 1, depending if the current
example contains one of the determined keywords or not.
B (T) Maximum of M bigrams occurring at least N times are deter-
mined for all training examples. The value of this feature is
either 0 or 1, depending if the current example contains one
of the determined bigrams or not. Bigrams are ordered using the
Dice coefficient
VB (L) The first verb before   .
VA (L) The first verb after   .
NB (L) The first noun before   .
NA (L) The first noun after   .
VO (L) Verb-object relation involving  
SV (L) Subject-verb relation involving  
Table 1: Commonly used features for word sense disam-
biguation.

denotes the current (ambiguous) word.
Feature type is indicated as local (L) or topical (T).
3.2 Features that are good indicators of word sense
Previous work on word sense disambiguation has ac-
knowledged several local and topical features as good in-
dicators of word sense. These include surrounding words
and their part of speech tags, collocations, keywords in
contexts. More recently, other possible features have been
investigated: bigrams, named entities, syntactic features,
semantic relations with other words in context. Table 1
lists commonly used features in word sense disambigua-
tion (list drawn from a larger set of features compiled by
(Mihalcea, 2002)).
3.3 Supervised learning for word sense
disambiguation
Related work in supervised word sense disambigua-
tions includes experiments with a variety of learning
algorithms, with varying degrees of success, including
Bayesian learning, decision trees, decision lists, memory
based learning, and others. (Yarowsky and Florian, 2002)
give a comprehensive examination of learning methods
and their combination.
3.4 Basic Classifiers for Word Sense Disambiguation
Several basic word sense disambiguation classifiers can
be implemented using feature combinations from Table 1,
and feature vectors can be plugged into any learning algo-
rithm. We use Naive Bayes, since it was previously shown
that in combination with the features we consider, can lead
to a state-of-the-art disambiguation system (Lee and Ng,
2002). Moreover, Naive Bayes is particularly suitable for
co-training and self-training, since it provides confidence
scores and is efficient in terms of training and testing time.
The two separate views required for co-training are de-
fined using a local versus topical feature split. For self-
training, a global classifier with no feature split is defined.
A local classifier
A local classifier was implemented using all local features
listed in Table 1.
A topical classifier
The topical classifier relies on features extracted from a
large context, in particular keywords specific to each indi-
vidual sense. We use the SK feature, and extract at most
ten keywords for each word sense, each occurring for at
least three times in the annotated corpus.
A global classifier
Finally, the global classifier integrates all local and topical
features, also in a Naive Bayes classifier. This classifier
is basically a combination of the previous two local and
topical classifiers.
4 Co-training and Self-training for Word
Sense Disambiguation
We investigate the application of co-training and self-
training to the problem of supervised word sense disam-
biguation, and explore methods for selecting values for
the bootstrapping parameters.
The data set used in this study consists in training and
test data made available during the English lexical sample
task in the SENSEVAL-2 evaluation exercise. In addition
to these data sets, a large raw corpus of unlabeled exam-
ples is constructed for each word, with text snippets con-
sisting of three consecutive sentences extracted from the
British National Corpus. Given the large number of runs
performed for each word, the experiments focus on nouns
only. Similar observations are however expected to hold
for other parts of speech.
For co-training, we use the local and topical classifiers
described in Section 3.4, which represent two different
views for this problem, generated by a ?local versus top-
ical? feature split. Self-training requires only one basic
classifier, and we use a global classifier, which combines
the features from both local and topical views for a com-
plete global ?view?.
Unlike previous applications of co-training and self-
training to natural language learning, where one general
classifier is build to cover the entire problem space, su-
pervised word sense disambiguation implies a different
classifier for each individual word, resulting eventually
in thousands of different classifiers, each with its own
characteristics (learning rate, sensitivity to new examples,
etc.). Given this heterogeneous space of classifiers, our
hypothesis is that co-training and self-training will them-
selves have a heterogeneous behavior, and therefore best
co-training and self-training parameters are different for
each classifier.
To explore this hypothesis, a range of experiments is
performed. First, for all the words in the experimental
data set, an optimal parameter setting is determined. This
can be considered as an upper bound for improvements
achieved with co-training and self-training, since the se-
lection of parameters is performed through measurements
that are collected directly on test data. Second, we explore
several algorithms to select the bootstrapping parameters,
independent of the test set: (1) Best overall parameter set-
ting; (2) Best individual parameter settings; (3) Best per-
word parameter selection; (4) A new method consisting in
an improved bootstrapping scheme using majority voting
across several bootstrapping iterations.
4.1 Optimal settings
Optimal parameter settings are determined through mea-
surements performed directly on the test set. For the
growth size G, a value is chosen from the set:   1, 10, 20,
30, 40, 50, 100, 150, 200  . The pool size P takes one of
these values:   1, 100, 500, 1000, 1500, 2000, 5000  . For
each setting, 40 iterations are performed. This results in
an average of 2,120 classification runs per word. At each
run, a pool of P raw examples is annotated, and G most
confidently labeled examples are added to the training set
from the previous iteration. The performance of the clas-
sifier using the augmented training set is evaluated on the
test data, and the precision is recorded.
Separate experiments are performed for both co-
training and self-training, for all the nouns in the
SENSEVAL-2 data set (for a total of about 120,000 runs).
For each word, the parameter setting (growth size G / pool
size P / iterations I) leading to the highest improvement is
determined. Table 21 lists, for each word: size of train-
ing, test, raw data2; precision of the basic classifier (the
global classifier is used as a baseline); maximum preci-
sion obtained with co-training and self-training, and the
parameters for which this maximum is achieved. When
several parameter settings lead to the same performance,
the first setting is recorded.
Discussion
Surprisingly, under optimal settings, both co-training and
self-training perform about the same, leading to an aver-
age error reduction of 25.5%. Self-training leads to the
highest precision for nine words, while co-training is win-
ning for eight words; there is a tie with equal performance
for both co-training and self-training for the remaining
twelve words.
There are three words (chair, holiday, spade) for which
no improvement could be obtained with either co-training
or self-training, and therefore no optimal setting is indi-
cated. These are among the four words with the best per-
forming basic classifier (baseline higher than 75%). The
fact that no improvement was obtained agrees with previ-
ous observations that classifiers that are too accurate can-
not be improved with bootstrapping (Pierce and Cardie,
2001). Note that even very weak classifiers, with preci-
sions below 40%, can still be improved, sometimes with
as much as 45% error reduction (e.g. the classifier for
feeling).
There are no clear commonalities between the param-
eters leading to maximum precision for different classi-
fiers. Some classifiers benefit more from an ?aggressive?
augmentation of the training data with new examples ?
for instance the self-trained classifier for nature achieves
its highest peak for a growth size of 200 from a pool of
500 examples. Others instead work better by taking ?short
steps? ? for instance self-training for the word dyke works
better for a growth size of 1 from a pool of 1.
1The numbers listed in Table 2 for training/test data size and
basic classifier precision refer to data sets obtained after remov-
ing examples with collocations that include the target word. This
explains why the numbers do not always match figures previ-
ously reported in SENSEVAL-2 literature. If collocations are
added back to the data sets, the precision of the basic classifier
is measured at 60.2% ? comparable to figures obtained by other
systems participating in SENSEVAL-2
2The raw corpus for each word is formed with all examples
retrieved from the British National Corpus. While this ensures
a natural distribution for each word (in terms of number of ex-
amples occurring in a balanced corpus), it also leads to discrep-
ancies in terms of raw data size. For words with less than 5000
raw examples, the pool size recorded in the ?optimal setting??
column represents a round-up to the nearest number from the set
of allowed pool values.
Size Basic Self-training Co-training
Word train test raw classifier Max.prec. Optimal setting Max.prec. Optimal setting
art 123 52 8012 48.07% 59.61% 100/1500/5 59.61% 200/1000/20
authority 157 80 11034 50.00% 58.75% 50/1500/3 62.50% 20/1000/3
bar 205 124 5526 31.45% 35.48% 10/1000/3 34.67% 1/1500/20
bum 79 43 361 37.20% 58.13% 100/100/13 46.51% 1/1/26
chair 121 63 5889 80.95% 80.95% - 80.95% -
channel 78 44 1744 43.18% 45.45% 1/1/1 47.72% 1/2000/1
child 117 60 14192 63.33% 68.33% 1/500/3 68.33% 1/100/30
church 81 36 7775 52.77% 72.22% 1/500/2 69.44% 1/500/2
circuit 108 57 1891 40.35% 52.63% 30/2000/10 47.36% 1/100/8
day 245 123 50883 45.52% 60.16% 100/5000/20 62.60% 50/5000/2
detention 46 24 638 79.16% 91.66% 30/500/22 91.66% 100/500/10
dyke 52 26 116 38.61% 42.30% 1/1/23 50.00% 50/500/1
facility 110 55 1959 67.27% 78.18% 10/100/3 78.18% 1/1500/11
fatigue 69 42 437 73.80% 76.19% 10/500/1 76.19% 10/500/1
feeling 100 51 11214 39.21% 66.66% 1/2000/6 60.78% 1/2000/11
grip 72 39 1718 53.46% 64.10% 50/500/12 66.66% 150/500/1
hearth 60 29 334 44.82% 55.17% 1/100/21 65.51% 50/500/3
holiday 55 26 5604 84.61% 84.61% - 84.61% -
lady 75 39 4677 61.53% 84.61% 20/100/39 82.05% 1/1000/3
material 120 59 10663 37.28% 59.32% 50/5000/6 59.32% 100/5000/24
mouth 109 56 8044 50.00% 62.5% 150/1000/3 64.28% 150/1000/3
nation 60 26 4073 65.38% 76.92% 40/1000/21 76.92% 30/1500/14
nature 70 38 14218 39.47% 57.89% 200/500/4 57.89% 30/1000/3
post 105 58 10611 37.93% 48.27% 20/1000/3 48.27% 20/500/1
restraint 87 43 881 60.46% 67.44% 1/500/5 72.09% 10/500/1
sense 83 36 19048 50.00% 63.88% 1/100/37 61.11% 1/100/3
spade 48 28 235 78.57% 78.57% - 78.57% -
stress 77 38 3549 36.84% 52.63% 1/500/7 57.89% 20/1500/4
yew 50 20 167 70.00% 100.00% 20/500/2 95.00% 1/100/11
AVERAGE 95 48 7085 53.84% 65.61% - 65.75% -
Table 2: Size of training, test, and raw data, precision of basic classifiers, and maximum precision obtained for optimal
parameter settings for self-training and co-training. The optimal settings column lists the values for the three parameters
(growth size G / pool size P / iteration I) for which the maximum precision was observed.
The improvements obtained under these optimal set-
tings can be considered as an upper bound for self-training
and co-training. Under some ideal conditions, where the
optimal parameters can be identified, this is the highest
improvement that can be achieved for the given labeled
set. However, most of the times, it may not be possible
to find these optimal parameter values. In the following,
we explore empirical solutions for finding values for these
parameters, independent of the test data.
4.2 Empirical Settings
The methods described in this section make use of the data
collected during self-training and co-training runs, for dif-
ferent parameter settings. Evaluations are performed on a
validation set consisting of about 20% of the training data
? which was set apart for this purpose. For each run, infor-
mation is collected about: initial size of labeled data set,
growth size, pool size, iteration number, precision of basic
classifier, precision of boosted classifier. With the range
of values for growth size, pool size, and number of itera-
tions specified in Section 4.1, about 60,000 such records
are collected for both co-training and self-training.
4.2.1 Best overall parameter setting
One simple method to select a parameter setting is to
determine a global setting that leads to the highest overall
boost in precision. Starting with the information collected
for the 60,000 runs, for each possible parameter setting,
the total relative growth in performance is determined, by
adding up the relative improvements for all the runs for
that particular setting. For co-training, the best global set-
ting identified in this way is growth size of 50, pool size
of 5000, iteration 2. For self-training, the best setting is
growth size of 1, pool size of 1500, iteration 2. The pre-
cision obtained for these settings is listed in Table 3 un-
der the global settings column. On average, using this
scheme for parameter selection, co-training brings 4% er-
ror reduction, while self-training has only a small error
reduction of 1%.
In a similar approach, the value for each parameter is
determined independent of the other parameters. Instead
of selecting the best value for all parameters at once, val-
ues are selected individually. Again, for each possible pa-
rameter value, the total relative growth in performance is
determined, and the value leading to the highest growth
is selected. Interestingly, for both co-training and self-
training, the best values identified in this way for the three
Basic Global setting Per-word setting
Word classifier co-train self-train co-train setting self-train setting
art 48.07% 53.85% 50.00% 44.23% 50/1500/32 34.67% 10/100/1
authority 50.00% 51.25% 50.00% 57.50% 150/2000/2 37.50% 30/2000/3
bar 31.45% 31.45% 31.45% 29.83% 10/1000/5 24.19% 150/1000/17
bum 37.20% 39.53% 41.86% 46.51% 1/1/40 46.51% 1/1/34
chair 80.95% 77.78% 77.78% 77.77% 1/1500/16 76.19% 30/100/1
channel 43.18% 34.09% 43.18% 43.18% 1/2000/2 43.18% 1/2000/3
child 63.33% 50.00% 60.00% 55.00% 10/1500/1 53.33% 50/1500/17
church 52.77% 55.56% 58.33% 47.22% 1/100/23 55.55% 1/100/10
circuit 40.35% 45.61% 40.35% 31.57% 100/100/3 42.10% 1/1000/18
day 45.52% 52.03% 39.84% 50.43% 150/1500/3 49.59% 10/200/40
detention 79.16% 83.33% 79.17% 79.16% - 79.16% -
dyke 38.61% 50.00% 38.46% 34.61% 1/2000/19 34.61% 1/2000/11
facility 67.27% 69.09% 72.73% 58.18% 150/1500/13 70.9% 100/100/7
fatigue 73.80% 71.43% 71.43% 71.43% 1/1000/1 71.43% 1/500/9
feeling 39.21% 39.21% 37.25% 50.98% 10/500/5 33.33% 1/500/25
grip 53.46% 43.59% 51.28% 41.02% 20/100/20 58.97% 40/100/40
hearth 44.82% 48.28% 44.82% 41.37% 1/1000/2 44.82% 1/2000/1
holiday 84.61% 84.61% 84.61% 84.61% - 84.61% -
lady 61.53% 74.36% 58.97% 33.07% 150/2000/10 69.23% 100/500/40
material 37.28% 45.76% 40.68% 45.76% 100/2000/6 30.50% 10/2000/9
mouth 50.00% 51.79% 50.00% 53.57% 1/1/40 50.00% 20/2000/17
nation 65.38% 69.23% 57.69% 69.23% 100/2000/23 61.53% 150/2000/16
nature 39.47% 50.00% 42.11% 44.73% 10/100/3 50.00% 200/2000/34
post 37.93% 44.82% 39.66% 44.82% 1/500/28 36.20% 10/100/2
restraint 60.46% 58.14% 58.14% 53.48% 1/1000/2 62.79% 1/1000/15
sense 50.00% 47.22% 50.00% 25.00% 150/500/40 47.22% 1/2000/12
spade 78.57% 75.00% 78.57% 78.57% - 78.57% -
stress 36.84% 52.63% 47.37% 47.36% 1/1500/7 36.84% 10/1000/1
yew 70.00% 65.00% 75.00% 70.00% - 70.00% -
AVERAGE 53.84% 55.67% 54.16% 51.73% - 52.88% -
Table 3: Precision obtained with co-training and self-training, for global and per-word parameter selection.
parameters are growth size of 1, pool size of 1, iteration
1 (i.e. the best classifier is the one ?closest? to the basic
classifier). The average results are however worse than
the baseline ? only 53.49% for co-training, and 53.67%
for self-training.
4.2.2 Best per-word parameter setting
In a second experiment, best parameter settings are
identified separately for each word. The setting yielding
to maximum precision on the validation set is selected as
the best setting for a given word, and evaluated on the test
data. If multiple settings are identified as leading to max-
imum precision, settings are prioritized based on (in this
order): smallest growth size; largest pool size; number of
iterations. Results for both co-training and self-training
are listed in Table 3 under the per-word settings column,
together with the setting identified as optimal on the val-
idation set. There are several words for which significant
improvement is observed over the baseline. However, on
the average, the performance of the boosted classifiers is
worse than the baseline.
4.2.3 Smoothed co-training and self-training with
majority voting
There is a common trend observed for learning curves
for co-training or self-training, consisting in an increase
in performance followed by a decline. Different classi-
fiers exhibit however a different point of raise or decline
in precision, depending on the number of iterations. For
instance, the classifier for circuit achieves its highest peak
at iteration 10 (see Table 2), while the classifier for nation
has the highest boost at iteration 21 ? where the perfor-
mance for circuit is already below the baseline. Given this
heterogeneous behavior, it is difficult to identify a point of
maximum for each classifier, or at least a point where the
performance is not below the baseline. Ideally, we would
like the learning curves to have a more stable behavior ?
without sharp raises or drops in precision, and with larger
intervals with constant performance, so that the chance of
selecting a good number of iterations for each classifier is
increased.
We introduce a new bootstrapping scheme that com-
bines co-training or self-training with majority voting.
During the bootstrapping process, the classifier at each it-
eration is replaced with a majority voting scheme applied
to all classifiers constructed at previous iterations. This
change has the effect of ?smoothing? the learning curves:
it slows down the learning rate, but also yields a larger
interval with constant high performance3.
3Notice that in smoothed co-training, majority voting is ap-
plied on classifiers consisting of iterations of the co-training pro-
cess itself, and therefore voting is applied on bootstrapped clas-
 35
 40
 45
 50
 55
 60
 65
 0  5  10  15  20  25  30  35  40
Pr
ec
is
io
n 
(%
)
Number of iterations
Learning curves for simple and smoothed co-training
simple co-training
smoothed co-training
baseline
Figure 2: Learning curves for the classifier for the noun
authority: baseline, simple co-training, and co-training
smoothed with majority voting.
To some extent, smoothed co-training is related to
boosting, since both algorithms rely on a growing ensem-
ble of classifiers trained on resamples of the data. How-
ever, boosting assumes labeled data and is error-driven,
whereas smoothed co-training combines both labeled and
unlabeled data and is confidence-driven4.
Figure 2 shows the learning curves for simple co-
training, and co-training ?smoothed? with majority vot-
ing, for the word authority (for a growth size of 1 and
pool size of 1). Notice that the trend for the smoothed
curve is still the same ? a raise, followed by a decline ? but
at a significantly lower pace. With smoothed co-training,
any number of iterations selected in the interval 5-40 still
leads to significant improvement over the baseline, unlike
the simple unsmoothed curve, where only iterations in the
range 3-10 bring improvement over the baseline (followed
by two other iterations at random intervals).
The methods for global parameter settings and per-
word parameter settings are evaluated again, this time us-
ing smoothed co-training or self-training. Table 4 lists
the results obtained with basic and smoothed co-training
for the same global/per-word setting. Since the major-
ity voting scheme requires an odd number of classifiers,
sifiers across co-training iterations, with the effect of improv-
ing the performance of basic co-training. This is fundamen-
tally different from the approach proposed in (Ng and Cardie,
2003), where they also apply majority voting in a bootstrapping
framework, but in a different setting. They use a majority voting
scheme applied to classifiers build on subsets of the labeled data
(bagging) to induce several views for the co-training process. In
their approach, majority voting is used at each co-training itera-
tion to enable co-training by predicting labels on unlabeled data.
4Thanks to one of the anonymous reviewers for suggesting
this analogy.
Global setting Per-word setting
Basic Co-training Co-training
Word classifier basic smoothed basic smoothed
art 48.07% 53.85% 53.85% 44.23% 53.85%
authority 50.00% 51.25% 57.50% 57.50% 58.75%
bar 31.45% 31.45% 31.45% 29.83% 35.48%
bum 37.20% 39.53% 44.18% 46.51% 44.18%
chair 80.95% 77.78% 79.36% 77.78% 80.95%
channel 43.18% 34.09% 43.18% 43.18% 45.45%
child 63.33% 50.00% 51.66% 55.00% 65.00%
church 52.77% 55.56% 58.33% 47.22% 58.33%
circuit 40.35% 45.61% 49.12% 31.57% 42.10%
day 45.52% 52.03% 53.65% 50.43% 55.28%
detention 79.16% 83.33% 83.33% 79.16% 79.16%
dyke 38.61% 50.00% 46.15% 34.61% 38.46%
facility 67.27% 69.09% 69.09% 58.18% 58.18%
fatigue 73.80% 71.43% 71.43% 71.43% 71.43%
feeling 39.21% 39.22% 50.98% 50.98% 35.29%
grip 53.46% 43.59% 48.71% 41.02% 60.00%
hearth 44.82% 48.28% 44.82% 41.37% 44.82%
holiday 84.61% 84.61% 84.61% 84.61% 84.61%
lady 61.53% 74.36% 76.92% 33.07% 66.66%
material 37.28% 45.76% 42.37% 45.76% 49.15%
mouth 50.00% 51.79% 57.14% 53.57% 50.00%
nation 65.38% 69.23% 69.23% 69.23% 73.07%
nature 39.47% 50.00% 47.36% 44.73% 47.36%
post 37.93% 44.83% 48.27% 44.82% 41.37%
restraint 60.46% 58.14% 60.46% 53.48% 60.46%
sense 50.00% 47.22% 58.34% 25.00% 33.33%
spade 78.57% 75.00% 78.57% 78.57% 78.57%
stress 36.84% 52.63% 55.26% 47.36% 52.63%
yew 70.00% 65.00% 75.00% 70.00% 70.00%
AVERAGE 53.84% 55.67% 58.35% 51.73% 56.68%
Table 4: Basic and smoothed co-training, with global and
per-word parameter settings (same settings as listed in Ta-
ble 3)
the number of iterations is rounded up to the next even
number (the first iteration is iteration 0, representing the
basic classifier, which is also considered during voting).
The same type of experiments were also performed for
self-training, but the majority voting scheme did not bring
any significant improvements. We believe that the learn-
ing curves for self-training are less steep, and therefore
majority voting applied to classifiers across various itera-
tions does not have the same strong smoothing effect as
with co-training.
Discussion
For parameter selection using global settings (Table 3) co-
training improves over the basic classifiers, and outper-
forms self-training. As previously noticed (Nigam and
Ghani, 2000), it is hard to identify conditionally inde-
pendent views for real-data problems. Even though we
use a ?local versus topical? feature split, which divides
the features into two separate views on sense classifica-
tion, there might be some natural dependencies between
the features, since they are extracted from the same con-
text, which may weaken the independence condition, and
may sometime make the behavior of co-training similar to
a self-training process. However, as theoretically shown
in (Abney, 2002), and then empirically in (Clark et al,
2003), co-training still works under a weaker indepen-
dence assumption, and the results we obtain concur with
these previous observations.
Despite the fact that parameters observed for optimal
settings (Table 2) are different for each classifier, in em-
pirical settings, one unique set of parameters for all clas-
sifiers seems to perform better than an individual set of
parameters customized to each word. The bootstrapping
scheme is improved even more when coupled with ma-
jority voting across various iterations. Overall, the high-
est error reduction is achieved with smoothed co-training
using global parameter settings, where an average error
reduction of 9.8% is observed with respect to the basic
classifier.
A comparative analysis of words that benefit from ba-
sic/smoothed co-training with global parameter settings,
versus words with little or no improvement obtained
through bootstrapping reveals several observations:
(1) Words with accurate basic classifiers cannot be im-
proved through co-training, which agrees with previous
observations (Pierce and Cardie, 2001). For instance, no
improvement was obtained for chair, holiday, or spade,
which have the basic classifier performing above 75%.
(2) Words with high number of senses (e.g. bar ? 10
senses, channel ? 7 senses, grip ? 11 senses) achieve min-
imal improvements through co-training. This is probably
explained by the fact that the classifiers are misled by the
large number of classes (senses), and a large number of
errors is introduced since the early stages of co-training.
(3) Words that have a large number of senses not belong-
ing to well-defined topical domains show little or no ben-
efit from a bootstrapping procedure. Using the domains
attached to word senses, as introduced in (Magnini et al,
2002), we observed that words that have a large subset of
their senses not belonging to a specific domain (e.g. re-
straint, facility) achieve little or no improvement through
co-training, which is perhaps explained again by the noisy
automatic annotation that introduces errors since the early
iterations of co-training.
Even though not all words show benefit from co-
training, smoothed co-training with global parameter set-
tings does bring an overall error reduction of 9.8% with
respect to the basic classifier, which proves that bootstrap-
ping through co-training is a potentially useful technique
for word sense disambiguation.
5 Conclusion
This paper investigated the application of co-training and
self-training to supervised word sense disambiguation. If
the right parameters for co-training and self-training can
be identified for each individual classifier, an average error
reduction of 25.5% is achieved, with similar performance
observed for both co-training and self-training. Given
that these optimal settings cannot always be identified in
practical applications, several algorithms for empirical pa-
rameter selection were investigated: global settings de-
termined as the best set of parameters across all classi-
fiers, and per-word settings, identified separately for each
classifier, both using a validation set. An improved co-
training method was also introduced, that combines co-
training with majority voting, with the effect of smooth-
ing the learning curves, and improving the average perfor-
mance. This improved co-training algorithm, applied with
a global parameter selection scheme, brought a significant
error reduction of 9.8% with respect to the basic classi-
fier, which shows that co-training can be successfully em-
ployed in practice for bootstrapping sense classifiers.
Acknowledgments
Many thanks to Carlo Strapparava and the three anony-
mous reviewers for useful comments and suggestions.
This work was partially supported by a National Science
Foundation grant IIS-0336793.
References
S. Abney. 2002. Bootstrapping. In Proceedings of the 40st Annual Meeting
of the Association for Computational Linguistics ACL 2002, pages 360?367,
Philadelphia, PA, July.
A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-
training. In COLT: Proceedings of the Workshop on Computational Learning
Theory, Morgan Kaufmann Publishers.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrapping POS taggers using
unlabelled data. In Walter Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 49?55. Edmonton, Canada.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of knowledge sources and
learning algorithms for word sense disambiguation. In Proceedings of the 2002
Conference on Empirical Methods in Natural Language Processing (EMNLP
2002), pages 41?48, Philadelphia, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo. 2002. Using domain in-
formation for word sense disambiguation. In Proceedings of Senseval-2 Work-
shop, Association of Computational Linguistics, pages 111?115, Toulouse,
France.
R. Mihalcea. 2002. Instance based learning with automatic feature selection ap-
plied to Word Sense Disambiguation. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING-ACL 2002), Taipei, Tai-
wan, August.
V. Ng and C. Cardie. 2003. Weakly supervised natural language learning without
redundant views. In Human Language Technology/Conference of the North
American Chapter of the Association for Computational Linguistics (HLT-
NAACL), Edmonton, Canada, May.
K. Nigam and R. Ghani. 2000. Analyzing the effectiveness and applicability of
co-training. In Proceedings of the Conference on Information and Knowledge
Management CIKM-2000, pages 86?93, McLean, Virginia.
D. Pierce and C. Cardie. 2001. Limitations of co-training for natural language
learning from large datasets. In Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP-2001), Pittsburgh,
PA.
A. Sarkar. 2001. Applying cotraining methods to statistical parsing. In Proceed-
ings of the North American Chapter of the Association for Compuatational
Linguistics, NAACL 2001, Pittsburg, June.
D. Yarowsky and R. Florian. 2002. Evaluating sense disambiguation across di-
verse parameter spaces. JNLE Special Issue on Evaluating Word Sense Disam-
biguation Systems. forthcoming.
TextRank: Bringing Order into Texts
Rada Mihalcea and Paul Tarau
Department of Computer Science
University of North Texas
 
rada,tarau  @cs.unt.edu
Abstract
In this paper, we introduce TextRank ? a graph-based
ranking model for text processing, and show how this
model can be successfully used in natural language
applications. In particular, we propose two innova-
tive unsupervised methods for keyword and sentence
extraction, and show that the results obtained com-
pare favorably with previously published results on
established benchmarks.
1 Introduction
Graph-based ranking algorithms like Kleinberg?s
HITS algorithm (Kleinberg, 1999) or Google?s
PageRank (Brin and Page, 1998) have been success-
fully used in citation analysis, social networks, and
the analysis of the link-structure of the World Wide
Web. Arguably, these algorithms can be singled out
as key elements of the paradigm-shift triggered in
the field of Web search technology, by providing a
Web page ranking mechanism that relies on the col-
lective knowledge of Web architects rather than in-
dividual content analysis of Web pages. In short, a
graph-based ranking algorithm is a way of deciding
on the importance of a vertex within a graph, by tak-
ing into account global information recursively com-
puted from the entire graph, rather than relying only
on local vertex-specific information.
Applying a similar line of thinking to lexical
or semantic graphs extracted from natural language
documents, results in a graph-based ranking model
that can be applied to a variety of natural language
processing applications, where knowledge drawn
from an entire text is used in making local rank-
ing/selection decisions. Such text-oriented ranking
methods can be applied to tasks ranging from auto-
mated extraction of keyphrases, to extractive summa-
rization and word sense disambiguation (Mihalcea et
al., 2004).
In this paper, we introduce the TextRank graph-
based ranking model for graphs extracted from nat-
ural language texts. We investigate and evaluate the
application of TextRank to two language processing
tasks consisting of unsupervised keyword and sen-
tence extraction, and show that the results obtained
with TextRank are competitive with state-of-the-art
systems developed in these areas.
2 The TextRank Model
Graph-based ranking algorithms are essentially a
way of deciding the importance of a vertex within
a graph, based on global information recursively
drawn from the entire graph. The basic idea im-
plemented by a graph-based ranking model is that
of ?voting? or ?recommendation?. When one ver-
tex links to another one, it is basically casting a vote
for that other vertex. The higher the number of votes
that are cast for a vertex, the higher the importance
of the vertex. Moreover, the importance of the vertex
casting the vote determines how important the vote
itself is, and this information is also taken into ac-
count by the ranking model. Hence, the score asso-
ciated with a vertex is determined based on the votes
that are cast for it, and the score of the vertices cast-
ing these votes.
Formally, let 
	 be a directed graph with
the set of vertices  and set of edges  , where  is a
subset of  . For a given vertex  , let  be
the set of vertices that point to it (predecessors), and
let ffProceedings of the ACL Workshop on Building and Using Parallel Texts, pages 65?74,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Word Alignment for Languages with Scarce Resources
Joel Martin
National Research Council
Ottawa, ON, K1A 0R6
Joel.Martin@cnrc-nrc.gc.ca
Rada Mihalcea
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Ted Pedersen
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
This paper presents the task definition,
resources, participating systems, and
comparative results for the shared task
on word alignment, which was organized
as part of the ACL 2005 Workshop on
Building and Using Parallel Texts. The
shared task included English?Inuktitut,
Romanian?English, and English?Hindi
sub-tasks, and drew the participation of ten
teams from around the world with a total of
50 systems.
1 Defining a Word Alignment Shared Task
The task of word alignment consists of finding cor-
respondences between words and phrases in parallel
texts. Assuming a sentence aligned bilingual corpus
in languages L1 and L2, the task of a word alignment
system is to indicate which word token in the corpus
of language L1 corresponds to which word token in
the corpus of language L2.
This year?s shared task follows on the success of
the previous word alignment evaluation that was or-
ganized during the HLT/NAACL 2003 workshop on
?Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond? (Mihalcea and Ped-
ersen, 2003). However, the current edition is dis-
tinct in that it has a focus on languages with scarce
resources. Participating teams were provided with
training and test data for three language pairs, ac-
counting for different levels of data scarceness: (1)
English?Inuktitut (2 million words training data),
(2) Romanian?English (1 million words), and (3)
English?Hindi (60,000 words).
Similar to the previous word alignment evaluation
and with the Machine Translation evaluation exercises
organized by NIST, two different subtasks were de-
fined: (1) Limited resources, where systems were al-
lowed to use only the resources provided. (2) Un-
limited resources, where systems were allowed to use
any resources in addition to those provided. Such re-
sources had to be explicitly mentioned in the system
description.
Test data were released one week prior to the dead-
line for result submissions. Participating teams were
asked to produce word alignments, following a com-
mon format as specified below, and submit their out-
put by a certain deadline. Results were returned to
each team within three days of submission.
1.1 Word Alignment Output Format
The word alignment result files had to include one line
for each word-to-word alignment. Additionally, they
had to follow the format specified in Figure 1. Note
that the
  
and confidence fields overlap in their
meaning. The intent of having both fields available
was to enable participating teams to draw their own
line on what they considered to be a Sure or Probable
alignment. Both these fields were optional, with some
standard values assigned by default.
1.1.1 A Running Word Alignment Example
Consider the following two aligned sentences:
[English]  s snum=18  They had gone .  /s 
[French]  s snum=18  Ils e?taient alle?s .  /s 
A correct word alignment for this sentence is:
18 1 1
18 2 2
18 3 3
18 4 4
65
sentence no position L1 position L2 [    ] [confidence]
where:
sentence no represents the id of the sentence within the
test file. Sentences in the test data already have an id as-
signed. (see the examples below)
position L1 represents the position of the token that is
aligned from the text in language L1; the first token in each
sentence is token 1. (not 0)
position L2 represents the position of the token that is
aligned from the text in language L2; again, the first token
is token 1.
S P can be either S or P, representing a Sure or Probable
alignment. All alignments that are tagged as S are also con-
sidered to be part of the P alignments set (that is, all align-
ments that are considered ?Sure? alignments are also part of
the ?Probable? alignments set). If the    field is missing, a
value of S will be assumed by default.
confidence is a real number, in the range (0-1] (1 meaning
highly confident, 0 meaning not confident); this field is op-
tional, and by default confidence number of 1 was assumed.
Figure 1: Word Alignment file format
stating that: all the word alignments pertain to sen-
tence 18, the English token 1 They aligns with the
French token 1 Ils, the English token 2 had aligns with
the French token 2 e?taient, and so on. Note that punc-
tuation is also aligned (English token 4 aligned with
French token 4), and counts toward the final evalua-
tion figures.
Alternatively, systems could also provide an
  
marker and/or a confidence score, as shown in the fol-
lowing example:
18 1 1 1
18 2 2 P 0.7
18 3 3 S
18 4 4 S 1
with missing
   
fields considered by default S, and
missing confidence scores considered by default 1.
1.2 Annotation Guide for Word Alignments
The word alignment annotation guidelines are similar
to those used in the 2003 evaluation.
1. All items separated by a white space are consid-
ered to be a word (or token), and therefore have
to be aligned (punctuation included).
2. Omissions in translation use the NULL token,
i.e. token with id 0.
3. Phrasal correspondences produce multiple word-
to-word alignments.
2 Resources
The shared task included three different language
pairs, accounting for different language and data
characteristics. Specifically, the three subtasks ad-
dressed the alignment of words in English?Inuktitut,
Romanian?English, and English?Hindi parallel texts.
For each language pair, training data were provided to
participants. Systems relying only on these resources
were considered part of the Limited Resources sub-
task. Systems making use of any additional resources
(e.g. bilingual dictionaries, additional parallel cor-
pora, and others) were classified under the Unlimited
Resources category.
2.1 Training Data
Three sets of training data were made available. All
data sets were sentence-aligned, and pre-processed
(i.e. tokenized and lower-cased), with identical pre-
processing procedures used for training, trial, and test
data.
English?Inuktitut. A collection of sentence-
aligned English?Inuktitut parallel texts from the
Legislative Assembly of Nunavut (Martin et al,
2003). This collection consists of approximately
2 million Inuktitut tokens (1.6 million words) and
4 million English tokens (3.4 million words). The
Inuktitut data was originally encoded in Unicode
representing a syllabics orthography (qaniujaaqpait),
but was transliterated to an ASCII encoding of the
standardized roman orthography (qaliujaaqpait) for
this evaluation.
Romanian?English. A set of Romanian?English
parallel texts, consisting of about 1 million Romanian
words, and about the same number of English words.
This is the same training data set as used in the 2003
word alignment evaluation (Mihalcea and Pedersen,
2003). The data consists of:
 Parallel texts collected from the Web using a
semi-supervised approach. The URLs format
for pages containing potential parallel transla-
tions were manually identified (mainly from the
archives of Romanian newspapers). Next, texts
were automatically downloaded and sentence
aligned. A manual verification of the alignment
was also performed. These data collection pro-
cess resulted in a corpus of about 850,000 Roma-
nian words, and about 900,000 English words.
66
 Orwell?s 1984, aligned within the MULTEXT-
EAST project (Erjavec et al, 1997), with about
130,000 Romanian words, and a similar number
of English words.
 The Romanian Constitution, for about 13,000
Romanian words and 13,000 English words.
English?Hindi. A collection of sentence aligned
English?Hindi parallel texts, from the Emille project
(Baker et al, 2004), consisting of approximately En-
glish 60,000 words and about 70,000 Hindi words.
The Hindi data was encoded in Unicode Devangari
script, and used the UTF?8 encoding. The English?
Hindi data were provided by Niraj Aswani and Robert
Gaizauskas from University of Sheffield (Aswani and
Gaizauskas, 2005b).
2.2 Trial Data
Three sets of trial data were made available at the
same time training data became available. Trial sets
consisted of sentence aligned texts, provided together
with manually determined word alignments. The
main purpose of these data was to enable participants
to better understand the format required for the word
alignment result files. For some systems, the trial data
has also played the role of a validation data set used
for system parameter tuning. Trial sets consisted of
25 English?Inuktitut and English?Hindi aligned sen-
tences, and a larger set of 248 Romanian?English
aligned sentences (the same as the test data used in
the 2003 word alignment evaluation).
2.3 Test Data
A total of 75 English?Inuktitut, 90 English?Hindi,
and 200 Romanian?English aligned sentences were
released one week prior to the deadline. Participants
were required to run their word alignment systems on
one or more of these data sets, and submit word align-
ments. Teams were allowed to submit an unlimited
number of results sets for each language pair.
2.3.1 Gold Standard Word Aligned Data
The gold standard for the three language pair align-
ments were produced using slightly different align-
ment procedures.
For English?Inuktitut, annotators were instructed to
align Inuktitut words or phrases with English phrases.
Their goal was to identify the smallest phrases that
permit one-to-one alignments between English and
Inuktitut. These phrase alignments were converted
into word-to-word alignments in the following man-
ner. If the aligned English and Inuktitut phrases
each consisted of a single word, that word pair was
assigned a Sure alignment. Otherwise, all possi-
ble word-pairs for the aligned English and Inuktitut
phrases were assigned a Probable alignment. Dis-
agreements between the two annotators were decided
by discussion.
For Romanian?English and English?Hindi, anno-
tators were instructed to assign an alignment to all
words, with specific instructions as to when to as-
sign a NULL alignment. Annotators were not asked
to assign a Sure or Probable label. Instead, we had an
arbitration phase, where a third annotator judged the
cases where the first two annotators disagreed. Since
an inter-annotator agreement was reached for all word
alignments, the final resulting alignments were con-
sidered to be Sure alignments.
3 Evaluation Measures
Evaluations were performed with respect to four dif-
ferent measures. Three of them ? precision, recall,
and F-measure ? represent traditional measures in In-
formation Retrieval, and were also frequently used
in previous word alignment literature. The fourth
measure was originally introduced by (Och and Ney,
2000), and proposes the notion of quality of word
alignment.
Given an alignment   , and a gold standard align-
ment  , each such alignment set eventually consist-
ing of two sets   ,   , and  ,  corresponding
to Sure and Probable alignments, the following mea-
sures are defined (where  is the alignment type, and
can be set to either S or P).
	

 


 

 (1)




 




 (2)



 
	





 (3)



ffProceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13?18,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Measuring the Semantic Similarity of Texts
Courtney Corley and Rada Mihalcea
Department of Computer Science
University of North Texas
{corley,rada}@cs.unt.edu
Abstract
This paper presents a knowledge-based
method for measuring the semantic-
similarity of texts. While there is a large
body of previous work focused on find-
ing the semantic similarity of concepts
and words, the application of these word-
oriented methods to text similarity has not
been yet explored. In this paper, we in-
troduce a method that combines word-
to-word similarity metrics into a text-to-
text metric, and we show that this method
outperforms the traditional text similarity
metrics based on lexical matching.
1 Introduction
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vectorial
model in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their similarity to the given query (Salton and
Lesk, 1971). Text similarity has been also used for
relevance feedback and text classification (Rocchio,
1971), word sense disambiguation (Lesk, 1986), and
more recently for extractive summarization (Salton
et al, 1997b), and methods for automatic evaluation
of machine translation (Papineni et al, 2002) or text
summarization (Lin and Hovy, 2003).
The typical approach to finding the similarity be-
tween two text segments is to use a simple lexical
matching method, and produce a similarity score
based on the number of lexical units that occur in
both input segments. Improvements to this simple
method have considered stemming, stop-word re-
moval, part-of-speech tagging, longest subsequence
matching, as well as various weighting and normal-
ization factors (Salton et al, 1997a). While success-
ful to a certain degree, these lexical matching simi-
larity methods fail to identify the semantic similarity
of texts. For instance, there is an obvious similarity
between the text segments I own a dog and I have
an animal, but most of the current text similarity
metrics will fail in identifying any kind of connec-
tion between these texts. The only exception to this
trend is perhaps the latent semantic analysis (LSA)
method (Landauer et al, 1998), which represents
an improvement over earlier attempts to use mea-
sures of semantic similarity for information retrieval
(Voorhees, 1993), (Xu and Croft, 1996). LSA aims
to find similar terms in large text collections, and
measure similarity between texts by including these
additional related words. However, to date LSA has
not been used on a large scale, due to the complex-
ity and computational cost associated with the algo-
rithm, and perhaps also due to the ?black-box? ef-
fect that does not allow for any deep insights into
why some terms are selected as similar during the
singular value decomposition process.
In this paper, we explore a knowledge-based
method for measuring the semantic similarity of
texts. While there are several methods previ-
ously proposed for finding the semantic similar-
ity of words, to our knowledge the application of
these word-oriented methods to text similarity has
not been yet explored. We introduce an algorithm
13
that combines the word-to-word similarity metrics
into a text-to-text semantic similarity metric, and we
show that this method outperforms the simpler lex-
ical matching similarity approach, as measured in a
paraphrase identification application.
2 Measuring Text Semantic Similarity
Given two input text segments, we want to auto-
matically derive a score that indicates their similar-
ity at semantic level, thus going beyond the simple
lexical matching methods traditionally used for this
task. Although we acknowledge the fact that a com-
prehensive metric of text semantic similarity should
take into account the relations between words, as
well as the role played by the various entities in-
volved in the interactions described by each of the
two texts, we take a first rough cut at this problem
and attempt to model the semantic similarity of texts
as a function of the semantic similarity of the com-
ponent words. We do this by combining metrics of
word-to-word similarity and language models into
a formula that is a potentially good indicator of the
semantic similarity of the two input texts.
2.1 Semantic Similarity of Words
There is a relatively large number of word-to-word
similarity metrics that were previously proposed in
the literature, ranging from distance-oriented mea-
sures computed on semantic networks, to metrics
based on models of distributional similarity learned
from large text collections. From these, we chose to
focus our attention on six different metrics, selected
mainly for their observed performance in natural
language processing applications, e.g. malapropism
detection (Budanitsky and Hirst, 2001) and word
sense disambiguation (Patwardhan et al, 2003), and
for their relatively high computational efficiency.
We conduct our evaluation using the following
word similarity metrics: Leacock & Chodorow,
Lesk, Wu & Palmer, Resnik, Lin, and Jiang & Con-
rath. Note that all these metrics are defined be-
tween concepts, rather than words, but they can be
easily turned into a word-to-word similarity metric
by selecting for any given pair of words those two
meanings that lead to the highest concept-to-concept
similarity. We use the WordNet-based implemen-
tation of these metrics, as available in the Word-
Net::Similarity package (Patwardhan et al, 2003).
We provide below a short description for each of
these six metrics.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) similarity is determined as:
Simlch = ? log
length
2 ? D (1)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk similarity of two concepts is defined as a
function of the overlap between the corresponding
definitions, as provided by a dictionary. It is based
on an algorithm proposed in (Lesk, 1986) as a solu-
tion for word sense disambiguation.
The Wu and Palmer (Wu and Palmer, 1994) simi-
larity metric measures the depth of the two concepts
in the WordNet taxonomy, and the depth of the least
common subsumer (LCS), and combines these fig-
ures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(2)
The measure introduced by Resnik (Resnik, 1995)
returns the information content (IC) of the LCS of
two concepts:
Simres = IC(LCS) (3)
where IC is defined as:
IC(c) = ? log P (c) (4)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The next measure we use in our experiments is the
metric introduced by Lin (Lin, 1998), which builds
on Resnik?s measure of similarity, and adds a nor-
malization factor consisting of the information con-
tent of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(5)
Finally, the last similarity metric we consider is
Jiang & Conrath (Jiang and Conrath, 1997), which
returns a score determined by:
Simjnc =
1
IC(concept1) + IC(concept2) ? 2 ? IC(LCS)
(6)
14
2.2 Language Models
In addition to the semantic similarity of words, we
also want to take into account the specificity of
words, so that we can give a higher weight to a se-
mantic matching identified between two very spe-
cific words (e.g. collie and sheepdog), and give less
importance to the similarity score measured between
generic concepts (e.g. go and be). While the speci-
ficity of words is already measured to some extent
by their depth in the semantic hierarchy, we are re-
inforcing this factor with a corpus-based measure of
word specificity, based on distributional information
learned from large corpora.
Language models are frequently used in natural
language processing applications to account for the
distribution of words in language. While word fre-
quency does not always constitute a good measure of
word importance, the distribution of words across an
entire collection can be a good indicator of the speci-
ficity of the words. Terms that occur in a few docu-
ments with high frequency contain a greater amount
of discriminatory ability, while terms that occur in
numerous documents across a collection with a high
frequency have inherently less meaning to a docu-
ment. We determine the specificity of a word us-
ing the inverse document frequency introduced in
(Sparck-Jones, 1972), which is defined as the total
number of documents in the corpus, divided by the
total number of documents that include that word.
In the experiments reported in this paper, we use the
British National Corpus to derive the document fre-
quency counts, but other corpora could be used to
the same effect.
2.3 Semantic Similarity of Texts
Provided a measure of semantic similarity between
words, and an indication of the word specificity, we
combine them into a measure of text semantic sim-
ilarity, by pairing up those words that are found to
be most similar to each other, and weighting their
similarity with the corresponding specificity score.
We define a directional measure of similarity,
which indicates the semantic similarity of a text seg-
ment Ti with respect to a text segment Tj . This def-
inition provides us with the flexibility we need to
handle applications where the directional knowledge
is useful (e.g. entailment), and at the same time it
gives us the means to handle bidirectional similarity
through a simple combination of two unidirectional
metrics.
For a given pair of text segments, we start by cre-
ating sets of open-class words, with a separate set
created for nouns, verbs, adjectives, and adverbs.
In addition, we also create a set for cardinals, since
numbers can also play an important role in the un-
derstanding of a text. Next, we try to determine pairs
of similar words across the sets corresponding to the
same open-class in the two text segments. For nouns
and verbs, we use a measure of semantic similarity
based on WordNet, while for the other word classes
we apply lexical matching1.
For each noun (verb) in the set of nouns (verbs)
belonging to one of the text segments, we try to iden-
tify the noun (verb) in the other text segment that has
the highest semantic similarity (maxSim), accord-
ing to one of the six measures of similarity described
in Section 2.1. If this similarity measure results in a
score greater than 0, then the word is added to the set
of similar words for the corresponding word class
WSpos2. The remaining word classes: adjectives,
adverbs, and cardinals, are checked for lexical sim-
ilarity with their counter-parts and included in the
corresponding word class set if a match is found.
The similarity between the input text segments Ti
and Tj is then determined using a scoring function
that combines the word-to-word similarities and the
word specificity:
sim(Ti, Tj)Ti =
?
pos
(
?
wk?{WSpos}
(maxSim(wk) ? idfwk ))
?
wk?{Tipos}
idfwk
(7)
This score, which has a value between 0 and 1, is
a measure of the directional similarity, in this case
computed with respect to Ti. The scores from both
directions can be combined into a bidirectional sim-
ilarity using a simple average function:
sim(Ti, Tj) =
sim(Ti, Tj)Ti + sim(Ti, Tj)Tj
2 (8)
1The reason behind this decision is the fact that most of the
semantic similarity measures apply only to nouns and verbs, and
there are only one or two relatedness metrics that can be applied
to adjectives and adverbs.
2All similarity scores have a value between 0 and 1. The
similarity threshold can be also set to a value larger than 0,
which would result in tighter measures of similarity.
15
Text Segment 1: The jurors were taken into the courtroom in
groups of 40 and asked to fill out a questionnaire.
? SetNN = {juror, courtroom, group, questionnaire}
SetV B = {be, take, ask, fill}
SetRB = {out}
SetCD = {40}
Text Segment 2: About 120 potential jurors were being asked
to complete a lengthy questionnaire.
? SetNN = {juror, questionnaire}
SetV B = {be, ask, complete}
SetJJ = {potential, lengthy}
SetCD = {120}
Figure 1: Two text segments and their corresponding
word class sets
3 A Walk-Through Example
We illustrate the application of the text similarity
measure with an example. Given two text segments,
as shown in Figure 1, we want to determine a score
that reflects their semantic similarity. For illustration
purposes, we restrict our attention to one measure of
word-to-word similarity, the Wu & Palmer metric.
First, the text segments are tokenized, part-of-
speech tagged, and the words are inserted into their
corresponding word class sets. The sets obtained for
the given text segments are illustrated in Figure 1.
Starting with each of the two text segments, and
for each word in its word class sets, we determine
the most similar word from the corresponding set in
the other text segment. As mentioned earlier, we
seek a WordNet-based semantic similarity for nouns
and verbs, and only lexical matching for adjectives,
adverbs, and cardinals. The word semantic similar-
ity scores computed starting with the first text seg-
ment are shown in Table 3.
Text 1 Text 2 maxSim IDF
jurors jurors 1.00 5.80
courtroom jurors 0.30 5.23
questionnaire questionnaire 1.00 3.57
groups questionnaire 0.29 0.85
were were 1.00 0.09
taken asked 1.00 0.28
asked asked 1.00 0.45
fill complete 0.86 1.29
out ? 0 0.06
40 ? 0 1.39
Table 1: Wu & Palmer word similarity scores for
computing text similarity with respect to text 1
Next, we use equation 7 and determine the seman-
tic similarity of the two text segments with respect
to text 1 as 0.6702, and with respect to text 2 as
0.7202. Finally, the two figures are combined into
a bidirectional measure of similarity, calculated as
0.6952 based on equation 8.
Although there are a few words that occur in both
text segments (e.g. juror, questionnaire), there are
also words that are not identical, but closely related,
e.g. courtroom found similar to juror, or fill which
is related to complete. Unlike traditional similar-
ity measures based on lexical matching, our metric
takes into account the semantic similarity of these
words, resulting in a more precise measure of text
similarity.
4 Evaluation
To test the effectiveness of the text semantic simi-
larity metric, we use this measure to automatically
identify if two text segments are paraphrases of
each other. We use the Microsoft paraphrase cor-
pus (Dolan et al, 2004), consisting of 4,076 training
pairs and 1,725 test pairs, and determine the number
of correctly identified paraphrase pairs in the cor-
pus using the text semantic similarity measure as the
only indicator of paraphrasing. In addition, we also
evaluate the measure using the PASCAL corpus (Da-
gan et al, 2005), consisting of 1,380 test?hypothesis
pairs with a directional entailment (580 development
pairs and 800 test pairs).
For each of the two data sets, we conduct two
evaluations, under two different settings: (1) An un-
supervised setting, where the decision on what con-
stitutes a paraphrase (entailment) is made using a
constant similarity threshold of 0.5 across all exper-
iments; and (2) A supervised setting, where the op-
timal threshold and weights associated with various
similarity metrics are determined through learning
on training data. In this case, we use a voted percep-
tron algorithm (Freund and Schapire, 1998)3.
We evaluate the text similarity metric built on top
of the various word-to-word metrics introduced in
Section 2.1. For comparison, we also compute three
baselines: (1) A random baseline created by ran-
domly choosing a true or false value for each text
pair; (2) A lexical matching baseline, which only
3Classification using this algorithm was determined optimal
empirically through experiments.
16
counts the number of matching words between the
two text segments, while still applying the weighting
and normalization factors from equation 7; and (3)
A vectorial similarity baseline, using a cosine sim-
ilarity measure as traditionally used in information
retrieval, with tf.idf term weighting. For compari-
son, we also evaluated the corpus-based similarity
obtained through LSA; however, the results obtained
were below the lexical matching baseline and are not
reported here.
For paraphrase identification, we use the bidirec-
tional similarity measure, and determine the sim-
ilarity with respect to each of the two text seg-
ments in turn, and then combine them into a bidi-
rectional similarity metric. For entailment identifi-
cation, since this is a directional relation, we only
measure the semantic similarity with respect to the
hypothesis (the text that is entailed).
We evaluate the results in terms of accuracy, rep-
resenting the number of correctly identified true or
false classifications in the test data set. We also mea-
sure precision, recall and F-measure, calculated with
respect to the true values in each of the test data sets.
Tables 2 and 3 show the results obtained in the
unsupervised setting, when a text semantic similar-
ity larger than 0.5 was considered to be an indica-
tor of paraphrasing (entailment). We also evaluate a
metric that combines all the similarity measures us-
ing a simple average, with results indicated in the
Combined row.
The results obtained in the supervised setting are
shown in Tables 4 and 5. The optimal combination
of similarity metrics and optimal threshold are now
determined in a learning process performed on the
training set. Under this setting, we also compute an
additional baseline, consisting of the most frequent
label, as determined from the training data.
5 Discussion and Conclusions
For the task of paraphrase recognition, incorporating
semantic information into the text similarity mea-
sure increases the likelihood of recognition signifi-
cantly over the random baseline and over the lexi-
cal matching baseline. In the unsupervised setting,
the best performance is achieved using a method that
combines several similarity metrics into one, for an
overall accuracy of 68.8%. When learning is used to
find the optimal combination of metrics and optimal
threshold, the highest accuracy of 71.5% is obtained
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
J & C 0.683 0.724 0.846 0.780
L & C 0.680 0.724 0.838 0.777
Lesk 0.680 0.724 0.838 0.777
Lin 0.679 0.717 0.855 0.780
W & P 0.674 0.722 0.831 0.773
Resnik 0.672 0.725 0.815 0.768
Combined 0.688 0.741 0.817 0.777
Baselines
LexMatch 0.661 0.722 0.798 0.758
Vectorial 0.654 0.716 0.795 0.753
Random 0.513 0.683 0.500 0.578
Table 2: Text semantic similarity for paraphrase
identification (unsupervised)
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
J & C 0.573 0.543 0.908 0.680
L & C 0.569 0.543 0.870 0.669
Lesk 0.568 0.542 0.875 0.669
Resnik 0.565 0.541 0.850 0.662
Lin 0.563 0.538 0.878 0.667
W & P 0.558 0.534 0.895 0.669
Combined 0.583 0.561 0.755 0.644
Baselines
LexMatch 0.545 0.530 0.795 0.636
Vectorial 0.528 0.525 0.588 0.555
Random 0.486 0.486 0.493 0.489
Table 3: Text semantic similarity for entailment
identification (unsupervised)
by combining the similarity metrics and the lexical
matching baseline together.
For the entailment data set, although we do not
explicitly check for entailment, the directional sim-
ilarity computed for textual entailment recognition
does improve over the random and lexical matching
baselines. Once again, the combination of similar-
ity metrics gives the highest accuracy, measured at
58.3%, with a slight improvement observed in the
supervised setting, where the highest accuracy was
measured at 58.9%. Both these figures are compet-
itive with the best results achieved during the PAS-
CAL entailment evaluation (Dagan et al, 2005).
Although our method relies on a bag-of-words ap-
proach, as it turns out the use of measures of seman-
tic similarity improves significantly over the tradi-
tional lexical matching metrics4. We are nonetheless
4The improvement of the combined semantic similarity met-
ric over the simpler lexical matching measure was found to be
statistically significant in all experiments, using a paired t-test
(p < 0.001).
17
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
Lin 0.702 0.706 0.947 0.809
W & P 0.699 0.705 0.941 0.806
L & C 0.699 0.708 0.931 0.804
J & C 0.699 0.707 0.935 0.805
Lesk 0.695 0.702 0.929 0.800
Resnik 0.692 0.705 0.921 0.799
Combined 0.715 0.723 0.925 0.812
Baselines
LexMatch 0.671 0.693 0.908 0.786
Vectorial 0.665 0.665 1.000 0.799
Most frequent 0.665 0.665 1.000 0.799
Table 4: Text semantic similarity for paraphrase
identification (supervised)
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
L & C 0.583 0.573 0.650 0.609
W & P 0.580 0.570 0.648 0.607
Resnik 0.579 0.572 0.628 0.598
Lin 0.574 0.568 0.620 0.593
J & C 0.575 0.566 0.643 0.602
Lesk 0.573 0.566 0.633 0.597
Combined 0.589 0.579 0.650 0.612
Baselines
LexMatch 0.568 0.573 0.530 0.551
Most frequent 0.500 0.500 1.000 0.667
Vectorial 0.479 0.484 0.645 0.553
Table 5: Text semantic similarity for entailment
identification (supervised)
aware that a bag-of-words approach ignores many of
important relationships in sentence structure, such as
dependencies between words, or roles played by the
various arguments in the sentence. Future work will
consider the investigation of more sophisticated rep-
resentations of sentence structure, such as first order
predicate logic or semantic parse trees, which should
allow for the implementation of more effective mea-
sures of text semantic similarity.
References
A. Budanitsky and G. Hirst. 2001. Semantic distance in word-
net: An experimental, application-oriented evaluation of five
measures. In Proceedings of the NAACL Workshop on Word-
Net and Other Lexical Resources, Pittsburgh, June.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL
recognising textual entailment challenge. In Proceedings of
the PASCAL Workshop.
W.B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics, Geneva, Switzerland.
Y. Freund and R.E. Schapire. 1998. Large margin classifica-
tion using the perceptron algorithm. In Proceedings of the
11th Annual Conference on Computational Learning The-
ory, pages 209?217, New York, NY. ACM Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based on
corpus statistics and lexical taxonomy. In Proceedings of
the International Conference on Research in Computational
Linguistics, Taiwan.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to
latent semantic analysis. Discourse Processes, 25.
C. Leacock and M. Chodorow. 1998. Combining local context
and WordNet sense similiarity for word sense disambigua-
tion. In WordNet, An Electronic Lexical Database. The MIT
Press.
M.E. Lesk. 1986. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from an
ice cream cone. In Proceedings of the SIGDOC Conference
1986, Toronto, June.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the 15th International Conference on
Machine Learning, Madison, WI.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu:
a method for automatic evaluation of machine translation.
In Proceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002), Philadel-
phia, PA, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using mea-
sures of semantic relatedness for word sense disambiguation.
In Proceedings of the Fourth International Conference on
Intelligent Text Processing and Computational Linguistics,
Mexico City, February.
P. Resnik. 1995. Using information content to evaluate seman-
tic similarity. In Proceedings of the 14th International Joint
Conference on Artificial Intelligence, Montreal, Canada.
J. Rocchio, 1971. Relevance feedback in information retrieval.
Prentice Hall, Ing. Englewood Cliffs, New Jersey.
G. Salton and M.E. Lesk, 1971. Computer evaluation of index-
ing and text processing, pages 143?180. Prentice Hall, Ing.
Englewood Cliffs, New Jersey.
G. Salton, , and A. Bukley. 1997a. Term weighting approaches
in automatic text retrieval. In Readings in Information Re-
trieval. Morgan Kaufmann Publishers, San Francisco, CA.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997b. Auto-
matic text structuring and summarization. Information Pro-
cessing and Management, 2(32).
K. Sparck-Jones. 1972. A statistical interpretation of term
specificity and its applicatoin in retrieval. Journal of Doc-
umentation, 28(1):11?21.
E. Voorhees. 1993. Using wordnet to disambiguate word
senses for text retrieval. In Proceedings of the 16th annual
international ACM SIGIR conference, Pittsburgh, PA.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical se-
lection. In Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics, Las Cruces, New
Mexico.
J. Xu and W. B. Croft. 1996. Query expansion using local and
global document analysis. In Proceedings of the 19th annual
international ACM SIGIR conference, Zurich, Switzerland.
18
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 70?74,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 14: Affective Text
Carlo Strapparava
FBK ? irst
Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Povo, Trento, Italy
strappa@itc.it
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX, 76203, USA
rada@cs.unt.edu
Abstract
The ?Affective Text? task focuses on the
classification of emotions and valence (pos-
itive/negative polarity) in news headlines,
and is meant as an exploration of the connec-
tion between emotions and lexical seman-
tics. In this paper, we describe the data set
used in the evaluation and the results ob-
tained by the participating systems.
1 Introduction
All words can potentially convey affective mean-
ing. Every word, even those apparently neutral, can
evoke pleasant or painful experiences due to their
semantic relation with emotional concepts or cate-
gories. Some words have emotional meaning with
respect to an individual story, while for many others
the affective power is part of the collective imagina-
tion (e.g., words such as ?mum?, ?ghost?, ?war?).
The automatic detection of emotion in texts is
becoming increasingly important from an applica-
tive point of view. Consider for example the tasks
of opinion mining and market analysis, affective
computing, or natural language interfaces such as
e-learning environments or educational/edutainment
games. Possible beneficial effects of emotions on
memory and attention of the users, and in general on
fostering their creativity are also well-known in the
field of psychology.
For instance, the following represent examples
of applicative scenarios in which affective analysis
would give valuable and interesting contributions:
Sentiment Analysis. Text categorization according
to affective relevance, opinion exploration for
market analysis, etc. are just some exam-
ples of application of these techniques. While
positive/negative valence annotation is an ac-
tive field of sentiment analysis, we believe that
a fine-grained emotion annotation would in-
crease the effectiveness of these applications.
Computer Assisted Creativity. The automated
generation of evaluative expressions with
a bias on some polarity orientation are a
key component for automatic personalized
advertisement and persuasive communication.
Verbal Expressivity in Human Computer Interaction.
Future human-computer interaction, accord-
ing to a widespread view, will emphasize
naturalness and effectiveness and hence the
incorporation of models of possibly many hu-
man cognitive capabilities, including affective
analysis and generation. For example, emo-
tion expression by synthetic characters (e.g.,
embodied conversational agents) is considered
now a key element for their believability.
Affective words selection and understanding is
crucial for realizing appropriate and expressive
conversations.
The ?Affective Text? task was intended as an ex-
ploration of the connection between lexical seman-
tics and emotions, and an evaluation of various au-
tomatic approaches to emotion recognition.
The task is not easy. Indeed, as (Ortony et
al., 1987) indicates, besides words directly refer-
ring to emotional states (e.g., ?fear?, ?cheerful?) and
for which an appropriate lexicon would help, there
are words that act only as an indirect reference to
70
emotions depending on the context (e.g. ?monster?,
?ghost?). We can call the former direct affective
words and the latter indirect affective words (Strap-
parava et al, 2006).
2 Task Definition
We proposed to focus on the emotion classification
of news headlines extracted from news web sites.
Headlines typically consist of a few words and are
often written by creative people with the intention
to ?provoke? emotions, and consequently to attract
the readers? attention. These characteristics make
this type of text particularly suitable for use in an
automatic emotion recognition setting, as the affec-
tive/emotional features (if present) are guaranteed to
appear in these short sentences.
The structure of the task was as follows:
Corpus: News titles, extracted from news web sites
(such as Google news, CNN) and/or newspa-
pers. In the case of web sites, we can easily
collect a few thousand titles in a short amount
of time.
Objective: Provided a set of predefined six emotion
labels (i.e., Anger, Disgust, Fear, Joy, Sadness,
Surprise), classify the titles with the appropri-
ate emotion label and/or with a valence indica-
tion (positive/negative).
The emotion labeling and valence classification
were seen as independent tasks, and thus a team was
able to participate in one or both tasks. The task
was carried out in an unsupervised setting, and con-
sequently no training was provided. The reason be-
hind this decision is that we wanted to emphasize the
study of emotion lexical semantics, and avoid bias-
ing the participants toward simple ?text categoriza-
tion? approaches. Nonetheless supervised systems
were not precluded from participation, and in such
cases the teams were allowed to create their own su-
pervised training sets.
Participants were free to use any resources they
wanted. We provided a set words extracted from
WordNet Affect (Strapparava and Valitutti, 2004),
relevant to the six emotions of interest. However,
the use of this list was entirely optional.
2.1 Data Set
The data set consisted of news headlines drawn from
major newspapers such as New York Times, CNN,
and BBC News, as well as from the Google News
search engine. We decided to focus our attention on
headlines for two main reasons. First, news have
typically a high load of emotional content, as they
describe major national or worldwide events, and are
written in a style meant to attract the attention of the
readers. Second, the structure of headlines was ap-
propriate for our goal of conducting sentence-level
annotations of emotions.
Two data sets were made available: a develop-
ment data set consisting of 250 annotated headlines,
and a test data set with 1,000 annotated headlines.
2.2 Data Annotation
To perform the annotations, we developed a Web-
based annotation interface that displayed one head-
line at a time, together with six slide bars for emo-
tions and one slide bar for valence. The interval for
the emotion annotations was set to [0, 100], where 0
means the emotion is missing from the given head-
line, and 100 represents maximum emotional load.
The interval for the valence annotations was set to
[?100, 100], where 0 represents a neutral headline,
?100 represents a highly negative headline, and 100
corresponds to a highly positive headline.
Unlike previous annotations of sentiment or sub-
jectivity (Wiebe et al, 2005; Pang and Lee, 2004),
which typically relied on binary 0/1 annotations, we
decided to use a finer-grained scale, hence allow-
ing the annotators to select different degrees of emo-
tional load.
The test data set was independently labeled by six
annotators. The annotators were instructed to select
the appropriate emotions for each headline based on
the presence of words or phrases with emotional
content, as well as the overall feeling invoked by
the headline. Annotation examples were also pro-
vided, including examples of headlines bearing two
or more emotions to illustrate the case where sev-
eral emotions were jointly applicable. Finally, the
annotators were encouraged to follow their ?first in-
tuition,? and to use the full-range of the annotation
scale bars.
71
2.3 Inter-Annotator Agreement
We conducted inter-tagger agreement studies for
each of the six emotions and for the valence an-
notations. The agreement evaluations were carried
out using the Pearson correlation measure, and are
shown in Table 1. To measure the agreement among
the six annotators, we first measured the agreement
between each annotator and the average of the re-
maining five annotators, followed by an average
over the six resulting agreement figures.
EMOTIONS
Anger 49.55
Disgust 44.51
Fear 63.81
Joy 59.91
Sadness 68.19
Surprise 36.07
VALENCE
Valence 78.01
Table 1: Pearson correlation for inter-annotator
agreement
2.4 Fine-grained and Coarse-grained
Evaluations
Fine-grained evaluations were conducted using the
Pearson measure of correlation between the system
scores and the gold standard scores, averaged over
all the headlines in the data set.
We have also run a coarse-grained evaluation,
where each emotion was mapped to a 0/1 classifica-
tion (0 = [0,50), 1 = [50,100]), and each valence was
mapped to a -1/0/1 classification (-1 = [-100,-50],
0 = (-50,50), 1 = [50,100]). For the coarse-grained
evaluations, we calculated accuracy, precision, and
recall. Note that the accuracy is calculated with re-
spect to all the possible classes, and thus it can be
artificially high in the case of unbalanced datasets
(as some of the emotions are, due to the high num-
ber of neutral headlines). Instead, the precision and
recall figures exclude the neutral annotations.
3 Participating Systems
Five teams have participated in the task, with five
systems for valence classification and three systems
for emotion labeling. The following represents a
short description of the systems.
UPAR7: This is a rule-based system using a lin-
guistic approach. A first pass through the data ?un-
capitalizes? common words in the news title. The
system then used the Stanford syntactic parser on
the modified title, and tried to identify what is being
said about the main subject by exploiting the depen-
dency graph obtained from the parser.
Each word was first rated separately for each emo-
tion (the six emotions plus Compassion) and for va-
lence. Next, the main subject rating was boosted.
Contrasts and accentuations between ?good? or
?bad? were detected, making it possible to identify
surprising good or bad news. The system also takes
into account: human will (as opposed to illness or
natural disasters); negation and modals; high-tech
context; celebrities.
The lexical resource used was a combination
of SentiWordNet (Esuli and Sebastiani, 2006) and
WordNetAffect (Strapparava and Valitutti, 2004),
which were semi-automatically enriched on the ba-
sis of the original trial data.
SICS: The SICS team used a very simple ap-
proach for valence annotation based on a word-space
model and a set of seed words. The idea was to cre-
ate two points in a high-dimensional word space -
one representing positive valence, the other repre-
senting negative valence - and then projecting each
headline into this space, choosing the valence whose
point was closer to the headline.
The word space was produced from a lemmatized
and stop list filtered version of the LA times cor-
pus (consisting of documents from 1994, released
for experimentation in the Cross Language Eval-
uation Forum (CLEF)) using documents as con-
texts and standard TFIDF weighting of frequencies.
No dimensionality reduction was used, resulting in
a 220,220-dimensional word space containing pre-
dominantly syntagmatic relations between words.
Valence vectors were created in this space by sum-
ming the context vectors of a set of manually se-
lected seed words (8 positive and 8 negative words).
For each headline in the test data, stop words and
words with frequency above 10,000 in the LA times
corpus were removed. The context vectors of the re-
maining words were then summed, and the cosine of
the angles between the summed vector and each of
the valence vectors were computed, and the head-
line was ascribed the valence value (computed as
72
[cosine * 100 + 50]) of the closest valence vector
(headlines that were closer to the negative valence
vector were assigned a negative valence value). In
11 cases, a value of -0.0 was ascribed either because
no words were left in the headline after frequency
and stop word filtering, or because none of the re-
maining words occurred in the LA times corpus and
thus did not have any context vector.
CLaC: This team submitted two systems to the
competition: an unsupervised knowledge-based sys-
tem (ClaC) and a supervised corpus-based system
(CLaC-NB). Both systems were used for assigning
positive/negative and neutral valence to headlines on
the scale [-100,100].
CLaC: The CLaC system relies on a knowledge-
based domain-independent unsupervised approach
to headline valence detection and scoring. The
system uses three main kinds of knowledge: a
list of sentiment-bearing words, a list of valence
shifters and a set of rules that define the scope and
the result of the combination of sentiment-bearing
words and valence shifters. The unigrams used for
sentence/headline classification were learned from
WordNet dictionary entries. In order to take advan-
tage of the special properties of WordNet glosses
and relations, we developed a system that used the
list of human-annotated adjectives from (Hatzivas-
siloglou and McKeown, 1997) as a seed list and
learned additional unigrams from WordNet synsets
and glosses. The list was then expanded by adding
to it all the words annotated with Positive or Neg-
ative tags in the General Inquirer. Each unigram in
the resulting list had the degree of membership in the
category of positive or negative sentiment assigned
to it using the fuzzy Net Overlap Score method de-
scribed in the team?s earlier work (Andreevskaia and
Bergler, 2006). Only words with fuzzy member-
ship score not equal to zero were retained in the
list. The resulting list contained 10,809 sentiment-
bearing words of different parts of speech.
The fuzzy Net Overlap Score counts were com-
plemented with the capability to discern and take
into account some relevant elements of syntactic
structure of the sentences. Two components were
added to the system to enable this capability: (1)
valence shifter handling rules and (2) parse tree
analysis. The list of valence shifters was a com-
bination of a list of common English negations
and a subset of the list of automatically obtained
words with increase/decrease semantics, comple-
mented with manual annotation. The full list con-
sists of 450 words and expressions. Each entry in
the list of valence shifters has an action and scope
associated with it, which are used by special han-
dling rules that enable the system to identify such
words and phrases in the text and take them into ac-
count in sentence sentiment determination. In order
to correctly determine the scope of valence shifters
in a sentence, the system used a parse tree analysis
using MiniPar.
As a result of this processing, every headline re-
ceived a system score assigned based on the com-
bined fuzzy Net Overlap Score of its constituents.
This score was then mapped into the [-100 to 100]
scale as required by the task.
CLaC-NB: In order to assess the performance of
basic Machine Learning techniques on headlines,
a second system ClaC-NB was also implemented.
This system used a Na??ve Bayes classifier in order to
assign valence to headlines. It was trained on a small
corpus composed of the development corpus of 250
headlines provided for this competition, plus an ad-
ditional 200 headlines manually annotated and 400
positive and negative news sentences. The probabil-
ities assigned by the classifier were mapped to the [-
100, 100] scale as follows: all negative headlines re-
ceived the score of -100, all positive headlines were
assigned the score of +100, and the neutral headlines
obtained the score of 0.
UA: In order to determine the kind and the amount
of emotions in a headline, statistics were gathered
from three different web Search Engines: MyWay,
AlltheWeb and Yahoo. This information was used to
observe the distribution of the nouns, the verbs, the
adverbs and the adjectives extracted from the head-
line and the different emotions.
The emotion scores were obtained through Point-
wise Mutual Information (PMI). First, the number
of documents obtained from the three web search
engines using a query that contains all the headline
words and an emotion (the words occur in an inde-
pendent proximity across the web documents) was
divided by the number of documents containing only
an emotion and the number of documents containing
all the headline words. Second, an associative score
between each content word and an emotion was es-
73
timated and used to weight the final PMI score. The
obtained results were normalized in the 0-100 range.
SWAT: SWAT is a supervised system using an u-
nigram model trained to annotate emotional content.
Synonym expansion on the emotion label words was
also performed, using the Roget Thesaurus. In addi-
tion to the development data provided by the task
organizers, the SWAT team annotated an additional
set of 1000 headlines, which was used for training.
Fine Coarse
r Acc. Prec. Rec. F1
CLaC 47.70 55.10 61.42 9.20 16.00
UPAR7 36.96 55.00 57.54 8.78 15.24
SWAT 35.25 53.20 45.71 3.42 6.36
CLaC-NB 25.41 31.20 31.18 66.38 42.43
SICS 20.68 29.00 28.41 60.17 38.60
Table 2: System results for valence annotations
Fine Coarse
r Acc. Prec. Rec. F1
Anger
SWAT 24.51 92.10 12.00 5.00 7.06
UA 23.20 86.40 12.74 21.6 16.03
UPAR7 32.33 93.60 16.67 1.66 3.02
Disgust
SWAT 18.55 97.20 0.00 0.00 -
UA 16.21 97.30 0.00 0.00 -
UPAR7 12.85 95.30 0.00 0.00 -
Fear
SWAT 32.52 84.80 25.00 14.40 18.27
UA 23.15 75.30 16.23 26.27 20.06
UPAR7 44.92 87.90 33.33 2.54 4.72
Joy
SWAT 26.11 80.60 35.41 9.44 14.91
UA 2.35 81.80 40.00 2.22 4.21
UPAR7 22.49 82.20 54.54 6.66 11.87
Sadness
SWAT 38.98 87.70 32.50 11.92 17.44
UA 12.28 88.90 25.00 0.91 1.76
UPAR7 40.98 89.00 48.97 22.02 30.38
Surprise
SWAT 11.82 89.10 11.86 10.93 11.78
UA 7.75 84.60 13.70 16.56 15.00
UPAR7 16.71 88.60 12.12 1.25 2.27
Table 3: System results for emotion annotations
4 Results
Tables 2 and 3 show the results obtained by the par-
ticipating systems. The tables show both the fine-
grained Pearson correlation measure and the coarse-
grained accuracy, precision and recall figures.
While further analysis is still needed, the results
indicate that the task of emotion annotation is diffi-
cult. Although the Pearson correlation for the inter-
tagger agreement is not particularly high, the gap
between the results obtained by the systems and the
upper bound represented by the annotator agreement
suggests that there is room for future improvements.
Acknowledgments
Carlo Strapparava was partially supported by the
HUMAINE Network of Excellence.
References
A. Andreevskaia and S. Bergler. 2006. Senses and senti-
ments: Sentiment tagging of adjectives at the meaning
level. In Proceedings of the 19th Canadian Confer-
ence on Artificial Intelligence, AI?06, Quebec, Canada.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion mining.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
Genoa, Italy, May.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In Proceedings
of the 35th Annual Meeting of the ACL, Madrid, Spain,
July.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, (11).
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain, July.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, Lisbon.
C. Strapparava, A. Valitutti, and O. Stock. 2006. The
affective weight of lexicon. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
74
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406?409,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT-Yahoo: SuperSenseLearner: Combining SenseLearner with
SuperSense and other Coarse Semantic Features
Rada Mihalcea and Andras Csomai
University of North Texas
rada@cs.unt.edu,csomaia@unt.edu
Massimiliano Ciaramita
Yahoo! Research Barcelona
massi@yahoo-inc.com
Abstract
We describe the SUPERSENSELEARNER
system that participated in the English all-
words disambiguation task. The system re-
lies on automatically-learned semantic mod-
els using collocational features coupled with
features extracted from the annotations of
coarse-grained semantic categories gener-
ated by an HMM tagger.
1 Introduction
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a poly-
semous word within a given context. Applications
such as machine translation, knowledge acquisition,
common sense reasoning, and others, require knowl-
edge about word meanings, and word sense disam-
biguation is considered essential for all these tasks.
Most of the efforts in solving this problem
were concentrated so far toward targeted supervised
learning, where each sense tagged occurrence of a
particular word is transformed into a feature vector,
which is then used in an automatic learning process.
The applicability of such supervised algorithms is
however limited only to those few words for which
sense tagged data is available, and their accuracy
is strongly connected to the amount of labeled data
available at hand.
Instead, methods that address all words in unre-
stricted text have received significantly less atten-
tion. While the performance of such methods is usu-
ally exceeded by their supervised lexical-sample al-
ternatives, they have however the advantage of pro-
viding larger coverage.
In this paper, we describe SUPERSENSE-
LEARNER ? a system for solving the semantic am-
biguity of all words in unrestricted text. SUPER-
SENSELEARNER brings together under one system
the features previously used in the SENSELEARNER
(Mihalcea and Csomai, 2005) and the SUPERSENSE
(Ciaramita and Altun, 2006) all-words word sense
disambiguation systems. The system is using a rel-
atively small pre-existing sense-annotated data set
for training purposes, and it learns global semantic
models for general word categories.
2 Learning for All-Words Word Sense
Disambiguation
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm gen-
eral enough to be able to disambiguate as many
content words as possible in a text, and efficient
enough so that large amounts of text can be anno-
tated in real time. SUPERSENSELEARNER is at-
tempting to learn general semantic models for var-
ious word categories, starting with a relatively small
sense-annotated corpus. We base our experiments
on SemCor (Miller et al, 1993), a balanced, se-
mantically annotated dataset, with all content words
manually tagged by trained lexicographers.
The input to the disambiguation algorithm con-
sists of raw text. The output is a text with word
meaning annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with part-
406
of-speech tags; collocations are identified using a
sliding window approach, where a collocation is de-
fined as a sequence of words that forms a compound
concept defined in WordNet (Miller, 1995).
Next, a semantic model is learned for all pre-
defined word categories, where a word category is
defined as a group of words that share some com-
mon syntactic or semantic properties. Word cate-
gories can be of various granularities. For instance,
a model can be defined and trained to handle all the
nouns in the test corpus. Similarly, using the same
mechanism, a finer-grained model can be defined to
handle all the verbs for which at least one of the
meanings is of type e.g., ?<move>?. Finally, small
coverage models that address one word at a time, for
example a model for the adjective ?small,? can be
also defined within the same framework. Once de-
fined and trained, the models are used to annotate the
ambiguous words in the test corpus with their corre-
sponding meaning. Sections 3 and 4 below provide
details on the features implemented by the various
models.
Note that the semantic models are applicable only
to: (1) words that are covered by the word category
defined in the models; and (2) words that appeared
at least once in the training corpus. The words that
are not covered by these models (typically about 10-
15% of the words in the test corpus) are assigned the
most frequent sense in WordNet.
3 SenseLearner Semantic Models
Different semantic models can be defined and
trained for the disambiguation of different word cat-
egories. Although more general than models that
are built individually for each word in a test corpus
(Decadt et al, 2004), the applicability of the seman-
tic models built as part of SENSELEARNER is still
limited to those words previously seen in the train-
ing corpus, and therefore their overall coverage is
not 100%.
Starting with an annotated corpus consisting of
all the annotated files in SemCor, augmented with
the SENSEVAL-2 and SENSEVAL-3 all-words data
sets, a separate training data set is built for each
model. There are seven models provided with the
current SENSELEARNER distribution, implementing
the following features:
3.1 Noun Models
modelNN1: A contextual model that relies on the
first noun, verb, or adjective before the target noun,
and their corresponding part-of-speech tags.
modelNNColl: A collocation model that imple-
ments collocation-like features based on the first
word to the left and the first word to the right of the
target noun.
3.2 Verb Models
modelVB1 A contextual model that relies on the
first word before and the first word after the target
verb, and their part-of-speech tags.
modelVBColl A collocation model that implements
collocation-like features based on the first word to
the left and the first word to the right of the target
verb.
3.3 Adjective Models
modelJJ1 A contextual model that relies on the first
noun after the target adjective.
modelJJ2 A contextual model that relies on the first
word before and the first word after the target adjec-
tive, and their part-of-speech tags.
modelJJColl A collocation model that implements
collocation-like features using the first word to the
left and the first word to the right of the target adjec-
tive.
Based on previous performance in the
SENSEVAL-2 and SENSEVAL-3 evaluations,
we selected the noun and verb collocational models
for inclusion in the SUPERSENSELEARNER system
participating in the SEMEVAL all-words task.
4 SuperSenses and other Coarse-Grained
Semantic Features
A great deal of work has focused in recent years
on shallow semantic annotation tasks such as named
entity recognition and semantic role labeling. In the
former task, systems analyze text to detect mentions
of instances of coarse-grained semantic categories
such as ?person?, ?organization? and ?location?. It
seems natural to ask if this type of shallow seman-
tic information can be leveraged to improve lexical
disambiguation. Particularly, since the best perform-
ing taggers typically implement sequential decoding
schemes, e.g., Viterbi decoding, which have linear
407
complexity and can be performed quite efficiently.
In practice thus, this type of pre-processing resem-
bles POS-tagging and could provide the WSD sys-
tem with useful additional evidence.
4.1 Tagsets
We use three different tagsets. The first is the set of
WordNet supersenses (Ciaramita and Altun, 2006):
a mapping of WordNet?s synsets to 45 broad lexi-
cographers categories, 26 for nouns, 15 for verbs,
3 for adjectives and 1 for adverbs. The second
tagset is based on the ACE 2007 English data for
entity mention detection (EMD) (ACE, 2007). This
tagset defines seven entity types: Facility, Geo-
Political Entity, Location, Organization, Person, Ve-
hicle, Weapon; further subdivided in 44 subtypes.
The third tagset is derived from the BBN Entity
Corpus (BBN, 2005) which complements the Wall
Street Journal Penn Treebank with annotations of a
large set of entities: 12 named entity types (Person,
Facility, Organization, GPE, Location, Nationality,
Product, Event, Work of Art, Law, Language, and
Contact-Info), nine nominal entity types (Person,
Facility, Organization, GPE, Product, Plant, Animal,
Substance, Disease and Game), and seven numeric
types (Date, Time, Percent, Money, Quantity, Ordi-
nal and Cardinal). Several of these types are further
divided into subtypes, for a total of 105 classes.1
4.2 Taggers
We annotate the training and evaluation data using
three sequential taggers, one for each tagset. The
tagger is a Hidden Markov Model trained with the
perceptron algorithm introduced in (Collins, 2002),
which applies Viterbi decoding and is regularized
using averaging. Label to label dependencies are
limited to the previous tag (first order HMM). We
use a generic feature set for NER based on words,
lemmas, POS tags, and word shape features, in addi-
tion we use as a feature of each token the supersense
of a first (super)sense baseline. A detailed descrip-
tion of the features used and the tagger can be found
in (Ciaramita and Altun, 2006). The supersense tag-
ger is trained on the Brown sections one and two of
SemCor. The BBN tagger is trained on sections 2-
21 of the BBN corpus. The ACE tagger is trained
1BBN Corpus documentation.
on the 599 ACE 2007 training files. The accuracy
of the tagger is, approximately, 78% F-score for su-
persenses and ACE, and 87% F-score for the BBN
corpus.
4.3 Features
The taggers disregard the lemmatization of the eval-
uation data. In practice, this means that multiword
lemmas such as ?take off?, are split into their ba-
sic components. In fact, the goal of the tagger is
to guess the elements of the instances of semantic
categories by means of the usual BIO encoding. In
other words, the tagger predicts a labeled bracket-
ing of the tokens in each sentence. As an exam-
ple, the supersense tagger annotates the tokens in the
phrase ?substance abuse? as ?substanceB?noun.act?
and ?abuseI?noun.act?, although the gold standard
segmentation of the data does not identify the phrase
as one lemma. We use the labels generated in this
way as features of each token to disambiguate.
5 Feature Combination
For the final system we create a combined feature set
for each target word, consisting of the lemma, the
part of speech, the collocational SENSELEARNER
features, and the three coarse grained semantic tags
of the target word. Note that the semantic fea-
tures are represented as lemma TAG to avoid over-
generalization.
In the training stage, a feature vector is con-
structed for each sense-annotated word covered by
a semantic model. The features are model-specific,
and feature vectors are added to the training set
pertaining to the corresponding model. The label
of each such feature vector consists of the target
word and the corresponding sense, represented as
word#sense. Table 1 shows the number of feature
vectors constructed in this learning stage for each
semantic model. To annotate new text, similar vec-
tors are created for all the content-words in the raw
text. Similar to the training stage, feature vectors
are created and stored separately for each semantic
model.
Next, word sense predictions are made for all the
test examples, with a separate learning process run
for each semantic model. For learning, we are using
the Timbl memory based learning algorithm (Daele-
408
Training RESULTS
mode size Precision Recall
noun 89052 0.658 0.228
verb 48936 0.539 0.353
all 137988 0.583 0.583
Table 1: Precision and recall for the SUPERSENSE-
LEARNER semantic models.
Training RESULTS
mode size Precision Recall
noun 89052 0.666 0.233
verb 48936 0.554 0.360
all 137988 0.593 0.593
Table 2: Precision and recall for the SUPERSENSE-
LEARNER semantic models - without U labels.
mans et al, 2001), which was previously found use-
ful for the task of word sense disambiguation (Hoste
et al, 2002; Mihalcea, 2002).
Following the learning stage, each vector in the
test data set is labeled with a predicted word and
sense. If the word predicted by the learning algo-
rithm coincides with the target word in the test fea-
ture vector, then the predicted sense is used to an-
notate the test instance. Otherwise, if the predicted
word is different from the target word, no annota-
tion is produced, and the word is left for annotation
in a later stage (e.g., using the most frequent sense
back-off method).
6 Results
The SUPERSENSELEARNER system participated in
the SEMEVAL all-words word sense disambigua-
tion task. Table 1 shows the results obtained for
each part-of-speech (nouns and verbs), as well as
the overall results. We have also ran a separate
evaluation excluding the U (unknown) tag, which
is shown in Table 2. SUPERSENSELEARNER was
ranked the third among the fourteen participating
systems, proving the validity of the approach.
Acknowledgments
We would like to thank Mihai Surdeanu for provid-
ing a pre-processed version of the ACE data.
References
2007. Automatic content extraction workshop.
http://www.nist.gov/speech/tests/ace/ace07/index.htm.
2005. BBN pronoun coreference and entity type cor-
pus. Linguistic Data Consortium (LDC) catalog num-
ber LDC2005T33.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, July. Association for
Computational Linguistics.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner,
version 4.0, reference guide. Technical report, Univer-
sity of Antwerp.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. Gambl, genetic algorithm optimization
of memory-based wsd. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Barcelona, Spain, July.
V. Hoste, W. Daelemans, I. Hendrickx, and A. van den
Bosch. 2002. Evaluating the results of a memory-
based word-expert approach to unrestricted word sense
disambiguation. In Proceedings of the ACL Workshop
on ?Word Sense Disambiguatuion: Recent Successes
and Future Directions?, Philadelphia, July.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
R. Mihalcea. 2002. Instance based learning with auto-
matic feature selection applied to Word Sense Disam-
biguation. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei, Taiwan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Commu-
nication of the ACM, 38(11):39?41.
409
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410?413,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT: SubFinder: Combining Knowledge Sources for
Automatic Lexical Substitution
Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea?
Department of Computer Science and Engineering
University of North Texas
samer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.edu
Abstract
This paper describes the University of North
Texas SUBFINDER system. The system is
able to provide the most likely set of sub-
stitutes for a word in a given context, by
combining several techniques and knowl-
edge sources. SUBFINDER has successfully
participated in the best and out of ten (oot)
tracks in the SEMEVAL lexical substitution
task, consistently ranking in the first or sec-
ond place.
1 Introduction
Lexical substitution is defined as the task of identify-
ing the most likely alternatives (substitutes) for a tar-
get word, given its context (McCarthy, 2002). Many
natural language processing applications can bene-
fit from the availability of such alternative words,
including word sense disambiguation, lexical ac-
quisition, machine translation, information retrieval,
question answering, text simplification, and others.
The task is closely related to the problem of word
sense disambiguation, with the substitutes acting as
synonyms for the input word meaning. Unlike word
sense disambiguation however, lexical substitution
is not performed with respect to a given sense inven-
tory, but instead candidate synonyms are generated
?on the fly? for a given word occurrence. Thus, lexi-
cal substitution can be regarded in a way as a hybrid
task that combines word sense disambiguation and
distributional similarity, targeting the identification
of semantically similar words that fit the context.
2 A system for lexical substitution
SUBFINDER is a system able to provide the most
likely set of substitutes for a word in a given context.
?Contact author.
In SUBFINDER, the lexical substitution task is car-
ried out as a sequence of two steps. First, candidates
are extracted from a variety of knowledge sources;
so far, we experimented with WordNet (Fellbaum,
1998), Microsoft Encarta encyclopedia, Roget, as
well as synonym sets generated from bilingual dic-
tionaries, but additional knowledge sources can be
integrated as well. Second, provided a list of candi-
dates, a number of ranking methods are applied in
a weighted combination, resulting in a final list of
lexical substitutes ranked by their semantic fit with
both the input target word and the context.
3 Candidate Extraction
Candidates are extracted using several lexical re-
sources, which are combined into a larger compre-
hensive resource.
WordNet: WordNet is a large lexical database of
English, with words grouped into synonym sets
called synsets. A problem we encountered with this
resource is that often times the only candidate in the
synset is the target word itself. Thus, to enlarge the
set of candidates, we use both the synonyms and the
hypernyms of the target word. We also remove the
target word from the synset, to ensure that only vi-
able candidates are considered.
Microsoft Encarta encyclopedia: The Microsoft
Encarta is an online encyclopedia and thesaurus re-
source, which provides for each word the part of
speech and a list of synonyms. Using the part of
speech as identified in the context, we are able to ex-
tract synsets for the target word. An important fea-
ture in the Encarta Thesaurus is that the first word
in the synset acts as a definition for the synset, and
therefore disambiguates the target word. This defi-
nition is maintained as a separate entry in the com-
410
prehensive resource, and it is also added to its corre-
sponding synset.
Other Lexical Resources: We have also experi-
mented with two other lexical resources, namely the
Roget thesaurus and a thesaurus built using bilingual
dictionaries. In evaluations carried out on the devel-
opment data set, the best results were obtained using
only WordNet and Encarta, and thus these are the
resources used in the final SUBFINDER system.
All these resources entail different forms of synset
clustering. In order to merge them, we use the
largest overlap among them. It is important to note
that the choice of the first resource considered has
a bearing on the way the synsets are clustered. In
experiments ran on the development data set, the
best results were obtained using a lexical resource
constructed starting with the Microsoft Encarta The-
saurus and then mapping the WordNet synsets to it.
4 Candidate Ranking
Several ranking methods are used to score the can-
didate substitutes, as described below.
Lexical Baseline (LB): In this approach we use
the pre-existing lexical resources to provide a rank-
ing over the candidate substitutes. We rank the can-
didates based on their occurrence in the two selected
lexical resources WordNet and Encarta, with those
occurring in both resources being assigned a higher
ranking. This technique emphasizes the resources
annotators? agreement that the candidates belong in-
deed to the same synset.
Machine Translation (MT): We use machine
translation to translate the test sentences back-and-
forth between English and a second language. From
the resulting English translation, we extract the re-
placement that the machine translation engine pro-
vides for the target word. To locate the translated
word we scan the translation for any of the can-
didates (and their inflections) as obtained from the
comprehensive resource, and score the candidate
synset accordingly.
We experimented with a range of languages such
as French, Italian, Spanish, Simplified Chinese, and
German, but the best results obtained on the devel-
opment data were based on the French translations.
This could be explained because French is part of
the Romance languages family and synonyms to En-
glish words often find their roots in Latin. If we
consider again the word bright, it was translated
into French as intelligent and then translated back
into English as intelligent for obvious reasons. In
one instance, intelligent was the best replacement
for bright in the trial data. Despite the fact that we
also used Italian and Spanish (which are both Latin-
based) we can only assume that French worked bet-
ter because translation engines are better trained on
French. From the resulting English translation, we
extract the replacement that the machine translation
engine provides for the target word. To locate the
translated word we scan the translation for any of the
candidates (and their inflections) as obtained from
the comprehensive resource, and score the candidate
synset accordingly. The translation process was car-
ried out using Google and AltaVista translation en-
gines resulting in two systems MTG and MTA re-
spectively. The translation systems feature high pre-
cision when a candidate is found (about 20% of the
time), at the cost of low recall. The lexical baseline
method is therefore used when no candidates are re-
turned by the translation method.
Most Common Sense (MCS): Another method
we use for ranking candidates is to consider the
first word appearing in the first synset returned by
WordNet. When no words other than the target
word are available in this synset, the method recur-
sively searches the next synset available for the tar-
get word. In order to guarantee a sufficient number
of candidates, we use the lexical baseline method as
a baseline.
Language Model (LM): We model the semantic
fit of a candidate substitute within the given context
using a language model, expressed using the condi-
tional probability:
P (c|g) = P (c, g)/P (g) ? Count(c, g) (1)
where c represents a possible candidate and g rep-
resents the context. The probability P (g) of the
context is the same for all the candidates, hence we
can ignore it and estimate P (c|g) as the N-gram fre-
quency of the context where the target word is re-
placed by the proposed candidate. To avoid skewed
counts that can arise from the different morpholog-
ical inflections of the target word or the candidate
and the bias that the context might have toward any
specific inflection, we generalize P (c|g) to take into
account all the inflections of the selected candidate
as shown in equation 2.
Pn(c|g) ?
n
?
i=1
Count(ci, g) (2)
where n is the number of possible inflections for the
candidate c.
We use the Google N-gram dataset to calculate the
term Count(ci g). The Google N-gram corpus is a
411
collection of English N-grams, ranging from one to
five N-grams, and their respective frequency counts
observed on the Web (Brants and Franz, 2006). In
order for the model to give high preference to the
longer N-grams, while maintaining the relative fre-
quencies of the shorter N-grams (typically more fre-
quent), we augment the counts of the higher order
N-grams with the maximum counts of the lower or-
der N-grams, hence guaranteeing that the score as-
signed to an N-gram of order N is higher than the
the score of an N-gram of order N ? 1.
Semantic Relatedness using Latent Semantic
Analysis (LSA): We expect to find a strong se-
mantic relationship between a good candidate and
the target context. A relatively simple and efficient
way to measure such a relatedness is the Latent Se-
mantic Analysis (Landauer et al, 1998). Documents
and terms are mapped into a 300 dimensional latent
semantic space, providing the ability to measure the
semantic relatedness between two words or a word
and a context. We use the InfoMap package from
Stanford University?s Center for the Study of Lan-
guage and Information, trained on a collection of
approximately one million Wikipedia articles. The
rank of a candidate is given by its semantic related-
ness to the entire context sentence.
Information Retrieval (IR): Although the Lan-
guage Model approach is successful in ranking the
candidates, it suffers from the small N-gram size im-
posed by using the Google N-grams corpus. Such
a restriction is obvious in the following 5-gram ex-
ample who was a bright boy in which the context
is not sufficient to disambiguate between happy and
smart as possible candidates. As a result, we adapt
an information retrieval approach which uses all the
content words available in the given context. Similar
to the previous models, the target word in the con-
text is replaced by all the generated inflections of
the selected candidate and then queried using a web
search engine. The resulting rank represents the sum
of the total number of pages in which the candidate
or any of its inflections occur together with the con-
text. This also reflects the semantic relatedness or
the relevance of the candidate to the context.
Word Sense Disambiguation (WSD): Since pre-
vious work indicated the usefulness of word sense
disambiguation systems in lexical substitution (Da-
gan et al, 2006), we use the SenseLearner word
sense disambiguation tool (Mihalcea and Csomai,
2005) to disambiguate the target word and, accord-
ingly, to propose its synonyms as candidates.
Final System: Our candidate ranking methods are
aimed at different aspects of what constitutes a good
candidate. On one hand, we measure the semantic
relatedness of a candidate with the original context
(the LSA and WSD methods fall under this cate-
gory). On the other hand, we also want to ensure
that the candidate fits the context and leads to a well
formed English sentence (e.g., the language model
method). Given that the methods described earlier
aim at orthogonal aspects of the problem, it is ex-
pected that a combination of these will provide a
better overall ranking.
We use a voting mechanism, where we consider
the reciprocal of the rank of each candidates as given
by one of the described methods. The final score of
a candidate is given by the decreasing order of the
weighted sum of the reciprocal ranks:
score (ci) =
?
m?rankings
?m
1
rmci
To determine the weight ? of each individual
ranking we run a genetic algorithm on the develop-
ment data, optimized for the mode precision and re-
call. Separate sets of weights are obtained for the
best and oot tasks. Table 1 shows the weights of
the individual ranking methods. As expected, for
the best task, the language model type of methods
obtain higher weights, whereas for the oot task, the
semantic methods seem to perform better.
5 Results and Discussion
The SUBFINDER system participated in the best and
the oot tracks of the lexical substitution task. The
best track calls for any number of best guesses,
with the most promising one listed first. The credit
for each correct guess is divided by the number of
guesses. The oot track allows systems to make up to
10 guesses, without penalizing, and without being of
any benefit if less than 10 substitutes are provided.
The ordering of guesses in the oot metric is unim-
portant.
For both tracks, the evaluation is carried out using
precision and recall, calculated based on the number
of matching responses between the system and the
human annotators, respectively. A ?mode? evalua-
tion is also conducted, which measures the ability of
the systems to capture the most frequent response
(the ?mode?) from the gold standard annotations.
For details, please refer to the official task descrip-
tion document (McCarthy and Navigli, 2007).
Tables 2 and 3 show the results obtained by SUB-
FINDER in the best and oot tracks respectively. The
tables also show a breakdown of the results based
412
on: only target words that were not identified as
multiwords (NMWT); only substitutes that were not
identified as multiwords (NMWS); only items with
sentences randomly selected from the Internet cor-
pus (RAND); only items with sentences manually se-
lected from the Internet corpus (MAN).
WSD LSA IR LB MCS MTA MTG LM
best 34 2 64 63 56 69 38 97
oot 6 82 7 28 46 14 32 68
Table 1: Weights of the individual ranking methods
P R Mode P Mode R
OVERALL 12.77 12.77 20.73 20.73
Further Analysis
NMWT 13.46 13.46 21.63 21.63
NMWS 13.79 13.79 21.59 21.59
RAND 12.85 12.85 20.18 20.18
MAN 12.69 12.69 21.35 21.35
Baselines
WORDNET 9.95 9.95 15.28 15.28
LIN 8.84 8.53 14.69 14.23
Table 2: BEST results
P R Mode P Mode R
OVERALL 49.19 49.19 66.26 66.26
Further Analysis
NMWT 51.13 51.13 68.03 68.03
NMWS 54.01 54.01 70.15 70.15
RAND 51.71 51.71 68.04 68.04
MAN 46.26 46.26 64.24 64.24
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
Table 3: OOT results
Compared to other systems participating in this
task, our system consistently ranks on the first or
second place. SUBFINDER clearly outperforms all
the other systems for the ?mode? evaluation, show-
ing the ability of the system to find the substitute
most often preferred by the human annotators. In
addition, the system exceeds by a large margin all
the baselines calculated for the task, which select
substitutes based on existing lexical resources (e.g.,
WordNet or Lin distributional similarity).
Separate from the ?official? submission, we ran
a second experiment where we optimized the com-
bination weights targeting high precision and recall
(rather than high mode). An evaluation of the system
using this new set of weights yields a precision and
recall of 13.34 with a mode of 21.71 for the best task,
surpassing the best system according to the anony-
mous results report. For the oot task, the precision
and recall increased to 50.30, still maintaining sec-
ond place.
6 Conclusions
The lexical substitution task goes beyond simple
word sense disambiguation. To approach such a
task, we first need a good comprehensive and precise
lexical resource for candidate extraction. Secondly,
we need to semantically filter the highly diverse and
ambiguous set of candidates, while taking into ac-
count their fitness in the context in order to form
a proper linguistic expression. To accomplish this,
we built a system that incorporates lexical, semantic,
and probabilistic methods to capture both the seman-
tic similarity with the target word and the semantic
fit in the context. Compared to other systems partic-
ipating in this task, our system consistently ranks on
the first or second place. SUBFINDER clearly out-
performs all the other systems for the ?mode? eval-
uation, proving its ability to find the substitute most
often preferred by the human annotators.
Acknowledgments
This work was supported in part by the Texas Ad-
vanced Research Program under Grant #003594.
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for many useful discussions and
feedback on this work.
References
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the In-
ternational Conference on Computational Linguistics
ACL/COLING 2006.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
D. McCarthy and R. Navigli. 2007. The semeval English
lexical substitution task. In Proceedings of the ACL
Semeval workshop.
D. McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
413
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 210?218,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Encyclopedic Knowledge for Automatic Topic Identification
Kino Coursey
University of North Texas and Daxtron Laboratories, Inc.
kino@daxtron.com
Rada Mihalcea and William Moen
University of North Texas
rada,wemoen@unt.edu
Abstract
This paper presents a method for automatic
topic identification using an encyclopedic
graph derived from Wikipedia. The sys-
tem is found to exceed the performance of
previously proposed machine learning algo-
rithms for topic identification, with an annota-
tion consistency comparable to human anno-
tations.
1 Introduction
With exponentially increasing amounts of text be-
ing generated, it is important to find methods that
can annotate and organize documents in meaning-
ful ways. In addition to the content of the document
itself, other relevant information about a document
such as related topics can often enable a faster and
more effective search or classification. Document
topics have been used for a long time by librarians to
improve the retrieval of a document, and to provide
background or associated information for browsing
by human users. They can also assist search, back-
ground information gathering and contextualization
tasks, and enhanced relevancy measures.
The goal of the work described in this paper is to
automatically find topics that are relevant to an input
document. We refer to this task as ?topic identifica-
tion? (Medelyan and Witten, 2008). For instance,
starting with a document on ?United States in the
Cold War,? we want to identify relevant topics such
as ?history,? ?Global Conflicts,? ?Soviet Union,? and
so forth. We propose an unsupervised method for
topic identification, based on a biased graph cen-
trality algorithm applied to a large knowledge graph
built from Wikipedia.
The task of topic identification goes beyond key-
word extraction (Mihalcea and Csomai, 2007), since
relevant topics may not be necessarily mentioned in
the document, and instead have to be obtained from
some repositories of external knowledge. The task
is also different from text classification (Gabrilovich
and Markovitch, 2006), since the topics are either
not known in advance or are provided in the form of
a controlled vocabulary with thousands of entries,
and thus no classification can be performed. In-
stead, with topic identification, we aim to find topics
(or categories1) that are relevant to the document at
hand, which can be used to enrich the content of the
document with relevant external knowledge.
2 Dynamic Ranking of Topic Relevance
Our method is based on the premise that external
encyclopedic knowledge can be used to identify rel-
evant topics for a given document.
The method consists of two main steps. In the first
step, we build a knowledge graph of encyclopedic
concepts based on Wikipedia, where the nodes in the
graph are represented by the entities and categories
that are defined in this encyclopedia. The edges be-
tween the nodes are represented by their relation of
proximity inside the Wikipedia articles. The graph
is built once and then it is stored offline, so that it
can be efficiently use for the identification of topics
in new documents.
In the second step, for each input document, we
first identify the important encyclopedic concepts in
the text, and thus create links between the content of
the document and the external encyclopedic graph.
Next, we run a biased graph centrality algorithm on
the entire graph, so that all the nodes in the exter-
nal knowledge repository are ranked based on their
relevance to the input document. We use a variation
1Throughout the paper, we use the terms ?topic? and ?cate-
gory? interchangeably.
210
of the PageRank (Brin and Page, 1998) algorithm,
which accounts for both the relation between the
nodes in the document and the encyclopedic graph,
as well as the relation between the nodes in the en-
cyclopedic graph itself.
In the following, we first describe the structure
of Wikipedia, followed by a brief description of the
Wikify! system that automatically identifies the en-
cyclopedic concepts in a text, and finally a descrip-
tion of the dynamic ranking process on the encyclo-
pedic graph.
2.1 Wikipedia
Wikipedia (http://wikipedia.org) is a free online en-
cyclopedia, representing the outcome of a continu-
ous collaborative effort of a large number of vol-
unteer contributors. Virtually any Internet user can
create or edit a Wikipedia webpage, and this ?free-
dom of contribution? has a positive impact on both
the quantity (fast-growing number of articles) and
the quality (potential mistakes are quickly corrected
within the collaborative environment) of this re-
source.
Wikipedia has grown to become one of the largest
online repositories of encyclopedic knowledge, with
millions of articles available for a large number of
languages. In fact, Wikipedia editions are available
for more than 250 languages, with a number of en-
tries varying from a few pages to close to three mil-
lion articles per language.
The basic entry in Wikipedia is an article (or
page), which defines an entity or an event, and con-
sists of a hypertext document with hyperlinks to
other pages within or outside Wikipedia. The role
of the hyperlinks is to guide the reader to pages
that provide additional information about the enti-
ties or events mentioned in an article. Each article
in Wikipedia is uniquely referenced by an identifier,
which consists of one or more words separated by
spaces or underscores, and occasionally a parenthet-
ical explanation. The current version of the English
Wikipedia consists of about 2.75 million articles.
In addition to articles, Wikipedia also includes a
large number of categories, which represent topics
that are relevant to a given article (the July 2008 ver-
sion of Wikipedia includes about 390,000 such cate-
gories). The category links are organized hierarchi-
cally, and vary from broad topics such as ?history?
or ?games? to highly focused topics such as ?mili-
tary history of South Africa during World War II? or
Figure 1: A snapshot from the encyclopedic graph.
?role-playing game publishing companies.?
We use the entire English Wikipedia to build an
encyclopedic graph for use in the topic identifica-
tion process. The nodes in the graph are represented
by all the article and category pages in Wikipedia,
and the edges between the nodes are represented by
their relation of proximity inside the articles. The
graph contains 5.8 million nodes, and 65.5 million
edges. Figure 1 shows a small section of the knowl-
edge graph, as built starting with the article on ?Cor-
pus Linguistics?.
2.2 Wikify!
In order to automatically identify the important en-
cyclopedic concepts in an input text, we use the un-
supervised system Wikify! (Mihalcea and Csomai,
2007), which identifies the concepts in the text that
are likely to be highly relevant (i.e., ?keywords?)
for the input document, and links them to Wikipedia
concepts.
Wikify! works in three steps, namely: (1) candi-
date extraction, (2) keyword ranking, and (3) word
sense disambiguation. The candidate extraction step
parses the input document and extracts all the pos-
sible n-grams that are also present in the vocabulary
used in the encyclopedic graph (i.e., anchor texts for
links inside Wikipedia or article or category titles).
Next, the ranking step assigns a numeric value to
each candidate, reflecting the likelihood that a given
candidate is a valuable keyword. Wikify! uses a
?keyphraseness? measure to estimate the probabil-
ity of a term W to be selected as a keyword in a
211
document, by counting the number of documents
where the term was already selected as a keyword
count(Dkey) divided by the total number of docu-
ments where the term appeared count(DW ). These
counts are collected from all the Wikipedia articles.
P (keyword|W ) ? count(Dkey)count(DW )
(1)
This probability can be interpreted as ?the more
often a term was selected as a keyword among its
total number of occurrences, the more likely it is that
it will be selected again.?
Finally, a simple word sense disambiguation
method is applied, which identifies the most likely
article in Wikipedia to which a concept should
be linked to. This step is trivial for words or
phrases that have only one corresponding article in
Wikipedia, but it requires an explicit disambiguation
step for those words or phrases that have multiple
meanings (e.g., ?plant?) and thus multiple candidate
pages to link to. The algorithm is based on statistical
methods that identify the frequency of meanings in
text, combined with symbolic methods that attempt
to maximize the overlap between the current docu-
ment and the candidate Wikipedia articles. See (Mi-
halcea and Csomai, 2007) for more details.
2.3 Biased Ranking of the Wikipedia Graph
Starting with the graph of encyclopedic knowledge,
and knowing the nodes that belong to the input doc-
ument, we want to rank all the nodes in the graph
so that we obtain a score that indicates their impor-
tance relative to the given document. We can do this
by using a graph-ranking algorithm biased toward
the nodes belonging to the input document.
Graph-based ranking algorithms such as PageR-
ank are a way of deciding the importance of a vertex
within a graph, based on global information recur-
sively drawn from the entire graph. One formula-
tion is in terms of a random walk through a directed
graph. A ?random surfer? visits nodes of the graph,
and has some probability of jumping to some other
random node of the graph, and the remaining proba-
bility of continuing their walk from the current node
to one in its outdegree list. The rank of a node is an
indication of the probability that the surfer would be
found at that node at any given time.
Formally, let G = (V,E) be a directed graph with
the set of vertices V and set of edges E, where E is
a subset of V ? V . For a given vertex Vi, let In(Vi)
be the set of vertices that point to it (predecessors),
and let Out(Vi) be the set of vertices that vertex Vi
points to (successors). The score of a vertex Vi is
defined as follows (Brin and Page, 1998):
S(Vi) = (1? d) + d ?
?
j?In(Vi)
1
|Out(Vj)|
S(Vj) (2)
where d is a damping factor usually set to 0.85.
Given the ?random surfer? interpretation of the
ranking process, the (1 ? d) portion represents the
probability that a surfer will jump to a given node
from any other node at random, and the summation
portion indicates that the process will enter the node
via edges directly connected to it.
We introduce a bias in this graph-based rank-
ing algorithm by extending the framework of per-
sonalization of PageRank proposed by (Haveliwala,
2002). We modify the formula so that the (1 ? d)
component also accounts for the importance of the
concepts found in the input document, and it is sup-
pressed for all the nodes that are not found in the
input document.
S(Vi) = (1?d)?Bias(Vi)+d?
?
j?In(Vi)
1
|Out(Vj)|
S(Vj)
(3)
where Bias(Vi) is only defined for those nodes ini-
tially identified in the input document:
Bias(Vi) = f(Vi)?
j?InitalNodeSet
f(Vj)
and 0 for all other nodes in the graph.
InitalNodeSet is the set of nodes belonging
to the input document.
Note that f(Vi) can vary in complexity from a de-
fault value of 1 to a complex knowledge-based es-
timation. In our implementation, we use a combi-
nation of the ?keyphraseness? score assigned to the
node Vi and its distance from the ?Fundamental?
category in Wikipedia.
The use of the Bias assigned to each node means
the surfer random jumps will be limited to only those
nodes connected to the original query. Thus the
graph-ranking process becomes biased and focused
on those topics directly related to the input. It also
accumulates activation at those nodes not directly
found in the input text, but linked through indirect
means, thus reinforcing the nodes where patterns of
activation intersect and creating a constructive in-
terference pattern in the network. These reinforced
nodes are the ?implied related topics? of the text.
212
3 Illustration
To illustrate the ranking process, consider as an ex-
ample the following sentence ?The United States
was involved in the Cold War.?
First the text is passed through the Wikify! sys-
tem, which returns the articles ?United States? and
?Cold War.? Taking into account their ?keyphrase-
ness? as calculated by Wikify!, the selections are
given an initial bias of 0.5492 (?United States?) and
0.4508 (?Cold War?).
After the first iteration the initial activation
spreads out into the encyclopedic graph, the nodes
find a direct connection to one another, and cor-
respondingly their scores are changed to 0.3786
(?United States?) and 0.3107 (?Cold War?). After
the second iteration, new nodes are identified from
the encyclopedic graph, a subset of which is shown
in Figure2. The process will eventually continue for
several iterations until the scores of the nodes do not
change. The nodes with the highest scores in the
final graph are considered to be the most closely re-
lated to the input sentence, and thus selected as rel-
evant topics.
Figure 2: Sub-graph between ?United States? and ?Cold
War?
In order to see the effect of the initial bias, con-
sider as an example the ranking of the nodes in
the encyclopedic graph when biased with the sen-
tence ?The United States was involved in the Cold
War,? versus the sentence ?Microsoft applies Com-
puter Science.? A comparison between the scores of
the nodes when activated by each of these sentences
is shown in Table 1.
Wikipedia entry US/CW MS/CS Diff.
A: United States 0.393636 0.006578 0.387058
C: Computer Science 0.000004 0.003576 -0.003571
A: World War II 0.007102 0.003674 0.003428
A: United Kingdom 0.005346 0.002670 0.002676
C: Microsoft 0.000001 0.001839 -0.001837
C: Cold War 0.001695 0.000006 0.001689
C: Living People 0.000835 0.002223 -0.001387
C: Mathematics 0.000029 0.001337 -0.001307
C: Computing 0.000008 0.001289 -0.001280
C: Computer Pioneers 0.000002 0.001238 -0.001235
Table 1: Node ranking differences when the encyclo-
pedic graph is biased with different inputs: (1) ?United
States? and ?Cold War? (US/CW) vs. (2) ?Microsoft?
and ?Computer Science? (MS/CS). The nodes are either
article pages (A) or category pages (C).
4 Experiments
In order to measure the effectiveness of the topic
ranking process, we run three sets of experiments,
aimed at measuring the relevancy of the automati-
cally identified topics with respect to manually an-
notated gold standard data sets.
In the first experiment, the identification of the
important concepts in the input text (used to bias the
topic ranking process) is performed manually, by the
Wikipedia users. In the second and third experiment,
the identification of these important concepts is done
automatically, by the Wikify! system. In all the ex-
periments, the ranking of the concepts from the en-
cyclopedic graph is done automatically by using the
dynamic ranking process described in Section 2.
In the first two experiments, we use a data set
consisting of 150 articles from Wikipedia, which
have been explicitly removed from the encyclope-
dic graph. All the articles in this data set include
manual annotations of the relevant categories, as as-
signed by the Wikipedia users, against which we
can measure the quality of the automatic topic as-
signments. The 150 articles have been randomly se-
lected while following the constraint that they each
contain at least three article links and at least three
category links. Our task is to rediscover the relevant
categories for each page. Note that the task is non-
trivial, since there are approximately 390,000 cate-
gories to choose from. We evaluate the quality of
our system through the standard measures of preci-
213
sion and recall.
4.1 Manual Annotation of the Input Text
In this first experiment, the articles in the gold stan-
dard data set alo include manual annotations of the
important concepts in the text, i.e., the links to other
Wikipedia articles as created by the Wikipedia users.
Thus, in this experiment we only measure the accu-
racy of the dynamic topic ranking process, without
interference from the Wikify! system.
There are two main parameters that can be set dur-
ing a system run. First, the set of initial nodes used
as bias in the ranking can include: (1) the initial set
of articles linked to by the original document (via
the Wikipedia links); (2) the categories listed in the
articles linked to by the original document2; and (3)
both. Second, the dynamic ranking process can be
run through propagation on an encyclopedic graph
that includes (1) all the articles from Wikipedia; (2)
all the categories from Wikipedia; or (3) all the arti-
cles and the categories from Wikipedia.
Figures 3, 4 and 5 show the precision, recall and
F-measure obtained for the various settings. In the
plots, Bias and Propagate indicate the selections
made for the two parameters, which can be each set
to Articles, Categories, or Both. Each of these
correspond to the options listed before.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0  20  40  60  80  100
P
re
ci
si
on
Top N topics returned
BiasArticles- PropCategories
BiasCategories- PropArticles PropCategories
BiasArticles BiasCategories- PropCategories
BiasArticles- PropArticles PropCategories
BiasArticles BiasCategories- PropArticles
BiasCategories- PropArticles
BiasArticles- PropArticles
BiasArticles BiasCategories- PropArticles PropCategories
BiasCategories- PropCategories
Baseline
Figure 3: Precision for manual input text annotations.
As seen in the figures, the best results are obtained
for a setting where both the initial bias and the prop-
agation include all the available nodes, i.e., both ar-
ticles and categories. Although the primary task is
2These should not be confused with the categories included
in the document itself, which represent the gold standard anno-
tations and are not used at any point.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  20  40  60  80  100
R
ec
al
l
Top N topics returned
BiasArticles- PropCategories
BiasCategories- PropArticles PropCategories
BiasArticles BiasCategories- PropCategories
BiasArticles- PropArticles PropCategories
BiasArticles BiasCategories- PropArticles
BiasCategories- PropArticles
BiasArticles- PropArticles
BiasArticles BiasCategories- PropArticles PropCategories
BiasCategories- PropCategories
Baseline
Figure 4: Recall for manual input text annotations.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0  20  40  60  80  100
F
-M
ea
su
re
Top N topics returned
BiasArticles- PropCategories
BiasCategories- PropArticles PropCategories
BiasArticles BiasCategories- PropCategories
BiasArticles- PropArticles PropCategories
BiasArticles BiasCategories- PropArticles
BiasCategories- PropArticles
BiasArticles- PropArticles
BiasArticles BiasCategories- PropArticles PropCategories
BiasCategories- PropCategories
Baseline
Figure 5: F-measure when using Wikipedia article anno-
tations.
the identification of the categories, the addition of
the article links improves the system performance.
To place results in perspective, we also calculate a
baseline (labeled as ?Baseline? in the plots), which
selects by default all the categories listed in the arti-
cles linked to by the original document. Each base-
line article assigns 1/N to each of its N possible
categories, with categories pointed to by multiple ar-
ticles receiving the summation.
4.2 Automatic Annotation of the Input Text
The second experiment is similar to the first one, ex-
cept that rather than using the manual annotations
of the important concepts in the input document,
we use instead the Wikify! system that automat-
ically identifies these important concepts by using
the method briefly described in Section 2.2. The ar-
ticle links identified by Wikify! are treated in the
same way as the human anchor annotations from the
previous experiment. In this experiment, we have
214
an additional parameter, which consists of the per-
centage of links selected by Wikify! out of the total
number of words in the document. We refer to this
parameter as keyRatio. The higher the keyRatio, the
more terms are added, but also the higher the poten-
tial of noise due to mis-disambiguation.
Figures 6, 7 and 8 show the effect of varying the
value of the keyRatio parameter used by Wikify! has
on the precision, recall and F-measure of the system.
Note that in this experiment, we only use the best
setting for the other two parameters as identified in
the previous experiment, namely an initial bias and
a propagation step that include all available nodes,
i.e., both articles and categories.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  20  40  60  80  100
P
re
ci
si
on
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 6: Precision for automatic input text annotations
(Wikipedia data set)
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0  20  40  60  80  100
R
ec
al
l
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 7: Recall for automatic input text annotations
(Wikipedia data set)
The system?s best performance occurs for a
keyRatio of 0.04 to 0.06, which coincides with the
ratio found optimal in previous experiments using
the Wikify! system (Mihalcea and Csomai, 2007).
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0.09
 0.1
 0.11
 0  20  40  60  80  100
F
-M
ea
su
re
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 8: F-measure for automatic input text annotations
(Wikipedia data set)
As before, we also calculate a baseline, which se-
lects by default all the categories listed in the articles
linked to by the original document, with the links
being automatically identified with the Wikify! sys-
tem. The baseline is calculated for a keyRatio of
0.04, which is one of the values that were found to
work well for the ranking system itself and in previ-
ous Wikify! experiments.
Overall, the system manages to find many relevant
topics for the documents in the evaluation data set,
despite the large number of candidate topics (close
to 390,000). Our system exceeds the baseline by a
large margin, demonstrating the usefulness of using
the biased ranking on the encyclopedic graph.
4.3 Article Selection for Computer Science
Texts
In the third experiment, we use again the Wikify!
system to annotate the input documents, but this
time we run the evaluations on a data set consist-
ing of computer science documents. We use the data
set introduced in previous work on topic identifica-
tion (Medelyan and Witten, 2008), where 20 doc-
uments in the field of computer science were inde-
pendently annotated by 15 teams of two computer
science undergraduates. The teams were asked to
read the texts and assign to each of them the title
of the five Wikipedia articles they thought were the
most relevant and the other groups would also se-
lect. Thus, the consistency of the annotations was
an important measure for this data set. (Medelyan
and Witten, 2008) define consistency as a measure
of agreement:
Consistency = 2CA+B
215
where A and B are the number of terms assigned
by two indexing teams, and C is the number of
terms they have in common. In the annotations ex-
periments reported in (Medelyan and Witten, 2008),
the human teams consistency ranged from 21.4% to
37.1%, with 30.5% being the average.3
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  20  40  60  80  100
P
re
ci
si
on
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 9: Precision for automatic input text annotations
(Waikato data set)
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  20  40  60  80  100
R
ec
al
l
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 10: Recall for automatic input text annotations
(Waikato data set)
Figures 10, 9, 11 and 12 show the performance
of our system on this data set, by using the Wikify!
annotations for the initial bias, and then propagat-
ing to both articles and categories. The plots also
show a baseline that selects all the articles automat-
ically identified in the original document by using
the Wikify! system with a keyRatio set to 0.04.
3The consistency for one team is measured as the average of
the consistencies with the remaining 14 teams.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  20  40  60  80  100
F
-M
ea
su
re
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 11: F-measure for automatic input text annota-
tions (Waikato data set)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100
C
on
si
st
en
cy
Top N topics returned
keyRatio= 0.02
keyRatio= 0.04
keyRatio= 0.08
keyRatio= 0.16
Baseline keyRatio= 0.04
Figure 12: Consistency for automatic input text annota-
tions (Waikato data set)
When selecting the top five topics returned by our
system (the same number of topics as provided by
the human teams), the average consistency with re-
spect to the 15 human teams was measured at 34.5%,
placing it between the 86% and 93% percentile of
the human participants, with only two human teams
doing better. We can compare this result with the
one reported in previous work for the same data
set. Using a machine learning system, (Medelyan
and Witten, 2008) reported a consistency of 30.5%.
Thus, our result of 34.5% is significantly better, de-
spite the fact that our method is unsupervised.
In a second evaluation, we also considered the
union of all the terms assigned by the 15 teams. On
average, each document was assigned 35.5 differ-
ent terms by the human teams. If allowed to pro-
vide more annotations, our system peaks with a con-
216
sistency of 66.6% for the top 25 topics returned.
The system has the ability to identify possible rele-
vant alternative topics using the comprehensive cata-
log of Wikipedia computer science articles and their
possible associations. A human team may not nec-
essarily consider all of the possibilities or even be
aware that some of the articles, possibly known and
used by the other teams, exist.
5 Related Work
The work closest to ours is perhaps the one de-
scribed in (Medelyan and Witten, 2008), where top-
ics relevant to a given document are automatically
selected by using a machine learning system. Unlike
our unsupervised approach, (Medelyan and Witten,
2008) learn what makes a good topic by training on
previously annotated data.
Also related is the Wikify! system concerned
with the automatic annotation of documents with
Wikipedia links (Mihalcea and Csomai, 2007).
However, Wikify! is purely extractive, and thus it
cannot identify important topics or articles that are
not explicitly mentioned in the input text.
Explicit semantic analysis (Gabrilovich and
Markovitch, 2006) was also introduced as a way to
determine the relevancy of the Wikipedia articles
with respect to a given input text. The resulting
vector however is extremely large, and while it was
found useful for the task of text classification with a
relatively small number of categories, it would be
difficult to adapt for topic identification when the
number of possible topics grows beyond the approx-
imately 390,000 under consideration. In a similar
line of work, (Bodo et al, 2007) examined the use
of Wikipedia and latent semantic analysis for the
purposes of text categorization, but reported nega-
tive results when used for the categorization of the
Reuters-21578 dataset.
Others are exploring the use of graph propagation
for deriving semantic information. (Hughes and Ra-
mage, 2007) described the use of a biased PageRank
over the WordNet graph to compute word pair se-
mantic relatedness using the divergence of the prob-
ability values over the graph created by each word.
(Ollivier and Senellart, 2007) describes a method to
determine related Wikipedia article using a Markov
chain derived value called the green measure. Dif-
ferences exist between the PageRank based meth-
ods used as a baseline in their work and the method
proposed here, since our system can use the content
of the article, multiple starting points, and tighter
control of the random jump probability via the bias
value. Finally, (Syed et al, 2008) reported positive
results by using various methods for topic prediction
including the use of text similarity and spreading ac-
tivation. The method was tested by using randomly
selected Wikipedia articles, where in addition to the
categories listed on a Wikipedia page, nearby sub-
suming categories were also included as acceptable.
6 Conclusions and Future Work
In this paper, we introduced a system for automatic
topic identification, which relies on a biased graph
centrality algorithm applied on a richly intercon-
nected encyclopedic graph built from Wikipedia.
Experiments showed that the integration of ency-
clopedic knowledge consistently adds useful infor-
mation when compared to baselines that rely exclu-
sively on the text at hand. In particular, when tested
on a data set consisting of documents manually an-
notated with categories by Wikipedia users, the top-
ics identified by our system were found useful as
compared to the manual annotations. Moreover, in
a second evaluation on a computer science data set,
the system exceeded the performance of previously
proposed machine learning algorithms, which is re-
markable given the fact that our system is unsuper-
vised. In terms of consistency with manual anno-
tations, our system?s performance was found to be
comparable to human annotations, with only two out
of 15 teams scoring better than our system.
The system provides a means to generate a dy-
namic ranking of topics in Wikipedia within a
framework that has the potential to utilize knowl-
edge or heuristics through additional resources (like
ontologies) converted to graph form. This capabil-
ity is not present in resources like search engines
that provide access to a static ranking of Wikipedia.
Future work will examine the integration of addi-
tional knowledge sources and the application of the
method for metadata document annotations.
Acknowledgments
This work has been partially supported by an award
#CR72105 from the Texas Higher Education Coor-
dinating Board and by an award from Google Inc.
The authors are grateful to the Waikato group for
making their data set available.
217
References
Z. Bodo, Z. Minier, and L. Csato. 2007. Text categoriza-
tion experiments using Wikipedia. In Proceedings of
the International Conference on Knowledge Engineer-
ing ,Principles and Techniques, Cluj-Napoca (Roma-
nia).
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
E. Gabrilovich and S. Markovitch. 2006. Overcoming
the brittleness bottleneck using Wikipedia: Enhancing
text categorization with encyclopedic knowledge. In
Proceedings of the National Conference on Artificial
Intelligence (AAAI), Boston.
T. Haveliwala. 2002. Topic-sensitive PageRank. In
Proceedings of the Eleventh International World Wide
Web Conference, May.
T. Hughes and D. Ramage. 2007. Lexical semantic
knowledge with random graph walks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Prague, Czech Republic.
O. Medelyan and I. H. Witten. 2008. Topic indexing
with Wikipedia. In Proceedings of the AAAI WikiAI
workshop.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In Proceedings
of the Sixteenth ACM Conference on Information and
Knowledge Management, Lisbon, Portugal.
Y. Ollivier and P. Senellart. 2007. Finding related pages
using green measures: An illustration with wikipedia.
In Association for the Advancement of Artificial In-
telligence Conference on Artificial Intelligence (AAAI
2007).
Z. Syed, T. Finin, and A. Joshi. 2008. Wikipedia as an
Ontology for Describing Documents. In Proceedings
of the Second International Conference on Weblogs
and Social Media. AAAI Press, March.
218
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76?81,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
Ravi Sinha
University of North Texas
ravisinha@unt.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution task,
which is based on the English Lexical Substi-
tution task run at SemEval-2007. In the En-
glish version of the task, annotators and sys-
tems had to find an alternative substitute word
or phrase for a target word in context. In this
paper we propose a task where the target word
and contexts will be in English, but the substi-
tutes will be in Spanish. In this paper we pro-
vide background and motivation for the task
and describe how the dataset will differ from
a machine translation task and previous word
sense disambiguation tasks based on parallel
data. We describe the annotation process and
how we anticipate scoring the system output.
We finish with some ideas for participating
systems.
1 Introduction
The Cross-Lingual Lexical Substitution task is
based on the English Lexical Substitution task run at
SemEval-2007. In the 2007 English Lexical Substi-
tution Task, annotators and systems had to find an al-
ternative substitute word or phrase for a target word
in context. In this cross-lingual task the target word
and contexts will be in English, but the substitutes
will be in Spanish.
An automatic system for cross-lingual lexical sub-
stitution would be useful for a number of applica-
tions. For instance, such a system could be used
to assist human translators in their work, by provid-
ing a number of correct translations that the human
translator can choose from. Similarly, the system
could be used to assist language learners, by pro-
viding them with the interpretation of the unknown
words in a text written in the language they are learn-
ing. Last but not least, the output of a cross-lingual
lexical substitution system could be used as input to
existing systems for cross-language information re-
trieval or automatic machine translation.
2 Background: The English Lexical
Substitution Task
The English Lexical substitution task (hereafter re-
ferred to as LEXSUB) was run at SemEval-2007 fol-
lowing earlier ideas on a method of testing WSD
systems without predetermining the inventory (Mc-
Carthy, 2002). The issue of which inventory is ap-
propriate for the task has been a long standing is-
sue for debate, and while there is hope that coarse-
grained inventories will allow for increased system
performance (Ide and Wilks, 2006) we do not yet
know if these will make the distinctions that will
most benefit practical systems (Stokoe, 2005) or re-
flect cognitive processes (Kilgarriff, 2006). LEXSUB
was proposed as a task which, while requiring con-
textual disambiguation, did not presuppose a spe-
cific sense inventory. In fact, it is quite possible to
use alternative representations of meaning (Schu?tze,
1998; Pantel and Lin, 2002).
The motivation for a substitution task was that it
would reflect capabilities that might be useful for
natural language processing tasks such as paraphras-
ing and textual entailment, while only focusing on
one aspect of the problem and therefore not requir-
ing a complete system that might mask system capa-
bilities at a lexical level and at the same time make
76
participation in the task difficult for small research
teams.
The task required systems to produce a substitute
word for a word in context. For example a substitute
of tournament might be given for the second oc-
currence of match (shown in bold) in the following
sentence:
The ideal preparation would be a light meal
about 2-2 1/2 hours pre-match, followed by a
warm-up hit and perhaps a top-up with extra fluid
before the match.
In LEXSUB, the data was collected for 201 words
from open class parts-of-speech (PoS) (i.e. nouns,
verbs, adjectives and adverbs). Words were selected
that have more than one meaning with at least one
near synonym. Ten sentences for each word were
extracted from the English Internet Corpus (Sharoff,
2006). There were five annotators who annotated
each target word as it occurred in the context of a
sentence. The annotators were each allowed to pro-
vide up to three substitutes, though they could also
provide a NIL response if they could not come up
with a substitute. They had to indicate if the target
word was an integral part of a multiword.
A development and test dataset were provided,
but no training data. Any system that relied on train-
ing data, such as sense annotated corpora, had to use
resources available from other sources. The task had
eight participating teams. Teams were allowed to
submit up to two systems and there were a total of
ten different systems. The scoring was conducted
using recall and precision measures using:
? the frequency distribution of responses from
the annotators and
? the mode of the annotators (the most frequent
response).
The systems were scored using their best guess as
well as an out-of-ten score which allowed up to 10
attempts. 1 The results are reported in McCarthy and
Navigli (2007) and in more detail in McCarthy and
Navigli (in press).
1The details are available at
http://nlp.cs.swarthmore.edu/semeval/tasks/
task10/task10documentation.pdf.
3 Motivation and Related Work
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained al-
ternative translation records of typical usages of a
test word, also referred to as a translation mem-
ory. Systems could either select the most appropri-
ate translation memory record for each instance and
were scored against a gold-standard set of annota-
tions, or they could provide a translation that was
scored by translation experts after the results were
submitted. In contrast to this work, we propose to
provide actual translations for target instances in ad-
vance, rather than predetermine translations using
lexicographers or rely on post-hoc evaluation, which
does not permit evaluation of new systems after the
competition.
Previous standalone WSD tasks based on parallel
data have obtained distinct translations for senses as
listed in a dictionary (Ng and Chan, 2007). In this
way fine-grained senses with the same translations
can be lumped together, however this does not fully
allow for the fact that some senses for the same
words may have some translations in common but
also others that are not. An example from Resnik
and Yarowsky (2000) (table 4 in that paper) is the
first two senses from WordNet for the noun interest:
WordNet sense Spanish Translation
monetary e.g. on loan intere?s, re?dito
stake/share intere?s,participacio?n
For WSD tasks, a decision can be made to lump
senses with such overlap, or split them using the dis-
tinctive translation and then use the distinctive trans-
lations as a sense inventory. This sense inventory is
then used to collect training from parallel data (Ng
and Chan, 2007). We propose that it would be in-
teresting to collect a dataset where the overlap in
translations for an instance can remain and that this
will depend on the token instance rather than map-
ping to a pre-defined sense inventory. Resnik and
Yarowsky (2000) also conducted their experiments
using words in context, rather than a predefined
77
sense-inventory as in (Ng and Chan, 2007; Chan and
Ng, 2005), however in these experiments the anno-
tators were asked for a single preferred translation.
We intend to allow annotators to supply as many
translations as they feel are equally valid. This will
allow us to examine more subtle relationships be-
tween usages and to allow partial credit to systems
which get a close approximation to the annotators?
translations. Unlike a full blown machine transla-
tion task (Carpuat and Wu, 2007), annotators and
systems will not be required to translate the whole
context but just the target word.
4 The Cross-Lingual Lexical Substitution
Task
Here we discuss our proposal for a Cross-Lingual
Lexical Substitution task. The task will follow LEX-
SUB except that the annotations will be translations
rather than paraphrases.
Given a target word in context, the task is to pro-
vide several correct translations for that word in a
given language. We will use English as the source
language and Spanish as the target language. Mul-
tiwords are ?part and parcel? of natural language.
For this reason, rather than try and filter multiwords,
which is very hard to do without assuming a fixed
inventory, 2 we will ask annotators to indicate where
the target word is part of a multiword and what that
multiword is. This way, we know what the substitute
translation is replacing.
We will provide both development and test sets,
but no training data. As for LEXSUB, any sys-
tems requiring data will need to obtain it from other
sources. We will include nouns, verbs, adjectives
and adverbs in both development and test data. Un-
like LEXSUB, the annotators will be told the PoS of
the current target word.
4.1 Annotation
We are going to use four annotators for our task, all
native Spanish speakers from Mexico, with a high
level of proficiency in English. The annotation in-
terface is shown in figure 1. We will calculate inter-
tagger agreement as pairwise agreement between
2The multiword inventories that do exist are far from com-
plete.
sets of substitutes from annotators, as was done in
LEXSUB.
4.2 An Example
One significant outcome of this task is that there
will not necessarily be clear divisions between us-
ages and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This will mean that there can
be usages that overlap to different extents with each
other but do not have identical translations. An ex-
ample from our preliminary annotation trials is the
target adverb severely. Four sentences are shown in
figure 2 with the translations provided by one an-
notator marked in italics and {} braces. Here, all
the token occurrences seem related to each other in
that they share some translations, but not all. There
are sentences like 1 and 2 that appear not to have
anything in common. However 1, 3, and 4 seem to
be partly related (they share severamente), and 2, 3,
and 4 are also partly related (they share seriamente).
When we look again, sentences 1 and 2, though not
directly related, both have translations in common
with sentences 3 and 4.
4.3 Scoring
We will adopt the best and out-of-ten precision and
recall scores from LEXSUB. The systems can supply
as many translations as they feel fit the context. The
system translations will be given credit depending
on the number of annotators that picked each trans-
lation. The credit will be divided by the number of
annotator responses for the item and since for the
best score the credit for the system answers for an
item is also divided by the number of answers the
system provides, this allows more credit to be given
to instances where there is less variation. For that
reason, a system is better guessing the translation
that is most frequent unless it really wants to hedge
its bets. Thus if i is an item in the set of instances
I , and Ti is the multiset of gold standard translations
from the human annotators for i, and a system pro-
vides a set of answers Si for i, then the best score
for item i will be:
best score(i) =
?
s?Si frequency(s ? Ti)
|Si| ? |Ti|
(1)
78
Figure 1: The Cross-Lingual Lexical Substitution Interface
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely stressed
by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now. {altamente,
seriamente, exageradamente}
Figure 2: Translations from one annotator for the adverb severely
Precision is calculated by summing the scores for
each item and dividing by the number of items that
the system attempted whereas recall divides the sum
of scores for each item by |I|. Thus:
best precision =
?
i best score(i)
|i ? I : defined(Si)| (2)
best recall =
?
i best score(i)
|I| (3)
The out-of-ten scorer will allow up to ten system
responses and will not divide the credit attributed
to each answer by the number of system responses.
This allows the system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select a
perfectly good translation that the annotators had not
thought of. By allowing up to ten translations in the
out-of-ten task the systems can hedge their bets to
find the translations that the annotators supplied.
oot score(i) =
?
s?Si frequency(s ? Ti)
|Ti|
(4)
oot precision =
?
i oot score(i)
|i ? I : defined(Si)| (5)
79
oot recall =
?
i oot score(i)
|I| (6)
We will refine the scores before June 2009 when
we will release the development data for this cross-
lingual task. We note that there was an issue that the
original LEXSUB out-of-ten scorer allowed dupli-
cates (McCarthy and Navigli, in press). The effect
of duplicates is that systems can get inflated scores
because the credit for each item is not divided by
the number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (in press) describe this oversight, identify
the systems that had included duplicates and explain
the implications. For our task there is an option for
the out-of-ten score. Either:
1. we remove duplicates before scoring or,
2. we allow duplicates so that systems can boost
their scores with duplicates on translations with
higher probability
We will probably allow duplicates but make this
clear to participants.
We may calculate additional best and out-of-ten
scores against the mode from the annotators re-
sponses as was done in LEXSUB, but we have not
decided on this yet. We will not run a multiword
task, but we will use the items identified as multi-
words as an optional filter to the scoring i.e. to see
how systems did without these items.
We will provide baselines and upper-bounds.
5 Systems
In the cross-lingual LEXSUB task, the systems will
have to deal with two parts of the problem, namely:
1. candidate collection
2. candidate selection
The first sub-task, candidate collection, refers to
consulting several resources and coming up with a
list of potential translation candidates for each tar-
get word and part of speech. We do not provide any
inventories, as with the original LEXSUB task, and
thus leave this task of coming up with the most suit-
able translation list (in contrast to the synonym list
required for LEXSUB) to the participants. As was
observed with LEXSUB, it is our intuition that the
quality of this translation list that the systems come
up with will determine to a large extent how well
the final performance of the system will be. Partici-
pants are free to use any ideas. However, a few pos-
sibilities might be to use parallel corpora, bilingual
dictionaries, a translation engine that only translates
the target word, or a machine translation system that
translates the entire sentences. Several of the bilin-
gual dictionaries or even other resources might be
combined together to come up with a comprehen-
sive translation candidate list, if that seems to im-
prove performance.
The second phase, candidate selection, concerns
fitting the translation candidates in context, and thus
coming up with a ranking as to which translations
are the most suitable for each instance. The highest
ranking candidate will be the output for best, and the
list of the top 10 ranking candidates will be the out-
put for out-of-ten. Again, participants are free to use
their creativity in this, while a range of possible al-
gorithms might include using a machine translation
system, using language models, word sense disam-
biguation models, semantic similarity-based tech-
niques, graph-based models etc. Again, combina-
tions of these might be used if they are feasible as
far as time and space are concerned.
We anticipate a minor practical issue to come up
with all participants, and that is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. This is directly re-
lated to the issue of dealing with characters with di-
acritics, and in our experience not all available soft-
ware packages and programs are able to handle dia-
critics and different character encodings in the same
way. This issue is inherent in all cross-lingual tasks,
and we leave it up to the discretion of the partici-
pants to effectively deal with it.
6 Post Hoc Issues
In LEXSUB a post hoc evaluation was conducted us-
ing fresh annotators to ensure that the substitutes
the systems came up with were not typically bet-
ter than those of the original annotators. This was
done as a sanity check because there was no fixed
inventory for the task and there will be a lot of varia-
80
tion in the task and sometimes the systems might do
better than the annotators. The post hoc evaluation
demonstrated that the post hoc annotators typically
preferred the substitutes provided by humans.
We have not yet determined whether we will run
a post hoc evaluation because of the costs of do-
ing this and the time constraints. Another option is
to reannotate a portion of our data using a new set
of annotators but restricting them to the translations
supplied by the initial set of annotations and other
translations from available resources. This would be
worthwhile but it could be done at any stage when
funds permit because we do not intend to supply a
set of candidate translations to the annotators since
we wish to evaluate candidate collection as well as
candidate selection.
7 Conclusions
In this paper we have outlined the cross-lingual lex-
ical substitution task to be run under the auspices
of SemEval-2010. The task will require annotators
and systems to find translations for a target word in
context. Unlike machine translation tasks, the whole
text is not translated and annotators are encouraged
to supply as many translations as fit the context. Un-
like previous WSD tasks based on parallel data, be-
cause we allow multiple translations and because we
do not restrict translations to those that provide clear
cut sense distinctions, we will be able to use the
dataset collected to investigate more subtle represen-
tations of meaning.
8 Acknowledgements
The work of the first and third authors has been partially
supported by a National Science Foundation CAREER
award #0747340. The work of the second author has been
supported by a Royal Society UK Dorothy Hodgkin Fel-
lowship.
References
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005), pages 1010?1015,
Edinburgh, Scotland.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Adam Kilgarriff. 2006. Word senses. In Eneko
Agirre and Phil Edmonds, editors, Word Sense Disam-
biguation, Algorithms and Applications, pages 29?46.
Springer.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese transla-
tion task. In Proceedings of the SENSEVAL-2 work-
shop, pages 37?44.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53, Prague,
Czech Republic.
Diana McCarthy and Roberto Navigli. in press. The en-
glish lexical substitution task. Language Resources
and Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and Be-
yond.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109?115, Philadelphia, USA.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-
2007 task 11: English lexical sample task via
English-Chinese parallel text. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), pages 54?58, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 613?619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
Christopher Stokoe. 2005. Differentiating homonymy
and polysemy in information retrieval. In Proceedings
of the joint conference on Human Language Technol-
ogy and Empirical methods in Natural Language Pro-
cessing, pages 403?410, Vancouver, B.C., Canada.
81
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 56?59,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Explorations in Automatic Image Annotation using Textual Features
Chee Wee Leong
Computer Science & Engineering
University of North Texas
cheeweeleong@my.unt.edu
Rada Mihalcea
Computer Science & Engineering
University of North Texas
rada@cs.unt.edu
Abstract
In this paper, we report our work on
automatic image annotation by combining
several textual features drawn from the
text surrounding the image. Evaluation of
our system is performed on a dataset of
images and texts collected from the web.
We report our findings through compar-
ative evaluation with two gold standard
collections of manual annotations on the
same dataset.
1 Introduction
Despite the usefulness of images in expressing
ideas, machine understanding of the meaning of
an image remains a daunting task for comput-
ers, as the interplay between the different visual
components of an image does not conform to any
fixed pattern that allows for formal reasoning of
its semantics. Often, the machine interpretation of
the concepts present in an image, known as auto-
matic image annotation, can only be inferred by
its accompanying text or co-occurrence informa-
tion drawn from a large corpus of texts and im-
ages (Li and Wang, 2008; Barnard and Forsyth,
2001). Not surprisingly, humans have the innate
ability to perform this task reliably, but given a
large database of images, manual annotation is
both labor-intensive and time-consuming.
Our work centers around the question : Pro-
vided an image with its associated text, can we
use the text to reliably extract keywords that rel-
evantly describe the image ? Note that we are not
concerned with the generation of keywords for an
image, but rather their extraction from the related
text. Our goal eventually is to automate this task
by leveraging on texts which are naturally occur-
ring with images. In all our experiments, we only
consider the use of nouns as annotation keywords.
2 Related Work
Although automatic image annotation is a popu-
lar task in computer vision and image processing,
there are only a few efforts that leverage on the
multitude of resources available for natural lan-
guage processing to derive robust linguistic based
image annotation models. Most of the work has
posed the annotation task as a classification prob-
lem, such as (Li and Wang, 2008), where images
are annotated using semantic labels associated to
a semantic class.
The most recent work on image annotation us-
ing linguistic features (Feng and Lapata, 2008)
involves implementing an extended version of
the continuous relevance model that is proposed
in (Jeon et al, 2003). The basic idea underlying
their work is to perform annotation of a test im-
age by using keywords shared by similar training
images. Evaluation of their system performance
is based on a dataset collected from the news do-
main (BBC). Unlike them, in this paper, we at-
tempt to perform image annotation on datasets
from unrestricted domains. We are also interested
in extending the work pursued in (Deschacht and
Moens, 2007), where visualness and salience are
proposed as important textual features for discov-
ering named entities present in an image, by ex-
tracting other textual features that can further im-
prove existing image annotation models.
3 Data Sets
We use 180 images collected from the Web, from
pages that have a single image within a specified
size range (width and height of 275 to 1000 pix-
els). 110 images are used for development, while
the remaining 70 are used for test. We create two
different gold standards. The first, termed as Intu-
itive annotation standard (GSintuition), presents a
user with the image in the absence of its associated
text, and asks the user for the 5 most relevant anno-
tations. The second, called Contextual annotation
standard (GScontext), provides the user with a list
of candidates1 for annotation, with the user free to
choose any of the candidates deemed relevant to
describe the image. The user, however, is not con-
1Union of candidates proposed by all systems participat-
ing in the evaluation, including the baseline system
56
strained to choose any candidate word, nor is she
obligated to choose a specified number of candi-
dates. For each image I in the evaluation set, we
invited five users to perform the annotation task
per gold standard. The agreement is 7.78% for
GSintuition and 22.27% for GScontext, where we
consider an annotation that is proposed by three or
more users as one that is being agreed upon. The
union of their inputs forms the set GSintuition(I)
and GScontext(I) respectively. We do not consider
image captions for use as a gold standard here due
to their absence in many of the images ? a ran-
dom sampling of 15 images reveals that 7 of them
lack captions. Contrary to their use as a proxy for
annotation keywords in (Feng and Lapata, 2008;
Deschacht and Moens, 2007), where evaluation is
performed on datasets gleaned from authoritative
news websites, most captions in our dataset are
not guaranteed to be noise free. However, they are
used as part of the text for generating annotations
where they exist.
4 Automatic Image Annotation
We approach the task of automatic image anno-
tation using four methods. Due to the orthogo-
nal nature in their search for keywords, the out-
put for each method is generated separately and
later combined in an unsupervised setting. How-
ever, all four methods perform their discrimination
of words by drawing information exclusively from
the text associated to the image, using no image
visual features in the process.
4.1 Semantic Cloud (Sem)
Every text describes at least one topic that can be
semantically represented by a collection of words.
Intuitively, there exists several ?clouds? of seman-
tically similar words that form several, possibly
overlapping, sets of topics. Our task is to se-
lect the dominant topic put forward in the text,
with the assumption that such a topic is being
represented by the largest set of words. We use
an adapted version of the K-means clustering ap-
proach, which attempts to find natural ?clusters?
of words in the text by grouping words with a com-
mon centroid. Each centroid is the semantic cen-
ter of the group of words and the distance between
each centroid and the words are approximated by
ESA (Gabrilovich and Markovitch, 2007). Fur-
ther, we perform our experiments with the follow-
ing assumptions : (1) To maximize recall, we as-
sume that there are only two topics in every text.
(2) Every word or collocation in the text must be
classified under one of these two topics, but not
both. In cases, where there is a tie, the classi-
fication is chosen randomly. For each dominant
cluster extracted, we rank the words in decreasing
order of their ESA distance to the centroid. To-
gether, they represent the gist of the topic and are
used as a set of candidates for labeling the image.
4.2 Lexical Distance (Lex)
Words that are lexically close to the picture in the
document are generally well-suited for annotat-
ing the image. The assumption is drawn from the
observation that the caption of an image is usu-
ally located close to the image itself. For images
without captions, we consider words surrounding
the image as possible candidates for annotation.
Whenever a word appears multiple times within
the text, its occurrence closest to the image is used
to calculate the lexical distance. To discriminate
against general words, we weigh the Lexical Dis-
tance Score (LDS) for each word by its tf * idf
score as in the equation shown below :
LDS(Wi) = tf * idf(Wi)/LS(Wi) (1)
where LS(Wi) is the minimum lexical distance of
Wi to the image, and idf is calculated using counts
from the British National Corpus.
4.3 Saliency (Sal)
To our knowledge, all word similarity metrics pro-
vide a symmetric score between a pair of words
w1 and w2 to indicate their semantic similarity.
Intuitively, this is not always the case. In psy-
cholinguistics terms, uttering w1 may bring into
mind w2, while the appearance of w2 without any
contextual clues may not associate with w1 at all.
Thus, the degree of similarity of w1 with respect
to w2 should be separated from that of w2 with
respect to w1. We use a directional measure of
similarity:
DSim(wi, wj) =
Cij
Ci
? Sim(wi, wj) (2)
where Cij is the count of articles in Wikipedia
containing words wi and wj , Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the two words. The directional weight (Cij /Ci)
amounts to the degree of association of wi with re-
spect to wj . Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of ?recommenda-
tion? where we sayw1 recommendsw2 if and only
if there is a directed edge from w1 to w2, with
the weight of the recommendation being the di-
rectional similarity score. By employing the graph
iteration algorithm proposed in (Mihalcea and Ta-
rau, 2004), we can compute the rank of a vertex in
57
the entire graph. The output generated is a sorted
list of words in decreasing order of their ranks,
which serves as a list of candidates for annotating
the image. Note that the top-ranked word must in-
fer some or all of the words in the text.
Table 1: An image annotation example
Sem symptoms, treatment, medical treat-
ment, medical care, sore throat, fluids,
cough, tonsils, strep throat, swab
Lex strep throat, cotton swab, lymph nodes,
rheumatic fever, swab, strep, fever, sore
throat, lab, scarlet fever
Sal strep, swab, nemours, teens, ginger ale,
grapefruit juice, sore, antibiotics, kids,
fever
Pic throat, runny nose, strep throat, sore
throat, hand washing, orange juice, 24
hours, medical care, beverages, lymph
nodes
Combined treatment, cough, tonsils, swab, fluids,
strep throat
Doc Title strep throat
tf * idf strep, throat, antibiotics, symptoms,
child, swab, fever, treatment, teens,
nemours
GScontext medical care, medical treatment, doc-
tor, cotton swab, treatment, tonsils, sore
throat, swab, throat, sore, sample, symp-
toms, throat, cough, medication, bacte-
ria, lab, scarlet fever, strep throat, teens,
culture, kids, child, streptococcus, doctor,
strep
GSintuition tongue, depressor, exam, eyes, cartoon,
doctor, health, child, tonsils, fingers, hair,
mouth, dentist, sample, cloth, curly, tip,
examine
4.4 Picturable Cues (Pic)
Some words are more picturable than others. For
instance, it is easy to find a picture that describes
the word banana than another word paradigm.
Clearly, picturable words in the associated text of
an image are natural candidates for labeling it. Un-
like the work in (Deschacht and Moens, 2007),
we employ a corpus-based approach to compute
word to word similarity. We collect a list of 200
manually-annotated words2 that are deemed to be
picturable by humans. We use this list of words
as our set of seed words, Sseed. We then iterate a
bootstrapping process where each word in the text
is compared to every word in the set of seed words,
and any word having a maximum ESA score of
2http://simple.wikipedia.org/wiki/Wikipedia:
Basic English picture wordlist
greater than 0.95 is added to Sseed. Similarly, the
maximum ESA score of each word over all Sseed
words is recorded. This is the picturability score
of the word.
5 Experiments and Evaluations
We investigate the performance of each of the four
annotation methods individually, followed by a
combined approach using all of them. In the in-
dividual setting, we simply obtain the set of candi-
dates proposed by each method as possible anno-
tation keywords for the image. In the unsupervised
combined setting, only the labels proposed by all
individual methods are selected, and listed in re-
verse order of their combined rankings.
We allow each system to produce a re-ranked
list of top k words to be the final annotations for a
given image. A system can discretionary generate
less (but not more) than k words that is appropri-
ate to its confidence level. Similar to (Feng and
Lapata, 2008), we evaluate our systems using pre-
cision, recall and F-measure for k=10, k=15 and
k=20 words.
For comparison, we also implemented two
baselines systems: tf * idf and Doc Title, which
simply takes all the words in the title of the
web page and uses them as annotation labels for
the image. In the absence of a document title,
we use the first sentence in the document. The
results for GSintuition and GScontext are tabu-
lated in Tables 2 and 3 respectively. We fur-
ther illustrate our results with an annotation ex-
ample (an image taken from a webpage discussing
strep throat among teens) in Table 1. Words in
bold matches GScontext while those underlined
matches GSintuition.
6 Discussion
As observed, the system implementing the Se-
mantic Cloud method significantly outperforms
the rest of the systems in terms of recall and F-
measure using the gold standard GSintuition. The
unsupervised combined system yields the high-
est precision at 16.26% (at k=10,15,20) but at
a low recall of 1.52%. Surprisingly, the base-
line system using tf * idf performs relatively well
across all the experiments using the gold stan-
dard GSintuition, outperforming two of our pro-
posed methods Salience (Sal) and Picturability
Cues (Pic) consistently for all k values. The other
baseline, Doc Title, records the highest precision
at 16.33% at k=10 with a low recall of 3.81%. For
k=15 and k=20, the F-measure scored 6.31 and
6.29 respectively, both lower than that scored by
tf * idf.
58
Table 2: Results for Automatic Image Annotation for GSintuition. In both Tables 2 and 3, statistically
significant results are marked with ?(measured against Doc Title, p<0.05, paired t-test), ?(measured
against tf*idf, p<0.1, paired t-test), ?(measured against tf*idf, p<0.05, paired t-test).
GSintuition
k=10 k=15 k=20
P R F P R F P R F
Sem 11.71 6.25? 8.15 11.31 8.91?? 9.97?? 10.36 9.45?? 9.88??
Lex 9.00 4.80 6.26 7.33 5.86 6.51 7.14 7.62 7.37
Sal 4.57 2.43 3.17 6.28 5.03 5.59 6.38 6.78 6.57
Pic 7.14 3.81 4.97 6.09 4.87 5.41 5.64 6.02 5.82
Combined 16.26 1.52 2.78 16.26? 1.52 2.78 16.26? 1.52 2.78
Doc Title 16.33 3.81 6.18 15.56 3.96 6.31 15.33 3.96 6.29
tf * idf 9.71 5.18 6.76 8.28 6.63 7.36 7.14 7.62 7.37
Table 3: Results for Automatic Image Annotation for GScontext
GScontext
k=10 k=15 k=20
P R F P R F P R F
Sem 71.57 26.20?? 38.36?? 68.00 37.34?? 48.21?? 64.56 47.17?? 54.51??
Lex 61.00 22.23 32.59 58.95 32.37 41.79 56.92 41.68 48.12
Sal 46.42 16.99 24.88 51.14 28.08 36.25 54.59 39.80 46.04
Pic 51.71 21.12 29.99 56.85 31.22 40.31 56.35 41.26 47.64
Combined 75.60?? 4.86 9.13 75.60?? 4.86 9.13 75.60?? 4.86 9.13
Doc Title 32.67 5.23 9.02 32.33 5.64 9.60 32.15 5.70 9.68
tf * idf 55.85 20.44 29.93 54.19 29.75 38.41 49.07 35.93 41.48
When performing evaluations using the gold
standard GScontext, significantly higher precision,
recall and F-measure values are scored by all the
systems, including both baselines. This is perhaps
due to the availability of candidates that suggests
a form of cued recall, rather than free recall, as
is the case with GSintuitive. The user is able to
annotate an image with higher accuracy e.g. la-
belling a Chihuahua as a Chihuahua instead of a
dog. Again, the Semantic Cloud method contin-
ues to outperform all the other systems in terms of
recall and F-measure consistently for k=10, k=15
and k=20 words. A similar trend as observed us-
ing the gold standard of GSintuition is seen here,
where again our combined system favors precision
over recall at all values of k.
A possible explanation for the poor perfor-
mance of the Saliency method is perhaps due to
over-specific words that infer all other words in the
text, yet unknown to the knowledge of most hu-
man annotators. For instance, the word Mussolini,
referring to the dictator Benito Mussolini, was not
selected as an annotation for an image showing
scenes of World War II depicting the Axis troops,
though it suggests the concepts of war, World War
II and so on. The Pic method is also not perform-
ing as well as expected under the two gold anno-
tation standards, mainly due to the fact that it fo-
cuses on selecting picturable nouns but not nec-
essarily those that are semantically linked to the
image itself.
7 Future Work
The use of the semantic cloud method to generate
automatic annotations is promising. Future work
will consider using additional semantic resources
such as ontological information and ency-
clopaedic knowledge to enhance existing models.
We are also interested to pursue human knowledge
modeling to account for the differences in annota-
tors in order create a more objective gold standard.
References
Kobus Barnard and David Forsyth. 2001. Learning the se-
mantics of words and pictures. In Proceedings of Interna-
tional Conference on Computer Vision.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In Proceedings
of the Association for Computational Linguisticd.
Yansong Feng and Mirella Lapata. 2008. Automatic image
annotation using auxiliary text information. In Proceed-
ings of the Association for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based explicit
semantic analysis. In International Joint Conferences on
Artificial Intelligence.
J Jeon, V Lavrenko, and R Manmatha. 2003. Automatic im-
age annotation and retrieval using cross-media relevance
models. In Proceedings of the ACM SIGIR Conference on
Research and Development in Information Retrieval.
Jia Li and James Wang. 2008. Real-time computerized an-
notation of pictures. In Proceedings of International Con-
ference on Computer Vision.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing
order into texts. In in Proceedings of Empirical Methods
in Natural Language Processing.
59
 
			ffThe Role of Lexico-Semantic Feedback in Open-Domain Textual
Question-Answering
Sanda Harabagiu, Dan Moldovan
Marius Pas?ca, Rada Mihalcea, Mihai Surdeanu,
Ra?zvan Bunescu, Roxana G??rju, Vasile Rus and Paul Mora?rescu
Department of Computer Science and Engineering
Southern Methodist University
Dallas, TX 75275-0122
 
sanda  @engr.smu.edu
Abstract
This paper presents an open-domain
textual Question-Answering system
that uses several feedback loops to en-
hance its performance. These feedback
loops combine in a new way statistical
results with syntactic, semantic or
pragmatic information derived from
texts and lexical databases. The paper
presents the contribution of each feed-
back loop to the overall performance of
76% human-assessed precise answers.
1 Introduction
Open-domain textual Question-Answering
(Q&A), as defined by the TREC competitions1 ,
is the task of identifying in large collections of
documents a text snippet where the answer to
a natural language question lies. The answer
is constrained to be found either in a short (50
bytes) or a long (250 bytes) text span. Frequently,
keywords extracted from the natural language
question are either within the text span or in
its immediate vicinity, forming a text para-
graph. Since such paragraphs must be identified
throughout voluminous collections, automatic
and autonomous Q&A systems incorporate an
index of the collection as well as a paragraph
retrieval mechanism.
Recent results from the TREC evaluations
((Kwok et al, 2000) (Radev et al, 2000) (Allen
1The Text REtrieval Conference (TREC) is a series of
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
et al, 2000)) show that Information Retrieval (IR)
techniques alone are not sufficient for finding an-
swers with high precision. In fact, more and more
systems adopt architectures in which the seman-
tics of the questions are captured prior to para-
graph retrieval (e.g. (Gaizauskas and Humphreys,
2000) (Harabagiu et al, 2000)) and used later in
extracting the answer (cf. (Abney et al, 2000)).
When processing a natural language question two
goals must be achieved. First we need to know
what is the expected answer type; in other words,
we need to know what we are looking for. Sec-
ond, we need to know where to look for the an-
swer, e.g. we must identify the question keywords
to be used in the paragraph retrieval.
The expected answer type is determined based
on the question stem, e.g. who, where or how
much and eventually one of the question concepts,
when the stem is ambiguous (for example what),
as described in (Harabagiu et al, 2000) (Radev et
al., 2000) (Srihari and Li, 2000). However finding
question keywords that retrieve all candidate an-
swers cannot be achieved only by deriving some
of the words used in the question. Frequently,
question reformulations use different words, but
imply the same answer. Moreover, many equiv-
alent answers are phrased differently. In this pa-
per we argue that the answer to complex natural
language questions cannot be extracted with sig-
nificant precision from large collections of texts
unless several lexico-semantic feedback loops are
allowed.
In Section 2 we survey the related work
whereas in Section 3 we describe the feedback
loops that refine the search for correct answers.
Section 4 presents the approach of devising key-
word alternations whereas Section 5 details the
recognition of question reformulations. Section 6
evaluates the results of the Q&A system and Sec-
tion 7 summarizes the conclusions.
2 Related work
Mechanisms for open-domain textual Q&A were
not discovered in the vacuum. The 90s witnessed
a constant improvement of IR systems, deter-
mined by the availability of large collections of
texts and the TREC evaluations. In parallel, In-
formation Extraction (IE) techniques were devel-
oped under the TIPSTER Message Understand-
ing Conference (MUC) competitions. Typically,
IE systems identify information of interest in a
text and map it to a predefined, target represen-
tation, known as template. Although simple com-
binations of IR and IE techniques are not practical
solutions for open-domain textual Q&A because
IE systems are based on domain-specific knowl-
edge, their contribution to current open-domain
Q&A methods is significant. For example, state-
of-the-art Named Entity (NE) recognizers devel-
oped for IE systems were readily available to be
incorporated in Q&A systems and helped recog-
nize names of people, organizations, locations or
dates.
Assuming that it is very likely that the answer
is a named entity, (Srihari and Li, 2000) describes
a NE-supported Q&A system that functions quite
well when the expected answer type is one of the
categories covered by the NE recognizer. Un-
fortunately this system is not fully autonomous,
as it depends on IR results provided by exter-
nal search engines. Answer extractions based on
NE recognizers were also developed in the Q&A
presented in (Abney et al, 2000) (Radev et al,
2000) (Gaizauskas and Humphreys, 2000). As
noted in (Voorhees and Tice, 2000), Q&A sys-
tems that did not include NE recognizers per-
formed poorly in the TREC evaluations, espe-
cially in the short answer category. Some Q&A
systems, like (Moldovan et al, 2000) relied both
on NE recognizers and some empirical indicators.
However, the answer does not always belong
to a category covered by the NE recognizer. For
such cases several approaches have been devel-
oped. The first one, presented in (Harabagiu et
al., 2000), the answer type is derived from a large
answer taxonomy. A different approach, based on
statistical techniques was proposed in (Radev et
al., 2000). (Cardie et al, 2000) presents a method
of extracting answers as noun phrases in a novel
way. Answer extraction based on grammatical
information is also promoted by the system de-
scribed in (Clarke et al, 2000).
One of the few Q&A systems that takes into
account morphological, lexical and semantic al-
ternations of terms is described in (Ferret et al,
2000). To our knowledge, none of the cur-
rent open-domain Q&A systems use any feed-
back loops to generate lexico-semantic alterna-
tions. This paper shows that such feedback loops
enhance significantly the performance of open-
domain textual Q&A systems.
3 Textual Q&A Feedback Loops
Before initiating the search for the answer to a
natural language question we take into account
the fact that it is very likely that the same ques-
tion or a very similar one has been posed to the
system before, and thus those results can be used
again. To find such cached questions, we measure
the similarity to the previously processed ques-
tions and when a reformulation is identified, the
system returns the corresponding cached correct
answer, as illustrated in Figure 1.
When no reformulations are detected, the
search for answers is based on the conjecture that
the eventual answer is likely to be found in a
text paragraph that (a) contains the most repre-
sentative question concepts and (b) includes a tex-
tual concept of the same category as the expected
answer. Since the current retrieval technology
does not model semantic knowledge, we break
down this search into a boolean retrieval, based
on some question keywords and a filtering mech-
anism, that retains only those passages containing
the expected answer type. Both the question key-
words and the expected answer type are identified
by using the dependencies derived from the ques-
tion parse.
By implementing our own version of the pub-
licly available Collins parser (Collins, 1996), we
also learned a dependency model that enables the
mapping of parse trees into sets of binary rela-
tions between the head-word of each constituent
and its sibling-words. For example, the parse tree
of TREC-9 question Q210: ?How many dogs pull
a sled in the Iditarod ?? is:
JJ
S
Iditarod
VP
NP
PP
NP
NNPDTINNN
NP
DTVBPNNS
NP
manyHow
WRB
dogs pull a sled in the
For each possible constituent in a parse tree,
rules first described in (Magerman, 1995) and
(Jelinek et al, 1994) identify the head-child and
propagate the head-word to its parent. For the
parse of question Q210 the propagation is:
NP (sled)
DT NN DTIN
manyHow
WRB
dogs
NNSJJ
NP (dogs)
VBP
pull a sled in the Iditarod
NNP (Iditarod)
NP (Iditarod)
PP (Iditarod)
NP (sled)
VP (pull)
S (pull)
When the propagation is over, head-modifier
relations are extracted, generating the following
dependency structure, called question semantic
form in (Harabagiu et al, 2000).
dogs IditarodCOUNT pull sled
In the structure above, COUNT represents the
expected answer type, replacing the question stem
?how many?. Few question stems are unambigu-
ous (e.g. who, when). If the question stem is am-
biguous, the expected answer type is determined
by the concept from the question semantic form
that modifies the stem. This concept is searched
in an ANSWER TAXONOMY comprising several
tops linked to a significant number of WordNet
noun and verb hierarchies. Each top represents
one of the possible expected answer types imple-
mented in our system (e.g. PERSON, PRODUCT,
NUMERICAL VALUE, COUNT, LOCATION). We
encoded a total of 38 possible answer types.
In addition, the question keywords used for
paragraph retrieval are also derived from the ques-
tion semantic form. The question keywords are
organized in an ordered list which first enumer-
ates the named entities and the question quota-
tions, then the concepts that triggered the recogni-
tion of the expected answer type followed by all
adjuncts, in a left-to-right order, and finally the
question head. The conjunction of the keywords
represents the boolean query applied to the doc-
ument index. (Moldovan et al, 2000) details the
empirical methods used in our system for trans-
forming a natural language question into an IR
query.
Answer Semantic Form
No
No
Yes
Lexical 
Alternations
Semantic
Alternations
Question Semantic Form
Answer Logical Form
S-UNIFICATIONS
Expected Answer Type 
Question Logical Form
ABDUCTIVE   PROOF
in paragraph
No
Yes
No
Yes
LOOP 2
Filter out paragraph
Expected Answer Type
Question Keywords
Min<Number Paragraphs<Max No
LOOP 1Index
Yes LOOP 3
Yes
PARSE
	



Retrieval
Cached Questions
Cached Answers
    
Question
REFORMULATION
Figure 1: Feedbacks for the Answer Search.
It is well known that one of the disadvantages
of boolean retrieval is that it returns either too
many or too few documents. However, for ques-
tion answering, this is an advantage, exploited by
the first feedback loop represented in Figure 1.
Feedback loop 1 is triggered when the number of
retrieved paragraphs is either smaller than a min-
imal value or larger than a maximal value deter-
mined beforehand for each answer type. Alterna-
tively, when the number of paragraphs is within
limits, those paragraphs that do not contain at
least one concept of the same semantic category
as the expected answer type are filtered out. The
remaining paragraphs are parsed and their depen-
dency structures, called answer semantic forms,
are derived.
Feedback loop 2 illustrated in Figure 1 is acti-
vated when the question semantic form and the
answer semantic form cannot by unified. The uni-
fication involves three steps:
 Step 1: The recognition of the expected answer
type. The first step marks all possible concepts
that are answer candidates. For example, in the
case of TREC -9 question Q243: ?Where did the
ukulele originate ??, the expected answer type is
LOCATION. In the paragraph ?the ukulele intro-
duced from Portugal into the Hawaiian islands?
contains two named entities of the category LO-
CATION and both are marked accordingly.
 Step 2: The identification of the question con-
cepts. The second step identifies the question
words, their synonyms, morphological deriva-
tions or WordNet hypernyms in the answer se-
mantic form.
 Step 3: The assessment of the similarities of
dependencies. In the third step, two classes of
similar dependencies are considered, generating
unifications of the question and answer semantic
forms:Semantic Indexing using WordNet Senses 
Rada Mihalcea and Dan Moldovan 
Department  of Computer  Science and Engineering 
Southern Methodist  University 
Dallas, Texas, 75275-0122 
{rada, moldovan}@seas.smu.edu 
Abst rac t  
We describe in this paper a boolean 
Information l~.etrieval system that 
adds word semantics to the classic 
word based indexing. Two of the 
main tasks of our system, namely 
the indexing and retrieval compo- 
nents, are using a combined word- 
based and sense-based approach. 
The key to our system is a methodol- 
ogy for building semantic represen- 
tations of open text, at word and col- 
location level. This new technique, 
called semantic indexing, shows im- 
proved effectiveness over the classic 
word based indexing techniques. 
1 In t roduct ion  
The main problem with the traditional 
boolean word-based approach to Information 
Retrieval (IR) is that it usually returns too 
many results or wrong results to be useful. 
Keywords have often multiple lexical func- 
tionalities (i.e. can have various parts of 
speech) or have several semantic senses. Also, 
relevant information can be missed by not 
specifying the exact keywords. 
The solution is to include more information 
in the documents to be indexed, such as to 
enable a system to retrieve documents based 
on the words, regarded as lexical strings, or 
based on the semantic meaning of the words. 
With this idea in mind, we designed an 
IR system which performs a combined word- 
based and sense-based indexing and retrieval. 
The inputs to ~ systems consist of a ques- 
tion/query and a set of documents from which 
the information has to be retrieved. We add 
lexical and semantic information to both the 
query and the documents, during a prepro- 
cessing phase in which the input question 
and the texts are disambiguated. The disam- 
biguation process relies on contextual infor- 
mation, and identify the meaning of the words 
based on WordNet 1 (FeUbaum, 1998) senses. 
As described in the fourth section, we have 
opted for a disambiguation algorithm which 
is semi-complete (it dis~mbiguates about 55% 
of the nouns and verbs), but is highly precise 
(over 92~ accuracy), instead of using a com- 
plete but less precise disambiguation. A part 
of speech tag is also appended to each word. 
After adding these lexical and semantic tags 
to the words, the documents are ready to be 
indexed: the index is created using the words 
as lexical strings (to ensure a word-based re- 
trieval), and the semantic tags (for the sense- 
based retrieval). 
Once the index is created, an input query is 
~n~wered using the document retrieval com- 
ponent of our system. First, the query is fully 
disambiguated; then, it is adapted to a spe- 
cific format which incorporates semantic in- 
formation, as found in the index, and uses 
the AND and OR operators implemented in
the retrieval module. 
Hence, using semantic indexing, we try to 
solve the two main problems of the m systems 
described earlier. (1) relevant information is 
not missed by not specifying the exact key- 
words; with the new tags added to the words, 
we also retrieve words which are semantically 
related to the input keywords; (2) using the 
sense-based component of our retrieval sys- 
XWordNet 1.6 is used in our system. 
35 
tern, the number of results returned from a 
search can be reduced, by specifying exactly 
the lexical functionality and/or the meaning 
of an input keyword. 
The system was tested using the Cran- 
field standard test collection. This collec- 
tion consists of 1400 docllments, SGML for- 
mated, from the aerodynamics field. From 
the 225 questions associated with this data 
set, we have randomly selected 50 questions 
and build for each of them three types of 
queries: (1) a query that uses only keywords 
selected from the question, stemmed using the 
WordNet stemmer2; (2) a query that uses the 
keywords from the question and the synsets 
3 for these keywords and (3) a query that 
uses the keywords from the question, the 
synsets for these keywords and the synsets for 
the keywords hypernyms. All these types of 
queries have been run against the semantic 
index described in this paper. Comparative 
results indicate the performance benefits of a 
retrieval system that uses a combined word- 
based and synset-based indexing and retrieval 
over the classic word based indexing. 
2 Re la ted  Work  
There are three main approaches reported 
in the literature regarding the incorpora- 
tion of semantic information into IR systems: 
(1)conceptual inde~ng, (2) query expansion 
and (3) semantic indexing. The former is 
based on ontological taxonomies, while the 
last two make use of Word Sense Disambigua- 
tion aigorithm~. 
2.1 Conceptual indexlr~g 
The usage of concepts for document index- 
ing is a relatively new trend within the IR 
field. Concept matching is a technique that 
has been used in limited domains, like the le- 
gal field were conceptual indexing has been 
applied by (Stein, 1997). The FERRET sys- 
tem (Mauldin, 1991) is another example of 
2WordNet stemmer = words are stemmed based on 
WordNet definitions (using the morphstr function) 
3The words in  WordNet are organized in synonym 
sets, called synsets. A synset is associated with a par- 
ticular sense of a word, and thus we use sense-based 
and synset-based interchangeably. 
how concept identification can improve II:t 
systems. 
To our knowledge, the most intensive work 
in this direction was performed by Woods 
(Woods, 1997), at Sun Microsystems Labo- 
ratories. He creates ome custom built onto- 
logical taxonomies based on subsumtion and 
morphology for the purpose of indexing and 
retrieving documents. Comparing the per- 
formance of the system that uses conceptual 
indexing, with the performance obtained us- 
ing classical retrieval techniques, resulted in 
an increased performance and recall. He de- 
fines also a new measure, called success rate 
which indicates if a question has an answer 
in the top ten documents returned by a re- 
trieval system. The success rate obtained in 
the case of conceptual indexing was 60%, re- 
spect to a maximum of 45~0 obtained using 
other retrieval systems. This is a signi~cant 
improvement and shows that semantics can 
have a strong impact on the effectiveness of
IR systems. 
The experiments described in (Woods, 
1997) refer to small collections of text, as 
for example the Unix manual pages (about 
10MB of text). But, as shown in (Ambroziak, 
1997), this is not a limitation; conceptual in- 
dexing can be successfully applied to much 
larger text collections, and even used in Web 
browsing. 
2.2 Query  Expungion 
Query expansion has been proved to have 
positive effects in retrieving relevant informa- 
tion (Lu and Keefer, 1994). The purpose of 
query extension can be either to broaden the 
set of documents retrieved or to increase the 
retrieval precision. In the former case, the 
query is expanded with terms similar with 
the words from the original query, while in 
the second case the expansion procedure adds 
completely new terms. 
There are two main techniques used in ex- 
panding an original query. The first one con- 
siders the use of Machine Readable Dictio- 
nary; (Moldovan and Mihaicea, 2000) and 
(Voorhees, 1994) are making use of WordNet 
to enlarge the query such as it includes words 
36 
which are semantically related to the concepts 
from the original query. The basic semantic 
relation used in their systems is the synonymy 
relation. This technique requires the disam- 
biguation of the words in the input query and 
it was reported that this method can be useful 
if the sense disambiguation is highly accurate. 
The other technique for query expan.qion is
to use relevance f edback, as used in SMART 
(Buckley et al, 1994). 
2.3 Semantic indexing 
The usage of word senses in the process of 
document indexing is a pretty much debated 
field of discussions. The basic idea is to in- 
dex word meanings, rather than words taken 
as lexical strings. A survey of the efforts of 
incorporating WSD into IR is presented in 
(Sanderson, 2000). Experiments performed 
by different researchers led to various, some- 
time contradicting results. Nevertheless, the 
conclusion which can be drawn from all these 
experiments i that a highly accurate Word 
Sense Disambiguation algorithm is needed in 
order to obtain an increase in the performance 
of IR systems. 
Ellen Voorhees (Voorhees, 1998) (Voorhees, 
1999) tried to resolve word ambiguity in the 
collection of documents, as well as in the 
query, and then she compared the results ob- 
tained with the performance of a standard 
run. Even if she used different weighting 
schemes, the overall results have shown a 
degradation in IR effectiveness when word 
meanings were used for indexing. Still, as she 
pointed out, the precision of the WSD tech- 
nique has a dramatic influence on these re- 
sults. She states that a better WSD can lead 
to an increase in IR performance. 
A rather "artificial" experiment in the same 
direction of semantic indexing is provided in 
(Sanderson, 1994). He uses pseudo-words 
to test the utility of disambiguation i IR. 
A pseudo-word is an artificially created am- 
biguous word, like for example "banana-door" 
(pseudo-words have been introduced for the 
first time in (Yarowsky, 1993), as means of 
testing WSD accuracy without the costs as- 
sociated with the acquisition of sense tagged 
corpora). Different levels of ambiguity were 
introduced in the set of documents prior to in- 
dexing. The conclusion drawn was that WSD 
has little impact on IR performance, to the 
point that only a WSD algorithm with over 
90% precision could help IR systems. 
The reasons for the results obtained by 
Sanderson have been discussed in (Schutze 
and Pedersen, 1995). They argue that the 
usage of pseudo-words does not always pro- 
vide an accurate measure of the effect of WSD 
over IR performance. It is shown that in the 
case of pseudo-words, high-frequency word 
types have the majority of senses of a pseudo- 
word, i.e. the word ambiguity is not realisti- 
cally modeled. More than this, (Schutze and 
Pedersen, 1995) performed experiments which 
have shown that semantics can actually help 
retrieval performance. They reported an in- 
crease in precision of up to 7% when sense 
based indexing is used alone, and up to 14% 
for a combined word based and sense based 
indexing. 
One of the largest studies regarding the 
applicability of word semantics to IR is re- 
ported by Krovetz (Krovetz and Croft, 1993), 
(Krovetz, 1997). When talking about word 
ambiguity, he collapses both the morpholog- 
ical and semantic aspects of ambiguity, and 
refers them as polysemy and homonymy. He 
shows that word senses hould be used in ad- 
dition to word based indexing, rather than 
indexing on word senses alone, basically be- 
cause of the uncertainty involved in sense dis- 
ambiguation. He had extensively studied the 
effect of lexical ambiguity over ~ the ex- 
periments described provide a clear indication 
that word meanings can improve the perfor- 
mance of a retrieval system. 
(Gonzalo et al, 1998) performed experi- 
ments in sense based indexing: they used the 
SMART retrieval system and a manually dis- 
ambiguated collection (Semcor). It turned 
out that indexing by synsets can increase re- 
call up to 29% respect to word based indexing. 
Part of their experiments was the simulation 
of a WSD algorithm with error rates of 5%, 
10%, 20%, 30% and 60%: they found that er- 
ror rates of up to 10% do not substantially af- 
37 
fect precision, and a system with WSD errors 
below 30% still perform better than a stan- 
dard run. The results of their experiments 
are encouraging, and proved that an accurate 
WSD algorithm can significantly help IR sys- 
tems. 
We propose here a system which tries 
to combine the benefits of word-based and 
synset-based indexing. Both words and 
synsets are indexed in the input text, and the 
retrieval is then performed using either one or 
both these sources of information. The key to 
our system is a WSD method for open text. 
3 System Arch i tec ture  
There are three main modules used by this 
system: 
1. Word  Sense Dis~rnbiguation (WSD) 
module, which performs a semi-complete 
but precise disambiguation f the words 
in the documents. Besides semantic in- 
formation, this module also adds part of 
speech tags to each word and stems the 
word using the WordNet stemmlug algo- 
rithm. Every document in the input set 
of documents i  processed with this mod- 
ule. The output is a new document in 
which each word is replaced with the new 
format 
PoslStemlPOSlO.f.f set 
where: Pos is the position of the word 
in the text; Stem is the stemmed form of 
the word; POS is the part of speech and 
Offset is the offset of the WordNet synset 
in which this word occurs. 
In the case when no sense is assigned by 
the WSD module or if the word cannot 
be found in WordNet, the last field is left 
empty. 
2. Indexing module, which indexes the 
documents, after they are processed by 
the WSD module. From the new for- 
mat of a word, as returned by the WSD 
function, the Stem and, separately, the 
Offset{POS are added to the index. This 
enables the retrieval of the words, re- 
garded as lexical strings, or the retrieval 
of the synset of the words (this actually 
means the retrieval of the given sense of 
the word and its synonyms). 
. Retr ieval  module, which retrieves doc- 
uments, based on an input query. As 
we are using a combined word-based and 
synset-based indexing, we can retrieve 
documents containing either (1) the in- 
put keywords, (2) the input keywords 
with an assigned sense or (3) synonyms 
of the input keywords. 
4 Word  Sense  D is~mbiguat ion  
As stated earlier, the WSD is performed for 
both the query and the documents from which 
we have to retrieve information. 
The WSD algorithm used for this purpose 
is an iterative algorithm; it was for the first 
time presented in (Mihalcea and Moldovan, 
2000). It determines, in a given text, a set of 
nouns and verbs which can be disambiguated 
with high precision. The semantic tagging is 
performed using the senses defined in Word- 
Net. 
In this section, we present the various 
methods used to identify the correct sense of a 
word. Then, we describe the main algorithm 
in which these procedures are invoked in an 
iterative manner. 
PROCEDUP.~ 1. This procedure identifies the 
proper nonn.q in the text, and marked them 
as having sense ~1. 
Example. c C Hudson,, is identified as a 
proper noun and marked with sense #1. 
PROCEDURE 2. Identify the words having 
only one sense in WordNet (monosemous 
words). Mark them with sense #1. 
Example. The noun subco~ait tee has one 
sense defined in WordNet. Thus, it is a 
monosemous word and can be marked as hav- 
ing sense #1. 
PROCEDURE 3. For a given word Wi, at po- 
sition i in the text, form two pairs, one with 
the word before W~ (pair Wi-l-Wi) and the 
other one with the word after Wi (pair Wi- 
Wi+i). Determiners or conjunctions cannot 
38 
be part of these pairs. Then, we extract all 
the occurrences of these pairs found within 
the semantic tagged corpus formed with the 
179 texts from SemCor(Miller et al, 1993). If, 
in all the occurrences, the word Wi has only 
one sense #k,  and the number of occurrences 
of this sense is larger than 3, then mark the 
word Wi as having sense #k.  
Example. Consider the word approva l  in 
the text fragment ' ' commit tee approva l  
o f '  '. The pairs formed are ' ~cown-ittee 
approva l '  and ' ~ approva l  of  ' '. No oc- 
currences of the first pair are found in the 
corpus. Instead, there are four occurrences of 
the second pair, and in all these occurrences 
the sense of approva l  is sense #1.  Thus, 
approva l  is marked with sense #1.  
PROCEDURE 4. For a given noun N in the 
text, determine the noun-context of each of 
its senses. This noun-context is actually a list 
of nouns which can occur within the context 
of a given sense i of the noun N. In order to 
form the noun-context for every sense Ni, we 
are determining all the concepts in the hyper- 
nym synsets of Ni. Also, using SemCor, we 
determine all the nouns which occur within a 
window of 10 words respect o Ni. 
All of these nouns, determined using Word- 
Net and SemCor, constitute the noun-context 
of Ni. We can now calculate the number of 
common words between this noun-context and 
the original text in which the noun N is found. 
Applying this procedure to all the senses of 
the noun N will provide us with an ordering 
over its possible senses. We pick up the sense 
i for the noun N which: (1) is in the top of 
this ordering and (2) has the distance to the 
next sense in this ordering larger than a given 
threshold. 
Example. The word d iameter ,  as it appears 
in the document 1340 from the Cranfield col- 
lection, has two senses. The common words 
found between the noun-contexts of its senses 
and the text are: for d iameter#l :  { property, 
hole, ratio } and for d iameter#2: { form}. 
For this text, the threshold was set to 1, and 
thus we pick d:i.ameter#1 as the correct sense 
(there is a difference larger than 1 between 
the number of nouns in the two sets). 
PROCEDURE 5. Find words which are se- 
mantically connected to the already disam- 
biguated words for which the connection dis- 
tance is 0. The distance is computed based 
on the Word_Net hierarchy; two words are se- 
mantically connected at a distance of 0 if they 
belong to the same synset. 
Example. Consider these two words ap- 
pearing in the text to be disambiguated: 
author i ze  and c lear .  The verb author i ze  
is a monosemous word, and thus it is disam- 
biguated with procedure 2. One of the senses 
of the verb c lear ,  namely sense #4,  appears 
in the same synset with author i ze#I ,  and 
thus c lear  is marked as having sense #4.  
PROCEDURE 6. Find words which are seman- 
tically connected, and for which the connec- 
tion distance is 0. This procedure is weaker 
than procedure 5: none of the words con- 
sidered by this procedure are already disamo 
biguated. We have to consider all the senses 
of both words in order to determine whether 
or not the distance between them is 0, and 
this makes this procedure computationally in- 
tensive. 
Example. For the words measure and b i l l ,  
both of them ambiguous, this procedure tries 
to find two possible senses for these words, 
which are at a distance of 0, i.e. they be- 
long to the same synset. The senses found 
are measure#4 and b i l l# l ,  and thus the two 
words are marked with their corresponding 
senses .  
PROCEDURE 7. Find words which are se- 
mantically connected to the already disam- 
biguated words, and for which the connection 
distance is maximum 1. Again, the distance 
is computed based on the WordNet hierar- 
chy; two words are semantically connected at 
a maximum distance of 1 if they are synonyms 
or they belong to a hypernymy/hyponymy re- 
lation. 
Example. Consider the nouns subcommittee 
and committee. The first one is disarm 
biguated with procedure 2, and thus it is 
marked with sense #1.  The word committee 
with its sense #1 is semantically linked with 
the word subcommittee by a hypernymy re- 
lation. Hence, we semantically tag this word 
39 
with sense ~1. 
PROCEDURE 8. Find words which are se- 
mantically connected between them, and for 
which the connection distance is maximum 1. 
This procedure is similar with procedure 6: 
both words are ambiguous, and thus all their 
senses have to be considered in the process of 
finding the distance between them. 
Example. The words g i f t  and donat ion 
are both ambiguous. This procedure finds 
g i f t  with sense #1 as being the hypernym 
of donat ion,  also with sense ~1. Therefore, 
both words are disambiguated and marked 
with their assigned senses. 
The procedures presented above are applied 
iteratively. This allows us to identify a set of 
nouns and verbs which can be disambiguated 
with high precision. About 55% of the nouns 
and verbs are disambiguated with over 92% 
accuracy. 
A lgor i thm 
Step 1. Pre-process the text. This implies 
tokenization and part-of-speech tagging. The 
part-of-speech tagging task is performed with 
high accuracy using an improved version of 
Brill's tagger (Brill, 1992). At this step, we 
also identify the complex nominals, based on 
WordNet definitions. For example, the word 
sequence ' 'p ipel ine companies '  ' is found 
in WordNet and thus it is identified as a single 
concept. There is also a list of words which 
we do not attempt o dis~.mbiguate. These 
words are marked with a special flag to in- 
dicate that they should not be considered in 
the disrtmbiguation process. So far, this list 
consists of three verbs: be, have, do. 
Step 2. Initi~\]i~.e the Set of Disambiguated 
Words (SDW) with the empty set SDW={}. 
Initialize the Set of Ambiguous Words (SAW) 
with the set formed by all the nouns and verbs 
in the input text. 
Step 3. Apply procedure 1. The named en- 
tities identified here are removed from SAW 
and added to SDW. 
Step 4. Apply procedure 2. The monosemous 
words found here axe removed from SAW and 
added to SDW. 
Step 5. Apply procedure 3. This step allows 
us to disambiguate words based on their oc- 
currence in the semantically tagged corpus. 
The words whose sense is identified with this 
procedure are removed from SAW and added 
to SDW. 
Step 6. Apply procedure 4. This will identify 
a set of nouns which can be disambiguated 
band on their noun-contexts. 
Step 7. Apply procedure 5. This procedure 
tries to identify a synonymy relation between 
the words from SAW and SDW. The words 
disambiguated are removed from SAW and 
added to SDW. 
Step 8. Apply procedure 6. This step is dif- 
ferent from the previous one, as the synonymy 
relation is sought among words in SAW (no 
SDW words involved). The words disam- 
biguated are removed from SAW and added 
to SDW. 
Step 9. Apply procedure 7. This step tries 
to identify words from SAW which are linked 
at a distance of maximum 1 with the words 
from SDW. Remove the words dis ambiguated 
from SAW and add them to SDW. 
Step 10. Apply procedure 8. This procedure 
finds words from SAW connected at a distance 
of maximum I. As in step 8, no words from 
SDW are involved. The words disambiguated 
are removed from SAW and added to SDW. 
Resu l ts  
To determine the accuracy and the recall 
of the disambiguation method presented here, 
we have performed tests on 6 randomly se- 
lected files from SemCor. The following files 
have been used: br-a01, br-a02, br-k01, br- 
k18, br-m02, br-r05. Each of these files was 
split into smaller files with a maximum of 15 
lines each. This size limit is based on our 
observation that small contexts reduce the 
applicability of procedures 5-8, while large 
contexts become a source of errors. Thus, 
we have created a benchmark with 52 texts, 
on which we have tested the disambiguation 
method. 
In table 1, we present he results obtalned 
for these 52 texts. The first cohlmn indicates 
the file for which the results are presented. 
The average number of no, ms and verbs con- 
sidered by the disambiguation algorithm for 
each of these files is shown in the second col- 
40 
Table I: Results for the WSD algorithm applied on 52 texts 
No. Proc.l+2 Proc.3 Proc.4 Proc.5+6 Proc.7+8 
File words No. Ace. No. Ace. No. Acc. No. Ace. No. Acc. 
br-a01 132 40 100% 43 99.7~ 58.5 94.6% 63.8 92.7% 73.2 89.3% 
br-a02 135 49 100% 52.5 98.5% 68.6 94% 75.2 92.4% 81.2 91.4% 
br-k01 -68.1 17.2 100% 23.3 99.7% 38.1 97.4% 40.3 97.4% 41.8 96.4% 
br-k18 60.4 18.1 100% 20.7 99.1% 26.6 96.9% 27.8 95.3% 29.8 93.2% 
br-m02 63 17.3 100% 20.3 98.1% 26.1 95% 26.8 94.9% 30.1 93.9% 
br-r05 72.5 14.3 100% 16.6 98.1% 27 93.2% 30.2 91.5% 34.2 89.1% 
AVERAGE 88.5 25.9 100% 29.4 98.8% 40.8 95.2% 44 94% 48.4 92.2% 
umn. In columns 3 and 4, there are presented 
the average number of words disambiguated 
with procedures 1 and 2, and the accuracy 
obtained with these procedures. Column 5 
and 6 present the average number of words 
disambiguated and the accuracy obtained af- 
ter applying procedure 3 (cumulative results). 
The cumulative results obtained after apply- 
ing procedures 3, 4 and 5, 6 and 7, are shown 
in columns 7 and 8, 9 and 10, respectively 
columns 10 and 11. 
The novelty of this method consists of the 
fact that the disambiguation process is done 
in an iterative manner. Several procedures, 
described above, are applied such as to build 
a set of words which are disambiguated with 
high accuracy: 55% of the nouns and verbs 
are disambiguated with a precision of 92.22%. 
The most important improvements which 
are expected to be achieved on the WSD prob- 
lem are precision and speed. In the case of 
our approach to WSD, we can also talk about 
the need for an increased fecal/, meaning that 
we want to obtain a larger number of words 
which can be disambiguated in the input text. 
The precision of more than 92% obtained 
during our experiments i very high, consid- 
ering the fact that Word.Net, which is the dic- 
tionary used for sense identification, is very 
fine grained and sometime the senses are very 
close to each other. The accuracy obtained is 
close to the precision achieved by humans in 
sense disambiguation. 
5 Index ing  and  Ret r ieva l  
The indexing process takes a group of docu- 
ment files and produces a new index. Such 
things as unique document identifiers, proper 
SGML tags, and other artificial constructs are 
ignored. In the current version of the system, 
we are using only the AND and OR boolean 
operators. Future versions will consider the 
implementation of the NOT and NEAR oper- 
ators. 
The information obtained from the WSD 
module is used by the main indexing process, 
where the word stem and location are indexed 
along with the WordNet synset (if present). 
Collocations are indexed at each location that 
a member of the collocation occurs. 
All elements of the document are indexed. 
This includes, but is not limited to, dates, 
numbers, document identifiers, the stemmed 
words, collocations, WordNet synsets (if 
available), and even those terms which other 
indexers consider stop words. The only items 
currently excluded from the index are punc- 
tuation marks which are not part of a word 
or collocation. 
The benefit of this form of indexing is that 
documents may be retrieved using stemmed 
words, or using synset offsets. Using synset 
offset values has the added benefit of retriev- 
ing documents which do not contain the orig- 
inal stemmed word, but do contain synonyms 
of the original word. 
The retrieval process is limited to the use of 
the Boolean operators AND and OR. There 
is an auxiliary front end to the retrieval en- 
gine which allows the user to enter a textual 
query, such as, "What financial institutions 
are .found along the banks of the Nile?" The 
auxiliary front end will then use the WSD to 
disambiguate the query and build a Boolean 
query for the standard retrieval engine. 
For the preceding example, the auxil- 
41 
iary front end would build the query: (fi- 
nanciaLinstitution OR 60031M\[NN) AND 
(bank OR 68002231NN) AND (Nile OR 
68261741NN) where the numbers in the pre- 
vious query represent the offsets of the synsets 
in which the words with their determined 
meaning occur. 
Once a list of documents meeting the query 
requirements has been determined, the com- 
plete text of each matching document is re- 
trieved and presented to the user. 
6 An  Example  
Consider, for example, the following ques- 
tion: "Has anyone investigated the effect of 
surface mass transfer on hypersonic viscous 
interactionsf'. The question processing in- 
volves part of speech tagging, stemming and 
word sense disambiguation. 
The question be- 
comes: "Has anyone investigate I VB1535831 
the effectlNN 17766144 o/surfacelN~3447223 
massl NN139234 35 transferl Nhq132095 
on hypersoniclJJ viscouslJJ interactionlNNl 
7840572". 
The selection of the keywords is not an 
easy task, and it is performed using the set 
of 8 heuristics presented in (Moldovan et al, 
1999). Because of space limitations, we are 
not going to detail here the heuristics and the 
algorithm used for keywords election. The 
main idea is that an initial nnmber of key- 
words is determined using a subset of these 
heuristics. If no documents are retrieved, 
more keywords are added, respectively a too 
large number of documents will imply that 
some of the keywords are dropped in the re- 
versed order in which they have been entered. 
For each question, three types of query are 
formed, using the AND and OR. operators. 
1. QwNstem. Keywords from the question, 
stemmed based on WordNet, concate- 
nated with the AND operator. 
2. QwNoffset. Keywords from the ques- 
tion, stemmed based on WordNet, con- 
catenated using the OR. operator with 
the associated synset offset, and con- 
catenated with the AND operator among 
them. 
. QwNHyperOfSset. Keywords from the 
question, stemmed based on WordNet, 
concatenated using the OR operator with 
the associated synset offset and with the 
offset of the hypernym synset, and con- 
catenated with the AND operator among 
them. 
All these types of queries are run against 
the semantic index created based on words 
and synset offsets. We denote these rime with 
RWNStem, RWNOyfset and RWNHyperOffset. 
The three query formats, for the given ques- 
tion, are presented below: 
QwNstern. effect AND surface AND mass 
AND flow AND interaction 
QwNoyyset. (effect OR 77661441NN) AND 
(surface OR 3447223\[NN) AND (mass OR 
392343651NN) AND (transfer OR 1320951NN) 
AND (interaction OR 78405721NN) 
QWNHyperOf fset (effect OR 77661441NN OR 
20461\]NN) AND (surface OR 
3447223\]NN OR 119371NN ) AND (mass OR. 
39234351NN OR 3912591\[NN) AND (transfer 
OR 1320951NN OR. 1304701NN) AND (inter- 
action OR. 784057?~NN OR. 7770957~NN) 
Using the first type of query, 7 documents 
were found out of which 1 was considered 
to be relevant. With the second and third 
types of query, we obtained 11, respectively 
17 documents, out of which 4 were found rel- 
evant, and actually contained the answer to 
the question. 
(sample answer) ... the present report gives an ac- 
count  of the development o\] an approzimate theory to 
the problem of hypersonic strong viscous interaction 
on a fiat plate with mass-transfer at the plate surface. 
the disturbance flow region is divided into inviscid and 
viscous flo~ regions .... (craniield0305). 
77 Resu l ts  
The system was tested on the Cranfield col- 
lection, including 1400 documents, SGML 
formated 4. From the 225 questions provided 
4Demo available online at 
http://pdpl 3.seas.smu.edu/rada/sem.ind./ 
42 
with this collection, we randomly selected 50 
questions and used them to create a bench- 
mark against which we have performed the 
three runs described in the previous ections: 
RW N Stem , RW N O f f set and 1-~W N HyperO f f set.  
For each of .these questions, the system 
forms three types of queries, as described 
above. Below, we present 10 of these ques- 
tions and show the results obtained in Table 
2. 
I .  Has  anyone investigated the effect of surface mass trans- 
fer  on hypersonic ~'L~cwas interactions? 
$. What is the combined effect of surface heat and mass 
transfer on hypersonic f low? 
3. What are the existing solutions for hypersonic viscous in- 
teractions over an insulated fiat plate? 
4. What controls leading-edge attachment at transonic ve- 
locities ?
5. What are wind-tunnel corrections for a two-dimensional 
aerofoil mounted off-centre in a tunnel? 
6. What is the present state of the theory of quasi-conical 
flows ? 
7. References on the methods available for accurately esti- 
mating aerodynamic heat transfer to conical bodies for both 
laminar and turbulent flow. 
8. What parameters can seriously influence natural transi- 
tion from laminar to turbulent f low on a model in a wind 
tunnel? 
9. Can a satisfactory e~perimental technique be devel- 
oped for measuring oscillatory derivatives on slender sting- 
mounted models in supersonic wind tunnels? 
I0. Recent data on shock-induced boundary-layer separation. 
Three measures are used in the evaluation 
of the system performance: (1) precision, de.. 
fined as the number of relevant documents re- 
trieved over the total number of documents 
retrieved; (2) real/, defined as the number 
of relevant documents retrieved over the total 
number of relevant documents found in the 
collection and (3) F-measure, which combines 
both the precision and recall into a single for- 
mula: 
Fmeas~re = (32 + l'O) * P * R 
? P) + R 
where P is the precision, R is the recall and 
is the relative importance given to recall 
over precision. In our case, we consider both 
precision and recall of equal importance, and 
thus the factor fl in our evaluation is 1. 
The tests over the entire set of 50 questions 
led to 0.22 precision and 0.25 recall when the 
WordNet stemmer is used, 0.23 precision and 
0.29 recall when using a combined word-based 
and synset-based indexing. The usage of hy- 
pernym synsets led to a recall of 0.32 and a 
precision of 0.21. 
The relative gain of the combined word- 
based and synset-based indexing respect o 
the basic word-based indexing was 16% in- 
crease in recall and 4% increase in precision. 
When using the hypernym synsets, there is a 
28% increase in recall, with a 9% decrease in 
precision. 
The conclusion of these experiments is that 
indexing by synsets, in addition to the clas- 
sic word-based indexing, can actually improve 
IR effectiveness. More than that, this is the 
first time to our knowledge when a WSD algo- 
rithm for open text was actually used to au- 
tomaticaUy disambiguate a collection of texts 
prior to indexing, with a disambiguation ac- 
curacy high enough to actually increase the 
recall and precision of an IR system. 
An issue which can be raised here is the ef- 
ficiency of such a system: we have introduced 
a WSD stage into the classic IR process and it 
is well known that WSD algorithm.~ are usu- 
ally computationally intensive; on the other 
side, the disambiguation f a text collection 
is a process which can be highly parallelized, 
and thus this does not constitute a problem 
anymore. 
8 Conc lus ions  
The full understanding of text is still an elu- 
sive goal. Short of that, semantic indexing 
offers an improvement over current IR tech- 
niques. The key to semantic indexing is fast 
WSD of large collections of documents. 
In this paper we offer a WSD method for 
open domains that is fast and accurate. Since 
only 55% of the words can be disambiguated 
so far, we use a hybrid indexing approach that 
combines word-based and sense-based index- 
ing. The senses in WordNet are fine grain and 
the WSD method has to cope with this. The 
43 
Table 2: Results for 10 questions run against he three indices created on the Cranlleld collection. The bottom 
line shows the results for the entire set of questions. 
Question .RW N Stcm 
number recall precision Lmeasure 
1 0.08 0.14 0.05 
2 0.06 0.17 0.04 
3 0.47 0.70 0.28 
4 0.25 0.60 0.18 
5 0.33 0.50 0.20 
6 0.00 0.00 0.00 
7 0.17 0.17 0.09 
8 0.20 0.II 0.07 
9 0.67 0.50 0.29 
10 0.29 0.07 0.06 
Avo/50 0.25 0.22 0.09 
recall 
0.31 0.36 0.17 
0.25 0.44 0.16 
0.47 0.70 0.28 
0.25 0.60 0.18 
1.00 0.25 0.20 
0.00 0.00 0.00 
0.17 0.17 0.09 
0.20 0.II 0.07 
0.67 0.50 0.29 
0.29 0.07 0.06 
Query type 
Rw N o f f ~et RW l~ H ~rO y.f set 
precision f-measure recall precc~mn f-measure 
0.29 0.23 0.11 
0.31 0.24 0.14 
0.25 0.31 0.14 
0.53 0.67 0.30 
0.25 0.60 0.18 
1.00 0.19 0.16 
0.00 0.00 0.00 
0.17 0.17 0.09 
0.20 0.11 0.07 
1.00 0.38 0.28 
0.29 0.06 0.05 
0.32 0.21 0.10 
WSD algorithm presented here is new for the 
NLP community and proves to be well suited 
for a task such as semantic indexing. 
The continuously increasing amount of in- 
formation available today requires more and 
more sophisticated IR techniques, and seman- 
tic indexing is one of the new trends when try- 
ing to improve IR effectiveness. With seman- 
tic indexing, the search may be expanded to 
other forms of semantically related concepts 
as done by Woods (Woods, 1997). Finally, 
semantic indexing can have an impact on the 
semantic Web technology that is under con- 
sideration (Hellman, 1999). 
Re ferences  
J. Ambroziak. 1997. Conceptually assisted Web 
browsing. In Sixth International World Wide 
Web conference, Santa Clara, CA. full paper 
available online at http://www.scope.gmd.de\[ 
info/www6/posters/702/guide2.html. 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of the 3rd Conference on 
Applied Natural Language Processing, Trento, 
Italy. 
C. Buckley, G. Salton, J. Allan, and A. Singhal. 
1994. Automatic query expansion using smart: 
Trec 3. In Proceedings of the Text REtrieval 
Conference (TREC-3), pages 69--81. 
C. Fellbaurn. 1998. WordNet, An Electronic Lex- 
ical Database. The MIT Press. 
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigar- 
ran. 1998. Indexing with WordNet synsets 
can improve text retrieval. In Proceedings 
of COLING-ACL '98 Workshop on Usage of 
Word.Net in Natural Language Processing Sys- 
tems, Montreal, Canada, August. 
R. HeUman. 1999. A semantic approach adds 
meaning to the Web. Computer, pages 13-16. 
R. Krovetz and W.B. Croft. 1993. Lexical ambi- 
guity and in_formation retrieval. A CM Transac- 
tions on Information Systems, 10(2):115--141. 
R. Krovetz. 1997. Homonymy and polysemy in in- 
formation retrieval. In Proceedings of the 35th 
Annual Meeting of the Association for Compu- 
tational Linguistics (A CL-97}, pages 72-79. 
X.A. Lu and R.B. Keefer. 1994. Query expan- 
sion/reduction and its impact on retrieval ef- 
fectiveness. In The Text REtrieval Conference 
(TREC-3), pages 231-240. 
M.L. Mauldin. 1991. Retrieval performance 
in FERRET: a conceptual information re- 
trieval system. In Proceedings of the lSth 
International ACM-SIGIR Conference on Re- 
search and Development in Information Re- 
trieval, pages 347-355, Chicago, IL, October. 
R. Mihalcea and D.I. Moldovan. 2000. An iter- 
ative approach to word sense disambiguation. 
In Proceedings of FLAIRS-2000, pages 219-223, 
Orlando, FL, May. 
G. Miller, C. Leacock, T. Randee, and R. Bunker. 
1993. A semantic oncordance. In Proceedings 
of the 3rd DARPA Workshop on Human Lan- 
guage Technology, pages 303-308, Plaln~boro, 
New Jersey. 
D Moldovan and tL Mihalcea. 2000. Using Word- 
Net and lexical operators to improve Internet 
searches. IEEE Internet Computing, 4(1):34-- 
43. 
44 
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihal- 
cea, R. Goodrum, R. Girju, and V. Rus. 1999. 
LASSO: A tool for surfing the answer net. In 
Proceedings of the Text Retrieval Conference 
(TREU-8), November. 
M. Sanderson. 1994. Word sense disambiguation 
and information retrieval. In Proceedings of the 
17th Annual International ACM-SIGIR Con- 
ference on Research and Development in In- 
formation Retrieval, pages 142-151, Springer- 
Verlag. 
M. Sanderson. 2000. Retrieving with good sense. 
Information Retrieval, 2(1):49--69. 
H. Schutze and J. Pedersen. 1995. Information re- 
trieval based on word senses. In Proceedings of 
the 4th Annual Symposium on Document Anal- 
ysis and Information Retrieval, pages 161-175. 
J.A. Stein. 1997. Alternative methods of index- 
ing legal material: Development ofa conceptual 
index. In Proceedings of the Conference "Law 
Via the Internet g7", Sydney, Australia. 
E.M. Voorhees. 1994. Query expansion using 
lexical-semantic relations. In Proceedings of the 
17th Annual International ACM SIGIR, Con- 
ference on Research and Development in Infor- 
mation Retrieval, pages 61-69, Dublin, Ireland. 
E.M. Voorhees. 1998. Using WordNet for text 
retrieval. In WordNet, An Electronic Lexical 
Database, pages 285-303. The MIT Press. 
E.M. Voorhees. 1999. Natural language pro- 
eessing and information retrieval. In Infor- 
mation Extraction: towards scalable, adaptable 
systems. Lecture notes in Artificial Intelligence, 
#1714, pages 32-48. 
W.A. Woods. 1997. Conceptual indexing: A 
better way to organize knowledge. Techni- 
cal Report SMLI TR-97-61, Sun Mierosys- 
terns Laboratories, April. available online 
at: http:l/www.sun.com I researeh/techrep/ 
1997/abstract-61.html. 
D. Yarowsky. 1993. One sense per collocation. 
In Proceedings o\] the ARPA Human Language 
Technology Workshop. 
45 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 28?36,
Beijing, August 2010
Multilingual Subjectivity: Are More Languages Better?
Carmen Banea, Rada Mihalcea
Department of Computer Science
University of North Texas
carmenbanea@my.unt.edu
rada@cs.unt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
While subjectivity related research in
other languages has increased, most of the
work focuses on single languages. This
paper explores the integration of features
originating from multiple languages into
a machine learning approach to subjectiv-
ity analysis, and aims to show that this
enriched feature set provides for more ef-
fective modeling for the source as well
as the target languages. We show not
only that we are able to achieve over
75% macro accuracy in all of the six lan-
guages we experiment with, but also that
by using features drawn from multiple
languages we can construct high-precision
meta-classifiers with a precision of over
83%.
1 Introduction
Following the terminology proposed by (Wiebe
et al, 2005), subjectivity and sentiment analysis
focuses on the automatic identification of private
states, such as opinions, emotions, sentiments,
evaluations, beliefs, and speculations in natural
language. While subjectivity classification labels
text as either subjective or objective, sentiment or
polarity classification adds an additional level of
granularity, by further classifying subjective text
as either positive, negative or neutral.
To date, a large number of text processing ap-
plications have used techniques for automatic sen-
timent and subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 1990), tracking sentiment timelines in on-line
forums and news (Balog et al, 2006; Lloyd et al,
2005), and mining opinions from product reviews
(Hu and Liu, 2004). In many natural language
processing tasks, subjectivity and sentiment clas-
sification has been used as a first phase filtering to
generate more viable data. Research that benefited
from this additional layering ranges from ques-
tion answering (Yu and Hatzivassiloglou, 2003),
to conversation summarization (Carenini et al,
2008), and text semantic analysis (Wiebe and Mi-
halcea, 2006; Esuli and Sebastiani, 2006a).
Although subjectivity tends to be preserved
across languages ? see the manual study in (Mi-
halcea et al, 2007), (Banea et al, 2008) hypoth-
esize that subjectivity is expressed differently in
various languages due to lexicalization, formal
versus informal markers, etc. Based on this obser-
vation, our research seeks to answer the following
questions. First, can we reliably predict sentence-
level subjectivity in languages other than English,
by leveraging on a manually annotated English
dataset? Second, can we improve the English sub-
jectivity classification by expanding the feature
space through the use of multilingual data? Sim-
ilarly, can we also improve the classifiers in the
other target languages? Finally, third, can we ben-
efit from the multilingual subjectivity space and
build a high-precision subjectivity classifier that
could be used to generate subjectivity datasets in
the target languages?
The paper is organized as follows. We intro-
duce the datasets and the general framework in
Section 2. Sections 3, 4, and 5 address in turn each
of the three research questions mentioned above.
Section 6 describes related literature in the area
of multilingual subjectivity. Finally, we draw our
conclusions in Section 7.
2 Multilingual Datasets
Corpora that are manually annotated for subjec-
tivity, polarity, or emotion, are available in only
select languages, since they require a consider-
able amount of human effort. Due to this im-
pediment, the focus of this paper is to create a
method for extrapolating subjectivity data devel-
28
SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
90.4% 34.2% 46.6% 82.4% 30.7% 44.7% 86.7% 32.6% 47.4%
Table 1: Results obtained with a rule-based subjectivity classifier on the MPQA corpus (Wiebe and
Riloff, 2005)
oped in a source language and to transfer it to
other languages. Multilingual feature spaces are
generated to create even better subjectivity classi-
fiers, outperforming those trained on the individ-
ual languages alone.
We use the Multi-Perspective Question An-
swering (MPQA) corpus, consisting of 535
English-language news articles from a variety
of sources, manually annotated for subjectivity
(Wiebe et al, 2005). Although the corpus is an-
notated at the clause and phrase levels, we use
the sentence-level annotations associated with the
dataset in (Wiebe and Riloff, 2005). A sentence
is labeled as subjective if it has at least one pri-
vate state of strength medium or higher. Other-
wise the sentence is labeled as objective. From the
approximately 9700 sentences in this corpus, 55%
of them are labeled as subjective, while the rest
are objective. Therefore, 55% represents the ma-
jority baseline on this corpus. (Wiebe and Riloff,
2005) apply both a subjective and an objective
rule-based classifier to the MPQA corpus data and
obtain the results presented in Table 1.1
In order to generate parallel corpora to MPQA
in other languages, we rely on the method we pro-
posed in (Banea et al, 2008). We experiment with
five languages other than English (En), namely
Arabic (Ar), French (Fr), German (De), Roma-
nian (Ro) and Spanish (Es). Our choice of lan-
guages is motivated by several reasons. First,
we wanted languages that are highly lexicalized
and have clear word delimitations. Second, we
were interested to cover languages that are simi-
lar to English as well as languages with a com-
pletely different etymology. Consideration was
given to include Asian languages, such as Chi-
nese or Japanese, but the fact that their script with-
1For the purpose of this paper we follow this abbreviation
style: Subj stands for subjective, Obj stands for objective,
and All represents overall macro measures, computed over
the subjective and objective classes; P, R, F, and MAcc cor-
respond to precision, recall, F-measure, and macro-accuracy,
respectively.
out word-segmentation preprocessing does not di-
rectly map to words was a deterrent. Finally, an-
other limitation on our choice of languages is the
need for a publicly available machine translation
system between the source language and each of
the target languages.
We construct a subjectivity annotated corpus
for each of the five languages by using machine
translation to transfer the source language data
into the target language. We then project the orig-
inal sentence level English subjectivity labeling
onto the target data. For all languages, other than
Romanian, we use the Google Translate service,2
a publicly available machine translation engine
based on statistical models. The reason Roma-
nian is not included in this group is that, at the
time when we performed the first experiments,
Google Translate did not provide a translation ser-
vice for this language. Instead, we used an al-
ternative statistical translation system called Lan-
guageWeaver,3 which was commercially avail-
able, and which the company kindly allowed us
to use for research purposes.
The raw corpora in the five target lan-
guages are available for download at
http://lit.csci.unt.edu/index.php/Downloads,
while the English MPQA corpus can be obtained
from http://www.cs.pitt.edu/mpqa.
Given the specifics of each language, we em-
ploy several preprocessing techniques. For Ro-
manian, French, English, German and Spanish,
we remove all the diacritics, numbers and punc-
tuation marks except - and ?. The exceptions are
motivated by the fact that they may mark contrac-
tions, such as En: it?s or Ro: s-ar (may be), and
the component words may not be resolved to the
correct forms. For Arabic, although it has a dif-
ferent encoding, we wanted to make sure to treat
it in a way similar to the languages with a Roman
2http://www.google.com/translate t
3http://www.languageweaver.com/
29
alphabet. We therefore use a library4 that maps
Arabic script to a space of Roman-alphabet letters
supplemented with punctuation marks so that they
can allow for additional dimensionality.
Once the corpora are preprocessed, each sen-
tence is defined by six views: one in the origi-
nal source language (English), and five obtained
through automatic translation in each of the tar-
get languages. Multiple datasets that cover all
possible combinations of six languages taken one
through six (a total of 63 combinations) are gen-
erated. These datasets feature a vector for each
sentence present in MPQA (approximately 9700).
The vector contains only unigram features in one
language for a monolingual dataset. For a mul-
tilingual dataset, the vector represents a cumu-
lation of monolingual unigram features extracted
from each view of the sentence. For example, one
of the combinations of six taken three is Arabic-
German-English. For this combination, the vector
is composed of unigram features extracted from
each of the Arabic, German and English transla-
tions of the sentence.
We perform ten-fold cross validation and train
Na??ve Bayes classifiers with feature selection on
each dataset combination. The top 20% of the fea-
tures present in the training data are retained. For
datasets resulting from combinations of all lan-
guages taken one, the classifiers are monolingual
classifiers. All other classifiers are multilingual,
and their feature space increases with each addi-
tional language added. Expanding the feature set
by encompassing a group of languages enables us
to provide an answer to two problems that can ap-
pear due to data sparseness. First, enough training
data may not be available in the monolingual cor-
pus alone in order to correctly infer labeling based
on statistical measures. Second, features appear-
ing in the monolingual test set may not be present
in the training set and therefore their information
cannot be used to generate a correct classification.
Both of these problems are further explained
through the examples below, where we make the
simplifying assumption that the words in italics
are the only potential carriers of subjective con-
tent, and that, without them, their surrounding
4Lingua::AR::Word PERL library.
contexts would be objective. Therefore, their as-
sociation with an either objective or subjective
meaning imparts to the entire segment the same
labeling upon classification.
To explore the first sparseness problem, let us
consider the following two examples extracted
from the English version of the MPQA dataset,
followed by their machine translations in German:
?En 1: rights group Amnesty Interna-
tional said it was concerned about the
high risk of violence in the aftermath?
?En 2: official said that US diplomats
to countries concerned are authorized
to explain to these countries?
?De 1: Amnesty International sagte, es
sei besorgt u?ber das hohe Risiko von
Gewalt in der Folgezeit?
?De 2: Beamte sagte, dass US-
Diplomaten betroffenen La?nder
berechtigt sind, diese La?nder zu
erkla?ren?
We focus our discussion on the word con-
cerned, which in the first example is used in its
subjective sense, while in the second it carries an
objective meaning (as it refers to a group of coun-
tries exhibiting a particular feature defined ear-
lier on in the context). The words in italics in
the German contexts represent the translations of
concerned into German, which are functionally
different as they are shaped by their surrounding
context. By training a classifier on the English ex-
amples alone, under the data sparseness paradigm,
the machine learning model may not differentiate
between the word?s objective and subjective uses
when predicting a label for the entire sentence.
However, appending the German translation to the
examples generates additional dimensions for this
model and allows the classifier to potentially dis-
tinguish between the senses and provide the cor-
rect sentence label.
For the second problem, let us consider two
other examples from the English MPQA and their
respective translations into Romanian:
?En 3: could secure concessions on Tai-
wan in return for supporting Bush on is-
sues such as anti-terrorism and?
30
Lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF MAcc
En 74.01% 83.64% 78.53% 75.89% 63.68% 69.25% 74.95% 73.66% 73.89% 74.72%
Ro 73.50% 82.06% 77.54% 74.08% 63.40% 68.33% 73.79% 72.73% 72.94% 73.72%
Es 74.02% 82.84% 78.19% 75.11% 64.05% 69.14% 74.57% 73.44% 73.66% 74.44%
Fr 73.83% 83.03% 78.16% 75.19% 63.61% 68.92% 74.51% 73.32% 73.54% 74.35%
De 73.26% 83.49% 78.04% 75.32% 62.30% 68.19% 74.29% 72.90% 73.12% 74.02%
Ar 71.98% 81.47% 76.43% 72.62% 60.78% 66.17% 72.30% 71.13% 71.30% 72.22%
Table 2: Na??ve Bayes learners trained on six individual languages
?En 4: to the potential for change
from within America. Supporting our
schools and community centres is a
good?
?Ro 3: ar putea asigura concesii cu
privire la Taiwan, ??n schimb pentru
sust?inerea lui Bush pe probleme cum ar
fi anti-terorismului s?i?
?Ro 4: la potent?ialul de schimbare din
interiorul Americii. Sprijinirea s?colile
noastre s?i centre de comunitate este un
bun?
In this case, supporting is used in both English ex-
amples in senses that are both subjective; the word
is, however, translated into Romanian through two
synonyms, namely sust?inerea and sprijinirea. Let
us assume that sufficient training examples are
available to strengthen a link between support-
ing and sust?inerea, and the classifier is presented
with a context containing sprijinirea, unseen in
the training data. A multilingual classifier may be
able to predict a label for the context using the co-
occurrence metrics based on supporting and ex-
trapolate a label when the context contains both
the English word and its translation into Roma-
nian as sprijinirea. For a monolingual classifier,
such an inference is not possible, and the fea-
ture is discarded. Therefore a multi-lingual classi-
fier model may gain additional strength from co-
occurring words across languages.
3 Question 1
Can we reliably predict sentence-level sub-
jectivity in languages other than English, by
leveraging on a manually annotated English
dataset?
In (Banea et al, 2008), we explored several meth-
ods for porting subjectivity annotated data from
a source language (English) to a target language
(Romanian and Spanish). Here, we focus on the
transfer of manually annotated corpora through
the usage of machine translation by projecting the
original sentence level annotations onto the gener-
ated parallel text in the target language. Our aim
is not to improve on that method, but rather to ver-
ify that the results are reliable across a number of
languages. Therefore, we conduct this experiment
in several additional languages, namely French,
German and Arabic, and compare the results with
those obtained for Spanish and Romanian.
Table 2 shows the results obtained using Na??ve
Bayes classifiers trained in each language individ-
ually, with a macro accuracy ranging from 71.30%
(for Arabic) to 73.89% (for English).5 As ex-
pected, the English machine learner outperforms
those trained on other languages, as the original
language of the annotations is English. However,
it is worth noting that all measures do not deviate
by more than 3.27%, implying that classifiers built
using this technique exhibit a consistent behavior
across languages.
4 Question 2
Can we improve the English subjectivity clas-
sification by expanding the feature space
through the use of multilingual data? Simi-
larly, can we also improve the classifiers in the
other target languages?
We now turn towards investigating the impact on
subjectivity classification of an expanded feature
space through the inclusion of multilingual data.
In order to methodically assess classifier behavior,
we generate multiple datasets containing all pos-
5Note that the experiments conducted in (Banea et al,
2008) were made on a different test set, and thus the results
are not directly comparable across the two papers.
31
No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%
2 74.59% 83.14% 78.63% 75.70% 64.97% 69.92% 75.15% 74.05% 74.28%
3 75.04% 83.27% 78.94% 76.06% 65.75% 70.53% 75.55% 74.51% 74.74%
4 75.26% 83.36% 79.10% 76.26% 66.10% 70.82% 75.76% 74.73% 74.96%
5 75.38% 83.45% 79.21% 76.41% 66.29% 70.99% 75.90% 74.87% 75.10%
6 75.43% 83.66% 79.33% 76.64% 66.30% 71.10% 76.04% 74.98% 75.21%
Table 3: Average measures for a particular number of languages in a combination (from one through
six) for Na??ve Bayes classifiers using a multilingual space
sible combinations of one through six languages,
as described in Section 2. We then train Na??ve
Bayes learners on the multilingual data and av-
erage our results per each group comprised of a
particular number of languages. For example, for
one language, we have the six individual classi-
fiers described in Section 3; for the group of three
languages, the average is calculated over 20 pos-
sible combinations; and so on.
Table 3 shows the results of this experiment.
We can see that the overall F-measure increases
from 73.08% ? which is the average over one lan-
guage ? to 75.21% when all languages are taken
into consideration (8.6% error reduction). We
measured the statistical significance of these re-
sults by considering on one side the predictions
made by the best performing classifier for one lan-
guage (i.e., English), and on the other side the
predictions made by the classifier trained on the
multilingual space composed of all six languages.
Using a paired t-test, the improvement was found
to be significant at p = 0.001. It is worth men-
tioning that both the subjective and the objective
precision measures increase to 75% when more
than 3 languages are considered, while the overall
recall level stays constant at 74%.
To verify that the improvement is due indeed
to the addition of multilingual features, and it is
not a characteristic of the classifier, we also tested
two other classifiers, namely KNN and Rocchio.
Figure 1 shows the average macro-accuracies ob-
tained with these classifiers. For all the classi-
fiers, the accuracies of the multilingual combina-
tions exhibit an increasing trend, as a larger num-
ber of languages is used to predict the subjectivity
annotations. The Na??ve Bayes algorithm has the
best performance, and a relative error rate reduc-
 0.6
 0.65
 0.7
 0.75
 0.8
 1  2  3  4  5  6
Number of languages
NBKNNRocchio
Figure 1: Average Macro-Accuracy per group of
languages (combinations of 6 taken one through
six)
tion in accuracy of 8.25% for the grouping formed
of six languages versus one, while KNN and Roc-
chio exhibit an error rate reduction of 5.82% and
9.45%, respectively. All of these reductions are
statistically significant.
In order to assess how the proposed multilin-
gual expansion improves on the individual lan-
guage classifiers, we select one language at a time
to be the reference, and then compute the aver-
age accuracies of the Na??ve Bayes learner across
all the language groupings (from one through six)
that contain the language. The results from this
experiment are illustrated in Figure 2. The base-
line in this case is represented by the accuracy ob-
tained with a classifier trained on only one lan-
guage (this corresponds to 1 on the X-axis). As
more languages are added to the feature space,
we notice a steady improvement in performance.
When the language of reference is Arabic, we ob-
tain an error reduction of 15.27%; 9.04% for Ro-
32
 0.72
 0.73
 0.74
 0.75
 0.76
 1  2  3  4  5  6
Number of languages
ArDeEnEsFrRo
Figure 2: Average macro-accuracy progression
relative to a given language
manian; 7.80% for German; 6.44% for French;
6.06% for Spanish; and 4.90 % for English. Even
if the improvements seem minor, they are consis-
tent, and the use of a multilingual feature set en-
ables every language to reach a higher accuracy
than individually attainable.
In terms of the best classifiers obtained for
each grouping of one through six, English pro-
vides the best accuracy among individual clas-
sifiers (74.71%). When considering all possible
combinations of six classifiers taken two, German
and Spanish provide the best results, at 75.67%.
Upon considering an additional language to the
mix, the addition of Romanian to the German-
Spanish classifier further improves the accuracy
to 76.06%. Next, the addition of Arabic results
in the best performing overall classifier, with an
accuracy of 76.22%. Upon adding supplemental
languages, such as English or French, no further
improvements are obtained. We believe this is
the case because German and Spanish are able to
expand the dimensionality conferred by English
alone, while at the same time generating a more
orthogonal space. Incrementally, Romanian and
Arabic are able to provide high quality features
for the classification task. This behavior suggests
that languages that are somewhat further apart are
more useful for multilingual subjectivity classifi-
cation than intermediary languages.
5 Question 3
Can we train a high precision classifier with a
good recall level which could be used to gen-
erate subjectivity datasets in the target lan-
guages?
Since we showed that the inclusion of multilingual
information improves the performance of subjec-
tivity classifiers for all the languages involved, we
further explore how the classifiers? predictions can
be combined in order to generate high-precision
subjectivity annotations. As shown in previous
work, a high-precision classifier can be used to
automatically generate subjectivity annotated data
(Riloff and Wiebe, 2003). Additionally, the data
annotated with a high-precision classifier can be
used as a seed for bootstrapping methods, to fur-
ther enrich each language individually.
We experiment with a majority vote meta-
classifier, which combines the predictions of the
monolingual Na??ve Bayes classifiers described in
Section 3. For a particular number of languages
(one through six), all possible combinations of
languages are considered. Each combination sug-
gests a prediction only if its component classifiers
agree, otherwise the system returns an ?unknown?
prediction. The averages are computed across all
the combinations featuring the same number of
languages, regardless of language identity.
The results are shown in Table 4. The
macro precision and recall averaged across groups
formed using a given number of languages are
presented in Figure 3. If the average monolingual
classifier has a precision of 74.07%, the precision
increases as more languages are considered, with
a maximum precision of 83.38% obtained when
the predictions of all six languages are consid-
ered (56.02% error reduction). It is interesting to
note that the highest precision meta-classifier for
groups of two languages includes German, while
for groups with more than three languages, both
Arabic and German are always present in the top
performing combinations. English only appears
in the highest precision combination for one, five
and six languages, indicating the fact that the pre-
dictions based on Arabic and German are more
robust.
We further analyze the behavior of each lan-
guage considering only those meta-classifiers that
include the given language. As seen in Figure 4,
all languages experience a boost in performance
33
No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%
2 76.88% 76.39% 76.63% 80.17% 54.35% 64.76% 78.53% 65.37% 70.69%
3 78.56% 72.42% 75.36% 82.58% 49.69% 62.02% 80.57% 61.05% 68.69%
4 79.61% 69.50% 74.21% 84.07% 46.54% 59.89% 81.84% 58.02% 67.05%
5 80.36% 67.17% 73.17% 85.09% 44.19% 58.16% 82.73% 55.68% 65.67%
6 80.94% 65.20% 72.23% 85.83% 42.32% 56.69% 83.38% 53.76% 64.46%
Table 4: Average measures for a particular number of languages in a combination (from one through
six) for meta-classifiers
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 1  2  3  4  5  6
Number of languages
Macro-PrecisionMacro-Recall
Figure 3: Average Macro-Precision and Recall
across a given number of languages
as a result of paired language reinforcement. Ara-
bic gains an absolute 11.0% in average precision
when considering votes from all languages, as
compared to the 72.30% baseline consisting of the
precision of the classifier using only monolingual
features; this represents an error reduction in pre-
cision of 66.71%. The other languages experi-
ence a similar boost, including English which ex-
hibits an error reduction of 50.75% compared to
the baseline. Despite the fact that with each lan-
guage that is added to the meta-classifier, the re-
call decreases, even when considering votes from
all six languages, the recall is still reasonably high
at 53.76%.
The results presented in table 4 are promis-
ing, as they are comparable to the ones obtained
in previous work. Compared to (Wiebe et al,
2005), who used a high-precision rule-based clas-
sifier on the English MPQA corpus (see Table 1),
our method has a precision smaller by 3.32%, but
a recall larger by 21.16%. Additionally, unlike
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 1  2  3  4  5  6
Number of languages
ArDeEnEsFrRo
Figure 4: Average Macro-Precision relative to a
given language
(Wiebe et al, 2005), which requires language-
specific rules, making it applicable only to En-
glish, our method can be used to construct a high-
precision classifier in any language that can be
connected to English via machine translation.
6 Related Work
Recently, resources and tools for sentiment anal-
ysis developed for English have been used as
a starting point to build resources in other lan-
guages, via cross-lingual projections or mono-
lingual and multi-lingual bootstrapping. Several
directions were followed, focused on leveraging
annotation schemes, lexica, corpora and auto-
mated annotation systems. The English annota-
tion scheme developed by (Wiebe et al, 2005)
for opinionated text lays the groundwork for the
research carried out by (Esuli et al, 2008) when
annotating expressions of private state in the Ital-
ian Content Annotation Bank. Sentiment and
subjectivity lexica such as the one included with
34
the OpinionFinder distribution (Wiebe and Riloff,
2005), the General Inquirer (Stone et al, 1967), or
the SentiWordNet (Esuli and Sebastiani, 2006b)
were transfered into Chinese (Ku et al, 2006; Wu,
2008) and into Romanian (Mihalcea et al, 2007).
English corpora manually annotated for subjec-
tivity or sentiment such as MPQA (Wiebe et al,
2005), or the multi-domain sentiment classifica-
tion corpus (Blitzer et al, 2007) were subjected
to experiments in Spanish, Romanian, or Chinese
upon automatic translation by (Banea et al, 2008;
Wan, 2009). Furthermore, tools developed for En-
glish were used to determine sentiment or sub-
jectivity labeling for a given target language by
transferring the text to English and applying an
English classifier on the resulting data. The labels
were then transfered back into the target language
(Bautin et al, 2008; Banea et al, 2008). These ex-
periments are carried out in Arabic, Chinese, En-
glish, French, German, Italian, Japanese, Korean,
Spanish, and Romanian.
The work closest to ours is the one proposed
by (Wan, 2009), who constructs a polarity co-
training system by using the multi-lingual views
obtained through the automatic translation of
product-reviews into Chinese and English. While
this work proves that leveraging cross-lingual in-
formation improves sentiment analysis in Chinese
over what could be achieved using monolingual
resources alone, there are several major differ-
ences with respect to the approach we are propos-
ing here. First, our training set is based solely
on the automatic translation of the English corpus.
We do not require an in-domain dataset available
in the target language that would be needed for
the co-training approach. Our method is therefore
transferable to any language that has an English-to
target language translation engine. Further, we fo-
cus on using multi-lingual data from six languages
to show that the results are reliable and replicable
across each language and that multiple languages
aid not only in conducting subjectivity research in
the target language, but also in improving the ac-
curacy in the source language as well. Finally,
while (Wan, 2009) research focuses on polarity
detection based on reviews, our work seeks to de-
termine sentence-level subjectivity from raw text.
7 Conclusion
Our results suggest that including multilingual
information when modeling subjectivity can not
only extrapolate current resources available for
English into other languages, but can also improve
subjectivity classification in the source language
itself. We showed that we can improve an English
classifier by using out-of-language features, thus
achieving a 4.90% error reduction in accuracy
with respect to using English alone. Moreover, we
also showed that languages other than English can
achieve an F-measure in subjectivity annotation
of over 75%, without using any manually crafted
resources for these languages. Furthermore, by
combining the predictions made by monolingual
classifiers using a majority vote learner, we are
able to generate sentence-level subjectivity anno-
tated data with a precision of 83% and a recall
level above 50%. Such high-precision classifiers
may be later used not only to create subjectivity-
annotated data in the target language, but also to
generate the seeds needed to sustain a language-
specific bootstrapping.
To conclude and provide an answer to the ques-
tion formulated in the title, more languages are
better, as they are able to complement each other,
and together they provide better classification re-
sults. When one language cannot provide suffi-
cient information, another one can come to the
rescue.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
References
Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.
1990. Emotions from text: machine learning for text-
based emotion prediction. Intelligence.
Balog, Krisztian, Gilad Mishne, and Maarten De Rijke.
2006. Why Are They Excited? Identifying and Explain-
ing Spikes in Blog Mood Levels. In Proceedings of the
35
11th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006), Trento,
Italy.
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual Subjectivity Analysis Using
Machine Translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2008), pages 127?135, Honolulu.
Bautin, Mikhail, Lohit Vijayarenu, and Steven Skiena. 2008.
International Sentiment Analysis for News and Blogs. In
Proceedings of the International Conference on Weblogs
and Social Media (ICWSM-2008), Seattle, Washington.
Blitzer, John, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders: Do-
main Adaptation for Sentiment Classification. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational (ACL-2007), pages 440?447, Prague,
Czech Republic. Association for Computational Linguis-
tics.
Carenini, Giuseppe, Raymond T Ng, and Xiaodong Zhou.
2008. Summarizing Emails with Conversational Cohe-
sion and Subjectivity. In Proceedings of the Association
for Computational Linguistics: Human Language Tech-
nologies (ACL- HLT 2008), pages 353?361, Columbus,
Ohio.
Esuli, Andrea and Fabrizio Sebastiani. 2006a. Determining
Term Subjectivity and Term Orientation for Opinion Min-
ing. In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguistics
(EACL-2006), volume 2, pages 193?200, Trento, Italy.
Esuli, Andrea and Fabrizio Sebastiani. 2006b. SentiWord-
Net: A Publicly Available Lexical Resource for Opinion
Mining. In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation, pages 417?422.
Esuli, Andrea, Fabrizio Sebastiani, and Ilaria C Urciuoli.
2008. Annotating Expressions of Opinion and Emotion
in the Italian Content Annotation Bank. In Proceedings
of the Sixth International Language Resources and Eval-
uation (LREC-2008), Marrakech, Morocco.
Hu, Minqing and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proceedings of ACM Confer-
ence on Knowledge Discovery and Data Mining (ACM-
SIGKDD-2004), pages 168?177, Seattle, Washington.
Ku, Lun-wei, Yu-ting Liang, and Hsin-hsi Chen. 2006.
Opinion Extraction, Summarization and Tracking in
News and Blog Corpora. In Proceedings of AAAI-2006
Spring Symposium on Computational Approaches to An-
alyzing Weblogs, number 2001, Boston, Massachusetts.
Lloyd, Levon, Dimitrios Kechagias, and Steven Skiena,
2005. Lydia : A System for Large-Scale News Analysis
( Extended Abstract ) News Analysis with Lydia, pages
161?166. Springer, Berlin / Heidelberg.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe. 2007.
Learning Multilingual Subjective Language via Cross-
Lingual Projections. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics
(ACL-2007), pages 976?983, Prague, Czech Republic.
Riloff, Ellen and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2003), pages 105?112, Sap-
poro, Japan.
Stone, Philip J, Marshall S Smith, Daniel M Ogilivie, and
Dexter C Dumphy. 1967. The General Inquirer: A Com-
puter Approach to Content Analysis. /. The MIT Press,
1st edition.
Wan, Xiaojun. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009), Singapore.
Wiebe, Janyce and Rada Mihalcea. 2006. Word Sense and
Subjectivity. In Proceedings of the joint conference of
the International Committee on Computational Linguis-
tics and the Association for Computational Linguistics
(COLING-ACL-2006), Sydney, Australia.
Wiebe, Janyce and Ellen Riloff. 2005. Creating Subjec-
tive and Objective Sentence Classifiers from Unannotated
Texts. In Proceeding of CICLing-05, International Con-
ference on Intelligent Text Processing and Computational
Linguistics, pages 486?497, Mexico City, Mexico.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 39(2-
3):165?210.
Wu, Yejun. 2008. Classifying attitude by topic aspect for
English and Chinese document collections.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from opin-
ions and identifying the polarity of opinion sentence. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
36
Coling 2010: Poster Volume, pages 647?655,
Beijing, August 2010
Text Mining for Automatic Image Tagging
Chee Wee Leong and Rada Mihalcea and Samer Hassan
Department of Computer Science and Engineering
University of North Texas
cheeweeleong@my.unt.edu, rada@cs.unt.edu, samer@unt.edu
Abstract
This paper introduces several extractive
approaches for automatic image tagging,
relying exclusively on information mined
from texts. Through evaluations on two
datasets, we show that our methods ex-
ceed competitive baselines by a large mar-
gin, and compare favorably with the state-
of-the-art that uses both textual and image
features.
1 Introduction
With continuously increasing amounts of images
available on the Web and elsewhere, it is impor-
tant to find methods to annotate and organize im-
age databases in meaningful ways. Tagging im-
ages with words describing their content can con-
tribute to faster and more effective image search
and classification. In fact, a large number of ap-
plications, including the image search feature of
current search engines (e.g., Yahoo!, Google) or
the various sites providing picture storage services
(e.g., Flickr, Picasa) rely exclusively on the tags
associated with an image in order to search for rel-
evant images for a given query.
However, the task of developing accurate and
robust automatic image annotation models entails
daunting challenges. First, the availability of large
and correctly annotated image databases is cru-
cial for the training and testing of new annotation
models. Although a number of image databases
have emerged to serve as evaluation benchmarks
for different applications, including image anno-
tation (Duygulu et al, 2002), content-based im-
age retrieval (Li and Wang, 2008) and cross
language information retrieval (Grubinger et al,
2006), such databases are almost exclusively cre-
ated by manual labeling of keywords, requiring
significant human effort and time. The content of
these image databases is often restricted only to a
few domains, such as medical and natural photo
scenes (Grubinger et al, 2006), and specific ob-
jects like cars, airplanes, or buildings (Fergus et
al., 2003). For obvious practical reasons, it is im-
portant to develop models trained and evaluated
on more realistic and diverse image collections.
The second challenge concerns the extraction
of useful image and text features for the construc-
tion of reliable annotation models. Most tradi-
tional approaches relied on the extraction of image
colors and textures (Li and Wang, 2008), or the
identification of similar image regions clustered as
blobs (Duygulu et al, 2002) to derive correlations
between image features and annotation keywords.
In comparison, there are only a few efforts that
leverage on the multitude of resources available
for natural language processing to derive robust
linguistic-based image annotation models. One
of the earliest efforts involved the use of captions
for face recognition in photographs through the
construction of a specific lexicon that integrates
linguistic and photographic information (Srihari
and Burhans, 1994). More recently, several ap-
proaches have proposed the use of WordNet as
a knowledge-base to improve content-based im-
age annotation models, either by removing noisy
keywords through semantic clustering (Jin et al,
2005) or by inducing a hierarchical classification
of candidate labels (Srikanth et al, 2005).
In this paper, we explore the use of several natu-
ral language resources to construct image annota-
tion models that are capable of automatically tag-
ging images from unrestricted domains with good
accuracy. Unlike traditional image annotation
methodologies that generate tags using image-
based features, we propose to extract them in a
manner analogous to keyword extraction. Given a
target image and its surrounding text, we extract
those words and phrases that are most likely to
represent meaningful tags. More importantly, we
647
are interested to investigate the potential of such
linguistic-based models on image annotation ac-
curacy and reliability. Our work is motivated by
the need for annotation models that can be effi-
ciently applied on a very large scale (e.g. har-
vesting images from the web), which are required
in applications that cannot afford the complexity
and time associated with current image process-
ing techniques.
The paper makes the following contributions.
We first propose a new evaluation framework for
image tagging, which is based on an analogy
drawn between the tasks of image labeling and
lexical substitution. Next, we present three extrac-
tive approaches for the task of image annotation.
The methods proposed are based only on the text
surrounding an image, without the use of image
features. Finally, by combining several orthogo-
nal methods through machine learning, we show
that it is possible to achieve a performance that is
competitive to a state-of-the-art image annotation
system that relies on visual and textual features,
thus demonstrating the effectiveness of text-based
extractive annotation models.
2 Related Work
Several online systems have sprung into exis-
tence to achieve annotation of real world images
through human collaborative efforts (Flickr) and
stimulating competition (von Ahn and Dabbish,
2004). Although a large number of image tags can
be generated in short time, these approaches de-
pend on the availability of human annotators and
are far from being automatic. Similarly, research
in the other direction via text-to-image synthesis
(Li and Fei-Fei, 2008; Collins et al, 2008; Mi-
halcea and Leong, 2009) has also helped to har-
vest images, mostly for concrete words, by refin-
ing image search engines.
Most approaches to automatic image annota-
tion have focused on the generation of image la-
bels using annotation models trained with image
features and human annotated keywords (Barnard
and Forsyth, 2001; Jeon et al, 2003; Makadia et
al., 2008; Wang et al, 2009). Instead of predict-
ing specific words, these methods generally target
the generation of semantic classes (e.g. vegeta-
tion, animal, building, places etc), which they can
achieve with a reasonable amount of success. Re-
cent work has also considered the generation of
labels for real-world images (Li and Wang, 2008;
Feng and Lapata, 2008). To our knowledge, we
are unaware of any other work that performs ex-
tractive annotation for images from unrestricted
domains through the exclusive use of textual fea-
tures.
3 Dataset
As the methods we propose are extractive, stan-
dard image databases with no surrounding text
such as Corel (Duygulu et al, 2002) are not suit-
able, nor are they representative for the challenges
associated with raw data from unrestricted do-
mains. We thus create our own dataset using im-
ages randomly extracted from the Web.
To avoid sparse searches, we use a list of the
most frequent words in the British National Cor-
pus as seed words, and query the web using the
Google Image API. A webpage is randomly se-
lected from the query results if it contains a single
image in the specified size range (width and height
of 275 to 1000 pixels1) and its text contains more
than 10 words. Next, we use a Document Object
Model (DOM) HTML parser2 to extract the con-
tent of the webpage. Note that we do not perform
manual filtering of our images except where they
contain undesirable qualities (e.g. porn, corrupted
or blank images).
In total, we collected 300 image-text pairs from
the web. The average image size is 496 pixels
width and 461 pixels height. The average text
length is 278 tokens and the average document ti-
tle length is 6 tokens. In total, there are 83,522
words and the total vocabulary is 8,409 words.
For each image, we also create a gold stan-
dard of manually assigned tags, by using the la-
bels assigned by five human annotators. The im-
age annotation is conducted via Amazon Mechan-
ical Turk, which was shown in the past to produce
reliable annotations (Snow et al, 2008). For in-
creased annotation reliability, we only accept an-
notators with an approval rating of 98%.
Given an image, an annotator extracts from
the associated text a minimum of five words or
collocations.Annotators can choose words freely
from the text, while collocation candidates are re-
stricted to a fixed set obtained from the n-grams (n
? 7) in the text that also appear as article names or
surface forms in Wikipedia. Moreover, when in-
terpreting the image, the annotators are instructed
to focus on both the denotational and conotational
attributes present in the image3.
1Empirically determined to filter advertisements, banners
and undersized images.
2http://search.cpan.org/dist/HTML-ContentExtractor/
3Annotation instructions, dataset and gold standard can
648
Normal Image Mode Image
Gold standard czech (5), festival (5), oklahoma (4), yukon (4),
october (4), web page (2), the first (2), event (2),
success (1), every (1), year (1)
train (5), station (4), steam (4), trans siberian (4),
steam train (4), travel (3), park (3), siberian (3),
old (3), photo (1), trans (2), yekaterinburg (2),
the web (2), photo host (1)
Table 1: Two sample images. The number besides each label indicates the number of human annotators
agreeing on that label. Note that the mode image has a tag (i.e.?train?) in the gold standard set most
frequently selected by the annotators
4 A New Evaluation Framework : Image
Tagging as Lexical Substitution
While evaluations of previous work in image an-
notation were often based on labels provided with
the images, such as tags or image captions, in our
dataset such annotations are either missing or un-
reliable. We rely instead on human-produced ex-
tractive annotations (as described in the previous
section), and formulate a new evaluation frame-
work based on the intuition that an image can be
substituted with one or more tags that convey the
same meaning as the image itself. Ideally, there is
a single tag that ?best? describes the image over-
all (i.e. the gold standard tag agreed by the major-
ity of human annotators), but there are also mul-
tiple tags that describe the fine-grained concepts
present in the image. Our evaluation framework
is inspired by the lexical substitution task (Mc-
Carthy and Navigli, 2007), where a system at-
tempts to generate a word (or a set of words) to
replace a target word, such that the meaning of
the sentence is preserved.
Given this analogy, the evaluation metrics used
for lexical substitution can be adapted to the eval-
uation of image tagging. Specifically, we measure
the precision and the recall of a tagging method
using four subtasks: best normal: provides preci-
sion and recall for the top-ranked tag returned by a
method; best mode: provides precision and recall
only if the top-ranked tag by a method matches the
tag in the gold standard that was most frequently
selected by the annotators; out of ten (oot) nor-
be downloaded at
http://lit.csci.unt.edu/index.php/Downloads
mal: provides precision and recall for the top ten
tags by the system; and out of ten (oot) mode:
similar to best mode, but it considers the top ten
tags returned by the system instead of one. Table
1 show examples of a normal and a mode image.
Formally, let us assume that H is the set
of annotators, namely {h1, h2, h3, ...}, and I,
{i1, i2, i3, ...} is the set of images for which each
human annotator provide at least five tags. For
each ij, we calculate mj, which is the most fre-
quent tag for that image, if available. We also col-
lect all rkj, which is the set of tags for the image
ij from the annotator hk.
Let the set of those images where there is a tag
agreed upon by the most annotators (i.e. the im-
ages with a mode) be denoted by IM, such that
IM ? I. Also, let A ? I be the set of images for
which the system provides more than one tag. Let
the corresponding set for the images with modes
be denoted by AM, such that AM ? IM. Let aj ? A
be the set of system?s extracted tags for the image
ij.
Thus, for each image ij, we have the set of tags
extracted by the system, and the set of tags from
the human annotators. As the next step, the multi-
set union of the human tags is calculated, and the
frequencies of the unique tags is noted. Therefore,
for image ij, we calculate Rj, which is
?
rkj, and
the individual unique tag in Rj, say res, will have
a frequency associated with it, namely freqres.
Given this setting, the precision (P ) and recall
(R) metrics we use are defined below.
649
Best measures:
P =
?
aj :ij?A
?
res?aj
freqres
|aj |
|Rj |
|A|
R =
?
aj :ij?I
?
res?aj
freqres
|aj |
|Rj |
|I|
modeP =
?
bestguessj?AM (1if best guess = mj)
|AM |
modeR =
?
bestguessj?IM (1if best guess = mj)
|IM |
Out of ten (oot) measures:
P =
?
aj :ij?A
?
res?aj
freqres
|Rj |
|A|
R =
?
aj :ij?I
?
res?aj
freqres
|Rj |
|I|
modeP =
?
aj :ij?AM (1if any guess ? aj = mj)
|AM |
modeR =
?
aj :ij?IM (1if any guess ? aj = mj)
|IM |
As a simplified example (with less tags), con-
sider ij showing a picture of a Chihuahua being
labeled by five annotators with the following tags :
Annotator Tags
1 dog,pet
2 chihuahua
3 animal,dog
4 dog,chihuahua
5 dog
In this case, r1j = {dog,pet}, r2j = {chihuahua},
r3j = {animal,dog} and so on. The tag ?dog? ap-
pears the most frequent among the five annotators,
hence mj = {dog}. Rj={dog, dog, dog, dog, chi-
huahua, chihuahua, animal, pet}. The res with
associated frequencies would be dog 4, chihuahua
2, animal 1, pet 1. If the system?s proposed tag for
ij is {dog, animal}, then the numerator of P and
R for best subtask would be
4+1
2
8 = 0.313. Simi-
larly, the numerator of P and R for oot subtask is
4+1
8 = 0.625.
5 Extractive Image Annotation
The main idea underlying our work is that we can
perform effective image annotation using infor-
mation drawn from the associated text. Follow-
ing (Feng and Lapata, 2008), we propose that an
image can be annotated with keywords capturing
the denotative (entities or objects depicted) and
connotative (semantics or ideologies interpreted)
attributes in the image. For instance, a picture
showing a group of athletes and a ball may also be
tagged with words like ?soccer,? or ?sports activ-
ity.? Specifically, we use a combination of knowl-
edge sources to model the denotative quality of a
word as its picturability, and the connotative at-
tribute as its saliency. The idea of visualness and
salience as textual features for discovering named
entities in an image was first pursued by (De-
schacht and Moens, 2007), using data from the
news domain. In contrast, we are able to per-
form annotation of images from unrestricted do-
mains using content words (nouns, verbs and ad-
jectives). In the following, we first describe three
unsupervised extractive approaches for image an-
notation, followed by a supervised method using a
re-ranking hypothesis that combines all the meth-
ods.
5.1 Flickr Picturability
Featuring a repository of four billion images,
Flickr (http://www.flickr.com) is one of the most
comprehensive image resources on the web. As a
photo management and sharing application, it pro-
vides users with the ability to tag, organize, and
share their photos online. Interestingly, an inspec-
tion of Flickr tags for randomly selected images
reveal that users tend to describe the denotational
attributes of images, using concrete and picturable
words such as cat, bug, car etc. This observation
lends evidence to Flickr?s suitability as a resource
to model the picturability of words.
Given the text (T ) of an image, we can use
the getRelatedTags API to retrieve the most fre-
quent Flickr tags associated with a given word,
and use them as corpus evidence to filter or pro-
mote words in the text. In the filtering phase
we ignore any words that return an empty list of
Flickr?s related tags, based on the assumption that
these words are not used in the Flickr tags repos-
itory. We also discard words with a length that is
less than three characters (?=3). In the promotion
phase, we reward any retrieved tags that appear as
surface forms in the text. This reward is propor-
tional to the term frequency of these tags in the
650
Algorithm 1 Flickr Picturability Algorithm
Start : L[]=? , TF[]=tf of each word in T
for each word in T do
if length(word) ? ? then
RelatedTags=getRelatedTags(word);
if size(RelatedTags) > 0 then
L[word]+=?*TF[word]
for each tag in RelatedTags do
if exists TF [tag] then
L[tag]+=TF[tag]
end if
end for
end if
end if
end for
text. Additionally, we also include in the final la-
bel set any word that returns a non-empty related
tags set with a discounted weight (?=0.5) of its
term frequency, to the end of enriching our labels
set while assuring more credit are given to the pic-
turable words.
To extract multiword labels, we locate all n-
grams formed exclusively from our extracted set
of possible labels. The subsequent score for each
of these n-grams is:
L[wi..wi+k] = (
j=i+k?
j=i
L[wj])/k
By reverse sorting the associative array in L, we
can retrieve the top K words to label the image.
For illustration, let us consider the following text
snippet.
On the Origin of Species, published by
Charles Darwin in 1859, is considered
to be the foundation of evolutionary bi-
ology.
After removing stopwords, we consider the re-
maining words as candidate labels. For each
of these candidates wi (i.e. origin, species,
published, charles, darwin, foundation,
evolutionary, and biology), we query Flickr and
obtain their related tag set Ri. origin, published,
and foundation return an empty set of related
tags and hence are removed from our set of can-
didate labels, leaving species, charles, darwin,
evolutionary, and biology as possible annotation
keywords with the initial score of 0.5. In the pro-
motion phase, we score each wi based on the num-
ber of votes it receives from the remaining wj
(Figure 1). Each vote represents an occurrence
of the candidate tag wi in the related tag set Rj
of the candidate tag wj . For example, darwin
appeared in the Flickr related tags for charles,
evolutionary, and biology, hence it has a weight
of 3.5. The final list of candidate labels are shown
in Table 2.
... Species, published by Charles Darwin ? founda!on of evolu!onary biology
Figure 1: Flickr Picturability Labels
Label S(wi)
darwin 3.5
charles darwin 2.5
charles 1.5
biology 1.5
evolutionary biology 1.0
evolutionary 0.5
species 0.5
Table 2: Candidate labels obtained for a sample
text using the Flickr model
5.2 Wikipedia Salience
We hypothesize that an image often describes the
most important concepts in the associated text.
Thus, the keywords selected from a text could be
used as candidate labels for the image. We use
a graph-based keyword extraction method similar
to (Mihalcea and Tarau, 2004), enhanced with a
semantic similarity measure. Starting with a text,
we extract all the candidate labels and add them as
vertices in the graph. A measure of word similar-
ity is then used to draw weighted edges between
the nodes. Using the PageRank algorithm, the
words are assigned with a score indicating their
salience within the given text.
To determine the similarity between words, we
use a directed measure of similarity. Most word
similarity metrics provide a single-valued score
between a pair of words w1 and w2 to indicate
their semantic similarity. Intuitively, this is not al-
ways the case, as w1 may be represented by con-
cepts that are entirely embedded in other concepts,
represented by w2. In psycholinguistics terms, ut-
tering w1 may bring to mind w2, while the appear-
ance of w2 without any contextual clues may not
associate with w1. For example, Obama brings
to mind the concept of president, but president
651
may trigger other concepts such as Washington,
Lincoln, Ford etc., depending on the existing
contextual clues. Thus, the degree of similarity
of w1 with respect to w2 should be separated from
that of w2 with respect to w1. Specifically, we use
the following measure of similarity, based on the
Explicit Semantic Analysis (ESA) vectors derived
from Wikipedia (Gabrilovich and Markovitch,
2007):
DSim(wi, wj) =
Cij
Ci
? Sim(wi, wj)
where Cij is the count of articles in Wikipedia
containing words wi and wj , Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the input words.The directional weight (Cij /Ci)
amounts to the degree of association of wi with re-
spect to wj . Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of ?recommenda-
tion? where we say w1 recommends w2 if and
only if there is a directed edge from w1 to w2, with
the weight of the recommendation being the direc-
tional similarity score. Starting with this graph,
we use the graph iteration algorithm from (Mi-
halcea and Tarau, 2004) to calculate a score for
each vertex in the graph. The output is a sorted
list of words in decreasing order of their ranks,
which are used as candidate labels to annotate the
image. This is achieved by using Cj instead of Ci
for the denominator in the directional weight. As
an example, consider the text snippet :
Microsoft Corporation is a multina-
tional computer technology corporation
that develops, manufactures, licenses,
and supports a wide range of software
products for computing devices
after stopword removal, the list of nouns ex-
tracted is Microsoft, computer, corporation, de-
vices, products, technology, software. Note that
the top-ranked word must infer some or all of the
words in the text. In this case, the word Microsoft
infers the terms computer, technology and soft-
ware.
To calculate the semantic relatedness between
two collocations, we use a simplified version of
the text-to-text relatedness technique proposed by
and (Mihalcea et al, 2006) that incorporate the
directional inferential similarity as an underlying
semantic metric.
5.3 Topical Modeling
Intuitively, every text is written with a topic in
mind, and the associated image serves as an illus-
tration of the text meaning. In this paper, we in-
vestigate the effect of topical modeling on image
annotation accuracy directly. We use the Pachinko
Allocation Model (PAM) (Li and McCallum,
2006) to model the topics in a text, where key-
words forming the dominant topic are assumed as
our set of annotation keywords. Compared with
previous topic modeling approaches, such as La-
tent Dirichlet alocation (LDA) or its improved
variant Correlated Topic Model (CTM) (Blei and
Lafferty, 2007), PAM captures correlations be-
tween all the topic pairs using a directed acyclic
graph (DAG). It also supports finer-grained topic
modeling, and has state-of-the-art performance on
the tasks of document classification and topical
keyword coherence. Given a text, we use the PAM
model to infer a list of super-topics and sub-topics
together with words weighted according to the
likelihood that they belong to each of these topics.
For each text, we retrieve the top words belong-
ing to the dominant super-topic and sub-topic. We
use 50 super-topics and 100 sub-topics as operat-
ing parameters for PAM, since these values were
found to provide good results in previous work on
topic modeling. Default values are used for other
parameters in the model.
5.4 Supervised Learning
The three tagging methods target different aspects
of what constitutes a good label for an image. We
use them as features in a machine learning frame-
work, and introduce a final rank attribute S(tj),
which is a linear combination of the reciprocals of
the rank of each tag as given by each method,
S(tj) =
?
m?methods
?m
1
rmtj
where rmtj is the rank for tag tj given by method
m. The weight of each method ?m is estimated
from the training set using information gain val-
ues. Since our predicted variable (mode precision
or recall) is continuous, we use the Support Vec-
tor Algorithm (nu-SVR) implementation of SVM
(Chang and Lin, 2001) to perform regression anal-
ysis on the weights for each method via a radial
basis function kernel. A ten-fold cross-validation
is applied on the entire dataset of 300 images.
652
Best out-of-ten (oot)
Normal Mode Normal Mode
Models P R P R P R P R
Flickr picturability 6.32 6.32 78.57 78.57 35.61 35.61 92.86 92.86
Wikipedia Salience 6.40 6.40 7.14 7.14 35.19 35.19 92.86 92.86
Topic modeling 5.99 5.99 42.86 42.86 37.13 37.13 85.71 85.71
Combined (SVM) 6.87 6.87 67.49 67.49 37.85 37.85 100.00 100.00
Doc Title 6.40 6.40 75.00 75.00 18.97 18.97 82.14 82.14
tf * idf 5.94 5.94 14.29 14.29 38.40 38.40 78.57 78.57
Random 3.76 3.76 3.57 3.57 30.20 30.20 50.00 50.00
Upper bound (human) 12.23 12.07 81.48 81.48 82.44 81.55 100.00 100.00
Table 3: Results obtained on the Web dataset
6 Experiments and Evaluations
We evaluate the performance of each of the three
tagging methods separately, followed by an eval-
uation of the combined method. Each system pro-
duces a ranked list of K words or collocations
as tags assigned to a given image. A system can
discretionary generate less (but not more) than K
tags, depending on its confidence level.
For comparison, we implement three baselines:
tf*idf, Doc Title and Random. For tf*idf, we use
the British National Corpus to calculate the idf
scores, while the frequency of a term is calcu-
lated from the entire text associated with an im-
age. The Doc Title baseline is similar, except that
the term frequency is calculated based on the title
of the document. The Random baseline randomly
selects words from a co-occurrence window of
size K before and after an image as its annota-
tion. Following other tagging methods, we apply a
pre-processing stage, where we part-of-speech tag
the text (to retain only nouns), followed by stem-
ming. We also determine an upper bound, which
is calculated as follows. For each image, the la-
bels assigned by each of the five annotators are
in turn evaluated against a gold standard consist-
ing of the annotations of the other four annotators.
The best performing annotator is then recorded.
This process is repeated for each of the 300 im-
ages, and the average precision and recall are cal-
culated. This represents an upper bound, as it is
the best performance that a human can achieve on
this dataset. Table 3 shows our experimental re-
sults.
Among the individual methods, the method im-
plementing Flickr picturability has the highest in-
dividual score for best and oot modes, yielding
a precision and recall of 78.57% and 92.86% re-
spectively. The Wikipedia Saliency method also
scores the highest (jointly with Flickr) in the oot
mode, but for the best mode achieves a score only
marginally better than the random baseline. A
plausible explanation is that it tends to favor ?all-
inferring? over-specific labels, while the most fre-
quently selected tags in mode pictures are typi-
cally more ?picturable? than being specific (e.g.
?train? for the mode picture in Table 1). The topic
modeling method has mixed results: its scores
for oot normal and mode are somewhat compet-
itive with tf*idf, but it scores consistently lower
than the DocTitle in the best subtask, possibly
due to the absence of a more sophisticated re-
ranking algorithm tailored for the image annota-
tion task other than the intrinsic ranking mecha-
nism in PAM. It is worth noting that the combined
supervised system provides the overall best results
(6.87%) on the best normal, and achieves a perfect
precision and recall (100%) for oot mode, which
means perfect agreement with the human tagging.
7 Comparison with Related Work
We also compare our work against (Feng and Lap-
ata, 2008) as it allows for a direct comparison with
models using both image and textual features un-
der a standard evaluation framework. We obtained
the BBC dataset used in their experiments, which
consists of 3121 training and 240 testing images.
In this dataset, images are implicitly tagged with
captions by the author of the corresponding BBC
article. The evaluations are run against these cap-
tions.
In their experiments, Feng and Lapata created
four annotation models. The first two (tf*idf and
Document Title) are the same as used in our base-
line experiments. The third model (Lavrenko03)
is an application of the continuous relevance
model in (Jeon et al, 2003), trained with the BBC
image features and captions. Finally, the forth
(ExtModel) is an extension of the relevance model
using additional information in auxiliary texts.
Briefly, the model assumes a multiple Bernoulli
distribution for words in a caption, and generates
tags for a test image using a weighted combina-
tion of the accompanying document, caption and
image features learned during training.
653
Top 10 Top 15 Top 20
Models P R F1 P R F1 P R F1
tf*idf 4.37 7.09 5.41 3.57 8.12 4.86 2.65 8.89 4.00
DocTitle 9.22 7.03 7.20 9.22 7.03 7.20 9.22 7.03 7.20
Lavrenko03 9.05 16.01 11.81 7.73 17.87 10.71 6.55 19.38 9.79
ExtModel 14.72 27.95 19.82 11.62 32.99 17.18 9.72 36.77 15.39
Flickr picturability 12.13 22.82 15.84 9.52 26.82 14.05 8.23 29.80 12.90
Wikipedia Salience 11.63 21.89 15.18 9.28 26.20 13.70 7.81 29.41 12.35
Topic Modeling 11.42 21.49 14.91 9.28 26.20 13.70 7.86 29.57 12.42
Combined (SVM) 13.38 25.17 17.47 11.08 31.29 16.37 9.50 35.76 15.01
Table 4: Results obtained on the BBC dataset used in (Feng and Lapata, 2008)
The experimental setup is similar to the earlier
section, but a few modifications are made for a fair
and direct comparison. First, we extend our mod-
els coverage to include content words (i.e. nouns,
verbs, adjectives) determined using the Tree Tag-
ger (Schmid, 1994). Second, no collocations are
used. Third, we adopt the evaluation framework
used by Feng and Lapata to extract the top 10, 15
and 20 tags. Note that in our methods, the extrac-
tion of tags for a test image is only done on the
document surrounding the image, after excluding
the caption. As the number of negative examples
(words not present in the caption) greatly outnum-
ber the positive instances, we employ an under-
sampling method (Kubat and Matwin, 1997) to
balance the dataset for training.
The results are shown in Table 4. Interest-
ingly, all our unsupervised extraction-based mod-
els perform consistently above the supervised
Lavrenko03 model, indicating that textual fea-
tures are more informative than captions and im-
age features taken together. Comparing with mod-
els using significantly less document informa-
tion (tf*idf and Doc title), our models gain even
greater advantage. Note that the title of any BBC
article does not exceed 10 words, hence compar-
ison is only meaningful given the top 10 tags re-
trieved.
Feng and Lapata used LDA to perform rerank-
ing of final candidates in their ExtModel. How-
ever, when used as a model alone, the PAM topic
model achieved promising scores in all the cate-
gories, performing best for top 10 keywords (F1
of 14.91%). Flickr picturability stands out as
the best performing unsupervised method, scor-
ing the highest precision (12.13%, top 10), recall
(29.80%, top 20) and F1 (15.84%, top 10).
Overall, this comparative evaluation yields
some important insights. First, our combined
model using SVM is statistically better (p<0.1 for
top 10, 15, 20) than the Laverenko03 model, but
not statistically different from the ExtModel. This
demonstrates the effectiveness of textual-based
models over traditional models trained with im-
age features and captions. While it is intuitively
clear that image features help in improving tag-
ging performance, we show that mining only the
text surrounding an image, where it exists, can
yield a performance that is comparable to a state-
of-the-art system that uses both textual and vi-
sual features. Moreover, an increase in complex-
ity of a model by using more features may hinder
its applicability to large datasets, but not neces-
sarily improving annotation performance (Maka-
dia et al, 2008). On this, text-based annotation
models can provide a desirable compromise. For
instance, our unsupervised models implementing
Flickr picturability and Wikipedia Salience are
able to extract annotations from a BBC article (av-
erage 133.85 tokens) in approximately 1 second
and 20 seconds respectively.
8 Conclusions and Future Work
In this paper, we introduced several text-based ex-
tractive approaches for automatic image annota-
tion and showed that they compare favorably with
the state-of-the-art in image annotation using both
text and image features. We believe our work
has practical applications in mining and annotat-
ing images over the Web, where texts are nat-
urally associated with images, and scalability is
important. Our next direction seeks to derive ro-
bust annotation models using additional ontolog-
ical knowledge-bases. We would also like to ad-
vance the the state-of-the-art by augmenting cur-
rent textual models with image features.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Sci-
ence Foundation.
654
References
Kobus Barnard and David Forsyth. 2001. Learning the
semantics of words and pictures. In Proceedings of
International Conference on Computer Vision.
David Blei and John Lafferty. 2007. A correlated topic
model of science. In Annals of Applied Statistics,
volume 1, pages 17?35.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei.
2008. Towards scalable dataset construction: An
active learning approach. In Proceedings of Euro-
pean Conference on Computer Vision.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the Association for Computational
Linguistics.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as ma-
chine translation:learning a lexicon for a fixed im-
age vocabulary. In Proceedings of the 7th European
Conference on Computer Vision.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of the Association for Computa-
tional Linguistics.
Rob Fergus, Pietro Perona, and Andrew Zisserman.
2003. Object class recognition by unsupervised
scale-invariant learning. In Proceedings of the In-
ternational Conference on Computer Vision and
Pattern Recognition.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In International
Joint Conferences on Artificial Intelligence.
Michael Grubinger, Clough Paul, Mller Henning, and
Deselaers Thomas. 2006. The iapr benchmark: A
new evaluation resource for visual information sys-
tems. In International Conference on Language Re-
sources and Evaluation.
Jiwoon Jeon, Victor Lavrenko, and R Manmatha.
2003. Automatic image annotation and retrieval us-
ing cross-media relevance models. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.
Yohan Jin, Latifur Khan, Lei Wang, and Mamoun
Awad. 2005. Image annotations by combining mul-
tiple evidence & wordnet. In Proceedings of Annual
ACM Multimedia.
Miroslav Kubat and Stan Matwin. 1997. Addressing
the curse of imbalanced training sets: one-sided se-
lection. In Proceedings of International Conference
on Machine Learning.
Li-Jia Li and Li Fei-Fei. 2008. Optimol: au-
tomatic online picture collection via incremental
model learning. In International Journal of Com-
puter Vision.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the International Con-
ference on Machine learning.
Jia Li and James Wang. 2008. Real-time computer-
ized annotation of pictures. In Proceedings of Inter-
national Conference on Computer Vision.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation. In
Proceedings of European Conference on Computer
Vision.
Diana McCarthy and Roberto Navigli. 2007. The se-
meval English lexical substitution task. In Proceed-
ings of the ACL Semeval workshop.
Rada Mihalcea and Chee Wee Leong. 2009. To-
wards communicating simple sentences using pic-
torial representations. In Machine Translation, vol-
ume 22, pages 153?173.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of Association for the Advancement of Artificial
Intelligence, pages 775?780.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of Empirical Methods
in Natural Language Processing.
Srihari and Burhans. 1994. Visual semantics: Extract-
ing visual information from text accompanying pic-
tures. In Proceedings of the American Association
for Artificial Intelligence.
Munirathnam Srikanth, Joshua Varner, Mitchell Bow-
den, and Dan Moldovan. 2005. Exploiting ontolo-
gies for automatic image annotation. In Proceed-
ings of the ACM Special Interest Group on Research
and Development in Information Retrieval.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
ACM Special Interest Group on Computer Human
Interaction.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Si-
multaneous image classification and annotation. In
Proceedings of IEEE Conference on Computer Vi-
sion and Pattern Recognition.
655
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1057?1067,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Cross Language Text Classification by Model Translation and
Semi-Supervised Learning
Lei Shi
Yahoo! Global R&D
Beijing, China
lshi@yahoo-inc.com
Rada Mihalcea
University of North Texas
Denton, TX, U.S.A.
rada@cs.unt.edu
Mingjun Tian
Yahoo! Global R&D
Beijing, China
mingjun@yahoo-inc.com
Abstract
In this paper, we introduce a method that au-
tomatically builds text classifiers in a new lan-
guage by training on already labeled data in
another language. Our method transfers the
classification knowledge across languages by
translating the model features and by using
an Expectation Maximization (EM) algorithm
that naturally takes into account the ambigu-
ity associated with the translation of a word.
We further exploit the readily available un-
labeled data in the target language via semi-
supervised learning, and adapt the translated
model to better fit the data distribution of the
target language.
1 Introduction
Given the accelerated growth of the number of mul-
tilingual documents on the Web and elsewhere, the
need for effective multilingual and cross-lingual text
processing techniques is becoming increasingly im-
portant. There is a growing number of methods that
use data available in one language to build text pro-
cessing tools for another language, for diverse tasks
such as word sense disambiguation (Ng et al, 2003),
syntactic parsing (Hwa et al, 2005), information re-
trieval (Monz and Dorr, 2005), subjectivity analysis
(Mihalcea et al, 2007), and others.
In this paper, we address the task of cross-lingual
text classification (CLTC), which builds text classi-
fiers for multiple languages by using training data in
one language, thereby avoiding the costly and time-
consuming process of labeling training data for each
individual language. The main idea underlying our
approach to CLTC is that although content can be
expressed in different forms in different languages,
there is a significant amount of knowledge that is
shared for similar topics that can be effectively used
to port topic classifiers across languages.
Previous methods for CLTC relied mainly on ma-
chine translation, by translating the training data into
the language of the test data or vice versa, so that
both training and test data belong to the same lan-
guage. Monolingual text classification algorithms
can then be applied on these translated data. Al-
though intuitive, these methods suffer from two ma-
jor drawbacks.
First, most off-the-shelf machine translation sys-
tems typically generate only their best translation for
a given text. Since machine translation is known
to be a notoriously hard problem, applying mono-
lingual text classification algorithms directly on the
erroneous translation of training or test data may
severely deteriorate the classification accuracy.
Second, similar to domain adaptation in statisti-
cal machine learning, due to the discrepancy of data
distribution between the training domain and test do-
main, data distribution across languages may vary
because of the difference of culture, people?s inter-
ests, linguistic expression in different language re-
gions. So even if the translation of training or test
data is perfectly correct, the cross language classi-
fier may not perform as well as the monolingual one
trained and tested on the data from the same lan-
guage.
In this paper, we propose a new approach to
CLTC, which trains a classification model in the
source language and ports the model to the target
language, with the translation knowledge learned us-
ing the EM algorithm. Unlike previous methods
based on machine translation (Fortuna and Shawe-
Taylor, 2005), our method takes into account dif-
1057
ferent possible translations for model features. The
translated model serves as an initial classifier for a
semi-supervised process, by which the model is fur-
ther adjusted to fit the distribution of the target lan-
guage. Our method does not require any labeled
data in the target language, nor a machine transla-
tion system. Instead, the only requirement is a rea-
sonable amount of unlabeled data in the target lan-
guage, which is often easy to obtain.
In the following sections, we first review related
work. In section 3, we introduce our method that
translates the classification model with the trans-
lation knowledge learned using the EM algorithm.
Section 4 describes model adaptation by training the
translated model with unlabeled documents in the
target language. Experiments and evaluations are
presented in section 5 and finally we conclude the
paper in section 6.
2 Related Work
Text classification has rightfully received a lot of at-
tention from both the academic and industry com-
munities, being one of the areas in natural language
processing that has a very large number of practi-
cal applications. Text classification techniques have
been applied to many diverse problems, ranging
from topic classification (Joachims, 1997), to genre
detection (Argamon et al, 1998), opinion identifica-
tion (Pang and Lee, 2004), spam detection (Sahami
et al, 1998), gender and age classification (Schler et
al., 2006).
Text classification is typically formulated as a
learning task, where a classifier learns how to distin-
guish between categories in a given set, using fea-
tures automatically extracted from a collection of
documents. In addition to the learning methodol-
ogy itself, the accuracy of the text classifier also de-
pends to a large extent upon the amount of training
data available at hand. For instance, distinguish-
ing between two categories for which thousands of
manually annotated examples are already available
is expected to perform better than trying to separate
categories that have only a handful of labeled docu-
ments.
Some of the most successful approaches to date
for text classification involve the use of machine
learning methods, which assume that enough an-
notated data is available such that a classification
model can be automatically learned. These include
algorithms such as Naive Bayes (Joachims, 1997;
McCallum and Nigam, 1998), Rocchio classifiers
(Joachims, 1997; Moschitti, 2003), Maximum En-
tropy (Nigam et al, 1999) or Support Vector Ma-
chines (Vapnik, 1995; Joachims, 1998). If only
a small amount of annotated data is available, the
alternative is to use semi-supervised bootstrapping
methods such as co-training or self-training, which
can also integrate raw unlabeled data into the learn-
ing model (Blum and Mitchell, 1998; Nigam and
Ghani, 2000).
Despite the attention that monolingual text clas-
sification has received from the research commu-
nity, there is only very little work that was done
on cross-lingual text classification. The work that
is most closely related to ours is (Gliozzo and Strap-
parava, 2006), where a multilingual domain kernel is
learned from comparable corpora, and subsequently
used for the cross-lingual classification of texts. In
experiments run on Italian and English, Gliozzo and
Strapparava showed that the multilingual domain
kernel exceeds by a large margin a bag-of-words ap-
proach. Moreover, they demonstrated that the use
of a bilingual dictionary can drastically improve the
performance of the models learned from corpora.
(Fortuna and Shawe-Taylor, 2005; Olsson et al,
2005) studied the use of machine translation tools
for the purpose of cross language text classification
and mining. These approaches typically translate
the training data or test data into the same language,
followed by the application of a monolingual classi-
fier. The performance of such classifiers very much
depends on the quality of the machine translation
tools. Unfortunately, the development of statistical
machine translation systems (Brown et al, 1993) is
hindered by the lack of availability of parallel cor-
pora and the quality of their output is often erro-
neous. Several methods were proposed (Shi et al,
2006; Nie et al, 1999) to automatically acquire a
large quantity of parallel sentences from the web,
but such web data is however predominantly con-
fined to a limited number of domains and language
pairs.
(Dai et al, 2007) experimented with the use of
transfer learning for text classification. Although in
this method the transfer learning is performed across
1058
different domains in the same language, the under-
lying principle is similar to CLTC in the sense that
different domains or languages may share a signif-
icant amount of knowledge in similar classification
tasks. (Blum and Mitchell, 1998) employed semi-
supervised learning for training text classifiers. This
method bootstraps text classifiers with only unla-
beled data or a small amount of labeled training data,
which is close to our setting that tries to leverage la-
beled data and unlabeled data in different languages
to build text classifiers.
Finally, also closely related is the work carried out
in the field of sentiment and subjectivity analysis
for cross-lingual classification of opinions. For in-
stance, (Mihalcea et al, 2007) use an English corpus
annotated for subjectivity along with parallel text to
build a subjectivity classifier for Romanian. Sim-
ilarly, (Banea et al, 2008) propose a method based
on machine translation to generate parallel texts, fol-
lowed by a cross-lingual projection of subjectivity
labels, which are used to train subjectivity annota-
tion tools for Romanian and Spanish. A related, yet
more sophisticated technique is proposed in (Wan,
2009), where a co-training approach is used to lever-
age resources from both a source and a target lan-
guage. The technique is tested on the automatic sen-
timent classification of product reviews in Chinese,
and showed to successfully make use of both cross-
language and within-language knowledge.
3 Cross Language Model Translation
To make the classifier applicable to documents in
a foreign language, we introduce a method where
model features that are learned from the training
data are translated from the source language into
the target language. Using this translation process,
a feature associated with a word in the source lan-
guage is transferred to a word in the target language
so that the feature is triggered when the word occurs
in the target language test document.
In a typical translation process, the features would
be translated by making use of a bilingual dictio-
nary. However, this translation method has a major
drawback, due to the ambiguity usually associated
with the entries in a bilingual dictionary: a word in
one language can have multiple translations in an-
other language, with possibly disparate meanings.
If an incorrect translation is selected, it can distort
the classification accuracy, by introducing erroneous
features into the learning model. Therefore, our goal
is to minimize the distortion during the model trans-
lation process, in order to maximize the classifica-
tion accuracy in the target language.
In this paper, we introduce a method that em-
ploys the EM algorithm to automatically learn fea-
ture translation probabilities from labeled text in the
source language and unlabeled text in the target lan-
guage. Using the feature translation probabilities,
we can derive a classification model for the target
language from a mixture model with feature transla-
tions.
3.1 Learning Feature Translation Probabilities
with EM Algorithm
Given a document d from the document collectionD
in the target language, the probability of generating
the document P (d) is the mixture of generating d
with different classes c ? C:
P (d) =
?
c
P (d|c)P (c)
In our cross-lingual setting, we view the generation
of d given a class c as a two step process. In the
first step, a pseudo-document d? is generated in the
source language, followed by a second step, where
d? is translated into the observed document d in the
target language. In this generative model, d? is a la-
tent variable that cannot be directly observed. Since
d could have multiple translations d? in the source
language, the probability of generating d can then
be reformulated as a mixture of probabilities as in
the following equation.
P (d) =
?
c
P (c)
?
d?
P (d|d?, c)P (d?|c)
According to the bag-of-words assumption,
the document translation probability P (d|d?, c) is
the product of the word translation probabilities
P (wi|w?i, c) , where w?i in d? is the source language
word that wi is translated from. P (d?|c) is the prod-
uct of P (w?i|c). The formula is rewritten as:
P (d) =
?
c
P (c)
?
d?
l
?
i=1
P (wi|w?i, c)P (w?i|c)
1059
where wi is the ith word of the document d with l
words. The prior probability P (c) and the proba-
bility of the source language word w? given class c
are estimated using the labeled training data in the
source language, so we use them as known parame-
ters. P (wi|w?i, c) is the probability of translating the
word w?i in the source language to the word wi in
the target language given class c, and these are the
parameters we want to learn from the corpus in the
target language.
Using the Maximum Likelihood Estimation
(MLE) framework, we learn the model parameters ?
? the translation probability P (wi|w?i, c) ? by max-
imizing the log likelihood of a collection of docu-
ments in the target language:
?? = argmax?
m
?
j=1
log(P (dj , ?))
= argmax?
m
?
j=1
log(
?
c
P (c)
?
d?
lj
?
i=1
P (wi|w?i, c)P (w?i|c))
where m is the number of documents in the corpus
in the target language and lj is the number of words
in the document dj .
In order to estimate the optimal values of the pa-
rameters, we use the EM algorithm (Dempster et al,
1977). At each iteration of EM we determine those
values by maximizing the expectation using the pa-
rameters from the previous iteration and this itera-
tive process stops when the change in the parameters
is smaller than a given threshold. We can repeat the
following two steps for the purpose above.
? E-step
P (w?c|w) ? P (cw
?w)
P (w)
= P (w|w
?c)P (w?c)
?
c
?
w? P (w|w?c)P (w?c)
(1)
? M-step
P (w|w?c)? f(w)P (w
?c|w)
?
w?K f(w)P (w?c|w)
(2)
Algorithm 1 EM algorithm for learning translation
probabilities
Dl ? labeled data in the source language
Du ? unlabeled data in the target language
L? bilingual lexicon
1: Initialize P0(w|w?c) = 1nw? , where (w,w
?) ? L,
otherwise P0(w|w?c) = 0;
2: Compute P (w?c) with Dl according to equa-
tion 3
3: repeat
4: Calculate Pt(w?c|w) with Du based on
Pt?1(w|w?c) according to equation 1
5: Calculate Pt(w|w?c) based on Pt?1(w?c|w)
according to equation 2
6: until change of P (w|w?c) is smaller than the
threshold
7: return P (w|w?c)
Here f(w) is the occurrence frequency of the word
w in the corpus. K is the set of translation candi-
dates in the target language for the source language
wordw? according to the bilingual lexicon. P(w?c) is
the probability of occurrence of the source language
word w? under the class c. It can be estimated from
the labeled source language training data available
as follows and it is regarded as a known parameter
of the model.
P (w?c) = f(w
?c)
?
w??V f(w?c)
(3)
where V is the vocabulary of the source language.
Algorithm 1 illustrates the EM learning process,
where nw? denotes the number of translation candi-
dates for w? according to the bilingual lexicon.
Our method requires no labeled training data
in the target language. Many statistical machine
translation systems such as IBM models (Brown
et al, 1993) learn word translation probabilities
from millions of parallel sentences which are mu-
tual translations. However, large scale parallel cor-
pora rarely exist for most language pairs. (Koehn
and Knight, 2000) proposed to use the EM algo-
rithm to learn word translation probabilities from
non-parallel monolingual corpora. However, this
method estimates only class independent transla-
tion probabilities P (wi|w?i), while our approach is
able to learn class specific translation probabilities
1060
P (wi|w?i, c) by leveraging available labeled training
data in the source language. For example, the prob-
ability of translating ?bush? as ???? (small trees)
is higher than translating as ???? (U.S. president)
when the category of the text is ?botany.?
3.2 Model Translation
In order to classify documents in the target language,
a straightforward approach to transferring the classi-
fication model learned from the labeled source lan-
guage training data is to translate each feature from
the bag-of-words model according to the bilingual
lexicon. However, because of the translation ambi-
guity of each word, a model in the source language
could be potentially translated into many different
models in the target language. Thus, we think of
the probability of the class of a target language doc-
ument as the mixture of the probabilities by each
translated model from the source language model,
weighed by their translation probabilities.
P (c|d,mt) ?
?
m?t
P (m?t|ms, c)P (c|d,m?t)
where mt is the target language classification model
and m?t is a candidate model translated from the
model ms trained on the labeled training data in
the source language. This is a very generic rep-
resentation for model translation and the model m
could be any type of text classification. Specifically
in this paper, we take the Maximum Entropy (ME)
model(Berger et al, 1996) as an example for the
model translation across languages, since the ME
model is one of the most widely used text classifica-
tion models. The maximum entropy classifier takes
the form
P (c|d) = 1
Z(d)
?
w?V
e?wf(w,c)
where: V is the vocabulary of the language; f(w, c)
is the feature function associated with the word w
and class c and its value is set to 1 when w occurs in
d and the class is c or otherwise 0. ?w is the feature
weight for f(wi, c) indicating the importance of the
feature in the model. During model translation, the
feature weight for f(wi, c) is transferred to f(w?i, c)
in the target language model, where w?i is the trans-
lation of wi. Z(d) is the normalization factor which
is invariant to c and hence we can omit it for classi-
fication since our objective is to find the best c. Ac-
cording to the formulation of the Maximum Entropy
model, the document can be classified as follows.
c? = argmaxc?C
?
m?t
P (m?t|ms, c)
v
?
i=1
e?wisf(w
i
t,c)
The model translation probability P (m?t|ms, c) can
be modeled as the product of the translation proba-
bilities of each of its individual bag-of-words fea-
tures P (m?t|ms, c) ?
?l
i=1 P (wit|wis, c) and the
classification model can be further written as
c? = argmaxc?C
?
m?t
v
?
i=1
P (wit|wis, c)e
?wisf(w
i
t,c)
where feature translation probabilities P (wit|wis, c)
are estimated with the EM algorithm described in
the previous section. Note that if the average number
of translations for a word w is n and v is the num-
ber of words in the vocabulary there are nv possible
models m?t translated from ms. However, we can
do the following mathematical transformation on the
equation which leads to a polynomial time complex-
ity algorithm. The idea is that instead of enumerat-
ing the exponential number of different translations
of the entire model, we will instead handle one fea-
ture at a time.
?
m?t
v
?
i=1
P (wit|wis, c)e
?wisf(w
i
t,c) =
n1
?
j=1
P (w1jt |w1s , c)e?1f(w
1j
t ,c)
?
m2,vt
v
?
i=2
P (wit|wis, c)e?if(w
i
t,c)
Here w1 is the first word in the vocabulary of the
source language and w1j is a translation of w1 in the
target language with n denoting the number its trans-
lations according to the bilingual lexicon.
?
m2,vt
are all the target language models translated from
the model consisting of the rest of the words w2 ...
wv in the source language. This process is recur-
sive until the last word wvs of the vocabulary and this
transforms the equation into a polynomial form as
1061
follows.
?
m?t
v
?
i=1
P (wit|wis, c)e
?wisf(w
i
t,c)
=
v
?
i=1
ni
?
j=1
P (wijt |wis, c)e
?wisf(w
ij
t ,c)
Based on the above transformation, the class c? for
the target language document d is then calculated
with the following equation.
c? = argmaxc?C
v
?
i=1
ni
?
j=1
P (wijt |wis, c)e
?wisf(w
ij
t ,c)
The time complexity of computing the above equa-
tion is n? v.
4 Model Adaptation with Semi-
Supervised Learning
In addition to translation ambiguity, another chal-
lenge in building a classifier using training data in
a foreign language is the discrepancy of data distri-
bution in different languages. Direct application of a
classifier translated from a foreign model may not fit
well the distribution of the current language. For ex-
ample, a text about ?sports? in (American) English
may talk about ?American football,? ?baseball,? and
?basketball,? whereas Chinese tend to discuss about
?soccer? or ?table tennis.?
To alleviate this problem, we employ semi-
supervised learning in order to adapt the model to
the target language. Specifically, we first start by us-
ing the translated classifier from English as an initial
classifier to label a set of Chinese documents. The
initial classifier is able to correctly classify a num-
ber of unlabeled Chinese documents with the knowl-
edge transferred from English training data. For
instance, words like ?game(??),? ?score(??),?
?athlete(???),? learned from English can still ef-
fectively classify Chinese documents. We then pick
a set of labeled Chinese documents with high con-
fidence to train a new Chinese classifier. The new
classifier can then learn new knowledge from these
Chinese documents. E.g. it can discover that words
like ?soccer(??)? or ?badminton(???)? occur
frequently in the Chinese ?sports? documents, while
words that are frequently occurring in English doc-
uments such as ?superbowl(???)? and ?NHL(?
Algorithm 2 Semi-supervised learning for cross-
lingual text classification
Ls ? labeled data in the source language
Ut ? unlabeled data in the target lan-
guage
1: Cs = train(Ls)
2: Ct = translate(Cs)
3: repeat
4: Label(U,Ct)
5: L? select(confidence(U,Ct))
6: Ct ? train(L)
7: until stopping criterion is met
8: return Ct
?????)? do not occur as often. Re-training the
classifier with the Chinese documents can adjust the
feature weights for these words so that the model fits
better the data distribution of Chinese documents,
and thus it improves the classification accuracy. The
new classifier then re-labels the Chinese documents
and the process is repeated for several iterations. Al-
gorithm 2 illustrates this semi-supervised learning
process.
The confidence score associated with the docu-
ments is calculated based on the probabilities of the
class. For a binary classifier the confidence of clas-
sifying the document d is calculated as:
confidence(d) =
?
?
?
?
log(P (c|d)
P (c|d)
)
?
?
?
?
An unlabeled document is selected as training
data for a new classifier when its confidence score
is above a threshold.
5 Experiments and Evaluation
To evaluate the effectiveness of our method, we
carry out several experiments. First, we compare the
performance of our method on five different cate-
gories, from five different domains, in order to see
its generality and applicability on different domains.
We also run experiments with two different language
pairs - English-Chinese and English-French - to see
if the distance between language families influences
the effectiveness of our method.
To determine the performance of the method with
respect to other approaches, we compare the classi-
fication accuracy with that of a machine translation
1062
approach that translates the training (test) data from
the source language to the target language, as well
as with a classifier trained on monolingual training
data in the target language.
Finally, we evaluate the performance of each of
the two steps of our proposed method. First, we
evaluate the model translated with the parameters
learned with EM, and then the model after the semi-
supervised learning for data distribution adaptation
with different parameters, including the number of
iterations and different amounts of unlabeled data.
5.1 Data Set
Since a standard evaluation benchmark for cross-
lingual text classification is not available, we built
our own data set from Yahoo! RSS news feeds. The
news feed contains news articles from October 1st
2009 to December 31st 2009. We collected a total
of 615731 news articles, categorized by their edi-
tors into topics such as ?sports? or ?business?. We
selected five categories for our experiments, namely
?sports?, ?health?, ?business?, ?entertainment?, ?ed-
ucation?. The Yahoo! RSS news feed includes
news in many languages, including English, Chi-
nese, French, Spanish, and others.
We experimented on two language pairs, English-
Chinese and English-French, selected for their diver-
sity: English and Chinese are disparate languages
with very little common vocabulary and syntax,
whereas English and French are regarded as more
similar. We expect to evaluate the impact of the
distance of languages on the effectiveness of our
method. In both cases, English is regarded as the
source language, where training data are available,
and Chinese and French are the target languages
for which we want to build text classifiers. Note
that regardless of the language, the documents are
assigned with one of the five category labels men-
tioned above. Table 1 shows the distribution of doc-
uments across categories and across languages.
Category English Chinese French
sports 23764 14674 18398
health 15627 11769 12745
business 34619 23692 28740
entertainment 26876 21470 23756
education 16488 14353 15753
Table 1: number of documents in each class
Before building the classification model, several
preprocessing steps are applied an all the docu-
ments. First, the HTML tags are removed, and ad-
vertisements and navigational information are also
eliminated. For the Chinese corpus, all the Chinese
characters with BIG5 encoding are converted into
GB2312 and the Chinese texts are segmented into
words. For the translation, we use the LDC bilin-
gual dictionary1 for Chinese English and ?stardict?
2 for Spanish English.
5.2 Model Translation
To transfer a model learned in one language to an-
other, we can translate all the bag-of-word features
according to a bilingual lexicon. Due to the trans-
lation ambiguity of each feature word, we com-
pare three different ways of model translation. One
method is to equally assign probabilities to all the
translations for a given source language word, and
to translate a word we randomly pick a translation
from all of its translation candidates. We denote this
as ?EQUAL? and it is our baseline method. Another
way is to calculate the translation probability based
on the frequencies of the translation words in the tar-
get language itself. For instance, the English word
?bush? can be translated into ???? , ???? or ??
?? . We can obtain the following unigram counts
of these translation words in our Yahoo! RSS news
corpus.
count translation sense
582 ?? Goerge W. Bush
43 ?? small trees
2 ?? canula
We can estimate that P (??|bush) = 582/(582 +
43+2) = 92.8% and so forth. This method often al-
lows us to estimate reasonable translation probabili-
ties and we use ?UNIGRAM? to denote this method.
And finally the third model translation approach is
to use the translation probability learned with the
EM algorithm proposed in this paper. The initial
parameters of the EM algorithm are set to the prob-
abilities calculated with the ?UNIGRAM? method
and we use 4000 unlabeled documents in Chinese
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002L27
2http://stardict.sourceforge.net/Dictionaries.php
1063
to learn translation probabilities with EM. We first
train an English classification model for the topic of
?sport? and then translate the model into Chinese us-
ing translation probabilities estimated by the above
three different methods. The three translated models
are applied to Chinese test data and we measure the
precision, recall and F-score as shown in Table 2.
Method P R F
EQUAL 71.1 70.6 70.8
UNIGRAM 79.5 77.8 78.6
EM 83.1 84.7 83.9
Table 2: Comparison of different methods for model
translation
From this table we can see that the baseline method
has lowest classification accuracy due to the fact that
it is unable to handle translation ambiguity since
picking any one of the translation word is equally
likely. ?UNIGRAM? shows significant improve-
ment over ?EQUAL? as the occurrence count of the
translation words in the target language can help
disambiguate the translations. However occurrence
count in a monolingual corpus may not always be
the true translation probability. For instance, the
English word ?work? can be translated into ??
?(labor)? and ???(factory)? in Chinese. How-
ever, in our Chinese monolingual news corpus, the
count for ???(factory)? is more than that of ??
?(labor)? even though ???(labor)? should be a
more likely translation for ?work?. The ?EM? algo-
rithm has the best performance as it is able to learn
translation probabilities by looking at documents in
both source language and target language instead of
just a single language corpus.
5.3 Cross Language Text Classification
To evaluate the effectiveness of our method on cross
language text classification, we implement several
methods for comparison. In each experiment, we
run a separate classification for each class, using a
one-versus-all binary classification.
ML (Monolingual). We build a monolingual
text classifier by training and testing the text classi-
fication system on documents in the same language.
This method plays the role of an upper-bound, since
the best classification results are expected when
monolingual training data is available.
MT (Machine Translation). We use the Sys-
tran 5.0 machine translation system to translate
the documents from one language into the other
in two directions. The first direction translates the
training data from the source language into the
target language, and then trains a model in the target
language. This direction is denoted as MTS. The
second direction trains a classifier in the source
language and translates the test data into the source
language. This direction is denoted as MTT. In
our experiments, Systran generates the single best
translation of the text as most off-the-shelf machine
translation tools do.
EM (Model Translation with EM). This is the
first step of our proposed method. We used 4,000
unlabeled documents to learn translation proba-
bilities with the EM algorithm and the translation
probabilities are leveraged to translate the model.
The rest of the unlabeled documents are used for
other experimental purpose.
SEMI (Adapted Model after Semi-Supervised
Learning). This is our proposed method, after both
model translation and semi-supervised learning.
In the semi-supervised learning, we use 6,000
unlabeled target language documents with three
training iterations.
In each experiment, the data consists of 4,000 la-
beled documents and 1,000 test documents (e.g., in
the cross-lingual experiments, we use 4,000 English
annotated documents and 1,000 Chinese or French
test documents). For a given language, the same test
data is used across all experiments.
Table 3 shows the performance of the various
classification methods. The ML (Monolingual)
classifier has the best performance, as it is trained
on labeled data in the target language, so that there
is no information loss and no distribution discrep-
ancy due to a model translation. The MT (ma-
chine translation) based approach scores the lowest
accuracy, probably because the machine translation
software produces only its best translation, which
is often error-prone, thus leading to poor classifi-
cation accuracy. In addition, the direct application
of a classification model from one language to an-
1064
English? Chinese
Category ML MTS MTT EM SEMI
P R F P R F P R F P R F P R F
sports 96.1 94.3 95.2 80.6 81.7 81.2 81.7 83.8 82.7 83.1 84.7 83.9 92.1 91.8 91.9
health 95.1 93.1 94.1 80.8 81.5 81.2 81.6 83.5 82.6 84.5 85.8 85.2 90.2 91.7 90.9
business 91.6 93.1 92.4 81.3 81.9 81.6 80.7 81.0 80.9 81.6 82.0 81.8 87.3 89.3 88.3
entertainment 88.1 88.3 88.2 76.1 78.8 77.5 75.3 78.9 77.1 76.8 79.7 78.2 83.2 83.8 83.5
education 79.1 82.2 80.6 70.2 72.5 71.8 71.1 72.0 71.6 71.2 73.7 72.5 76.2 79.8 78.0
English? French
sports 95.8 95.0 95.4 82.8 83.6 83.2 82.1 83.0 82.5 85.3 87.1 86.2 92.5 92.1 92.3
health 94.2 94.5 94.3 82.6 83.9 83.2 81.8 83.0 82.4 86.2 87.2 86.6 92.0 92.2 92.1
business 90.1 92.2 91.1 81.4 82.1 81.7 81.3 81.8 81.8 84.4 84.3 84.4 88.3 89.2 88.8
entertainment 87.4 87.2 87.3 76.6 79.1 77.8 76.0 78.8 77.4 78.9 81.0 80.0 84.3 85.5 84.9
education 78.8 81.8 80.3 72.1 74.8 73.5 72.3 72.7 72.5 73.8 76.2 75.0 76.3 80.1 78.2
Table 3: Comparison of different methods and different language pairs
other does not adapt to the distribution of the sec-
ond language, even if the documents belong to the
same domain. Comparing the two MT alternatives,
we can see that translating the training data (MTS)
has better performance than translating the test data
(MTT). The reason is that when the model is trained
on the translated training data, the model parame-
ters are learned over an entire collection of translated
documents, which is less sensitive to translation er-
rors than translating a test document on which the
classification is performed individually.
Our EM method for translating model features
outperforms the machine translation approach, since
it does not only rely on the best translation by the
machine translation system, but instead takes into
account all possible translations with knowledge
learned specifically from the target language. Ad-
ditionally, the SEMI (semi-supervised) learning is
shown to further improve the classification accuracy.
The semi-supervised learning is able to not only help
adapt the translated model to fit the words distribu-
tion in the target language, but it also compensates
the distortion or information loss during the model
translation process as it can down-weigh the incor-
rectly translated features.
The improvement in performance for both the
EM and the SEMI methods is consistent across
the five different domains, which indicates that the
methods are robust and they are insensitive to the
domain of the data.
The performance of the two language pairs
English-Chinese and English-French shows a dif-
ference as initially hypothesized. In both the EM
and the SEMI models, the classification accuracy
of English-French exceeds that of English-Chinese,
which is probably explained by the fact that there is
less translation ambiguity in similar languages, and
they have more similar distributions. Note that the
monolingual models in French and Chinese perform
comparably, which means the difficulty of the test
data is similar between the two target languages.
5.4 Model Adaptation with Semi-Supervised
Learning
Finally, to gain further insights into our proposed
adaptation method, we run several experiments with
different parameters for the semi-supervised learn-
ing stage. As these experiments are very time con-
suming, we run them only on Chinese.
For each of the five categories, we train a classi-
fication model using the 4,000 training documents
in English and then translate the model into Chinese
with the translation parameters learned with EM on
20,000 unlabeled Chinese documents. Then we fur-
ther train the translated model on a set of unlabeled
Chinese documents using a different number of it-
erations and a different amount of unlabeled docu-
ments. Figures 1 and 2 show the results of these
evaluations.
As the plots show, the use of unlabeled data in
the target language can improve the cross-language
classification by learning new knowledge in the
target language. Larger amounts of unlabeled
data in general help, although the marginal bene-
fit drops with increasing amounts of data. Regard-
ing the number of iterations, the best performance is
1065
 70
 75
 80
 85
 90
 95
 0  1000  2000  3000  4000  5000  6000
Cla
ssif
ica
tion
 F-
sco
re
Size of unlabeled data
sportshealthbusiness
entertainment
education
Figure 1: Change in classification F-score for an increas-
ing amount of unlabeled data in the target language
 70
 75
 80
 85
 90
 95
 0  1  2  3  4  5  6
Cla
ssif
ica
tion
 F-
sco
re
Number of iterations
sportshealthbusiness
entertainment
education
Figure 2: Change in classification F-score for a different
number of iterations
achieved after 3-4 iterations.
6 Conclusions
In this paper, we proposed a novel method for cross-
lingual text classification. Our method ports a clas-
sification model trained in a source language to a tar-
get language, with the translation knowledge being
learned using the EM algorithm. The model is fur-
ther tuned to fit the distribution in the target language
via semi-supervised learning. Experiments on dif-
ferent datasets covering different languages and dif-
ferent domains show significant improvement over
previous methods that rely on machine translation.
Moreover, the cross-lingual classification accuracy
obtained with our method was found to be close to
the one achieved using monolingual text classifica-
tion.
Acknowledgments
The work of the second author has been partially
supported by National Science Foundation awards
#0917170 and #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Science
Foundation.
References
S. Argamon, M. Koppel, and G. Avneri. 1998. Style-
based text categorization: What newspaper am i read-
ing? In AAAI-98 Workshop on Learning for Text Cat-
egorization, Madison.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. 2008.
Multilingual subjectivity analysis using machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), Honolulu, Hawaii.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory, Morgan Kaufmann Publishers, June.
P. Brown, S. della Pietra, V. della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: parameter estimation. Computational Linguis-
tics, 19(2).
W. Dai, G. Xue, Q. Yang, and Y. Yu. 2007. Transfer-
ring naive bayes classifiers for text classification. In In
Proceedings of the 22nd AAAI Conference on Artificial
Intell igence, pages 540?545.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1).
B. Fortuna and J. Shawe-Taylor. 2005. The use of
machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at
the 22nd International Conference on Machine Learn-
ing (ICML).
A. Gliozzo and C. Strapparava. 2006. Exploiting com-
parable corpora and bilingual dictionaries for cross-
language text categorization. In Proceedings of the
Conference of the Association for Computational Lin-
guistics, Sydney, Australia.
1066
R. Hwa, P. Resnik, and A. Weinberg. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Natural Language Engineering. Special issue
on Parallel Texts, editors R. Mihalcea and M. Simard.
T. Joachims. 1997. A probabilistic analysis of the Roc-
chio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Confer-
ence on Machine Learning, Nashville, US.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
P. Koehn and K. Knight. 2000. Estimating word transla-
tion probabilities from unrelated monolingua l corpora
using the em algorithm. In National Conference on
Artificial Intelligence (AAAI 2000) Lang kilde, pages
711?715.
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.
C. Monz and B.J. Dorr. 2005. Iterative translation dis-
ambiguation for cross-language information retrieval.
In Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Salvador, Brazil.
A. Moschitti. 2003. A study on optimal paramter tun-
ing for Rocchio text classifier. In Proceedings of the
European Conference on Information Retrieval, Italy.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003), Sapporo, Japan, July.
J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999.
Cross-language information retrieval based on parallel
texts and automatic mining of parallel texts from the
Web. In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval.
K. Nigam and R. Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceed-
ings of the Conference on Information and Knowledge
Management (CIKM 2000), McLean, VA, November.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using
maximum entropy for text classification. In IJCAI-99
Workshop on Machine Learning for Information Fil-
tering.
J.S. Olsson, D. W. Oard, and J. Hajic. 2005. Cross-
language text classification. In Proceedings of the 28th
Annual international ACM SIGIR Conference on Re-
search and Development in information Retrieval.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain, July.
M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
1998. A Bayesian approach to filtering junk e-mail.
In AAAI-98 Workshop on Learning for Text Catego-
rization, Madison.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. In Pro-
ceedings of 2006 AAAI Spring Symposium on Com-
putational Approaches for Analyzing Weblogs, pages
199?204, Stanford.
L. Shi, C. Niu, M. Zhou, and J. Gao. 2006. A dom
tree alignment model for mining parallel data from the
web. In Proceedings of the Annual Meeting of the As-
sociation for Computational Lingusitics (ACL 2006),
Sydney, Australia.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Conference
of the Association of Computational Linguistics and
the International Joint Conference on Natural Lan-
guage Processing, Singapore, August.
1067
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 590?599, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Lyrics, Music, and Emotions
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Carlo Strapparava
FBK-irst
strappa@fbk.eu
Abstract
In this paper, we explore the classification of
emotions in songs, using the music and the
lyrics representation of the songs. We intro-
duce a novel corpus of music and lyrics, con-
sisting of 100 songs annotated for emotions.
We show that textual and musical features can
both be successfully used for emotion recog-
nition in songs. Moreover, through compar-
ative experiments, we show that the joint use
of lyrics and music brings significant improve-
ments over each of the individual textual and
musical classifiers, with error rate reductions
of up to 31%.
1 Introduction
Language and music are peculiar characteristics of
human beings. The capability of producing and
enjoying language and music appears in every hu-
man society, regardless of the richness of its culture
(Nettl, 2000).
Importantly, language and music complement
each other in many different ways. For instance,
looking at music and language in terms of fea-
tures, we can observe that music organizes pitch
and rhythm in ways that language does not, and it
lacks the specificity of language in terms of seman-
tic meaning. On the other hand, language is built
from categories that are absent in music (e.g., nouns
and verbs), whereas music seems to have a deeper
power over our emotions than does ordinary speech.
Composers, musicians, and researchers in poetry
and literature alike have been long fascinated by the
combination of language and music, even since the
time of the earliest written records of music encoun-
tered in musical settings for poetry. Despite this in-
terest, and despite the long history of the interaction
between music and lyrics, there is only little work
that explicitly focuses on the connection between
music and lyrics.
In this paper, we focus on the connection between
the musical and linguistic representations in popu-
lar songs, and their role in the expression of affect.
We introduce a novel corpus of lyrics and music, an-
notated for emotions at line level, and explore the
automatic recognition of emotions using both tex-
tual and musical features. Through comparative ex-
periments, we show that emotion recognition can be
performed using either textual or musical features,
and that the joint use of lyrics and music can im-
prove significantly over classifiers that use only one
dimension at a time. We believe our results demon-
strate the promise of using joint music-lyric models
for song processing.
2 Related Work
The literature on music analysis is noticeably large,
and there are several studies concerning the music?s
power over emotions (Juslin and Sloboda, 2001),
thinking (Rauscher et al1993), or physical effort
(Karageorghis and Priest, 2008).
In particular, there has been significant research
in music and psychology focusing on the idea of a
parallel between affective cues in music and speech
(Sundberg, 1982; Scherer, 1995). For instance,
(Scherer, 2004) investigated the types of emotions
that can be induced by music, their mechanisms, and
how they can be empirically measured. (Juslin and
590
Laukka, 2003) conducted a comprehensive review
of vocal expressions and music performance, find-
ing substantial overlap in the cues used to convey
basic emotions in speech and music.
The work most closely related to ours is the com-
bination of audio and lyrics for emotion classifica-
tion in songs, as thoroughly surveyed in (Kim et al
2010). Although several methods have been pro-
posed, including a combination of textual features
and beats per minute and MPEG descriptors (Yang
and Lee, 2004); individual audio and text classifiers
for arousal and valence, followed by a combination
through meta-learning (Yang et al2008); and the
use of crowdsourcing labeling from Last.fm to col-
lect large datasets of songs annotated for emotions
(Laurier et al2008; Hu et al2009), all this pre-
vious work was done at song level, and most of
it focused on valence-arousal classifications. None
of the previous methods considered the fine-grained
classification of emotions at line level, as we do, and
none of them considered the six Ekman emotions
used in our work.
Other related work consists of the development
of tools for music accessing, filtering, classification,
and retrieval, focusing primarily on music in digital
format such as MIDI. For instance, the task of music
retrieval and music recommendation has received a
lot of attention from both the arts and the computer
science communities (see for instance (Orio, 2006)
for an introduction to this task). There are also sev-
eral works on MIDI analysis. Among them, partic-
ularly relevant to our research is the work by (Das
et al2000), who described an analysis of predom-
inant up-down motion types within music, through
extraction of the kinematic variables of music veloc-
ity and acceleration from MIDI data streams. (Catal-
tepe et al2007) addressed music genre classifica-
tion (e.g., classic, jazz, pop) using MIDI and au-
dio features, while (Wang et al2004) automati-
cally aligned acoustic musical signals with their cor-
responding textual lyrics. MIDI files are typically
organized into one or more parallel ?tracks? for in-
dependent recording and editing. A reliable system
to identify the MIDI track containing the melody1
is very relevant for music information retrieval, and
1A melody can be defined as a ?cantabile? sequence of
notes, usually the sequence that a listener can remember after
hearing a song.
there are several approaches that have been proposed
to address this issue (Rizo et al2006; Velusamy et
al., 2007).
Another related study concerned with the interac-
tion of lyrics and music using an annotated corpus is
found in (O?Hara, 2011), who presented preliminary
research that checks whether the expressive meaning
of a particular harmony or harmonic sequence could
be deduced from the lyrics it accompanies, by us-
ing harmonically annotated chords from the Usenet
group alt.guitar.tab.
Finally, in natural language processing, there are
a few studies that mainly exploited the lyrics com-
ponent of the songs, while generally ignoring the
musical component. For instance, (Mahedero et al
2005) dealt with language identification, structure
extraction, and thematic categorization for lyrics.
(Xia et al2008) addressed the task of sentiment
classification in lyrics, recognizing positive and neg-
ative moods in a large dataset of Chinese pop songs,
while (Yang and Lee, 2009) approached the problem
of emotion identification in lyrics, classifying songs
from allmusic.com using a set of 23 emotions.
3 A Corpus of Music and Lyrics
Annotated for Emotions
To enable our exploration of emotions in songs, we
compiled a corpus of 100 popular songs (e.g., Danc-
ing Queen by ABBA, Hotel California by Eagles,
Let it Be by The Beatles). Popular songs exert a
lot of power on people, both at an individual level
as well as on groups, mainly because of the mes-
sage and emotions they convey. Songs can lift our
moods, make us dance, or move us to tears. Songs
are able to embody deep feelings, usually through a
combined effect of both music and lyrics.
The corpus is built starting with the MIDI tracks
of each song, by extracting the parallel alignment
of melody and lyrics. Given the non-homogeneous
quality of the MIDI files available on the Web, we
asked a professional MIDI provider for high quality
MIDI files produced for singers and musicians. The
MIDI files, which were purchased from the provider,
contain also lyrics that are synchronized with the
notes. In these MIDI files, the melody channel is un-
equivocally decided by the provider, making it easier
to extract the music and the corresponding lyrics.
591
MIDI format. MIDI is an industry-standard pro-
tocol that enables electronic musical instruments,
computers, and other electronic equipment to com-
municate and synchronize with each other. Unlike
analog devices, MIDI does not transmit an audio
signal: it sends event messages about musical no-
tation, pitch, and intensity, control signals for pa-
rameters such as volume, vibrato, and panning, and
cues and clock signals to set the tempo. As an elec-
tronic protocol, it is notable for its widespread adop-
tion throughout the music industry.
MIDI files are typically created using computer-
based sequencing software that organizes MIDI
messages into one or more parallel ?tracks? for in-
dependent recording, editing, and playback. In most
sequencers, each track is assigned to a specific MIDI
channel, which can be then associated to specific in-
strument patches. MIDI files can also contain lyrics,
which can be displayed in synchrony with the music.
Starting with the MIDI tracks of a song, we ex-
tract and explicitly encode the following features.
At the song level, the key of the song (e.g., G ma-
jor, C minor). At the line level, we represent the
raising, which is the musical interval (in half-steps)
between the first note in the line and the most impor-
tant note (i.e., the note in the line with the longest
duration). Finally, at the note level, we encode the
time code of the note with respect to the beginning
of the song; the note aligned with the corresponding
syllable; the degree of the note with relation to the
key of the song; and the duration of the note.
Table 1 shows statistics on the corpus. An exam-
ple from the corpus, consisting of the first two lines
from the Beatles? song A hard day?s night, is illus-
trated in Figure 3.
SONGS 100
SONGS IN ?MAJOR? KEY 59
SONGS IN ?MINOR? KEY 41
LINES 4,976
ALIGNED SYLLABLES / NOTES 34,045
Table 1: Some statistics of the corpus
Emotion Annotations with Mechanical Turk. In
order to explore the classification of emotions in
songs, we needed a gold standard consisting of man-
ual emotion annotations of the songs. Following
previous work on emotion annotation of text (Alm
et al2005; Strapparava and Mihalcea, 2007), to
annotate the emotions in songs we use the six ba-
sic emotions proposed by (Ekman, 1993): ANGER,
DISGUST, FEAR, JOY, SADNESS, SURPRISE. To col-
lect the annotations, we use the Amazon Mechanical
Turk service, which was previously found to pro-
duce reliable annotations with a quality comparable
to those generated by experts (Snow et al2008).
The annotations are collected at line level, with a
separate annotation for each of the six emotions. We
collect numerical annotations using a scale between
0 and 10, with 0 corresponding to the absence of an
emotion, and 10 corresponding to the highest inten-
sity. Each HIT (i.e., annotation session) contains an
entire song, with a number of lines ranging from 14
to 110, for an average of 50 lines per song.
The annotators were instructed to: (1) Score the
emotions from the writer perspective, not their own
perspective; (2) Read and interpret each line in con-
text; i.e., they were asked to read and understand
the entire song before producing any annotations;
(3) Produce the six emotion annotations independent
from each other, accounting for the fact that a line
could contain none, one, or multiple emotions. In
addition to the lyrics, the song was also available
online, so they could listen to it in case they were
not familiar with it. The annotators were also given
three different examples to illustrate the annotation.
While the use of crowdsourcing for data annota-
tion can result in a large number of annotations in
a very short amount of time, it also has the draw-
back of potential spamming that can interfere with
the quality of the annotations. To address this aspect,
we used two different techniques to prevent spam.
First, in each song we inserted a ?checkpoint? at a
random position in the song ? a fake line that reads
?Please enter 7 for each of the six emotions.? Those
annotators who did not follow this concrete instruc-
tion were deemed as spammers who produce anno-
tations without reading the content of the song, and
thus removed. Second, for each remaining annota-
tor, we calculated the Pearson correlation between
her emotion scores and the average emotion scores
of all the other annotators. Those annotators with a
correlation with the average of the other annotators
below 0.4 were also removed, thus leaving only the
reliable annotators in the pool.
592
<token time=5760 orig?note=D? degree=5 duration=810>HARD </token>
<song filename=AHARDDAY.m2a>
<key time=0>G major</key>
<line pvers=1 raising=3 anger=1.5 disgust=0.7 sadness=2.5 surprise=0.8 > 
</line>
<line pvers=2 raising=5 anger=3.5 disgust=2 sadness=1.2 surprise=0.2 > 
</line>
<token time=5040 orig?note=B degree=3 duration=210>IT</token>
<token time=5050 orig?note=B degree=3 duration=210>?S </token>
<token time=5280 orig?note=C? degree=4 duration=210>BEEN </token>
<token time=5520 orig?note=B degree=3 duration=210>A </token>
<token time=6720 orig?note=D? degree=5 duration=570>DAY</token>
<token time=6730 orig?note=D? degree=5 duration=570>?S </token>
<token time=7440 orig?note=D? degree=5 duration=690>NIGHT</token>
<token time=8880 orig?note=C? degree=4 duration=212>AND </token>
<token time=9120 orig?note=D? degree=5 duration=210>I</token>
<token time=9130 orig?note=D? degree=5 duration=210>?VE </token>
<token time=9600 orig?note=D? degree=5 duration=210>WOR</token>
<token time=9840 orig?note=F? degree=7? duration=930>KING </token>
<token time=10800 orig?note=D? degree=5 duration=210>LI</token>
<token time=11040 orig?note=C? degree=4 duration=210>KE </token>
<token time=11050 orig?note=C? degree=4 duration=210>A </token>
<token time=11280 orig?note=D? degree=5 duration=330>D</token>
<token time=11640 orig?note=C? degree=4 duration=90>O</token>
<token time=11760 orig?note=B degree=3 duration=330>G</token>
<token time=9360 orig?note=C? degree=4 duration=210>BEEN </token>
Figure 1: Two lines of a song in the corpus: It-?s been a hard day-?s night, And I-?ve been wor-king li-ke a d-o-g
For each song, we start by asking for ten annota-
tions. After spam removal, we were left with about
two-five annotations per song. The final annotations
are produced by averaging the emotions scores pro-
duced by the reliable annotators. Figure 3 shows
an example of the emotion scores produced for two
lines. The overall correlation between the remain-
ing reliable annotators was calculated as 0.73, which
represents a strong correlation.
For each of the six emotions, Table 2 shows the
number of lines that had that emotion present (i.e.,
the score of the emotion was different from 0), as
well as the average score for that emotion over all
4,976 lines in the corpus. Perhaps not surprisingly,
the emotions that are dominant in the corpus are JOY
and SADNESS ? which are the emotions that are of-
ten invoked by people as the reason behind a song.
Note that the emotions do not exclude each other:
i.e., a line that is labeled as containing JOY may also
contain a certain amount of SADNESS, which is the
reason for the high percentage of songs containing
both JOY and SADNESS. The emotional load for
the overlapping emotions is however very different.
For instance, the lines that have a JOY score of 5
or higher have an average SADNESS score of 0.34.
Conversely, the lines with a SADNESS score of 5 or
Number
Emotion lines Average
ANGER 2,516 0.95
DISGUST 2,461 0.71
FEAR 2,719 0.77
JOY 3,890 3.24
SADNESS 3,840 2.27
SURPRISE 2,982 0.83
Table 2: Emotions in the corpus of 100 songs: number
of lines including a certain emotion, and average emotion
score computed over all the 4,976 lines.
higher have a JOY score of 0.22.
4 Experiments and Evaluations
Through our experiments, we seek to determine the
extent to which we can automatically determine the
emotional load of each line in a song, for each of the
six emotion dimensions.
We use two main classes of features: textual fea-
tures, which build upon the textual representation of
the lyrics; and musical features, which rely on the
musical notation associated with the songs. We run
three sets of experiments. The first one is intended to
determine the usefulness of the textual features for
593
emotion classification. The second set specifically
focuses on the musical features. Finally, the last set
of experiments makes joint use of textual and musi-
cal features.
The experiments are run using linear regression,2
and the results are evaluated by measuring the Pear-
son correlation between the classifier predictions
and the gold standard. For each experiment, a ten-
fold cross validation is run on the entire dataset.3
4.1 Textual Features
First, we attempt to identify the emotions in a line
by relying exclusively on the features that can be de-
rived from the lyrics of the song. We decided to fo-
cus on those features that were successfully used in
the past for emotion classification (Strapparava and
Mihalcea, 2008). Specifically, we use: (1) unigram
features obtained from a bag-of-words representa-
tion, which are the features typically used by corpus-
based methods; and (2) lexicon features, indicating
the appartenance of a word to a semantic class de-
fined in manually crafted lexicons, which are often
used by knowledge-based methods.
Unigrams. We use a bag-of-words representation
of the lyrics to derive unigram counts, which are
then used as input features. First, we build a vo-
cabulary consisting of all the words, including stop-
words, occurring in the lyrics of the training set. We
then remove those words that have a frequency be-
low 10 (value determined empirically on a small de-
velopment set). The remaining words represent the
unigram features, which are then associated with a
value corresponding to the frequency of the unigram
inside each line. Note that we also attempted to use
higher order n-grams (bigrams and trigrams), but
evaluations on a small development dataset did not
show any improvements over the unigram model,
and thus all the experiments are run using unigrams.
Semantic Classes. We also derive and use coarse
textual features, by using mappings between words
and semantic classes. Specifically, we use the Lin-
2We use the Weka machine learning toolkit.
3There is no clear way to determine a baseline for these
experiments. A simple baseline that we calculated, which as-
sumed by default an emotional score equal to the average of the
scores on the training data, and measured the correlation be-
tween these default scores and the gold standard, consistently
led to correlations close to 0 (0.0081-0.0221).
guistic Inquiry and Word Count (LIWC) and Word-
Net Affect (WA) to derive coarse textual features.
LIWC was developed as a resource for psycholin-
guistic analysis (Pennebaker and Francis, 1999;
Pennebaker and King, 1999). The 2001 version of
LIWC includes about 2,200 words and word stems
grouped into about 70 broad categories relevant to
psychological processes (e.g., emotion, cognition).
WA (Strapparava and Valitutti, 2004) is a resource
that was created starting with WordNet, by annotat-
ing synsets with several emotions. It uses several re-
sources for affective information, including the emo-
tion classification of Ortony (Ortony et al1987).
From WA, we extract the words corresponding to
the six basic emotions used in our experiments. For
each semantic class, we infer a feature indicating the
number of words in a line belonging to that class.
Table 3 shows the Pearson correlations obtained
for each of the six emotions, when using only uni-
grams, only semantic classes, or both.
Semantic All
Emotion Unigrams Classes Textual
ANGER 0.5525 0.3044 0.5658
DISGUST 0.4246 0.2394 0.4322
FEAR 0.3744 0.2443 0.4041
JOY 0.5636 0.3659 0.5769
SADNESS 0.5291 0.3006 0.5418
SURPRISE 0.3214 0.2153 0.3392
AVERAGE 0.4609 0.2783 0.4766
Table 3: Evaluations using textual features: unigrams,
semantic classes, and all the textual features.
4.2 Musical Features.
In a second set of experiments, we explore the role
played by the musical features. While the musical
notation of a song offers several characteristics that
could be potentially useful for our classification ex-
periments (e.g., notes, measures, dynamics, tempo),
in these initial experiments we decided to focus on
two main features, namely the notes and the key.
Notes. A note is a sign used in the musical nota-
tion associated with a song, to represent the relative
duration and pitch of a sound. In traditional mu-
sic theory, the notes are represented using the first
seven letters of the alphabet (C-D-E-F-G-A-B), al-
594
though other notations can also be used. Notes can
be modified by ?accidentals? ? a sharp or a flat sym-
bol that can change the note by half a tone. A written
note can also have associated a value, which refers
to its duration (e.g., whole note; eighth note). Simi-
lar to the unigram features, for each note, we record
a feature indicating the frequency of that note inside
a line.
Key. The key of a song refers to the harmony or
?pitch class? used for a song, e.g., C major, or F#.
Sometime the term minor or major can be appended
to a key, to indicate a minor or a major scale. For
instance, a song in ?the key of C minor? means that
the song is harmonically centered on the note C, and
it makes use of the minor scale whose first note is C.
The key system is the structural foundation of most
of the Western music. We use a simple feature that
reflects the key of the song. Note that with a few ex-
ceptions, when more than one key is used in a song,
all the lines in a song will have the same key.
Table 4 shows the results obtained in these clas-
sification experiments, when using only the notes as
features, only the key, or both.
All
Emotion Notes Key Musical
ANGER 0.2453 0.4083 0.4405
DISGUST 0.1485 0.2922 0.3199
FEAR 0.1361 0.2203 0.2450
JOY 0.1533 0.3835 0.4001
SADNESS 0.1738 0.3502 0.3762
SURPRISE 0.0983 0.2241 0.2412
AVERAGE 0.1592 0.3131 0.3371
Table 4: Evaluations using musical features: notes, key,
and all the musical features.
4.3 Joint Textual and Musical Features.
To explore the usefulness of the joint lyrics and mu-
sic representation, we also run a set of experiments
that use all the textual and musical features. Table 5
shows the Pearson correlations obtained when us-
ing all the features. To facilitate the comparison,
the table also includes the results obtained with the
textual-only and musical-only features (reported in
Tables 3 and 4).
All All Textual &
Emotion Textual Musical Musical
ANGER 0.5658 0.4405 0.6679
DISGUST 0.4322 0.3199 0.5068
FEAR 0.4041 0.2450 0.4384
JOY 0.5769 0.4001 0.6456
SADNESS 0.5418 0.3762 0.6193
SURPRISE 0.3392 0.2412 0.3855
AVERAGE 0.4766 0.3371 0.5439
Table 5: Evaluations using both textual and musical fea-
tures.
5 Discussion
One clear conclusion can be drawn from these ex-
periments: the textual and musical features are both
useful for the classification of emotions in songs,
and, more importantly, their joint use leads to the
highest classification results. Specifically, the joint
model gives an error rate reduction of 12.9% with
respect to the classifier that uses only textual fea-
tures, and 31.2% with respect to the classifier that
uses only musical features. This supports the idea
that lyrics and music represent orthogonal dimen-
sions for the classification of emotions in songs.
Among the six emotions considered, the largest
improvements are observed for JOY, SADNESS, and
ANGER. This was somehow expected for the first
two emotions, since they appear to be dominant in
the corpus (see Table 2), but comes as a surprise
for ANGER, which is less dominant. Further explo-
rations are needed to determine the reason for this
effect.
Looking at the features considered, textual fea-
tures appear to be the most useful. Nonetheless,
the addition of the musical features brings clear im-
provements, as shown in the last column from the
same table.
Additionally, we made several further analyses of
the results, as described below.
Feature ablation. To determine the role played by
each of the feature groups we consider, we run an
ablation study where we remove one feature group
at a time from the complete set of features and mea-
sure the accuracy of the resulting classifier. Table 6
shows the feature ablation results. Note that feature
ablation can also be done in the reverse direction, by
595
All features, excluding
All Semantic Semantic Classes
Emotion Features Unigrams Classes Notes Key and Notes
ANGER 0.6679 0.4996 0.5525 0.6573 0.6068 0.6542
DISGUST 0.5068 0.3831 0.4246 0.5013 0.4439 0.4814
FEAR 0.4384 0.3130 0.3744 0.4313 0.4150 0.4114
JOY 0.6456 0.5141 0.5636 0.6432 0.5829 0.6274
SADNESS 0.6193 0.4586 0.5291 0.6176 0.5540 0.6029
SURPRISE 0.3855 0.3083 0.3214 0.3824 0.3421 0.3721
AVERAGE 0.5439 0.4127 0.4609 0.5388 0.4908 0.5249
Table 6: Ablation studies excluding one feature group at a time.
Textual and
Emotion Baseline Textual Musical Musical
ANGER 89.27% 91.14% 89.63% 92.40%
DISGUST 93.85% 94.67% 93.85% 94.77%
FEAR 93.58% 93.87% 93.58% 93.87%
JOY 50.26% 70.92% 61.95% 75.64%
SADNESS 67.40% 75.84% 70.65% 79.42%
SURPRISE 94.83% 94.83% 94.83% 94.83%
AVERAGE 81.53% 86.87% 84.08% 88.49%
Table 7: Evaluations using a coarse-grained binary classification.
keeping only one group of features at a time; the re-
sults obtained with the individual feature groups are
already reported in Tables 3 and 4.
The ablation studies confirm the findings from our
earlier experiments: while the unigrams and the keys
are the most predictive features, the semantic classes
and the notes are also contributing to the final clas-
sification even if to a lesser extent. To measure the
effect of these groups of somehow weaker features
(semantic classes and notes), we also perform an ab-
lation experiment where we remove both these fea-
ture groups from the feature set. The results are re-
ported in the last column of Table 6.
Coarse-grained classification. As an additional
evaluation, we transform the task into a binary clas-
sification by using a threshold empirically set at 3.
Thus, to generate the coarse binary annotations, if
the score of an emotion is below 3, we record it as
?negative? (i.e., the emotion is absent), whereas if
the score is equal to or above 3, we record it as ?pos-
itive? (i.e., the emotion is present).
For the classification, we use Support Vector Ma-
chines (SVM), which are binary classifiers that seek
to find the hyperplane that best separates a set of pos-
itive examples from a set of negative examples, with
maximum margin (Vapnik, 1995). Applications of
SVM classifiers to text categorization led to some of
the best results reported in the literature (Joachims,
1998).
Table 7 shows the results obtained for each of the
six emotions, and for the three major settings that
we considered: textual features only, musical fea-
tures only, and a classifier that jointly uses the tex-
tual and the musical features. As before, the classi-
fication accuracy for each experiment is reported as
the average of the accuracies obtained during a ten-
fold cross-validation on the corpus. The table also
shows a baseline, computed as the average of the
accuracies obtained when using the most frequent
class observed on the training data for each fold.
As seen from the table, on average, the joint use of
textual and musical features is also beneficial for this
binary coarser-grained classification. Perhaps not
surprisingly, the effect of the classifier is stronger for
596
1,000 news headlines 4,976 song lines
Best result (Strapparava Joint Text
Emotion SEMEVAL?07 and Mihalcea, 08) and Music
ANGER 0.3233 0.1978 0.6679
DISGUST 0.1855 0.1354 0.5068
FEAR 0.4492 0.2956 0.4384
JOY 0.2611 0.1381 0.6456
SADNESS 0.4098 0.1601 0.6193
SURPRISE 0.1671 0.1235 0.3855
AVERAGE 0.2993 0.1750 0.5439
Table 8: Results obtained in previous work on emotion classification.
those emotions that are dominant in the corpus, i.e.,
JOY and SADNESS (see Table 2). The improvement
obtained with the classifiers is much smaller for the
other emotions (or even absent, e.g., for SURPRISE),
which is also explained by their high baseline of over
90%.
Comparison to previous work. There is no pre-
vious research that has considered the joint use of
lyrics and songs representations for emotion classifi-
cation at line level, and thus we cannot draw a direct
comparison with other work on emotion classifica-
tion in songs.
Nonetheless, as a point of reference, we consider
the previous work done on emotion classification of
texts. Table 8 shows the results obtained in previ-
ous work for the recognition of emotions in a corpus
consisting of 1,000 news headlines (Strapparava and
Mihalcea, 2007) annotated for the same six emo-
tions. Specifically, the table shows the best over-
all correlation results obtained by the three emotion
recognition systems in the SEMEVAL task on Affec-
tive Text (Strapparava and Mihalcea, 2007): (Chau-
martin, 2007; Kozareva et al2007; Katz et al
2007). The table also shows the best results obtained
in follow up work carried out on the same dataset
(Strapparava and Mihalcea, 2008).
Except for one emotion (FEAR), the correlation
figures we obtain are significantly higher than those
reported in previous work. As mentioned before,
however, a direct comparison cannot be made, since
the earlier work used a different, smaller dataset.
Moreover, our corpus of songs is likely to be more
emotionally loaded than the news titles used in pre-
vious work.
6 Conclusions
Popular songs express universally understood mean-
ings and embody experiences and feelings shared by
many, usually through a combined effect of both mu-
sic and lyrics. In this paper, we introduced a novel
corpus of music and lyrics, annotated for emotions at
line level, and we used this corpus to explore the au-
tomatic recognition of emotions in songs. Through
experiments carried out on the dataset of 100 songs,
we showed that emotion recognition can be per-
formed using either textual or musical features, and
that the joint use of lyrics and music can improve
significantly over classifiers that use only one di-
mension at a time.
The dataset introduced in this paper is available
by request from the authors of the paper.
Acknowledgments
The authors are grateful to Rajitha Schellenberg
for her help with collecting the emotion annota-
tions. Carlo Strapparava was partially supported by
a Google Research Award. Rada Mihalcea?s work
was in part supported by the National Science Foun-
dation award #0917170. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of the National Science Foun-
dation.
References
C. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion predic-
tion. In Proceedings of the Conference on Empirical
597
Methods in Natural Language Processing, pages 347?
354, Vancouver, Canada.
Z. Cataltepe, Y. Yaslan, and A. Sonmez. 2007. Mu-
sic genre classification using MIDI and audio features.
Journal on Advances in Signal Processing.
F.R. Chaumartin. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), Prague, Czech Republic, June.
M. Das, D. Howard, and S. Smith. 2000. The kinematic
analysis of motion curves through MIDI data analysis.
Organised Sound, 5(1):137?145.
P. Ekman. 1993. Facial expression of emotion. Ameri-
can Psychologist, 48:384?392.
X. Hu, J. S. Downie, and A. F. Ehmann. 2009. Lyric
text mining in music mood classification. In Proceed-
ings of the International Society for Music Information
Retrieval Conference, Kobe, Japan.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142, Chemnitz, Germany.
P. Juslin and P. Laukka. 2003. Communication of emo-
tion in vocal expression and music performance: Dif-
ferent channels, same code? Psychological Bulletin,
129:770?814.
P. N. Juslin and J. A. Sloboda, editors. 2001. Music and
Emotion: Theory and Research. Oxford University
Press.
C. Karageorghis and D. Priest. 2008. Music in sport and
exercise : An update on research and application. The
Sport Journal, 11(3).
P. Katz, M. Singleton, and R. Wicentowski. 2007.
Swat-mp:the semeval-2007 systems for task 5 and
task 14. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic, June.
Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richard-
son, J. Scott, J. Speck, and D. Turnbull. 2010. Mu-
sic emotion recognition: A state of the art review. In
International Symposium on Music Information Re-
trieval.
Z. Kozareva, B. Navarro, S. Vazquez, and A. Mon-
toyo. 2007. Ua-zbsa: A headline emotion classifi-
cation through web information. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, June.
C. Laurier, J. Grivolla, and P. Herrera. 2008. Multimodal
music mood classification using audio and lyrics. In
Proceedings of the International Conference on Ma-
chine Learning and Applications, Barcelona, Spain.
J. Mahedero, A. Martinez, and P. Cano. 2005. Natu-
ral language processing of lyrics. In Proceedings of
MM?05, Singapore, November.
B. Nettl. 2000. An ethnomusicologist contemplates
universals in musical sound and musical culture. In
N. Wallin, B. Merker, and S. Brown, editors, The
origins of music, pages 463?472. MIT Press, Cam-
bridge, MA.
T. O?Hara. 2011. Inferring the meaning of chord se-
quences via lyrics. In Proceedings of 2nd Workshop
on Music Recommendation and Discovery (WOMRAD
2011), Chicago, IL, October.
N. Orio. 2006. Music retrieval: A tutorial and re-
view. Foundations and Trends in Information Re-
trieval, 1(1):1?90, November.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, (11).
J. Pennebaker and M. Francis. 1999. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
J. Pennebaker and L. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. Journal of Per-
sonality and Social Psychology, (77).
F. Rauscher, G. Shaw, and K. Ky. 1993. Music and spa-
tial task performance. Nature, 365.
D. Rizo, P. Ponce de Leon, C. Perez-Sancho, A. Pertusa,
and J. Inesta. 2006. A pattern recognition approach
for melody track selection in MIDI files. In Proceed-
ings of 7th International Symposyum on Music Infor-
mation Retrieval (ISMIR-06), pages 61?66, Victoria,
Canada, October.
K. Scherer. 1995. Expression of emotion in voice and
music. Journal of Voice, 9:235?248.
K. Scherer. 2004. Which emotions can be induced by
music? what are the underlying mechanisms? and
how can we measure them ? Journal of New Music
Research, 33:239?251.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Honolulu, Hawaii.
C. Strapparava and R. Mihalcea. 2007. Semeval-2007
task 14: Affective text. In Proceedings of the 4th
International Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic.
C. Strapparava and R. Mihalcea. 2008. Learning to iden-
tify emotions in text. In Proceedings of the ACM Con-
ference on Applied Computing ACM-SAC 2008, Fort-
aleza, Brazile.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, Lisbon.
J. Sundberg. 1982. Speech, song, and emotions. In
M. Clynes, editor, Music, Mind and Brain: The Neu-
ropsychology of Music. Plenum Press, New York.
598
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
S. Velusamy, B. Thoshkahna, and K. Ramakrishnan.
2007. Novel melody line identification algorithm for
polyphonic MIDI music. In Proceedings of 13th In-
ternational Multimedia Modeling Conference (MMM
2007), Singapore, January.
Y. Wang, M. Kan, T. Nwe, A. Shenoy, and J. Yin. 2004.
LyricAlly: Automatic synchronization of acoustic mu-
sical signals and textual lyrics. In Proceedings of
MM?04, New York, October.
Y. Xia, L. Wang, K.F. Wong, and M. Xu. 2008. Lyric-
based song sentiment classification with sentiment
vector space model. In Proceedings of the Association
for Computational Linguistics, Columbus, Ohio.
D. Yang and W. Lee. 2004. Disambiguating music
emotion using software agents. In Proceedings of the
International Conference on Music Information Re-
trieval, Barcelona, Spain.
D. Yang and W. Lee. 2009. Music emotion identification
from lyrics. In Proceedings of 11th IEEE Symposium
on Multimedia.
Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-C. Ho,
and H. Chen. 2008. Toward multi-modal music emo-
tion classification. In Proceedings of the 9th Pacific
Rim Conference on Multimedia: Advances in Multi-
media Information Processing.
599
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 269?278,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Iterative Constrained Clustering for Subjectivity Word Sense
Disambiguation
Cem Akkaya, Janyce Wiebe
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is a supervised and application-
specific word sense disambiguation task
disambiguating between subjective and
objective senses of a word. Not sur-
prisingly, SWSD suffers from the knowl-
edge acquisition bottleneck. In this work,
we use a ?cluster and label? strategy to
generate labeled data for SWSD semi-
automatically. We define a new algo-
rithm called Iterative Constrained Cluster-
ing (ICC) to improve the clustering purity
and, as a result, the quality of the gener-
ated data. Our experiments show that the
SWSD classifiers trained on the ICC gen-
erated data by requiring only 59% of the
labels can achieve the same performance
as the classifiers trained on the full dataset.
1 Introduction
Subjectivity lexicons (e.g., (Turney, 2002;
Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al., 2007; Andreevskaia and Bergler,
2008; Agarwal et al., 2009)) play an important
role in opinion, sentiment, and subjectivity
analysis. These systems typically look for the
presence of clues in text. Recently, in (Akkaya
et al., 2009), we showed that subjectivity clues
are fairly ambiguous as to whether they express
subjectivity or not ? words in such lexicons may
have both subjective and objective usages. We
call this problem subjectivity sense ambiguity.
Consider the following sentence containing the
clue ?attack?:
(1) He was attacked by Milosevic for at-
tempting to carve out a new party from the
Socialists.
Knowing that ?attack? is a subjectivity clue with
negative polarity will help a system recognize the
negative sentiment in the sentence. But for (2), the
same information is simply misleading, because
the clue is used with an objective meaning.
(2) A new treatment based on training T-cells
to attack cancerous cells ...
Any opinion analysis system which relies on a
subjectivity lexicon will be misled by subjectiv-
ity clues used with objective senses (false hits).
In (Akkaya et al., 2009), we introduced the task,
Subjectivity Word Sense Disambiguation, which is
to automatically determine which word instances
in a corpus are being used with subjective senses,
and which are being used with objective senses.
SWSD can be considered as a coarse-grained
and application-specific word sense disambigua-
tion task. We showed that sense subjectivity in-
formation about clues can be fed to subjectiv-
ity and sentiment analysis resulting in substantial
improvement for both subjectivity and sentiment
analysis by avoiding false hits.
Although SWSD is a promising tool, it suf-
fers from the knowledge acquisition bottleneck.
SWSD is defined as a supervised task, and fol-
lows a targeted approach common in the WSD lit-
erature for performance reasons. This means, for
each target clue, a different classifier is trained re-
quiring separate training data for each target clue.
It is expensive and time-consuming to obtain an-
notated datasets to train SWSD classifiers limit-
ing scalability. As a countermeasure, in (Akkaya
et al., 2011), we showed that non-expert annota-
tions collected through Amazon Mechanical Turk
(MTurk) can replace expert annotations success-
fully and might be used to apply SWSD on a large
scale.
Although non-expert annotations are cheap and
fast, they still incur some cost. In this work, we
aim to reduce the human annotation effort needed
269
to generate the same amount of subjectivity sense
tagged data by using a ?cluster and label? strategy.
We hypothesize that we can obtain large sets of
labeled data by labelling clusters of instances of a
target word instead of single instances.
The main contribution of this work is a novel
constrained clustering algorithm called Iterative
Constrained Clustering (ICC) utilizing an active
constraint selection strategy. A secondary con-
tribution is a mixed word representation that is a
combination of previously proposed context rep-
resentations. We show that a ?cluster and label?
strategy relying on these two proposed compo-
nents generates training data of good purity. The
resulting data has sufficient purity to train reli-
able SWSD classifiers. SWSD classifiers trained
on only 59% of the data achieve the same perfor-
mance as classifiers trained on 100% of the data,
resulting in a significant reduction in the annota-
tion effort. Our results take SWSD another step
closer to large scale application.
2 Cluster and Label
Our approach is inspired by a method lexicogra-
phers commonly employ to create sense invento-
ries, where they create inventories based on ev-
idence found in corpora. They use concordance
information to mine frequent usage patterns. (Kil-
garriff, 1997) describes this process in detail. A
lexicographer collects usages of a word in cor-
pora and groups them into coherent sets. The in-
stances in a set should have more in common with
each other than with the instances in other sets,
according to the criteria the lexicographer consid-
ers. After generating the sets, the lexicographer
codes each set as a dictionary definition based on
the common attributes of the instances. Our goal
is similar. Instead of generating dictionary defini-
tions, we are only interested in generating coher-
ent sets of usages of a word, so that we can label
each induced set ? with its instances ? to obtain
labeled data for SWSD. Our high-level grouping
criterion is that the instances in a cluster should be
similar subjective (objective) usages of the word.
Training data for an SWSD classifier consists
of instances of the target word tagged as having
a subjective sense (S) or an objective sense (O)
(subjectivity sense tagged data). We train a dif-
ferent SWSD classifier for each target word as in
(Akkaya et al., 2009). Thus, we need a different
training dataset for each target word. Our ultimate
goal is to reduce the human annotation effort re-
quired to create training data for SWSD classifiers.
For this purpose, we utilize a ?cluster and label?
strategy relying on context clustering. Each in-
stance of a word is represented as a feature vector
(i.e., a context vector). The annotation process has
the following steps: (1) cluster the context vectors
of word instances, (2) label the induced clusters
as S or O, (3) propagate the given label to all in-
stances in a cluster.
The induced clusters represent different usage
patterns of a word. Thus, we build more than two
clusters, even though SWSD is a binary task. This
implies that two different instances of a word can
both be subjective, but end up in different clusters,
if they are different usages of the word.
Since we are labelling clusters as a whole, we
will introduce noise in the labeled data. Thus, in
developing the clustering process, we need to min-
imize that noise and find as pure clusters as possi-
ble.
The first step is to define the context representa-
tion of the instances. This is addressed in Section
3. Then, we turn in Section 4.2 to the clustering
process itself.
To evaluate our ?cluster and label? strategy, we
use two gold standard subjectivity sense tagged
datasets.
1
. The first one is called senSWSD gen-
erated in (Akkaya et al., 2009) and the second
one is called mturkSWSD generated in (Akkaya
et al., 2011). They consist of subjectivity sense
tagged data for disjoint sets of 39 and 90 words,
respectively. In this paper, we opt to use the
smaller dataset senSWSD as our development set,
on which we evaluate various context representa-
tions (in Section 3) and our proposed constrained
clustering algorithm (in Section 4.2). Then, on
mturkSWSD, we evaluate the quality of semi-
automatically generated data for SWSD classifi-
cation (in Section 4.3.2).
3 Context Representations
There has been much work on context representa-
tions of words for various NLP tasks. Clustering
word instances in order to discriminate senses of
a word is called Word Sense Discrimination. Con-
text representations for this task rely on two main
types of models: distributional semantic models
(DSM) and feature-based models.
1
Available at http://mpqa.cs.pitt.edu/
corpora
270
(Schutze, 1998), which is still a competi-
tive model for word-sense discrimination by con-
text clustering, relies on a distributional semantic
model (DSM) (Turney and Pantel, 2010; Sahlgren,
2006; Bullinaria and Levy, 2007). A DSM is usu-
ally a word-to-word co-occurrence matrix ? also
called semantic space ? such that each row repre-
sents the distribution of a target word in a large
text corpus. Each row gives the semantic sig-
nature of a word, which is basically a high di-
mensional numeric vector. Note that this high di-
mensional vector represents word types, not word
tokens. Thus, it cannot model a word instance
in context. For token-based treatment, (Schutze,
1998) utilizes a second-order representation by av-
eraging co-occurrence vectors of the words (cor-
responding to rows of the co-occurrence matrix)
that occur in that particular context. It is impor-
tant to note that (Schutze, 1998) uses an addi-
tive model for compositional representation. Re-
cently, in (Akkaya et al., 2012), we found that a
DSM built using multiplicative composition ? pro-
posed by (Mitchell and Lapata, 2010) for a differ-
ent task ? gives better performance than the model
described by (Schutze, 1998).
We test both methods in this paper, using the
same semantic space. The space is built from a
corpus consisting of 120 million tokens. The rows
of the space correspond to word forms and the
columns correspond to word lemmas present in the
corpus. We adopt the parameters for our semantic
space from (Mitchell and Lapata, 2010): window
size of 10 and dimension size of 2000 (i.e., the
2000 most frequent lemmas). We do not filter out
stop words, since they have been shown to be use-
ful for various semantic similarity tasks in (Bulli-
naria and Levy, 2007). We use positive point-wise
mutual information to compute values of the vec-
tor components, which has also been shown to be
favourable in (Bullinaria and Levy, 2007).
Purandere and Pedersen is the prominent repre-
sentative of feature-based models. (Purandare and
Pedersen, 2004) creates context vectors from local
feature representations similar to the feature vec-
tors found in supervised WSD. In this work, we
use the following features from (Mihalcea, 2002)
to build the local feature representation: (1) the
target word itself and its part of speech, (2) sur-
rounding context of 3 words and their part of
speech, (3) the head of the noun phrase, (4) the
first noun and verb before the target word, (5) the
first noun and verb after the target word.
skew local dsm add dsm mul mix rep
average 79.90 80.50 80.50 83.53 85.23
appear-v 53.83 54.85 54.85 57.40 69.39
fine-a 70.07 72.26 70.07 74.45 75.18
interest-n 54.41 54.78 55.88 81.62 81.62
restraint-n 70.45 71.97 75.00 71.21 81.82
Table 1: Evaluation of Various Context Representations
3.1 Evaluation of Context Representations
In this section, we evaluate context representations
for the context clustering task on the subjectivity
sense tagged data, senSWSD. The evaluation is
done separately for each word.
We use the same clustering algorithm for all
context representations: agglomerative hierarchi-
cal clustering with average linkage criteria. In all
our experiments throughout the paper, we fix the
cluster size to 7 as it is done in (Purandare and
Pedersen, 2004). We think that is reasonable num-
ber since SENSEVAL III reports that the average
number of senses per word is 6.47. We choose
cluster purity as our evaluation metric. To com-
pute cluster purity, we assign each cluster to a
sense label, which is the most frequent one in the
cluster. The number of the correctly assigned in-
stances divided by the number of all the clustered
instances gives us cluster purity.
Row 1 of Table 1 holds the cumulative results
over all the words in senSWSD (micro averages).
The table also reports detailed results for 4 sample
selected words from senSWSD. skew stands for
the percentage of the most frequent label. dsm add
is the representation based on (Schutze, 1998),
dsm mul stands for the representation as described
in (Akkaya et al., 2012) and local features is the
local feature representation based on (Purandare
and Pedersen, 2004). The results show that among
dsm mul, dsm add, and local features; dsm mul
performs the best.
When we look at the context clustering re-
sults for single words separately, we observe
that the performance of different representations
vary. There is not a single winner among all
words. Thus, perhaps choosing one single repre-
sentation for all the words is not optimal. Hav-
ing that in mind, we try merging the dsm mul
and local features representations. We leave out
dsm add representation, since both dsm mul and
dsm add rely on the same type of semantic infor-
mation (i.e., a DSM). We hypothesize that the two
271
representations, one relying on a semantic space
and the other relying on local WSD features, may
complement each other.
To merge the representations, we concatenate
the two feature vectors into one. First, however,
we normalize each vector to unit length, since the
individual vectors have different scales and would
have unequal contribution, otherwise. We call this
mixed representation mix rep.
In Table 1, we see that, overall, mix rep per-
forms better than all the other representations. The
improvement is statistically significant at the p <
.05 level on a paired t-test. We observe that, even
when mix rep does not perform the best, it is never
bad. mix rep is the winner or ties for the winner
for 25 out of 39 words. This number is 13, 13, and
15 for dsm add, dsm mul and local features, re-
spectively. For the words for which mix rep is not
the winner, it is, on average, 1.47 points lower than
the winner. This number is 4.22, 6.83, and 7.07
for the others. The results provide evidence that
mix rep is consistently good and reliable. Thus, in
our experiments, mix rep will be our choice as the
context representation.
4 Clustering Process
We now turn to the clustering process. In a ?clus-
ter and label? strategy, in order to be able to label
clusters, we need to annotate some of the instances
in each cluster. Then, we can accept the majority
label found in a cluster as its label. Thus, some
manual labelling is required, preferably a small
amount.
We propose to provide this small amount of an-
notated data prior to clustering, and then perform
semi-supervised clustering. This way the provided
labels will guide the clustering algorithm to gener-
ate the clusters that are more suitable for our end
task, namely clusters where subjective and objec-
tive instances are grouped together.
4.1 Constrained Clustering
Constrained clustering (Grira et al., 2004) also
known as semi-supervised clustering is a recent
development in the clustering literature. In addi-
tion to the similarity information required by un-
supervised clustering, constrained clustering re-
quires pairwise constraints. There are two types
of constraints: (1) must-link and (2) cannot-link
constraints. A must-link constraint dictates that
two instances should be in the same cluster and a
cannot-link dictates that two instances should not
be in the same cluster. In this work, we only con-
sider cannot-links, because of the definition of our
SWSD task. Two instances sharing the same label
do not need to be in the same cluster, since the in-
duced clusters represent different usage patterns of
a word. For example, two instances labeled S need
not be similar to each other. They can be different
usages, both having a subjective meaning. On the
other hand, if two instances are labeled having op-
posing labels, we do not want them to be in the
same cluster. Thus, we utilize cannot-links but not
must-links.
Constraints can be obtained from domain
knowledge or from available instance labels. In
our work, constraints are generated from instance
labels. Each instance pair with opposing labels is
considered to be cannot-linked.
There are two general strategies to incorporate
constraints into clustering. The first is to adapt
the similarity between instances (Xing et al., 2002;
Klein et al., 2002) by adjusting the underlying dis-
tance metric. The main idea is to make the dis-
tance between must-linked instances ? their neigh-
bourhoods ? smaller and the distance between
cannot-linked instances ? their neighbourhoods ?
larger. The second strategy is modifying the clus-
tering algorithm itself so that search is biased to-
wards a partitioning for which the constraints hold
(Wagstaff and Cardie, 2000; Basu et al., 2002;
Demiriz et al., 1999).
Our proposed constrained clustering method re-
lies on some ideas from (Klein et al., 2002). Thus,
we explain it in more detail. (Klein et al., 2002)
utilizes agglomerative hierarchical clustering with
complete-linkage. The algorithm imposes con-
straints by changing the distance matrix accord-
ing to the given constraints. The distances be-
tween must-linked instances are set to 0. That is
not enough by itself, since if two instances are
must-linked, other instances close to them should
also get closer to each other. This means there is
a need to propagate the constraints. This is done
by calculating the shortest path between all the in-
stances and updating the distance matrix accord-
ingly. To impose cannot-links, the distance be-
tween two cannot-linked instances is set to some
large number. The complete-linkage property
indirectly propagates the cannot-link constraints,
since it will not allow two clusters to be merged if
they contain instances that are cannot-linked.
Although previous work report on average sub-
272
stantial improvement in the clustering purity,
(Davidson et al., 2006) shows that even if the
constraints are generated from gold-standard data,
some constraint sets can decrease clustering pu-
rity. The results vary significantly depending on
the specific set of constraints used. To our knowl-
edge, there have been two approaches for select-
ing informative constraint sets (Basu et al., 2004;
Klein et al., 2002). The method described in
(Basu et al., 2004) uses the farthest-first traversal
scheme. That strategy is not suitable in our setting,
since we have only two labels. After selecting
just one instance from both labels, this method be-
comes the same as random selection. The strategy
described in (Klein et al., 2002) is more general.
At first, the hierarchical clustering algorithm fol-
lows in a unconstrained fashion until some moder-
ate number of clusters are remaining. Then, the al-
gorithm starts to request constraints between roots
whenever two clusters are merged.
4.2 Iterative Constrained Clustering
Our proposed algorithm is closely related to (Klein
et al., 2002). We share the same backbone:
(1) the agglomerative hierarchical clustering with
complete-linkage and (2) the mechanism to im-
pose cannot-link constraints described in Section
4.1. For our algorithm, we implement a second
mechanism for imposing constraints proposed by
(Xing et al., 2002) (Section 4.2.1) and use both
mechanisms in combination. We also propose a
novel constraint selection method (Section 4.2.2).
4.2.1 Imposing Constraints
(Klein et al., 2002) imposes cannot-link con-
straints by adjusting the distance between cannot-
linked pairs heuristically and by relying on com-
plete linkage for propagation. Although this ap-
proach was shown to be effective, we believe it
does not make full use of the provided constraints.
We believe that learning a new distance metric will
result in more reliable distance estimates between
all instances. For this purpose, we learn a Maha-
lanobis distance function following the method de-
scribed in (Davis et al., 2007). (Davis et al., 2007)
formulate the problem of distance metric learn-
ing as minimizing the differential relative entropy
between two multivariate Gaussians under con-
straints. Note that using distance metric learning
for imposing constraints was previously proposed
by (Xing et al., 2002). (Xing et al., 2002) pose
metric learning as a convex optimization problem.
The reason we choose the metric learning method
(Davis et al., 2007) over (Xing et al., 2002) is that
it is computationally more efficient.
(Klein et al., 2002) has a favourable property we
want to keep. The constraints are imposed strictly,
meaning that no cannot-linked instances can ap-
pear in the same cluster. I.e., they are hard con-
straints. In the case of metric learning, the con-
straints are not imposed strictly. In a new learned
distance metric, two cannot-linked instances will
be relatively distant, but there is no guarantee they
will not end up in the same cluster. Although we
think that metric learning makes a better use of
provided constraints, we do not want to lose the
benefit of hard constraints. Thus, we use both
mechanisms in combination to impose constraints.
We first learn a Mahalanobis distance based on the
provided constraints. Then, we compute distance
matrix and employ the mechanism proposed by
(Klein et al., 2002) on the learned distance matrix.
4.2.2 Active Constraint Generation
As mentioned before, the choice of the set of con-
straints affects the quality of the end clustering. In
this work, we define a novel method to choose in-
formative instances, which we believe will have
maximum impact on the end cluster quality, when
they are labeled and used to generate constraints
for our task. We use an iterative approach. Each
iteration consists of three steps: (1) generating
clusters by the process described in Section 4.2.1
imposing available constraints, (2) choosing the
most informative instance, considering the cluster
boundaries, and acquiring its label, (3) extending
the available constraints with the ones we generate
from the newly labeled instance.
We consider an instance to be informative if
there is a high probability that the knowledge of
its label may change the cluster boundaries. The
more probable that change is, the more informa-
tive is the instance. The basic idea is that if an
instance is in a cluster holding instances of type
a and it is close to another cluster holding in-
stances of type b, that instance is most likely mis-
clustered. Thus, it should be queried. Our hypoth-
esis is that, in each iteration, the algorithm will
choose the most problematic ? informative ? in-
stance that will end up changing cluster bound-
aries. This will result in each iteration in a more
reliable distance metric, which in return will pro-
vide more reliable estimates of problematic in-
stances in future iterations. The imposed con-
273
Algorithm 1 Iterative Constrained Clustering
1: C = cluster(I)
2: I
{L}
= labelprototypes(C)
3: while
?
?
I
{L}
?
?
< stop do
4: Con = createconstraints(I
{L}
)
5: Matrix
dist
= learnmetric(I,Con)
6: C = constraintedcluster(Matrix
dist
,Con)
7: L = labelmostinformative(C)
8: I
{L}
= I
{L}
? L
9: end while
10: propagatelabels(I
{L}
, C) {C...Clusters; Con...Constraints;
I...Instances; I
{L}
...Labeled Instances; Matrix
dist
...Distance Matrix}
straints will move the clustering in each iteration
towards better separation of S and O instances.
To define informativeness, we define a scoring
function, which is used to score each data point on
its goodness. The lower the score, the more likely
it is that the instance is mis-clustered. Choosing
the data point with the lowest score will likely
change clustering borders in the next iteration.
Our scoring function is based on the silhouette co-
efficient, a popular unsupervised cluster validation
metric to measure goodness (Tan et al., 2005) of
a cluster member. Basically, the silhouette score
assigns a cluster member that is close to another
cluster a lower score, and a cluster member that
is closer to the cluster center a higher score. That
is partly what we want. In addition, we do not
want to penalize a cluster member that is close to
another cluster having members with the same la-
bel. For this purpose, we calculate the silhouette
score only over clusters with an opposing label
(i.e., holding members with an opposing label). In
addition, we consider only instances labeled so far
when computing the score. We call this new coef-
ficient silh
const
. It is computed as follows: (1) for
an instance i, compute its average distance from
the other instances in its cluster x
i
which are al-
ready labeled, (2) for an instance i, compute its
average distance from the labeled instances of the
clusters from an opposing label and take the mini-
mum of these averages y
i
, (3) compute the silhou-
ette coefficient as (y
i
-x
i
) / max(y
i
,x
i
).
The silh
const
coefficient has favourable proper-
ties. First, it scores members that are close to
a cluster with an opposing label lower than the
members that are close to a cluster with the same
label. According to our definition, these mem-
bers are more informative. Figure 1 holds a sam-
ple cluster setting. The shape of a member de-
notes its label and its fill denotes whether or not it
has been queried. In this example, silh
const
scores
3 1 
2 
Figure 1: Behaviour of selection function
members 2 and 3 lower than 1. Thus, member 1
will not be selected, which is the right decision in
this example. Both members 2 and 3 are close to
clusters with an opposing label. In this example
silh
const
scores member 3 lower, which is farther
away from already labeled members in the clus-
ter. Thus, member 3 will be selected to be labeled.
This type of behaviour results in an explorative
strategy.
The active selection strategy proposed by (Klein
et al., 2002) is single pass. Thus, it does not have
the opportunity to observe the complete cluster
structure before choosing constraints. We hypoth-
esize that our strategy will provide more informa-
tive constraints, since it has the advantage of be-
ing able to base the decision of which constraints
to generate on fully observed cluster structure in
each iteration.
We call our proposed algorithm Iterative Con-
strained Clustering (ICC). In our final implemen-
tation, ICC starts by simply clustering the in-
stances without any constraints. The algorithm
queries the label of the prototypical member ?
the member closest to the cluster center ? of each
cluster. Then, the described iterations begin. Al-
gorithm 1 contains the complete ICC algorithm.
Note that line 6 is equivalent to the algorithm of
(Klein et al., 2002).
4.3 Experiments
This section gives details on experiments to evalu-
ate the purity of the semi-automatically generated
subjectivity sense tagged data by our ?cluster and
label? strategy. We carry out detailed analysis to
quantify the effect of the proposed active selec-
tion strategy and of metric learning on the purity
of the generated data. We compare our active se-
lection strategy to random selection and also to
(Klein et al., 2002). The comparison is done on
the senSWSD dataset. SenSWSD consists of three
subsets, SENSEVAL I,II and III. Since we devel-
274
Figure 2: Label Purity ? ICC vs. random selection
oped our active selection algorithm on the SEN-
SEVAL I subset, we use only SENSEVAL II and
III subsets for comparison. We apply ICC to each
word in the comparison set separately, and report
cumulative results for the purity of the generated
data. We report results for different percentages of
the queried data amount (e.g. 10% means that the
algorithm queried 10% of the data to create con-
straints). This way, we obtain a learning curve.
We fix the cluster number to 7 as in the context
representation experiments.
4.3.1 Effect of Active Selection Strategy
Figure 2 holds the comparison of ICC with
silh
const
selection to a random selection baseline.
?majority? stands for majority label frequency in
the dateset. We see that silh
const
performs better
than the random selection. By providing labels to
only 25% of the data, we can achieve 87.67% pure
fully labeled data.
For comparison, we also evaluate the perfor-
mance of (Klein et al., 2002) with their active con-
straint selection strategy as described in Section
4.1. Note that originally (Klein et al., 2002) re-
quests the constraint between two roots. In our
setting, it requests labels of the roots and then gen-
erates constraints from the obtained labels. Since
we have a binary task, querying labels makes more
sense. This has the advantage that more con-
straints from each request are obtained. More-
over, it allows a direct comparison to our algo-
rithm. (Klein et al., 2002) does not use any metric
learning. Thus, we run our algorithm also without
metric learning, in order to compare the effective-
ness of both active selection strategies fairly. In
Figure 3, we see that silh
const
performs better than
the active selection strategy described in (Klein et
al., 2002). We also see that metric learning results
Figure 3: Label Purity ? ICC vs. Klein
Figure 4: SWSD accuracy on ICC generated data
in a big improvement. In addition, metric learn-
ing results in a smoother learning curve, which is
a favourable property for a real-world application.
4.3.2 SWSD on semi-automatically generated
annotations
Now that we have a tool to generate training data
for SWSD, we want to evaluate it on the actual
SWSD task. We want to see if the obtained purity
is enough to create reliable SWSD classifiers. For
this purpose, we test ICC on mturkSWSD dataset.
For each word in our dataset, we conduct 10-
fold cross-validation experiments. ICC is ap-
plied to training folds to label instances semi-
automatically. We train SWSD classifiers on the
generated training fold labels and test the classi-
fiers on the corresponding test fold. We distin-
guish between queried instances and propagated
labels. The queried instances are weighted as
1 and the instances with propagated labels are
weighted by their silh
const
score, since that mea-
sure gives the goodness of an instance. The score
is defined between -1 and 1. This score is normal-
ized between 0 and 1, before it is used as a weight.
SVM classifiers from the Weka package (Witten
and Frank., 2005) with its default settings are used
275
as in (Akkaya et al., 2011).
We implement two baselines. The first is sim-
ple random sampling and the second is uncer-
tainty sampling, which is an active learning (AL)
method. We use ?simple margin? selection as de-
scribed in (Tong and Koller, 2001). It selects, in
each iteration, the instance closest to the decision
boundary of the trained SVM. Each method is run
until it reaches the accuracy of training fully on
the gold-standard data. ICC reaches that bound-
ary when provided only 59% of the labels in the
dataset. For uncertainty sampling and random
sampling, these values are 92% and 100%, respec-
tively. In Figure 4, we see the SWSD accuracy for
different queried data percentages. ?full? stands
for training fully on gold-standard data. We see
that training SWSD on semi-automatically labeled
data by ICC does consistently better than uncer-
tainty sampling and random sampling.
It is surprising to see that uncertainty sampling
overall does not do better than random sampling.
We believe that it might be because of sampling
bias. During AL, as more and more labels are
obtained, the training set quickly diverges from
the underlying data distribution. (Sch?utze et al.,
2006) states that AL can explore the feature space
in such a biased way that it can end up ignoring en-
tire clusters of unlabeled instances. We think that
SWSD is highly prone for the mentioned missed
cluster problem because of its unique nature. As
mentioned, SWSD is a binary task where we dis-
tinguish between subjective and objective usages
of a subjectivity word. Although the classifica-
tion is binary, the underlying usages are grouped
into multiple clusters corresponding to senses of
the word. It is possible that two groups of usages
which are represented quite differently in the fea-
ture space are both subjective or objective. More-
over, one usage group might be closer to a usage
group from the opposing label than to a group with
the same label.
We see that our method reduces the annotation
amount by 36% in comparison to uncertainty sam-
pling and by 41% in comparison to random sam-
pling to reach the performance of the SWSD sys-
tem trained on fully annotated data.
5 Related Work
One related line of research is constrained clus-
tering also known as semi-supervised clustering
(Xing et al., 2002; Wagstaff and Cardie, 2000;
Grira et al., 2004; Demiriz et al., 1999). It has
been applied to various datasets and tasks such
as image and document categorization. To our
knowledge, we are the first to utilize constrained
clustering for a difficult NLP task.
There have been only two previous works se-
lecting constraints for constrained clustering ac-
tively (Basu et al., 2004; Klein et al., 2002). The
biggest difference of our approach is that it is iter-
ative as opposed to single pass.
Active Learning (AL) (Settles, 2009; Settles
and Craven, 2008; Hwa, 2004; Tong and Koller,
2001) builds another important set of related work.
Our method is inspired by uncertainty sampling.
We accomplish active selection in the clustering
setting.
6 Conclusions
In this paper, we explore a ?cluster and la-
bel? strategy to reduce the human annotation ef-
fort needed to generate subjectivity sense-tagged
data. In order to keep the noise in the semi-
automatically labeled data minimal, we investigate
different feature space types and evaluate their ex-
pressiveness. More importantly, we define a new
algorithm called iterative constrained clustering
(ICC) with an active constraint selection strategy.
We show that we can obtain a fairly reliable la-
beled data when we utilize ICC.
We show that the active selection strategy
we propose outperforms a previous approach by
(Klein et al., 2002) for generating subjectivity
sense-tagged data. Training SWSD classifiers on
ICC generated data improves over random sam-
pling and uncertainty sampling (Tong and Koller,
2001). We can achieve on mturkSWSD 36% an-
notation reduction over uncertainty sampling and
41% annotation reduction over random sampling
in order to reach the performance of SWSD clas-
sifiers trained on fully annotated data.
To our knowledge, this work is the first applica-
tion of constrained clustering to a hard NLP prob-
lem. We showcase the power of constrained clus-
tering. We hope that the same ?cluster and label?
strategy will be applicable to Word Sense Disam-
biguation. This will be part of our future work.
7 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
276
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24?
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
190?199, Singapore, August. Association for Com-
putational Linguistics.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87?96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2012. Utilizing semantic composition in distribu-
tional semantic models for word sense discrimina-
tion and word sense disambiguation. In ICSC, pages
45?51.
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Over-
coming domain dependence in sentiment tagging.
In Proceedings of ACL-08: HLT, pages 290?298,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Sugato Basu, Arindam Banerjee, and R. Mooney.
2002. Semi-supervised clustering by seeding. In
In Proceedings of 19th International Conference on
Machine Learning (ICML-2002).
Sugato Basu, Arindam Banerjee, and Raymond J.
Mooney. 2004. Active semi-supervision for pair-
wise constrained clustering. In SDM.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
John Bullinaria and Joseph Levy. 2007. Ex-
tracting semantic representations from word
co-occurrence statistics: A computational
study. Behavior Research Methods, 39:510?526.
10.3758/BF03193020.
Ian Davidson, Kiri Wagstaff, and Sugato Basu. 2006.
Measuring constraint-set utility for partitional clus-
tering algorithms. In PKDD, pages 115?126.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In Proceedings of the 24th interna-
tional conference on Machine learning, ICML ?07,
pages 209?216, New York, NY, USA. ACM.
Ayhan Demiriz, Kristin Bennett, and Mark J. Em-
brechts. 1999. Semi-supervised clustering using
genetic algorithms. In In Artificial Neural Networks
in Engineering (ANNIE-99, pages 809?814. ASME
Press.
Nizar Grira, Michel Crucianu, and Nozha Boujemaa.
2004. Unsupervised and semi-supervised cluster-
ing: a brief survey. In in A Review of Ma-
chine Learning Techniques for Processing Multime-
dia Content, Report of the MUSCLE European Net-
work of Excellence.
Rebecca Hwa. 2004. Sample selection for statis-
tical parsing. Comput. Linguist., 30(3):253?276,
September.
Adam Kilgarriff. 1997. I dont believe in word senses.
Computers and the Humanities, 31(2):91?113.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the Twentieth International Conference on Compu-
tational Linguistics (COLING 2004), pages 1267?
1373, Geneva, Switzerland.
D. Klein, K. Toutanova, I.T. Ilhan, S.D. Kamvar, and
C. Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceed-
ings of the ACL Workshop on ?Word Sense Dis-
ambiguatuion: Recent Successes and Future Direc-
tions, pages 74?80, July.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL
2004), Boston.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2003), pages
105?112, Sapporo, Japan.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Hinrich Sch?utze, Emre Velipasaoglu, and Jan O. Ped-
ersen. 2006. Performance thresholding in practical
text classification. In Proceedings of the 15th ACM
international conference on Information and knowl-
edge management, CIKM ?06, pages 662?671, New
York, NY, USA. ACM.
277
H. Schutze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 1070?1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Burr Settles. 2009. Active Learning Literature Survey.
Technical Report 1648, University of Wisconsin?
Madison.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining, (First Edi-
tion). Addison-Wesley Longman Publishing Co.,
Inc., Boston, MA, USA.
Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. March.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02), pages 417?424, Philadelphia,
Pennsylvania.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning (ICML-2000), pages 1103?1110.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal taxonomies for sen-
timent analysis. In Proceedings of CIKM-05, the
ACM SIGIR Conference on Information and Knowl-
edge Management, Bremen, DE.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and
Stuart J. Russell. 2002. Distance metric learning
with application to clustering with side-information.
In NIPS, pages 505?512.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003), pages 129?136, Sapporo, Japan.
278
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903?911,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Quantifying the Limits and Success of Extractive Summarization Systems
Across Domains
Hakan Ceylan and Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
{hakan,rada}@unt.edu
Umut ?Ozertem
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
umut@yahoo-inc.com
Elena Lloret and Manuel Palomar
Department of
Software and Computing Systems
University of Alicante
San Vicente del Raspeig
Alicante 03690, Spain
{elloret,mpalomar}@dlsi.ua.es
Abstract
This paper analyzes the topic identification
stage of single-document automatic text sum-
marization across four different domains, con-
sisting of newswire, literary, scientific and le-
gal documents. We present a study that ex-
plores the summary space of each domain
via an exhaustive search strategy, and finds
the probability density function (pdf) of the
ROUGE score distributions for each domain.
We then use this pdf to calculate the per-
centile rank of extractive summarization sys-
tems. Our results introduce a new way to
judge the success of automatic summarization
systems and bring quantified explanations to
questions such as why it was so hard for the
systems to date to have a statistically signifi-
cant improvement over the lead baseline in the
news domain.
1 Introduction
Topic identification is the first stage of the gener-
ally accepted three-phase model in automatic text
summarization, in which the goal is to identify the
most important units in a document, i.e., phrases,
sentences, or paragraphs (Hovy and Lin, 1999; Lin,
1999; Sparck-Jones, 1999). This stage is followed
by the topic interpretation and summary generation
steps where the identified units are further processed
to bring the summary into a coherent, human read-
able abstract form. The extractive summarization
systems, however, only employ the topic identifi-
cation stage, and simply output a ranked list of the
units according to a compression ratio criterion. In
general, for most systems sentences are the preferred
units in this stage, as they are the smallest grammat-
ical units that can express a statement.
Since the sentences in a document are reproduced
verbatim in extractive summaries, it is theoretically
possible to explore the search space of this problem
through an enumeration of all possible extracts for
a document. Such an exploration would not only
allow us to see how far we can go with extractive
summarization, but we would also be able to judge
the difficulty of the problem by looking at the dis-
tribution of the evaluation scores for the generated
extracts. Moreover, the high scoring extracts could
also be used to train a machine learning algorithm.
However, such an enumeration strategy has an
exponential complexity as it requires all possible
sentence combinations of a document to be gener-
ated, constrained by a given word or sentence length.
Thus the problem quickly becomes impractical as
the number of sentences in a document increases and
the compression ratio decreases. In this work, we try
to overcome this bottleneck by using a large cluster
of computers, and decomposing the task into smaller
problems by using the given section boundaries or a
linear text segmentation method. As a result of this
exploration, we generate a probability density func-
tion (pdf) of the ROUGE score (Lin, 2004) distri-
butions for four different domains, which shows the
distribution of the evaluation scores for the gener-
ated extracts, and allows us to assess the difficulty
of each domain for extractive summarization.
Furthermore, using these pdfs, we introduce a
new success measure for extractive summarization
systems. Namely, given a system?s average score
over a data set, we show how to calculate the per-
903
centile rank of this system from the corresponding
pdf of the data set. This allows us to see the true
improvement a system achieves over another, such
as a baseline, and provides a standardized scoring
scheme for systems performing on the same data set.
2 Related Work
Despite the large amount of work in automatic
text summarization, there are only a few studies
in the literature that employ an exhaustive search
strategy to create extracts, which is mainly due to
the prohibitively large search space of the prob-
lem. Furthermore, the research regarding the align-
ment of abstracts to original documents has shown
great variations across domains (Kupiec et al, 1995;
Teufel and Moens, 1997; Marcu, 1999; Jing, 2002;
Ceylan and Mihalcea, 2009), which indicates that
the extractive summarization techniques are not ap-
plicable to all domains at the same level.
In order to automate the process of corpus
construction for automatic summarization systems,
(Marcu, 1999) used exhaustive search to generate
the best Extract from a given (Abstract, Text) tuple,
where the best Extract contains a set of clauses from
Text that have the highest similarity to the given Ab-
stract.
In addition, (Donaway et al, 2000) used exhaus-
tive search to create all the sentence extracts of
length three starting with 15 TREC Documents, in
order to judge the performance of several summary
evaluation measures suggested in their paper.
Finally, the study most similar to ours was done
by (Lin and Hovy, 2003), who used the articles with
less than 30 sentences from the DUC 2001 data set
to find oracle extracts of 100 and 150 (?5) words.
These extracts were compared against one summary
source, selected as the one that gave the highest
inter-human agreement. Although it was concluded
that a 10% improvement was possible for extrac-
tive summarization systems, which typically score
around the lead baseline, there was no report on how
difficult it would be to achieve this improvement,
which is the main objective of our paper.
3 Description of the Data Set
Our data set is composed of four different domains:
newswire, literary, scientific and legal. For all the
Domain ?Dw ?Sw ?R ?C ?Cw
Newswire 641 101 84% 1 641
Literary 4973 1148 77% 6 196
Scientific 1989 160 92% 9 221
Legal 3469 865 75% 18 192
Table 1: Statistical properties of the data set. ?Dw, and
?Sw represent the average number of words for each doc-
ument and summary respectively; ?R indicates the av-
erage compression ratio; and ?C and ?Cw represent the
average number of sections for each document, and the
average number of words for each section respectively.
domains we used 50 documents and only one sum-
mary for each document, except for newswire where
we used two summaries per document. For the
newswire domain, we selected the articles and their
summaries from the DUC 2002 data set,1. For the
literary domain, we obtained 10 novels that are lit-
erature classics, and available online in text format.
Further, we collected the corresponding summaries
for these novels from various websites such as
CliffsNotes (www.cliffsnotes.com) and SparkNotes
(www.sparknotes.com), which make available hu-
man generated abstracts for literary works. These
sources give a summary for each chapter of the
novel, so each chapter can be treated as a sepa-
rate document. Thus we evaluate 50 chapters in to-
tal. For the scientific domain, we selected the ar-
ticles from the medical journal Autoimmunity Re-
views2 were selected, and their abstracts are used
as summaries. Finally, for the legal domain, we
gathered 50 law documents and their corresponding
summaries from the European Legislation Website,3
which comprises four types of laws - Council Di-
rectives, Acts, Communications, and Decisions over
several topics, such as society, environment, educa-
tion, economics and employment.
Although all the summaries are human generated
abstracts for all the domains, it is worth mention-
ing that the documents and their corresponding sum-
maries exhibit a specific writing style for each do-
main, in terms of the vocabulary used and the length
of the sentences. We list some of the statistical prop-
erties of each domain in Table 1.
1http://www-nlpir.nist.gov/projects/duc/data.html
2http://www.elsevier.com/wps/product/cws home/622356
3http://eur-lex.europa.eu/en/legis/index.htm
904
4 Experimental Setup
As mentioned in Section 1, an exhaustive search
algorithm requires generating all possible sentence
combinations from a document, and evaluating each
one individually. For example, using the values from
Table 1, and assuming 20 words per sentence, we
find that the search space for the news domain con-
tains approximately
(32
5
)
? 50 = 10, 068, 800 sum-
maries. The same calculation method for the sci-
entific domain gives us
(99
8
)
? 50 = 8.56 ? 1012
summaries. Obviously the search space gets much
bigger for the legal and literary domains due to their
larger text size.
In order to be able to cope with such a huge
search space, the first thing we did was to modify
the ROUGE 1.5.54 Perl script by fixing the parame-
ters to those used in the DUC experiments,5 and also
by modifying the way it handles the input and output
to make it suitable for streaming on the cluster.
The resulting script evaluates around 25-30 sum-
maries per second on an Intel 2.33 GHz processor.
Next, we streamed the resulting ROUGE script for
each (document, summary) pair on a large cluster
of computers running on an Hadoop Map-Reduce
framework.6 Based on the size of the search space
for a (document, summary) pair, the number of com-
puters allocated in the cluster ranged from just a few
to more than one thousand.
Although the combination of a large cluster and a
faster ROUGE is enough to handle most of the doc-
uments in the news domain in just a few hours, a
simple calculation shows that the problem is still im-
practical for the other domains. Hence for the scien-
tific, legal, and literary domains, rather than consid-
ering each document as a whole, we divide them into
sections, and create extracts for each section such
that the length of the extract is proportional to the
length of the section in the original document. For
the legal and scientific domains, we use the given
section boundaries (without considering the subsec-
tions for scientific documents). For the novels, we
treat each chapter as a single document (since each
chapter has its own summary), which is further di-
vided into sections using a publicly available linear
4http://berouge.com
5
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0
6http://hadoop.apache.org/
text segmentation algorithm by (Utiyama and Isa-
hara, 2001).7 In all cases, we let the algorithm pick
the number of segments automatically.
To evaluate the sections, we modified ROUGE
further so that it applies the length constraint to the
extracts only, not to the model summaries. This is
due to the fact that we evaluate the extracts of each
section individually against the whole model sum-
mary, which is larger than the extract. This way,
we can get an overall ROUGE recall score for a
document extract, simply by summing up the re-
call scores of each section extracts. The precision
score for the entire document can also be found by
adding the weighted precision scores for each sec-
tion, where the weight is proportional to the length
of the section in the original document. In our study,
however, we only use recall scores.
Note that, since for the legal, scientific, and lit-
erary domains we consider each section of a doc-
ument independently, we are not performing a true
exhaustive search for these domains, but rather solv-
ing a suboptimal problem, as we divide the number
of words in the model summary to each section pro-
portional to the section?s length. However, we be-
lieve that this is a fair assumption, as it has been
shown repeatedly in the past that text segmentation
helps improving the performance of text summariza-
tion systems (yen Kan et al, 1998; Nakao, 2000;
Mihalcea and Ceylan, 2007).
5 Exhaustive Search Algorithm
Let Eik = Si1 , Si2 , ..., Sik be the ith extract that
has k sentences, and generated from a document
D with n sentences D = S1, S2, . . . , Sn. Further,
let len(Sj) give the number of words in sentence
Sj . We enforce that Eik satisfies the following con-
straints:
len(Eik) = len(Si1) + . . . + len(Sik) ? L
len(Eik?1) = len(Si1) + . . . + len(Sik?1) < L
where L is the length constraint on all the extracts
of document D. We note that for any Eik , the or-
der of the sentences in Eik?1 does not affect the
ROUGE scores, since only the last sentence may be
7http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-
1.211.tar.gz
905
chopped off due to the length constraint.8 Hence, we
start generating sentence combinations
(n
r
)
in lexico-
graphic order, for r = 1...n, and for each combina-
tion Eik = Si1 , Si2 , ..., Sik where k > 1, we gener-
ate additional extracts E?ik by successfully swapping
Sij with Sik for j = 1, ..., k? 1 and checking to see
if the above constraints are still satisfied. Therefore
from a combination with k sentences that satisfies
the constraints, we might generate up to k ? 1 ad-
ditional extracts. Finally, we stop the process either
when r = n and the last combination is generated,
or we cannot find any extract that satisfies the con-
straints for r.
6 Generating pdfs
Once the extracts for a document are generated and
evaluated, we go through each result and assign its
recall score to a range, which we refer to as a bin.
We use 1, 000 equally spaced bins between 0 and
1. As an example, a recall score of 0.46873 would
be assigned to the bin [0.468, 0.469]. By keeping
a count for each bin, we are in fact building a his-
togram of scores for the document. Let this his-
togram be h, and h[j] be the value in the jth bin of
the histogram. We then define the normalized his-
togram h? as:
h?[j] =
N
?N
i=1 h[j]
h[j] (1)
where N = 1, 000 is the number of bins in the his-
togram. Note that since the width of each bin is 1N ,
the Riemann sum of the normalized histogram h? is
equal to 1, so h? can be used as an approximation
to the underlying pdf. As an example, we show the
histogram h? for the newswire document AP890323-
0218 in Figure 1.
We combine the normalized histograms of all the
documents in a domain in order to find the pdf for
that domain. This requires multiplying the value
of each bin in a document?s histogram, with all
the other possible combinations of bin values taken
from each of the remaining histograms, and assign-
ing the result to the average bin for each combina-
8Note that we do not take the coherence of extracts into ac-
count, i.e. the sentences in an extract do not need to be sorted
in order of their appearance in the original document. We also
do not change the position of the words in a sentence.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  100  200  300  400  500  600  700  800  900  1000
"AP890323-0218.dat"
Figure 1: The normalized histogram h? of ROUGE-1 re-
call scores for the newswire document AP890323-0218.
tion. This can be done iteratively by keeping a mov-
ing average. We illustrate this procedure in Algo-
rithm 1, where K represents the number of docu-
ments in a domain.
Algorithm 1 Combine h?i?s for i = 1, . . . ,K to cre-
ate hd, the histogram for domain d.
1: hd := {}
2: for i = 1 to N do
3: hd[i] := h?1[i]
4: end for
5: for i = 2 to K do
6: ht = {}
7: for j = 1 to N do
8: for k = 1 to N do
9: a = round(((k ? (i? 1)) + j)/i)
10: ht[a] = ht[a] + (hd[k] ? h?i[j])
11: end for
12: end for
13: hd := ht
14: end for
The resulting histogram hd, when normalized us-
ing Equation 1, is an approximation to the pdf for
domain d. Furthermore, we used the round() func-
tion in line 9, which rounds a number to the nearest
integer, as the bins are indexed by integers. Note
that this rounding introduces an error, which is dis-
tributed uniformly due to the nature of the round()
function. It is also possible to lower the affect of this
error with higher resolutions (i.e. larger number of
bins). In Figure 2, we show a sample hd, obtained
by combining 10 documents from the newswire do-
906
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
"newswire_10-ROUGE-1.dat"
Figure 2: An example pdf obtained by combining 10 doc-
ument histograms of ROUGE-1 recall scores from the
newswire domain. The x-axis is normalized to [0,1].
main.
Recall from Section 4 that the documents in
the literary, legal, and scientific domains are di-
vided into sections either by using the given section
boundaries or by applying a text segmentation al-
gorithm, and the extracts of each section are then
evaluated individually. Hence for these domains, we
first calculate the histogram of each section individ-
ually, and then combine them to find the histogram
of a document. The combination procedure for the
section histograms is similar to Algorithm 1, except
that in this case we do not keep a moving average,
but rather sum up the bins of the sections. Note
that when bin i and j are added, the resulting val-
ues should be expected to be half the times in bin
i + j, and half the times in i + j ? 1.
7 Calculating Percentile Ranks
Given a pdf for a domain, the success of a system
having a ROUGE recall score of S could be sim-
ply measured by finding the area bounded by S.
This gives us the percentile rank of the system in
the overall distribution. Assuming 0 ? S ? 1, let
S? = ?N ?S?, then the formula to calculate the per-
centile rank can be simply given as:
PR(S) =
100
N
bS?
i=1
h?d[i] (2)
ROUGE-1
Domain ? ? max min
Newswire 39.39 0.87 65.70 20.20
Literary 45.20 0.47 63.90 28.40
Scientific 45.99 0.68 71.90 24.20
Legal 72.82 0.28 82.40 62.80
ROUGE-2
Domain ? ? max min
Newswire 11.57 0.79 37.40 1.60
Literary 5.41 0.34 16.90 1.80
Scientific 10.98 0.60 33.30 1.30
Legal 28.74 0.29 40.90 19.60
ROUGE-SU4
Domain ? ? max min
Newswire 15.33 0.69 38.10 6.40
Literary 13.28 0.30 24.30 6.90
Scientific 16.13 0.50 35.80 6.20
Legal 35.63 0.25 45.70 28.70
Table 2: Statistical properties of the pdfs
8 Results
The ensemble distributions of ROUGE-1 recall
scores per document are shown in Figure 3. The
ensemble distributions tell us that the performance
of the extracts, especially for the news and the sci-
entific domains, are mostly uniform for each docu-
ment. This is due to the fact that documents in these
domains, and their corresponding summaries, are
written with a certain conventional style. There is
however a little scattering in the distributions of the
literary and the legal domains. This is an expected
result for the literary domain, as there is no specific
summarization style for these documents, but some-
how surprising for the legal domain, where the effect
is probably due to the different types of legal docu-
ments in the data set.
The pdf plots resulting from the ROUGE-1 recall
scores are shown in Figure 4.9 In order to analyze
the pdf plots, and better understand their differences,
Table 2 lists the mean (?) and the standard deviation
(?) measures of the pdfs, as well as the average min-
imum and maximum scores that an extractive sum-
marization system can get for each domain.
By looking at the pdf plots and the minimum and
maximum columns from Table 2, we notice that for
9Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4,
even if at a different scale.
907
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45  50
"Ensemble-Newswire-50-ROUGE-1.dat"
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Literary-50-Ensemble-ROUGE-1.dat"
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Medical-50-Ensemble-ROUGE-1.dat"
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Legal-50-Ensemble-ROUGE-1.dat"
Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains,
respectively from left to right.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0.36  0.38  0.4  0.42  0.44
"Newswire-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Literary-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Medical-50-ROUGE-1.dat"
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0.7  0.72  0.74  0.76  0.78  0.8
"Legal-50-ROUGE-1.dat"
Figure 4: Probability Density Functions of ROUGE-1 recall scores for the Newswire, Literary, Scientific and Legal
Domains, respectively from left to right. The resolution of the x-axis is increased to 0.1.
all the domains, the pdfs are long-tailed distribu-
tions. This immediately implies that most of the
extracts in a summary space are clustered around
the mean, which means that for automatic summa-
rization systems, it is very easy to get scores around
this range. Furthermore, we can judge the hardness
of each domain by looking at the standard devia-
tion values. A lower standard deviation indicates a
steeper curve, which implies that improving a sys-
tem would be harder. From the table, we can in-
fer that the legal domain is the hardest while the
newswire is the easiest.
Comparing Table 2 with the values in Table 1,
we also notice that the compression ratio affects the
performance differently for each domain. For ex-
ample, although the scientific domain has the high-
est compression ratio, it has a higher mean than
the literary and the newswire domains for ROUGE-
1 and ROUGE-SU4 recall scores. This implies
that although the abstracts of the medical journals
are highly compressed, they have a high overlap
with the document, probably caused by their writ-
ing style. This was in fact confirmed earlier by the
experiments in (Kupiec et al, 1995), where it was
found out that for a data set of 188 scientific arti-
cles, 79% of the sentences in the abstracts could be
perfectly matched with the sentences in the corre-
sponding documents.
Next, we confirm our experiments by testing three
different extractive summarization systems on our
data set. The first system that we implement is called
Random, and gives a random score between 1 and
100 to each sentence in a document, and then se-
lects the top scoring sentences. The second system,
Lead, implements the lead baseline method which
takes the first k sentences of a document until the
length limit is reached. Finally, the last system that
we implement is TextRank, which uses a variation of
the PageRank graph centrality algorithm in order to
identify the most important sentences in a document
(Page et al, 1999; Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). We selected TextRank as it has a
performance competitive with the top systems par-
ticipating in DUC ?02 (Mihalcea and Tarau, 2004).
We would also like to mention that for the literary,
scientific, and legal domains, the systems apply the
algorithms for each section and each section is eval-
uated independently, and their resulting recall scores
are summed up. This is needed in order to be con-
sistent with our exhaustive search experiments.
The ROUGE recall scores of the three systems are
shown in Table 3. As expected, for the literary and
legal domains, the Random, and the Lead systems
score around the mean. This is due to the fact that
the leading sentences for these two domains do not
indicate any significance, hence the Lead system just
behaves like Random. However for the scientific and
newswire domains, the leading sentences do have
908
ROUGE-1
Domain Random Lead TextRank
Newswire 39.13 45.63 44.43
Literary 45.39 45.36 46.12
Scientific 45.75 47.18 49.26
Legal 73.04 72.42 74.82
ROUGE-2
Domain Random Lead TextRank
Newswire 11.39 19.60 17.99
Literary 5.33 5.41 5.92
Scientific 10.73 12.07 12.76
Legal 28.56 28.92 31.06
ROUGE-SU4
Domain Random Lead TextRank
Newswire 15.07 21.58 20.46
Literary 13.21 13.28 13.81
Scientific 15.92 17.12 17.85
Legal 35.41 35.55 37.64
Table 3: ROUGE recall scores of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
importance so the Lead system consistently outper-
forms Random. Furthermore, although TextRank is
the best system for the literary, scientific, and legal
domains, it gets outperformed by the Lead system
on the newswire domain. This is also an expected re-
sult as none of the single-document summarization
systems were able to achieve a statistically signifi-
cant improvement over the lead baseline in the previ-
ous Document Understanding Conferences (DUC).
The ROUGE scoring scheme does not tell us how
much improvement a system achieved over another,
or how far it is from the upper bound. Since we now
have access to the pdf of each domain in our data set,
we can find this information simply by calculating
the percentile rank of each system using the formula
given in Equation 2.
The percentile ranks of all three systems for each
domain are shown in Table 4. Notice how different
the gap is between the scores of each system this
time, compared to the scores in Table 3. For ex-
ample, we see in Table 3 that TextRank on scientific
domain has only a 3.51 ROUGE-1 score improve-
ment over a system that randomly selects sentences
to include in the extract. However, Table 4 tells us
that this improvement is in fact 57.57%.
From Table 4, we see that both TextRank and
the Lead system are in the 99.99% percentile of
ROUGE-1
Domain Random Lead TextRank
Newswire %39.18 %99.99 %99.99
Literary %62.89 %62.89 %97.90
Scientific %42.30 %95.56 %99.87
Legal %79.47 %16.19 %99.99
ROUGE-2
Domain Random Lead TextRank
Newswire %39.57 %99.99 %99.99
Literary %42.20 %54.32 %94.34
Scientific %35.6 %96.03 %99.79
Legal %36.68 %75.38 %99.99
ROUGE-SU4
Domain Random Lead TextRank
Newswire %40.68 %99.99 %99.99
Literary %46.39 %46.39 %96.84
Scientific %36.37 %97.69 %99.94
Legal %23.53 %42.00 %99.99
Table 4: Percentile rankings of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
the newswire domain although the systems have
1.20, 1.61, and 1.12 difference in their ROUGE-1,
ROUGE-2, and ROUGE-SU4 scores respectively.
The high percentile for the Lead system explains
why it was so hard to improve over these baseline in
previous evaluations on newswire data (e.g., see the
evaluations from the Document Understanding Con-
ferences). Furthermore, we see from Table 2 that the
upper bounds corresponding to these scores are 65.7,
37.4, and 38.1 respectively, which are well above
both the TextRank and the Lead systems. There-
fore, the percentile rankings of the Lead and the Tex-
tRank systems for this domain do not seem to give
us clues about how the two systems compare to each
other, nor about their actual distance from the up-
per bounds. There are two reasons for this: First,
as we mentioned earlier, most of the summary space
consists of easy extracts, which make the distribu-
tion long-tailed.10 Therefore even though we have
quite a bit of systems achieving high scores, their
number is negligible compared to the millions of ex-
tracts that are clustered around the mean. Secondly,
we need a higher resolution (i.e. larger number of
bins) in constructing the pdfs in order to be able to
10This also accounts for the fact that even though we might
have two very close ROUGE scores that are not statistically sig-
nificant, their percentile rankings might differ quite a bit.
909
see the difference more clearly between the two sys-
tems. Finally, when comparing two successful sys-
tems using percentile ranks, we believe the use of
error reduction would be more beneficial.
As a final note, we also randomly sampled ex-
tracts from documents in the scientific and legal do-
mains, but this time without considering the section
boundaries and without performing any segmenta-
tion. We kept the number of samples for each doc-
ument equal to the number of extracts we generated
from the same document using a divide-and-conquer
approach. We evaluated the samples using ROUGE-
1 recall scores, and obtained pdfs for each domain
using the same strategy discussed earlier in the pa-
per. The resulting pdfs, although they exhibit simi-
lar characteristics, they have mean values (?) around
10% lower than the ones we listed in Table 2, which
supports the findings from earlier research that seg-
mentation is useful for text summarization.
9 Conclusions and Future Work
In this paper, we described a study that explores the
search space of extractive summaries across four dif-
ferent domains. For the news domain we generated
all possible extracts of the given documents, and
for the literary, scientific, and legal domains we fol-
lowed a divide-and-conquer approach by chunking
the documents into sections, handled each section
independently, and combined the resulting scores at
the end. We then used the distributions of the eval-
uations scores to generate the probability density
functions (pdfs) for each domain. Various statistical
properties of these pdfs helped us asses the difficulty
of each domain. Finally, we introduced a new scor-
ing scheme for automatic text summarization sys-
tems that can be derived from the pdfs. The new
scheme calculates a percentile rank of the ROUGE-
1 recall score of a system, which gives scores in the
range [0-100]. This lets us see how far each sys-
tem is from the upper bound, and thus make a better
comparison among the systems. The new scoring
system showed us that while there is a 20.1% gap
between the upper bound and the lead baseline for
the news domain, closing this gap is difficult, as the
percentile rank of the lead baseline system, 99.99%,
indicates that the system is already very close to the
upper bound.
Furthermore, except for the literary domain, the
percentile rank of the TextRank system is also very
close to the upperbound. This result does not sug-
gest that additional improvements cannot be made
in these domains, but that making further improve-
ments using only extractive summarization will be
considerably difficult. Moreover, in order to see
these future improvements, a higher resolution (i.e.
larger number of bins) will be needed when con-
structing the pdfs.
In all our experiments we used the ROUGE
(Lin, 2004) evaluation package and its ROUGE-
1, ROUGE-2, and ROUGE-SU4 recall scores. We
would like to note that since ROUGE performs its
evaluations based on the n-gram overlap between
the peer and the model summary, it does not take
other summary quality metrics such as coherence
and cohesion into account. However, our goal in this
paper was to analyze the topic-identification stage
only, which concentrates on selecting the right con-
tent from the document to include in the summary,
and the ROUGE scores were found to correlate well
with the human judgments on assessing the content
overlap of summaries.
In the future, we would like to apply a similar ex-
haustive search strategy, but this time with differ-
ent compression ratios, in order to see the impact
of compression ratios on the pdf of each domain.
Furthermore, we would also like to analyze the
high scoring extracts found by the exhaustive search,
in terms of coherence, position and other features.
Such an analysis would allow us to see whether these
extracts exhibit certain properties which could be
used in training machine learning systems.
Acknowledgments
The authors would like to thank the anonymous re-
viewers of NAACL-HLT 2010 for their feedback.
The work of the first author has been partly sup-
ported by an award from Google, Inc. The work of
the fourth and fifth authors has been supported by an
FPI grant (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation, under the project
TEXT-MESS (TIN2006-15265-C06-01) funded by
the Spanish Government, and the project PROME-
TEO Desarrollo de Tcnicas Inteligentes e Interacti-
vas de Minera de Textos (2009/119) from the Valen-
cian Government.
910
References
Hakan Ceylan and Rada Mihalcea. 2009. The decompo-
sition of human-written book summaries. In CICLing
?09: Proceedings of the 10th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, pages 582?593, Berlin, Heidelberg. Springer-
Verlag.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In NAACL-
ANLP 2000 Workshop on Automatic summarization,
pages 69?78, Morristown, NJ, USA. Association for
Computational Linguistics.
G. Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research, 22.
Eduard H. Hovy and Chin Yew Lin. 1999. Automated
text summarization in summarist. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?97. MIT Press.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28(4):527?543.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95: Pro-
ceedings of the 18th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 68?73, New York, NY, USA.
ACM.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73?80, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Chin-Yew Lin. 1999. Training a selection function for
extraction. In CIKM ?99: Proceedings of the eighth
international conference on Information and knowl-
edge management, pages 55?62, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ?99: Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 137?144, New
York, NY, USA. ACM.
Rada Mihalcea and Hakan Ceylan. 2007. Explorations in
automatic book summarization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 380?
389, Prague, Czech Republic, June. Association for
Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Yoshio Nakao. 2000. An algorithm for one-page sum-
marization of a long text based on thematic hierarchy
detection. In ACL ?00: Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 302?309, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
InfoLab.
Karen Sparck-Jones. 1999. Automatic summarising:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1?13. MIT Press.
Simone Teufel and Marc Moens. 1997. Sentence ex-
traction as a classification task. In Proceedings of the
ACL?97/EACL?97 Workshop on Intelligent Scallable
Text Summarization, Madrid, Spain, July.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 491?498.
Min yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 1998. Linear segmentation and segment sig-
nificance. In In Proceedings of the 6th International
Workshop of Very Large Corpora, pages 197?205.
911
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752?762,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning to Grade Short Answer Questions using Semantic Similarity
Measures and Dependency Graph Alignments
Michael Mohler
Dept. of Computer Science
University of North Texas
Denton, TX
mgm0038@unt.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, Ohio
bunescu@ohio.edu
Rada Mihalcea
Dept. of Computer Science
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
In this work we address the task of computer-
assisted assessment of short student answers.
We combine several graph alignment features
with lexical semantic similarity measures us-
ing machine learning techniques and show
that the student answers can be more accu-
rately graded than if the semantic measures
were used in isolation. We also present a first
attempt to align the dependency graphs of the
student and the instructor answers in order to
make use of a structural component in the au-
tomatic grading of student answers.
1 Introduction
One of the most important aspects of the learning
process is the assessment of the knowledge acquired
by the learner. In a typical classroom assessment
(e.g., an exam, assignment or quiz), an instructor or
a grader provides students with feedback on their
answers to questions related to the subject matter.
However, in certain scenarios, such as a number of
sites worldwide with limited teacher availability, on-
line learning environments, and individual or group
study sessions done outside of class, an instructor
may not be readily available. In these instances, stu-
dents still need some assessment of their knowledge
of the subject, and so, we must turn to computer-
assisted assessment (CAA).
While some forms of CAA do not require sophis-
ticated text understanding (e.g., multiple choice or
true/false questions can be easily graded by a system
if the correct solution is available), there are also stu-
dent answers made up of free text that may require
textual analysis. Research to date has concentrated
on two subtasks of CAA: grading essay responses,
which includes checking the style, grammaticality,
and coherence of the essay (Higgins et al, 2004),
and the assessment of short student answers (Lea-
cock and Chodorow, 2003; Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), which is the fo-
cus of this work.
An automatic short answer grading system is one
that automatically assigns a grade to an answer pro-
vided by a student, usually by comparing it to one
or more correct answers. Note that this is different
from the related tasks of paraphrase detection and
textual entailment, since a common requirement in
student answer grading is to provide a grade on a
certain scale rather than make a simple yes/no deci-
sion.
In this paper, we explore the possibility of im-
proving upon existing bag-of-words (BOW) ap-
proaches to short answer grading by utilizing ma-
chine learning techniques. Furthermore, in an at-
tempt to mirror the ability of humans to understand
structural (e.g. syntactic) differences between sen-
tences, we employ a rudimentary dependency-graph
alignment module, similar to those more commonly
used in the textual entailment community.
Specifically, we seek answers to the following
questions. First, to what extent can machine learn-
ing be leveraged to improve upon existing ap-
proaches to short answer grading. Second, does the
dependency parse structure of a text provide clues
that can be exploited to improve upon existing BOW
methodologies?
752
2 Related Work
Several state-of-the-art short answer grading sys-
tems (Sukkarieh et al, 2004; Mitchell et al, 2002)
require manually crafted patterns which, if matched,
indicate that a question has been answered correctly.
If an annotated corpus is available, these patterns
can be supplemented by learning additional pat-
terns semi-automatically. The Oxford-UCLES sys-
tem (Sukkarieh et al, 2004) bootstraps patterns by
starting with a set of keywords and synonyms and
searching through windows of a text for new pat-
terns. A later implementation of the Oxford-UCLES
system (Pulman and Sukkarieh, 2005) compares
several machine learning techniques, including in-
ductive logic programming, decision tree learning,
and Bayesian learning, to the earlier pattern match-
ing approach, with encouraging results.
C-Rater (Leacock and Chodorow, 2003) matches
the syntactical features of a student response (i.e.,
subject, object, and verb) to that of a set of correct
responses. This method specifically disregards the
BOW approach to take into account the difference
between ?dog bites man? and ?man bites dog? while
still trying to detect changes in voice (i.e., ?the man
was bitten by the dog?).
Another short answer grading system, AutoTutor
(Wiemer-Hastings et al, 1999), has been designed
as an immersive tutoring environment with a graph-
ical ?talking head? and speech recognition to im-
prove the overall experience for students. AutoTutor
eschews the pattern-based approach entirely in favor
of a BOW LSA approach (Landauer and Dumais,
1997). Later work on AutoTutor(Wiemer-Hastings
et al, 2005; Malatesta et al, 2002) seeks to expand
upon their BOW approach which becomes less use-
ful as causality (and thus word order) becomes more
important.
A text similarity approach was taken in (Mohler
and Mihalcea, 2009), where a grade is assigned
based on a measure of relatedness between the stu-
dent and the instructor answer. Several measures are
compared, including knowledge-based and corpus-
based measures, with the best results being obtained
with a corpus-based measure using Wikipedia com-
bined with a ?relevance feedback? approach that it-
eratively augments the instructor answer by inte-
grating the student answers that receive the highest
grades.
In the dependency-based classification compo-
nent of the Intelligent Tutoring System (Nielsen et
al., 2009), instructor answers are parsed, enhanced,
and manually converted into a set of content-bearing
dependency triples or facets. For each facet of the
instructor answer each student?s answer is labelled
to indicate whether it has addressed that facet and
whether or not the answer was contradictory. The
system uses a decision tree trained on part-of-speech
tags, dependency types, word count, and other fea-
tures to attempt to learn how best to classify an an-
swer/facet pair.
Closely related to the task of short answer grading
is the task of textual entailment (Dagan et al, 2005),
which targets the identification of a directional in-
ferential relation between texts. Given a pair of two
texts as input, typically referred to as text and hy-
pothesis, a textual entailment system automatically
finds if the hypothesis is entailed by the text.
In particular, the entailment-related works that are
most similar to our own are the graph matching tech-
niques proposed by Haghighi et al (2005) and Rus
et al (2007). Both input texts are converted into a
graph by using the dependency relations obtained
from a parser. Next, a matching score is calculated,
by combining separate vertex- and edge-matching
scores. The vertex matching functions use word-
level lexical and semantic features to determine the
quality of the match while the the edge matching
functions take into account the types of relations and
the difference in lengths between the aligned paths.
Following the same line of work in the textual en-
tailment world are (Raina et al, 2005), (MacCartney
et al, 2006), (de Marneffe et al, 2007), and (Cham-
bers et al, 2007), which experiment variously with
using diverse knowledge sources, using a perceptron
to learn alignment decisions, and exploiting natural
logic.
3 Answer Grading System
We use a set of syntax-aware graph alignment fea-
tures in a three-stage pipelined approach to short an-
swer grading, as outlined in Figure 1.
In the first stage (Section 3.1), the system is pro-
vided with the dependency graphs for each pair of
instructor (Ai) and student (As) answers. For each
753
Figure 1: Pipeline model for scoring short-answer pairs.
node in the instructor?s dependency graph, we com-
pute a similarity score for each node in the student?s
dependency graph based upon a set of lexical, se-
mantic, and syntactic features applied to both the
pair of nodes and their corresponding subgraphs.
The scoring function is trained on a small set of man-
ually aligned graphs using the averaged perceptron
algorithm.
In the second stage (Section 3.2), the node simi-
larity scores calculated in the previous stage are used
to weight the edges in a bipartite graph representing
the nodes in Ai on one side and the nodes in As on
the other. We then apply the Hungarian algorithm
to find both an optimal matching and the score asso-
ciated with such a matching. In this stage, we also
introduce question demoting in an attempt to reduce
the advantage of parroting back words provided in
the question.
In the final stage (Section 3.4), we produce an
overall grade based upon the alignment scores found
in the previous stage as well as the results of several
semantic BOW similarity measures (Section 3.3).
Using each of these as features, we use Support Vec-
tor Machines (SVM) to produce a combined real-
number grade. Finally, we build an Isotonic Regres-
sion (IR) model to transform our output scores onto
the original [0,5] scale for ease of comparison.
3.1 Node to Node Matching
Dependency graphs for both the student and in-
structor answers are generated using the Stanford
Dependency Parser (de Marneffe et al, 2006) in
collapse/propagate mode. The graphs are further
post-processed to propagate dependencies across the
?APPOS? (apposition) relation, to explicitly encode
negation, part-of-speech, and sentence ID within
each node, and to add an overarching ROOT node
governing the main verb or predicate of each sen-
tence of an answer. The final representation is a
list of (relation,governor,dependent) triples, where
governor and dependent are both tokens described
by the tuple (sentenceID:token:POS:wordPosition).
For example: (nsubj, 1:provide:VBZ:4, 1:pro-
gram:NN:3) indicates that the noun ?program? is a
subject in sentence 1 whose associated verb is ?pro-
vide.?
If we consider the dependency graphs output by
the Stanford parser as directed (minimally cyclic)
graphs,1 we can define for each node x a set of nodes
Nx that are reachable from x using a subset of the
relations (i.e., edge types)2. We variously define
?reachable? in four ways to create four subgraphs
defined for each node. These are as follows:
? N0x : All edge types may be followed
? N1x : All edge types except for subject types,
ADVCL, PURPCL, APPOS, PARATAXIS,
ABBREV, TMOD, and CONJ
? N2x : All edge types except for those in N1x plus
object/complement types, PREP, and RCMOD
? N3x : No edge types may be followed (This set
is the single starting node x)
Subgraph similarity (as opposed to simple node
similarity) is a means to escape the rigidity involved
in aligning parse trees while making use of as much
of the sentence structure as possible. Humans intu-
itively make use of modifiers, predicates, and subor-
dinate clauses in determining that two sentence en-
tities are similar. For instance, the entity-describing
phrase ?men who put out fires? matches well with
?firemen,? but the words ?men? and ?firemen? have
1The standard output of the Stanford Parser produces rooted
trees. However, the process of collapsing and propagating de-
pendences violates the tree structure which results in a tree
with a few cross-links between distinct branches.
2For more information on the relations used in this experi-
ment, consult the Stanford Typed Dependencies Manual at
http://nlp.stanford.edu/software/dependencies manual.pdf
754
less inherent similarity. It remains to be determined
how much of a node?s subgraph will positively en-
rich its semantics. In addition to the complete N0x
subgraph, we chose to include N1x and N2x as tight-
ening the scope of the subtree by first removing
more abstract relations, then sightly more concrete
relations.
We define a total of 68 features to be used to train
our machine learning system to compute node-node
(more specifically, subgraph-subgraph) matches. Of
these, 36 are based upon the semantic similarity
of four subgraphs defined by N [0..3]x . All eight
WordNet-based similarity measures listed in Sec-
tion 3.3 plus the LSA model are used to produce
these features. The remaining 32 features are lexico-
syntactic features3 defined only for N3x and are de-
scribed in more detail in Table 2.
We use ?(xi, xs) to denote the feature vector as-
sociated with a pair of nodes ?xi, xs?, where xi is
a node from the instructor answer Ai and xs is a
node from the student answer As. A matching score
can then be computed for any pair ?xi, xs? ? Ai ?
As through a linear scoring function f(xi, xs) =
wT?(xi, xs). In order to learn the parameter vec-
tor w, we use the averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002).
As training data, we randomly select a subset of
the student answers in such a way that our set was
roughly balanced between good scores, mediocre
scores, and poor scores. We then manually annotate
each node pair ?xi, xs? as matching, i.e. A(xi, xs) =
+1, or not matching, i.e. A(xi, xs) = ?1. Overall,
32 student answers in response to 21 questions with
a total of 7303 node pairs (656 matches, 6647 non-
matches) are manually annotated. The pseudocode
for the learning algorithm is shown in Table 1. Af-
ter training the perceptron, these 32 student answers
are removed from the dataset, not used as training
further along in the pipeline, and are not included in
the final results. After training for 50 epochs,4 the
matching score f(xi, xs) is calculated (and cached)
for each node-node pair across all student answers
for all assignments.
3Note that synonyms include negated antonyms (and vice
versa). Hypernymy and hyponymy are restricted to at most
two steps).
4This value was chosen arbitrarily and was not tuned in anyway
0. set w ? 0, w? 0, n? 0
1. repeat for T epochs:
2. foreach ?Ai;As?:
3. foreach ?xi, xs? ? Ai ?As:
4. if sgn(wT?(xi, xs)) 6= sgn(A(xi, xs)):
5. set w ? w+A(xi, xs)?(xi, xs)
6. set w ? w+w, n? n+ 1
7. return w/n.
Table 1: Perceptron Training for Node Matching.
3.2 Graph to Graph Alignment
Once a score has been computed for each node-node
pair across all student/instructor answer pairs, we at-
tempt to find an optimal alignment for the answer
pair. We begin with a bipartite graph where each
node in the student answer is represented by a node
on the left side of the bipartite graph and each node
in the instructor answer is represented by a node
on the right side. The score associated with each
edge is the score computed for each node-node pair
in the previous stage. The bipartite graph is then
augmented by adding dummy nodes to both sides
which are allowed to match any node with a score of
zero. An optimal alignment between the two graphs
is then computed efficiently using the Hungarian al-
gorithm. Note that this results in an optimal match-
ing, not a mapping, so that an individual node is as-
sociated with at most one node in the other answer.
At this stage we also compute several alignment-
based scores by applying various transformations to
the input graphs, the node matching function, and
the alignment score itself.
The first and simplest transformation involves the
normalization of the alignment score. While there
are several possible ways to normalize a matching
such that longer answers do not unjustly receive
higher scores, we opted to simply divide the total
alignment score by the number of nodes in the in-
structor answer.
The second transformation scales the node match-
ing score by multiplying it with the idf 5 of the in-
structor answer node, i.e., replace f(xi, xs) with
idf(xi) ? f(xi, xs).
The third transformation relies upon a certain
real-world intuition associated with grading student
5Inverse document frequency, as computed from the British Na-
tional Corpus (BNC)
755
Name Type # features Description
RootMatch binary 5 Is a ROOT node matched to: ROOT, N, V, JJ, or Other
Lexical binary 3 Exact match, Stemmed match, close Levenshtein match
POSMatch binary 2 Exact POS match, Coarse POS match
POSPairs binary 8 Specific X-Y POS matches found
Ontological binary 4 WordNet relationships: synonymy, antonymy, hypernymy, hyponymy
RoleBased binary 3 Has as a child - subject, object, verb
VerbsSubject binary 3 Both are verbs and neither, one, or both have a subject child
VerbsObject binary 3 Both are verbs and neither, one, or both have an object child
Semantic real 36 Nine semantic measures across four subgraphs each
Bias constant 1 A value of 1 for all vectors
Total 68
Table 2: Subtree matching features used to train the perceptron
answers ? repeating words in the question is easy
and is not necessarily an indication of student under-
standing. With this in mind, we remove any words
in the question from both the instructor answer and
the student answer.
In all, the application of the three transforma-
tions leads to eight different transform combina-
tions, and therefore eight different alignment scores.
For a given answer pair (Ai, As), we assemble the
eight graph alignment scores into a feature vector
?G(Ai, As).
3.3 Lexical Semantic Similarity
Haghighi et al (2005), working on the entailment
detection problem, point out that finding a good
alignment is not sufficient to determine that the
aligned texts are in fact entailing. For instance, two
identical sentences in which an adjective from one is
replaced by its antonym will have very similar struc-
tures (which indicates a good alignment). However,
the sentences will have opposite meanings. Further
information is necessary to arrive at an appropriate
score.
In order to address this, we combine the graph
alignment scores, which encode syntactic knowl-
edge, with the scores obtained from semantic sim-
ilarity measures.
Following Mihalcea et al (2006) and Mohler
and Mihalcea (2009), we use eight knowledge-
based measures of semantic similarity: shortest path
[PATH], Leacock & Chodorow (1998) [LCH], Lesk
(1986), Wu & Palmer(1994) [WUP], Resnik (1995)
[RES], Lin (1998), Jiang & Conrath (1997) [JCN],
Hirst & St. Onge (1998) [HSO], and two corpus-
based measures: Latent Semantic Analysis [LSA]
(Landauer and Dumais, 1997) and Explicit Seman-
tic Analysis [ESA] (Gabrilovich and Markovitch,
2007).
Briefly, for the knowledge-based measures, we
use the maximum semantic similarity ? for each
open-class word ? that can be obtained by pairing
it up with individual open-class words in the sec-
ond input text. We base our implementation on
the WordNet::Similarity package provided by Ped-
ersen et al (2004). For the corpus-based measures,
we create a vector for each answer by summing
the vectors associated with each word in the an-
swer ? ignoring stopwords. We produce a score in
the range [0..1] based upon the cosine similarity be-
tween the student and instructor answer vectors. The
LSA model used in these experiments was built by
training Infomap6 on a subset of Wikipedia articles
that contain one or more common computer science
terms. Since ESA uses Wikipedia article associa-
tions as vector features, it was trained using a full
Wikipedia dump.
3.4 Answer Ranking and Grading
We combine the alignment scores ?G(Ai, As) with
the scores ?B(Ai, As) from the lexical seman-
tic similarity measures into a single feature vector
?(Ai, As) = [?G(Ai, As)|?B(Ai, As)]. The fea-
ture vector ?G(Ai, As) contains the eight alignment
scores found by applying the three transformations
in the graph alignment stage. The feature vector
?B(Ai, As) consists of eleven semantic features ?
the eight knowledge-based features plus LSA, ESA
and a vector consisting only of tf*idf weights ? both
with and without question demoting. Thus, the en-
tire feature vector ?(Ai, As) contains a total of 30
features.
6http://Infomap-nlp.sourceforge.net/
756
An input pair (Ai, As) is then associated with a
grade g(Ai, As) = uT?(Ai, As) computed as a lin-
ear combination of features. The weight vector u is
trained to optimize performance in two scenarios:
Regression: An SVM model for regression (SVR)
is trained using as target function the grades as-
signed by the instructors. We use the libSVM 7 im-
plementation of SVR, with tuned parameters.
Ranking: An SVM model for ranking (SVMRank)
is trained using as ranking pairs all pairs of stu-
dent answers (As, At) such that grade(Ai, As) >
grade(Ai, At), where Ai is the corresponding in-
structor answer. We use the SVMLight 8 implemen-
tation of SVMRank with tuned parameters.
In both cases, the parameters are tuned using a
grid-search. At each grid point, the training data is
partitioned into 5 folds which are used to train a tem-
porary SVM model with the given parameters. The
regression passage selects the grid point with the
minimal mean square error (MSE), and the SVM-
Rank package tries to minimize the number of dis-
cordant pairs. The parameters found are then used to
score the test set ? a set not used in the grid training.
3.5 Isotonic Regression
Since the end result of any grading system is to give
a student feedback on their answers, we need to en-
sure that the system?s final score has some mean-
ing. With this in mind, we use isotonic regression
(Zadrozny and Elkan, 2002) to convert the system
scores onto the same [0..5] scale used by the an-
notators. This has the added benefit of making the
system output more directly related to the annotated
grade, which makes it possible to report root mean
square error in addition to correlation. We train the
isotonic regression model on each type of system
output (i.e., alignment scores, SVM output, BOW
scores).
4 Data Set
To evaluate our method for short answer grading,
we created a data set of questions from introductory
computer science assignments with answers pro-
vided by a class of undergraduate students. The as-
signments were administered as part of a Data Struc-
7http://www.csie.ntu.edu.tw/?cjlin/libsvm/
8http://svmlight.joachims.org/
tures course at the University of North Texas. For
each assignment, the student answers were collected
via an online learning environment.
The students submitted answers to 80 questions
spread across ten assignments and two examina-
tions.9 Table 3 shows two question-answer pairs
with three sample student answers each. Thirty-one
students were enrolled in the class and submitted an-
swers to these assignments. The data set we work
with consists of a total of 2273 student answers. This
is less than the expected 31 ? 80 = 2480 as some
students did not submit answers for a few assign-
ments. In addition, the student answers used to train
the perceptron are removed from the pipeline after
the perceptron training stage.
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both human
judges were graduate students in the computer sci-
ence department; one (grader1) was the teaching as-
sistant assigned to the Data Structures class, while
the other (grader2) is one of the authors of this pa-
per. We treat the average grade of the two annotators
as the gold standard against which we compare our
system output.
Difference Examples % of examples
0 1294 57.7%
1 514 22.9%
2 231 10.3%
3 123 5.5%
4 70 3.1%
5 9 0.4%
Table 4: Annotator Analysis
The annotators were given no explicit instructions
on how to assign grades other than the [0..5] scale.
Both annotators gave the same grade 57.7% of the
time and gave a grade only 1 point apart 22.9% of
the time. The full breakdown can be seen in Table
4. In addition, an analysis of the grading patterns
indicate that the two graders operated off of differ-
ent grading policies where one grader (grader1) was
more generous than the other. In fact, when the two
differed, grader1 gave the higher grade 76.6% of the
time. The average grade given by grader1 is 4.43,
9Note that this is an expanded version of the dataset used by
Mohler and Mihalcea (2009)
757
Sample questions, correct answers, and student answers Grades
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior of portions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
more manageable problems. 4, 4
Table 3: A sample question with short answers provided by students and the grades assigned by the two human judges
while the average grade given by grader2 is 3.94.
The dataset is biased towards correct answers. We
believe all of these issues correctly mirror real-world
issues associated with the task of grading.
5 Results
We independently test two components of our over-
all grading system: the node alignment detection
scores found by training the perceptron, and the
overall grades produced in the final stage. For the
alignment detection, we report the precision, recall,
and F-measure associated with correctly detecting
matches. For the grading stage, we report a single
Pearson?s correlation coefficient tracking the anno-
tator grades (average of the two annotators) and the
output score of each system. In addition, we re-
port the Root Mean Square Error (RMSE) for the
full dataset as well as the median RMSE across each
individual question. This is to give an indication of
the performance of the system for grading a single
question in isolation.10
5.1 Perceptron Alignment
For the purpose of this experiment, the scores as-
sociated with a given node-node matching are con-
verted into a simple yes/no matching decision where
positive scores are considered a match and negative
10We initially intended to report an aggregate of question-level
Pearson correlation results, but discovered that the dataset
contained one question for which each student received full
points ? leaving the correlation undefined. We believe that
this casts some doubt on the applicability of Pearson?s (or
Spearman?s) correlation coefficient for the short answer grad-
ing task. We have retained its use here alongside RMSE for
ease of comparison.
scores a non-match. The threshold weight learned
from the bias feature strongly influences the point
at which real scores change from non-matches to
matches, and given the threshold weight learned by
the algorithm, we find an F-measure of 0.72, with
precision(P) = 0.85 and recall(R) = 0.62. However,
as the perceptron is designed to minimize error rate,
this may not reflect an optimal objective when seek-
ing to detect matches. By manually varying the
threshold, we find a maximum F-measure of 0.76,
with P=0.79 and R=0.74. Figure 2 shows the full
precision-recall curve with the F-measure overlaid.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Sc
or
e
Recall
Precision
F-Measure
Threshold
Figure 2: Precision, recall, and F-measure on node-level
match detection
5.2 Question Demoting
One surprise while building this system was the con-
sistency with which the novel technique of question
demoting improved scores for the BOW similarity
measures. With this relatively minor change the av-
erage correlation between the BOW methods? sim-
758
ilarity scores and the student grades improved by
up to 0.046 with an average improvement of 0.019
across all eleven semantic features. Table 5 shows
the results of applying question demoting to our
semantic features. When comparing scores using
RMSE, the difference is less consistent, yielding an
average improvement of 0.002. However, for one
measure (tf*idf), the improvement is 0.063 which
brings its RMSE score close to the lowest of all
BOW metrics. The reasons for this are not entirely
clear. As a baseline, we include here the results of
assigning the average grade (as determined on the
training data) for each question. The average grade
was chosen as it minimizes the RMSE on the train-
ing data.
? w/ QD RMSE w/ QD Med. RMSE w/ QD
Lesk 0.450 0.462 1.034 1.050 0.930 0.919
JCN 0.443 0.461 1.022 1.026 0.954 0.923
HSO 0.441 0.456 1.036 1.034 0.966 0.935
PATH 0.436 0.457 1.029 1.030 0.940 0.918
RES 0.409 0.431 1.045 1.035 0.996 0.941
Lin 0.382 0.407 1.069 1.056 0.981 0.949
LCH 0.367 0.387 1.068 1.069 0.986 0.958
WUP 0.325 0.343 1.090 1.086 1.027 0.977
ESA 0.395 0.401 1.031 1.086 0.990 0.955
LSA 0.328 0.335 1.065 1.061 0.951 1.000
tf*idf 0.281 0.327 1.085 1.022 0.991 0.918
Avg.grade 1.097 1.097 0.973 0.973
Table 5: BOW Features with Question Demoting (QD).
Pearson?s correlation, root mean square error (RMSE),
and median RMSE for all individual questions.
5.3 Alignment Score Grading
Before applying any machine learning techniques,
we first test the quality of the eight graph alignment
features ?G(Ai, As) independently. Results indicate
that the basic alignment score performs comparably
to most BOW approaches. The introduction of idf
weighting seems to degrade performance somewhat,
while introducing question demoting causes the cor-
relation with the grader to increase while increasing
RMSE somewhat. The four normalized components
of ?G(Ai, As) are reported in Table 6.
5.4 SVM Score Grading
The SVM components of the system are run on the
full dataset using a 12-fold cross validation. Each of
the 10 assignments and 2 examinations (for a total
of 12 folds) is scored independently with ten of the
remaining eleven used to train the machine learn-
Standard w/ IDF w/ QD w/ QD+IDF
Pearson?s ? 0.411 0.277 0.428 0.291
RMSE 1.018 1.078 1.046 1.076
Median RMSE 0.910 0.970 0.919 0.992
Table 6: Alignment Feature/Grade Correlations using
Pearson?s ?. Results are also reported when inverse doc-
ument frequency weighting (IDF) and question demoting
(QD) are used.
ing system. For each fold, one additional fold is
held out for later use in the development of an iso-
tonic regression model (see Figure 3). The param-
eters (for cost C and tube width ) were found us-
ing a grid search. At each point on the grid, the data
from the ten training folds was partitioned into 5 sets
which were scored according to the current param-
eters. SVMRank and SVR sought to minimize the
number of discordant pairs and the mean absolute
error, respectively.
Both SVM models are trained using a linear ker-
nel.11 Results from both the SVR and the SVMRank
implementations are reported in Table 7 along with
a selection of other measures. Note that the RMSE
score is computed after performing isotonic regres-
sion on the SVMRank results, but that it was unnec-
essary to perform an isotonic regression on the SVR
results as the system was trained to produce a score
on the correct scale.
We report the results of running the systems on
three subsets of features ?(Ai, As): BOW features
?B(Ai, As) only, alignment features ?G(Ai, As)
only, or the full feature vector (labeled ?Hybrid?).
Finally, three subsets of the alignment features are
used: only unnormalized features, only normalized
features, or the full alignment feature set.
B CA ? Ten Folds
B CA ? Ten Folds
B CA ? Ten FoldsIR Model
SVM Model
Features
Figure 3: Dependencies of the SVM/IR training stages.
11We also ran the SVR system using quadratic and radial-basis
function (RBF) kernels, but the results did not show signifi-
cant improvement over the simpler linear kernel.
759
Unnormalized Normalized Both
IAA Avg. grade tf*idf Lesk BOW Align Hybrid Align Hybrid Align Hybrid
SVMRank
Pearson?s ? 0.586 0.327 0.450 0.480 0.266 0.451 0.447 0.518 0.424 0.493
RMSE 0.659 1.097 1.022 1.050 1.042 1.093 1.038 1.015 0.998 1.029 1.021
Median RMSE 0.605 0.973 0.918 0.919 0.943 0.974 0.903 0.865 0.873 0.904 0.901
SVR
Pearson?s ? 0.586 0.327 0.450 0.431 0.167 0.437 0.433 0.459 0.434 0.464
RMSE 0.659 1.097 1.022 1.050 0.999 1.133 0.995 1.001 0.982 1.003 0.978
Median RMSE 0.605 0.973 0.918 0.919 0.910 0.987 0.893 0.894 0.877 0.886 0.862
Table 7: The results of the SVM models trained on the full suite of BOW measures, the alignment scores, and the
hybrid model. The terms ?normalized?, ?unnormalized?, and ?both? indicate which subset of the 8 alignment features
were used to train the SVM model. For ease of comparison, we include in both sections the scores for the IAA, the
?Average grade? baseline, and two of the top performing BOW metrics ? both with question demoting.
6 Discussion and Conclusions
There are three things that we can learn from these
experiments. First, we can see from the results that
several systems appear better when evaluating on a
correlation measure like Pearson?s ?, while others
appear better when analyzing error rate. The SVM-
Rank system seemed to outperform the SVR sys-
tem when measuring correlation, however the SVR
system clearly had a minimal RMSE. This is likely
due to the different objective function in the corre-
sponding optimization formulations: while the rank-
ing model attempts to ensure a correct ordering be-
tween the grades, the regression model seeks to min-
imize an error objective that is closer to the RMSE.
It is difficult to claim that either system is superior.
Likewise, perhaps the most unexpected result of
this work is the differing analyses of the simple
tf*idf measure ? originally included only as a base-
line. Evaluating with a correlative measure yields
predictably poor results, but evaluating the error rate
indicates that it is comparable to (or better than) the
more intelligent BOW metrics. One explanation for
this result is that the skewed nature of this ?natural?
dataset favors systems that tend towards scores in
the 4 to 4.5 range. In fact, 46% of the scores output
by the tf*idf measure (after IR) were within the 4 to
4.5 range and only 6% were below 3.5. Testing on
a more balanced dataset, this tendency to fit to the
average would be less advantageous.
Second, the supervised learning techniques are
clearly able to leverage multiple BOW measures to
yield improvements over individual BOW metrics.
The correlation for the BOW-only SVM model for
SVMRank improved upon the best BOW feature
from .462 to .480. Likewise, using the BOW-only
SVM model for SVR reduces the RMSE by .022
overall compared to the best BOW feature.
Third, the rudimentary alignment features we
have introduced here are not sufficient to act as a
standalone grading system. However, even with a
very primitive attempt at alignment detection, we
show that it is possible to improve upon grade learn-
ing systems that only consider BOW features. The
correlations associated with the hybrid systems (esp.
those using normalized alignment data) frequently
show an improvement over the BOW-only SVM sys-
tems. This is true for both SVM systems when con-
sidering either evaluation metric.
Future work will concentrate on improving the
quality of the answer alignments by training a model
to directly output graph-to-graph alignments. This
learning approach will allow the use of more com-
plex alignment features, for example features that
are defined on pairs of aligned edges or on larger
subtrees in the two input graphs. Furthermore, given
an alignment, we can define several phrase-level
grammatical features such as negation, modality,
tense, person, number, or gender, which make bet-
ter use of the alignment itself.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation CAREER award #0747340. Any
opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
760
References
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 165?170. Association for
Computational Linguistics.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
M.C. de Marneffe, T. Grenager, B. MacCartney, D. Cer,
D. Ramage, C. Kiddon, and C.D. Manning. 2007.
Aligning semantic graphs for textual inference and
machine reading. In Proceedings of the AAAI Spring
Symposium. Citeseer.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6?12.
A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Ro-
bust textual inference via graph matching. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 387?394. Association for Computa-
tional Linguistics.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student
essays. In Proceedings of the annual meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as repre-
sentations of contexts for the detection and correction
of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of the International Conference on Research in
Computational Linguistics, Taiwan.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389?405.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C.D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page 48. As-
sociation for Computational Linguistics.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. Proceedings of the 6th International
Computer Assisted Assessment (CAA) Conference.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
R.D. Nielsen, W. Ward, and J.H. Martin. 2009. Recog-
nizing entailment in intelligent tutoring systems. Nat-
ural Language Engineering, 15(04):479?501.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic Short
Answer Marking. ACL WS Bldg Ed Apps using NLP.
R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,
K. Toutanova, B. MacCartney, M.C. de Marneffe, C.D.
Manning, and A.Y. Ng. 2005. Robust textual infer-
ence using diverse knowledge sources. Recognizing
Textual Entailment, page 57.
761
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal, Canada.
V. Rus, A. Graesser, and K. Desai. 2007. Lexico-
syntactic subsumption for textual entailment. Recent
Advances in Natural Language Processing IV: Se-
lected Papers from RANLP 2005, page 187.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-
Marking 2: An Update on the UCLES-Oxford Univer-
sity research into using Computational Linguistics to
Score Short, Free Text Responses. International Asso-
ciation of Educational Assessment, Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tu-
tor?s comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535?542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005.
Initial results and mixed directions for research meth-
ods tutor. In AIED2005 - Supplementary Proceedings
of the 12th International Conference on Artificial In-
telligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proceedings of the 32nd Annual Meeting
of the Association for Computational Linguistics, Las
Cruces, New Mexico.
B. Zadrozny and C. Elkan. 2002. Transforming classifier
scores into accurate multiclass probability estimates.
Edmonton, Alberta.
762
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 103?108,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
An Efficient Indexer for Large N-Gram Corpora
Hakan Ceylan
Department of Computer Science
University of North Texas
Denton, TX 76203
hakan@unt.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Abstract
We introduce a new publicly available tool
that implements efficient indexing and re-
trieval of large N-gram datasets, such as the
Web1T 5-gram corpus. Our tool indexes the
entire Web1T dataset with an index size of
only 100 MB and performs a retrieval of any
N-gram with a single disk access. With an
increased index size of 420 MB and dupli-
cate data, it also allows users to issue wild
card queries provided that the wild cards in the
query are contiguous. Furthermore, we also
implement some of the smoothing algorithms
that are designed specifically for large datasets
and are shown to yield better language mod-
els than the traditional ones on the Web1T 5-
gram corpus (Yuret, 2008). We demonstrate
the effectiveness of our tool and the smooth-
ing algorithms on the English Lexical Substi-
tution task by a simple implementation that
gives considerable improvement over a basic
language model.
1 Introduction
The goal of statistical language modeling is to cap-
ture the properties of a language through a proba-
bility distribution so that the probabilities of word
sequences can be estimated. Since the probability
distribution is built from a corpus of the language
by computing the frequencies of the N-grams found
in the corpus, the data sparsity is always an issue
with the language models. Hence, as it is the case
with many statistical models used in Natural Lan-
guage Processing (NLP), the models give a much
better performance with larger data sets.
However the large data sets, such as the Web1T
5-Gram corpus of (Brants and Franz, 2006), present
a major challenge. The language models built from
these sets cannot fit in memory, hence efficient ac-
cessing of the N-gram frequencies becomes an is-
sue. Trivial methods such as linear or binary search
over the entire dataset in order to access a single
N-gram prove inefficient, as even a binary search
over a single file of 10,000,000 records, which is
the case of the Web1T corpus, requires in the worst
case dlog2(10, 000, 000)e = 24 accesses to the disk
drive.
Since the access to N-grams is costly for these
large data sets, the implementation of further im-
provements such as smoothing algorithms becomes
impractical. In this paper, we overcome this problem
by implementing a novel, publicly available tool1
that employs an indexing strategy that reduces the
access time to any N-gram in the Web1T corpus to a
single disk access. We also make a second contribu-
tion by implementing some of the smoothing models
that take into account the size of the dataset, and are
shown to yield up to 31% perplexity reduction on the
Brown corpus (Yuret, 2008). Our implementation is
space efficient, and provides a fast access to both the
N-gram frequencies, as well as their smoothed prob-
abilities.
2 Related Work
Language modeling toolkits are used extensively for
speech processing, machine translation, and many
other NLP applications. The two of the most pop-
ular toolkits that are also freely available are the
CMU Statistical Language Modeling (SLM) Toolkit
(Clarkson and Rosenfeld, 1997), and the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002). However,
1Our tool can be freely downloaded from the download sec-
tion under http://lit.csci.unt.edu
103
even though these tools represent a great resource
for building language models and applying them to
various problems, they are not designed for very
large corpora, such as the Web1T 5-gram corpus
(Brants and Franz, 2006), hence they do not provide
efficient implementations to access these data sets.
Furthermore, (Yuret, 2008) has recently shown
that the widely popular smoothing algorithms for
language models such as Kneser-Ney (Kneser and
Ney, 1995), Witten-Bell (Witten and Bell, 1991), or
Absolute Discounting do not realize the full poten-
tials of very large corpora, which often come with
missing counts. The reason for the missing counts
is due to the omission of low frequency N-grams in
the corpus. (Yuret, 2008) shows that with a modified
version of Kneser-Ney smoothing algorithm, named
as the Dirichlet-Kneser-Ney, a 31% reduction in per-
plexity can be obtained on the Brown corpus.
A tool similar to ours that uses a hashing tech-
nique in order to provide a fast access to the Web1T
corpus is presented in detail in (Hawker et al, 2007).
The tool provides access to queries with wild card
symbols, and the performance of the tool on 106
queries on a 2.66 GHz processor with 1.5 GBytes
of memory is given approximately as one hour. An-
other tool, Web1T5-Easy, described in (Evert, 2010),
provides indexing of the Web1T corpus via rela-
tional database tables implemented in an SQLite en-
gine. It allows interactive searches on the corpus as
well as collocation discovery. The indexing time of
this tool is reported to be two weeks, while the non-
cached retrieval time is given to be in order of a few
seconds. Other tools that implement a binary search
algorithm as a simpler, yet less efficient method are
also given in (Giuliano et al, 2007; Yuret, 2007).
3 The Web1T 5-gram Corpus
The Web1T 5-gram corpus (Brants and Franz, 2006)
consists of sequences of words (N-grams) and their
associated counts extracted from a Web corpus of
approximately one trillion words. The length of each
sequence, N , ranges from 1 to 5, and the size of the
entire corpus is approximately 88GB (25GB in com-
pressed form). The unigrams form the vocabulary
of the corpus and are stored in a single file which
includes around 13 million tokens and their associ-
ated counts. The remaining N-grams are stored sep-
arately across multiple files in lexicographic order.
For example, there are 977,069,902 distinct trigrams
in the dataset, and they are stored consecutively in
98 files in lexicographic order. Furthermore, each
N-gram file contains 10,000,000 N-grams except the
last one, which contains less. It is also important to
note that N-grams with counts less than 40 are ex-
cluded from the dataset for N = 2, 3, 4, 5, and the
tokens with less than 200 are excluded from the un-
igrams.
4 The Indexer
4.1 B+-trees
We used a B+-tree structure for indexing. A B+-
tree is essentially a balanced search tree where each
node has several children. Indexing large files us-
ing B+ trees is a popular technique implemented
by most database systems today as the underlying
structure for efficient range queries. Although many
variations of B+-trees exist, we use the definition for
primary indexing given in (Salzberg, 1988). There-
fore we assume that the data, which is composed of
records, is only stored in the leaves of the tree and
the internal nodes store only the keys.
The data in the leaves of a B+-tree is grouped
into buckets, where the size of a bucket is deter-
mined by a bucket factor parameter, bkfr. Therefore
at any given time, each bucket can hold a number of
records in the range [1, bkfr]. Similarly, the num-
ber of keys that each internal node can hold is deter-
mined by the order parameter, v. By definition, each
internal node except the root can have any number of
keys in the range [v, 2v], and the root must have at
least one key. Finally, an internal node with k keys
has k + 1 children.
4.2 Mapping Unigrams to Integer Keys
A key in a B+-tree is a lookup value for a record,
and a record in our case is an N-gram together with
its count. Therefore each line of an N-gram file in
the Web1T dataset makes up a record. Since each
N-gram is distinct, it is possible to use the N-gram
itself as a key. However in order to reduce the stor-
age requirements and make the comparisons faster
during a lookup, we map each unigram to an inte-
ger, and form the keys of the records using the inte-
ger values instead of the tokens themselves.2
To map unigrams to integers, we use the unigrams
sorted in lexicographic order and assign an integer
value to each unigram starting from 1. In other
words, if we let the m-tuple U = (t1, t2, ..., tm) rep-
resent all the unigrams sorted in lexicographic order,
2This method does not give optimal storage, for which one
should implement a compression Huffman coding scheme.
104
then for a unigram ti, i gives its key value. The key
of trigram ?ti tj tk? is simply given as ?i j k.? Thus,
the comparison of two keys can be done in a similar
fashion to the comparison of two N-grams; we first
compare the first integer of each key, and in case of
equality, we compare the second integers, and so on.
We stop the comparison as soon as an inequality is
found. If all the comparisons result in equality then
the two keys (N-grams) are equal.
4.3 Searching for a Record
We construct a B+-tree for each N-gram file in the
dataset for N = 2, 3, 4, 5, and keep the key of the
first N-gram for each file in memory. When a query
q is issued, we first find the file that contains q by
comparing the key of q to the keys in memory. Since
this is an in-memory operation, it can be simply
done by performing a binary search. Once the cor-
rect file is found, we then search the B+-tree con-
structed for that file for the N-gram q by using its
key.
As is the case with any binary search tree, a search
in a B+-tree starts at the root level and ends in the
leaves. If we let ri and pj represent a key and a
pointer to the child of an internal node respectively,
for i = 1, 2, ..., k and j = 1, 2, ..., k + 1, then to
search an internal node, including the root, for a key
q, we first find the key rm that satisfies one of the
following:
? (q < rm) ? (m = 1)
? (rm?1 ? q) ? (rm > q) for 1 < m ? k
? (q > rm) ? (m = k)
If one of the first two cases is satisfied, the search
continues on the child node found by following pm,
whereas if the last condition is satisfied, the pointer
pm+1 is followed. Since the keys in an internal node
are sorted, a binary search can be performed to find
rm. Finally, when a leaf node is reached, the entire
bucket is read into memory first, then a record with
a key value of q is searched.
4.4 Constructing a B+-tree
The construction of a B+-tree is performed through
successive record insertions.3 Given a record, we
3Note that this may cause efficiency issues for very large
files as memory might become full during the construction pro-
cess, hence in practice, the file is usually sorted prior to index-
ing.
first compute its key, find the leaf node it is supposed
to be in, and insert it if the bucket is not full. Other-
wise, the leaf node is split into two nodes, each con-
taining dbkfr/2e, and bbkfr/2c+1 records, and the
first key of the node containing the larger key values
is placed into the parent internal node together with
the node?s pointer. The insertion of a key to an in-
ternal node is similar, only this time both split nodes
contain v values, and the middle key value is sent up
to the parent node.
Note that not all the internal nodes of a B+-tree
have to be kept on the disk, and read from there each
time we do a search. In practice, all but the last two
levels of a B+-tree are placed in memory. The rea-
son for this is the high branching factor of the B+-
trees together with their effective storage utilization.
It has been shown in (Yao, 1978) that the nodes of a
high-order B+-tree are ln2 ? 69% full on average.
However, note that the tree will be fixed in our
case, i.e., once it is constructed we will not be in-
serting any other N-gram records. Therefore we do
not need to worry about the 69% space utilization,
but instead try to make each bucket, and each in-
ternal node full. Thus, with a bkfr = 1250, and
v = 100, an N-gram file with 10,000,000 records
would have 8,000 leaf nodes on level 3, 40 inter-
nal nodes on level 2, and the root node on level 1.
Furthermore, let us assume that integers, disk and
memory pointers all hold 8 bytes of space. There-
fore a 5-gram key would require 40 bytes, and a full
internal node in level 2 would require (200x40) +
(201x8) = 9, 608 bytes. Thus the level 2 would re-
quire 9, 608x40 ? 384 Kbytes, and level 1 would
require (40?40)+(41?8) = 1, 928 bytes. Hence, a
Web1T 5-gram file, which has an average size of 286
MB can be indexed with approximately 386 Kbytes.
There are 118 5-gram files in the Web1T dataset, so
we would need 386x118 ? 46 MBytes of memory
space in order to index all of them. A similar calcu-
lation for 4-grams, trigrams, and bigrams for which
the bucket factor values are selected as 1600, 2000,
and 2500 respectively, shows that the entire Web1T
corpus, except unigrams, can be indexed with ap-
proximately 100 MBytes, all of which can be kept
in memory, thereby reducing the disk access to only
one. As a final note, in order to compute a key
for a given N-gram quickly, we keep the unigrams
in memory, and use a hashing scheme for mapping
tokens to integers, which additionally require 178
Mbytes of memory space.
The choice of the bucket factor and the inter-
105
nal node order parameters depend on the hard-disk
speed, and the available memory.4. Recall that even
to fetch a single N-gram record from the disk, the en-
tire bucket needs to be read. Therefore as the bucket
factor parameter is reduced, the size of the index will
grow, but the access time would be faster as long as
the index could be entirely fit in memory. On the
other hand, with a too large bucket factor, although
the index can be made smaller, thereby reducing the
memory requirements, the access time may be un-
acceptable for the application. Note that a random
reading of a bucket of records from the hard-disk
requires the disk head to first go to the location of
the first record, and then do a sequential read.5 As-
suming a hard-disk having an average transfer rate
of 100 MBytes, once the disk head finds the correct
location, a 40 bytes N-gram record can be read in
4x10?7 seconds. Thus, assuming a seek time around
8-10 ms, even with a bucket factor of 1,000, it can be
seen that the seek time is still the dominating factor.
Therefore, as the bucket size gets smaller than 1,000,
even though the index size will grow, there would be
almost no speed up in the access time, which justi-
fies our parameter choices.
4.5 Handling Wild Card Queries
Having described the indexing scheme, and how to
search for a single N-gram record, we now turn our
attention to queries including one or more wild card
symbols, which in our case is the underscore char-
acter ? ?, as it does not exist among the unigram
tokens of the Web1T dataset. We manually add the
wild card symbol to our mapping of tokens to inte-
gers, and map it to the integer 0, so that a search for a
query with a wild card symbol would be unsuccess-
ful but would point to the first record in the file that
replaces the wild card symbol with a real token as
the key for the wild card symbol is guaranteed to be
the smallest. Having found the first record we per-
form a sequential read until the last read record does
not match the query. The reason this strategy works
is because the N-grams are sorted in lexicographic
order in the data set, and also when we map unigram
tokens to integers, we preserve their order, i.e., the
first token in the lexicographically sorted unigram
list is assigned the value 1, the second is assigned
4We used a 7200 RPM disk-drive with an average read seek
time of 8.5 ms, write seek time of 10.0 ms, and a data transfer
time up to 3 GBytes per second.
5A rotational latency should also be taken into account be-
fore the sequential reading can be done.
2, and so forth. For example, for a given query Our
Honorable , the record that would be pointed at the
end of search in the trigram file 3gm-0041 is the N-
gram Our Honorable Court 186, which is the first
N-gram in the data set that starts with the bigram
Our Honorable.
Note however that the methodology that is de-
scribed to handle the queries with wild card sym-
bols will only work if the wild card symbols are
the last tokens of the query and they are contigu-
ous. For example a query such as Our Court will
not work as N-grams satisfying this query are not
stored contiguously in the data set. Therefore in or-
der to handle such queries, we need to store addi-
tional copies of the N-grams sorted in different or-
ders. When the last occurrence of the contiguous
wild card symbols is in position p of a query N-gram
for p = 0, 1, ..., N ? 1, then the N-grams sorted lex-
icographically starting from position (p + 1)modN
needs to be searched. A lexicographical sort for a
position p, for 0 ? p ? (N ? 1) is performed by
moving all the tokens in positions 0...(p ? 1) to the
end for each N-gram in the data set. Thus, for all
the bigrams in the data set, we need one extra copy
sorted in position 1, for all the trigrams, we need
two extra copies; one sorted in position 1, and an-
other sorted in position 2, and so forth. Hence, in
order to handle the contiguous wild card queries in
any position, in addition to the 88 GBytes of origi-
nal Web1T data, we need an extra disk space of 265
GBytes. Furthermore, the indexing cost of the du-
plicate data is an additional 320 MBytes. Thus, the
total disk cost of the system will be approximately
353 GBytes plus the index size of 420 MBytes, and
since we keep the entire index in memory, the final
memory cost of the system will be 420 MBytes +
178 MBytes = 598 MBytes.
4.6 Performance
Given that today?s commodity hardware comes with
at least 4 GBytes of memory and 1 TBytes of hard-
disk space, the requirements of our tool are rea-
sonable. Furthermore, our tool is implemented in
a client-server architecture, and it allows multiple
clients to submit multiple queries to the server over
a network. The server can be queried with an N-
gram query either for its count in the corpus, or
its smoothed probability with a given smoothing
method. The queries with wild cards can ask for
the retrieval of all the N-grams satisfying a query, or
only for the total count so the network overhead can
106
be avoided depending on the application needs.
Our program requires about one day of offline
processing due to resorting the entire data a few
times. Note that some of the files in the corpus
need to be sorted as many as four times. For the
sorting process, the files are first individually sorted,
and then a k-way merge is performed. In our im-
plementation, we used a min heap structure for this
purpose, and k is always chosen as the number of
files for a given N. The index construction however
is relatively fast. It takes about an hour to construct
the index for the 5-grams. Once the offline process-
ing is done, it only takes a few minutes to start the
server, and from that point the online performance
of our tool is very fast. It takes about 1-2 seconds to
process 1000 randomly picked 5-gram queries (with
no wild card symbols), which may or may not exist
in the corpus. For the queries asking for the fre-
quencies only, our tool implements a small caching
mechanism that takes the temporal locality into ac-
count. The mechanism is very useful for wild card
queries involving stop words, such as ?the ?, and
?of the ? which occur frequently, and take a long
time to process due to the sequential read of a large
number of records from the data set.
5 Lexical Substitution
In this section we demonstrate the effectiveness of
our tool by using it on the the English Lexical Sub-
stitution task, which was first introduced in SemEval
2007 (McCarthy and Navigli, 2007). The task re-
quires both the human annotators and the participat-
ing systems to replace a target word in a given sen-
tence with the most appropriate alternatives. The de-
scription of the tasks, the data sets, the performance
of the participating systems as well as a post analy-
sis of the results is given in (McCarthy and Navigli,
2009).
Although the task includes three subtasks, in this
evaluation we are only concerned with one of them,
namely the best subtask. The best subtask asks the
systems and the annotators to provide only one sub-
stitute for the target words ? the most appropriate
one. Two separate datasets were provided with this
task: a trial dataset was first provided in order for
the participants to get familiar with the task and train
their systems. The trial data used a lexical sample of
30 words with 10 instances each. The systems were
then tested on a larger test data, which used a lexical
sample of 171 words each again having 10 instances.
Our methodology for this task is very simple; we
Model Precision Mod Precision
No Smoothing 10.13 14.78
Absolute Discounting 11.05 16.75
KN with Missing Counts 11.19 16.75
Dirichlet KN 10.98 15.76
Table 1: Results on the trial data
Model Precision Mod Precision
No Smoothing 9.01 14.15
Absolute Discounting 11.64 18.62
KN with Missing Counts 11.61 18.54
Dirichlet KN 11.03 17.48
Best Baseline 9.95 15.28
Best SEMEVAL System 12.90 20.65
Table 2: Results on the test data
replace the target word with an alternative from a list
of candidates, and find the probability of the context
with the new word using a language model. The can-
didate that gives the highest probability is provided
as the system?s best guess. The list of candidates is
obtained from two different lexical sources, Word-
Net (Fellbaum, 1998) and Roget?s Thesaurus (The-
saurus.com, 2007). We retrieve all the synonyms
for all the different senses of the word from both re-
sources and combine them. We did not consider any
lexical relations other than synonymy, and similarly
we did not consider any words at a further semantic
distance.
We start with a simple language model that cal-
culates the probability of the context of a word,
and then continue with three smoothing algorithms
discussed in (Yuret, 2008), namely Absolute Dis-
counting, Kneser-Ney with Missing Counts, and the
Dirichlet-Kneser-Ney Discounting. Note that all
three are interpolated models, i.e., they do not just
back-off to a lower order probability when an N-
gram is not found, but rather use the higher and
lower order probabilities all the time in a weighted
fashion.
The results on the trial dataset are shown in Ta-
ble 1, and the results on the test dataset are shown
in Table 2. In all the experiments we use the trigram
models, i.e., we keep N fixed to 3. Since our sys-
tem makes a guess for all the target words in the set,
our precision and recall scores, as well as the mod
precision and the mod recall scores are the same,
so only one from each is shown in the table. Note
that the highest achievable score for this task is not
100%, but is restricted by the frequency of the best
substitute, and it is given as 46.15%. The highest
scoring participating system achieved 12.9%, which
107
gave a 2.95% improvement over the baseline (Yuret,
2008; McCarthy and Navigli, 2009); the scores ob-
tained by the best SEMEVAL system as well as the
best baseline calculated using the synonyms for the
first synset in WordNet are also shown in Table 2.
On both the trial and the test data, we see that the
interpolated smoothing algorithms consistently im-
prove over the naive language modeling, which is
an encouraging result. Perhaps a surprising result
for us was the performance of the Dirichlet-Kneser-
Ney Smoothing Algorithm, which is shown to give
minimum perplexity on the Brown corpus out of the
given models. This might suggest that the parame-
ters of the smoothing algorithms need adjustments
for each task.
It is important to note that this evaluation is meant
as a simple proof of concept to demonstrate the use-
fulness of our indexing tool. We thus used a very
simple approach for lexical substitution, and did not
attempt to integrate several lexical resources and
more sophisticated algorithms, as some of the best
scoring systems did. Despite this, the performance
of our system exceeds the best baseline, and is better
than five out of the eight participating systems (see
(McCarthy and Navigli, 2007)).
6 Conclusions
In this paper we described a new publicly avail-
able tool that provides fast access to large N-gram
datasets with modest hardware requirements. In
addition to providing access to individual N-gram
records, our tool also handles queries with wild card
symbols, provided that the wild cards in the query
are contiguous. Furthermore, the tool also imple-
ments smoothing algorithms that try to overcome
the missing counts that are typical to N-gram cor-
pora due to the omission of low frequencies. We
tested our tool on the English Lexical Substitution
task, and showed that the smoothing algorithms give
an improvement over simple language modeling.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #0917170
and #1018613. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram corpus
version 1. Linguistic Data Consortium.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the cmu-cambridge toolkit. In Pro-
ceedings of ESCA Eurospeech, pages 2707?2710.
S. Evert. 2010. Google web 1t 5-grams made easy (but
not for the computer). In Proceedings of the NAACL
HLT 2010 Sixth Web as Corpus Workshop, WAC-6 ?10,
pages 32?40.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
C. Giuliano, A. Gliozzo, and C. Strapparava. 2007. Fbk-
irst: lexical substitution task exploiting domain and
syntagmatic coherence. In SemEval ?07: Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, pages 145?148.
T. Hawker, M. Gardiner, and A. Bennetts. 2007. Practi-
cal queries of a massive n-gram database. In Proceed-
ings of the Australasian Language Technology Work-
shop 2007, pages 40?48, Melbourne, Australia.
R. Kneser and H. Ney. 1995. Improved backing-off for
n-gram language modeling. In Acoustics, Speech, and
Signal Processing, 1995. ICASSP-95., 1995 Interna-
tional Conference on, volume 1, pages 181?184 vol.1.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In SemEval ?07:
Proceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 48?53.
D. McCarthy and R. Navigli. 2009. The english lexical
substitution task. Language Resources and Evalua-
tion, 43:139?159.
B. Salzberg. 1988. File structures: an analytic ap-
proach. Prentice-Hall, Inc., Upper Saddle River, NJ,
USA.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Thesaurus.com. 2007. Rogets new millennium the-
saurus, first edition (v1.3.1).
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
A. Chi-Chih Yao. 1978. On random 2-3 trees. Acta Inf.,
9:159?170.
D. Yuret. 2007. Ku: word sense disambiguation by sub-
stitution. In SemEval ?07: Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, pages
207?213.
D. Yuret. 2008. Smoothing a tera-word language model.
In HLT ?08: Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
on Human Language Technologies, pages 141?144.
108
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 259?263,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Word Epoch Disambiguation:
Finding How Words Change Over Time
Rada Mihalcea
Computer Science and Engineering
University of North Texas
rada@cs.unt.edu
Vivi Nastase
Institute for Computational Linguistics
University of Heidelberg
nastase@cl.uni-heidelberg.de
Abstract
In this paper we introduce the novel task of
?word epoch disambiguation,? defined as the
problem of identifying changes in word us-
age over time. Through experiments run us-
ing word usage examples collected from three
major periods of time (1800, 1900, 2000), we
show that the task is feasible, and significant
differences can be observed between occur-
rences of words in different periods of time.
1 Introduction
Most current natural language processing works
with language as if it were a constant. This how-
ever, is not the case. Language is continually chang-
ing: we discard or coin new senses for old words;
metaphoric and metonymic usages become so en-
grained that at some point they are considered lit-
eral; and we constantly add new words to our vocab-
ulary. The purpose of the current work is to look at
language as an evolutionary phenomenon, which we
can investigate and analyze and use when working
with text collections that span a wide time frame.
Until recently, such task would not have been
possible because of the lack of large amounts of
non-contemporary data.1 This has changed thanks
to the Google books and Google Ngrams historical
projects. They make available in electronic format
a large amount of textual data starting from the 17th
century, as well as statistics on word usage. We will
exploit this data to find differences in word usage
across wide periods of time.
1While the Brown corpus does include documents from dif-
ferent years, it is far from the scale and time range of Google
books.
The phenomena involved in language change are
numerous, and for now we focus on word usage in
different time epochs. As an example, the word gay,
currently most frequently used to refer to a sexual
orientation, was in the previous century used to ex-
press an emotion. The word run, in the past used in-
transitively, has acquired a transitive sense, common
in computational circles where we run processes,
programs and such.
The purpose of the current research is to quan-
tify changes in word usage, which can be the ef-
fect of various factors: changes in meaning (ad-
dition/removal of senses), changes in distribution,
change in topics that co-occur more frequently with
a given word, changes in word spelling, etc. For now
we test whether we can identify the epoch to which a
word occurrence belongs. We use two sets of words
? one with monosemous words, the other with poly-
semous ones ? to try and separate the effect of topic
change over time from the effect of sense change.
We use examples from Google books, split into
three epochs: 1800+/-25 years, 1900+/-25, 2000+/-
25. We select open-class words that occur frequently
in all these epochs, and words that occur frequently
only in one of them. We then treat each epoch as
a ?class,? and verify whether we can correctly pre-
dict this class for test instances from each epoch for
the words in our lists. To test whether word usage
frequency or sense variation have an impact on this
disambiguation task, we use lists of words that have
different frequencies in different epochs as well as
different polysemies. As mentioned before, we also
compare the performance of monosemous ? and thus
(sensewise) unchanged through time ? and polyse-
mous words, to verify whether we can in fact predict
sense change as opposed to contextual variation.
259
2 Related Work
The purpose of this paper is to look at words and
how they change in time. Previous work that looks
at diachronic language change works at a higher lan-
guage level, and is not specifically concerned with
how words themselves change.
The historical data provided by Google has
quickly attracted researchers in various fields, and
started the new field of culturomics (Michel et al,
2011). The purpose of such research is to analyse
changes in human culture, as evidenced by the rise
and fall in usage of various terms.
Reali and Griffiths (2010) analyse the similarities
between language and genetic evolution, with the
transmission of frequency distributions over linguis-
tic forms functioning as the mechanism behind the
phenomenon of language change.
Blei and Lafferty (2006) and Blei and Lafferty
(2007) track changes in scientific topics through a
discrete dynamic topic model (dDTM) ? both as
types of scientific topics at different time points, and
as changing word probability distributions within
these topics. The ?Photography? topic for example
has changed dramatically since the beginning of the
20th century, with words related to digital photog-
raphy appearing recently, and dominating the most
current version of the topic.
Wang and McCallum (2006), Wang et al (2008)
develop time-specific topic models, where topics,
as patterns of word use, are tracked across a time
changing text collection, and address the task of
(fine-grained) time stamp prediction.
Wijaya and Yeniterzi (2011) investigate through
topic models the change in context of a specific en-
tity over time, based on the Google Ngram corpus.
They determine that changes in this context reflect
events occurring in the same period of time.
3 Word Epoch Disambiguation
We formulate the task as a disambiguation prob-
lem, where we automatically classify the period of
time when a word was used, based on its surround-
ing context. We use a data-driven formulation, and
draw examples from word occurrences over three
different epochs. For the purpose of this work, we
consider an epoch to be a period of 50 years sur-
rounding the beginning of a new century (1800+/-
25 years, 1900+/-25, 2000+/-25). The word usage
examples are gathered from books, where the publi-
cation year of a book is judged to be representative
for the time when that word was used. We select
words with different characteristics to allow us to in-
vestigate whether there is an effect caused by sense
change, or the disambiguation performance comes
from the change of topics and vocabulary over time.
4 Experimental Setting
Target Words. The choice of target words for our
experiments is driven by the phenomena we aim to
analyze. Because we want to investigate the behav-
ior of words in different epochs, and verify whether
the difference in word behavior comes from changes
in sense or changes in wording in the context, we
choose a mixture of polysemous words and monose-
mous words (according to WordNet and manually
checked against Webster?s dictionary editions from
1828, 1913 and the current Merriam-Webster edi-
tion), and also words that are frequent in all epochs,
as well as words that are frequent in only one epoch.
According to these criteria, for each open class
(nouns, verbs, adjectives, adverbs) we select 50
words, 25 of which have multiple senses, 25 with
one sense only. Each of these two sets has a 10-
5-5-5 distribution: 10 words that are frequent in all
three epochs, and 5 per each epoch such that these
words are only frequent in one epoch. To avoid part-
of-speech ambiguity we also choose words that are
unambiguous from this point of view. This selection
process was done based on Google 1gram historical
data, used for computing the probability distribution
of open-class words for each epoch. 2
The set of target words consists thus of 200
open class words, uniformly distributed over the 4
parts of speech, uniformly distributed over multiple-
sense/unique sense words, and with the frequency
based sample as described above. From this initial
set of words, we could not identify enough examples
in the three epochs considered for 35,3 which left us
with a final set of 165 words.
Data. For each target word in our dataset, we collect
the top 100 snippets returned by a search on Google
Books for each of the three epochs we consider.
2For each open class word we create ranked lists of words,
where the ranking score is an adjusted tfidf score ? the epochs
correspond to documents. To choose words frequent only in one
epoch, we choose the top words in the list, for words frequent
in all epochs we choose the bottom words in this list.
3A minimum of 30 total examples was required for a word
to be considered in the dataset.
260
All the extracted snippets are then processed: the
text is tokenized and part-of-speech tagged using the
Stanford tagger (Toutanova et al, 2003), and con-
texts that do not include the target word with the
specified part-of-speech are removed. The position
of the target word is also identified and recorded as
an offset alng with the example.
For illustration, we show below an example drawn
from each epoch for two different words,
dinner:
1800: On reaching Mr. Crane?s house, dinner
was set before us ; but as is usual here in many
places on the Sabbath, it was both dinner and
tea combined into a single meal.
1900: The average dinner of today consists
of relishes; of soup, either a consomme (clear
soup) or a thick soup.
2000: Preparing dinner in a slow cooker is
easy and convenient because the meal you?re
making requires little to no attention while it
cooks.
and surgeon:
1800: The apothecaries must instantly dis-
pense what medicines the surgeons require
for the use of the regiments.
1900: The surgeon operates, collects a fee,
and sends to the physician one-third or one-
half of the fee, this last transaction being un-
known to the patient.
2000: From a New York plastic surgeon
comes all anyone ever wanted to know?and
never imagined?about what goes on behind
the scenes at the office of one of the world?s
most prestigious plastic surgeons.
Disambiguation Algorithm. The classification al-
gorithm we use is inspired by previous work on data-
driven word sense disambiguation. Specifically, we
use a system that integrates both local and topical
features. The local features include: the current
word and its part-of-speech; a local context of three
words to the left and right of the ambiguous word;
the parts-of-speech of the surrounding words; the
first noun before and after the target word; the first
verb before and after the target word. The topical
features are determined from the global context and
are implemented through class-specific keywords,
which are determined as a list of at most five words
occurring at least three times in the contexts defin-
ing a certain word class (or epoch). This feature set
is similar to the one used by (Ng and Lee, 1996).
No. Avg. no.
POS words examples Baseline WED
Noun 46 190 42.54% 66.17%
Verb 49 198 42.25% 59.71%
Adjective 26 136 48.60% 60.13%
Adverb 44 213 40.86% 59.61%
AVERAGE 165 190 42.96% 61.55%
Table 1: Overall results for different parts-of-speech.
The features are then integrated in a Naive Bayes
classifier (Lee and Ng, 2002).
Evaluation. To evaluate word epoch disambigua-
tion, we calculate the average accuracy obtained
through ten-fold cross-validations applied on the
data collected for each word. To place results in per-
spective, we also calculate a simple baseline, which
assigns the most frequent class by default.
5 Results and Discussion
Table 1 summarizes the results obtained for the 165
words. Overall, the task appears to be feasible,
as absolute improvements of 18.5% are observed.
While improvements are obtained for all parts-of-
speech, the nouns lead to the highest disambiguation
results, with the largest improvement over the base-
line, which interestingly aligns with previous obser-
vations from work on word sense disambiguation
(Mihalcea and Edmonds, 2004; Agirre et al, 2007).
Among the words considered, there are words that
experience very large improvements over the base-
line, such as ?computer? (with an absolute increase
over the baseline of 42%) or ?install? (41%), which
are words that are predominantly used in one of the
epochs considered (2000), and are also known to
have changed meaning over time. There are also
words that experience very small improvements,
such as ?again? (3%) or ?captivate? (7%), which are
words that are frequently used in all three epochs.
There are even a few words (seven) for which the
disambiguation accuracy is below the baseline, such
as ?oblige? (-1%) or ?cruel? (-15%).
To understand to what extent the change in fre-
quency over time has an impact on word epoch dis-
ambiguation, in Table 2 we report results for words
that have high frequency in all three epochs consid-
ered, or in only one epoch at a time. As expected,
the words that are used more often in an epoch
are also easier to disambiguate.4 For instance, the
4The difference in results does not come from difference in
261
verb ?reassert? has higher frequency in 2000, and it
has a disambiguation accuracy of 67.25% compared
to a baseline of 34.15%. Instead, the verb ?con-
ceal,? which appears with high frequency in all three
epochs, has a disambiguation accuracy of 44.70%,
which is a relatively small improvement over the
baseline of 38.04%.
No. Avg. no.
POS words examples Baseline WED
High frequency in all epochs
Noun 18 180 42.31% 65.77%
Verb 19 203 43.45% 56.43%
Adjective 7 108 46.27% 57.75%
Adverb 17 214 40.32% 56.41%
AVERAGE 61 188 42.56% 59.33%
High frequency in one epoch
Noun 28 196 42.68% 66.42%
Verb 30 194 41.50% 61.80%
Adjective 19 146 49.47% 61.02%
Adverb 27 213 41.20% 61.63%
AVERAGE 104 191 43.20% 62.86%
Table 2: Results for words that have high frequency in all
epochs, or in one epoch at a time
The second analysis that we perform is concerned
with the accuracy observed for polysemous words as
compared to monosemous words. Comparative re-
sults are reported in Table 3. Monosemous words do
not have sense changes over time, so being able to
classify them in different epochs relies exclusively
on variations in their context over time. Polysemous
words?s context change because of both changes in
topics/vocabulary over time, and changes in word
senses. The fact that we see a difference in ac-
curacy between disambiguation results for monose-
mous and polysemous words is an indication that
word sense change is reflected and can be captured
in the context.
To better visualize the improvements obtained
with word epoch disambiguation with respect to the
baseline, Figure 1 plots the results.
6 Conclusions
In this paper, we introduced the novel task of word
epoch disambiguation, which aims to quantify the
changes in word usage over time. Using examples
collected from three major periods of time, for 165
words, we showed that the word epoch disambigua-
tion algorithm can lead to an overall absolute im-
size in the data, as the number of examples extracted for words
of high or low frequency is approximately the same.
allEpochs
1Epoch
polysemous
monosemous
 10
 12
 14
 16
 18
 20
 22
 24
Noun Verb Adj Adv Avg.
W
ED
?B
as
el
in
e
POS
By epoch frequency
 10
 12
 14
 16
 18
 20
 22
 24
 26
Noun Verb Adj Adv Avg.
W
ED
?B
as
el
in
e
POS
By number of senses
Figure 1: Word epoch disambiguation compared to the
baseline, for words that are frequent/not frequent (in a
given epoch), and monosemous/polysemous.
No. Avg. no.
POS words examples Baseline WED
Polysemous words
Noun 24 191 41.89% 66.55%
Verb 25 214 42.71% 58.84%
Adjective 12 136 45.40% 57.42%
Adverb 23 214 39.38% 60.03%
AVERAGE 84 196 41.94% 61.16%
Monosemous words
Noun 22 188 43.25% 65.77%
Verb 24 181 41.78% 60.63%
Adjective 14 136 51.36% 62.47%
Adverb 21 213 42.49% 59.15%
AVERAGE 81 183 44.02% 61.96%
Table 3: Results for words that are polysemous or
monosemous.
provement of 18.5%, as compared to a baseline that
picks the most frequent class by default. These re-
sults indicate that there are significant differences
between occurrences of words in different periods
of time. Moreover, additional analyses suggest that
changes in usage frequency and word senses con-
tribute to these differences. In future work, we plan
to do an in-depth analysis of the features that best
characterize the changes in word usage over time,
and develop representations that allow us to track
sense changes.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Science
Foundation.
262
References
E. Agirre, L. Marquez, and R. Wicentowski, editors.
2007. Proceedings of the 4th International Workshop
on Semantic Evaluations, Prague, Czech Republic.
D. Blei and J. Lafferty. 2006. Dynamic topic models. In
Proceedings of the 23rd International Conference on
Machine Learning.
D. Blei and J. Lafferty. 2007. A correlated topic model of
Science. The Annals of Applied Science, 1(1):17?35.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of
knowledge sources and learning algorithms for word
sense disambiguation. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), Philadelphia, June.
J.-B. Michel, Y.K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak, and E. L. Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science, 331(6014):176?182, Jan-
uary.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3, Association for Computational
Linguistics Workshop, Barcelona, Spain.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
examplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics (ACL 1996), Santa Cruz.
F. Reali and T. Griffiths. 2010. Words as alleles: con-
necting language evolution with bayesian learners to
models of genetic drift. Proceedings of the Royal So-
ciety, 277(1680):429?436.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Hu-
man Language Technology Conference (HLT-NAACL
2003), Edmonton, Canada, May.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In Conference on Knowledge Discovery and Data
Mining (KDD).
C. Wang, D. Blei, and D. Heckerman. 2008. Continuous
time dynamic topic models. In International Confer-
ence on Machine Learning (ICML).
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In Proc. of the
Workshop on Detecting and Exploiting Cultural Diver-
sity on the Social Web (DETECT) 2011.
263
Tutorial Abstracts of ACL 2012, page 4,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Multilingual Subjectivity and Sentiment Analysis
Rada Mihalcea
University of North Texas
Denton, Tx
rada@cs.unt.edu
Carmen Banea
University of North Texas
Denton, Tx
carmenbanea@my.unt.edu
Janyce Wiebe
University of Pittsburgh
Pittsburgh, Pa
wiebe@cs.pitt.edu
Abstract
Subjectivity and sentiment analysis focuses on
the automatic identification of private states,
such as opinions, emotions, sentiments, evalu-
ations, beliefs, and speculations in natural lan-
guage. While subjectivity classification labels
text as either subjective or objective, sentiment
classification adds an additional level of gran-
ularity, by further classifying subjective text as
either positive, negative or neutral.
While much of the research work in this
area has been applied to English, research
on other languages is growing, including
Japanese, Chinese, German, Spanish, Ro-
manian. While most of the researchers in
the field are familiar with the methods ap-
plied on English, few of them have closely
looked at the original research carried out in
other languages. For example, in languages
such as Chinese, researchers have been look-
ing at the ability of characters to carry sen-
timent information (Ku et al, 2005; Xiang,
2011). In Romanian, due to markers of po-
liteness and additional verbal modes embed-
ded in the language, experiments have hinted
that subjectivity detection may be easier to
achieve (Banea et al, 2008). These addi-
tional sources of information may not be avail-
able across all languages, yet, various arti-
cles have pointed out that by investigating a
synergistic approach for detecting subjectiv-
ity and sentiment in multiple languages at the
same time, improvements can be achieved not
only in other languages, but in English as
well. The development and interest in these
methods is also highly motivated by the fact
that only 27% of Internet users speak En-
glish (www.internetworldstats.com/stats.htm,
Oct 11, 2011), and that number diminishes
further every year, as more people across the
globe gain Internet access.
The aim of this tutorial is to familiarize the
attendees with the subjectivity and sentiment
research carried out on languages other than
English in order to enable and promote cross-
fertilization. Specifically, we will review work
along three main directions. First, we will
present methods where the resources and tools
have been specifically developed for a given
target language. In this category, we will
also briefly overview the main methods that
have been proposed for English, but which can
be easily ported to other languages. Second,
we will describe cross-lingual approaches, in-
cluding several methods that have been pro-
posed to leverage on the resources and tools
available in English by using cross-lingual
projections. Finally, third, we will show how
the expression of opinions and polarity per-
vades language boundaries, and thus methods
that holistically explore multiple languages at
the same time can be effectively considered.
References
C. Banea, R. Mihalcea, and J. Wiebe. 2008. A Boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC 2008, Marrakech, Morocco.
L. W. Ku, T. H. Wu, L. Y. Lee, and H. H. Chen. 2005.
Construction of an Evaluation Corpus for Opinion Ex-
traction. In Proceedings of NTCIR-5, Tokyo, Japan.
L. Xiang. 2011. Ideogram Based Chinese Sentiment
Word Orientation Computation. Computing Research
Repository, page 4, October.
4
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973?982,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Utterance-Level Multimodal Sentiment Analysis
Vero?nica Pe?rez-Rosas and Rada Mihalcea
Computer Science and Engineering
University of North Texas
veronicaperezrosas@my.unt.edu, rada@cs.unt.edu
Louis-Philippe Morency
Institute for Creative Technologies
University of Southern California
morency@ict.usc.edu
Abstract
During real-life interactions, people are
naturally gesturing and modulating their
voice to emphasize specific points or to
express their emotions. With the recent
growth of social websites such as YouTube,
Facebook, and Amazon, video reviews are
emerging as a new source of multimodal
and natural opinions that has been left al-
most untapped by automatic opinion anal-
ysis techniques. This paper presents a
method for multimodal sentiment classi-
fication, which can identify the sentiment
expressed in utterance-level visual datas-
treams. Using a new multimodal dataset
consisting of sentiment annotated utter-
ances extracted from video reviews, we
show that multimodal sentiment analysis
can be effectively performed, and that the
joint use of visual, acoustic, and linguistic
modalities can lead to error rate reductions
of up to 10.5% as compared to the best
performing individual modality.
1 Introduction
Video reviews represent a growing source of con-
sumer information that gained increasing interest
from companies, researchers, and consumers. Pop-
ular web platforms such as YouTube, Amazon,
Facebook, and ExpoTV have reported a signifi-
cant increase in the number of consumer reviews
in video format over the past five years. Compared
to traditional text reviews, video reviews provide a
more natural experience as they allow the viewer to
better sense the reviewer?s emotions, beliefs, and
intentions through richer channels such as intona-
tions, facial expressions, and body language.
Much of the work to date on opinion analysis has
focused on textual data, and a number of resources
have been created including lexicons (Wiebe and
Riloff, 2005; Esuli and Sebastiani, 2006) or large
annotated datasets (Maas et al, 2011). Given the
accelerated growth of other media on the Web and
elsewhere, which includes massive collections of
videos (e.g., YouTube, Vimeo, VideoLectures), im-
ages (e.g., Flickr, Picasa), audio clips (e.g., pod-
casts), the ability to address the identification of
opinions in the presence of diverse modalities is be-
coming increasingly important. This has motivated
researchers to start exploring multimodal clues for
the detection of sentiment and emotions in video
content (Morency et al, 2011; Wagner et al, 2011).
In this paper, we explore the addition of speech
and visual modalities to text analysis in order to
identify the sentiment expressed in video reviews.
Given the non homogeneous nature of full-video
reviews, which typically include a mixture of posi-
tive, negative, and neutral statements, we decided
to perform our experiments and analyses at the ut-
terance level. This is in line with earlier work on
text-based sentiment analysis, where it has been
observed that full-document reviews often contain
both positive and negative comments, which led to
a number of methods addressing opinion analysis
at sentence level. Our results show that relying
on the joint use of linguistic, acoustic, and visual
modalities allows us to better sense the sentiment
being expressed as compared to the use of only one
modality at a time.
Another important aspect of this paper is the in-
troduction of a new multimodal opinion database
annotated at the utterance level which is, to our
knowledge, the first of its kind. In our work, this
dataset enabled a wide range of multimodal senti-
ment analysis experiments, addressing the relative
importance of modalities and individual features.
The following section presents related work
in text-based sentiment analysis and audio-visual
emotion recognition. Section 3 describes our new
multimodal datasets with utterance-level sentiment
annotations. Section 4 presents our multimodal sen-
973
timent analysis approach, including details about
our linguistic, acoustic, and visual features. Our
experiments and results on multimodal sentiment
classification are presented in Section 5, with a
detailed discussion and analysis in Section 6.
2 Related Work
In this section we provide a brief overview of re-
lated work in text-based sentiment analysis, as well
as audio-visual emotion analysis.
2.1 Text-based Subjectivity and Sentiment
Analysis
The techniques developed so far for subjectivity
and sentiment analysis have focused primarily on
the processing of text, and consist of either rule-
based classifiers that make use of opinion lexicons,
or data-driven methods that assume the availability
of a large dataset annotated for polarity. These tools
and resources have been already used in a large
number of applications, including expressive text-
to-speech synthesis (Alm et al, 2005), tracking
sentiment timelines in on-line forums and news
(Balog et al, 2006), analysis of political debates
(Carvalho et al, 2011), question answering (Oh et
al., 2012), conversation summarization (Carenini et
al., 2008), and citation sentiment detection (Athar
and Teufel, 2012).
One of the first lexicons used in sentiment anal-
ysis is the General Inquirer (Stone, 1968). Since
then, many methods have been developed to auto-
matically identify opinion words and their polarity
(Hatzivassiloglou and McKeown, 1997; Turney,
2002; Hu and Liu, 2004; Taboada et al, 2011), as
well as n-gram and more linguistically complex
phrases (Yang and Cardie, 2012).
For data-driven methods, one of the most widely
used datasets is the MPQA corpus (Wiebe et al,
2005), which is a collection of news articles manu-
ally annotated for opinions. Other datasets are also
available, including two polarity datasets consist-
ing of movie reviews (Pang and Lee, 2004; Maas et
al., 2011), and a collection of newspaper headlines
annotated for polarity (Strapparava and Mihalcea,
2007).
While difficult problems such as cross-domain
(Blitzer et al, 2007; Li et al, 2012) or cross-
language (Mihalcea et al, 2007; Wan, 2009; Meng
et al, 2012) portability have been addressed, not
much has been done in terms of extending the ap-
plicability of sentiment analysis to other modalities,
such as speech or facial expressions.
The only exceptions that we are aware of are the
findings reported in (Somasundaran et al, 2006;
Raaijmakers et al, 2008; Mairesse et al, 2012;
Metze et al, 2009), where speech and text have
been analyzed jointly for the purpose of subjectiv-
ity or sentiment identification, without, however,
addressing other modalities such as visual cues;
and the work reported in (Morency et al, 2011;
Perez-Rosas et al, 2013), where multimodal cues
have been used for the analysis of sentiment in
product reviews, but where the analysis was done
at the much coarser level of full videos rather than
individual utterances as we do in our work.
2.2 Audio-Visual Emotion Analysis.
Also related to our work is the research done on
emotion analysis. Emotion analysis of speech sig-
nals aims to identify the emotional or physical
states of a person by analyzing his or her voice
(Ververidis and Kotropoulos, 2006). Proposed
methods for emotion recognition from speech fo-
cus both on what is being said and how is be-
ing said, and rely mainly on the analysis of the
speech signal by sampling the content at utterance
or frame level (Bitouk et al, 2010). Several re-
searchers used prosody (e.g., pitch, speaking rate,
Mel frequency coefficients) for speech-based emo-
tion recognition (Polzin and Waibel, 1996; Tato et
al., 2002; Ayadi et al, 2011).
There are also studies that analyzed the visual
cues, such as facial expressions and body move-
ments (Calder et al, 2001; Rosenblum et al, 1996;
Essa and Pentland, 1997). Facial expressions are
among the most powerful and natural means for
human beings to communicate their emotions and
intentions (Tian et al, 2001). Emotions can be
also expressed unconsciously, through subtle move-
ments of facial muscles such as smiling or eyebrow
raising, often measured and described using the
Facial Action Coding System (FACS) (Ekman et
al., 2002).
De Silva et. al. (De Silva et al, 1997) and Chen
et. al. (Chen et al, 1998) presented one of the
early works that integrate both acoustic and visual
information for emotion recognition. In addition to
work that considered individual modalities, there
is also a growing body of work concerned with
multimodal emotion analysis (Silva et al, 1997;
Sebe et al, 2006; Zhihong et al, 2009; Wollmer et
al., 2010).
974
Utterance transcription Label
En este color, creo que era el color frambuesa. neu
In this color, I think it was raspberry
Pinta hermosisimo. pos
It looks beautiful.
Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes. pos
Honestly, talking about how they looks and hydrates, yes they are very hydrant.
Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca. neg
But the problem with those lipsticks is that when you apply them, they leave a very nasty taste
Sinceramente, es no es que sea el olor sino que es mas bien el gusto. neg
Honestly, is not the smell, it is the taste.
Table 1: Sample utterance-level annotations. The labels used are: pos(itive), neg(ative), neu(tral).
More recently, two challenges have been or-
ganized focusing on the recognition of emotions
using audio and visual cues (Schuller et al,
2011a; Schuller et al, 2011b), which included sub-
challenges on audio-only, video-only, and audio-
video, and drew the participation of many teams
from around the world. Note however that most of
the previous work on audio-visual emotion analy-
sis has focused exclusively on the audio and video
modalities, and did not consider textual features, as
we do in our work.
3 MOUD: Multimodal Opinion
Utterances Dataset
For our experiments, we created a dataset of ut-
terances (named MOUD) containing product opin-
ions expressed in Spanish.1 We chose to work with
Spanish because it is a widely used language, and
it is the native language of the main author of this
paper.
We started by collecting a set of videos from
the social media web site YouTube, using several
keywords likely to lead to a product review or rec-
ommendation. Starting with the YouTube search
page, videos were found using the following key-
words: mis products favoritos (my favorite prod-
ucts), products que no recomiendo (non recom-
mended products), mis perfumes favoritos (my fa-
vorite perfumes), peliculas recomendadas (recom-
mended movies), peliculas que no recomiendo (non
recommended movies) and libros recomendados
(recommended books), libros que no recomiendo
(non recommended books). Notice that the key-
words are not targeted at a specific product type;
rather, we used a variety of product names, so that
the dataset has some degree of generality within
the broad domain of product reviews.
1Publicly available from the authors webpage.
Among all the videos returned by the YouTube
search, we selected only videos that respected the
following guidelines: the speaker should be in front
of the camera; her face should be clearly visible,
with a minimum amount of face occlusion during
the recording; there should not be any background
music or animation. The final video set includes 80
videos randomly selected from the videos retrieved
from YouTube that also met the guidelines above.
The dataset includes 15 male and 65 female speak-
ers, with their age approximately ranging from 20
to 60 years.
All the videos were first pre-processed to elimi-
nate introductory titles and advertisements. Since
the reviewers often switched topics when express-
ing their opinions, we manually selected a 30 sec-
onds opinion segment from each video to avoid
having multiple topics in a single review.
3.1 Segmentation and Transcription
All the video clips were manually processed to
transcribe the verbal statements and also to extract
the start and end time of each utterance. Since the
reviewers utter expressive sentences that are nat-
urally segmented by speech pauses, we decided
to use these pauses (>0.5seconds) to identify the
beginning and the end of each utterance. The tran-
scription and segmentation were performed using
the Transcriber software.
Each video was segmented into an average of
six utterances, resulting in a final dataset of 498
utterances. Each utterance is linked to the corre-
sponding audio and video stream, as well as its
manual transcription. The utterances have an aver-
age duration of 5 seconds, with a standard deviation
of 1.2 seconds.
975
Figure 1: Multimodal feature extraction
3.2 Sentiment Annotation
To enable the use of this dataset for sentiment de-
tection, we performed sentiment annotations at ut-
terance level. Annotations were done using Elan,2
which is a widely used tool for the annotation of
video and audio resources. Two annotators indepen-
dently labeled each utterance as positive, negative,
or neutral. The annotation was done after seeing
the video corresponding to an utterance (along with
the corresponding audio source). The transcription
of the utterance was also made available. Thus, the
annotation process included all three modalities: vi-
sual, acoustic, and linguistic. The annotators were
allowed to watch the video segment and their cor-
responding transcription as many times as needed.
The inter-annotator agreement was measured at
88%, with a Kappa of 0.81, which represents good
agreement. All the disagreements were reconciled
through discussions.
Table 1 shows the five utterances obtained from a
video in our dataset, along with their corresponding
2http://tla.mpi.nl/tools/tla-tools/elan/
sentiment annotations. As this example illustrates,
a video can contain a mix of positive, negative, and
neutral utterances. Note also that sentiment is not
always explicit in the text: for example, the last
utterance ?Honestly, it is not the smell, it is the
taste? has an implicit reference to the ?nasty taste?
expressed in the previous utterance, and thus it was
also labeled as negative by both annotators.
4 Multimodal Sentiment Analysis
The main advantage that comes with the analysis of
video opinions, as compared to their textual coun-
terparts, is the availability of visual and speech cues.
In textual opinions, the only source of information
consists of words and their dependencies, which
may sometime prove insufficient to convey the ex-
act sentiment of the user. Instead, video opinions
naturally contain multiple modalities, consisting of
visual, acoustic, and linguistic datastreams. We hy-
pothesize that the simultaneous use of these three
modalities will help create a better opinion analysis
model.
976
4.1 Feature Extraction
This section describes the process of automatically
extracting linguistic, acoustic and visual features
from the video reviews. First, we obtain the stream
corresponding to each modality, followed by the
extraction of a representative set of features for
each modality, as described in the following sub-
sections. These features are then used as cues to
build a classifier of positive or negative sentiment.
Figure 1 illustrates this process.
4.1.1 Linguistic Features
We use a bag-of-words representation of the video
transcriptions of each utterance to derive unigram
counts, which are then used as linguistic features.
First, we build a vocabulary consisting of all the
words, including stopwords, occurring in the tran-
scriptions of the training set. We then remove
those words that have a frequency below 10 (value
determined empirically on a small development
set). The remaining words represent the unigram
features, which are then associated with a value
corresponding to the frequency of the unigram in-
side each utterance transcription. These simple
weighted unigram features have been successfully
used in the past to build sentiment classifiers on
text, and in conjunction with Support Vector Ma-
chines (SVM) have been shown to lead to state-of-
the-art performance (Maas et al, 2011).
4.1.2 Acoustic Features
Acoustic features are automatically extracted from
the speech signal of each utterance. We used the
open source software OpenEAR (Schuller, 2009)
to automatically compute a set of acoustic features.
We include prosody, energy, voicing probabilities,
spectrum, and cepstral features.
? Prosody features. These include intensity,
loudness, and pitch that describe the speech
signal in terms of amplitude and frequency.
? Energy features. These features describe the
human loudness perception.
? Voice probabilities. These are probabilities
that represent an estimate of the percentage of
voiced and unvoiced energy in the speech.
? Spectral features. The spectral features are
based on the characteristics of the human ear,
which uses a nonlinear frequency unit to simu-
late the human auditory system. These fea-
tures describe the speech formants, which
model spoken content and represent speaker
characteristics.
? Cepstral features. These features emphasize
changes or periodicity in the spectrum fea-
tures measured by frequencies; we model
them using 12 Mel-frequency cepstral coeffi-
cients that are calculated based on the Fourier
transform of a speech frame.
Overall, we have a set of 28 acoustic features.
During the feature extraction, we use a frame sam-
pling of 25ms. Speaker normalization is performed
using z-standardization. The voice intensity is
thresholded to identify samples with and without
speech, with the same threshold being used for all
the experiments and all the speakers. The features
are averaged over all the frames in an utterance, to
obtain one feature vector for each utterance.
4.1.3 Facial Features
Facial expressions can provide important clues for
affect recognition, which we use to complement
the linguistic and acoustic features extracted from
the speech stream.
The most widely used system for measuring and
describing facial behaviors is the Facial Action
Coding System (FACS), which allows for the de-
scription of face muscle activities through the use
of a set of Action Units (AUs). According with
(Ekman, 1993), there are 64 AUs that involve the
upper and lower face, including several face posi-
tions and movements.3 AUs can occur either by
themselves or in combination, and can be used to
identify a variety of emotions. While AUs are fre-
quently annotated by certified human annotators,
automatic tools are also available. In our work, we
use the Computer Expression Recognition Toolbox
(CERT) (Littlewort et al, 2011), which allows us to
automatically extract the following visual features:
? Smile and head pose estimates. The smile
feature is an estimate for smiles. Head pose
detection consists of three-dimensional esti-
mates of the head orientation, i.e., yaw, pitch,
and roll. These features provide information
about changes in smiles and face positions
while uttering positive and negative opinions.
? Facial AUs. These features are the raw es-
timates for 30 facial AUs related to muscle
movements for the eyes, eyebrows, nose, lips,
3http://www.cs.cmu.edu/afs/cs/project/face/www/facs.htm
977
and chin. They provide detailed information
about facial behaviors from which we expect
to find differences between positive and nega-
tive states.
? Eight basic emotions. These are estimates
for the following emotions: anger, contempt,
disgust, fear, joy, sad, surprise, and neutral.
These features describe the presence of two or
more AUs that define a specific emotion. For
example, the unit A12 describes the pulling
of lip corners movement, which usually sug-
gests a smile but when associated with a
check raiser movement (unit A6), represents
a marker for the emotion of happiness.
We extract a total of 40 visual features, each
of them obtained at frame level. Since only one
person is present in each video clip, most of the
time facing the camera, the facial tracking was
successfully applied for most of our data. For the
analysis, we use a sampling rate of 30 frames per
second. The features extracted for each utterance
are averaged over all the valid frames, which are
automatically identified using the output of CERT.4
Segments with more than 60% of invalid frames
are simply discarded.
5 Experiments and Results
We run our sentiment classification experiments
on the MOUD dataset introduced earlier. From
the dataset, we remove utterances labeled as neu-
tral, thus keeping only the positive and negative
utterances with valid visual features. The removal
of neutral utterances is done for two main reasons.
First, the number of neutral utterances in the dataset
is rather small. Second, previous work in subjec-
tivity and sentiment analysis has demonstrated that
a layered approach (where neutral statements are
first separated from opinion statements followed
by a separation between positive and negative state-
ments) works better than a single three-way classifi-
cation. After this process, we are left with an exper-
imental dataset of 412 utterances, 182 of which are
labeled as positive, and 231 are labeled as negative.
From each utterance, we extract the linguis-
tic, acoustic, and visual features described above,
which are then combined using the early fusion
(or feature-level fusion) approach (Hall and Llinas,
4There is a small number of frames that CERT could not
process, mostly due to the brief occlusions that occur when
the speaker is showing the product she is reviewing.
Modality Accuracy
Baseline 55.93%
One modality at a time
Linguistic 70.94%
Acoustic 64.85%
Visual 67.31%
Two modalities at a time
Linguistic + Acoustic 72.88%
Linguistic + Visual 72.39%
Acoustic + Visual 68.86%
Three modalities at a time
Linguistic+Acoustic+Visual 74.09%
Table 2: Utterance-level sentiment classification
with linguistic, acoustic, and visual features.
1997; Atrey et al, 2010). In this approach, the fea-
tures collected from all the multimodal streams are
combined into a single feature vector, thus result-
ing in one vector for each utterance in the dataset
which is used to make a decision about the senti-
ment orientation of the utterance.
We run several comparative experiments, using
one, two, and three modalities at a time. We use
the entire set of 412 utterances and run ten fold
cross validations using an SVM classifier, as imple-
mented in the Weka toolkit.5 In line with previous
work on emotion recognition in speech (Haq and
Jackson, 2009; Anagnostopoulos and Vovoli, 2010)
where utterances are selected in a speaker depen-
dent manner (i.e., utterances from the same speaker
are included in both training and test), as well as
work on sentence-level opinion classification where
document boundaries are not considered in the split
performed between the training and test sets (Wil-
son et al, 2004; Wiegand and Klakow, 2009), the
training/test split for each fold is performed at ut-
terance level regardless of the video they belong
to.
Table 2 shows the results of the utterance-level
sentiment classification experiments. The baseline
is obtained using the ZeroR classifier, which as-
signs the most frequent label by default, averaged
over the ten folds.
6 Discussion
The experimental results show that sentiment clas-
sification can be effectively performed on multi-
modal datastreams. Moreover, the integration of
5http://www.cs.waikato.ac.nz/ml/weka/
978
Figure 2: Visual and acoustic feature weights. This
graph shows the relative importance of the infor-
mation gain weights associated with the top most
informative acoustic-visual features.
visual, acoustic, and linguistic features can improve
significantly over the use of one modality at a time,
with incremental improvements observed for each
added modality.
Among the individual classifiers, the linguistic
classifier appears to be the most accurate, followed
by the classifier that relies on visual clues, and by
the audio classifier. Compared to the best indi-
vidual classifier, the relative error rate reduction
obtained with the tri-modal classifier is 10.5%.
The results obtained with this multimodal utter-
ance classifier are found to be significantly better
than the best individual results (obtained with the
text modality), with significance being tested with
a t-test (p=0.05).
Feature analysis.
To determine the role played by each of the vi-
sual and acoustic features, we compare the fea-
ture weights assigned by the learning algorithm,
as shown in Figure 2. Interestingly, a distressed
brow is the strongest indicator of sentiment, fol-
lowed, this time not surprisingly, by the smile fea-
ture. Other informative features for sentiment clas-
sification are the voice probability, representing the
energy in speech, the combined visual features that
represent an angry face, and two of the cepstral
coefficients.
To reach a better understanding of the relation
between features, we also calculate the Pearson
correlation between the visual and acoustic fea-
tures. Table 3 shows a subset of these correlation
figures. As we expected, correlations between fea-
tures of the same type are higher. For example,
the correlation between features AU6 and AU12
or the correlation between intensity and loudness
is higher than the correlation between AU6 and in-
tensity. Nonetheless, we still find some significant
correlations between features of different types, for
instance AU12 and AU45 which are both signifi-
cantly correlated with the intensity and loudness
features. This give us confidence about using them
for further analysis.
Video-level sentiment analysis.
To understand the role played by the size of the
video-segments considered in the sentiment classi-
fication experiments, as well as the potential effect
of a speaker-independence assumption, we also run
a set of experiments where we use full videos for
the classification.
In these experiments, once again the sentiment
annotation is done by two independent annotators,
using the same protocol as in the utterance-based
annotations. Videos that were ambivalent about
the general sentiment were either labeled as neu-
tral (and thus removed from the experiments), or
labeled with the dominant sentiment. The inter-
annotator agreement for this annotation was mea-
sured at 96.1%. As before, the linguistic, acoustic,
and visual features are averaged over the entire
video, and we use an SVM classifier in ten-fold
cross validation experiments.
Table 4 shows the results obtained in these
video-level experiments. While the combination of
modalities still helps, the improvement is smaller
than the one obtained during the utterance-level
classification. Specifically, the combined effect of
acoustic and visual features improves significantly
over the individual modalities. However, the com-
bination of linguistic features with other modalities
does not lead to clear improvements. This may be
due to the smaller number of feature vectors used
in the experiments (only 80, as compared to the
412 used in the previous setup). Another possi-
ble reason is the fact that the acoustic and visual
modalities are significantly weaker than the lin-
guistic modality, most likely due to the fact that
the feature vectors are now speaker-independent,
which makes it harder to improve over the linguis-
tic modality alone.
7 Conclusions
In this paper, we presented a multimodal approach
for utterance-level sentiment classification. We
introduced a new multimodal dataset consisting
979
AU6 AU12 AU45 AUs 1,1+4 Pitch Voice probability Intensity Loudness
AU6 1.00 0.46* -0.03 -0.05 0.06 -0.14* -0.04 -0.02
AU12 1.00 -0.23* -0.33* 0.04 0.05 0.15* 0.16*
AU45 1.00 0.05 -0.05 -0.11* -.163* 0.16*
AUs 1,1+4 1.00 -0.11* -0.16* 0.06 0.07
Pitch 1.00 -0.04 -0.01 -0.08
Voice probability 1.00 0.19* 0.38*
Intensity 1.00 0.85*
Loudness 1.00
Table 3: Correlations between several visual and acoustic features. Visual features: AU6 Cheek raise,
AU12 Lip corner pull, AU45 Blink eye and closure, AU1,1+4 Distress brow. Acoustic features: Pitch,
Voice probability, Intensity, Energy. *Correlation is significant at the 0.05 level (1-tailed)
.
Modality Accuracy
Baseline 55.93%
One modality at a time
Linguistic 73.33%
Acoustic 53.33%
Visual 50.66%
Two modalities at a time
Linguistic + Acoustic 72.00%
Linguistic + Visual 74.66%
Acoustic + Visual 61.33%
Three modalities at a time
Linguistic+Acoustic+Visual 74.66%
Table 4: Video-level sentiment classification with
linguistic, acoustic, and visual features.
of sentiment annotated utterances extracted from
video reviews, where each utterance is associated
with a video, acoustic, and linguistic datastream.
Our experiments show that sentiment annotation
of utterance-level visual datastreams can be ef-
fectively performed, and that the use of multiple
modalities can lead to error rate reductions of up to
10.5% as compared to the use of one modality at a
time. In future work, we plan to explore alternative
multimodal fusion methods, such as decision-level
and meta-level fusion, to improve the integration
of the visual, acoustic, and linguistic modalities.
Acknowledgments
We would like to thank Alberto Castro for his help
with the sentiment annotations. This material is
based in part upon work supported by National Sci-
ence Foundation awards #0917170 and #1118018,
by DARPA-BAA-12-47 DEFT grant #12475008,
and by a grant from U.S. RDECOM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation, the Defense Advanced
Research Projects Agency, or the U.S. Army Re-
search, Development, and Engineering Command.
References
C. Alm, D. Roth, and R. Sproat. 2005. Emotions
from text: Machine learning for text-based emotion
prediction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, Canada.
C. Anagnostopoulos and E. Vovoli. 2010. Sound pro-
cessing features for speaker-dependent and phrase-
independent emotion recognition in berlin database.
In Information Systems Development, pages 413?
421. Springer.
A. Athar and S. Teufel. 2012. Context-enhanced cita-
tion sentiment detection. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Montre?al, Canada, June.
P. K. Atrey, M. A. Hossain, A. El Saddik, and
M. Kankanhalli. 2010. Multimodal fusion for mul-
timedia analysis: a survey. Multimedia Systems, 16.
M. El Ayadi, M. Kamel, and F. Karray. 2011. Survey
on speech emotion recognition: Features, classifica-
tion schemes, and databases. Pattern Recognition,
44(3):572 ? 587.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in
blog mood levels. In Proceedings of the 11th Meet-
ing of the European Chapter of the As sociation for
Computational Linguistics (EACL-2006).
Dmitri Bitouk, Ragini Verma, and Ani Nenkova. 2010.
Class-level spectral features for emotion recognition.
Speech Commun., 52(7-8):613?625, July.
980
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In As-
sociation for Computational Linguistics.
A. J. Calder, A. M. Burton, P. Miller, A. W. Young, and
S. Akamatsu. 2001. A principal component analysis
of facial expressions. Vision research, 41(9):1179?
1208, April.
G. Carenini, R. Ng, and X. Zhou. 2008. Summarizing
emails with conversational cohesion and subjectivity.
In Proceedings of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT 2008), Columbus, Ohio.
P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva.
2011. Liars and saviors in a sentiment annotated
corpus of comments to political debates. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL 2011), Portland, OR.
L. S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu.
1998. Multimodal human emotion/expression recog-
nition. In Proceedings of the 3rd. International Con-
ference on Face & Gesture Recognition, pages 366?,
Washington, DC, USA. IEEE Computer Society.
L C De Silva, T Miyasato, and R Nakatsu, 1997. Facial
emotion recognition using multi-modal information,
volume 1, page 397401. IEEE Signal Processing So-
ciety.
P. Ekman, W. Friesen, and J. Hager. 2002. Facial ac-
tion coding system.
P. Ekman. 1993. Facial expression of emotion. Ameri-
can Psychologist, 48:384?392.
I.A. Essa and A.P. Pentland. 1997. Coding, analy-
sis, interpretation, and recognition of facial expres-
sions. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 19(7):757 ?763, jul.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation (LREC 2006), Gen-
ova, IT.
D.L. Hall and J. Llinas. 1997. An introduction to mul-
tisensor fusion. IEEE Special Issue on Data Fusion,
85(1).
S. Haq and P. Jackson. 2009. Speaker-dependent
audio-visual emotion recognition. In International
Conference on Audio-Visual Speech Processing.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 174?181.
M. Hu and B. Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Seattle, Washing-
ton.
F. Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu. 2012.
Cross-domain co-extraction of sentiment and topic
lexicons. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
Jeju Island, Korea.
G. Littlewort, J. Whitehill, Tingfan Wu, I. Fasel,
M. Frank, J. Movellan, and M. Bartlett. 2011. The
computer expression recognition toolbox (cert). In
Automatic Face Gesture Recognition and Workshops
(FG 2011), 2011 IEEE International Conference on,
pages 298 ?305, march.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and
C. Potts. 2011. Learning word vectors for sentiment
analysis. In Proceedings of the Association for Com-
putational Linguistics (ACL 2011), Portland, OR.
F. Mairesse, J. Polifroni, and G. Di Fabbrizio. 2012.
Can prosody inform sentiment analysis? experi-
ments on short spoken reviews. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE Inter-
national Conference on, pages 5093 ?5096, march.
X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang.
2012. Cross-lingual mixture model for sentiment
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea.
F. Metze, T. Polzehl, and M. Wagner. 2009. Fusion
of acoustic and linguistic features for emotion detec-
tion. In Semantic Computing, 2009. ICSC ?09. IEEE
International Conference on, pages 153 ?160, sept.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In Proceedings of the Association for
Computational Linguistics, Prague, Czech Republic.
L.P. Morency, R. Mihalcea, and P. Doshi. 2011. To-
wards multimodal sentiment analysis: Harvesting
opinions from the web. In Proceedings of the In-
ternational Conference on Multimodal Computing,
Alicante, Spain.
J. Oh, K. Torisawa, C. Hashimoto, T. Kawada,
S. De Saeger, J. Kazama, and Y. Wang. 2012.
Why question answering using sentiment analysis
and word classes. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, Jeju Island, Korea.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, July.
981
V. Perez-Rosas, R. Mihalcea, and L.-P. Morency. 2013.
Multimodal sentiment analysis of spanish online
videos. IEEE Intelligent Systems.
T. Polzin and A. Waibel. 1996. Recognizing emotions
in speech. In In ICSLP.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
466?474, Honolulu, Hawaii.
M. Rosenblum, Y. Yacoob, and L.S. Davis. 1996. Hu-
man expression recognition from motion using a ra-
dial basis function network architecture. Neural Net-
works, IEEE Transactions on, 7(5):1121 ?1138, sep.
B. Schuller, M. Valstar, R. Cowie, and M. Pantic, edi-
tors. 2011a. Audio/Visual Emotion Challenge and
Workshop (AVEC 2011).
B. Schuller, M. Valstar, F. Eyben, R. Cowie, and
M. Pantic, editors. 2011b. Audio/Visual Emotion
Challenge and Workshop (AVEC 2011).
F. Eyben M. Wollmer B. Schuller. 2009. Openear in-
troducing the munich open-source emotion and af-
fect recognition toolkit. In ACII.
N. Sebe, I. Cohen, T. Gevers, and T.S. Huang. 2006.
Emotion recognition based on joint visual and audio
cues. In ICPR.
D. Silva, T. Miyasato, and R. Nakatsu. 1997. Facial
emotion recognition using multi-modal information.
In Proceedings of the International Conference on
Information and Communications Security.
S. Somasundaran, J. Wiebe, P. Hoffmann, and D. Lit-
man. 2006. Manual annotation of opinion cate-
gories in meetings. In Proceedings of the Work-
shop on Frontiers in Linguistically Annotated Cor-
pora 2006.
P. Stone. 1968. General Inquirer: Computer Approach
to Content Analysis. MIT Press.
C. Strapparava and R. Mihalcea. 2007. Semeval-2007
task 14: Affective text. In Proceedings of the 4th In-
ternational Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic.
M. Taboada, J. Brooke, M. Tofiloski, K. Voli, and
M. Stede. 2011. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, 37(3).
R. Tato, R. Santos, R. Kompe, and J. M. Pardo. 2002.
Emotional space improves emotion recognition. In
In Proc. ICSLP 2002, pages 2029?2032.
Y.-I. Tian, T. Kanade, and J.F. Cohn. 2001. Recogniz-
ing action units for facial expression analysis. Pat-
tern Analysis and Machine Intelligence, IEEE Trans-
actions on, 23(2):97 ?115, feb.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417?424, Philadelphia.
D. Ververidis and C. Kotropoulos. 2006. Emotional
speech recognition: Resources, features, and meth-
ods. Speech Communication, 48(9):1162?1181,
September.
J. Wagner, E. Andre, F. Lingenfelser, and Jonghwa
Kim. 2011. Exploring fusion methods for multi-
modal emotion recognition with missing data. Af-
fective Computing, IEEE Transactions on, 2(4):206
?218, oct.-dec.
X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the Association of Computational Linguistics
and the International Joint Conference on Natural
Language Processing, Singapore, August.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2005) (invited paper), Mexico
City, Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2-3):165?
210.
M. Wiegand and D. Klakow. 2009. The role of
knowledge-based features in polarity classification
at sentence level. In Proceedings of the Interna-
tional Conference of the Florida Artificial Intelli-
gence Research Society.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? finding strong and weak opinion clauses.
In Proceedings of the American Association for Arti-
ficial Intelligence.
M. Wollmer, B. Schuller, F. Eyben, and G. Rigoll.
2010. Combining long short-term memory and dy-
namic bayesian networks for incremental emotion-
sensitive artificial listening. IEEE Journal of Se-
lected Topics in Signal Processing, 4(5), October.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
Jeju Island, Korea.
Z. Zhihong, M. Pantic G.I. Roisman, and T.S. Huang.
2009. A survey of affect recognition methods: Au-
dio, visual, and spontaneous expressions. PAMI,
31(1).
982
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440?445,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cross-cultural Deception Detection
Ver
?
onica P
?
erez-Rosas
Computer Science and Engineering
University of North Texas
veronicaperezrosas@my.unt.edu
Rada Mihalcea
Computer Science and Engineering
University of Michigan
mihalcea@umich.edu
Abstract
In this paper, we address the task of
cross-cultural deception detection. Using
crowdsourcing, we collect three deception
datasets, two in English (one originating
from United States and one from India),
and one in Spanish obtained from speakers
from Mexico. We run comparative experi-
ments to evaluate the accuracies of decep-
tion classifiers built for each culture, and
also to analyze classification differences
within and across cultures. Our results
show that we can leverage cross-cultural
information, either through translation or
equivalent semantic categories, and build
deception classifiers with a performance
ranging between 60-70%.
1 Introduction
The identification of deceptive behavior is a task
that has gained increasing interest from researchers
in computational linguistics. This is mainly moti-
vated by the rapid growth of deception in written
sources, and in particular in Web content, including
product reviews, online dating profiles, and social
networks posts (Ott et al, 2011).
To date, most of the work presented on deception
detection has focused on the identification of deceit
clues within a specific language, where English is
the most commonly studied language. However, a
large portion of the written communication (e.g.,
e-mail, chats, forums, blogs, social networks) oc-
curs not only between speakers of English, but also
between speakers from other cultural backgrounds,
which poses important questions regarding the ap-
plicability of existing deception tools. Issues such
as language, beliefs, and moral values may influ-
ence the way people deceive, and therefore may
have implications on the construction of tools for
deception detection.
In this paper, we explore within- and across-
culture deception detection for three different cul-
tures, namely United States, India, and Mexico.
Through several experiments, we compare the per-
formance of classifiers that are built separately for
each culture, and classifiers that are applied across
cultures, by using unigrams and word categories
that can act as a cross-lingual bridge. Our results
show that we can achieve accuracies in the range of
60-70%, and that we can leverage resources avail-
able in one language to build deception tools for
another language.
2 Related Work
Research to date on automatic deceit detection has
explored a wide range of applications such as the
identification of spam in e-mail communication,
the detection of deceitful opinions in review web-
sites, and the identification of deceptive behavior
in computer-mediated communication including
chats, blogs, forums and online dating sites (Peng
et al, 2011; Toma et al, 2008; Ott et al, 2011;
Toma and Hancock, 2010; Zhou and Shi, 2008).
Techniques used for deception detection fre-
quently include word-based stylometric analysis.
Linguistic clues such as n-grams, count of used
words and sentences, word diversity, and self-
references are also commonly used to identify de-
ception markers. An important resource that has
been used to represent semantic information for the
deception task is the Linguistic Inquiry and Word
Count (LIWC) dictionary (Pennebaker and Francis,
1999). LIWC provides words grouped into seman-
tic categories relevant to psychological processes,
which have been used successfully to perform lin-
guistic profiling of true tellers and liars (Zhou et al,
2003; Newman et al, 2003; Rubin, 2010). In addi-
tion to this, features derived from syntactic Context
Free Grammar parse trees, and part of speech have
also been found to aid the deceit detection (Feng et
al., 2012; Xu and Zhao, 2012).
440
While most of the studies have focused on En-
glish, there is a growing interest in studying decep-
tion for other languages. For instance, (Fornaciari
and Poesio, 2013) identified deception in Italian by
analyzing court cases. The authors explored several
strategies for identifying deceptive clues, such as
utterance length, LIWC features, lemmas and part
of speech patterns. (Almela et al, 2012) studied the
deception detection in Spanish text by using SVM
classifiers and linguistic categories, obtained from
the Spanish version of the LIWC dictionary. A
study on Chinese deception is presented in (Zhang
et al, 2009), where the authors built a deceptive
dataset using Internet news and performed machine
learning experiments using a bag-of-words repre-
sentation to train a classifier able to discriminate
between deceptive and truthful cases.
It is also worth mentioning the work conducted
to analyze cross-cultural differences. (Lewis and
George, 2008) presented a study of deception in
social networks sites and face-to-face communi-
cation, where authors compare deceptive behavior
of Korean and American participants, with a sub-
sequent study also considering the differences be-
tween Spanish and American participants (Lewis
and George, 2009). In general, research findings
suggest a strong relation between deception and
cultural aspects, which are worth exploring with
automatic methods.
3 Datasets
We collect three datasets for three different cul-
tures: United States (English-US), India (English-
India), and Mexico (Spanish-Mexico). Following
(Mihalcea and Strapparava, 2009), we collect short
deceptive and truthful essays for three topics: opin-
ions on Abortion, opinions on Death Penalty, and
feelings about a Best Friend.
For English-US and English-India, we use Ama-
zon Mechanical Turk with a location restriction, so
that all the contributors are from the country of in-
terest (US and India). We collect 100 deceptive and
100 truthful statements for each of the three topics.
To avoid spam, each contribution is manually veri-
fied by one of the authors of this paper.For Spanish-
Mexico, while we initially attempted to collect data
also using Mechanical Turk, we were not able to
receive enough contributions. We therefore cre-
ated a separate web interface to collect data, and
recruited participants through contacts of the pa-
per?s authors. The overall process was significantly
more time consuming than for the other two cul-
tures, and resulted in fewer contributions, namely
39+39 statements for Abortion, 42+42 statements
for Death Penalty, and 94+94 statements for Best
Friend. For all three cultures, the participants first
provided their truthful responses, followed by the
deceptive ones.
Interestingly, for all three cultures, the average
number of words for the deceptive statements (62
words) is significantly smaller than for the truthful
statements (81 words), which may be explained by
the added difficulty of the deceptive process, and
is in line with previous observations about the cues
of deception (DePaulo et al, 2003).
4 Experiments
Through our experiments, we seek answers to the
following questions. First, what is the perfor-
mance for deception classifiers built for different
cultures? Second, can we use information drawn
from one culture to build a deception classifier for
another culture? Finally, what are the psycholin-
guistic classes most strongly associated with de-
ception/truth, and are there commonalities or dif-
ferences among languages?
In all our experiments, we formulate the decep-
tion detection task in a machine learning frame-
work, where we use an SVM classifier to discrimi-
nate between deceptive and truthful statements.
1
4.1 What is the performance for deception
classifiers built for different cultures?
We represent the deceptive and truthful statements
using two different sets of features. First we use
unigrams obtained from the statements correspond-
ing to each topic and each culture. To select the
unigrams, we use a threshold of 10, where all the
unigrams with a frequency less than 10 are dropped.
Since previous research suggested that stopwords
can contain linguistic clues for deception, no stop-
word removal is performed.
Experiments are performed using a ten-fold
cross validation evaluation on each dataset.Using
the same unigram features, we also perform cross-
topic classification, so that we can better under-
stand the topic dependence. For this, we train
the SVM classifier on training data consisting of a
merge of two topics (e.g., Abortion + Best Friend)
and test on the third topic (e.g., Death Penalty). The
results for both within- and cross-topic are shown
in the last two columns of Table 1.
1
We use the SVM classifier implemented in the Weka
toolkit, with its default settings.
441
LIWC Unigrams
Topic Linguistic Psychological Relativity Personal All Within-topic Cross-topic
English-US
Abortion 72.50% 68.75% 44.37% 67.50% 73.03% 63.75% 80.36%
Best Friend 75.98% 68.62% 58.33% 54.41% 73.03% 74.50% 60.78%
Death Penalty 60.36% 54.50% 49.54% 50.45% 58.10% 58.10% 77.23%
Average 69.61% 63.96% 50.75% 57.45% 69.05% 65.45% 72.79%
English-India
Abortion 56.00% 48.50% 46.50% 48.50% 56.00% 46.00% 50.00%
Best Friend 68.18% 68.62% 54.55% 53.18% 71.36% 60.45% 57.23%
Death Penalty 56.00% 52.84% 57.50% 53.50% 63.50% 57.50% 54.00%
Average 60.06% 59.19% 52.84% 51.72% 63.62% 54.65% 53.74%
Spanish-Mexico
Abortion 73.17% 67.07% 48.78% 51.22% 62.20% 52.46% 57.69%
Best Friend 72.04% 74.19% 67.20% 54.30% 75.27% 66.66% 50.53%
Death Penalty 73.17% 67.07% 48.78% 51.22% 62.20% 54.87% 63.41%
Average 72.79% 69.45% 54.92% 52.25% 67.89% 57.99% 57.21%
Table 1: Within-culture classification, using LIWC word classes and unigrams. For LIWC, results are
shown for within-topic experiments, with ten-fold cross validation. For unigrams, both within-topic
(ten-fold cross validation on the same topic) and cross-topic (training on two topics and testing on the
third topic) results are reported.
Second, we use the LIWC lexicon to extract fea-
tures corresponding to several word classes. LIWC
was developed as a resource for psycholinguistic
analysis (Pennebaker and Francis, 1999). The 2001
version of LIWC includes about 2,200 words and
word stems grouped into about 70 classes relevant
to psychological processes (e.g., emotion, cogni-
tion), which in turn are grouped into four broad cat-
egories
2
namely: linguistic processes, psychologi-
cal processes, relativity, and personal concerns. A
feature is generated for each of the 70 word classes
by counting the total frequency of the words belong-
ing to that class. We perform separate evaluations
using each of the four broad LIWC categories, as
well as using all the categories together. The re-
sults obtained with the SVM classifier are shown
in Table 1.
Overall, the results show that it is possible to
discriminate between deceptive and truthful cases
using machine learning classifiers, with a perfor-
mance superior to a random baseline which for all
datasets is 50% given an even class distribution.
Considering the unigram results, among the three
cultures considered, the deception discrimination
works best for the English-US dataset, and this is
also the dataset that benefits most from the larger
amount of training data brought by the cross-topic
experiments. In general, the cross-topic evaluations
suggest that there is no high topic dependence in
this task, and that using deception data from differ-
2
http://www.liwc.net/descriptiontable1.php
ent topics can lead to results that are comparable
to the within-topic data. Interestingly, among the
three topics considered, the Best Friend topic has
consistently the highest within-topic performance,
which may be explained by the more personal na-
ture of the topic, which can lead to clues that are
useful for the detection of deception (e.g., refer-
ences to the self or personal relationships).
Regarding the LIWC classifiers, the results show
that the use of the LIWC classes can lead to per-
formance that is generally better than the one ob-
tained with the unigram classifiers. The explicit cat-
egorization of words into psycholinguistic classes
seems to be particularly useful for the languages
where the words by themselves did not lead to very
good classification accuracies. Among the four
broad LIWC categories, the linguistic category ap-
pears to lead to the best performance as compared
to the other categories. It is notable that in Spanish,
the linguistic category by itself provides results that
are better than when all the LIWC classes are used,
which may be due to the fact that Spanish has more
explicit lexicalization for clues that may be relevant
to deception (e.g., verb tenses, formality).
4.2 Can we use information drawn from one
culture to build a deception classifier in
another culture?
In the next set of experiments, we explore the de-
tection of deception using training data originating
from a different culture. As with the within-culture
442
Topic Linguistic Psychological Relativity Personal All LIWC Unigrams
Training: English-US Test: English-India
Abortion 58.00% 51.00% 48.50% 51.50% 52.25% 57.89%
Best Friend 66.36% 47.27% 48.64% 50.45% 59.54% 51.00%
Death Penalty 54.50% 50.50% 50.00% 48.50% 53.5% 59.00%
Average 59.62% 49.59% 49.05% 50.15% 55.10% 55.96%
Training: English-India Test: English-US
Abortion 71.32% 47.49% 43.38% 45.82% 62.50% 55.51%
Best Friend 59.74% 49.35% 51.94% 49.36% 55.84% 53.20%
Death Penalty 51.47% 44.11% 54.88% 50.98% 39.21% 50.71%
Average 60.87% 46.65% 50.06% 48.72% 52.51% 54.14%
Training: English-US Test: Spanish-Mexico
Abortion 70.51% 46.15% 50.00% 52.56% 53.85% 61.53%
Best Friend 69.35% 52.69% 51.08% 46.77% 67.74% 65.03%
Death Penalty 54.88% 54.88% 53.66% 50.00% 62.19% 59.75%
Average 64.92% 51.24% 51.58% 49.78% 61.26% 62.10%
Training: English-India Test: Spanish-Mexico
Abortion 48.72% 50.00% 47.44% 42.31% 43.58% 55.12 %
Best Friend 68.28% 63.44% 56.45% 54.84% 60.75% 67.20%
Death Penalty 60.98% 53.66% 54.88% 60.98% 59.75% 51.21%
Average 59.32% 55.70% 52.92% 52.71% 54.69% 57.84%
Table 2: Cross-cultural experiments using LIWC categories and unigrams
experiments, we use unigrams and LIWC features.
For consistency across the experiments, given that
the size of the Spanish dataset is different com-
pared to the other two datasets, we always train on
one of the English datasets.
To enable the unigram based experiments, we
translate the two English datasets into Spanish by
using the Bing API for automatic translation.
3
As
before, we extract and keep only the unigrams
with frequency greater or equal to 10. The results
obtained in these cross-cultural experiments are
shown in the last column of Table 2.
In a second set of experiments, we use the LIWC
word classes as a bridge between languages. First,
each deceptive or truthful statement is represented
using features based on the LIWC word classes.
Next, since the same word classes are used in both
the English and the Spanish LIWC lexicons, this
LIWC-based representation is independent of lan-
guage, and therefore can be used to perform cross-
cultural experiments. Table 2 shows the results
obtained with each of the four broad LIWC cate-
gories, as well as with all the LIWC word classes.
We also attempted to combine unigrams and
LIWC features. However, in most cases, no im-
provements were noticed with respect to the use
of unigrams or LIWC features alone. We are not
reporting these results due to space limitation.
These cross-cultural evaluations lead to several
3
http://http://http://www.bing.com/dev/en-us/dev-center
findings. First, we can use data from a culture
to build deception classifiers for another culture,
with performance figures better than the random
baseline, but weaker than the results obtained with
within-culture data. An important finding is that
LIWC can be effectively used as a bridge for cross-
cultural classification, with results that are com-
parable to the use of unigrams, which suggests
that such specialized lexicons can be used for
cross-cultural or cross-lingual classification. More-
over, using only the linguistic category from LIWC
brings additional improvements, with absolute im-
provements of 2-4% over the use of unigrams. This
is an encouraging result, as it implies that a seman-
tic bridge such as LIWC can be effectively used
to classify deception data in other languages, in-
stead of using the more costly and time consuming
unigram method based on translations.
4.3 What are the psycholinguistic classes
most strongly associated with
deception/truth?
The final question we address is concerned with
the LIWC classes that are dominant in deceptive
and truthful text for different cultures. We use the
method presented in (Mihalcea and Strapparava,
2009), which consists of a metric that measures the
saliency of LIWC classes in deceptive versus truth-
ful data. Following their strategy, we first create a
corpus of deceptive and truthful text using a mix
of all the topics in each culture. We then calculate
443
Class Score Sample words Class Score Sample words
English-US
Deceptive Truthful
Metaph 1.77 Die,died,hell,sin,lord Insight 0.68 Accept,believe,understand
Other 1.46 He,her,herself,him I 0.66 I,me,my,myself,
You 1.41 Thou,you Optimism 0.65 accept, hope, top, best
Othref 1.18 He,her,herself,him We 0.55 Our,ourselves,us,we,
Negemo 1.18 Afraid,agony,awful,bad Friends 0.46 Buddies,friend
English-India
Deceptive Truthful
Negate 1.49 Cannot,neither,no,none Past 0.78 Happened,helped,liked,listened
Physical 1.46 Heart,ill,love,loved, I 0.66 I,me,mine,my
Future 1.42 Be,may,might,will Optimism 0.65 Accept,accepts,best,bold,
Other 1.17 He,she, himself,herself We 0.55 Our,ourselves,us,we
Humans 1.08 Adult,baby,children,human Friends 0.46 Buddies,companion,friend,pal
Spanish-Mexico
Deceptive Truthful
Certain 1.47 Jam?as(never),siempre(always) Optimism 0.66 Aceptar(accept),animar(cheer)
Humans 1.28 Beb?e(baby),persona(person) Self 0.65 Conmigo(me),tengo(have),soy(am)
You 1.26 Eres(are),estas(be),su(his/her) We 0.58 Estamos(are),somos(be),tenemos(have)
Negate 1.25 Jam?as(never),tampoco(neither) Friends 0.37 Amigo/amiga(friend),amistad(friendship)
Other 1.22 Es(is),esta(are),otro(other) Past 0.32 Compartimos(share),vivimos(lived)
Table 3: Top ranked LIWC classes for each culture, along with sample words
the dominance for each LIWC class, and rank the
classes in reversed order of their dominance score.
Table 3 shows the most salient classes for each
culture, along with sample words.
This analysis shows some interesting patterns.
There are several classes that are shared among the
cultures. For instance, the deceivers in all cultures
make use of negation, negative emotions, and refer-
ences to others. Second, true tellers use more opti-
mism and friendship words, as well as references to
themselves. These results are in line with previous
research, which showed that LIWC word classes
exhibit similar trends when distinguishing between
deceptive and non-deceptive text (Newman et al,
2003). Moreover, there are also word classes that
only appear in some of the cultures; for example,
time classes (Past, Future) appear in English-India
and Spanish-Mexico, but not in English-US, which
in turn contains other classes such as Insight and
Metaph.
5 Conclusions
In this paper, we addressed the task of deception
detection within- and across-cultures. Using three
datasets from three different cultures, each cover-
ing three different topics, we conducted several
experiments to evaluate the accuracy of deception
detection when learning from data from the same
culture or from a different culture. In our evalua-
tions, we compared the use of unigrams versus the
use of psycholinguistic word classes.
The main findings from these experiments are:
1) We can build deception classifiers for different
cultures with accuracies ranging between 60-70%,
with better performance obtained when using psy-
cholinguistic word classes as compared to simple
unigrams; 2) The deception classifiers are not sen-
sitive to different topics, with cross-topic classifi-
cation experiments leading to results comparable
to the within-topic experiments; 3) We can use
data originating from one culture to train decep-
tion detection classifiers for another culture; the
use of psycholinguistic classes as a bridge across
languages can be as effective or even more effec-
tive than the use of translated unigrams, with the
added benefit of making the classification process
less costly and less time consuming.
The datasets introduced in this paper are publicly
available from http://nlp.eecs.umich.edu.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #1344257
and #1355633 and by DARPA-BAA-12-47 DEFT
grant #12475008. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the National Science Founda-
tion or the Defense Advanced Research Projects
Agency.
444
References
?
A. Almela, R. Valencia-Garc??a, and P. Cantos. 2012.
Seeing through deception: A computational ap-
proach to deceit detection in written communication.
In Proceedings of the Workshop on Computational
Approaches to Deception Detection, pages 15?22,
Avignon, France, April. Association for Computa-
tional Linguistics.
B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,
K. Charlton, and H. Cooper. 2003. Cues to decep-
tion. Psychological Bulletin, 129(1).
S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic
stylometry for deception detection. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume
2, ACL ?12, pages 171?175, Stroudsburg, PA, USA.
Association for Computational Linguistics.
T. Fornaciari and M. Poesio. 2013. Automatic decep-
tion detection in italian court cases. Artificial Intelli-
gence and Law, 21(3):303?340.
C. Lewis and J. George. 2008. Cross-cultural de-
ception in social networking sites and face-to-face
communication. Comput. Hum. Behav., 24(6):2945?
2964, September.
C. Lewis and Giordano G. George, J. 2009. A cross-
cultural comparison of computer-mediated decep-
tive communication. In Proceedings of Pacific Asia
Conference on Information Systems.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition of
deceptive language. In Proceedings of the Associa-
tion for Computational Linguistics (ACL 2009), Sin-
gapore.
M. Newman, J. Pennebaker, D. Berry, and J. Richards.
2003. Lying words: Predicting deception from lin-
guistic styles. Personality and Social Psychology
Bulletin, 29.
M. Ott, Y. Choi, C. Cardie, and J. Hancock. 2011.
Finding deceptive opinion spam by any stretch of
the imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 309?319, Stroudsburg, PA, USA.
Association for Computational Linguistics.
H. Peng, C. Xiaoling, C. Na, R. Chandramouli, and
P. Subbalakshmi. 2011. Adaptive context mod-
eling for deception detection in emails. In Pro-
ceedings of the 7th international conference on Ma-
chine learning and data mining in pattern recogni-
tion, MLDM?11, pages 458?468, Berlin, Heidelberg.
Springer-Verlag.
J. Pennebaker and M. Francis. 1999. Linguistic in-
quiry and word count: LIWC. Erlbaum Publishers.
V. Rubin. 2010. On deception and deception detec-
tion: Content analysis of computer-mediated stated
beliefs. Proceedings of the American Society for In-
formation Science and Technology, 47(1):1?10.
C. Toma and J. Hancock. 2010. Reading between
the lines: linguistic cues to deception in online dat-
ing profiles. In Proceedings of the 2010 ACM con-
ference on Computer supported cooperative work,
CSCW ?10, pages 5?8, New York, NY, USA. ACM.
C. Toma, J. Hancock, and N. Ellison. 2008. Separating
fact from fiction: An examination of deceptive self-
presentation in online dating profiles. Personality
and Social Psychology Bulletin, 34(8):1023?1036.
Q. Xu and H. Zhao. 2012. Using deep linguistic fea-
tures for finding deceptive opinion spam. In Pro-
ceedings of COLING 2012: Posters, pages 1341?
1350, Mumbai, India, December. The COLING
2012 Organizing Committee.
H. Zhang, S. Wei, H. Tan, and J. Zheng. 2009. Decep-
tion detection based on svm for chinese text in cmc.
In Information Technology: New Generations, 2009.
ITNG ?09. Sixth International Conference on, pages
481?486, April.
L. Zhou and D. Shi, Y.and Zhang. 2008. A statisti-
cal language modeling approach to online deception
detection. IEEE Trans. on Knowl. and Data Eng.,
20(8):1077?1081, August.
L Zhou, D. Twitchell, T Qin, J. Burgoon, and J. Nuna-
maker. 2003. An exploratory study into decep-
tion detection in text-based computer-mediated com-
munication. In Proceedings of the 36th Annual
Hawaii International Conference on System Sci-
ences (HICSS?03) - Track1 - Volume 1, HICSS ?03,
pages 44.2?, Washington, DC, USA. IEEE Com-
puter Society.
445
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 9?14,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Ravi Sinha
University of North Texas
ravisinha@unt.edu
Diana McCarthy
Lexical Computing Ltd.
diana@dianamccarthy.co.uk
Abstract
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution
task, where given an English target word
in context, participating systems had to
find an alternative substitute word or
phrase in Spanish. The task is based on
the English Lexical Substitution task run
at SemEval-2007. In this paper we pro-
vide background and motivation for the
task, we describe the data annotation pro-
cess and the scoring system, and present
the results of the participating systems.
1 Introduction
In the Cross-Lingual Lexical Substitution task, an-
notators and systems had to find an alternative
substitute word or phrase in Spanish for an En-
glish target word in context. The task is based
on the English Lexical Substitution task run at
SemEval-2007, where both target words and sub-
stitutes were in English.
An automatic system for cross-lingual lexical
substitution would be useful for a number of ap-
plications. For instance, such a system could be
used to assist human translators in their work, by
providing a number of correct translations that the
human translator can choose from. Similarly, the
system could be used to assist language learners,
by providing them with the interpretation of the
unknown words in a text written in the language
they are learning. Last but not least, the output
of a cross-lingual lexical substitution system could
be used as input to existing systems for cross-
language information retrieval or automatic ma-
chine translation.
2 Motivation and Related Work
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained
alternative translation records of typical usages of
a test word, also referred to as a translation mem-
ory. Systems could either select the most appro-
priate translation memory record for each instance
and were scored against a gold-standard set of an-
notations, or they could provide a translation that
was scored by translation experts after the results
were submitted. In contrast to this work, in our
task we provided actual translations for target in-
stances in advance, rather than predetermine trans-
lations using lexicographers or rely on post-hoc
evaluation, which does not permit evaluation of
new systems after the competition.
Previous standalone WSD tasks based on par-
allel data have obtained distinct translations for
senses as listed in a dictionary (Ng and Chan,
2007). In this way fine-grained senses with the
same translations can be lumped together, how-
ever this does not fully allow for the fact that some
senses for the same words may have some transla-
tions in common but also others that are not (Sinha
et al, 2009).
In our task, we collected a dataset which al-
lows instances of the same word to have some
translations in common, while not necessitating
a clustering of translations from a specific re-
source into senses (in comparison to Lefever and
Hoste (2010)). 1 Resnik and Yarowsky (2000) also
1Though in that task note that it is possible for a transla-
tion to occur in more than one cluster. It will be interesting to
9
conducted experiments using words in context,
rather than a predefined sense-inventory however
in these experiments the annotators were asked for
a single preferred translation. In our case, we al-
lowed annotators to supply as many translations
as they felt were equally valid. This allows us
to examine more subtle relationships between us-
ages and to allow partial credit to systems that
get a close approximation to the annotators? trans-
lations. Unlike a full blown machine translation
task (Carpuat and Wu, 2007), annotators and sys-
tems are not required to translate the whole context
but just the target word.
3 Background: The English Lexical
Substitution Task
The English Lexical substitution task (hereafter
referred to as LEXSUB) was run at SemEval-
2007 (McCarthy and Navigli, 2007; McCarthy and
Navigli, 2009). LEXSUB was proposed as a task
which, while requiring contextual disambiguation,
did not presuppose a specific sense inventory. In
fact, it is quite possible to use alternative rep-
resentations of meaning, such as those proposed
by Schu?tze (1998) and Pantel and Lin (2002).
The motivation for a substitution task was that
it would reflect capabilities that might be useful
for natural language processing tasks such as para-
phrasing and textual entailment, while not requir-
ing a complete system that might mask system ca-
pabilities at a lexical level and make participation
in the task difficult for small research teams.
The task required systems to produce a substi-
tute word for a word in context. The data was
collected for 201 words from open class parts-of-
speech (PoS) (i.e. nouns, verbs, adjectives and ad-
verbs). Words were selected that have more than
one meaning with at least one near synonym. Ten
sentences for each word were extracted from the
English Internet Corpus (Sharoff, 2006). There
were five annotators who annotated each target
word as it occurred in the context of a sentence.
The annotators were each allowed to provide up to
three substitutes, though they could also provide
a NIL response if they could not come up with a
substitute. They had to indicate if the target word
was an integral part of a multiword.
see the extent that this actually occurred in their data and the
extent that the translations that our annotators provided might
be clustered.
4 The Cross-Lingual Lexical
Substitution Task
The Cross-Lingual Lexical Substitution task fol-
lows LEXSUB except that the annotations are
translations rather than paraphrases. Given a tar-
get word in context, the task is to provide several
correct translations for that word in a given lan-
guage. We used English as the source language
and Spanish as the target language.
We provided both development and test sets, but
no training data. As for LEXSUB, any systems re-
quiring training data had to obtain it from other
sources. We included nouns, verbs, adjectives and
adverbs in both development and test data. We
used the same set of 30 development words as in
LEXSUB, and a subset of 100 words from the LEX-
SUB test set, selected so that they exhibit a wide
variety of substitutes. For each word, the same ex-
ample sentences were used as in LEXSUB.
4.1 Annotation
We used four annotators for the task, all native
Spanish speakers from Mexico, with a high level
of proficiency in English. As in LEXSUB, the an-
notators were allowed to use any resources they
wanted to, and were required to provide as many
substitutes as they could think of.
The inter-tagger agreement (ITA) was calcu-
lated as pairwise agreement between sets of sub-
stitutes from annotators, as done in LEXSUB. The
ITA without mode was determined as 0.2777,
which is comparable with the ITA of 0.2775 de-
termined for LEXSUB.
4.2 An Example
One significant outcome of this task is that there
are not necessarily clear divisions between usages
and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This means that there can be
usages that overlap to different extents with each
other but do not have identical translations. An
example is the target adverb severely. Four sen-
tences are shown in Figure 1 with the translations
provided by one annotator marked in italics and
{} braces. Here, all the token occurrences seem
related to each other in that they share some trans-
lations, but not all. There are sentences like 1
and 2 that appear not to have anything in com-
mon. However 1, 3, and 4 seem to be partly re-
lated (they share severamente), and 2, 3, and 4 are
also partly related (they share seriamente). When
10
we look again, sentences 1 and 2, though not di-
rectly related, both have translations in common
with sentences 3 and 4.
4.3 Scoring
We adopted the best and out-of-ten precision and
recall scores from LEXSUB (oot in the equations
below). The systems were allowed to supply as
many translations as they feel fit the context. The
system translations are then given credit depend-
ing on the number of annotators that picked each
translation. The credit is divided by the number
of annotator responses for the item and since for
the best score the credit for the system answers
for an item is also divided by the number of an-
swers the system provides, this allows more credit
to be given to instances where there is less varia-
tion. For that reason, a system is better guessing
the translation that is most frequent unless it re-
ally wants to hedge its bets. Thus if i is an item
in the set of instances I , and T
i
is the multiset of
gold standard translations from the human annota-
tors for i, and a system provides a set of answers
S
i
for i, then the best score for item i is2:
best score(i) =
?
s?S
i
frequency(s ? T
i
)
|S
i
| ? |T
i
|
(1)
Precision is calculated by summing the scores
for each item and dividing by the number of items
that the system attempted whereas recall divides
the sum of scores for each item by |I|. Thus:
best precision =
?
i
best score(i)
|i ? I : defined(S
i
)|
(2)
best recall =
?
i
best score(i)
|I|
(3)
The out-of-ten scorer allows up to ten system
responses and does not divide the credit attributed
to each answer by the number of system responses.
This allows a system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select
a perfectly good translation that the annotators had
not thought of. By allowing up to ten translations
in the out-of-ten task the systems can hedge their
bets to find the translations that the annotators sup-
plied.
2NB scores are multiplied by 100, though for out-of-ten
this is not strictly a percentage.
oot score(i) =
?
s?S
i
frequency(s ? T
i
)
|T
i
|
(4)
oot precision =
?
i
oot score(i)
|i ? I : defined(S
i
)|
(5)
oot recall =
?
i
oot score(i)
|I|
(6)
We note that there was an issue that the origi-
nal LEXSUB out-of-ten scorer allowed duplicates
(McCarthy and Navigli, 2009). The effect of du-
plicates is that systems can get inflated scores be-
cause the credit for each item is not divided by the
number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (2009) describe this oversight, identify the
systems that had included duplicates and explain
the implications. For our task, we decided to con-
tinue to allow for duplicates, so that systems can
boost their scores with duplicates on translations
with higher probability.
For both the best and out-of-ten measures, we
also report a mode score, which is calculated
against the mode from the annotators responses as
was done in LEXSUB. Unlike the LEXSUB task,
we did not run a separate multi-word subtask and
evaluation.
5 Baselines and Upper bound
To place results in perspective, several baselines as
well as the upper bound were calculated.
5.1 Baselines
We calculated two baselines, one dictionary-based
and one dictionary and corpus-based. The base-
lines were produced with the help of an on-
line Spanish-English dictionary3 and the Spanish
Wikipedia. For the first baseline, denoted by DICT,
for all target words, we collected all the Spanish
translations provided by the dictionary, in the or-
der returned on the online query page. The best
baseline was produced by taking the first transla-
tion provided by the online dictionary, while the
out-of-ten baseline was produced by taking the
first 10 translations provided.
The second baseline, DICTCORP, also ac-
counted for the frequency of the translations
within a Spanish dictionary. All the translations
3www.spanishdict.com
11
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely
stressed by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now.
{altamente, seriamente, exageradamente}
Figure 1: Translations from one annotator for the adverb severely
provided by the online dictionary for a given target
word were ranked according to their frequencies in
the Spanish Wikipedia, producing the DICTCORP
baseline.
5.2 Upper bound
The results for the best task reflect the inherent
variability as less credit is given where annotators
express differences. The theoretical upper bound
for the best recall (and precision if all items are
attempted) score is calculated as:
best
ub
=
?
i?I
freq
most freq substitute
i
|T
i
|
|I|
? 100
= 40.57 (7)
Note of course that this upper bound is theoretical
and assumes a human could find the most frequent
substitute selected by all annotators. Performance
of annotators will undoubtedly be lower than the
theoretical upper bound because of human vari-
ability on this task. Since we allow for duplicates,
the out-of-ten upper bound assumes the most fre-
quent word type in T
i
is selected for all ten an-
swers. Thus we would obtain ten times the best
upper bound (equation 7).
oot
ub
=
?
i?I
freq
most freq substitute
i
?10
|T
i
|
|I|
? 100
= 405.78 (8)
If we had not allowed duplicates then the out-
of-ten upper bound would have been just less than
100% (99.97). This is calculated by assuming the
top 10 most frequent responses from the annota-
tors are picked in every case. There are only a cou-
ple of cases where there are more than 10 transla-
tions from the annotators.
6 Systems
Nine teams participated in the task, and several
of them entered two systems. The systems used
various resources, including bilingual dictionar-
ies, parallel corpora such as Europarl or corpora
built from Wikipedia, monolingual corpora such
as Web1T or newswire collections, and transla-
tion software such as Moses, GIZA or Google.
Some systems attempted to select the substitutes
on the English side, using a lexical substitu-
tion framework or word sense disambiguation,
whereas some systems made the selection on the
Spanish side using lexical substitution in Spanish.
In the following, we briefly describe each par-
ticipating system.
CU-SMT relies on a phrase-based statistical ma-
chine translation system, trained on the Europarl
English-Spanish parallel corpora.
The UvT-v and UvT-g systems make use of k-
nearest neighbour classifiers to build one word ex-
pert for each target word, and select translations
on the basis of a GIZA alignment of the Europarl
parallel corpus.
The UBA-T and UBA-W systems both use can-
didates from Google dictionary, SpanishDict.com
and Babylon, which are then confirmed using par-
allel texts. UBA-T relies on the automatic trans-
lation of the source sentence using the Google
Translation API, combined with several heuristics.
The UBA-W system uses a parallel corpus auto-
matically constructed from DBpedia.
SWAT-E and SWAT-S use a lexical substitution
framework applied to either English or Spanish.
The SWAT-E system first performs lexical sub-
12
stitution in English, and then each substitute is
translated into Spanish. SWAT-S translates the
source sentences into Spanish, identifies the Span-
ish word corresponding to the target word, and
then it performs lexical substitution in Spanish.
TYO uses an English monolingual substitution
module, and then it translates the substitution can-
didates into Spanish using the Freedict and the
Google English-Spanish dictionary.
FCC-LS uses the probability of a word to be
translated into a candidate based on estimates ob-
tained from the GIZA alignment of the Europarl
corpus. These translations are subsequently fil-
tered to include only those that appear in a trans-
lation of the target word using Google translate.
WLVUSP determines candidates using the best
N translations of the test sentences obtained with
the Moses system, which are further filtered us-
ing an English-Spanish dictionary. USPWLV uses
candidates from an alignment of Europarl, which
are then selected using various features and a clas-
sifier tuned on the development data.
IRST-1 generates the best substitute using a PoS
constrained alignment of Moses translations of the
source sentences, with a back-off to a bilingual
dictionary. For out-of-ten, dictionary translations
are filtered using the LSA similarity between can-
didates and the sentence translation into Spanish.
IRSTbs is intended as a baseline, and it uses only
the PoS constrained Moses translation for best,
and the dictionary translations for out-of-ten.
ColEur and ColSlm use a supervised word sense
disambiguation algorithm to distinguish between
senses in the English source sentences. Trans-
lations are then assigned by using GIZA align-
ments from a parallel corpus, collected for the
word senses of interest.
7 Results
Tables 1 and 2 show the precision P and recall
R for the best and out-of-ten tasks respectively,
for normal and mode. The rows are ordered by
R. The out-of-ten systems were allowed to pro-
vide up to 10 substitutes and did not have any ad-
vantage by providing less. Since duplicates were
allowed so that a system can put more emphasis
on items it is more confident of, this means that
out-of-ten R and P scores might exceed 100%
because the credit for each of the human answers
is used for each of the duplicates (McCarthy and
Navigli, 2009). Duplicates will not help the mode
scores, and can be detrimental as valuable guesses
which would not be penalised are taken up with
Systems R P Mode R Mode P
UBA-T 27.15 27.15 57.20 57.20
USPWLV 26.81 26.81 58.85 58.85
ColSlm 25.99 27.59 56.24 59.16
WLVUSP 25.27 25.27 52.81 52.81
SWAT-E 21.46 21.46 43.21 43.21
UvT-v 21.09 21.09 43.76 43.76
CU-SMT 20.56 21.62 44.58 45.01
UBA-W 19.68 19.68 39.09 39.09
UvT-g 19.59 19.59 41.02 41.02
SWAT-S 18.87 18.87 36.63 36.63
ColEur 18.15 19.47 37.72 40.03
IRST-1 15.38 22.16 33.47 45.95
IRSTbs 13.21 22.51 28.26 45.27
TYO 8.39 8.62 14.95 15.31
DICT 24.34 24.34 50.34 50.34
DICTCORP 15.09 15.09 29.22 29.22
Table 1: best results
duplicates. In table 2, in the column marked dups,
we display the number of test items for which at
least one duplicate answer was provided. 4 Al-
though systems were perfectly free to use dupli-
cates, some may not have realised this. 5 Dupli-
cates help when a system is fairly confident of a
subset of its 10 answers.
We had anticipated a practical issue to come up
with all participants, which is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. While we were
counting on the participants to clean their files and
provide us with clean characters only, we ended up
with result files following different encodings (e.g,
UTF-8, ANSI), some of them including diacrit-
ics, and some of them containing malformed char-
acters. We were able to perform a basic cleaning
of the files, and transform the diacritics into their
diacriticless counterparts, however it was not pos-
sible to clean all the malformed characters without
a significant manual effort that was not possible
due to time constraints. As a result, a few of the
participants ended up losing a few points because
their translations, while being correct, contained
an invalid, malformed character that was not rec-
ognized as correct by the scorer.
There is some variation in rank order of the sys-
tems depending on which measures are used. 6
4Please note that any residual character encoding issues
were not considered by the scorer and so the number of du-
plicates may be slightly higher than if diacritics/different en-
codings had been considered.
5Also, note that some systems did not supply 10 transla-
tions. Their scores would possibly have improved if they had
done so.
6There is not a big difference between P and R because
13
Systems R P Mode R Mode P dups
SWAT-E 174.59 174.59 66.94 66.94 968
SWAT-S 97.98 97.98 79.01 79.01 872
UvT-v 58.91 58.91 62.96 62.96 345
UvT-g 55.29 55.29 73.94 73.94 146
UBA-W 52.75 52.75 83.54 83.54 -
WLVUSP 48.48 48.48 77.91 77.91 64
UBA-T 47.99 47.99 81.07 81.07 -
USPWLV 47.60 47.60 79.84 79.84 30
ColSlm 43.91 46.61 65.98 69.41 509
ColEur 41.72 44.77 67.35 71.47 125
TYO 34.54 35.46 58.02 59.16 -
IRST-1 31.48 33.14 55.42 58.30 -
FCC-LS 23.90 23.90 31.96 31.96 308
IRSTbs 8.33 29.74 19.89 64.44 -
DICT 44.04 44.04 73.53 73.53 30
DICTCORP 42.65 42.65 71.60 71.60 -
Table 2: out-of-ten results
UBA-T has the highest ranking on R for best. US-
PWLV is best at finding the mode, for best how-
ever the UBA-W and UBA-T systems (particularly
the former) both have exceptional performance for
finding the mode in the out-of-ten task, though
note that SWAT-S performs competitively given
that its duplicate responses will reduce its chances
on this metric. SWAT-E is the best system for out-
of-ten, as several of the items that were empha-
sized through duplication were also correct.
The results are much higher than for LEX-
SUB (McCarthy and Navigli, 2007). There are sev-
eral possible causes for this. It is perhaps easier
for humans, and machines to come up with trans-
lations compared to paraphrases. Though the ITA
figures are comparable on both tasks, our task con-
tained only a subset of the data in LEXSUB and we
specifically avoided data where the LEXSUB an-
notators had not been able to come up with a sub-
stitute or had labelled the instance as a name e.g.
measurements such as pound, yard or terms such
as mad in mad cow disease. Another reason for
this difference may be that there are many parallel
corpora available for training a system for this task
whereas that was not the case for LEXSUB.
8 Conclusions
In this paper we described the SemEval-2010
cross-lingual lexical substitution task, including
the motivation behind the task, the annotation pro-
cess and the scoring system, as well as the partic-
ipating systems. Nine different teams with a total
systems typically supplied answers for most items. However,
IRST-1 and IRSTbs did considerably better on precision com-
pared to recall since they did not cover all test items.
of 15 different systems participated in the task, us-
ing a variety of resources and approaches. Com-
parative evaluations using different metrics helped
determine what works well for the selection of
cross-lingual lexical substitutes.
9 Acknowledgements
The work of the first and second authors has been partially
supported by a National Science Foundation CAREER award
#0747340. The work of the third author has been supported
by a Royal Society UK Dorothy Hodgkin Fellowship. The
authors are grateful to Samer Hassan for his help with the
annotation interface.
References
Marine Carpuat and Dekai Wu. 2007. Improving statis-
tical machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-CoNLL
2007), pages 61?72, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese translation
task. In Proceedings of the SENSEVAL-2 workshop, pages
37?44.
Els Lefever and Veronique Hoste. 2010. SemEval-2007 task
3: Cross-lingual word sense disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic Eval-
uations (SemEval-2010), Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 48?53, Prague, Czech Re-
public.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Eval-
uation Special Issue on Computational Semantic Analysis
of Language: SemEval-2007 and Beyond, 43(2):139?159.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-2007 task
11: English lexical sample task via English-Chinese paral-
lel text. In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), pages 54?58,
Prague, Czech Republic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, pages
613?619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguishing
systems and distinguishing senses: New evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
Serge Sharoff. 2006. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435?462.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea. 2009.
Semeval-2010 task 2: Cross-lingual lexical substitution.
In Proceedings of the NAACL-HLT Workshop SEW-2009
- Semantic Evaluations: Recent Achievements and Future
Directions, Boulder, Colorado, USA.
14
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 20?29,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Measuring Semantic Relatedness using Multilingual Representations
Samer Hassan
University of North Texas
Denton, TX
samer@unt.edu
Carmen Banea
University of North Texas
Denton, TX
carmenbanea@my.unt.edu
Rada Mihalcea
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
This paper explores the hypothesis that se-
mantic relatedness may be more reliably in-
ferred by using a multilingual space, as com-
pared to the typical monolingual representa-
tion. Through evaluations using several state-
of-the-art semantic relatedness systems, ap-
plied on standard datasets, we show that a
multilingual approach is better suited for this
task, and leads to improvements of up to 47%
with respect to the monolingual baseline.
1 Introduction
Semantic relatedness is the task of quantifying the
strength of the semantic connection between tex-
tual units, be they words, sentences, or documents.
For instance, one may want to determine how se-
mantically related are two words such as car and
automobile, or two pieces of text such as I love an-
imals and I own a pet. It is one of the main tasks
explored in the field of natural language processing,
as it lies at the core of a large number of applica-
tions such as information retrieval (Ponte and Croft,
1998), query reformulation (Metzler et al, 2007;
Yih and Meek, 2007; Sahami and Heilman, 2006;
Broder et al, 2008), image retrieval (Leong and Mi-
halcea, 2009; Goodrum, 2000), plagiarism detection
(Hoad and Zobel, 2003; Shivakumar and Garcia-
Molina, 1995; Broder et al, 1997; Heintze, 1996;
Brin et al, 1995; Manber, 1994), information flow
(Metzler et al, 2005), sponsored search (Broder et
al., 2008), short answer grading (Mohler and Mihal-
cea, 2009a; Pulman and Sukkarieh, 2005; Mitchell
et al, 2002), and textual entailment (Dagan et al,
2005).
The typical approach to semantic relatedness is to
either measure the distance between the constituent
words by using a knowledge base such as Word-
Net or Roget (e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Jarmasz and Szpakowicz, 2003; Peder-
sen et al, 2004)), or to calculate the similarity be-
tween the word distributions in very large corpora
(e.g., (Landauer et al, 1991; Lin, 1998; Gabrilovich
and Markovitch, 2007)). With almost no exception,
these methods have been applied on one language at
a time ? English, most of the time, although mea-
sures of relatedness have also been explored on lan-
guages such as German (Zesch et al, 2007), Chinese
(Li et al, 2005), Japanese (Kazama et al, 2010), and
others.
In this paper, we take a step further and ex-
plore a joint multilingual semantic relatedness met-
ric, which aggregates semantic relatedness scores
measured on several different languages. Specifi-
cally, in our method, in order to measure the re-
latedness of two textual units, we first determine
their relatedness in multiple languages, and conse-
quently infer a final relatedness score by averaging
the scores calculated in the individual languages.
Our hypothesis is that a multilingual representa-
tion can enrich the relatedness space and address
relevant issues such as polysemy (i.e., find that two
occurrences of the same word in language L1 rep-
resent two different meanings because of different
translations in language L2) and synonymy (i.e., find
that two words in language L1 are related because
they have the same translation in language L2). We
show that by measuring relatedness in a multilingual
space, we are able to improve over a traditional re-
latedness measure that relies exclusively on a mono-
lingual representation.
Through experiments using several state-of-the-
art measures of relatedness, applied on a multilin-
gual space including English, Arabic, Spanish, and
Romanian, we aim to answer the following research
20
questions: (1) Does the task of semantic relatedness
benefit from a multilingual representation, as com-
pared to a monolingual one? (2) Does the translation
quality affect the results? and (3) Do the findings
hold for different relatedness datasets?
The paper is organized as follows. First, we
overview related work on word and text related-
ness, and on multilingual natural language process-
ing. We then briefly describe three corpus-based
measures of relatedness, and present several word
and text datasets that have been used in the past to
evaluate relatedness. We then present evaluations
and experiments addressing each of the three re-
search questions, and discuss our findings.
2 Related Work
Semantic relatedness. The approaches for seman-
tic relatedness that have been considered to date
can be grouped into knowledge-based and corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to mea-
sure definitional overlap (Lesk, 1986), term dis-
tance within a graphical taxonomy (Leacock and
Chodorow, 1998), term depth in the taxonomy as a
measure of specificity (Wu and Palmer, 1994), and
others. The application of such measures to a lan-
guage other than English requires the availability of
the lexical resource in that language; furthermore,
even though taxonomies such as WordNet (Miller,
1995) are available in a number of languages1, their
coverage is still limited, and often times they are not
publicly available. For these reasons, in multilingual
settings, these measures often become untractable.
On the other side, corpus-based measures
such as Latent Semantic Analysis (LSA) (Lan-
dauer et al, 1991), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011), Pointwise Mutual Information (PMI)
(Church and Hanks, 1990), PMI-IR (Turney, 2001),
Second Order PMI (Islam and Inkpen, 2006), Hy-
perspace Analogues to Language (HAL) (Burgess
et al, 1998) and distributional similarity (Lin, 1998)
employ probabilistic approaches to decode the se-
mantics of words. They consist of unsupervised
methods that utilize the contextual information and
patterns observed in raw text to build semantic pro-
files of words, and thus they can be easily transferred
1http://www.illc.uva.nl/EuroWordNet/
to a new language provided that a large corpus in that
language is available.
Multilingual natural language processing. Also
relevant is the work done on multilingual text pro-
cessing, which attempts to improve the performance
of different natural language processing tasks by
integrating information drawn from multiple lan-
guages. For instance, (Cohn and Lapata, 2007) ex-
plore the use of triangulation for machine transla-
tion, where multiple translation models are learned
using multilingual parallel corpora. The model was
found especially beneficial for languages where the
training dataset was small, thus suggesting that this
method may be particularly useful for languages
with scarce resources. (Davidov and Rappoport,
2009) experiment with the use of multiple languages
to enhance an existing lexicon. In their experiments,
using three source languages and 45 intermediate
languages, they find that the multilingual resources
can lead to significant improvements in concept ex-
pansion. (Banea et al, 2010) explore the use of
parallel multilingual corpora to improve subjectivity
classification in a target language, finding that the
use of multilingual representations for subjectivity
analysis improves over the monolingual classifiers.
Similarly, (Banea and Mihalcea, 2011) investigate
the use of multilingual contexts for word sense dis-
ambiguation. By leveraging on the translations of
the annotated contexts in multiple languages, a mul-
tilingual thematic space emerges that better disam-
biguates target words.
Finally, there are two lines of work that explore
semantic distances in a multilingual space. First,
(Besanc?on and Rajman, 2002) examine the notion
that the distances between document vectors within
a language correlate with the distances between their
corresponding vectors in a parallel corpus. These
findings provide clues about the possibility of reli-
able semantic knowledge transfer across language
boundaries. Second, (Hassan and Mihalcea, 2009)
propose a framework to compute semantic relat-
edness between two words in different languages,
by considering Wikipedia articles in multiple lan-
guages. The method differs from the one proposed
here, as we aggregate relatedness over monolingual
spaces rather than measuring cross-lingual related-
ness, and we do not specifically use the inter-wiki
links between Wikipedia pages.
21
3 Measures of Text Relatedness
In this work, we focus on corpus-based metrics
because of their unsupervised nature, their flexi-
bility, scalability, and portability to different lan-
guages. Specifically, we utilize three popular mod-
els, LSA (Landauer et al, 1991), ESA (Gabrilovich
and Markovitch, 2007), and SSA (Hassan and Mi-
halcea, 2011). In these models, the semantic profile
of a word is expressed in terms of the explicit (ESA),
implicit (LSA), or salient (SSA) concepts. All three
models are trained on the Wikipedia 2010 corpora
corresponding to the four languages of interest (En-
glish, Arabic, Spanish, Romanian).
Explicit Semantic Analysis. ESA (Gabrilovich
and Markovitch, 2007) uses encyclopedic knowl-
edge in an information retrieval framework to gen-
erate a semantic interpretation of words. Since en-
cyclopedic knowledge is typically organized into
concepts (or topics), each concept is described us-
ing definitions and examples. ESA relies on the
distribution of words inside the encyclopedic de-
scriptions. It builds semantic representations for
a given word using a word-document association,
where each document represents a Wikipedia article.
In this vector representation, the semantic interpre-
tation of a text can be modeled as an aggregation of
the semantic vectors of its individual words.
Latent Semantic Analysis. In LSA (Landauer et
al., 1991), term-context associations are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-by-
context matrix T, where the matrix is induced from
a large corpus. This reduction entails the abstraction
of meaning by collapsing similar contexts and dis-
counting noisy and irrelevant ones, hence transform-
ing the real world term-context space into a word-
latent-concept space which achieves a much deeper
and concrete semantic representation of words.
Salient Semantic Analysis. SSA (Hassan and
Mihalcea, 2011) incorporates a similar semantic
abstraction and interpretation of words, by using
salient concepts gathered from encyclopedic knowl-
edge, where a concept is defined as an unambigu-
ous word or phrase with a concrete meaning, which
can afford an encyclopedic definition. The links
available between Wikipedia articles, obtained ei-
ther through manual annotation by the Wikipedia
users or using an automatic annotation process, are
regarded as clues or salient features within the text
that help define and disambiguate its context. This
method seeks to determine the semantic relatedness
of words by measuring the distance between their
concept-based profiles, where a profile consists of
co-occurring salient concepts found within a given
window size in a very large corpus.
4 Datasets
To evaluate the representation strength of a multilin-
gual semantic relatedness model we employ several
standard word-to-word and text-to-text datasets. For
each of these datasets, we make use of their repre-
sentation in the four languages of interest.
4.1 Word Relatedness
We construct our multilingual word-to-word
datasets building upon three word relatedness
datasets that have been widely used in the past.
Rubenstein and Goodenough (Rubenstein and
Goodenough, 1965) (RG65) consists of 65 word
pairs ranging from synonymy pairs (e.g., car -
automobile) to completely unrelated words (e.g.,
noon - string). The participating terms in all the
pairs are non-technical nouns annotated by 51 hu-
man judges on a scale from 0 (unrelated) to 4 (syn-
onyms).
Miller-Charles (Miller and Charles, 1991) (MC30)
is a subset of RG65, consisting of 30 word pairs an-
notated for relatedness by 38 human subjects, using
the same 0 to 4 scale.
WordSimilarity-353 (Finkelstein et al, 2001)
(WS353), also known as Finkelstein-353, consists
of 353 word pairs annotated by 13 human experts,
on a scale from 0 (unrelated) to 10 (synonyms).
While containing the MC30 set, it poses an addi-
tional degree of difficulty by also including phrases
(e.g., ?Wednesday news?), proper names and tech-
nical terms.
To enable a multilingual representation, we use
the multilingual datasets introduced by (Hassan and
Mihalcea, 2009), which are based upon MC30 and
WS353. These multilingual datasets are built us-
ing manual translations, following the same guide-
lines adopted for the generation and the annotation
of their original English counterparts. These manu-
ally translated collections, available in Arabic, Span-
ish, and Romanian, allow us to infer an upper bound
for the multilingual semantic relatedness model.
Moreover, in order to provide a more realistic
scenario, where manual translations are not avail-
able, we also create multilingual datasets by auto-
matically translating the three English datasets into
22
Arabic, Spanish and Romanian.2 Similar to how the
manually translated datasets were created by provid-
ing the bilingual speakers with one word pair at a
time, for the automatic translation each word pair is
processed as a single query to the translation engine.
Thus, the co-occurrence metrics derived from large
corpora are able to play a role in providing a dis-
ambiguated translation instead of defaulting to the
most frequently used sense if the words were to be
processed individually. This allows for the embed-
ded word pair relatedness to be transferred to other
languages as well.
4.2 Text Relatedness
We use three standard text-to-text datasets.
Lee50 (Lee and Welsh, 2005) is a compilation of
50 documents collected from the Australian Broad-
casting Corporation?s news mail service. Each doc-
ument is scored by ten annotators on a scale from 1
(unrelated) to 5 (alike) based on its semantic related-
ness to all the other documents. The users? annota-
tion is then averaged per document pair, resulting in
2,500 document pairs annotated with their similarity
scores. Since it was found that there was no signif-
icant difference between annotations given a differ-
ent order of the documents in a pair (Lee and Welsh,
2005), the evaluations are carried out on only 1225
document pairs after ignoring duplicates.
Li30 (Li et al, 2006) is a sentence pair similar-
ity dataset obtained by replacing each of the RG65
word-pairs with their respective definitions extracted
from the Collins Cobuild dictionary (Sinclair, 2001).
Each sentence pair was scored between 0 (unrelated)
to 4 (alike) by 32 native English speakers, and their
annotations were averaged. Due to the skew in the
scores toward low similarity sentence-pairs, they se-
lected a subset of 30 sentences from the 65 sentence
pairs to maintain an even relatedness distribution.
AG400 (Mohler and Mihalcea, 2009b) is a domain
specific dataset from the field of computer science,
used to evaluate the application of semantic relat-
edness measures to real world applications such as
short answer grading. We employ the version pro-
posed by (Hassan and Mihalcea, 2011) which con-
sists of 400 student answers along with the corre-
sponding questions and correct instructor answers.
Each student answer was graded by two judges on
a scale from 0 (completely wrong) to 5 (perfect an-
swer). The correlation between human judges was
2For all the automatic translations we used the Google
Translate service.
measured at 0.64.
First, we construct a multilingual, manually trans-
lated text-to-text relatedness dataset based on the
standard Li30 corpus.3 Native speakers of Spanish,
Romanian and Arabic, who were also highly profi-
cient in English, were asked to translate the entries
drawn from the English collection. They were pre-
sented with one sentence at a time, and asked to pro-
vide the appropriate translation into their native lan-
guage. Since we had five Spanish, two Arabic, and
two Romanian translators, an arbitrator (native to the
language) was charged with merging the candidate
translations by proposing one sentence per language.
Furthermore, to test the abstraction of semantics
from the choice of underlying language, we asked
three different Spanish human experts to re-score the
Spanish text-pair translations on the same scale used
in the construction of the English collection. The
correlation between the relatedness scores assigned
during this experiment and the scores assigned to the
original English experiment was 0.77 ? 0.86, indi-
cating that the translations provided by the bilingual
judges were correct and preserved the semantics of
the original English text-pairs. As was the case
for the manually constructed word-to-word datasets
previously described, the metrics obtained on the
manually translated Li30 dataset will also act as an
upper bound for the text-to-text evaluations.
Finally, for a more sensible scenario where the
text fragments do not require manual translations
in order to compute their semantic relatedness, we
create a multilingual version of the three English
datasets by employing statistical machine translation
to translate the texts into the other three languages.
Each text pair was processed through two separate
queries to the translation engine, since the two text
fragments contain sufficient information to prompt
an in-context translation on their own.
5 Framework
We generate SSA, LSA and ESA vectorial models
for English, Romanian, Arabic, and Spanish, using
the same Wikipedia 2010 versions for all the sys-
tems (e.g., the SSA, LSA and ESA relatedness
measures for Spanish are all trained on the same
Spanish Wikipedia version).
We construct a multilingual model by considering
a word- or text-pair from a source language along
3Dataset is available for download at lit.csci.unt.
edu/index.php?P=research/downloads
23
with its translations in the other languages. To eval-
uate this multilingual model in a way that reduces
the bias that may arise from choosing one language
over the other, we do the following: we start from a
source language and generate all the possible combi-
nations of this language with the available language
set {ar, en, es, ro}. Within each combination, we
average the monolingual model scores for the lan-
guages in this combination with respect to the target
word- or text-pair into a final relatedness score.
For example, let us consider Spanish as the source
language, then the possible combinations of the lan-
guages that include the source language will be
{{es}, {es, ar}, {es, ro}, {es, en}, {es, ar, en},
{es, ar, ro}, {es, en, ro}, and {es, ar, en, ro}}.
For each possible combination, we aggregate the
scores of the languages in that combination. In this
setting, a combination of size (cardinality) one will
always be the source language and will serve as the
baseline. For every combination (e.g. {es, ar}),
we average the individual monolingual relatedness
scores for a given word- or text-pair in this set.
Finally, to calculate the overall correlation of
these generated multilingual models (one system per
combination size) with the human scores, we av-
erage the correlation scores achieved over all the
datasets in a given combination (e.g., {es, ar}) with
all correlation scores achieved under other combina-
tions of the same size (e.g., {es, ro}, {es, en}). This
in effect allows us to observe the cumulative perfor-
mance irrespective of language choice, as we extend
the multilingual model to include more languages.
Formally, let N be the number of languages, Cn
be the set of all language combinations of size n, and
ci be one of the possible combinations of size n,
Cn = {ci | |ci| = n, 0 < i <
(
N
n
)
} (1)
then the relatedness of a word- or text-pair p from
the dataset P under this combination can be repre-
sented as:
Simci(p) =
1
|ci|
?
l?ci
Siml(p) (2)
where Siml(p) is the relatedness score of the word-
or text-pair p in the monolingual model of language
l. To evaluate the performance of the multilingual
model, let Di be the generated relatedness distribu-
tion for the dataset P using the combination ci:
Di = {?p, Simci(p)? | p ? P}. (3)
Then, the correlation between the gold standard
distribution G and the generated scores can be cal-
culated as follows:
CorrelCn(D,G) =
1
|Cn|
?
ci?Cn
Correlci(Di, G),
(4)
where Correl can stand for Pearson (r), Spearman
(?), or their harmonic mean (?), as also reported in
(Hassan and Mihalcea, 2011).
6 Evaluations
In this section we revisit the questions formulated in
the introduction, and based on different experiment
setups following the framework introduced in Sec-
tion 5, we provide an answer to each one of them.
Does the task of semantic relatedness benefit
from a multilingual representation? We evalu-
ate the three semantic relatedness models, namely
LSA, ESA and SSA on our manually constructed
multilingual word relatedness (MC30, WS353)
and text relatedness datasets (LI30), as described in
Section 4.
Figure 1 plots the correlation scores achieved
across all the languages against the gold stan-
dard and then averaged across all the multilingual
datasets. The figure shows a clear and steady im-
provement (25% - 28% with respect to the mono-
lingual baseline) achieved when more languages are
incorporated into the relatedness model. It is worth
noting that both the Pearson and Spearman correla-
tions exhibit the same improvement pattern, which
confirms our hypothesis that adding more languages
has a positive impact on the relatedness scores. The
fact that this trend is visible across all the systems
supports the idea that a multilingual representation
constitutes a better model for determining semantic
relatedness. Furthermore, we notice that SSA is the
best performing system under these settings, with a
correlation improvement of approximately 15%.
To further analyze the role of the multilingual
model and to explore whether some languages ben-
efit from using this abstraction more than others,
we plot the correlation scores achieved by the indi-
vidual languages averaged over all the systems and
the datasets in Figure 2. We notice a sharp rise in
performance associated with the addition of more
languages to the Arabic (42%) and the Romanian
(47%) models, and a slower rise for Spanish (23%).
The performance of English is also affected, but on
a smaller scale (4%) when compared to the other
24
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ESA
LSA
SSA
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 1: Manual translation - average correlation (?,
r, ?) obtained from incorporating scores from models in
other languages
languages. Not surprisingly, this correlates with the
size of each corpus, where Arabic and Romanian are
the smallest, while English is the largest.
The results support the notion that resource poor
languages can benefit from languages with richer
and larger resources, such as English or Spanish.
Furthermore, incorporating additional languages to
English also leads to small improvements, which in-
dicates that the benefit, while disproportionate, is
mutual.
Does the quality of translations affect the results?
As a natural next step, we investigate the role played
by the manual translations in the performance of the
multilingual model. Since the previous evaluations
require the availability of the word- or text-pairs
in multiple languages, we attempt to see if we can
eliminate this restriction by automating the trans-
lation process using statistical machine translation
(MT). Therefore, for a multilingual model employ-
ing automated settings, the manual models proposed
previously constitute an upper bound.
We use the Google MT engine4 to translate our
multilingual datasets into the target languages (en,
es, ar, and ro). We then repeat all the evaluations
using the newly constructed datasets.
Figure 3 shows the correlation scores achieved
across all the languages and averaged across all the
multilingual datasets constructed using automatic
translation. We again see a clear and steady im-
4This API is now offered as a paid service; Microsoft or
Babelfish automatic translation services are publicly available.
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ar
en
es
ro
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 2: Manual translation - average correlation (?,
r, ?) obtained by supplementing a source language with
scores from other languages
provement (12% - 35% with respect to the mono-
lingual baseline) similar to the observed pattern in
the corresponding manual evaluations (Figure 1).
While the overall achieved performance for SSA
has dropped (from ? = 0.793 to ? = 0.71) when
compared to the manual settings, we are still able
to improve over the baseline (? = 0.635). LSA
seems to experience the highest relative improve-
ment (35%), which might be due to its ability to
handle noise in these automatic settings. Over-
all Pearson and Spearman correlations exhibit the
same improvement pattern, which supports the no-
tion that even with the possibility of introducing
noise through miss-translations, the models overall
benefit from the additional clues provided by the
multilingual representation.
To explore the effect of automatic translation on
the individual languages, we plot the correlation
scores achieved vis-a`-vis a reference language, and
average over all the systems and the automatically
translated datasets in Figure 4, in a similar fashion
to Figure 2.
We notice the similar rise in performance asso-
ciated with the addition of more languages to the
Arabic (20%) and the Romanian (37%) models, and
a slower rise for Spanish (16%) and English (8%).
The effect of the automatic translation quality is ev-
ident for the Arabic language where the automatic
translation seems to slow down the improvement
when compared to the manual translations (Figure
2). A similar behavior is also observed in Spanish
and Romanian but on a lower scale.
25
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ESA
LSA
SSA
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 3: Automatic translation - average correlation (?,
r, ?) obtained from incorporating scores from models in
other languages
A very interesting consideration is that English
experiences a stronger improvement when using au-
tomatic translations (8%) compared to manual trans-
lations (4%). This can be attributed to the trans-
lation engine quality in transferring English text to
other languages and to the fact that the statistical
translation (when accurate) can lead to a transla-
tion that makes use of more frequently used words,
which contribute to more robust relatedness mea-
sures. When presented with a word pair, human
judges may provide a translation influenced by the
form/root of the word in the source language, which
may not be as commonly used as the output of a
MT system. For example, when presented with the
pair ?coast - shore,? a Romanian translator may be
tempted to provide ?coasta?? as a translation candi-
date for the first word in the pair, as it resembles the
English word in form. However, the Romanian word
is highly ambiguous, and in an authoritative Roma-
nian dictionary5 its primary sense is that of rib, fol-
lowed by side, slope, and ultimately coast. Thus, a
MT system using a statistical inference may provide
a stronger translation such as ?t?a?rm? that is far less
ambiguous, and whose primary meaning is the one
intended by the original pair.
Overall, the trend is positive and follows the
pattern previously observed on the manually con-
structed datasets. This suggests that an automatic
translation, even if more noisy, is beneficial and pro-
vides a way to reinforce semantic relatedness in a
5http://dexonline.ro/definitie/coasta
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ar
en
es
ro
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 4: Automatic translation - average correlation (?,
r, ?) obtained by supplementing a source language with
scores from other languages
given language with information coming from mul-
tiple languages with no manual effort.
Do our findings hold for different relatedness
datasets? At last, encouraged by the small perfor-
mance difference between the use of manual ver-
sus automatic translations, we seek to explore how
this multilingual model behaves under the different
paradigms dictated by word relatedness versus text
relatedness scenarios. Since our previous experi-
ments were constrained to collections for which we
also had a manual translation, we perform a larger
scale evaluation by including automatically trans-
lated word relatedness (RG65) and text relatedness
(LEE50 and AG400) datasets into all the languages
in our language set, and repeat all the word-to-word
and text-to-text evaluations.
Table 1 shows the correlation scores achieved us-
ing automatic translations on the word relatedness
datasets. Most models on most datasets benefit from
the multilingual representation (as shown by the fig-
ures in bold). Specifically, the SSA model has an
improvement in ? of 26% for WS353 and 15% for
MC30. This improvement is most evident in the
case of the largest dataset WS353, where all the
multilingual models exhibit a consistent and strong
performance.
Table 2 reports the results obtained for the text
relatedness datasets using automatic translation.
While the ESA performance suffers in the multi-
lingual model, it is overshadowed by the improve-
ment experienced by LSA and SSA. The multilin-
26
r ? ?
Models MC30 RG65 WS353 MC30 RG65 WS353 MC30 RG65 WS353
ESAen 0.645 0.644 0.487 0.742 0.768 0.525 0.690 0.701 0.506
ESAml 0.723 0.741 0.515 0.766 0.759 0.519 0.744 0.75 0.517
LSAen 0.509 0.450 0.435 0.525 0.499 0.436 0.517 0.473 0.436
LSAml 0.538 0.566 0.487 0.484 0.569 0.517 0.510 0.567 0.502
SSAen 0.771 0.824 0.543 0.688 0.772 0.553 0.727 0.797 0.548
SSAml 0.873 0.807 0.674 0.803 0.795 0.713 0.836 0.801 0.693
Table 1: Automatic translation - r, ?, ? correlations on the word relatedness datasets using multilingual models.
r ? ?
Models LI30 LEE50 AG400 LI30 LEE50 AG400 LI30 LEE50 AG400
ESAen 0.792 0.756 0.434 0.797 0.48 0.392 0.795 0.587 0.412
ESAml 0.776 0.648 0.382 0.742 0.339 0.358 0.759 0.445 0.369
LSAen 0.829 0.776 0.400 0.824 0.523 0.359 0.826 0.625 0.379
LSAml 0.856 0.765 0.46 0.855 0.502 0.404 0.856 0.606 0.43
SSAen 0.840 0.744 0.520 0.843 0.371 0.501 0.841 0.495 0.510
SSAml 0.829 0.743 0.539 0.87 0.41 0.521 0.849 0.528 0.53
Table 2: Automatic translation - r, ?, ? correlations on the text relatedness datasets using multilingual models.
gual model reports some of the best scores in the
literature, such as a correlations of r = 0.856 and
? = 0.87 for LI30 achieved by LSA and SSA, re-
spectively. Not surprisingly, SSA is still a top con-
tender, achieving the highest scores for AG400 and
LI30. In AG400, SSA reports a ? of 0.53 which
represents a 4% improvement over the English SSA
model (? = 0.51) and a 16% improvement over the
best knowledge-based system J&C (? = 0.457).
It is important to note that the evaluation in Ta-
bles 1 and 2 are restricted to data translated from En-
glish into a target language. English, as a resource-
rich language, has an extensive and robust monolin-
gual model, yet it can still be enhanced with addi-
tional clues originating from other languages. Ac-
cordingly, we only expected small improvements in
these two experiments, unlike the cases where we
start from resource-poor languages such as Roma-
nian or Arabic (see Figures 2 and 4).
7 Conclusion
In this paper, we showed how a semantic relatedness
measure computed in a multilingual space is able
to acquire and leverage additional information from
the multilingual representation, and thus be strength-
ened as more languages are taken into considera-
tion. Our experiments seem to suggest that combi-
nations of multiple languages supply additional in-
formation to derive a semantic relatedness between
texts in an automatic framework. Since establishing
semantic relatedness requires us to employ cogni-
tive processes that are in large part independent of
the language that we speak, it comes at no surprise
that using relatedness clues originating from more
than one language allows for a better identification
of relationships between texts. While efficiency may
be a concern, it is worth noting that the method is
highly parallelizable, as the individual relatedness
measures obtained before the aggregation step can
be calculated in parallel.
Notably, all the relatedness measures that we ex-
perimented with exhibited the same improvement
trend. While this framework allows languages with
scarce electronic resources, such as Romanian and
Arabic, to obtain very large improvements in seman-
tic relatedness as compared to the monolingual mea-
sures, improvements are also noticed for languages
with richer resources such as English.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
27
References
C. Banea and R. Mihalcea. 2011. Word sense disam-
biguation with multilingual features. In International
Conference on Semantic Computing, Oxford, UK.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 28?36, Bei-
jing, China, August.
R. Besanc?on and M. Rajman. 2002. Evaluation of a vec-
tor space similarity measure in a multilingual frame-
work. In Proceedings of the Third International Con-
ference on Language Resource and Evaluation (LREC
2002), Las Palmas, Spain.
S. Brin, J. Davis, and H. Garcia-Molina. 1995. Copy de-
tection mechanisms for digital documents. In ACM In-
ternational Conference on Management of Data (SIG-
MOD 1995).
A. Z. Broder, S. C. Glassman, M. S. Manasse, and
G. Zweig. 1997. Syntactic clustering of the web.
Comput. Netw. ISDN Syst., 29(8-13):1157?1166.
A Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich,
V. Josifovski, and L. Riedel. 2008. Search advertising
using web relevance feedback. In CIKM ?08: Pro-
ceeding of the 17th ACM conference on Information
and knowledge management, pages 1013?1022, New
York, NY, USA. ACM.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: making effective use of multi-parallel
corpora. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
728?735, Prague, Czech Republic.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
D. Davidov and A. Rappoport. 2009. Enhancement
of lexical concepts using cross-lingual web mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 852?
861, Singapore.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In ACM
Press, editor, The Tenth International World Wide Web
Conference, pages 406?414, Hong Kong.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
A. Goodrum. 2000. Image information retrieval: An
overview of current research. Informing Science,
3(2):63?66.
S. Hassan and R. Mihalcea. 2009. Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192?
1201, Singapore. Association for Computational Lin-
guistics.
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
N. Heintze. 1996. Scalable document fingerprinting. In
In Proc. USENIX Workshop on Electronic Commerce.
T. C. Hoad and J. Zobel. 2003. Methods for identifying
versioned and plagiarized documents. J. Am. Soc. Inf.
Sci. Technol., 54(3):203?215.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s thesaurus
and semantic similarity. In Proceedings of the confer-
ence on Recent Advances in Natural Language Pro-
cessing RANLP-2003, Borovetz, Bulgaria, September.
J. Kazama, S. De Saeger, K. Kuroda, M. Murata, and
K. Torisawa. 2010. A bayesian method for robust
estimation of distributional similarities. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1991. How well can passage meaning be
derived without using word order? A comparison of
latent semantic analysis and humans. In Proceedings
of the 19th annual meeting of the Cognitive Science
Society, pages 412?417, Mawhwah, N. Erlbaum.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. D. Lee and M. Welsh. 2005. An empirical evaluation
of models of text document similarity. In Proceedings
of the 27th annual meeting of the Cognitive Science
Society, pages 1254?1259, Stresa, Italy.
C. W. Leong and R. Mihalcea. 2009. Explorations in
automatic image annotation using textual features. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 56?59, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
28
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries. In Proceedings of the
5th annual international conference on Systems docu-
mentation - SIGDOC ?86, pages 24?26, Toronto, On-
tario. ACM Press.
W. Li, Q. Lu, and R. Xu. 2005. Similarity based chinese
synonym collocation extraction. International Journal
of Computational Linguistics and Chinese Language
Processing, 10(1).
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150, August.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
U. Manber. 1994. Finding similar files in a large file sys-
tem. In USENIX WINTER 1994 TECHNICAL CON-
FERENCE, pages 1?10.
D. Metzler, Y. Bernstein, W. Bruce Croft, A. Moffat,
and J. Zobel. 2005. Similarity measures for track-
ing information flow. In CIKM ?05: Proceedings
of the 14th ACM international conference on Infor-
mation and knowledge management, pages 517?524,
New York, NY, USA. ACM.
D. Metzler, S. T. Dumais, and C. Meek. 2007. Similarity
measures for short segments of text. In Giambattista
Amati, Claudio Carpineto, and Giovanni Romano, edi-
tors, ECIR, volume 4425 of Lecture Notes in Computer
Science, pages 16?27. Springer.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. In roceedings of the 6th Interna-
tional Computer Assisted Assessment (CAA) Confer-
ence, Loughborough, UK. Loughborough University.
M. Mohler and R. Mihalcea. 2009a. Text-to-text seman-
tic similarity for automatic short answer grading. In
EACL, pages 567?575. The Association for Computer
Linguistics.
M. Mohler and R. Mihalcea. 2009b. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 567?575, Stroudsburg, PA, USA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-04),
demonstrations, San Jose, CA.
J. Ponte and W. Croft. 1998. A language modeling ap-
proach to information retrieval. In Proceedings of the
Annual International SIGIR Conference on Research
and Development in Information Retrieval, pages 275?
281, Melbourne, Australia.
S. G. Pulman and J. Z. Sukkarieh. 2005. Automatic
short answer marking. In EdAppsNLP 05: Proceed-
ings of the second workshop on Building Educational
Applications Using NLP, pages 9?16, Morristown, NJ,
USA. Association for Computational Linguistics.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633, October.
M. Sahami and T. D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In WWW ?06: Proceedings of the 15th inter-
national conference on World Wide Web, pages 377?
386, New York, NY, USA. ACM.
N. Shivakumar and H. Garcia-Molina. 1995. Scam: A
copy detection mechanism for digital documents. In
2nd International Conference in Theory and Practice
of Digital Libraries (DL 1995).
J. Sinclair. 2001. Collins cobuild English dictionary for
advanced learners. Harper Collins, 3rd edition.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
W. T. Yih and C. Meek. 2007. Improving similarity mea-
sures for short segments of text. In AAAI?07: Pro-
ceedings of the 22nd national conference on Artificial
intelligence, pages 1489?1494. AAAI Press.
T. Zesch, I. Gurevych, and M. Mu?hlha?user. 2007. Com-
paring Wikipedia and German Wordnet by Evaluating
Semantic Relatedness on Multiple Datasets. In Pro-
ceedings of Human Language Technologies: The An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics.
29
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 30?37,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Towards Building a Multilingual Semantic Network:
Identifying Interlingual Links in Wikipedia
Bharath Dandala
Dept. of Computer Science
University of North Texas
Denton, TX
BharathDandala@my.unt.edu
Rada Mihalcea
Dept. of Computer Science
University of North Texas
Denton, TX
rada@cs.unt.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, Ohio
bunescu@ohio.edu
Abstract
Wikipedia is a Web based, freely available
multilingual encyclopedia, constructed in a
collaborative effort by thousands of contribu-
tors. Wikipedia articles on the same topic in
different languages are connected via interlin-
gual (or translational) links. These links serve
as an excellent resource for obtaining lexical
translations, or building multilingual dictio-
naries and semantic networks. As these links
are manually built, many links are missing
or simply wrong. This paper describes a su-
pervised learning method for generating new
links and detecting existing incorrect links.
Since there is no dataset available to evaluate
the resulting interlingual links, we create our
own gold standard by sampling translational
links from four language pairs using distance
heuristics. We manually annotate the sampled
translation links and used them to evaluate the
output of our method for automatic link detec-
tion and correction.
1 Introduction
In recent years, Wikipedia has been used as a re-
source of world knowledge in many natural lan-
guage processing applications. A diverse set of
tasks such as text categorization, information ex-
traction, information retrieval, question answering,
word sense disambiguation, semantic relatedness,
and named entity recognition have been shown to
benefit from the semi-structured text of Wikipedia.
Most approaches that use the world knowledge en-
coded in Wikipedia are statistical in nature and
therefore their performance depends significantly
on the size of Wikipedia. Currently, the English
Wikipedia alone has four million articles. However,
the combined Wikipedias for all other languages
greatly exceed the English Wikipedia in size, yield-
ing a combined total of more than 10 million arti-
cles in more than 280 languages.1 The rich hyper-
link structure of these Wikipedia corpora in different
languages can be very useful in identifying various
relationships between concepts.
Wikipedia articles on the same topic in different
languages are often connected through interlingual
links. These links are the small navigation links
that show up in the ?Languages? sidebar in most
Wikipedia articles, and they connect an article with
related articles in other languages. For instance,
the interlingual links for the Wikipedia article about
?Football? connect it to 20 articles in 20 different
languages. In the ideal case, a set of articles con-
nected directly or indirectly via such links would all
describe the same entity or concept. However, these
links are produced either by polyglot editors or by
automatic bots. Editors commonly make mistakes
by linking articles that have conceptual drift, or by
linking to a concept at a different level of granularity.
For instance, if a corresponding article in one of the
languages does not exist, a similar article or a more
general article about the concept is sometimes linked
instead. Various bots also add new interlingual links
or attempt to correct existing ones. The downside of
a bot is that an error in a translational link created
by editors in Wikipedia for one language propagates
to Wikipedias in other languages. Thus, if a bot in-
troduces a wrong link, one may have to search for
1http://en.wikipedia.org/wiki/Wikipedia:Size of Wikipedia
30
Language Code Articles Redirects Users
English en 4,674,066 4,805,557 16,503,562
French fr 3,298,615 789,408 1,250,266
German de 3,034,238 678,288 1,398,424
Italian it 2,874,747 319,179 731,750
Polish pl 2,598,797 158,956 481,079
Spanish es 2,587,613 504,062 2,162,925
Dutch nl 2,530,250 226,201 446,458
Russian ru 2,300,769 682,402 819,812
Japanese jp 1,737,565 372,909 607,152
Chinese cn 1,199,912 333,436 1,171,148
Table 1: Number of articles, redirects, and users for the top nine Wikipedia editions plus Chinese. The total number
of articles also includes the disambiguation pages.
the underlying error in a different language version
of Wikipedia.
The contributions of the research described in this
paper are two-fold. First, we describe the construc-
tion of a dataset of interlingual links that are auto-
matically sampled from Wikipedia based on a set of
distance heuristics. This dataset is manually anno-
tated in order to enable the evaluation of methods
for translational link detection. Second, we describe
an automatic model for correcting existing links and
creating new links, with the aim of obtaining a more
stable set of interlingual links. The model?s param-
eters are estimated on the manually labeled dataset
using a supervised machine learning approach.
The remaining of this paper is organized as fol-
lows: Section 2 briefly describes Wikipedia and
the relevant terminology. Section 3 introduces our
method of identifying a candidate set of translational
links based on distance heuristics, while Section 4
introduces the methodology for building a manually
annotated dataset. Section 5 describes the machine
learning experiments for detecting or correcting in-
terlingual links. Finally, we present related work in
Section 6, and concluding remarks in Section 7.
2 Wikipedia
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
webpage, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential errors
are quickly corrected within the collaborative envi-
ronment) of this online resource.
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or an
event, and consists of a hypertext document with hy-
perlinks to other pages within or outside Wikipedia.
The role of the hyperlinks is to guide the reader to
pages that provide additional information about the
entities or events mentioned in an article. Articles
are organized into categories, which in turn are or-
ganized into category hierarchies. For instance, the
article automobile is included in the category vehi-
cle, which in turn has a parent category named ma-
chine, and so forth.
Each article in Wikipedia is uniquely referenced
by an identifier, consisting of one or more words
separated by spaces or underscores and occasionally
a parenthetical explanation. For example, the article
for bar with the meaning of ?counter for drinks? has
the unique identifier bar (counter).
Wikipedia editions are available for more than
280 languages, with a number of entries vary-
ing from a few pages to three millions articles or
more per language. Table 1 shows the nine largest
Wikipedias (as of March 2012) and the Chinese
Wikipedia, along with the number of articles and ap-
proximate number of contributors.2
The ten languages mentioned above are also the
languages used in our experiments. Note that Chi-
2http://meta.wikimedia.org/wiki/List of Wikipedias
#Grand Total
31
Relation Exists Via
SYMMETRY
en=Ball de=Ball Yes -
en=Hentriacontane it=Entriacontano No -
TRANSITIVITY
en=Deletion (phonology) fr=Amu??ssement Yes nl=Deletie (taalkunde)
en=Electroplating fr=Galvanoplastie No -
REDIRECTIONS
en=Gun Dog de=Schiesshund Yes de=Jagdhund
en=Ball de=Ball No -
Table 2: Symmetry, transitivity, and redirections in Wikipedia
nese is the twelfth largest Wikipedia, but we decided
to include it at the cost of not covering the tenth
largest Wikipedia (Portuguese), which has close
similarities with other languages already covered
(e.g., French, Italian, Spanish).
Relevant for the work described in this paper are
the interlingual links, which explicitly connect arti-
cles in different languages. For instance, the English
article for bar (unit) is connected, among others, to
the Italian article bar (unita? di misura) and the Pol-
ish article bar (jednostka). On average, about half of
the articles in a Wikipedia version include interlin-
gual links to articles in other languages. The number
of interlingual links per article varies from an aver-
age of five in the English Wikipedia, to ten in the
Spanish Wikipedia, and as many as 23 in the Arabic
Wikipedia.
3 Identifying Interlingual Links in
Wikipedia
The interlingual links connecting Wikipedias in dif-
ferent languages should ideally be symmetric and
transitive. The symmetry property indicates that if
there is an interlingual link A? ? A? between two
articles, one in language ? and one in language ?,
then the reverse link A? ? A? should also exist
in Wikipedia. According to the transitivity property,
the presence of two links A? ? A? and A? ? A?
indicates that the link A? ? A? should also exist
in Wikipedia, where ?, ? and ? are three different
languages. While these properties are intuitive, they
are not always satisfied due to Wikipedia?s editorial
policy that accredits editors with the responsibility
of maintaining the articles. Table 2 shows actual
Link Total number Newly added
type of links links
DL 26,836,572 -
RL 26,836,572 1,277,760
DP2/RP2 25,763,689 853,658
DP3/RP3 23,383,535 693,262
DP4/RP4 21,560,711 548,354
Table 3: Number of links identified in Wikipedia, as di-
rect, symmetric, or transitional links. The number of
newly added links, not known in the previous set of links,
is also indicated (e.g., DP3/RP3 adds 693,262 new links
not found by direct or symmetric links, or by direct or
reverse paths of length two).
cases in Wikipedia where these properties fail due
to missing interlingual links. The table also shows
examples where the editors link an article from one
language to a redirect page in another language.
In order to generate a normalized set of inter-
lingual links between Wikipedias, we replace all the
redirect pages with the corresponding original arti-
cles, so that each concept in a language is repre-
sented by one unique article. We then identify the
following four types of simple interlingual paths be-
tween articles in different languages:
DL: Direct links A? ? A? between two articles.
RL: Reverse links A? ? A? between two articles.
DPk: Direct, simple paths of length k between two
articles.
RPk: Reverse, simple paths of length k between
two articles.
32
Relation Number of paths
DL
en=Ball de=Ball 1
en=Ball it=Palla (sport) 1
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
RL
en=Ball de=Ball 1
en=Ball it=Palla(sport) 1
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
DP2
en=Ball de=Ball 1
en=Ball it=Palla (sport) 2
en=Ball fr=Boule (solide) 1
de=Ball fr=Ballon (sport) 2
DP3
en=Ball de=Ball 1
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 1
de=Ball fr=Ballon (sport) 1
DP4
en=Ball de=Ball 0
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 1
de=Ball fr=Ballon (sport) 0
RP2
en=Ball de=Ball 1
en=Ball it=Palla (sport) 2
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 2
RP3
en=Ball de=Ball 1
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 1
RP4
en=Ball de=Ball 0
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
Table 4: A subset of the direct links, reverse links, and
inferred direct and reverse paths for the graph in Figure 1
en=Ball
de=Ball
it=Palla(sport) fr=Boule(solide)
fr=Ballon(sport)
Figure 1: A small portion of the multilingual Wikipedia
graph.
Figure 1 shows a small portion of the Wikipedia
graph, connecting Wikipedias in four languages:
English, German, Italian, and French. Correspond-
ingly, Table 4 shows a subset of the direct links DL,
reverse links RL, direct translation paths DPk and
reverse translation paths RPk of lengths k = 2, 3, 4
for the graph in the figure.
Using these distance heuristics, we are able to
extract or infer a very large number of interlingual
links. Table 3 shows the number of direct links ex-
tracted from the ten Wikipedias we currently work
with, as well as the number of paths that we add by
enforcing the symmetry and transitivity properties.
4 Manual Evaluation of the Interlingual
Links
The translation links in Wikipedia, whether added
by the Wikipedia editors (direct links), or inferred by
the heuristics described in the previous section, are
not guaranteed for quality. In fact, previous work (de
Melo and Weikum, 2010b) has shown that a large
number of the links created by the Wikipedia users
are incorrect, connecting articles that are not transla-
tions of each other, subsections of articles, or disam-
biguation pages. We have therefore decided to run
a manual annotation study in order to determine the
quality of the interlingual links. The resulting anno-
tation can serve both as a gold standard for evaluat-
ing the quality of predicted links, and as supervision
for a machine learning model that would automati-
cally detect translation links.
33
Language pair 0 1 2 3 4
(English, German) 46 8 29 2 110
(English, Spanish) 22 19 19 13 123
(Italian, French) 30 7 19 7 132
(Spanish, Italian) 21 8 17 13 136
Table 6: Number of annotations on a scale of 0-4 for each
pair of languages
From the large pool of links directly available in
Wikipedia or inferred automatically through sym-
metry and transitivity, we sampled and then man-
ually annotated 195 pairs of articles for each of
four language pairs: (English, German), (English,
Spanish), (Italian, French), and (Spanish, Italian).
The four language pairs were determined based on
the native or near-native knowledge available in the
group of annotators in our research group. The sam-
pling of the article pairs was done such that it cov-
ers all the potentially interesting cases obtained by
combining the heuristics used to identify interlin-
gual links. The left side of Table 5 shows the com-
bination of heuristics used to select the article pairs.
For each such combination, and for each language
pair, we randomly selected 15 articles. Furthermore,
we added 15 randomly selected pairs for the highest
quality combination (Case 1).
For each language pair, the sampled links were
annotated by one human judge, with the exception of
the (English, Spanish) dataset, which was annotated
by two judges so that we could measure the inter-
annotator agreement. The annotators were asked to
check the articles in each link and annotate the link
on a scale from 0 to 4, as follows:
4: Identical concepts that are perfect translations
of each other.
3: Concepts very close in meaning, which are
good translations of each other, but a better
translation for one of the concepts in the pair
also exists. The annotators are not required to
identify a better translation in Wikipedia, they
only have to use their own knowledge of the
language, e.g. ?building? (English) may be a
good translation for ?tore? (Spanish), yet a bet-
ter translation is known to exist.
2: Concepts that are closely related but that are not
translations of each other.
1: Concepts that are remotely related and are not
translations of each other.
0: Completely unrelated concepts or links be-
tween an article and a portion of another arti-
cle.
To determine the quality of the annotations,
we ran an inter-annotator study for the (English-
Spanish) language pair. The two annotators had a
Pearson correlation of 70%, which indicates good
agreement. We also calculated their agreement
when grouping the ratings from 0 to 4 in only two
categories: 0, 1, and 2 were mapped to no transla-
tion, whereas 3 and 4 were mapped to translation.
On this coarse scale, the annotators agreed 84% of
the time, with a kappa value of 0.61, which once
again indicate good agreement.
The annotations are summarized in the right side
of Table 5. For each quality rating, the table shows
the number of links annotated with that rating. Note
that this is a summary over the annotations of five
annotators, corresponding to the four language pairs,
as well as an additional annotation for (English,
Spanish).
Not surprisingly, the links that are ?supported? by
all the heuristics considered (Case 1) are the links
with the highest quality. These are interlingual links
that are present in Wikipedia and that can also be
inferred through transitive path heuristics. Interest-
ingly, links that are only guaranteed to have a direct
link (DL) and no reverse link (RL) (Case 2) have a
rather low quality, with only 68% of the links being
considered to represent a perfect or a good transla-
tion (score of 3 or 4).
Table 6 summarizes the annotations per language
pair. There appear to be some differences in the
quality of interlingual links extracted or inferred for
different languages, with (Spanish, Italian) being the
pair with the highest quality of links (76% of the
links are either perfect or good translations), while
English to German seems to have the lowest quality
(only 57% of the links are perfect or good). For the
(English, Spanish) pair, we used the average of the
two annotators? ratings, rounded up to the nearest
integer.
34
Combinations of heuristics to extract or infer interlingual links Link quality on a 0-4 scale
Cases DL RL DP2 RP2 DP3 RP3 DP4 RP4 Samples 0 1 2 3 4
Case 1 y y y y y y y y 30 6 3 6 6 129
Case 2 y n - - - - - - 15 15 3 6 3 48
Case 3 n y - - - - - - 15 13 3 8 4 47
Case 4 n n y y - - - - 15 6 3 16 4 46
Case 5 n n - - y y - - 15 13 9 12 4 28
Case 6 n n - - - - y y 15 15 8 3 8 37
Case 7 n n n n - - - - 15 19 8 11 5 31
Case 8 n n - - n n - - 15 13 8 11 5 32
Case 9 n n - - - - n n 15 25 4 11 2 33
Case 10 y y n n - - - - 15 6 3 4 3 59
Case 11 y y - - n n - - 15 6 2 3 0 64
Case 12 y y - - - - n n 15 3 6 2 4 60
Table 5: Left side of the table: distance heuristics and number of samples based on each distance heuristic. ?y? indicates
that the corresponding path should exist, ?n? indicates that the corresponding path should not exist, ?-? indicates that
we don?t care whether the corresponding path exists or not. Right side of the table: manual annotations of the quality
of links, on a scale of 0 to 4, with 4 meaning perfect translations.
5 Machine Learning Experiments
The manual annotations described above are good
indicators of the quality of the interlingual links that
can be extracted and inferred in Wikipedia. But such
manual annotations, because of the human effort in-
volved, do not scale up, and therefore we cannot ap-
ply them on the entire interlingual Wikipedia graph
to determine the links that should be preserved or the
ones that should be removed.
Instead, we experiment with training machine
learning models that would automatically determine
the quality of an interlingual link. As features, we
use the presence or absence of direct or symmet-
ric links, along with the number of inferred paths of
length k = 2, 3, 4, as defined in Section 3. Table 7
shows the feature vectors for the same four pairs of
articles that were used in Table 4. The feature val-
ues are computed based on the sample network of
interlingual links from Figure 1. Each feature vector
is assigned a numerical class, corresponding to the
manual annotation provided by the human judges.
We conduct two experiments, at a fine-grained
and a coarse-grained level. In both experiments, we
use all the annotations for all four language pairs to-
gether (i.e., a total of 780 examples), and perform
evaluations in a ten-fold cross validation scenario.
For the fine-grained experiments, we use all five
numerical classes in a linear regression model.3 We
determine the correctness of the predictions on the
test data by calculating the Pearson correlation with
respect to the gold standard. The resulting corre-
lation was measured at 0.461. For comparison, we
also run an experiment where we only keep the pres-
ence or absence of the direct links as a feature (DL).
In this case, the correlation was measured at 0.418,
which is substantially below the correlation obtained
when using all the features. This indicates that the
interlingual links inferred through our heuristics are
indeed useful.
In the coarse-grained experiments, the quality rat-
ings 0, 1, and 2 are mapped to the no translation
label, while ratings 3 and 4 are mapped to the trans-
lation label. We used the Ada Boost classifier with
decision stumps as the binary classification algo-
rithm. When using the entire feature vectors, the
accuracy is measured at 73.97%, whereas the use
of only the direct links results in an accuracy of
69.35%. Similar to the fine-grained linear regres-
sion experiments, these coarse-grained experiments
further validate the utility of the interlingual links
inferred through the transitive path heuristics.
3We use the Weka machine learning toolkit.
35
Concept pair DL RL DP2 DP3 DP4 RP2 RP3 RP4 Class
en=Ball de=Ball 1 1 1 1 0 1 1 0 4
en=Ball it=Palla (sport) 1 1 2 0 0 2 0 0 4
en=Ball fr=Boule (solide) 0 0 1 1 1 0 0 0 1
de=Ball fr=Ballon (sport) 0 0 2 1 0 2 1 0 4
Table 7: Examples of feature vectors generated for four interlingual links, corresponding to the concept pairs listed in
Table 4
6 Related Work
The multilingual nature of Wikipedia has been al-
ready exploited to solve several number of language
processing tasks. A number of projects have used
Wikipedia to build a multilingual semantic knowl-
edge base by using the existing multilingual nature
of Wikipedia. For instance, (Ponzetto and Strube,
2007) derived a large scale taxonomy from the ex-
isting Wikipedia. In related work, (de Melo and
Weikum, 2010a) worked on a similar problem in
which they combined all the existing multilingual
Wikipedias to build a stable, large multilingual tax-
onomy.
The interlingual links have also been used for
cross-lingual information retrieval (Nguyen et al,
2009) or to generate bilingual parallel corpora (Mo-
hammadi and QasemAghaee, 2010). (Ni et al,
2011) used multilingual editions of Wikipedia to
mine topics for the task of cross lingual text clas-
sification, while (Hassan and Mihalcea, 2009) used
Wikipedias in different languages to measure cross-
lingual semantic relatedness between concepts and
texts in different languages. (Bharadwaj et al, 2010)
explored the use of the multilingual links to mine
dictionaries for under-resourced languages. They
developed an iterative approach to construct a par-
allel corpus, using the interlingual links, info boxes,
category pages, and abstracts, which they then be
used to extract a bilingual dictionary. (Navigli and
Ponzetto, 2010) explored the connections that can
be drawn between Wikipedia and WordNet. While
no attempts were made to complete the existing link
structure of Wikipedia, the authors made use of ma-
chine translation to enrich the resource.
The two previous works most closely related to
ours are the systems introduced in (Sorg and Cimi-
ano, 2008) and (de Melo and Weikum, 2010a; de
Melo and Weikum, 2010b). (Sorg and Cimiano,
2008) designed a system that predicts new interlin-
gual links by using a classification based approach.
They extract certain types of links from bilingual
Wikipedias, which are then used to create a set of
features for the machine learning system. In follow-
up work, (Erdmann et al, 2008; Erdmann et al,
2009) used an expanded set of features, which also
accounted for direct links, redirects, and links be-
tween articles in Wikipedia, to identify entries for a
bilingual dictionary. In this line of work, the focus is
mainly on article content analysis, as a way to detect
new potential translations, rather than link analysis
as done in our work.
Finally, (de Melo and Weikum, 2010b) designed
a system that detects errors in the existing interlin-
gual links in Wikipedia. They show that there are a
large number of links that are imprecise or wrong,
and propose the use of a weighted graph to produce
a more consistent set of consistent interlingual links.
Their work is focusing primarily on correcting ex-
isting links in Wikipedia, rather than inferring new
links as we do.
7 Conclusions
In this paper, we explored the identification of trans-
lational links in Wikipedia. By using a set of heuris-
tics that extract and infer links between Wikipedias
in different languages, along with a machine learn-
ing algorithm that builds upon these heuristics to
determine the quality of the interlingual links, we
showed that we can both correct existing transla-
tional links in Wikipedia as well as discover new
interlingual links. Additionally, we have also con-
structed a manually annotated dataset of interlingual
links, covering different types of links in four pairs
of languages, which can serve as a gold standard for
evaluating the quality of predicted links, and as su-
pervision for the machine learning model.
36
In future work, we plan to experiment with ad-
ditional features to enhance the performance of the
classifier. In particular, we would like to also include
content-based features, such as content overlap and
interlinking.
The collection of interlingual links for the ten
Wikipedias considered in this work, as well as the
manually annotated dataset are publicly available at
http://lit.csci.unt.edu.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation IIS
awards #1018613 and #1018590 and CAREER
award #0747340. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the National Science Foundation.
References
G.R. Bharadwaj, N. Tandon, and V. Varma. 2010.
An iterative approach to extract dictionaries from
Wikipedia for under-resourced languages. Kharagpur,
India.
G. de Melo and G. Weikum. 2010a. MENTA: induc-
ing multilingual taxonomies from Wikipedia. In Pro-
ceedings of the 19th ACM international conference on
Information and knowledge management, pages 1099?
1108, New York, NY, USA. ACM.
G. de Melo and G. Weikum. 2010b. Untangling the
cross-lingual link structure of Wikipedia. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 844?853, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from Wikipedia. In Proceedings of the 13th In-
ternational Conference on Database Systems for Ad-
vanced Applications.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2009. Improving the extraction of bilingual termi-
nology from Wikipedia. ACM Transactions on Multi-
media Computing, Communications and Applications,
5(4):31:1?31:17.
S. Hassan and R. Mihalcea. 2009. Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Suntec, Sin-
gapore.
M. Mohammadi and N. QasemAghaee. 2010. Build-
ing bilingual parallel corpora based on Wikipedia. In-
ternational Conference on Computer Engineering and
Applications, 2:264?268.
R. Navigli and S. Ponzetto. 2010. Babelnet: Building a
very large multilingual semantic network. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden.
D. Nguyen, A. Overwijk, C. Hauff, D. Trieschnigg,
D. Hiemstra, and F. De Jong. 2009. WikiTrans-
late: query translation for cross-lingual information re-
trieval using only Wikipedia. In Proceedings of the
9th Cross-language evaluation forum conference on
Evaluating systems for multilingual and multimodal
information access, pages 58?65, Berlin, Heidelberg.
Springer-Verlag.
X. Ni, J. Sun, J. Hu, and Z. Chen. 2011. Cross lingual
text classification by mining multilingual topics from
Wikipedia. In Proceedings of the fourth ACM inter-
national conference on Web search and data mining,
pages 375?384, New York, NY, USA. ACM.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
national conference on Artificial intelligence - Volume
2, pages 1440?1445. AAAI Press.
P. Sorg and P. Cimiano. 2008. Enriching the crosslingual
link structure of Wikipedia - a classification-based ap-
proach. In Proceedings of the AAAI 2008 Workshop
on Wikipedia and Artificial Intelligence.
37
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 1: English Lexical Simplification
Lucia Specia
Department of Computer Science
University of Sheffield
L.Specia@sheffield.ac.uk
Sujay Kumar Jauhar
Research Group in Computational Linguistics
University of Wolverhampton
Sujay.KumarJauhar@wlv.ac.uk
Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu
Abstract
We describe the English Lexical Simplifica-
tion task at SemEval-2012. This is the first
time such a shared task has been organized
and its goal is to provide a framework for the
evaluation of systems for lexical simplification
and foster research on context-aware lexical
simplification approaches. The task requires
that annotators and systems rank a number of
alternative substitutes ? all deemed adequate ?
for a target word in context, according to how
?simple? these substitutes are. The notion of
simplicity is biased towards non-native speak-
ers of English. Out of nine participating sys-
tems, the best scoring ones combine context-
dependent and context-independent informa-
tion, with the strongest individual contribution
given by the frequency of the substitute re-
gardless of its context.
1 Introduction
Lexical Simplification is a subtask of Text Simpli-
fication (Siddharthan, 2006) concerned with replac-
ing words or short phrases by simpler variants in a
context aware fashion (generally synonyms), which
can be understood by a wider range of readers. It
generally envisages a certain human target audience
that may find it difficult or impossible to understand
complex words or phrases, e.g., children, people
with poor literacy levels or cognitive disabilities, or
second language learners. It is similar in many re-
spects to the task of Lexical Substitution (McCarthy
and Navigli, 2007) in that it involves determining
adequate substitutes in context, but in this case on
the basis of a predefined criterion: simplicity.
A common pipeline for a Lexical Simplification
system includes at least three major components: (i)
complexity analysis: selection of words or phrases
in a text that are considered complex for the reader
and/or task at hand; (ii) substitute lookup: search
for adequate replacement words or phrases deemed
complex in context, e.g., taking synonyms (with
the same sense) from a thesaurus or finding similar
words/phrases in a corpus using distributional simi-
larity metrics; and (iii) context-based ranking: rank-
ing of substitutes according to how simple they are
to the reader/task at hand.
As an example take the sentence: ?Hitler com-
mitted terrible atrocities during the second World
War.? The system would first identify complex
words, e.g. atrocities, then search for substitutes
that might adequately replace it. A thesaurus lookup
would yield the following synonyms: abomination,
cruelty, enormity and violation, but enormity should
be dropped as it does not fit the context appropri-
ately. Finally, the system would determine the sim-
plest of these substitutes, e.g., cruelty, and use it
to replace the complex word, yielding the sentence:
?Hitler committed terrible cruelties during the sec-
ond World War.?.
Different from other subtasks of Text Simplifica-
tion like Syntactic Simplification, which have been
relatively well studied, Lexical Simplification has
received less attention. Although a few recent at-
tempts explicitly address dependency on context (de
Belder et al, 2010; Yatskar et al, 2010; Biran et al,
2011; Specia, 2010), most approaches are context-
independent (Candido et al, 2009; Devlin and Tait,
1998). In addition, a general deeper understanding
347
of the problem is yet to be gained. As a first attempt
to address this problem in the shape of a shared task,
the English Simplification task at SemEval-2012 fo-
cuses on the third component, which we believe is
the core of the Lexical Simplification problem.
The SemEval-2012 shared task on English Lexi-
cal Simplification has been conceived with the fol-
lowing main purposes: advancing the state-of-the-
art Lexical Simplification approaches, and provid-
ing a common framework for evaluation of Lexical
Simplification systems for participants and other re-
searchers interested in the field. Another central mo-
tive of such a shared task is to bring awareness to the
general vagueness associated with the notion of lex-
ical simplicity. Our hypothesis is that in addition to
the notion of a target application/reader, the notion
of simplicity is highly context-dependent. In other
words, given the same list of substitutes for a given
target word with the same sense, we expect different
orderings of these substitutes in different contexts.
We hope that participation in this shared task will
help discover some underlying traits of lexical sim-
plicity and furthermore shed some light on how this
may be leveraged in future work.
2 Task definition
Given a short context, a target word in English,
and several substitutes for the target word that are
deemed adequate for that context, the goal of the
English Simplification task at SemEval-2012 is to
rank these substitutes according to how ?simple?
they are, allowing ties. Simple words/phrases are
loosely defined as those which can be understood by
a wide range of people, including those with low lit-
eracy levels or some cognitive disability, children,
and non-native speakers of English. In particular,
the data provided as part of the task is annotated by
fluent but non-native speakers of English.
The task thus essentially involves comparing
words or phrases and determining their order of
complexity. By ranking the candidates, as opposed
to categorizing them into specific labels (simple,
moderate, complex, etc.), we avoid the need for a
fixed number of categories and for more subjective
judgments. Also ranking enables a more natural and
intuitive way for humans (and systems) to perform
annotations by preventing them from treating each
individual case in isolation, as opposed to relative
to each other. However, the inherent subjectivity
introduced by ranking entails higher disagreement
among human annotators, and more complexity for
systems to tackle.
3 Corpus compilation
The trial and test corpora were created from the cor-
pus of SemEval-2007 shared task on Lexical Sub-
stitution (McCarthy and Navigli, 2007). This de-
cision was motivated by the similarity between the
two tasks. Moreover the existing corpus provided an
adequate solution given time and cost constraints for
our corpus creation. Given existing contexts with the
original target word replaced by a placeholder and
the lists of substitutes (including the target word),
annotators (and systems) are required to rank substi-
tutes in order of simplicity for each context.
3.1 SemEval-2007 - LS corpus
The corpus from the shared task on Lexical Substi-
tution (LS) at SemEval-2007 is a selection of sen-
tences, or contexts, extracted from the English Inter-
net Corpus of English (Sharoff, 2006). It contains
samples of English texts crawled from the web.
This selection makes up the dataset of a total of
2, 010 contexts which are divided into Trial and Test
sets, consisting of 300 and 1710 contexts respec-
tively. It covers a total of 201 (mostly polysemous)
target words, including nouns, verbs, adjectives and
adverbs, and each of the target words is shown in
10 different contexts. Annotators had been asked to
suggest up to three different substitutes (words or
short phrases) for each of the target words within
their contexts. The substitutes were lemmatized un-
less it was deemed that the lemmatization would al-
ter the meaning of the substitute. Annotators were
all native English speakers and each annotated the
entire dataset. Here is an example of a context for
the target word ?bright?:
<lexelt item="bright.a">
<instance id="1">
<context>During the siege, George
Robertson had appointed Shuja-ul-Mulk,
who was a <head>bright</head> boy
only 12 years old and the youngest surviv-
ing son of Aman-ul-Mulk, as the ruler of
Chitral.</context>
348
</instance> ... </lexelt>
The gold-standard document contains each target
word along with a ranked list of its possible substi-
tutes, e.g., for the context above, three annotators
suggested ?intelligent? and ?clever? as substitutes
for ?bright?, while only one annotator came up with
?smart?:
bright.a 1:: intelligent 3; clever 3; smart 1;
3.2 SemEval-2012 Lexical Simplification
corpus
Given the list of contexts and each respective list
of substitutes we asked annotators to rank substi-
tutes for each individual context in ascending order
of complexity. Since the notion of textual simplic-
ity varies from individual to individual, we carefully
chose a group of annotators in an attempt to cap-
ture as much of a common notion of simplicity as
possible. For practical reasons, we selected annota-
tors with high proficiency levels in English as sec-
ond language learners - all with a university first de-
gree in different subjects.
The Trial dataset was annotated by four people
while the Test dataset was annotated by five peo-
ple. In both cases each annotator tagged the com-
plete dataset.
Inter-annotator agreement was computed using an
adaptation of the kappa index with pairwise rank
comparisons (Callison-Burch et al, 2011). This is
also the primary evaluation metric for participating
systems in the shared task, and it is covered in more
detail in Section 4.
The inter-annotator agreement was computed for
each pair of annotators and averaged over all possi-
ble pairs for a final agreement score. On the Trial
dataset, a kappa index of 0.386 was found, while
for the Test dataset, a kappa index of 0.398 was
found. It may be noted that certain annotators dis-
agreed considerably with all others. For example,
on the Test set, if annotations from one judge are re-
moved, the average inter-annotator agreement rises
to 0.443. While these scores are apparently low, the
highly subjective nature of the annotation task must
be taken into account. According to the reference
values for other tasks, this level of agreement is con-
sidered ?moderate? (Callison-Burch et al, 2011).
It is interesting to note that higher inter-annotator
agreement scores were achieved between annota-
tors with similar language and/or educational back-
grounds. The highest of any pairwise annotator
agreement (0.52) was achieved between annotators
of identical language and educational background,
as well as very similar levels of English proficiency.
High agreement scores were also achieved between
annotators with first languages belonging to the
same language family.
Finally, it is also worth noticing that this agree-
ment metric is highly sensitive to small differences
in annotation, thus leading to overly pessimistic
scores. A brief analysis reveals that annotators often
agree on clusters of simplicity and the source of the
disagreement comes from the rankings within these
clusters.
Finally, the gold-standard annotations for the
Trial and Test datasets ? against which systems are
to be evaluated ? were generated by averaging the
annotations from all annotators. This was done
context by context where each substitution was at-
tributed a score based upon the average of the rank-
ings it was ascribed. The substitutions were then
sorted in ascending order of scores, i.e., lowest score
(highest average ranking) first. Tied scores were
grouped together to form a single rank. For exam-
ple, assume that for a certain context, four annota-
tors provided rankings as given below, where multi-
ple candidates between {} indicate ties:
Annotator 1: {clear} {light} {bright} {lumi-
nous} {well-lit}
Annotator 2: {well-lit} {clear} {light}
{bright} {luminous}
Annotator 3: {clear} {bright} {light} {lumi-
nous} {well-lit}
Annotator 4: {bright} {well-lit} {luminous}
{clear} {light}
Thus the word ?clear?, having been ranked 1st,
2nd, 1st and 4th by each of the annotators respec-
tively is given an averaged ranking score of 2. Sim-
ilarly ?light? = 3.25, ?bright? = 2.5, ?luminous? =
4 and ?well-lit? = 3.25. Consequently the gold-
standard ranking for this context is:
Gold: {clear} {bright} {light, well-lit} {lumi-
nous}
349
3.3 Context-dependency
As mentioned in Section 1, one of our hypothe-
ses was that the notion of simplicity is context-
dependent. In other words, that the ordering of sub-
stitutes for different occurrences of a target word
with a given sense is highly dependent on the con-
texts in which such a target word appears. In order
to verify this hypothesis quantitatively, we further
analyzed the gold-standard annotations of the Trial
and Test datasets. We assume that identical lists of
substitutes for different occurrences of a given tar-
get word ensure that such a target word has the same
sense in all these occurrences. For every target word,
we then generate all pairs of contexts containing the
exact same initial list of substitutes and check the
proportion of these contexts for which human an-
notators ranked the substitutes differently. We also
check for cases where only the top-ranked substitute
is different. The numbers obtained are shown in Ta-
ble 1.
Trial Test
1) # context pairs 1350 7695
2) # 1) with same list 60 242
3) # 2) with different rankings 24 139
4) # 2) with different top substitute 19 38
Table 1: Analysis on the context-dependency of the no-
tion of simplicity.
Although the proportion of pairs of contexts with
the same list of substitutes is very low (less than
5%), it is likely that there are many other occur-
rences of a target word with the same sense and
slightly different lists of substitutes. Further man-
ual inspection is necessary to determine the actual
numbers. Nevertheless, from the observed sample
it is possible to conclude that humans will, in fact,
rank the same set of words (with the same sense)
differently depending on the context (on an average
in 40-57% of the instances).
4 Evaluation metric
No standard metric has yet been defined for eval-
uating Lexical Simplification systems. Evaluating
such systems is a challenging problem due to the
aforementioned subjectivity of the task. Since this
is a ranking task, rank correlation metrics are desir-
able. However, metrics such as Spearman?s Rank
Correlation are not reliable on the limited number of
data points available for comparison on each rank-
ing (note that the nature of the problem enforces a
context-by-context ranking, as opposed to a global
score), Other metrics for localized, pairwise rank
correlation, such as Kendall?s Tau, disregard ties, ?
which are important for our purposes ? and are thus
not suitable.
The main evaluation metric proposed for this
shared task is in fact a measure of inter-annotator
agreement, which is used for both contrasting two
human annotators (Section 3.2) and contrasting a
system output to the average of human annotations
that together forms the gold-standard.
Out metric is based on the kappa index (Cohen,
1960) which in spite of many criticisms is widely
used for its simplicity and adaptability for different
applications. The generalized form of the kappa in-
dex is
? =
P (A)? P (E)
1? P (E)
where P (A) denotes the proportion of times two
annotators agree and P (E) gives the probability of
agreement by chance between them.
In order to apply the kappa index for a ranking
task, we follow the method proposed by (Callison-
Burch et al, 2011) for measuring agreement over
judgments of translation quality. This method de-
fines P (A) and P (E) in such a way that it now
counts agreement whenever annotators concur upon
the order of pairwise ranks. Thus, if one annotator
ranked two given words 1 and 3, and the second an-
notator ranked them 3 and 7 respectively, they are
still in agreement. Formally, assume that two anno-
tators A1 and A2 rank two instance a and b. Then
P (A) = the proportion of times A1 and A2 agree
on a ranking, where an occurrence of agreement is
counted whenever rank(a < b) or rank(a = b) or
rank(a > b).
P (E) (the likelihood that annotators A1 and A2
agree by chance) is based upon the probability that
both of them assign the same ranking order to a and
b. Given that the probability of getting rank(a <
b) by any annotator is P (a < b), the probability
that both annotators get rank(a < b) is P (a < b)2
(agreement is achieved when A1 assigns a < b by
chance and A2 also assigns a < b). Similarly, the
350
probability of chance agreement for rank(a = b)
and rank(a > b) are P (a = b)2 and P (a > b)2
respectively. Thus:
P (E) = P (a < b)2 + P (a = b)2 + P (a > b)2
However, the counts of rank(a < b) and
rank(a > b) are inextricably linked, since for any
particular case of a1 < b1, it follows that b1 >
a1, and thus the two counts must be incremented
equally. Therefore, over the entire space of ranked
pairs, the probabilities remain exactly the same. In
essence, after counting for P (a = b), the remaining
probability mass is equally split between P (a < b)
and P (a > b). Therefore:
P (a < b) = P (a > b) =
1? P (a = b)
2
Kappa is calculated for every pair of ranked items
for a given context, and then averaged to get an over-
all kappa score:
? =
|N |?
n=1
Pn(A)? Pn(E)
1? Pn(E)
|N |
where N is the total number of contexts, and Pn(A)
and Pn(E) are calculated based on counts extracted
from the data on the particular context n.
The functioning of this evaluation metric is illus-
trated by the following example:
Context: During the siege, George Robert-
son had appointed Shuja-ul-Mulk, who was a
_____ boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
Gold: {intelligent} {clever} {smart} {bright}
System: {intelligent} {bright} {clever,
smart}
Out of the 6 distinct unordered pairs of lexical
items, system and gold agreed 3 times. Conse-
quently, Pn(A) = 36 . In addition, count(a =
b) = 1. Thus, Pn(a = b) = 112 . Which gives a
P (E) = 4196 and the final kappa score for this partic-
ular context of 0.13.
The statistical significance of the results from two
systems A and B is measured using the method
of Approximate Randomization, which has been
shown to be a robust approach for several NLP tasks
(Noreen, 1989). The randomization is run 1, 000
times and if the p-value is ? 0.05 the difference be-
tween systems A and B is asserted as being statisti-
cally significance.
5 Baselines
We defined three baseline lexical simplification sys-
tems for this task, as follows.
L-Sub Gold: This baseline uses the gold-standard
annotations from the Lexical Substitution cor-
pus of SemEval-2007 as is. In other words, the
ranking is based on the goodness of fit of sub-
stitutes for a context, as judged by human anno-
tators. This method also serves to show that the
Lexical Substitution and Lexical Simplification
tasks are indeed different.
Random: This baseline provides a randomized or-
der of the substitutes for every context. The
process of randomization is such that is allows
the occurrence of ties.
Simple Freq.: This simple frequency baseline uses
the frequency of the substitutes as extracted
from the Google Web 1T Corpus (Brants and
Franz, 2006) to rank candidate substitutes
within each context.
The results in Table 2 show that the ?L-Sub Gold?
and ?Random? baselines perform very poorly on
both Trial and Test sets. In particular, the reason for
the poor scores for ?L-Sub Gold? can be attributed
to the fact that it yields many ties, whereas the gold-
standard presents almost no ties. Our kappa met-
ric tends to penalize system outputs with too many
ties, since the probability of agreement by chance is
primarily computed on the basis of the number of
ties present in the two rankings being compared (see
Section 4).
The ?Simple Freq.? baseline, on the other hand,
performs very strongly, in spite of its simplistic ap-
proach, which is entirely agnostic to context. In fact
it surpasses the average inter-annotator agreement
on both Trial and Test datasets. Indeed, the scores on
the Test set approach the best inter-annotator agree-
ment scores between any two annotators.
351
Trial Test
L-Sub Gold 0.050 0.106
Random 0.016 0.012
Simple Freq. 0.397 0.471
Table 2: Baseline kappa scores on trial and test sets
6 Results and Discussion
6.1 Participants
Five sites submitted one or more systems to the task,
totaling nine systems:
ANNLOR-lmbing: This system (Ligozat et al,
2012) relies on language models probabili-
ties, and builds on the principle of the Sim-
ple Frequency baseline. While the baseline
uses Google n-grams to rank substitutes, this
approach uses Microsoft Web n-grams in the
same way. Additionally characteristics, such
as the contexts of each term to be substituted,
were integrated into the system. Microsoft Web
N-gram Service was used to obtain log likeli-
hood probabilities for text units, composed of
the lexical item and 4 words to the left and right
from the surrounding context.
ANNLOR-simple: The system (Ligozat et al,
2012) is based on Simple English Wikipedia
frequencies, with the motivation that the lan-
guage used in this version of Wikipedia is
targeted towards people who are not first-
language English speakers. Word n-grams (n =
1-3) and their frequencies were extracted from
this corpus using the Text-NSP Perl module
and a ranking of the possible substitutes of a
target word according to these frequencies in
descending order was produced.
EMNLPCPH-ORD1: The system performs a se-
ries of pairwise comparisons between candi-
dates. A binary classifier is learned purpose
using the Trial dataset and artificial unlabeled
data extracted based on Wordnet and a corpus
in a semi-supervised fashion. A co-training
procedure that lets each classifier increase the
other classifier?s training set with selected in-
stances from the unlabeled dataset is used. The
features include word and character n-gram
probabilities of candidates and contexts using
web corpora, distributional differences of can-
didate in a corpus of ?easy? sentences and a
corpus of normal sentences, syntactic complex-
ity of documents that are similar to the given
context, candidate length, and letter-wise rec-
ognizability of candidate as measured by a tri-
gram LM. The first feature sets for co-training
combines the syntactic complexity, character
trigram LM and basic word length features, re-
sulting in 29 features against the remaining 21.
EMNLPCPH-ORD2: This is a variant of the
EMNLPCPH-ORD1 system where the first fea-
ture set pools all syntactic complexity fea-
tures and Wikipedia-based features (28 fea-
tures) against all the remaining 22 features in
the second group.
SB-mmSystem: The approach (Amoia and Ro-
manelli, 2012) builds on the baseline defini-
tion of simplicity using word frequencies but
attempt at defining a more linguistically mo-
tivated notion of simplicity based on lexical
semantics considerations. It adopts different
strategies depending on the syntactic complex-
ity of the substitute. For one-word substitutes
or common collocations, the system uses its
frequency from Wordnet as a metric. In the
case of multi-words substitutes the system uses
?relevance? rules that apply (de)compositional
semantic criteria and attempts to identify a
unique content word in the substitute that might
better approximate the whole expression. The
expression is then assigned the frequency asso-
ciated to this content word for the ranking. Af-
ter POS tagging and sense disambiguating all
substitutes, hand-written rules are used to de-
compose the meaning of a complex phrase and
identify the most relevant word conveying the
semantics of the whole.
UNT-SimpRank: The system (Sinha, 2012) uses
external resources, including the Simple En-
glish Wikipedia corpus, a set of Spoken En-
glish dialogues, transcribed into machine read-
able form, WordNet, and unigram frequencies
(Google Web1T data). SimpRank scores each
substitute by a sum of its unigram frequency, its
352
frequency in the Simple English Wikipedia, its
frequency in the spoken corpus, the inverse of
its length, and the number of senses the sub-
stitute has in WordNet. For a given context,
the substitutes are then reverse-ranked based on
their simplicity scores.
UNT-SimpRankLight: This is a variant of Sim-
pRank which does not use unigram frequen-
cies. The goal of this system is to check
whether a memory and time-intensive and non-
free resource such as the Web1T corpus makes
a difference over other free and lightweight re-
sources.
UNT-SaLSA: The only resource SaLSA depends
on is the Web1T data, and in particular only
3-grams from this corpus. It leverages the con-
text provided with the dataset by replacing the
target placeholder one by one with each of the
substitutes and their inflections thus building
sets of 3-grams for each substitute in a given
instance. The score of any substitute is then the
sum of the 3-gram frequencies of all the gener-
ated 3-grams for that substitute.
UOW-SHEF-SimpLex: The system (Jauhar and
Specia, 2012) uses a linear weighted ranking
function composed of three features to pro-
duce a ranking. These include a context sen-
sitive n-gram frequency model, a bag-of-words
model and a feature composed of simplicity
oriented psycholinguistic features. These three
features are combined using an SVM ranker
that is trained and tuned on the Trial dataset.
6.2 Pairwise kappa
The official task results and the ranking of the sys-
tems are shown in Table 3.
Firstly, it is worthwhile to note that all the top
ranking systems include features that use frequency
as a surrogate measure for lexical simplicity. This
indicates a very high correlation between distribu-
tional frequency of a given word and its perceived
complexity level. Additionally, the top two systems
involve context-dependent and context-independent
features, thus supporting our hypothesis of the com-
posite nature of the lexical simplification problem.
Rank Team - System Kappa
1 UOW-SHEF-SimpLex 0.496
2
UNT-SimpRank 0.471
Baseline-Simple Freq. 0.471
ANNLOR-simple 0.465
3 UNT-SimpRankL 0.449
4 EMNLPCPH-ORD1 0.405
5 EMNLPCPH-ORD2 0.393
6 SB-mmSystem 0.289
7 ANNLOR-lmbing 0.199
8 Baseline-L-Sub Gold 0.106
9 Baseline-Random 0.013
10 UNT-SaLSA -0.082
Table 3: Official results and ranking according to the pair-
wise kappa metric. Systems are ranked together when the
difference in their kappa score is not statistically signifi-
cant.
Few of the systems opted to use some form of
supervised learning for the task, due to the limited
number of training examples given. As pointed out
by some participants who checked learning curves
for their systems, the performance is likely to im-
prove with larger training sets. Without enough
training data, context agnostic approaches such as
the ?Simple Freq.? baseline become very hard to
beat.
We speculate that the reason why the effects of
context-aware approaches are somewhat mitigated is
because of the isolated setup of the shared task. In
practice, humans produce language at an even level
of complexity, i.e. consistently simple, or consis-
tently complex. In the shared task?s setup, systems
are expected to simplify a single target word in a
context, ignoring the possibility that sometimes sim-
ple words may not be contextually associated with
complex surrounding words. This not only explains
why context-aware approaches are less successful
than was originally expected, but also gives a reason
for the good performance of context-agnostic sys-
tems.
6.3 Recall and top-rank
As previously noted, the primary evaluation met-
ric is very susceptible to penalize slight changes,
making it overly pessimistic about systems? perfor-
mance. Hence, while it may be an efficient way to
compare and rank systems within the framework of
353
a shared task, it may be unnecessarily devaluing the
practical viability of approaches. We performed two
post hoc evaluations that assess system output from
a practical point of view. We check how well the
top-ranked substitute, i.e., the simplest substitute ac-
cording to a given system (which is most likely to
be used in a real simplification task) compares to the
top-ranked candidate from the gold standard. This is
reported in the TRnk column of Table 4: the percent-
age of contexts in which the intersection between the
simplest substitute set from a system?s output and
the gold standard contained at least one element.
We note that while ties are virtually inexistent in the
gold standard data, ties in the system output can af-
fect this metric: a system that naively predicts all
substitutes as the simplest (i.e., a single tie includ-
ing all candidates) will score 100% in this metric.
We also measured the ?recall-at-n" values for 1 ?
n ? 3, which gives the ratio of candidates from the
top n substitute sets to those from the gold-standard.
For a given n, we only consider contexts that have
at least n+1 candidates in the gold-standard (so that
there is some ranking to be done). Table 4 shows the
results of this additional analysis.
Team - System TRnk n=1 n=2 n=3
UOW-SHEF-SimpLex 0.602 0.575 0.689 0.769
UNT-SimpRank 0.585 0.559 0.681 0.760
Baseline-Simple Freq. 0.585 0.559 0.681 0.760
ANNLOR-simple 0.564 0.538 0.674 0.768
UNT-SimpRankL 0.567 0.541 0.674 0.753
EMNLPCPH-ORD1 0.539 0.513 0.645 0.727
EMNLPCPH-ORD2 0.530 0.503 0.637 0.722
SB-mmSystem 0.477 0.452 0.632 0.748
ANNLOR-lmbing 0.336 0.316 0.494 0.647
Baseline-L-Sub Gold 0.454 0.427 0.667 0.959
Baseline-Random 0.340 0.321 0.612 0.825
UNT-SaLSA 0.146 0.137 0.364 0.532
Table 4: Additional results according to the top-rank
(TRnk) and recall-at-n metrics.
These evaluation metrics favour systems that pro-
duce many ties. Consequently the baselines ?L-Sub
Gold" and ?Random" yield overly high scores for
recall-at-n for n=2 and n= 3. Nevertheless the rest
of the results are by and large consistent with the
rankings from the kappa metric.
The results for recall-at-2, e.g., show that most
systems, on average 70% of the time, are able to
find the simplest 2 substitute sets that correspond
to the gold standard. This indicates that most ap-
proaches are reasonably good at distinguishing very
simple substitutes from very complex ones, and that
the top few substitutes will most often produce ef-
fective simplifications.
These results correspond to our experience from
the comparison of human annotators, who are easily
able to form clusters of simplicity with high agree-
ment, but who strongly disagree (based on personal
biases towards perceptions of lexical simplicity) on
the internal rankings of these clusters.
7 Conclusions
We have presented the organization and findings of
the first English Lexical Simplification shared task.
This was a first attempt at garnering interest in the
NLP community for research focused on the lexical
aspects of Text Simplification.
Our analysis has shown that there is a very strong
relation between distributional frequency of words
and their perceived simplicity. The best systems on
the shared task were those that relied on this asso-
ciation, and integrated both context-dependent and
context-independent features. Further analysis re-
vealed that while context-dependent features are im-
portant in principle, their applied efficacy is some-
what lessened due to the setup of the shared task,
which treats simplification as an isolated problem.
Future work would involve evaluating the im-
portance of context for lexical simplification in the
scope of a simultaneous simplification to all the
words in a context. In addition, the annotation of
the gold-standard datasets could be re-done taking
into consideration some of the features that are now
known to have clearly influenced the large variance
observed in the rankings of different annotators,
such as their background language and the educa-
tion level. One option would be to select annotators
that conform a specific instantiation of these fea-
tures. This should result in a higher inter-annotator
agreement and hence a simpler task for simplifica-
tion systems.
Acknowledgments
We would like to thank the annotators for their hard
work in delivering the corpus on time.
354
References
Marilisa Amoia and Massimo Romanelli. 2012. SB-
mmSystem: Using Decompositional Semantics for
Lexical Simplification. In English Lexical Simplifica-
tion. Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal,
Canada.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496?501,
Portland, Oregon.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland.
Arnaldo Candido, Jr., Erick Maziero, Caroline Gasperin,
Thiago A. S. Pardo, Lucia Specia, and Sandra M.
Aluisio. 2009. Supporting the adaptation of texts for
poor literacy readers: a text simplification editor for
Brazilian Portuguese. In Proceedings of the Fourth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 34?42, Boulder, Col-
orado.
J Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46, April.
Jan de Belder, Koen Deschacht, and Marie-Francine
Moens. 2010. Lexical simplification. In Proceedings
of Itec2010: 1st International Conference on Inter-
disciplinary Research on Technology, Education and
Communication, Kortrijk, Belgium.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text for
aphasic readers. Linguistic Databases, pages 161?
173.
Sujay Kumar Jauhar and Lucia Specia. 2012. UOW-
SHEF: SimpLex - Lexical Simplicity Ranking based
on Contextual and Psycholinguistic Features. In En-
glish Lexical Simplification. Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Montreal, Canada.
Anne-Laure Ligozat, Cyril Grouin, Anne Garcia-
Fernandez, and Delphine Bernhard. 2012. ANNLOR:
A Naive Notation-system for Lexical Outputs Rank-
ing. In English Lexical Simplification. Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Montreal, Canada.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48?53.
E. Noreen. 1989. Computer-intensive methods for test-
ing hypotheses. New York: Wiley.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
Advaith Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Computa-
tion, 4:77?109.
Ravi Sinha. 2012. UNT-SimpRank: Systems for Lex-
ical Simplification Ranking. In English Lexical Sim-
plification. Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Lucia Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th international
conference on Computational Processing of the Por-
tuguese Language, PROPOR?10, pages 30?39, Berlin,
Heidelberg. Springer-Verlag.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California.
355
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635?642,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNT: A Supervised Synergistic Approach
to Semantic Text Similarity
Carmen Banea, Samer Hassan, Michael Mohler, Rada Mihalcea
University of North Texas
Denton, TX, USA
{CarmenBanea,SamerHassan,MichaelMohler}@my.unt.edu, rada@cs.unt.edu
Abstract
This paper presents the systems that we par-
ticipated with in the Semantic Text Similar-
ity task at SEMEVAL 2012. Based on prior
research in semantic similarity and related-
ness, we combine various methods in a ma-
chine learning framework. The three varia-
tions submitted during the task evaluation pe-
riod ranked number 5, 9 and 14 among the 89
participating systems. Our evaluations show
that corpus-based methods display a more ro-
bust behavior on the training data, yet com-
bining a variety of methods allows a learning
algorithm to achieve a superior decision than
that achievable by any of the individual parts.
1 Introduction
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vector-
space model used in information retrieval, where the
document most relevant to an input query is deter-
mined by ranking documents in a collection in re-
versed order of their similarity to the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and more recently for
extractive summarization (Salton et al, 1997), and
methods for automatic evaluation of machine trans-
lation (Papineni et al, 2002) or text summarization
(Lin and Hovy, 2003). Measures of text similarity
were also found useful for the evaluation of text co-
herence (Lapata and Barzilay, 2005).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stop-word removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is
an obvious similarity between the text segments I
own a dog and I have an animal, but most of the
current text similarity metrics will fail in identifying
any kind of connection between these texts.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus measures
such as Latent Semantic Analysis (Landauer et al,
1997), Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), or Salient Semantic Analysis
(Hassan and Mihalcea, 2011).
In this paper, we describe the system with which
635
we participated in the SEMEVAL 2012 task on se-
mantic text similarity (Agirre et al, 2012). The sys-
tem builds upon our earlier work on corpus-based
and knowledge-based methods of text semantic sim-
ilarity (Mihalcea et al, 2006; Hassan and Mihal-
cea, 2011; Mohler et al, 2011), and combines all
these previous methods into a meta-system by us-
ing machine learning. The framework provided by
the task organizers also enabled us to perform an in-
depth analysis of the various components used in our
system, and draw conclusions concerning the role
played by the different resources, features, and al-
gorithms in building a state-of-the-art semantic text
similarity system.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
The system we proposed for the SEMEVAL 2012
Semantic Textual Similarity task builds upon both
knowledge- and corpus-based methods previously
described in (Mihalcea et al, 2006; Hassan and Mi-
halcea, 2011; Mohler et al, 2011). The predictions
of these independent systems, paired with additional
salient features, are leveraged by a meta-system that
employs machine learning. In this section, we will
elaborate further on the resources we use, our fea-
tures, and the components of our machine learning
system. We will start by describing the task setup.
3.1 Task Setup
The training data released by the task organiz-
ers consists of three datasets showcasing two sen-
tences per line and a manually assigned similarity
score ranging from 0 (no relation) to 5 (semanti-
cally equivalent). The datasets1 provided are taken
from the Microsoft Research Paraphrase Corpus
(MSRpar), the Microsoft Research Video Descrip-
tion Corpus (MSRvid), and the WMT2008 devel-
opment dataset (Europarl section)(SMTeuroparl);
they each consist of about 750 sentence pairs with
the class distribution varying with each dataset. The
testing data contains additional sentences from the
same collections as the training data as well as
from two additional unknown sets (OnWN and
SMTnews); they range from 399 to 750 sentence
pairs. The reader may refer to (Agirre et al, 2012)
for additional information regarding this task.
3.2 Resources
Wikipedia2 is a free on-line encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
web page, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential mis-
takes are quickly corrected within the collaborative
environment) of this on-line resource. The basic en-
try in Wikipedia is an article which describes an en-
tity or an event, and which, in addition to untagged
1http://www.cs.york.ac.uk/semeval-2012/
task6/data/uploads/datasets/train-readme.
txt
2www.wikipedia.org
636
content, also consists of hyperlinked text to other
pages within or outside of Wikipedia. These hyper-
links are meant to guide the reader to pages that pro-
vide additional information / clarifications, so that
a better understanding of the primary concept can
be achieved. The structure of Wikipedia in terms of
pages and hyperlinks is exploited directly by seman-
tic similarity methods such as ESA (Gabrilovich and
Markovitch, 2007), or SSA (Hassan and Mihalcea,
2011).
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
between basic units of meaning, or synsets. A synset
groups together senses of different words that share
a very similar meaning, which act in a particu-
lar context as synonyms. Each synset is accompa-
nied by a gloss or definition, and one or two ex-
amples illustrating usage in the given context. Un-
like a traditional thesaurus, the structure of Word-
Net is able to encode additional relationships be-
side synonymy, such as antonymy, hypernymy, hy-
ponymy, meronymy, entailment, etc., which vari-
ous knowledge-based methods use to derive seman-
tic similarity.
3.3 Features
Our meta-system uses several features, which can
be grouped into knowledge-based, corpus-based,
and bipartite graph matching, as described below.
The abbreviations appearing between parentheses
by each method allow for easy cross-referencing
with the evaluations provided in Table 1.
3.3.1 Knowledge-based Semantic Similarity
Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for
each open-class word in one of the input texts, we
compute the maximum semantic similarity (using
the WordNet::Similarity package (Pedersen et al,
2004)) that can be obtained by pairing it with any
open-class word in the other input text. All the
word-to-word similarity scores obtained in this way
are summed and normalized to the length of the two
input texts. We provide below a short description
for each of the similarity metrics employed by this
system3.
The shortest path (Path) similarity is determined
as:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting (including
the end nodes).
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) similarity is determined
as:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
3We point out that the similarity metric proposed by Hirst &
St. Onge was not considered due to the time constraints associ-
ated with the STS task.
637
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
Each of the measures listed above is used as a fea-
ture by our meta-system.
3.3.2 Corpus-based Semantic Similarity
Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
stand out as different, since they rely on a concept-
space representation. In these methods, the semantic
profile of a word is expressed in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch. In the
experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia
download from October 2008, with approximately
6 million articles, and more than 9.5 million hyper-
links.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. Since
encyclopedic knowledge is typically organized into
concepts (or topics), each concept is further de-
scribed using definitions and examples. ESA relies
on the distribution of words inside the encyclopedic
descriptions. It builds semantic representations for
a given word using a word-document association,
where the document represents a Wikipedia article
(concept). ESA is in effect a Vector Space Model
(VSM) built using Wikipedia corpus, where vectors
represents word-articles association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction and interpretation of words as ESA,
yet it uses salient concepts gathered from encyclo-
pedic knowledge, where a ?concept? represents an
unambiguous word or phrase with a concrete mean-
ing, and which affords an encyclopedic definition.
Saliency in this case is determined based on the
word being hyperlinked (either trough manual or au-
tomatic annotations) in context, implying that they
are highly relevant to the given text. SSA is an ex-
ample of Generalized Vector Space Model (GVSM),
where vectors represent word-concepts associations.
In order to determine the similarity of two text
fragments , we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail below.
Both variations were paired with the LSA, ESA,
and SSA systems resulting in six similarity scores
that were used as features by our meta-system,
namely LSAcos, LSAalign, ESAcos, ESAalign,
SSAcos, and SSAalign.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a + b
(8)
where ? is the number of shared terms between the
text fragments and ?i is the similarity score for the
ith pairing.
3.3.3 Bipartite Graph Matching
In an attempt to move beyond the bag-of-words
paradigm described thus far, we attempt to compute
638
a set of dependency graph alignment scores based
on previous work in automatic short-answer grading
(Mohler et al, 2011). This score, computed in two
stages, is used as a feature by our meta-system.
In the first stage, the system is provided with the
dependency graphs for each pair of sentences4. For
each node in one dependency graph, we compute a
similarity score for each node in the other depen-
dency graph based upon a set of lexical, semantic,
and syntactic features applied to both the pair of
nodes and their corresponding subgraphs (i.e. the set
of nodes reachable from a given node by following
directional governor-to-dependant links). The scor-
ing function is trained on a small set of manually
aligned graphs using the averaged perceptron algo-
rithm.
We define a total of 64 features5 to be used to train
a machine learning system to compute subgraph-
subgraph similarity. Of these, 32 are based upon the
bag-of-words semantic similarity of the subgraphs
using the metrics described in Section 3.3.1 as well
as a Wikipedia-trained LSA model. The remaining
32 features are lexico-syntactic features associated
with the parent nodes of the subgraphs and are de-
scribed in more detail in our earlier paper.
We then calculate weights associated with these
features using an averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002) trained on a set of 32 manually annotated
instructor/student answer pairs selected from the
short-answer grading corpus (MM2011). These
pairs contain 7303 node pairs (656 matches, 6647
non-matches). Once the weights are calculated, a
similarity score for each pair of nodes can be com-
puted by taking the dot product of the feature vector
with the weights.
In the second stage, the node similarity scores cal-
culated in the previous step are used to find an op-
timal alignment for the pair of dependency graphs.
We begin with a bipartite graph where each node
in one graph is represented by a node on the left
side of the bipartite graph and each node in the other
4We here use the output of the Stanford Dependency Parser
in collapse/propagate mode with some modifications as de-
scribed in our earlier work.
5With the exception of the four features based upon the Hirst
& St.Onge similarity metric, these are equivalent to the features
used in previous work.
graph is represented by a node on the right side. The
weight associated with each edge is the score com-
puted for each node-node pair in the previous stage.
The bipartite graph is then augmented by adding
dummy nodes to both sides which are allowed to
match any node with a score of zero. An optimal
alignment between the two graphs is then computed
efficiently using the Hungarian algorithm. Note that
this results in an optimal matching, not a mapping,
so that an individual node is associated with at most
one node in the other answer. After finding the opti-
mal match, we produce four alignment-based scores
by optionally normalizing by the number of nodes
and/or weighting the node-alignments according to
the idf scores of the words.6 This results in four
alignment scores listed as graphnone, graphnorm,
graphidf , graphidfnorm.
3.3.4 Baselines
As a baseline, we also utilize several lexical bag-
of-words approaches where each sentence is repre-
sented by a vector of tokens and the similarity of the
two sentences can be computed by finding the co-
sine of the angle between their representative vectors
using term frequency (tf ) or term frequency mul-
tiplied by inverse document frequency (tf.idf )6, or
by using simple overlap between the vectors? dimen-
sions (overlap).
3.4 Machine Learning
3.4.1 Algorithms
All the systems described above are used to gen-
erate a score for each training and test sample (see
Section 3.1). These scores are then aggregated per
sample, and used in a supervised learning frame-
work. We decided to use a regression model, instead
of classification, since the requirements for the task
specify that we should provide a score in the range of
0 to 5. We could have used classification paired with
bucketed ranges, yet classification does not take into
consideration the underlying ordinality of the scores
(i.e. a score of 4.5 is closer to either 4 or 5, but
farther away from 0), which is a noticeable handi-
cap in this scenario. We tried both linear and sup-
6The document frequency scores were taken from the British
National Corpus (BNC).
639
port vector regression7 by performing 10 fold cross-
validation on the train data, yet the latter algorithm
consistently performs better, no matter what kernel
was chosen. Thus we decided to use support vec-
tor regression (Smola and Schoelkopf, 1998) with a
Pearson VII function-based kernel.
Due to its different learning methodology, and
since it is suited for predicting continuous classes,
our second system uses the M5P decision tree al-
gorithm (Quinlan, 1992; Wang and Witten, 1997),
which outperforms support vector regression on the
10 fold cross-validation performed on the SMTeu-
roparl train set, while providing competitive results
on the other train sets (within .01 Pearson correla-
tion).
3.4.2 Setup
We submitted three system variations, namely
IndividualRegression, IndividualDecTree,
and CombinedRegression. The first word de-
scribes the training data; for individual, for the
known test sets we trained on the corresponding
train sets, while for the unknown test sets we trained
on all the train sets combined; for combined,
for each test set we trained on all the train sets
combined. The second word refers to the learning
methodology, where Regression stands for support
vector regression, and DecTree stands for M5P
decision tree.
4 Results and Discussion
We include in Table 1 the Pearson correlations ob-
tained by comparing the predictions of each fea-
ture to the gold standard for the three train datasets.
We notice that the corpus based metrics display a
consistent performance across the three train sets,
when compared to the other methods, including
knowledge-based. Furthermore, the best alignment
strategy (align) for corpus based models outper-
forms similarity scores based on traditional cosine
similarity. It is interesting to note that simple base-
lines such as tf , tf.idf and overlap offer signifi-
cant correlations with all the train sets without ac-
cess to additional knowledge inferred by knowledge
or corpus-based methods. In the case of the bipar-
7Implementations provided through the Weka framework
(Hall et al, 2009).
System MSRpar MSRvid SMTeuroparl
Path 0.49 0.62 0.50
LCH 0.48 0.49 0.45
Lesk 0.48 0.59 0.50
WUP 0.46 0.38 0.42
RES 0.47 0.55 0.48
Lin 0.49 0.54 0.48
JCN 0.49 0.63 0.51
LSAalign 0.44 0.57 0.61
LSAcos 0.37 0.74 0.56
ESAalign 0.52 0.70 0.62
ESAcos 0.30 0.71 0.53
SSAalign 0.46 0.61 0.65
SSAcos 0.22 0.63 0.39
graphnone 0.42 0.50 0.21
graphnorm 0.48 0.43 0.59
graphidf 0.16 0.67 0.16
graphidfnorm 0.08 0.60 0.19
tf.idf 0.45 0.63 0.41
tf 0.45 0.69 0.51
overlap 0.44 0.69 0.27
Table 1: Correlation of individual features for the training
sets with the gold standards
tite graph matching, the graphnorm variation pro-
vides the strongest correlation results across all the
datasets.
We include the evaluation results provided by the
task organizers in Table 2. They indicate that our in-
tuition in using a support vector regression strategy
was correct. While the IndividualRegression was
our strongest system on the training data, the same
ranking applies to the test data (including the addi-
tional two surprise datasets) as well, earning it the
fifth place among the 89 participating systems, with
a Pearson correlation of 0.7846.
Regarding the decision tree based learning
(IndividualDecTree), despite its more robust be-
havior on the train sets, it achieved slightly lower
outcome on the test data, at 0.7677 correlation. We
believe this happened because decision trees have a
tendency to overfit training data, as they generate a
rigid structure which is unforgiving to minor devia-
tions in the test data. Nonetheless, this second vari-
ation still ranks in the top 10% of the submitted sys-
tems.
As an alternative approach to handle unknown test
data (e.g. different distributions, genres), we opted
640
Run ALL Rank Mean RankMean MSRpar MSRvid SMTeuroparl OnWN SMTnews
IndividualRegression 0.7846 5 0.6162 13 0.5353 0.8750 0.4203 0.6715 0.4033
IndividualDecTree 0.7677 9 0.5947 25 0.5693 0.8688 0.4203 0.6491 0.2256
CombinedRegression 0.7418 14 0.6159 14 0.5032 0.8695 0.4797 0.6715 0.4033
Table 2: Evaluation results and ranking published by the task organizers
to also include the CombinedRegression strategy
as our third variation. This seems to have been fruit-
ful for MSRvid, SMTeuroparl, and the two sur-
prise datasets (ONWn and SMTnews). In the
case of SMTeuroparl, this expanded training set
achieves a better performance than learning from
the corresponding training set alne, gaining an im-
provement of 0.0776 correlation points. Unfortu-
nately, the variation has some losses, particularly for
the MSRpar dataset (0.0321), yet it is able to con-
sistently model and handle a wider variety of text
types.
5 Conclusion
This paper describes the three system variations our
team participated with in the Semantic Text Similar-
ity task in SEMEVAL 2012. Our focus has been to
produce a synergistic approach, striving to achieve a
superior result than attainable by each system indi-
vidually. We have considered a variety of methods
for inferring semantic similarity, including knowl-
edge and corpus-based methods. These were lever-
aged in a machine-learning framework, where our
preferred learning algorithm is support vector re-
gression, due to its ability to deal with continuous
classes and to dampen the effect of noisy features,
while augmenting more robust ones. While it is al-
ways preferable to use similar test and train sets,
when information regarding the test dataset is un-
available, we show that a robust performance can
be achieved by combining all train data from dif-
ferent sources into a single set and allowing a ma-
chine learner to make predictions. Overall, it was
interesting to note that corpus-based methods main-
tain strong results on all train datasets in compari-
son to knowledge-based methods. Our three systems
ranked number 5, 9 and 14 among the 89 systems
participating in the task.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
641
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1997. How well can passage meaning be
derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M.E. Lesk, 1971. The SMART Retrieval
System: Experiments in Automatic Document Process-
ing, chapter Computer evaluation of indexing and text
processing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
642
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 22?31, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Coarse to Fine Grained Sense Disambiguation in Wikipedia
Hui Shen
School of EECS
Ohio University
Athens, OH 45701, USA
hui.shen.1@ohio.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, OH 45701, USA
bunescu@ohio.edu
Rada Mihalcea
Department of CSE
University of North Texas
Denton, TX 76203, USA
rada@cs.unt.edu
Abstract
Wikipedia articles are annotated by volunteer
contributors with numerous links that connect
words and phrases to relevant titles. Links
to general senses of a word are used concur-
rently with links to more specific senses, with-
out being distinguished explicitly. We present
an approach to training coarse to fine grained
sense disambiguation systems in the presence
of such annotation inconsistencies. Experi-
mental results show that accounting for anno-
tation ambiguity in Wikipedia links leads to
significant improvements in disambiguation.
1 Introduction and Motivation
The vast amount of world knowledge available in
Wikipedia has been shown to benefit many types
of text processing tasks, such as coreference res-
olution (Ponzetto and Strube, 2006; Haghighi and
Klein, 2009; Bryl et al, 2010; Rahman and Ng,
2011), information retrieval (Milne, 2007; Li et al,
2007; Potthast et al, 2008; Cimiano et al, 2009),
or question answering (Ahn et al, 2004; Kaisser,
2008; Ferrucci et al, 2010). In particular, the user
contributed link structure of Wikipedia has been
shown to provide useful supervision for training
named entity disambiguation (Bunescu and Pasca,
2006; Cucerzan, 2007) and word sense disambigua-
tion (Mihalcea, 2007; Ponzetto and Navigli, 2010)
systems. Articles in Wikipedia often contain men-
tions of concepts or entities that already have a cor-
responding article. When contributing authors men-
tion an existing Wikipedia entity inside an article,
they are required to link at least its first mention to
the corresponding article, by using links or piped
links. Consider, for example, the following Wiki
source annotations: The [[capital city|capital]] of
Georgia is [[Atlanta]]. The bracketed strings iden-
tify the title of the Wikipedia articles that describe
the corresponding named entities. If the editor wants
a different string displayed in the rendered text, then
the alternative string is included in a piped link, af-
ter the title string. Based on these Wiki processing
rules, the text that is rendered for the aforementioned
example is: The capital of Georgia is Atlanta.
Since many words and names mentioned in
Wikipedia articles are inherently ambiguous, their
corresponding links can be seen as a useful source
of supervision for training named entity and word
sense disambiguation systems. For example,
Wikipedia contains articles that describe possible
senses of the word ?capital?, such as CAPITAL CITY,
CAPITAL (ECONOMICS), FINANCIAL CAPITAL, or
HUMAN CAPITAL, to name only a few. When dis-
ambiguating a word or a phrase in Wikipedia, a con-
tributor uses the context to determine the appropriate
Wikipedia title to include in the link. In the exam-
ple above, the editor of the article determined that
the word ?capital? was mentioned with the political
center meaning, consequently it was mapped to the
article CAPITAL CITY through a piped link.
In order to useWikipedia links for training aWSD
system for a given word, one needs first to define a
sense repository that specifies the possible meanings
for that word, and then use the Wikipedia links to
create training examples for each sense in the repos-
itory. This approach might be implemented using
the following sequence of steps:
22
In global climate models, the state and properties of the [[atmosphere]] are specified at a number of discrete locations
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A ? A(S) ? AE
The principal natural phenomena that contribute gases to the [[Atmosphere of Earth|atmosphere]] are emissions from volcanoes
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A ? A(S) ? AE
An aerogravity assist is a spacecraft maneuver designed to change velocity when arriving at a body with an [[atmosphere]]
General = ATMOSPHERE; Specific = ATMOSPHERE ? generic; Label = A ? A(G)
Assuming the planet?s [[atmosphere]] is close to equilibrium, it is predicted that 55 Cancri d is covered with water clouds
General = ATMOSPHERE; Specific = ATMOSPHERE OF CANCRI ? missing; A ? A(G)
Figure 1: Coarse and fine grained sense annotations in Wikipedia (bold). The proposed hierarchical Label (right).
A(S) = ATMOSPHERE (S), A(G) = ATMOSPHERE (G), A = ATMOSPHERE, AE = ATMOSPHERE OF EARTH.
1. Collect all Wikipedia titles that are linked from
the ambiguous anchor word.
2. Create a repository of senses from all titles that
have sufficient support in Wikipedia i.e., titles
that are referenced at least a predefined min-
imum number of times using the ambiguous
word as anchor.
3. Use the links extracted for each sense in the
repository as labeled examples for that sense
and train a WSD model to distinguish between
alternative senses of the ambiguous word.
Taking the word ?atmosphere? as an example, the
first step would result in a wide array of titles,
ranging from the general ATMOSPHERE and its in-
stantiations ATMOSPHERE OF EARTH or ATMO-
SPHERE OF MARS, to titles as diverse as ATMO-
SPHERE (UNIT), MOOD (PSYCHOLOGY), or AT-
MOSPHERE (MUSIC GROUP). In the second step,
the most frequent titles for the anchor word ?at-
mosphere? would be assembled into a repository R
= {ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,
STELLAR ATMOSPHERE, ATMOSPHERE (UNIT),
ATMOSPHERE (MUSIC GROUP)}. The classifier
trained in the third step would use features ex-
tracted from the context to discriminate between
word senses.
This Wikipedia-based approach to creating train-
ing data for word sense disambiguation has a ma-
jor shortcoming. Many of the training examples ex-
tracted for the title ATMOSPHERE could very well
belong to more specific titles such as ATMOSPHERE
OF EARTH or ATMOSPHERE OF MARS. Whenever
the word ?atmosphere? is used in a context with the
sense of ?a layer of gases that may surround a ma-
terial body of sufficient mass, and that is held in
place by the gravity of the body,? the contributor
has the option of adding a link either to the title AT-
MOSPHERE that describes this general sense of the
word, or to the title of an article that describes the
atmosphere of the actual celestial body that is re-
ferred in that particular context, as shown in the first
2 examples in Figure 1. As shown in bold in Fig-
ure 1, different occurrences of the same word may
be tagged with either a general or a specific link, an
ambiguity that is pervasive in Wikipedia for words
like ?atmosphere? that have general senses that sub-
sume multiple, popular specific senses. There does
not seem to be a clear, general rule underlying the
decision to tag a word or a phrase with a general
or specific sense link in Wikipedia. We hypothesize
that, in some cases, editors may be unaware that an
article exists in Wikipedia for the actual reference
of a word or for a more specific sense of the word,
and therefore they end up using a link to an article
describing the general sense of the word. There is
also the possibility that more specific articles are in-
troduced only in newer versions of Wikipedia, and
thus earlier annotations were not aware of these re-
cent articles. Furthermore, since annotating words
with the most specific sense available in Wikipedia
may require substantial cognitive effort, editors may
often choose to link to a general sense of the word, a
choice that is still correct, yet less informative than
the more specific sense.
2 Annotation Inconsistencies in Wikipedia
In order to get a sense of the potential magnitude
of the general vs. specific sense annotation ambi-
guity, we extracted all Wikipedia link annotations
23
for the words ?atmosphere?, ?president?, ?game?,
?dollar?, ?diamond? and ?Corinth?, and created
a special subset from those that were labeled by
Wikipedia editors with the general sense links AT-
MOSPHERE, PRESIDENT, GAME, DOLLAR, DIA-
MOND, and CORINTH, respectively. Then, for each
of the 7,079 links in this set, we used the context
to manually determine the corresponding more spe-
cific title, whenever such a title exists in Wikipedia.
The statistics in Tables 1 and 2 show a significant
overlap between the general and specific sense cate-
gories. For example, out of the 932 links from ?at-
mosphere? to ATMOSPHERE that were extracted in
total, 518 were actually about the ATMOSPHERE OF
EARTH, but the user linked them to the more general
sense category ATMOSPHERE. On the other hand,
there are 345 links to ATMOSPHERE OF EARTH that
were explicitly made by the user. We manually as-
signed general links (G) whenever the word is used
with a generic sense, or when the reference is not
available in the repository of titles collected for that
word because either the more specific title does not
exist in Wikipedia or the specific title exists, but it
does not have sufficient support ? at least 20 linked
anchors ? in Wikipedia. We grouped the more spe-
cific links for any given sense into a special cate-
gory suffixed with (S), to distinguish them from the
general links (generic use, or missing reference) that
were grouped into the category suffixed with (G).
For many ambiguous words, the annotation in-
consistencies appear when the word has senses
that are in a subsumption relationship: the ATMO-
SPHERE OF EARTH is an instance of ATMOSPHERE,
whereas a STELLAR ATMOSPHERE is a particular
type of ATMOSPHERE. Subsumed senses can be
identified automatically using the category graph in
Wikipedia. The word ?Corinth? is an interesting
case: the subsumption relationship between AN-
CIENT CORINTH and CORINTH appears because of
a temporal constraint. Furthermore, in the case of
the word ?diamond?, the annotation inconsistencies
are not caused by a subsumption relation between
senses. Instead of linking to the DIAMOND (GEM-
STONE) sense, Wikipedia contributors often link to
the related DIAMOND sense indicating the mineral
used in the gemstone.
A supervised learning algorithm that uses the ex-
tracted links for training aWSD classification model
atmosphere Size
ATMOSPHERE 932
Atmosphere (S) 559
Atmosphere of Earth 518
Atmosphere of Mars 19
Atmosphere of Venus 9
Stellar Atmosphere 13
Atmosphere (G) 373
ATMOSPHERE OF EARTH 345
ATMOSPHERE OF MARS 37
ATMOSPHERE OF VENUS 26
STELLAR ATMOSPHERE 29
ATMOSPHERE (UNIT) 96
ATMOSPHERE (MUSIC GROUP) 104
president Size
PRESIDENT 3534
President (S) 989
Chancellor (education) 326
President of the United States 534
President of the Philippines 42
President of Pakistan 27
President of France 22
President of India 21
President of Russia 17
President (G) 2545
CHANCELLOR (EDUCATION) 210
PRESIDENT OF THE UNITED STATES 5941
PRESIDENT OF THE PHILIPPINES 549
PRESIDENT OF PAKISTAN 192
PRESIDENT OF FRANCE 151
PRESIDENT OF INDIA 86
PRESIDENT OF RUSSIA 101
Table 1: Wiki (CAPS) and manual (italics) annotations.
to distinguish between categories in the sense repos-
itory assumes implicitly that the categories, and
hence their training examples, are mutually disjoint.
This assumption is clearly violated for words like
?atmosphere,? consequently the learned model will
have a poor performance on distinguishing between
the overlapping categories. Alternatively, we can
say that sense categories like ATMOSPHERE are ill
defined, since their supporting dataset contains ex-
amples that could also belong to more specific sense
categories such as ATMOSPHERE OF EARTH.
We see two possible solutions to the problem of
inconsistent link annotations. In one solution, spe-
cific senses are grouped together with the subsuming
general sense, such that all categories in the result-
ing repository become disjoint. For ?atmosphere?,
the general category ATMOSPHERE would be aug-
mented to contain all the links previously annotated
24
dollar Size
DOLLAR 379
Dollar (S) 231
United States dollar 228
Canadian dollar 3
Australian dollar 1
Dollar (G) 147
UNITED STATES DOLLAR 3516
CANADIAN DOLLAR 420
AUSTRALIAN DOLLAR 124
DOLLAR SIGN 290
DOLLAR (BAND) 30
DOLLAR, CLACKMANNANSHIRE 30
game Size
GAME 819
Game (S) 99
Video game 55
PC game 44
Game (G) 720
VIDEO GAME 312
PC GAME 24
GAME (FOOD) 232
GAME (RAPPER) 154
diamond Size
DIAMOND 716
Diamond (S) 221
Diamond (gemstone) 221
Diamond (G) 495
DIAMOND (GEMSTONE) 71
BASEBALL FIELD 36
MUSIC RECORDING SALES CERT. 36
Corinth Size
CORINTH 699
Corinth (S) 409
Ancient Corinth 409
Corinth (G) 290
ANCIENT CORINTH 92
CORINTH, MISSISSIPPI 72
Table 2: Wiki (CAPS) and manual (italics) annotations.
as ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,
or STELLAR ATMOSPHERE. This solution is
straightforward to implement, however it has the
disadvantage that the resulting WSD model will
never link words to more specific titles in Wikipedia
like ATMOSPHERE OF MARS.
Another solution is to reorganize the original
sense repository into a hierarchical classification
scheme such that sense categories at each classifi-
cation level become mutually disjoint. The resulting
WSD system has the advantage that it can make fine
grained sense distinctions for an ambiguous word,
despite the annotation inconsistencies present in the
training data. The rest of this paper describes a feasi-
ble implementation for this second solution that does
not require any manual annotation beyond the links
that are already provided by Wikipedia volunteers.
3 Learning for Coarse to Fine Grained
Sense Disambiguation
Figure 2 shows our proposed hierarchical classifica-
tion scheme for disambiguation, using ?atmosphere?
as the ambiguous word. Shaded leaf nodes show
the final categories in the sense repository for each
word, whereas the doted elliptical frames on the
second level in the hierarchy denote artificial cate-
gories introduced to enable a finer grained classifi-
cation into more specific senses. Thick dotted ar-
rows illustrate the classification decisions that are
made in order to obtain a fine grained disambigua-
tion of the word. Thus, the word ?atmosphere?
is first classified to have the general sense ATMO-
SPHERE, i.e. ?a layer of gases that may surround a
material body of sufficient mass, and that is held in
place by the gravity of the body?. In the first so-
lution, the disambiguation process would stop here
and output the general sense ATMOSPHERE. In the
second solution, the disambiguation process contin-
ues and further classifies the word to be a reference
to ATMOSPHERE OF EARTH. To get to this final
classification, the process passes through an inter-
mediate binary classification level where it deter-
mines whether the word has a more specific sense
covered in Wikipedia, corresponding to the artificial
category ATMOSPHERE (S). If the answer is no, the
system stops the disambiguation process and out-
puts the general sense category ATMOSPHERE. This
basic sense hierarchy can be replicated depending
on the existence of even finer sense distinctions in
Wikipedia. For example, Wikipedia articles describ-
ing atmospheres of particular stars could be used to
further refine STELLAR ATMOSPHERE with two ad-
ditional levels of the type Level 2 and Level 3. Over-
all, the proposed disambiguation scheme could be
used to relabel the ATMOSPHERE links in Wikipedia
with more specific, and therefore more informative,
senses such as ATMOSPHERE OF EARTH. In gen-
eral, the Wikipedia category graph could be used
to automatically create hierarchical structures for re-
25
Figure 2: Hierarchical disambiguation scheme, from coarse to fine grained senses.
lated senses of the same word.
Training word sense classifiers for Levels 1 and 3
is straightforward. For Level 1, Wikipedia links that
are annotated by users as ATMOSPHERE, ATMO-
SPHERE OF EARTH, ATMOSPHERE OF MARS, AT-
MOSPHERE OF VENUS, or STELLAR ATMOSPHERE
are collected as training examples for the general
sense category ATMOSPHERE. Similarly, links that
are annotated as ATMOSPHERE (UNIT) and ATMO-
SPHERE (MUSIC GROUP) will be used as training
examples for the two categories, respectively. A
multiclass classifier is then trained to distinguish be-
tween the three categories at this level. For Level 3,
a multiclass classifiers is trained on Wikipedia links
collected for each of the 4 specific senses.
For the binary classifier at Level 2, we could
use as training examples for the category ATMO-
SPHERE (G) all Wikipedia links that were anno-
tated as ATMOSPHERE, whereas for the category
ATMOSPHERE (S) we could use as training exam-
ples all Wikipedia links that were annotated specif-
ically as ATMOSPHERE OF EARTH, ATMOSPHERE
OF MARS, ATMOSPHERE OF VENUS, or STELLAR
ATMOSPHERE. A traditional binary classification
SVM could be trained on this dataset to distinguish
between the two categories. We call this approach
Naive SVM, since it does not account for the fact that
a significant number of the links that are annotated
by Wikipedia contributors as ATMOSPHERE should
actually belong to the ATMOSPHERE (S) category ?
about 60% of them, according to Table 1. Instead,
we propose treating all ATMOSPHERE links as unla-
beled examples. If we consider the specific links in
ATMOSPHERE (S) to be positive examples, then the
problem becomes one of learning with positive and
unlabeled examples.
3.1 Learning with positive and unlabeled
examples
This general type of semi-supervised learning has
been studied before in the context of tasks such
as text classification and information retrieval (Lee
and Liu, 2003; Liu et al, 2003), or bioinformat-
ics (Elkan and Noto, 2008; Noto et al, 2008). In
this setting, the training data consists of positive ex-
amples x ? P and unlabeled examples x ? U .
Following the notation of Elkan and Noto (2008),
we define s(x) = 1 if the example is positive and
s(x) = ?1 if the example is unlabeled. The true
label of an example is y(x) = 1 if the example
is positive and y(x) = ?1 if the example is neg-
ative. Thus, x ? P ? s(x) = y(x) = 1 and
x ? U ? s(x) = ?1 i.e., the true label y(x) of an
unlabeled example is unknown. For the experiments
reported in this paper, we use our implementation
of two state-of-the-art approaches to Learning with
Positive and Unlabeled (LPU) examples: the Biased
SVM formulation of Lee and Liu (2003) and the
Weighted Samples SVM formulation of Elkan and
Noto (2008). The original version of Biased SVM
was designed to maximize the product between pre-
cision and recall. In the next section we describe a
26
modification to the Biased SVM approach that can
be used to maximize accuracy, a measure that is of-
ten used to evaluate WSD performance.
3.1.1 The Biased SVM
In the Biased SVM formulation (Lee and Liu,
2003; Liu et al, 2003), all unlabeled examples are
considered to be negative and the decision function
f(x) = wT?(x) + b is learned using the standard
soft-margin SVM formulation shown in Figure 3.
minimize: 12?w?
2 + CP
?
x?P
?x + CU
?
x?U
?x
subject to: s(x) (wT?(x) + b) ? 1? ?x
?x ? 0, ?x ? P ? U
Figure 3: Biased SVM optimization problem.
The capacity parameters CP and CU control how
much we penalize errors on positive examples vs. er-
rors on unlabeled examples. Since not all unlabeled
examples are negative, one would want to select ca-
pacity parameters satisfying CP > CU , such that
false negative errors are penalized more than false
positive errors. In order to find the best capacity pa-
rameters to use during training, the Biased SVM ap-
proach runs a grid search on a separate development
dataset. This search is aimed at finding values for
the parameters CP and CU that maximize pr, the
product between precision p = p(y = 1|f = 1) and
recall r = p(f = 1|y = 1). Lee and Liu (2003)
show that maximizing the pr criterion is equivalent
with maximizing the objective r2/p(f = 1), where
both r = p(f = 1|y = 1) and p(f = 1) can be es-
timated using the trained decision function f(x) on
the development dataset.
Maximizing the pr criterion in the original Biased
SVM formulation was motivated by the need to opti-
mize the F measure in information retrieval settings,
where F = 2pr(p+ r). In the rest of this section we
show that classification accuracy can be maximized
using only positive and unlabeled examples, an im-
portant result for problems where classification ac-
curacy is the target performance measure.
The accuracy of a binary decision function f(x)
is, by definition, acc = p(f = 1|y = 1) + p(f =
?1|y = ?1). Since the recall is r = p(f = 1|y =
1), the accuracy can be re-written as:
acc = r + 1? p(f = 1|y = ?1) (1)
Using Bayes? rule twice, the false positive term
p(f = 1|y = ?1) can be re-written as:
p(f = 1|y = ?1) = p(f = 1)p(y = ?1|f = 1)p(y = ?1)
= p(f = 1)p(y = ?1) ? (1? p(y = 1|f = 1))
= p(f = 1)p(y = ?1) ?
p(f = 1)
p(y = ?1) ?
p(y = 1)p(f = 1|y = 1)
p(f = 1)
= p(f = 1)? p(y = 1)? rp(y = ?1) (2)
Plugging identity 2 in Equation 1 leads to:
acc = 1 + r + r ? p(y = 1)? p(f = 1)p(y = ?1)
= 1 + r ? p(f = 1)p(y = ?1) (3)
Since p(y = ?1) can be assimilated with a con-
stant, Equation 3 implies that maximizing accu-
racy is equivalent with maximizing the criterion
r ? p(f = 1), where both the recall r and p(f = 1)
can be estimated on the positive and unlabeled ex-
amples from a separate development dataset.
In conclusion, one can use the original Biased
SVM formulation to maximize r2/p(f = 1), which
has been shown by Lee and Liu (2003) to maximize
pr, a criterion that has a similar behavior with the
F-measure used in retrieval applications. Alterna-
tively, if the target performance measure is accuracy,
we can choose instead to maximize r ? p(f = 1),
which we have shown above to correspond to accu-
racy maximization.
3.1.2 The Weighted Samples SVM
Elkan and Noto (2008) introduced two ap-
proaches for learning with positive and unlabeled
data. Both approaches are based on the assumption
that labeled examples {x|s(x) = 1} are selected at
random from the positive examples {x|y(x) = 1}
i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1). Their
best performing approach uses the positive and unla-
beled examples to train two distinct classifiers. First,
the dataset P ? U is split into a training set and a
validation set, and a classifier g(x) is trained on the
27
labeling s to approximate the label distribution i.e.
g(x) = p(s = 1|x). The validation set is then used
to estimate p(s = 1|y = 1) as follows:
p(s=1|y=1) = p(s=1|x, y=1) = 1|P |
?
x?P
g(x) (4)
The second and final classifier f(x) is trained on a
dataset of weighted examples that are sampled from
the original training set as follows:
? Each positive example x ? P is copied as a
positive example in the new training set with
weight p(y = 1|x, s = 1) = 1.
? Each unlabeled example x ? U is duplicated
into two training examples in the new dataset:
a positive example with weight p(y = 1|x, s =
0) and a negative example with weight p(y =
?1|x, s = 0) = 1? p(y = 1|x, s = 0).
Elkan and Noto (2008) show that the weights above
can be derived as:
p(y=1|x, s=0) = 1?p(s=1|y=1)p(s=1|y=1) ?
p(s=1|x)
1?p(s=1|x) (5)
The output of the first classifier g(x) is used to
approximate the probability p(s = 1|x), whereas
p(s = 1|y = 1) is estimated using Equation 4.
The two classifiers g and f are trained using
SVMs and a linear kernel. Platt scaling is used with
the first classifier to obtain the probability estimates
g(x) = p(s = 1|x), which are then converted into
weights following Equations 4 and 5, and used dur-
ing the training of the second classifier.
4 Experimental Evaluation
We ran disambiguation experiments on the 6 am-
biguous words atmosphere, president, dollar, game,
diamond andCorinth. The correspondingWikipedia
sense repositories have been summarized in Tables 1
and 2. All WSD classifiers used the same set of stan-
dard WSD features (Ng and Lee, 1996; Stevenson
and Wilks, 2001), such as words and their part-of-
speech tags in a window of 3 words around the am-
biguous keyword, the unigram and bigram content
words that are within 2 sentences of the current sen-
tence, the syntactic governor of the keyword, and
its chains of syntactic dependencies of lengths up to
two. Furthermore, for each example, a Wikipedia
specific feature was computed as the cosine similar-
ity between the context of the ambiguous word and
the text of the article for the target sense or reference.
The Level1 and Level3 classifiers were trained us-
ing the SVMmulti component of the SVMlight pack-
age.1 TheWSD classifiers were evaluated in a 4-fold
cross validation scenario in which 50% of the data
was used for training, 25% for tuning the capacity
parameter C, and 25% for testing. The final accu-
racy numbers, shown in Table 3, were computed by
averaging the results over the 4 folds. Since the word
president has only one sense on Level1, no classifier
needed to be trained for this case. Similarly, words
diamond andCorinth have only one sense on Level3.
atmosphere president dollar
Level1 93.1% ? 94.1%
Level3 85.6% 82.2% 90.8%
game diamond Corinth
Level1 82.9% 95.5% 92.7%
Level3 92.9% ? ?
Table 3: Disambiguation accuracy at Levels 1 & 3.
The evaluation of the binary classifiers at the sec-
ond level follows the same 4-fold cross validation
scheme that was used for Level1 and Level3. The
manual labels for specific senses and references in
the unlabeled datasets are always ignored during
training and tuning and used only during testing.
We compare the Naive SVM, Biased SVM, and
Weighted SVM in the two evaluation settings, using
for all of them the same train/development/test splits
of the data and the same features. We emphasize
that our manual labels are used only for testing pur-
poses ? the manual labels are ignored during train-
ing and tuning, when the data is assumed to contain
only positive and unlabeled examples. We imple-
mented the Biased SVM approach on top of the bi-
nary SVMlight package. TheCP andCU parameters
of the Biased SVM were tuned through the c and j
parameters of SVMlight (c = CU and j = CP /CU ).
Eventually, all three methods use the development
data for tuning the c and j parameters of the SVM.
However, whereas the Naive SVM tunes these pa-
rameters to optimize the accuracy with respect to the
noisy label s(x), the Biased SVM tunes the same pa-
rameters to maximize an estimate of the accuracy or
1http://svmlight.joachims.org
28
F-measure with respect to the true label y(x). The
Weighted SVM approach was implemented on top
of the LibSVM2 package. Even though the original
Weighted SVM method of Elkan and Noto (2008)
does not specify tuning any parameters, we noticed
it gave better results when the capacity c and weight
j parameters were tuned for the first classifier g(x).
Table 4 shows the accuracy results of the three
methods for Level2, whereas Table 5 shows the F-
measure results. The Biased SVM outperforms the
Naive SVM on all the words, in terms of both ac-
curacy and F-measure. The most dramatic increases
are seen for the words atmosphere, game, diamond,
and Corinth. For these words, the number of pos-
itive examples is significantly smaller compared to
the total number of positive and unlabeled examples.
Thus, the percentage of positive examples relative to
the total number of positive and unlabeled examples
is 31.9% for atmosphere, 29.1% for game, 9.0% for
diamond, and 11.6% for Corinth. The positive to to-
tal ratio is however significantly larger for the other
two words: 67.2% for president and 91.5% for dol-
lar. When the number of positive examples is large,
the false negative noise from the unlabeled dataset
in the Naive SVM approach will be relatively small,
hence the good performance of Naive SVM in these
cases. To check whether this is the case, we have
also run experiments where we used only half of
the available positive examples for the word presi-
dent and one tenth of the positive examples for the
word dollar, such that the positive datasets became
comparable in size with the unlabeled datasets. The
results for these experiments are shown in Tables 4
and 5 in the rows labeled presidentS and dollarS . As
expected, the difference between the performance of
Naive SVM and Biased SVM gets larger on these
smaller datasets, especially for the word dollar.
The Weighted SVM outperforms the Naive SVM
on five out of the six words, the exception being the
word president. Comparatively, the Biased SVM
has a more stable behavior and overall results in a
more substantial improvement over the Naive SVM.
Based on these initial results, we see the Biased
SVM as the method of choice for learning with pos-
itive and unlabeled examples in the task of coarse to
fine grained sense disambiguation in Wikipedia.
2http://www.csie.ntu.edu.tw/?cjlin/libsvm
Word NaiveSVM BiasedSVM WeightedSVM
atmosphere 39.9% 79.6% 75.0%
president 91.9% 92.5% 89.5%
dollar 96.0% 97.0% 97.1%
game 83.8% 87.1% 84.6%
diamond 70.2% 74.5% 75.1%
Corinth 46.2% 75.1% 51.9%
presidentS 88.1% 90.6% 87.4%
dollarS 70.3% 84.9% 70.6%
Table 4: Disambiguation accuracy at Level2.
Word NaiveSVM BiasedSVM WeightedSVM
atmosphere 30.5% 86.0% 83.2%
president 94.4% 95.0% 92.8%
dollar 97.9% 98.4% 98.5%
game 75.1% 81.8% 77.5%
diamond 8.6% 53.5% 46.3%
Corinth 15.3% 81.2% 68.0%
presidentS 90.0% 92.4% 89.5%
dollarS 77.9% 91.2% 78.2%
Table 5: Disambiguation F-measure at Level2.
In a final set of experiments, we compared the
traditional flat classification approach and our pro-
posed hierarchical classifier in terms of their over-
all disambiguation accuracy. In these experiments,
the sense repository contains all the leaf nodes as
distinct sense categories. For example, the word
atmosphere would correspond to the sense repos-
itory R = {ATMOSPHERE (G), ATMOSPHERE OF
EARTH, ATMOSPHERE OF MARS, ATMOSPHERE
OF VENUS, STELLAR ATMOSPHERE, ATMO-
SPHERE (UNIT), ATMOSPHERE (MUSIC GROUP)}.
The overall accuracy results are shown in Table 6
and confirm the utility of using the LPU framework
in the hierarchical model, which outperforms the tra-
ditional flat model, especially on words with low ra-
tio of positive to unlabeled examples.
atmosphere president dollar
Flat 52.4% 89.4% 90.0%
Hierarchical 79.7% 91.0% 90.1%
game diamond Corinth
Flat 83.6% 65.7% 42.6%
Hierarchical 87.2% 76.8% 72.1%
Table 6: Flat vs. Hierarchical disambiguation accuracy.
29
5 Future Work
Annotation inconsistencies in Wikipedia were cir-
cumvented by adapting two existing approaches that
use only positive and unlabeled data to train binary
classifiers. This binary classification constraint led
to the introduction of the artificial specific (S) cat-
egory on Level2 in our disambiguation framework.
In future work, we plan to investigate a direct exten-
sion of learning with positive and unlabeled data to
the case of multiclass classification, which will re-
duce the number of classification levels from 3 to 2.
We also plan to investigate the use of unsupervised
techniques in order to incorporate less popular refer-
ences of a word in the hierarchical classification.
Conclusion
We presented an approach to training coarse to fine
grained sense disambiguation systems that treats
annotation inconsistencies in Wikipedia under the
framework of learning with positive and unlabeled
examples. Furthermore, we showed that the true ac-
curacy of a decision function can be optimized us-
ing only positive and unlabeled examples. For test-
ing purposes, we manually annotated 7,079 links be-
longing to six ambiguous words 3. Experimental
results demonstrate that accounting for annotation
ambiguity in Wikipedia links leads to consistent im-
provements in disambiguation accuracy. The man-
ual annotations were only used for testing and were
ignored during training and development. Conse-
quently, the proposed framework of learning with
positive and unlabeled examples for sense disam-
biguation could be applied on the entire Wikipedia
without any manual annotations. By augmenting
general sense links with links to more specific ar-
ticles, such an application could have a significant
impact on Wikipedia itself.
Acknowledgments
This work was supported in part by the Na-
tional Science Foundation IIS awards #1018613 and
#1018590, and an allocation of computing time from
the Ohio Supercomputer Center.
3Data and code will be made publicly available.
References
D. Ahn, V. Jijkoun, G. Mishne, K. Muller, M. de Ri-
jke, and S. Schlobach. 2004. Using Wikipedia at the
TREC QA track. In Proceedings of the 13th Text Re-
trieval Conference (TREC 2004).
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 2010 conference on ECAI 2010: 19th
European Conference on Artificial Intelligence, pages
759?764, Amsterdam, The Netherlands.
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceesings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9?16, Trento, Italy.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information re-
trieval. In International Joint Conference on Artificial
Intelligence (IJCAI-09, pages 1513?1518, Pasadena,
CA, july.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 708?716.
Charles Elkan and Keith Noto. 2008. Learning clas-
sifiers from only positive and unlabeled data. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?08, pages 213?220.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building watson: An overview of the deepqa
project. AI Magazine, 31(3):59?79.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161, Singapore, August.
M. Kaisser. 2008. The QuALiM question answering
demo: Supplementing answers with paragraphs drawn
from Wikipedia. In Proceedings of the ACL-08 Hu-
man Language Technology Demo Session, pages 32?
35, Columbus, Ohio.
Wee Sun Lee and Bing Liu. 2003. Learning with pos-
itive and unlabeled examples using weighted logistic
regression. In Proceedings of the Twentieth Interna-
tional Conference on Machine Learning (ICML, pages
448?455, Washington, DC, August.
30
Y. Li, R. Luk, E. Ho, and K. Chung. 2007. Improv-
ing weak ad-hoc queries using Wikipedia as external
corpus. In Proceedings of the 30th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 797?798,
Amsterdam, Netherlands.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and
Philip S. Yu. 2003. Building text classifiers using pos-
itive and unlabeled examples. In Proceedings of the
Third IEEE International Conference on Data Mining,
ICDM ?03, pages 179?186, Washington, DC, USA.
R. Mihalcea. 2007. Using Wikipedia for automatic word
sense disambiguation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 196?203, Rochester, New York, April.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence, Hamilton, New Zealand.
Hwee Tou Ng and H. B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics (ACL-96), pages 40?47, Santa Cruz, CA.
Keith Noto, Milton H. Saier, Jr., and Charles Elkan.
2008. Learning to find relevant biological articles
without negative training examples. In Proceedings of
the 21st Australasian Joint Conference on Artificial In-
telligence: Advances in Artificial Intelligence, AI ?08,
pages 202?213.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 192?199.
M. Potthast, B. Stein, and M. A. Anderka. 2008.
Wikipedia-based multilingual retrieval model. In Pro-
ceedings of the 30th European Conference on IR Re-
search, Glasgow.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 814?824, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349, Septem-
ber.
31
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560?565,
Dublin, Ireland, August 23-24, 2014.
SimCompass: Using Deep Learning Word Embeddings
to Assess Cross-level Similarity
Carmen Banea, Di Chen,
Rada Mihalcea
?
University of Michigan
Ann Arbor, MI
Claire Cardie
Cornell University
Ithaca, NY
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
Abstract
This article presents our team?s partici-
pating system at SemEval-2014 Task 3.
Using a meta-learning framework, we
experiment with traditional knowledge-
based metrics, as well as novel corpus-
based measures based on deep learning
paradigms, paired with varying degrees of
context expansion. The framework en-
abled us to reach the highest overall per-
formance among all competing systems.
1 Introduction
Semantic textual similarity is one of the key
components behind a multitude of natural lan-
guage processing applications, such as informa-
tion retrieval (Salton and Lesk, 1971), relevance
feedback and text classification (Rocchio, 1971),
word sense disambiguation (Lesk, 1986; Schutze,
1998), summarization (Salton et al., 1997; Lin
and Hovy, 2003), automatic evaluation of machine
translation (Papineni et al., 2002), plagiarism de-
tection (Nawab et al., 2011), and more.
To date, semantic similarity research has pri-
marily focused on comparing text snippets of simi-
lar length (see the semantic textual similarity tasks
organized during *Sem 2013 (Agirre et al., 2013)
and SemEval 2012 (Agirre et al., 2012)). Yet,
as new challenges emerge, such as augmenting a
knowledge-base with textual evidence, assessing
similarity across different context granularities is
gaining traction. The SemEval Cross-level seman-
tic similarity task is aimed at this latter scenario,
and is described in more details in the task paper
(Jurgens et al., 2014).
?
{carmennb,chenditc,mihalcea}@umich.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness us-
ing methods that are either knowledge-based or
corpus-based. Knowledge-based methods derive
a measure of relatedness by utilizing lexical re-
sources and ontologies such as WordNet (Miller,
1995) or Roget (Rog, 1995) to measure defi-
nitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy
as a measure of specificity. There are many
knowledge-based measures that were proposed in
the past, e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;
Hughes and Ramage, 2007).
On the other side, corpus-based measures such
as Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and
Mihalcea, 2011), Pointwise Mutual Informa-
tion (PMI) (Church and Hanks, 1990), PMI-IR
(Turney, 2001), Second Order PMI (Islam and
Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional
similarity (Lin, 1998) employ probabilistic ap-
proaches to decode the semantics of words. They
consist of unsupervised methods that utilize the
contextual information and patterns observed in
raw text to build semantic profiles of words. Un-
like knowledge-based methods, which suffer from
limited coverage, corpus-based measures are able
to induce the similarity between any two words, as
long as they appear in the corpus used for training.
3 System Description
3.1 Generic Features
Our system employs both knowledge and corpus-
based measures as detailed below.
560
Knowledge-based features
Knowledge-based metrics were shown to provide
high correlation scores with the goldstandard in
text similarity tasks (Agirre et al., 2012; Agirre et
al., 2013). We used three WordNet-based simi-
larity measures that employ information content.
We chose these metrics because they are able to
incorporate external information derived from a
large corpus: Resnik (Resnik, 1995) (RES), Lin
(Lin, 1998) (LIN ), and Jiang & Conrath (Jiang
and Conrath, 1997) (JCN ).
Corpus based features
Our corpus based features are derived from a
deep learning vector space model that is able to
?understand? word meaning without human in-
put. Distributed word embeddings are learned us-
ing a skip-gram recurrent neural net architecture
running over a large raw corpus (Mikolov et al.,
2013b; Mikolov et al., 2013a). A primary advan-
tage of such a model is that, by breaking away
from the typical n-gram model that sees individual
units with no relationship to each other, it is able to
generalize and produce word vectors that are simi-
lar for related words, thus encoding linguistic reg-
ularities and patterns (Mikolov et al., 2013b). For
example, vec(Madrid)-vec(Spain)+vec(France) is
closer to vec(Paris) than any other word vec-
tor (Mikolov et al., 2013a). We used the pre-
trained Google News word2vec model (WTV )
built over a 100 billion words corpus, and con-
taining 3 million 300-dimension vectors for words
and phrases. The model is distributed with the
word2vec toolkit.
1
Since the methods outlined above provide similar-
ity scores at the sense or word level, we derive text
level metrics by employing two methods.
VectorSum. We add the vectors corresponding to
the non-stopwords tokens in bag of words (BOW)
A and B, resulting in vectors V
A
and V
B
, respec-
tively. The assumption is that these vectors are
able to capture the semantic meaning associated
with the contexts, enabling us to gauge their relat-
edness using cosine similarity.
Align. Given two BOW A and B as input, we
compare them using a word-alignment-based sim-
ilarity measure (Mihalcea et al., 2006). We calcu-
late the pairwise similarity between the words in
A and B, and match each word in A with its most
similar counterpart in B. For corpus-based fea-
1
https://code.google.com/p/word2vec/
tures, the similarity measure represents the aver-
age over these scores, while for knowledge-based
measures, we consider the top 40% ranking pairs.
We use the DKPro Similarity package (B?ar et
al., 2013) to compute knowledge-based metrics,
and the word2vec implementation from the Gen-
sim toolkit (Rehurek and Sojka, 2010).
3.2 Feature Variations
Since our system participated in all four lexical
levels evaluations, we describe below the modifi-
cations pertaining to each.
word2sense. At the word2sense level, we em-
ploy both knowledge and corpus-based features.
Since the information available in each pair is ex-
tremely limited (only a word and a sense key)
we infuse contextual information by drawing on
WordNet (Miller, 1995). In WordNet, the sense
of each word is encapsulated in a uniquely iden-
tifiable synset, consisting of the definition (gloss),
usage examples and its synonyms. We can derive
three variations (where the word and sense com-
ponents are represented by BOW A and B, respec-
tively): a) no expansion (A={word}, B={sense}),
b) expand right (R) (A={word}, B={sense gloss
& example}), c) expand left (L) & right (R)
(A={word glosses & examples}, B={sense gloss
& example}). After applying the Align method,
we obtain measures JNC, LIN , RES and
WTV 1; VectorSum results in WTV 2.
phrase2word. As this lexical level also suf-
fers from low context, we adapt the above vari-
ations, where the phrase and word components
are represented by BOW A and BOW B, re-
spectively. Thus, we have: a) no expan-
sion (A={phrase}, B={word}), b) expand R
(A={phrase}, B={word glosses and examples}),
c) expand L & R (A={phrase glosses & exam-
ples}, B={word glosses and examples}). We ex-
tract the same measures as for word2sense.
sentence2phrase. For this variation, we use only
corpus based measures; BOW A represents the
sentence component, B, the phrase. Since there is
sufficient context available, we follow the no ex-
pansion variation, and obtain metrics WTV 1 (by
applying Align) and WTV 2 (using VectorSum).
paragraph2sentence. At this level, due to the
long context that entails one-to-many mappings
between the words in the sentence and those in
the paragraph, we use a text clustering technique
prior to calculating the features? weights.
561
a) no clustering. We use only corpus based mea-
sures, where the paragraph represents BOW A,
and the sentence represents BOW B. Then we ap-
ply Align and VectorSum, resulting in WTV 1 and
WTV 2, respectively.
b) paragraph centroids extraction. Since the
longer text contains more information compared
to the shorter one, we extract k topic vectors after
K-means clustering the left context.
2
These cen-
troids are able to model topics permeating across
sentences, and by comparing them with the word
vectors pertaining to the short text, we seek to cap-
ture how much of the information is covered in the
shorter text. Each word is paired with the centroid
that it is closest to, and the average is computed
over these scores, resulting in WTV 3.
c) sentence centroids extraction. Under a dif-
ferent scenario, assuming that one sentence cov-
ers only a few strongly expressed topics, unlike
a paragraph that may digress and introduce unre-
lated noise, we apply clustering on the short text.
The centroids thus obtained are able to capture
the essence of the sentence, so when compared to
every word in the paragraph, we can gauge how
much of the short text is reflected in the longer
one. Each centroid is paired with the word that it is
most similar to, and we average these scores, thus
obtaining WTV 4. In a way, methods b) and c)
provide a macro, respectively micro view of how
the topics are reflected across the two spans of text.
3.3 Meta-learning
The measures of similarity described above pro-
vide a single score per each long text - short text
pair in the training and test data. These scores then
become features for a meta-learner, which is able
to optimize their impact on the prediction process.
We experimented with multiple regression algo-
rithms by conducting 10 fold cross-validation on
the training data. The strongest performer across
all lexical levels was Gaussian processes with a
radial basis function (RBF) kernel. Gaussian pro-
cesses regression is an efficient probabilistic pre-
diction framework that assumes a Gaussian pro-
cess prior on the unobservable (latent) functions
and a likelihood function that accounts for noise.
An individual classifier
3
was trained for each lex-
ical level and applied to the test data sets.
2
Implementation provided in the Scikit library (Pedregosa
et al., 2011), where k is set to 3.
3
Implementation available in the WEKA machine learn-
ing software (Hall et al., 2009) using the default parameters.
4 Evaluations & Discussion
Our system participated in all cross-level subtasks
under the name SimCompass, competing with 37
other systems developed by 20 teams.
Figure 1 highlights the Pearson correlations at
the four lexical levels between the gold standard
and each similarity measure introduced in Section
3, as well as the predictions ensuing as a result
of meta-learning. The left and right histograms in
each subfigure present the scores obtained on the
train and test data, respectively.
In the case of word2sense train data, we no-
tice that expanding the context provides additional
information and improves the correlation results.
For corpus-based measures, the correlations are
stronger when the expansion involves only the
right side of the tuple, namely the sense. We
notice an increase of 0.04 correlation points for
WTV1 and 0.09 for WTV2. As soon as the word
is expanded as well, the context incorporates too
much noise, and the correlation levels drop. In
the case of knowledge-based measures, expanding
the context does not seem to impact the results.
However, these trends do not carry out to the test
data, where the corpus-based features without ex-
pansion reach a correlation higher than 0.3, while
the knowledge-based features score significantly
lower (by 0.16). Once all these measures are used
as features in a meta learner (All) using Gaus-
sian processes regression (GP), the correlation in-
creases over the level attained by the best perform-
ing individual feature, reaching 0.45 on the train
data and 0.36 on the test data. SimCompass ranks
second in this subtask?s evaluations, falling short
of the leading system by 0.025 correlation points.
Turning now to the phrase2word subfigure, we
notice that the context already carries sufficient
information, and expanding it causes the perfor-
mance to drop (the more extensive the expan-
sion, the steeper the drop). Unlike the scenario
encountered for word2sense, the trend observed
here on the training data also gets mirrored in the
test data. Same as before, knowledge-based mea-
sures have a significantly lower performance, but
deep learning-based features based on word2vec
(WTV) only show a correlation variation by at
most 0.05, proving their robustness. Leveraging
all the features in a meta-learning framework en-
ables the system to predict stronger scores for both
the train and the test data (0.48 and 0.42, respec-
tively). Actually, for this variation, SimCompass
562
 0
 0.1
 0.2
 0.3
 0.4
 0.5
w
o
rd
2s
en
se
Train Test
 0
 0.1
 0.2
 0.3
 0.4
 0.5
BL JNC
LIN
RES
W
TV
1
W
TV
2
G
P
JN
C
LIN
RES
W
TV
1
W
TV
2
G
P
ph
ra
se
2w
or
d
No expansion Expansion R Expansion L&R All
 0
 0.2
 0.4
 0.6
 0.8
s
e
n
te
n
ce
2p
hr
as
e
Train Test
BL WTV
1
W
TV
2
W
TV
3
W
TV
4
G
P
W
TV
1
W
TV
2
W
TV
3
W
TV
4
G
P
 0
 0.2
 0.4
 0.6
 0.8
pa
ra
gr
ap
h2
se
nt
en
ce
Figure 1: Pearson correlation of individual measures on the train and test data sets. As these measures be-
come features in a regression algorithm (GP), prediction correlations are included as well. BL represents
the baseline computed by the organizers.
obtains the highest score among all competing sys-
tems, surpassing the second best by 0.10.
Noticing that expansion is not helpful when suf-
ficient context is available, for the next variations
we use the original tuples. Also, due to the re-
duced impact of knowledge-based features on the
training outcome, we only focus on deep learning
features (WTV1, WTV2, WTV3, WTV4).
Shifting to sentence2phrase, WTV2 (con-
structed using VectorSum) is the top perform-
ing feature, surpassing the baseline by 0.19,
and attaining 0.69 and 0.73 on the train and
test sets, respectively. Despite also considering
a lower performing feature (WTV1), the meta-
learner maintains high scores, surpassing the cor-
relation achieved on the train data by 0.04 (from
0.70 to 0.74). In this variation, our system ranks
fifth, at 0.035 from the top system.
For the paragraph2sentence variation, due to
the availability of longer contexts, we introduce
WTV3 and WTV4 that are based on clustering the
left and the right sides of the tuple, respectively.
WTV2 fares slightly better than WTV3 and WTV4.
WTV1 surpasses the baseline this time, leaving its
mark on the decision process. When training the
GP learner on all features, we obtain 0.78 correla-
tion on the train data, and 0.81 on test data, 0.10
higher than those attained by the individual fea-
tures alone. SimCompass ranks seventh in perfor-
mance on this subtask, at 0.026 from the first.
Considering the overall system performance,
SimCompass is remarkably versatile, ranking
among the top at each lexical level, and taking the
first place in the SemEval Task 3 overall evalu-
ation with respect to both Pearson (0.58 average
correlation) and Spearman correlations.
5 Conclusion
We described SimCompass, the system we partic-
ipated with at SemEval-2014 Task 3. Our exper-
iments suggest that traditional knowledge-based
features are cornered by novel corpus-based word
meaning representations, such as word2vec, which
emerge as efficient and strong performers under
a variety of scenarios. We also explored whether
context expansion is beneficial to the cross-level
similarity task, and remarked that only when the
context is particularly short, this enrichment is vi-
able. However, in a meta-learning framework, the
information permeating from a set of similarity
measures exposed to varying context expansions
can attain a higher performance than possible with
individual signals. Overall, our system ranked first
among 21 teams and 38 systems.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation CAREER
award #1361274 and IIS award #1018613 and
by DARPA-BAA-12-47 DEFT grant #12475008.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
563
of the National Science Foundation or the Defense
Advanced Research Projects Agency.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012), Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pi-
lot on Typed-Similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013).
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121?126,
Sofia, Bulgaria.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: words, sentences,
discourse. Discourse Processes, 25(2):211?257.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606?1611, Hyderabad, India.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Samer Hassan and Rada Mihalcea. 2011. Measuring
semantic relatedness using salient encyclopedic con-
cepts. Artificial Intelligence, Special Issue.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic knowledge with random graph walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Prague, Czech
Republic.
Aminul Islam and Diana Zaiu Inkpen. 2006. Second
order co-occurrence PMI for determining the seman-
tic similarity of words. In Proceedings of the Fifth
Conference on Language Resources and Evaluation,
volume 2, pages 1033?1038, Genoa, Italy, July.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings
of the conference on Recent Advances in Natural
Language Processing RANLP-2003, Borovetz, Bul-
garia, September.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceeding of the International Confer-
ence Research on Computational Linguistics (RO-
CLING X), Taiwan.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review, 104.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database. The MIT Press.
Michael E. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Pro-
ceedings of the SIGDOC Conference 1986, Toronto,
June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of Human Lan-
guage Technology Conference (HLT-NAACL 2003),
Edmonton, Canada, May.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In American
Association for Artificial Intelligence (AAAI-2006),
Boston, MA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Distributed Representations of Words
and Phrases and their Compositionality . In NIPS,
pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL HLT, pages 746?
751, Atlanta, GA, USA.
George A. Miller. 1995. WordNet: a Lexical database
for English. Communications of the Association for
Computing Machinery, 38(11):39?41.
Rao Muhammad Adeel Nawab, Mark Stevenson, and
Paul Clough. 2011. External plagiarism detection
using information retrieval and sequence alignment:
564
Notebook for PAN at CLEF 2011. In Proceedings
of the 5th International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2011).
Kishore. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448?453, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
1995. Roget?s II: The New Thesaurus. Houghton Mif-
flin.
Gerard Salton and Michael E. Lesk, 1971. The SMART
Retrieval System: Experiments in Automatic Doc-
ument Processing, chapter Computer evaluation of
indexing and text processing. Prentice Hall, Ing. En-
glewood Cliffs, New Jersey.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 2(32).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
565
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 195?203,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Alexander Conrad
University of Pittsburgh
conrada@cs.pitt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
Amazon Mechanical Turk (MTurk) is a mar-
ketplace for so-called ?human intelligence
tasks? (HITs), or tasks that are easy for hu-
mans but currently difficult for automated pro-
cesses. Providers upload tasks to MTurk
which workers then complete. Natural lan-
guage annotation is one such human intelli-
gence task. In this paper, we investigate us-
ing MTurk to collect annotations for Subjec-
tivity Word Sense Disambiguation (SWSD),
a coarse-grained word sense disambiguation
task. We investigate whether we can use
MTurk to acquire good annotations with re-
spect to gold-standard data, whether we can
filter out low-quality workers (spammers), and
whether there is a learning effect associated
with repeatedly completing the same kind of
task. While our results with respect to spam-
mers are inconclusive, we are able to ob-
tain high-quality annotations for the SWSD
task. These results suggest a greater role for
MTurk with respect to constructing a large
scale SWSD system in the future, promising
substantial improvement in subjectivity and
sentiment analysis.
1 Introduction
Many Natural Language Processing (NLP) systems
rely on large amounts of manually annotated data
that is collected from domain experts. The anno-
tation process to obtain this data is very laborious
and expensive. This makes supervised NLP systems
subject to a so-called knowledge acquisition bottle-
neck. For example, (Ng, 1997) estimates an effort of
16 person years to construct training data for a high-
accuracy domain independent Word Sense Disam-
biguation (WSD) system.
Recently researchers have been investigating
Amazon Mechanical Turk (MTurk) as a source of
non-expert natural language annotation, which is a
cheap and quick alternative to expert annotations
(Kaisser and Lowe, 2008; Mrozinski et al, 2008).
In this paper, we utilize MTurk to obtain training
data for Subjectivity Word Sense Disambiguation
(SWSD) as described in (Akkaya et al, 2009). The
goal of SWSD is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with ob-
jective senses. SWSD is a new task which suffers
from the absence of a substantial amount of anno-
tated data and thus can only be applied on a small
scale. SWSD has strong connections to WSD. Like
supervised WSD, it requires training data where tar-
get word instances ? words which need to be dis-
ambiguated by the system ? are labeled as having
an objective sense or a subjective sense. (Akkaya
et al, 2009) show that SWSD may bring substantial
improvement in subjectivity and sentiment analysis,
if it could be applied on a larger scale. The good
news is that training data for 80 selected keywords is
enough to make a substantial difference (Akkaya et
al., 2009). Thus, large scale SWSD is feasible. We
hypothesize that annotations for SWSD can be pro-
vided by non-experts reliably if the annotation task
is presented in a simple way.
The annotations obtained from MTurk workers
are noisy by nature, because MTurk workers are
not trained for the underlying annotation task. That
is why previous work explored methods to assess
annotation quality and to aggregate multiple noisy
annotations for high reliability (Snow et al, 2008;
Callison-Burch, 2009). It is understandable that not
every worker will provide high-quality annotations,
195
depending on their background and interest. Un-
fortunately, some MTurk workers do not follow the
annotation guidelines and carelessly submit annota-
tions in order to gain economic benefits with only
minimal effort. We define this group of workers
as spammers. We believe it is essential to distin-
guish between workers as well-meaning annotators
and workers as spammers who should be filtered out
as a first step when utilizing MTurk. In this work,
we investigate how well the built-in qualifications in
MTurk function as such a filter.
Another important question about MTurk workers
is whether they learn to provide better annotations
over time in the absence of any interaction and feed-
back. The presence of a learning effect may support
working with the same workers over a long time and
creating private groups of workers. In this work, we
also examine if there is a learning effect associated
with MTurk workers.
To summarize, in this work we investigate the fol-
lowing questions:
? Can MTurk be utilized to collect reliable train-
ing data for SWSD ?
? Are the built-in methods provided by MTurk
enough to avoid spammers ?
? Is there a learning effect associated with MTurk
workers ?
The remainder of the paper is organized as fol-
lows. In Section 2, we give general background in-
formation on the Amazon Mechanical Turk service.
In Section 3, we discuss sense subjectivity. In Sec-
tion 4, we describe the subjectivity word sense dis-
ambiguation task. In Section 5, we discuss the de-
sign of our experiment and our filtering mechanisms
for workers. In Section 6, we evaluate MTurk anno-
tations and relate results to our questions. In Section
7, we review related work. In Section 8, we draw
conclusions and discuss future work.
2 Amazon Mechanical Turk
Amazon Mechanical Turk (MTurk)1 is a market-
place for so-called ?human intelligence tasks,? or
HITs. MTurk has two kinds of users: providers and
1http://mturk.amazon.com
workers. Providers create HITs using the Mechan-
ical Turk API and, for a small fee, upload them to
the HIT database. Workers search through the HIT
database, choosing which to complete in exchange
for monetary compensation. Anyone can sign up as
a provider and/or worker. Each HIT has an associ-
ated monetary value, and after reviewing a worker?s
submission, a provider may choose whether to ac-
cept the submission and pay the worker the promised
sum or to reject it and pay the worker nothing. HITs
typically consist of tasks that are easy for humans
but difficult or impossible for computers to complete
quickly or effectively, such as annotating images,
transcribing speech audio, or writing a summary of
a video.
One challenge for requesters using MTurk is that
of filtering out spammers and other workers who
consistently produce low-quality annotations. In or-
der to allow requesters to restrict the range of work-
ers who can complete their tasks, MTurk provides
several types of built-in statistics, known as quali-
fications. One such qualification is approval rating,
a statistic that records a worker?s ratio of accepted
HITs compared to the total number of HITs sub-
mitted by that worker. Providers can require that a
worker?s approval rating be above a certain threshold
before allowing that worker to submit one of his/her
HITs. Country of residence and lifetime approved
number of HITs completed also serve as built-in
qualifications that providers may check before al-
lowing workers to access their HITs.2 Amazon also
allows providers to define their own qualifications.
Typically, provider-defined qualifications are used to
ensure that HITs which require particular skills are
only completed by qualified workers. In most cases,
workers acquire provider-defined qualifications by
completing an online test.
Amazon also provides a mechanism by which
multiple unique workers can complete the same HIT.
The number of times a HIT is to be completed is
known as the number of assignments for the HIT.
By having multiple workers complete the same HIT,
2According to the terms of use, workers are prohibited from
having more than one account, but to the writer?s knowledge
there is no method in place to enforce this restriction. Thus,
a worker with a poor approval rating could simply create a
new account, since all accounts start with an approval rating
of 100%.
196
Subjective senses:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
Objective senses:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
Figure 1: Subjective and objective word sense examples.
techniques such as majority voting among the sub-
missions can be used to aggregate the results for
some types of HITs, resulting in a higher-quality
final answer. Previous work (Snow et al, 2008)
demonstrates that aggregating worker submissions
often leads to an increase in quality.
3 Word Sense Subjectivity
(Wiebe and Mihalcea, 2006) define subjective ex-
pressions as words and phrases being used to ex-
press mental and emotional states, such as specula-
tions, evaluations, sentiments, and beliefs. Many ap-
proaches to sentiment and subjectivity analysis rely
on lexicons of such words (subjectivity clues). How-
ever, such clues often have both subjective and ob-
jective senses, as illustrated by (Wiebe and Mihal-
cea, 2006). Figure 1 provides subjective and objec-
tive examples of senses.
(Akkaya et al, 2009) points out that most sub-
jectivity lexicons are compiled as lists of keywords,
rather than word meanings (senses). Thus, subjec-
tivity clues used with objective senses ? false hits ?
are a significant source of error in subjectivity and
sentiment analysis. SWSD specifically deals with
this source of errors. (Akkaya et al, 2009) shows
that SWSD helps with various subjectivity and sen-
timent analysis systems by ignoring false hits.
4 Annotation Task
4.1 Subjectivity Word Sense Disambiguation
Our target task is Subjectivity Word Sense Disam-
biguation (SWSD). SWSD aims to determine which
word instances in a corpus are being used with sub-
jective senses and which are being used with ob-
jective senses. It can be considered to be a coarse-
grained application-specific WSD that distinguishes
between only two senses: (1) the subjective sense
and (2) the objective sense.
Subjectivity word sense annotation is done in the
following way. We try to keep the annotation task
for the worker as simple as possible. Thus, we do
not directly ask them if the instance of a target word
has a subjective or an objective sense (without any
sense inventory), because the concept of subjectivity
is fairly difficult to explain to someone who does not
have any linguistics background. Instead we show
MTurk workers two sets of senses ? one subjective
set and one objective set ? for a specific target word
and a text passage in which the target word appears.
Their job is to select the set that best reflects the
meaning of the target word in the text passage. The
specific sense set automatically gives us the subjec-
tivity label of the instance. This makes the annota-
tion task easier for them as (Snow et al, 2008) shows
that WSD can be done reliably by MTurk workers.
This approach presupposes a set of word senses that
have been annotated as subjective or objective. The
annotation of senses in a dictionary for subjectivity
is not difficult for an expert annotator. Moreover,
it needs to be done only once per target word, al-
lowing us to collect hundreds of subjectivity labeled
instances for each target word through MTurk.
In this annotation task, we do not inform the
MTurk workers about the nature of the sets. This
means the MTurk workers have no idea that they are
annotating subjectivity of senses; they are just se-
lecting the set which contains a sense matching the
usage in the sentence or being as similar to it as pos-
sible. This ensures that MTurk workers are not bi-
ased by the contextual subjectivity of the sentence
while tagging the target word instance.
197
Sense Set1 (Subjective)
{ look, appear, seem } ? give a certain impression or have a
certain outward aspect; ?She seems to be sleeping?; ?This ap-
pears to be a very difficult problem?; ?This project looks fishy?;
?They appeared like people who had not eaten or slept for a
long time?
{ appear, seem } ? seem to be true, probable, or apparent; ?It
seems that he is very gifted?; ?It appears that the weather in
California is very bad?
Sense Set2 (Objective)
{ appear } ? come into sight or view; ?He suddenly appeared
at the wedding?; ?A new star appeared on the horizon?
{ appear, come out } ? be issued or published, as of news in a
paper, a book, or a movie; ?Did your latest book appear yet??;
?The new Woody Allen film hasn?t come out yet?
{ appear, come along } ? come into being or existence, or ap-
pear on the scene; ?Then the computer came along and changed
our lives?; ?Homo sapiens appeared millions of years ago?
{ appear } ? appear as a character on stage or appear in a play,
etc.; ?Gielgud appears briefly in this movie?; ?She appeared in
?Hamlet? on the London
{ appear } ? present oneself formally, as before a (judicial) au-
thority; ?He had to appear in court last month?; ?She appeared
on several charges of theft?
Figure 2: Sense sets for target word ?appear?.
Below, we describe a sample annotation problem.
An MTurk worker has access to the following two
sense sets of the target word ?appear?, as seen in
Figure 2. The information that the first sense set is
subjective and second sense set is objective is not
available to the worker. The worker is presented
with the following text passage holding the target
word ?appear?.
It?s got so bad that I don?t even know what
to say. Charles |target| appeared |target|
somewhat embarrassed by his own behav-
ior. The hidden speech was coming, I
could tell.
In this passage, the MTurk worker should be able
to understand that ?appeared? refers to the outward
impression given by ?Charles?. This use of appear is
most similar to the first entry in sense set one; thus,
the correct answer for this problem is Sense Set-1.
4.2 Gold Standard
The gold standard dataset, on which we evaluate
MTurk worker annotations, is provided by (Akkaya
et al, 2009). This dataset (called subjSENSEVAL)
consists of target word instances in a corpus labeled
as S or O, indicating whether they are used with
a subjective or objective sense. It is based on the
lexical sample corpora from SENSEVAL1 (Kilgar-
riff and Palmer, 2000), SENSEVAL2 (Preiss and
Yarowsky, 2001), and SENSEVAL3 (Mihalcea and
Edmonds, 2004). SubjSENSEVAL consists of in-
stances for 39 ambiguous (having both subjective
and objective meanings) target words.
(Akkaya et al, 2009) also provided us with sub-
jectivity labels for word senses which are used in the
creation of subjSENSEVAL. Sense labels of the tar-
get word senses are defined on the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3). This means the target words
from SENSEVAL1 have their senses annotated in
the Hector dictionary, while the target words from
SENSEVAL2 and SENSEVAL3 have their senses
annotated in WordNet1.7. We make use of these la-
beled sense inventories to build our subjective and
objective sets of senses, which we present to the
MTurk worker as Sense Set1 and Sense Set2 re-
spectively. We want to have a uniform sense rep-
resentation for the words we ask subjectivity sense
labels for. Thus, we consider only SENSEVAL2 and
SENSEVAL3 subsets of subjSENSEVAL, because
SENSEVAL1 relies on a sense inventory other than
WordNet.
5 Experimental Design
We chose randomly 8 target words that have a distri-
bution of subjective and objective instances in sub-
jSENSEVAL with less skew than 75%. That is, no
more than 75% of a word?s senses are subjective or
objective. Our concern is that using skewed data
might bias the workers to choose from the more fre-
quent label without thinking much about the prob-
lem. Another important fact is that these words with
low skew are more ambiguous and responsible for
more false hits. Thus, these target words are the ones
for which we really need subjectivity word sense
disambiguation. For each of these 8 target words, we
select 40 passages from subjSENSEVAL in which
the target word appears, to include in our experi-
ments. Table 1 summarizes the selected target words
198
Word FLP Word FLP
appear 55% fine 72.5%
judgment 65% solid 55%
strike 62.5% difference 67.5%
restraint 70% miss 50%
Average 62.2%
Table 1: Frequent label percentages for target words.
and their label distribution. In this table, frequent la-
bel percentage (FLP) represents the skew for each
word. A word?s FLP is equal to the percent of the
senses that are of the most frequently occurring type
of sense (subjective or objective) for that word.
We believe this annotation task is a good candi-
date for attracting spammers. This task requires only
binary annotations, where the worker just chooses
from one of the two given sets, which is not a dif-
ficult task. Since it is easy to provide labels, we
believe that there will be a distinct line, with re-
spect to quality of annotations, between spammers
and mediocre annotators.
For our experiments, we created three different
HIT groups each having different qualification re-
quirements but sharing the same data. To be con-
crete, each HIT group consists of the same 320 in-
stances: 40 instances for each target word listed in
Table 1. Each HIT presents an MTurk worker with
four instances of the same word in a text passage
? this makes 80 HITs for each HIT group ? and
asks him to choose the set to which the activated
sense belongs. We know for each HIT the mapping
between sense set numbers and subjectivity. Thus,
we can evaluate each HIT response on our gold-
standard data, as discussed in Section 4.2. We pay
seven cents per HIT. We consider this to be generous
compensation for such a simple task.
There are many builtin qualifications in MTurk.
We concentrated only on three of them: location,
HIT approval rate, and approved HITs, as discussed
in Section 2. In our experience, these qualifications
are widely used for quality assurance. As mentioned
before, we created three different HIT groups in or-
der to see how well different built-in qualification
combinations do with respect to filtering spammers.
These groups ? starting from the least constrained to
the most constrained ? are listed in Table 2.
Group1 Location: USA
Group2 Location: USAHIT Approval Rate > 96%
Group3
Location: USA
HIT Approval Rate > 96%
Approved HITs > 500
Table 2: Constraints for each HIT group.
Group1 required only that the MTurk workers are
located in the US. This group is the least constrained
one. Group2 additionally required an approval rate
greater than 96%. Group3 is the most constrained
one, requiring a lifetime approved HIT number to
be greater than 500, in addition to the qualifications
in Group1 and Group2.
We believe that neither location nor approval rate
and location together is enough to avoid spammers.
While being a US resident does to some extent guar-
antee English proficiency, it does not guarantee well-
thought answers. Since there is no mechanism in
place preventing users from creating new MTurk
worker accounts at will and since all worker ac-
counts are initialized with a 100% approval rate, we
do not think that approval rate is sufficient to avoid
serial spammers and other poor annotators. We hy-
pothesize that the workers with high approval rate
and a large number of approved HITs have a reputa-
tion to maintain, and thus will probably be careful in
their answers. We think it is unlikely that spammers
will have both a high approval rate and a large num-
ber of completed HITs. Thus, we anticipated that
Group3?s annotations will be of higher quality than
those of the other groups.
Note that an MTurk worker who has access to the
HITs in one of the HIT groups also has access to
HITs in less constrained groups. For example, an
MTurk worker who has access to HITs in Group3
also has access to HITs in Group2 and Group1. We
did not prevent MTurk workers from working in
multiple HIT groups because we did not want to
influence worker behavior, but instead simulate the
most realistic annotation scenario.
In addition to the qualifications described above,
we also required each worker to take a qualification
test in order to prove their competence in the anno-
tation task. The qualification test consists of 10 sim-
199
Figure 3: Venn diagram illustrating worker distribution.
ple annotation questions identical in form to those
present in the HITs. These questions are split evenly
between two target words, ?appear? and ?restraint?.
There are a total of five subjective and five objective
usages in the test. We required an accuracy of 90%
in the qualification test, corresponding to a Kappa
score of .80, before a worker was allowed to submit
any of our HITs. If a worker failed to achieve a score
of 90% on an attempt, that worker could try the test
again after a delay of 4 hours.
We collected three sets of assignments within
each HIT group. In other words, each HIT was com-
pleted three times by three different workers in each
group. This gives us a total of 960 assignments in
each HIT group. A total of 26 unique workers par-
ticipated in the experiment: 17 in Group1, 17 in
Group2 and 8 in Group3. As mentioned before, a
worker is able to participate in all the groups for
which he is qualified. Thus the unique worker num-
bers in each group does not sum up to the total num-
ber of workers in the experiment, since some work-
ers participated in the HITs for more than one group.
Figure 3 summarizes how workers are distributed
between groups.
6 Evaluation
We are interested in how accurate the MTurk annota-
tions are with respect to gold-standard data. We are
also interested in how the accuracy of each group
differs from the others. We evaluate each group it-
self separately on the gold-standard data. Addition-
ally, we evaluate each worker?s performance on the
gold-standard data and inspect their distribution in
various groups.
6.1 Group Evaluation
As mentioned in the previous section, we collect
three annotations for each HIT. They are assigned to
respective trials in the order submitted by the work-
ers. The results are summarized in Table 3. Trials
are labeled as TX and MV is the majority vote an-
notation among the three trials. The final column
contains the baseline agreement where a worker la-
bels each instance of a word with the most frequent
label of that word in the gold-standard data. It is
clear from this table that, since worker accuracy
always exceeds the baseline agreement, subjectiv-
ity word sense annotation can be done reliably by
MTurk workers. This is very promising. Consid-
ering the low cost and low time required to obtain
MTurk annotations, a large scale SWSD is realis-
tic. For example, (Akkaya et al, 2009) shows that
the most frequent 80 lexicon keywords are respon-
sible for almost half of the false hits in the MPQA
Corpus3 (Wiebe et al, 2005; Wilson, 2008), a cor-
pus annotated for subjective expressions. Utilizing
MTurk to collect training data for these 80 lexicon
keywords will be quick and cheap and most impor-
tantly reliable.
When we compare groups with each other, we
see that the best trial result is achieved in Group3.
However, according to McNemar?s test (Dietterich,
1998), there is no statistically significant difference
between any trial of any group. On the other hand,
the best majority vote annotation is achieved in
Group2, but again there is no statistically significant
difference between any majority vote annotation of
any group. These results are surprising to us, since
we do not see any significant difference in the qual-
ity of the data throughout different groups.
6.2 Worker Evaluation
In this section, we evaluate all 26 workers and group
them as either spammers or well-meaning workers.
All workers who deviate from the gold-standard by a
3http://www.cs.pitt.edu/mpqa/
200
Group3 Group2 Group1 baseline
T1 T2 T3 MV T1 T2 T3 MV T1 T2 T3 MV
Accuracy 89.7 86.9 86.6 88.4 87.2 86.3 88.1 90.3 84.4 87.5 87.5 88.4 62.2
Kappa .79 .74 .73 .77 .74 .73 .76 .81 .69 .75 .75 .77
Table 3: Accuracy and kappa scores for each group of workers.
Threshold 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
Spammer Count
G1 2 2 2 2 2 4 7 9
G2 1 2 2 2 2 3 5 8
G3 0 0 0 0 0 0 2 2
Spammer Percentage
G1 12% 12% 12% 12% 12% 24% 41% 53%
G2 6% 12% 12% 12% 12% 12% 29% 42%
G3 0% 0% 0% 0% 0% 0% 25% 25%
Table 4: Spammer representation in groups.
large margin beyond a certain threshold will be con-
sidered to be spammers. As discussed in Section 5,
we require all participating workers to pass a quali-
fication test before answering HITs. Thus, we know
that they are competent to do subjectivity sense an-
notations, and providing consistently erroneous an-
notations means that they are probably spammers.
We think a kappa score of 0.6 is a good threshold
to distinguish spammers from well-meaning work-
ers. For this threshold, we had 2 spammers par-
ticipating in Group1, 2 spammers in Group2 and
0 spammers in Group3. Table 4 presents spammer
count and spammer percentage in each group for
various threshold values. We see that Group3 has
consistently fewer spammers and a smaller spammer
percentage. The lowest kappa scores for Group1,
Group2, and Group3 are .35, .40, and .69, respec-
tively. The mean kappa scores for Group1, Group2,
and Group3 are .73, .75, and .77, respectively.
These results indicate that Group3 is less prone
to spammers, apparently contradicting Section 6.1.
We see the reason when we inspect the data more
closely. It turns out that spammers contributed in
Group1 and Group2 only minimally. On the other
hand there are two mediocre workers (Kappa of
0.69) who submit around 1/3 of the HITs in Group3.
This behavior might be a coincidence. In the face of
contradicting results, we think that we need a more
extensive study to derive conclusions about the rela-
tion between spammer distribution and built-in qual-
ification.
6.3 Learning Effect
Expert annotators can learn to provide more accu-
rate annotations over time. (Passonneau et al, 2006)
reports a learning effect early in the annotation pro-
cess. This might be due to the formal and informal
interaction between annotators. Another possibility
is that the annotators might get used to the annota-
tion task over time. This is to be expected if there is
not an extensive training process before the annota-
tion takes place.
On the other hand, the MTurk workers have no
interaction among themselves. They do not receive
any formal training and do not have access to true
annotations except a few examples if provided by
the requester. These properties make MTurk work-
ers a unique annotation workforce. We are interested
if the learning effect common to expert annotators
holds in this unique workforce in the absence of any
interaction and feedback. That may justify working
with the same set of workers over a long time by
creating private groups of workers.
We sort annotations of a worker after the submis-
sion date. This way, we get for each worker an or-
dered list of annotations. We split the list into bins
of size 40 and we test for an increasing trend in
the proportion of successes over time. We use the
Chi-squared Test for binomial proportions (Rosner,
2006). Using this test, we find that all of the p-values
201
are substantially larger than 0.05. Thus, there is no
increasing trend in the proportion of successes and
no learning effect. This is true for both mediocre
workers and very reliable workers. We think that the
results may differ for harder annotation tasks where
the input is more complex and requires some adjust-
ment.
7 Related Work
There has been recently an increasing interest in
Amazon Mechanical Turk. Many researchers have
utilized MTurk as a source of non-expert natural
language annotation to create labeled datasets. In
(Mrozinski et al, 2008), MTurk workers are used to
create a corpus of why-questions and corresponding
answers on which QA systems may be developed.
(Kaisser and Lowe, 2008) work on a similar task.
They make use of MTurk workers to identify sen-
tences in documents as answers and create a corpus
of question-answer sentence pairs. MTurk is also
considered in other fields than natural language pro-
cessing. For example, (Sorokin and Forsyth, 2008)
utilizes MTurk for image labeling. Our ultimate goal
is similar; namely, to build training data (in our case
for SWSD).
Several studies have concentrated specifically on
the quality aspect of the MTurk annotations. They
investigated methods to assess annotation quality
and to aggregate multiple noisy annotations for high
reliability. (Snow et al, 2008) report MTurk an-
notation quality on various NLP tasks (e.g. WSD,
Textual Entailment, Word Similarity) and define
a bias correction method for non-expert annota-
tors. (Callison-Burch, 2009) uses MTurk workers
for manual evaluation of automatic translation qual-
ity and experiments with weighed voting to com-
bine multiple annotations. (Hsueh et al, 2009) de-
fine various annotation quality measures and show
that they are useful for selecting annotations leading
to more accurate classifiers. Our work investigates
the effect of built-in qualifications on the quality of
MTurk annotations.
(Hsueh et al, 2009) applies MTurk to get senti-
ment annotations on political blog snippets. (Snow
et al, 2008) utilizes MTurk for affective text annota-
tion task. In both works, MTurk workers annotated
larger entities but on a more detailed scale than we
do. (Snow et al, 2008) also provides a WSD anno-
tation task which is similar to our annotation task.
The difference is the MTurk workers are choosing
an exact sense not a sense set.
8 Conclusion and Future Work
In this paper, we address the question of whether
built-in qualifications are enough to avoid spam-
mers. The investigation of worker performances
indicates that the lesser constrained a group is the
more spammers it attracts. On the other hand, we did
not find any significant difference between the qual-
ity of the annotations for each group. It turns out that
workers considered as spammers contributed only
minimally. We do not know if it is just a coincidence
or if it is correlated to the task definition. We did not
get conclusive results. We need to do more extensive
experiments before arriving at conclusions.
Another aspect we investigated is the learning ef-
fect. Our results show that there is no improvement
in annotator reliability over time. We should not ex-
pect MTurk workers to provide more consistent an-
notations over time. This will probably be the case
in similar annotation tasks. For harder annotation
tasks (e.g. parse tree annotation) things may be dif-
ferent. An interesting follow-up would be whether
showing the answers of other workers on the same
HIT will promote learning.
We presented our subjectivity sense annotation
task to the worker in a very simple way. The an-
notation results prove that subjectivity word sense
annotation can be done reliably by MTurk workers.
This is very promising since the MTurk annotations
can be collected for low costs in a short time pe-
riod. This implies that a large scale general SWSD
component, which can help with various subjectivity
and sentiment analysis tasks, is feasible. We plan to
work with selected workers to collect new annotated
data for SWSD and use this data to train a SWSD
system.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation awards IIS-
0916046 and IIS-0917170 and by Department of
Homeland Security award N000140710152. The au-
thors are grateful to the three paper reviewers for
their helpful suggestions.
202
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 286?295, Morristown, NJ,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10:1895?1923.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In HLT ?09: Proceedings
of the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 27?35, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Kaisser and John Lowe. 2008. Creat-
ing a research collection of question answer sen-
tence pairs with amazons mechanical turk. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a why-question corpus for develop-
ment and evaluation of an automatic QA-system. In
Proceedings of ACL-08: HLT, pages 443?451, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hwee Tou Ng. 1997. Getting serious about word sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why,What, and How?
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC).
Bernard Rosner. 2006. Fundamentals of Biostatistics.
Thompson Brooks/Cole.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
A. Sorokin and D. Forsyth. 2008. Utility data annotation
with amazon mechanical turk. pages 1 ?8, june.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In (ACL-06), Sydney, Australia.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Theresa Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
203
Word Sense Disambiguation with Multilingual Features
Carmen Banea and Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
carmenbanea@my.unt.edu, rada@cs.unt.edu
Abstract
This paper explores the role played by a multilingual feature representation for the task of word
sense disambiguation. We translate the context of an ambiguous word in multiple languages, and
show through experiments on standard datasets that by using a multilingual vector space we can
obtain error rate reductions of up to 25%, as compared to a monolingual classifier.
1 Introduction
Ambiguity is inherent to human language. In particular, word sense ambiguity is prevalent in all natural
languages, with a large number of the words in any given language carrying more than one meaning.
For instance, the English noun plant can mean green plant or factory; similarly the French word feuille
can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context
where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of
automatically assigning the most appropriate meaning to a polysemous word within a given context.
Among the various knowledge-based (Lesk, 1986; Mihalcea et al, 2004) and data-driven (Yarowsky,
1995; Ng and Lee, 1996) word sense disambiguation methods that have been proposed to date, supervised
systems have been constantly observed as leading to the highest performance. In these systems, the sense
disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence
of a particular word is transformed into a feature vector which is then used in an automatic learning
process. One of the main drawbacks associated with these methods is the fact that their performance is
closely connected to the amount of labeled data available at hand.
In this paper, we investigate a new supervised word sense disambiguation method that is able to take
additional advantage of the sense-labeled examples by exploiting the information that can be obtained
from a multilingual representation. We show that by representing the features in a multilingual space,
we are able to improve the performance of a word sense disambiguation system by a significant margin,
as compared to a traditional system that uses only monolingual features.
2 Related Work
Despite the large number of word sense disambiguation methods that have been proposed so far, targeting
the resolution of word ambiguity in different languages, there are only a few methods that try to explore
more than one language at a time. The work that is perhaps most closely related to ours is the bilin-
gual bootstrapping method introduced in (Li and Li, 2002), where word translations are automatically
disambiguated using information iteratively drawn from two languages. Unlike that approach, which
iterates between two languages to select the correct translation for a given target word, in our method we
simultaneously use the features extracted from several languages. In fact, our method can handle more
than two languages at a time, and we show that the accuracy of the disambiguation algorithm increases
with the number of languages used.
There have also been a number of attempts to exploit parallel corpora for word sense disambiguation
(Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al, 2003), but in that line of work the parallel
25
texts were mainly used as a way to induce word senses or to create sense-tagged corpora, rather than as
a source of additional multilingual views for the disambiguation features. Another related technique is
concerned with the selection of correct word senses in context using large corpora in a second language
(Dagan and Itai, 1994), but as before, the additional language is used to help distinguishing between the
word senses in the original language, and not as a source of additional information for the disambiguation
context.
Also related is the recent SEMEVAL task that has been proposed for cross-lingual lexical substitution,
where the word sense disambiguation task was more flexibly formulated as the identification of cross-
lingual lexical substitutes in context (Mihalcea et al, 2010). A number of different approaches have been
proposed by the teams participating in the task, and although several of them involved the translation of
contexts or substitutes from one language to another, none of them attempted to make simultaneous use
of the information available in the two languages.
Finally, although the multilingual subjectivity classifier proposed in Banea et al (2010) is not directly
applicable to the disambiguation task we address in this paper, their findings are similar to ours. In that
paper, the authors showed how a natural language task can benefit from the use of features drawn from
multiple languages, thus supporting the hypothesis that multilingual features can be effectively used to
improve the accuracy of a monolingual classifier.
3 Motivation
Our work seeks to explore the expansion of a monolingual feature set with features drawn from multiple
languages in order to generate a more robust and more effective vector-space representation that can be
used for the task of word sense disambiguation. While traditional monolingual representations allow a
supervised learning systems to achieve a certain accuracy, we try to surpass this limitation by infusing
additional information in the model, mainly in the form of features extracted from the machine translated
view of the monolingual data. A statistical machine translation (MT) engine does not only provide
a dictionary-based translation of the words surrounding a given ambiguous word, but it also encodes
the translation knowledge derived from very large parallel corpora, thus accounting for the contextual
dependencies between the words.
In order to better explain why a multilingual vector space provides for a better representation for
the word sense disambiguation task, consider the following examples centered around the ambiguous
verb build.1 For illustration purposes, we only show examples for four out of the ten possible meanings
in WordNet (Fellbaum, 1998), and we only show the translations in one language (French). All the
translations are performed using the Google Translate engine.
En 1: Telegraph Co. said it will spend $20 million to build a factory in Guadalajara, Mex-
ico, to make telephone answering machines. (sense id 1)
Fr 1: Telegraph Co. a annonce? qu?il de?pensera 20 millions de dollars pour construire une
usine a? Guadalajara, au Mexique, pour faire re?pondeurs te?le?phoniques.
En 2: A member in the House leadership and skilled legislator, Mr. Fazio nonetheless found
himself burdened not only by California?s needs but by Hurricane Hugo amendments he ac-
cepted in a vain effort to build support in the panel. (sense id 3)
Fr 2: Un membre de la direction de la Chambre et le le?gislateur compe?tent, M. Fazio a
ne?anmoins conclu lui-me?me souffre, non seulement par les besoins de la Californie, mais
par l?ouragan Hugo amendements qu?il a accepte? dans un vain effort pour renforcer le sou-
tien dans le panneau.
En 3: Burmah Oil PLC, a British independent oil and specialty-chemicals marketing con-
cern, said SHV Holdings N.V. has built up a 7.5% stake in the company. (sense id 3)
1The sentences provided and their annotations are extracted from the SEMEVAL corpus.
26
Fr 3: Burmah Oil PLC, une huile inde?pendant britannique et le souci de commercialisation
des produits chimiques de spe?cialite?, a de?clare? SHV Holdings NV a acquis une participation
de 7,5% dans la socie?te?.
En 4: Plaintiffs? lawyers say that buildings become ?sick? when inadequate fresh air and
poor ventilation systems lead pollutants to build up inside. (sense id 2)
Fr 4: Avocats des plaignants disent que les ba?timents tombent malades quand l?insuffisance
d?air frais et des syste`mes de ventilation insuffisante de plomb polluants de s?accumuler a`
l?inte?rieur.
As illustrated by these examples, the multilingual representation helps in two important ways. First,
it attempts to disambiguate the target ambiguous word by assigning it a different translation depending
on the context where it occurs. For instance, the first example includes a usage for the verb build in its
most frequent sense, namely that of construct (WordNet: make by combining materials and parts), and
this sense is correctly translated into French as construire. In the second sentence, build is used as part
of the verbal expression build support where it means to form or accumulate steadily (WordNet), and
it is accurately translated in both French sentences as renforcer. For sentences three and four, build is
followed by the adverb up, yet in the first case, its sense id in WordNet is 3, build or establish something
abstract, while in the second one is 2, form or accumulate steadily. Being able to infer from the co-
occurrence of additional words appearing the context, the MT engine differentiates the two usages in
French, translating the first occurrence as acquis and the second one as accumuler.
Second, the multilingual representation also significantly enriches the feature space, by adding fea-
tures drawn from multiple languages. For instance, the feature vector for the first example will not only
include English features such as factory and make, but it will also include additional French features
such as usine and faire. Similarly, the second example will have a feature vector including words such
as buildings and systems, and also ba?timents and syste`mes. While this multilingual representation can
sometime result in redundancy when there is a one-to-one translation between languages, in most cases
however the translations will enrich the feature space, by either indicating that two features in English
share the same meaning (e.g., the words manufactory and factory will both be translated as usine in
French), or by disambiguating ambiguous English features using different translations (e.g., the context
word plant will be translated in French as usine or plante, depending on its meaning).
Appending therefore multilingual features to the monolingual vector generates a more orthogonal
vector space. If, previously, the different senses of buildwere completely dependent on their surrounding
context in the source language, now they are additionally dependent on the disambiguated translation of
build given its context, as well as the context itself and the translation of the context.
4 Multilingual Vector Space Representations for WSD
4.1 Datasets
We test our model on two publicly available word sense disambiguation datasets. Each dataset includes
a number of ambiguous words. For each word, a number of sample contexts were extracted and then
manually labeled with their correct sense. Therefore, both datasets follow a Zipfian distribution of senses
in context, given their natural usage. Note also that senses do not cross part-of-speech boundaries.
The TWA2 (two-way ambiguities) dataset contains sense tagged examples for six words that have
two-way ambiguities (bass, crane, motion, palm, plant, tank). These are words that have been previously
used in word sense disambiguation experiments reported in (Yarowsky, 1995; Mihalcea, 2003). Each
word has approximately 100 to 200 examples extracted from the British National Corpus. Since the
words included in this dataset have only two homonym senses, the classification task is easier.
2http://www.cse.unt.edu/?rada/downloads.html\#twa
27
Expanded multilingual features 
De 
Es 
Fr En 
English features 
En: suppose you let me 
explain actually 
Es: supongamos que 
vamos explicar la 
verdad 
Fr: supposons que vous 
laissez moi vous 
expliquer en fait 
De: angenommen sie 
lassen mir eigentlich 
erkl?ren 
Figure 1: Construction of a multilingual vector (combinations of target languages C(3, k), where k = 0..3
The second dataset is the SEMEVAL corpus 2007 (Pradhan et al, 2007),3 consisting of a sample of 35
nouns and 65 verbs with usage examples extracted from the Penn Treebank as well as the Brown corpus,
and annotated with OntoNotes sense tags (Hovy et al, 2006). These senses are more coarse grained
when compared to the traditional sense repository encoded in the WordNet lexical database. While
OntoNotes attains over 90% inter-annotator agreement, rendering it particularly useful for supervised
learning approaches, WordNet is too fine grained even for human judges to agree (Hovy et al, 2006).
The number of examples available per word and per sense varies greatly; some words have as few as
50 examples, while some others can have as many as 2,000 examples. Some of these contexts are
considerably longer than those appearing in TWA, containing around 200 words. For the experiments
reported in this paper, given the limitations imposed by the number of contexts that can be translated by
the online translation engine,4 we randomly selected a subset of 31 nouns and verbs from this dataset.
4.2 Model
In order to generate a multilingual representation for the TWA and SEMEVAL datasets, we rely on the
method proposed in Banea et al (2010) and use Google Translate to transfer the data from English into
several other languages and produce multilingual representations. We experiment with three languages,
namely French (Fr), German (De) and Spanish (Es). Our choice is motivated by the fact that when
Google made public their statistical machine translation system in 2007, these were the only languages
covered by their service, and we therefore assume that the underlying statistical translation models are
also the most robust. Upon translation, the data is aligned at instance level, so that the original English
context is augmented with three mirroring contexts in French, German, and Spanish, respectively.
We extract the word unigrams from each of these contexts, and then generate vectors that consist of
the original English unigrams followed by the multilingual portion resulted from all possible combina-
tions of the three languages taken 0 through 3 at a time, or more formally C(3, k), where k = 0..3 (see
Figure 1). For instance, a vector resulting from C(3, 0) is the traditional monolingual vector, whereas a
vector built from the combination C(3, 3) contains features extracted from all languages.
3http://nlp.cs.swarthmore.edu/semeval/tasks/task17/description.shtml
4We use Google Translate (http://translate.google.com/), which has a limitation of 1,000 translations per day.
28
0.00
0.50
1.00
1.50
2.00
2.50
3.00
3.50
4.00
Fe
at
ur
e 
W
ei
gh
t 
         ND ?2 = 5 A=1 
         ND ?2 = 5 A=20 
Figure 2: Example of sentence whose words are weighted based on a normal distribution with variance of 5, and
an amplification factor of 20
4.2.1 Feature Weighting
For weighting, we use a parametrized weighting based on a normal distribution scheme, to better leverage
the multilingual features. Let us consider the following sentence:
We made the non-slip surfaces by stippling the tops with a <head> bass </head> broom a
fairly new one works best.
Every instance in our datasets contains an XML-marking before and after the word to be disam-
biguated (also known as a headword), in order to identify it from the context. For instance, in the
example above, the headword is bass. The position of this headword in the context can be considered
the mean of a normal distribution. When considering a ?2 = 5, five words to the left and right of the
mean are activated with a value above 10?2 (see the dotted line in Figure 2). However, all the features
are actually activated by some amount, allowing this weighting model to capture a continuous weight
distribution across the entire context. In order to attain a higher level of discrepancy between the weight
of consecutive words, we amplify the normal distribution curve by an empirically determined factor of
20, effectively mapping the values to an interval ranging from 0 to 4. We apply this amplified activation
to every occurrence of a headword in a context. If two activation curves overlap, meaning that a given
word has two possible weights, the final weight is set to the highest (generated by the closest headword in
context). Similar weighting is also performed on the translated contexts, allowing for the highest weight
to be attained by the headword translated into the target language, and a decrementally lower weight for
its surrounding context.
This method therefore allows the vector-space model to capture information pertaining to both the
headword and its translations in the other languages, as well as a language dependent gradient of the
neighboring context usage. While a traditional bigram or trigram model only captures an exact expres-
sion, a normal distribution based model is able to account for wild cards, and transforms the traditionally
sparse feature space into one that is richer and more compact at the same time.
4.3 Adjustments
We encountered several technical difficulties in translating the XML-formatted datasets, which we will
expand on in this section.
29
4.3.1 XML-formatting and alignment
First of all, as every instance in our datasets contains an XML-marked headword (as shown in Section
4.2.1), the tags interfere with the MT system, and we had to remove them from the context before
proceeding with the translation. The difficulty came from the fact that the translated context provides
no way of identifying the translation of the original headword. In order to acquire candidate translations
of the English headword we query the Google Multilingual Dictionary5 (setting the dictionary direction
from English to the target language) and consider only the candidates listed under the correct part-of-
speech. We then scan the translated context for any of the occurrences mined from the dictionary, and
locate the candidates.
In some of the cases we also identify candidate headwords in the translated context that do not mirror
the occurrence of a headword in the English context (i.e., the number of candidates is higher than the
number of headwords in English). We solve this problem by relying on the assumption that there is an
ideal position for a headword candidate, and this ideal position should reflect the relative position of the
original headword with regard to its context. This alignment procedure is supported by the fact that the
languages we use follow a somewhat similar sentence structure; given parallel paragraphs of text, these
cross-lingual ?context anchors? will lie in close vicinity. We therefore create two lists: the first list is
the reference English list, and contains the indexes of the English headwords (normalized to 100); the
second list contains the normalized indexes of the candidate headwords in the target language context.
For each candidate headword in the target language, we calculate the shortest distance to a headword
appearing in the reference English list. Once the overall shortest distance is found, both the candidate
headword?s index in the target language and its corresponding English headword?s index are removed
from their respective list. The process continues until the reference English list is empty.
4.3.2 Inflections
There are also cases when we are not able to identify a headword due to the fact that we are trying to find
the lemma (extracted from the multilingual dictionary) in a fully inflected context, where most probably
the candidate translation is inflected as well. As French, German and Spanish are all highly inflected
languages, we are faced with two options: to either lemmatize the contexts in each of the languages,
which requires a lemmatizer tuned for each language individually, or to stem them. We chose the latter
option, and used the Lingua::Stem::Snowball,6 which is a publicly available implementation of the Porter
stemmer in multiple languages.
To summarize, all the translations are stemmed to obtain maximum coverage, and alignment is performed
when the number of candidate entries found in a translated context does not match the frequency of
candidate headwords in the reference English context. Also, all the contexts are processed to remove any
special symbols and numbers.
5 Results and Discussion
5.1 Experimental Setup
In order to determine the effect of the multilingual expanded feature space on word sense disambiguation,
we conduct several experiments using the TWA and SEMEVAL datasets. The results are shown in Tables
1 and 2.
Our proposed model relies on a multilingual vector space, where each individual feature is weighted
using a scheme based on a modified normal distribution (Section 4.2.1). As eight possible combinations
are available when selecting one main language (English) and combinations of three additional languages
5http://www.google.com/dictionary
6http://search.cpan.org/dist/Lingua-Stem-Snowball/lib/Lingua/Stem/Snowball.pm
30
taken 0 through 3 at a time (Spanish, French and German), we train eight Na??ve Bayes learners7 on the
resulted datasets: one monolingual (En), three bilingual (En-De, En-Fr, En-Es), three tri-lingual (En-
De-Es, En-De-Fr, En-Fr-Es), and one quadri-lingual (En-Fr-De-Es). Each dataset is evaluated using ten
fold cross-validation; the resulting micro-accuracy measures are averaged across each of the language
groupings and they appear in Tables 1 and 2 in ND-L1 (column 4), ND-L2 (column 5), ND-L3 (column
6), and ND-L4 (column 7), respectively. Our hypothesis is that as more languages are added to the mix
(and therefore the number of features increases), the learner will be able to distinguish better between
the various senses.
5.2 Baselines
Our baseline consists of the predictions made by a majority class learner, which labels all examples with
the predominant sense encountered in the training data.8 Note that the most frequent sense baseline
is often times difficult to surpass because many of the words exhibit a disproportionate usage of their
main sense (i.e., higher than 90%), such as the noun bass or the verb approve. Despite the fact that the
majority vote learner provides us with a supervised baseline, it does not take into consideration actual
features pertaining to the instances. We therefore introduce a second, more informed baseline that relies
on binary-weighted features extracted from the English view of the datasets and we train a multinomial
Na??ve Bayes learner on this data. For every word included in our datasets, the binary-weighted Na??ve
Bayes learner achieves the same or higher accuracy as the most frequent sense baseline.
5.3 Experiments
Comparing the accuracies obtained when training on the monolingual data, the binary weighted baseline
surpasses the normal distribution-based weighting model in only three out of six cases on the TWA
dataset (difference ranging from .5% to 4.81%), and in 6 out of 31 cases on the SEMEVAL dataset
(difference ranging from .53% to 7.57%, where for 5 of the words, the difference is lower than 3%). The
normal distribution-based model is thus able to activate regions around a particular headword, and not
an entire context, ensuring more accurate sense boundaries, and allowing this behavior to be expressed
in multilingual vector spaces as well (as seen in columns 7-9 in Tables 1 and 2).
When comparing the normal distribution-based model using one language versus more languages,
5 out of 6 words in TWA score highest when the expanded feature space includes all languages, and
one scores highest for combinations of 3 languages (only .17% higher than the accuracy obtained for
all languages). We notice the same behavior in the SEMEVAL dataset, where 18 of the words exhibit
their highest accuracy when all four languages are taken into consideration, and 3 achieve the highest
score for three-language groupings (at most .37% higher than the accuracy obtained for the four language
grouping). While the model displays a steady improvement as more languages are added to the mix, four
of the SEMEVAL words are unable to benefit from this expansion, namely the verbs buy (-0.61%), care
(-1.45%), feel (-0.29%) and propose (-2.94%). Even so, we are able to achieve error rate reductions
ranging from 6.52% to 63.41% for TWA, and from 3.33% to 34.62% for SEMEVAL.
To summarize the performance of the model based on the expanded feature set and the proposed
baselines, we aggregate all the accuracies from Tables 1 and 2, and present the results obtained in Table 3.
The monolingual modified normal-distribution model is able to exceed the most common sense baseline
and the binary-weighted Na??ve Bayes learner for both datasets, proving its superiority as compared to
a purely binary-weighted model. Furthermore, we notice a consistent increase in accuracy as more
languages are added to the vector space, displaying an average increment of 1.7% at every step for
TWA, and 0.67% for SEMEVAL. The highest accuracy is achieved when all languages are taken into
consideration: 86.02% for TWA and 83.36% for SEMEVAL, corresponding to an error reduction of
25.96% and 10.58%, respectively.
7We use the multinomial Na??ve Bayes implementation provided by the Weka machine learning software (Hall et al, 2009).
8Our baseline it is not the same as the traditional most common sense baseline that uses WordNet?s first sense heuristic,
because our data sets are not annotated with WordNet senses.
31
1 2 3 4 5 6 7 8 9 10
Word # Inst # Senses MCS BIN-L1 ND-L1 ND-L2 ND-L3 ND-L4 Error Red.
bass.n 107 2 90.65 90.65 90.65 91.28 91.90 92.52 20.00
crane.n 95 2 75.79 75.79 76.84 76.14 76.49 78.95 9.09
motion.n 201 2 70.65 81.09 79.60 86.73 89.88 92.54 63.41
palm.n 201 2 71.14 73.13 87.06 88.89 89.72 89.55 19.23
plant.n 187 2 54.55 79.14 74.33 77.90 81.82 83.96 37.50
tank.n 201 2 62.69 77.61 77.11 76.29 76.45 78.61 6.52
Table 1: Accuracies obtained on the TWA dataset; Columns: 1 - words contained in the corpus, 2 - number of
examples for a given word, 3 - number of senses covered by the examples, 4 - micro-accuracy obtained when
using the most common sense (MCS), 5 - micro-accuracy obtained using the multinomial Na??ve Bayes classifier
on binary weighted monolingual features in English, 6 - 9 - average micro-accuracy computed over all possible
combinations of English and 3 languages taken 0 through 3 at a time, resulted from features weighted following
a modified normal distribution with ?2 = 5 and an amplification factor of 20 using a multinomial Na??ve Bayes
learner, where 6 - one language, 7 - 2 languages, 8 - 3 languages, 9 - 4 languages, 10 - error reduction calculated
between ND-L1 (6) and ND-L4 (9)
6 Conclusion
This paper explored the cumulative ability of features originating from multiple languages to improve
on the monolingual word sense disambiguation task. We showed that a multilingual model is suited to
better leverage two aspects of the semantics of text by using a machine translation engine. First, the
various senses of a target word may be translated into other languages by using different words, which
constitute unique, yet highly salient features that effectively expand the target word?s space. Second, the
translated context words themselves embed co-occurrence information that a translation engine gathers
from very large parallel corpora. This information is infused in the model and allows for thematic spaces
to emerge, where features from multiple languages can be grouped together based on their semantics,
leading to a more effective context representation for word sense disambiguation. The average micro-
accuracy results showed a steadily increasing progression as more languages are added to the vector
space. Using two standard word sense disambiguation datasets, we showed that a classifier based on a
multilingual representation can lead to an error reduction ranging from 10.58% (SEMEVAL) to 25.96%
(TWA) as compared to the monolingual classifier.
Acknowledgments
This material is based in part upon work supported by the National Science Foundation CAREER award
#0747340 and IIS award #1018613. Any opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily reflect the views of the National
Science Foundation.
References
Banea, C., R. Mihalcea, and J. Wiebe (2010, August). Multilingual subjectivity: Are more languages
better? In Proceedings of the 23rd International Conference on Computational Linguistics (Coling
2010), Beijing, China, pp. 28?36.
Dagan, I. and A. Itai (1994). Word sense disambiguation using a second language monolingual corpus.
Computational Linguistics 20(4), 563?596.
Diab, M. and P. Resnik (2002, July). An unsupervised method for word sense tagging using parallel
corpora. In Proceedings of the 40st Annual Meeting of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
32
1 2 3 4 5 6 7 8 9 10
Word # Inst # Senses MCS BIN-L1 ND-L1 ND-L2 ND-L3 ND-L4 Error Red.
approve.v 53 2 94.34 94.34 94.34 94.34 95.60 96.23 33.33
ask.v 348 6 64.94 68.39 72.41 73.66 74.71 75.00 9.37
bill.n 404 3 65.10 88.12 90.59 91.75 92.41 92.82 23.68
buy.v 164 5 78.66 78.66 78.05 77.64 77.44 77.44 -2.78
capital.n 278 4 92.81 92.81 92.81 92.81 93.17 93.53 10.00
care.v 69 3 78.26 78.26 86.96 86.47 85.99 85.51 -11.11
effect.n 178 3 82.02 82.02 84.83 85.96 86.33 85.96 7.41
exchange.n 363 5 71.90 73.83 78.51 82.37 84.85 85.95 34.62
explain.v 85 2 88.24 88.24 88.24 88.24 88.24 88.24 0.00
feel.v 347 3 82.13 82.13 82.13 82.04 81.94 81.84 -1.61
grant.v 19 2 63.16 73.68 73.68 71.93 71.93 78.95 20.00
hold.v 129 8 34.88 45.74 43.41 43.41 43.41 43.41 0.00
hour.n 187 4 84.49 84.49 83.96 83.78 83.78 84.49 3.33
job.n 188 3 74.47 74.47 80.32 80.67 82.62 84.04 18.92
part.n 481 4 81.91 81.91 82.12 83.30 84.13 85.45 18.60
people.n 754 4 91.11 91.11 91.11 91.29 92.22 93.37 25.37
point.n 469 9 71.64 73.99 77.61 82.09 83.51 84.22 29.52
position.n 268 7 27.61 60.82 61.19 66.17 68.91 68.66 19.23
power.n 251 3 47.81 84.46 76.89 81.94 82.87 83.27 27.59
president.n 879 3 86.23 89.87 87.14 88.28 89.34 90.79 28.32
promise.v 50 2 88.00 88.00 86.00 86.67 87.33 88.00 14.29
propose.v 34 2 85.29 85.29 88.24 87.25 86.27 85.29 -25.00
rate.n 1009 2 84.64 86.92 87.02 88.07 88.64 89.30 17.56
remember.v 121 2 99.17 99.17 99.17 99.17 99.17 99.17 0.00
rush.v 28 2 92.86 92.86 92.86 92.86 92.86 92.86 0.00
say.v 2161 5 97.78 97.78 97.78 97.78 97.78 97.78 0.00
see.v 158 6 44.94 47.47 49.37 51.05 51.69 52.53 6.25
state.n 617 3 83.14 83.95 85.25 85.25 85.47 85.74 3.30
system.n 450 5 55.56 72.44 74.00 73.85 75.26 75.78 6.84
value.n 335 3 89.25 89.25 89.25 89.35 89.45 89.85 5.56
work.v 230 7 64.78 65.65 66.96 68.26 68.99 68.70 5.26
Table 2: Accuracies obtained on the SEMEVAL dataset; Columns: 1 - words contained in the corpus, 2 - number
of examples for a given word, 3 - number of senses covered by the examples, 4 - micro-accuracy obtained when
using the most common sense (MCS), 5 - micro-accuracy obtained using the multinomial Na??ve Bayes classifier
on binary weighted monolingual features in English, 6 - 9 - average micro-accuracy computed over all possible
combinations of English and 3 languages taken 0 through 3 at a time, resulted from features weighted following
a modified normal distribution with ?2 = 5 and an amplification factor of 20 using a multinomial Na??ve Bayes
learner, where 6 - one language, 7 - 2 languages, 8 - 3 languages, 9 - 4 languages, 10 - error reduction calculated
between ND-L1 (6) and ND-L4 (9)
1 2 3 4 5 6 7 8
Dataset MCS BIN-L1 ND-L1 ND-L2 ND-L3 ND-L4 Error Red.
TWA 70.91 79.57 80.93 82.87 84.38 86.02 25.96
SEMEVAL 75.71 80.52 81.36 82.18 82.78 83.36 10.58
Table 3: Aggregate accuracies obtained on the TWA and SEMEVAL datasets; Columns: 1 - dataset, 2 - average
micro-accuracy obtained when using the most common sense (MCS), 3 - average micro-accuracy obtained using
the multinomial Na??ve Bayes classifier on binary weighted monolingual features in English, 4 - 7 - average micro-
accuracy computed over all possible combinations of English and 3 languages taken 0 through 3 at a time, resulted
from features weighted following a modified normal distribution with ?2 = 5 and an amplification factor of 20
using a multinomial Na??ve Bayes learner, where 4 - one language, 5 - 2 languages, 6 - 3 languages, 7 - 4 languages,
8 - error reduction calculated between ND-L1 (4) and ND-L4 (7)
33
Fellbaum, C. (1998). WordNet, An Electronic Lexical Database. The MIT Press.
Hall, M., E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten (2009). The weka data
mining software: An update. SIGKDD Explorations 11(1).
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). Ontonotes: the 90In Proceed-
ings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers
on XX, NAACL ?06, Morristown, NJ, USA, pp. 57?60. Association for Computational Linguistics.
Lesk, M. (1986, June). Automatic sense disambiguation using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference 1986, Toronto.
Li, C. and H. Li (2002). Word translation disambiguation using bilingual bootstrapping. In Proceedings
of 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Pennsylvania.
Mihalcea, R. (2003, September). The role of non-ambiguous words in natural language disambiguation.
In Proceedings of the conference on Recent Advances in Natural Language Processing RANLP-2003,
Borovetz, Bulgaria.
Mihalcea, R., R. Sinha, and D. McCarthy (2010). Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the ACL Workshop on Semantic Evaluations, Uppsala, Sweden.
Mihalcea, R., P. Tarau, and E. Figa (2004). PageRank on semantic networks, with application to word
sense disambiguation. In Proceedings of the 20st International Conference on Computational Lin-
guistics (COLING 2004), Geneva, Switzerland.
Ng, H. and H. Lee (1996). Integrating multiple knowledge sources to disambiguate word sense: An
examplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 1996), Santa Cruz.
Ng, H., B. Wang, and Y. Chan (2003, July). Exploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of the 41st Annual Meeting of the Association for Computational
Linguistics (ACL 2003), Sapporo, Japan.
Pradhan, S., E. Loper, D. Dligach, and M. Palmer (2007, June). Semeval-2007 task-17: English lex-
ical sample, srl and all words. In Proceedings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Republic.
Resnik, P. and D. Yarowsky (1999). Distinguishing systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Language Engineering 5(2), 113?134.
Yarowsky, D. (1995, June). Unsupervised word sense disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL 1995),
Cambridge, MA.
34
Measuring the semantic relatedness between words and images
Chee Wee Leong and Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
cheeweeleong@my.unt.edu, rada@cs.unt.edu
Abstract
Measures of similarity have traditionally focused on computing the semantic relatedness between
pairs of words and texts. In this paper, we construct an evaluation framework to quantify cross-modal
semantic relationships that exist between arbitrary pairs of words and images. We study the effec-
tiveness of a corpus-based approach to automatically derive the semantic relatedness between words
and images, and perform empirical evaluations by measuring its correlation with human annotators.
1 Introduction
Traditionally, a large body of research in natural language processing has focused on formalizing word
meanings. Several resources developed to date (e.g., WordNet (Miller, 1995)) have enabled a systematic
encoding of the semantics of words and exemplify their usage in different linguistic frameworks. As a
result of this formalization, computing semantic relatedness between words has been possible and has
been used in applications such as information extraction and retrieval, query reformulation, word sense
disambiguation, plagiarism detection and textual entailment.
In contrast, while research has shown that the human cognitive system is sensitive to visual informa-
tion and incorporating a dual linguistic-and-pictorial representation of information can actually enhance
knowledge acquisition (Potter and Faulconer, 1975), the meaning of an image in isolation is not well-
defined and it is mostly task-specific. A given image, for instance, may be simultaneously labeled by a
set of words using an automatic image annotation algorithm, or classified under a different set of seman-
tic tags in the image classification task, or simply draw its meaning from a few representative regions
following image segmentation performed in an object localization framework.
Given that word meanings can be acquired and disambiguated using dictionaries, we can perhaps
express the meaning of an image in terms of the words that can be suitably used to describe it. Specif-
ically, we are interested to bridge the semantic gap (Smeulders et al, 2000) between words and images
by exploring ways to harvest the information extracted from visual data in a general framework. While a
large body of work has focused on measuring the semantic similarity of words (e.g., (Miller and Charles,
1998)), or the similarity between images based on image content (e.g., (Goldberger et al, 2003)), very
few researchers have considered the measure of semantic relatedness1 between words and images.
But, how exactly is an image related to a given word? In reality, quantification of such a cross-
modal semantic relation is impossible without supplying it with a proper definition. Our work seeks to
address this challenge by constructing a standard evaluation framework to derive a semantic relatedness
metric for arbitrary pairs of words and images. In our work, we explore methods to build a representa-
tion model consisting of a joint semantic space of images and words by combining techniques widely
adopted in computer vision and natural language processing, and we evaluate the hypothesis that we can
automatically derive a semantic relatedness score using this joint semantic space.
Importantly, we acknowledge that it is significantly harder to decode the semantics of an image, as its
interpretation relies on a subjective and perceptual understanding of its visual components (Biederman,
1In our paper, we are concerned with semantic relatedness, which is a more general concept than semantic similarity.
Similarity is concerned with entities related by virtues of their likeness, e.g., bank-trust company, but dissimilar entities may
also be related, e.g., hot-cold. A full treatment of the topic can be found in Budanitsky and Hirst (2005).
185
1987). Despite this challenge, we believe this is a worthy research direction, as many important problems
can benefit from the association of image content in relation to word meanings, such as automatic image
annotation, image retrieval and classification (e.g., (Leong et al, 2010)) as well as tasks in the domains
of of text-to-image synthesis, image harvesting and augmentative and alternative communication.
2 Related Work
Despite the large amount of work in computing semantic relatedness between words or similarity be-
tween images, there are only a few studies in the literature that associate the meaning of words and
pictures in a joint semantic space. The work most similar to ours was done by Westerveld (2000), who
employed LSA to combine textual words with simple visual features extracted from news images using
colors and textures. Although it was concluded that such a joint textual-visual representation model was
promising for image retrieval, no intensive evaluation was performed on datasets on a large scale, or
datasets other than the news domain. Similarly, Hare et al (2008) compared different methods such as
LSA and probabilistic LSA to construct joint semantic spaces in order to study their effects on automatic
image annotation and semantic image retrieval, but their evaluation was restricted exclusively to the
Corel dataset, which is somewhat idealistic and not reflective of the challenges presented by real-world,
noisy images.
Another related line of work by Barnard and Forsyth (2001) used a generative hierarchical model
to learn the associative semantics of words and images for improving information retrieval tasks. Their
approach was supervised and evaluated again only on the Corel dataset.
More recently, Feng and Lapata (2010) showed that it is possible to combine visual representations
of word meanings into a joint bimodal representation constructed by using latent topics. While their
work focused on unifying meanings from visual and textual data via supervised techniques, no effort
was made to compare the semantic relatedness between arbitrary pairs of word and image.
3 Bag of Visual Codewords
Inspired by the bag-of-words approach employed in information retrieval, the ?bag of visual codewords?
is a similar technique used mainly for scene classification (Yang et al, 2007). Starting with an image
collection, visual features are first extracted as data points from each image, characterizing its appear-
ance. By projecting data points from all the images into a common space and grouping them into a large
number of clusters such that similar data points are assigned to the same cluster, we can treat each cluster
as a ?visual codeword? and express every image in the collection as a ?bag of visual codewords?. This
representation enables the application of methods used in text retrieval to tasks in image processing and
computer vision.
Typically, the type of visual features selected can be global ? suitable for representation in all images,
or local ? specific to a given image type and task requirement. Global features are often described using a
continuous feature space, such as color histogram in three different color spaces (RGB, HSV and LAB),
or textures using Gabor and Haar wavelets (Makadia et al, 2008). In comparison, local features such as
key points (Fei-Fei and Perona, 2005) are often distinct across different objects or scenes. Regardless of
the features used, visual codeword generation involves the following three important phases.
1. Feature Detection: The image is divided into partitions of varying degrees of granularity from
which features can be extracted and represented. Typically, we can employ normalized cuts to
divide an image into irregular regions, or apply uniform segmentation to break it into smaller
but fixed grids, or simply locate information-rich local patches on the image using interest point
detectors.
2. Feature Description: A descriptor is selected to represent the features that are being extracted
from the image. Typically, feature descriptors (global or local) are represented as numerical vec-
tors, with each vector describing the feature extracted in each region. This way, an image is
represented by a set of vectors from its constituent regions.
186
Figure 1: An illustration of the process of generating ?Bag of Visual Codewords?
3. Visual Codeword Generation: Clustering methods are applied to group vectors into clusters,
where the center of each cluster is defined as a visual codeword, and the entire collection of clusters
defines the visual vocabulary for that image collection. Each image region or patch abstracted in
feature detection is now represented by the visual codeword mapped from its corresponding feature
vector.
The process of visual codeword generation is illustrated in Figure 1. Fei-Fei and Perona (2005) has
shown that, unlike most previous work on object or scene classification that focused on adopting global
features, local features are in fact extremely powerful cues. In our work, we use the Scale-Invariant
Feature Transform (SIFT) introduced by Lowe (2004) to describe distinctive local features of an image
in the feature description phase. SIFT descriptors are selected for their invariance to image scale, rotation,
differences in 3D viewpoints, addition of noise, and change in illumination. They are also robust across
affine distortions.
4 Semantic Vector Models
The underlying idea behind semantic vector models is that concepts can be represented as points in a
mathematical space, and this representation is learned from a collection of documents such that concepts
related in their meanings are near to one another in that space. In the past, semantic vector models
have been widely adopted by natural language processing researchers for tasks ranging from information
retrieval and lexical acquisition, to word sense disambiguation and document segmentation. Several
variants have been proposed, including the original vector space model (Salton et al, 1997) and the
Latent Semantic Analysis (Landauer and Dumais, 1997). Generally, vector models are attractive because
they can be constructed using unsupervised methods of distributional corpus analysis and assume little
language-specific requirements as long as texts can be reliably tokenized. Furthermore, various studies
(Kanerva, 1998) have shown that by using collaborative, distributive memory units to represent semantic
vectors, a closer correspondence to human cognition can be achieved.
While vector-space models typically require nontrivial algebraic machinery, reducing dimensions is
often key to uncover the hidden (latent) features of the terms distribution in the corpus, and to circumvent
the sparseness issue. There are a number of methods that have been developed to reduce dimensions ?
see e.g., Widdows and Ferraro (2008) for an overview. Here, we briefly describe one commonly used
187
technique, namely the Latent Semantic Analysis (LSA), noted for its effectiveness in previous works for
reducing dimensions.
In LSA, term co-occurrences in a corpus are captured by means of a dimensionality reduction op-
erated by a Singular Value Decomposition (SVD) on the term-by-document matrix T representing the
corpus. SVD is a well-known operation in linear algebra, which can be applied to any rectangular matrix
in order to find correlations among its rows and columns. SVD decomposes the term-by-document ma-
trix T into three matrices T = U?kVT where ?k is the diagonal k ? k matrix containing the singular k
values of T, ?1 ? ?2 ? ... ? ?k and U and V are column-orthogonal matrices. When the three matrices
are multiplied together the original term-by-document matrix is re-composed. Typically we can choose
k?  k obtaining the approximation T ' U?k?VT .
5 Semantic Relatedness between Words and Images
Although the bag of visual codewords has been extensively used in image classification and retrieval
tasks, and vector-space models are well explored in natural language processing, there has been little
connection between the two streams of research. Specifically, to our knowledge, there is no research work
that combines the two techniques to model multimodal meaning relatedness. Since we are exploring new
grounds, it is important to clarify what we mean by computing the semantic relatedness between a word
and an image, and how the nature of this task impacts our hypothesis. The assumptions below are
necessary to validate our findings:
1. Computing semantic relatedness between a word and an image involves comparing the concepts
invoked by the word and the salient objects in the image as well as their interaction. This goes
beyond simply identifying the presence or absence of specific objects indicated by a given word.
For instance, we expect a degree of relatedness between an image showing a soccer ball and the
word ?jersey,? since both invoke concepts like {sports, soccer, teamwork} and so on.
2. The semantics of an image is dependent on the focus, size and position of distinct objects identi-
fied through image segmentation. During labeling, we expect this segmentation to be performed
implicitly by the annotators. Although it is possible to focus one?s attention on specific objects via
bounding boxes, we are interested to harvest the meaning of an image using a holistic approach.
3. In the case of measuring the relatedness of a word that has multiple senses with a given image,
humans are naturally inclined to choose the sense that provides the highest relatedness inside the
pair. For example, an image of a river bank expectedly calls upon the ?river bank? sense of the
word ?bank? (and not ?financial bank? or other alternative word senses).
4. A degree of semantic relatedness can exist between any arbitrary word and image, on a scale
ranging from being totally unrelated to perfectly synonymous with each other. This is trivially
true, as the same property holds when measuring similarity between words and texts.
Next, we evaluate our hypothesis that we can measure the relatedness between a word and an image
empirically, using a parallel corpus of words and images as our dataset.
5.1 ImageNet
We use the ImageNet database (Deng et al, 2009), which is a large-scale ontology of images devel-
oped for advancing content-based image search algorithms, and serving as a benchmarking standard for
various image processing and computer vision tasks. ImageNet exploits the hierarchical structure of
WordNet by attaching relevant images to each synonym set (known as ?synset?), hence providing picto-
rial illustrations of the concept associated with the synset. On average, each synset contains 500-1000
images that are carefully audited through a stringent quality control mechanism.
Compared to other image databases with keyword annotations, we believe that ImageNet is suitable
for evaluating our hypothesis for three reasons. First, by leveraging on reliable keyword annotations in
WordNet (i.e., words in the synset and their gloss naturally serve as annotations for the corresponding
images), we can effectively circumvent the propagation of errors caused by unreliable annotations, and
consequently hope to reach more conclusive results for this study. Second, unlike other image databases,
188
ImageNet consists of millions of images, and it is a growing resource with more images added on a
regular basis. This aligns with our long-term goal of building a large-scale joint semantic space of images
and words. Finally, third, although we can search for relevant images using keywords in ImageNet,2
there is currently no method to query it in the reverse direction. Given a test image, we must search
through millions of images in the database to find the most similar image and its corresponding synset.
A joint semantic model can hopefully augment this shortcoming by allowing queries to be made in both
directions. Figure 2 shows an example of a synset and the corresponding images in ImageNet.
(a)
(b)
Joint Semantic Space of Words and Images
Synsets 167
Images 230,864
Words 1144
Nouns 783
Verbs 140
Adjectives 221
Image:Words ratio 202:1
Figure 2: (a) A subset of images associated with a node in ImageNet. The WordNet synset illustrated
here is {Dog, domestic dog, Canis familiaris} with the gloss: A member of the genus Canis (probably
descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in
many breeds; ?the dog barked all night? (b) A table showing statistical information on our joint semantic
space model
5.2 Dataset
For our experiments, we randomly select 167 synsets3 from ImageNet, covering a wide range of concepts
such as plants, mammals, fish, tools, vehicles etc. We perform a simple pre-processing step using Tree
Tagger (Schmid, 1994) and extract only the nouns. Multiwords are explicitly recognized as collocations
or named entities in the synset. Not considering part-of-speech distinctions, the vocabulary for synset
words is 352. The vocabulary for gloss words is 777. The shared vocabulary between them is 251.
There are a total of 230,864 images associated with the 167 synsets, with an average of 1383 images
per synset. We randomly select an image for each synset, thus obtaining a set of 167 test images in
total. The technique explained in Section 3 is used to generate visual codewords for each image in this
dataset.4 Each image is first pre-processed to have a maximum side length of 300 pixels. Next, SIFT
descriptors are obtained by densely sampling the image on 20x20 overlapping patches spaced 10 pixels
apart. K-means clustering is applied on a random subset of 10 million SIFT descriptors to derive a visual
vocabulary of 1,000 codewords. Each descriptor is then quantized into a visual codeword by assigning it
to the nearest cluster.
To create the gold-standard relatedness annotation, for each test image, six nouns are randomly se-
lected from its associated synset and gloss words, and six other nouns are again randomly selected from
the shared vocabulary words.5 In all, we have 167 x 12 = 2004 word-image pairs as our test dataset. Sim-
ilar to previous word similarity evaluations (Miller and Charles, 1998), we ask human annotators to rate
each pair on a scale of 0 to 10 to indicate their degree of semantic relatedness using the evaluation frame-
work outlined below, with 0 being totally unrelated and 10 being perfectly synonymous with each other.
To ensure quality ratings, for each word-image pair we used 15 annotators from Amazon Mechanical
2http://www.image-net.org/
3Not all synsets in ImageNet are annotated with images. We obtain our dataset from the Spring 2010 version of ImageNet
built around Wordnet 3.0.
4For our experiments, we obtained the visual codewords computed a priori from ImageNet. Test images are not used to
construct the model
512 data points are generally considered sufficient for reliable correlation measures (Vania Kovic, p.c.).
189
Synset {sunflower, helianthus} Synset {oxygen-mask} Synset {submarine , pigboat ,
sub , U-boat}
Gloss any plant of the genus
Helianthus having large flower
heads with dark disk florets and
showy yellow rays
Gloss a breathing device that
is placed over the mouth and
nose; supplies oxygen from an
attached storage tank
Gloss a submersible warship
usually armed with torpedoes
Relatedness Scores Relatedness Scores Relatedness Scores
color (5.13) dog (0.53) basketball (0.20) central (1.53) africa (0.80) brass (1.73)
floret (6.53) flower (9.67) device (5.47) family (0.80) door (1.67) good (2.40)
freshwater (2.40) hair (1.00) iron-tree (0.47) mouth (5.13) pacific (2.40) pigboat (6.47)
garden (6.60) head (3.80) oxygen-mask (7.73) tank (4.47) sub (8.20) submarine (9.67)
plant (8.47) ray (3.67) storage (3.07) supply (5.20) tail (0.93) torpedo (7.60)
sunflower (9.80) reed (2.27) nose (6.20) time (1.13) u-boat (7.47) warship (8.73)
Table 1: A sample of test images with their synset words and glosses : The number in parenthesis rep-
resents the numerical association of the word with the image (0-10). Human annotations reveal different
degree of semantic relatedness between the image and words in the synset or gloss.
Turk.6 Finally, the average of all 15 annotations for each word-image pair is taken as its gold-standard
relatedness score7. Note that only the pairs of images and words are provided to the annotators, and not
their synsets and gloss definitions.
The set of standard criteria underlying the cross-modal similarity evaluation framework shown here
is inspired by the semantic relations defined in Wordnet. These criteria were provided to the human
annotators, to help them decide whether a word and an image are related to each other.
1. Instance of itself: Does the image contain an entity that is represented by the word itself (e.g. an
image of ?Obama? vs the word ?Obama?) ?
2. Member-of Relation: Does the image contain an entity that is a member of the class suggested
by the word or vice versa (e.g. an image of an ?apple? vs the word ?fruits?) ?
3. Part-of Relation: Does the image contain an entity that is a part of a larger entity represented by
the word or vice versa (e.g. an image of a ?tree? vs the word ?forest?) ?
4. Semantically Related: Do both the word and the image suggest concepts that are related (e.g. an
image of troops at war vs the word ?peace?) ?
5. Semantically Close: Do both the word and the image suggest concepts that are not only related
but also close in meaning? (e.g. an image of troops at war vs the word ?gun?) ?
Criterion (1) basically tests for synonym relation. Criteria (2) and (3) are modeled after the hyponym-
hypernym and meronym-holonym relations in WordNet, which are prevalent among nouns. Note that
none of the criteria is preemptive over the others. Rather, we provide these criteria as guidelines in
a subjective evaluation framework, similar to the word semantic similarity task in Miller and Charles
(1998). Importantly, criterion (4) models dissimilar but related concepts, or any other relation that indi-
cates frequent association, while criterion (5) serves to provide additional distinction for pairs of words
and images on a higher level of relatedness toward similarity. In Table 1, we show sample images from
our test dataset, along with the annotations provided by the human annotators.
6We only allowed annotators with an approval rating of 97% or higher. Here, we expect some variance in the degree of
relatedness between the candidate words and images, hence annotations marked with all 10s or 0s are discarded due to lack of
distinctions in similarity relatedness
7Annotation guidelines and dataset can be downloaded at http://lit.csci.unt.edu/index.php/Downloads
190
5.3 Experiments
Following Erk and McCarthy (2009), who argued that word meanings are graded over their senses, we
believe that the meaning of an image is not limited to a set of ?best fitting? tags, but rather it exists as
a distribution over arbitrary words with varying degrees of association. Specifically, the focus of our
experiments is to investigate the correlation between automatic measures of such relatedness scores with
respect to human judgments.
To construct the joint semantic space of words and images, we use the SVD described in Section 4
to reduce the number of dimensions. To build each model, we use the 167 synsets from ImageNet and
their associated images (minus the held out test data), hence accounting for 167 latent dimensions. We
first represent the synsets as a collection of documents D, each document containing visual codewords
used to describe their associated images as well as textual words extracted from their gloss and synset
words. Thus, computing a cross-modal relatedness distance amounts to comparing the cosine similarity
of vectors representing an image to the vector representing a word in the term-document vector space.
Note that, unlike textual words, an image is represented by multiple visual codewords. Prior to computing
the actual cosine distance, we perform a weighted addition of vectors representing each visual codeword
for that image.
To illustrate, consider a single document di, representing the synset ?snail,? which consists of {cw0,
cw555, cw23, cw124, cw876, snail, freshwater, mollusk, spiral, shell}, where cwX represents a particular
visual codeword indexed from 0-9998, and the textual words are nouns extracted from the associated
synset and gloss. Given a test image I , it can be expressed as a bag of visual codewords {cw1 , ... , cwk}.
We first represent each visual codeword in I as a vector of length |D| using term-frequency inverse-
document-frequency (tf idf ) weighting, e.g., cwk=<0.4*d1, 0.2*d2, ... , 0.9*dm>, where m=167, and
perform an addition of k such vectors to form a final vector vi. To measure the semantic relatedness
between image I and a word w, e.g., ?snail,? we simply compute the cosine similarity between vi and
vw, where vw is also a vector of length |D| calculated using tf idf .
This paper seeks answers to the following questions. First, what is the relation between the discrim-
inability of the visual codewords and their ability to capture semantic relatedness between a word and an
image, as compared to the gold-standard annotation by humans? Second, given the unbalanced dataset
of images and words, can we use a relatively small number of visual codewords to derive such semantic
relatedness measures reliably? Third, what is the efficiency of an unsupervised vector semantic model in
measuring such relatedness, and is it applicable to large datasets?
Analogous to text-retrieval methods, we measure the discriminability of the visual codewords using
two weighting factors. The first is term-frequency (tf), which measures the number of times a codeword
appears in all images for a particular synset, while the second, image-term-frequency (itf), captures the
number of images using the codeword in a synset. For the two weighting schemes, we apply normal-
ization by using the total number of codewords for a synset (for tf weighting) and the total number of
images in a synset (for itf weighting).
We are interested to quantify the relatedness for pairs of words and images under two scenarios. By
ranking the 12 words associated with an image in reverse order of their relatedness to the image, we
can determine the ability of our models to identify the most related words for a given image (image-
centered). In the second scenario, we measure the relatedness of words and images regardless of the
synset they belong to, thus evaluating the ability of our methods to capture the relatedness between any
word and any image. This allows us to capture the correlation in an (arbitrary-image) scenario. For the
evaluations, we use the Spearman?s Rank correlation.
To place our results in perspective, we implemented two baselines and an upper bound for each of
the two scenarios above. The Random baseline randomly assigns ratings to each word-image pair on the
same 0 to 10 scale, and then measures the correlation to the human gold-standard. The Vector-Based (VB)
method is a stronger baseline aimed to study the correlation performance in the absence of dimensionality
reduction. As an upper bound, the Inter-Human-Agreement (IHA) measures the correlation of the rating
by each annotator against the average of the ratings of the rest of the annotators, averaged over the 167
synsets (for the image-centered scenario) and over the 2004 word-image pairs (for the arbitrary-image
scenario).
8For simplicity, we only show the top 5 visual codewords
191
Spearman?s Rank Coefficient (image-centered)
Top K codewords 100 200 300 400 500 600 700 800 900 1000
LSA tf 0.228 0.325 0.273 0.242 0.185 0.181 0.107 0.043 -0.018 0.000
LSA tf (norm) 0.233 0.339 0.293 0.254 0.202 0.180 0.124 0.047 -0.012 0.000
LSA tf*itf 0.268 0.317 0.256 0.248 0.219 0.166 0.081 -0.004 -0.037 0.000
LSA tf*itf (norm) 0.252 0.327 0.257 0.246 0.211 0.153 0.097 0.002 -0.042 0.000
VB tf 0.243 0.168 0.101 0.055 -0.021 -0.084 -0.157 -0.210 -0.236 -0.332
VB tf (norm) 0.240 0.181 0.110 0.062 -0.010 -0.082 -0.152 -0.204 -0.235 -0.332
VB tf*itf 0.262 0.181 0.107 0.065 -0.019 -0.081 -0.156 -0.211 -0.241 -0.332
VB tf*itf (norm) 0.257 0.180 0.116 0.068 -0.014 -0.079 -0.150 -0.250 -0.237 -0.332
Random 0.001 0.018 0.016 -0.008 0.008 0.005 -0.001 0.014 -0.035 0.012
IHA 0.687
Spearman?s Rank Coefficient (arbitrary-image)
Top K codewords 100 200 300 400 500 600 700 800 900 1000
LSA tf 0.236 0.341 0.291 0.249 0.208 0.183 0.106 0.033 -0.039 0.000
LSA tf (norm) 0.230 0.353 0.301 0.271 0.220 0.186 0.115 0.032 -0.029 0.000
LSA tf*itf 0.291 0.332 0.289 0.262 0.235 0.172 0.092 0.008 -0.041 0.000
LSA tf*itf (norm) 0.277 0.345 0.292 0.269 0.234 0.164 0.098 0.015 -0.046 0.000
VB tf 0.272 0.195 0.119 0.059 -0.012 -0.088 -0.164 -0.218 -0.240 -0.339
VB tf (norm) 0.277 0.207 0.130 0.069 -0.003 -0.083 -0.160 -0.215 -0.242 -0.339
VB tf*itf 0.287 0.206 0.127 0.062 -0.008 -0.085 -0.161 -0.214 -0.241 -0.339
VB tf*itf (norm) 0.286 0.212 0.132 0.071 -0.005 -0.081 -0.158 -0.214 -0.241 -0.339
Random -0.024 -0.014 0.015 -0.015 -0.004 -0.014 0.024 -0.009 -0.007 0.007
IHA 0.764
Table 2: Correlation of automatically generated scores with human annotations on cross-modal semantic
relatedness, as performed on the ImageNet test dataset of 2004 pairs of word and image. Correlation
figures scoring the highest within a weighting scheme are marked in bold, while those scoring the highest
across weighting schemes and within a visual vocabulary size are underlined.
6 Discussion
Our experimental results are shown in Table 2. A somewhat surprising observation is the consistency of
correlation figures between the two scenarios. In both scenarios, a representative set of 200 visual code-
words is sufficient to consistently score the highest correlation ratings across the 8 weighting schemes.
Intuitively, based on the experimental results, automatically choosing the top 10% or 20% of the visual
codewords seems to suffice and gives optimal correlation figures, but requires further justification. Con-
versely, the relatively simple weighting scheme using tf (normalized) produces the highest correlation in
six visual codeword sizes (K=200,300,400,700,800,900) for the image-centered scenario, as well as in
another six visual codeword sizes (K=200,300,400,600,700,900) for the arbitrary-image scenario. Un-
like stopwords in text retrieval accounting for most of the highest tf scores, visual codewords weighted
by the same scheme tf and a similar tf (normalized) scheme seem to be the most discriminative. The
correlation for including the entire visual vocabulary set (1000) produces identical results for all vector-
based and LSA weighting schemes, as images across synsets are now encoded by the same set of visual
codewords without discrimination between them.
Dimensionality reduction using SVD gains an advantage over the vector-based method for both sce-
narios, with the highest correlation rating in LSA (200 visual codeword, tf(norm)) achieving 0.077 points
better than the corresponding highest correlation in Vector-based (100 visual codeword, tf*itf ) for the
image-centered scenario, representing a 29.3% improvement. Similarly, in the arbitrary-image scenario,
the increase in correlation from 0.287 (VB tf*itf at 100 visual codeword) to 0.353 (LSA tf(norm) at
200 visual codeword) underlines a gain of approximately 23.0%. Overall, the arbitrary-image scenario
also scores consistently higher than the image-centered scenario under similar experimental conditions.
For instance, for the top 200 visual words, the same weighting schemes produce consistently lower
correlation figures for the image-centered scenario. This is also true for the Inter-Human-Agreement
score, which is higher in the arbitrary-image scenario (0.764) compared to the image-centered scenario
(0.687). Note that for all the experiments, the semantic relatedness scores generated from the semantic
vector space are significantly more correlated with the human gold-standard than the random baselines.
192
(a) (b)
Figure 3: (a) Correlation performance, and (b) Classification accuracy, as more data is added to construct
the semantic space model.
To investigate the effectiveness of the model when scaling up to large datasets, we employ the best
combination of weighting scheme and vocabulary size shown in Table 2, i.e., a visual vocabulary size
of 200 and tf (normalized) weighting for LSA, and vocabulary size of 100 and tf*itf weighting for the
vector-based model, and incrementally construct models ranging from 167 synsets to 800 synsets (all
randomly selected from ImageNet). We then measure the correlation of relatedness scores generated
using the same test dataset with respect to human annotations. The dataset was randomly selected to in-
crease by approximately five times, from a total of 230,864 images with 878 words to a total of 1,014,528
images with 3887 words. Furthermore, for each unseen test image taken from Synset Si and the associ-
ated 12 candidate words, we evaluate the ability of the model to identify which of the candidate words
actually appear in the gloss or the synset of Si, in a task we term as word classification. Here, the top
six words are predictably classified as those appearing in Si while the last six are classified as outside
of Si , after all 12 words are ranked in reverse order of their relatedness to the test image. We measure
the accuracy of the word classification task using TP+TN2004 , where TP is the number of words correctly
classified as synset or gloss words, and TN is the number of words correctly classified as outside of
synset or gloss, both summed over the 2004 pairs of words and images.
As shown in Figure 3, when a small number of synsets (33) was added to the original semantic space,
correlation with human ratings increased steeply to around 0.45 and higher for LSA in both scenarios,
while the vector-based method suffers a slight decrease in correlation ratings from 0.262 to 0.251 (image-
centered) and from 0.287 to 0.278 (arbitrary-image). As more images and words are added, correlation
for the vector-based model continues to decrease markedly. Comparatively, LSA is less sensitive to data
scaling, as correlation figures for both scenarios decreases slightly but stays within a 0.40 to 0.45 range.
Additionally, we infer that LSA is consistently more effective than the vector-based model in the words
classification task (as also seen in Figure 3). Even with more data added to the semantic space, word
classification accuracy stays consistently at 0.7 for LSA, while it drops to 0.535 for the vector-based
model at a synset size of 800.
7 Conclusion
In this paper, we provided a proof of concept in quantifying the semantic relatedness between words and
images through the use of visual codewords and textual words in constructing a joint semantic vector
space. Our experiments showed that the relatedness scores have a positive correlation to human gold-
standards, as measured using a standard evaluation framework.
We believe many aspects of this work can be explored further. For instance, other visual codeword
attributes, such as pixel coordinates, can be employed in a structured vector space along with the existing
model for improving vector similarity measures. To improve textual words coverage, a potentially effec-
193
tive way would be to create mappings from WordNet synsets to Wikipedia entries, where the concepts
represented by the synsets are discussed in detail. We also plan to study the applicability of the joint
semantic representation model to tasks such as automatic image annotation and image classification.
Acknowledgments
This material is based in part upon work supported by the National Science Foundation CAREER award
#0747340 and IIS award #1018613. Any opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily reflect the views of the National
Science Foundation.
References
Barnard, K. and D. Forsyth (2001). Learning the semantics of words and pictures. In Proceedings of International
Conference on Computer Vision.
Biederman, I. (1987). Recognition-by-components: A theory of human image understanding. In Psychological
Review, Volume 94, pp. 115?147.
Budanitsky, A. and G. Hirst (2005). Evaluating wordnet-based measures of lexical semantic relatedness. In
Computational Linguistics, Volume 32.
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei (2009). ImageNet: A Large-Scale Hierarchical Image
Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Erk, K. and D. McCarthy (2009). Graded word sense assignment. In Proceedings of Empirical Methods in Natural
Language Processing.
Fei-Fei, L. and P. Perona (2005). A bayesian hierarchical model for learning natural scene categories. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition.
Feng, Y. and M. Lapata (2010). Visual information in semantic representation. In Proceedings of the Annual
Conference of the North American Chapter of the ACL.
Goldberger, J., S. Gordon, and H. Greenspan (2003). An efficient image similarity measure based on approxima-
tions of kl-divergence between two gaussian mixtures. In Proceedings of IEEE International Conference on
Computer Vision.
Hare, J. S., S. Samangooei, P. H. Lewis, and M. S. Nixon (2008). Investigating the performance of auto-annotation
and semantic retrieval using semantic spaces. In Proceedings of the international conference on content-based
image and video retrieval.
Kanerva, P. (1998). Sparse distributed memory. In MIT Press.
Landauer, T. and S. Dumais (1997). A solution to platos problem: The latent semantic analysis theory of acquisi-
tion. In Psychological Review, Volume 104, pp. 211?240.
Leong, C. W., R. Mihalcea, and S. Hassan (2010). Text mining for automatic image tagging. In Proceedings of the
International Conference on Computational Linguistics.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. In International Journal of Computer
Vision.
Makadia, A., V. Pavlovic, and S. Kumar (2008). A new baseline for image annotation. In Proceedings of European
Conference on Computer Vision.
Miller, G. (1995). Wordnet: A lexical database for english. In Communications of the ACM, Volume 38, pp. 39?41.
Miller, G. and W. Charles (1998). Contextual correlates of semantic similarity. Language and Cognitive Pro-
cesses 6(1).
Potter, M. C. and B. A. Faulconer (1975). Time to understand pictures and words. In Nature, Volume 253, pp.
437?438.
Salton, G., A. Wong, and C. Yang (1997). A vector space model for automatic indexing. In Readings in Information
Retrieval, pp. 273?280. San Francisco, CA: Morgan Kaufmann Publishers.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International
Conference on New Methods in Language Processing.
Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and R. Jain (2000). Content-based image retrieval at the
end of the early years. In IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 22, pp.
1349?1380.
Westerveld, T. (2000). Image retrieval: Context versus context. In Content-Based Multimedia Information Access.
Widdows, D. and K. Ferraro (2008). Semantic vectors: a scalable open source package and online technology man-
agement application. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08).
Yang, J., Y.-G. Jiang, A. G. Hauptmann, and C.-W. Ngo (2007). Evaluating bag-of-visual-words representations
in scene classification. In ACM Multimedia Information Retrieval Workshop.
194
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 87?96,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Improving the Impact of Subjectivity Word Sense Disambiguation on
Contextual Opinion Analysis
Cem Akkaya, Janyce Wiebe, Alexander Conrad
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe,conrada}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is automatically determining which
word instances in a corpus are being used with
subjective senses, and which are being used
with objective senses. SWSD has been shown
to improve the performance of contextual
opinion analysis, but only on a small scale and
using manually developed integration rules.
In this paper, we scale up the integration of
SWSD into contextual opinion analysis and
still obtain improvements in performance,
by successfully gathering data annotated by
non-expert annotators. Further, by improving
the method for integrating SWSD into con-
textual opinion analysis, even greater benefits
from SWSD are achieved than in previous
work. We thus more firmly demonstrate the
potential of SWSD to improve contextual
opinion analysis.
1 Introduction
Often, methods for opinion, sentiment, and sub-
jectivity analysis rely on lexicons of subjective
(opinion-carrying) words (e.g., (Turney, 2002;
Whitelaw et al, 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al, 2007; Andreevskaia and Bergler, 2008;
Agarwal et al, 2009)). Examples of such words are
the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
However, even manually developed subjectiv-
ity lexicons have significant degrees of subjectivity
sense ambiguity (Su and Markert, 2008; Gyamfi et
al., 2009). That is, many clues in these lexicons have
both subjective and objective senses. This ambiguity
leads to errors in opinion and sentiment analysis, be-
cause objective instances represent false hits of sub-
jectivity clues. For example, the following sentence
contains the keywords from (1) used with objective
senses:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
Recently, in (Akkaya et al, 2009), we introduced
the task of subjectivity word sense disambiguation
(SWSD), which is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with objec-
tive senses. We developed a supervised system for
SWSD, and exploited the SWSD output to improve
the performance of multiple contextual opinion anal-
ysis tasks.
Although the reported results are promising, there
are three obvious shortcomings. First, we were able
to apply SWSD to contextual opinion analysis only
on a very small scale, due to a shortage of anno-
tated data. While the experiments show that SWSD
improves contextual opinion analysis, this was only
on the small amount of opinion-annotated data that
was in the coverage of our system. Two questions
arise: is it feasible to obtain greater amounts of
the needed data, and do SWSD performance im-
provements on contextual opinion analysis hold on a
87
larger scale. Second, the annotations in (Akkaya et
al., 2009) are piggy-backed on SENSEVAL sense-
tagged data, which are fine-grained word sense an-
notations created by trained annotators. A concern
is that SWSD performance improvements on con-
textual opinion analysis can only be achieved using
such fine-grained expert annotations, the availability
of which is limited. Third, (Akkaya et al, 2009) uses
manual rules to apply SWSD to contextual opinion
analysis. Although these rules have the advantage
that they transparently show the effects of SWSD,
they are somewhat ad hoc. Likely, they are not opti-
mal and are holding back the potential of SWSD to
improve contextual opinion analysis.
To address these shortcomings, in this paper, we
investigate (1) the feasibility of obtaining a substan-
tial amount of annotated data, (2) whether perfor-
mance improvements on contextual opinion analy-
sis can be realized on a larger scale, and (3) whether
those improvements can be realized with subjectiv-
ity sense tagged data that is not built on expert full-
inventory sense annotations. In addition, we explore
better methods for applying SWSD to contextual
opinion analysis.
2 Subjectivity Word Sense Disambiguation
2.1 Annotation Tasks
We adopt the definitions of subjective (S) and ob-
jective (O) from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2007). Subjective expressions
are words and phrases being used to express mental
and emotional states, such as speculations, evalua-
tions, sentiments, and beliefs. A general covering
term for such states is private state (Quirk et al,
1985), an internal state that cannot be directly ob-
served or verified by others. Objective expressions
instead are words and phrases that lack subjectivity.
The contextual opinion analysis experiments de-
scribed in Section 3 include both S/O and polar-
ity (positive,negative, neutral) classifications. The
opinion-annotated data used in those experiments is
from the MPQA Corpus (Wiebe et al, 2005; Wilson,
2007),1 which consists of news articles annotated for
subjective expressions, including polarity.
1Available at http://www.cs.pitt.edu/mpqa
2.1.1 Subjectivity Sense Labeling
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjectiv-
ity. As noted in (Wiebe and Mihalcea, 2006), sen-
tences containing objective senses may not be objec-
tive. Thus, objective senses are defined as follows:
Classifying a sense as O means that, when the sense
is used in a text or conversation, one does not expect
it to express subjectivity and, if the phrase or sen-
tence containing it is subjective, the subjectivity is
due to something else.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) performed agreement studies of the
scheme and report that good agreement can be
achieved between human annotators labeling the
subjectivity of senses (? values of 0.74 and 0.79, re-
spectively).
(Akkaya et al, 2009) followed the same annota-
tion scheme to annotate the senses of the words used
in the experiments. For this paper, we again use
the same scheme and annotate WordNet senses of
90 new words (the process of selecting the words is
described in Section 2.4).
2.1.2 Subjectivity Sense Tagging
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, in-
dicating whether they are used with a subjective or
objective sense.
Because there was no such tagged data at the time,
(Akkaya et al, 2009) created a data set by com-
bining two types of sense annotations: (1) labels of
senses within a dictionary as S or O (i.e., the subjec-
tivity sense labels of the previous section), and (2)
sense tags of word instances in a corpus (i.e., SEN-
SEVAL sense-tagged data).2 The subjectivity sense
labels were used to collapse the sense labels in the
sense-tagged data into the two new senses, S and O.
The target words (Akkaya et al, 2009) chose are the
words tagged in SENSEVAL that are also members
2Please see the paper for details on the SENSEVAL data
used in the experiments.
88
Sense Set1 (Subjective)
{ attack, round, assail, lash out, snipe, assault } ? attack in
speech or writing; ?The editors attacked the House Speaker?
{ assail, assault, set on, attack } ? attack someone emotionally;
?Nightmares assailed him regularly?
Sense Set2 (Objective)
{ attack } ? begin to injure; ?The cancer cells are attacking his
liver?; ?Rust is attacking the metal?
{ attack, aggress } ? take the initiative and go on the offensive;
?The visiting team started to attack?
Figure 1: Sense sets for target word ?attack? (abridged).
of the subjectivity lexicon of (Wilson et al, 2005;
Wilson, 2007).3 There are 39 such words. (Akkaya
et al, 2009) chose words from a subjectivity lexicon
because such words are known to have subjective
usages.
For this paper, subjectivity sense-tagged data was
obtained from the MTurk workers using the anno-
tation scheme of (Akkaya et al, 2010). A goal is to
keep the annotation task as simple as possible. Thus,
the workers are not directly asked if the instance of
a target word has a subjective or an objective sense,
because the concept of subjectivity would be diffi-
cult to explain in this setting. Instead the workers
are shown two sets of senses ? one subjective set and
one objective set ? for a specific target word and a
text passage in which the target word appears. Their
job is to select the set that best reflects the meaning
of the target word in the text passage. The set they
choose gives us the subjectivity label of the instance.
A sample annotation task is shown below. An
MTurk worker has access to two sense sets of the
target word ?attack? as seen in Figure 1. The S and
O labels appear here only for the purpose of this pa-
per; the workers do not see them. The worker is pre-
sented with the following text passage holding the
target word ?attack?:
Ivkovic had been a target of intra-party
feuding that has shaken the party. He was
attacked by Milosevic for attempting to
carve out a new party from the Socialists.
In this passage, the use of ?attack? is most similar
to the first entry in sense set one; thus, the correct
answer for this problem is Sense Set-1.
3Available at http://www.cs.pitt.edu/mpqa
(Akkaya et al, 2010) carried out a pilot study
where a subjectivity sense-tagged dataset was cre-
ated for eight SENSEVAL words through MTurk.
(Akkaya et al, 2010) evaluated the non-expert la-
bel quality against gold-standard expert labels which
were obtained from (Akkaya et al, 2009) relying
on SENSEVAL. The non-expert annotations are reli-
able, achieving ? scores around 0.74 with the expert
annotations.
For some words, there may not be a clean split be-
tween the subjective and objective senses. For these,
we opted for another strategy for obtaining MTurk
annotations. Rather than presenting the workers
with WordNet senses, we show them a set of objec-
tive usages, a set of subjective usages, and a text pas-
sage in which the target word appears. The workers?
job is to judge which set of usages the target instance
is most similar to.
2.2 SWSD System
We follow the same approach as in (Akkaya et al,
2009) to build our SWSD system. We train a differ-
ent supervised SWSD classifier for each target word
separately. This means the overall SWSD system
consists of as many SWSD classifiers as there are
target words. We utilize the same machine learning
features as in (Akkaya et al, 2009), which are com-
monly used in Word Sense Disambiguation (WSD).
2.3 Expert SWSD vs. Non-expert SWSD
Before creating a large subjectivity sense-tagged
corpus via MTurk, we want to make sure that non-
expert annotations are good enough to train reliable
SWSD classifiers. Thus, we decided to compare
the performance of a SWSD system trained on non-
expert annotations and on expert annotations. For
this purpose, we need a subjectivity sense-tagged
corpus where word instances are tagged both by ex-
pert and non-expert annotations. Fortunately, we
have such a corpus. As discussed in Section 3,
(Akkaya et al, 2009) created a subjecvitivity sense-
tagged corpus piggybacked on SENSEVAL. This
gives us a gold-standard corpus tagged by experts.
There is also a small subjectivity sense-tagged cor-
pus consisting of eight target words obtained from
non-expert annotators in (Akkaya et al, 2010). This
corpus is a subset of the gold-standard corpus from
(Akkaya et al, 2009) and it consists of 60 tagged
89
Acc p-value
SWSDGOLD 79.2 -
SWSDMJL 78.4 0.542
SWSDMJC 78.8 0.754
Table 1: Comparison of SWSD systems
instances for each target word.
Actually, (Akkaya et al, 2010) gathered three la-
bels for each instance. This gives us two options
to train the non-expert SWSD system: (1) training
the system on the majority vote labels (SWSDMJL)
(2) training three systems on the three separate la-
bel sets and taking the majority vote prediction
(SWSDMJC). Additionally, we train an expert SWSD
system (SWSDGOLD) ? a system trained on gold
standard expert annotations. All these systems are
trained on 60 instances of the eight target words for
which we have both non-expert and expert annota-
tions and are evaluated on the remaining instances
of the gold-standard corpus. This makes a total of
923 test instances for the eight target words with a
majority class baseline of 61.8.
Table 1 reports micro-average accuracy of each
system and the two-tailed p-value between the ex-
pert SWSD system and the two non-expert SWSD
systems. The p-value is calculated with McNemar?s
test. It shows that there is no statistically signif-
icant difference between classifiers trained on ex-
pert gold-standard annotations and non-expert anno-
tations. We adopt SWSDMJL in all our following ex-
periments, because it is more efficient.
2.4 Corpus Creation
For our experiments, we have multiple goals, which
effect our decisions on how to create the subjectiv-
ity sense-tagged corpus via MTurk. First, we want
to be able to disambiguate more target words than
(Akkaya et al, 2009). This way, SWSD will be able
to disambiguate a larger portion of the MPQA Cor-
pus allowing us to evaluate the effect of SWSD on
contextual opinion analysis on a larger scale. This
will also allow us to investigate additional integra-
tion methods of SWSD into contextual opinion anal-
ysis rather than simple ad hoc manual rules utilized
in (Akkaya et al, 2009). Second, we want to show
that we can rely on non-expert annotations instead of
expert annotations, which will make an annotation
effort on a larger-scale both practical and feasible,
timewise and costwise. Optimally, we could have
annotated via MTurk the same subjectivity sense-
tagged corpus from (Akkaya et al, 2009) in order to
compare the effect of a non-expert SWSD system on
contextual opinion analysis directly with the results
reported for an expert SWSD system in (Akkaya et
al., 2009). But, this would have diverted our re-
sources to reproduce the same corpus and contradict
our goal to extend the subjectivity sense-tagged cor-
pus to new target words. Moreover, we have already
shown in Section 2.3 that non-expert annotations can
be utilized to train reliable SWSD classifiers. It is
reasonable to believe that similar performance on
the SWSD task will reflect to similar improvements
on contextual opinion analysis. Thus, we decided
to prioritize creating a subjectivity sense-tagged cor-
pus for a totally new set of words. We aim to show
that the favourable results reported in (Akkaya et al,
2009) will still hold on new target words relying on
non-expert annotations.
We chose our target words from the subjectivity
lexicon of (Wilson et al, 2005), because we know
they have subjective usages. The contextual opin-
ion systems we want to improve rely on this lexicon.
We call the words in the lexicon subjectivity clues.
At this stage, we want to concentrate on the fre-
quent and ambiguous subjectivity clues. We chose
frequent ones, because they will have larger cov-
erage in the MPQA Corpus. We chose ambiguous
ones, because these clues are the ones that are most
important for SWSD. Choosing most frequent and
ambiguous subjectivity clues guarantees that we uti-
lize our limited resources in the most efficient way.
We judge a clue to be ambiguous if it appears more
than 25% and less than 75% of the times in a sub-
jective expression. We get these statistics by simply
counting occurrences in the MPQA Corpus inside
and outside of subjective expressions.
There are 680 subjectivity clues that appear in the
MPQA Corpus and are ambiguous. Out of those, we
selected the 90 most frequent that have to some ex-
tent distinct objective and subjective senses in Word-
Net, as judged by the co-authors. The co-authors an-
notated the WordNet senses of those 90 target words.
For each target word, we selected approximately 120
instances randomly from the GIGAWORD Corpus.
In a first phase, we collected three sets of MTurk an-
90
notations for the selected instances. In this phase,
MTurk workers base their judgements on two sense
sets they observe. This way, we get training data to
build SWSD classifiers for these 90 target words.
The quality of these classifiers is important, be-
cause we will exploit them for contextual opinion
analysis. Thus, we evaluate them by 10-fold cross-
validation. We split the target words into three
groups. If the majority class baseline of a word is
higher than 90%, it is considered as skewed (skewed
words have a performance at least as good as the ma-
jority class baseline). If a target word improves over
its majority class baseline by 25% in accuracy, it is
considered as good. Otherwise, it is considered as
mediocre. This way, we end up with 24 skewed, 35
good, and 31 mediocre words. There are many pos-
sible reasons for the less reliable performance for
the mediocre group. We hypothesize that a major
problem is the similarity between the objective and
subjective sense sets of a word, thus leading to poor
annotation quality. To check this, we calculate the
agreement between three annotation sets and report
averages. The agreement in the mediocre group is
78.68%, with a ? value of 0.57, whereas the aver-
age agreement in the good group is 87.51%, with
a ? value of 0.75. These findings support our hy-
pothesis. Thus, the co-authors created usage inven-
tories for the words in the mediocre group as de-
scribed in Section 2.1.1. We initiated a second phase
of MTurk annotations. We collect for the mediocre
group another three sets of MTurk annotations for
120 instances, this time utilizing usage inventories.
The 10-fold cross-validation experiments show that
nine of the 31 words in the mediocre group shift to
the good group. Only for these nine words, we ac-
cept the annotations collected via usage inventories.
For all other words, we use the annotations collected
via sense inventories. From now on, we will refer
to this non-expert subjectivity sense-tagged corpus
consisting of the tagged data for all 90 target words
as the MTurkSWSD Corpus (agreement on the entire
MTurkSWSD corpus is 85.54%, ?:0.71).
3 SWSD Integration
Now that we have the MTurkSWSD Corpus, we
are ready to evaluate the effect of SWSD on con-
textual opinion analysis. In this section, we ap-
ply our SWSD system trained on MTurkSWSD to
both expression-level classifiers from (Akkaya et al,
2009): (1) the subjective/objective (S/O) classifier
and (2) the contextual polarity classifier. Both clas-
sifiers are introduced in Section 3.1
Our SWSD system can disambiguate 90 target
words, which have 3737 instances in the MPQA
Corpus. We refer to this subset of the MPQA Corpus
as MTurkMPQA. This subset makes up the cover-
age of our SWSD system. Note that MTurkMPQA
is 5.2 times larger than the covered MPQA subset
in (Akkaya et al, 2009) referred as senMPQA. We
try different strategies to integrate SWSD into the
contextual classifiers. In Section 3.2, we follow the
same rule-based strategy as in (Akkaya et al, 2009)
for completeness. In Section 3.3, we introduce two
new learning strategies for SWSD integration out-
performing existing rule-based strategy. We evalu-
ate the improvement gained by SWSD on MTurkM-
PQA.
3.1 Contextual Classifiers
The original contextual polarity classifier is intro-
duced in (Wilson et al, 2005). We use the same im-
plementation as in (Akkaya et al, 2009). This classi-
fier labels clue instances in text as contextually neg-
ative/positive/neutral. The gold standard is defined
on the MPQA Corpus as follows. If a clue instance
appears in a positive expression, it is contextually
positive (Ps). If it appears in a negative expression,
it is contextually negative (Ng). If it is in an objec-
tive expression or in a neutral subjective expression,
it is contextually neutral (N). The contextual polar-
ity classifier consists of two separate steps. The first
step is an expression-level neutral/polar (N/P) clas-
sifier. The second step classifies only polar instances
further into positive and negative classes. This way,
the overall system performs a three-way classifica-
tion (Ng/Ps/N).
The subjective/objective classifier is introduced in
(Akkaya et al, 2009). It relies on the same machine
learning features as the N/P classifier (i.e. the first
step of the contextual polarity classifier). The only
difference is that the classes are S/O instead of N/P.
The gold standard is defined on the MPQA Corpus
in the following way. If a clue instance appears in
a subjective expression, it is contextually S. If it ap-
pears in an objective expression, it is contextually O.
Both contextual classifiers are supervised.
91
Baseline Acc OF SF
MTurkMPQA 52.4% (O)
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
senMPQA 63.1% (O)
OS/O 75.4 65.4 80.9
R1R2 81.3 75.9 84.8
Table 2: S/O classifier with and without SWSD.
3.2 Rule-Based SWSD Integration
(Akkaya et al, 2009) integrates SWSD into a con-
textual classifier by simple rules. The rules flip the
output of the contextual classifier if some conditions
hold. They make use of following information: (1)
SWSD output, (2) the contextual classifier?s confi-
dence and (3) the presence of another subjectivity
clue ? any clue from the subjectivity lexicon ? in the
same expression.
For the contextual S/O classifier, (Akkaya et al,
2009) defines two rules: one flipping the S/O classi-
fier?s output from O to S (R1) and one flipping from
S to O (R2). R1 is defined as follows : if the contex-
tual classifier decides a target word instance is con-
textually O and SWSD decides that it is used in a S
sense, then SWSD overrules the contextual S/O clas-
sifier?s output and flips it from O to S, because an
instance in a S sense will make the surrounding ex-
pression subjective. R2 is a little bit more complex.
It is defined as follows: If the contextual classifier la-
bels a clue instance as S but (1) SWSD decides that
it is used in an O sense, (2) the contextual classifier?s
confidence is low, and (3) there is no other subjec-
tivity clue in the same expression, then R2 flips the
contextual classifier?s output from S to O. The ra-
tionale behind R2 is that even if the target word in-
stance has an O sense, there might be another reason
(e.g. the presence of another subjectivity clue in the
same expression) for the expression enclosing it to
be subjective.
We use the exact same rules and adopt the same
confidence threshold. Table 2 holds the comparison
of the original contextual classifier and the classi-
fier with SWSD support on senMPQA as reported in
(Akkaya et al, 2009) and on MTurkMPQA. OS/O is
the original S/O classifier; R1R2 is the system with
SWSD support utilizing both rules. We report only
R1R2, since (Akkaya et al, 2009) gets highest im-
provement utilizing both rules.
Baseline Acc NF PF
MTurkMPQA 70.6% (P)
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
senMPQA 73.9% (P)
ON/P 79.0 86.7 50.3
R4 81.6 88.6 52.3
Table 3: N/P classifier with and without SWSD
In Table 2 we see that R1R2 achieves 4% percent-
age points improvement in accuracy over OS/O on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test. It
is accompanied with improvements both in subjec-
tive F-measure (SF) and objective F-measure (OF).
It is not possible to directly compare improvements
on senMPQA and MTurkMPQA since they are dif-
ferent subsets of the MPQA Corpus. SWSD support
brings 24% error reduction on senMPQA over the
original S/O classifier. In comparison, on MTurkM-
PQA, the error reduction is 12%. We see that the im-
provements on the large MTurkMPQA set still hold,
but not as strong as in (Akkaya et al, 2009).
(Akkaya et al, 2009) uses a similar rule to
make the contextual polarity classifier sense-aware.
Specifically, the rule is applied to the output of the
first step (N/P classifier). The rule, R4, flips P to N
and is analogous to R2. If the contextual classifier
labels a clue instance as P but (1) SWSD decides
that it is used in an O sense, (2) the contextual clas-
sifier?s confidence is low, and (3) there is no other
clue instance in the same expression, then R4 flips
the contextual classifier?s output from P to N.
Table 3 holds the comparison of the original N/P
classifier with and without SWSD support on sen-
MPQA as reported in (Akkaya et al, 2009) and on
MTurkMPQA. ON/P is the original N/P classifier; R4
is the system with SWSD support utilizing rule R4.
Since our main focus is not rule-based integration,
we did not run the second step of the polarity classi-
fier. We report the second step result below for the
learning-based SWSD integration in section 3.4.
In Table 3, we see that R4 achieves 2.2 percent-
age points improvement in accuracy over ON/P on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test.
It is accompanied with improvement only in objec-
tive F-measure (OF). SWSD support brings 12.4%
error reduction on senMPQA (Akkaya et al, 2009).
92
On MTurkMPQA, the error reduction is 8%. We see
that the rule-based SWSD integration still improves
both contextual classifiers on MTurkMPQA, but the
gain is not as large as on senMPQA. This might be
due to the brittleness of the rule-based integration.
3.3 Learning SWSD Integration
Now that we can disambiguate a larger portion of
the MPQA Corpus than in (Akkaya et al, 2009),
we can investigate machine learning methods for
SWSD integration to deal with the brittleness of the
rule-based integration. In this section, we introduce
two learning methods to apply SWSD to the contex-
tual classifiers. For the learning methods, we rely on
exactly the same information as the rule-based inte-
gration: (1) SWSD output, (2) the contextual clas-
sifier?s output, (3) the contextual classifier?s confi-
dence, and (4) the presence of another clue instance
in the same expression. The rationale is the same as
for the rule-based integration, namely to relate sense
subjectivity and contextual subjectivity.
3.3.1 Method1
In the first method, we extend the machine learn-
ing features of the underlying contextual classifiers
by adding (1) and (4) from above. We evaluate the
extended contextual classifiers on MTurkMPQA via
10-fold cross-validation. Tables 4 and 5 hold the
comparison of Method1 (EXTS/O, EXTN/P) to the
original contextual classifiers (OS/O, ON/P) and to the
rule-based SWSD integration (R1R2, R4). We see
substantial improvement for Method1. It achieves
39% error reduction over OS/O and 25% error reduc-
tion over ON/P. For both classifiers, the improvement
in accuracy over the rule-based integration is statisti-
cally significant at the p< .01 level with McNemar?s
test.
3.3.2 Method2
This method defines a third classifier that accepts
as input the contextual classifier?s output and the
SWSD output and predicts what the contextual clas-
sifier?s output should have been. We can think of
this third classifier as the learning counterpart of
the manual rules from Section 3.2, since it actu-
ally learns when to flip the contextual classifier?s
output considering SWSD evidence. Specifically,
this merger classifier relies on four machine learn-
ing features (1), (2), (3), (4) from above (the ex-
Acc OF SF
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
EXTS/O 80.0 81.4 78.3
MERGERS/O 78.2 80.3 75.5
Table 4: S/O classifier with learned SWSD integration
Acc NF PF
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
EXTN/P 79.1 85.7 61.1
MERGERN/P 80.4 86.7 62.8
Table 5: N/P classifier with learned SWSD integration
act same information used in rule-based integration).
Because it is a supervised classifier, we need train-
ing data where we have clue instances with cor-
responding contextual classifier and SWSD predic-
tions. Fortunately, we can use senMPQA for this
purpose. We train our merger classifier on senM-
PQA (we get contextual classifier predictions via 10-
fold cross-validation on the MPQA Corpus) and ap-
ply it to MTurkMPQA. We use SVM classifier from
the Weka package (Witten and Frank., 2005) with
its default settings. Tables 4 and 5 hold the com-
parison of Method2 (MERGERS/O, MERGERN/P) to
the original contextual classifiers (Oo/s, ON/P) and
the rule-based SWSD integration (R1R2, R4). It
achieves 29% error reduction over OS/O and 29% er-
ror reduction over ON/P. The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. Method2
performs better (statistically significant at the p <
.05 level) than Method1 for the N/P classifier but
worse (statistically significant at the p < .01 level)
for the S/O classifier.
3.4 Improving Contextual Polarity
Classification
We have seen that Method2 is the best method to
improve the N/P classifier, which is the first step
of the contextual polarity classifier. To assess the
overall improvement in polarity classification, we
run the second step of the contextual polarity clas-
sifier after correcting the first step with Method2.
Table 6 summarizes the improvement propagated to
93
Acc NF NgF PsF
MTurkMPQA
OPs/Ng/N 72.1 83.0 34.2 15.0
MERGERN/P 77.8 87.4 53.0 27.7
senMPQA
OPs/Ng/N 77.6 87.2 39.5 40.0
R4 80.6 89.1 43.2 44.0
Table 6: Polarity classifier with and without SWSD.
Ps/Ng/N classification. For comparison, we also
include results from (Akkaya et al, 2009) on sen-
MPQA. Method2 results in 20% error reduction in
accuracy over OPs/Ng/N (R4 achieves 13.4% error
reduction on senMPQA). The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. More im-
portantly, the F-measure for all the labels improves.
This indicates that non-expert MTurk annotations
can replace expert annotations for our end-goal ? im-
proving contextual opinion analysis ? while reduc-
ing time and cost requirements by a large margin.
Moreover, we see that the improvements in (Akkaya
et al, 2009) scale up to new subjectivity clues.
4 Related Work
One related line of research is to automatically
assign subjectivity and/or polarity labels to word
senses in a dictionary (Valitutti et al, 2004; An-
dreevskaia and Bergler, 2006; Wiebe and Mihalcea,
2006; Esuli and Sebastiani, 2007; Su and Markert,
2009). In contrast, the task in our paper is to auto-
matically assign labels to word instances in a corpus.
Recently, some researchers have exploited full
word sense disambiguation in methods for opinion-
related tasks. For example, (Mart??n-Wanton et al,
2010) exploit WSD for recognizing quotation polar-
ities, and (Rentoumi et al, 2009; Mart??n-Wanton et
al., 2010) exploit WSD for recognizing headline po-
larities. None of this previous work investigates per-
forming a coarse-grained variation of WSD such as
SWSD to improve their application results, as we do
in this work.
A notable exception is (Su and Markert, 2010),
who exploit SWSD to improve the performance on
a contextual NLP task, as we do. While the task
in our paper is subjectivity and sentiment analy-
sis, their task is English-Chinese lexical substitu-
tion. As (Akkaya et al, 2009) did, they anno-
tated word senses, and exploited SENSEVAL data
as training data for SWSD. They did not directly an-
notate words in context with S/O labels, as we do in
our work. Further, they did not separately evaluate a
SWSD system component.
Many researchers work on reducing the granular-
ity of sense inventories for WSD (e.g., (Palmer et al,
2004; Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)). Their criteria for grouping senses are syn-
tactic and semantic similarities, while the groupings
in work on SWSD are driven by the goals to improve
contextual subjectivity and sentiment analysis.
5 Conclusions and Future Work
In this paper, we utilized a large pool of non-expert
annotators (MTurk) to collect subjectivity sense-
tagged data for SWSD. We showed that non-expert
annotations are as good as expert annotations for
training SWSD classifiers. Moreover, we demon-
strated that SWSD classifiers trained on non-expert
annotations can be exploited to improve contextual
opinion analysis.
The additional subjectivity sense-tagged data en-
abled us to evaluate the benefits of SWSD on con-
textual opinion analysis on a corpus of opinion-
annotated data that is five times larger. Using the
same rule-based integration strategies as in (Akkaya
et al, 2009), we found that contextual opinion anal-
ysis is improved by SWSD on the larger datasets.
We also experimented with new learning strategies
for integrating SWSD into contextual opinion analy-
sis. With the learning strategies, we achieved greater
benefits from SWSD than the rule-based integration
strategies on all of the contextual opinion analysis
tasks.
Overall, we more firmly demonstrated the poten-
tial of SWSD to improve contextual opinion analy-
sis. We will continue to gather subjectivity sense-
tagged data, using sense inventories for words that
are well represented in WordNet for our purposes,
and with usage inventories for those that are not.
6 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
94
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 24?32. Asso-
ciation for Computational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 190?199, Singa-
pore, August. Association for Computational Linguis-
tics.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 195?203, Los Angeles, June. Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006).
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In Pro-
ceedings of ACL-08: HLT, pages 290?298, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
424?431, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2009), pages
10?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of the Human Language Technology
Conference of the NAACL, Companion Volume: Short
Papers, New York City.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twen-
tieth International Conference on Computational Lin-
guistics (COLING 2004), pages 1267?1373, Geneva,
Switzerland.
Tamara Mart??n-Wanton, Aurora Pons-Porrata, Andre?s
Montoyo-Guijarro, and Alexandra Balahur. 2010.
Opinion polarity detection - using word sense disam-
biguation to determine the polarity of opinions. In
ICAART 2010 - Proceedings of the International Con-
ference on Agents and Artificial Intelligence, Volume
1, pages 483?486.
R. Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applications.
In HLT-NAACL 2004 Workshop: 2nd Workshop on
Scalable Natural Language Understanding, Boston,
Massachusetts.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sentiment
analysis of figurative language using a word sense
disambiguation approach. In Proceedings of the In-
ternational Conference RANLP-2009, pages 370?375,
Borovets, Bulgaria, September. Association for Com-
putational Linguistics.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-2003), pages 105?
112, Sapporo, Japan.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008), Manch-
ester.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 1?9, Boulder, Colorado, June. Associ-
ation for Computational Linguistics.
95
Fangzhong Su and Katja Markert. 2010. Word sense
subjectivity for cross-lingual lexical substitution. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 357?
360, Los Angeles, California, June. Association for
Computational Linguistics.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-02), pages 417?424, Philadelphia, Pennsyl-
vania.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology, 2(1):61?83.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05, the ACM SIGIR
Conference on Information and Knowledge Manage-
ment, Bremen, DE.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1065?1072, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practical
Machine Learning Tools and Techniques, Second Edi-
tion. Morgan Kaufmann, June.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2003), pages 129?136, Sapporo, Japan.
96
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 96?104,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Topic Modeling on Historical Newspapers
Tze-I Yang
Dept. of Comp. Sci. & Eng.
University of North Texas
tze-iyang@my.unt.edu
Andrew J. Torget
Dept. of History
University of North Texas
andrew.torget@unt.edu
Rada Mihalcea
Dept. of Comp. Sci. & Eng.
University of North Texas
rada@cs.unt.edu
Abstract
In this paper, we explore the task of automatic
text processing applied to collections of his-
torical newspapers, with the aim of assisting
historical research. In particular, in this first
stage of our project, we experiment with the
use of topical models as a means to identify
potential issues of interest for historians.
1 Newspapers in Historical Research
Surviving newspapers are among the richest sources
of information available to scholars studying peo-
ples and cultures of the past 250 years, particularly
for research on the history of the United States.
Throughout the nineteenth and twentieth centuries,
newspapers served as the central venues for nearly
all substantive discussions and debates in American
society. By the mid-nineteenth century, nearly every
community (no matter how small) boasted at least
one newspaper. Within these pages, Americans ar-
gued with one another over politics, advertised and
conducted economic business, and published arti-
cles and commentary on virtually all aspects of so-
ciety and daily life. Only here can scholars find edi-
torials from the 1870s on the latest political contro-
versies, advertisements for the latest fashions, arti-
cles on the latest sporting events, and languid poetry
from a local artist, all within one source. Newspa-
pers, in short, document more completely the full
range of the human experience than nearly any other
source available to modern scholars, providing win-
dows into the past available nowhere else.
Despite their remarkable value, newspapers have
long remained among the most underutilized histor-
ical resources. The reason for this paradox is quite
simple: the sheer volume and breadth of informa-
tion available in historical newspapers has, ironi-
cally, made it extremely difficult for historians to
go through them page-by-page for a given research
project. A historian, for example, might need to
wade through tens of thousands of newspaper pages
in order to answer a single research question (with
no guarantee of stumbling onto the necessary infor-
mation).
Recently, both the research potential and prob-
lem of scale associated with historical newspapers
has expanded greatly due to the rapid digitization of
these sources. The National Endowment for the Hu-
manities (NEH) and the Library of Congress (LOC),
for example, are sponsoring a nationwide historical
digitization project, Chronicling America, geared to-
ward digitizing all surviving historical newspapers
in the United States, from 1836 to the present. This
project recently digitized its one millionth page (and
they project to have more than 20 million pages
within a few years), opening a vast wealth of his-
torical newspapers in digital form.
While projects such as Chronicling America have
indeed increased access to these important sources,
they have also increased the problem of scale that
have long prevent scholars from using these sources
in meaningful ways. Indeed, without tools and
methods capable of handling such large datasets ?
and thus sifting out meaningful patterns embedded
within them ? scholars find themselves confined to
performing only basic word searches across enor-
mous collections. These simple searches can, in-
deed, find stray information scattered in unlikely
96
places. Such rudimentary search tools, however,
become increasingly less useful to researchers as
datasets continue to grow in size. If a search for a
particular term yields 4,000,000 results, even those
search results produce a dataset far too large for any
single scholar to analyze in a meaningful way us-
ing traditional methods. The age of abundance, it
turns out, can simply overwhelm historical scholars,
as the sheer volume of available digitized historical
newspapers is beginning to do.
In this paper, we explore the use of topic mod-
eling, in an attempt to identify the most important
and potentially interesting topics over a given pe-
riod of time. Thus, instead of asking a historian
to look through thousands of newspapers to identify
what may be interesting topics, we take a reverse
approach, where we first automatically cluster the
data into topics, and then provide these automati-
cally identified topics to the historian so she can nar-
row her scope to focus on the individual patterns in
the dataset that are most applicable to her research.
Of more utility would be where the modeling would
reveal unexpected topics that point towards unusual
patterns previously unknown, thus help shaping a
scholar?s subsequent research.
The topic modeling can be done for any periods
of time, which can consist of individual years or can
cover several years at a time. In this way, we can
see the changes in the discussions and topics of in-
terest over the years. Moreover, pre-filters can also
be applied to the data prior to the topic modeling.
For instance, since research being done in the His-
tory department at our institution is concerned with
the ?U. S. cotton economy,? we can use the same ap-
proach to identify the interesting topics mentioned in
the news articles that talk about the issue of ?cotton.?
2 Topic Modeling
Topic models have been used by
Newman and Block (2006) and Nelson (2010)1
on newspaper corpora to discover topics and trends
over time. The former used the probabilistic latent
semantic analysis (pLSA) model, and the latter
used the latent Dirichlet alocation (LDA) model, a
method introduced by Blei et al (2003). LDA has
also been used by Griffiths and Steyvers (2004) to
1http://americanpast.richmond.edu/dispatch/
find research topic trends by looking at abstracts of
scientific papers. Hall et al (2008) have similarly
applied LDA to discover trends in the computational
linguistics field. Both pLSA and LDA models are
probabilistic models that look at each document as
a mixture of multinomials or topics. The models
decompose the document collection into groups of
words representing the main topics. See for instance
Table 1, which shows two topics extracted from our
collection.
Topic
worth price black white goods yard silk made ladies
wool lot inch week sale prices pair suits fine quality
state states bill united people men general law gov-
ernment party made president today washington war
committee country public york
Table 1: Example of two topic groups
Boyd-Graber et al (2009) compared several topic
models, including LDA, correlated topic model
(CTM), and probabilistic latent semantic indexing
(pLSI), and found that LDA generally worked com-
parably well or better than the other two at pre-
dicting topics that match topics picked by the hu-
man annotators. We therefore chose to use a par-
allel threaded SparseLDA implementation to con-
duct the topic modeling, namely UMass Amherst?s
MAchine Learning for LanguagE Toolkit (MAL-
LET)2 (McCallum, 2002). MALLET?s topic mod-
eling toolkit has been used by Walker et al (2010)
to test the effects of noisy optical character recog-
nition (OCR) data on LDA. It has been used by
Nelson (2010) to mine topics from the Civil War
era newspaper Dispatch, and it has also been used
by Blevins (2010) to examine general topics and to
identify emotional moments from Martha Ballards
Diary.3
3 Dataset
Our sample data comes from a collection of digi-
tized historical newspapers, consisting of newspa-
pers published in Texas from 1829 to 2008. Issues
are segmented by pages with continuous text con-
taining articles and advertisements. Table 2 provides
more information about the dataset.
2http://mallet.cs.umass.edu/
3http://historying.org/2010/04/01/
97
Property
Number of titles 114
Number of years 180
Number of issues 32,745
Number of pages 232,567
Number of tokens 816,190,453
Table 2: Properties of the newspaper collection
3.1 Sample Years and Categories
From the wide range available, we sampled sev-
eral historically significant dates in order to evaluate
topic modeling. These dates were chosen for their
unique characteristics (detailed below), which made
it possible for a professional historian to examine
and evaluate the relevancy of the results.
These are the subcategories we chose as samples:
? Newspapers from 1865-1901: During this pe-
riod, Texans rebuilt their society in the after-
math of the American Civil War. With the abo-
lition of slavery in 1865, Texans (both black
and white) looked to rebuild their post-war
economy by investing heavily in cotton pro-
duction throughout the state. Cotton was con-
sidered a safe investment, and so Texans pro-
duced enough during this period to make Texas
the largest cotton producer in the United States
by 1901. Yet overproduction during that same
period impoverished Texas farmers by driving
down the market price for cotton, and thus a
large percentage went bankrupt and lost their
lands (over 50 percent by 1900). As a re-
sult, angry cotton farmers in Texas during the
1890s joined a new political party, the Pop-
ulists, whose goal was to use the national gov-
ernment to improve the economic conditions of
farmers. This effort failed by 1896, although it
represented one of the largest third-party polit-
ical revolts in American history.
This period, then, was dominated by the rise
of cotton as the foundation of the Texas econ-
omy, the financial failures of Texas farmers,
and their unsuccessful political protests of the
1890s as cotton bankrupted people across the
state. These are the issues we would expect to
emerge as important topics from newspapers in
this category. This dataset consists of 52,555
pages over 5,902 issues.
? Newspapers from 1892: This was the year of
the formation of the Populist Party, which a
large portion of Texas farmers joined for the
U. S. presidential election of 1892. The Pop-
ulists sought to have the U. S. federal gov-
ernment become actively involved in regulat-
ing the economy in places like Texas (some-
thing never done before) in order to prevent
cotton farmers from going further into debt. In
the 1892 election, the Populists did surprisingly
well (garnering about 10 percent of the vote na-
tionally) and won a full 23 percent of the vote
in Texas. This dataset consists of 1,303 pages
over 223 issues.
? Newspapers from 1893: A major economic
depression hit the United States in 1893, dev-
astating the economy in every state, including
Texas. This exacerbated the problem of cotton
within the states economy, and heightened the
efforts of the Populists within Texas to push for
major political reforms to address these prob-
lems. What we see in 1893, then, is a great deal
of stress that should exacerbate trends within
Texas society of that year (and thus the con-
tent of the newspapers). This dataset consists
of 3,490 pages over 494 issues.
? Newspapers from 1929-1930: These years
represented the beginning and initial onset in
the United States of the Great Depression. The
United States economy began collapsing in Oc-
tober 1929, when the stock market crashed and
began a series of economic failures that soon
brought down nearly the entire U. S. econ-
omy. Texas, with its already shaky economic
dependence on cotton, was as devastated as any
other state. As such, this period was marked
by discussions about how to save both the cot-
ton economy of Texas and about possible gov-
ernment intervention into the economy to pre-
vent catastrophe. This dataset consists of 6,590
pages over 973 issues.
Throughout this era, scholars have long recog-
nized that cotton and the economy were the domi-
nating issues. Related to that was the rise and fall
98
load
documents
spelling
correction
named
entity
tagger
stemmer
topic
modeling
topic
modeling
topic
modeling
Figure 1: Work flow
of the Populist Party during the 1890s, as farmers
sought to use the political system as a means of
dealing with their economic problems. As such, we
would expect to see these concerns as major (per-
haps dominating) topics in the newspapers from the
time.
3.1.1 ?Cotton? data
Within the date ranges listed above, we also se-
lect all mentions of the topic ?cotton? ? as pertain-
ing to possible discussion relevant to the ?U. S. cot-
ton economy.? Cotton was the dominating economic
force in Texas throughout this period, and historians
have long recognized that issues related to the crop
wielded tremendous influence on the political, so-
cial, and economic development of the state during
this era. Problems related to cotton, for example,
bankrupted half of all Texas farmers between 1865
and 1900, and those financial challenges pushed
farmers to create a major new political party during
the 1890s.
3.2 Data Processing
Before applying topic modeling on our data, some
pre-processing steps were applied. Some challenges
in processing the dataset come from errors intro-
duced by the OCR processing, missing punctua-
tions, and unclear separation between different ar-
ticles on the same page. Multi-stage pre-processing
of the dataset was performed to reduce these errors,
as illustrated in Figure 1.
The first phase to reduce errors starts with spelling
correction, which replaces words using the As-
pell dictionary and de-hyphenates words split across
lines. Suggested replacements are used if they are
within the length normalized edit distance of the
originals. An extra dictionary list of location names
is used with Aspell.
Next, the spelling corrected dataset is run through
the Stanford Named Entity Recognizer (NER).4
Stanford NER system first detects sentences in the
data then labels four classes of named entities: PER-
SON, ORGANIZATION, LOCATION, and MIS-
CELLANEOUS (Finkel et al, 2005). The model
used in conjunction with the tagger is provided by
the software and was trained on the CoNLL 2003
training data using distributional similarity features.
The output is then massaged so that entities with
multiple words would stay together in the topic mod-
eling phase.
Property # of Unique # of Total
LOC entities 1,508,432 8,620,856
ORG entities 6,497,111 14,263,391
PER entities 2,846,906 12,260,535
MISC entities 1,182,845 3,594,916
Named entities 12,035,294 38,739,698
Table 3: Properties of the newspaper collection after
named entity recognition
Lastly, the words that are not tagged as named
entities pass through an English stemmer while the
named entities stay unchanged. We are using the
Snowball stemmer.5
At the end of each of the pre-processing stage, we
extract subsets from the data corresponding to the
sample years mentioned earlier (1865-1901, 1892,
1893, and 1929-1930), which are then used for fur-
ther processing in the topic modeling phase.
We made cursory comparisons of the outputs
of the topic modeling at each of the three stages
(spelling correction, NER, stemming). Table 4
shows sample topic groups generated at the three
stages. We found that skipping the named entity
tagging and stemming phases still gives compara-
ble results. While the named entity tags may give us
additional information (?dallas? and ?texas? are lo-
cations), tagging the entire corpus takes up a large
slice of processing time. Stemming after tagging
4http://nlp.stanford.edu/software/
5http://snowball.tartarus.org
99
Topic: spell
worth fort city texas county gazette tex special state
company dallas time made yesterday night business
line railroad louis
Topic: spell + NER
city county texas location company yesterday night
time today worth made state morning fort special
business court tex dallas location meeting
Topic: spell + NER + stemmer
state counti citi texas location year ani time made
worth fort peopl good line special tex land busi work
compani
Table 4: Comparison of the three topic output stages:
Each entry contains the top terms for a single topic
may collapse multiple versions of a word together,
but we found that the stemmed words are very hard
to understand such as the case of ?business? becom-
ing ?busi?. In future work, we may explore using
a less aggressive stemmer that only collapses plu-
rals, but so far the first stage output seems to give
fairly good terms already. Thus, the rest of the pa-
per will discuss using the results of topic modeling
at the spelling correction stage.
4 Historical Topics and Trends
We are interested in automatically discovering gen-
eral topics that appear in a large newspaper corpus.
MALLET is run on each period of interest to find
the top one general topic groups. We use 1000 it-
erations with stopword removal. An extra stopword
list was essential to remove stopwords with errors
introduced by the OCR process. Additionally, we
run MALLET on the 1865-1901 dataset to find the
top ten topic groups using 250 iterations.
In addition, we also find the topics more strongly
associated with ?cotton.? The ?cotton? examples are
found by extracting each line that contains an in-
stance of ?cotton? along with a window of five lines
on either side. MALLET is then run on these ?cot-
ton? examples to find the top general topic groups
over 1000 iterations with stopword removal.
5 Evaluation and Discussion
The topic modeling output was evaluated by a histo-
rian (the second author of this paper), who special-
izes in the U.S.-Mexican borderlands in Texas and
is an expert in the historical chronology, events, and
language patterns of our newspaper collection. The
evaluator looked at the output, and determined for
each topic if it was relevant to the period of time un-
der consideration.
The opinion from our expert is that the topic mod-
eling yielded highly useful results. Throughout the
general topics identified for our samples, there is a
consistent theme that a historian would expect from
these newspapers: a heavy emphasis on the eco-
nomics of cotton. For example, we often see words
like ?good,? ?middling,? and ?ordinary,? which were
terms for evaluating the quality of a cotton crop be-
fore it went to market. Other common terms, such as
?crop,? ?bale,? ?market,? and ?closed? (which sug-
gests something like ?the price closed at X?) evoke
other topics of discussion of aspects of the buying
and selling of cotton crops.
Throughout the topics, market-oriented language
is the overwhelming and dominate theme through-
out, which is exactly what our expert expected as a
historian of this region and era. You can see, for ex-
ample, that much of the cotton economy was geared
toward supplies the industrial mills in England. The
word ?Liverpool,? the name of the main English port
to where Texas cotton was shipped, appears quite
frequently throughout the samples. As such, these
results suggest a high degree of accuracy in identi-
fying dominate and important themes in the corpus.
Within the subsets of these topics, we find more
fine-grained patterns that support this trend, which
lend more credence to the results.
Table 5 summarizes the results for each of
the three analyzes, with accuracy calculated as
follows: Accuracy(topics) = # of relevant topicstotal # of topics
Accuracy(terms) = # of relevant terms in all topicstotal # of terms in all topics . Ta-
bles 6, 7 and 8 show the actual analyzes.
5.1 Interesting Finding
Our historian expert found the topic containing
?houston april general hero san? for the 1865-1901
general results particularly interesting and hypoth-
esized that they may be referring to the Battle of
San Jacinto. The Battle of San Jacinto was the fi-
nal fight in the Texas Revolution of 1836, as Texas
sought to free themselves from Mexican rule. On
April 21, 1836, General Sam Houston led about 900
100
Topics Explanation
black* price* worth* white* goods* yard* silk*
made* lot* week ladies wool* inch* ladles* sale*
prices* pair* suits* fine*
Reflects discussion of the market and sales of goods, with
some words that relate to cotton and others that reflect
other goods being sold alongside cotton (such as wool).
state* people* states* bill* law* made united* party*
men* country* government* county* public* presi-
dent* money* committee* general* great question*
Political language associated with the political debates
that dominated much of newspaper content during this
era. The association of the topic ?money? is particularly
telling, as economic and fiscal policy were particularly
important discussion during the era.
clio worth mid city alie fort lino law lour lug thou hut
fur court dally county anil tort iron
Noise and words with no clear association with one an-
other.
tin inn mid tint mill* till oil* ills hit hint lull win hut
ilia til ion lot lii foi
Mostly noise, with a few words associated with cotton
milling and cotton seed.
texas* street* address* good wanted houston* office*
work city* sale main* house* apply man county* av-
enue* room* rooms* land*
These topics appear to reflect geography. The inclusion
of Houston may either reflect the city?s importance as a
cotton market or (more likely) the large number of news-
papers from the collection that came from Houston.
worth* city* fort* texas* county* gazette tex* com-
pany* dallas* miss special yesterday night time john
state made today louis*
These topics appear to reflect geography in north Texas,
likely in relation to Fort Worth and Dallas (which appear
as topics) and probably as a reflection that a large portion
of the corpus of the collection came from the Dallas/Ft.
Worth area.
houston* texas* today city* company post* hero*
general* night morning york men* john held war*
april* left san* meeting
These topics appear to an unlikely subject identified by
the modeling. The words Houston, hero, general,
april and san (perhaps part of San Jacinto) all fit
together for a historian to suggest a sustained discussion
in the newspapers of the April 1836 Battle of San Jac-
into, when General Sam Houston defeated Santa Anna of
Mexico in the Texas Revolution. This is entirely unex-
pected, but the topics appear to fit together closely. That
this would rank so highly within all topics is, too, a sur-
prise. (Most historians, for example, have argued that few
Texans spent much time memorializing such events until
after 1901. This would be quite a discovery if they were
talking about it in such detail before 1901.)
man time great good men years life world long made
people make young water woman back found women
work
Not sure what the connections are here, although the top-
ics clearly all fit together in discussion of the lives of
women and men.
market* cotton* york* good* steady* closed* prices*
corn* texas* wheat* fair* stock* choice* year*
lower* receipts* ton* crop* higher*
All these topics reflect market-driven language related to
the buying and selling cotton and, to a much smaller ex-
tent, other crops such as corn.
tube tie alie time thaw art ton ion aid ant ore end hat
ire aad lour thee con til
Noise with no clear connections.
Table 6: 10 topic groups found for the 1865-1901 main set. Asterisks denote meaningful topic terms.
101
Period Topics Explanation
1865-1901 texas* city* worth* houston* good*
county* fort* state* man* time*
made* street* men* work* york today
company great people
These keywords appear to be related to three things:
(1) geography (reflected in both specific places like
Houston and Fort Worth and more general places
like county, street, and city), (2) discussions of
people (men and man) and (3) time (time and today).
1892 texas* worth* gazette* city* tex*
fort* county* state* good* march*
man* special* made* people* time*
york men days feb
As with the 1865-1901 set, these keywords also appear to
be related to three things: (1) geography, (2) discussions
of people and (3) time.
1893 worth* texas* tin* city* tube* clio*
time* alie* man* good* fort* work*
made street year men county state tex
As with the 1865-1901 set, these keywords also appear to
be related to three things: (1) geography, (2) discussions
of people and (3) time.
1929-1930 tin* texas* today* county* year*
school* good* time* home* city* oil*
man* men* made* work* phone night
week sunday
As with the 1865-1901 set, these keywords also appear to
be related to three things: (1) geography, (2) discussions
of people and (3) time. The time discussion here appears
to be heightened, and the appearance of economic issues
for Texas (oil) makes sense in the context of the onset
of the Great Depression in 1929-30.
Table 7: Main topics for years of interest for the main set
Period Topics Explanation
1865-1901 cotton* texas* good* crop* bales*
county* york* houston* spot mid-
dling* year* corn* market* worth*
oil* closed* special* ordinary* today
All market-oriented language that reflects all aspects of
the cotton market, in particular the evaluation of cotton
quality. The geography of New York (york) and Hous-
ton could reflect their importance in the cotton market or
(just as important) sources of news and information (with
Houston being a central producer of the newspapers in
our corpus).
1892 cotton* bales* spot gazette* special*
march middling* ordinary* steady*
closed* futures* lots* good* texas*
sales* feb low* ton* oil*
Market-oriented language that reflects, in particular, the
buying and selling of cotton on the open market. The
inclusion of February and March 1892, in the context of
these other words associated with the selling of cotton,
suggest those were important months in the marketing of
the crop for 1892.
1893 cotton* ordinary* texas* worth* belt
middling* closed* year bales* good*
route* crop* city* cents* spot oil*
corn* low* return*
Market-oriented language focused on the buying and sell-
ing of cotton.
1929-1930 cotton* texas* county crop* year
good* today* york* points* oil* mar-
ket* farm* made* seed* state* price*
tin bales* july*
Market-oriented language concerning cotton. What is
interesting here is the inclusion of words like state,
market, and price, which did not show up in the pre-
vious sets. The market-language here is more broadly as-
sociated with the macro-economic situation (with explicit
references to the market and price, which seems to
reflect the heightened concern at that time about the fu-
ture of the cotton market with the onset of the Great De-
pression and what role the state would play in that.
Table 8: Main topics for the cotton subset
102
Accuracy
Topic Groups Topics Terms
G
en
er
al
Ten for 1865-1901 60% 45.79% (74.56%)
One for 1865-1901 100% 73.68%
One for 1892 100% 78.95%
One for 1893 100% 63.16%
One for 1929-1930 100% 78.95%
Co
tto
n
One for 1865-1901 100% 89.47%
One for 1892 100% 84.21%
One for 1893 100% 84.21%
One for 1929-1930 100% 84.21%
Table 5: Accuracy of topic modeling: In parenthesis is
the term accuracy calculated using relevant topics only.
Texans against Mexican general Antonio Lopez de
Santa Anna. Over the course of an eighteen minute
battle, Houston?s forces routed Santa Anna?s army.
The victory at San Jacinto secured the independence
of Texas from Mexico and became a day of celebra-
tion in Texas during the years that followed.
Most historians have argued that Texas paid little
attention to remembering the Battle of San Jacinto
until the early twentieth century. These topic mod-
eling results, however, suggest that far more atten-
tion was paid to this battle in Texas newspapers than
scholars had previously thought.
We extracted all the examples from the corpus for
the years 1865-1901 that contain ten or more of the
top terms in the topic and also contain the word ?jac-
into?. Out of a total of 220 snippets that contain
?jacinto?, 125 were directly related to the battle and
its memory. 95 were related to other things. The ma-
jority of these snippets came from newspapers pub-
lished in Houston, which is located near San Jacinto,
with a surge of interest in the remembrance of the
battle around the Aprils of 1897-1899.
6 Conclusions
In this paper, we explored the use of topical models
applied on historical newspapers to assist historical
research. We have found that we can automatically
generate topics that are generally good, however we
found that once we generated a set of topics, we can-
not decide if it is mundane or interesting without an
expert and, for example, would have been oblivious
to the significance of the San Jacinto topic. We agree
with Block (2006) that ?topic simulation is only a
tool? and have come to the conclusion that it is es-
sential that an expert in the field contextualize these
topics and evaluate them for relevancy.
We also found that although our corpus contains
noise from OCR errors, it may not need expen-
sive error correction processing to provide good re-
sults when using topic models. We may explore
combining the named entity tagged data with a less
aggressive stemmer and, additionally, evaluate the
usefulness of not discarding the unstemmed words
but maintaining their association with their stemmed
counterpart.
Acknowledgment
We would like to thank Jon Christensen and
Cameron Blevins from Stanford, who have been
working with us on the larger project, ?Mapping
Historical Texts: Combining Text-Mining and Geo-
Visualization to Unlock the Research Potential of
Historical Newspapers?, which subsumes the work
we have presented in this paper. This work has
been partly supported by the NEH under Digital
Humanities Start-Up Grant (HD-51188-10). Any
views, findings, conclusions or recommendations
expressed in this publication do not necessarily rep-
resent those of the National Endowment for the Hu-
manities.
References
[Blei et al2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. 2003. Latent dirichlet alocation.
The Journal of Machine Learning Research, 3:993?
1022.
[Blevins2010] Cameron Blevins. 2010. Topic Modeling
Martha Ballard?s Diary.
[Block2006] Sharon Block. 2006. Doing More with Dig-
itization: An Introduction to Topic Modeling of Early
American Sources. Common-Place, 6(2), January.
[Boyd-Graber et al2009] Jonathan Boyd-Graber, Jordan
Chang, Sean Gerrish, Chong Wang, and David M.
Blei. 2009. Reading tea leaves: How humans interpret
topic models. In Proceedings of the 23rd Annual Con-
ference on Neural Information Processing Systems.
[Finkel et al2005] Jenny Rose Finkel, Trond Grenager,
and Christopher Manning. 2005. Incorporating non-
local information into information extraction systems
by gibbs sampling. In Proceedings of the 43nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2005), pages 363?370.
103
[Griffiths and Steyvers2004] Thomas L. Griffiths and
Mark Steyvers. 2004. Finding scientific topics. Pro-
ceedings of the National Academy of Sciences of the
United States of America, 101(Suppl 1):5228.
[Hall et al2008] David Hall, Daniel Jurafsky, and
Christopher Manning. 2008. Studying the History of
Ideas Using Topic Models. In Proceedings from the
EMNLP 2008: Conference on Empirical Methods in
Natural Language Processing, October.
[McCallum2002] Andrew Kachites McCallum. 2002.
MALLET: A Machine Learning for Language Toolkit.
[Nelson2010] Robert K. Nelson. 2010. Mining the Dis-
patch.
[Newman and Block2006] David J. Newman and Sharon
Block. 2006. Probabilistic topic decomposition of
an eighteenth-century American newspaper. Journal
of the American Society for Information Science and
Technology, 57(6):753?767.
[Walker et al2010] Daniel D. Walker, William B. Lund,
and Eric K. Ringger. 2010. Evaluating models of
latent document semantics in the presence of OCR
errors. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 240?250. Association for Computational Lin-
guistics.
104
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, page 45,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Invited talk presentation
Multilingual Natural Language Processing
Rada Mihalcea
University of North Texas
USA
rada@cs.unt.edu
Title
Multilingual Natural Language Processing
Abstract
With rapidly growing online resources, such as
Wikipedia, Twitter, or Facebook, there is an in-
creasing number of languages that have a Web
presence, and correspondingly there is a growing
need for effective solutions for multilingual natu-
ral language processing. In this talk, I will explore
the hypothesis that a multilingual representation
can enrich the feature space for natural language
processing tasks, and lead to significant improve-
ments over traditional solutions that rely exclu-
sively on a monolingual representation. Specif-
ically, I will describe experiments performed on
three different tasks: word sense disambiguation,
subjectivity analysis, and text semantic similarity,
and show how the use of a multilingual represen-
tation can leverage additional information from
the languages in the multilingual space, and thus
improve over the use of only one language at a
time. This is joint work with Samer Hassan and
Carmen Banea.
Bio
Rada Mihalcea is an Associate Professor in the
Department of Computer Science and Engineer-
ing at the University of North Texas. Her research
interests are in computational linguistics, with
a focus on lexical semantics, graph-based algo-
rithms for natural language processing, and mul-
tilingual natural language processing. She serves
or has served on the editorial boards of the Jour-
nals of Computational Linguistics, Language Re-
sources and Evaluations, Natural Language Engi-
neering, and Research in Language in Computa-
tion. She was a program co-chair for the Confer-
ence of the Association for Computational Lin-
guistics (2011), and the Conference on Empirical
Methods in Natural Language Processing (2009).
She is the recipient of a National Science Foun-
dation CAREER award (2008) and a Presidential
Early Career Award for Scientists and Engineers
(2009).
45
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, page 1,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Multimodal Sentiment Analysis
(Abstract of Invited Talk)
Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
P. O. Box 311366
Denton, TX 76203-6886, U.S.A.
rada@cs.unt.edu
Abstract
With more than 10,000 new videos posted
online every day on social websites such as
YouTube and Facebook, the internet is be-
coming an almost infinite source of informa-
tion. One important challenge for the com-
ing decade is to be able to harvest relevant
information from this constant flow of mul-
timodal data. In this talk, I will introduce
the task of multimodal sentiment analysis, and
present a method that integrates linguistic, au-
dio, and visual features for the purpose of
identifying sentiment in online videos. I will
first describe a novel dataset consisting of
videos collected from the social media web-
site YouTube, which were annotated for senti-
ment polarity. I will then show, through com-
parative experiments, that the joint use of vi-
sual, audio, and textual features greatly im-
proves over the use of only one modality at
a time. Finally, by running evaluations on
datasets in English and Spanish, I will show
that the method is portable and works equally
well when applied to different languages.
This is joint work with Veronica Perez-Rosas
and Louis-Philippe Morency.
1
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 251?259,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Using N-gram and Word Network Features for
Native Language Identification
Shibamouli Lahiri Rada Mihalcea
Computer Science and Engineering
University of North Texas
Denton, TX 76207, USA
shibamoulilahiri@my.unt.edu, rada@cs.unt.edu
Abstract
We report on the performance of two different
feature sets in the Native Language Identification
Shared Task (Tetreault et al, 2013). Our feature
sets were inspired by existing literature on native
language identification and word networks. Exper-
iments show that word networks have competitive
performance against the baseline feature set, which
is a promising result. We also present a discussion
of feature analysis based on information gain, and an
overview on the performance of different word net-
work features in the Native Language Identification
task.
1 Introduction
Native Language Identification (NLI) is a well-
established problem in NLP, where the goal is to
identify a writer?s native language (L1) from his/her
writing in a second language (L2), usually English.
NLI is generally framed as a multi-class classifi-
cation problem (Koppel et al, 2005; Brooke and
Hirst, 2011; Wong and Dras, 2011), where native
languages (L1) are considered class labels, and writ-
ing samples in L2 are used as training and test data.
The NLI problem has recently seen a big surge in
interest, sparked in part by three influential early pa-
pers on this problem (Tomokiyo and Jones, 2001;
van Halteren and Oostdijk, 2004; Koppel et al,
2005). Apart from shedding light on the way non-
native learners (also called ?L2 learners?) learn a
new language, the NLI task allows constrastive anal-
ysis (Wong and Dras, 2009), study of different types
of errors that people make while learning a new lan-
guage (Kochmar, 2011; Bestgen et al, 2012; Jarvis
et al, 2012), and identification of language trans-
fer patterns (Brooke and Hirst, 2012a; Jarvis and
Crossley, 2012), thereby helping L2-students im-
prove their writing styles and expediting the learn-
ing process. It also helps L2 educators to concen-
trate their efforts on particular areas of a language
that cause the most learning difficulty for different
L1s.
The NLI task is closely related to traditional NLP
problems of authorship attribution (Juola, 2006; Sta-
matatos, 2009; Koppel et al, 2009) and author pro-
filing (Kes?elj et al, 2003; Estival et al, 2007a; Esti-
val et al, 2007b; Bergsma et al, 2012), and shares
many of the same features. Like authorship attri-
bution, NLI is greatly benefitted by having function
words and character n-grams as features (Brooke
and Hirst, 2011; Brooke and Hirst, 2012b). Native
languages form a part of an author?s socio-cultural
and psychological profiles, thereby being related to
author profiling (van Halteren and Oostdijk, 2004;
Torney et al, 2012).
Researchers have used different types of features
for the NLI problem, including but not limited to
function words (Brooke and Hirst, 2012b); char-
acter, word and POS n-grams (Brooke and Hirst,
2012b); spelling and syntactic errors (Koppel et al,
2005); CFG productions (Brooke and Hirst, 2012b);
Tree Substitution Grammar productions (Swanson
and Charniak, 2012); dependencies (Brooke and
Hirst, 2012b); Adaptor Grammar features (Wong et
al., 2012); L1-influence (Brooke and Hirst, 2012a);
stylometric features (Golcher and Reznicek, 2011;
251
Crossley and McNamara, 2012; Jarvis et al, 2012);
recurrent n-grams on words and POS (Bykh and
Meurers, 2012); and features derived from topic
models (Wong et al, 2011). State-of-the-art re-
sults are typically in the 80%-90% range, with re-
sults above 90% reported in some cases (Brooke
and Hirst, 2012b). Note, however, that results vary
greatly across different datasets, depending on the
number of languages being considered, size and dif-
ficulty of data, etc.
2 Our Approach
The NLI 2013 Shared Task (Tetreault et al, 2013)
marks an effort in bringing together the NLI research
community to share and compare their results and
evaluations on a common dataset - TOEFL11 (Blan-
chard et al, 2013) - consisting of 12,100 unique En-
glish essays written by non-native learners of eleven
different languages.1 The dataset has 9,900 essays
for training, 1,100 essays for test, and 1,100 essays
for development. Each of the three sets is balanced
across different L1s.
Inspired by previous work in NLI, in our different
NLI systems submissions we used several different
types of character, word, and POS n-gram features
(cf. Section 2.1). Although not included in the sys-
tems submitted, we also experimented with a family
of new features derived from a word network repre-
sentation of natural language text (cf. Section 2.2).
We used Weka (Hall et al, 2009) for all our classifi-
cation experiments. The systems that were submit-
ted gave best 10-fold cross-validation accuracy on
training data among different feature-classifier com-
binations (Section 3). Word network features - al-
though competitive against the baseline n-gram fea-
tures - were not able to beat the baseline features
on the training set, so we did not submit that sys-
tem for evaluation. Section 2.1 discusses our n-gram
features, followed by a discussion of word network
features in Section 2.2.
2.1 N-gram Features
We used several baseline n-gram features based on
words, characters, and POS. We experimented with
the raw frequency, normalized frequency, and binary
1Arabic, Chinese, French, German, Hindi, Italian, Japanese,
Korean, Spanish, Telugu and Turkish.
presence/absence indicator on top 100, 200, 500 and
1000 n-grams:2
1. word n-grams (n = 1, 2, 3), with and without
punctuation.
2. character n-grams (n = 1, 2, 3), with and with-
out space characters.
3. POS n-grams (n = 1, 2, 3), with and without
punctuation.3
We experimented with punctuation because pre-
vious research indicates that punctuation is help-
ful (Wong and Dras, 2009; Kochmar, 2011). In total,
there are 216 types of n-gram feature vectors (with
dimensions 100, 200, 500 and 1000) for a particular
document. Because of size restrictions (e.g., some n-
gram dictionaries are smaller than the specified fea-
ture vector dimensions), we ended up with 168 types
of feature vectors per document (cf. Tables 2 to 4).
2.2 Word Networks
A ?word network? of a particular document is a net-
work (graph) of unique words found in that docu-
ment. Each node (vertex) in this network is a word.
Edges between two nodes (unique words) can be
constructed in several different ways. The simplest
type of edge connects word A to word B, if word
A is followed by word B in the document at least
once. In our work, we have assumed a directed edge
with direction from word A to word B. Note that we
could have used undirected edges as well (cf. (Mi-
halcea and Tarau, 2004)). Moreover, edges can
be weighted/unweighted. We assumed unweighted
edges.
A deeper issue with this network construction
process concerns what we should do with stopwords.
Should we keep them, or should we remove them?
Since stopwords and function words have proved to
be of special importance in previous native language
identification studies (Wong and Dras, 2009; Brooke
and Hirst, 2012b), we chose to keep them in our
word networks.
Two other choices we made in the construction
of our word networks concern sentence boundaries
2Note that these most frequent n-grams were extracted from
the training+development set.
3We used CRFTagger (Phan, 2006) for POS tagging.
252
Figure 1: Word network of the sentence ?the quick brown
fox jumped over the lazy dog?.
and word co-occurrence. Word networks can be
constructed either by respecting sentence boundaries
(where the last word of sentence 1 does not link
to the first word of sentence 2), or by disregard-
ing them. In our case, we disregarded all sentence
boundaries. Moreover, a network edge can either
link two words that appeared side-by-side in the
original document, or it can link two words that ap-
peared within a window of n words in the document
(cf. (Mihalcea and Tarau, 2004)). In our case, we
chose the first option - linking unique words that ap-
peared side-by-side at least once. Finally, we did
not perform any stemming/morphological analysis
to retain subtle cues that might be revealed from in-
flected/derived words.
The word network of an example sentence (?the
quick brown fox jumped over the lazy dog?) is
shown in Figure 1. Note that the word ?the? ap-
peared twice in this sentence, so the correspond-
ing network contains a cycle that starts at ?the?
and ends at ?the?. In a realistic word network of
a large document, there can be many such cycles.
In addition, it is observed that such word networks
show power-law degree distribution and a small-
world structure (i Cancho and Sole?, 2001; Matsuo
et al, 2001).
Once the word networks have been constructed,
we extract a set of simple features from these net-
works4 that represent local properties of individual
nodes. We have extracted ten local features for each
node in a word network:
1. in-degree, out-degree and degree
2. in-coreness, out-coreness and coreness5
3. in-neighborhood size (order 1), out-
neighborhood size (order 1) and neighborhood
size (order 1)
4. local clustering coefficient
We take a set of representative words, and convert
a document into a local feature vector - each local
feature pertaining to one word in the set of repre-
sentative words. For example, when we use the top
200 most frequent words as the representative set,
a document can be represented as the degree vec-
tor of these 200 words in the document?s word net-
work, or as the local clustering coefficient vector of
these words in the word network, or as the coreness
vector of the words (and so on). A document can
also be represented as a concatenation (mixture) of
these vectors. For example, it can be represented
as concat(degree vector, coreness vector) of top
200 most frequent words. We are yet to explore
how such mixed feature sets perform in the NLI
task, and this constitutes a part of our future work
(Section 4). We experimented with top k most fre-
quent words (with k = 100, 200, 500, 1000) on train-
ing+development data as our representative word-
set.
3 Results
Table 1 describes the three systems we submitted.
The first two systems (UNT-closed-1.csv and UNT-
closed-2.csv) were based on a bag of words model
using all the words from the training set. The
systems used a home-grown implementation of the
Na??ve Bayes classifier, and achieved 10-fold cross-
validation accuracy of 64.5% and 65.1% respec-
tively, on the training set. The first system used raw
4We used the igraph (Csardi and Nepusz, 2006) software
package for graph feature extraction.
5Coreness is an index given to a particular vertex based
on its position in the k-core decomposition of the word net-
work (Batagelj and Zaversnik, 2003).
253
Submitted System
10-fold CV Accuracy on Accuracy on
Description
Training Set (%) Test Set (%)
UNT-closed-1.csv 64.50 63.20
Raw frequency of all words in the training set
including stopwords. Na??ve Bayes classifier.
UNT-closed-2.csv 65.10 63.70
Raw frequency of all words in the training set
except stopwords. Na??ve Bayes classifier.
UNT-closed-3.csv 62.46 64.50
Raw frequency of 1000 most frequent words
in the training+development set including punctuation.
SVM (SMO) classifier.
Table 1: Performance summary and description of the systems we submitted.
term frequency of all words including stopwords as
features, and the second system used raw term fre-
quency of all words except stopwords. These two
systems achieved test set accuracy of 63.2% and
63.7%, respectively.
The third system we submitted (UNT-closed-
3.csv) was based on n-gram features (cf. Sec-
tion 2.1). We used the raw frequency of top 1000
word unigrams, including punctuation, as features.
The Weka SMO implementation of SVM (Hall et
al., 2009) was used as classifier with default param-
eter settings. This system gave us the best 10-fold
cross-validation accuracy of 62.46% in the training
set, among all n-gram features. Note that this system
was also the top performer among the systems we
submitted in NLI evaluation, with a test set accuracy
of 64.5%, and a 10-fold CV accuracy of 63.77% on
the training+development set folds specified by the
organizers.
We will now describe in the following two sub-
sections how our n-gram features and word network
features performed on the training set. All results re-
ported here reflect best 10-fold cross-validation ac-
curacy in the training set among different classifiers
(SVM, Na??ve Bayes, 1-nearest-neighbor (1NN), J48
decision tree, and AdaBoost). SVM and Na??ve
Bayes gave best results in our experiments, so only
these two are shown in Tables 2 to 5.
3.1 Performance of N-gram Features
Recall from Section 2.1 that we extracted 168 differ-
ent n-gram feature vectors corresponding to the raw
frequency, normalized frequency, and binary pres-
ence/absence indicator of top k n-grams (with k =
100, 200, 500, 1000) in the training+development
set. Performance of these n-gram features is given
in Tables 2 to 4. A general observation with Tables 2
to 4 is that cross-validation performance improves as
k increases, although there are a few exceptions. We
marked those exceptions with an asterisk (?*?).
It is interesting to note that top k word unigrams
with punctuation were the top performers in most
of the cases. Also interesting is the fact that SVM
mostly gave best performance on n-gram features
among different classifiers. Note that Na??ve Bayes
was best performer in a few cases (Table 4). Per-
formance of raw and normalized frequency features
were mostly comparable (Tables 2 and 3), whereas
binary presence/absence indicator achieved worse
accuracy values in general than raw and normalized
frequency features (Table 4).
Among different n-grams, word unigrams per-
formed better than bigrams and trigrams, POS bi-
grams performed better than POS trigrams, and
character bigrams and character trigrams performed
comparably well (Tables 2 and 3). Exceptions to
this observation are seen in Table 4, where character
trigrams performed better than character bigrams,
and word bigrams sometimes performed better than
word unigrams. In general, word n-grams performed
the best, followed by POS and character n-grams.
3.2 Performance of Word Network Features
Word networks and word network features were de-
scribed in Section 2.2. We extracted ten local fea-
tures on four different representative sets of words
- the top k most frequent words (k = 100, 200, 500,
1000) on the training+development set, respectively.
Performance of these features is given in Table 5.
Note that in general, word network features per-
254
N-gram Feature
Best Cross-validation Accuracy (%) on Top k Most Frequent N-grams
k = 100 k = 200 k = 500 k = 1000
Word unigram
w/ punctuation 45.07 (SVM) 52.85 (SVM) 60.14 (SVM) 62.46 (SVM)
w/o punctuation 41.63 (SVM) 50.15 (SVM) 58.33 (SVM) 60.85 (SVM)
Word bigram
w/ punctuation 39.54 (SVM) 44.75 (SVM) 51.70 (SVM) 56.06 (SVM)
w/o punctuation 33.40 (SVM) 39.34 (SVM) 47.54 (SVM) 51.86 (SVM)
Word trigram
w/ punctuation 30.62 (SVM) 35.26 (SVM) 41.56 (SVM) 44.97 (SVM)
w/o punctuation 26.67 (SVM) 30.14 (SVM) 36.68 (SVM) 41.22 (SVM)
POS unigram
w/ punctuation N/A N/A N/A N/A
w/o punctuation N/A N/A N/A N/A
POS bigram
w/ punctuation 41.79 (SVM) 45.87 (SVM) 48.11 (SVM) 47.49 (SVM)*
w/o punctuation 35.95 (SVM) 39.23 (SVM) 41.23 (SVM) 39.58 (SVM)*
POS trigram
w/ punctuation 34.97 (SVM) 38.78 (SVM) 43.17 (SVM) 44.52 (SVM)
w/o punctuation 29.73 (SVM) 34.31 (SVM) 37.58 (SVM) 38.40 (SVM)
Character unigram
w/ space N/A N/A N/A N/A
w/o space N/A N/A N/A N/A
Character bigram
w/ space 42.48 (SVM) 48.43 (SVM) 55.87 (SVM) 56.12 (SVM)
w/o space 36.84 (SVM) 45.93 (SVM) 51.11 (SVM) 53.41 (SVM)
Character trigram
w/ space 41.65 (SVM) 48.68 (SVM) 54.54 (SVM) 57.77 (SVM)
w/o space 36.64 (SVM) 43.44 (SVM) 51.46 (SVM) 55.52 (SVM)
Table 2: Performance of raw frequency of n-gram features. Stratified ten-fold cross-validation accuracy values on
TOEFL11 training set are shown, along with the classifiers that achieved these accuracy values. Best results in different
columns are boldfaced. Table cells marked ?N/A? are the ones that correspond to an n-gram dictionary size < k.
N-gram Feature
Best Cross-validation Accuracy (%) on Top k Most Frequent N-grams
k = 100 k = 200 k = 500 k = 1000
Word unigram
w/ punctuation 44.65 (SVM) 52.21 (SVM) 59.81 (SVM) 62.35 (SVM)
w/o punctuation 41.15 (SVM) 50.41 (SVM) 58.18 (SVM) 60.61 (SVM)
Word bigram
w/ punctuation 39.63 (SVM) 44.69 (SVM) 52.31 (SVM) 56.08 (SVM)
w/o punctuation 33.44 (SVM) 39.11 (SVM) 47.61 (SVM) 52.56 (SVM)
Word trigram
w/ punctuation 30.42 (SVM) 34.97 (SVM) 41.89 (SVM) 45.68 (SVM)
w/o punctuation 26.08 (SVM) 30.03 (SVM) 37.16 (SVM) 42.39 (SVM)
POS unigram
w/ punctuation N/A N/A N/A N/A
w/o punctuation N/A N/A N/A N/A
POS bigram
w/ punctuation 41.08 (SVM) 45.04 (SVM) 48.23 (SVM) 47.78 (SVM)*
w/o punctuation 34.85 (SVM) 38.95 (SVM) 41.16 (SVM) 40.84 (SVM)*
POS trigram
w/ punctuation 34.74 (SVM) 38.38 (SVM) 42.89 (SVM) 44.86 (SVM)
w/o punctuation 28.74 (SVM) 33.67 (SVM) 36.93 (SVM) 38.64 (SVM)
Character unigram
w/ space N/A N/A N/A N/A
w/o space N/A N/A N/A N/A
Character bigram
w/ space 41.93 (SVM) 47.79 (SVM) 56.31 (SVM) 56.22 (SVM)*
w/o space 36.21 (SVM) 45.18 (SVM) 51.58 (SVM) 53.63 (SVM)
Character trigram
w/ space 40.70 (SVM) 47.90 (SVM) 54.40 (SVM) 57.36 (SVM)
w/o space 35.84 (SVM) 42.79 (SVM) 50.94 (SVM) 55.71 (SVM)
Table 3: Performance of normalized frequency of n-gram features. Stratified ten-fold cross-validation accuracy values
on TOEFL11 training set are shown, along with the classifiers that achieved these accuracy values. Best results in
different columns are boldfaced. Table cells marked ?N/A? are the ones that correspond to an n-gram dictionary size
< k.
255
N-gram Feature
Best Cross-validation Accuracy (%) on Top k Most Frequent N-grams
k = 100 k = 200 k = 500 k = 1000
Word unigram
w/ punctuation 33.42 (SVM) 42.49 (SVM) 50.63 (Na??ve Bayes) 56.95 (SVM)
w/o punctuation 33.05 (SVM) 42.82 (SVM) 50.13 (SVM) 55.91 (SVM)
Word bigram
w/ punctuation 37.74 (SVM) 40.99 (SVM) 46.16 (SVM) 52.66 (SVM)
w/o punctuation 32.02 (SVM) 37.24 (SVM) 42.29 (SVM) 48.36 (SVM)
Word trigram
w/ punctuation 29.87 (SVM) 33.79 (SVM) 38.48 (SVM) 42.00 (SVM)
w/o punctuation 25.75 (SVM) 28.79 (SVM) 34.14 (SVM) 37.80 (SVM)
POS unigram
w/ punctuation N/A N/A N/A N/A
w/o punctuation N/A N/A N/A N/A
POS bigram
w/ punctuation 29.75 (SVM) 35.50 (SVM) 40.39 (Na??ve Bayes) 41.11 (Na??ve Bayes)
w/o punctuation 25.47 (SVM) 31.41 (SVM) 33.33 (Na??ve Bayes) 33.78 (Na??ve Bayes)
POS trigram
w/ punctuation 29.20 (SVM) 33.28 (SVM) 38.98 (Na??ve Bayes) 43.74 (Na??ve Bayes)
w/o punctuation 23.71 (SVM) 28.98 (SVM) 32.21 (SVM) 37.49 (Na??ve Bayes)
Character unigram
w/ space N/A N/A N/A N/A
w/o space N/A N/A N/A N/A
Character bigram
w/ space 15.26 (SVM) 23.69 (SVM) 40.07 (SVM) 41.76 (SVM)
w/o space 15.73 (SVM) 25.27 (SVM) 37.05 (SVM) 41.52 (SVM)
Character trigram
w/ space 20.42 (SVM) 28.17 (SVM) 37.61 (SVM) 47.93 (SVM)
w/o space 23.85 (SVM) 30.38 (SVM) 37.39 (SVM) 45.60 (SVM)
Table 4: Performance of binary presence/absence indicator on n-gram features. Stratified ten-fold cross-validation
accuracy values on TOEFL11 training set are shown, along with the classifiers that achieved these accuracy values.
Best results in different columns are boldfaced. Table cells marked ?N/A? are the ones that correspond to an n-gram
dictionary size < k.
Word Network Feature
Best Cross-validation Accuracy (%) on Top k Most Frequent Words
k = 100 k = 200 k = 500 k = 1000
Clustering Coefficient 15.31 (SVM) 17.73 (SVM) 19.96 (SVM) 20.71 (SVM)
In-degree 39.89 (SVM) 49.28 (SVM) 56.83 (SVM) 59.47 (SVM)
Out-degree 40.66 (SVM) 49.67 (SVM) 57.16 (SVM) 59.62 (SVM)
Degree 41.05 (SVM) 50.74 (SVM) 58.17 (SVM) 60.21 (SVM)
In-coreness 32.52 (SVM) 42.44 (SVM) 51.09 (SVM) 55.50 (SVM)
Out-coreness 32.41 (SVM) 43.15 (SVM) 51.34 (SVM) 55.39 (SVM)
Coreness 35.32 (SVM) 45.84 (SVM) 53.54 (SVM) 57.18 (SVM)
In-neighborhood Size
40.54 (SVM) 50.08 (SVM) 56.92 (SVM) 59.69 (SVM)
(order 1)
Out-neighborhood Size
41.09 (SVM) 50.09 (SVM) 57.71 (SVM) 59.73 (SVM)
(order 1)
Neighborhood Size
41.83 (SVM) 50.68 (SVM) 57.40 (SVM) 60.41 (SVM)
(order 1)
Table 5: Performance of word network features. Stratified ten-fold cross-validation accuracy values on TOEFL11
training set are shown, along with the classifiers that achieved these accuracy values. Best results in different columns
are boldfaced.
256
Rank Word Network Feature Information Gain
1 Degree of the word a 0.1058
2 Neighborhood size of the word a 0.1054
3 Out-neighborhood size of the word a 0.1050
4 Outdegree of the word a 0.1049
5 In-neighborhood size of the word a 0.1017
6 Indegree of the word a 0.1016
7 Neighborhood size of the word however 0.0928
8 Degree of the word however 0.0928
9 Indegree of the word however 0.0928
10 In-neighborhood size of the word however 0.0928
11 Outdegree of the word however 0.0916
12 Out-neighborhood size of the word however 0.0916
13 Out-coreness of the word however 0.0851
14 Coreness of the word however 0.0851
15 In-coreness of the word however 0.0850
16 Outdegree of the word the 0.0793
17 Out-neighborhood size of the word the 0.0790
18 Degree of the word the 0.0740
19 Neighborhood size of the word the 0.0740
20 Coreness of the word a 0.0710
Table 6: Ranking of word network features based on Information Gain, on TOEFL11 training set. We took 1000 most
frequent words on the training+development set, and collected all their word network features in a single file. This
ranking reflects the top 20 features in that file, along with their information gain values.
formed quite well, with the best result (60.41% CV
accuracy on the train set) being competitive against
(but slightly worse than) the baseline n-gram fea-
tures (62.46% CV accuracy on the train set). Perfor-
mance improved with increasing k, thereby corrob-
orating our general observation from Tables 2 to 4.
Clustering coefficient performed poorly, and seems
rather unsuitable for the NLI task. But degree, core-
ness, and neighborhood size performed good. Here
also, SVM turned out to be the best classifier, giving
best CV accuracy in all cases.
We experimented with the in-, out-, and over-
all versions of degree, coreness and neighborhood
size. Their performance was mostly comparable
with each other (Table 5). To investigate which word
network features are the most discriminatory in this
task, we collected all ten word network features of
the top 1000 words in a single file, and then ranked
those features on the training set based on Infor-
mation Gain (IG). The 20 top-ranking features are
shown in Table 6, along with their corresponding
IG values. Note that the words a, the, and however
were among the most discriminatory, and different
versions of degree, neighborhood size and coreness
appeared among the top, which is in line with our
earlier observation that clustering coefficients were
not very discriminatory at the native language clas-
sification task.
4 Conclusions and Future Work
In this paper, we described experiments with the NLI
task using a baseline set of n-gram features, and a
set of novel features derived from a word network
representation of text documents. Useful and less
useful n-gram features were identified, along with
the fact that SVM was the best classifier in most
of the cases. We learned that when using raw or
normalized frequency, lower-order n-grams perform
at least as good as higher-order n-grams; moreover,
Na??ve Bayes sometimes give good results when bi-
nary presence/absence indicator variables are used
as features.
We described the construction of our word net-
works in detail, and discussed experiments with
word network features. These features are compet-
itive against the baseline n-gram features, and we
need to fine-tune our classifiers to see if they can
exceed the performance of the baseline. Cluster-
ing coefficients were found to be less useful for the
NLI task, and feature ranking based on information
257
gain helped us identify the most important word net-
work features in a collection of top 1000 words in
the training+development set.
Future work consists of experimenting with com-
bined word network features; mixed word network
features and baseline n-gram features; and the one-
vs-all classification scheme instead of the multiclass
classification scheme.
References
Vladimir Batagelj and Matjaz Zaversnik. 2003. An
O(m) Algorithm for Cores Decomposition of Net-
works. CoRR, cs.DS/0310049.
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric Analysis of Scientific Articles. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 327?
337, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Yves Bestgen, Sylviane Granger, and Jennifer Thewis-
sen. 2012. Error Patterns and Automatic L1 Identifi-
cation. In Scott Jarvis and Scott A. Crossley, editors,
Approaching Language Transfer through Text Classi-
fication, pages 127?153. Multilingual Matters.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012a. Measuring
Interlanguage: Native Language Identification with
L1-influence Metrics. In Nicoletta Calzolari, Khalid
Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012), pages 779?784, Istanbul,
Turkey, May. European Language Resources Associ-
ation (ELRA). ACL Anthology Identifier: L12-1016.
Julian Brooke and Graeme Hirst. 2012b. Robust, Lex-
icalized Native Language Identification. In Proceed-
ings of COLING 2012, pages 391?408, Mumbai, In-
dia, December. The COLING 2012 Organizing Com-
mittee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence. In
Proceedings of COLING 2012, pages 425?440, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Scott A. Crossley and Danielle McNamara. 2012. De-
tecting the First Language of Second Language Writ-
ers Using Automated Indices of Cohesion, Lexical
Sophistication, Syntactic Complexity and Conceptual
Knowledge. In Scott Jarvis and Scott A. Crossley,
editors, Approaching Language Transfer through Text
Classification, pages 106?126. Multilingual Matters.
Gabor Csardi and Tamas Nepusz. 2006. The igraph soft-
ware package for complex network research. Inter-
Journal, Complex Systems:1695.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007a. Author pro-
filing for English emails. In Proceedings of the 10th
Conference of the Pacific Association for Computa-
tional Linguistics, pages 263?272, Melbourne, Aus-
tralia.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007b. TAT: An Au-
thor Profiling Tool with Application to Arabic Emails.
In Proceedings of the Australasian Language Technol-
ogy Workshop 2007, pages 21?30, Melbourne, Aus-
tralia, December.
Felix Golcher and Marc Reznicek. 2011. Stylometry and
the interplay of topic and L1 in the different annotation
layers in the FALKO corpus. QITL-4?Proceedings of
Quantitative Investigations in Theoretical Linguistics,
4:29?34.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Ramon Ferrer i Cancho and Ricard V. Sole?. 2001. The
Small World of Human Language. Proceedings: Bio-
logical Sciences, 268(1482):pp. 2261?2265.
Scott Jarvis and Scott A. Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classifica-
tion: Explorations in the Detection-based Approach,
volume 64. Multilingual Matters Limited, Bristol,
UK.
Scott Jarvis, Yves Bestgen, Scott A. Crossley, Syl-
viane Granger, Magali Paquot, Jennifer Thewissen,
and Danielle McNamara. 2012. The Comparative
and Combined Contributions of n-Grams, Coh-Metrix
Indices and Error Types in the L1 Classification of
Learner Texts. In Scott Jarvis and Scott A. Crossley,
editors, Approaching Language Transfer through Text
Classification, pages 154?177. Multilingual Matters.
Patrick Juola. 2006. Authorship Attribution. Found.
Trends Inf. Retr., 1(3):233?334, December.
258
Vlado Kes?elj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for au-
thorship attribution. In Proceedings of the Conference
Pacific Association for Computational Linguistics, PA-
CLING, volume 3, pages 255?264.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9?26, Jan-
uary.
Yutaka Matsuo, Yukio Ohsawa, and Mitsuru Ishizuka.
2001. A Document as a Small World. In Proceedings
of the Joint JSAI 2001 Workshop on New Frontiers in
Artificial Intelligence, pages 444?448, London, UK,
UK. Springer-Verlag.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 404?411,
Barcelona, Spain, July. Association for Computational
Linguistics.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger.
Efstathios Stamatatos. 2009. A survey of modern author-
ship attribution methods. J. Am. Soc. Inf. Sci. Technol.,
60(3):538?556, March.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from?round here, are you?: naive Bayes de-
tection of non-native utterance text. In Proceedings of
the second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies, pages 1?8, Pittsburgh, PA. Asso-
ciation for Computational Linguistics.
Rosemary Torney, Peter Vamplew, and John Yearwood.
2012. Using psycholinguistic features for profiling
first language of authors. Journal of the Ameri-
can Society for Information Science and Technology,
63(6):1256?1269.
Hans van Halteren and Nelleke Oostdijk. 2004. Linguis-
tic profiling of texts for the purpose of language ver-
ification. In Proceedings of Coling 2004, pages 966?
972, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
259
