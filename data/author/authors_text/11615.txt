Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105?1113,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Lattice-based System Combination for Statistical Machine Translation
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn
Abstract
Current system combination methods usu-
ally use confusion networks to find consensus
translations among different systems. Requir-
ing one-to-one mappings between the words
in candidate translations, confusion networks
have difficulty in handling more general situa-
tions in which several words are connected to
another several words. Instead, we propose a
lattice-based system combination model that
allows for such phrase alignments and uses
lattices to encode all candidate translations.
Experiments show that our approach achieves
significant improvements over the state-of-
the-art baseline system on Chinese-to-English
translation test sets.
1 Introduction
System combination aims to find consensus transla-
tions among different machine translation systems.
It has been proven that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
In recent several years, the system combination
methods based on confusion networks developed
rapidly (Bangalore et al, 2001; Matusov et al, 2006;
Sim et al, 2007; Rosti et al, 2007a; Rosti et al,
2007b; Rosti et al, 2008; He et al, 2008), which
show state-of-the-art performance in benchmarks. A
confusion network consists of a sequence of sets of
candidate words. Each candidate word is associated
with a score. The optimal consensus translation can
be obtained by selecting one word from each set to
maximizing the overall score.
To construct a confusion network, one first need
to choose one of the hypotheses (i.e., candidate
translations) as the backbone (also called ?skeleton?
in the literature) and then decide the word align-
ments of other hypotheses to the backbone. Hy-
pothesis alignment plays a crucial role in confusion-
network-based system combination because it has a
direct effect on selecting consensus translations.
However, a confusion network is restricted in
such a way that only 1-to-1 mappings are allowed
in hypothesis alignment. This is not the fact even
for word alignments between the same languages. It
is more common that several words are connected
to another several words. For example, ?be capa-
ble of? and ?be able to? have the same meaning.
Although confusion-network-based approaches re-
sort to inserting null words to alleviate this problem,
they face the risk of producing degenerate transla-
tions such as ?be capable to? and ?be able of?.
In this paper, we propose a new system combina-
tion method based on lattices. As a more general
form of confusion network, a lattice is capable of
describing arbitrary mappings in hypothesis align-
ment. In a lattice, each edge is associated with a
sequence of words rather than a single word. There-
fore, we select phrases instead of words in each
candidate set and minimize the chance to produce
unexpected translations such as ?be capable to?.
We compared our approach with the state-of-the-art
confusion-network-based system (He et al, 2008)
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to-
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.
1105
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(a) unidirectional alignments
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(b) bidirectional alignments
He feels like ? apples
? prefer of
is fond
(c) confusion network
he feels like apples
? prefer
is fond of
(d) lattice
Figure 1: Comparison of a confusion network and a lat-
tice.
2 Background
2.1 Confusion Network and Lattice
We use an example shown in Figure 1 to illustrate
our idea. Suppose that there are three hypotheses:
He feels like apples
He prefer apples
He is fond of apples
We choose the first sentence as the backbone.
Then, we perform hypothesis alignment to build a
confusion network, as shown in Figure 1(a). Note
that although ?feels like? has the same meaning with
?is fond of?, a confusion network only allows for
one-to-one mappings. In the confusion network
shown in Figure 1(c), several null words ? are in-
serted to ensure that each hypothesis has the same
length. As each edge in the confusion network only
has a single word, it is possible to produce inappro-
priate translations such as ?He is like of apples?.
In contrast, we allow many-to-many mappings
in the hypothesis alignment shown in Figure 2(b).
For example, ?like? is aligned to three words: ?is?,
?fond?, and ?of?. Then, we use a lattice shown in
Figure 1(d) to represent all possible candidate trans-
lations. Note that the phrase ?is fond of? is attached
to an edge. Now, it is unlikely to obtain a translation
like ?He is like of apples?.
A lattice G = ?V,E? is a directed acyclic graph,
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
The nodes in a lattice are usually labeled according
to an appropriate numbering to reflect how to pro-
duce a translation. Each edge in a lattice is attached
with a sequence of words as well as the associated
probability.
As lattice is a more general form of confusion
network (Dyer et al, 2008), we expect that replac-
ing confusion networks with lattices will further im-
prove system combination.
2.2 IHMM-based Alignment Method
Since the candidate hypotheses are aligned us-
ing Indirect-HMM-based (IHMM-based) alignment
method (He et al, 2008) in both direction, we briefly
review the IHMM-based alignment method first.
Take the direction that the hypothesis is aligned to
the backbone as an example. The conditional prob-
ability that the hypothesis is generated by the back-
bone is given by
p(e
?
1
J
|e
I
1
) =
?
a
J
1
J
?
j=1
[p(a
j
|a
j?1
, I)p(e
?
j
|e
a
j
)]l (1)
Where eI
1
= (e
1
, ..., e
I
) is the backbone, e?J
1
=
(e
?
1
, ..., e
?
J
) is a hypothesis aligned to eI
1
, and aJ
1
=
(a
1
, .., a
J
) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is
aligned to.
The translation probability p(e?
j
|e
i
) is a linear in-
terpolation of semantic similarity p
sem
(e
?
j
|e
i
) and
surface similarity p
sur
(e
?
j
|e
i
) and ? is the interpo-
lation factor:
p(e
?
j
|e
i
) = ??p
sem
(e
?
j
|e
i
)+(1??)?p
sur
(e
?
j
|e
i
) (2)
The semantic similarity model is derived by using
the source word sequence as a hidden layer, so the
bilingual dictionary is necessary. The semantic sim-
1106
ilarity model is given by
p
sem
(e
?
j
|e
i
) =
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
, e
i
)
?
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
)
(3)
The surface similarity model is estimated by calcu-
lating the literal matching rate:
p
sur
(e
?
j
|e
i
) = exp{? ? [s(e
?
j
, e
i
)? 1]} (4)
where s(e?
j
, e
i
) is given by
s(e
?
j
, e
i
) =
M(e
?
j
, e
i
)
max(|e
?
j
|, |e
i
|)
(5)
where M(e?
j
, e
i
) is the length of the longest matched
prefix (LMP) and ? is a smoothing factor that speci-
fies the mapping.
The distortion probability p(a
j
= i|a
j?1
= i
?
, I)
is estimated by only considering the jump distance:
p(i|i
?
, I) =
c(i? i
?
)
?
I
i=1
c(l ? i
?
)
(6)
The distortion parameters c(d) are grouped into 11
buckets, c(? ?4), c(?3), ..., c(0), ..., c(5), c(? 6).
Since the alignments are in the same language, the
distortion model favor monotonic alignments and
penalize non-monotonic alignments. It is given in
a intuitive way
c(d) = (1 + |d? 1|)
?K
, d = ?4, ..., 6 (7)
where K is tuned on held-out data.
Also the probability p
0
of jumping to a null word
state is tuned on held-out data. So the overall distor-
tion model becomes
p(i|i
?
, I) =
{
p
0
if i = null state
(1? p
0
) ? p(i|i
?
, I) otherwise
3 Lattice-based System Combination
Model
Lattice-based system combination involves the fol-
lowing steps:
(1) Collect the hypotheses from the candidate sys-
tems.
(2) Choose the backbone from the hypotheses.
This is performed using a sentence-level Minimum
Bayes Risk (MBR) method. The hypothesis with the
minimum cost of edits against all hypotheses is se-
lected. The backbone is significant for it influences
not only the word order, but also the following align-
ments. The backbone is selected as follows:
E
B
= argmin
E
?
?E
?
E?E
TER(E
?
, E) (8)
(3) Get the alignments of the backbone and hy-
pothesis pairs. First, each pair is aligned in both di-
rections using the IHMM-based alignment method.
In the IHMM alignment model, bilingual dictionar-
ies in both directions are indispensable. Then, we
apply a grow-diag-final algorithm which is widely
used in bilingual phrase extraction (Koehn et al,
2003) to monolingual alignments. The bidirec-
tional alignments are combined to one resorting to
the grow-diag-final algorithm, allowing n-to-n map-
pings.
(4)Normalize the alignment pairs. The word or-
der of the backbone determines the word order of
consensus outputs, so the word order of hypotheses
must be consistent with that of the backbone. All
words of a hypotheses are reordered according to
the alignment to the backbone. For a word aligned
to null, an actual null word may be inserted to the
proper position. The alignment units are extracted
first and then the hypothesis words in each unit are
shifted as a whole.
(5) Construct the lattice in the light of phrase
pairs extracted on the normalized alignment pairs.
The expression ability of the lattice depends on the
phrase pairs.
(6) Decode the lattice using a model similar to the
log-linear model.
The confusion-network-based system combina-
tion model goes in a similar way. The first two steps
are the same as the lattice-based model. The differ-
ence is that the hypothesis pairs are aligned just in
one direction due to the expression limit of the con-
fusion network. As a result, the normalized align-
ments only contain 1-to-1 mappings (Actual null
words are also needed in the case of null alignment).
In the following, we will give more details about the
steps which are different in the two models.
1107
4 Lattice Construction
Unlike a confusion network that operates words
only, a lattice allows for phrase pairs. So phrase
pairs must be extracted before constructing a lat-
tice. A major difficulty in extracting phrase pairs
is that the word order of hypotheses is not consistent
with that of the backbone. As a result, hypothesis
words belonging to a phrase pair may be discon-
tinuous. Before phrase pairs are extracted, the hy-
pothesis words should be normalized to make sure
the words in a phrase pair is continuous. We call a
phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e?
1
e
?
3
, e
2
) should be
a phrase pair, but /e?
1
0 and /e?
3
0 are discontin-
uous, so the phrase pair can not be extracted. Only
after the words of the hypothesis are reordered ac-
cording to the corresponding words in the backbone
as shown in Figure 2 (b), /e?
1
0 and /e?
3
0 be-
come continuous and the phrase pair (e?
1
e
?
3
, e
2
) can
be extracted. The procedure of reordering is called
alignment normalization
E
h
: e?
1
e
?
2
e
?
3
E
B
:
e
1
e
2
e
3
(a)
E
h
: e?
2
e
?
1
e
?
3
E
B
:
e
1
e
2
e
3
(b)
Figure 2: An example of alignment units
4.1 Alignment Normalization
After the final alignments are generated in the grow-
diag-final algorithm, minimum alignment units are
extracted. The hypothesis words of an alignment
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All mini-
mum alignment units are as follows: (e?
2
, e
1
), (e?
1
e
?
3
,
e
2
) and (?, e
3
). (e?
1
e
?
2
e
?
3
, e
1
e
2
) is an alignment unit,
but not a minimum alignment unit.
Let a?
i
= (e?
?
i
, e?
i
) denote a minimum alignment
unit, and assume that the word string e??
i
covers words
e
?
i
1
,..., e
?
i
m
on the hypothesis side, and the word
string e?
i
covers the consecutive words e
i
1
,..., e
i
n
on
the backbone side. In an alignment unit, the word
string on the hypothesis side can be discontinuous.
The minimum unit a?
i
= (e?
?
i
, e?
i
) must observe the
following rules:
E
B
: e
1
e
2
e
3
E
h
:
e
?
1
e
?
2 (a)
e
1
e
2
e
3
e
?
2
?
e
?
1
E
B
: e
1
e
2
E
h
: e
?
1
e
?
2
e
?
3
e
1
e
2
e
?
1
e
?
3
e
?
1
e
?
2
e
?
3
(b)
E
B
: e
1
e
2
E
h
:
e
?
1
e
?
2
e
?
3
e
1
?
e
2
e
?
1
e
?
2
e
?
3
(c)
Figure 3: Different cases of null insertion
? ? e
?
i
k
? e?
?
i
, e
a
?
i
k
? e?
i
? ? e
i
k
? e?
i
, e
?
a
i
k
= null or e?
a
i
k
? e?
?
i
? ? a?
j
= (e?
?
j
, e?
j
), e?
j
= e
i
1
, ..., e
i
k
or e?
j
=
e
i
k
, ..., e
i
n
, k ? [1, n]
Where a?
i
k
denotes the position of the word in the
backbone that e?
i
k
is aligned to, and a
i
k
denotes the
position of the word in the hypothesis that e
i
k
is
aligned to.
An actual null word may be inserted to a proper
position if a word, either from the hypothesis or from
the backbone, is aligned to null. In this way, the
minimum alignment set is extended to an alignment
unit set, which includes not only minimum align-
ment units but also alignment units which are gener-
ated by adding null words to minimum alignment
units. In general, the following three conditions
should be taken into consideration:
? A backbone word is aligned to null. A null
word is inserted to the hypothesis as shown in
Figure 3 (a).
? A hypothesis word is aligned to null and it is
between the span of a minimum alignment unit.
A new alignment unit is generated by insert-
ing the hypothesis word aligned to null to the
minimum alignment unit. The new hypothesis
string must remain the original word order of
the hypothesis. It is illustrated in Figure 3 (b).
? A hypothesis word is aligned to null and it is
not between the hypothesis span of any mini-
mum alignment unit. In this case, a null word
1108
e1
e
2
?
e
3
e?
?
4
e?
?
5
e?
?
6
(a)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
(b)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
(c)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
(d)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
e?
?
6
(e)
Figure 4: A toy instance of lattice construction
are inserted to the backbone. This is shown in
Figure 3 (c).
4.2 Lattice Construction Algorithm
The lattice is constructed by adding the normalized
alignment pairs incrementally. One backbone arc in
a lattice can only span one backbone word. In con-
trast, all hypothesis words in an alignment unit must
be packed into one hypothesis arc. First the lattice is
initialized with a normalized alignment pair. Then
given all other alignment pairs one by one, the lat-
tice is modified dynamically by adding the hypothe-
sis words of an alignment pair in a left-to-right fash-
ion.
A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction. Assume the cur-
rent inputs are: an alignment pair as in Figure 4 (a),
and a lattice as in Figure 4 (b). The backbone words
of the alignment pair are compared to the backbone
words of the lattice one by one. The procedure is as
follows:
? e
1
is compared with e
1
. Since they are the
same, the hypothesis arc e??
4
, which comes from
the same node with e
1
in the alignment pair,
is compared with the hypothesis arc e??
1
, which
comes from the same node with e
1
in the lat-
tice. The two hypothesis arcs are not the same,
so e??
4
is added to the lattice as shown in Figure
4(c). Both go to the next backbone words.
? e
2
is compared with ?. The lattice remains the
same. The lattice goes to the next backbone
word e
2
.
? e
2
is compared with e
2
. There is no hypothesis
arc coming from the same node with the bone
arc e
2
in the alignment pair, so the lattice re-
mains the same. Both go to the next backbone
words.
? ? is compared with e
3
. A null backbone arc is
inserted into the lattice between e
2
and e
3
. The
hypothesis arc e??
5
is inserted to the lattice, too.
The modified lattice is shown in Figure 4(d).
The alignment pair goes to the next backbone
word e
3
.
? e
3
is compared with e
3
. For they are the same
and there is no hypothesis arc e??
6
in the lattice,
e?
?
6
is inserted to the lattice as in Figure 4(e).
? Both arrive at the end and it is the turn of the
next alignment pair.
When comparing a backbone word of the given
alignment pair with a backbone word of the lattice,
the following three cases should be handled:
? The current backbone word of the given align-
ment pair is a null word while the current back-
bone word of the lattice is not. A null back-
bone word is inserted to the lattice.
? The current backbone word of the lattice is a
null word while the current word of the given
alignment pair is not. The current null back-
bone word of the lattice is skipped with nothing
to do. The next backbone word of the lattice is
compared with the current backbone word of
the given alignment pair.
1109
Algorithm 1 Lattice construction algorithm.
1: Input: alignment pairs {p
n
}
N
n=1
2: L? p
1
3: Unique(L)
4: for n? 2 .. N do
5: pnode = p
n
? first
6: lnode = L ? first
7: while pnode ? barcnext 6= NULL do
8: if lnode ? barcnext = NULL or pnode ?
bword = null and lnode ? bword 6= null then
9: INSERTBARC(lnode, null)
10: pnode = pnode ? barcnext
11: else
12: if pnode ? bword 6= null and lnode ?
bword = null then
13: lnode = lnode ? barcnext
14: else
15: for each harc of pnode do
16: if NotExist(lnode, pnode ? harc)
then
17: INSERTHARC(lnode, pnode ?
harc)
18: pnode = pnode ? barcnext
19: lnode = lnode ? barcnext
20: Output: lattice L
? The current backbone words of the given align-
ment pair and the lattice are the same. Let
{harc
l
} denotes the set of hypothesis arcs,
which come from the same node with the cur-
rent backbone arc in the lattice, and harc
h
de-
notes one of the corresponding hypothesis arcs
in the given alignment pair. In the {harc
l
},
if there is no arc which is the same with the
harc
h
, a hypothesis arc projecting to harc
h
is
added to the lattice.
The algorithm of constructing a lattice is illus-
trated in Algorithm 1. The backbone words of the
alignment pair and the lattice are processed one by
one in a left-to-right manner. Line 2 initializes the
lattice with the first alignment pair, and Line 3 re-
moves the hypothesis arc which contains the same
words with the backbone arc. barc denotes the back-
bone arc, storing one backbone word only, and harc
denotes the hypothesis arc, storing the hypothesis
words. For there may be many alignment units span
the same backbone word range, there may be more
than one harc coming from one node. Line 8 ? 10
consider the condition 1 and function InsertBarc in
Line 9 inserts a null bone arc to the position right
before the current node. Line 12?13 deal with con-
dition 2 and jump to the next backbone word of the
lattice. Line 15?19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same
hypothesis words and the same backbone word span
with the current hypothesis arc.
5 Decoding
In confusion network decoding, a translation is gen-
erated by traveling all the nodes from left to right.
So a translation path contains all the nodes. While
in lattice decoding, a translation path may skip some
nodes as some hypothesis arcs may cross more than
one backbone arc.
Similar to the features in Rosti et al (2007a), the
features adopted by lattice-based model are arc pos-
terior probability, language model probability, the
number of null arcs, the number of hypothesis arcs
possessing more than one non-null word and the
number of all non-null words. The features are com-
bined in a log-linear model with the arc posterior
probabilities being processed specially as follows:
log p(e/f) =
N
arc
?
i=1
log (
N
s
?
s=1
?
s
p
s
(arc))
+ ?L(e) + ?N
nullarc
(e)
+ ?N
longarc
(e) + ?N
word
(e)
(9)
where f denotes the source sentence, e denotes a
translation generated by the lattice-based system,
N
arc
is the number of arcs the path of e covers,
N
s
is the number of candidate systems and ?
s
is the
weight of system s. ? is the language model weight
and L(e) is the LM log-probability. N
nullarcs
(e) is
the number of the arcs which only contain a null
word, and N
longarc
(e) is the number of the arcs
which store more than one non-null word. The
above two numbers are gotten by counting both
backbone arcs and hypothesis arcs. ? and ? are the
corresponding weights of the numbers, respectively.
N
word
(e) is the non-null word number and ? is its
weight.
Each arc has different confidences concerned with
different systems, and the confidence of system s
is denoted by p
s
(arc). p
s
(arc) is increased by
1110
1/(k+1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al, 2007a; He et al, 2008).
Cube pruning algorithm with beam search is em-
ployed to search for the consensus output (Huang
and Chiang, 2005). The nodes in the lattice are
searched in a topological order and each node re-
tains a list of N best candidate partial translations.
6 Experiments
The candidate systems participating in the system
combination are as listed in Table 1: System A is a
BTG-based system using a MaxEnt-based reorder-
ing model; System B is a hierarchical phrase-based
system; System C is the Moses decoder (Koehn et
al., 2007); System D is a syntax-based system. 10-
best hypotheses from each candidate system on the
dev and test sets were collected as the input of the
system combination.
In our experiments, the weights were all tuned on
the NIST MT02 Chinese-to-English test set, includ-
ing 878 sentences, and the test data was the NIST
MT05 Chinese-to-English test set, including 1082
sentences, except the experiments in Table 2. A 5-
gram language model was used which was trained
on the XinHua portion of Gigaword corpus. The re-
sults were all reported in case sensitive BLEU score
and the weights were tuned in Powell?s method to
maximum BLEU score. The IHMM-based align-
ment module was implemented according to He et
al. (2008), He (2007) and Vogel et al (1996). In all
experiments, the parameters for IHMM-based align-
ment module were set to: the smoothing factor for
the surface similarity model, ? = 3; the controlling
factor for the distortion model, K = 2.
6.1 Comparison with
Confusion-network-based model
In order to compare the lattice-based system with
the confusion-network-based system fairly, we used
IHMM-based system combination model on behalf
of the confusion-network-based model described in
He et al (2008). In both lattice-based and IHMM-
based systems, the bilingual dictionaries were ex-
tracted on the FBIS data set which included 289K
sentence pairs. The interpolation factor of the simi-
larity model was set to ? = 0.1.
The results are shown in Table 1. IHMM stands
for the IHMM-based model and Lattice stands for
the lattice-based model. On the dev set, the lattice-
based system was 3.92 BLEU points higher than the
best single system and 0.36 BLEU point higher than
the IHMM-based system. On the test set, the lattice-
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.
System MT02 MT05
BLEU% BLEU%
SystemA 31.93 30.68
SystemB 32.16 32.07
SystemC 32.09 31.64
SystemD 33.37 31.26
IHMM 36.93 34.57
Lattice 37.29 35.80
Table 1: Results on the MT02 and MT05 test sets
The results on another test sets are reported in Ta-
ble 2. The parameters were tuned on the newswire
part of NIST MT06 Chinese-to-English test set, in-
cluding 616 sentences, and the test set was NIST
MT08 Chinese-to-English test set, including 1357
sentences. The BLEU score of the lattice-based sys-
tem is 0.93 BLEU point higher than the IHMM-
based system and 3.0 BLEU points higher than the
best single system.
System MT06 MT08
BLEU% BLEU%
SystemA 32.51 25.63
SystemB 31.43 26.32
SystemC 31.50 23.43
SystemD 32.41 26.28
IHMM 36.05 28.39
Lattice 36.53 29.32
Table 2: Results on the MT06 and MT08 test sets
We take a real example from the output of the
two systems (in Table 3) to show that higher BLEU
scores correspond to better alignments and better
translations. The translation of System C is selected
as the backbone. From Table 3, we can see that
because of 1-to-1 mappings, ?Russia? is aligned to
?Russian? and ??s? to ?null? in the IHMM-based
model, which leads to the error translation ?Russian
1111
Source: ?dIE?h?i??dIEd?i?1??
SystemA: Russia merger of state-owned oil company and the state-run gas company in Russia
SystemB: Russia ?s state-owned oil company is working with Russia ?s state-run gas company mergers
SystemC: Russian state-run oil company is combined with the Russian state-run gas company
SystemD: Russia ?s state-owned oil companies are combined with Russia ?s state-run gas company
IHMM: Russian ?s state-owned oil company working with Russia ?s state-run gas company
Lattice: Russia ?s state-owned oil company is combined with the Russian state-run gas company
Table 3: A real translation example
?s?. Instead, ?Russia ?s? is together aligned to ?Rus-
sian? in the lattice-based model. Also due to 1-to-
1 mappings, null word aligned to ?is? is inserted.
As a result, ?is? is missed in the output of IHMM-
based model. In contrast, in the lattice-based sys-
tem, ?is working with? are aligned to ?is combined
with?, forming a phrase pair.
6.2 Effect of Dictionary Scale
The dictionary is important to the semantic similar-
ity model in IHMM-based alignment method. We
evaluated the effect of the dictionary scale by using
dictionaries extracted on different data sets. The dic-
tionaries were respectively extracted on similar data
sets: 30K sentence pairs, 60K sentence pairs, 289K
sentence pairs (FBIS corpus) and 2500K sentence
pairs. The results are illustrated in Table 4. In or-
der to demonstrate the effect of the dictionary size
clearly, the interpolation factor of similarity model
was all set to ? = 0.1.
From Table 4, we can see that when the cor-
pus size rise from 30k to 60k, the improvements
were not obvious both on the dev set and on the
test set. As the corpus was expanded to 289K, al-
though on the dev set, the result was only 0.2 BLEU
point higher, on the test set, it was 0.63 BLEU point
higher. As the corpus size was up to 2500K, the
BLEU scores both on the dev and test sets declined.
The reason is that, on one hand, there are more noise
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear-
ing on the test set. So we can conclude that in or-
der to get better results, the dictionary scale must be
up to some certain scale. If the dictionary is much
smaller, the result will be impacted dramatically.
MT02 MT05
BLEU% BLEU%
30k 36.94 35.14
60k 37.09 35.17
289k 37.29 35.80
2500k 37.14 35.62
Table 4: Effect of dictionary scale
6.3 Effect of Semantic Alignments
For the IHMM-based alignment method, the transla-
tion probability of an English word pair is computed
using a linear interpolation of the semantic similar-
ity and the surface similarity. So the two similarity
models decide the translation probability together
and the proportion is controlled by the interpolation
factor. We evaluated the effect of the two similarity
models by varying the interpolation factor ?.
We used the dictionaries extracted on the FBIS
data set. The result is shown in Table 5. We got the
best result with ? = 0.1. When we excluded the
semantic similarity model (? = 0.0) or excluded the
surface similarity model (? = 1.0), the performance
became worse.
7 Conclusion
The alignment model plays an important role in
system combination. Because of the expression
limitation of confusion networks, only 1-to-1 map-
pings are employed in the confusion-network-based
model. This paper proposes a lattice-based system
combination model. As a general form of confusion
networks, lattices can express n-to-n mappings. So
a lattice-based model processes phrase pairs while
1112
MT02 MT05
BLEU% BLEU%
? = 1.0 36.41 34.92
? = 0.7 37.21 35.65
? = 0.5 36.43 35.02
? = 0.4 37.14 35.55
? = 0.3 36.75 35.66
? = 0.2 36.81 35.55
? = 0.1 37.29 35.80
? = 0.0 36.45 35.14
Table 5: Effect of semantic alignments
a confusion-network-based model processes words
only. As a result, phrase pairs must be extracted be-
fore constructing a lattice.
On NIST MT05 test set, the lattice-based sys-
tem gave better results with an absolute improve-
ment of 1.23 BLEU points over the confusion-
network-based system (He et al, 2008) and 3.73
BLEU points over the best single system. On
NIST MT08 test set, the lattice-based system out-
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single
system by 3.0 BLEU points.
8 Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China Contract 60736014, Na-
tional Natural Science Foundation of China Con-
tract 60873167 and High Technology R&D Program
Project No. 2006AA010108. Thank Wenbin Jiang,
Tian Xia and Shu Cai for their help. We are also
grateful to the anonymous reviewers for their valu-
able comments.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. of
IEEE ASRU, pages 351?354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL/HLT 2008, pages 1012?1020, Colum-
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP, pages
95?100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for computing outputs from ma-
chine translation systems. In Proc. of EMNLP, pages
98?107.
Xiaodong He. 2007. Using word-dependent translation
models in hmm based word alignment for statistical
machine translation. In Proc. of COLING-ACL, pages
961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages 53?
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstration
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of IEEE EACL, pages 33?40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. of ACL,
pages 312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple ma-
chine translation systems. In Proc. of NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translaiton system combination. In
Proc. of the Third ACL WorkShop on Statistical Ma-
chine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. of ICASSP, pages
105?108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836?841.
1113
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576?584,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Joint Decoding with Multiple Translation Models
Yang Liu and Haitao Mi and Yang Feng and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,htmi,fengyang,liuqun}@ict.ac.cn
Abstract
Current SMT systems usually decode with
single translation models and cannot ben-
efit from the strengths of other models in
decoding phase. We instead propose joint
decoding, a method that combines multi-
ple translation models in one decoder. Our
joint decoder draws connections among
multiple models by integrating the trans-
lation hypergraphs they produce individu-
ally. Therefore, one model can share trans-
lations and even derivations with other
models. Comparable to the state-of-the-art
system combination technique, joint de-
coding achieves an absolute improvement
of 1.5 BLEU points over individual decod-
ing.
1 Introduction
System combination aims to find consensus trans-
lations among different machine translation sys-
tems. It proves that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
Recent several years have witnessed the rapid
development of system combination methods
based on confusion networks (e.g., (Rosti et al,
2007; He et al, 2008)), which show state-of-the-
art performance in MT benchmarks. A confusion
network consists of a sequence of sets of candidate
words. Each candidate word is associated with a
score. The optimal consensus translation can be
obtained by selecting one word from each set of
candidates to maximizing the overall score. While
it is easy and efficient to manipulate strings, cur-
rent methods usually have no access to most infor-
mation available in decoding phase, which might
be useful for obtaining further improvements.
In this paper, we propose a framework for com-
bining multiple translation models directly in de-
coding phase. 1 Based on max-translation decod-
ing and max-derivation decoding used in conven-
tional individual decoders (Section 2), we go fur-
ther to develop a joint decoder that integrates mul-
tiple models on a firm basis:
? Structuring the search space of each model
as a translation hypergraph (Section 3.1),
our joint decoder packs individual translation
hypergraphs together by merging nodes that
have identical partial translations (Section
3.2). Although such translation-level combi-
nation will not produce new translations, it
does change the way of selecting promising
candidates.
? Two models could even share derivations
with each other if they produce the same
structures on the target side (Section 3.3),
which we refer to as derivation-level com-
bination. This method enlarges the search
space by allowing for mixing different types
of translation rules within one derivation.
? As multiple derivations are used for finding
optimal translations, we extend the minimum
error rate training (MERT) algorithm (Och,
2003) to tune feature weights with respect
to BLEU score for max-translation decoding
(Section 4).
We evaluated our joint decoder that integrated
a hierarchical phrase-based model (Chiang, 2005;
Chiang, 2007) and a tree-to-string model (Liu et
al., 2006) on the NIST 2005 Chinese-English test-
set. Experimental results show that joint decod-
1It might be controversial to use the term ?model?, which
usually has a very precise definition in the field. Some
researchers prefer to saying ?phrase-based approaches? or
?phrase-based systems?. On the other hand, other authors
(e.g., (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2007))
do use the expression ?phrase-based models?. In this paper,
we use the term ?model? to emphasize that we integrate dif-
ferent approaches directly in decoding phase rather than post-
processing system outputs.
576
S ? ?X1,X1?
X ? ?fabiao X1, give a X1?
X ? ?yanjiang, talk?
Figure 1: A derivation composed of SCFG rules
that translates a Chinese sentence ?fabiao yan-
jiang? into an English sentence ?give a talk?.
ing with multiple models achieves an absolute im-
provement of 1.5 BLEU points over individual de-
coding with single models (Section 5).
2 Background
Statistical machine translation is a decision prob-
lem where we need decide on the best of target
sentence matching a source sentence. The process
of searching for the best translation is convention-
ally called decoding, which usually involves se-
quences of decisions that translate a source sen-
tence into a target sentence step by step.
For example, Figure 1 shows a sequence of
SCFG rules (Chiang, 2005; Chiang, 2007) that
translates a Chinese sentence ?fabiao yanjiang?
into an English sentence ?give a talk?. Such se-
quence of decisions is called a derivation. In
phrase-based models, a decision can be translating
a source phrase into a target phrase or reordering
the target phrases. In syntax-based models, deci-
sions usually correspond to transduction rules. Of-
ten, there are many derivations that are distinct yet
produce the same translation.
Blunsom et al (2008) present a latent vari-
able model that describes the relationship between
translation and derivation clearly. Given a source
sentence f , the probability of a target sentence e
being its translation is the sum over all possible
derivations:
Pr(e|f) =
?
d??(e,f)
Pr(d, e|f) (1)
where ?(e, f) is the set of all possible derivations
that translate f into e and d is one such derivation.
They use a log-linear model to define the con-
ditional probability of a derivation d and corre-
sponding translation e conditioned on a source
sentence f :
Pr(d, e|f) = exp
?
m ?mhm(d, e, f)
Z(f) (2)
where hm is a feature function, ?m is the asso-
ciated feature weight, and Z(f) is a constant for
normalization:
Z(f) =
?
e
?
d??(e,f)
exp
?
m
?mhm(d, e, f) (3)
A feature value is usually decomposed as the
product of decision probabilities: 2
h(d, e, f) =
?
d?d
p(d) (4)
where d is a decision in the derivation d.
Although originally proposed for supporting
large sets of non-independent and overlapping fea-
tures, the latent variable model is actually a more
general form of conventional linear model (Och
and Ney, 2002).
Accordingly, decoding for the latent variable
model can be formalized as
e? = argmax
e
{
?
d??(e,f)
exp
?
m
?mhm(d, e, f)
}
(5)
where Z(f) is not needed in decoding because it
is independent of e.
Most SMT systems approximate the summa-
tion over all possible derivations by using 1-best
derivation for efficiency. They search for the 1-
best derivation and take its target yield as the best
translation:
e? ? argmax
e,d
{
?
m
?mhm(d, e, f)
}
(6)
We refer to Eq. (5) as max-translation decoding
and Eq. (6) as max-derivation decoding, which are
first termed by Blunsom et al (2008).
By now, most current SMT systems, adopting
either max-derivation decoding or max-translation
decoding, have only used single models in decod-
ing phase. We refer to them as individual de-
coders. In the following section, we will present
a new method called joint decoding that includes
multiple models in one decoder.
3 Joint Decoding
There are two major challenges for combining
multiple models directly in decoding phase. First,
they rely on different kinds of knowledge sources
2There are also features independent of derivations, such
as language model and word penalty.
577
Sgive
0-1
talk
1-2
give a talk
0-2
give talks
0-2
S
give
0-1
speech
1-2
give a talk
0-2
make a speech
0-2
S
give
0-1
talk
1-2
speech
1-2
give a talk
0-2
give talks
0-2
make a speech
0-2
packing(a) (b)
(c)
Figure 2: (a) A translation hypergraph produced by one model; (b) a translation hypergraph produced by
another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote
the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating
that the two models produce the same translations.
and thus need to collect different information dur-
ing decoding. For example, taking a source parse
as input, a tree-to-string decoder (e.g., (Liu et al,
2006)) pattern-matches the source parse with tree-
to-string rules and produces a string on the tar-
get side. On the contrary, a string-to-tree decoder
(e.g., (Galley et al, 2006; Shen et al, 2008)) is a
parser that applies string-to-tree rules to obtain a
target parse for the source string. As a result, the
hypothesis structures of the two models are funda-
mentally different.
Second, translation models differ in decoding
algorithms. Depending on the generating order
of a target sentence, we distinguish between two
major categories: left-to-right and bottom-up. De-
coders that use rules with flat structures (e.g.,
phrase pairs) usually generate target sentences
from left to right while those using rules with hier-
archical structures (e.g., SCFG rules) often run in
a bottom-up style.
In response to the two challenges, we first ar-
gue that the search space of an arbitrary model can
be structured as a translation hypergraph, which
makes each model connectable to others (Section
3.1). Then, we show that a packed translation hy-
pergraph that integrates the hypergraphs of indi-
vidual models can be generated in a bottom-up
topological order, either integrated at the transla-
tion level (Section 3.2) or the derivation level (Sec-
tion 3.3).
3.1 Translation Hypergraph
Despite the diversity of translation models, they all
have to produce partial translations for substrings
of input sentences. Therefore, we represent the
search space of a translation model as a structure
called translation hypergraph.
Figure 2(a) demonstrates a translation hyper-
graph for one model, for example, a hierarchical
phrase-based model. A node in a hypergraph de-
notes a partial translation for a source substring,
except for the starting node ?S?. For example,
given the example source sentence
0 fabiao 1 yanjiang 2
the node ??give talks?, [0, 2]? in Figure 2(a) de-
notes that ?give talks? is one translation of the
source string f21 = ?fabiao yanjiang?.
The hyperedges between nodes denote the deci-
sion steps that produce head nodes from tail nodes.
For example, the incoming hyperedge of the node
??give talks?, [0, 2]? could correspond to an SCFG
rule:
X ? ?X1 yanjiang,X1 talks?
Each hyperedge is associated with a number of
weights, which are the feature values of the corre-
sponding translation rules. A path of hyperedges
constitutes a derivation.
578
Hypergraph Decoding
node translation
hyperedge rule
path derivation
Table 1: Correspondence between translation hy-
pergraph and decoding.
More formally, a hypergraph (Klein and Man-
ning., 2001; Huang and Chiang, 2005) is a tuple
?V,E,R?, where V is a set of nodes, E is a set
of hyperedges, and R is a set of weights. For a
given source sentence f = fn1 = f1 . . . fn, each
node v ? V is in the form of ?t, [i, j]?, which de-
notes the recognition of t as one translation of the
source substring spanning from i through j (that
is, fi+1 . . . fj). Each hyperedge e ? E is a tuple
e = ?tails(e), head(e), w(e)?, where head(e) ?
V is the consequent node in the deductive step,
tails(e) ? V ? is the list of antecedent nodes, and
w(e) is a weight function from R|tails(e)| to R.
As a general representation, a translation hyper-
graph is capable of characterizing the search space
of an arbitrary translation model. Furthermore,
it offers a graphic interpretation of decoding pro-
cess. A node in a hypergraph denotes a translation,
a hyperedge denotes a decision step, and a path
of hyperedges denotes a derivation. A translation
hypergraph is formally a semiring as the weight
of a path is the product of hyperedge weights and
the weight of a node is the sum of path weights.
While max-derivation decoding only retains the
single best path at each node, max-translation de-
coding sums up all incoming paths. Table 1 sum-
marizes the relationship between translation hy-
pergraph and decoding.
3.2 Translation-Level Combination
The conventional interpretation of Eq. (1) is that
the probability of a translation is the sum over all
possible derivations coming from the same model.
Alternatively, we interpret Eq. (1) as that the
derivations could come from different models.3
This forms the theoretical basis of joint decoding.
Although the information inside a derivation
differs widely among translation models, the be-
ginning and end points (i.e., f and e, respectively)
must be identical. For example, a tree-to-string
3The same for all d occurrences in Section 2. For exam-
ple, ?(e, f) might include derivations from various models
now. Note that we still use Z for normalization.
model first parses f to obtain a source tree T (f)
and then transforms T (f) to the target sentence
e. Conversely, a string-to-tree model first parses
f into a target tree T (e) and then takes the surface
string e as the translation. Despite different inside,
their derivations must begin with f and end with e.
This situation remains the same for derivations
between a source substring f ji and its partial trans-
lation t during joint decoding:
Pr(t|f ji ) =
?
d??(t,fji )
Pr(d, t|f ji ) (7)
where d might come from multiple models. In
other words, derivations from multiple models
could be brought together for computing the prob-
ability of one partial translation.
Graphically speaking, joint decoding creates a
packed translation hypergraph that combines in-
dividual hypergraphs by merging nodes that have
identical translations. For example, Figure 2 (a)
and (b) demonstrate two translation hypergraphs
generated by two models respectively and Fig-
ure 2 (c) is the resulting packed hypergraph. The
solid lines denote the hyperedges of the first model
and the dashed lines denote those of the second
model. The shaded nodes are shared by both mod-
els. Therefore, the two models are combined at the
translation level. Intuitively, shared nodes should
be favored in decoding because they offer consen-
sus translations among different models.
Now the question is how to decode with multi-
ple models jointly in just one decoder. We believe
that both left-to-right and bottom-up strategies can
be used for joint decoding. Although phrase-based
decoders usually produce translations from left to
right, they can adopt bottom-up decoding in prin-
ciple. Xiong et al (2006) develop a bottom-up de-
coder for BTG (Wu, 1997) that uses only phrase
pairs. They treat reordering of phrases as a binary
classification problem. On the other hand, it is
possible for syntax-based models to decode from
left to right. Watanabe et al (2006) propose left-
to-right target generation for hierarchical phrase-
based translation. Although left-to-right decod-
ing might enable a more efficient use of language
models and hopefully produce better translations,
we adopt bottom-up decoding in this paper just for
convenience.
Figure 3 demonstrates the search algorithm of
our joint decoder. The input is a source language
sentence fn1 , and a set of translation models M
579
1: procedure JOINTDECODING(fn1 , M )
2: G? ?
3: for l ? 1 . . . n do
4: for all i, j s.t. j ? i = l do
5: for all m ?M do
6: ADD(G, i, j,m)
7: end for
8: PRUNE(G, i, j)
9: end for
10: end for
11: end procedure
Figure 3: Search algorithm for joint decoding.
(line 1). After initializing the translation hyper-
graph G (line 2), the decoder runs in a bottom-
up style, adding nodes for each span [i, j] and for
each model m. For each span [i, j] (lines 3-5),
the procedure ADD(G, i, j,m) add nodes gener-
ated by the model m to the hypergraph G (line 6).
Each model searches for partial translations inde-
pendently: it uses its own knowledge sources and
visits its own antecedent nodes, just running like
a bottom-up individual decoder. After all mod-
els finishes adding nodes for span [i, j], the pro-
cedure PRUNE(G, i, j) merges identical nodes and
removes less promising nodes to control the search
space (line 8). The pruning strategy is similar to
that of individual decoders, except that we require
there must exist at least one node for each model
to ensure further inference.
Although translation-level combination will not
offer new translations as compared to single mod-
els, it changes the way of selecting promising can-
didates in a combined search space and might po-
tentially produce better translations than individ-
ual decoding.
3.3 Derivation-Level Combination
In translation-level combination, different models
interact with each other only at the nodes. The
derivations of one model are unaccessible to other
models. However, if two models produce the same
structures on the target side, it is possible to com-
bine two models within one derivation, which we
refer to as derivation-level combination.
For example, although different on the source
side, both hierarchical phrase-based and tree-to-
string models produce strings of terminals and
nonterminals on the target side. Figure 4 shows
a derivation composed of both hierarchical phrase
IP(x1:VV, x2:NN) ? x1 x2
X ? ?fabiao, give?
X ? ?yanjiang, a talk?
Figure 4: A derivation composed of both SCFG
and tree-to-string rules.
pairs and tree-to-string rules. Hierarchical phrase
pairs are used for translating smaller units and
tree-to-string rules for bigger ones. It is appealing
to combine them in such a way because the hierar-
chical phrase-based model provides excellent rule
coverage while the tree-to-string model offers lin-
guistically motivated non-local reordering. Sim-
ilarly, Blunsom and Osborne (2008) use both hi-
erarchical phrase pairs and tree-to-string rules in
decoding, where source parse trees serve as condi-
tioning context rather than hard constraints.
Depending on the target side output, we dis-
tinguish between string-targeted and tree-targeted
models. String-targeted models include phrase-
based, hierarchical phrase-based, and tree-to-
string models. Tree-targeted models include
string-to-tree and tree-to-tree models. All models
can be combined at the translation level. Models
that share with same target output structure can be
further combined at the derivation level.
The joint decoder usually runs as max-
translation decoding because multiple derivations
from various models are used. However, if all
models involved belong to the same category, a
joint decoder can also adopt the max-derivation
fashion because all nodes and hyperedges are ac-
cessible now (Section 5.2).
Allowing derivations for comprising rules from
different models and integrating their strengths,
derivation-level combination could hopefully pro-
duce new and better translations as compared with
single models.
4 Extended Minimum Error Rate
Training
Minimum error rate training (Och, 2003) is widely
used to optimize feature weights for a linear model
(Och and Ney, 2002). The key idea of MERT is
to tune one feature weight to minimize error rate
each time while keep others fixed. Therefore, each
580
xf(x)
t1
t2
t3
(0, 0) x1 x2
Figure 5: Calculation of critical intersections.
candidate translation can be represented as a line:
f(x) = a? x + b (8)
where a is the feature value of current dimension,
x is the feature weight being tuned, and b is the
dotproduct of other dimensions. The intersection
of two lines is where the candidate translation will
change. Instead of computing all intersections,
Och (2003) only computes critical intersections
where highest-score translations will change. This
method reduces the computational overhead sig-
nificantly.
Unfortunately, minimum error rate training can-
not be directly used to optimize feature weights of
max-translation decoding because Eq. (5) is not a
linear model. However, if we also tune one dimen-
sion each time and keep other dimensions fixed,
we obtain a monotonic curve as follows:
f(x) =
K
?
k=1
eak?x+bk (9)
where K is the number of derivations for a can-
didate translation, ak is the feature value of cur-
rent dimension on the kth derivation and bk is the
dotproduct of other dimensions on the kth deriva-
tion. If we restrict that ak is always non-negative,
the curve shown in Eq. (9) will be a monotoni-
cally increasing function. Therefore, it is possible
to extend the MERT algorithm to handle situations
where multiple derivations are taken into account
for decoding.
The key difference is the calculation of criti-
cal intersections. The major challenge is that two
curves might have multiple intersections while
two lines have at most one intersection. Fortu-
nately, as the curve is monotonically increasing,
we need only to find the leftmost intersection of
a curve with other curves that have greater values
after the intersection as a candidate critical inter-
section.
Figure 5 demonstrates three curves: t1, t2, and
t3. Suppose that the left bound of x is 0, we com-
pute the function values for t1, t2, and t3 at x = 0
and find that t3 has the greatest value. As a result,
we choose x = 0 as the first critical intersection.
Then, we compute the leftmost intersections of t3
with t1 and t2 and choose the intersection closest
to x = 0, that is x1, as our new critical intersec-
tion. Similarly, we start from x1 and find x2 as the
next critical intersection. This iteration continues
until it reaches the right bound. The bold curve de-
notes the translations we will choose over different
ranges. For example, we will always choose t2 for
the range [x1, x2].
To compute the leftmost intersection of two
curves, we divide the range from current critical
intersection to the right bound into many bins (i.e.,
smaller ranges) and search the bins one by one
from left to right. We assume that there is at most
one intersection in each bin. As a result, we can
use the Bisection method for finding the intersec-
tion in each bin. The search process ends immedi-
ately once an intersection is found.
We divide max-translation decoding into three
phases: (1) build the translation hypergraphs, (2)
generate n-best translations, and (3) generate n?-
best derivations. We apply Algorithm 3 of Huang
and Chiang (2005) for n-best list generation. Ex-
tended MERT runs on n-best translations plus n?-
best derivations to optimize the feature weights.
Note that feature weights of various models are
tuned jointly in extended MERT.
5 Experiments
5.1 Data Preparation
Our experiments were on Chinese-to-English
translation. We used the FBIS corpus (6.9M +
8.9M words) as the training corpus. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT Evaluation test
set as our development set, and used the NIST
2005 test set as test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
Our joint decoder included two models. The
581
Max-derivation Max-translationModel Combination Time BLEU Time BLEU
hierarchical N/A 40.53 30.11 44.87 29.82
tree-to-string N/A 6.13 27.23 6.69 27.11
translation N/A N/A 55.89 30.79both derivation 48.45 31.63 54.91 31.49
Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec-
onds/sentence) and BLEU score (case-insensitive).
first model was the hierarchical phrase-based
model (Chiang, 2005; Chiang, 2007). We obtained
word alignments of training data by first running
GIZA++ (Och and Ney, 2003) and then applying
the refinement rule ?grow-diag-final-and? (Koehn
et al, 2003). About 2.6M hierarchical phrase pairs
extracted from the training corpus were used on
the test set.
Another model was the tree-to-string model
(Liu et al, 2006; Liu et al, 2007). Based on
the same word-aligned training corpus, we ran a
Chinese parser on the source side to obtain 1-best
parses. For 15,157 sentences we failed to obtain
1-best parses. Therefore, only 93.7% of the train-
ing corpus were used by the tree-to-string model.
About 578K tree-to-string rules extracted from the
training corpus were used on the test set.
5.2 Individual Decoding Vs. Joint Decoding
Table 2 shows the results of comparing individ-
ual decoding and joint decoding on the test set.
With conventional max-derivation decoding, the
hierarchical phrase-based model achieved a BLEU
score of 30.11 on the test set, with an average de-
coding time of 40.53 seconds/sentence. We found
that accounting for all possible derivations in max-
translation decoding resulted in a small negative
effect on BLEU score (from 30.11 to 29.82), even
though the feature weights were tuned with respect
to BLEU score. One possible reason is that we
only used n-best derivations instead of all possi-
ble derivations for minimum error rate training.
Max-derivation decoding with the tree-to-string
model yielded much lower BLEU score (i.e.,
27.23) than the hierarchical phrase-based model.
One reason is that the tree-to-string model fails
to capture a large amount of linguistically unmo-
tivated mappings due to syntactic constraints. An-
other reason is that the tree-to-string model only
used part of the training data because of pars-
ing failure. Similarly, accounting for all possible
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
 0  1  2  3  4  5  6  7  8  9  10  11
pe
rc
en
ta
ge
span width
Figure 6: Node sharing in max-translation de-
coding with varying span widths. We retain at
most 100 nodes for each source substring for each
model.
derivations in max-translation decoding failed to
bring benefits for the tree-to-string model (from
27.23 to 27.11).
When combining the two models at the trans-
lation level, the joint decoder achieved a BLEU
score of 30.79 that outperformed the best result
(i.e., 30.11) of individual decoding significantly
(p < 0.05). This suggests that accounting for
all possible derivations from multiple models will
help discriminate among candidate translations.
Figure 6 demonstrates the percentages of nodes
shared by the two models over various span widths
in packed translation hypergraphs during max-
translation decoding. For one-word source strings,
89.33% nodes in the hypergrpah were shared by
both models. With the increase of span width, the
percentage decreased dramatically due to the di-
versity of the two models. However, there still ex-
ist nodes shared by two models even for source
substrings that contain 33 words.
When combining the two models at the deriva-
tion level using max-derivation decoding, the joint
decoder achieved a BLEU score of 31.63 that out-
performed the best result (i.e., 30.11) of individ-
582
Method Model BLEU
hierarchical 30.11individual decoding
tree-to-string 27.23
system combination both 31.50
joint decoding both 31.63
Table 3: Comparison of individual decoding, sys-
tem combination, and joint decoding.
ual decoding significantly (p < 0.01). This im-
provement resulted from the mixture of hierarchi-
cal phrase pairs and tree-to-string rules. To pro-
duce the result, the joint decoder made use of
8,114 hierarchical phrase pairs learned from train-
ing data, 6,800 glue rules connecting partial trans-
lations monotonically, and 16,554 tree-to-string
rules. While tree-to-string rules offer linguistically
motivated non-local reordering during decoding,
hierarchical phrase pairs ensure good rule cover-
age. Max-translation decoding still failed to sur-
pass max-derivation decoding in this case.
5.3 Comparison with System Combination
We re-implemented a state-of-the-art system com-
bination method (Rosti et al, 2007). As shown
in Table 3, taking the translations of the two indi-
vidual decoders as input, the system combination
method achieved a BLEU score of 31.50, slightly
lower than that of joint decoding. But this differ-
ence is not significant statistically.
5.4 Individual Training Vs. Joint Training
Table 4 shows the effects of individual training and
joint training. By individual, we mean that the two
models are trained independently. We concatenate
and normalize their feature weights for the joint
decoder. By joint, we mean that they are trained
together by the extended MERT algorithm. We
found that joint training outperformed individual
training significantly for both max-derivation de-
coding and max-translation decoding.
6 Related Work
System combination has benefited various NLP
tasks in recent years, such as products-of-experts
(e.g., (Smith and Eisner, 2005)) and ensemble-
based parsing (e.g., (Henderson and Brill, 1999)).
In machine translation, confusion-network based
combination techniques (e.g., (Rosti et al, 2007;
He et al, 2008)) have achieved the state-of-the-
art performance in MT evaluations. From a dif-
Training Max-derivation Max-translation
individual 30.70 29.95
joint 31.63 30.79
Table 4: Comparison of individual training and
joint training.
ferent perspective, we try to combine different ap-
proaches directly in decoding phase by using hy-
pergraphs. While system combination techniques
manipulate only the final translations of each sys-
tem, our method opens the possibility of exploit-
ing much more information.
Blunsom et al (2008) first distinguish between
max-derivation decoding and max-translation de-
coding explicitly. They show that max-translation
decoding outperforms max-derivation decoding
for the latent variable model. While they train the
parameters using a maximum a posteriori estima-
tor, we extend the MERT algorithm (Och, 2003)
to take the evaluation metric into account.
Hypergraphs have been successfully used in
parsing (Klein and Manning., 2001; Huang and
Chiang, 2005; Huang, 2008) and machine trans-
lation (Huang and Chiang, 2007; Mi et al, 2008;
Mi and Huang, 2008). Both Mi et al (2008) and
Blunsom et al (2008) use a translation hyper-
graph to represent search space. The difference is
that their hypergraphs are specifically designed for
the forest-based tree-to-string model and the hier-
archical phrase-based model, respectively, while
ours is more general and can be applied to arbi-
trary models.
7 Conclusion
We have presented a framework for including mul-
tiple translation models in one decoder. Repre-
senting search space as a translation hypergraph,
individual models are accessible to others via shar-
ing nodes and even hyperedges. As our decoder
accounts for multiple derivations, we extend the
MERT algorithm to tune feature weights with re-
spect to BLEU score for max-translation decod-
ing. In the future, we plan to optimize feature
weights for max-translation decoding directly on
the entire packed translation hypergraph rather
than on n-best derivations, following the lattice-
based MERT (Macherey et al, 2008).
583
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60873167
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done while
Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Yajuan Lu?, Liang Huang, Nguyen
Bach, Andreas Zollmann, Vamshi Ambati, and
Kevin Gimpel for their helpful feedback.
References
Phil Blunsom and Mile Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP08.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL08.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL05.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP94.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL06.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proc. of
EMNLP08.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proc. of EMNLP99.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT05.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL07.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL08.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proc. of ACL08.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL03.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL06.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL07.
Wolfgang Macherey, Franz J. Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proc. of EMNLP08.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP08.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL08.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL02.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL02.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proc. of ACL07.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL08.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. of ACL05.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proc. of ICSLP02.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL06.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL06.
584
Coling 2010: Poster Volume, pages 285?293,
Beijing, August 2010
An Efficient Shift-Reduce Decoding Algorithm for Phrased-Based
Machine Translation
Yang Feng, Haitao Mi, Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{fengyang,htmi,yliu,liuqun}@ict.ac.cn
Abstract
In statistical machine translation, decod-
ing without any reordering constraint is
an NP-hard problem. Inversion Transduc-
tion Grammars (ITGs) exploit linguistic
structure and can well balance the needed
flexibility against complexity constraints.
Currently, translation models with ITG
constraints usually employs the cube-time
CYK algorithm. In this paper, we present
a shift-reduce decoding algorithm that can
generate ITG-legal translation from left to
right in linear time. This algorithm runs
in a reduce-eager style and is suited to
phrase-based models. Using the state-of-
the-art decoder Moses as the baseline, ex-
periment results show that the shift-reduce
algorithm can significantly improve both
the accuracy and the speed on different
test sets.
1 Introduction
In statistical machine translation, for the diver-
sity of natural languages, the word order of
source and target language may differ and search-
ing through all possible translations is NP-hard
(Knight, 1999). So some measures have to be
taken to reduce search space: either using a search
algorithm with pruning technique or restricting
possible reorderings.
Currently, beam search is widely used (Till-
mann and Ney, 2003; Koehn, 2004) to reduce
search space. However, the pruning technique
adopted by this algorithm is not risk-free. As a
result, the best partial translation may be ruled out
during pruning. The more aggressive the prun-
ing is, the more likely the best translation escapes.
There should be a tradeoff between the speed and
the accuracy. If some heuristic knowledge is em-
ployed to guide the search, the search algorithm
can discard some implausible hypotheses in ad-
vance and focus on more possible ones.
Inversion Transduction Grammars (ITGs) per-
mit a minimal extra degree of ordering flexibility
and are particularly well suited to modeling or-
dering shifts between languages (Wu, 1996; Wu,
1997). They can well balance the needed flex-
ibility against complexity constraints. Recently,
ITG has been successfully applied to statistical
machine translation (Zens and Ney, 2003; Zens
et al, 2004; Xiong et al, 2006). However, ITG
generally employs the expensive CYK parsing al-
gorithm which runs in cube time. In addition, the
CYK algorithm can not calculate language model
exactly in the process of decoding, as it can not
catch the full history context of the left words in a
hypothesis.
In this paper, we introduce a shift-reduce de-
coding algorithm with ITG constraints which runs
in a left-to-right manner. This algorithm parses
source words in the order of their corresponding
translations on the target side. In the meantime,
it gives all candidate ITG-legal reorderings. The
shift-reduce algorithm is different from the CYK
algorithm, in particular:
? It produces translation in a left-to-right man-
ner. As a result, language model probability
can be calculated more precisely in the light
of full history context.
? It decodes much faster. Applied with distor-
285
target side target side target side
(a) straight (b) inverted (c) discontinuous
Figure 1: Orientation of two blocks.
tion limit, shift-reduce decoding algorithm
can run in linear time, while the CYK runs
in cube time.
? It holds ITG structures generated during de-
coding. That is to say, it can directly give
ITG-legal spans, which leads to faster de-
coding. Furthermore, it can be extended to
syntax-based models.
We evaluated the performance of the shift-
reduce decoding algorithm by adding ITG con-
straints to the state-of-the-art decoder Moses. We
did experiments on three data sets: NIST MT08
data set, NIST MT05 data set and China Work-
shop on Machine Translation 2007 data set. Com-
pared to Moses, the improvements of the accuracy
are 1.59, 0.62, 0.8 BLEU score, respectively, and
the speed improvements are 15%, 24%, 30%, re-
spectively.
2 Decoding with ITG constraints
In this paper, we employ the shift-reduce algo-
rithm to add ITG constraints to phrase-based ma-
chine translation model. It is different from the
traditional shift-reduce algorithm used in natural
language parsing. On one hand, as natural lan-
guage parsing has to cope with a high degree of
ambiguity, it need take ambiguity into considera-
tion. As a result, the traditional one often suffers
shift-reduce divergence. Nonetheless, the shift-
reduce algorithm in this paper does not pay atten-
tion to ambiguity and acts in a reduce-eager man-
ner. On the other hand, the traditional algorithm
can not ensure that all reorderings observe ITG
constraints, so we have to modify the traditional
algorithm to import ITG constraints.
We will introduce the shift-reduce decoding al-
gorithm in the following two steps: First, we
1\1
zairu1
??2
shijian2
N3
diaocha3
]4
ziliaode4
>M5
diannao5
;?6
zaoqie6
The laptop with inquiry data on the event was stolen
(a)
A1
The laptop
diannao5
with
A2
zairu1
inquiry
A3
diaocha3
data
A4
ziliaode4
A5
on the event
shijian2
A6
was stolen
zaoqie6
A7
A8
A9
A10
A11
(b)
Figure 2: A Chinese-to-English sentence pair and
its corresponding ITG tree.
will deduce how to integrate the shift-reduce al-
gorithm and ITG constraints and show its correct-
ness (Section 2.1). Second, we will describe the
shift-reduce decoding algorithm in details (Sec-
tion 2.2).
2.1 Adding ITG constraints
In the process of decoding, a source phrase is re-
garded as a block and a source sentence is seen
as a sequence of blocks. The orientation of two
blocks whose translations are adjacent on the tar-
get side can be straight, inverted or discontinu-
ous, as shown in Figure 1. According to ITG,
two blocks which are straight or inverted can be
merged into a single block. For parsing, differ-
ent mergence order of a sequence of continuous
blocks may yield different derivations. In con-
trast, the phrase-based machine translation does
not compute reordering probabilities hierarchi-
cally, so the mergence order will not impact the
computation of reordering probabilities. As a
result, the shift-reduce decoding algorithm need
not take into consideration the shift-reduce diver-
gence. It merges two continuous blocks as soon
as possible, acting in a reduce-eager style.
Every ITG-legal sentence pair has a corre-
286
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry
(a) (b) (c)
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(d) (e) (f)
 
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(g) (h) (i)
Figure 3: The partial translation procedure of the sentence in Figure 2.
sponding ITG tree, and source words covered
by every node (eg. A1, ..., A11 in Figure 2(b))
in the ITG tree can be seen as a block. By
watching the tree in Figure 2, we can find that
a block must be adjacent to the block either on
its left or on its right, then they can be merged
into a larger block. For example, A2 matches
the block [zairu1] and A8 matches the block
[shijian2 diaocha3 ziliaode4].1 The two blocks
are adjacent and they are merged into a larger
block [zairu1 shijian2 diaocha3 ziliaode4],
covered by A9. The procedure of translating
zairu1 shijian2 diaocha3 ziliaode4 diannao5
is illustrated in Figure 3.
For a hypothesis during decoding, we assign it
three factors: the current block, the left neigh-
boring uncovered span and the right neighbor-
ing uncovered span. For example, in Figure
3(c), the current block is [diaocha3] and the left
neighboring uncovered span is [shijian2] and the
right neighboring uncovered span is [ziliaode4].
[zaoqie6] is not thought of as the right neighbor-
ing block, for it is not adjacent to [diaocha3]. The
next covered block is [ziliaode4] (as shown in
Figure 3(d)). For [diaocha3] and [ziliaode4] are
adjacent, they are merged. In Figure 3(e), the cur-
rent block is [diaocha3 ziliaode4].
A sentence is translated with ITG constraints iff
1The words within a block are sorted by their order in the
source sentence.
its source side can be covered by an ITG tree. That
is to say, for every hypothesis during decoding, the
next block to cover must be selected from the left
or right neighboring uncovered span.
First, we show that if the next block to cover is
selected in this way, the translation must observe
ITG constraints. For every hypothesis during de-
coding, the immediate left and right words of the
current block face the following three conditions:
(1) The immediately left word is not covered
and the immediately right word is covered, then
the next block to cover must be selected from the
left neighboring uncovered span, eg. for the cur-
rent block [diaocha3 ziliaode4] in Figure 3(e). In
this condition, the ITG tree can be constructed in
the following two ways: either all words in the left
neighboring uncovered span are translated first,
then this span is merged with the current span
(taking three nodes as an example, this case is
shown in Figure 4(a)), or the right part of the left
neighboring uncovered span is merged with the
current block first, then the new block is merged
with the rest part of the left neighboring uncov-
ered span (shown in Figure 4(b)). In a word, only
after all words in the left neighboring uncovered
span are covered, other words can be covered.
(2) The immediately right word is not covered
and the immediately left word is covered. Simi-
larly, only after all words in the right neighboring
uncovered span are covered, other words can be
287
(a) (b)
Figure 4: The two ways that the current block is
merged with its left neighboring uncovered span.
The third node in the first row denotes the current
block, the first and second nodes in the first row
denote left and right parts of the left neighboring
uncovered span, respectively.
covered.
(3) The immediately left and right words are
neither covered. The next block can be selected
from either the left or the right neighboring uncov-
ered span until the immediate left or right word is
covered.
The above operations can be performed recur-
sively until the whole source sentence is merged
into a single block, so the reordering observes ITG
constraints.
Now, we show that translation which is not gen-
erated in the above way must violate ITG con-
straints.
If the next block is selected out of the neighbor-
ing uncovered spans, the current block can be nei-
ther adjacent to the last covered block nor adjacent
to the selected next block, so the current block can
not be merged with any block and the whole sen-
tence can not be covered by an ITG tree. As in
Figure 3(b), if the next block to cover is [zaoqie6],
then [zairu1] is neither adjacent to [diannao5]
nor adjacent to [zaoqie6].
We can conclude that if we select the next block
from the left or right neighboring uncovered span
of the current block, then the translation must ob-
serve ITG constraints.
2.2 Shift-Reduce Decoding Algorithm
In order to generate the translation with ITG con-
straints, the shift-reduce algorithm have to keep
trace of covered blocks, left and right neighboring
uncovered spans. Formally, the shift-reduce de-
coding algorithm uses the following three stacks:
? St: the stack for covered blocks. The blocks
are pushed in the order that they are covered,
not the order that they are in the source sen-
tence.
? Sl : the stack for the left uncovered spans of
the current block. When a block is pushed
into St, its corresponding left neighboring
uncovered span is pushed into Sl.
? Sr :the stack for the right uncovered spans of
the current block. When a block is pushed
into St, its corresponding right neighboring
uncovered span is pushed into Sr.
A translation configuration is a triple c =
?St, Sl, Sr?. Given a source sentence f =
f1, f2, ..., fm, we import a virtual start word and
the whole translation procedure can be seen as
a sequence of transitions from cs to ct, where
cs = ?[0], ?, [1,m]? is the initial configura-
tion, ct = ?[0,m], ?, ?? is the terminal con-
figuration. The configuration for Figure 3 (e) is
?[0][5][1][3, 4], [2], [6]?.
We define three types of transitions from
a configuration to another . Assume the cur-
rent configuration c = ? [ft11, ft12]...[ftk1, ftk2],
[fl11, fl12]...[flu1, flu2], [frv1, frv2]...[fr11, fr12] ?,
then :
? Transitions LShift pop the top element
[flu1, flu2] from Sl and select a block [i, j]
from [flu1, flu2] to translate. In addition,
they push [i, j] into St, and if i 6= flu1, they
push [flu1, i ? 1] into Sl, and if j 6= flu2,
they push [j+1, flu2] into Sr. The precondi-
tion to operate the transition is that Sl is not
null and the top span of Sl is adjacent to the
top block of St. Formally, the precondition
is flu2 + 1 = ftk1.
? Transitions RShift pop the top element
[frv1, frv2] of Sr and select a block [i, j]
from [frv1, frv2] to translate. In addition,
they push [i, j] into St, and if i 6= frv1, they
push [frv1, i?1] into Sl, and if j 6= frv2, they
push [j + 1, frv2] into Sr. The precondition
is that Sr is not null and the top span of Sr is
288
adjacent to the top block of St. Formally, the
precondition is ftk2 + 1 = frv1.
? Transitions Reduce pop the top two blocks
[ftk?11, ftk?12], [ftk1, ftk2] from St and push
the merged span [ftk?11, ftk2] into St. The
precondition is that the top two blocks are ad-
jacent. Formally, the precondition is ftk?12+
1 = ftk1
The transition sequence of the example in Fig-
ure 2 is listed in Figure 5. For the purpose of
efficiency, transitions Reduce are integrated with
transitions LShift and RShift in practical imple-
mentation. Before transitions LShift and RShift
push [i, j] into St, they check whether [i, j] is ad-
jacent to the top block of St. If so, they change
the top block into the merged block directly.
In practical implementation, in order to further
restrict search space, distortion limit is applied be-
sides ITG constraints: a source phrase can be cov-
ered next only when it is ITG-legal and its distor-
tion does not exceed distortion limit. The distor-
tion d is calculated by d = |starti ? endi?1 ? 1|,
where starti is the start position of the current
phrase and endi?1 is the last position of the last
translated phrase.
3 Related Work
Galley and Manning (2008) present a hierarchi-
cal phrase reordering model aimed at improving
non-local reorderings. Via the hierarchical mer-
gence of two blocks, the orientation of long dis-
tance words can be computed. Their shift-reduce
algorithm does not import ITG constraints and ad-
mits the translation violating ITG constraints.
Zens et al (2004) introduce a left-to-
right decoding algorithm with ITG constraints
on the alignment template system (Och et al,
1999). Their algorithm processes candidate
source phrases one by one through the whole
search space and checks if the candidate phrase
complies with ITG constraints. Besides, their al-
gorithm checks validity via cover vector and does
not formalize ITG structure. The shift-reduce de-
coding algorithm holds ITG structure via three
stacks. As a result, it can offer ITG-legal spans
directly and decode faster. Furthermore, with
Transition St Sl Sr
[0] ? [1, 6]
RShift [0][5] [1, 4] [6]
LShift [0][5][1] ? [2, 4][6]
RShift [0][5][1][3] [2] [4][6]
RShift [0][5][1][3][4] [2] [6]
Reduce [0][5][1][3, 4] [2] [6]
LShift [0][5][1][3, 4][2] ? [6]
Reduce [0][5][1][2, 4] ? [6]
Reduce [0][5][1, 4] ? [6]
Reduce [0][1, 5] ? [6]
Reduce [0, 5] ? [6]
RShift [0, 5][6] ? ?
Reduce [0, 6] ? ?
Figure 5: Transition sequence for the example in
Figure 2. The top nine transitions correspond to
Figure 3 (a), ... , Figure 3 (i), respectively.
the help of ITG structure, it can be extended to
syntax-based models easily.
Xiong et al (2006) propose a BTG-based
model, which uses the context to determine the
orientation of two adjacent spans. It employs the
cube-time CYK algorithm.
4 Experiments
We compare the shift-reduce decoder with the
state-of-the-art decoder Moses (Koehn et al,
2007). The shift-reduce decoder was imple-
mented by modifying the normal search algo-
rithm of Moses to our shift-reduce algorithm,
without cube pruning (Huang and Chiang, 2005).
We retained the features of Moses: four trans-
lation features, three lexical reordering features
(straight, inverted and discontinuous), linear dis-
tortion, phrase penalty, word penalty and language
model, without importing any new feature. The
decoding configurations used by all the decoders,
including beam size, phrase table limit and so on,
were the same, so the performance was compared
fairly.
First, we will show the performance of shift-
reduce algorithm on three data sets with large
training data sets (Section 4.1). Then, we will
analyze the performance elaborately in terms of
accuracy, speed and search ability with a smaller
289
training data set (Section 4.2). All experiments
were done on Chinese-to-English translation tasks
and all results are reported with case insensitive
BLEU score. Statistical significance were com-
puted using the sign-test described in Collins et
al. (Collins et al, 2005).
4.1 Performance Evaluation
We did three experiments to compare the perfor-
mance of the shift-reduce decoder, Moses and the
decoder with ITG constraints using cover vector
(denoted as CV). 2 The shift-reduce decoder de-
coded with two sets of parameters: one was tuned
by itself (denoted as SR) and the other was tuned
by Moses (denoted as SR-same), using MERT
(Och, 2003). Two searching algorithms of Moses
are considered: one is the normal search algorithm
without cubing pruning (denoted as Moses), the
other is the search algorithm with cube pruning
(denoted as Moses-cb). For all the decoders, the
distortion limit was set to 6, the nbest size was set
to 100 and the phrase table limit was 50.
In the first experiment, the development set is
part of NIST MT06 data set including 862 sen-
tences, the test set is NIST MT08 data set and
the training data set contains 5 million sentence
pairs. We used a 5-gram language model which
were trained on the Xinhua and AFP portion of
the Gigaword corpus. The results are shown in
Table 1(a).
In the second experiment, the development data
set is NIST MT02 data set and the test set is NIST
MT05 data set. Language model and the training
data set are the same to that of the first experiment.
The result is shown in Table 1(b).
In the third experiment, the development set
is China Workshop on Machine Translation 2008
data set (denoted as CWMT08) and the test set
is China Workshop on Machine Translation 2007
data set (denoted as CWMT07). The training set
contains 2 Million sentence pairs and the language
model are a 6-gram language model trained on
the Reuter corpus and English corpus. Table 1(c)
gives the results.
In the above three experiments, SR decoder
2The decoder CV is implemented by adding the ITG con-
straints to Moses using the algorithm described in (Zens et
al., 2004).
NIST06 NIST08 speed
Moses 30.24 25.08 4.827
Moses-cb 30.27 23.80 1.501
CV 30.35 26.23** 4.335
SR-same ?? 25.09 3.856
SR 30.47 26.67** 4.126
(a)
NIST02 NIST05 speed
Moses 35.68 35.80 7.142
Moses-cb 35.42 35.03 1.811
CV 35.45 36.56** 6.276
SR-same ?? 35.84 5.008
SR 35.99* 36.42** 5.432
(b)
CWMT08 CWMT07 speed
Moses 27.75 25.91 3.061
Moses-cb 27.82 25.16 0.548
CV 27.71 26.58** 2.331
SR-same ?? 25.97 1.988
SR 28.14* 26.71** 2.106
(c)
Table 1: Performance comparison. Moses: Moses
without cube pruning, Moses-cb: Moses with
cube pruning, CV: the decoder using cover vector,
SR-same: the shift-reduce decoder decoding with
parameters tunes by Moses, SR: the shift-reduce
decoder with parameters tuned by itself. The sec-
ond column stands for develop set, the third col-
umn stands for test set and speed column shows
the average time (seconds) of translating one sen-
tence in the test set. **: significance at the .01
level.
improves the accuracy by 1.59, 0.62, 0.8 BLEU
score (p < .01), respectively, and improves the
speed by 15%, 24%, 30%, respectively. we can
see that SR can improve both the accuracy and
the speed while SR-same can increase the speed
significantly with a slight improvement on the ac-
curacy. As both SR and CV decode with ITG
constraints, they match each other on the accu-
290
27.00
27.50
28.00
28.50
29.00
29.50
30.00
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17
B
LE
U
average decoding speed (s)
d=-1
d=-1
d=-1
SR
SR-same
Moses
Figure 6: Performance comparison on NIST05.
For a curve, the dots correspond to distortion limit
4, 6, 8, 10, 14 and no distortion from left to right.
d = ?1 stands for no distortion limit.
racy. However, the speed of SR is faster than CV.
Cube pruning can improve decoding speed dra-
matically, but it is not risk-free pruning technol-
ogy, so the BLEU score declines obviously.
4.2 Performance Analysis
We make performance analysis with the same ex-
periment configuration as the second experiment
in Section 4.1, except that the training set in
the analysis experiment is FBIS corpus, includ-
ing 289k sentence pairs. In the following exper-
iments, Moses employs the normal search algo-
rithm without cube pruning.
For the decoders employ the linear distortion
feature, the distortion limit will influence the
translation accuracy. Besides, with different dis-
tortion limit, the proportion of ITG-legal transla-
tion generated by Moses will differ. The smaller
the distortion limit is, the greater the proportion is.
So we first compare the performance with differ-
ent distortion limit.
We compare the shift-reduce decoder with
Moses using different distortion limit. The re-
sults are shown in Figure 6. When distortion limit
is set to 6, every decoder gets a peak value and
SR has an improvement of 0.66 BLEU score over
Moses. From the curves, we can see that the
BLEU score of SR-same with distortion limit 8
28.00
28.50
29.00
29.50
30.00
30.50
31.00
31.50
32.00
32.50
33.00
33.50
34.00
34.50
35.00
35.50
36.00
36.50
37.00
 4  6  8  10  12  14  16
B
LE
U
distortion limit
SR
SR-same
Moses
(a) ITG set
25.00
25.50
26.00
26.50
27.00
27.50
28.00
 4  6  8  10  12  14  16
BL
EU
distortion limit
SR
SR-same
Moses
(b) rest set
Figure 7: Accuracy comparison on the ITG set
and rest set of NIST05. The ITG set includes the
sentences the translations of which generated by
Moses are ITG-legal, and the rest set contains the
rest sentences. distortion limit = 16 denotes no
distortion limit.
is lower than that of Mose with distortion limit
6. This is because the decoding speed of SR-
same with distortion limit 8 is not faster than that
of Moses with distortion limit 6. On the whole,
compared to Moses, SR-same can improve the ac-
curacy slightly with much faster decoding speed,
and SR can obtain improvements on both the ac-
curacy and the speed.
We split the test set into two sets: one contains
the sentences, the translations of which generated
by Moses are ITG-legal (denoted as ITG set) and
the other contains the rest (denoted as rest set).
From Figure 7, we can see that no matter on the
ITG set or on the rest set, SR decoder can gain ob-
vious accuracy improvements with all distortion
291
ITG restd
Moses SR-same total < = > Moses SR-same total < = >
4 28.67 28.68 1050 8 1042 0 25.61 25.82 32 0 0 32
6 31.34 31.42 758 51 705 2 25.78 25.72 324 32 2 290
8 32.59 32.93* 594 72 516 6 25.68 25.65 488 82 3 403
10 34.36 34.99** 456 80 365 11 26.04 26.50* 626 147 3 476
12 33.16 33.61** 454 63 380 11 27.01 27.13 628 165 1 462
14 35.98 36.25* 383 60 316 7 26.35 26.67* 699 203 1 495
-1 34.13 34.96** 351 39 308 4 26.17 26.78** 731 154 0 577
Table 2: Search ability comparison. The ITG set and the rest set of NIST05 were tested, respectively.
On the ITG set, the following six factors are reported from left to right: BLEU score of Moses, BLEU
score of SR-same, the number of sentences in the ITG set, the number of sentences the translation
probabilities of which computed by Moses, compared to that computed by SR, is lower, equal and
greater. The rest set goes similarly. *: significance at the .05 level, **: significance at the .01 level.
limit. While SR-same decoder only gets better re-
sults on the ITG set with all distortion limit. This
may result from the use of the linear distortion
feature. Moses may generate hypotheses the dis-
tortion of which is forbidden in the shift-reduce
decoder. This especially sharpens on the rest set.
So SR-same may suffer from an improper linear
distortion parameter.
The search ability of Moses and the shift-
reduce decoder are evaluated, too. The translation
must be produced with the same set of parameters.
In our experiments, we employed the parameters
tuned by Moses. The test was done on the ITG and
the rest set, respectively. The results are shown in
Table 2. As the distortion limit becomes greater,
the number of the ITG-legal translation generated
by Moses becomes smaller. On the ITG set, trans-
lation probabilities from the shift-reduce decoder
is either greater or equal to that from Moses on
most sentences, and BLEU scores of shift-reduce
decoder is greater than that of Moses with all
distortion limit. Although the search space of
shift-reduce decoder is smaller than that of Moses,
shift-reduce decoder can give the translation that
Moses can not reach. On the rest set, for most sen-
tences, the translation probabilities from Moses is
greater than that from shift-reduce decoder. But
only when distortion limit is 6 and 8, the BLEU
score of Moses is greater than that of the shift-
reduce decoder. We may conclude that greater
score does not certainly lead to greater BLEU
score.
5 Conclusions and Future Work
In this paper, we present a shift-reduce decod-
ing algorithm for phrase-based translation model
that can generate the ITG-legal translation in lin-
ear time. The algorithm need not consider shift-
reduce divergence and performs reduce operation
as soon as possible. We compare the performance
of the shift-reduce decoder with the state-of-the-
art decoder Moses. Experiment results show that
the shift-reduce algorithm can improve both the
accuracy and the speed significantly on different
test sets. We further analyze the performance and
find that on the ITG set, the shift-reduce decoder
is superior over Moses in terms of accuracy, speed
and search ability, while on the rest set, it does
not display advantage, suffering from improper
parameters.
Next, we will extend the shift-reduce algorithm
to syntax-based translation models, to see whether
it works.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China Contract 60736014,
National Natural Science Foundation of China
Contract 60873167 and High Technology R&D
Program Project No. 2006AA010108. We are
grateful to the anonymous reviewers for their
valuable comments.
292
References
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Galley, Michel and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages
53?64.
Knight, Kevin. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25:607?615.
Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstra-
tion Session.
Koehn, Philipp. 2004. Pharaoh: A beam search de-
coder for phrased-based statistical machine transla-
tion. In Proc. of AMTA, pages 115?124.
Och, Frans J., Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical
machine translation. In Proc. of EMNLP, pages 20?
28.
Och, Frans J. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL,
pages 160?167.
Tillmann, Chirstoph and Hermann Ney. 2003.
Word reordering and a dynamic programming beam
search algorithm for statistical machine translation.
Computational Linguistics, 29:97?133.
Wu, Dekai. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. of ACL,
pages 152?158.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL, pages
521?528.
Zens, Richard and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proc. of ACL, pages 144?151.
Zens, Richard, Hermann Ney, Taro Watanable, and
Eiichiro Sumita. 2004. Reordering constraints
for phrase-based statistical machine translation. In
Proc. of COLING, pages 205?211.
293
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1191?1200, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Left-to-Right Tree-to-String Decoding with Prediction
Yang Feng? Yang Liu? Qun Liu? Trevor Cohn?
? Department of Computer Science
The University of Sheffield, Sheffield, UK
{y.feng, t.cohn}@sheffield.ac.uk
? State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
liuqun@ict.ac.cn
Abstract
Decoding algorithms for syntax based ma-
chine translation suffer from high compu-
tational complexity, a consequence of in-
tersecting a language model with a con-
text free grammar. Left-to-right decoding,
which generates the target string in order,
can improve decoding efficiency by simpli-
fying the language model evaluation. This
paper presents a novel left to right decod-
ing algorithm for tree-to-string translation, us-
ing a bottom-up parsing strategy and dynamic
future cost estimation for each partial trans-
lation. Our method outperforms previously
published tree-to-string decoders, including a
competing left-to-right method.
1 Introduction
In recent years there has been rapid progress in the
development of tree-to-string models for statistical
machine translation. These models use the syntac-
tic parse tree of the source language to inform its
translation, which allows the models to capture con-
sistent syntactic transformations between the source
and target languages, e.g., from subject-verb-object
to subject-object-verb word orderings. Decoding al-
gorithms for grammar-based translation seek to find
the best string in the intersection between a weighted
context free grammar (the translation mode, given a
source string/tree) and a weighted finite state accep-
tor (an n-gram language model). This intersection
is problematic, as it results in an intractably large
grammar, and makes exact search impossible.
Most researchers have resorted to approximate
search, typically beam search (Chiang, 2007). The
decoder parses the source sentence, recording the
target translations for each span.1 As the partial
translation hypothesis grows, its component ngrams
are scored and the hypothesis score is updated. This
decoding method though is inefficient as it requires
recording the language model context (n? 1 words)
on the left and right edges of each chart cell. These
contexts allow for boundary ngrams to be evaluated
when the cell is used in another grammar produc-
tion. In contrast, if the target string is generated
in left-to-right order, then only one language model
context is required, and the problem of language
model evaluation is vastly simplified.
In this paper, we develop a novel method of left-
to-right decoding for tree-to-string translation using
a shift-reduce parsing strategy. A central issue in
any decoding algorithm is the technique used for
pruning the search space. Our left-to-right decod-
ing algorithm groups hypotheses, which cover the
same number of source words, into a bin. Pruning
requires the evaluation of different hypotheses in the
same bin, and elimating the least promising options.
As each hypotheses may cover different sets of tree
1The process is analogous for tree-to-string models, except
that only rules and spans matching those in the source trees are
considered. Typically nodes are visited according to a post-
order traversal.
1191
nodes, it is necessary to consider the cost of uncov-
ered nodes, i.e., the future cost. We show that a good
future cost estimate is essential for accurate and effi-
cient search, leading to high quality translation out-
put.
Other researchers have also considered the left-
to-right decoding algorithm for tree-to-string mod-
els. Huang and Mi (2010) developed an Earley-
style parsing algorithm (Earley, 1970). In their ap-
proach, hypotheses covering the same number of
tree nodes were binned together. Their method uses
a top-down depth-first search, with a mechanism for
early elimation of some rules which lead to dead-
ends in the search. Huang and Mi (2010)?s method
was shown to outperform the traditional post-order-
traversal decoding algorithm, considering fewer hy-
potheses and thus decoding much faster at the same
level of performance. However their algorithm used
a very rough estimate of future cost, resulting in
more search errors than our approach.
Our experiments show that compared with the
Earley-style left-to-right decoding (Huang and Mi,
2010) and the traditional post-order-traversal de-
coding (Liu et al 2006) algorithms, our algorithm
achieves a significant improvement on search capac-
ity and better translation performance at the same
level of speed.
2 Background
A typical tree-to-string system (Liu et al 2006;
Huang et al 2006) searches through a 1-best source
parse tree for the best derivation. It transduces the
source tree into a target-language string using a Syn-
chronous Tree Substitution Grammar (STSG). The
grammar rules are extracted from bilingual word
alignments using the GHKM algorithm (Galley et
al., 2004).
We will briefly review the traditional decoding al-
gorithm (Liu et al 2006) and the Earley-style top-
down decoding algorithm (Huang and Mi, 2010) for
the tree-to-string model.
2.1 Traditional Decoding
The traditional decoding algorithm processes source
tree nodes one by one according to a post-order
traversal. For each node, it applies matched STSG
rules by substituting each non-terminal with its cor-
in theory beam search
traditional O(nc|?V |4(g?1)) O(ncb2)
top-down O(c(cr)d|V |g?1) O(ncb)
bottom-up O((cr)d|V |g?1) O(nub)
Table 1: Time complexity of different algorithms. tra-
ditional : Liu et al(2006), top-down : Huang and Mi
(2010). n is the source sentence length, b is the beam
width, c is the number of rules used for each node, V
is the target word vocabulary, g is the order of the lan-
guage model, d is the depth of the source parse tree, u is
the number of viable prefixes for each node and r is the
maximum arity of each rule.
responding translation. For the derivation in Figure
1 (b), the traditional algorithm applies r2 at node
NN2
r2 : NN2 (jieguo) ? the result,
to obtain ?the result? as the translation of NN2. Next
it applies r4 at node NP,
r4 : NP ( NN1 (toupiao), x1 : NN2 )
? x1 of the vote
and replaces NN2 with its translation ?the result?,
then it gets the translation of NP as ?the result of the
vote?.
This algorithm needs to contain boundary words
at both left and right extremities of the target string
for the purpose of LM evaluation, which leads to a
high time complexity. The time complexity in the-
ory and with beam search (Huang and Mi, 2010) is
shown in Table 1.
2.2 Earley-style Top-down Decoding
The Earley-style decoding algorithm performs a top-
down depth-first parsing and generates the target
translation left to right. It applies Context-Free
Grammar (CFG) rules and employs three actions:
predict, scan and complete (Section 3.1 describes
how to convert STSG rules into CFG rules). We can
simulate its translation process using a stack with a
dot  indicating which symbol to process next. For
the derivation in Figure 1(b) and CFG rules in Fig-
ure 1(c), Figure 2 illustrates the whole translation
process.
The time complexity is shown in Table 1 .
1192
3 Bottom-Up Left-to-Right Decoding
We propose a novel method of left-to-right decoding
for tree-to-string translation using a bottom-up pars-
ing strategy. We use viable prefixes (Aho and John-
son, 1974) to indicate all possible target strings the
translations of each node should starts with. There-
fore, given a tree node to expand, our algorithm
can drop immediately to target terminals no matter
whether there is a gap or not. We say that there is a
gap between two symbols in a derivation when there
are many rules separating them, e.g. IP r6? ... r4?
NN2. For the derivation in Figure 1(b), our algo-
rithm starts from the root node IP and applies r2
first although there is a gap between IP and NN2.
Then it applies r4, r5 and r6 in sequence to generate
the translation ?the result of the vote was released
at night?. Our algorithm takes the gap as a black-
box and does not need to fix which partial deriva-
tion should be used for the gap at the moment. So it
can get target strings as soon as possible and thereby
perform more accurate pruning. A valid derivation
is generated only when the source tree is completely
matched by rules.
Our bottom-up decoding algorithm involves the
following steps:
1. Match STSG rules against the source tree.
2. Convert STSG rules to CFG rules.
3. Collect the viable prefix set for each node in a
post-order transversal.
4. Search bottom-up for the best derivation.
3.1 From STSG to CFG
After rule matching, each tree node has its applica-
ble STSG rule set. Given a matched STSG rule, our
decoding algorithm only needs to consider the tree
node the rule can be applied to and the target side,
so we follow Huang and Mi (2010) to convert STSG
rules to CFG rules. For example, an STSG rule
NP ( NN1 (toupiao), x1 : NN2 ) ? x1 of the vote
can be converted to a CFG rule
NP ? NN2 of the vote
The target non-terminals are replaced with corre-
sponding source non-terminals. Figure 1 (c) shows
all converted CFG rules for the toy example. Note
IP
NP
NN1
to?up?`ao
NN2
j??eguo?
VP
NT
wa?nsha`ng
VV
go?ngbu`
(a) Source parse tree
r6: IP
NP VP
? ?
r4: NP
NN1
to?up?`ao
NN2
r5: VP
NT
wa?nsha`ng
VV
go?ngbu`
?
r2: NN2
j??eguo?
the result of the vote was released at night
(b) A derivation
r1: NN1 ? the vote
r2: NN2 ? the result
r3: NP ? NN2 of NN1
r4: NP ? NN2 of the vote
r5: VP ? was released at night
r6: IP ? NP VP
r7: IP ? NN2 of the vote VP
r8: IP ? VP NP
(c) Target-side CFG rule set
Figure 1: A toy example.
that different STSG rules might be converted to the
same CFG rule despite having different source tree
structures.
3.2 Viable Prefix
During decoding, how do we decide which rules
should be used next given a partial derivation, es-
pecially when there is a gap? A key observation is
that some rules should be excluded. For example,
any derivation for Figure 1(a) will never begin with
r1 as there is no translation starting with ?the vote?.
In order to know which rules can be excluded for
each node, we can recursively calculate the start-
ing terminal strings for each node. For example,
1193
NN1: {the vote} NN2: {the result}
NT: ? VV: ?
NP: {the result}
VP: {was released at night}
IP: {the result, was released at night}
Table 2: The Viable prefix sets for Figure 1 (c)
according to r1, the starting terminal string of the
translation for NN1 is ?the vote?. According to r2,
the starting terminal string for NN2 is ?the result?.
According to r3, the starting terminal string of NP
must include that of NN2. Table 2 lists the starting
terminal strings of all nodes in Figure 1(a). As the
translations of node IP should begin with either ?the
result? or ?was released at night?, the first rule must
be either r2 or r5. Therefore, r1 will never be used
as the first rule in any derivation.
We refer to starting terminal strings of a node as
a viable prefixes, a term borrowed from LR pars-
ing (Aho and Johnson, 1974). Viable prefixes are
used to decide which rule should be used to ensure
efficient left-to-right target generation. Formally, as-
sume that VN denotes the set of non-terminals (i.e.,
source tree node labels), VT denotes the set of ter-
minals (i.e., target words), v1, v2 ? VN , w ? VT ,
pi ? {VT ? VN}?, we say that w is a viable prefix of
v1 if and only if:
? v1 ? w, or
? v1 ? wv2pi, or
? v1 ? v2pi, and w is a viable prefix of v2.
Note that we bundle all successive terminals in one
symbol.
3.3 Shift-Reduce Parsing
We use a shift-reduce algorithm to search for the
best deviation. The algorithm maintains a stack of
dotted rules (Earley, 1970). Given the source tree in
Figure 1(a), the stack is initialized with a dotted rule
for the root node IP:
[ IP].
Then, the algorithm selects one viable prefix of IP
and appends it to the stack with the dot at the begin-
ning (predict):
[ IP] [ the result]2.
Then, a scan action is performed to produce a partial
translation ?the result?:
[ IP] [the result ].
Next, the algorithm searches for the CFG rules start-
ing with ?the result? and gets r2. Then, it pops the
rightmost dotted rule and append the left-hand side
(LHS) of r2 to the stack (complete):
[ IP] [NN2 ].
Next, the algorithm chooses r4 whose right-hand
side ?NN2 of the vote? matches the rightmost dot-
ted rule in the stack3 and grows the rightmost dotted
rule:
[ IP] [NN2  of the vote].
Figure 3 shows the whole process of derivation
generation.
Formally, we define four actions on the rightmost
rule in the stack:
? Predict. If the symbol after the dot in the right-
most dotted rule is a non-terminal v, this action
chooses a viable prefix w of v and generates a
new dotted rule for w with the dot at the begin-
ning. For example:
[ IP] predict?? [ IP] [ the result]
? Scan. If the symbol after the dot in the right-
most dotted rule is a terminal string w, this ac-
tion advances the dot to update the current par-
tial translation. For example:
[ IP] [ the result] scan?? [ IP] [the result ]
? Complete. If the rightmost dotted rule ends
with a dot and it happens to be the right-hand
side of a rule, then this action removes the
right-most dotted rule. Besides, if the symbol
after the dot in the new rightmost rule corre-
sponds to the same tree node as the LHS non-
terminal of the rule, this action advance the dot.
For example,
[ IP] [NP  VP] [was released at night ]
complete?? [ IP] [NP VP ]
2There are another option: ?was released at night?
3Here there is an alternative: r3 or r7
1194
step action rule used stack hypothesis
0 [ IP]
1 p r6 [ IP] [ NP VP]
2 p r4 [ IP] [ NP VP] [ NN2 of the vote]
3 p r2 [ IP] [ NP VP] [ NN2 of the vote] [ the result]
4 s [ IP] [ NP VP] [ NN2 of the vote] [the result ] the result
5 c [ IP] [ NP VP] [NN2  of the vote] the result
6 s [ IP] [ NP VP] [NN2 of the vote ] the result of the vote
7 c [ IP] [NP  VP] the result of the vote
8 p r5 [ IP] [NP  VP] [ was released at night] the result of the vote
9 s [ IP] [NP  VP] [was released at night ] the ... vote was ... night
10 c [ IP] [NP VP ] the ... vote was ... night
11 c [IP ] the ... vote was ... night
Figure 2: Simulation of top-down translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete. ?the ... vote? and ?was ... released? are the abbreviated form of ?the result of the vote? and ?was released at
night?, respectively.
step action rule used stack number hypothesis
0 [ IP] 0
1 p [ IP] [ the result] 0
2 s [ IP] [the result ] 1 the result
3 c r2 [ IP] [NN2 ] 1 the result
4 g r4 or r7 [ IP] [NN2  of the vote] 1 the result
5 s [ IP] [NN2 of the vote ] 2 the result of the vote
6 c r4 [ IP] [NP ] 2 the result of the vote
7 g r6 [ IP] [NP  VP] 2 the result of the vote
8 p [ IP] [NP  VP] [ was released at night] 2 the result of the vote
9 s [ IP] [NP  VP] [was released at night ] 4 the ... vote was ... night
10 c r5 [ IP] [NP VP ] 4 the ... vote was ... night
11 c r6 [IP ] 4 the ... vote was ... night
Figure 3: Simulation of bottom-up translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete; g, grow. The column of number gives the number of source words the hypothesis covers.
If the string cannot rewrite on the frontier non-
terminal, then we add the LHS to the stack with
the dot after it. For example:
[ IP] [the result ] complete?? [ IP] [NN2 ]
? Grow. If the right-most dotted rule ends with
a dot and it happens to be the starting part of
a CFG rule, this action appends one symbol of
the remainder of that rule to the stack 4. For
example:
4We bundle the successive terminals in one rule into a sym-
bol
[ IP] [NN2 ]
grow?? [ IP] [NN2  of the vote]
From the above definition, we can find that there
may be an ambiguity about whether to use a com-
plete action or a grow action. Similarly, predict ac-
tions must select a viable prefix form the set for a
node. For example in step 5, although we select
to perform complete with r4 in the example, r7 is
applicable, too. In our implementation, if both r4
and r7 are applicable, we apply them both to gener-
ate two seperate hypotheses. To limit the exponen-
tial explosion of hypotheses (Knight, 1999), we use
beam search over bins of similar partial hypotheses
(Koehn, 2004).
1195
IP
NP
NN2 of NN1
of the vote
VP
was released at night
r7
r4 r5
r6
r3
Figure 4: The translation forest composed of applicable
CFG rules for the partial derivation of step 3 in Figure 3.
3.4 Future Cost
Partial derivations covering different tree nodes may
be grouped in the same bin for beam pruning5. In
order to performmore accurate pruning, we take into
consideration future cost, the cost of the uncovered
part. The merit of a derivation is the covered cost
(the cost of the covered part) plus the future cost.
We borrow ideas from the Inside-Outside algorithm
(Charniak and Johnson, 2005; Huang, 2008; Mi et
al., 2008) to compute the merit. In our algorithm,
the merit of a derivation is just the Viterbi inside cost
? of the root node calculated with the derivations
continuing from the current derivation.
Given a partial derivation, we calculate its future
cost by searching through the translation forest de-
fined by all applicable CFG rules. Figure 4 shows
the translation forest for the derivation of step 3. We
calculate the future cost for each node as follows:
given a node v, we define its cost function f(v) as
f(v) =
?
?
?
?
?
1 v is completed
lm(v) v is a terminal string
maxr?Rv f(r)
?
pi?rhs(r) f(pi) otherwise
where VN is the non-terminal set, VT is the terminal
set, v, pi ? VN ? VT+, Rv is the set of currently ap-
plicable rules for v, rhs(r) is the right-hand symbol
set of r, lm is the local language model probability,
f(r) is calculated using a linear model whose fea-
tures are bidirectional translation probabilities and
lexical probabilities of r. For the translation forest
in Figure 4, if we calculate the future cost of NP with
5Section 3.7 will describe the binning scheme
r4, then
f(NP ) = f(r4) ? f(NN2) ? lm(of the vote)
= f(r4) ? 1 ? lm(of the vote)
Note that we calculate lm(of the vote) locally and do
not take ?the result? derived from NN2 as the con-
text. The lm probability of ?the result? has been in-
cluded in the covered cost.
As a partial derivation grows, some CFG rules
will conflict with the derivation (i.e. inapplicable)
and the translation forest will change accordingly.
For example, when we reach step 5 from step 3 (see
Figure 4 for its translation forest), r3 is inapplica-
ble and thereby should be ruled out. Then the nodes
on the path from the last covered node (it is ?of the
vote? in step 5) to the root node should update their
future cost, as they may employ r3 to produce the
future cost. In step 5, NP and IP should be updated.
In this sense, we say that the future cost is dynamic.
3.5 Comparison with Top-Down Decoding
In order to generate the translation ?the result? based
on the derivation in Figure 1(b), Huang and Mi?s
top-down algorithm needs to specify which rules to
apply starting from the root node until it yields ?the
result?. In this derivation, rule r6 is applied to IP, r4
to NP, r2 to NN2. That is to say, it needs to repre-
sent the partial derivation from IP to NN2 explicitly.
This can be a problem when combined with beam
pruning. If the beam size is small, it may discard the
intermediate hypotheses and thus never consider the
string. In our example with a beam of 1, we must
select a rule for IP among r6, r7 and r8 although we
do not get any information for NP and VP.
Instead, our bottom-up algorithm allows top-
down and bottom-up information to be used together
with the help of viable prefixes. This allows us to
encode more candidate derivations than the purely
top-down method. In the above example, our al-
gorithm does not specify the derivation for the gap
from IP and ?the result?. In fact, all derivations
composed of currently applicable rules are allowed.
When needed, our algorithm derives the derivation
dynamically using applicable rules. So when our
algorithm performs pruning at the root node, it has
got much more information and consequently intro-
duces fewer pruning errors.
1196
3.6 Time Complexity
Assume the depth of the source tree is d, the max-
imum number of matched rules for each node is c,
the maximum arity of each rule is r, the language
model order is g and the target-language vocabulary
is V, then the time complexity of our algorithm is
O((cr)d|V |g?1). Analysis is as follows:
Our algorithm expands partial paths with termi-
nal strings to generate new hypotheses, so the time
complexity depends on the number of partial paths
used. We split a path which is from the root node to a
leaf node with a node on it (called the end node) and
get the segment from the root node to the end node
as a partial path, so the length of the partial path is
not definite with a maximum of d. If the length is
d?(d? ? d), then the number of partial paths is (cr)d? .
Besides, we use the rightest g ? 1 words to signa-
ture each partial path, so we can get (cr)d? |V |g?1
states. For each state, the number of viable prefixes
produced by predict operation is cd?d? , so the total
time complexity is f = O((cr)d? |V |g?1cd?d?) =
O(cdrd? |V |g?1) = O((cr)d|V |g?1).
3.7 Beam Search
Tomake decoding tractable, we employ beam search
(Koehn, 2004) and choose ?binning? as follows: hy-
potheses covering the same number of source words
are grouped in a bin. When expanding a hypothe-
sis in a beam (bin), we take series of actions until
new terminals are appended to the hypothesis, then
add the new hypothesis to the corresponding beam.
Figure 3 shows the number of source words each hy-
pothesis covers.
Among the actions, only the scan action changes
the number of source words each hypothesis cov-
ers. Although the complete action does not change
source word number, it changes the covered cost of
hypotheses. So in our implementation, we take scan
and complete as ?closure? actions. That is to say,
once there are some complete actions after a scan ac-
tion, we finish all the compete actions until the next
action is grow. The predict and grow actions decide
which rules can be used to expand hypotheses next,
so we update the applicable rule set during these two
actions.
Given a source sentence with n words, we main-
tain n beams, and let each beam hold b hypotheses
at most. Besides, we prune viable prefixes of each
node up to u, so each hypothesis can expand to u
new hypotheses at most, so the time complexity of
beam search is O(nub).
4 Related Work
Watanabe et al(2006) present a novel Earley-
style top-down decoding algorithm for hierarchical
phrase-based model (Chiang, 2005). Their frame-
work extracts Greibach Normal Form rules only,
which always has at least one terminal on the left
of each rule, and discards other rules.
Dyer and Resnik (2010) describe a translation
model that combines the merits of syntax-based
models and phrase-based models. Their decoder
works in two passes: for first pass, the decoder col-
lects a context-free forest and performs tree-based
source reordering without a LM. For the second
pass, the decoder adds a LM and performs bottom-
up CKY decoding.
Feng et al(2010) proposed a shift-reduce algo-
rithm to add BTG constraints to phrase-based mod-
els. This algorithm constructs a BTG tree in a
reduce-eager manner while the algorithm in this pa-
per searches for a best derivation which must be de-
rived from the source tree.
Galley and Manning (2008) use the shift-reduce
algorithm to conduct hierarchical phrase reordering
so as to capture long-distance reordering. This al-
gorithm shows good performance on phrase-based
models, but can not be applied to syntax-based mod-
els directly.
5 Experiments
In the experiments, we use two baseline systems:
our in-house tree-to-string decoder implemented ac-
cording to Liu et al(2006) (denoted as traditional)
and the Earley-style top-down decoder implemented
according to Huang and Mi (2010) (denoted as top-
down), respectively. We compare our bottom-up
left-to-right decoder (denoted as bottom-up) with
the baseline in terms of performance, translation
quality and decoding speed with different beam
sizes, and search capacity. Lastly, we show the in-
fluence of future cost. All systems are implemented
in C++.
1197
5.1 Data Setup
We used the FBIS corpus consisting of about 250K
Chinese-English sentence pairs as the training set.
We aligned the sentence pairs using the GIZA++
toolkit (Och and Ney, 2003) and extracted tree-to-
string rules according to the GHKM algorithm (Gal-
ley et al 2004). We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus.
We used the 2002 NIST MT Chinese-English test
set (571 sentences) as the development set and the
2005 NIST MT Chinese-English test set (1082 sen-
tences) as the test set. We evaluated translation qual-
ity using BLEU-metric (Papineni et al 2002) with
case-insensitive n-gram matching up to n = 4. We
used the standard minimum error rate training (Och,
2003) to tune feature weights to maximize BLEU
score on the development set.
5.2 Performance Comparison
Our bottom-up left-to-right decoder employs the
same features as the traditional decoder: rule proba-
bility, lexical probability, language model probabil-
ity, rule count and word count. In order to compare
them fairly, we used the same beam size which is 20
and employed cube pruning technique (Huang and
Chiang, 2005).
We show the results in Table 3. From the re-
sults, we can see that the bottom-up decoder out-
performs top-down decoder and traditional decoder
by 1.1 and 0.8 BLEU points respectively and the
improvements are statistically significant using the
sign-test of Collins et al(2005) (p < 0.01). The
improvement may result from dynamically search-
ing for a whole derivation which leads to more ac-
curate estimation of a partial derivation. The addi-
tional time consumption of the bottom-up decoder
against the top-down decoder comes from dynamic
future cost computation.
Next we compare decoding speed versus transla-
tion quality using various beam sizes. The results
are shown in Figure 5. We can see that our bottom-
up decoder can produce better BLEU score at the
same decoding speed. At small beams (decoding
time around 0.5 second), the improvement of trans-
lation quality is much bigger.
System BLEU(%) Time (s)
Traditional 29.8 0.84
Top-down 29.5 0.41
Bottom-up 30.6 0.81
Table 3: Performance comparison.
29.4
29.6
29.8
30.0
30.2
30.4
30.6
30.8
 0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
BL
EU
 S
co
re
Avg Decoding Time (secs per sentence)
bottom-up
top-down
traditional
Figure 5: BLEU score against decoding time with various
beam size.
5.3 Search Capacity Comparison
We also compare the search capacity of the bottom-
up decoder and the traditional decoder. We do this
in the following way: we let both decoders use the
same weights tuned on the traditional decoder, then
we compare their translation scores of the same test
sentence.
From the results in Table 4, we can see that for
many test sentences, the bottom-up decoder finds
target translations with higher score, which have
been ruled out by the traditional decoder. This may
result from more accurate pruning method. Yet for
some sentences, the traditional decoder can attain
higher translation score. The reason may be that the
traditional decoder can hold more than two nonter-
minals when cube pruning, while the bottom-up de-
coder always performs dual-arity pruning.
Next, we check whether higher translation scores
bring higher BLEU scores. We compute the BLEU
score of both decoders on the test sentence set on
which bottom-up decoder gets higher translation
scores than the traditional decoder does. We record
the results in Figure 6. The result shows that higher
score indeed bring higher BLEU score, but the im-
provement of BLEU score is not large. This is be-
cause the features we use don?t reflect the real statis-
1198
28.0
29.0
30.0
31.0
32.0
33.0
34.0
35.0
 10  20  30  40
BL
EU
 S
co
re
Beam Size
bottom-up
traditional
Figure 6: BLEU score with various beam sizes on the sub
test set consisting of sentences on which the bottom-up
decoder gets higher translation score than the traditional
decoder does.
b > = <
10 728 67% 347 32% 7 1%
20 657 61% 412 38% 13 1%
30 615 57% 446 41% 21 2%
40 526 49% 523 48% 33 3%
50 315 29% 705 65% 62 6%
Table 4: Search capacity comparison. The first column is
beam size, the following three columns denote the num-
ber of test sentences, on which the translation scores of
the bottom-up decoder are greater, equal to, lower than
that of the traditional decoder.
System BLEU(%) Time (s)
with 30.6 0.81
without 28.8 0.39
Table 5: Influence of future cost. The results of the
bottom-up decoder with and without future cost are given
in the second and three rows, respectively.
tical distribution of hypotheses well. In addition, the
weights are tuned on the traditional decoder, not on
the bottom-up decoder. The bottom-up decoder can
perform better with weights tuned by itself.
5.4 Influence of Future Cost
Next, we will show the impact of future cost via ex-
periments. We give the results of the bottom-up de-
coder with and without future cost in Table 5. From
the result, we can conclude that future cost plays a
significant role in decoding. If the bottom-up de-
coder does not employ future cost, its performance
will be influenced dramatically. Furthermore, cal-
culating dynamic future cost is time consuming. If
the bottom-up decoder does not use future cost, it
decodes faster than the top-down decoder. This is
because the top-down decoder has |T | beams, while
the bottom-up decoder has n beams, where T is the
source parse tree and n is the length of the source
sentence.
6 Conclusions
In this paper, we describe a bottom-up left-to-right
decoding algorithm for tree-to-string model. With
the help of viable prefixes, the algorithm generates
a translation by constructing a target-side CFG tree
according to a post-order traversal. In addition, it
takes into consideration a dynamic future cost to es-
timate hypotheses.
On the 2005 NIST Chinese-English MT transla-
tion test set, our decoder outperforms the top-down
decoder and the traditional decoder by 1.1 and 0.8
BLEU points respectively and shows more powerful
search ability. Experiments also prove that future
cost is important for more accurate pruning.
7 Acknowledgements
We would like to thank Haitao Mi and Douwe
Gelling for their feedback, and anonymous review-
ers for their valuable comments and suggestions.
This work was supported in part by EPSRC grant
EP/I034750/1 and in part by High Technology R&D
Program Project No. 2011AA01A207.
References
A. V. Aho and S. C. Johnson. 1974. Lr parsing. Com-
puting Surveys, 6:99?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL, pages 173?180.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
1199
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proc. of NAACL,
pages 858?866, June.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13:94?102.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling, pages
285?293.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT, pages 53?64.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL,
pages 586?594.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25:607?615.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrased-based statistical machine translation. In
Proc. of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING, pages
777?784.
1200
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950?958,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Hierarchical Chunk-to-String Translation?
Yang Feng? Dongdong Zhang? Mu Li? Ming Zhou? Qun Liu?
? Department of Computer Science ? Microsoft Research Asia
University of Sheffield dozhang@microsoft.com
Sheffield, UK muli@microsoft.com
y.feng@shef.ac.uk mingzhou@microsoft.com
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
liuqun@ict.ac.cn
Abstract
We present a hierarchical chunk-to-string
translation model, which can be seen as a
compromise between the hierarchical phrase-
based model and the tree-to-string model,
to combine the merits of the two models.
With the help of shallow parsing, our model
learns rules consisting of words and chunks
and meanwhile introduce syntax cohesion.
Under the weighed synchronous context-free
grammar defined by these rules, our model
searches for the best translation derivation
and yields target translation simultaneously.
Our experiments show that our model signif-
icantly outperforms the hierarchical phrase-
based model and the tree-to-string model on
English-Chinese Translation tasks.
1 Introduction
The hierarchical phrase-based model (Chiang, 2007)
makes an advance of statistical machine translation
by employing hierarchical phrases, which not only
uses phrases to learn local translations but also uses
hierarchical phrases to capture reorderings of words
and subphrases which can cover a large scope. Be-
sides, this model is formal syntax-based and does
not need to specify the syntactic constituents of
subphrases, so it can directly learn synchronous
context-free grammars (SCFG) from a parallel text
without relying on any linguistic annotations or as-
sumptions, which makes it used conveniently and
widely.
?This work was done when the first author visited Microsoft
Research Asia as an intern.
However, it is often desirable to consider syntac-
tic constituents of subphrases, e.g. the hierarchical
phrase
X ? ?X 1 for X 2 , X 2 de X 1 ?
can be applied to both of the following strings in
Figure 1
?A request for a purchase of shares?
?filed for bankruptcy?,
and get the following translation, respectively
?goumai gufen de shenqing?
?pochan de shenqing?.
In the former, ?A request? is a NP and this rule acts
correctly while in the latter ?filed? is a VP and this
rule gives a wrong reordering. If we specify the first
X on the right-hand side to NP, this kind of errors
can be avoided.
The tree-to-string model (Liu et al, 2006; Huang
et al, 2006) introduces linguistic syntax via source
parse to direct word reordering, especially long-
distance reordering. Furthermore, this model is for-
malised as Tree Substitution Grammars, so it ob-
serves syntactic cohesion. Syntactic cohesion means
that the translation of a string covered by a subtree
in a source parse tends to be continuous. Fox (2002)
shows that translation between English and French
satisfies cohesion in the majority cases. Many pre-
vious works show promising results with an as-
sumption that syntactic cohesion explains almost
all translation movement for some language pairs
(Wu, 1997; Yamada and Knight, 2001; Eisner, 2003;
Graehl and Knight, 2004; Quirk et al, 2005; Cherry,
2008; Feng et al, 2010).
950
But unfortunately, the tree-to-string model re-
quires each node must be strictly matched during
rule matching, which makes it strongly dependent
on the relationship of tree nodes and their roles in
the whole sentence. This will lead to data sparse-
ness and being vulnerable to parse errors.
In this paper, we present a hierarchical chunk-to-
string translation model to combine the merits of the
two models. Instead of parse trees, our model intro-
duces linguistic information in the form of chunks,
so it does not need to care the internal structures and
the roles in the main sentence of chunks. Based on
shallow parsing results, it learns rules consisting of
either words (terminals) or chunks (nonterminals),
where adjacent chunks are packed into one nonter-
minal. It searches for the best derivation through the
SCFG-motivated space defined by these rules and
get target translation simultaneously. In some sense,
our model can be seen as a compromise between
the hierarchical phrase-based model and the tree-to-
string model, specifically
? Compared with the hierarchical phrase-based
model, it integrates linguistic syntax and sat-
isfies syntactic cohesion.
? Compared with the tree-to-string model, it only
needs to perform shallow parsing which intro-
duces less parsing errors. Besides, our model
allows a nonterminal in a rule to cover several
chunks, which can alleviate data sparseness and
the influence of parsing errors.
? we refine our hierarchical chunk-to-string
model into two models: a loose model (Section
2.1) which is more similar to the hierarchical
phrase-based model and a tight model (Section
2.2) which is more similar to the tree-to-string
model.
The experiments show that on the 2008 NIST
English-Chinese MT translation test set, both the
loose model and the tight model outperform the hi-
erarchical phrase-based model and the tree-to-string
model, where the loose model has a better perfor-
mance. While in terms of speed, the tight model
runs faster and its speed ranking is between the tree-
to-string model and the hierarchical phrase-based
model.
NP IN NP IN NP VBD VP
A request for a purchase of shares was made
goumai gufen de shenqing bei dijiao
?? ?? ? ?? ? ??
(a)
NP VBZ VBN IN NP
The bank has filed for bankruptcy
gai yinhang yijing shenqing pochan
? ?? ?? ?? ??
(b)
Figure 1: A running example of two sentences. For each
sentence, the first row gives the chunk sequence.
S
NP
DT
The
NN
bank
VP
VBZ
has
VP
VBN
filed
PP
IN
for
NP
NN
bankruptcy
(a) A parse tree
B-NP I-NP B-VBZ B-VBN B-IN B-NP
The bank has filed for bankruptcy
(b) A chunk sequence got from the parse tree
Figure 2: An example of shallow parsing.
2 Modeling
Shallow parsing (also chunking) is an analysis of
a sentence which identifies the constituents (noun
groups, verbs, verb groups, etc), but neither spec-
ifies their internal structures, nor their roles in the
main sentence. In Figure 1, we give the chunk se-
quence in the first row for each sentence. We treat
shallow parsing as a sequence label task, and a sen-
tence f can have many possible different chunk la-
bel sequences. Therefore, in theory, the conditional
probability of a target translation e conditioned on
the source sentence f is given by taking the chunk
label sequences as a latent variable c:
p(e|f) =
?
c
p(c|f)p(e|f , c) (1)
951
In practice, we only take the best chunk label se-
quence c? got by
c? = argmax
c
p(c|f) (2)
Then we can ignore the conditional probability
p(c?|f) as it holds the same value for each transla-
tion, and get:
p(e|f) = p(c?|f)p(e|f , c?)
= p(e|f , c?) (3)
We formalize our model as a weighted SCFG.
In a SCFG, each rule (usually called production in
SCFGs) has an aligned pair of right-hand sides ?
the source side and the target side, just as follows:
X ? ??, ?,??
where X is a nonterminal, ? and ? are both strings of
terminals and nonterminals, and ? denotes one-to-
one links between nonterminal occurrences in ? and
nonterminal occurrences in ?. A SCFG produces a
derivation by starting with a pair of start symbols
and recursively rewrites every two coindexed non-
terminals with the corresponding components of a
matched rule. A derivation yields a pair of strings
on the right-hand side which are translation of each
other.
In a weighted SCFG, each rule has a weight and
the total weight of a derivation is the production
of the weights of the rules used by the derivation.
A translation may be produced by many different
derivations and we only use the best derivation to
evaluate its probability. With d denoting a deriva-
tion and r denoting a rule, we have
p(e|f) = max
d
p(d,e|f , c?)
= max
d
?
r?d
p(r,e|f , c?) (4)
Following Och and Ney (2002), we frame our model
as a log-linear model:
p(e|f) = exp
?
k ?kHk(d,e, c?,f)
exp
?
d?,e?,k ?kHk(d?,e?, c?,f)
(5)
where Hk(d,e, c?,f) =
?
r
hk(f , c?, r)
So the best translation is given by
e? = argmax
e
?
k
?kHk(d,e, c?,f) (6)
We employ the same set of features for the log-
linear model as the hierarchical phrase-based model
does(Chiang, 2005).
We further refine our hierarchical chunk-to-string
model into two models: a loose model which is more
similar to the hierarchical phrase-based model and
a tight model which is more similar to the tree-to-
string model. The two models differ in the form of
rules and the way of estimating rule probabilities.
While for decoding, we employ the same decoding
algorithm for the two models: given a test sentence,
the decoders first perform shallow parsing to get the
best chunk sequence, then apply a CYK parsing al-
gorithm with beam search.
2.1 A Loose Model
In our model, we employ rules containing non-
terminals to handle long-distance reordering where
boundary words play an important role. So for the
subphrases which cover more than one chunk, we
just maintain boundary chunks: we bundle adjacent
chunks into one nonterminal and denote it as the first
chunk tag immediately followed by ?-? and next fol-
lowed by the last chunk tag. Then, for the string pair
<filed for bankruptcy, shenqing pochan>, we can
get the rule
r1 : X ? ?VBN 1 for NP 2 , VBN 1 NP 2 ?
while for the string pair <A request for a purchase
of shares, goumai gufen de shenqing>, we can get
r2 : X ? ?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?.
The rule matching ?A request for a purchase of
shares was? will be
r3 : X ? ?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.
We can see that in contrast to the method of rep-
resenting each chunk separately, this representation
form can alleviate data sparseness and the influence
of parsing errors.
952
?S 1 , S 1 ? ? ?S 2 X 3 , S 2 X 3 ?
? ?X 4 X 3 , X 4 X 3 ?
? ?NP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 ?
? ?NP 7 for NP-NP 8 VBD 6 X 3 , NP-NP 8 de NP 7 VBD 6 X 3 ?
? ?A request for NP-NP 8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 ?
? ?A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ?
? ?A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ?
? ?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(a) The loose model
?NP-VP 1 , NP-VP 1 ? ? ?NP-VBD 2 VP 3 , NP-VBD 2 VP 3 ?
? ?NP-NP 4 VBD 5 VP 3 , NP-NP 4 VBD 5 VP 3 ?
? ?NP 6 for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de NP 6 VBD 5 VP 3 ?
? ?A request for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de shenqing VBD 5 VP 3 ?
? ?A request for a purchase of shares VBD 5 VP 3 , goumai gufen de shenqing VBD 5 VP 3 ?
? ?A request for a purchase of shares was VP 3 , goumai gufen de shenqing bei VP 3 ?
? ?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(b) The tight model
Figure 3: The derivations of the sentence in Figure 1(a).
In these rules, the left-hand nonterminal symbol X
can not match any nonterminal symbol on the right-
hand side. So we need a set of rules such as
NP ? ?X 1 , X 1 ?
NP-NP ? ?X 1 , X 1 ?
and so on, and set the probabilities of these rules to
1. To simplify the derivation, we discard this kind of
rules and assume that X can match any nonterminal
on the right-hand side.
Only with r2 and r3, we cannot produce any
derivation of the whole sentence in Figure 1 (a). In
this case we need two special glue rules:
r4 : S ? ?S 1 X 2 , S 1 X 2 ?
r5 : S ? ?X 1 , X 1 ?
Together with the following four lexical rules,
r6 : X ? ?a request, shenqing?
r7 : X ? ?a purchase of shares, goumai gufen?
r8 : X ? ?was, bei?
r9 : X ? ?made, dijiao?
Figure 3(a) shows the derivation of the sentence in
Figure 1(a).
2.2 A Tight Model
In the tight model, the right-hand side of each rule
remains the same as the loose model, but the left-
hand side nonterminal is not X but the correspond-
ing chunk labels. If a rule covers more than one
chunk, we just use the first and the last chunk la-
bels to denote the left-hand side nonterminal. The
rule set used in the tight model for the example in
Figure 1(a) corresponding to that in the loose model
becomes:
r2 : NP-NP ? ?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?
r3 : NP-VBD ? ?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.
r6 : NP ? ?a request, shenqing?
r7 : NP-NP ? ?a purchase of shares, goumai gufen?
r8 : VBD ? ?was, bei?
r9 : VP ? ?made, dijiao?
During decoding, we first collect rules for each
span. For a span which does not have any matching
rule, if we do not construct default rules for it, there
will be no derivation for the whole sentence, then we
need to construct default rules for this kind of span
by enumerating all possible binary segmentation of
the chunks in this span. For the example in Figure
1(a), there is no rule matching the whole sentence,
953
so we need to construct default rules for it, which
should be
NP-VP ? ?NP-VBD 1 VP 2 , NP-VBD 1 VP 2 ?.
NP-VP ? ?NP-NP 1 VBD-VP 2 , NP-NP 1 VBD-VP 2 ?.
and so on.
Figure 3(b) shows the derivation of the sentence
in Figure 1(a).
3 Shallow Parsing
In a parse tree, a chunk is defined by a leaf node or
an inner node whose children are all leaf nodes (See
Figure 2 (a)). In our model, we identify chunks by
traversing a parse tree in a breadth-first order. Once
a node is recognized as a chunk, we skip its children.
In this way, we can get a sole chunk sequence given
a parse tree. Then we label each word with a label
indicating whether the word starts a chunk (B-) or
continues a chunk (I-). Figure 2(a) gives an example.
In this method, we get the training data for shallow
parsing from Penn Tree Bank.
We take shallow Parsing (chunking) as a sequence
label task and employ Conditional Random Field
(CRF)1 to train a chunker. CRF is a good choice for
label tasks as it can avoid label bias and use more
statistical correlated features. We employ the fea-
tures described in Sha and Pereira (2003) for CRF.
We do not introduce CRF-based chunkier in this pa-
per and more details can be got from Hammersley
and Clifford (1971), Lafferty et al (2001), Taskar et
al. (2002), Sha and Pereira (2003).
4 Rule Extraction
In what follows, we introduce how to get the rule
set. We learn rules from a corpus that first is bi-
directionally word-aligned by the GIZA++ toolkit
(Och and Ney, 2000) and then is refined using a
?final-and? strategy. We generate the rule set in two
steps: first, we extract two sets of phrases, basic
phrases and chunk-based phrases. Basic phrases are
defined using the same heuristic as previous systems
(Koehn et al, 2003; Och and Ney, 2004; Chiang,
2005). A chunk-based phrase is such a basic phrase
that covers one or more chunks on the source side.
1We use the open source toolkit CRF++ got in
http://code.google.com/p/crfpp/ .
We identity chunk-based phrases ?cj2j1 ,f
j2
j1 ,e
i2
i1? as
follows:
1. A chunk-based phrase is a basic phrase;
2. cj1 begins with ?B-?;
3. fj2 is the end word on the source side or cj2+1
does not begins with ?I-?.
Given a sentence pair ?f ,e,??, we extract rules for
the loose model as follows
1. If ?f j2j1 ,e
i2
i1? is a basic phrase, then we can have
a rule
X ? ?f j2j1 ,e
i2
i1?
2. Assume X ? ??, ?? is a rule with ? =
?1f j2j1 ?2 and ? = ?1e
i2
i1?2, and ?f
j2
j1 ,e
i2
i1? is
a chunk-based phrase with a chunk sequence
Yu ? ? ?Yv, then we have the following rule
X ? ??1Yu-Yv k ?2, ?1Yu-Yv k ?2?.
We evaluate the distribution of these rules in the
same way as Chiang (2007).
We extract rules for the tight model as follows
1. If ?f j2j1 ,e
i2
i1? is a chunk-based phrase with a
chunk sequence Ys ? ? ?Yt, then we can have a
rule
Ys-Yt ? ?f j2j1 ,e
i2
i1?
2. Assume Ys-Yt ? ??, ?? is a rule with ? =
?1f j2j1 ?2 and ? = ?1e
i2
i1?2, and ?f
j2
j1 ,e
i2
i1? is
a chunk-based phrase with a chunk sequence
Yu ? ? ?Yv, then we have the following rule
Ys-Yt ? ??1Yu-Yv k ?2, ?1Yu-Yv k ?2?.
We evaluate the distribution of rules in the same way
as Liu et al (2006).
For the loose model, the nonterminals must be co-
hesive, while the whole rule can be noncohesive: if
both ends of a rule are nonterminals, the whole rule
is cohesive, otherwise, it may be noncohesive. In
contrast, for the tight model, both the whole rule and
the nonterminal are cohesive.
Even with the cohesion constraints, our model
still generates a large number of rules, but not all
954
of the rules are useful for translation. So we follow
the method described in Chiang (2007) to filter the
rule set except that we allow two nonterminals to be
adjacent.
5 Related Works
Watanabe et al (2003) presented a chunk-to-string
translation model where the decoder generates a
translation by first translating the words in each
chunk, then reordering the translation of chunks.
Our model distinguishes from their model mainly
in reordering model. Their model reorders chunks
resorting to a distortion model while our model re-
orders chunks according to SCFG rules which retain
the relative positions of chunks.
Nguyen et al (2008) presented a tree-to-string
phrase-based method which is based on SCFGs.
This method generates SCFGs through syntac-
tic transformation including a word-to-phrase tree
transformation model and a phrase reordering model
while our model learns SCFG-based rules from
word-aligned bilingual corpus directly
There are also some works aiming to introduce
linguistic knowledge into the hierarchical phrase-
based model. Marton and Resnik (2008) took the
source parse tree into account and added soft con-
straints to hierarchical phrase-based model. Cherry
(2008) used dependency tree to add syntactic cohe-
sion. These methods work with the original SCFG
defined by hierarchical phrase-based model and use
linguistic knowledge to assist translation. Instead,
our model works under the new defined SCFG with
chunks.
Besides, some other researchers make efforts on
the tree-to-string model by employing exponentially
alternative parses to alleviate the drawback of 1-best
parse. Mi et al (2008) presented forest-based trans-
lation where the decoder translates a packed forest
of exponentially many parses instead of i-best parse.
Liu and Liu (2010) proposed to parse and to trans-
late jointly by taking tree-based translation as pars-
ing. Given a source sentence, this decoder produces
a parse tree on the source side and a translation on
the target side simultaneously. Both the models per-
form in the unit of tree nodes rather than chunks.
6 Experiments
6.1 Data Preparation
Data for shallow parsing We got training data and
test data for shallow parsing from the standard Penn
Tree Bank (PTB) English parsing task by splitting
the sections 02-21 on the Wall Street Journal Portion
(Marcus et al, 1993) into two sets: the last 1000
sentences as the test set and the rest as the training
set. We filtered the features whose frequency was
lower than 3 and substituted ?? and ?? with ? to
keep consistent with translation data. We used L2
algorithm to train CRF.
Data for Translation We used the NIST training
set for Chinese-English translation tasks excluding
the Hong Kong Law and Hong Kong Hansard2 as the
training data, which contains 470K sentence pairs.
For the training data set, we first performed word
alignment in both directions using GIZA++ toolkit
(Och and Ney, 2000) then refined the alignments
using ?final-and?. We trained a 5-gram language
model with modified Kneser-Ney smoothing on the
Xinhua portion of LDC Chinese Gigaword corpus.
For the tree-to-string model, we parsed English sen-
tences using Stanford parser and extracted rules us-
ing the GHKM algorithm (Galley et al, 2004).
We used our in-house English-Chinese data set
as the development set and used the 2008 NIST
English-Chinese MT test set (1859 sentences) as the
test set. Our evaluation metric was BLEU-4 (Pap-
ineni et al, 2002) based on characters (as the tar-
get language is Chinese), which performed case-
insensitive matching of n-grams up to n = 4 and
used the shortest reference for the brevity penalty.
We used the standard minimum error-rate training
(Och, 2003) to tune the feature weights to maximize
the BLEU score on the development set.
6.2 Shallow Parsing
The standard evaluation metrics for shallow parsing
are precision P, recall R, and their harmonic mean
F1 score, given by:
P = number of exactly recognized chunks
number of output chunks
R = number of exactly recognized chunks
number of reference chunks
2The source side and target side are reversed.
955
Word number Chunk number Accuracy %
23861 12258 94.48
Chunk type P % R % F1 % Found
All 91.14 91.35 91.25 12286
One 90.32 90.99 90.65 5236
NP 93.97 94.47 94.22 5523
ADVP 82.53 84.30 83.40 475
VP 93.66 92.04 92.84 284
ADJP 65.68 69.20 67.39 236
WHNP 96.30 95.79 96.04 189
QP 83.06 80.00 81.50 183
Table 1: Shallow parsing result. The collum Found gives
the number of chunks recognized by CRF, the row All
represents all types of chunks, and the row One represents
the chunks that consist of one word.
F1 =
2 ? P ? R
P +R
Besides, we need another metric, accuracy A, to
evaluate the accurate rate of individual labeling de-
cisions of every word as
A = number of exactly labeled words
number of words
For example, given a reference sequence
B-NP I-NP I-NP B-VP I-VP B-VP, CRF out-
puts a sequence O-NP I-NP I-NP B-VP I-VP I-NP,
then P = 33.33%, A = 66.67%.
Table 1 summaries the results of shallow parsing.
For ?? and ?? were substituted with ? , the perfor-
mance was slightly influenced.
The F1 score of all chunks is 91.25% and the F1
score of One and NP, which in number account for
about 90% of chunks, is 90.65% and 94.22% respec-
tively. F score of NP chunking approaches 94.38%
given in Sha and Pereira (2003).
6.3 Performance Comparison
We compared our loose decoder and tight decoder
with our in-house hierarchical phrase-based decoder
(Chiang, 2007) and the tree-to-string decoder (Liu et
al., 2006). We set the same configuration for all the
decoders as follows: stack size = 30, nbest size = 30.
For the hierarchical chunk-based and phrase-based
decoders, we set max rule length to 5. For the tree-
to-string decoder, we set the configuration of rule
System Dev NIST08 Speed
phrase 0.2843 0.3921 1.163
tree 0.2786 0.3817 1.107
tight 0.2914 0.3987 1.208
loose 0.2936 0.4023 1.429
Table 2: Performance comparison. Phrase represents
the hierarchical phrase-based decoder, tree represents the
tree-to-string decoder, tight represents our tight decoder
and loose represents our loose decoder. The speed is re-
ported by seconds per sentence. The speed for the tree-to-
string decoder includes the parsing time (0.23s) and the
speed for the tight and loose models includes the shallow
parsing time, too.
extraction as: the height up to 3 and the number of
leaf nodes up to 5.
We give the results in Table 2. From the results,
we can see that both the loose and tight decoders
outperform the baseline decoders and the improve-
ment is significant using the sign-test of Collins et
al. (2005) (p < 0.01). Specifically, the loose model
has a better performance while the tight model has a
faster speed.
Compared with the hierarchical phrase-based
model, the loose model only imposes syntactic cohe-
sion cohesion to nonterminals while the tight model
imposes syntax cohesion to both rules and nonter-
minals which reduces search space, so it decoders
faster. We can conclude that linguistic syntax can
indeed improve the translation performance; syntac-
tic cohesion for nonterminals can explain linguis-
tic phenomena well; noncohesive rules are useful,
too. The extra time consumption against hierarchi-
cal phrase-based system comes from shallow pars-
ing.
By investigating the translation result, we find that
our decoder does well in rule selection. For exam-
ple, in the hierarchical phrase-based model, this kind
of rules, such as
X ? ?X of X, ??, X ? ?X for X, ??
and so on, where ? stands for the target component,
are used with a loose restriction as long as the ter-
minals are matched, while our models employ more
stringent constraints on these rules by specifying the
syntactic constituent of ?X?. With chunk labels, our
models can make different treatment for different
situations.
956
System Dev NIST08 Speed
cohesive 0.2936 0.4023 1.429
noncohesive 0.2937 0.3964 1.734
Table 3: Influence of cohesion. The row cohesive rep-
resents the loose system where nonterminals satisfy co-
hesion, and the row noncohesive represents the modified
version of the loose system where nonterminals can be
noncohesive.
Compared with the tree-to-string model, the re-
sult indicates that the change of the source-side lin-
guistic syntax from parses to chunks can improve
translation performance. The reasons should be our
model can reduce parse errors and it is enough to use
chunks as the basic unit for machine translation. Al-
though our decoders and tree-to-string decoder all
run in linear-time with beam search, tree-to-string
model runs faster for it searches through a smaller
SCFG-motivated space.
6.4 Influence of Cohesion
We verify the influence of syntax cohesion via the
loose model. The cohesive model imposes syntax
cohesion on nonterminals to ensure the chunk is re-
ordered as a whole. In this experiment, we introduce
a noncohesive model by allowing a nonterminal to
match part of a chunk. For example, in the nonco-
hesive model, it is legal for a rule with the source
side
?NP for NP-NP?
to match
?request for a purchase of shares?
in Figure 1 (a), where ?request? is part of NP. As
well, the rule with the source side
?NP for a NP-NP?
can match
?request for a purchase of shares?.
In this way, we can ensure all the rules used in the
cohesive system can be used in the noncohesive sys-
tem. Besides cohesive rules, the noncohesive system
can use noncohesive rules, too.
We give the results in Table 3. From the results,
we can see that cohesion helps to reduce search
space, so the cohesive system decodes faster. The
noncohesive system decoder slower, as it employs
System Number Dev NIST08 Speed
loose two 0.2936 0.4023 1.429
loose three 0.2978 0.4037 2.056
tight two 0.2914 0.3987 1.208
tight three 0.2954 0.4026 1.780
Table 4: The influence of the number of nonterminals.
The column number lists the number of nonterminals
used at most in a rule.
more rules, but this does not bring any improvement
of translation performance. As other researches said
in their papers, syntax cohesion can explain linguis-
tic phenomena well.
6.5 Influence of the number of nonterminals
We also tried to allow a rule to hold three nonter-
minals at most. We give the result in Table 4. The
result shows that using three nonterminals does not
bring a significant improvement of translation per-
formance but quite more time consumption. So we
only retain two nonterminals at most in a rule.
7 Conclusion
In this paper, we present a hierarchical chunk-
to-string model for statistical machine translation
which can be seen as a compromise of the hierarchi-
cal phrase-based model and the tree-to-string model.
With the help of shallow parsing, our model learns
rules consisting of either words or chunks and com-
presses adjacent chunks in a rule to a nonterminal,
then it searches for the best derivation under the
SCFG defined by these rules. Our model can com-
bine the merits of both the models: employing lin-
guistic syntax to direct decoding, being syntax co-
hesive and robust to parsing errors. We refine the hi-
erarchical chunk-to-string model into two models: a
loose model (more similar to the hierarchical phrase-
based model) and a tight model (more similar to the
tree-to-string model).
Our experiments show that our decoder can im-
prove translation performance significantly over the
hierarchical phrase-based decoder and the tree-to-
string decoder. Besides, the loose model gives a bet-
ter performance while the tight model gives a faster
speed.
957
8 Acknowledgements
We would like to thank Trevor Cohn, Shujie Liu,
Nan Duan, Lei Cui and Mo Yu for their help,
and anonymous reviewers for their valuable com-
ments and suggestions. This work was supported
in part by EPSRC grant EP/I034750/1 and in part
by High Technology R&D Program Project No.
2011AA01A207.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. of ACL, pages
72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL, pages
205?208.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling:Posters,
pages 285?293.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304?
3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. of HLT-NAACL, pages 105?112.
J Hammersley and P Clifford. 1971. Markov fields on
finite graphs and lattices. In Unpublished manuscript.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML, pages 282?289.
Yang Liu and Qun Liu. 2010. Joint parsing and transla-
tion. In Proc. of COLING, pages 707?715.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of COLING-ACL, pages 609?616.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19:313?330.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. of ACL, pages 1003?1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho,
Minh Le Nguyen, and Vinh Van Nguyen. 2008. A
tree-to-string phrase-based model for statistical ma-
chine translation. In Proc. of CoNLL, pages 143?150.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL, pages 295?
302.
Frans J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417?449.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL, pages 271?279.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of HLT-
NAACL, pages 134?141.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Eighteenth Conference on Uncertainty in Artificial
Intelligence.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno.
2003. Chunk-based statistical translation. In Proc. of
ACL, pages 303?310.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL, pages
523?530.
958
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 333?342,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Markov Model of Machine Translation using
Non-parametric Bayesian Inference
Yang Feng and Trevor Cohn
Department of Computer Science
The University of Sheffield
Sheffield, United Kingdom
yangfeng145@gmail.com and t.cohn@sheffield.ac.uk
Abstract
Most modern machine translation systems
use phrase pairs as translation units, al-
lowing for accurate modelling of phrase-
internal translation and reordering. How-
ever phrase-based approaches are much
less able to model sentence level effects
between different phrase-pairs. We pro-
pose a new model to address this im-
balance, based on a word-based Markov
model of translation which generates tar-
get translations left-to-right. Our model
encodes word and phrase level phenom-
ena by conditioning translation decisions
on previous decisions and uses a hierar-
chical Pitman-Yor Process prior to pro-
vide dynamic adaptive smoothing. This
mechanism implicitly supports not only
traditional phrase pairs, but also gapping
phrases which are non-consecutive in the
source. Our experiments on Chinese to
English and Arabic to English translation
show consistent improvements over com-
petitive baselines, of up to +3.4 BLEU.
1 Introduction
Recent years have witnessed burgeoning develop-
ment of statistical machine translation research,
notably phrase-based (Koehn et al, 2003) and
syntax-based approaches (Chiang, 2005; Galley
et al, 2006; Liu et al, 2006). These approaches
model sentence translation as a sequence of sim-
ple translation decisions, such as the application
of a phrase translation in phrase-based methods
or a grammar rule in syntax-based approaches.
In order to simplify modelling, most MT mod-
els make an independence assumption, stating that
the translation decisions in a derivation are in-
dependent of one another. This conflicts with
the intuition behind phrase-based MT, namely that
translation decisions should be dependent on con-
text. On one hand, the use of phrases can mem-
orize local context and hence helps to generate
better translation compared to word-based models
(Brown et al, 1993; Och and Ney, 2003). On the
other hand, this mechanism requires each phrase
to be matched strictly and to be used as a whole,
which precludes the use of discontinuous phrases
and leads to poor generalisation to unseen data
(where large phrases tend not to match).
In this paper we propose a new model to drop
the independence assumption, by instead mod-
elling correlations between translation decisions,
which we use to induce translation derivations
from aligned sentences (akin to word alignment).
We develop a Markov model over translation de-
cisions, in which each decision is conditioned on
previous n most recent decisions. Our approach
employs a sophisticated Bayesian non-parametric
prior, namely the hierarchical Pitman-Yor Process
(Teh, 2006; Teh et al, 2006) to represent back-
off from larger to smaller contexts. As a result,
we need only use very simple translation units
? primarily single words, but can still describe
complex multi-word units through correlations be-
tween their component translation decisions. We
further decompose the process of generating each
target word into component factors: finishing the
translating, jumping elsewhere in the source, emit-
ting a target word and deciding the fertility of the
source words.
Overall our model has the following features:
1. enabling model parameters to be shared be-
tween similar translation decisions, thereby
obtaining more reliable statistics and gener-
alizing better from small training sets.
2. learning a much richer set of transla-
tion fragments, such as gapping phrases,
e.g., the translation for the German werde
. . . ankommen in English is will arrive . . . .
3. providing a unifying framework spanning
word-based and phrase-based model of trans-
lation, while incorporating explicit transla-
333
tion, insertion, deletion and reordering com-
ponents.
We demonstrate our model on Chinese-English
and Arabic-English translation datasets. The
model produces uniformly better translations than
those of a competitive phrase-based baseline,
amounting to an improvement of up to 3.4 BLEU
points absolute.
2 Related Work
Word based models have a long history in machine
translation, starting with the venerable IBM trans-
lation models (Brown et al, 1993) and the hid-
den Markov model (Vogel et al, 1996). These
models are still in wide-spread use today, albeit
only as a preprocessing step for inferring word
level alignments from sentence-aligned parallel
corpora. They combine a number of factors, in-
cluding distortion and fertility, which have been
shown to improve word-alignment and translation
performance over simpler models. Our approach
is similar to these works, as we also develop a
word-based model, and explicitly consider simi-
lar translation decisions, alignment jumps and fer-
tility. We extend these works in two important
respects: 1) while they assume a simple parame-
terisation by making iid assumptions about each
translation factor, we instead allow for rich cor-
relations by modelling sequences of translation
decisions; and 2) we develop our model in the
Bayesian framework, using a hierarchical Pitman-
Yor Process prior with rich backoff semantics be-
tween high and lower order sequences of transla-
tion decisions. Together this results in a model
with rich expressiveness but can still generalize
well to unseen data.
More recently, a number of authors have pro-
posed Markov models for machine translation.
Vaswani et al (2011) propose a rule Markov
model for a tree-to-string model which models
correlations between pairs of mininal rules, and
use Kneser-Ney smoothing to alleviate the prob-
lems of data sparsity. Similarly, Crego et al
(2011) develop a bilingual language model which
incorporates words in the source and target lan-
guages to predict the next unit, which they use as
a feature in a translation system. This line of work
was extended by Le et al (2012) who develop a
novel estimation algorithm based around discrimi-
native projection into continuous spaces. Also rel-
evant is Durrani et al (2011), who present a se-
quence model of translation including reordering.
Our work also uses bilingual information, using
the source words as part of the conditioning con-
text. In contrast to these approaches which pri-
marily address the decoding problem, we focus on
the learning problem of inferring alignments from
parallel sentences. Additionally, we develop a full
generative model using a Bayesian prior, and in-
corporate additional factors besides lexical items,
namely jumps in the source and word fertility.
Another aspect of this paper is the implicit sup-
port for phrase-pairs that are discontinous in the
source language. This idea has been developed
explicitly in a number of previous approaches, in
grammar based (Chiang, 2005) and phrase-based
systems (Galley and Manning, 2010). The latter is
most similar to this paper, and shows that discon-
tinuous phrases compliment standard contiguous
phrases, improving expressiveness and translation
performance. Unlike their work, here we develop
a complimentary approach by constructing a gen-
erative model which can induce these rich rules
directly from sentence-aligned corpora.
3 Model
Given a source sentence, our model infers a la-
tent derivation which produces a target translation
and meanwhile gives a word alignment between
the source and the target. We consider a pro-
cess in which the target string is generated using
a left-to-right order, similar to the decoding strat-
egy used by phrase-based machine translation sys-
tems (Koehn et al, 2003). During this process we
maintain a position in the source sentence, which
can jump around to allow for different sentence
ordering in the target vs. source languages. In
contrast to phrase-based models, we use words as
our basic translation unit, rather than multi-word
phrases. Furthermore, we decompose the deci-
sions involved in generating each target word to
a number of separate factors, where each factor is
modelled separately and conditioned on a rich his-
tory of recent translation decisions.
3.1 Markov Translation
Our model generates target translation left-to-
right word by word. The generative process
employs the following recursive procedure to
construct the target sentence conditioned on the
source:
i? 1
while Not finished do
Decide whether to finish the translation, ?i
334
Step Source sentence Translation finish jump emission
0 Je le prends
1 Je le prends I no monotone Je? I
2 Je le prends I ?ll no insert null? ?ll
3 Je le prends I ?ll take no forward prends? take
4 Je le prends I ?ll take that no backward le? that
5 Je le prends I ?ll take that one no stay le? one
6 Je le prends I ?ll take that one yes
Figure 1: Translation agenda of Je le prends? I ?ll take that one.
if ?i = false then
Select a source word to jump to
Emit a target word for the source word
end if
i? i+ 1
end while
In the generation of each target word, our model
includes three separate factors: the binary finish
decision, a jump decision to move to a different
source word, and emission which translates or oth-
erwise inserts a word in the target string. This gen-
erative process resembles the sequence of transla-
tion decisions considered by a standard MT de-
coder (Koehn et al, 2003), but note that our ap-
proach differs in that there is no constraint that all
words are translated exactly once. Instead source
words can be skipped or repeatedly translated.
This makes the approach more suitable for learn-
ing alignments, e.g., to account for word fertilities
(see ?3.3), while also permitting inference using
Gibbs sampling (?4).
More formally, we can express our probabilistic
model as
pbs(eI1, aI1|fJ1 ) =
I+1?
i=1
p(?i|f i?1ai?n, e
i?1
i?n)
?
I?
i=1
p(?i|f i?1ai?n, e
i?1
i?n)
?
I?
i=1
p(ei|?i, f iai?n, ei?1i?n) (1)
where ?i is the finish decision for target posi-
tion i, ?i is the jump decision to source word fai
and f iai?n is the source words for target positions
i ? n, i ? n + 1, ..., i. Each of the three distribu-
tions (finish, jump and emission) is drawn respec-
tive from hierarchical Pitman-Yor Process priors,
as described in Section 3.2.
The jump decision ?i in Equation 1 demands
further explanation. Instead of modelling jump
distances explicitly, which poses problems for
generalizing between different lengths of sen-
tences and general parameter explosion, we con-
sider a small handful of types of jump based on
the distance between the current source word ai
and the previous source word ai?1, i.e., di =
ai ? ai?1.1 We bin jumps into five types:
a) insert;
b) backward, if di < 0;
c) stay, if di = 0;
d) monotone, if di = 1;
e) forward, if di > 1.
The special jump type insert handles null align-
ments, denoted ai = 0 which licence spurious in-
sertions in the target string.
To illustrate this translation process, Figure 1
shows the example translation <Je le prends, I ?ll
take that one>. Initially we set the source position
before the first source word Je. Then in step 1,
we decide not to finish (finish=no), jump to source
word Je and translate it as I. Next, we again de-
cide not to finish, jump to the null source word
and insert ?ll. The process continues until in step
6 we elect to finish (finish=yes), at which point the
translation is complete, with target string I ?ll take
that one.
3.2 Hierarchical Pitman-Yor Process
The Markov assumption limits the context of each
distribution to the n most recent translation deci-
sions, which limits the number of model param-
eters. However for any non-trivial value n >
0, overfitting is a serious concern. We counter
the problem of a large parameter space using a
Bayesian non-parametric prior, namely the hier-
archical Pitman-Yor Process (PYP). The PYP de-
scribes distributions over possibly infinite event
spaces that follow a power law, with few events
taking the majority of the probability mass and a
long tail of less frequent events. We consider a hi-
erarchical PYP, where a sequence of chained PYP
1For a target position aligned to null, we denote its source
word as null and set its aligned source position as that of the
previous target word that is aligned to non-null.
335
priors allow backoff from larger to smaller con-
texts such that our model can learn rich contextual
models for known (large) contexts while also still
being able to generalize well to unseen contexts
(using smaller histories).
3.2.1 Pitman-Yor Process
A PYP (Pitman and Yor, 1997) is defined by its
discount parameter 0 ? a < 1, strength parameter
b > ?a and base distribution G0. For a distri-
bution drawn from a PYP, G ? PYP(a, b,G0),
marginalising out G leads to a simple distribution
which can be described using a variant of the Chi-
nese Restaurant Process (CRP). In this analogy we
imagine a restaurant has an infinite number of ta-
bles and each table can accommodate an infinite
number of customers. Each customer (a sample
from G) walks in one at a time and seats them-
selves at a table. Finally each table is served a
communal dish (a draw from G0), which is served
to each customer seated at the table. The assign-
ment of customers to tables is such that popular
tables are more likely to be chosen, and this rich-
get-richer dynamic produces power-law distribu-
tions with few events (the dishes at popular tables)
dominating the distribution.
More formally, at time n a customer enters and
selects a table k which is either a table having been
seated (1 ? k ? K?) or an empty table (k =
K? + 1) by
p(tn = k|t?n) =
{
c?tk?a
n?1+b 1 ? k ? K?
aK?+b
n?1+b k = K? + 1
where tn is the table selected by the customer n,
t?n is the seating arrangement of previous n ? 1
customers, c?tk is the number of customers seatedat table k in t?n andK? = K(t?n) is the number
of tables in t?n.
If the customer sits at an empty table, a dish h
is served to his table by the probability of G0(h),
otherwise, he can only share with others the dish
having been served to his table.2 Overall, the prob-
ability of the customer being served a dish h is
p(on = h|t?n,o?n) =
c?oh ? aK?h
n? 1 + b
+ (aK
? + b)
n? 1 + b G0(h)
where on is the dish served to the customer n, o?n
is the dish accommodation of previous n? 1 cus-
tomers, c?oh is the number of customers who are
2We also say the customer is served with this dish.
served with the dish h in o?n and K?h is the num-ber of tables served with the dish h in t?n.
The hierarchical PYP (hPYP; Teh (2006)) is an
extension of the PYP in which the base distribu-
tion G0 is itself a PYP distribution. This parent
(base) distribution can itself have a PYP as a base
distribution, giving rise to hierarchies of arbitrary
depth. Like the PYP, inference under the hPYP
can be also described in terms of CRP whereby
each table in one restaurant corresponds to a dish
in the next deeper level, and is said to share the
same dish. Whenever an empty table is seated in
one level, a customer must enter the restaurant in
the next deeper level and find a table to sit. This
process continues until the customer is assigned a
shared table or the deepest level of the hierarchy
is reached. A similar process occurs when a cus-
tomer leaves, where newly emptied tables must be
propagated up the hierarchy in the form of depart-
ing customers. There is not space for a complete
treatment of the hPYP and the particulars of infer-
ence; we refer the interested reader to Teh (2006).
3.2.2 A Hierarchical PYP Translation Model
We draw the distributions for the various transla-
tion factors from respective hierarchical PYP pri-
ors, as shown in Figure 2 for the finish, jump and
emission factors. For the emission factor (Fig-
ure 2c), we draw the target word ei from a distribu-
tion conditioned on the last two source and target
words, as well as the current source word, fai and
the current jump type ?i. Here the draw of a tar-
get word corresponds to a customer entering and
which target word to emit corresponds to which
dish to be served to the customer in the CRP. The
hierarchical prior encodes a backoff path in which
the jump type is dropped first, followed by pairs of
source and target words from least recent to most
recent. The final backoff stages drop the current
source word, terminating with the uniform base
distribution over the target vocabulary V .
The distributions over the other two factors in
Figure 2 follow a similar pattern. Note however
that these distributions don?t condition on the cur-
rent source word, and consequently have fewer
levels of backoff. The terminating base distribu-
tion for the finish factor is a uniform distribution
with equal probability for finishing versus contin-
uing. The jump factor has an additional condition-
ing variable t which encodes whether the previous
alignment is near the start or end of the source sen-
tence. This information affects which of the jump
values are legal from the current position, such
336
?i|f i?1ai?2, ei?1i?2 ? G?f i?1ai?2,ei?1i?2
G?f i?1ai?2,ei?1i?2 ? PYP(a
?3, b?3, G?fai?1,ei?1)
G?fai?1,ei?1 ? PYP(a?2, b?2, G?)
G? ? PYP(a?1, b?1, G?0)
G?0 ? U(12)
(a) Finish factor
?i|f i?1ai?2, ei?1i?2, t ? G?f i?1ai?2,ei?1i?2,t
G?f i?1ai?2,ei?1i?2,t ? PYP(a
?3 , b?3 , G?fai?1,ei?1,t)
G?fai?1,ei?1,t ? PYP(a?2 , b?2 , G?t )
G?t ? PYP(a?1 , b?1 , G?0,t)
G?0,t ? U
(b) Jump factor
ei|?i, f iai?2, ei?1i?2 ?Ge?i,f iai?2,ei?1i?2
Ge?i,f iai?2,ei?1i?2 ? PYP(a
e5, be5, Gef iai?2,ei?1i?2)
Gef iai?2,ei?1i?2 ? PYP(a
e4, be4, Gef iai?1,ei?1)
Gef iai?1,ei?1 ? PYP(a
e3, be3, Gefai)
Gefai ? PYP(ae2, be2, Ge)
Ge ? PYP(ae1, be1, Ge0)
Ge0 ? U( 1|V |)
(c) Emission factor
Figure 2: Distributions over the translation factors and their hierarchical priors.
that a jump could not go outside the bounds of the
source sentence. Accordingly we maintain sepa-
rate distributions for each setting, and each has a
different uniform base distribution parameterized
according to the number of possible jump types.
3.3 Fertility
For each target position, our Markov model may
select a source word which has been covered,
which means a source word may be linked to sev-
eral target positions. Therefore, we introduce fer-
tility to denote the number of target positions a
source word is linked to in a sentence pair. Brown
et al (1993) have demonstrated the usefulness of
fertility in probability estimation: IBM models 3?
5 exhibit large improvements over models 1?2. On
these grounds, we include fertility to produce our
advanced model,
pad(eI1, aI1|fJ1 )=pbs(eI1, aI1|fJ1 )
J?
j=1
p(?j |f jj?n) (2)
where ?j is the fertility of source word fj in the
sentence pair < fJ1 , eI1 > and pbs is the basic
model defined in Eq. 1. In order to avoid prob-
lems of data sparsity, we bin fertility into three
types, a) zero, if ? = 0; b) single, if ? = 1;
and c) multiple, if ? > 1.
We draw the fertility variables from a hierarchi-
cal PYP distribution, using three levels of backoff,
?j |f jj?1 ? G
?
fjj?1
G?fjj?1
? PYP(a?3 , b?3 , G?fj )
G?fj ? PYP(a
?
2 , b
?
2 , G?)
G? ? PYP(a?1 , b?1 , G?0 )
G?0 ? U(
1
3)
where we condition the fertility of each word to-
ken on the token to its left, which we drop during
the first stage of backoff to simple word-based fer-
tility. The last level of backoff further generalises
to a shared fertility across all words. In this way
we gain the benefits of local context on fertility,
while including more general levels to allow wider
applicability.
4 Gibbs Sampling
To train the model, we use Gibbs sampling, a
Markov Chain Monte Carlo (MCMC) technique
for posterior inference. Specifically we seek to
infer the latent sequence of translation decisions
given a corpus of sentence pairs. Given the struc-
ture of our model, a word alignment uniquely
specifies the translation decisions and the se-
quence follows the order of the target sentence left
to right. Our Gibbs sampler operates by sampling
an update to the alignment of each target word
in the corpus. It visits each sentence pair in the
corpus in a random order and resamples the align-
ments for each target position as follows. First we
discard the alignment to the current target word
and decrement the counts of all factors affected
by this alignment in their top level distributions
(which will percolate down to the lower restau-
rants). Next we calculate posterior probabilities
for all possible alignment to this target word based
on the table occupancies in the hPYP. Finally we
draw an alignment and increment the table counts
for the translation decisions affected by the new
alignment.
More specifically, we consider sampling from
Equation 2 with n = 2. When changing the align-
ment to a target word ei from j? to j, the fin-
ish, jump and emission for three target positions
i, i+ 1, i+ 2 and fertility for two source positions
j, j? may be affected. This leads to the following
337
decrement increment
?(no | null, ?ll, Je, I) ?(no | null, ?ll, Je, I)
?(no | p..s, take, null, ?ll) ?(no | Je, take, null, ?ll)
?(no | le, that, p..s, take) ?(no | le, that, Je, take)
?(f | null, ?ll, Je, I) ?(s| null, ?ll, Je, I)
?(b | p..s, take, null, ?ll) ?(m| Je, take, null, ?ll)
?(s | le, that, p..s, take) ?(s| le, that, Je, take)
e(take |f , p..s, null, ?ll, Je, I) e(take |s, Je, null, ?ll, Je, I)
e(that |b, le, p..s, take, null, ?ll) e(that |m, le, Je, take, null, ?ll)
e(one |s, le, le, that, p..s, take) e(one |s, le, le, that, Je, take)
?(single | p..s, le) ?(multiple | Je, <s>)
Table 1: The count update when changing the
aligned source word of take from prends to Je in
Figure 1. Key: f?forward s?stay b?backward m?
monotone p..s?prends.
posterior probability
p(ai = j|t?i,o?i) ?
i+2?
l=i
p(?l)p(?l)p(el)
? p(?j + 1)p(?j? ? 1)p(?j)p(?j?)
(3)
where ?j , ?j? are the fertilities before changing the
link and for brevity we omit the conditioning con-
texts. For example, in Figure 1, we sample for
target word take and change the aligned source
word from prends to Je, then the items for which
we need to decrement and increment the counts by
one are shown in Table 1 and the posterior prob-
ability corresponding to the new alignment is the
product of the hierarchical PYP probabilities of all
increment items divided by the probability of the
fertility of prends being single.
Maintaining the current state of the hPYP as
events are incremented and decremented is non-
trivial and the naive approach requires significant
book-keeping and has poor runtime behaviour. For
this we adopt the approach of Blunsom et al
(2009b), who present a method for maintaining
table counts without needing to record the table
assignments for each translation decision. Briefly,
this algorithm samples the table assignment during
the increment and decrement operations, which is
then used to maintain aggregate table statistics.
This can be done efficiently and without the need
for explicit table assignment tracking.
4.1 Hyperparameter Inference
In our model, we treat all hyper-parameters
{(ax, bx), x ? (?, ?, e, ?)} as latent random vari-
ables rather than fixed parameters. This means our
model is parameter free, and requires no user inter-
vention when adapting to different data sets. For
the discount parameter, we employ a uniform Beta
distribution ax ? Beta(1, 1) while for the strength
parameter, we employ a vague Gamma distribu-
tion bx ? Gamma(10, 0.1). All restaurants in
the same level share the same hyper-prior and the
hyper-parameters for all levels are resampled us-
ing slice sampling (Johnson and Goldwater, 2009)
every 10 iterations.
4.2 Parallel Implementation
As mentioned above, the hierarchical PYP takes
into consideration a rich history to evaluate the
probabilities of translation decisions. But this
leads to difficulties when applying the model to
large data sets, particularly in terms of tracking
the table and customer counts. We apply the tech-
nique from Blunsom et al (2009a) of using multi-
ple processors to perform approximate Gibbs sam-
pling which they showed achieved equivalent per-
formance to the exact Gibbs sampler. Each pro-
cess performs sampling on a subset of the corpus
using local counts, and communicates changes to
these counts after each full iteration. All the count
deltas are then aggregated by each process to re-
fresh the counts at the end of each iteration. In
this way each process uses slightly ?out-of-date?
counts, but can process the data independently of
the other processes. We found that this approxi-
mation improved the runtime significantly with no
noticeable effect on accuracy.
5 Experiments
In principle our model could be directly used as a
MT decoder or as a feature in a decoder. However
in this paper we limit our focus to inducing word
alignments, i.e., by using the model to infer align-
ments which are then used in a standard phrase-
based translation pipeline. We leave full decod-
ing for later work, which we anticipate would fur-
ther improve performance by exploiting gapping
phrases and other phenomena that implicitly form
part of our model but are not represented in the
phrase-based decoder. Decoding under our model
would be straight-forward in principle, as the gen-
erative process was designed to closely parallel the
search procedure in the phrase-based model.3
Three data sets were used in the experi-
ments: two Chinese to English data sets on small
(IWSLT) and larger corpora (FBIS), and Arabic
3However the reverse translation probability would be in-
tractable, as this does not decompose following a left-to-right
generation order in the target language.
338
to English translation. Our experiments seek to
test how the model compares to a GIZA++ base-
line, quantifies the effect of each factor in the
probabilistic model (i.e., jump, fertility), and the
effect of different initialisations of the sampler.
We present results on translation quality and word
alignment.
5.1 Data Setup
The Markov order of our model in all experiments
was set to n = 2, as shown in Equation 2. For each
data set, Gibbs sampling was performed on the
training set in each direction (source-to-target and
target-to-source), initialized using GIZA++.4 We
used the grow heuristic to combine the GIZA++
alignments in both directions (Koehn et al, 2003),
which we then intersect with the predictions of
GIZA++ in the relevant translation direction. This
initialisation setup gave the best results (we com-
pare other initialisations in ?5.2). The two Gibbs
samplers were ?burned in? for the first 1000 it-
erations, after which we ran a further 500 itera-
tions selecting every 50th sample. A phrase ta-
ble was constructed using these 10 sets of multi-
ple alignments after combining each pair of direc-
tional alignments using the grow-diag-final heuris-
tic. Using multiple samples in this way constitutes
Monte Carlo averaging, which provides a better
estimate of uncertainty cf. using a single sample.5
The alignment used for the baseline results was
produced by combining bidirectional GIZA++
alignments using the grow-diag-final heuristic.
We used the Moses machine translation decoder
(Koehn et al, 2007), using the default features
and decoding settings. We compared the perfor-
mance of Moses using the alignment produced by
our model and the baseline alignment, evaluating
translation quality using BLEU (Papineni et al,
2002) with case-insensitive n-gram matching with
n = 4. We used minimum error rate training (Och,
2003) to tune the feature weights to maximise the
BLEU score on the development set.
5.2 IWSLT Corpus
The first experiments are on the IWSLT data set
for Chinese-English translation. The training data
consists of 44k sentences from the tourism and
travel domain. For the development set we use
both ASR devset 1 and 2 from IWSLT 2005, and
4All GIZA++ alignments used in our experiments were
produced by IBM model4.
5The effect on translation scores is modest, roughly
amounting to +0.2 BLEU versus using a single sample.
System Dev IWSLT05
baseline 45.78 49.98
Markov+fs+e 49.13 51.54
Markov+fs+e+j 49.68 52.55
Markov+fs+e+j+ft 51.32 53.41
Table 2: Impact of adding factors to our Markov
model, showing BLEU scores on IWSLT. Key: fs?
finish e?emission j?jump ft?fertility.
for the test set we use the IWSLT 2005 test set.
The language model is a 3-gram language model
trained using the SRILM toolkit (Stolcke, 2002)
on the English side of the training data. Because
the data set is small, we performed Gibbs sampling
on a single processor.
First we check the effect of the model factors
jump and fertility. Both emission and finish fac-
tors are indispensable to the generative translation
process, and consequently these two factors are in-
cluded in all runs. Table 2 shows translation result
for various models, including a baseline and our
Markov model with different combinations of fac-
tors. Note that even the simplest Markov model far
outperforms the GIZA++ baseline (+1.5 BLEU)
despite the baseline (IBM model 4) including a
number of advanced features (e.g., jump, fertility)
that are not present in the basic Markov model.
This improvement is a result of the Markov model
making use of rich bilingual contextual informa-
tion coupled with sophisticated backoff, as op-
posed to GIZA++ which considers much more lo-
cal events, with nothing larger than word-class bi-
grams. Our model shows large improvements as
the extra factors are included. Jump yields an im-
provement of +1 BLEU by capturing consistent re-
ordering patterns. Adding fertility results in a fur-
ther +1 BLEU point improvement. Like the IBM
models, our approach allows each source word to
produce any number of target words. This capac-
ity allows for many non-sensical alignments such
as dropping many source words, or aligning sin-
gle source words to several target words. Explic-
itly modelling fertility allows for more consistent
alignments, especially for special words such as
punctuation which usually have a fertility of one.
Next we check the stability of our model with
different initialisations. We compare different
combination techniques for merging the GIZA++
alignments: grow-diag-final (denoted as gdf ), in-
tersection and grow. Table 3 shows that the dif-
ferent initialisations have only a small effect on
339
system gdf intersection grow
baseline 49.98 48.44 50.11
our model 52.96 52.79 53.41
Table 3: Machine translation performance in
BLEU % on the IWSLT 2005 Chinese-English test
set. The Gibbs samplers were initialized with three
different alignments, shown as columns.
the results of our model. While the baseline re-
sults vary by up to 1.7 BLEU points for the differ-
ent alignments, our Markov model provided more
stable results with the biggest difference of 0.6.
Among the three initialisations, we get the best
result with the initialisation of grow. Gdf of-
ten introduces alignment links involving function
words which should instead be aligned to null. In-
tersection includes many fewer alignments, typi-
cally only between content words, and the sparsity
means that words can only have a fertility of ei-
ther 0 or 1. This leads to the initialisation being a
strong mode which is difficult to escape from dur-
ing sampling. Despite this problem, it has only
a mild negative effect on the performance of our
model, which is probably due to improvements
in the alignments for words that truly should be
dropped or aligned only to one word. Grow pro-
vides a good compromise between gdf and inter-
section, and we use this initialisation in all our
subsequent experiments.
Figure 3 shows an example comparing align-
ments produced by our model and the GIZA++
baseline, in both cases after combining the two di-
rectional models. Note that GIZA++ has linked
many function words which should be left un-
aligned, by using rare English terms as garbage
collectors. Consequently this only allows for the
extraction of few large phrase-pairs (e.g. <?
?, ?m looking for>) and prevents the extraction
of some good phrases (e.g. <?? ?? ?,
grill-type>, for ??? and ?? ?? are wrongly
aligned to ?grill-type?). In contrast, our model
better aligns the function words, such that many
more useful phrase pairs can be extracted, i.e.,
<?, ?m>,<?, looking for>,<????, grill-
type> and their combinations with neighbouring
phrase pairs.
5.3 FBIS Corpus
Theoretically, Bayesian models should out-
perform maximum likelihood approaches on small
data sets, due to their improved modelling of un-
(a) GIZA++ baseline
? ? ? ? ? ? ? ? , ?
?
? ?
?
?
?
? ?
?
?
i
'm
looking
for
a
nice
,
quiet
grill-type
restaurant
.
(b) our model
Figure 3: Comparison of an alignment inferred by
the baseline vs. our approach.
certainty. For larger datasets, however, the dif-
ference between the two techniques should nar-
row. Hence one might expect that upon moving
to larger translation datasets our gains might evap-
orate. This chain of reasoning ignores the fact that
our model is considerably richer than the baseline
IBM models, in that we model rich contextual cor-
relations between translation decisions, and con-
sequently our approach has a lower inductive bias.
For this reason our model should continue to im-
prove with more data, by inferring better estimates
of translation decision n-grams. A caveat though
is that inference by sampling becomes less effi-
cient on larger data sets due to stronger modes,
requiring more iterations for convergence.
To test whether our improvements carry over to
larger datasets, we assess the performance of our
model on the FBIS Chinese-English data set. Here
the training data consists of the non-UN portions
and non-HK Hansards portions of the NIST train-
ing corpora distributed by the LDC, totalling 303k
sentence pairs with 8m and 9.4m words of Chi-
nese and English, respectively. For the develop-
ment set we use the NIST 2002 test set, and eval-
uate performance on the test sets from NIST 2003
340
NIST02 NIST03 NIST05
baseline 33.31 30.09 29.01
our model 33.83 31.02 30.23
Table 4: Translation performance on Chinese to
English translation, showing BLEU% for models
trained on the FBIS data set.
and 2005. The language model is a 3-gram LM
trained on Xinhua portion of the Gigaword corpus
using the SRILM toolkit with modified Kneser-
Ney smoothing. As the FBIS data set is large, we
employed 3-processor MPI for each Gibbs sam-
pler, which ran in half the time compared to using
a single processor.
Table 4 shows the results on the FBIS data set.
Our model outperforms the baseline on both test
sets by about 1 BLEU. This provides evidence that
our model performs well in the large data setting,
with our rich modelling of context still proving
useful. The non-parametric nature of the model al-
lows for rich dynamic backoff behaviour such that
it can learn accurate models in both high and low
data scenarios.
5.4 Arabic English translation
Translation between Chinese and English is very
difficult, particularly due to word order differences
which are not handled well by phrase-based ap-
proaches. In contrast Arabic to English translation
needs less reordering, and phrase-based models
produce better translations. This translation task
is a good test for the generality of our approach.
Our Ar-En training data comprises several LDC
corpora,6 using the same experimental setup as in
Blunsom et al (2009a). Overall there are 276k
sentence pairs and 8.21m and 8.97m words in Ara-
bic and English, respectively. We evaluate on the
NIST test sets from 2003 and 2005, and the 2002
test set was used for MERT training.
Table 5 shows the results. On all test sets our
approach outperforms the baseline, and for the
NIST03 test set the improvement is substantial,
with a +0.74 BLEU improvement. In general
the improvements are more modest than for the
Chinese-English results above. We suggest that
this is due to the structure of Arabic-English trans-
lation better suiting the modelling assumptions be-
hind IBM model 4, particularly its bias towards
monotone translations. Consequently the addi-
6LDC2004E72, LDC2004T17, LDC2004T18,
LDC2006T02
F1% NIST02 NIST03 NIST05
baseline 64.9 57.00 48.75 48.93
our model 65.7 57.14 49.49 48.96
Table 5: Translation performance on Arabic to
English translation, showing BLEU%. Also shown
is word-alignment alignment accuracy.
tional context provided by our model is less im-
portant. Table 5 also reports alignment results on
manually aligned Ar-En sentence pairs,7 measur-
ing the F1 score for the GIZA++ baseline align-
ments and the alignment from the final sample
with our model.8 Our model outperforms the base-
line, although the improvement is modest.
6 Conclusions and Future Work
This paper proposes a word-based Markov model
of translation which correlates translation deci-
sions by conditioning on recent decisions, and
incorporates a hierarchical Pitman-Yor process
prior permitting elaborate backoff behaviour. The
model can learn sequences of translation deci-
sions, akin to phrases in standard phrase-based
models, while simultaneously learning word level
phenomena. This mechanism generalises the
concept of phrases in phrase-based MT, while
also capturing richer phenomena such as gapping
phrases in the source. Experiments show that our
model performs well both on the small and large
datasets for two different translation tasks, con-
sistently outperforming a competitive baseline. In
this paper the model was only used to infer word
alignments; in future work we intend to develop
a decoding algorithm for directly translating with
the model.
Acknowledgements
This work was supported by the EPSRC (grant
EP/I034750/1).
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009a. A Gibbs sampler for phrasal
synchronous grammar induction. In Proc. of ACL-
IJCNLP, pages 782?790.
7LDC2012T16
8Directional alignments are intersected using the grow-
diag-final heuristic.
341
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and
Mark Johnson. 2009b. A note on the implemen-
tation of hierarchical dirichlet processes. In Proc. of
ACL-IJCNLP, pages 337?340.
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263?331.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL, pages 263?270.
Josep Maria Crego, Franc?ois Yvon, and Jose? B.
Marin?o. 2011. Ncode: an open source bilingual n-
gram SMT toolkit. Prague Bull. Math. Linguistics,
96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proc. of ACL:HLT, pages
1045?1054.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Proc. of NAACL, pages 966?974.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961?968.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proc. of HLT-NAACL, pages 317?325.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proc. of NAACL, pages 39?48.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING-ACL, pages 609?
616, July.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proc. of ICSLP.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566?1581.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. of ACL, pages 985?992.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proc. of ACL, pages 856?864.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. of COLING, pages 836?841.
342
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 151?159,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Factored Markov Translation with Robust Modeling
Yang Feng
?
Trevor Cohn
?
Xinkai Du
?
Information Sciences Institue
?
Computing and Information Systems
Computer Science Department The University of Melbourne
University of Southern California VIC 3010 Australia
{yangfeng145, xinkaid}@gmail.com t.cohn@unimelb.edu.au
Abstract
Phrase-based translation models usually
memorize local translation literally and
make independent assumption between
phrases which makes it neither generalize
well on unseen data nor model sentence-
level effects between phrases. In this pa-
per we present a new method to model
correlations between phrases as a Markov
model and meanwhile employ a robust
smoothing strategy to provide better gen-
eralization. This method defines a re-
cursive estimation process and backs off
in parallel paths to infer richer structures.
Our evaluation shows an 1.1?3.2% BLEU
improvement over competitive baselines
for Chinese-English and Arabic-English
translation.
1 Introduction
Phrase-based methods to machine translation
(Koehn et al., 2003; Koehn et al., 2007) have dras-
tically improved beyond word-based approaches,
primarily by using phrase-pairs as translation
units, which can memorize local lexical con-
text and reordering patterns. However, this lit-
eral memorization mechanism makes it general-
ize poorly to unseen data. Moreover, phrase-based
models make an independent assumption, stating
that the application of phrases in a derivation is in-
dependent to each other which conflicts with the
underlying truth that the translation decisions of
phrases should be dependent on context.
There are some work aiming to solve the two
problems. Feng and Cohn (2013) propose a
word-based Markov model to integrate translation
and reordering into one model and use the so-
phisticated hierarchical Pitman-Yor process which
backs off from larger to smaller context to pro-
vide dynamic adaptive smoothing. This model
shows good generalization to unseen data while
it uses words as the translation unit which can-
not handle multiple-to-multiple links in real word
alignments. Durrani et al. (2011) and Durrani et
al. (2013) propose an operation sequence model
(OSM) which models correlations between mini-
mal translation units (MTUs) and evaluates proba-
bilities with modified Kneser-Ney smoothing. On
one hand the use of MTUs can help retain the
multiple-to-multiple alignments, on the other hand
its definition of operations where source words
and target words are bundled into one operation
makes it subjected to sparsity. The common fea-
ture of the above two methods is they both back off
in one fixed path by dropping least recent events
first which precludes some useful structures. For
the segment pairs <b?a t?a k?aol`v j`?nq`u, take it into
account> in Figure 1, the more common structure
is <b?a ... k?aol`v j`?nq`u, take ... into account>. If
we always drop the least recent events first, then
we can only learn the pattern <... t?a k?aol`v j`?nq`u,
... it into account>.
On these grounds, we propose a method with
new definition of correlations and more robust
probability modeling. This method defines a
Markov model over correlations between minimal
phrases where each is decomposed into three fac-
tors (source, target and jump). In the meantime
it employs a fancier smoothing strategy for the
Markov model which backs off by dropping mul-
tiple conditioning factors in parallel in order to
learn richer structures. Both the uses of factors
and parallel backoff give rise to robust modeling
against sparsity. In addition, modeling bilingual
information and reorderings into one model in-
stead of adding them to the linear model as sep-
arate features allows for using more sophisticated
estimation methods rather than get a loose weight
for each feature from tuning algorithms.
We compare the performance of our model with
that of the phrase-based model and the hierarchi-
cal phrase-based model on the Chinese-English
and Arabic-English NIST test sets, and get an im-
151
Figure 1: Example Chinese-English sentence pair
with word alignments shown as filled grid squares.
provement up to 3.2 BLEU points absolute.
1
2 Modelling
Our model is phrase-based and works like a
phrase-based decoder by generating target trans-
lation left to right using phrase-pairs while jump-
ing around the source sentence. For each deriva-
tion, we can easily get its minimal phrase (MPs)
sequence where MPs are ordered according to the
order of their target side. Then this sequence of
events is modeled as a Markov model and the log
probability under this Markov model is included
as an additional feature into the linear SMT model
(Och, 2003).
A MP denotes a phrase which cannot contain
other phrases. For example, in the sentence pair
in Figure 1, <b?a t?a , take it> is a phrase but not
a minimal phrase, as it contains smaller phrases
of <b?a , take> and <t?a , it>. MPs are a com-
plex event representation for sequence modelling,
and using these naively would be a poor choice
because few bigrams and trigrams will be seen
often enough for reliable estimation. In order
to reason more effectively from sparse data, we
consider more generalized representations by de-
composing MPs into their component events: the
source phrase (source
?
f ), the target phrase (tar-
get e?) and the jump distance from the preceding
MP (jump j), where the jump distance is counted
in MPs, not in words. For sparsity reasons, we
do not use the jump distance directly but instead
group it into 12 buckets:
{insert,? ?5,?4,?3,?2,?1, 0, 1, 2, 3, 4,? 5},
where the jump factor is denoted as insert when
the source side is NULL. For the sentence pair in
1
We will contribute the code to Moses.
Figure 1, the MP sequence is shown in Figure 2.
To evaluate the Markov model, we condition
each MP on the previous k ? 1 MPs and model
each of the three factors separately based on a
chain rule decomposition. Given a source sentence
f and a target translation e, the joint probability is
defined as
p(
?
e
I
1
, j
I
1
,
?
f
I
1
) =
I
?
i=1
p(e?
i
|
?
f
i
i?k+1
, j
i
i?k+1
, e?
i?1
i?k+1
)
?
I
?
i=1
p(
?
f
i
|
?
f
i?1
i?k+1
, j
i
i?k+1
, e?
i?1
i?k+1
)
?
I
?
i=1
p(j
i
|
?
f
i?1
i?k+1
, j
i?1
i?k+1
, e?
i?1
i?k+1
)
(1)
where
?
f
i
, e?
i
and j
i
are the factors of MP
i
,
?
f
I
1
=
(
?
f
1
,
?
f
2
, . . . ,
?
f
I
) is the sequence of source MPs,
?
e
I
1
= (e?
1
, e?
2
, . . . , e?
I
) is the sequence of tar-
get MPs, and j
I
1
= (j
1
, j
2
, . . . , j
I
) is the vec-
tor of jump distance between MP
i?1
and MP
i
, or
insert for MPs with null source sides.
2
To eval-
uate each of the k-gram models, we use modified
Keneser-Ney smoothing to back off from larger
context to smaller context recursively.
In summary, adding the Markov model into the
decoder involves two passes: 1) training a model
over the MP sequences extracted from a word
aligned parallel corpus; and 2) calculating the
probability of the Markov model for each trans-
lation hypothesis during decoding. This Markov
model is combined with a standard phrase-based
model
3
(Koehn et al., 2007) and used as an addi-
tional feature in the linear model.
In what follows, we will describe how to estati-
mate the k-gram Markov model, focusing on back-
off (?2.1) and smoothing (?2.2).
2.1 Parallel Backoff
Backoff is a technique used in language model ?
when estimating a higher-order gram, instead of
using the raw occurrence count, only a portion is
used and the remainder is computed using a lower-
order model in which one of the context factors
2
Note that factors at indices 0,?1, . . . ,?(k ? 1) are set
to a sentinel value to denote the start of sentence.
3
The phrase-based model considers larger phrase-pairs
than just MPs, while our Markov model consider only MPs.
As each phrase-pair is composed of a sequence of MPs un-
der fixed word alignment, by keeping the word alignment for
each phrase, a decoder derivation unambiguously specifies
the MP sequence for scoring under our Markov model.
152
index sentence pair minimal phrase sequence
w?om?en y
?
ingg?ai b?a t?a y?e k?aol`v j`?nq`u jump source target
1 We T
1
1 w?om?en We
2 should T
2
1 y
?
ingg?ai should
3 also T
3
3 y?e also
4 take T
4
-2 b?a take
5 it T
5
1 t?a it
6 into account T
6
2 k?aol`v j`?nq`u into account
Figure 2: The minimal phrase sequence T
1
, ..., T
6
extracted from the sentence pair in Figure 1.
step 3-gram e?
3
|
?
f
3
, j
3
, e?
2
,
?
f
2
, j
2
, e?
1
,
?
f
1
, j
1
0 into account | k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2
? 1
1 into account | k?aol`v j`?nq`u, 2, it, t?a, ?, take, b?a, -2
? t?a
2 into account | k?aol`v j`?nq`u, 2, it, ?, ?, take, b?a, -2
? it
3 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, b?a, -2
? -2
4 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, b?a, ?
? b?a
5 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, take, ?, ?
? take
6 into account | k?aol`v j`?nq`u, 2, ?, ?, ?, ?, ?, ?
? 2
7 into account | k?aol`v j`?nq`u, ?, ?, ?, ?, ?, ?, ?
? k?aol`v j`?nq`u
8 into account | ?, ?, ?, ?, ?, ?, ?, ?
Figure 3: One backoff path for the 3-gram in
Equation 2. The symbols besides each arrow mean
the current factor to drop; ??? is a placeholder for
factors which can take any value.
is dropped. Here the probabilities of the lower-
order which is used to construct the higher-order is
called the backoff probability of the higher-order
gram. Different from standard language models
which drop the least recent words first, we em-
ploy a different backoff strategy which considers
all possible backoff paths. Taking as an example
the 3-gram T
4
T
5
T
6
in Figure 2, when estimating
the probability of the target factor
p(into account | k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2 ) ,
(2)
Figure 4: The backoff graph for the 3-gram model
of the target factor. The symbol beside each arrow
is the factor to drop.
we consider two backoff paths: path
1
drops the
factors in the order -2, b?a, take, 1, t?a, it, 2,
k?aol`v j`?nq`u; path
2
uses order 1, t?a, it, -2, b?a,
take, 2, k?aol`v j`?nq`u. Figure 3 shows the backoff
process for path
2
. In this example with two back-
off paths, the backoff probability g is estimated as
g(into acc.|c) =
1
2
p(into acc.|c
?
)+
1
2
p(into acc.|c
??
) ,
where c =< k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, -2 >,
c
?
=< k?aol`v j`?nq`u, 2, it, t?a, 1, take, b?a, ? > and
c
??
=< k?aol`v j`?nq`u, 2, it, t?a, ?, take, b?a, -2 >.
Formally, we use the notion of backoff graph to
define the recursive backoff process of a k-gram
153
and denote as nodes the k-gram and the lower-
order grams generated by the backoff. Once one
node occurs in the training data fewer than ? times,
then estimates are calculated by backing off to the
nodes in the next lower level where one factor is
dropped (denoted using the placeholder ? in Fig-
ure 4). One node can have one or several candidate
backoff nodes. In the latter case, the backoff prob-
ability is defined as the average of the probabilities
of the backoff nodes in the next lower level.
We define the backoff process for the 3-gram
model predicting the target factor, e?
3
, as illustrated
in Figure 4. The top level is the full 3-gram, from
which we derive two backoff paths by dropping
factors from contextual events, one at a time. For-
mally, the backoff strategy is to drop the previ-
ous two MPs one by one while for each MP the
dropping routine is first the jump factor, then the
source factor and final the target factor. Each step
on the path corresponds to dropping an individ-
ual contextual factor from the context. The paths
converge when only the third MP left, then the
backoff proceeds by dropping the jump action, j
3
,
then finally the source phrase,
?
f
3
. The paths B-
D-F-H-J and C-E-G-I-K show all the possible or-
derings (corresponding to c
??
and c
?
, respectively)
for dropping the two previous MPs. The exam-
ple backoff in Figure 3 corresponds the path A-
B-D-F-H-J-L-M-N in Figure 4, shown as heavier
lines. When generizing to the k-gram for target
p(e?
k
|
?
f
k
1
, j
k
1
, e?
k?1
1
), the backoff strategy is to first
drop the previous k-1 MPs one by one (for each
MP, still drops in the order of jump, source and
target), then the kth jump factor and finally the kth
source factor. According to the strategy, the top
node has k-1 nodes to back off to and for the node
e?
k
|
?
f
k
2
, j
k
2
, e?
k?1
2
where only the factors of MP
1
are
dropped, there are k-2 nodes to back off to.
2.2 Probability Estimation
We adopt the technique used in factor language
models (Bilmes and Kirchhoff, 2003; Kirchhoff et
al., 2007) to estimate the probability of a k-gram
p(e?
i
|c) where c =
?
f
i
i?k+1
, j
i
i?k+1
, e?
?1
i?k+1
. Ac-
cording to the definition of backoff, only when the
count of the k-gram exceeds some given threshold,
its maximum-likelihood estimate, p
ML
(e?
k
|c) =
N(e?
k
,c)
N(c) is used, where N(?) is the count of an
event and/or context. Otherwise, only a portion of
p
ML
(e?
k
|c) is used and the remainder is constructed
from a lower-level (by dropping a factor). In or-
der to ensure valid probability estimates, i.e. sums
to unity, probability mass needs to be ?stolen?
from the higher level and given to the lower level.
Hence, the whole definition is
p(e?
i
|c) =
{
d
N(e?
i
,c)pml(e?i|c) if N(e?i, c) > ?k
?(c)g(e?
i
, c) otherwise
(3)
where d
N(e?
i
,c) is a discount parameter which re-
serves probability from the maximum-likelihood
estimate for backoff smoothing at the next lower-
level, and we estimate d
N(e?
i
,c) using modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1996); ?
k
is the threshold for
the count of the k-gram, ?(c) is the backoff weight
used to make sure the entire distribution still sums
to unity,
?(c) =
1?
?
e?:N(e?,c)>?
k
d
N(e?,c)pML(e?|c)
?
e?:N(e?,c)??
k
g(e?, c)
,
and g(e?
i
, c) is the backoff probability which we
estimate by averaging over the nodes in the next
lower level,
g(e?
i
, c) =
1
?
?
c?
p(e?
i
|c
?
) ,
where ? is the number of nodes to back off, c
?
is
the lower-level context after dropping one factor
from c.
The k-gram for the source and jump factors are
estimated in the same way, using the same backoff
semantics.
4
Note (3) is applied independently to
each of the three models, so the use of backoff may
differ in each case.
3 Discussion
As a part of the backoff process our method
can introduce gaps in estimating rule probabili-
ties; these backoff patterns often bear close re-
semblance to SCFG productions in the hierarchi-
cal phrase-based model (Chiang, 2007). For ex-
ample, in step 0 in Figure 3, as all the jump factors
are present, this encodes the full ordering of the
MPs and gives rise to the aligned MP pairs shown
in Figure 5 (a). Note that an X
1
placeholder is
included to ensure the jump distance from the pre-
vious MP to the MP <b?a, take> is -2. The ap-
proximate SCFG production for the MP pairs is
<b?a t?a X
1
k?aol`v j`?nq`u, X
1
take it into account>.
4
Although there are fewer final steps, L-M-N in Fig. 4,
as we assume the MP is generated in the order jump, source
phrase then target phrase in a chain rule decomposition.
154
Figure 5: Approximate SCFG patterns for step 0,
3 of Figure 3. X is a non-terminal which can only
be rewritten by one MP. ? and ? ? ? denote gaps
introduced by the left-to-right decoding algorithm
and ? can only cover one MP while ? ? ? can
cover zero or more MPs.
In step 1, as the jump factor 1 is dropped, we do
not know the orientation between b?a and t?a. How-
ever several jump distances are known: from X
1
to b?a is distance -2 and t?a to k?aol`v j`?nq`u is 2. In
this case, the source side can be
b?a t?a X
1
k?aol`v j`?nq`u,
b?a ? X
1
? ? ? t?a ? k?aol`v j`?nq`u,
t?a b?a k?aol`v j`?nq`u X
1
,
t?a ? k?aol`v j`?nq`u ? ? ? b
?a ? X
1
,
where X and ? can only hold one MP while ? ? ?
can cover zero or more MPs. In step 3 after drop-
ping t?a and it, we introduce a gap X
2
as shown in
Figure 5 (b).
From above, we can see that our model has two
kinds of gaps: 1) in the source due to the left-to-
right target ordering (such as the ? in step 3); and
2) in the target, arising from backoff (such as the
X
2
in step 3). Accordingly our model supports
rules than cannot be represented by a 2-SCFG
(e.g., step 3 in Figure 5 requires a 4-SCFG). In
contrast, the hierarchical phrase-based model al-
lows only 2-SCFG as each production can rewrite
as a maximum of two nonterminals. On the other
hand, our approach does not enforce a valid hier-
archically nested derivation which is the case for
Chiang?s approach.
4 Related Work
The method introduced in this paper uses fac-
tors defined in the same manner as in Feng and
Cohn (2013), but the two methods are quite differ-
ent. That method (Feng and Cohn, 2013) is word-
based and under the frame of Bayesian model
while this method is MP-based and uses a sim-
pler Kneser-Ney smoothing method. Durrani et
al. (2013) also present a Markov model based on
MPs (they call minimal translation units) and fur-
ther define operation sequence over MPs which
are taken as the events in the Markov model. For
the probability estimation, they use Kneser-Ney
smoothing with a single backoff path. Different
from operation sequence, our method gives a neat
definition of factors which uses jump distance di-
rectly and avoids the bundle of source words and
target words like in their method, and hence miti-
gates sparsity. Moreover, the use of parallel back-
off infers richer structures and provides robust
modeling.
There are several other work focusing on mod-
eling bilingual information into a Markov model.
Crego et al. (2011) develop a bilingual language
model which incorporates words in the source and
target languages to predict the next unit, and use
it as a feature in a translation system. This line
of work was extended by Le et al. (2012) who de-
velop a novel estimation algorithm based around
discriminative projection into continuous spaces.
Neither work includes the jump distance, and nor
155
do they consider dynamic strategies for estimating
k-gram probabilities.
Galley and Manning (2010) propose a method
to introduce discontinuous phrases into the phrase-
based model. It makes use of the decoding mecha-
nism of the phrase-based model which jumps over
the source words and hence can hold discontin-
uous phrases naturally. However, their method
doesn?t touch the correlations between phrases and
probability modeling which are the key points we
focus on.
5 Experiments
We design experiments to first compare our
method with the phrase-based model (PB), the op-
eration sequence model (OSM) and the hierarchi-
cal phrase-based model (HPB), then we present
several experiments to test:
1. how each of the factors in our model and par-
allel backoff affect overall performance;
2. how the language model order affects the rel-
ative gains, in order to test if we are just learn-
ing a high order LM, or something more use-
ful;
3. how the Markov model interplay with the
distortion and lexical reordering models of
Moses, and are they complemenatary;
4. whether using MPs as translation units is bet-
ter in our approach than the simpler tactic of
using only word pairs.
5.1 Data Setup
We consider two language pairs: Chinese-English
and Arabic-English. The Chinese-English paral-
lel training data is made up of the non-UN por-
tions and non-HK Hansards portions of the NIST
training corpora, distributed by the LDC, having
1,658k sentence pairs with 40m and 44m Chinese
and English words. We used the NIST 02 test set
as the development set and evaluated performance
on the test sets from NIST 03 and 05.
For the Arabic-English task, the training data
comprises several LDC corpora,
5
including 276k
sentence pairs and 8.21m and 8.97m words in Ara-
bic and English, respectively. We evaluated on the
NIST test sets from 2003 and 2005, and the NIST
02 test set was used for parameter tuning.
On both cases, we used the factor language
model module (Kirchhoff et al., 2007) of the
SRILM toolkit (Stolcke, 2002) to train a Markov
5
LDC2004E72, LDC2004T17, LDC2004T18,
LDC2006T02
model with the order = 3 over the MP sequences.
6
The threshold count of backoff for all nodes was
? = 2.
We aligned the training data sets by first using
GIZA++ toolkit (Och and Ney, 2003) to produce
word alignments on both directions and then com-
bining them with the diag-final-and heuristic. All
experiments used a 5-gram language model which
was trained on the Xinhua portion of the GIGA-
WORD corpus using the SRILM toolkit. Transla-
tion performance was evaluated using BLEU (Pa-
pineni et al., 2002) with case-insensitive n ? 4-
grams. We used minimum error rate training (Och,
2003) to tune the feature weights to maximize the
BLEU score on the development set.
We used Moses for PB and Moses-chart for
HPB with the configuration as follows. For both,
max-phrase-length=7, ttable-limit
7
=20, stack-
size=50 and max-pop-limit=500; For Moses,
search-algorithm=1 and distortion-limit=6; For
Moses-chart, search-algorithm=3 and max-char-
span
8
=20 for Moses-chart. We used both the dis-
tortion model and the lexical reordering model for
Moses (denoted as Moses-l) except in ?5.5 we only
used the distortion model (denoted as Moses-d).
We implemented the OSM according to Durrani
et al. (2013) and used the same configuration with
Moses-l. For our method we used the same config-
uration as Moses-l but adding an additional feature
of the Markov model over MPs.
5.2 Performance Comparison
We first give the results of performance compar-
ison. Here we add another system (denoted as
Moses-l+trgLM): Moses-l together with the target
language model trained on the training data set,
using the same configuration with Moses-l. This
system is used to test whether our model gains im-
provement just for using additional information on
the training set. We use the open tool of Clark et
al. (2011) to control for optimizer stability and test
statistical significance.
The results are shown in Tables 1 and 2. The
two language pairs we used are quite different:
Chinese has a much bigger word order differ-
ence c.f. English than does Arabic. The results
show that our system can outperform the baseline
6
We only employed MPs with the length ? 3. If a MP had
more than 3 words on either side, we omitted the alignment
links to the first target word of this MP and extracted MPs
according to the new alignment.
7
The maximum number of lexical rules for each source
span.
8
The maximum span on the source a rule can cover.
156
System NIST 02 (dev) NIST 03 NIST 05
Moses-l 36.0 32.8 32.0
Moses-chart 36.9 33.6 32.6
Moses-l+trgLM 36.4 33.9 32.9
OSM 36.6 34.0 33.1
our model 37.9 36.0 35.1
Table 1: BLEU % scores on the Chinese-English
data set.
System NIST 02 (dev) NIST 03 NIST 05
Moses-l 60.4 52.0 52.8
Moses-chart 60.7 51.8 52.4
Moses-l+trgLM 60.8 52.6 53.3
OSM 61.1 52.9 53.4
our model 62.2 53.6 53.9
Table 2: BLEU % scores on the Arabic-English
data set.
systems significantly (with p < 0.005) on both
language pairs, nevertheless, the improvement on
Chinese-English is bigger. The big improvement
over Moses-l+trgLM proves that the better perfor-
mance of our model does not solely comes from
the use of the training data. And the gain over
OSM means our definition of factors gives a better
handling to sparsity. We also notice that HPB does
not give a higher BLEU score on Arabic-English
than PB. The main difference between HPB and
PB is that HPB employs gapped rules, so this re-
sult suggests that gaps are detrimental for Arabic-
English translation. In ?5.3, we experimentally
validate this claim with our Markov model.
5.3 Impact of Factors and Parallel Backoff
We now seek to test the contribution of target,
jump, source factors, as well as the parallel back-
off technique in terms of BLEU score. We
performed experiments on both Chinese-English
and Arabic-English to test whether the contri-
bution was related to language pairs. We de-
signed the experiments as follows. We first
trained a 3-gram Markov model only over tar-
get factors, p(
?
e
I
1
|
?
f
I
1
) =
?
I
i=1
p(e?
i
|e?
i?1
i?2
), de-
noted +t. Then we added the jump fac-
tor (+t+j), such that we now considering
both target and jump events, p(
?
e
I
1
,
?
j
I
1
|
?
f
I
1
) =
?
I
i=1
p(e?
i
|
?
j
i
i?2
, e?
i?1
i?2
)p(
?
j
i
|
?
j
i?1
i?2
, e?
i?1
i?2
). Next we
added the source factor (+t+j+s) such that now all
three factors are included from Equation 1. For
the above three Markov models we used simple
least-recent backoff (akin to a standard language
model), and consequently these methods cannot
represent gaps in the target. Finally, we trained an-
System Chinese-English Arabic-English
NIST 02 NIST 03 NIST 02 NIST 03
Moses-l 36.0 32.8 60.4 52.0
+t 36.3 33.8 60.9 52.4
+t+j 37.1 34.7 62.1 53.4
+t+j+s 37.6 34.8 62.5 53.9
+t+j+s+p 37.9 36.0 62.2 53.6
Table 3: The impact of factors and parallel back-
off. Key: t?target, j?jump, s?source, p?parallel
backoff.
System 2gram 3gram 4gram 5gram 6gram
Moses-l 27.2 32.4 33.0 32.8 33.2
our method 31.6 34.0 35.8 36.0 36.2
Table 4: The impact of the order of the standard
language models.
other Markov model by introducing parallel back-
off to the third one as described in ?2.1. Each
of the four Markov model approaches are imple-
mented as adding an additional feature, respec-
tively, into the Moses-l baseline.
The results are shown in Table 3. Observe that
adding each factor results in near uniform per-
formance improvements on both language pairs.
The jump factor gives big improvements of about
1% BLEU in both language pairs. However when
using parallel backoff, the performance improves
greatly for Chinese-English but degrades slightly
on Arabic-English. The reason may be parallel
backoff is used to encode common structures to
capture the different word ordering between Chi-
nese and English while for Arabic-English there
are fewer consistent reordering patterns. This is
also consistent with the results in Table 1 and 2
where HPB gets a little bit lower BLEU scores.
5.4 Impact of LM order
Our system resembles a language model in com-
mon use in SMT systems, in that it uses a Markov
model over target words, among other factors.
This raises the question of whether its improve-
ments are due to it functioning as a target language
model. Our experiments use order k = 3 over MP
sequences and each MP can have at most 3 words.
Therefore the model could in principle memorize
9-grams, although usually MPs are much smaller.
To test whether our improvements are from using
a higher-order language model or other reasons,
we evaluate our system and the baseline system
with a range of LMs of different order. If we can
get consistent improvements over the baseline for
157
System NIST 02 (dev) NIST 03
Moses-d 35.1 31.3
Moses-l 36.0 32.8
Moses-d+M 36.4 34.8
Moses-l+M 37.9 36.0
Table 5: Comparison between our Markov model
(denoted as M) and the lexical reordering model
of Moses.
both small and large n, this suggests it?s not the
long context that plays the key role but is other
information we have learned (e.g., jumps or rich
structures).
Table 4 shows the results of using standard lan-
guage models with orders 2 ? 6 in Moses-l and
our method. We can see that language model or-
der is very important. When we increase the order
from 2 to 4, the BLEU scores for both systems in-
creases drastically, but levels off for 4-gram and
larger. Note that our system outperforms Moses-l
by 4.4, 1.6, 2.8, 3.2 and 3.0 BLEU points, respec-
tively. The large gain for 2-grams is likely due to
the model behaving like a LM, however the fact
that consistent gains are still realized for higher
k suggests that the approach brings considerable
complementary information, i.e., it is doing much
more than simply language modelling.
5.5 Comparison with Lexical Reordering
Our Markov model learns a joint model of jump,
source and target factors and this is similar to the
lexical reordering model of Moses (Koehn et al.,
2007), which learns general orientations of pairs
of adjacent phrases (classed as monotone, swap or
other). Our method is more complex, by learning
explicit jump distances, while also using broader
context. Here we compare the two methods, and
test whether our approach is complementary by re-
alizing gains over the lexicalized reordering base-
line. We test this hypothesis by comparing the
results of Moses with its simple distortion model
(Moses-d), then with both simple distortion and
lexicalized reordering (Moses-l), and then with our
Markov model (denoted as Moses-d+M or Moses-
l+M, for both baselines respectively).
The results are shown in Table 5. Comparing
the results of Moses-l and Moses-d, we can see that
the lexical reordering model outperforms the dis-
tortion model by a margin of 1.5% BLEU. Com-
paring Moses-d+M with Moses-l, our Markov
model provides further improvements of 2.0%
System NIST 02 (dev) NIST 03
Moses-l 36.0 32.8
Moses-l+word 36.9 34.0
Moses-l+MP 37.6 34.8
Table 6: Comparison between the MP-based
Markov model and the word-based Markov model.
BLEU. Our approach does much more than model
reordering, so it is unlikely that this improvement
is solely due to being better a model of distor-
tion. This is underscored by the final result in
Table 5, for combining lexicalized distortion with
our model (Moses-l+M) which gives the highest
BLEU score, yielding another 1.2% increase.
5.6 Comparison with Word-based Markov
Our approach uses minimal phrases as its basic
unit of translation, in order to preserve the many-
to-many links found from the word alignments.
However we now seek to assess the impact of the
choice of these basic units, considering instead a
simpler word-based setting which retains only 1-
to-1 links in a Markov model. To do this, we
processed target words left-to-right and for tar-
get words with multiple links, we only retained
the link which had the highest lexical translation
probability. Then we trained a 3-gram word-based
Markov model which backs off by dropping the
factors of the least recent word pairs in the order of
first jump then source then target. This model was
included as a feature in the Moses-l baseline (de-
noted as Moses-l+word), which we compared to a
system using a MP-based Markov model backing
off in the same way (denoted as Moses-l+MP).
According to the results in Table 6, using MPs
leads to better performance. Surprisingly even
the word based method outperforms the baseline.
This points to inadequate phrase-pair features in
the baseline, which can be more robustly esti-
mated using a Markov decomposition. In addition
to allowing for advanced smoothing, the Markov
model can be considered to tile phrases over one
another (each k-gram overlaps k?1 others) rather
than enforcing a single segmentation as is done in
the PB and HPB approaches. Fox (2002) states
that phrases tend to move as a whole during re-
ordering, i.e., breaking MPs into words opens the
possibility of making more reordering errors. We
could easily use larger phrase pairs as the basic
unit, such as the phrases used during decoding.
However, doing this involves a hard segmentation
158
and would exacerbate issues of data sparsity.
6 Conclusions
In this paper we try to give a solution to the prob-
lems in phrase-based models, including weak gen-
eralization to unseen data and negligence of cor-
relations between phrases. Our solution is to de-
fine a Markov model over minimal phrases so as
to model translation conditioned on context and
meanwhile use a fancy smoothing technique to
learn richer structures such that can be applied to
unseen data. Our method further decomposes each
minimal phrase into three factors and operates in
the unit of factors in the backoff process to provide
a more robust modeling.
In our experiments, we prove that our defini-
tion of factored Markov model provides comple-
mentary information to lexicalized reordering and
high order language models and the use of paral-
lel backoff infers richer structures even those out
of the reach of 2-SCFG and hence brings big per-
formance improvements. Overall our approach
gives significant improvements over strong base-
lines, giving consistent improvements of between
1.1 and 3.2 BLEU points on large scale Chinese-
English and Arabic-English evaluations.
7 Acknowledges
The first author is supported by DARPA BOLT,
contract HR0011-12-C-0014. The second au-
thor is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105). Thank the anonymous reviews for
their insightful comments.
References
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proc. of HLT-NAACL.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proc. of ACL, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL-HLT, pages 176?
181.
Josep Maria Crego, Franc?ois Yvon, and Jos?e B.
Mari?no. 2011. Ncode: an open source bilingual
n-gram smt toolkit. Prague Bull. Math. Linguistics,
96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proc. of ACL-HLT, pages
1045?1054, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proc. of NAACL, pages 1?11.
Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In Proc. of ACL, pages 333?
342.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304?
311, July.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Proc. of NAACL, pages 966?974.
Katrin Kirchhoff, Jeff Bilmes, and Kevin Duh. 2007.
Factored language models tutorial.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume 1, pages 181?184.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Ses-
sion.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proc. of NAACL, pages 39?48.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
159
